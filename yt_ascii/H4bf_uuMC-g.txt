today we're going to be talking about big data. how big is big? so well, first of all, there is no precise definition as a rule. so kind of be standard what people would say is when we can no longer reasonably deal with the data using traditional methods so that we kind of think what's a traditional method? well, it might be can we process the data on a single computer? can we store the data on a single computer? and if we can't then we're probably dealing with big data, so you need to have new methods in order to be able to handle and process this data as computers getting faster and bigger capacities and more memory and things that the concept of what becomes big is is changing, right? so kind of but a lot of it isn't really as i'll talk about later isn't how much power you can get in a single computer it's more how we can use multiple computers to split the data up process everything and then throw it back like in the mapreduce framework then we talked about the for in with big data there's something called the five es which kind of defines some features and problems that are common amongst any big data things we have the five es and the first three that were defined. i think these were defined in 2001 so that's kind of how having talked about four. so first of all, we've got some volume. so this is the most obvious one it's just simply how large the dataset it's the second one is velocity so a lot of the time these days huge amounts of data are being generated in a very short amount of time so you think of how much data facebook is generating people liking stuff people uploading content that's happening constantly all throughout the day the amount of data they generate every day it's just huge basically so they need to process that in real time and the third one is variety traditionally the data we would have and we would store it in a traditional single database. it would be in a very structured format so you've got columns and rows everywhere. he would have values for the columns these days we've got data coming in in a lot of different formats so as well as the traditional kind of structured data, we have unstructured data so you've got stuff coming like web dream cliques, we've got like social media likes coming in we've got stuff like images and audio and video so we need to be able to handle all these different types of data and extract what we need from them and the first one is value yeah, so there's no point in us collecting huge amounts of data and then doing nothing with it so we want to know what we want to obtain from the data and then think of ways to go about that so something some form of value could just be getting humans to understand what is happening in that data. so for example if you have a fleet of lorries they will all have telematics sensors in that we collecting sensor data of what the lawyers are doing so it's of a lot of value to the fleet manager to then be able to easily visualize huge amounts of data coming in and see what it's happening. so as well as processing and storing this stuff we also want to be able to visualize it and show it humans in an easily understandable format oh, the value stuff is just finding patterns machine learning algorithms from all of this data see then the fifth and final one is veracity this is basically how trustworthy the data is how reliable it is so we've got data coming in from a lot of different sources so is it being generated with statistical bias? are there missing values if we use think for example the sensor data, we need to realize that maybe the sensors are faulty they're giving slightly off readings so it's important to understand how? reliable the data we're looking at is and so these are kind of the five standard features of big data some people try and add more. there's another seven v's a big data at the 10 meter producer i see. i'm sure we will keep going up and up they are doing things like don't like vulnerability. so obviously when we're storing a lot of data a lot of that is quite personal data so making sure that's secure but these are the kind of the five main ones the first thing the big big data obviously is just the sheer volume so one way of dealing with this is to split the data across multiple computers so you could think okay. so we've got too much data to fit on one machine. we'll just get a more powerful computer we'll get more cpu power. we'll get larger memory that very quickly becomes quite difficult to manage because every time you need to scale it up again because you've got even more data you to buy computer or new hardware so what tends to happen instead and all like they see all companies or just have like a cluster of computers? so rather than a single machine they'll have say a massive mean warehouse basically if you wind loads and loads and loads of computers and what this means that we can do is we can do distributed storage so each of those machines will store a portion of the data and then we can also do the computation split across those machines rather than having one computer going through? i know a billion database records you can have each computer going through a thousand of those database records let me take a really naive way of saying right. ok, let's do it. alphabetically, i'll load more records. come in for say zed that's easy. stick it on the end load more records coming for p. this y in the middle, right? how do you manage that? and so there's computing frameworks that will help with this so for example, if you're storing data industry to fashion than this the hadoop distributed file system and that will manage kind of the cluster resources where the files are stored and those frameworks will also provide fault tolerance and reliability so if one of the nose goes down, then it you've not lost that data. there will have been some replication across other nodes so that yeah losing a single node isn't going to cause you a lot of problems and what using a cluster also allows you to do is whenever you want to scale it up all you do is just add more computers into the network and you're done and you can get by on relatively cheap hardware rather than having to keep buying a new supercomputer in a big data system there tends to be a pretty standard workflow so the first thing you would want to do is have a measure to ingest the data remember, we've got a huge variety of data coming in. it's all coming in from different sources so we need a way to kind of aggregators and move it on to further down the pipeline so there's some frameworks for this. there's an apache capra and like apache flume for example and loads and loads of others as well so basically aggregate all the data push it on to the rest of the system so then the second thing that you probably want to do is store that data so like we just spoke about the distributed file system you store is in a distributed manner across the cluster then you want to process this data and you may skip out storage entirely so in some cases you may not want to store your data you just want to process it use it to update some machine learning model somewhere and then discard it and we don't care about long-term storage so you're processing the data again do it in disputed fashion using frameworks such as mapreduce or apache spark designing the algorithms to do that processing requires a little bit more thought than maybe doing a traditional algorithm with the frameworks we'll hide some of it but you need to be thinking that even if we're doing it through a framework we've still got data on different computers if we need to share messages between these computers during the computation it becomes quite expensive if we keep moving a lot of data across the network so it's designing algorithms that limit data movement around and it's the principle of data locality so you want to keep the computation close to the data? don't move the data around sometimes it's unavoidable, but we limit it. so the other thing about processing is that there's different ways of doing it there's batch processing so you already have all of your data or whatever you protected so far you take all of that data across the cluster you process all of that get your results and you're done the other thing we can do is real-time processing. so again because the velocity of the data is coming in we don't want to constantly have to take all the day to detective well produce it get results and then we've got a ton more data i want to do the same get all the data bring it back process all of it so instead we would do real-time processing so as each data item arrives? we process that we don't have to look at all the data we've got so far. we just incrementally process everything and that's coming up in another video when we talk about data streaming so the other thing that you might want to do before processing is something called pre-processing remember i talked about unstructured data so maybe getting that data into a format that we specifically can use for the purpose we want to so that would be a stage in the pipeline before processing the other thing with huge amounts of data there's likely to be a lot of noise a lot of outliers so we can remove those we can also remove one instances, so if you think we're getting a ton of instances in and we want them she learning algorithm there'll be a lot of instances that are very very similar see an instance is say in a database it's like a single line in the database. so for htv sensor reading it would be everything for that lorry at that point in time cs speed directions traveling reducing. the number of instances is about reducing the granularity so part of it is saying if we store a rather than storing data for a continuous period of time so every minute for an hour if those states are very similar across that we can just say okay for this period this is what happens and put it in a single line or we could say for example a machine learning algorithm if there's instances with very very similar features and then a very very similar class we can take a single one of those instances and that will suitably represent all of those instances so we can very very quickly reduce a huge data set down to a much smaller one by saying there's a lot of redundancy here and we don't need a hundred very similar instances when we one would do just as well so if you've got a hundred instances and you reduce it down to one is does not have an impact on how important those instances are in the scheme of things yes, so techniques that deal with this stuff. some of them would just purely say okay now this is a single instance and that's all you ever know others of them would have yet have a waiting? so some way of saying this is a more important one because it's very similar to 100 others that we got rid of this one's really not as important because there are least three others that were similar to it so we can wait instances to kind of reflect their importance. there are specific frameworks with big data streaming as well so there's technologies such as the spark streaming module' for apache' spark or there's newer ones such as apache plink that can be used to do that. so they kind of abstracts away from the streaming aspects of it so you can focus just in what you want to do a little thinking all this data is coming through very fast, obviously my limited brain is thinking streaming relates to video. but you're talking about just data that is happening in real time. is that right? yes, so going back to the lori's as they're driving down the motorway. they may be sending out a sense of read every minute or so and that since the reading goes back we get all the sense readings from all the lorries coming in as a data stream so it's kind of a very quick roundup of the basics of big data and there's a lot of applications this obviously so thanks, we'll have huge volumes of transaction data that you can extract patterns of value from that and see what is normal they can do kind of fraud detection on that again. the previous example of fleet managers understanding what is going on basically any industry will now have ways of being able to extract value from the data that they have so in the next video we're going to talk about data stream processing and more about how we actually deal with the problems that we all time data can presenters over very very large bios this kind of computation is a lot more efficient if you can distribute at because doing this map phase of saying, okay this is one occurrence. the letter a that's independent of anything else and see most interested in you're probably only interested when a button is pressed or so on the only times positive