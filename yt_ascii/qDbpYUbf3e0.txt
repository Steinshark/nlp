well today, i want to talk data mining which is what i'm really interested in and i want to explain a little bit about the inner workings of data mining a little bit of the sort of terms that you might have heard when you read - the first lecture or the first book i want to talk about supervised learning, unsupervised learning, what exactly are these things, and then i want to get on to something new semi-supervised learning and also what's the research at the moment in this area? it's called machine learning that's the sort of applied artificial [intelligence] machine learning if you get a data you want to mine the data and broadly there's kind of two categories of methods how this works, so if [i] could pull up my prop. yes, i've carefully prepared here are some items of data that i have brought along the first method may be that i should explain is unsupervised learning because it perhaps the easier way, it's called unsupervised learning because we don't have any examples that are labeled, so it's an unlabeled learning yeah i guess the idea is a  supervisor knows the answer and we don't have anybody who knows the answer so we get the data to begin with and we don't really know anything about it we know obviously the attributes. we know the values, but we don't know what categories are they let's say that's a problem so unsupervised learning very often is just sorting off the data so unsupervised learning very often is just sorting of the data so you get your first date item and you put it somewhere and then comes another data item and you basically go let's do colors is this similar or is this different and now this is quite different. we put it there and then comes another date item. oh it's this similar or is it different it's a little bit similar to the yellow ones so we'll put it a little bit closer to the yellow one and then comes another data item and no this is obviously quite similar to the yellow one so we put it closer to here and then so over time you get all these data items in and they might end up a bit like something maybe a bit like that so what have i done? i've done a sorting of the data and the approach i've done is something based on similarity measures these unsupervised methods they all use the similarity measure in this case i've done kind of by color the other way these methods usually work is to actually start out by saying but how many groups would? you like your data to be in how many clusters would you like it to be in? so let's say you want them in three clusters well, then maybe solution might look like this, it's clustered in by the color three clusters if there would have been four clusters maybe the solution would have looked like this and if there was maybe two clusters it might even looked like this, so you might ask okay? so so what's the data mining about the sorting of the data well? once we sorted the data in this way. we can of course have a look at all so what ended up together maybe these things have ended up together? and maybe now we can say oh, this is the light colors. this is the dark colors, and we certainly have two groups i mean we wouldn't normally sort color cubes you would  sometimes saw patients and are they really ill or are they very ill and you know that sort of thing we could sort about this now most of the unsupervised method spoke exactly like i described to the worker by sorting it the differences that [had] [a] measure the difference between things so is it a statistical similarity is it a algebraic similarities that your metric measure you can imagine or so many ways you can measure the difference between things unsupervised learning is sort of quite a simple way of doing it i mean, it's quite quick the algorithms, but it's not as powerful as other methods what's the problem with it? one of the problems with it is actually quite straightforward let's say we end up with this solution. well, is this  a good solution, or it's not a good solution it's actually really hard. it's really hard to evaluate because we obviously don't know about the data we don't know so we're looking at it going which looks okay? but maybe not and then very often what happens actually if you look at the data from one way it looks like a good solution, but now i do my reveal we sort of turn the data a bit and you know suddenly we have another angle on the data and like actually now. it's a mess they're not really sorted variable at them or are they well often what happens? that's often what happens with unsupervised learning you sort them in one way, and they look quite good but then we look at the data differently and actually this hasn't quite worked and it's not so great the other downside with unsupervised learning is the algorithms really only work when you tell them how many groups you want to data to be in two groups, three groups, four groups for some problems you might notice maybe you have like i say ill patients and healthy patients and you know there is two groups but very often actually how many groups you have is the whole question so you can't really use these methods that well, if you want to know some technical terms kmeans for example, it's a classic unsupervised method that's very popular. so if you can look it up, you'll learn a bit more about it now... second way of doing learning would be the supervised way we said unsupervised there must be that must be a supervised way. here the difference is that you have some data which has some answers attached to it already so you can learn from it from this data really learn from it and a classic way of doing it is [them] well well neural networks forms one of the best-known ones. how does that work, okay? well? so have some date again, and this time let's say we want to do something a bit different we want to just sort them in light colors and dark colors for example and what happens is i get my data in and already somebody has labeled the data for me they said these are light colors, these are dark colors so we already know the answer for this data we don't know it for some other data, but we know it for this this is our training data and now i'm going to do a new learning neural network the first data item comes in it goes here the next occurred item comes in and goes here and i keep doing this and maybe i end up with something that looks like this and now of course i can assess the quality of the solution and go... oh well algorithm, you've done okay, but you haven't done it really well because these two should be over there this one should really be there fix the function a bit and do it again [okay] back and we might end up like this. it's like okay,  that was better but he's still got one wrong fix the function again and do it against this called back propagation neural network and we'll do this again and of course if you do this long enough eventually the algorithm will learn the perfect function how to sort things and then the idea is a new data item comes along and it will go to the same function and because the function is now perfect it you will end up exactly the right place no problem and then ah and then no problem so it's supervised because we have labels and because [of] labels we can assess the quality and in neural networks it's the classic way of doing this and in general supervised learning is very powerful because as long as we have enough data with enough labels, we can always learn the function, and then it should work really well but well there wouldn't be research if we're finished with it so there's obviously a problem with this as well. the problem with this is that it can lead to overfitting what does overfitting mean? means like tight jeans you know. no, not that. it means that you have too much emphasis on getting the function right you make it too right. so the function is absolutely perfect in fact it's so perfect, it's brittle it's it's it's just not good anymore so what happens is a new data item comes along one that you've not seen before i got one and the unsupervised method wouldn't have a problem with this because it just goes by similarity and we'll go it's kind of a light color you probably end up here but a supervised method has never seen this color before and the function goes like what do i do with this and it pftttt breaks or it puts you just at a random place like maybe here so supervised learning is really good but if you overdo it, then you've overfitted and the problem is that you actually make the system worse again. you made it brittle the other downside of supervised learning is you must actually have enough data with labels which for some problems you have it's fine but for some problems, you don't really have it, so let's talk about a practical problem that i was working on so i was working with doctors in a hospital clinicians who look after colon cancer patients and they took many years to collect the data of about 500 patients of classic medical data so we've got age, critical medical history we've got genetic values, blood values, and so on and so on and so on and they get diagnose the different categories of illness some more serious some less serious and the doctors wanted some help with this categorization the most serious cases and the least serious cases they're quite clear, but it's just this whole group in the middle and i wanted to make sure can we split them a bit better and so we were working with this with them, and so this is a classic problem and in that case there was 500 patients that were already categorized as in what category of illness they were in so actually a supervised approach was really good because we could learn from those 500 and build up a picture and as long as we're careful to not overdo it we'll be fine but then what actually happened and  this leads me on to what my research is at the moment what happened is.... not for all the 500 patients did they have all the labels because some of the technology has been changing over the years so there's more modern things now  that i didn't have ten years ago so actually for the last 50 patients they had some additional labels that i didn't have for all the others and so we were talking about what to do with this and there's a method called semi-supervised learning which is kind of what the research is on why can we take the best of both worlds and maybe combine it a bit so what if you've got a few labels? it's not enough to learn perfectly, but maybe we can do something so what we've done is a semi-supervised method and it's kind of a mixture of the two you get your data and let's just say we want to split them in light and dark colors it's basically our more serious patients and our less serious patients and you might end up sorting the data something like that because it's an unsupervised approach first of all we don't know exactly how good this is but then for some of the data items we have a label and we can look up what's the number on them and because for some of you have a label now we can say okay all the ones have the same label or with a similar label are there in the same group so suddenly we can assess the quality of this so we don't have a label for all of these, but have a label for some of them are they in the same group yes, and then the same labels are in the same group yeah that looks like a good solution semi-supervised learning is probably the future because as data sets get bigger and bigger and bigger you don't have labels anymore for everything because nobody has time to label everything and computers can't really label things very well so you'll have the experts labeling a few things and semi-supervised learning will be where this is going but then the next step really would be to have it interactive that would be even better so that's kind of what we're working on right now. it's called a man in the loop or human in the loop learning where you maybe have no labels at all or maybe just very few and then you do some sorting of the data and then we asked the expert has the sorting gone well? has it not gone well? well, what about this one item, what would be the label you would give it and it sort of a bit interactive and i think they'll be much better because then you can you know there is [more] in real time and you can actually also latest developments can come in tacit knowledge that you might not even have in the data so that's like spot checking? yeah, exactly it's like spot checking it and but then putting that knowledge back into the algorithm so the algorithm can learn from it again and it's a sort of reinforce a bit that's a single-car. that's basically controlling the robot twice 864 processes. which is more than a robot will usually get. where are we going now? i'll show you the big machine. that's it.