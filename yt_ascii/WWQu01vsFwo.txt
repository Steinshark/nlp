regular listeners will know i'm a bit of a fan of apache kafka i think it's a great system for storing data for processing it and just for shipping it between different parts of an organization at scale it's great if moving data from a to b is your job and so often it is and in software terms kafka is a very mature project it's been around for over a decade it's got a large stable user base it's got a large stable committer base who are actively moving it forwards and it's a backbone piece of infrastructure for a lot of big companies so in one sense cfas arrived and i got really intrigued when i heard about a company that took a look at apache kafka and said let's keep the protocols the same but we'll do a complete rewrite of the implementation in c++ that's a bold move for any company why what's their motivation what's so important that you want to take on a project of that size and maturity and what are you actually rewriting are you rewriting the entire stack or just parts of it are you being completely compatible or are you deciding to make tasteful breaking changes and most of all what do you think you can do differently this time around now the whole industry is older and wiser lots of questions to ask so today we're talking to christina lynn of red panda red panda that upstart company taking on an apache staple and i've seen in the wild i've seen people using a mixture of kafka and reed panda in their tech teams so clearly red panda have added something to the software world let's see what they've got to add the conversation i'm your host chris jenkins this is developer voices and today's voice is christina [music] lynn [music] christina lynn thanks for joining us hi chris how are you i'm very well i'm glad to see you here how are you doing h it's my pleasure to be here i'm doing i'm doing great great i've got i've got many questions for you because we share we kind of share a background you are currently head of developer relations for a red p for red panda yes i was heavily into developer relations for a cfa related company so on two sides of the same wall right i love that i love that i guess we can have really good conversation then yeah yeah yeah so let's start off with with the big issue which is okay red panda kafka similar space but why do you even want to be in this space yeah i can kind of talk a little bit about it from my perspective as my career evolved so i was doing a lot of java programming back in the days right so and then i started a a position in a insurance company and at the time we're doing a lot of like soa and a lot of work and you know when it's soa it's a lot of like system integrations and ser services integrations and that is when i get into well we started using a lot of like ibm con like ibm solutions but it becomes very clumsy so and that's when i started doing a lot of like apachi camo so that's how i get into the demo space so i was doing a lot of data integration and at the time i also was working for j boss and that was why i got into redhead doing a lot of j boss work plus i got to do a lot of my like very like lovely camo project as well with the camo crew and all that so it was great a lot of system integrations and i kind of see why like a lot of the need of you know having data integrated and we were using messaging q at the back right for for soa integrations and all that so messaging q was i was heavily using messaging cues from ibm mq to you know active mqs and reb mqs and all that kind of stuff and that fits in naturally when cfa came out we were like awesome this is a a lot faster you know streaming services for us to kind of quickly get data in and out like it was able we get a lot get a lot of i throughputs for that so we love that and so that's why i kind of work and caml was kind of introducing kafka as part of as ecosystem so that's why i got into kafka and kind of using that as the backbone for system integrations to pass through to build that you know microservices backbones and stuff like that and then i when i was looking for other solutions and i was kind of looking into quarcus and we were kind of introducing it was kind of at the time where we're using quarcus and i saw this like really tiny container because cauas was introducing this like really tiny container so people doesn't have to spin up the whole like you know backand services you can developer can quickly run things and i saw this red panda solution i was like what is this thing why is it because at the time kka still needs you know zookeepers and you know all that you know brokers and all that i was like why do i need why why do i don't need all that kind of stuff to actually start developing so that's why i started looking into red panda and that's why i got into this space of you know kaa and rependa i kind of i hope that answers your question but that's how my how i learn about the whole data thing and really like the data aspect of things so i'm slowly getting back into the data engineerings and learned how data engineers were using you know the streaming platforms and i think there's a lot of potentials for future development as well so that's kind of how i got in that yeah yeah that's a st the shape of a story i've heard a lot it starts with desperately wanting to ship data between different systems and then grows right yep and often as much as we talk about real-time data it's it's just sheer the effort of connecting data from a to b yes is the killer use case yes so from what i know about red panda like you talk about smaller container sizes that's one of their big pictures right compared to apache cfa red panda is kind of going chasing the size performance deployment ease umit i think when when repa first got out it was very well known for its performance and theze of its container and how much like resource it needs to adopt right so i think that is i think that's why people start to adopt red pandai just because of the simplicities and the size of it needs to execute the similar stuff right so that's kind of how i see it and how people first got into it but it's it's a curious road they've taken to get there to do okay to look at something like kfar and say i'm assuming this is how it originated you look at something like kafker and you say i would like this to be smaller and faster i'll rewrite the whole thing in c++ right i think well this question should go directly to our ceo our founder because he actually wrote the entire project but when i ask him about this question because i think he was very well into the low latency streaming rpc at the time when he was working for concord and he was doing a lot of like low latency streaming development at the time so for him i think when he was starting to develop this application he was facing a lot of unpredictabilities and you know a lot of the the things when when they have higher traffic throughputs things crash or when you get a machine that's able to handle the load it is super expensive so i think as a developer he's trying to figure out a way to kind of become more efficient on processing the data and getting data streamed so that's why i think he started to look into ai kfka because he was solving that problem and then he was able to find a way to kind of develop things and i think he didn't first start with c++ actually asking like why did you choose c++ and he was like no actually i started with rust okay right and then i think at the time when he was developing things the rust wasn't mature enough i guess it is now but at the time it wasn't so he has to go back to his favorite c++ he was developing a lot of c++ program already and i think it was one of the the framework in c++ actually solve a lot of problems um in c++ that actually solve a lot of problems for for for us right it's the c sear framework yeah sear framework is a very unique one because it allows a lot of the par asynchronous programming inside your computer because if you think about your computer your computer is is consists of cpu and cpu is split into many cores right yeah and and how a lot of the a lot of the programs were developed they were developed in a where that you know a lot of things were taken you were kind of use op operating system to kind of make things a lot more flexible so it lose a lot of the control over how the cpu works and how the memory works right and i think kafka was developed in java i think java is a great program i am a java developer for okay two decades now i love java and i think it's a great program but i still think there's a benefit of different tools for different for different purpose and i think in terms of controlling the hardwares c+ c++ is a better language for that because of how you can kind of use utilize course so instead of you know remaining relying all the me memory management for jvms relying on your operating system to do contact switching for your cpus the sear framework allows us to control everything so you know so we can we can kind of allocate the amount of memory per core so this core will control this part of a memory and that fits great with the cfa nature because you know remember how ca works it has a lot of petitions and then you're writing logs in into each partitions so basically we can assign multiple partitions to a single core and this core will just take care of all the things in this petition and this would avoid contact switching because i think a lot of people has this misunderstanding of what is causing the latencies of writing data into my system right a lot of people think the problem was the disk but it is not anymore because we're not in a very old dis space where we have one single like you know like little pin head we're trying to kind of write data into yeah we're no longer spinning a magnetic d exactly we're not spending anymore we're working working with ssds with mvme architectures meaning that i can write multiple times into the disk and having them having all this data allocated in our disk in very different place because how the file structure works as you know the xf xfs sorry about my pronunciation is allows you to parallely grab your data quickly from your your disk and then put it back to the memory so in terms of that your dis is no longer the problem your cpu time is the problem and being able to utilize cpu allows us to quickly grab the data and provide it to the users and and vice versa doing the same thing to quickly put it back into the storage and that's why mak r panda super fast with that sear and i think alex saw that and that's why he implementing everything with the sear framework and that's why it's the secret behind why repen i super fast in terms of you know performances and you know the lower latencies u low tail latencies and all that kind of stuff okay what what kind of year are we talking about for this this whole i think it was three four years ago maybe really that recently yeah okay okay i know some people will be disappointed that you didn't stick with rust three or four years ago but i mean and i think it was just at the time maybe a little bit earlier i don't i didn't exactly know when alex want was implementing ross maybe it's a little bit earlier but yeah this is from what i know okay fair enough then then there's the question of like okay if you decided you're going to rewrite cfa do you know why it's decided to stick to the exact protocol because you can as i understand it you can drop in red panda and largely have your writers and readers your producer producers and consumers transparently work with either yeah it's just because i think the number of users we don't want to because i think from ring zero which is the broker side and then you've got the client side right so i think when alex wants to do it he wants to kind of also support that ecosystem for the cfa users so that's why we choose to adapt the kfka apis instead of inventing our own so we can provide that faster ring zero experience but also providing that for the existing kfka users so that's something that we try to solve it's similar i always put that as analogy of cars right car is a very good thing where can hop on it driving it and then you can get to the places but how you build a car is different so over the times we have really nice diesel cars petrol cars and then now we have electronic cars i think it's just how you build the cars a little bit different but the way you drive it we wanted to we want the user to have the same experiences and kind of how things are built but internally how it's done is it is up to the the car manufacturer right so no matter how they implement it you've got pretty much the same user interface on the car right exactly yes except i can never find where the heated mirrors are online always in the wrong place yes okay i i can see i can see that the that you want to keep the same protocol to like reach the same user space right exactly yes what where do you draw the lines around that like how much of wider cfa do you want to rewrite in c++ is it just broker are you targeting kafka connect next for instance what what's what's the scope of the project currently i think what we're going towards is more on the again the ring zero experience right so not not actually the connector itself but the actual broker we wanted to kind of make because rependa is a very small company compared to the large community out there for kafka and even k is a multi-million dollar company where they have a lot of money to throw around but you know r is very small so we want to make sure that we invest things in the right place so we want to make sure that we give people the best experience from we can how we can kind of control so we focus on the ring zero experience on that so we don't expect to expand a lot outside of the connectors we expect to work with the connectors we work with the community so for example frco which is the go library for connecting to kafka was developed by one of our engineers he was kind of donating to the community and you know kind of show people you know how this is done and we we were talking to them we're we're try to we we try not to be the enemy of of the community we try to be friends because we all work in the same we're trying to make this space better right so giving people more choices this is what i think we should be and how the community should work all together so i think we should so so to answer your questions i think for us is more focusing on the broker itself so things on top of that i think we're you know we added the the the reped penda console which allows us to kind of have a very nice gooey interface for developers for them to see what's going on ins i know there's a lot of like you know projects out there that can do the same thing but i think it's for us just to kind make it more more smooth transition from you know people to see what's going on and maybe use that on top of even kava well and all that kind of stuff that's where i think diplomatically i would say there's plenty of room for more development in the cfa user interface yeah i think you know like you know different opinion sparks different ideas and i think that's a good energy to the community not just you know if if you all have people speaking the same thing then you're not going to have innovations yeah do you know it makes it reminds me i was at another company talking to some people that worked there last year and they used kafar in production but all their developers locally were using red panda that's the first time i heard about red panda and i want to ask you why do you think they would have that divide you know i think we're seeing the same thing like from me from the community side i i see the same thing i see a lot of developers were kind of using red panda internally as a development and they have like kafka because their operation staff kind of decided on using kafka i think it's just the easiness of how to use rena how to start it i think you know adopting kafka from as a beginner i don't think it's the most easiest thing first you have to understand java and a lot of the the people i talk to right now are python developers go developers rust developers i don't for them like java is intimidating and then to download the package and then at the time you you still have to start the zookeeper and then you start the you know the the broker it takes for them a lot of they just it's harder to get around so for them to start repand i one single binary they start it up and they can start working i think for them it's easier and then the footprint is a lot smaller compared to what it's taking up so i think that's one they want to do even quaas well i i think i mentioned that the first time i heard about is quaas and when they were doing the the death suite it was when they the death suite is a way that for developers to not not care about you know setting up the environment that need to work on it would just work for them and you know the death suets can has reped pandai in there speaking that because they when i i actually talk to the people that adopted they say it's smaller in footprint it's easier kind of to get it started so i think that's why and i was working i was working on a a project a test container i don't know i don't know if you heard about test containers but i think they're a little bit pretty well used in spring brute community right because they were using that for you know testing and local development as well they have i i've seen their download rate for compared to kafka and repanda i think repanda is pretty good in terms of usage people can just download it and use that as their inter internal development platform so they don't have to get all that things started so i think that also contribute to that do you i mean that i got to push you a bit on that cuz that that landscape must be changing a bit where kafka's moving away from having a separate consensus protocol of zookeeper to it being internal meanwhile you've got red panda i mean i'm guessing if you want to do something really interesting with it you end up wanting to bring in kafka connect or maybe kafka streams do you think that single binary argument is still valid or is it eroding i think it's still valid right i mean for me i so the the developers i talked to to doesn't really use the connect that much they are using python to develop and connect to k car so i don't see that happening and c car streams ca stream this is just an api you put on top of the broker whatever broker that you're using so i don't think that's going to be a determined issue for using kka at all right and kafka connect and and since i'm am i am a camo developer a very longtime camo developer vel opers i write my own camo components so for me that is independent whatever i put underneath the hood doesn't really matter i'm writing the integration on the top so i for me i don't think that's a big issue for for that and especially now i don't i i'm starting le to hear less and less about kavka streams i think more people are going to flink yeah there been a lot of push yeah for the past two years it's all about flink every every time when i do a flink course you know people just come in and they want to hear about flink right so i think it's the push of flink that kind of added benefit to us because you're kind of free from you know embedding a particular solution for that streaming streaming services right so i don't think that's a big problem for us so from that point of view presumably you're looking at flink as a as a good thing competitively yeah fre you up from having to compete with like higher level infrastructure stuff exactly and think i think see that yeah because and i think a lot of people were adopting flink because the sql nature and then i think ksql was introduced it was introd it was a good idea but i don't think it was there were problems with ky cor right so i think you know fln kind of solves that extra layers problem for yeah gna try and keep my knowledge in this space and my bias is out of it but i i wanted ksql to succeed more than it seems to have done it's just say that i liked it a lot i would have liked to have seen it continue to a glorious future that i don't think is going to happen now sadly but this this raises the question of how are you seeing people use red panda in i mean a lot of python developers you can tell us a bit about oaml when you talk to users what are they doing with all this so currently our major customer base are more performance oriented that's the beginning like if you look into most of our customer base right now i'm seeing a lot of users that they were they were they were having problems with cfa basically with the performances and the number of machines they need to start up and a lot of management problems and i think was the past past two or three years i think building up all that so they were kind of switching to rep pena that's the most of the use use case i see for now but now i'm starting to see a lot more since we added the the byoc offering from from cloud so i think we kind of convert a lot of the people that wanted to have a quick cluster running on their own cloud account you know we kind of have that and so i'm starting to see a lot more less experienced c before it was all very experienced like kka users so we got really hard question to solve like how do i get like you know number of performances like latency numbers you know up for for less hardware usage and all that so that was a lot of problems we had to solve but now we're seeing a lot more you know like in the mid mid journey range of people where they're trying to get into cfa but they know how to do it and kind of we help them to do all that kind of stuff okay so you're seeing more people just casually moving over from be starting their journey still with kafka i think most of them when they come to us they would know a little bit about kafka well i' see one or two exceptions but they're very little right compared to the majority of the people i see they're still they still know kafka but they want to experiment with other things so i think they're just looking for the second opinion and you know different options okay for these typ of users my loyalty here is to the ideas and the architectures rather than specific implementation so i've got no skin in the game on that one okay so when you said you you have a go client this is another thing i wanted to look into so you said people are using people are intimidated by java but this is c++ i can't believe the general programmer is less or more intimidated by c++ than java you've got go as a main client what what's how are people interacting with red panda language-wise and how are you supporting that what development are you doing in that world for us i think we kind of just that go france go project was kind of donated as a open source project right so anybody wants to use that even connecting it to kafka we're happy to do so right so that's something that we do but our main focus are still in the in the broker space where we're developing a lot of like auto rebalancing leader leadership rebalancing and all that kind of stuff and then we are also i don't know we're also working on something called wasum i don't know if you heard about our wasm project web assembly yeah yes yeah our web assembly project is something that we're working on so we're trying to cu we' seen this thing where people are building you know very simple data pipelines where they're only doing very simple transformation stuff or validations or very simple you know data conversion val masking and it needs to have a lot of data ping pong because you need to get it out from a socket and then put it back in right so this is something that people do and we were thinking if we can have this in the broker that the broker do all the processing so before it just it reads out from the memory does its thing and then put it back in the memory so there's no huge roundtrip between the networks and all that that would make things a lot faster so can we have this buil into the broker and without you know having external things going on and the idea of bringing bringing web assembly was because web web assembly is a very flexible engine we can use that to compile different type of languages like you know rust with go and with java with python so developers can freely choose whatever language they want to use for this very simple you know transformation pipelines and put that in the broker so i think externally the flink services will be outside of of your broker which is doing a lot of like traditional complex event processing and a lot of like time window based processing internally with these you know very simple stateless you know transformations everything can be done in the broker level so you don't have to do a lot of you know data trans data transfer in the networks and we see a lot of networking costs occurred for you know people using a lot of petitions you can see replication all around so if we can kind of eliminate that that would help a lot so i could see that for like this is just splitting a comma separ separated string into a list you should you would want to do that in a more lightweight way than flink correct yes exactly l yeah i can see i can see that makes sense are you in any way enforcing a boundary for what you consider simple or complex there yes so we do so we think that you know if it's something that you need to kind of hold it state that is a complex one so for things that you don't need to kind of you don't have to keep it state then it's a simple stuff right because the reason because is when we deploy the the the data pipeline into the broker there's several brokers in your cluster so repa needs to kind of copy all your pipelines across the broker in order to get all things done inside its own little machine right so in so we can't so keeping the state of where the pipeline is is is going to be a very huge workload and you kind of have to know all the status like all the other worker status so we don't want to do that we want to make sure everything is simple and easy so we want to sure so if it's stateless it's simple then it is but but again like the the reason we're doing that is because there's a lot of people they're not utilizing their entire hardware but if you already have a very busy broker that's taking up 100% of your cpu time this is probably not a good idea so you probably need a couple of different notes for that so it all depending on situation but situations but i think it's going to eliminate a lot of like u network cost in late and season all that yeah i can see provided it's contained that's going to put you always want to separate storage and comp but once you've separated them you want them to be really close exactly right would be sitting right next to each other but still separate exactly you want to like the fastest time r time of that so okay i i could happily dive into the guts of how that works in implementing the wasm runtime engine it it's still working progress for us right now we're still developing it currently we got the go engine running for wasm but we're still working on the the python and java part of the mbe the python because majority of our users are python users but then we still have java users we want to do but i think that is a something that we're working on simply because the way that they handle memory is a little bit free compar freely compared to whates so how we write the assembly engine to make it work for internally with the c++ with our you know thread core architecture it needs time we need time time to actually get that thing work more efficiently we can just plug that in but that's that's not that is going to causes performance issues we don't want to do that so we want to make sure that we can of see we can manage all this the the memory usage and like how we treat all the core how was used in inside your computer that's something that we working on you've got that classic sandboxing problem when you let users execute arbitrary code on your machine you've got arbitrary problems right yes that's right yeah okay so so that's something that we're working on and we're also working other things like you know also not just the where the the broker part but also the cloud services part we're trying to launch our serous solutions which is like something for developers so they can kind of spin up spin up spin up a a topic where if they wanted to do that so with the more to to actually get more exposure to developers because now they they most of them were just running in locally but we wanted to have that you know presence in the cloud as well other than byc i think byc kind of does a better for us but i think we want to included in that footprint as well that connects to something else i saw about red panda because if you're doing bring your own cloud right i want to know what you if i bring my own cloud what you offer on that and also there's something about you using native object storage in red panda which i think is related here i wanted to ask you about okay yeah that would be good because i was actually i i talk about this in current 20 this year i inan current bc and actually some of the people that was managing the confence cl came to me and said you know this is an awesome idea and they were the back engineers and like they they were like we should do that i was like no don't but yeah but i think it was the way that we did it is we don't want it to be direct management we don't want like it's it's a you know a user or from red panda to go directly into the cloud because it's not safe and we don't want it to be very manual in order to because that's going to cost us a lot so we wanted to be automated safe so the way we did it is we had an agent which is a very small vm that gets deployed into the the the customer's cloud so this enclosure is customer cloud we don't have access to it they the customer needs to install the agent in their cloud and we'll be controlling everything from outside but we don't go directly into the cloud we have the agent to pull requests from our controller plane and then does things on their does all aut things on the cloud so everything is isolated and with this particular data plan it when even when things goes wrong with the control ples or whatever we want to do with with repanda your things still stay intact because you actually have your own little ecosystem of your own cluster you're working with the cluster you're working with everything and then and then the controller pre is just kind of doing all the update and all the patchings you know all that kind of stuff for you so basically we just issue a bunch of commands and an agent just pulls it it just does that on your own like cloud and just kind of fix things for you there okay so something goes wrong with that link you can't change anything but nothing stops running yeah everything running yeah i can totally see that argument yeah and i think for people what they like about it most is working inside their vpc because because that vpc is a cost right for for the traffic goes in and out so i think what people askes about is like oh i can put that in my vpc yeah that makes sense so again there seems to be kind of cultural focus on the developer experience side is that something is that something you're deliberately focusing on as kind of a competitive advantage or is it just when you're smaller that's easier to do i think we put a lot of effort in terms of user experiences there's a user experience group internally inside our company where we think about user experience a lot right so when we push out a a a new feature we want to think about how how easy is that for from a users's perspective how can i make that easy for developers we constantly make changes i think from what you're saying if the company is smaller is easier yes of course it's always easier because you don't have to go through 300 meetings in order to get one feature change right right yeah so definitely yes that's going to contribute to it but i think it's the way that we f we we design our product i think it's kind of make it so for instance the u the data rebalancing that that can be done automatically so repanda will automatically detects what's going on inside each partitions if it's gets too busy within one node will shift things around automatically for you without you knowing tell me more about that i didn't know about that so yeah of course so we do like automat data rebalancing because the way that we do things it's very different so so kafka claims that they do k raft but k raft only applies to the controller side of the story so where it used to do things in zookeeper they move it to kraft but underneath the hood kra is doing whatever zookeeper was doing and then you still got your petition your isrs and all that doing all the replications and all that but red panda underneath the hood itself we don't have a actual controller of things all the petitions forms a a raft ring so you can see a lot of rings inside red panda so there's no one single you know bottleneck where if it breaks everything breaks right so it doesn't have that bottleneck so all the petitions elic on leader leader and then you know just figure things out so that's how like how independent each one of the the petition works so in terms of that so we get a better you know fail over rate and all that kind of stuff right other than that i think we also have a lot better communications between like each noes so we know we prioritize each petitions like hey if your petition like if in this noe has too much petition we automatically you know kind of figure out which one is the less busier node and will kind of rebalance everything so you rebalance who is currently managing that partition right dynamically yes okay and is that transparent to the user do i see i don't see that on the consumer producer yeah i i you don't see that everything is done internally within the broker itself and we just tell the consumer hey the leader has changed and then this is where you gonna get it and all that kind of stuff it's very similar to thing but i think kafka is a little unique in that terms because kfka client itself is very smart kka client itself does a lot of you know you know determining where do i go and how can how sticky am i am and kind of figure it out the broker itself is is dumber compared to the messaging queue we used to have right it's it's it just does all the i know it's doing a lot but it's it's just doing the replication all that so i think a lot of it becomes the the job of the the client so the client needs to know a lot in orderers to kind of speak to that so we have to also obey that protocol from the client so we can kind of fit into all different versions of that of course we have a limit right so some of the less efficient consumers you know lgorithm will probably just not do that anymore we'll just say we only support to this versions of client and it's it's it's good for the users as well because it's more efficient so we'll do that so we'll we kind of work with that client code in order to get better balancing for both consumers leaders and data partitioning okay surely that's going to once you switch the partition leader some point that's got to trigger consumer group rebalance the client does see though yes yes so we're prior the the cause and the management of it is automatic but the client the client will will the programmer doesn't have to change anything but the client will do some work in cooperation with that rebalance yes yes and then we kind of we we kind of honor that right so you know like the the sticky corporate way of doing things we pref we prefer to kind of stick to where it was before so there's less movement and all that would we do would do the same so that's why we have priorities right so we'll shift the one that has less consumers and you kind of see which one is we them in the i can kind of send you a link of like how we prioritize it but there's a lot of like logics you need to kind of figure out like you know yeah the cpu the bandwidth and then how many consumers and like all that and then how how fast is like how and then also i ops and all that and it becomes a determined factor and then we can kind of rebalance everything do you do things like watch how long that problem lasts before you do something about it yes there's a timeout thing as well okay yeah it's interesting that makes me think so one the the way consumer group rebalancing used to work was stop the world rebalance everyone's start again right and it wasn't too long ago that they changed that to only if you're being reassigned you have to stop your work and give up and come back which i'm assuming you do but that raises the question which version of the cfa protocol you actually following and how far back do you go yeah i currently i can't really tell i i don't have a vivid number like where we go but i think we we we actually try to match the most current one actually we bring up some of i think from i've heard that from one of engineer he actually started talking to the like the client community like talking about ideas of you know how how how can it be more efficient from the client side you order to have that communication so he actually does work with the the client groups to actually get that that thing working as well so yeah but which so you're tracking the latest version of the cfa protocol are you say yes we do yes you must have a fairly large team just dedicated to doing that that's enough work in itself right i don't know exactly how many people were there but i know there's not too many okay i mean how large is the company just give me an idea from that size i think we're about 200ish 200 okay yeah in silicon valley size i don't know how many exactly because because we're still growing we're still hiring and all that so i don't think we have a lot of people right now okay so if you're growing where are you going with this you've mentioned yeah yes and then serus of course in order in order to support all the customers so we also have managed kafka of course that's already there manag rependa services that's already there byoc and then we have now doing on servus and then wasm and then we're also working on this so we already have tier storage which is i think kfka doesn't have it but conone already has it which is kind of offload the the ret the the non retain data offloaded to the the object store right so keep keep the hot stuff in ssd and the rest you push to s3 or whatever yeah ex yes so we do that and then we also added something called re remote replica which is you can rehydrate your an entire different cluster so you have your operational cluster here i think a lot of the data engineers loves that you know that implementation was they can still have the operational side of the the house of working with the current clusters but when they're trying to you know do you know backfill all the the jobs that they need to run say say they have to kind of go back to three days ago four days ago to run a long large and process a large streaming data what they can do is they can have a separate cluster which is rehydrated with historical data and they can kind of use that as as a part where they can kind of use that to run their their process and their jobs there so it's called re- remote replica which is kind of use reusing that you know f3 object store that we have to store historical data as well all right so it's not quite the same as something like cluster linking you join one cluster to another it's are you more like reading the same object storage for a different cluster exactly yeah so that that's more for dat data i think data engineers love use that a lot more for for example for the microservices user this is a lot less because i think for them is this is just a traffic control centers where they can relay their messages but for for backend you know data engineers they need a lot of historical data that wasn't store somewhere or that wasn't stored into databases or that is in data lake but it's actually easier to kind of see the see the flow of how the data was coming in the logic of data and like the historical like timestamps of it that they like to see that that way so they'll process it from there i'm curious about that why would you not just connect to the create a new consumer going further back on the existing cluster because that's going to sacrifice the performance of your current running brokers right you can also do that but then you're gna rehydrate a lot of the data from your historical data into your current broker so you're sacrificing part of your broker performances so in once you're in the teered storage world you're going to mess up that hot cash section yeah exactly right and it's going to that' be an issue yeah yeah okay and and then the huge i think what i see in the company right now is a is a split between you know operational side of the house and then the analytic side of the house i see a lot of you know i came from the operational side transactional side where you know everybody's doing microservices you know it's everything's containerized and you got micro you got kafka underneath the hood and all that but then same time i see the new side of the house where analytics where they were just picking up the streaming side of the story they were starting to learn kafka and they used to use a lot of spark they used a lot of like map ruce you know that kind of thing and they don't they're starting to kind of get the data they're starting to get real- time data because now for machine learning they have they need to kind of produce a lot of data sets and the data sets are the the the way the data science request them were kind of random i don't know like depends on how they implement it but sometimes they have to go back way back for a lot of data and they need that and also they want to have like real- time machine learning i think a lot of them were adopting that realtime machine learning so they're trying to get that thing hooked up and i think from this side there's a huge gap compared to where operational people were so full like so confident like they know how to use c cing already but here i think they're still doing that yeah yeah you kind of want to isolate yourself when you're massively experimenting right right exactly yeah i can see that okay and how about you where where as head of devil for red panda where are you taking things into the future personally so i'm still so i you know when when you say devil for a startup company i think most of the star company doesn't know what dev real is so i think i'm doing a lot of technical marketing instead of doing a lot of their real stuff because there's just this huge gap of things that needs to be filled in order to you know do a lot of before i can go off and you know start speaking because once i do that nobody's creating content and you actually need people creating technical content that speaks to the developers and the people that knows the technical stuff stuff instead of like really fuffy you know marketing messages right so yeah i'm doing a lot more of that and i want to grow the educational side of the story because i feel like there's not enough educational content for rena for instance teaching people how thical works and teaching people how how does that inte how does that what does that mean in order to maintain a repa cluster and i wanted to kind of make that easier for and i think there's not a lot enough course there's a lot of course for job developers already for using kafka but not enough for other languages so i want to like build on top of that so maybe build a better like you know educational content for for my users as well yeah yeah i can totally see that and agree with it but that spreads to you very thin yes we do have two people like in our team but it's still this we still need a lot of things to do yeah there's always more education to do okay final question and this is really like the two final questions if i want to go and use it there are two things i know need to know the first is what's the license for red panda so it's it's actually you can use it if you don't use if you don't use it production so okay h what is it i forgot the name like bs bsl bdl b bdl bs i forgot like sorry i forget which yeah i can always forgot about the the terms but yes but i think it's it it is free for use if you use for development and if you using it for production that's when you kind of needed to reach out to us and there are community features you can use freely in production there were just a few enterprise paid features that you need to reach out to us in order to kind of get it started so for instance the auto balancing part where i mentioned and then the u the tier storage the re remote replica which is the part you kind of have to you know work with the license ri and up custer okay okay and if i want to get started what do i do next well you what you need to do is go to our website and then there's a getting started with red penda that's the easiest pa and then we also have a university that teaches you how to get started with red penda oh very nice very do you get a plaque yes you get a plaque certification well dean lyn i will leave you to go off and write more courses at that point thank you very much for talking to us thank you christina thank you very much i'm going to go and check out red panda i'm going to kick the tires and see how similar it is in practice and one thing i'm definitely going to check out is the gooey i hear whispers on the grap vine that that's how certain people from the kafka world have got hooked in so let's see what it's like if you want to check it out as always i'll put links in the show notes if you've enjoyed this episode as always please do take the time to like it and share it and rate it and whatever and click subscribe if you haven't already to catch next week's episode but until next week's episode i've been your host chris jenkins this has been developer voices with christina lynn thanks for [music] listening