this is not the future of the pc this is if the enthusiast in you screams that no you can't just replace gpus and cpus with integrated socs i am here to tell you that apple silicon gober apple obviously knows it and make no mistake intel amd and nvidia all know it too the writing is on the wall and now it's just a matter of time pcs need to change and i'm going to tell you how and why right after i tell you about our sponsor glasswire glasswire lets you see past and present network activity detect malware on your pc or android device and block its connections to prevent things from getting worse use offer code linus for 25 off at the link below a lot of you really liked our how to build a pc the last guide you'll ever need and i'm really glad that you do because it took so much effort to make but look at that run time and it could have been even longer think about how many components go into a typical pc build and then look at this unassuming little box the mac studio in our testing the m1 ultra variant pulled a grand total of 223.4 watts from the wall under the heaviest load that we could give it and the m1 max version didn't even break three digits if we ignore the rosetta results they achieved 88 and 61 percent respectively of the core i9 12900k and rtx 3090 bench's overall performance in our productivity testing at just 34 and 15 of the total system power at most the 12 900k the cpu alone can draw more power than the whole m1 ultra mac studio and developers are still figuring out how to optimize for apple silicon now look at that pc and all its components again all the raw materials that go into making it what it is it's wasteful right in almost every way except economically it's not that x86 is inherently bad but it has limitations that even intel have tried to overcome themselves with projects like titanium while extensions have given it more life its complex instruction set design is at its core an old one that dates back to the late 1970s literally every x86 based cpu boots up in 8086 compatibility mode and can in theory run software made for the og ibm pc precious die space and transistors are used for supporting instructions and modes that might be redundant or not even remotely useful today but must be there to maintain compatibility with the architecture and now if i'm intel or amd looking at my options i'm sweating a little and weighing a switch to arm while neither currently sells an armed cpu both hold armed licenses and amd is on record saying that they're willing and able to do it it wasn't that long ago even that intel had their own reduced instruction set cpu called the i860 and i-960 and you'd be kidding yourself if you thought they weren't prototyping stuff on the side like apple was when they were suffering through 14 nanometer plus plus plus a change in the pc industry would of course have a teething period like apple silicon is currently experiencing but it could eventually be a good thing and not just for efficiency think about it intel and amd are alone in the cpu race right now just like amd and nvidia are currently alone in the gpu race and if intel had their way in the 90s or in 2015 there'd be only one thanks to intel's control over performance and efficiency enhancing extensions used on modern cpus like sse and avx and amd's control over the 64-bit extensions that every desktop cpu since core 2 has used we don't have any other desktop cpu manufacturers anyone entering the x86 market would have to reverse engineer those extensions to compete because amd and intel have a technology sharing agreement that forbids sub licensing and expires if either company changes hands by contrast arm doesn't sell anything directly they develop the base specifications and then they license that to other companies who then have the option of tweaking and adding to those designs to tailor them to suit their needs this is how apple silicon came to be and if arm took off on the desktop we could see not only mobile players like qualcomm samsung and mediatek enter the ring but maybe google or even hp they used to make their own cpus once too and we can't forget nvidia whose last consumer arm soc uses gpu cores from 2014 and lives on in the nintendo switch and shield tv plus actually it's kind of bizarre that nvidia hasn't continued to produce consumer socs like perhaps contractual obligations within and outside of nvidia's control had something to do with it because not only were they close to straight up buying arm a sign that they knew times were changing but they're also still actively developing arm socs just not for consumers their grace data center soc for ali acceleration has a surprisingly similar level of integration to apple silicon on paper and is just begging to be scaled down so that consumers can use it just like the tesla's and not quadro gpus do with g-force but let's back up a sec duopoly aside if x86 is so bad and arm is so good why is x86 still the king of the pc well prior to apple silicon you'd be forgiven for thinking that arm chips simply can't perform like intel and amd qualcomm has a virtual monopoly over non-apple arm pcs and a literal monopoly in the windows ecosystem thanks to an exclusivity agreement with microsoft when that deal was penned it probably did make some sense qualcomm was the third biggest chip maker even as of mid-2020 and there's still a giant who's recently been eyeing a slice of the arm pie for themselves so assuming qualcomm would become the biggest supplier of desktop arm chips it made sense for microsoft to want to optimize for that hardware unfortunately for microsoft and for us there is no polite way to put this even qualcomm's latest chips suck which recently prompted google to follow samsung and apple and going their own way with tensor and may eventually prompt microsoft to do the same qualcomm's utter failure to compete in the pc space so far has given intel amd and especially nvidia precious breathing room and we're not saying that the pc as we know it is dead today it definitely still has some life left and they'll first probably try to integrate x86 socs more tightly before they consider switching over intel for example could very well be working on an soc that combines alder lake's efficiency cores with a larger number of xc graphics cores and ai engine blocks taking the dye space that would otherwise have been allocated to the bigger performance cores even so reduced instruction set architectures proved their worth in the late 90s with early arm licenses and with power pc which was famously used in the first imax clock for clock power pc tended to outperform x86 by a wide margin though this did depend on the task and eventually powerpc hit a performance and efficiency wall just like x86 is today intel's titanium for its part was supposed to be a sort of successor to risk as a concept by using one hp termed explicitly parallel instruction computing or epic yes naming has always been bad unlike our swag it from ltdstore.com it describes exactly what it is a sweater and a jacket as we know today titanium didn't work out so well still even intel saw that the x86's days were numbered their plan for pentium 4 was more or less to just clock it as high as it would go and hope that the numbers kept going up and now that we're back to this phase again where we can't physically fit more cores on a die because new process nodes are providing less additional density something's got to give the current solution stacking dies will only get you so far when you're already pushing 241 watts by comparison a lot of arm's advantages today come not strictly from its reduced instruction set but from how it's typically bundled into a system-on-chip configuration where virtually the entire computer is super close to the cpu cores why does this matter when we already have apus it's all about density and power draw simply expanding the size of the die isn't normally a good idea from power and thermal perspective but arm cores are typically much smaller than x86 and sip far less power so not only can you fit more of them into the same space you're also not as limited in terms of die size in general m1 ultra's die area is bigger than the rtx 3090 ti let's do a case study amd's 8 core ryzen 7 5700g desktop apus are somewhere between an m1 and m1 pro in terms of die size the apu dedicates just over a quarter of the utilized die space to cpu cores and cache alone and just 10 to the gpu for thermal management there's also a fair amount of unused white space even within the cores themselves meanwhile even m1's big performance cpu cores are tiny by comparison with all cores taking just over 20 percent of that smaller die area the gpu takes up double the relative die space as the apu which means there's way more gpu horsepower on tap and still room for other functions like the massive media engine neural engine and storage controller there's very little white space by comparison not only are those arm cores efficient enough to handle such a compact layout but when so much of the system is contained within it significantly less energy has to be used for signaling which also happens to be much faster if we tried to push an x86 processor to these densities in proximity with this many gpu cores it would quickly become a thermal meltdown event sort of like the opposite of how alex had to save a frozen core i9 12900 ks literally frozen go check it out on flow plane along with more exclusive when you're done here and it's scales m1 max sports a larger die than the rtx 3070 and m1 ultra is about a third again bigger than the rtx 3090 ti with the die that large the computy bits of the 3090 ti would take up roughly half the m1 ultra's die area while apple currently dedicates just over a third that means that if apple built a dedicated big chip design and weren't using their own metal api separate from the rest of the industry they legitimately could reach high-end gpu levels of real world performance a shame that apple had to go their own way but imagine if nvidia were to build such a thing they could corner the arm desktop or notebook space very quickly if developers were on board especially it's not as simple as gluing them together obviously and that geforce power consumption is still a problem but it's also not an impossible hurdle and with the success of apple silicon despite its growing pains it's impossible for the industry to ignore the advantages to this approach you can have excellent performance coupled with incredibly low power draw on a much more open architecture that allows for bigger or more aspirational system integrators to tweak things as they see fit this potential future will suck for enthusiasts who like getting their hands dirty but the alternative is laid bare before us power targets shooting up at record pace to the point where we're looking at new connectors to help deliver all the juice this generation of hardware now is already incredibly power hungry with intel and amd squeezing higher and higher frequencies out of their cores and nvidia rumored to require up to 600 watts for their next-gen top-end gpu there's really no backing down we have to ask ourselves is it worth it and at what point does it stop being worth it at least it's always worth listening to the segways the sponsors like backblaze starting at just seven dollars a month backblaze offers an affordable and easy to use cloud backup solution they make it simple by allowing you to backup almost anything from your mac or pc and access it from anywhere in the world via their web or mobile apps backblaze even lets you restore your data by mail a hard drive with your data will be shipped right to your door and once you're done you can return the hard drive within 30 days for a full refund with over 55 billion files restored and two exabytes of data under their management backblaze has got you covered so don't be that person who forgets to back up their important files sign up and get a 15 day free trial with no credit card required today at backblaze.com ltt thanks for watching guys this one's a bit more philosophical than usual so maybe go check out something more down and dirty for a palette cleanser like our 6.9 gigahertz gpu released on six nine it's nice