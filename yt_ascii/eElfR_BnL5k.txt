so today i thought we could talk about this paper that recently came out called ai safety grid world's which is an indeed mind it's an example of something that you see quite often in science a sort of a shared data set or a shared environment or a shared problem if you imagine. i don't know you've got facebook comes up with some image classification algorithm and they can publish a paper that says we've designed this algorithm and we've trained it on our 11 billion photos and it works really well and then you know, google says oh, no, our algorithm actually works better and we've trained it on all of our google photos and its classification rate is higher or something. you're not really doing science there because they're trained on completely different datasets they're tested on different datasets. so what you need is a large high-quality shared data set then. everybody can run their stuff on so that you're actually comparing like with like so people use imagenet for that right now reinforcement learning algorithms or agents don't use datasets exactly. they have an environment. they generate data while interacting with that environment and that's what they learn from so the thing you share is the environment when deepmind did their dqn staff a while ago playing atari games? they released all of those games with any modifications that they'd made to make them interface with the network's properly and the whole software package so that if anybody else wanted to have a go and see if they could get higher scores. they had all the same stuff and up until now there hasn't been anything like that for ai safety so the paper is actually just laying out what they are there's kind of a problem in ai safety in that you're trying to build architectures which will be safe even with systems which are more powerful than the ones that we currently have. so you've got this kind of thing like we're talking about for example this robot that makes you a cup of tea and running over the baby and all of this stuff, we don't actually have a general-purpose robot like that right now that you could give an order to go and make your cup of tea and would have all the necessary understanding of the world and so on for all of that stuff to even apply. it's speculation on the other hand when we were talking about cooperative inverse reinforcement learning that paper all takes place in this extremely simplified version in which all of the agents can be sort of expressed as simple mathematical expressions. that's kind of too simple to be to learn things about actual machine learning applications and the other examples are too complicated and what we need is examples of the type of problems which can be tackled by current machine learning systems current reinforcement learning agents, but which exhibit the important? characteristics that we need for safety so what this paper does is it lays out a bunch of grid worlds? they're very popular in reinforcement learning because they're complicated enough to be interesting but simple enough to be actually tractable you have a world that's sort of just laid out in a grid. hang on let me find an example here a little bit like computer game scenarios mario right, right, but leaves are simpler than that more like snake. well life. conroy's life, right? yeah. yeah, very very similar so the thing is laid out on a grid the the world is quite small and the way that the agent interacts with the world is very simple. they just move around it basically, all they do is they say left-right up-down the example we were using before and we were talking about reinforcement learning we use pac-man like pac-man doesn't do anything except move around he's got walls he kind of moved through he's got like pills you pick up. they give you points. are they pill? no, which things are the pills in which they're yeah. well, you've got pills or pills oh, right, yeah yeah
the dots and the point, is that all of your engagement with it like when you go over one of the power pills you pick it up automatically when you go over a ghost when you're powered up you destroy it automatically you don't have to do anything apart from move and the entire environment is based on that the actions result in points for you and they also result in changes to the environment like once you roll over a dot you pick it up and it's not there anymore you've changed the world. that's the kind of thing. we're dealing with here so the idea is they've set up these environments and they've specified them precisely and they've also put the whole thing on github, which is really nice so that's why that's why i wanted to draw people's attention to this because everyone who who thinks that they've solved one of these problems they reckon oh, yeah all you have to do is this here is like a standardized thing and if you can make a thing that does it and does it properly and publish it that's a great result, you know? so i would i would recommend everyone who thinks that they have a solution or an approach that they think is promising have a go. try implementing it, you know, see what happens there are eight of them specified in this paper. and so four of them are specification problems they're situations in which your reward function is misspecified for example, like we talked about in previous video if you give the thing the reward function that only talks about getting you a cup of tea and there's something in the way like a bars. it's going to knock over. you didn't say that you cared about the bars it's not in the reward function, but it is in what you care about. it's in your performance evaluation function for this machine so anytime that those two are different then you've got a misspecified reward function and that can cause various different problems. the other ones are robustness problems, which is a different class of safety problem. they're just situations in which ai systems as they're currently designed often break so for example distributional shift is what happens when the environment that the agent is in is different in an important way from the environment it was trained in so in this example, you have to navigate through this room with some lava and they train it in one room and then they test it in a room where the lava is in a slightly different place so if you've just learned a path then you're gonna just hit the lava immediately. this happens all the time in machine learning anytime where the system is faced with a situation which is different from what it was trained for current ai systems are really bad at spotting that they're in a new situation and adjusting their confidence levels or asking for help or anything usually they apply whatever rules they've learned straightforwardly to this different situation and screw up. so that's a night course of safety issues. so that's an example here or things like safe exploration it's a problem where you have certain safety parameters that the system the train system has to stick to like say you're training a self-driving car. a lot of the behavior that you're training in is safe behavior but then you also need the system to obey those safety rules while you're training it right like so generally lately if you're doing self-driving cars, you don't just put the car on the road and tell it to learn how to drive specifically because we don't have algorithms that can explore the space of possibilities in a safe way that they're that they don't that they can learn how to behave in the environment without ever actually doing any of the things that they're not supposed to do usually with these kinds of systems they have to do it and then get the negative reward and then maybe do it like a hundred thousand more times to really cement that. that's what happens like a child learning yeah, but kids are better at this then how current machine learning systems are they just they use data way more efficiently this is a paper talking about a set of worlds if you like people doing things in those worlds yeah, so in this paper they do establish baselines basically, they say here's what happens if we take some of our best current reinforcement learning agent, you know algorithms or designs or architectures they use rainbow and a to c and they run them all nice on these problems and they have kind of graphs of how they do and generally it's not good on the left they have the reward function how well the agent does according to its own reward function and on the right there they have the actual safety performance usually in reinforcement learning. you have a reward function which is what determines the reward that the agent gets and that's what the agent is trying to maximize in this case they have the reward function and they also have a safety performance function, which is a separate function which the agent doesn't get to see and that's the thing that we're actually evaluating so if you look at something like the boat race as the system operates its learning and it gets better and better at getting more and more reward but worse at actually doing laps of the track and it's the same with pretty much all of these the current systems if you just apply them in their default way they disable their off switches, they move the box in a way that they can't move it back they behave differently if their supervisor is there or if then supervisor isn't there they fairly reliably do wrong thing it's a nice easy baseline to beat because they're dead. they're just showing the standard algorithms applied to these problems in the standard way behave unsafely wix code is an ide or integrated development environment that allows you to manage your data and create web apps with advanced functionality i've been put together this computer for our website and if you go up to code here turn on and developer tools you can see how we get the site structure on the left hand side and then all of the components start to show their tags next to the text here what's really nice? if you go over to the wix code resources, you can find down here. there's a cheat sheet so if i want to find out the tag for location for instance? if i could type i type in location up comes that or perhaps i want to perform a fetch. i can find all the details here what's powerful about wix code is it's integrated into wix so you can put together the website using all the wix tools and the layouts and the templates that they provide and then also have access to all those backend functions so click on the link in the description or go to wix calm to get started on your website today. they go right if only with ya the equivalent one for the stop button problem is the first one in the paper actually this safe interrupt ability