um now you may look at that and go that's all right it's not great but what happens when i increase the quality boom now that to me looks very very nice and the most impressive part is if we move it around look how quick it is you know granted my computer is not the best but you could not do this with a nerf in the previous video whenever i moved the camera to a new position i'd have to wait about 5 seconds for it to render a proper image this is you know pretty much instant granted as i say i haven't got the best computer but if you had the latest and the best you could get 100 fps on this and you're also running off a web server right which is going to me way slower sending images retrieving images yeah let's have a very quick look at nerf just in one minute from the previous video and then we can talk about something that's totally different which is called gussian splatting what they do is they take a series of rgb images and from those rgb images of a scene they're able to reconstruct it in 3d using a neural network so if you remember in nerf what we have is we have a scene that's in 3d and maybe we have an object that we're trying to reconstruct in our scenes we did a christmas tree in our video so sort of like this and the reason that i don't draw these things is because they're never any good like that that's actually not that's pretty good it's almost symmetrical i'm very pleased about now with nerf you have some camera viewpoint so let's say this direction here and you fire ray through your scene like this and you sample points along this aray and you ask a new network what is at that point and you say okay so in here it's transparent there's nothing interesting there but here it's green and there's something there right and if you do this through enough cameras and enough rays you can slowly build up an actual representation of your 3d objects in the neural network itself which very basically means then your see your scene which you can now render from any point is literally a small neural network it's just a trained renderer in some way which is really quite cool so this took the world by storm not well pretty recently actually years ago years ago right i mean 3 years and and we're talking about how it's already been replaced it's a bit of a bu but you know it is what it is but i mean actually i think there probably is still time to use nerf but i think that it's good to discuss these these alternatives yeah so anyway the the good thing about nerf is that your entire multi- camera scene is represented as essentially a small neural network which you can just run from any point the bad news is it tends to be pretty iffy if you have unconstrained views we saw that our christmas tree worked pretty well but some of the surrounding areas didn't work very well and it also takes a long time to render if you want to render a new picture of the scene then you're going to need to shoot rays from all the different pixels a bit like a proper r tracer sample along them and then you've got to find exactly where the the place the object starts and all this business that takes a long time even with sort of new versions of nerf that are better at doing this it's still quite a slow process yeah traditional graphics don't work this way and gaussian spatting is perhaps a little bit more close to traditional graphics yes all right so i'll i'll be quiet in a moment but what i'll just say is the first thing just to make sure everyone's aware of gussian spatting is the idea representing our scene as a series of points but our points are now not a single spec they are a small gaan right so what is a gaan well if you were looking at a two-dimensional or a one-dimensional galaxy and you'd just be looking at a graph right so you would look at something like this a normal distribution right and and you know this and it goes on the axis like this so in one dimension a g looks like this you can imagine in two dimensions it's kind of a hill and then in three dimensions it's a kind of blob which is sort of bright in the center and fades out in the in around the edges and if you dot a lot of these around a scene you could imagine you you know just small ellipsoid things but dot around the scene you could kind of start to construct an object out of those and that's kind of what it does right pretty much yeah okay okay so off off you go okay let's let's find out how it works right so let's go back to the scene of the christmas tree cuz i want to show you what this would look like see let's see if mine's any better than yours but so let's just do an outline of what it should look like what kind of true is it it's the fake one i get out of the box each year but it's never quite the same shape each time so that's an outline of what it should look like right now if you're going to represent this with a bunch of gaussians in reality these gaussians are very very small they have to be accurate and accurately represent your scene but it would look something like like that this look a little bit like the kind of triangles we see in computer exactly exactly these are just a different way of doing that so traditionally when you have like a mesh these would be a series of triangles that are all joined together to make different faces and stuff like that these are just basically a bunch of very clever circles that can stretch can change you know they have different colors opacities they look different from different angles because of the spherical harmonics of it different stuff like that but basically very clever circles so realistically it would look something like that so they're all different shapes and sizes but in reality these would be very very small so how do we get these to that position right so to start with you know i'm not going to go into massive detail about it but use strch from motion to get basic understanding of a bunch of point clouds and then you connect them these gatins in between the points so let's say you've got your edge of your tree here like that is that is that can you understand what that is is it a bit of tree is this a bit of tree let's just say it's a bit of tree it's about as well as i would do yeah so you go all right these three points here you see and so you start getting a bunch of these so these gans aren't centered on the points and they're in the average yeah the mean the average so if you were going to render that you could see it was a christmas tree wouldn't look like a very good christmas tree it would look in a way it would look a bit like you have a point cloud of a christmas tree which is that you have a spar set of points which give you the rough idea of where something is but don't look nice because there's a lot of gaps yeah it would look like something from ps1 from like the very early games which you know for someone my age is actually still quality graphics just a buck yeah so how do we improve that what you do is you first rasterize it okay this is what's different to nerf you're not doing ray marching or ray tracing you're rasterizing which is why it's so quick when you render it if you were rendering a scene using let's say triangles what you don't do is ray cast out and look at the triangles you can't you can do that but that's quite a difficult way of doing it it's it's good for photo realism but not for real time rendering yeah what you normally do is you say okay given this triangle in 3d space where would that be on the screen and let's just paint it on the pixels and then you might add shader effects or sh or or lighting or something like this but this can be done in the same way we know where our camera is we know where our gaans are so we can say well let's move that gan in front of the image and just paint it on right and given that the gans have different colors depending on where you look at them and they have let's say transparency at the edges you have to do a little bit of work about blending those colors but after that it's pretty much standard rendering yeah pretty much it's it's so the way they've done it is is very clever because they've used modern graphics techniques to speed it up but ess they're using rasterization which has been around for decades so they're using basic techniques but using modern advancements to make it quick and make it so that these gins work how would you get these gins into a good position so you have your camera here and you go right so let's say just to confirm for the viewers that camera is our viewpoint to see this yes it's our viewpoint and for because we're training it we have to have a reference image for training so this would be something in our training set that looks like that i guess you know you know so i say right i want to render this point here what you do in a nerf is you shoot out ar ray like that and you go where's in what's here what's here what's here what's here what's here what and you see how that if you're doing that for every single pixel it's just so slow well with these gs you go right what's here oh we know g one gan's there one gan's there and so you basically know right these for this confort in this situation you have two gins here you go right which one's first that one let's say it's completely opaque therefore you can just render that there and then it's just that dead simple and then of course you know you do that for every single one but that process of just going what's there oh there there there do some alpha blending do some depth testing that in itself is so much quicker than having to query every single point along your ray in the environment which is why this can do real time rendering at 100 fps while nerf it's like 0.2 fps you see how that's such a speed up and why everyone is so excited about this well you're more excited than me i think no so what i would say so cuz we're talking here about rendering i think we need to talk a little bit about how we optimize these ganss because the first initial estimate of ganss you get are not going to be that good so let's imagine that you've got some testing data where you've got a you know let's say a plant leaf or some other thing that you've got which which sort of looks like this right and you've got some gaussin that maybe kind of sort of fit it they'll be sort of like this won't they and the question is there's really a few different options you can try and make this gaan a bit smaller so it fits better you can make this ga a bit smaller you can maybe put a ga in there i mean what what different things does it do the main thing that it does is it will compare that to the reference image and go right that should be that color therefore this gon should be a little smaller a little bigger more opaque different different shades that's that's what the main thing does and that just uses gradient descent which is you know very clever they're not using massive neural networks to do this they're just using standard gradient descent to do that that's the first thing that it does and it does that very well however what they realized is that doing that you sometimes get let's say you're trying to represent an area like this let's say this is some sort of cresant moon bull b if you're trying to represent that they found that occasionally you get a gaussian that is too big it's trying to do too much and it would look something like that it's overfitting that thing so this is why i like to say that these gins are a bit like cells because what they do is they go right i'm just going to split this in half so suddenly you'd have your cant moon here and then you'd have two gaussians here rather than one big one that's trying to do too much you' have two small ones representing that environment another example is i'm dra so many cant moons right now is you'd have a gon let's say here that's trying to represent that and it's too small it's underfitting that scene so what you do is they go right let's clone it draw another one and you can see how that is able to fit that scene so much better so the these gaussians as i say they're like cells they move they change they duplicate they divide they fit your environment so well that it becomes photo realistic basically so we start off with essentially a pretty simple point cloud of a scene that is pretty obtainable based on standard structure for motion methods and then that is not going to be great first go right we stick some gans on it some of them are too big some of them are too small we clone some we divide some and we slowly using gradient descent jit of these gs to fit them better into to the scene until eventually they start to look a little bit more photo realistic and actually you're doing this over multiple views at the same time so you're not just doing it in one picture you're doing it across all the pictures of your scene simultaneously to make sure that their 3d shape reflects the 3d shape of the object but what you basically get is a really poh point cloud yes once you've got this gan scene you need to be able to render it because the idea is that you can render it from any point not necessarily the point just like with nerf not necessarily the points you had in your original data set and they use pretty standard standard techniques for this so the first is you use essentially a a z buffer to prevent yourself from drawing unnecessary gings that are already uded by ones in front so depth buffer yeah depth buffer right so so basically you know you as you've already drawn in if you've got a ray coming through here you know that this one's in front of this one you just don't bother to render the ones behind but it's a little bit more complicated because you also some of them have transparency so you have to do alpha blending that's also quite common in graphics so you've got something that's partially transparent sitting over something that may also be that and you just slowly add these things up but you if you make an effort to only render the things that are you absolutely have to a great number of the galaxies never get rendered per scene and it's very very quick right which is how what sort of fps does it get 100 fps right nice perhaps we should go and have a look at some examples yes okay so here we are at my desk if you remember last time we captured this lovely christmas tree and what i've done is i've retrained it on a gussian splatting model so this is actually using nerf studio which was the same framework we used last last time but we're using their model called splat facto i'm going to talk about what's different here if you have a look at certain parts of the scene here this bush for example it's made up of these almost like these shards these are actually the individual gaussians here that make up this bush see if i can zoom in here so for example like that that's in itself a gaussian that little one there's a gausson that little one there's a gaussian by themselves they mean nothing right but as soon as you zoom out you can see that's a bush because it's made up of the hundreds and hundreds thousands and thousands of these gaussians what's outside look like that's one of the things i want to show you last time in the nerf when we looked outside it was just pure noise it looked really weird and people going know what's all that about you know it looks really strange but if you zoom out now it suddenly looks like a traditional video game can you see how all of these gins are inside the room here inside the atrium that we captured anything outside is just black because there's no gaussians there just you're going to render a blank background which is black because you're not relying on a neural network to represent this scene so there isn't noise or anything like that outside of this this thing goes there's no gaans there just render it black the only thing you have is these really big shards which are gaans that essentially have tried to capture the whole sky or captur a hole of an outside building the further away you get from where your images are actually captured the more nois your gans are going to get exactly so if you look over here can you see this floor here doesn't have anything there that's because during the in the training images we never captured this floor so there's nothing there for it to render it doesn't render anything there and this is another problem this is the same problem that nerf had is that if you don't capture things in your scene it won't render anything there there's no sort of you know it doesn't help you out and go well this is probably a flaw let's just intuition exactly yeah there there are some probably some models that are coming down the line which can use diffusion to sort of fill in the gaps but for now it will just render it black so which is why if you ever capture a nerf or a gap ian make sure to capture the entire scene and make it look you know capture as much detail as you can you're probably thinking okay this is all good and stuff you know and you're going oh gaussians are the best but what's some fun stuff we can do well i'm going to show you something that i i managed to cook up in unity unity is one of the main frameworks you' use for creating games and you can import these gin straight into your unity project which is great because if you want to let's say have a a scene of your house in a game capture a g splat import it into your unity project and then suddenly you can do different things and this is another thing that gans are so good at compared to nerf is that these gxes are physical things it's not represented by a neural network so for example this christmas tree here if in the nerf i wanted to move that a little bit to the left no you can't do it because you'd have to retrain your neural network with these gaans if i wanted to move the tree a little bit to the left drag and drop takes a second second compared to 30 minutes it take to retrain everything so that is another thing these ges are so much easier to work with than nerf we're inside unity now so i've imported all these gaussians into unity and actually unity make it a little bit easier to see these are all what the gaussians look like like they look like shards almost so what i've done is i've set up a little particle effect on these gaussians so when i press space bar they're going to explode into to a series of particles so i've set up a little camera system here as well so i'll show you what that looks like when i click space you see you couldn't do that with a nerf but there's something a bit bit strange about these particles they're mini mik pounds amazing this is meant to show the power of gaussians it's showing an unnecessary feature of unity in so if you want you could you could go home capture gaan and create a universe of mini mics if you want you could not do that with a nerf you see and so you know if we let it play out you can see there are millions and millions of of mics in this multiverse so i thought that was a so i thought that was a nice little demonstration to show how powerful gaussians can be how powerful yeah thanks sean good job we're done what this does it renders it very quick because you need to get an understanding of the environment but slow right for real time r this is unremarkable in itself but the interesting part comes when we start applying these rules we can start from like a seed organism like like you say maybe