well let's start with the obvious one passwords what's your password sean i'm not going to tell you but what i will tell you is i use a password manager to to use make it so that i don't really know my password so actually i don't know i don't know what my password is and i assume your password is randomly generated then randomly generated quite long yeah it's quite long and it's randomly generated and that's that's a good way to have a password why because it has a lot of entropy if you have a lot of equal combinations the entropy is just the base two logarithm of the number of combinations so obviously if you have for example a 15 character password with regular letters capital letters numbers special characters you know you have about 80 90 different characters in your set to the power 15 that's going to be a massive number then you take the base to the logarithm of it and you're still going to end up with a lot of entropy in fact we can take the base 2 logarithm because we want to have our answer expressed in bits of the number of characters which i said is roughly 80 and then there were 15 such characters so that's 80 to the power 15. and if you do the math you can ask a calculator to do it which i secretly have and you get 95 bits roughly so that's not 95 possible options that's 95 bits of that's right there are 80 to the power 15 possible options which you know as a guesstimate i would say would be 10 to the power roughly 27 28. a massive number of combinations so if you want to search through all those combinations you're not you're not going to get there so you want the number of bits to be as high as possible for every bit you increase your password entropy with you double the number of searches that someone will need to do in order to guess your password so you can imagine something like five bits entropy is nowhere near enough because that just means you need to do 32 guesses 2 to the power five to guess the password which is really easy in in most settings you know this is a good way of doing it but there's other ways of doing it so there's a famous xkcd comic which i'm sure everyone knows which says you can select four sort of semi-random words and i'm sure someone in the comments will correct me if i misremember it correct horse battery staple there we go a live correction and the argument there is that each word has a certain amount of entropy so the whole string is massive but it's not a random selection of characters so we can't simply take 26 to the power however many letters it has but we can look at the entropy of individual words if you simply select them from the dictionary so the claimed entropy of of a word in the comic is roughly 11 bits and then we get a much simpler computation and we have four words times 11 bit entropy each because they're independent for a total of roughly 44. it's of entropy so selecting four random words if you do it completely at random would get you 44 bits of entropy which by the way is not even less than half the size every bit is having the size so while this is probably good enough in practice it's unimaginably tiny compared to a properly randomly generated password that you don't have to remember but there's another issue with the scheme from xkcd which is that you really really need to make sure that your forwards are random you can't as a human come up with four words and and put them in order because then we don't have a uniform probability distribution and that's actually really bad for the entropy i don't want to go into too many details but you can imagine that people doing word association are not going to come up with the same set of combinations as a computer would it's going to be a more limited set and that's bad it's going to reduce your entropy if you hypothetically picked four words that were in the top five mike pound did a video on dictionary attacks so i'm not going to go into the further details on that side but i think it's interesting to note that the size of the dictionary correlate very nicely with information theory so information theory here provides a very useful shortcut to thinking about dictionary attacks without actually having to deal with the nitty-gritty details there are other applications of information theory in computer security one obvious one would be privacy one way to express the privacy that you you have is perhaps in the number of bits that are being leaked about you so consider a case where we have a hospital with a database of patients now it's valuable for research if researchers can access this information now obviously you don't want individual researchers to be able to query the database and say why did my own go to the hospital last week right so obviously you don't want researchers to be able to do that they only need aggregate information in other words they can pull for example all patients with condition x and they would get a big list but rather than having individuals on them it only has sort of aggregate information on them so it will say you know they can query how many percent of patients with condition x have condition y and then they will learn something about the condition right that's the goal but they will also learn something about the patients at the hospital and this uses the notion of conditional entropy we can see here we have a conditional query so it's not surprising that we have a notion for that so conditional entropy is written as the entropy of x given y and you simply take the entropy of x for given outcomes of y but then what are the outcomes of y there's multiple possible outcomes so you take the expectation in other words you're summing over all the possibilities of y and then taking the conditioned entropy which is something slightly different of the random variable x where we know that y has some specific outcome so we're looking at all the specific outcomes of y and we're taking the entropy of x in that case it simplifies to the sum of the different values of y where we take the entropy of x indicates where y happens to be the case times the probability that y is the case so we're just going through all the possible values of y saying what is a probability that we're in this situation and what is the entropy of x in that situation we can further simplify the entropy by applying the definition but you're just going to end up with a long formula like here now this conditional entropy is very useful in in measuring how much you know about the data set but there is also pitfalls let's go to the sort of cases that we want to capture with with this conditional entropy so let's say that ahead of time you don't know anything about the patients and now you learn somehow from the data set the patient a has condition x then what do you learn about patient a well you learn a certain amount of bits and how many bits exactly depends on how common condition x is in the general population so if it's a very common disease the probability will be close to one bit if it's a very rare disease and you learn this the the information you learn will actually go go up right and you can imagine if it's a one in a million disease you learn quite a lot by learning that this patient has that particular disease and and that's sort of how it's supposed to work that's what we want but you there's odd cases and in order to explain the odd case i'm going to make an analogy with dice because humans are good at reasoning about dice so let's say that i throw four dice die a b c and d and i tell you that die a has a value of 4. how much did you learn about the data set quarter a quarter because it's one of four dice that's not quite right but it was slow i can see why no what you learn is you basically reduce the total set of possibilities by a factor of six right if you don't know anything about the dice how many possibilities are there well six times six times six times six the entropy of that you just put a log 2 around it and this is some number it's not two because it's binary or is there another reason is that that's right so you could use any log you want but we computer scientists love log 2 because it gives you the answer in bits and we like bits you can use the natural log you get your answer in nets you can use at log 10 then you're an engineer so the log 2 of this number is about 10.3 but i told you the outcome of die one i told you the outcome was four so that means that there isn't 6 times 6 times 6 combinations there's one times six times six combinations so we have one times six times six times six and we want to take the log two of that but before we do that let's talk about an interesting property of the log which is that on the inside multiplication is addition on the outside so this is the same as saying well the log 2 of 6 times 6 times 6 times 6. is actually equal to the log 2 of 6 plus the log 2 of the remaining ones six times six times six these two values are of course the same right so we can actually look at each die individually and that is because they are independent events it's your what 11 15 coffee no no it's it turns off after a while and cleans itself okay so these two log values are the same and this log value is just the base to logarithm of six which is about 2.6 bits if you're not mistaken so we can actually look at this formula and say this is 2.6 bits this is at 7.6 bit roughly because i rounded the numbers they don't quite add up to 10.3 but i assure you this is just due to rounding which is exactly what is of interest here right because here we get the log to base log 2 of 6 times 6 times 6 of the other three dice which was 7.6 bits and then the difference between those two is how much we learned so we take what we knew before we take what we know after so the entropy before the entropy after the difference between that is how much we learned so how much did we learn about this dice roll 2.6 bits similarly if we learn that a patient in the database has a particular disease how much do we learn well how many combinations were possible before for the entire data set how many are possible after the difference expressed in bits is how much you learned this was the good case but let's go to a case where it's less obvious that this is a good measure so what if instead of telling you that day one had an outcome of four i told you that die 1 and die 2 had the same outcome i'm not telling you what the outcome was but i'm telling you that they are the same outcome okay so what's what are the possibilities for die one there's six possibilities what's the possibilities for diet two well this is not independent of die one because i told you they're the same so actually there's only one remaining possibility one times six we take the log 2 of that number and we get again 7.6 bits which means that by telling you die 1 and 2 have the same outcome you learn exactly the same amount about the data set as when i tell you the outcome of taiwan but in reality that doesn't quite match what we want because if i tell you that patients a and b have the same disease i would argue that you're not learning the same amount about anyone as when i tell you patient a has i don't know a specific disease right so these two are equal when it comes to entropy but not equal when it comes to sort of what we feel is right about privacy still i think this is a very useful application but we have to understand that just because you have a certain amount of information doesn't always mean that the information is useful so it's one bit of surprisal and similarly if we plug in the double coin flip your prediction if i'm not mistaken was heads and tails yeah and the probability of that being right is of course easier to find what the factorization is and then once you've factored it you can just do these steps to completely calculate the private key