so in today's video we're going to talk about relevance as a concept for understanding how good search engines are and that's because so in the last two videos we've talked about how search engines index different web pages and then after that we looked at how search engines rank different web pages the links for both of those are in the description below so in one of the comments in the last video someone asked about relevance because this is a pretty fundamental concept to search engines deciding if a result in the search engine is relevant or not relevant is kind of a key stage and there's lots of different ways you can use that to evaluate how good your search engine is so in order to discuss relevance what we need is several things we need a set of things we're going to search for so for this we're using my handy monopoly pieces and then we need a set of documents which we've got here which are results that the search engine could return so then the third thing is a list of which results are relevant for the search that you're looking for and for this i'm using nice big red hotel details for things that are relevant and small green houses for things that are not relevant so then ultimately what a search engine wants to do is to only return the relevant ones so just to get all the hotels out that would be nice if our search engine just retrieved all these and not get any of the houses back that would be the ideal situation for any search engine and that would be true for whatever we searched for now this is obviously not necessarily very easy to do so what we tend to have is two measures one is precision and one is recall now recall is the a measure of how many of the relevant ones we manag to get back so if we manage to get all of these hotels back nicely then what we have is 100% recoil but what's more likely is that we managed to get some portion of them back and so we have maybe 70% recoil now the second things we want is precision and this is the number of returned results which are actually relevant so say my search engine brought back these results then its precision is what five out of nine which is a random and a number of results to returns let's do that that's 5 out of 10 so it's got 50% precision now in this case its recall was five out of a possible i should have figured out how many there are of these so there 13 so the recall here is 5 out of 13 and the precision was five out of 10 so in order to compare all of these different algorithms we've produced to make our search engine better what we want is a nice score to be able to say what we've done so a score for precision is the number of results that you returned which were ir relevant divided by the overall number of results you return so this would be 5 divided by 10 if you want a score for recall then what you're doing is you're taking the number of relevant results you return divided by the total number you could have returned which is 5 divid 13 not an easy number to immediately come up with so if we take a single measure which combines both precision and recall we have what's called the f measure and the simplest form of the f measure is two ided 1 divided by the precision plus one divided by the recall it's yeah there's more complicated versions which add some weight and some likelihood of errors and things like that but the simplest version is that simple comparison and all that's doing is saying if i get a good precision score and a good recall score then i'll get a good f measure score people have produced sets of results where there are maybe a million results that could be returned and they've pred decided on six different searches that we will go for and they have pred decided that for this search certain set documents are ones which are relevant for this search there's different set of documents are relevant so what we have is a known set of documents a known set of searches and a known set of right answers and search engines have used this principle which is called the cranfield paradigm for about 60 years to improve search engines so read every time one research university or people from google or bing say we've got an improvement to our algorithm which we think is going to be better they can take a known set of data sets a known set of results a known set of right answers and they can say based on these in our algorithm we get a score which is slightly better and that score might be higher precision or it might be higher recall it's actually a number of different measures but it means essentially everybody's been able to compete for 60 years on who can get the best algorithm so if we wanted to build a system which always achieved 100% recall that would be very easy i've just returned everything and in there i definitely have 100% recall because i definitely got everything but that's not necessarily good but if you wanted 100% recall that would work another way to get the answer you want is if you want to make sure you always get 100% precision would be to just take your very best answer and say i'm pretty confident that's going to be a good one and i've got 100% precision i only returned one and it was relevant but i missed out on lots of things so our aim tends to be i want to get as many as the answers as possible so i want to get high recall but i want to do that without losing precision so tell me again what what it is it's 2 / by 1 divided by precision plus 1 divided by recall and so then what we can get is my set results are returned for this has one specific f score so now i'm going to bring in my separate sets so my first algorithm is bringing back my nice retro wooden houses and hotels and my brand new algorithm which is going to make me a million pounds is going to use these shiny plastic hotels and houses so we go two separate algorithms we did one search which was my shiny car but both algorithms return six relevant results and four not relevant results result s and so it doesn't really matter what our f score is because the f score is the same for both of them we didn't manage to differentiate between which algorithm was better so we start to ask questions how do we decide which algorithm is better and the first way of doing this is to say well a better algorithm if it's going to return six good ones out of 10 it's going to put those six good ones higher than the other one so if one algorithm is producing relevant relevant not relevant relevant relevant not relevant and so on down to the bottom then a better algorithm would have relevant relevant relevant relevant relevant relevant and then not relevant afterwards so what we want is a measure that will say this result set is better than that result set and so for this what we use is an average precision score and to calculate average precision the easy form of what we do is for every result we bring in we give it a score of one if it's relevant and we divide it by the fact we brought in a new number so here we have we brought in one new result and we divide it by one because that's how many documents we brought in here we have another relevant result so we've got two relevant ones now and we divide it by the fact that we've got two so far next time though we didn't get a relevant result so we're still only at two relevant ones but we brought three in and then here we're at three for four four for five four for six oh no five for seven and so on down to the bottom but over here we had one for one two for two 3 for three four for four and so on to the point here where we start getting only six for seven and so then what you do is you add up all these scores to get an average precision score so here we had this is one this is one but now we get 0.6 and then we get 0.75 for this one and 0.8 for this one but over here we had 1 one one one one one and so the sum of these is going to be lower than the sum of these and so this is a better response even though we got the same number that we relevant back we put them all at the top and got a higher score for doing it so essentially saying the case we actually got on on the left side divided by the ideal case and so the ideal cases we get them all at the top and here we actually did what we actually like his mean average vision we like to know that we get this type of better answer every single time so for my shiny car i got a better result from this algorithm cu the good ones were at the top but for my nice top hat suddenly we brought all bad results the top so this doesn't work for a top hat very well and even on this one it got two bad ones quite high up and so what you want is a score which is what you got for everything even to the point when you bring my awesome horse into the search engine with evil santa with weapons on top thanks to my son then for this particular score both of them did excellently because my search engine is optimized on pony horse. horse to really bring back horse based products but then what your mean average precision score gives you is the average score you got for every single query that you did when you know the answer so in general the average score of all searches was higher for this one compared to the first algorithm you had but if you had only one example where you got good results for my awesome horse but for everything else you were getting bad results before it so your mean average position allows you to say overall my queries which one which algorithm is performing best it's been a while since we've checked that website i wonder if it's changed since we last went there maybe it has different words in it now we'll go and check it and then when it gets there it says and we use that to sort of triangulate how far away things are and that's what they're doing here