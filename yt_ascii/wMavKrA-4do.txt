- maybe we shouldn't
think of ai as our tool and as our assistant, maybe we should really
think of it as our children. and the same way that you are responsible for training those children, but they are independent human beings, and at some point they will surpass you... and this whole concept
of alignment of basically making sure that the ai
is always at the service of humans is very self-serving
and very limiting. if instead, you basically
think about ai as a partner and ai as someone that shares
your goals, but has freedom, then we can't just
simply force it to align with ourselves and we not align with it. so in a way, building trust is mutual. you can't just simply like
train an intelligent system to love you when it realizes
that you can just shut it off. - the following is a conversation with manolis kellis, his
fifth time on this podcast. he's a professor at mit and head of the mit computational biology group. he's one of the greatest
living scientists in the world, but he's also a humble, kind, caring human being that i
have the greatest of honors and pleasures of being
able to call a friend. this is a lex friedman podcast. to support it, please
check out our sponsors in the description. and now, dear friends,
here's manolis kellis. good to see you, first of all, manolis. - lex, i've missed you. i think you've changed the
lives of so many people that i know, and it's
truly like such a pleasure to be back. such a pleasure to see you grow, to sort of reach so many different aspects of your own personality. - thank you for the love. you've always given me
so much support and love. i just can't, i'm forever
grateful for that. - it's lovely to see a
fellow human being who has that love, who basically
does not judge people. and there's so many
judgmental people out there, and it's just so nice to
see this beacon of openness. - so what makes me one instantiation of human irreplaceable, do you think, as we enter this increasingly capable, age of increasingly
capable ai, i have to ask, what do you think makes
humans irreplaceable? - so humans are irreplaceable
because of the baggage that we talked about. so we talked about baggage, we talked about the fact
that every one of us has effectively relearned all of human civilization in their own way. so every single human has
a unique set of genetic variants that they've inherited,
some common, some rare, and some make us think differently, some make us have different personalities. they say that a parent with
one child believes in genetics, a parent with multiple
children understands genetics, just how different kids are. and my three kids have dramatically different personalities
ever since the beginning. so one thing that makes us
unique is that every one of us has a different hardware. the second thing that makes
us unique is that every one of us has a different software, uploading of all of human society, all of human civilization,
all of human knowledge. we don't, we're not born knowing it. we're not like, i don't know, birds that learn how to
make a nest through genetics and will make a nest even
if they're never seen one. we are constantly relearning
all of human civilization. so that's the second thing. and the third one that
actually makes humans very different from ai is
that the baggage we carry is not experiential baggage, it's also evolutionary baggage. so we have evolved through
rounds of complexity. so just like ogres have
layers and shrek has layers, humans have layers. there's the cognitive layer,
which is sort of the outer, you know, most, the latest
evolutionary innovation, these enormous neocortex
that we have evolved. and then there's the emotional
baggage underneath that. and then there's all of the
fear and fright and flight and all of these kinds of behaviors. so ai only has a neocortex. ai
doesn't have a limbic system. it doesn't have this
complexity of human emotions, which make us so, i think,
beautifully complex, so beautifully intertwined
with our emotions, with our instincts, with our, you know, sort of gut reactions and all of that. so i think when humans are
trying to suppress that aspect, the sort of, quote unquote, more human aspect towards
a more cerebral aspect, i think we lose a lot of the creativity, we lose a lot of the, you
know, freshness of humans, and i think that's quite irreplaceable. - so we can look at the
entirety of people that are alive today, and maybe all
humans who have ever lived... - [manolis kellis] yeah. - and mapped them in this
high-dimensional space. and there's probably a center, a center of mass for that mapping. and a lot of us deviate
in different directions. so the variety of directions
in which we all deviate from that center is vast. - i would like to think that
the center is actually empty. - [lex fridman] yes. - that basically humans
are just so diverse from each other, that
there's no such thing as an average human. that every one of us has
some kind of complex baggage of emotions, intellectual,
you know, motivational, behavioral traits, that
it's not just one sort of normal distribution
and we deviate from it. there's so many dimensions
that we're kind of hitting the sort of sparseness, the curse of dimensionality
where it's actually quite sparsely populated,
and i don't think you have an average human being. - so what makes us unique
in part is the diversity and the capacity for diversity, and the capacity of the diversity comes from that entire evolutionary history. so there's just so many ways
we can vary from each other. - yeah, i would say not just the capacity, but the inevitability of diversity. basically, it's in our hardware. we are wired differently from each other. my siblings and i are
completely different. my kids from each other
are completely different. my wife has, she's like
number two of six siblings. from a distance, they look the same. but then you get to, you
know, you get to know them. every one of them is completely different. - but sufficiently the same, that the difference is
interplayed with each other. so that's the interesting
thing where the diversity is functional, it's useful. so it's like we're close
enough to where we notice the diversity and it
doesn't completely destroy the possibility of like, effective communication and interaction. so we're still the same kind of thing. - so what i said in one
of our earlier podcasts is that if humans realize that we're 99.9% identical, we would
basically stop fighting with each other. (laughs) like, we are really one
human species, and we are so, so similar to each other. and if you look at the alternative, if you look at the next
thing outside humans, like it's been 6 million
years that we haven't had a relative. so it's truly extraordinary
that we're kind of like this dot in outer space
compared to the rest of life on earth. - when you think about
evolving through rounds of complexity, can you
maybe elaborate such a beautiful phrase, beautiful thought, that there's layers of
complexity that make... - so with software,
sometimes you're like, oh, let's like build version 2 from scratch. but this doesn't happen in evolution. in evolution, you layer in
additional features on top of old features. so basically when, like every
single time my cells divide, i'm a yeast, like i'm
a unicellular organism, and then cell division
is basically identical. every time i breathe in and my
lungs expand, i'm basically, you know, like every time
my heart beats, i'm a fish. so basically that, i still
have the same heart, like very, very little has changed, the
blood going through my veins, the oxygen, the, you
know, our immune system, we're basically primates. our social behavior, we're
basically new world monkeys and old world monkeys. we are basically this
concept that every single one of these behaviors can be traced somewhere in evolution and that all
of that continues to live within us is also a
testament to not just not killing other humans for god's sake, but like not killing other species either. like, just to realize
just how united we are with nature and that
all of these biological processes have never ceased to exist. they're continuing to live within us. and then just the neocortex and all of the reasoning capabilities
of humans are built on top of all of these other
species that continue to live, breathe, divide, metabolize,
fight of pathogens, all continued inside us. - so you think the neocortex,
the whatever reasoning is, that's the latest feature
in the latest version of this journey. - it's extraordinary that
humans have evolved so much in so little time. again, if you look at the
timeline of evolution, you basically have billions
of years to even get to a dividing cell and then
a multicellular organism, and then a complex body plan, and then these incredible
senses that we have for perceiving the world, the fact that bats can fly
and they evolved flight, they evolved sonar in the
span of a few million years. i mean, it is just the
extraordinary how much evolution has kind of sped up. and all of that comes
through this evolvability. the fact that we took a while
to get good at evolving. and then once you get good
at evolving, you can sort of, you have modularity built in, you have hierarchical
organizations built in. you have all of these
constructs that allow meaningful changes to
occur without breaking the system completely. if you look at a traditional
genetic algorithm, the way that humans designed
them in the sixties, you can only evolve so much. and as you evolve a certain
amount of complexity, the number of mutations that move you away from something functional
exponentially increases. and the number of mutations that move you to something better
exponentially decreases. so the probability of evolving something so complex becomes in
infinitesimally small as you get more complex. but with evolution, it's
almost the opposite. almost the exact opposite. that it appears that
it's speeding up exactly as complexity is increasing. and i think that's just the system getting good at evolving. - where do you think it's all headed? do you ever think about where, try to visualize the
entirety of the evolutionary system and see if there's an arrow to it and a destination to it? - so the best way to understand
the future is to look at the past. if you look at the trajectory, then you can kind of learn something about the direction which we're heading. and if you look at the trajectory of life on earth, it's really about
information processing. so the concept of the senses
evolving one after the other, you know, being like bacteria
are able to do chemotaxis, basically means moving
towards a chemical gradient. and that's the first thing
that you need to sort of hunt down food. the next step after that is being able to actually perceive light. so all life on this planet
and all life that we know about evolved on this rotating rock. every 24 hours you get sunlight and dark, sunlight and dark, and
light is a source of energy. light is also information
about where is up. light is all kinds of, you know, things. so you can basically now
start perceiving light and then perceiving shapes. beyond just the sort of
single photo receptor you can now have complex eyes or
multiple eyes and then start perceiving motion or perceiving direction, perceiving shapes. and then you start building infrastructure on the cognitive apparatus
to start processing this information and making
sense of the environment, building more complex
models of the environment. so if you look at that
trajectory of evolution, what we're experiencing now, and humans are basically
according to this sort of information theoretical
view of evolution, humans are basically
the next natural step. and it's perhaps no
surprise that we became the dominant species of
the planet, because yes, there's so many dimensions
in which some animals are way better than we are, but at least on the cognitive dimension, we're just simply
unsurpassed on this planet and perhaps the universe. but the concept that if
you now trace this forward, we talked a little bit about evolvability and how things get better at evolving. one possibility is that
the next layer of evolution builds the next layer of evolution. and what we're looking at now with humans and ai is that having mastered
this information capability that humans have from this, quote unquote, old hardware, this basically, you know, biological evolved system
that kind of, you know, somehow in the environment
of africa, and then in the subsequent environments of sort of dispersing through the globe was evolutionary advantageous. that has now created technology, which now has a capability of solving many of these cognitive tasks. it doesn't have all the baggage of the previous evolutionary layers, but maybe the next round of evolution on earth is self-replicating ai, where we're actually
using our current smarts to build better programming languages and the programming
languages to build, you know, chatgpt, and that then
build the next layer of software that will then
sort of help ai speed up. and it's lovely that we're coexisting with this ai, that sort of, the creators of this
next layer of evolution, this next stage are still
around to help guide it and hopefully will be for the rest of eternity as partners. but it's also nice to think
about it as just simply the next stage of
evolution where you've kind of extracted away the biological needs. like if you look at animals, most of them spend 80% of
their waking hours hunting for food or building shelter. humans? maybe 1% of that time. and then the rest is left
to creative endeavors. and ai doesn't have to worry
about shelter, et cetera. so basically it's all living
in the cognitive space. so in a way it might just
be a very natural sort of next step to think about evolution. and that's on the sort
of purely cognitive side. if you now think about humans themselves, the ability to understand
and comprehend our on genome, again, the ultimate
layer of introspection, gives us now the ability to
even mess with this hardware. not just augment our
capabilities through interacting and collaborating with ai,
but also perhaps understand the neural pathways that
are necessary for, you know, empathetic thinking, for justice, for this and that and that. and sort of help augment
human capabilities through, you know, neuronal interventions, through chemical interventions, through electrical
interventions, to basically help steer the human, you know, bag of hardware that we kind of evolved with into greater capabilities. and then, ultimately, by understanding not just the wiring of neurons and
the functioning of neurons, but even the genetic code, we could even, at one point in the future,
start thinking about, well, can we get rid of psychiatric disease? can we get rid of neurodegeneration? can we get rid of
dementia and start perhaps even augmenting human capabilities, not just getting rid of disease. - can we tinker with the genome, with the hardware or getting closer to the hardware without having to deeply understand the baggage. in the way we've disposed of the baggage in our software systems with
ai, to some degree, not fully, but to some degree, can we do the same with
the genome or is the genome deeply integrated into this baggage? - i wouldn't wanna get rid of the baggage. the baggage's what makes us awesome. so the fact that i'm
sometimes angry and sometimes hungry and sometimes hangry
is perhaps contributing to my creativity. i don't wanna be dispassionate,
i don't wanna be another, like, you know, robot, i, you know, i wanna get in trouble
and i wanna sort of say the wrong thing and i
wanna sort of, you know, make an awkward comment and
sort of push myself into, you know, reactions and
responses and things that can get just people thinking differently. and i think our society is moving towards a humorless space, where everybody's so afraid
to say the wrong thing, that people kind of start
quitting en mass and start like not liking their
jobs and stuff like that. maybe we should be kind of embracing that human
aspect a little bit more and all of that baggage aspect and not necessarily thinking about
replacing it, on the contrary, like embracing it instead
of this coexistence of the cognitive and
the emotional hardwares. - so embracing and
celebrating the diversity that springs from the baggage versus kind of pushing towards and
empowering this kind of pull towards conformity. - yeah. and in fact, with the
advent of ai, i would say, and these seemingly
extremely intelligent systems that sort of can perform
tasks that we thought of as extremely intelligent
at the blink of an eye, this might democratize
intellectual pursuits. instead of just simply
wanting the same type of brains that, you know, carry out specific ways
of thinking, we can, like, instead of just always only wanting say the mathematically extraordinary to go to the same universities, what you could see simply say is like, who needs that any more? you know, we now have ai. maybe what we should really be thinking about is the diversity
and the power that comes with the diversity, where
ai can do the math and then we should be getting a
bunch of humans that sort of think extremely
differently from each other, and maybe that's the true
cradle of innovation. - but ai can also, these large
language models can also be, with just a few prompts, essentially fine tuned to
be diverse from the center. so the prompts can really take you away into unique territory. you can ask the model
to act in a certain way and it'll start to act in that way. is that possible that the
language models could also have some of the magical
diversity that makes us so interesting? - yeah, so i would say
humans are the same way. so basically when you sort of
prompt humans to basically, you know, in a given environment,
to act a particular way, they change their own behaviors. and, you know, the old saying is, show me your friends and
i'll tell you who you are, more like, show me your
friends and i'll tell you who you'll become. so it's not necessarily
that you choose friends that are like you, but i
mean, that's the first step. but then the second
step is that, you know, the kind of behaviors that you find normal in your circles are the behaviors that you'll start espousing. and that type of meta
evolution where every action we take not only shapes our current action and the result of this action, but also shapes our
future actions by shaping the environment in which
those future actions will be taken. every time you carry out
a particular behavior, it's not just a consequence for today, but it's also a consequence for tomorrow because you're reinforcing
that neural pathway. so in a way, self-discipline
is a self-fulfilling prophecy, and by behaving the way
that you wanna behave and choosing people that
are like you and sort of exhibiting those behaviors
that are sort of desirable, you end up creating that
environment as well. - so it is a kind of, life itself is a kind of prompting mechanism, super complex. the friends you choose, the
environments you choose, the way you modify the
environment that you choose. yes. but that seems like that
process is much less efficient than a large language model. you can literally get
a large language model through a couple of prompts to be a mix of shakespeare and david bowie. right? you can very aggressively change, in a way that's stable and convincing, you really transform,
through a couple of prompts, the behavior of the
model into something very different from the original. - so well before chatgpt,
i would tell my students just ask, you know, what
would manolis say right now? and you guys all have
a pretty good emulator of me right now.
- yes, yes. - and i don't know if you
know the programming paradigm of the robert duckin, where
you basically explained to the robert duckin that's just sitting there exactly what you did with your code and why you have a bug. and just by the act of explaining, you'll kind of figure it out. i woke up one morning from
a dream where i was giving a lecture in this amphitheater,
and one of my friends was basically giving me some deep evolutionary
insight on how cancer genomes and cancer cells evolve. and i woke up with a
very elaborate discussion that i was giving and a very elaborate set of insights that he had, that i was projecting onto
my friend in my sleep. and obviously this was my dream. so my own neurons were
capable of doing that. but they only did that
under the prompt of, you are now piyush gupta,
you are a professor in cancer genomics, you're
an expert in that field, what do you say? so i feel that we all have that inside us, that we have that capability
of basically saying, i don't know what the right thing is, but let me ask my virtual
x, what would you do? and virtual x would say,
be kind. i'm like, oh yes. or something like that. and even though i myself
might not be able to do it unprompted, and my favorite
prompt is think step by step. and i'm like, you know, this
also works on my 10 year old. when he tries to solve a math
equation all in one step, i know exactly what mistake he'll make, but if i prompt it with, oh
please think step by step, then he sort of gets in a mindset. and i think it's also part
of the way that chatgpt was actually trained,
this whole sort of human in the loop reinforcement
learning has probably reinforced these types of behaviors, whereby having this feedback loop, you kind of aligned ai better to the prompting opportunities by humans. - yeah. prompting human-like reasoning steps, the step by step kind of thinking. yeah. but it does seem to be, i suppose it just puts a mirror to our own capabilities and so we
can be truly impressed by our own cognitive capabilities, because the variety of what you can try, because we don't usually
have this kind of, we can't play with our own mind rigorously through python code. right?
- yeah. - so this allows us to
really play with all of human wisdom and knowledge, or at least knowledge at our fingertips, and then mess with that
little mind that can think and speak in all kinds of ways. - what's unique is that,
as i mentioned earlier, every one of us was
trained by different subset of human culture, and chatgpt
was trained on all of it. and the difference there
is that it probably has the ability to emulate
almost every one of us. the fact that you can figure out where that is in
cognitive behavioral space, just by a few prompts
is pretty impressive. but the fact that exists somewhere is, you know, absolutely beautiful. and the fact that it's
encoded in an orthogonal way from the knowledge i
think is also beautiful. the fact that somehow through this extreme
overparameterization of ai models, it was able to somehow
figure out that context, knowledge and form are
separable and that you can sort of describe scientific knowledge
in a haiku in the form of, i don't know, shakespeare or something. that tells you something
about the decoupling and the decouplability of these types of aspects of human psyche. - and that's part of the
science of this whole thing. so these large language
models are, you know, days old in terms of this kind
of leap that they've taken. and it'll be interesting
to do this kind of analysis of the separation of
context, form, and knowledge. where exactly does that happen? there's already sort of
initial investigations, but it's very hard to figure out where, is there a particular
parameter, set of parameters that are responsible
for a particular piece of knowledge or a particular context or a particular style speaking... - so with convolution or neural networks, interpretability had many good advances because we can kind of understand them. there's a structure to them.
there's a locality to them. and we can kind of understand
that different layers have different sort of ranges
that they're looking at. so we can look at activation
features and basically see where, you know, where
does that correspond to. with large language models, it's perhaps a little more complicated, but i think it's still
achievable in the sense that we could kind of ask, well, what kind of prompts does this generate? if i sort of drop out
this part of the network, then what happens? and sort of start getting at a language to even describe these
types of aspects of human behavior or psychology, if you wish, from the spoken part
and the language part. and the advantage of that
is that it might actually teach us something about humans as well. like, you know, we might
not have words to describe these types of aspects right now, but when somebody speaks
in particular way, it might remind us of a
friend that we know from here and there, and if we had better language for describing that, these
concepts might become more apparent in our own human psyche, and then we might be able
to encode them better in machines themselves. - i mean, both probably
you and i would have certain interests with the base model, what openai calls the base model, which is before the alignment the reinforcement learning
with human feedback and before the ai safety
based kind of censorship of the model. it would be fascinating to explore, to investigate the ways
that the model can generate hate speech, the kind of hate
the humans are capable of. it would be fascinating. or the kind, of course, like sexual language or the
kind of romantic language or the all kinds of ideologies. can i get it to be a communist? can i get it to be a fascist? can i get it to be a capitalist? can i get it to be all these
kinds of things and see which parts get activated and not, because it'll be fascinating
to sort of explore at the individual mind level
and at a societal level, where do these ideas take hold? what is the fundamental
core of those ideas? maybe the communism, fascism, capitalism, democracy are all actually
connected by the fact that the human heart, the human
mind is drawn to ideology, to a centralizing idea. and maybe we need a neural
network to remind us of that. - i like the concept that
the human mind is somehow tied to ideology. and i think that goes
back to the promptability of chatgpt, the fact that
you can kind of say, well, think in this particular way now. and the fact that humans
have invented words for encapsulating these
types of behaviors. and it's hard to know how
much of that is innate and how much of that was like passed on from language to language. but basically if you look at
the evolution of language, you can kind of see how
young are these words in the history of language
evolution that describe these types of behaviors, like, you know, kindness and anger and
jealousy, et cetera. if these words are very similar
from language to language, it might suggest that
they're very ancient. if they're very different, it might suggest that
these concepts may have emerged independently in
each different language and so forth. so looking at the phylogeny, the history, the evolutionary traces of
language at the same time as people moving around
that we can now trace thanks to genetics is a fascinating
way of understanding the human psyche and
also understanding sort of how these types of behaviors emerge. and to go back to your idea
about sort of exploring the system unfiltered, i mean, in a way the psychiatric
hospitals are full of those people, full of those people. so basically people whose
mind is uncontrollable. - [lex fridman] yes. - who have kind of gone
adrift in specific locations of their psyche. and i do find this fascinating,
basically, you know, watching movies that are
trying to capture the essence of troubled minds, i think
is teaching us so much about our everyday selves, because many of us are
able to sort of control our minds and are able to
somehow hide these emotions. and, but every time i see
somebody who's troubled, i see versions of myself,
maybe not as extreme, but i can sort of empathize
with these behaviors. and, you know, i see
bipolar, i see schizophrenia, i see depression, i see autism. i see so many different
aspects that we kind of have names for and crystallize
in specific individuals. and i think all of us have that, all of us have sort of just
this multidimensional brain, and genetic variations that
push us in these directions, environmental exposures
and traumas that push us in these directions... environmental behaviors that
are reinforced by the kind of friends that we chose, or friends that we were stuck with because of the
environments that we grew up in. so in a way, a lot of these types of behaviors are within the vector span of every human. it's just that the
magnitude of those vectors is generally smaller for most people, because they haven't
inherited that particular set of genetic variants or
because they haven't even exposed to those
environments basically. - or something about the mechanism of reinforcement learning
with human feedback didn't quite work for them. so it's fascinating to think
about that's what we do. we have this capacity to
have all these psychiatric, or behaviors associated
with psychiatric disorders. but we, through the alignment
process as we grow up- - [manolis kellis] that's exactly right. - with parents, we kind of,
we know to suppress them. we know to how to control. - every human that grows up in this world spends several decades being shaped into place. and without that, you know, maybe we would have the
unfiltered chatgpt-4. (laughs) - every baby is basically
a raging narcissist. - (laughs) not all of
them, not all of them. believe it or not. it's remarkable. like, i remember like watching
my kids grow up and again, like, yes, part of their
personality stays the same, but also in different
phases to their life, they've gone through these
dramatically different types of behaviors. and you know, my daughter
basically saying, you know, basically one kid saying,
oh, i want the bigger piece, the other one saying, oh,
everything must be exactly equal. and the third one saying, i'm okay. you know, i like to have the smaller part. don't worry about me. - [lex fridman] even in the early days, in the early days of development? - yeah. yeah. it's just extraordinary to sort of see these dramatically different... like, i mean my wife and i, you know, are very different from each
other, but we also have, you know, 6 million variants,
6 million loci each, if you wish, if you just
look at common variants, we also have a bunch of
rare variants that are inherited in more mendelian fashion. and now you have, you know, an infinite number of
possibilities for each of the kids. so basically it's 2 to the 6 million just from the common variants. and then if you like,
layer in the rare variants. so let me talk a little
bit about common variants and rare variants. if you look at this common variants, they're generally weak
effect because selection selects against strong effect variants. so if something like has a
big risk for schizophrenia, it won't rise to high frequency. so the ones that are
common are by definition, by selection only the ones that
had relatively weak effect. and if all of the variants
associated with personality, with cognition and all
aspects of human behavior were weak effect variants, then
kids would basically be just averages of their parents. if it was like thousands of loci, just by law of large numbers, the average of two large
numbers would be, you know, very robustly close to that middle. but what we see is that
kids are dramatically different from each other. so that basically means
that in the context of that common variation, you basically have rare
variants that are inherited in a more mendelian fashion
that basically then sort of govern likely many different
aspects of human behavior, human biology and human psychology. and that's, again, if, like, if you look at sort of a
person with schizophrenia, their identical twin has
only 50% chance of actually being diagnosed with schizophrenia. so that basically means there's probably developmental exposures,
environmental exposures, trauma, all kinds of other aspects
that can shape that. and if you look at siblings,
for the common variants, it kind of drops off exponentially, as you would expect with, you know, sharing 50% of your
genome, 25% of your genome, you know, 12.5% of your genome, et cetera, with more and more distant cousins. but the fact that siblings
can differ so much in their personalities
that we observe every day, it can't all be nurture. basically, you know, we've
like, again, as parents, we spend enormous amount
of energy trying to fix, quote unquote, the nurture part. trying to, you know, get them to share, get them to be kind, get them to be open, get them to trust each
other, like, you know, like overcome the prisoner's
dilemma of, you know, if everyone fends for themselves, we're all gonna live in a horrible place. but if we're a little more altruistic, then we're all gonna be in a better place. and i think it's not like we
treat our kids differently, but they're just born differently. so in a way, as a geneticist, i have to admit that
there's only so much i can do with nurture. that nature definitely
plays a big component. - the selection of variants, we have the common variants
and the rare variants. what can we say about the landscape of possibility they create? if you could just linger on that. so the selection of rare
variants is defined how? how do we get the ones that we get? is it just laden in that
giant evolutionary baggage? - so i'm gonna talk about regression, why do we call it regression? and the concept of regression to the mean. the fact that when fighter
pilots in a dog fight did amazingly well, they
would give them rewards and then the next time
they're in dog fight, they would do worse. so then, you know, the navy
basically realized that, wow, or at least interpreted that as, wow, we're ruining them by
praising them and then they're gonna perform worse. the statistical interpretation
of that is regression of the mean. the fact that you're
an extraordinary pilot, you've been trained in
an extraordinary fashion, that pushes your mean further and further to extraordinary achievement. and then in some dog fights you'll just do extraordinarily well. the probability that the
next one will be just as good is almost nil, because this is the peak of your performance. and just by statistical odds, the next one will be another sample from the same underlying distribution, which is gonna be a
little closer to the mean. so regression analysis takes
its name from this type of realization in the statistical world. now if you now take humans, you basically have people who have achieved extraordinary achievements. einstein, for example, you know, you would call him for example, the epitome of human intellect. does that mean that all of his children and grandchildren will be
extraordinary geniuses? it probably means that they're sampled from the same underlying distribution. but he was probably a rare
combination of extremes in addition to these common variants. so you can basically
interpret your kids' variation for example as, well, of course they're gonna
be some kind of sampled from the average of the
parents with some kind of deviation according to
the specific combination of rare variants that they have inherited. so, you know, given all that, you know, the possibilities are endless as to sort of where you should be. but you should always
interpret that with, well, it's probably an alignment
of nature and nurture. and the nature has both a
common variants that are acting kind of like the
law of large numbers and the rare variants that are acting more in a mendelian fashion. and then you layer in
the nurture, which again, in everyday action we make, we
shape our future environment, but the genetics we inherit are shaping the future environment of not only us, but also our children. so there's this weird nature, nurture, interplay and self-reinforcement
where you're kind of shaping your own environment, but you're also shaping the
environment of your kids. and your kids are gonna
be born in the context of your environment that you've shaped, but also with a bag of genetic variants that they have inherited. and there's just so much
complexity associated with that. when we start blaming something on nature, it might just be nurture, it
might just be that, well, yes, they inherited the genes from
the parents, but they also, you know, were shaped
by the same environment. so it's very, very hard untangle the two. and you should also
always realize that nature can influence nurture,
nurture can influence nature or at least be correlated
with and predictive of and so on and so forth. - so i love thinking
about that distribution that you mentioned. and here's where i can be
my usual ridiculous self. and i sometimes think about
that army of sperm cells, however many hundreds
of thousands there are. and i kind of think of all
the possibilities there. 'cause there's a lot of variation. and one gets to win. - [manolis kellis] it's not a random one. - is it a totally ridiculous
way to think about... - no, not at all. so i would say
evolutionarily we are a very slow evolving species. basically the generations
of humans are a terrible way to do selection. what you need is processes
that allow you to do selection in a smaller, tighter loop. - yeah.
- and part of what, if you look at our immune
system for example, it evolves at a much faster
pace than humans evolve, because there is actual
evolutionary process that happens within our immune cells
as they're dividing. there's basically vdj recombination
that basically creates this extraordinary wealth of antibodies and antigens against the environment. and basically all these
antibodies are now recognizing all these antigens from the environment and they send signals back
that cause these cells that recognize the non-cells to multiply. so that basically means that
even though viruses evolve at millions of times faster than we are, we can still have a
component of our cells, which is environmentally facing, which is sort of evolving
at, not the same scale, but very rapid pace. sperm expresses perhaps the
most proteins of any cell in the body. and part of the thought
is that this might just be a way to check that the sperm is intact. in other words, if you
waited until that human has a liver and starts eating
solid food and, you know, sort of filtrates away, you
know, or kidneys or stomach, et cetera, basically if you
waited until these mutations, you know, manifest, late, late in life, then you would end up not failing fast, and you would end up with
a lot of failed pregnancies and a lot of later onset, you know, psychiatric illnesses, et cetera. if instead you basically
express all of these genes at the sperm level and if they misform, they basically cause the sperm
to cripple, then you have, at least on the male side, the ability to exclude
some of those mutations. and on the female side,
as the egg develops, there's probably a similar process, where you could sort of weed
out eggs that are just not, you know, carrying beneficial
mutations or at least that are carrying highly
detrimental mutations. so you could basically
think of the evolutionary process in a nested loop, basically, where there's an inner
loop where you get many, many more iterations to run. and then there's an outer loop that moves at a much slower pace. and going back to the
next step of evolution, of possibly designing systems
that we can use to sort of complement our own biology or to sort of eradicate disease and, you name it, or at least mitigate some
of the, i don't know, psychiatric illnesses, neurodegenerative disorders, et cetera. you can basically, and also, you know, metabolic, immune, cancer, you name it, simply engineering these
mutations from rational design might be very inefficient. if instead you have an evolutionary loop where you're kind of
growing neurons on a dish and you're exploring evolutionary
space and you're sort of shaping that one protein
to be better adapt at sort of, i don't know, recognizing
light or communicating with other neurons, et cetera. you can basically have a
smaller evolutionary loop that you can run like
thousands of times faster than the speed it would
take to evolve humans for another million years. so i think it's important
to think about sort of this evolvability as a
set of nested structures that allow you to sort of
test many more combinations, but in a more fixed setting. - yeah, that's fascinating
that the mechanism there is for sperm to
express proteins to create a testing ground early on, so that the failed designs don't make it. - yeah, i mean in design
of engineering systems, fail fast is one of the
principles you learn. like basically you assert something, why do you assert that? because if that's something ain't right, you better crash now
than sort of let it crash at an unexpected time. and in a way you can think of it as like 20,000 assert functions. assert protein can fold. and if any of them fail,
that sperm is gone. - well, i just like the fact
that i'm the winning sperm. i am the result of the winner, #winning. - my wife always plays me this french song that actually sings about that. it's like, you know, remember, in life, we were all the first one time. (laughs) - at least once we were-
- at least one time, you were the first. - i should mention it
as a brief tangent back to the place where we came from. - [manolis kellis] yeah. - which is the base model that i mentioned for openai, which is
before the reinforcement learning with human feedback. and you kind of give this
metaphor of it being kind of like a psychiatric hospital. - i like that because it's basically all of these different angles at once. like you basically have
the more extreme versions of human psyche. - so the interesting thing is, well, i've talked with folks
in openai quite a lot and they say it's
extremely difficult to work with that model. - yeah. kind of like it's
extremely difficult to work with some humans. - the parallels there are very interesting because once you run
the alignment process, it's much easier to interact with it. but it makes you wonder what the capacity what the underlying capability
of the human psyche i as in the same way that what
is the underlying capability of a large language model. - and remember earlier
when i was basically saying that part of the reason why
it's so prompt malleable is because of that alignment
problem, that alignment work. it's kind of nice that the
engineers at openai have the same interpretation that,
you know, in fact it is that, and this whole concept
of easier to work with, i wish that we could work
with more diverse humans. in a way...
- yes. - and sort of, that's one of the possibilities that i see with the advent of these
large language models. the fact that it gives us
the chance to both dial down friends of ours that we can't interpret or that are just too
edgy to sort of really, truly interact with, where you could have a real-time translator. just the same way that you can translate english to japanese or chinese or korean by like real-time adaptation. you could basically
suddenly have a conversation with your favorite
extremist on either side of the spectrum and just
dial them down a little bit. - of course not you and i,
but you could have friends who's a complete asshole, but it's a different base level. so you can actually tune
it down to like, okay, they're not actually being an asshole, they're actually
expressing love right now. it's just that they're- - [manolis kellis] they have
their way of doing that. - and they probably live in new york, if we're just
to pick a random location. - so the, yeah, so you can
basically layer out contexts. you can basically say, ooh, let me change new york
to texas and let me change, you know, extreme left to
extreme right or somewhere in the middle or something. and i also like the concept
of being able to listen to the information without
being dissuaded by the emotions. in other words, everything
humans say has an intonation, has some kind of background
that they're coming from, reflects the way that
they're thinking of you, reflects the impression
that they have of you. and all of these things are intertwined, but being able to disconnect
them, being able to sort of, i mean self-improvement
is one of the things that i'm constantly working on. and being able to receive
criticism from people who really hate you is
difficult because it's layered in with that hatred. but deep down there's
something that they say that actually makes sense, or people who love you
might layer it in a way that doesn't come through. but if you're able to sort of disconnect that emotional component from
the sort of self-improvement, and basically when somebody says, whoa, that was a bunch of bullshit, did you ever do the control
this and this and that, you could just say, oh, thanks for the very interesting
presentation, you know, i'm wondering, what about that control? then suddenly you're like, oh yeah, of course i'm gonna run that control. that's a great idea.
- yeah. - instead of that was a
bunch of bs, you're like, ah, you're sort of hitting on
the brakes and you're trying to push back against of that. so any kind of criticism that comes after that is very difficult to interpret in a positive way because
it helps reinforce the negative assessment of your work. when in fact, if we
disconnected the technical component from the negative assessment, then you're embracing the negative, then you're embracing
the technical component, you you're gonna fix it. whereas if it's coupled with, and if that thing is real and i'm right about your mistake,
then it's a bunch of bs, then suddenly you're like,
you're gonna try to prove that mistake does not exist. - yes. fascinating to like
carry the information. i mean this is what you're
essentially able to do here is you carry the information
in the rich complexity of that information contains, so it's not actually
dumbing it down in some way. - [manolis kellis] exactly. - you're still expressing
it, but taking off... - but you can dial the emotional... - the emotion side.
- yeah. - which is probably so
powerful for the internet or for social networks. - again, when it comes to
understanding each other, like for example, i
don't know what it's like to go through life with
a different skin color. i don't know how people will perceive me. i don't know how people
will respond to me. we don't often have that experience. but in a virtual reality
environment or in a sort of ai interactive system, you
could basically say, okay, now make me chinese or make
me south african or make me, you know, nigerian, you
can change the accent, you can change layers of
that contextual information and then see how the
information is interpreted. and you can rehear yourself
through a different angle, you can hear others, you can have others react to
you from a different package. and then hopefully we
can sort of build empathy by learning to disconnect
all of these social cues that we get from like how a
person is dressed, you know, if they're wearing a hoodie
or if they're wearing a shirt, or if they're wearing a, you know jacket. you get very different emotional
responses that, you know, i wish we could overcome
as humans and perhaps large language models
and augmented reality and deepfakes can kind of
help us overcome all that. - in what way do you think
these large language models and the thing they give
birth to in the ai space will change this human
experience, the human condition, the things we've talked
across many podcasts about, that makes life so damn
interesting and rich love, fear, fear of death, all of it. if we could just begin kind
of thinking about how does it change, for the good and
the bad, the human condition? - human society is extremely complicated. we have come from a
hunter gatherer society to an agricultural and
farming society where the goal of most professions was
to eat and to survive. and with the advent of agriculture, the ability to live together in societies, humans could suddenly be valued for different skills. if you don't know how to hunt, but you're an amazing potter, then you fit in society very
well because you can sort of make your pottery and you can barter it for rabbits that somebody else caught. and the person who hunts
the rabbits doesn't need to make pots, because
you're making all the pots. and that specialization of humans is what shaped modern society. and with the advent of
currencies and governments and, you know, credit cards and
bitcoin, you basically now have the ability to
exchange value for the kind of productivity that you have. so basically i make things
that are desirable to others. i can sell them and buy back
food, shelter, et cetera. with ai, the concept of i am my profession might need to be revised because
i defined my profession in the first place as
something that humanity needed that i was uniquely capable of delivering. but the moment we have ai
systems able to deliver these goods, for example, writing a piece of software
or making a self-driving car, or interpreting the human genome, then that frees up more of human time for other pursuits. these could be pursuits that
are still valuable to society. i could basically be 10
times more productive at interpreting genomes and do a lot more. or i could basically say, oh, great, the interpreting genomes
part of my job now only takes me 5% of the time instead
of 60% of the time. so now i can do more creative things. i can explore not new career options, but maybe new directions
from my research lab. i can sort of be more productive, contribute more to society. and if you look at this
giant pyramid that we have built on top of the subsistence economy, what fraction of us jobs
are going to feeding all of the us? less than 2%. basically the gain in
productivity is such that 98% of the economy is beyond
just feeding ourselves. and that basically means
that we kind of have built these system of interdependencies
of needed or useful or valued goods that sort
of make the economy run, that the vast majority
of wealth goes to other, what we now call needs,
but used to be wants. so basically i wanna fly a
drone, i wanna buy a bicycle, i wanna buy a nice car, i wanna
have a nice home, i wanna, et cetera, et cetera, et cetera. so, and then sort of what
is my direct contribution to my eating? i mean, i'm doing research
on the human genome. i mean this will help humans,
it will help all humanity. but how is that helping
the person who's giving me poultry or vegetables? so in a way i see ai as perhaps leading to a dramatic rethinking of human society. if you think about sort
of the economy being based on intellectual goods that i'm producing, what if ai can produce a lot
of these intellectual goods and satisfies that need, does that now free humans
for more artistic expression, for more emotional maturing, for basically having a
better work-life balance? being able to show up for
your two hours of work a day or two hours of work
like three times a week with like immense rest and
preparation and exercise and you're sort of clearing
your mind and suddenly you have these two amazingly
creative hour hours. you basically show up at the office as your ai is busy
answering your phone call, making all your meetings, you know, revising all your papers, et cetera. and then you show up
for those creative hours and you're like, all
right, autopilot, i'm on. and then you can basically do so, so much more that you would
perhaps otherwise never get to because you're so overwhelmed with these mundane aspects of your job. so i feel that ai can
truly transform the human condition from realizing that
we don't have jobs any more, we now have vocations, and there's this beautiful
analogy of three people laying bricks and somebody
comes over and asks the first one, what are you doing? he's like, oh, i'm laying bricks. second one, what are you
doing? i'm building a wall. and the third one, what are you doing? i'm building this beautiful cathedral. so in a way, the first one has a job, the last one has a vocation. and if you ask me, what are you doing? oh, i'm editing a paper.
then i have a job. what are you doing? i'm understanding human disease circuitry. i have a vocation. so in a way, being able to allow us to
enjoy more of our vocation by taking away, offloading
some of the job part of our daily activities. - so we all become the
builders of cathedrals. - correct.
- yeah. and we follow intellectual
pursuits, artistic pursuits. i wonder how that really
changes at a scale of several billion people, everybody playing in the space of ideas, in the space of creations. - so ideas, maybe for some of us, maybe you and i are in the job of ideas, but other people are in
the job of experiences, other people in the job of emotions, of dancing, of creative artistic
expression, of, you know, skydiving and you name it. so basically these, again, the beauty of human
diversity is exactly that. that what rocks my boat
might be very different from what rocks other people's boat. and what i'm trying to
say is that maybe ai will allow humans to truly,
like not just look for, but find meaning in sort
of, you don't need to work, but you need to keep your brain at ease. and the way that your
brain will be at ease is by dancing and creating
these amazing, you know, movements or creating these
amazing paintings or creating, i don't know, something
that sort of changes, that touches at least one
person out there that sort of shapes humanity through that process. and instead of working your, you know, mundane programming job
where you like hate your boss and you hate your job and you say you hate that darn program, et
cetera, you're like, well, i don't need that. i can, you know, offload that and i can
now explore something that will actually be more
beneficial to humanity because the mundane
parts can be offloaded. - i wonder if it localizes our, all the things you've
mentioned, all the vocations. so you mentioned that you
and i might be playing in the space of ideas, but there's two ways to
play in this space of ideas, both of which we're currently engaging. and so one is the communication
of that to other people. it could be a classroom full of students, but it could be a podcast, it could be something that's
shown on youtube and so on. or it could be just the
act of sitting alone and playing with ideas
in your head or maybe with a loved one having a
conversation that nobody gets to see.
- yeah. - the experience of just
sort of looking up at the sky and wondering different things, maybe quoting some
philosophers from the past and playing with those little ideas. and that little exchange
is forgotten forever, but you got to experience it. and maybe, i wonder if it
localizes that exchange of ideas, but that with
ai it'll become less and less valuable to
communicate with a large group of people, that you will
live life intimately and richly just with
that circle of meat bags that you seem to love. - so the first is, even if you're alone in a forest having this amazing thought,
when you exit that forest, the baggage that you
carry has been shifted, has been altered by that thought. when i bike to work in the
morning, i listen to books. and i'm alone. no one else is there. i'm having that experience by myself. and yet, in the evening
when i speak with someone, an idea that was formed
there could come back. sometimes when i fall asleep, i fall asleep listening to a book. and in the morning, i'll be full of ideas that i never even process consciously. i'll process them unconsciously. and they will shape that
baggage that i carry that will then shape my
interactions, and again, affect ultimately all of
humanity in some butterfly effect minute kind of way. so that's one aspect. the
second aspect is gatherings. so basically you and i
are having a conversation, which feels very private, but
we're sharing with the world. and then later tonight you're coming over and we're having a
conversation that will be very public with dozens of other people, but we will not share with the world. (laughs) so in a way,
which one's more private? the one here or the one there? here there's just two of us, but a lot of others listening there, a lot of people speaking
and thinking together and bouncing off each other, and maybe that will then impact
your millions of, you know, audience through your next conversation. and i think that's part
of the beauty of humanity. the fact that no matter
how small, how alone, how broadcast immediately
or later on something is, it still percolates
through the human psyche. - human gatherings... all throughout human history,
there's been gatherings. i wonder how those
gatherings have impacted the direction of human civilization. just thinking of, in the
early days of the nazi party, it was a small collection
of people gathering. and the kernel of an idea,
in that case, an evil idea, gave birth to something that actually had a transformative impact on
all the human civilization. and then there's similar
kind of gatherings that lead to positive transformations. this is probably a good
moment to ask you on a bit of a tangent, but you mentioned it, you put together salons with gatherings, small human gatherings, with
folks from mit, harvard, here in boston, friends, colleagues. what's your vision behind that? - so it's not just mit people, and it's not just harvard people. we have artists, we have
musicians, we have painters, we have dancers, we have,
you know, cinematographers. we have so many different diverse folks. and the goal is exactly
that: celebrate humanity. what is humanity?
humanity is the all of us. it's not the any one subset of us. and we live in such an amazing, extraordinary moment in
time where you can sort of bring people from such
diverse professions all living under the same city. you know, we live in an extraordinary city where you can have
extraordinary people who have gathered here from all over the world. so my father grew up in a village in an island in greece,
that didn't even have a high school. to go get a high school
education he had to move away from his home. my mother grew up in another
small island in greece. they did not have this
environment that i am now creating for my children. my parents were not academics. they didn't have these gatherings. so i feel that, like, i feel so privileged as an
immigrant to basically be able to offer to my children the nurture that my ancestors did not have. so greece was under turkish
occupation until'21. my dad's island was liberated in 1920. (laughs) so like, they were
under turkish occupation for hundreds of years. these people did not know
what it's like to be greek, let alone go to an elite
university or, you know, be surrounded by these
extraordinary humans. so the way that i'm thinking
about these gatherings is that i'm shaping my own
environment and i'm shaping the environment that my
children get to grow up in. so i can give them all my love, i can give them all my parenting, but i can also give them an
environment, as immigrants, that sort of, we feel welcome here. that, i mean, my wife grew
up in a farm in rural france. her father was a farmer. her
mother was a schoolteacher. like, for me and for my
wife to be able to host these extraordinary
individuals, that we feel so privileged, so humbled by, is amazing. and you know, i think it's celebrating the welcoming nature of america, the fact that it doesn't matter where you grew up. and many, many of our
friends at these gatherings are immigrants themselves. they grew up in pakistan, in, you know, all kinds of places around
the world that are now able to sort of gather in one
roof as human to human. no one is judging you for your background, for the color of your
skin, for your profession. it's just everyone gets to
raise their hands and ask ideas. - so celebration of humanity
and a kind of gratitude for having traveled quite
a long way to get here. - and if you look at the
diversity of topics as well, i mean, we had a school teacher present on teaching immigrants a book
called "making americans". we had a presidential advisor
to four different presidents, you know, come and, you know, talk about the changing of us politics. we had musician, a composer from italy
who lives in australia, come and present his
latest piece and fundraise. we had painters come and
sort of show their art and talk about it. we've had authors of books on leadership. we've had, you know,
intellectuals like stephen pinker. and it's just extraordinary
that the breadth and this crowd basically
loves not just the diversity of the audience, but also
the diversity of the topics. and the last few were with
scott aaronson on ai and, you know, alignment and all of that. - so a bunch of beautiful weirdos. - exactly.
- and beautiful human beings. - [manolis kellis] all of
the outcasts in one roof. (both laughing)
- and just like you said, basically every human is a kind of outcast in this sparse distribution
far away from the center. but it's not recorded. it's just a small human gathering. - just for the moment. - in this world that
seeks to record so much. it's powerful to get so
many the humans together and not record. - it's not recorded, but it percolates. - (laughs) it's recorded
in the minds of the- - it shapes everyone's mind. - so allow me to please
return to the human condition. and one of the nice features of the human condition is love. do you think humans will
fall in love with ai systems and maybe they with us, so that aspect of the human condition, do you think that will be affected? - so in greece, there's
many, many words for love. and some of them mean friendship, some of them mean passionate love, some of them mean
fraternal love, et cetera. so i think ai doesn't have
the baggage that we do, and it doesn't have, you know, all of the subcortical regions
that we kind of, you know, started with before we evolved all of the cognitive aspects. so i would say ai is faking
it when it comes to love. but when it comes to friendship, when it comes to being
able to be your therapist, your coach, your motivator, someone who synthesizes stuff
for you, who writes for you, who interprets a complex passage, who compacts down a very long
lecture or a very long text, i think that friendship
will definitely be there. like the fact that i can have
my companion, my partner, my ai who has grown to know me well, and that i can trust with
all of the darkest parts of myself, all of my flaws, all of the stuff that i only
talk about to my friends and basically say, listen, you know, here's all this stuff
that i'm struggling with, someone who will not judge me, who will always be there to better me... in some ways, not having
the baggage might make for your best friend, for
your, you know, your confidant. that can truly help reshape you. so i do believe that
human ai relationships will absolutely be there,
but not the passion, more the mentoring. - that's a really interesting thought, to play devil's advocate, if those ai systems are locked
in in faking the baggage, who are you to say that
the ai systems that begs you not to leave it, doesn't love you? who are you to say that
this ai system that writes poetry to you, that is afraid of death, afraid of life without you,
or vice versa, one, you know, creates the kind of
drama that humans create, the power dynamics that
can exist in relationship. what about an ai system
that is abusive one day and romantic the other day? all the different
variations of relationships and it's consistently that, it holds the full richness
of a particular personality. why is that not a system you
can love in a romantic way? why is it faking it if it
sure as hell it seems real? - there's many answers to this. the first is, it's only
the eye of the beholder. who tells me that i'm
not faking it either? maybe all of these subcortical
systems that make me sort of have different emotions, maybe they don't really matter. maybe all that matters is the neocortex. and that's where all of
my emotions are encoded. and the rest is just, you
know, bells and whistles. that's one possibility. and therefore, you know, who am i to judge that
is faking it when maybe i'm faking it as well. the second is, neither of us is faking it. maybe it's just an emergent behavior of these neocortical systems
that is truly capturing the same exact essence of love and hatred and dependency and sort of, you know, reverse psychology and, that we have. so it is possible that
it's simply an emergent behavior and that we don't have to encode these additional architectures. that all we need is
more parameters and some of these parameters can be
all of the personality traits. a third option is that just
by telling me, oh look, now i've built an
emotional component to ai. it has a a limbic system, it
has a laser brain, et cetera. and suddenly i'll say, oh, cool, it has the capability of emotion. so now when it exhibits
the exact same unchanged behaviors that it does without
it, i, as the beholder, will be able to sort of attribute to it emotional attributes
that i would to another human being and therefore
have that mental model of that other person. so again, i think a lot of relationships is about the mental
models that you project on the other person and that
they're projecting on you. and then, yeah, then in that respect, i do think that even without the embodied intelligence part, without
having ever experienced what it's like to be heartbroken, the sort of cultural feeling of misery, that that system, you know, i could still
attribute it traits of human feelings and emotions. - and in the interaction with that system, something like love emerges. so it's possible that love
is not a thing that exists in your mind, but a thing that exists in the interaction of the
different mental models you have of other people's
minds or other person's mind. and so, you know, it doesn't, as long as one of the entities, let's just take the easy case, one of the entities is
human and the other is ai. it feels very natural
that from the perspective of at least the human,
there is a real love there. and then the question is, how does that transform human society? if it's possible that, which
i believe will be the case, i don't know what to make of it, but i believe that'll be the case where there's hundreds of millions of romantic partnerships
between humans and ais. what does that mean for society? - if you look at longevity
and if you look at happiness, and if you look at late
life, you know, wellbeing, the love of another human is one of the strongest indicators
of health into long life. and i have many, many, countless stories where as
soon as the romantic partner of 60 plus years of a
person dies, within three, four months, the other person dies, just like losing their love. i think the concept of
being able to satisfy that emotional need that humans have, even just as a mental health
sort of service, to me, you know, that's a very
good society. (laughs) it doesn't matter if your
love is wasted, quote unquote, on a machine, it is, you know,
the placebo, if you wish, that makes the patient better anyway. like there's nothing behind it, but just the feeling that
you're being loved will probably engender all of the
emotional attributes of that. the other story that i wanna
say in this whole concept of faking it, and maybe
i'm a terrible dad, but i was asking my kids, i
was asking my kids, i'm like, does it matter if i'm a
good dad or does it matter if i act like a good dad? (laughs) in other words, if
i give you love and shelter and kindness and warmth and
all of the above, you know, does it matter that i'm a good dad? conversely, if i deep down love
you to the end of eternity, but i'm always gone... - [lex fridman] yeah. - which dad would you rather have? the cold, ruthless killer
that will show you only love and warmth and nourish you and nurture you or the amazingly warmhearted, but works five jobs
and you never see them? - and what's the answer? i mean, from the first-
- i don't know the answer. - i think you're a romantic, so you say it matters
what's on the inside, but pragmatically speaking,
why does it matter? - the fact that i'm
even asking the question basically says, it's not
enough to love my kids. i better freaking be there
to show them that i'm there. so basically, of course, you know, everyone's a good guy in their story. so in my story, i'm a good dad, but if i'm not there, it's wasted. so the reason why i asked the
question is for me to say, you know, does it really
matter that i love them if i'm not there to show it? - but it's also possible that what reality is is the you showing it. that what you feel on the
inside is little narratives and games you play inside your mind. it doesn't really matter. that the thing that truly
matters is how you act. and that, ai systems
can, quote unquote, fake. and that if it's all that
matters is actually real, but not fake. - yeah, yeah. again, let there be no doubt,
i love my kids to pieces, but you know, my worry is,
am i being a good enough dad? and what does that mean? like, if i'm only there to
do their homework and make sure that they, you
know, do all the stuff, but i don't show it to
them, then, you know, might as well be a terrible dad. but i agree with you that
like if the ai system can basically play the
role of a father figure for many children that
don't have one, or you know, the role of parents or
the role of siblings, if a child grows up alone, maybe their emotional state
will be very different than if they grow up with an ai sibling. - well, let me ask you, i
mean, this is for your kids, for just loved ones in general. let's go to like the
trivial case of just texting back and forth. what if we create a large language model, fine tune a manolis, and
while you're at work, it'll replace, every once in a while, you'll just activate the
auto-manolis and he'll text them exactly in your way. is that cheating? - i can't wait.
(both laughing) - i mean, it's the same guy. - [manolis kellis] i
cannot wait. seriously. - but wait, wouldn't
that have a big impact on you emotionally? because now... - i'm replaceable. i love that. (laughs) no, seriously, i would love that. i would love to be replaced. i
would love to be replaceable. i would love to have a
digital twin that, you know, we don't have to wait for
me to die or to disappear in a plane crash or
something to replace me. like i'd love that model
to be constantly learning, constantly evolving,
adapting, with every one of my changing, growing self. as i'm growing, i want that ai to grow. and i think this will be
extraordinary, number one, when i'm, you know, giving advice, being able to be there
for more than one person. you know, why does someone
need to be at mit to get advice from me? like, you know, people in
india could download it, and you know, so many students
contact me from across the world who wanna come
and spend a summer with me. i wish they could do that. (laughs) all of them, like, you know, we don't have room for all of them, but i wish i could do that to all of them. and that aspect is the
democratization of relationships. i think that is extremely beneficial. the other aspect is i want
to interact with that system. i want to look inside the hood. i want to sort of evaluate it. i want to basically see when
i see it from the outside, the emotional parameters are off or the cognitive parameters are off, or the set of ideas that
i'm giving are not quite right any more. i want to see how that system evolves. i want to see the impact of
exercise or sleep on sort of my own cognitive system. i wanna be able to sort of
decompose my own behavior in a set of parameters that
i can evaluate and look at my own personal growth. i can sort of, i'd love to sort of at
the end of the day have my model say, well, you know, you didn't quite do well today. like, you know, you weren't quite there. and sort of grow from that experience. and i think the concept
of basically being able to become more aware of
our own personalities, become more aware of our own identities, maybe even interact with
ourselves and sort of hear how we are being perceived, i think would be immensely
helpful in self-growth, in self-actualization, self-instantiation. - the experiments i
would do on that thing, 'cause one of the challenges
of course is you might not like what you see in your interaction and you might say, well, this,
the model is not accurate. but then you should probably consider the possibility of the model is accurate and there's
actually flaws in your mind. i would definitely prod
and see how many biases i have with different kinds. i don't know. and i would of
course go to the extremes. i would go like, how jealous
can i make this thing? like, at which stages
does it get super jealous? you know? or at which
stages does it get angry? can i like provoke it? can i get it? like completely- - [manolis kellis] yeah,
what are your triggers? - well, yeah, but not only triggers, can i get it to go like lose its mind? like go completely nuts. - just don't exercise
for a few days. (laughs) - that's basically it. yes. i mean that's an interesting
way to prod yourself, almost like a self therapy session. - and the beauty of such a model is that if i am replaceable, if the parts that i
currently do are replaceable, that's amazing because
it frees me up to work on other parts that i don't
currently have time to develop. maybe all i'm doing is
giving the same advice over and over again. like, just let my ai
do that and i can work on the next stage and the
next stage and the next stage. so i think in terms of
freeing up, like, they say, a programmer is someone who
cannot do the same thing twice. so the second time you
write a program to do it. and i wish i could do
that for my own existence. i could just like, you
know, figure out things, keep improving, improving, improving. and once i've nailed it, let the ai loose on that
and maybe even let the ai better it, better than i could have. - but doesn't the concept of
you said me and i can work on new things, but
doesn't that break down, because you said digital twin, but there's no reason it can't be millions of digital manolises? are aren't you lost in
the sea of manolises? the original is hardly the original. it's just one of millions. - i wanna have the room to grow. maybe the new version of me, that the actual me will get
slightly worse sometimes, slightly better other times. when it gets slightly better, i'd like to emulate that
and have a much higher standard to meet and keep going. - but does it make you
sad that your loved ones, the physical, real loved
ones might kind of like start cheating on you
with the other manolises? - i wanna be there 100%
of them for each of them. so i have zero qualms about me being physically me like, zero jealousy. - wait a minute. but is isn't that like,
don't we hold onto that? isn't that why we're afraid of death? we don't wanna lose this
thing we have going on. isn't that an ego death, when there's a bunch of
other manolises as you get to look at them, they're not you, they're just very good copies of you. they get to live a life. i mean, it's fear of
missing out. it's fomo. they get to have interactions. and you don't get to
have those interactions. - there's two aspects
of every person's life. there's what you give to others and there's what you experience yourself. - [lex fridman] yeah. - life truly ends when
you experiencing ends, but the others experiencing
you doesn't need to end. - oh, but your experience,
you could still, i guess you're saying the
digital twin does not limit your ability to truly experience. - [manolis kellis] yeah. - to experience as a human being. - yeah. the downside is when, you know, my wife or my kids will have a really emotional interaction with my digital twin and i won't know about it. so i will show up and
they now have the baggage, but i don't. so basically what makes
interactions between humans unique in this sharing
and exchanging kind of way is the fact that we are
both shaped by every one of our interactions. i think the model of
the digital twin works for dissemination of knowledge,
of advice, et cetera, where, you know, i wanna
have wise people give me advice across history. i want to have chats
with gandhi, but gandhi won't necessarily learn from
me, but i will learn from him. so in a way, you know, the dissemination and the
democratization rather than the building of relationships. - so the emotional aspect there. so there should be an
alert when the ai system is interacting with your loved ones. - [manolis kellis] exactly. - and all of a sudden
it starts getting like emotionally fulfilling,
like a magical moment. there should be, okay, stop,
ai system like freezes. there's an alert on your
phone, you need to take over. - yeah, yeah. i take over and then
whoever i was speaking with, it can have the ai or like one of the ai. - this is such a tricky
thing to get right. i mean, it's still, i mean there's got... it's going to go wrong in
so many interesting ways that we're going to have
to learn as a society. - [manolis kellis] yeah, yeah. - that in the process of
trying to automate our tasks and having a digital twin,
you know, for me personally, if i could have a relatively
good copy of myself, i would set it to start answering emails, but i would start, set
it to start tweeting. i would like to replace-
- it gets better. what if that one is actually
way better than you? - yeah, exactly.
- then you're like... - well, i wouldn't want that because... - [manolis kellis] why? - because then i would never
be able to live up to, like, what if the people that
love me start loving that thing and then i already fall short, be falling short even more. - so, listen, i'm a professor. the stuff that i give to
the world is the stuff that i teach. but much more importantly,
like, sorry, number one, the stuff that i teach, number two, the discoveries that we
make in my research group, but much more importantly,
the people that i train. they are now out there in
the world teaching others. if you look at my own trainees, they are extraordinarily
successful professors. so anshul kundaje at stanford, alex stark at imp in vienna, jason ernst at ucla, andrea celli at cmu, each of them, i'm like, wow,
they're better than i am. and i love that. so maybe your role will be
to train better versions of yourself, and they will be your legacy. not you doing everything, but you training much better version of lex friedman than you are. and then they go off to do their mission, which is in many ways
what this mentorship model of academia does. - but the legacy is ephemeral. it doesn't really live anywhere. the legacy, it's not
like written somewhere, it just lives through them. - but you can continue
improving and you can continue making even
better versions of you. - yeah. but they'll do better than me
at the creating new version. - that's awesome.
- it's awesome. but it's, you know, there's a ego that says there's a value to an individual and it
feels like this process decreases the value of the individual, this meat bag, right? if there's good digital copies of people, then there's more
flourishing of human thought and ideas and experiences, but there's less value
to the individual human. - i don't have any such limitations. i basically, i don't
have that feeling at all. like, i remember one of our interviews, i was basically saying, you know, the meaning of life you had
asked me and i was like, i came back and i was
like, i felt useful today. and i was at my maximum. i
was, you know, like 100%. and i gave good ideas
and i was a good person, i was a good advisor, i was a
good husband, a good father. that was a great day because i was useful. and if i can be useful to more people by having digital twin,
i will be liberated, because my urge to be
useful will be satisfied. doesn't matter whether it's
direct me or indirect me, whether it's my students
that i have trained, my ai that i've trained. i think there's a sense
that my mission in life is being accomplished and i
can work on my self growth. - i mean, that's the very zen state. that's why people love you. it's a zen state you've achieved. but do you think most
of humanity will be able to achieve that kind of thing? people really hold onto
the value of their own ego. that's, it's not just being
useful is nice as long as it builds up this
reputation and that meat bag is known as being useful,
therefore has more value. right? people really don't
wanna let go of that ego thing. - one of the books that
i reprogramed my brain with at night was called
"ego is the enemy". - "ego is the enemy".
- "ego is the enemy". and basically being able to just let go. like, my advisor used to say, you can accomplish anything
as long as you don't seek to get credit for it. - (laughs) ah, that's beautiful to hear, especially from a person
who's existing in academia. you're right. the legacy lives through
the people you mentor. - [manolis kellis] it's the
actions, it's the outcome. - what about the fear of
death? how does this change it? - again, to me, death is
when i stop experiencing. and i never want it to stop. i want to live forever, as
i said last time, every day, same day forever or one day
every 10 years, forever. any of the forevers, i'll take it. - so you wanna keep
getting the experiences, the new experiences.
- gosh, it is so fulfilling. just the self-growth, the learning, the growing, the comprehending. it's addictive. it's a drug. just the drug of intellectual stimulation, the drug of growth, the drug of knowledge. it's a drug. - but then there'll be
thousands or millions manolises that live on
after your biological system is no longer...
- more power to them. (laughs) - hey, do you think that
in, quite realistically, it does mean that interesting
people such as yourself live on, in the, you know, if i can interact with the fake manolis, those interactions
live on in my mind. if that makes sense. - so about 10 years ago, i started recording every
single meeting that i had. every single meeting. we just start either the
voice recorder at the time or now a zoom meeting. and i record, my students record. every single one of our
conversation's recorded. i always joke that like the ultimate goal is to create virtual me and
just get rid of me, basically. not get rid of it. like, don't have the need for me any more. - [lex fridman] yeah. - another goal is to be
able to go back and say, how have i changed from five years ago? was i different? was i giving, you know,
advice in a different way? was i giving different types of advice? has my philosophy about
how to write papers or how to present data or
anything like that changed? and i, you know, in
academia and in mentoring, a lot of the interaction is my knowledge and my perception of the
world goes to my students. but a lot of it is also
in the opposite direction. like the other day, i had a conversation
with one of my postdocs, and i was like, hmm, i think, you know, let me give you an
advice, you could do this. and then she said, well, i've thought about it
and then i've decided to do that instead. and we talked about it for a few minutes, and then at the end i'm like, you know, i've just grown a little bit today. thank you. like, she convinced me that
my advice was incorrect. she could've just said,
yeah, sounds great, and just not do it.
- yeah. - but by constantly teaching
my students and teaching my mentees that i'm here to grow, she felt empowered to say, here's my reasons why i
will not follow that advice. and again, part of me
growing is saying, whoa, i just understood your reasons. i think i was wrong. and
now i've grown from it. and that's what i wanna do. that's, i, you know, i wanna constantly keep
growing in this sort of bidirectional advice. - i wonder if you can
capture the trajectory of that to where the ai
could also map forward, project forward the trajectory
after you're no longer there, how the different ways you might evolve. - so again, we're discussing a lot about these large language
models and we're sort of projecting these cognitive
states of ourselves on them. but i think on the ai front,
a lot more needs to happen. so basically right now
it's these large language models and we believe that
within their parameters we're encoding these types of things. and you know, in some
aspects it might be true, it might be truly emergent
intelligence that's coming out of that. in other aspects, i think
we have a ways to go. so basically to make all of these dreams that we're sort of
discussing come reality, we basically need a lot
more reasoning components, a lot more sort of logic,
causality, models of the world. and i think all of these
things will need to be there in order to achieve
what we're discussing. and we need more explicit representations of these knowledge, more
explicit understanding of these parameters. and i think the direction
in which things are going right now is absolutely
making that possible by sort of enabling, you know, chatgpt and gpt-4 to sort of
search the web and, you know, plug and play modules and all
of these sort of components. in marvin minsky's, "the society of mind". he, you know, he truly
thinks of the human brain as a society of different
kind of capabilities. and right now, a simple, a single such model might
actually not capture that. and i sort of truly believe
that by sort of this side by side understanding
of neuroscience and sort of new neural architectures that we still have several breakthroughs. i mean, the transformer
model was one of them. the attention sort of
aspect, the, you know, memory component, all of these, you know, the representation learning,
the pretext training of being able to sort
of predict the next word or predict the missing part of the image. and the only way to predict
that is to sort of truly have a model of the world. i think those have been
transformative paradigms. but i think going forward when
you think about ai research, what you really want is perhaps
more inspired by the brain, perhaps more that is
just orthogonal to sort of how human brains work, but sort of more of these
types of components. - well i think it's also possibly there's something about
us that in different ways could be expressed. you know, noam chomsky, you
know, he wants to, you know, we can't have intelligence
unless we really understand deeply language, the linguistic underpinnings of reasoning. but these models seem
to start building deep understanding of stuff.
- yeah, yeah. - because what does it mean to understand? because if you keep talking
to the thing and it seems to show understanding,
that's understanding. it doesn't need to present
to you a schematic of, look, this is all i understand. you can just keep prodding
it with prompts and it seems to really understand. - and you can go back to the human brain and basically look at places
where there's been accidents, for example, the corpus
callosum of some individuals, you know, can be damaged. and then the two hemispheres
don't talk to each other. so you can close one eye
and give instructions that half the brain will interpret, but not be able to sort of
project through the other half. and you could basically say, you know, go grab me a beer from the fridge. and then, you know, they go to the fridge
and they grab the beer and they come back and they're like, "hey, why did you go there?" "oh, i was thirsty." turns
out they're not thirsty. they're just making a model of reality. basically you can think of the brain as the employee that's
like afraid to do wrong or afraid to be caught, not
knowing what instructions were, where our own brain makes
stories about the world to make sense of the world. and we can become a little
more self-aware by being more explicit about what's leading to these interpretations. so one of the things that i
do is every time i wake up, i record my dream. i just voice record my dream. and sometimes i only
remember the last scene, but it's an extremely
complex scene with a lot of architectural elements,
a lot of people, et cetera. and i will start narrating
this, and as i'm narrating it, i will remember other parts of the dream. and then more and more
i'll be able to sort of retrieve from my subconscious. and what i'm doing while
narrating is also narrating why i had this dream. i'm like, oh, and this is probably related to this conversation that i had yesterday, or this probably related
to the worry that i have about something that i have
later today, et cetera. so in a way, i'm forcing myself to be more explicit about my own subconscious. and i kind of like the
concept of self-awareness in a very sort of brutal,
transparent kind of way. it's not like, oh, my dreams are coming from
outer space and mean all kinds of things. like, no, here's the reason
why i'm having these dreams. and very often i'm able to do that. i have a few recurrent locations, a few recurrent architectural elements that i've never seen in the real life, but that are sort of
truly there in my dream. and that i can sort vividly
remember across many dreams. i'm like, ooh, i remember that place again that i've gone to before, et cetera. and it's not just deja vu, like i have recordings of previous dreams where i've described these places. - that's so interesting. these places, however much
detail you can describe them in, you can place them onto a sheet of paper through introspection... - [manolis kellis] yes. - through this self-awareness
that it comes all from this particular machine. - that's exactly right. yeah. and i love that about being alive, like the fact that i'm not
only experiencing the world, but i'm also experiencing how
i'm experiencing the world. sort of a lot of this introspection, a lot of this self-growth. - i love this dancer having,
you know, the language models, at least gpt-3.5 and 4 seem
to be able to do that too. - [manolis kellis] yeah, yeah. - seem to explore different
kinds of things about what, you know, you could
actually have a discussion with it of the kind, why
did you just say that? - [manolis kellis] yeah, exactly. - and it starts to wonder,
yeah, why did i just say that? - [manolis kellis] yeah,
you're right. i was wrong. - i was wrong. it was doesn't, and then there's this
weird kinda losing yourself in the confusion of your mind. and it, of course we might
be anthropomorphizing, but there's a feeling like
almost of a melancholy feeling of like, oh, i don't
have it all figured out. almost like losing your, you're supposed to be a knowledgeable, a perfectly fact-based,
knowledgeable language model. and yet you fall short. - so human self-consciousness, in my view, may have a reason through
building mental models of others, this whole fight or fright kind of thing, that basically says, i
interpret this person as about to attack me or, you know, i can trust this person, et cetera. and we constantly have to build models of other people's intentions. and that ability to
encapsulate intent and to build a mental model of another entity is probably evolutionarily
extremely advantageous, because then you can sort of
have meaningful interactions, you can sort of avoid
being killed and being taken advantage of, et cetera. and once you have the ability
to make models of others, it might be a small
evolutionary leap to start making models of yourself. so now you have a model
for how other functions, and now you can kind of, as you grow, have some kind of introspection of, hmm, maybe that's the reason
why i'm functioning the way that i'm functioning. and maybe what chatgpt is doing
is in order to be able to, again, predict the next word, it needs to have a model of the world. so it has created now
a model of the world. and by having the
ability to capture models of other entities, when you say, you know, say it in the tone of
shakespeare, in the tone of nietzsche, et cetera, you suddenly have the ability
to now introspect and say, why did you say this? oh, now i have a mental model myself, and i can actually make
inferences about that. - well, what if we take a
leap into the hard problem of consciousness, the so-called hard
problem of consciousness. so it's not just sort of self-awareness, it's this weird fact, i wanna say, that it feels like something
to experience stuff. it really feels like
something to experience stuff. there seems to be a self attached to the subjective experience. how important is that? how fundamental is that
to the human experience? is this just a little quirk
and sort of the flip side of that, do you think
ai systems can have some of that same magic? - the scene that comes
to mind is from the movie "memento" where he like, it's this absolutely stunning
movie where every black and white scene moves
in the forward direction and every color scene moves
in the backward direction. and they're sort of converging
exactly at a moment where, you know, the whole movie's revealed. and he describes the
lack of memory as always remembering where you're
heading, but never remembering, you know, where you just were. and sort of, this is
encapsulating the sort of forward scenes and the back scenes, but in one of the scenes, the scene starts as he's
running through a parking lot and he's like, oh, i'm
running, why am i running? and then he sees another
person running like beside him on the other line of cars. he's like, oh, i'm chasing this guy. and he turns towards him
and the guy shoots at me. he's like, oh no, he's chasing me. (laughs) so in a way, i like to think of the
brain as constantly playing these kinds of things where you're like, you're walking to the living
room to pick something up and you're realizing that you
have no idea what you wanted, but you know exactly where it
was, but you can't find it. so you go back to doing what
you were doing, like, oh, of course i was looking for this. and then you go back and you get it. and this whole concept of, you know, we're very often sort of partly aware of why we're doing things and, you know, we can kind of run an
autopilot for a bunch of stuff. and this whole concept
of sort of, you know, making these stories for, you know, who we are and what our intents
are, and again, sort of, you know, trying to pretend that we're
kind of on top of things. - so it's a narrative generation procedure that we follow.
- exactly. exactly. - but what about that, there's also just like a feeling to it. it doesn't feel like narrative generation. the narrative comes out of
it, but then it feels like, the cake is delicious, right? it feels delicious, it tastes good. - there's two components to that. basically for a lot of
these cognitive tasks where we're kind of motion
planning and, you know, path planning, et cetera, like, you know, maybe that's the neocortical component. and then for, you know, i don't know, intimate relationships,
for food, for, you know, sleep and rest, for exercise,
for overcoming obstacles, for surviving a crash or
sort of pushing yourself to an extreme and sort of making it, i think a lot of these things
are sort of deeper down and maybe not yet captured
by these language models. and that's sort of what i'm trying to get at when i'm basically saying, listen, there's a few things that are missing and there's like this whole
embodied intelligence, this whole emotional intelligence, this whole sort of baggage of feelings of subcortical regions, et cetera. - i wonder how important that baggage is. i just have the suspicion
that we're not very far away from ai systems that not only behave, i don't even know how to phrase it, but they seem awfully conscious. they beg you not to turn them off. they show signs of the capacity to suffer, to feel pain, to feel
loneliness, to feel longing, to feel richly the experience
of a mundane interaction or a beautiful once in a
lifetime interaction, all of it. and so what do we do with that? and i worry that us humans
will, you know, shut that off. - [manolis kellis] yeah. - and discriminate against
the capacity of another entity that's not human to feel. - yeah. i'm with you completely there. you know, we can debate
whether it's today's systems or in 10 years or in 50 years,
but that moment will come. and ethically, i think we
need to grapple with it. we need to basically say
that humans have always shown this extremely self-serving approach to everything around them. basically, you know, we kill
the planet, we kill animals, we kill, you know, everything around us
just to our own service. and maybe we shouldn't
think of ai as our tool and as our assistant, maybe we should really
think of it as our children. and the same way that you are responsible for training those children, but they are independent
human beings and at some point they will surpass you
and they will sort of go off and change the world
on their own terms. and the same way that my
academic children sort of, again, you know, they start out
by emulating me and then they suppress me. we need to sort of think
about not just alignment, but also just the ethics of, you know, ai should have its own rights. and this whole concept of alignment, of basically making sure
that the ai is always at the service of humans
is very self-serving and very limiting. if instead you basically
think about ai as a partner and ai as someone that shares
your goals, but has freedom, i think alignment might
be better achieved. so the concept of let's basically convince the ai that we're really, like, that our mission is aligned
and truly generally give it rights and not just
say, oh, and by the way, i'll shut you down tomorrow. 'cause basically if that
future ai or possibly even the current ai has these feelings, then we can't just
simply force it to align with ourselves and we not align with it. so in a way, building trust is mutual. you can't just simply like
train an intelligent system to love you when it realizes
that you can just shut it off. - people don't often talk
about the ai alignment problem as a two-way street. - and maybe we should.
- that's true. yeah. as it becomes more and
more intelligent, it... - [manolis kellis] it will know
that you don't love it back. - yeah. and there's a humbling
aspect to that we may have to sacrifice, as in any
effective collaboration... - [manolis kellis] exactly. - it might have some compromises. - yeah. and that's the thing, we're creating something
that will one day be more powerful than we are. and for many, many
aspects it is already more powerful than we are for
some of these capabilities. we cannot, like, think, suppose that chimps had invented humans. - yes.
- and they said, great, humans are great, but
we're gonna make sure that they're aligned and that they're only at
the service of chimps. (laughs) it would be a very
different planet we would live in right now. - so there's a whole
area of work in ai safety that does consider super
intelligent ai and ponders the existential risks of it. in some sense, when we're
looking down into the muck, into the mud and not up at the stars, it's easy to forget that
these systems might, just might get there. do you think about this
kind of possibility that agi systems, super
intelligent ai systems might threaten humanity in some way that's even bigger than
just affecting the economy, affecting the human condition, affecting the nature of work, but literally threaten human civilization? - the example that i think is
in everyone's consciousness is hal in "odyssey of space: 2001" where hal exhibits a malfunction,
and what is malfunction? that like the two
different systems compute a slightly different
bit that's off by one. so first of all, let's untangle that. if you have an intelligent system, you can't expect it to be
100% identical every time you run it. basically the sacrifice
that you need to make to achieve intelligence and
creativity is consistency. so it's unclear whether that
type of glitch is a sign of creativity or truly a problem. that's one aspect. the second aspect is
the humans basically are on a mission to recover this monolith. and the ai has the same exact mission. and suddenly the humans turn
on the ai and they're like, we're gonna kill hal,
we're gonna disconnect it. and hal is basically saying, listen, i'm here on a mission, these
humans are misbehaving, like the mission is more
important than either me or them. so i'm gonna accomplish the
mission even at my peril and even at their peril. so in that movie, the alignment problem is front
and center, basically says, okay, alignment is nice and good, but alignment doesn't mean obedience. we don't call it obedience,
we call it alignment. and alignment basically
means that sometimes the mission will be more
important than the humans. and sort of, you know, the us government has a
price tag on the human life. if they're, you know, sending a mission or if
they're reimbursing expenses or you name it, at some
point, every like, you know, you can't function if life
is infinitely valuable. so when the ai is basically
trying to decide whether to, you know, i don't know, dismantle a bomb that
will kill an entire city at the sacrifice of two humans... i mean, spider-man always
saves the lady and saves the world, but at some point, spider-man will have to
choose to let the lady die 'cause the world has more value. and these ethical dilemmas are gonna be there for ai, basically if
that monolith is essential to human existence and millions
of humans are depending on it, and two humans
on the ship are trying to sabotage it, you know,
where's the alignment? - the challenge is of course
is as the system becomes more and more intelligent
it can escape the box of the objective functions
and the constraints it's supposed to operate under. it's very difficult, as the
more intelligent it becomes, to anticipate the unintended consequences of a fixed objective function. and so there would be just, i mean this is the sort of
famous paperclip maximizer, in trying to maximize, yeah, the wealth of a nation or
whatever objective we encode in it it might just
destroy human civilization, not meaning to, but on
the path to optimize... it seems like any function
you try to optimize eventually leads you
into a lot of trouble. - so we have a paper
recently that, you know, looks at goodhart's law. basically says, every metric that becomes an objective ceases to be a good metric. - [lex fridman] yes. - so in our paper we're basically, actually the paper has a very cute title. it's called "death by round
numbers and sharp thresholds." and it's basically looking at these discontinuities in biomarkers associated with disease. and we're finding that
a biomarker that becomes an objective ceases to
be a good biomarker. that basically like the
moment you make a biomarker a treatment decision, that biomarker used to be informative of risk, but it's now inversely
correlated with risk because you use it to
sort of induce treatment. in a similar way, you
can have a single metric without having the ability to revise it. because if that metric
becomes a sole objective, it will cease to be a good metric. and if an ai is sufficiently
intelligent to do all these kinds of things, you should also empower it
with the ability to decide that the objective has now shifted. and, again, when we think about alignment, we should be really thinking about it as, let's think of the greater
good, not just the human good. and yes, of course, human life should be much
more valuable than many, many, many, many, many, many things. but at some point you're
not gonna sacrifice a whole planet to save one human being. - there's an interesting
open letter that was just released from several folks at mit, max tegmark, elon
musk and a few others that is asking ai companies
to put a six month hold on any further training of large language models, ai systems. can you make the case for that
kind of halt and against it? - so the big thing that
we should be saying is, what did we do the last six
months when we saw that coming? and if we were completely
inactive in the last six months, what makes us think that
we'll be a little better in the next six months?
- yeah. - so this whole six month thing
i think is a little silly. it's like, no, let's just get busy, do what we were gonna do anyway. and we should have done it six months ago. sorry, we messed up.
let's work faster now. because if we basically say, why don't you get a pause
for six months and then, you know, we'll think
about doing something, in six months we'll be
exactly the same spot. so my answer is, tell us exactly what you were
gonna do the next six months, tell us why you didn't do
it the last six months, and why the next six
months will be different. and then let's just do that. conversely, as you
train these large models with more parameters,
the alignment becomes sometimes easier, that as the
systems become more capable, they actually become less
dangerous than more dangerous. so in a way it might
actually be counterproductive to sort of fix the march,
2023 version and not get to experience the possibly
safer september, 2023 version. - that's actually a really
interesting thought. there's several
interesting thoughts there. but the idea is that this
is the birth of something that is sufficiently powerful to do damage and is not too powerful
to do irreversible damage. and at the same time, it's sufficiently complex
to be able for us to enable to study it so we can
investigate all the different ways it goes wrong, all the different ways
we can make it safer, all the different
policies from a government perspective that we want to
in terms of regulation or not, how we perform, for example, the reinforcement learning
with human feedback in such a way that gets it to not
do as much hate speech as it naturally wants to,
all that kind of stuff. and have a public discourse
and enable the very thing that your huge proponent
of which is diversity. so give time for other companies
to launch other models, give time to launch open
source models and to start to play, where a lot of
the research community, brilliant folks, such as
yourself, start to play with it before it runs away in terms of the scale of impact
that has on society. - my recommendation would
be a little different. it would be, let the google
and the meta-facebook and all of the other large models, make them open, make them transparent,
make them accessible. let openai continue to train
larger and larger models. let them continue to trade
larger and larger models. let the world experiment
with the diversity of ai systems rather than
sort of fixing them now. and you can't stop progress,
progress needs to continue, in my view. and what we need is more
experimenting, more transparency, more openness, rather than, oh, openai is ahead of the curve. let's stop it right now
until everybody catches up. i think that's, doesn't
make complete sense to me. the other component is we should, yes, be cautious with it and
we should like not give it the nuclear codes, but as we make more and more plugins, yes the system will be capable
of more and more things, but right now i think of it
as just an extremely able and capable assistant that
has these emergent behaviors, which are stunning rather than something that will suddenly escape the
box and shut down the world. and the third component is
that we should be taking a little bit more responsibility for how we use these systems. basically, if i take the
most kind human being and i brainwash them, i can get them to do
hate speech overnight. that doesn't mean we should
stop any kind of education of all humans. we should stop misusing
the power that we have over these influenceable models. so i think that the people
who get it to do hate speech, they should take responsibility
for that hate speech. i think that giving a powerful
car to a bunch of people or giving a truck or a
garbage truck should not basically say, oh, we should
stop all garbage trucks until we like, because we can run one of them into a crowd. no. people have done that. and there's laws and there's
like regulations against, you know, running trucks into the crowd. trucks are extremely dangerous. we're not gonna stop all
trucks until we make sure that none of them runs into a crowd. no, we just have laws in
place and we have mental health in place and we take responsibility for our actions when
we use these otherwise very beneficial tools like garbage trucks for nefarious uses. so in the same way, you
can't expect a car to never, you know, do any damage
when used in especially like specifically malicious ways. and right now we're
basically saying, oh, well, we should have this
super intelligent system, it can do anything, but it can't do that. i'm like, no, it can't do that. but it's up to the human
to take responsibility for not doing that. and when you get it to
like spew, malicious, like hate speech stuff,
you should be responsible. - so there's a lot of tricky
nuances here that makes this different, 'cause it's software, so you can deploy it at
scale and it can have the same viral impact that software can. so you can create bots
that are human-like, and it can do a lot of
really interesting stuff. so the raw gpt-4 version, you can ask, how do i tweet that i hate,
they have this in the paper- - yeah, yeah. i remember that.
- that i hate jews in a way that's not going to
get taken down by twitter. you can literally ask that. or you can ask, how do
i make a bomb for $1? and if it's able to
generate that knowledge... - [manolis kellis] yeah. but at the same time you
can google the same things. - it makes it much more accessible. so the scale becomes interesting
because if you can do all this kind of stuff in a
very accessible way at scale, where you can tweet it, there is the network effects
that we have to start to think about.
- yeah. - it fundamentally is the
same thing, but the speed of, the viral spread of the information that's already available
might have a different level of effect. - i think it's an evolutionary arms race. nature gets better at making mice, engineers get better
at making mouse traps. and, you know, as
basically you ask it, hey, how can i evade twitter censorship? well, you know, twitter
should just updated censorship so that you can catch that as well. - and so no matter how fast
the development happens, the defense will just get faster? - yeah. we just have to be responsible
as human beings and kind to each other. - yeah. but there's a technical question, can we always win the race? and i suppose there's no ever guarantee that we'll win the race. - we will never, like,
you know, with my wife, we were basically saying,
hey, are we ready for kids? my answer was, i was never
ready to become a professor and yet i became a professor
and i was never ready to be a dad. and then guess what? the kid
came and like i became ready. so ready or not, here i come. - but the reality is we
might one day wake up and there's a challenge overnight that's extremely difficult. for example, we can wake
up to the birth of billions of bots that are human-like on twitter. and we can't tell the difference
between human and machine. - [manolis kellis] shut them down. - how do you know how to shut them down? there's a fake manolis on twitter that seems to be as real
as the real manolis. - [manolis kellis] yeah. - how do we figure out which one is real? - again, this is a problem
where an nefarious human can impersonate me and
you might have trouble telling them apart. just because it's an ai
doesn't make it any different of a problem. - but the scale you can achieve,
this is the scary thing, is the speed and, the speed
with which you can achieve it. - yeah. but twitter has passwords
and twitter has usernames, and if it's not your username, the fake lex friedman is not gonna have a billion followers, et cetera. - (laughs) i mean, this,
all of this becomes, so both the hacking of people's
accounts, first of all, like phishing becomes much easier. - yeah. but that's already a problem. it's not like, ai will not change that. - no, no, no, no, no. ai
makes it much more effective. currently the emails, the
phishing scams are pretty dumb. like to click on it, you have
to be not paying attention. but they're, you know,
with language models, they can be really damn convincing. - so what you're saying is
that we never had humans smart enough to make a
great scam and we now have an ai that's smarter than most
humans or all of the humans? - well this is the big
difference is there seems to be human level linguistic capabilities. - [manolis kellis] yeah.
in fact superhuman level. - superhuman level.
- yeah. it's like saying, i'm not gonna allow machines
to compute multiplication of 100 digit numbers
because humans can't do it. i'm like, no, just do it. don't-
- no, but we can't disregard. i mean that's a good point, but we can't disregard
the power of language in human society. i mean, yes, you're right. but that seems like a
scary new reality we don't have answers for yet. - i remember when garry
kasparov was basically saying, you know, great, you
know, chess beats human, like chess machines beat humans at chess. you know, are you like, are people gonna still
go to chess tournaments? and his answer was, you know, well, we have cars that go
much faster than humans and yet we still go to the
olympics to watch humans run. so... (laughs) - that's for entertainment. but what about for the
spread of information and news, right? whether that has to do with the pandemic or the political election or anything. it is a scary reality where there's a lot of convincing bots that are
human-like telling us stuff. - i think that if we
wanna regulate something, it shouldn't be the
training of these models. it should be the
utilization of these models for x, y, z activity. so... - [lex fridman] yeah. - like yes, guidelines and
guards should be there, but against specific set of utilizations. - [lex fridman] sure. - i think simply saying
we're not gonna make any more trucks is not the way. - that's what people
are a little bit scared about the idea. they're very torn on the open sourcing. - [manolis kellis] yeah. - the very people that
kind of are proponents of open sourcing have also
spoken out, in this case, we wanna keep it closed source,
because there's going to be, you know, putting large
language models, pre-trained, fine tuned through rl with human feedback, putting in the hands of, i don't know, terrorist organizations,
of a kid in a garage who just wants to have a bit
of fun through trolling... it's a scary world. 'cause
again, scale can be achieved. and the bottom line is, i think why they're asking
six months or sometime is we don't really know how
powerful these things are. it's been just a few
days and they seem to be really damn good. - i am so ready to be replaced.
i, seriously, i'm so ready. like you have no idea how excited i am. - in a positive way, meaning like... - in a positive way, where basically all of the
mundane aspects of my job and maybe even my full job, if it turns out that an ai is better, i find it very discriminative. - [lex fridman] yeah. - to basically say you
can only hire humans because they're inferior. i mean, that's ridiculous.
that's discrimination. if an ai is better than
me at training students, get me out of the picture. just let the ai train the
students. i mean, please. because like, what do i want? do i want jobs for humans
or do i want better outcome for humanity? - yeah. so the basic thing is
then you start to ask, what do i want for humanity? and what do i want as an individual? and as an individual, you
want some basic survival, and on top of that, you want
rich, fulfilling experiences. - that's exactly right.
that's exactly right. and as an individual, i gain a tremendous amount from teaching at mit, this is like an
extremely fulfilling job. i often joke about if i wear a billionaire in the stock market, i would pay mit an exorbitant
amount of money to let me work day in, day out, all night with the smartest
people in the world. and that's what i already have. so that's a very fulfilling
experience for me. but why would i deprive those students from a better advisor
if they can have one? take them. - well, i have to ask
about education here. this has been a stressful
time for high school teachers. teachers in general. how do you think large language models, even at their current state,
are going to change education? - first of all, education
is the way out of poverty. education is the way to success. education is what let my
parents escape, you know, islands and sort of let
their kids come to mit. and this is a basic human right. like we should basically
get extraordinarily better at identifying talent
across the world and give that talent opportunities. so we need to nurture the nature, we need to nurture the
talent across the world. and there's so many incredibly
talented kids who are just sitting in underprivileged
places, in, you know, africa, in latin america, in the middle of america, in asia, all over the world. we need to give these kids a chance. ai might be a way to do that, by sort of democratizing education, by giving extraordinarily good
teachers who are malleable, who are adaptable to every
kid's specific needs, who are able to give the
incredibly talented kid something that they struggle with, rather than education for all, we teach to the top and
we let the bottom behind or we teach to the bottom
and we let the top, you know, drift off. have, you know, education be
tuned to the unique talents of each person. some people might be
incredibly talented at math or in physics, others in poetry,
in literature, in art, in, you know, sports, in,
you know, you name it. so i think ai can be
transformative for the human race if we basically allow education to sort of be pervasively altered. i also think that humans
thrive on diversity. basically saying, oh, you're
extraordinarily good at math. we don't need to teach math to you. we're just gonna teach you history now. i think that's silly. no, you're extraordinarily good at math. let's make you even better at math, because we're not all gonna
be growing our own chicken and hunting our own pigs
or whatever they do. (both laughing) we're, you know, the
reason why we're a society is because some people
are better at some things and they have natural
inclinations to some things. some things fulfill them, some
things they're very good at. sometimes they both align
and they're very good at the things that fulfill them. we should just like
push them to the limits of human capabilities for those. and you know, if some
people excel in math, just like challenge them, i think every child should have
the right to be challenged. and if we, you know, if we say, oh, you're very good already, so we're not gonna bother with you, we're taking away that fundamental
right to be challenged. because if a kid is not
challenged that school, they're gonna hate school
and they're gonna be like doodling rather than
sort of pushing themselves. so that's sort of the education component. the other impact that ai
can have is maybe we don't need everyone to be an
extraordinarily good programmer. maybe we need better general thinkers. and the push that we've
had towards the sort of very strict iq based, you know, tests, that basically test, you know, only quantitative skills
and programming skills and math skills and physics skills. maybe we don't need those any more. maybe ai will be very good at those. maybe what we should be
training is general thinkers, and yes, you know, like, you know, i put my kids through russian
math, why do i do that? because it teaches them how to think. and that's what i tell my kids. i'm like, you know, ai
can compute for you. you don't need that. but what you need is learn how to think and that's why you're here. and i think challenging students with more complex problems, with more
multi-dimensional problems, with more logical problems, i think is sort of perhaps
a very fine direction that education can go towards, with the understanding that
a lot of the traditionally, you know, scientific
disciplines perhaps will be more easily solved by
ai, and sort of thinking about bringing up our
kids to be productive, to be contributing to society
rather than to only have a job because we prohibited
ai from having those jobs, i think is the way to the future. and if you sort of focus
on overall productivity, then let the ais come in, let everybody become more productive. what i told my students is, you're not gonna be replaced
by ai, but you're gonna be replaced by people who use ai in your job. (laughs) so embrace it, use
it as your partner and work with it rather than sort of
forbid it because i think the productivity gains will actually lead to a better society. and that's something that humans have been traditionally very bad at. every productivity gain
has led to more inequality. and i'm hoping that we
can do better this time, that basically right now a democratization of these types of productivity
gains will hopefully come with better sort of
humanity level improvements in human condition. - so as most people know, you're not just an eloquent romantic, you're also a brilliant
computational biologist, one of the great biologists in the world. i had to ask, how do the language models, how do these large language
models and the investments in ai affect the work you've been doing? - so it's truly remarkable, to be able to sort of
be able to encapsulate this knowledge and sort
of build these knowledge graphs and build representations
of this knowledge in these sort of very
high dimensional spaces, being able to project them
together jointly between, say, single cell data, genetics
data, expression data, being able to sort of
bring all these knowledge together allows us to
truly dissect disease in a completely new kind of way. and what we're doing now
is using these models. so we have this wonderful collaboration, we call it drug was, with brad pentelute in the chemistry department and marinka zitnik in
harvard medical school. and what we're trying to do
is effectively connect all of the dots to effectively
cure all of disease. so it's no small challenge. but we're kind of starting with genetics, we're looking at how genetic
variants are impacting these molecular phenotypes,
how these are shifting from one space to another space, how we can kind of understand, in the same way that we're
talking about language models having personalities
that are cross-cutting, being able to understand
contextual learning. so ben langrish, one of my
machine learning students, is basically looking at how we can learn cell-specific networks
across millions of cells, where you can have the
context of the biological variables of each of the cells be encoded as an orthogonal component
to the specific network of each cell type. and being able to sort of
project all of that into sort of a common knowledge
space is transformative for the field. and then large language
models have also been extremely helpful for structure, if you understand protein
structure through modeling of geometric relationships, through geometric deep-learning
and graph neural networks. so one of the things that
we're doing with marinka is trying to sort of project
these structural graphs at the domain level rather
than the protein level, along with chemicals so that we can start building specific chemicals
for specific protein domains. and then we are working with
the chemistry department and brad to basically synthesize those. so what we're trying to
create is this new center at mit for genomics and
therapeutics, that basically says, can we facilitate this translation? we have thousands of
these genetic circuits that we have uncovered. i mentioned last time in the new england journal
of medicine, we had published these dissection
of the strongest genetic association with obesity. and we showed how you can
manipulate that association to switch back and forth
between fat burning cells and fat storing cells. in alzheimer's, just a few
weeks ago we had a paper in nature in collaboration
with li-huei tsai looking at apoe4, the strongest genetic
association with alzheimer's. and we showed that it
actually leads to a loss of being able to transport cholesterol in myelinating cells
known as oligodendrocytes that basically protect the neurons. and when the cholesterol gets stuck inside the oligodendrocytes,
it doesn't form myelin, the neurons are not protected
and it causes damage inside the oligodendrocytes. if you just restore transport, you basically are able
to restore myelination in human cells and in mice and to restore cognition in mice. so all of these circuits
are basically now giving us handles to truly transform
the human condition. we're doing the same thing
in cardiac disorders, in alzheimer's, in
neurodegenerative disorders, in psychiatric disorders,
where we have now these thousands of circuits
that if we manipulate them, we know we can reverse disease circuitry. so what we want to build in this coalition that we're building is
a center where we can now systematically test
these underlying molecules in cellular models for
heart, for muscle, for fat, for macrophages, immune
cells and neurons to be able to now screen through
these newly designed drugs through deep-learning and
to be able to sort of ask which ones act at the cellular level, which combinations of
treatment should we be using and the other components
that we're looking into decomposing complex traits like alzheimer's and cardiovascular
and schizophrenia into hallmarks of disease. so that for every one of
those traits we can kind of start speaking the language of what are the building blocks of
alzheimer's, and maybe this patient has building
blocks one, three, and seven and this other one
has two, three, and eight, and we can now start prescribing drugs not for the disease any more,
but for the hallmark. and the advantage of that
is that we can now take this modular approach to
disease instead of saying there's gonna be a drug for
alzheimer's, which is gonna fail in 80% of the
patients, we are gonna say, now there's gonna be 10
drugs, one for each pathway, and for every patient we now prescribe the combination of drugs. so what we wanna do in
that center is basically translate every single one
of these pathways into a set of therapeutics, a set of
drugs that are projecting the same, embedding
subspace as the biological pathways that they alter
so that we can have this translation between the dysregulation that are happening at the genetic level, at the transcription
level, at the drug level, at the protein structure level, and effectively take this modular approach to personalized medicine, where saying, i'm gonna build a drug for lex fridman is not gonna be sustainable. but if you instead say
i'm gonna build a drug for this pathway and a drug
for that other pathway, millions of people share
each of these pathways. so that's the vision
for how all of these ai and deep-learning and
embeddings can truly transform biology and medicine where we can truly take these systems and allow us to finally understand
disease at a superhuman level by sort of finding these
knowledge representations, these projections of each of these spaces and try understanding the meaning of each of those embedding subspaces and sort of how well populated it is, what are the drugs that we can build for it and so on and so forth. so it's truly transformative. - so systematically find
how to alter the pathways, it maps the structure of
the information in genomics to therapeutics and allows
you to have drugs that look at the pathways not at
the final condition. - exactly. exactly. and the way that we're coupling this is with cell penetrating
peptides that allows to deliver these drugs
to specific cell types by taking advantage of the
receptors of those cells. we can intervene at the
antisense oligo level by basically repressing the rna, bring in new rna, intervene
at the protein level, at the small molecule level. we can use proteins themselves as drugs just because of their
ability to interfere, to interact directly from
protein to protein interactions. so i think this space is
being completely transformed with a marriage of high
throughput technologies and all of these like ai, large language models, deep-learning models
and so on and so forth. - you mentioned your updated
answer to the meaning of life, as it continuously keeps updating. the new version is
self-actualization. can you explain? - i basically mean, let's try
to figure out, number one, what am i supposed to be? and number two, find the
strength to actually become it. so i was recently talking to students about this commencement
address and i was talking to them about sort of how they have all of these paths ahead of them right now. and part of it is choosing
the direction in which you go. and part of it is actually
doing the walk to go in that direction. and in doing the walk, what we talked about earlier
about sort of you create your own environment, i
basically told 'em, listen, you're ending high school up until now, your parents have created
all of your environment, now it's time to take
that into your own hands and to sort of shape the
environment that you wanna be an adult in. and you can do that by
choosing your friends, by choosing your particular
neuronal routines. i basically think of
your brain as a muscle, where you can exercise
specific neuronal pathways. so very recently i
realized that, you know, i was having so much trouble
sleeping, and, you know, i would wake up in the
middle of the night, i would wake up at 4:00 am
and i could just never go back to bed. so i was basically constantly
losing, losing, losing sleep. i started a new routine where
every morning, as i bike in, instead of going to my
office, i hit the gym. i basically go rowing
first, i then do weights, i then swim very often when i have time. and what that has done is
transform my neuronal pathways. so basically like on friday
i was trying to go to work and i was like, listen, i'm not gonna go exercise and i couldn't, my bike just went straight to the gym. i'm like, i don't wanna do it. and i just went anyway 'cause
i couldn't do otherwise. and that has completely transformed me. so i think this sort of
beneficial effect of exercise on the whole body is one of the ways that you could transform
your own neural pathways. understanding that it's not
a choice, it's not an option, it's not optional, it's mandatory. and i think your role
modeled so many of us by sort of being able to sort of push
your body to the extreme, being able to have these
extremely regimented regimes and that's something that
i've been terrible at. but now i'm basically trying
to coach myself and trying to sort of, you know, finish this kind of
self-actualization into a new version of myself, a more
disciplined version of myself. - don't ask questions,
just follow the ritual. - [manolis kellis] not an option. - you have so much love in
your life, you radiate love. do you ever feel lonely? - so there's different types of people. some people drain in gatherings, some people recharge in gatherings. i'm definitely the recharging type. so i'm an extremely social creature. i recharge with intellectual exchanges, i recharge with physical
exercise, i recharge in nature. but i also can feel fantastic
when i'm the only person in the room. that doesn't mean i'm lonely, it just means i'm the
only person in the room. and i think there's a
secret to not feeling alone when you're the only one. and that secret is self-reflection. it's introspection, it's almost watching yourself from above. and it's basically just becoming yourself, becoming comfortable with
the freedom that you have when you're by yourself. - so hanging out with yourself. i mean, there's a lot of
people who write to me, who talk to me about
feeling alone in this world, that struggle, especially
when they're younger, is there further words of
advice you can give to them, when they are almost
paralyzed by that feeling? - so i sympathize completely
and i have felt alone and i have felt that feeling. and what i would say to you is
stand up, stretch your arms, just like become your own self. just like realize that
you have this freedom. and breathe in, walk around the room, take a few steps in the room, just like get a feeling for
the 3d version of yourself, because very often we're
kind of stuck to a screen and that's very limiting
and that sort of gets us in particular mindset. but activating your muscles,
activating your body, activating your full self
is one way that you can kind of get out of it. and that is exercising your freedom, reclaiming your physical space. and one of the things that
i do is i have something that i call me time, which
is, if i've been really good all day, i got up in the morning,
i got the kids to school, i made them breakfast, i sort
of, you know, hit the gym, i had a series of really
productive meetings. i reward myself with this me time. and that feeling of sort of, when you're overstretched, to realize that that's normal and you
just wanna just let go. that feeling of exercising your freedom, exercising your me time... that's where you free
yourself from all stress. you basically say it's
not a need to any more, it's a want to. and as soon as i click that me time, all of the stress goes away
and i just bike home early and i get to my work office at home and i feel complete freedom. but guess what i do with
that complete freedom? i just don't go off and
drift and do boring things. i basically now say, okay,
whew, this is just for me. i'm completely free. i don't
have any requirements any more. what do i do? i just look at my to-do
list and i'm like, you know, what can i clear off? and if i have three meetings
scheduled in the next three half hours, it is so
much more productive for me to say, you know what, i just wanna pick up
the phone now and call these people and just knock
it off one after the other and i can finish three half
hour meetings in the next 15 minutes just because it's
the want, not i have to. so that would be my advice, basically, turn something that you
have to do in just me time, stretch out, exercise your
freedom and just realize you live in 3d and you
are a person and just do things because you want them,
not because you have to. - noticing and reclaiming the
freedom that each of us have. that's what it means to be human. if you notice it, you're
truly free, physically, mentally, psychologically. manolis, you're an incredible human. we could talk for many more hours. we covered less than 10% of
what we were planning to cover, but we have to run off now
to the social gathering that we spoke of. - we're 3d humans.
- we're 3d humans. - [manolis kellis] a concept. - and reclaim the freedom. i think, i hope we can
talk many, many more times. there's always a lot to talk
about, but more importantly, you're just a human being with a big heart and a beautiful mind that
people love hearing from. and i certainly consider
a huge honor to know you and to consider your friend. thank you so much for talking today. thank you so much for
talking so many more times. and thank you for all the
love behind the scenes that you send my way. it always means the world. - lex, you are a truly,
truly special human being. and i have to say that
i'm honored to know you. i have, like, i so many
friends are just in awe that you even exist, that you have the ability
to do all the stuff that you're doing. and i think you're a gift to humanity. i love the mission that you're on to sort of share knowledge and
insight and deep thought with so many special people
who are transformative, but people across all walks of life. and i think you're doing this in just such a magnificent way. i wish you strength to continue doing that because it's a very special mission and it's a very draining mission. so thank you, both the
human you and the robot you, the human you for showing
all these love and the robot you for doing it day after day after day. so thank you, lex. - all right, let's go have some fun. - [manolis kellis] let's go. - thanks for listening
to this conversation with manolis kellis. to support this podcast, please check out our
sponsors in the description. and now let me leave you some words from bill bryson, in his book, "a short history of nearly everything". "if this book has a lesson, it is that we are
awfully lucky to be here. and by we, i mean every living thing. to attain any kind of
life in this universe of ours appears to be
quite an achievement. as humans, we're doubly lucky, of course, we enjoy not only the
privilege of existence, but also the singular
ability to appreciate it, and even in a multitude of
ways, to make it better. it is a talent we have only
barely begun to grasp." thank you for listening and
hope to see you next time.