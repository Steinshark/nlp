in an older video i talked about the immensely powerful and beautifully complicated image processing pipeline in today's smartphones i artfully left out the chips themselves mostly because i found those image sensors too difficult to explain and i was tired but i have received many requests about this fascinating silicon technology so for this video let's take a look at the chips that make digital cameras possible but first let me talk a bit about our sponsor blinkist blinkist offers over 5500 titles in 27 different categories in just 15 minutes you can get powerful insights into different topics while going about your everyday life learn about the world around us or how to be a better communicator or a happier personally i think there's a lot of overlap between what asianometry does and what blinkist provides densely packed information in 15 to 20 minute chunks i hear a lot of viewers of the channel just listen to it while they're cooking and cleaning i do the same thing with my blinkist audio titles i get you everything you want to know about a particular topic to complement your everyday habits and accelerate your learning today's video will mention something known as disruption theory and i realized i didn't explain this now famous theory much to get background on that i suggest this book the innovator's dilemma by clayton christensen this blink helps you learn why so many people in the technology industry talk about quote disruption end quote is kind of a meme now but that does not mean you shouldn't know it in it they talk about how convenience trumps quality consider what the convenience was in today's disruption example i also want to call out a new feature link is connect which lets you share your subscription to a friend for as long as you wish you can get more out of your current subscription which is good get 25 off blinka's premium and enjoy two memberships for the price of one with connect start your seven day trial today by clicking on the link in the description below thanks to blinkist for sponsoring the channel in'89 the german physicist heinrich hertz noticed that ultraviolet light somehow knocked electrons of certain atoms out of their normal orbits the number of electrons emitted from the metal atoms was directly proportional to the light's intensity and it happened almost instantaneously the kinetic energy of the ejected electrons was independent of the light's intensity this was a strange phenomenon that classical physics could not explain and it took many years to reconcile finally in the early 1900s a theoretical physicist proposed the theory to resolve this discrepancy he took a concept first proposed by max planck the concept that light consists of tiny packets of light called photons his theory said that the energy in that light is proportional to its frequency and that it only knocked out an electron if its energy was above a certain threshold this theory had many of its tenets proven in later experiments and won the 1921 nobel that theoretical physicist albert einstein really though the photoelectric effect as it would be called underpins how our modern electronic devices sense light the concept of solid-state photosensing or imaging is to have light fall onto a two-dimensional array of photosensitive elements that light ejects electrons from the array's materials changing its electrical properties we can then measure this for indirect information about the light itself if we can properly interpret this data then we can create an image one early device that would use these principles was a vacuum tubed based gadget called the photo multiplier tube or pmt it was suited for detecting ultra low levels of light a photo cathode at one end of the tube emits electrons when exposed to light via the photoelectric effect those electrons then travel towards an anode on the other side of the tube along the way they are consecutively multiplied using five to seven sets of diodes as a result you can turn one target photon into one hundred thousand or even 10 million electrons which is exactly the behavior you want for a photo detector pmts and other such classical devices paved the way for their silicon-based descendants down the line silicon is an excellent material for an image sensor capable of responding to a wide range of lights so researchers have wanted a solid-state silicon image sensor for a very long time but it was not until the 1960s that a viable contender emerged in the late 1960s a pair of scientists at bell labs george smith and willard boyle were researching semiconductor bubble memory this type of memory has a bunch of electrodes sitting on top of a silicon substrate with a layer of oxide insulation in between if you apply a certain voltage to the electrode then it creates a potential well in the substrate right underneath the electrode these potential wells are capable of storing a charge by capturing energy within it the charge in turn can represent a bit of data if you were to use this technology as a semiconductor memory another nifty thing that you can do with this series of electrodes is that if you were to pulse them in a certain sequence you can move the potential well and the charge it is storing across the substrate doesn't sound all that special right ultimately bubble memory was too slow to compete as a viable memory technology but researchers realize that these wells can also collect and convey analog signals so rather than using these wells to store charges as bits of memory what if you were to use them to collect and transport packets of electrons ejected from silicon via the photoelectric effect and there we have the operating principle of the ccd you take an array of electrodes which are modified to serve as your pixels when exposed to light the silicon in what is called a photodiode ejects electrons which are then collected by the electrodes a voltage cycle is then applied to carry those packets of electrons down or across the array like ants carrying leaves towards another unit at the array's edge this unit then converts and boosts these electron packets from the whole array into a sequence of voltages which external electronics can interpret has a 2d image if necessary the voltages are also digitized the whole thing is synced to a clock and happens in a cycle one of the biggest benefits of the ccd was its sensitivity the big challenge with prior solid state image sensors was their efficiency in other words how much of an electrical signal can they generate when exposed to light most image sensors had light efficiencies that were too low which meant that they were not at all sensitive to light this makes it harder to see the scene in front of it ccds are sensitive enough to light combining with superior dynamic range low levels of noise and smaller pixels ccds became the dominant solid state image sensor technology after their invention in the 1970s a variety of ccd makers entered the market including general electric ibm huge aircraft texas instruments rca fairchild phillips hitachi thompson intel and sony ccgs quickly took most of the imaging market providing the sensors for things like camcorders and digital cameras but they had their awards one of the big downsides of the ccd is that if the chip is still exposed to light during the voltage conversion stage sometimes called the frame readout then you can get undesired signal issues these issues manifest as smearing and manufacturers have dealt with it with a mechanical shutter sitting in front of the sensor or by separating one part of the chip into a dark area like a dark room the former is a moving thing that can break and also requires power the latter basically doubles the cost of fabbing the die furthermore when combined with its accompanying systems like the analog to digital converter the whole thing eats a whole lot of power i remember having a camcorder when i was a kid and those things always seem to run down their batteries in one to two hours so yes the ccd was a solid solid state image sensor and they set the standard for image quality but it also fell short of the slick and smooth dream of having a convenient digital camera on a chip complementary metal oxide semiconductors or cmos image chips have been around for a long time sporadically researched since the 1960s notably in 1964 ibm announced the invention of the scanister a small dime-sized chip that can produce an output signal depending on light ibm said that as canisters can scan documents for faxing purposes scientists at westinghouse fairchild and rca also demonstrated similar setups that time but their photo sensitivities were very low when ccds were invented in 1970 it sort of sucked out all the oxygen from the space few teams notably hitachi continued studying cmos image sensors mostly in the pursuit of a chip for camcorder applications hitachi chased the dream for a while introducing on-chip techniques to very chip exposure times and suppress the flicker you get with indoor light however they moved on after failing to sufficiently suppress random image noise artifacts a big deal in low light conditions so cmos image sensors languished a bit while ccds had their day in the limelight but the 1990s saw new interest and money flood back into the space with improving processes people saw an opportunity for further integration with the rest of the imaging system this means faster readout less power better packaging and the image functionalities enabled by digital processing the image processing pipeline cmos image sensors can do this while the increasingly specialized ccd cannot the critical technological turning point was the introduction of second generation active pixel sensors and now it is time for me to explain how this accursive thing works a cmos image sensor has an array of pixels each of those pixels in turn has a photodiode the light sensitive element which collects the silicon electrons ejected by light there are also control transistors included as well these serve several functions which we will discuss later when the system needs to receive and interpret the image we access or read out the pixel or row of pixels just like as if it were a memory changing the way we did this is at the heart of second generation cmos image sensor technology with first generation passive pixel technology invented back in the 1960s by a fairchild alumni there's just one transistor inside the pixel the control transistor switch that is triggered when we want to read out after that control transistor switches on the photodiodes charge flows into a wire towards an amplifier this amplifier at the end of the wire converts the charge into a voltage for image processing off chip the passive pixel approach gives you smaller simpler pixels with just one transistor each but because the pixels are so simple we have no way of amplifying the charge before it flows into the wire the charge tends to be very small a small charge into a big wire results in a high amount of noise over 12 times more than what you can get from a ccd noise is bad all of this noise means that the passive pixel approach does not scale the more pixels we put into the array or the faster we try to read out those pixels the more noise we get soon after the passive pixel was invented people recognize that they can fix its noise problem by adding a buffer or amplifier to the pixel to boost the charge before it gets processed this is the active pixel sensor with multiple transistors inside each pixel rather than just one in a way this is quite similar to how this ccd works it just packages more functionalities into a single pixel we no longer need to carry charges across the pixel array for conversion in a support system we do the work inside the pixel itself but easier said than done semiconductor processing nodes were not capable of actually pulling this off in a commercial sense until really the 1990s an early implementation of the active pixel sensor concept called the photodiode active pixel sensor was bandied around since the 1980s particularly in japan vls i vision limited also contributed some critical patterns in the space but it was not until 1993 that things really got going at nasa's jet propulsion lab or jpl spacecraft cameras at the time use ccds which as i mentioned earlier needed a lot of support systems and used a lot of energy nasa wanted something faster better cheaper so eric fossum jpl's chief of image sensor research and his team developed their own implementation of the active pixel sensor the photogate active pixel sensor they also introduced a new technique called correlated double sampling to significantly suppress noise and improve image quality this is where you sample the pixel's image data twice and take the difference to filter out to noise back in the 1960s nasa went all in on a strange new technology called integrated circuits but in the 1990s they turned up their noses to cmos image sensors so in 1995 fossum forumed a company called photobit to commercialize their implementation they are pioneering pb100 chip would power the intel easy pc camera logitech's quick cam and more they produced other chips for military medical and industrial use too photobit was acquired by micron in 2001 spun off as aptina imaging and then acquired by on semiconductor in 2014. what occurred in the late 1990s and early 2000s is a classic disruption story in the early days ccds had superior image quality the cmos active pixel image sensors because there are more transistors within each active pixel the pixels themselves are physically larger which limits how many of them we can have on the array furthermore having more transistors inside the pixel meant that less of the pixel's physical size can be allocated towards collecting light referred to as fill factor early fill factors could be as little as 26 percent this was in comparison to the ccd's 100 percent the counterpoint was that having multiple transistors in the pixel like a charge amplifier exposure timer and even an analog to digital converter meant that each pixel can deliver better image data faster furthermore cmos image sensors deliver digitized data were relatively cheap and ate less power this was why mobile phone makers started using cmos image chips in 1999. and moore's law changed everything cmos image sensors are compatible with traditional semiconductor processes while ccds are not so when advanced process nodes breach those 0.5 micrometers or 500 nanometers level in the mid-1990s the industry achieved a transistor density per pixel to be sufficiently competitive against existing ccd technology pixel density experienced its own version of moore's law with density doubling every' months soon you are able to go from a few thousand pixels on a sensor to what would eventually be a few million cmos image sensors became the fastest ramping product category in semiconductor history up until then with a million eight inch wafers produced from 2003 to 2005. by 2007 cmos image sensors had improved their image resolution weaknesses while also retaining their cost power and size advantages soon ccds only held the imaging quality advantage in low light or non-visible light scenarios camera phones like the nokia n95 and the iphone paired these sensors with a mobile computer supercharging image sensor development and unlocking the near unlimited benefits of computational photography smartphone applications take up to 70 percent of our modern cmos image sensor market ccd manufacturers were slow to adapt to this rapid advancement sony the larger ccd maker was one of the few to successfully make that transition perhaps because they made their own digital cameras and saw what consumers wanted in 2014 sony's imx 174 global shutter cmos chip outperformed even their most advanced ccd chips sales of the latter collapsed in january 2015 sony said that they were shutting down their ccd manufacturing operations and going forward with only cmos with this cmos had swept the market relegating the venerable ccd to the fossil bed now that cmos image sensors dominate the industry what is next for the industry for the past decade the smartphone industry has demanded better cameras this means higher resolutions more megapixels and thus smaller pixels but you want those smaller pixels without adding additional noise to the whole picture as the pixels get smaller its transistors get relatively larger and the photodiode gets smaller this shrinks the fill factor and thus adds more noise maintaining that signal-to-noise ratio as pixels shrink is a big challenge in the cmos image sensor industry each pixel size generation has a fundamental theoretical limit to its signal-to-noise ratio these smaller sizes are harder to achieve but it is still happening just this year the chinese fabulous semiconductor company omnivision announced a 0.56 micrometer image pixel using tsmc's n22 process a remarkable achievement that shows that better cmos image sensors are still possible all right everyone that's it for tonight thanks for watching and thanks to blinkus again for sponsoring the channel check the link below to subscribe i'll see you guys next time