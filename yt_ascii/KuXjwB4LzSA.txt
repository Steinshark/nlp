suppose i give you two different lists of numbers, or maybe two different functions,  and i ask you to think of all the ways you might combine those two lists to get  a new list of numbers, or combine the two functions to get a new function. maybe one simple way that comes to mind is to simply add them together term by term. likewise with the functions, you can add all the corresponding outputs. in a similar vein, you could also multiply the two lists  term by term and do the same thing with the functions. but there's another kind of combination just as fundamental as both of those,  but a lot less commonly discussed, known as a convolution. but unlike the previous two cases, it's not something that's  merely inherited from an operation you can do to numbers. it's something genuinely new for the context of lists of numbers or combining functions. they show up all over the place, they are ubiquitous in image processing,  it's a core construct in the theory of probability,  they're used a lot in solving differential equations,  and one context where you've almost certainly seen it, if not by this name,  is multiplying two polynomials together. as someone in the business of visual explanations, this is an especially great topic,  because the formulaic definition in isolation and without context can look kind of  intimidating, but if we take the time to really unpack what it's saying,  and before that actually motivate why you would want something like this,  it's an incredibly beautiful operation. and i have to admit, i actually learned a little something  while putting together the visuals for this project. in the case of convolving two different functions,  i was trying to think of different ways you might picture what that could mean,  and with one of them i had a little bit of an aha moment for why it is that  normal distributions play the role that they do in probability,  why it's such a natural shape for a function. but i'm getting ahead of myself, there's a lot of setup for that one. in this video, our primary focus is just going to be on the discrete case,  and in particular building up to a very unexpected but very clever algorithm for  computing these. and i'll pull out the discussion for the continuous case into a second part. it's very tempting to open up with the image processing examples,  since they're visually the most intriguing, but there are a couple bits of finickiness  that make the image processing case less representative of convolutions overall,  so instead let's kick things off with probability,  and in particular one of the simplest examples that i'm sure everyone here has  thought about at some point in their life, which is rolling a pair of dice and  figuring out the chances of seeing various different sums. and you might say, not a problem, not a problem. each of your two dice has six different possible outcomes,  which gives us a total of 36 distinct possible pairs of outcomes,  and if we just look through them all we can count up how many pairs have a given sum. and arranging all the pairs in a grid like this,  one pretty nice thing is that all of the pairs that have a constant sum are visible  along one of these different diagonals. so simply counting how many exist on each of those diagonals  will tell you how likely you are to see a particular sum. and i'd say, very good, very good, but can you think of  any other ways that you might visualize the same question? other images that can come to mind to think of  all the distinct pairs that have a given sum? and maybe one of you raises your hand and says, yeah, i've got one. let's say you picture these two different sets of possibilities each in a row,  but you flip around that second row. that way all of the different pairs which add up to seven line up vertically like this. and if we slide that bottom row all the way to the right,  then the unique pair that adds up to two, the snake eyes, are the only ones that align,  and if i schlunk that over one unit to the right,  the pairs which align are the two different pairs that add up to three. and in general, different offset values of this lower array,  which remember i had to flip around first, reveal all the distinct pairs that  have a given sum. as far as probability questions go, this still isn't especially interesting because  all we're doing is counting how many outcomes there are in each of these categories. but that is with the implicit assumption that there's  an equal chance for each of these faces to come up. but what if i told you i have a special set of dice that's not uniform? maybe the blue die has its own set of numbers describing the probabilities for  each face coming up, and the red die has its own unique distinct set of numbers. in that case, if you wanted to figure out, say,  the probability of seeing a 2, you would multiply the probability  that the blue die is a 1 times the probability that the red die is a 1. and for the chances of seeing a 3, you look at the two distinct  pairs where that's possible, and again multiply the corresponding  probabilities and then add those two products together. similarly, the chances of seeing a 4 involves multiplying together  three different pairs of possibilities and adding them all together. and in the spirit of setting up some formulas, let's name these top probabilities a 1,  a 2, a 3, and so on, and name the bottom ones b 1, b 2, b 3, and so on. and in general, this process where we're taking two different arrays of numbers,  flipping the second one around, and then lining them up at various different  offset values, taking a bunch of pairwise products and adding them up,  that's one of the fundamental ways to think about what a convolution is. so just to spell it out a little more exactly, through this process,  we just generated probabilities for seeing 2, 3, 4, on and on up to 12,  and we got them by mixing together one list of values, a, and another list of values, b. in the lingo, we'd say the convolution of those two sequences gives us this new sequence,  the new sequence of 11 values, each of which looks like some sum of pairwise products. if you prefer, another way you could think about the same operation is to first  create a table of all the pairwise products, and then add up along all these diagonals. again, that's a way of mixing together these two  sequences of numbers to get us a new sequence of 11 numbers. it's the same operation as the sliding windows thought, just another perspective. putting a little notation to it, here's how you might see it written down. the convolution of a and b, denoted with this little asterisk, is a new list,  and the nth element of that list looks like a sum,  and that sum goes over all different pairs of indices, i and j,  so that the sum of those indices is equal to n. it's kind of a mouthful, but for example, if n was 6,  the pairs we're going over are 1 and 5, 2 and 4, 3 and 3, 4 and 2, 5 and 1,  all the different pairs that add up to 6. but honestly, however you write it down, the notation is secondary in  importance to the visual you might hold in your head for the process. here, maybe it helps to do a super simple example,  where i might ask you what's the convolution of the list 1, 2, 3 with the list 4, 5, 6. you might picture taking both of these lists, flipping around that second one,  and then starting with its lid all the way over to the left. then the pair of values which align are 1 and 4,  multiply them together, and that gives us our first term of our output. slide that bottom array one unit to the right,  the pairs which align are 1, 5 and 2 and 4, multiply those pairs,  add them together, and that gives us 13, the next entry in our output. slide things over once more, and we'll take 1 times 6 plus 2 times 5 plus 3 times 4,  which happens to be 28. one more slide and we get 2 times 6 plus 3 times 5, and that gives us 27. and finally the last term will look like 3 times 6. if you'd like, you can pull up whatever your favorite programming  language is and your favorite library that includes various numerical operations,  and you can confirm i'm not lying to you. if you take the convolution of 1, 2, 3 against 4,  5, 6, this is indeed the result that you'll get. we've seen one case where this is a natural and desirable operation,  adding up to probability distributions, and another common example would be a  moving average. imagine you have some long list of numbers, and you take  another smaller list of numbers that all add up to 1. in this case, i just have a little list of 5 values and they're all equal to 1 5th. then if we do this sliding window convolution process,  and kind of close our eyes and sweep under the rug what happens at  the very beginning of it, once our smaller list of values entirely  overlaps with the bigger one, think about what each term in this convolution really means. at each iteration, what you're doing is multiplying each of the values  from your data by 1 5th and adding them all together,  which is to say you're taking an average of your data inside this little window. overall, the process gives you a smoothed out version of the original data,  and you could modify this starting with a different little list of numbers,  and as long as that little list all adds up to 1,  you can still interpret it as a moving average. in the example shown here, that moving average  would be giving more weight towards the central value. this also results in a smoothed out version of the data. if you do kind of a two-dimensional analog of this,  it gives you a fun algorithm for blurring a given image. and i should say the animations i'm about to show are modified from something  i originally made for part of a set of lectures i did with the julia lab at  mit for a certain open courseware class that included an image processing unit. there we did a little bit more to dive into the code behind all of this,  so if you're curious, i'll leave you some links. but focusing back on this blurring example, what's going on is i've got this  little 3x3 grid of values that's marching along our original image, and if we zoom in,  each one of those values is 1 9th, and what i'm doing at each iteration is  multiplying each of those values by the corresponding pixel that it sits on top of. and of course in computer science, we think of colors as little vectors of three values,  representing the red, green, and blue components. when i multiply all these little values by 1 9th and i add them together,  it gives us an average along each color channel,  and the corresponding pixel for the image on the right is defined to be that sum. the overall effect, as we do this for every single pixel on the image,  is that each one kind of bleeds into all of its neighbors,  which gives us a blurrier version than the original. in the lingo, we'd say that the image on the right is a  convolution of our original image with a little grid of values. or more technically, maybe i should say that it's the convolution  with a'0 degree rotated version of that little grid of values. not that it matters when the grid is symmetric,  but it's just worth keeping in mind that the definition of a convolution,  as inherited from the pure math context, should always invite you to think about  flipping around that second array. if we modify this slightly, we can get a much more elegant  blurring effect by choosing a different grid of values. in this case, i have a little 5x5 grid, but the distinction is not so much its size. if we zoom in, we notice that the value in the middle  is a lot bigger than the value towards the edges. and where this is coming from is they're all sampled from a bell curve,  known as a gaussian distribution. that way, when we multiply all of these values by the corresponding  pixel that they're sitting on top of, we're giving a lot more weight  to that central pixel, and much less towards the ones out at the edge. and just as before, the corresponding pixel on the right is defined to be this sum. as we do this process for every single pixel, it gives a blurring effect,  which much more authentically simulates the notion of putting your lens out of focus,  or something like that. but blurring is far from the only thing that you can do with this idea. for instance, take a look at this little grid of values,  which involves some positive numbers on the left,  and some negative numbers on the right, which i'll color with blue and red respectively. take a moment to see if you can predict and understand  what effect this will have on the final image. so in this case, i'll just be thinking of the image as grayscale instead of colored,  so each of the pixels is just represented by one number instead of three. and one thing worth noticing is that as we do this convolution,  it's possible to get negative values. for example, at this point here, if we zoom in,  the left half of our little grid sits entirely on top of black pixels,  which would have a value of zero, but the right half of negative values all sit on  top of white pixels, which would have a value of one. so when we multiply corresponding terms and add them together,  the results will be very negative. and the way i'm displaying this with the image on the right  is to color negative values red and positive values blue. another thing to notice is that when you're on a patch that's all the same color,  everything goes to zero, since the sum of the values in our little grid is zero. this is very different from the previous two examples where the sum of our little  grid was one, which let us interpret it as a moving average and hence a blur. all in all, this little process basically detects wherever there's  variation in the pixel value as you move from left to right,  and so it gives you a kind of way to pick up on all the vertical edges from your image. and similarly, if we rotated that grid around so that it varies as you move from  the top to the bottom, this will be picking up on all the horizontal edges,  which in the case of our little pie creature image does result in some pretty  demonic eyes. this smaller grid, by the way, is often called a kernel,  and the beauty here is how just by choosing a different kernel,  you can get different image processing effects, not just blurring your edge detection,  but also things like sharpening. for those of you who have heard of a convolutional neural network,  the idea there is to use data to figure out what the kernels should be in  the first place, as determined by whatever the neural network wants to detect. another thing i should maybe bring up is the length of the output. for something like the moving average example,  you might only want to think about the terms when both of the windows  fully align with each other. or in the image processing example, maybe you want  the final output to have the same size as the original. now, convolutions as a pure math operation always produce an  array that's bigger than the two arrays that you started with,  at least assuming one of them doesn't have a length of one. just know that in certain computer science contexts,  you often want to deliberately truncate that output. another thing worth highlighting is that in the computer science context,  this notion of flipping around that kernel before you let it march across  the original often feels really weird and just uncalled for, but again,  note that that's what's inherited from the pure math context,  where like we saw with the probabilities, it's an incredibly natural thing to do. and actually, i can show you one more pure math example where  even the programmers should care about this one,  because it opens the doors for a much faster algorithm to compute all of these. to set up what i mean by faster here, let me go back and pull up some python again,  and i'm going to create two different relatively big arrays. each one will have a hundred thousand random elements in it,  and i'm going to assess the runtime, of the convolve function from the numpy library. and in this case, it runs it for multiple different iterations, tries to find an average,  and it looks like, on this computer at least, it averages at 4.87 seconds. by contrast, if i use a different function from the scipy library called fftconvolve,  which is the same thing just implemented differently,  that only takes 4.3 milliseconds on average, so three orders of magnitude improvement. and again, even though it flies under a different name,  it's giving the same output that the other convolve function does,  it's just doing something to go about it in a cleverer way. remember how with the probability example, i said another way you could  think about the convolution was to create this table of all the pairwise products,  and then add up those pairwise products along the diagonals. there's of course nothing specific to probability. any time you're convolving two different lists of numbers,  you can think about it this way. create this kind of multiplication table with all pairwise products,  and then each sum along the diagonal corresponds to one of your final outputs. one context where this view is especially natural  is when you multiply together two polynomials. for example, let me take the little grid we already have and replace the top terms  with 1, 2x, and 3x squared, and replace the other terms with 4, 5x, and 6x squared. now, think about what it means when we're creating all of  these different pairwise products between the two lists. what you're doing is essentially expanding out the full product of  the two polynomials i have written down, and then when you add up along the diagonal,  that corresponds to collecting all like terms. which is pretty neat. expanding a polynomial and collecting like terms  is exactly the same process as a convolution. but this allows us to do something that's pretty cool,  because think about what we're saying here. we're saying if you take two different functions and you multiply them together,  which is a simple pointwise operation, that's the same thing as if you had  first extracted the coefficients from each one of those, assuming they're polynomials,  and then taken a convolution of those two lists of coefficients. what makes that so interesting is that convolutions feel,  in principle, a lot more complicated than simple multiplication. and i don't just mean conceptually they're harder to think about. i mean, computationally, it requires more steps to perform a convolution  than it does to perform a pointwise product of two different lists. for example, let's say i gave you two really big polynomials,  say each one with a hundred different coefficients. then if the way you multiply them was to expand out this product,  you know, filling in this entire 100 by 100 grid of pairwise products,  that would require you to perform 10,000 different products. and then, when you're collecting all the like terms along the diagonals,  that's another set of around 10,000 operations. more generally, in the lingo, we'd say the algorithm is o of n squared,  meaning for two lists of size n, the way that the number of  operations scales is in proportion to the square of n. on the other hand, if i think of two polynomials in terms of their outputs, for example,  sampling their values at some handful of inputs,  then multiplying them only requires as many operations as the number of samples,  since again, it's a pointwise operation. and with polynomials, you only need finitely many  samples to be able to recover the coefficients. for example, two outputs are enough to uniquely specify a linear polynomial,  three outputs would be enough to uniquely specify a quadratic polynomial,  and in general, if you know n distinct outputs,  that's enough to uniquely specify a polynomial that has n different coefficients. or, if you prefer, we could phrase this in the language of systems of equations. imagine i tell you i have some polynomial, but i don't tell you what the coefficients are. those are a mystery to you. in our example, you might think of this as the product that we're trying to figure out. and then, suppose i say, i'll just tell you what the outputs of this polynomial  would be if you inputted various different inputs, like 0, 1, 2, 3, on and on,  and i give you enough so that you have as many equations as you have unknowns. it even happens to be a linear system of equations, so that's nice,  and in principle, at least, this should be enough to recover the coefficients. so the rough algorithm outline then would be, whenever you want to convolve two lists  of numbers, you treat them like they're coefficients of two polynomials,  you sample those polynomials at enough outputs, multiply those samples point-wise,  and then solve this system to recover the coefficients as a sneaky backdoor way to find  the convolution. and, as i've stated it so far, at least, some of you could rightfully complain, grant,  that is an idiotic plan, because, for one thing,  just calculating all these samples for one of the polynomials we know already takes  on the order of n-squared operations. not to mention, solving that system is certainly going to be  computationally as difficult as just doing the convolution in the first place. so, like, sure, we have this connection between multiplication and convolutions,  but all of the complexity happens in translating from one viewpoint to the other. but there is a trick, and those of you who know about fourier  transforms and the fft algorithm might see where this is going. if you're unfamiliar with those topics, what i'm  about to say might seem completely out of the blue. just know that there are certain paths you could have  walked in math that make this more of an expected step. basically, the idea is that we have a freedom of choice here. if instead of evaluating at some arbitrary set of inputs, like 0, 1, 2, 3,  on and on, you choose to evaluate on a very specially selected set of complex numbers,  specifically the ones that sit evenly spaced on the unit circle,  what are known as the roots of unity, this gives us a friendlier system. the basic idea is that by finding a number where taking its powers falls into  this cycling pattern, it means that the system we generate is going to have a  lot of redundancy in the different terms that you're calculating,  and by being clever about how you leverage that redundancy,  you can save yourself a lot of work. this set of outputs that i've written has a special name. it's called the discrete fourier transform of the coefficients,  and if you want to learn more, i actually did another lecture for that same julia mit  class all about discrete fourier transforms, and there's also a really excellent video on  the channel reducible talking about the fast fourier transform,  which is an algorithm for computing these more quickly. also veritasium recently did a really good video on ffts, so you've got lots of options. and that fast algorithm really is the point for us. again, because of all this redundancy, there exists a method to go from  the coefficients to all of these outputs, where instead of doing on  the order of n squared operations, you do on the order of n times the  log of n operations, which is much much better as you scale to big lists. and, importantly, this fft algorithm goes both ways. it also lets you go from the outputs to the coefficients. so, bringing it all together, let's look back at our algorithm outline. now we can say, whenever you're given two long lists of numbers,  and you want to take their convolution, first compute the fast fourier transform of  each one of them, which, in the back of your mind,  you can just think of as treating them like they're the coefficients of a polynomial,  and evaluating it at a very specially selected set of points. then, multiply together the two results that you just got, point-wise,  which is nice and fast, and then do an inverse fast fourier transform,  and what that gives you is the sneaky backdoor way to compute the convolution that  we were looking for. but this time, it only involves o of n log n operations. that's really cool to me. this very specific context where convolutions show up,  multiplying two polynomials, opens the doors for an algorithm  that's relevant everywhere else where convolutions might come up. if you want to add probability distributions, do some large image processing,  whatever it might be, and i just think that's such a good example of why you should be  excited when you see some operation or concept in math show up in a lot of seemingly  unrelated areas. if you want a little homework, here's something that's fun to think about. explain why when you multiply two different numbers,  just ordinary multiplication the way we all learn in elementary school,  what you're doing is basically a convolution between the digits of those numbers. there's some added steps with carries and the like, but the core step is a convolution. in light of the existence of a fast algorithm,  what that means is if you have two very large integers,  then there exists a way to find their product that's faster than the method we learn  in elementary school, that instead of requiring o of n squared operations,  only requires o of n log n, which doesn't even feel like it should be possible. the catch is that before this is actually useful in practice,  your numbers would have to be absolutely monstrous. but still, it's cool that such an algorithm exists. and next up, we'll turn our attention to the continuous  case with a special focus on probability distributions.