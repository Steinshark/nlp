there was a brief and glorious moment in tech history where twitter was regularly in the news for technical reasons seems hard to believe now but they used to be and it was around those days that i first heard about something called the lambda architecture and the lambda architecture was proposed as a solution to twitter's big urgent problem you have these two roles you need to fill you've got a huge set of historical data that needs to be queried in all the usual ways but there's this much smaller set of live data that's really urgent has to be processed right now if it's to be of any value and there's a mismatch between those two kinds of data they have different storage requirements they have different access patterns and of course they have different performance constraints plus there's the extra problem that the new data is going to become old data so eventually every piece of data you have will live in both of those roles can be very difficult to deal with because you can't optimize one without sacrificing the other and the lambda architecture proposed this solution accept it you're going to have two different data systems and what you have to do is build them optimized for each use case and then do all that difficult integration work that makes those two systems appear like they're one system it's very hard i remember it taking a long time but they did it but what if you could get all that integration both of those systems neatly coupled together off the shelf it seems to me that that's the question being asked by this week's guest joe jong and his answer is the recently open source database proton well if that's his answer i have questions what's a database like that for what does it do well how does it work and how are you making that difficult integration piece simple enough that i can rely on it but transparent enough that i can forget about it that's the trick i suggest we find out i'm your host chris jenkins this is developer voices and today's voice is joe [music] jong i'm joined today by jo jong jo how are you i'm doing good hi chris so glad to enjoy this podcast i'm glad you've joined us you've got so you're in a world with a lot of people competing to try and solve this problem right of streaming data and it seems to me that everybody has a slightly different way of solving the problem and those architectural decisions that they make play out in different and interesting ways so i want to i want to pick your brains on the architectural decisions you've made and how it plays out in the system proton that you've ended up co-designing right but before we get into that i think we should try and ground ourselves in what's this for what's an end user going to actually use this for yeah sure yeah so portum u i would say there a for people who are not that familiar with idea of streaming processing you can just considering this is a next generation like etl even supp people know what is etl so you want to move data from a to b and you want this as fast as possible no latency you don't have to wait for the next batch say next one day next one hour whenever there something happening for example there's a new order come in there's a new people click your web page there's a person being attacked a server is going to be those information can be sent to the system in in the almost zero latency and you can apply some alert you can have some automated actions so at web at kind of at common use cases this is really good for build a real time or streaming etl so and beyond that that we also really is a true streaming processor so meaning that is it's not just fast enough it's also have this state for processing so the difference that is so we we can do some state for processing such as say i can count every one second how many click for that particular page then you know i accumulate a state within that one second or one minute even you can do some fancy things such as this a session window whenever a signal meaning that session start and the session end and we keep this session window for example when the user log in do a bunch of action until they check out or log out this is a session the session window can be can be as long as say two hours or can be just 5 second if the person is not patient enough it's the digital equivalent of someone comes into your shop and you want to kind of understand them until they leave right yeah yeah so all those simple transmission or filtering or those sophisticated st processing all those are in the category of stream processor so some of the big player in the space for example like apache flank and there's some other vendor as you briefly mention that is it's a yeah it's not a new space but still at early stage because people are really want to guess things more real time they may still struggle on the batch side but more and more people want to get to the next chapter which is streaming so this idea of stream processing analytics is really a new thing and people really want to try different solutions yeah this is the thing that interests me and i keep coming to like i want to hear about everyone because like everyone everyone in this space is scrabbling to come up with the ideal solution or the solution that's optimized for a particular subset of that market and everyone has a large piece of the answer but and everyone's saying that they've got the whole of the answer and i just kind of want to explore it right exactly first i'm going to pick you up on a word use a specific i'm going to nitpick your word that you said it's a true stream processor you said what's a true stream processor yeah again so there a this definition of stream processor is really about how to handle the streaming data in a meaningful way right the the streaming data is the data you constantly get so you get not not just batch by batch you always get new data all the time and you also want to process data in the streaming way not in the batch way so more like a common or na solution that is you always get the data from your olab system there's a lot ofab system i guess the market of olab system is even busier than our streaming processing market but anyway you can always put the data into your olab system and you can quiry it every one second right then if your system is powerful enough you get a quite nice dashboard or alert with the limitation of you get this every one second or every one minute i think most syst don't really happy to support per second query because that's too much too frequent and what is worse that is imagine you want to get hey what's my revenue this month and you want that number update every second or maybe every minute that if you are using a olab system then you query this every one second or one minute but you are wasting a lot of resource right because the number you get last time maybe it's say 10,000 and between this two query you might get a few transaction and you may get 10,200 but because you are run this as a batch query you end up with query all the data over over again even that's a very small incremental thing right so this is the common we call realtime solution and for the streaming processing we have this state so we know what's the last time we quered is this say 1,000 then there's a some new event we know the delta so we can give you the we get the data combining with the previous result we give you the new thing and what is more interesting that is usually this streaming processing is a long running process so you only run your query once and it never ends until you cancel it even the server start restart or scale out usually the system can still keep the state and resume what is left until you speciically cancel it so back to that previous example that is you want to know what's the live number of the this month's revenue and you just run one single query something like select some amount from stream and that streaming processing will keep give you new results whenever new data is available so you are happy to render this nay or you can set some alert so this is the streaming processing and mean for portum is really doing something very interesting that is we take this to the next level we also have our storage behind so you don't have to using your streaming processing to send the data to diys system such as olab or c car but you actually just do everything in our own system alone we we give you the serving layer for both real time part but also the historical part even you can have a quiry to cover both the historical part and and the real time part so this is i don't want to throw too many idea but you might get the the the the keyword like lambda or kaa architecture we need to get into that but yes yeah that that's something we can we can discuss for sure we will dive into that so is it fair to say that your your definition then of a true stream processor is something that live it's not very very fast batch or micro batching it has to be a continual transformation of a state by a single event yeah by by event by interval by other conditions you limit but you define your atom of change but it has to be sufficiently small to count as true stream processing yeah yeah yeah yeah the pom is again we we start from those etl use cases and we also uniquely design our system to have more historical part not just rely on the other system to do the historical part and yeah that's uh and also we made some design choices to not depend on too many cloud technology so that we can be easily deployed in those smaller servers even edge devices or ed or disputed way so that you can you can you can get this some data from the local sensors to some fing and aggregate together in a central way yeah okay right i'm going to get into that but next i mean we're going to unpack this slowly the the thing i find particularly curious about this is you seem to have a dual storage model inside proton take me through that and especially the why yes so u take a like other common choices of stream processing such as flink even i think recently it's recently two or three years the community of flink also involved right they they do very good on the computing side now they are adding the storage side so there's a project called apache pon originally it is called flink table store as a a sub project of the flink but they realize it's it's just more than flank and they are aiming for much bigger space they want create call stream data warehouse or streaming house have the both the computing side but also the storage side and the stor side can be it's yeah i'm not saying this is next or the other iceberg but they really being very ambitious and they want to support other system such as spike apachi spar so anyway i what i'm why i mention this that is in the flink community they they move from or they involve from a pure computing engine to add the storage part but in the case of proom or our compy 10 plus at the very beginning we see this is the future so on the day one we're not just focusing on the streaming processor on the computing side we really want to have the vision of hey in the future this should be one single system people really needed to process data no matter they're comfortable using streaming way or using the the batch way so we choose clear housee as our our key foundation and it's it's already a very mature batch based system orab system we really add extra streaming capability into this cbase so that you can process data in the streaming way just as a stream processor or stream etl very similar to patch flink or k db but in the same time the storage the data is stored in in the in the playhouse historical part so you can you can qu it say i want to not just know what is happening right now but i also want to know what is the pattern or trend in the last two weeks two months or even two years it is possible you just quate in in proton along without asking other system and also there's other interesting things such as backfield u backfield is a special term i think very popular in the fintech area that they want to figure out what is my best trading strateg based on those uh those ticks or those stock price or those offers things like that then right usually they want to build a good strateg for the real time part but also they needed to validate that strategy using the data in the past i don't know maybe one year or two year they call this back view and it is very common people have two teams one team building this streaming or or real time trading power the other team building the historical power even they can be using different language so it's at the extra cost and the inconsistency so in in our case everything is sec based and we are using the same engine so you can easily build the same code for both the real time trading and also the backf test so there some benefit to have a system to have both yeah so this is very much the lambda architecture that i i remember first hearing about at twitter where they're dealing with the historical tweet data i'm not going to call them x's the tweet data plus the live hot stream of people tweeting right now wanting to see timelines right now yeah yeah lambda again is a very popular and relative easy to understand but the people also see the challenge of having maybe two serving layer right one serving layer for the real time power the other serving layer for the historical power even data is saved into different places yeah it can be complex so that's why people come into ca architecture i would say in the case of proton it's more like coppa but it's more like ca in a single banner so because we we have everything written in f us as our core engine and it can be compiled to a single binary and that single binary can serve for both uh real time or streaming query but also the histal par query and all the storage is uh is also also unified so we internally we do have historical storage based on clear housee so this is a column based storage you know and it is very fast for you to accretion especially if you don't need to scan all columns right for example there are 10 or 15 columns you really want to do mean max for two column the beauty of column column database is that is you only get the data part for that two column without getting extra data and you can apply some uh same s sd vector processing to to get results in a single cpu call something like that without have to minut stuff but so that's is we inherit from k house but also in the meantime we have our own portum streaming storage we call it a native log so it's a it's a internal component but the purpose of that is all the data is arrived in that streaming storage first it's more like red head loog apparently very similar to cka but much more lightweight then the data is there then we can do some streaming stuff then the data also replicated to the hisorical power so that we can get both so you're saying internally there's a right head calf caress glog which you're querying for the hot set of data and also writing it across to in bundled click house for classic analytics yeah that's right yeah that's i can begin to see why that architecture has a balance to it but that sounds like a lot of work on your side to try and is it is it a problem keeping those consistent is it a difficult thing to know how do you know when to query which layer for instance yeah that's really an interesting thing we to discuss so yeah in the in the streaming where everything looks very beautiful and easy at the first cl so that's oh yeah of course why not but when you really work on that you might realize there's a there's a lot of challenge for example like this uh out of order right so you you might get data from different sensor you might get from different gateway they might using different clock or even certain certain channel may have a some certain delay so all when the data is consolidated into your system you are not guaranteed that the data will be in exact the same order maybe some some first yeah see that somebody's using a mobile app but they were on the underground or the subway and they sent the analytics data like half an hour late that yeah yeah we have some use cases people using our solution for those fleet management the sensor on their truck and if they go into a tunnel i don't know maybe it can be 5 minutes 10 minutes the signal will be very bad yeah so but they will still send try to send out and you get them late so there some out of order event there some late event and also how to keep this state manageable right so as i mentioned that is usually the streaming cq is long running as it's always wrong in the in the backand but if you are doing some calculation it's it's always add the new data in the state never reduce then no matter how big your memory is it will be om eventually right so how to have a certain way to control the size of the state and also stuff like how you can change on schema is changed or how you react when the server scale out or scale down so that's a lot of challenges in our case we we do have extra design one of key important part of that is we introduce our own format we call it 10 plus data format tdf so the tdf is our key data format to have some design to aiming to solve those challenges for example we have a a column of flag called sequence number so the sequence number is the important information to let us know say when you ingest for example three data in a single batch to to prot then then we know each each event have id and then we assign a sequence number and later on when there's the other event come in they have a a different or a newer sequence number those sequence number will the data will be in our streaming stage first then we will keep replicate them to our historical storage but also keeping that sequence number so that when we don't really need to in some cases the data can be in both site but when we quir it making sure based on that s number we only get the data once so i give example say in our streaming storage the data you you can set a retention policy for one day for example again this is configurable in the streaming storage the rotation policy is one day in the historical storage it can be say one month then in our case if you wanted to hey can you give me the number of event with this condition for the past one week then we start around our streaming storage and we get one day data and then we know it's not enough then we can go back to our historical storage to to get the data from from that one one week to that minus one day and because we know this sequence number so we we making sure there's no overlap so for example ex in the stream storage the the the six number last number is say 500 then the other one should be 5001 or or 499 i don't know but making sure that's a very smoothly we can we can grab data from both s side but we don't have any duplication so that that's how we can solve those uh old issues or other issues yeah where so that does sound like there's an extra storage layer in there because where are you storing that state the sta we today we still save in our native log which is our streaming storage it's essentially it's a file based yeah but we canate to other so you're kind of snapshotting back to a different right head log yes yeah and then we have because it's our own format we can do lot of a crazy optimization have to wait for the community yeah okay so your system goes down let's let's not say it crashes it it goes down neatly it comes back up you're going to then to reserve that query walk back along the event stream until you find a snapshot and then add that into the historical is that how it works yeah yeah so for example we do upgrade for example say the version is a and we need to upgrade to b and the ev you have to restart the server right sobe just a 2 second or two minute whatever then yeah we making sure when we start a new server we pick up where we left so what's the previous internal state as well as the what's the what's the last sequence number we scanned so the last sequence number for example is 1,000 but now in the our stream storage there's a 10 10 more event we we know those new event we never compute so we can use that sence number to figure out what's the what's the gap we should process right yeah i'm with you i'm with you okay let me give you another difficult architectural question so what's your approach with let's say my fleet of trucks is huge right and i'm dealing with i don't know what counts as huge these days let's say a million for sake of argument i'm dealing with a million trucks whose state i would like to calculate over time how do you deal with with when that gets too large to hold in memory what what's your what's your sharding strategy for very large stateful set yeah sure it's it's not so uncommon right so we cannot put everything in memory i know some database they really want to put everything in memory even have some claster version of a memory based system so again as you previous mentioned that is the problem is there are people tr to solve in different way different solution it's very difficult to have one thing fit for all everyone have their own assumption and design choices or even design preference or depends on which customer they talk to right so some customer hate cloud some people only use clouds for example yes in in in in the case of lar of state or data we do have this sharting strateg i think this is very common that they say u to make it simple everything is a single shot but f fre to design a shouting strateg for example you think oh maybe three shot is enough or 10 shot is enough then it's yeah shing partition those are just acronyms i think so you can design a sharing strategy based on certain key so that data in the same shot is a is more relevant or or similar in some way then combining the shouting stage with the cluster because you might know even on a single single host you can have multiple shot right so and essentially each shot can can be independent io so for example if you monitor observe that is in your server the i ps capacity is very high you you cannot get a higher throughput the cpu cpu is uh c the you for example you can you can create a more shot so that you can have more in parel data io so so that's the good reason why you might want to have multiple shot in in on single server but also you want to have some certain a have val or dis discovery then you you might want to have multiple servers then each servers will take different shot right so i guess this is quite similar to how cka work right you can have a multiple partition for one topic and you can have multiple server handle different partition even you can have a replication factor each partition need to be replicated say at least twice or three times so combining together you have a relative common distributed architecture so support almost any number of event depends on whether you have enough disk io or disk servers yeah and the network within them is good yeah okay that so that raises another question for me which is you've chosen to embed click house as an olap platform but you haven't chosen to embed kafka you've written your own wrer head log what's the thinking behind that decision we want a system can be deployed as a single boundary as simple as that al or it can be a cluster as i mentioned we do support cluster and even the way how spun support cluster that is it's also a single binary but you can run the same banner in different role say i want to run this as a master i want to run this as a indexer as a search head similarly in our case even it's a same banner but based on the configuration file or based on some le election different things can can can can mean different role and also because of the single binary story it is is this is purely it is possible even even today there's a community user in our slack asking can i use c car to replace native log and the answer is yes for sure okay so so we made this as a more like interface that is we need a stream storage and we have our own one we call native log it's part of our process but if you really want to configure external master box such as c call or even rap panda you can you can do so then we we we just leverage this as a streaming storage for put our letter log or put our tdf format so that is configurable so that state snapshot you're talking about that could be written to red panda instead yeah that's right the that's the data part but also besides the data there's also other for example metadata information how you define things like materialize view how you define a stream and do you create the stream on this node how other node aware of that right so those information or comm data information is i think today is still stored in the file file format and and then we replicate to other node using the ra protocol so again internally there can be some data in the streaming storage some data in the his storage some metadata information in the form of file and the file can be replicated to each other yeah okay okay that makes me want to talk about because you've talked about how this is embedded how there's a tradeoff between the historical and the real time stuff makes me want to think a bit about performance like what kind kind of what kind of queries does it perform really well on what kind of size can you optimize for this kind of hybrid internal architecture yes performance is really one of the key kind of a driving factor when we design things uh we have similar some of the similarity with r randa for example randa basically is a version of kavka right yeah yeah and the fair summary in some over simplified way pum is a cass version of flink or we call it native engine of flink you you know what is happening interesting right now that is there's some native engine for spark spark is written mainly in java some part written in scala right but in some use cases people are not very impressed for the performance of spark the java version gvm version so there's a few project coming from facebook coming from even apple i think they they are written this in either super pass or rust as a native engine for for that even you know the company behind spark data breaks they they have been building this for four or five years and used this for at least three years they call it i don't know how to pronounce it it's a photon it's ph t and it's very similar to proton but with the letter h so that that is a native engine i'm not sure whether it's r plus but it's a in in place replacement for spark so you can run your spark worklow but it's it's executed using the engine so you see people if really want to aiming for the next level of performance gvm may give you some something but may give you some limit right if you want to leverage a smd you want to leverage the vector computing you want to have some modern f system i ring the memory management those stuff you might have to do this manually yeah java is vastly faster than it was when it first came out it's incredibly fast but if you want to have an argument with the cpu directly you kind of need rust or c++ or c yeah i yeah yeah i even see some some folks they are tuning for arm there's a lot of details if you do this carefully you can get way more faster but it is under underestimated so i mean a lot of program should run faster on arm if they do properly but they require a lot of effort but anyway long story short that is a we choose c plus as our language implement we leverage a lot of high performance library we always use the latest seong vm all those cool things so we can get a very very low latency without require a lot of uh a lot of computing resource i give you example for example one of our community user they they need to do they call high cardinality groupy the term is fancy but essentially it's just a groupy you know in the cq groupy key right but what if the key is not 1,000 maybe say 10 million so imagine you have a 10 million unique key and each key you need get a certain agregation for example the count the mmax or even some crazy one like p99 all right so if you want to do the extreme case that is you have a 10 million unique key each key you need to do some p99 p99 is tricky because you need to get all the data and figure out which value is at the 99% right so it's small comp than sum or or or average yeah yeah yeah so but in such cases u many system cannot handle that but because we implement in we have our own control for the dat structure we can actually spin off multiple internal process to leverage the the modern cpu and the all the linux stuff to to get this result correctly without uh have a lot of memory i think i don't remember the details i think it's more like 4x or even 10x less memory compared to flink for the same high cality groupy and this is high con good is very common for some use cases such as cber security you may know for example if if you have firewall access log i mean you want to figure out which ip keep sending you some package and the ip can be easily a large number of so it's high conned and some other cases such as f you might have a lot of stock or or bitcoins like that especially if you go into like ipv6 grouping by that yeah that's that's big i'm not saying is a really big challenge yeah how is that what's you're saying you get 4x or whatever out of that what's the key part of that what's the thing really making the difference yeah i would say it's really we can leverage the lowlevel cpu or io or linux stuff and i think flink in many way it's designed in a very generic way and with also with some limitation of a gvm you might have to do certain things uh in the in the generic way but when you have the chance to handle things i give you very simple example for example as simple as this a a bunch of integer yeah right so again i i i haven't write for a long time but if you put your design your data format in the proper way for example every integer is for example two bytes or four bytes you you make this as a fixed waste and using the same representation as the memory then essentially it's just more like a memory dump right then then you you you do the minimal marshall and maral things i guess this is also similar idea as aach arrow right they have the memory representation of the memory in the in the network in the disk so same thing that is if if you do this in a careful enough way you are handling data package in a very efficient way i i i'm not quite sure whether you can do this easily using java yeah i i'm gonna i'm gna say you can't i don't think you can do it easily i'm sure someone will tell me that you can do it but it doesn't sound like it's a natural fit for a for a virtual machine that tries to abstract away from these things someone someone will educate us both on that one but okay so you've mentioned this a couple of times and while we're talking about architecture i have to ask this if you're talking about arm and embedded systems you've mentioned is this if i want to stick a raspberry pi out in a field gathering interesting data is this a good choice for that yeah it can be so i at earlier days i i use i i forgot what's the instance type name it's a t2 nano it's a very small instance on eab i guess it's a half cpu 1 gab memory okay i use that for a long time for my demo yeah because we our banner is so small it's a few hundred megabytes it can be even smaller but what is not that is if you just run some filtering or some bounded t window so it just press data cons constantly and the send data back to kafka for example then it don't really require a lot of memory so say again i i want to pick i don't want to pick up on fling but just as a standard that is you you want to have a fling you need to have a the right version of gvm there are so many different gvm versions different random want to have certain control on the gvm anyway you choose the gvm and you download flank maybe you put in a certain pass you start it then you yeah cool i want to get data from kka and then you you you copy the cq from the documentation and run it it show you a result that it c f and then you figure out oh i need to download the c ja file somewhere and put it into the library then you your server this yeah then then you you run this and you want to oh maybe i need to create a jav file and deploy that jav file as my customer code so and and the the the server require certain amount of cpu on the memory i know there are certain way you can customize your flank to consume less data but it's not default so my point that is uh yeah again flink is already a great product but you still need to figus on many details but in the case of prum because we are so focusing on the simplicity you just run a single command to download as a single binary or have a a very small doc image and it can connect to your cka immediately and we are using the lib rt cka i mean the superas library from kafka so it can connect to kafka and to read right and i think it's fairly easy to support for example the resource on the rest p we also support arm for for a long time because us we have multiple cluster and some of some of the cluster is based on the arm chip amazon is that called gravity something like that so graan yeah sorry about that so amazon amazon have a very very nice arm based chip with a as up to what's the number up to 30% discount or something like that so it's it's much cheaper than the than the md or x 64 chip so we use that for a lot of our internal test so that's there's some difference but not huge so if if you really want to run stream processing on your rasp on your embedded device i think port is almost the best choices even i think lambda no no i mean rapand is doing something similar it is might be possible to run c car in your in your helmet in your other device but it's much easier you run randa in those small devices and you can maybe put porton there to do some secq based stream processing okay yeah because that seems like fun to me to be able to do the processing as well as the storage all in one simple place yeah and yeah one one more thing to add that is we of even talk to some of the users they for example they are in the energy space they needed to get the oil and they have a lot of sight because sometimes it's just like lucky jaw you put a lot of stuff there and the figure out among those 10 say 3,000 s which one have the a lot of oil behind so you need to have a lot of sensor and those are remot space you don't really have good enough network even you apply like 4g 5g it can be very expensive yeah so the solution is more like you have a lot of a more like local data center or micro data center and you get the data you do some filtering processing you might able to consolid them into the central servers one day but if you can solve those disputed environment network air guy but if you can apply stream processing or real time processing they can fit out a lot of garbage data and figure out what's the real signal and help you to figure out which which well you should spend more money on because those kind of infield processes tend to pick up a lot of noise along with the signal if you could pre-process that in the field that makes sense yeah do you think there'll be or maybe there is already a way in the architecture to then say okay so i've got this proton node which has filtered out some of the uninteresting data but still got a lot of interesting data can i now sync that back into my mothership cluster when the connection comes up yeah yeah it's it's it's already ready right so this is this our design goal that is we each each node is i'm not saying self driving but it they can do everything by themselves do some filtering figure out those information that interesting stuff then they can send them to a cenal server when they detect the network is good okay so because we have some storage or like buffer right so you never lose data but when when the network is recovered or when you turn on the network then that kind of data can be synced to the central server the central server can know all the data along different side do some maybe correlation search or pattern detection yeah so it's it's pretty much like c car mulite or or the mirror thing it's similar to the that but yeah yeah yeah so we just implement this in the proon side okay so come the day that amazon have their drones flying around us in the sky you'll be hoping they've embedded a proton server in each one i i hope that is there a many choices but assume they have good like 4g 5g so they can generate the data and send back to a server yeah i get some some user really want to do everything locally yeah but uh yeah if the network or len say is okay i think having each sensor on the drone sent to central kesis or c car do the processing there will be easier i mean we don't have to looking for trouble but there's a lot of choices for sure that would be interesting trouble to get into yes and then you'll have to optimize your binary for the weight yes yeah yeah if someone wanted to kick the tires on proton where do we go first yeah of course yeah go go to our repos on the github you can also get this from our temp pass.com yeah that will be easier you just temp pass.com i think we have a big enough icon to guide you to our github and in our repos there's a command line it has many options right you can have using c to to grab our banner from our server and you can run this directly or you can install it i mean install it just meaning you install to a certain pass and with some confusion file but it's optional you can just run that banner in your current folder so everything is very easy and also we have dock image beside that we have a bunch of dock compost so dock compost is really a good way to oh yeah create some sample stuff right for example we have a sample how to using randa to generate some data about click stream and visualize using for example guana right i mean because i mentioned we have not just a stream processor but also a serving layer so that that's something if you are somehow no flank i don't think that's easily you can do that is using guana to visualize your flink query because flink essentially want to get the data process it maybe sent to sent to a cka or to ol lab then you can quy from there i mean flink is not really designed as a serving layer maybe they have something like gway recently but it's i ga it's trying to just do transformation and not do storage or serving yeah yeah yeah so you might have to send this at to clear housee then using guana or other b to query it but in our case of proton you can just using guana to query p directly showing ni chart yeah gana metabase red das will also port how's that working are you just embedding a web server in it and graph is going to there or is it going if you got some native connector yes that's also we largely leverage clle housee right so we again we portum is we call power by cl housee it's not it's not really running a separate clouse process in our stu because there's only one process is called pum if you choose the single node way right so it's interesting pum is a process then we just have this a lot of uh nice thing from clear housee and including this port right so for example clear housee support it's it's interesting people in cl house really focusing a lot on the simplicity and the on the flexibility so they have htp port they have ttp port they even have different port for different database for example you can you can turn on certain port to using postgress or my cq client to connect to clear housee it is oh really i didn't so so those stuff we get almost for free right so because p leverage that so we we really customize the clear housee guana pring so that it can have much better integration was ort but that that kind of a channel that ttp htp all those stuff is largely inherited from k house so that's the reason why we really get a lot of benefit from k house all those nice uh different driver for different language we have integration for bi systems but also we are contribute back to the cl house for example we we contribute some pr regarding how we do streaming processing back to the community so that if cl house team want to merge it then you can get some basic stream processing in the in official clear housee version but if you want to more features for sure porton will be the best way but we but in the same time port because we are a small team with can iterate try different things faster and we we willing to share all the stuff to kous te but they might be more focusing on i don't know maybe data warehouse market as a primary goal for them streaming is something you want to do but they they they willing to us as contributor but they may don't have enough team focusing on the stream a lot in today yeah yeah here's the code do do with it what you will yeah okay well i'm going to go and play with that then i i have installed and tried proton but i didn't know i could connect grafana to it and get nice pretty graphs for free so i'm going to go and do that joe thanks very much for joining me thank you thank you joe as usual you'll find links to all the things we discussed in the show notes including links to proton source and its docs and time plus and if that's left you curious about click housee specifically as an noap database i've added a link to a previous episode we we did with al brown where we did a deep dive into click house you might find that fun hat tip to al while we're here before you check out that or another of our many episodes we're closing in on 50 now please take the time to like this one maybe rate it share it review the podcast it's all good for my heart and my feedback and good for the future of developer voices a future i now leave you to i've been your host chris jenkins this has been developer voices with joe jong thanks for listening o