uber migrates 1 trillion records from dynamo to ledger store and saves $6 million annually i mean that sound that sounds exciting i mean it only took 40 engineers it took 40 engineers and $20 million a year in maintenance but man we're saving $6 million right now here we go now uber migrated all of its payment transaction data from dynamo db and blob storage into a new long-term solution a purpose-built data store named ledger store honestly this sounds like crypto ledger store and it's not tiger beetle so already feeling pretty you know already feeling like i don't trust this at all you know what i mean oh my gosh dynamo and blob i know uber wrote this mono thingy but it sounds insanely hard i'm sure it is the company was looking for a cost savings and had previously reduced the use of dynamo db to store hot data 12 weeks old the move resulted in significant savings and simplified the storage architecture uber built golf stream its payment platform in 2017 and used dynamo db for storage due to rising storage cost dynamo db was used only for the most recent data 12 weeks and older data was stored in tera blob an s3 like service in-house by uber okay interesting ch interesting choices isn't there like a lot of off-the-shelf stuff for just making storage of of data easier interesting you know companies companies creating their own stuff classic this is just classic you know there is a bit of not invented he syndrome at a lot of these larger companies you know and netflix we did it all the time is it completely it's completely normal honestly it's completely normal to find these things that happen in the meantime the company started working on the dedicated solution for storing financial transactions with data integrity guarantees casik tech lead at uber explains the unique challenges of creating a bespoke data store ledger store is an immutable storage solution at uber that provides verifiable data completeness and correctness guarantees to ensure data integrity for these transactions considering that ledger ledgers are the source of truth for any financial event or data movement at uber it is important to be able to look up ledgers from various access part patterns via ind indexes this brings this brings in the need for trillions of indexes to index hundreds of billions of ledgers it's a lot of data it is a lot of data should have just used tiger beetle this does feel like a tiger beetle moment uber is a very interesting because they are mature enough to require writing their own code software items but none of it involves the cloud in any way which makes which makes sense to me yeah i know it's it's very interesting it's very interesting because visa is at what 40,000 or what is it 40,000 requests per second times times what is that that's per minute 24 time 365 so how many requests are is visa doing they're doing 21 trillion effectively a year i'm curious what this equates to you know like what i what i mean by that is like what is what are they doing that require such intensity and what is visa doing i'm just curious like what these financial companies are using oh wait hold on yeah that oh yeah that's only 21 billion a year so how many requests per second are they doing are you telling me that uber does more requests per second than visa sorry you're totally right i think a visa has the advantage that in their data is extremely segmentable they're on ibm mainframe okay interesting location updates yeah well but they're well they're talking about ledger store is a mutable solution that provides verifiable completeness and correctness that guarantees to ensure data integrity for these transactions considering the ledger the source of truth for any financial event or data movement at uber oh data movement is data movement like the movement of the car visa is using cobalt is that cobalt mentioned i don't know it's just it that's what is confusing to me is like what what are what what is these needs that it's so it's that intense you know again obviously we don't work there we don't know all the requirements you know you you think you understand something but you truly don't understand something i think it's logs dogecoin mentioned potentially they use blockchain maybe ledger stores support strongly in eventually consistent indexes for strongly consistent indexes the data store uses a two-phase commit in the first let's see it first persists in the the indent on the index and subsequently persists the record lastly if the record right succeeds the intent is asynchronously committed or rolled back in the case of failure there was a lot of words that were almost the word index in there that got me a little bit confused it prof let's see it persists the indent in the index and if the intent is asynchronous committ this is a lot of this is a lot of there's a lot of in i in words in that one sentence i think indent is misspelled are you sure cuz i'm trying to figure out what it is and i'm not going to lie to you i don't really know what's happening here okay i don't know what an indent is but i'm starting to feel like i'm playing halo okay we got the index okay we got intent the graph makes more sense okay in look insert the indent with an intent of index what the hell's happening here okay application insert into the ledger store ledger store writes intent to index a okay writes it to intent or to b success said writes record table record so it has to write to these two indexes and then it writes to the record table and then it commits to these indexes yeah i guess i don't understand why why right thoughts on uml diagrams i don't mind sequence diagrams this is a sequence diagram i think sequence diagrams are really great on showing the arbiter this is very halo 5 but i think in i think sequence diagrams are really great at just showing you what's happening in what order cuz it it it doesn't it doesn't tell you code and that's okay i know a thing about the credit card company and private blockchain that i helped work on but i don't think i can say anything about it cool cool cool story shy right two- phase commit for strongly consistent indexes okay to offload older ledger data to cold storage ledger store uses time range indexes to support temporal queries uber removed from using dynamo db and doc store for storing time range indices the original solution utilized two tables in dynamo db one optimized for writes and a second for reads this design was dedicated by dynamos db capacity management and avoided hot partitions and throttling the new design uses the dock store database with a single table and leverages prefix scanning for efficient rates okay pretty cool pretty cool stuff here ledger supports the index life cycle management automating data rendering or reind indexing in the case of index definition changes the process creates a new index backfills the data from the old index performs relevant validation swaps indices and deletes the old one damn that's they they're really going to town on this thing i'm still shocked that they're they're doing this all all in store i i i swear i must be missing something here meanwhile there's probably some startup in ind in an indian state or that writes everything to the drive that to the drive cell phone in squeal light and sinks once a day you're right partition instead of partitioning like because sque light can have 2 billion databases right 2.2 billion databases so i mean theoretically you could just partition everything by by like car right you could just shard on car sink sink syn every 30 minutes lord maco appreciate that anyways it's very interesting these kind of things because once you get into this level of of what you need to do you really have to i i want you really have to come up with some pretty unique engineering ing solutions right you really do have to go a little bit further out but i you know i'm just curious about why why is something why is something like silo or these just larger distributed dbs that are fast and eventually consistent why why build your own i think i could say that they were experimenting with private blockchains but it didn't go anywhere with it because blockchain is stupid oh oh they are doing a little bit of there there's a blockchain play somewhere in here that could that could be reasonable if that if that happens that that could i could see that being its own its own thing l2 technology the company faced unique challenges while migrating pedabytes of financial transaction data into ledger stores that's so much information how do they h how have they done that many transactions it used shallow and offline validation to ensure the correctness of the migration and let's see and the performance and scalability of ledger store in the production environment for shad validation golf golf storm was double writing the data to dynamo db and ledger store and compare the data returned by reads between the two data stores oh that's a smart way to do that so just doing shadow traffic effectively you shadow traffic out to the ledger store and that way you can actually see am i getting the same results back have we actually created a onetoone surface and then you run that for a long time because that should give you enough variation that you should know if you've actually one to1 it or if you've goofed it up so that's pretty cool that's pretty cool i think two reasons that uber data might be complicated are that they're storing all the driving paths for each trip and probably all the inputs that resulted in surge pricing decision oh interesting yeah the surge pricing i never really thought about the surge pricing the surge pricing is probably a reasonable thing that you probably want to kind of keep you probably want to keep all the reasons why you're surge pricing because my guess is that surge pricing is probably one of the probably one of the most likely litigation vectors of uber and so if they can show you the exact reason why they chose to do it i work for uber they're okay that that's too all of this is n certification of uber yeah i mean i think the big takeaway that i have right here is that remember whenever you build something like this your company also has to fund this and these are one of the things that always typically never have enough people working on it you never quite have the tools you need to do what you want to do in this it's always just like it's always so under served and it can be very very frustrating i find that anytime we have a like anytime a spoke solution to a to like a more solved problem always ends up just being completely underserved and you'll always see it especially with like ancillary tools like being able to visualize what's happened or any sort of like debug or like like any of that inspection side of stuff always is just where it really falls apart and it can be so frustrating additionally uber implemented a line validation for historical data coupled with the incremental backfield job running in an apache spark the backfield process alone posed significant problems as the load generated by the process amounted to 10 times the usual production load and the overall process took 3 months i feel like if i was doing data transfer for three months you're telling me 10 people for a back end and 200 people for the front end and app i know you're telling me how would you feel having a process that ran for 3 months doing data transfer like i feel like i would go to bed stressed out i would wake up st stressed out i feel like at all points i'd be i'd be losing it i don't even like running dat like data transfers that take instantaneously that go from one one shape to another i already feel like the amount of just overall sweat that i produce for that one thing it may just be bliss really if you get this wrong you'll cost the company $2 million go for it prime lastly the team took a conservative approach towards the roll out and adopt a fallback to fetching the data from dynamo db in this case it wasn't found in ledger store the overall m migration was completed successfully and the company didn't experience any downtime or outage during or after the migration the move to ledger store resulted in significant cost savings with an estimated yearly savings of over $6 million so they're going to be able to hire about eight engineers in the savings maybe nine engineers tops with those $6 million savings cuz you got to remember anytime you anytime time you save a million dollars and you hire an engineer and you hire like six of them you have to hire a manager and then in that you also probably have an hrbp you also have all these other ancillary costs that goes with anytime you hire nine people and so it's just like there ain't no way that they're going to get that much i can't imagine writing your own data store is worth $6 million a year the best part about hiring n engineers is that you can now make the feature even slower it's a fact of life i don't know i'm just thinking about this like i i don't know if six million dollar is the win of writing my own database effectively my own ledger store yeah but i'm sure i'm sure again i don't understand the data and maybe this makes perfect sense and if you were on the inside this would just make more sense right i'm sure that does exist it's just hard for me to it's just hard for me to see it right now was it written in rust at least it better it was written it was written in ruby that's my guess did they open source it or is it 100% internal db my guess is it's 100% internal db as of right now cuz there's no open mentioning here let's see i think they do this for a lot of engagement but it doesn't really matter nor was it a large project it had to be large enough i mean they put their financial data in there i would say that it's large enough to to do something right i think just in the end the amount of engineering cost that is required to run something like this has to exceed $6 million it just has to because you're going to really have to have a team of engineers support engineers people on call sres like just the amount of of just things that have to exist to make this work is intense and so i'd be shocked if there's somehow if they run this thing with three engineers you know every last employee there is just wishing for more features if they run it with 20 engineers you know every employee there is probably still wishing for more features because there's just too many engineers and nothing's getting done but you just know for a fact that it's going to be underserved if this is their target goal of savings 125,000 rows a second for 3 months straight that sounds very very emotionally bruising are you sure it's not more about complying with regulators it could be regulations yeah it could be it could be but i'd be i'd be a little bit surprised if it is because there's already so many financial databases stuff that i would be a little shocked if this is about regulations because it kind of feels like it kind of feels like that already probably is already a pretty solved problem because you know there's all these other companies also taking in money you know