orders of magnitude are a real testing ground for architectures every time your demands on a system go up by a power of 10 i think the game changes and gets more interesting so this week we're going to think this through in the realms of databases one query a second that's easy mode even even an analog system like a podcast host can manage one question per second 10 concurrent queries a second we've just lost microsoft access last time i checked i think it had a single global lock system so no concurrency farewell access not going to miss you too much i must admit but pretty much any other database should be comfortable in the 10 to hundreds range for concurrent queries by the time we get up to a thousand the game begins to change some relational databases will be happy with that some are going to start to push you towards connection pools caching read only replicas and that's completely fair enough they have a lot of work to do which isn't just to do with querying but i think above the thousands range we are probably leaving a lot of pre- interet architectures behind okay next level up 10,000 queries a second that one might even tax your operating system depends of course but you're probably in the realm of tweaking kernel parameters definitely load balancer parameters at 10,000 we're maybe even thinking about clustering multiple machines and that is a game cher but even with that i reckon experienced but naive me i reckon i could build a system that handles 10,000 concurrent queries a second provided i get to choose the kinds of queries we support and i know that's a big caveat next level up i have got to call it quits i'm lost at 100,000 queries a second i don't know how you'd architect that i really don't so it's time to call in an expert cuz i know it's possible i know they manage that kind of scale at places like linkedin and uber with a little something called apache pino so who do i know who's a pino expert tim bergland and he's about to do a wonderful job of taking us through the architecture of penino and the constraints that go into building a query platform that gets a good balance of flexible arbitrary queries and a really high performance sweet spot i'm intrigued by that i want to know how that's built and tim is not afraid to go down into the guts and explain it and as a bonus this week tim has a wonderful voice for radio so this week the name developer voices lands particularly sweetly let's hear from him i'm your host chris jenkins this is developer voices and today's voice is tim berland [music] joining me today it's tim bergland tim how are you chris doing great as my grandpa used to say any better and i'd be twins it's good to see you i've not heard that one before it's no no you're grandfather must have been a creative man run in the family definitely was i so the reason i've got you in and i'm going to back i've almost literally just got off a call with a friend of mine who's kind of a junior developer looking to get into the larger world and he asked me why we why there are different kinds of database and particularly why there are transactional analytics databases i gave him my best answer but it wasn't recorded i'm not on record you see what this is going go on record why why do we have different never really thought about that one now [laughter] it is an excellent foundational question and it's good to remind ourselves of of some of some of the wise i've just been doing a lot of history of tech reading and interviewing and assembling and kind of looking through that from a historical perspective which we needn't entirely get into but you know computers digital computers started out as batch analytics machines so at first you had a a bunch of stuff on on punch cards you fed it in and you processed the data they were they were doing analysis yeah and then you hook up a terminal they get a little better a little faster a little cheaper you hook up a terminal and you can actually interact you know you can you can write an application where now some knowledge worker can can do things and as it were input transactions you know do do crud over entities yeah so now there's this idea of a database that's changing and and and yet you still need to find the story in the data fast forward you know a few decades let's just kind of aideed aideed that for purposes of of brevity i think it comes down to this when you're in transactional mode generally speaking okay as a as a first order approximation you're usually worried about a thing you're making a thing you're trying to select a thing by an id maybe to read it maybe to to read it and change it put it back but there's there's usually the one thing that you're optimized for and in analytics you're usually looking at many things you you want to find you know i get into this ideally not read through the whole database but but select in some intelligent way the things you're interested in get some measurement and and run a reducing function over that measurement basically you know that kind of filter aggregate or fil filter filter aggregate thing is is sort of the bread and butter of analytics and so you're concerned with these things so transaction world you have this thing analytics world you have these things and the the the physical world being what it is you know with the limitations of of just the way m ma mass works and electricity works you know you can only do things so fast it's it's probably impossible to build one system system that's optimized for both things if you look just think about caching okay okay just think about caching for a minute caching is going to be your friend because you're you're doing io and there's various tiers of ways of storing things and you're going to want to cash stuff in the transactional case there's you know you're you're reading and writing a certain number of of ids these these things and there's probably a small set of your overall number of objects documents in your database that you care about that is amenable to caching right that that works well you've got this power law distribution probably of of your stuff and your doing this well now layer on analytic queries on top of that and that distribution is at least going to be have a a a fatter tail you know you're you're now scanning all these things maybe all the things sometimes and and well that's not amenable to caching you don't get to do both of those at the same time so you're not going to be able to build one system that that does both well okay yeah that's how i think about it i think there's a lot of ways you could tackle this but yeah but this is why you know this is the joy of running this podcast i get to see different people's ways of thinking i'm i can hear someone in my head arguing that they have got a system already that's optimized for both but your definition of optimized is a lot more focused right i mean i can take i can take an oracle database i think and happily get 200 parallel connections yes and i i believe you wouldn't be impressed by the number 200 i well depends what you're doing you know there's there's the the old kind of departmental database of the 90s that that paradigm and moved into the world of the web at some scale you know that might be great if you're looking at a broadly scaled smartphone app or a website with you know user population in the hundreds of thousands or millions and a lot of concurrent users and serving queries you know live powering features in that application then that's a horse of a different color still really achievable in the transactional world with kind of the received tool set like we know how to do that in the analytic world that's that's a very different thing right no nobody until recent very recent years has thought oh yeah let's let's do that analytic filter and aggregate group and aggregate kind of thing in the user interaction loop that's a little bit of a leading edge kind of thing yeah yeah at that kind of scale so coming at this from a solid background of relational databases and how they query right how how do you solve that problem how do you go from the system that will happily do 200 parallel queries happy enough to hundreds of thousands yeah well if it's okay i'll take the the transa or the the the analytic end of that i mean there's there's a lot of answers to how to do this on on both sides of that divide yeah and there's lots of folks who kind of have solutions for the the the transactional side of things you've got the cassandra kind of approach you know you've got various ways of scaling a postgress kind of thing you've got cockr db and you know all all those on the the transactional side on the analytics side you you kind of have to engineer a system with those requirements in mind you have to say you know what what we want to do and and by the way we need to back up a step and say you said coming at this from a relational database background you know the the tradition of analytics sort of pre- big data in the the span of careers of of people of a certain age you know kind of going back to the the 90s all that stuff happened on relational databases traditionally right it was fine that that was the technology i mean you you built schemas that that didn't look anything like the third normal form you learned in college but relational databases can do these things but if you're going to go to this kind of scale it's it's a it's a different tool set if you're if you're has to change the architecture has to has to change because you've got these non-functional requirements that present themselves of you want you want you know orders of magnitude literally maybe 100 a thousand times more than those 200 concurrent queries so you got you got kind of radically scaled concurrency a freshness requirement a sort of a streaming era real time data era freshness where an event happens and it takes a couple three seconds to make it through a streaming pipeline and and then it needs to show up in results right it doesn't doesn't get to wait around you're not there's no bash job and latency again if if this is going to power not again the origin of all this is kind of printed reports right yeah originally printed on green bar then printed on laser printers that all turned into dashboards in a browser tab and and those are the typical dashboard is just a faster report right and if it's the data is 15 minutes old it's probably fine you know it's that's not a big deal if it takes a minute to refresh something that's great nobody's gonna get excited but now if it's a feature in your user interface i tap on a thing and and a query happens i'm not gonna wait right that that needs that's you got 100 millisecond you just don't wait that all no i i certainly don't i'm i'm it's not a positive character trait but notoriously for things like that even people like in marketing who want those reports or an analysts and they will wait 15 minutes cu they're being paid to they'd rather exactly but you know users making choices of things people like me that's make a break that's make a break so yeah you get this concurrency latency freshness triad of nonfunctional requirements and so you have to you have to start from the beginning and the i think the the fundamental kind of rate limiting thing that goes on inside an analytic database anol app database is scanning right you're you're getting to where there is some metric that you're trying to aggregate yeah you know you filtered and maybe you're grouping by some other thing but still you got to you got to scan values of some metric that's the hot spot and so you want to there there two things you can do do one is scan less and the other is scan faster and i i hope that that brilliant insight oh good tim thank you for explaining this yeah i'll go with that i'll go with that so far today's episode of developer voices is brought to you by right so you can scan less you can scan faster and so i i go oh i'll give you a third one you can split the scan over multiple machines exactly and that is uh it's a corollary to to scanning faster you're and it's and i it shouldn't be a given it should be stated but like any system like that these days if it's not duck db is is going to be a distributed system you know there's there is that interesting case of like hey what can i do if i don't want to be distributed you know yeah what are cool things i can i can do in analytics database and a system like duck dp is is all that but yeah you're going to split the scan over multiple machines so that you can in effect scan faster yeah okay so patchy pino i mean there are a few options out here and they they all take slightly different strategies to get to that glorious scan faster scan less scan over multiple machines thing and i'm interested in design choices that pino makes to get yeah shall we just kind of walk through it yeah let's just let's go right down into the give me some give me some date structures ex yeah right this is i said to my wife give me some data structures oh yeah we talk about that all the time okay that one might not work for me at the same way but so [music] let's yeah okay so the fundamentally in apache pino you've got tables there's there's nothing weird in terms of data model in terms of query language you've got tables they have columns columns have data types you query the tables with sql nobody gets hurt right of note skipping around to the side a little bit you don't create tables with sql you create tables with json and and that's i i it's an interesting design choice the the kind of classical way is like okay i'm going to create a table i'm going to give you the schema and then i'm going to tell you a little bit about some indexes that i'd like and we'll sort of build this thing up over time and mutate the thing you know yeah and typical vendor extensions to create table like well here's the schema and here's this custom bundle of key value pairs i'm going to put in there you know i think the approach that penino takes is that that this is this is going to be a complex thing let's just kind of back up and here's a here's a json spec and you'll you'll create a table define a table that way so anyway you got a table um and and that's not essential obviously there there could be a future change where more dml is is implemented and you can kind of do it syntactic sugar over the top of that exactly exactly but see you get tables tables are broken up into pieces called segments and this is the the fundamental structure that that leads to the distribution that is the distribution model in pino right so as a table is created there's two different ways to create a table one is out of batch data the other is out of streaming data however you're doing it you're creating segments and so a segment is a an approximately time ordered time time delimited chunk of rows and okay why time is your like segmenting strategy and and it's not it's not strictly i mean you wouldn't you wouldn't call pino time series database but you it's i say approximately because say say the the batch ingestion process right maybe there's a collection of parket files in a s3 bucket and you're you're ingesting those you're iterating over those and and you'll iterate over them and say well i've got enough i'm g to call that that's a segment i've built a segment let me let me load that into the cluster okay and so during during that process and in the streaming case when you're ingesting say from kafka that's a bit more explicit you know you have messages that you're consuming and then you cut that off okay it's yeah i probably shouldn't even say time because that gets us thinking of a bunch of questions and maybe we can dig into those but but but we're starting think about the difference between the logical table which you're saying feels very familiar and the underlying dis model which is where things get tasty hey it getss tasty yes you have these segments right segments and it would be worthwhile to let's just let's just call a segment a chunk of a table for now it be good to dig into what one looks like in a little bit but here's this chunk of rows and we need to put them somewhere so that's that's now let's get into kind of the physical architecture of of pina the the components of the cluster there is a component called the server or the servers and i don't i don't know if i'm the only one in the the pino world who maybe regrets that naming a little bit because it seems like kind of aren't they all servers but hey look there they are they're really the true heroes of the cluster they store segment data and they do when it comes time to compute queries they do most of the compute work okay yeah so the segments table is composed of segments segments are stored on servers and let's see where shall we go next would you like to talk about how segments are built or do you want to try to do a query let's let's go through a query and we can dig in on the way yeah okay so in ingest and more to say about that the inest process creates segments segments are distributed among servers through some process which is interesting in itself and now here they are you've got your table there these segments maybe there's you know one server in a docker container on your laptop maybe there's 300 in your cluster in production yeah to make a query happen we have another element we introduced these are called brokers there are usually fewer brokers than servers but it is again a scalable element and this is where a client process will actually connect to an endpoint you know an api endpoint on a broker submit a little json query document that basically just says hey here's the sql i'd like you to execute and that broker now will take that that query and say all right we've got some table is implicated there i see some predicates and here's you know maybe the you know the the aggregation operation not going to be of of so much concern to the broker but here's a table and here's some predicates let me try to figure out which servers ought to or need to do the work of doing of executing this query because it'll it'll based on metadata that it's that the broker knows should be able to make some good guesses about where the necessary segments live depending on the predicates depending on the table metadata it's going to try to route that query in a smart way in other words not just it to all servers that have any segment of that table sometimes which would have to do but you don't want to do that the the word guess is interesting there guessing yeah you say it makes some guesses about which servers to talk to okay that's i think me being a little anthropomorphizing a little too much it's actually not probabilistic it's deterministic we no you do know okay good good you know either we can prune certain segments from even needing to be consulted right or in you know in the worst case you can't and you're literally going to scatter to everything and and and do a lot of gathering so it'll it'll do that it'll figure out which servers should get the query and there are you know various levels of configuration and optimization and things that that you can introduce to to help that work well but in the basic case you've got say maybe there's a a time column and you're going to know well this segment definitely only i'm aware of that time column i was aware of it at inest time it really only has things from time t1 to t2 and your query has a predicate outside that range i'm not going to bother with this segment right yeah yeah what if i i may be derailing your architecture diagram but what if like i can see how let's say you've got all the food all the data coming in from uber eats right and i can see how that be spread over 300 machines whatever now i come along and i am hankering after barbecue food as an analyst so i want to know like how much money was spent in texas since the start of the year okay right so that feels like the start of the year isn't a very useful filter and i'm going to really have to hit a lot of the cluster yeah to find all the places with the texas in and that's okay i mean that's that's fine the and you as an analyst who likes barbecue food i definitely love that journey for you and want to help you with that query right but the the the main place pino is pointing itself is more towards the person who says i want some barbecue i'm in san antonio it's six o'clock you know what's going on right now what what what like what's my what's the delivery time now based on you know the recent history and and and things happening in the last few minutes even if it's 610 delivery time might be different than 557 okay so you are explicitly optimizing for queries that have a a sensible date range not so much that no i mean that that large date range is fine that's just going to be a lot of segments that's maybe going to be a query that takes longer there's more scanning happening but the the user facing analytics use case is more what pino is thinking than the internal business analyst facing thing now that's that's not exclusive there there are in interesting real-time internal facing business analyst cases but i think fewer of them than once analytics once once you take that same set of data and turn it into a feature in your application and deliver it to a user then that latency concurrency freshness triad of non-functional requirements impose themselves on you and you need all this stuff that pino does if okay if you're just saying you know which zip code sold the most barbecue at dinner time in january or since january yeah it's okay if that takes a minute i mean who do you think you are right just go ask snowflake it's going to be fun that's interesting because as i say design choices lead to different sweet spots right and how it plays out in the world it's interesting that you're optimizing specifically for a kind of interactive user and that's well that's how pino was born and i there's there's a whole bunch more architecture to get back to and i mean hey you're asking the question questions here so we're going to go there but do you want to do the origin story quickly or it's it's it's important i think yeah okay we benchmarking your origin story against madam web in cinemar for about three more minutes if you can do better than that you're doing actually if you can only do better than that that's not good enough that's not a lot yeah ouch well okay it was born at it was born at linkedin just after kafka so as the the kafka fication of linkedin was happening and and that was really you know the framing of that story as we get it really has to do with data pipelines and not so much reactive microservices and that that whole angle of kafka and i know there you know links in the show notes you've got some great episodes that that talk about both those things yeah thank you yeah the recent bobby calderwood interview i think for for reactive microservices i mean that's that's my description of it i don't know if that's what you guys you you or he would call but i thought was great so you've got kafka creating these real-time pipelines and and sort of having its impact at linkedin and now there's all this realtime streaming data but linkedin was still a resume and rolodex site it it wasn't the interactive social media site with a feed that we know today that i visit often and the first thing that they you know somebody conceived of this idea and i i need to find the pm interview them but the the who viewed my profile part of the site and yes that was the that was the the beginning of real-time data and they wanted it to be real time i mean strikes strikes me that you could make that a batch thing and it would work but that's not what they wanted to do so they built this and i they had it was either elastic search or something like elastic search they had a you know a search tool and they they built it and did the the what was essentially an analytics use case on top of that and they used like a thousand nodes of whatever this search search kind of was back then that is probably how you would have done it yes yes this is like 2013 said well that's bad and and it's kind of funny to hear kishu gopala krishna he's my boss co-founder of star tree co-creator of apache pino he didn't want to write a new database i mean then this is this is really the story of i think like a a relatively healthy person what's the one thing you don't want to do well you definitely don't want to write a new database that's terrible idea right something has to be wrong and he tried not to but that the team ended up building sort of primordial penino right yeah in the origin story they took that thousand nodes down to 70 with increased traffic and then decreased latency and just all these wins and like okay this might be something and so it was born of a user facing analytics requirement and then at linkedin they started using it for lots more things like that you know analytics on posts the feed is built on penino queries so you're you're doing lots and lots of pino spread there from from linkedin and this was this was the early days of of gig economy meal delivery so door dash was relatively new and uber was responding to door dash by introducing uber eats and they adopted pino to to drive some uber eats functionality and okay let's get back to the architecture you've brought me nice back to thinking about food so if i've got that what's going on in san antonio right now that i can order query take me through how the architecture of that is processed got okay so comes to a broker we're going to look at the wear clause and see can i and and well okay sorry we're a broker right yeah we are in possession of fairly comprehensive cluster metadata what tables there are of those tables what segments there are what servers they're hosted on how they're replicated and perhaps certain other things one might know about the data in a segment like time ranges there is a notion of partitioning which we'll come back to when we talk about the the kafka integration so a broker knows all those things and i'd like to just take a brief detour to introduce another comp opponent the broker knows those things because there's this element called the controller that really is the the metadata clearing house it just makes sure metadata changes happen to the controller and it's responsible for pushing them to brokers and making sure that that brokers are always up to date with the current state of the cluster okay so it's not the server telling the broker like i've changed the server tells the the sorry what did you just call it controller and actually what happens is when you load a new segment that is you know insert data you've you've ingested some new data you tell the controller and the controller then makes sure that data gets to the right servers and then pushes that information those metadata changes to the brokers okay so it's the it's the the place where metadata changes happen in the cluster and and needless to say there's a little zookeeper hanging off to the side actually remembering all these things you know inevitably it's it's and thinking of itself as the the true and perpetually undervalued hero in every distributed system yeah absolutely anyway that's we got up basically we might get to minions if there's time but that's the the the pieces on the map we can do a query now okay query comes in broker being in possession of all the metadata because the controller is helpful and does its job says all right based on my predicate i'm going to i i i see that there are these seven segments let's go with eight segments otherwise we think are you trying for power of two here yeah no no no actually we'll go go five there's five there's five segments okay five segments that are implemented or implicated in this query and so now the broker will say to to how many servers can i scatter this query because it would like to parallelize that work right right now i'm i'm gonna avoid going much down this rabbit hole there's a there's a page in the docs that does a pretty good job explaining this but you on the one hand want to scatter so that you parallelize that work that was one of the ways that we could scan faster right yeah on the other hand you know maybe not necessarily all the way all the time because that will increase tail latency that will reduce reliability you know you components that can fail or you might have a gc somewhere so there is this little bit of tension between i want lots of people doing this work and i don't necessarily want everything doing this work yeah parallelization isn't free so you you build in that cost exactly exactly query routing is a is a deeply complex subject with lots of little knobs in it but in basic case if you don't want to touch the knobs then this thing happens the segments are are distributed to servers the the broker will figure out which ones it will then scatter the query to those servers and one server might have two segments and and need to run this query across two segments might have five of them you know it's it's it's it's however that works out but let's just say it's it's nice like we were building a slide for a presentation and there are five segments and there's one each on five servers and it's it's a beautiful world and gets scattered those servers now do the filtering and scanning they they do the io on the segment and do the compute whatever that might be you know i've i've i've scanned this metric it's delivery time or it's temperature it's whatever and now i'll average it or do whatever it is i do create that result set and those servers then send that back back to the broker the broker collects all of the results it has scattered and does that final reducing operation whatever it is and and and you've got a result okay so there are shades of map produce inside this map produce always always pop it's down in the corner winking at you and you thought it's what 2024 you thought you know 11 years ago you you were too good for it and it was a pain and it's just there and saying yeah no i'm i'm still i'm here and you can't get away from me it gets better okay it gets a little more mapper here okay i mean the thing about ma ruce is it it it really was a quite general paradigm that that just simply is the case yeah yeah if it wasn't invented someone else would have it's kind of discovered invented it it was discovered and and keeps getting rediscovered so yeah that's the single stage query engine i just described right i can bu configuration or by an option on each query say i would like the multi-stage engine please because that single stage engine works real well for that filter and aggregate thing i just described right yeah but what if it's a join what if i have two large fact tables that i have to join and then do some computation on the result well that that would cause you know gigabytes or more of of results to stream back to the broker in you know a really ugly case and brokers aren't built for that so the way that works is in the multi-stage case the broker will compute this multi-stage plan it'll figure out well yeah okay to to begin with you know the kind of root scanning that we have to do to get started sure these are the servers that that are going to do that work but they will then stream and shuffle again m produce down there like i never left thinking of the the two astronauts looking back at the earth it's all mce isn't it always was yeah so anyway the that that first tier of servers will stream you know potentially lots of data because now this is built for this to another tier of servers not brokers who might say and to make this a little more explicit say there's a a join an inner join and then an aggregation on the results well you'll do the left join maybe a filter if you can sorry the the select for the left table may be a filter if possible the select for the right table maybe a filter if possible servers are doing this work they'll stream the left and the right results to another tier of servers who will actually do the join right stream to another tier that actually does the reduce the whatever the aggregation is and then stream back to the broker back to the broker the brokers so the initial the broker at first is doing this query plan and then almost drawing a map for the how the data is going to travel eventually back to me yes yes and the the the shuffle step which has to happen there is because if you're doing say an inner join you know at some point you you've got a leftand table and you got to go look at a thing that's your right-and table and what is that thing well it's going to be a key value map in your memory probably right so you got to get those keys in the right place and is it as it's shuffling that is it resharding it so you're trying to deal with a subset of the keys on each later stage um that's a good question i imagine the answer is yes that gets to a part of the engine i a level of detail in the engine that i don't know but there there can be many stages there and so that that is likely that that happens but i don't want to commit to that answer because i'm not sure okay that's fair that's fair this is this is but this isn't quite a job interview or an exam you don't call them exams you call them tests very different maybe it feels like an exam over here it well i i i know what this podcast is about this is not a podcast for fluff so i don't i don't all okay good okay so i can see how that works but since we've mentioned map produce let's mention it one more time my first association of map produce was hadoop as it was for many of us poor souls and i associate that with parallelization i do not associate it with speed no no you don't convince me that this process be quick yeah yeah well i mean the proof is in the pudding right there are folks who who publish results of of how this stuff works it always depends on how big the data is what the cluster is like i mean you know there are so many variables that that yeah you know we almost need like nascar you know there's like one car that you can build and then the driver is the one that that that really competes i can't believe i just use a nascar analogy with you i i i just this is none of this is lost on me but it's podcast british host let's talk about nascar i'll pretend i i do actually know what nascar is so we're we're on good ground there you go and and and no disrespect for the the whole ecosystem there i'm just not not a part of it but i mean the idea is the the car is standard and and the driver is what differs and if you had like a standard cluster and a standard data set then then you could start to make some some comparisons but this is all there's just everything's happens in oranges in this world but okay yeah no it's this this again the design of this thing of of of pino and the the the controlling kind of set of of non-functional requirements as constraints concurrency freshness latency that first kind of query you know the the filter and aggregate yeah thing bread and butter single single stage engine there are documented results you know users not star tree yeah of like 10 12 15 milliseconds p95 latency on on meaningful production data sets yeah yeah but what i mean is so i'm not doubting the speed what what i'm kind of asking is i mean what's going on cuz 12 milliseconds right from the architecture you've described you could easily have spent that already just on networking and serialization so yes and that's not a join right so one of these fact toa fact joins on large data sets and that this could be a query that takes a second he said with horror yes yes no that that's that's not a a typical pino kind of latency but again once you start coming off you know coming away from the hyper optimized thing into generalized joins well yeah it's going to take a little bit but not not hadoop time that's the whole reason and and the world into which pino was born was a world of hadoop and hadoop pivoting into spark and and same thing with spark i mean what are you going to do that that takes less than 10 seconds that's that's the that's a fast thing you know yeah that's anathema and we get into reasons for that but i there are interesting decisions about the coupling of compute and storage the pre-allocation of storage the pre-allocation of compute like are these things going to be on demand or reserved and and how tightly coupled are they going to be and there's kind of a you know you can make quadrants with those two and and pino lives in a place on that quadrant typically where storage is tightly coupled and storage and computer pre-allocated they're they're ready to go you don't go find resources the resources are there and the compute happens in the server right where the data is on an ssd on the other side of a pcie connection and and that that coupling is key to the scan faster right is it going to typically be the case that the brokers and the servers are on separate machines but in the same rack yes they will likely be in the same rack you don't know but you know same availability zone for a clout deployment and okay definitely separate machines right in that case i think to get the next piece of this puzzle we need to start talking about indexes because we haven't mentioned that at all no and that is a a key place where penino has decided to elaborate and introduce complexity and and differentiation and things so okay key key point is it a good it might be a good time to to just remind ourselves those segments that that we make when we ingest data those are columnar in nature so you've you've got inside a segment little little chunks of contiguous storage that store the column values of a table in no case you have a row all stored together that goes back to the are you transactional or analytic if you're transactional or oltp you want the thing and probably the whole thing so put the whole thing in one place because you want to deserialize it or serialize it all at once if you're analytic you're you're you're probably scanning over some chunk of metrics so metric values so put those in one place right yeah so so when a when a row comes in you're actually creating lots of different segments one for each one for each column right yeah well it's it's called a segment but internally in that segment you've got chunks of columns yeah right and indexes indexes a segment is all of the columns of the rows that it comprises plus the indexes that you've built on those so what have you got of of note there is an invert inverted index you know i've got this value can you tell me the the documents rows in which this value occurs that's good for you low cardinality dimension type columns city state country there are a couple of different text indexes if you've got a column that's a a blob of of text of unstructured text pino has essentially a text index that is lucine so anything lucine can do it can do because it is embedded lucine and it also has recently in the last year and a half grown a native text index that's like a higher performance subset of the the common kinds of prefix suffix phrase boolean regular expression stuff there is this is all building up to the cool index so just give me a second we're we're we're we're getting somewhere js index yes there's a json index so imagine i mean a lot of your data you may be ingesting could could be in json format that you're going to flatten and things but imagine if you've got a like a a sub document an object sub object nested object is the word i'm looking for in that json that is sparse you know there there are 200 keys that might show up but you usually only have five of them say right yeah the scheme is very flexible the data is quite narrow yeah yeah yeah you know you could have 200 columns that would be a life choice p gives you a json index where you could say let's just take that embedded object bring it in as json in that column and json index it so now you can you know index into fields and array values and and all the all the usual suspects of what you might want to do are you saying specifically which paths you want to index i don't think so no no you're not you're just saying this thing go index it and so look through all the paths in there and build something sensible yeah okay cool okay there's let's see we've got text we've got j is a geospatial index because uber was an early adopter obviously that's going to be a thing they kind of care about where things are so yeah got to know where that barbecue is that's a whole podcast in its own and i i i i will be clear i frankly know not much about the ge spatial index i haven't really put that through its paces but it's there and and highly productional lied let's get to what am i missing oh range ranges metric ranges so you know something yeah just a numeric column that that won't work well with an inverted index because the cardinality is very high you might have you know a number of unique values that's equal to or in the same order of magnitude of the number of rows in the table and so the range index will just chunk that up into ranges and build an inverted index out of those ranges same one right yeah yeah yeah and the cool one cool one they're all cool the star tree index not to be confused with the company called star tree which was named first the star tree index was named first right okay yes if you named the index after the company that would be dorky naming the company after the index is cool is cool exactly and it well it is an implementation detail that leaks leaked into the interface happens to everybody but it's cool and even if five years from now the star tree index is old news and there's newer and cooler indexes we'll still have a cool name yeah anyway this is basically like a like a a pivot table save to disk you pick columns and and and this is the use case here is a filter and aggregate query so there are some columns that may show up in the wear clause in some order some combination of them say three of them four of them we don't know which ones are going to be there and then you want an aggregation on some metric sum average whatever you can actually have several aggregations computed in in the index and it will actually build a tree of of the you know the the the different different values of the different predicates that show up you know the different columns in the that that are that are going to participate in the index and so now you've got this log in search through the index where you get the pre computed aggregate so you don't even you just read the index you don't even have to go scan the column anymore so this is where you get those crazy 12 m second latencies right if you if you can pick the columns that everyone's most frequently asking about and the kind of sums that everyone's frequently asking about yes you design the index because you know there are these queries that are going to happen and this the typical response is when a person first first you know configures the index and runs a query everybody always thinks something's broken or like this has to be in a cache somewhere this can't be real you know it's it's too fast it's too fast i just i can't believe it stop stop you're giving me too much no but it's it's a super cool index because that is you don't want to be limited to that right the filter on these few things and compute this aggregate that's a lot of what you do in an olap workload so you can really optimize the the daylights out of that makes me think of google analytics where okay so in their case you can't ask any question but there are certain combinations of fields you're pretty flexible about and you can get those quickly and yeah presumably they're using something very similar to that kind of index one imagine and failing to do the rest because i've got questions for google analytics that they can't answer but you power users well that'd be a that'd be a good google anal google analytics pm get them on the show yeah so you can get google people on the show but you can't get their lawyers to sign off on releasing it that's the thing anyway that's an aside i want to pick up on one quick thing you said which was i maybe i've misunderstood the size of these segments but if you're storing the data and the index that seems a bit strange to me because the whole point of an index is it tells you quickly where else to go but where else sounds like it's right by way you already are yes and there you want to think of a few hundred megabytes usually segments get get chunked off into you know something less than a gigabyte okay and that's variable but that's a that's a common sort of size but the indexes are are there remember that you've the broker is is doing its best to make good decisions about which segments are actually going to get asked questions and there's a little bit of metadata around that there's there's even an index it's it's it's called an index that's it's it's it's not exactly an index but it's called the bloom filter index so if you've got equality predicates known known equality predicates that are likely to happen on a particular column you can have the broker you know be able to consult that broom bloom filter and know sorry not the broker the server be able to consult that bloom filter and know should i even bother with this so the the the the reason the indexes go in the segment is because you've got these little bits of metadata where the broker is is trying to do a good job not just routing to smart smart places but pruning in a smart way and so right what you don't want to do is scan through all the values in a segment you really would like to try not to do that so you can more efficiently read indexes or consult portion of the index in memory and make better decisions about how to scan that's the scan less priority right yeah so there is there's a thing about indexes that tell you where to go efficiently and that part that role is happening on the broker and then there's a thing about indexes of gathering the data up quickly once you're there not and that's happening on the set there's there's table metadata that the broker has that is something that the process of building an index might tell you but it's not properly a part of the index it's just table metadata like you know we have this timestamp column we're going to tell you the first and last time the highest and lowest time in this segment or you're partitioning on this other column you know we're going to tell you the the values of the partition key that are in this segment so that metadata is owned by the broker and it's index is you know like i said could be the output of a notional index computation process but it's not a part of the index the role of the index on the server is to optimize the actual scanning of values in a segment yeah okay i can see i see the distinction but it's certainly allowing you to narrow down a good chunk yes that was just what you want to do scan faster scan less you know you can only optimize your io code so much yeah and so scan less in in smart ways is is where you're making money okay so i'm gonna get into i want i want to gradually get into getting the data in here yes i feel like we haven't we haven't gotten a lot of we must in order to get there i think cuz we're going to end up talking about kafar i know it this i don't know how we could it's sort of contractually obligated when you and two of us yeah yeah absolutely but you haven't really touched you've talked you've mentioned but you haven't really touched on you have two kinds of tables you have bat and streaming i think you said at the start but these all seem like batch tables to me yeah it's it's i i think when explaining this stuff i always start with batch tables and the the terms of art in pino are offline and real time now offline tables are quite available for queries all the time again we know where that word comes from but it's it's a little bit misleading so there's offline and real time tables and yet i think it's just easier to start thinking about the way real offline tables work right because real time tables are just offline tables plus this extra thing a l from a logical standpoint okay there are also hybrid tables which are actual combinations of ingested batch data sources and ingested streaming data sources but let's put that to the side for a minute and let's just okay talk about ingestion okay we'll save that for the christmas special that's for the christmas special all right so offline table ingestion batch table ingestion formally is done outside of the server and outside of the broker it's it's a it's a process quote unquote external to the cluster and you could you could literally there's an api you can write a spark job write a hadoop job if you're into that kind of thing that that takes some external data source and and makes it into a segment and then presents that segment to the controller for uploading into the the cluster and movement to the servers and all that stuff right so you can actually using pino apis create segments put them put them into the server and and that's that's a little bit i don't want to say it's weird i mean it's perfectly great thing to do if if if you've got a lot of lifting to do if we could just go back to the case of i've got all this csv in s3 or something like that or parquet files or avro whatever it is there are you know built in command line tools where you can point to those things and you've created the table and and you you table has a schema and you write a little ingestion job spec file the only place yaml really appears in in peno unfortunately those ingestion job specs are are yaml and there's another we're going to introduce another component called the minion servers and this is where and we just kind of know there's going to be these background things you need to do right you're you're going to need to go through and you know delete things for gdpr appliance or they'll they'll be reprocessing of segment data and so the the minion mechanism is that it's just a little distributed job distributed computation mechanism on these servers over here off to the side and batch ingestion is is typically done if you're not writing your own in spark or whatever it's done with with minions you can not even really know that right you're just kind of following the script and running the ingestion tool and pointing to the things and writing the spec and okay it's running and i can go to the ui and i can see it running and oh it's done now right but what's happening there is these minion tasks are being created and distributed among the available minion servers to read the inputs create the segment files present the segment files to the controller transfer them to the servers all that stuff yeah okay that's batch with good enough should we should we move to all right okay make it more complicated for me yes but more awesome so penino co-evolved with kafka right kafka was was young and i don't know what version i have to look at the history but it might have been like you know before kafka even had replication there was there was protop pino being built alongside and so the grapes were being squashed they were they were being squashed before kafka was was the force it is now and as a result its integration with kafka is very what's that first class filial you know they their brothers sisters growing up in the same home right and so the the the typical way if you have a database and you have kafka you know you have maybe something like kafka connect to read from a topic and write stuff into the database none of that here penino is its own kafka consumer and so when you have a real- time table part of the and and we could consume from pulsar and kinesis and other places too just keep keep talking about kafka here to keep it simple part of the table configuration which remember is this json file literally contains bootstrap server urls credentials topic name to be a real time table is to be connected to a streaming source oh i see okay what you do is you consume messages from that streaming source and have options like do i start at the beginning do i start at the most re all that stuff is is is there but that's that's what it means to be that table and so as soon as you create a real-time table it's going to try to connect to that kafka cluster and start consuming and and it happens okay is does that mean that the there's a minion looking at a cafka topic reading create constructing a segment and what's it going to do is it going to wait till it's got several hundred megabytes and send it off to one way to do it which would be terrible yeah and so the answer is no minions are not involved in realtime table ingestion only servers so the controller is going to make some decisions about you know how many partitions there are how many servers need to be involved and there's there's actually is it it's not time for this yet we're going to come back to this let's just let's just say the server well we'll add layers onto this but servers are kafka consumers directly and so the server is told by the controller this is your topic you consume from it it may even be told don't let the cluster tell you what partitions you get you are consuming from these partitions and that's a configuration difference fact we just go there there's the the difference between the high level and the low-level consumer and the highle is hey server just take stuff you know consumer group protocol you'll get rebalanced you'll get partitions nobody gets hurt low level is we are going to keep track as a cluster of the key of the partition key and its values and these partitions will be assigned to you these partitions will be assigned to you you you only get to consume from these two partitions and then that partition key if it's a column in the table and it shows up in queries later on now we've got a new way of pruning segments because we remember what we we know based on you know the segment that gets created we know what what partition it came from right right yes there's detail in there it would it would i would i would like to just keep it at that hand wavy it's trust me it's cool if you know you have that as a predicate now the server is a kafka consumer whichever way it works it's it's getting messages and as soon as it consumes a message it puts it into an inmemory data structure called the consuming segment and right that consuming segment is participates in queries just as if it were a segment on disk okay and so that's the key as soon as a mess message is successfully consumed from a topic it can show up in a query result so there's there's there's no additional latency there it's in the consuming segment we're done right okay that answers my question yeah that's yeah that's that's why it's not that terrible thing and then it fills up for whatever you know your configuration knobs that define what fills up means yeah now we create a segment and there's a whole process where the server you know has various ways of of talking to the controller and the controller is going to make decisions about what server that segment now lives on and it might not live on the same server as the one that was gathering it up it's possible for it not to and we we get into stuff here where i i i start getting fuzzy on the details of these things yeah we'll sa that for p 202 this yeah this is way down the it's just interesting that that can happen yeah and and yeah so you got the consuming segment it participates in queries and then it's just creating segments through a process that looks an awful lot like what a menu would do okay so it's quite a different mechanism under the hood but the end result is transparently the same segments on disk this segment like thing in memory that is a segment for all i can tell and yeah and again if if i've got a bunch of batch data sitting around that has the same schema as my kafka topic then i can create a hybrid table where i've ingested that batch data and i keep ingesting new stuff from the kafka topic and under the under the hood there are actually two tables but that gets abstracted away and i i just i just see one okay yeah it's great oh there's so much more i can go into but for time i think i should probably pull back out slightly to user space there you go yeah because i i know there are some fun ways that people are using this and i want to get some idea how it's being used in the wild so yes a fun thing last christmas early in the season last christmas i you know it's just i want to say you gave me your heart i knew you yeah very next day i gave you my heart very next day you gave it away you were supposed to save that for the christmas specialty there you go but it's you know every day can be christmas really you know i believe it was strike published a dashboard around american thanksgiving time that that weekend that was fed by real-time data of transactions on their payment platform seeing live updating numbers right now that was to some degree i think advertising on strip's part right like hey look we do lots of stuff and and they've got a cool story you know you think of mom and pop and small merchants and and it's great stuff to talk about but they are penino users and so all transactions through the stripe platform live in pino and so it it and there there was there was more to that story there are a bunch of interesting pieces to that that dashboard yeah on the application side and and all that but with all that data in pina well it becomes fairly straightforward to to build such a dashboard because those queries are are going to be cheap things that you just get to run and and show to web users and the application programmer there is genuinely expecting to run each time it wants to display that is it going to probably be running a the same query i mean are you expecting the the web server to run as many queries as there are users asking for the data now my my answer is in terms of the shape of what it feels like as an application developer to use pino my answer is yes and i don't know in the case of that since i'm talking about a specific thing that happened just a few months ago there could be more details there sure but me as a developer i would i would be i wouldn't be immediately thinking oh connection pools extra caching layers that exactly exactly you should you should guard yourself against thinking extra caching layer now caching is not bad caching rules everything around us there you go it's it's an absolutely necessary part of of computation at every meaningful scale in our lives i'm not trying to say it's somehow bad but when it comes to data particularly analytical data the answer used to be let me pre-compute this and put it somewhere i can read it let me run this job and fill up my key value store because that's the only thing that i can query with the latency concurrency and freshness requirements that i have to be user facing and now it's out of day and i have to precompute and if it's multi-dimensional my life is terrible and all this yeah when you when you go to that i'd better cash this that thinking is an artifact of the way things used to be and that's not how you want to use penino you want to use penino like you'd use an application database and i you know suppose there's been caching there as well but you'd rather not right especially if you're if you're actually reading and writing entities you know i don't want the cached version of my account i want to see what my actual account is right now yeah that's that's nice because i mean writing that caching code is never fun and always ends up like you've accidentally got a second time of database yes yes it's it's and again i'm i don't want to be heard as you know giving some sort of anti-h invective it's a thing but in it it's it's a very specialized kind of thing and you as an appc speaking to fellow application developers you don't want to build that that's going to be hard to get right and so kino is trying to say that's not a thing you need to do anymore we've got analytics just ask me the question and yeah yeah there you go nice because no one no one builds cashing because they want to they build it because they have to right right unless they are specifically doing and i want to build data infrastructure kind of motion which is great i'm glad there are people who do that and that's sort of what this episode is all about yeah but yeah you don't as your job as an application developer is to not build data infrastructure yeah yeah cool so if i want to go and give pino a try where do i go and what do i expect to happen to me next there you go you could go to devstar tree. that's my favorite place for people to go and there's a little link quick start at the top and the current form of this as we record this episode is is a oneliner it's a it's a docker image it's the the pino docker image and it's got some little built-in magical quick start things that simulate all the components being present give you a little web interface it's all kind of there in a can it's it's wonderful because it's one line you can play with it you can't then modify the docker compos file and look at the data and it's it's all you know you press the button the light comes on so we're in the process of building and by the time you hear this or few weeks later there that might be the form of a proper repo that you clone and a docker composed file and data coming coming very soon as we we talk about this you'll be able to kind of see what it looks like there's also pen. apache.org and docs. ao. doc doc. p. apache doc. aino that's not mario here to it's a modern tongue twister the doc site put link in the show notes pino links in the show notes i think is the best way to put that where you can read more and star tre's got a great youtube channel i've seen they've got just really handsome guy fronting that doing great work great voice i love that guy yeah no there's and and other people on the channel too but some some great tutorials and kind of intros and and a lot of this isn't i mean how penino works is always of interest if you are a technical person you want to know that but some of it is you know why is this not snowflake are you are you stupid and you're just trying to make a different snowflake you know it's not and there's some we've talked about these concepts but we've got some other little videos that just help drive those home like why would you even want this thing it's a different way of doing analytics and one i think is going to be a lot more important going forward cool well we will find out tim berland thank you very much for joining me thanks for thank you tim tim ended there with a bunch of links so i'll just remind you you can find them all in the show notes including a link to the episode i recorded with bobby coldwood tim mentioned it's an episode all about event systems so if you're into event systems or you don't yet know what they are check it out it was a very interesting discussion before you click away to there or wherever you're headed if you've enjoyed this week's discussion please take a moment to click like share rate or whatever feedback buttons your user interface currently offers you and make sure you've clicked subscribe because we'll be back next week with another discussion from the world of software until then i've been your host chris jenkins this has been developer voices with tim bergland thanks for listening for