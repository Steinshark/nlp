let's get this started hello and welcome tonight to the talk with ronald grimm about concurrency patterns ranagram really honored to have you tonight as a guest and sponsor so please take it away speak about concurrency parents sections and let me start i want to talk today about concurrency patterns i am a big fan of concurrency why because this is the challenging this is the fun stuff so let's start first of all a few words about only a few words i promise and then we dive directly into the concurrency part first of all what's a pattern this definition goes back to christopher alexander who was an architect but not in soft drafted a building architect each pattern is express a relation between the certain context and problem in the solution i make it short okay we have essentially three types of patterns we have architectural patterns which describe the entire architecture the fundamental structure of the system we have design patterns which you may know from the design patterns book for example they describe the interplay of the components out of which the architecture is built and they focus on the subsystem you see the architecture pattern focus on the entire system essentially and we have to finally idioms which are i texture the side pattern in a specific programming language and what i do today is i talk concurrency about design patterns and architecture patterns each pattern has a structure i will not bore you this is oil additional and this is the swatch i think this is the kind of force structure but the process structure is pretty similar so each pattern is name has alternative name a summary motivation context examples this is how it's described okay now let me jump into the interesting parts first of all i use or my my source of patterns consists of three books first of all petal oil soft architecture volume two and four and then conclude programming in java from douglas that actually is only his name by the way this is a book which was written by by people working for siemens in germany pattern oriented soft architecture in short i always say only pose up okay okay let's start with patterns of design pedals and then finally let's talk talk more about the architecture there are two metals two metals you should be which is you should follow when you do something concurrently and here are the two metals be as constant as possible work as local as possible and therefore i first talk about being local or meeting don't share and here being constant meaning dealing with rotation let me put a different d what you don't want to have is a data race at that race is concurrent writing and reading of shared state or share data this is what you don't want to have this is by definition undefined behavior and you and it is pretty easy to get no database because requirements for data race is shared mutable state when you only get rid of one of these both there could not be a data risk because when you don't share there is no data risk and when it's constant it cannot be mutated and therefore i start now with dealing with sharing in dealing with rotation so first of all copied value it's 763 please straightforward but this is straightforward there's no need to synchronize when a sweat takes its argument by copy and not by reference you see by copy means each swatch has its own data each thread works locally on its data therefore you don't that you cannot have data races let me show you something by the way i have always examples involved when you have the slides you can try them out f3 threats these are c plus plus 20 sweats she sweat all of them take a boolean shared one by copy one paragraph one by construct and then you see i change shared to true and because this these threats use this shield here this is a data race when you take when when this sweats when this function takes this shared by reference you see such as here such is here you also see when you when you look at this output here that both the function by reference and the function by construct modify shared to tool now you have concurrent writing and reading hear you read hear you write and maybe you don't believe me there's an extremely good extreme good tool which you should always use when you do concurred stuff and this tool is integrated in gcc line and therefore i can use it here use the sanitizer equal threat and now you see a key output and sanitizer detects a data race and let me see if i can see more line 16 line 16 slide 16 line 60 this line here and you see this is the b so you can also visualize this start race using sweat sanitizer i strongly suggest this is not a pattern this is my best practice whenever you do something regarding cold currency always use threat sanitizer switch sanitizer fines with high probability with extremely high probability the easy data races what do i mean the easy data is on single memory locations such as okay let's go further so this was copied value there's another way to don't share use threat specific storage typically use case of such specific storage are for example you have a single sweater program and you want to make this mighty sweating you want to put it to midas trading what people often do in the first case is they just make their shared state sweat local therefore each stretch gets its own copy of this shared state what is also typical use case of thread specific storage is a threat local lockup you want to know which thread creates created what therefore to make the locker threat local let me show you an example i use this time when box a different online compiler and what i want to do is the following i want to sum up where is it 100 million numbers and randomly created you see between 1 and 0 1 and 10 sorry and i have four threats several what should i do of course i for course what should i do i create false rats so i have a sweat t1 t2 t3 each one performs the function sum up here's the function sum up it takes an atomic unsite long long to store the value of this summation it takes the vector of arbitrary values answers from begin to end use this vector here and now i can go through this vector you see begin end this is my vector and i sum up all values and i do it on tmp some and here's the here's the key point team p sum is read local meeting each thread has its own copy of tmp some therefore there's no sharing therefore there's no doubt race and then when i'm done with this calculation of a fourth of this vector i just added together using some fetch or at and this is fine because sum is a let me see it here let me show it here some is a atomic here you have it and this is how should i say a sweat safe way to sum up in thick big vector and by making four pieces out of it in each thread uses one one part of it one quart of it okay and here you see that this thread gets its wearable by reference let's jump back i don't have an example here but this is most of the time the easiest way to do it just make a future promise spell this is which performs this function here this lambda function this should assume does not fraternal value it returns a handle to the value and now you can use get to draw the value out of this this is called promise once more we have a pair out of a promise and a future the promise promises to produce in value which the future picks up in the future by saying thought gets and the nice point about this construct is there are two extreme interesting properties first of all this channel between the promise and the future is safe by design runner there's a little technical issue i would like to address quickly with you your mouse pointer basically leaves copies everywhere on screen when you point yeah done now it's gone let me put it let me put it here and then i will i will not use my mouse pointers therefore you have to exactly yeah that's that's probably the solution i will i will observe myself in the second filter then i see when i have issue okay okay here we are future promise spell okay one smarter promise is the sender it produces something it sets something in the channel which the future the receiver here this should this should be fine the future the receiver gets or draws out of the channel and there are two interesting points first of all this the channel is protected no one can see the action which haven't is happening in the channel and second the channel is flexible as it extremely flexible meaning the promise and the future can can be different threats so once more there's no data sharing here this is how also we can solve the issue of data sharing so this wasn't about sharing now we deal with the next thing which means mutation location means we have different kinds of locking let me directly jump into this stuff first of all we have scope clocking scope blocking is essentially the ray idiom applied to locking and what's the ideal the idea of phrase the following you are require interconstructor the mutex meaning you lock it and you release it in the destructor so you make a card object which takes care of locking unlocking android mutex then use geoscope the lifetime of this thing and then finally the c plus plus runtime is responsible for invoking the structure and releasing the source because this object discard object is a local a scoped object and we have scope blocking in various flavors we have latka uni-clock since 11. oh now i have to be extremely since 70 we have scope locked i think since 14 we have shared i'm not 100 sure about shield but pretty sure and now let me show you an example here it seems to work okay this is this is good because this would be terrible if i cannot show what it what i wanted to say this is by the way modeled after blockout this takes the mutex by reference it locks the mutex in the constructor it unlocks it in the destructor and this is my i say scope clock you can also say my god and now i use it i feel mutex which is now the scope lock is responsible for it and therefore you see here the mutex is locked because i displayed the address of the mutex and here the same address therefore the mutex is unlocked meaning when is the mutex one unlocked when scope clock 1 goes out of scope and this is exactly like 47 and therefore this is the last time you see once mortise is right our ai resource acquisitions initialization here i have a mutex in a local scope therefore we see here before local scope after locker scope and here lock the matrix release the mutex and this pattern would this idiom would not be so good if it would not deal with exceptions have a look i throw here intentionally the exception beta lock but i this is not an issue because here before dry catch park after a catch block you see and here exactly 939 the scope lock 3 goes out of scope therefore mutex one is unlocked so this happens in the good case such as here but also in the bad case meaning an exceptional code and this is the benefit of rai you have input which is extremely nice deterministic destruction behavior exactly here to the structures called and you see here even before you display the exception stood back and look you see who leads the mutex this is rai in this case a special way of using it meaning scope lock let's jump back then we have strategize logging the idea of strategize logging is another idea the idea is the following assume you have some component which you want to use single threaded mighty sweating in different cases and this is a library and now depending on the where you use it you want to apply different locking strategies and what of course would work to always look but this makes no sense when you are in a single-stranded environment so so what you do is you want to apply exclusive locking shared locking or no locking depending on how you use your stuff and therefore you configure your locking strategy and essentially you can do it in both ways i will show you both ways you can do it at runtime or you can do it at compile time essentially strenges logging is the strategy pattern applied to login when you do it runtime what is what's the advantages you can even change it at runtime you can go for logging to no locking or something else what what are disadvantages okay it's a little bit more expensive you need a pointy direction when you do at a compile time it's essentially for free you get flatter object hierarchies but you know templates are a little bit known for that but i can deal with this effect when i show you the example you get maybe pretty that's pretty error messages let me show you straight chest looking this is at one time so i have here class lock and here i have a strategized locking in this threaded size locking gets in its constructor unlock and this lock has two fulfill this interface lock unlock you see pure two workshop an abstract base class now i have different classes deriving from lock i have low lock which uses internally the null object mutex and you see this is a mutex which does nothing then i have here a exclusive lock which uses a stood mutex and then i feel a shared look which uses a shared mutex and by the way being constant is a virtual you see but because i look for example here my mutex unlock here it cannot be constant and therefore animated mutable sapphire i can lock it or i can modify it in a const member function such as this one and that one this is a nice use case of mutable okay and then i just put it together i have strategize locking this now lock then i use the exclusive locking so my swatches logging has unloaded this exclusive locking and here i have my shared locking and here you see the interleaving of lock and unlock calls this lock here lives until line 81 so if we see here unlock and here you see it more concise shared lock lock unlock unlock and here i essentially use the three different blocking strategies the no op logging strategy the explosive lock here we are sorry exclusive locking strategy and final little shared locking strategy okay a little bit let me and then we can do it at compile time and instead of an interface i use define a concept basic lockable meanings i want to power my strategize locking on a thing which supports basic lockable and this is what it should do it should support lock and unlock and this is essentially all this is almost the same such as before the big difference is here we are the lock is attempted argument this is all but what this is a compile time decision therefore there's no run time penalty but also you cannot change it at runtime this is also the sweatshirt pattern blind you can also say you say here you hear here more often the policy you apply different policies but then essentially it's a spreadsheet pattern from the game of war book okay let me jump back next pattern threat safe interface let me go to the thread safe interface the sweatsafe interface extends the critical region to an object okay why there's an undie pattern i will show it to you in a few seconds this is the following you want to use an object in a threading environment and what you do in each fund member function you call first lock this has two extremely bad consequences first of all your performance will go down because you synchronize may be too much and second you have a high probability of deadlocks let me show you what i mean this is a class critical it has a method one which first cause lockout mute and then it has a message to which is called which also calls lock up mute and now let's see what's happening by the way i should give this lockout a name sorry there's a small error here but but see what's happening you lock here then you want to lock here once more therefore you have here deadlock assume that mutex is not recursive here it's not mutex efforts exactly speaking undefined behavior but you cannot assume catalog therefore you have here this this lock cannot be successful but you will not come back to unlock it because this doesn't work therefore this is a typical receipt for that log and here the thread safe interface pattern kicks in and this is the idea all interface member functions public should use a lock all implementation number functions such as protected and private must not use a lock and now a interface mental function can only call implementation member functions and when you follow this pattern you will look at the deadlock but of course this costs a little bit because you have always a kind of interaction going for an interface to implementation member functions and you have tools to carefully think when you have static members or virtual member functions because then it becomes more complicated but anyway here's the idea let me show it to you let me f interface one which caused the implementation function one nf interface 2 which costs two three one and these are the implementation function they are invoked without siliconization of course because you already synchronized here and when you now invoke it such as here 31 it performs this lambda which takes this critical data because interface one of it this two calls two and one and this one the main function costs one and two and here you see what's happening let's see if we can observe what's happening by the way these are the sweat ids this helps you a lot to see which thread is active this means first of all this function is invoked interface one calls implementation one then we have one two three with the same thread one two three and one this means the main function is invoked first interface one you see interface one calls implementation one two three one two three and finally once more interface one which cause only implementation one and so on so you can see from this output how to enter the form of thread light and this is a sweat safe interface okay and now to the last classic pattern and then we go in the direction of architecture cut suspension what is the idea of current suspension the idea is that you need that your object has a lock in a condition and the condition has to be fulfilled before you can call but first what can be active on the object this means does calling sweat will put itself to sleep if the condition is not met i'll show you an example in a few seconds the calling sweat uses a lock when it checks it condition and the lockport takes the calling thread from a database or deadlock okay let me let's make it more concrete this is not concrete i will show it to you here two examples we can do guarded suspension in many ways once more the ideas that you have an object which is protected by your mutex and you want to work on this object and to invoke something of the object you have to check the condition condition variable so we can do this in different ways the waiting spot is notified about the state change remember you want to call something an object you cannot do it and then when it's done you may be notified so we can do both the rating is what is not default or the state chance or ask for the stage change so i call this push or pull principle we only have to push principle this means you want to do something an object the guy who used the object before does not need it anymore therefore you can use a conditional variable a future promise per atomic slash paris or semaphross to make the sentence this cases here starting with atomics are only possible in 20. i will show you an example with this i think we'll selection the poor principle does we don't support it natively in c plus plus there are more variations how you can implement god's suspension you can wait with a time limit this can be done for example with a condition variable or future promise pair and finally you can send your notification not only to one thread but too many threats and this can be done with shared futures conditional atomic latches and you see so you see there are many many variations to do it and before i do now anything i will not show you an example to condition rabies you can be really really happy because conditioning labs are really really hard it really really complicated i only show you on future promise example and i show you a large example this is based on level this is based on fengine here's a small workflow based on on a few china shared future and this is my idea i use a promise sorry it looks strange but when i promise sends a notification it turns white then i draw the future out of this promise you see this my promises the future and now i have four workers i've heard f scott and f bionic and what they do is the following they perform this function object they get you see her prepared this is the promise by it's moved inside and the future is copied why because the future is a shared future so once more i have three threats huh scott in piano each gets and future sorry in promise and to share future and it uses the shared future to be notified by the boss when they can start to work and they use the promise to notify the boss that they are ready risk preparation let me show it here this is what my workers are doing they get the promise by hourly reference and the future by kobe they work for specific fun aperture time duration you see between half and two seconds and when they are done with the preparation they say prepare set value this is the notification here there is signal to this to their boss i'm done prepare my work and the boss is waiting here here wait for her wait for scott wait for vianney so the boss is waiting until here until the post gets notification of all his workers then he says the word says work prepared after such amount of milliseconds when all sweats or workers sent their notification the boss can continue and now the boss sends its notification set value which goes to all futures and these futures are shared therefore all this future wait until the boss sends its notification and then they are what should i say they are done with their work so do you see this is a two-time student conversation first a worker prepared their work they notify the boss and we are done the boss continues when all three got sent their notification and then the boss i himself sends the notification you can start your work you see prepare your work start your work okay this was a honestly a pretty simple workflow and i'm happy that i didn't have to explain it to you using condition rareness because here i strongly prefer futures and jet futures and promises also and here's the same workflow upgraded to c plus 20. and here i have an ledge two ledges one for the workers one for the boss and this ledge let me let me explain it here this is the welcome this is the first switch you see work down and this means exactly here when the threat performs this line this counter is decreased by one and when this counter becomes zero the worker continues until here he waits and this go home latch has a value of one why because this is the lich which is let me say signaled by the boss saying countdown so let me put it once more differently this is the main program i have workers have g sweat also trendy sweat so i perform the work help scott in piano and then the boss waits also the boss waits here until this counter becomes zero and this counter becomes zero when all three threats decrease the counter by one because the initial number is three and we have three slits and when this becomes zero this call is unblocked or this this right step is done and then on the contrary the process says go home which decreases this counter from one to zero and therefore the worker can continue here and this is how this workflows goes and this is a latch which is a nice abstraction c plus plus 20. and you see this is pretty easy to digest okay let me continue now i've become no sorry i come to the not so easy to digest stuff even my exams are pretty straightforward but this is more more challenging now i show you to architecture pattern i think the monitor object is natively supported in java but i implemented in c plus plus and i showed the active object the active object is a nice pattern detective object separates method execution from method invocation this means each object owns its own thread when a client wants to invoke something on a server this method invocation counts to a so-called activation list and a scheduler triggers the method execution which is shortness activation list i will show it to you with an example so previously it's a little bit more complicated so essentially we have you see here six participants first of all it's a client the client wants that some method is performed on an active object the active object runs the different threads so we have essentially two things with a client or clients in servers in between is an activation list on which does the shops that prepares jobs are slot so the proxy the client calls the proxy which triggers a member function of the active object this member function goes to the activation list and this boxy goes back to the client this boxy call also calls a message request which has all which is required to invoke the method message is not invoked it's just gone it's not it's thought on the activation list and this activation just take covers the client from the active object sweater and interactive object the ss scheduler which looks inside the activation list and decides which shop should be executed next in my simple example i just use the next one but you can do here a lot of more sophisticated stuff there are two additional components first of all just the servant this is the guy who performs the job in work by the client this guy also supports the interface of the proxy and finally there's the future the future presents the value the future represents a hint to the shot which is starting the activation list and which the client can use later to ask for the value if there is no value okay so let me explain it once more from the dynamic point of view the client invokes the method on the proxy the proxy creates a request and passes through the scheduler and just scheduler and use the request on the activation list and returns this is what you a queue on the activation list think about it as a promise in returns the future to the client if the request to return something now we have some shop description which can be performed think about a promise which didn't immediately start it's essentially i use the package task so the member function execution what is happening now the scheduler determines which request becomes runnable which one should run next it removes the request from the activation list in the this purchases to the servant who should perform the job and now what's happened in the completion step the servant stores this result in the future oh so it's a promise which the future can use to pick it up and then when the future has the value of course you can delete the promise this is the workflow and to make it simple this is the client sweat and this is the server thread and in between this activation list and you see the client as the proxy the proxy creates a future which becomes a request object which is stored into activation list i hope you can see me i can see my mouse sorry here it is it's an activation list then the scheduler take you something from the activation list performs it on the servant on the right and it puts through a site back and the client can use the future to pick up the result and now let me show you an example of first of all advances disadvantages and then let me show you an example and here's an example what is nice about this pattern is the following extremely nice the only thing which you have to synchronize is the activation list this is extremely nice this is the only stuff which must be sweat safe you have a clear separation between client and server the shop is done as synchronously it's just not answering constantly not done immediately therefore you have improved throughput and the scheduler can take the shops out of the activation in different ways you can write and write a sophisticated scheduler what are disadvantages of course there's a lot of indirection therefore there's a performance pedality here when the shops are too fine grained and of course the system is pretty complicated to debug when you see the structure you can immediately assume to know what's happening what can went wrong and so on and so on okay now i show you a extremely simplified example but honestly even this is simplified it's not simple here we are active object now this was the wrong button what was happening full screen mode i should chunk yeah does it work now it works okay i have a few clients here so this is my job and then i explain you what's happening this narrative object i have one two three four five clients which perform which are performed in five different threads each client put users random number 1998 23 2011 2014 and 2017 random numbers between 1 million and 1 billion and what is my job to decide if this random number is a prime or not and here you can already see the result to which i will come so what is happening first of all we have the active object here's the active object it has an extremely simple interface it has a member function to a tune shop it has a function to schedule to to to to to run the chops from the activationist and the activation list you can see it here is a deck of package tasks package task is a promise kind thing and what this package task has is a function a function object no sorry a callable and this callable gets no arguments and returns a pool in it in the pool signals is this prime knot and in this number and because this activation list must be protected i have a mutex for protecting this calculationist and now let me show you my program first of all i invoke the functions get futures with sketched activation object and number 1998 get futures is dysfunction it gets the activity active object and the numbers of primes or the numbers the the numbers which are created by the way these are so far no no prime numbers are just numbers so here i have in vector of futures pull into then i create here number primes random numbers and for each random number i create a job which i put on the activation list you see which is part of the active object let's see what enqueue task is doing this is enqueue task here we are we're expecting task here we are enqueue task first of all this is a function object which determines in its core operator if the number is prime or not you see if it's not prime if it's prime it's just a simple function who to terms if a number is bright and here's the interesting part this is dysfunction is prime this is the job which is part of this promise and this prime stores the number it should decide if it's prime or not then i out of this package task i create my promise you see cat future and sorry out of this future i create my out of this promise once more so out of this promise i create a future then i put this this promise is prime this promise new job i put it on the activation list you see and this is stuff i have to protect because this is done concurrently and what i return is only the future the handle to the result so the chop is on the activationist and then i got back to future and here is in q task and you see it here and qtask and here i push back the future so here i have all futures a vector of futures and each future returns a pair the bull signaling it's a prime or not the ins is the number so then i unwrap the first future the outermost future and i got back a vector of futures you see get and here for convenience reasons i put all all the futures together you see i just move them in the first future this makes it easier for me and now the stop shop starts i say active object one now this is the schedule which says now perform these jobs these promises or in this case more specific package tasks let's see what run is doing run course where's my run here's my run run cause this function run next task as long as there is a task so what is one next task here it is it uses a lockout because now it acts with the activation list it asks first if the activation is empty meaning all jobs are performed if not it draws the first package task out of the activation list it pops it from the activations to attack and here the promise runs now something is happening here the promise runs and i do it until it's done so when i'm here all promises are performed and now i will serve space for my future and what i do now i copy all the future results into my future vector pair pool in you see this is a vector of purple and where a pool is signaling its prime or not and in just the number and now i sort it why because when i saw this i got forced all which are non-prime because force is smaller than two and then i got the ones which are prime and therefore i search in this vector for the first occurrence for pay first which is the indication prime not nine prime if it's true meaning here the prime number starts and this is what i display you see this is what i call back an iterator and then i i return all from this iterator until its end and this is what you can see here by the way i got 544 prime numbers in theater prime numbers and i got 99 499 not non-prime numbers and here are they this is from the beginning until to true until the point i i got here i should not say point i should say iterator okay this was once more i i have to say it was a simple example but of course this is a a challenging architecture okay this was the active object and this was the hardest part of my almost complicated part of my talk now let me talk finally about another concurrent architecture pattern monitor object this is pretty similar to the active object but there are two big differences first of all declined in the server running the sales thread the active object to kind of several different sweats second it's an extremely simple scheduler it just takes the first job so each a monitor object synchronize that access to an object so that at most one member function can run at any point in time so for each object has a monitor lock and a monitor condition that only one client can execute a member function of the object and the monitor condition notifies the waiting clients because the guys who cannot run have to wait this is the yeah let's say this is a class circle this is a class where i come you see i have a monitor object which has a few synchronized methods which are by the way based on the sweat safe interface or should be implemented using the sweatsafe interface it has a monitor lock which lock and unlock call and it has a notified end range see in this case i use the condition variable okay so these are the company components monitor objects supports member functions which can run the thread of the client this member function has synchronized methods and what most member function can run at any time and they should apply to sweatsafe interface which i explained before then this monitor object has a lock which guarantees exclusive access to the member functions smart synchronization and it has also monitor condition in which the member function invocations are stored they have to wait until this object is not used by any by any threat anymore and when the current sweat is done which is member function execution this next thread is awoken to start here and this awakening mechanism is the job of the monetary condition what are the advantages benefit of course is the synchronization is imported into monitor object tagline does not have to care the member function has automatically stored and executed and the monitor object is a simple scheduler first comes first of what is not so good is there's only one way to perform the member functions first comes first of and you use under the hood the thread safe interface and here of course when you do it wrong there's a chance of attack when one public member function calls for nothing both use a lot you have an issue and now finally let me show you the example and here is the implementation of the monitor you see it has a lock in which it calls underhood monit mutex locks this mutex it has an unlock which unlocks the mutex it has a notify one which uses a conditional variable seat here to send a notification and it has to wait which has an additional predicate to don't get a victim to spirits wake up at last vegas this is a terrible thing this is a condition where this could be of victims of a spruce wake up and lost wake ups it's therefore you have to use an additional predicate and by the way here i don't use type name or class this is a concept from c plus plus 20. a predicate essential function which returns a boolean and to use it i have to use the header concepts and now i derive from monitor my sweat safe queue because what i do is i have a queue which i want to make thread safe therefore i derive for monitor my sweat save queue i have a function for adding something to this queue you see type parameter t and have a function from getting something from this queue and therefore i have two i say here lock we choose this mod network function lock i push something on the cube then i call unlock and notify one by the way is thread safe therefore i don't have to do here anything special second i have the weight coin first await checks if something is in the queue when so it locks it takes something out of the queue and it unlocks and you see this gives me a threat save you and it only supports two member function one for adding something what one for getting something okay and now i have this is my program and then we are done with this concurrency pattern stuff this is my sweat safe hue i have a function at lambda a function get lambda what atlanta is doing is the following it adds something to the queue is it displays the value it's solidity and this is by the way what you can see and here this is what attitude you see 1.1345 the reason is pretty simple because where is it i cannot find it i cannot find it then anyway thank you self-q at yeah i i create i let a dice run and i create random numbers between one and six so atlanta i'm a little bit irritated anyway save queue okay ah now i have it here i invoke the dice you see here i invoke my thighs and this red performs this function at lambda so this is what is performed and this is the random number this is what my atlanta needs and this is what my atlanta gets here and therefore we get here the numbers from one to six depending how this dice goes and in the other part this is my to my get lambda number function which runs also on its own thread and here because i do sweating using c plus plus 11 i have to join my threads explicitly and once more i have 10 spreads creating random numbers and here you see the thread ids therefore you can see the numbers are different therefore 10 different threats are running honestly 20 because i forgot to mention this one which i created here okay this was the monitor object and the idea is essentially you have object which has both a lock unlock facility in the notify weight veggie okay let me jump back because i'm i'm done so this were the patterns i presented these patterns are more fine-grained they deal with the two key concerns of concurrency sharing and mutation once more when you can get rid of one of them you are threat safe by the side because no data this can happen and these are the two patterns which deal with the architecture and this is the active object and when it object okay now i shut my mouth and i hope okay it's just in time and we have questioned highlands yes hello reiner nice talk so let's start with the questions speaking about std future what do you think about proposals like n number that proposes a continuation pattern for i know what you mean you i know what you mean this is a great idea and honestly this is the which of the futures we probably should have it with c plus plus 20 about due to discussion around executors it is delayed by a few years and we don't have it with c plus plus 23 maybe with 26 26. i'm a big fan because now you can you can combine futures we should interfaces such as dotson when all when any now you can build big big workflows out of consisting all of existing futures that is the future of the futures indeed what do you think about the need of lock if a data is only read in another thread concurrently whereas only one thread is writing to it it's looking still require still required or not when you have concurrent writing and reading you must protect this is extremely simple you can optimize it a little bit you can use a shared look and this means at one point in time arbitrary number of threads can read but only one can write so you can weight them in different ways but once more you have to protect or read it on my blog posts to get the details so when talking about architecture i just saw that your cr your member class had a public lock and unlock function and i wondered why are your implementations they're not like using scope level locks like lock art because of the key throw of the key you are using there you're making threats safe if that key throws then example exactly i'm not sure because all of from the the monitor object where you make the key thread safe ah no i see let me go back and you had a issue here okay i see your point now i have it and in between you called something and key right there let me think about it i i see a point so scroll down a little bit go to add okay when you would throw here an exception you would not unlock it here it's you're right this could be improved by using a scope card which is essentially a gap which takes care of the underlying mutex let me think about it good point let me think about it okay is sc promise and future pair a useful use case pattern for a controller thread to start some threads one by one in order and wait until each finish its initialization works so that next threat honestly i am not totally sure if i can answer your question exactly but let me just say a few words first of all a promise future one is a one-time thing so you can a promise can only sell once a notification and then you have to establish this pair once more this is really really a pity with c plus plus 11 because for a ping pong game so for respect works you need a condition with it but here's my point with c plus plus 20 you can even synchronize on atomics atomic flick or you can even use latches barriers or you could use summer force all of them are way faster than condition variables and way easier to use that when you have a one-time synchronization in 11 i strongly suggest that you use a promise future pair condition reps are terribly complicated i have by the way written a post about it if you don't believe me let me just show it to you and this is fun reason one of my most heavily used red post you see be aware of the types of condition variables more than 300 000 times you see they have many many issues i hope this answered your question it did why do we need scope block implementation of c plus supports lock guard i think that's like a beginning of a talk i think you just show like how this implemented how it works so that people understand that this is a good question we got the scope clock with c plus plus 17 the lookout is 11. the main difference is the following a lockup can only get one mutex a scope that can get an arbitrary number of lockable things and the issue of the scope lock is it guarantees that they are locked in an atomic step this is a main issue in the scope club supports it and when you when you study scope.on cpu reference you see it has it has to take at least one argument in an arbiter number of arguments so at least once let me put it this way and once more takia the key point is locking lockable things in the atomic step is a big big challenge and the scope track provides this and one of the reasons why it expresses that we could not go back and change the lock guard class to do that because there's problems with api stability this may be a reason i never thought about it but i i know they are the most extremely simple they don't support an interface the api was closed and they couldn't you know informators were like in a different object so the scope lock is also extremely simple it can only lock an arbitrary number of local buildings i say local business because it's essentially a concept okay what exactly does concurrency and parallelism mean could you show like an example of the same concurrency means this is essentially a question of definition i'm not sure if there's there's at least five different definitions this what i always do when i write a book is write a definition i use in my book therefore you at least know what my understanding of this term is palace means when something really runs in parallel you have four cores and it runs in parallel concurrency means when something runs at the same time you the skin still interleave you can simulate concurrency with one cpu because the scheduler always lets something different one and this is this is my definition or difference between concurrency and parliament many other people also have the same definition but this is my definition so paralysis means really cool doing something at the same time and concurrency simulating parallelism so i think the best explanation i've heard so far was that concurrency as basketball and parallelism as track so and then in concurrency you have a lot of dependencies on other processes and you communicate and and parallelism it's more about like you know sorting through a list of numbers with multiple threats there's like you know each thread has its own block of numbers starting it and then there's later like one big sword from the rest of the numbers i'm happy that you explained this picture to me because so far i i i know this picture but i couldn't understand it anyway yeah that's probably not not accessible to everyone okay so someone here says concurrency depending on data shared by all parallelism does not incur sharing yeah that's that's also a good take let me see we're not running out of questions so far if you want to upload questions that also helps me choose it's gonna drop those i already did and there's an architecture question how about that if you don't upload things then i just pick so i think architecturally do you prefer a single threaded object with a channel to share data or thread safe objects oh this is a complicated question honestly i don't know i i strongly assumed we don't have one solution which fits all and channels are extremely nice abstraction extremely nice in particular because they are protected and thread safe objects let me put it differently i assume that you have to take do you have to think way more about stretch safe object s compared to shared data compared to a channel because our channel is existed here you have two components and they are just connected with a sweat safe channel whatever that means but with thread safe objects you see you have to think about deadlocks you have to think about if this is implemented the right way you have to think about as we saw it before should i use a mutex or should i put this mutex into a lot cut to be safe in in a when an exception happens i think sweat safe objects are as a few steps more low level and therefore i now would say i would push out tennis but once more there are so many different abstractions here one is this future promising and there's so many different abstractions okay would you advocate for using mutex or recursive mutex by default y by default always go for the simplest solution this is the utex because it is probably the fastest one and when you want to have recursive mutexes or any honestly i never use sofa recursive mutexes but when you need them because you have to lock it two times and to unlock it two times then this is your case let me add an additional mark when you use a mutex from the same sweat a mutex not recursive a mutex from the same side record safely this is undefined behavior on but you get this high probability of catalog but it's undefined behavior yeah and and when debating what what is the best mutex and one also has to bring in that under windows you may want to look into the binary sizes of the new texas shared new text is smaller than new texts under windows as far as i know and the abi because really i did only a little bit of performance measurement i was really astonished how slows shared more taxes out but anyway of course you know make that decision what what kind of mutex you want to use you should measure that's this is so this is a different point this is a totally different point i think you measure i can i can say a lot how you should measure but this is a different story yeah but i've just played around with so many things and measured via what's the website but it's on my blog and i was like astonished like you know how i really like you can see a difference in in the same code with different compilers and like or even the same compiler on a different version so yeah this is exactly what i say when you measure something when you change anything in your environment you have to measure once more anything different flex different compiler different topic do it once more different c plus plus stand up do it once more example you should put your measure measuring functionality under version control so there was actually also a question on linkedin i should quickly bring on the screen someone i think this was during the active object is this how an asynchronous method invocation is implemented this is a question or remark i don't see it i'm not sure about but it's just something i thought i wanted to bring up to you like so i think that synchronous invocations can is probably one way to implement it but i think it's more like this is more like one thing which you use for rpc am i right like active object would you what you use there to implement an rpc i'm not i'm not you're probably a remote procedure called but you can do it maybe there's also a network in between but the essential point is you can make this bigger and bigger it's astring chronos and there's a there's a lot of additional infrastructure you can do here and there but the key idea is this activation list and you have you have which has a sweat safe interface essentially but you see there are proxy you can see it you can also create from an interface definition language and stuff in the skeleton which you have to implement so this goes direction of combat so there's a lot of additional stuff which in here you can talk here about all right let's see if there's a quick bench right quick benches inside that quick bench okay this one which is on compiled explorer also so i think yeah so there's one question which doesn't have anything to do with parallelism or concurrency someone else like what would they be able to learn about low latency programming techniques in your program and i think you mentioned that you plan to exactly let me just jump back to make this mod here this is probably but one of my last mentoring programs and this is about hpc and i i know of a lot of people which are highly active in the financial domain you may notice that like a few presentations in the last year there and they helped me a little bit to identify what is for them highly valuable and of course low latency is one of the key points here because when you can make your transaction faster than your neighbor you have a benefit so once more this is what i will address here okay yeah that's true and i think as yeah there's many ways where your courses will go in the future so i don't see any more questions which i was covering thank you for your time rana was great to have you as a guest and we'll be probably hanging out on the launch now if you have any more uh