cso i wanted to make a video about gpt - 2 because it's been in the news recently this very powerful language model from open ai and i thought it would make sense to start by just doing a video about transformers and language models in general because gpt 2 is a very large language model implemented as a transformer, but you have a previous video about generating youtube comments, which is the same kind of task, right? that's a language modeling task from language processing to generate new samples for cooling of the most complex or magnetic consistent brackets like a computer to expect found in creating organizations i believe that video was made october 2017 and this paper came out december 2017, which has kind of revolutionized the way that people carry out that kind of task. that's not the gpt -  2 that's something before that, right? that's the transformer, which is a new realm. yeah relatively new architecture for neural networks, that can do actually all kinds of tasks, but they're especially good at this kind of language modeling task a language model is a probability distribution over like sequences of tokens or symbols or words or whatever in a language? so for any given like sequence of tokens, it can tell you how likely that is so if you have a good language model of english it can look at a sequence of you know words or characters or whatever and say how likely that is to occur in english how likely that is to be an english phrase or sentence or whatever and when you have that you can use that for a lot of different tasks. so if you want to generate text, then you can you can just sort of sample from that distribution and keep giving it its own output so you you sample a word and then you say and to be clear sampling from a distribution means you're just taking your you're sort of rolling the dice on that probability distribution and taking whichever one comes out. so so you can like sample a word and then and then say okay conditioning on that given that the first word of this sentence is v what does the probability distribution look like for the second word? and then you sample from that distribution and then it's you know with a cat and you say given that it's the cat what's likely to come next and so on so you can you can build up a string of text by sampling from your distribution that's one of the things you could use it for most of us kind of have an example of this sort of in our pockets of its actual absolutely right and that's like that's the that's the way that most people interact with a language model i guess this is how i often start a sentence apparently with i i am not sure if you have any questions or concerns, please visit the plugin settings so i can do it for the first time in the future of that's no good here's a different option. let's just see what this way. maybe the same i am in the morning but i can't find it on the phone screen from the phone screen on the phone screen on the phone screen on the phone screen on the phone screen. i don't actually know how this is implemented it might be a neural network, but my guess is that it's some kind of like markov model markov chain type setup where you just for each word in your language you look at your data set and you see how often a particular how often each other word is following that word and then that's how you build your distribution so like for the word "i" the most common word to follow that is "am" and there are a few others, you know so this is like a very simple model and this sentence on the phone screen on the phone screen on the phone screen on the phone screen on the phone screen he's actually very unlikely, right? this is the super low probability sentence where i would somebody type this and the thing is it's like myopic it's only i'm not sure i even it's probably only looking at the previous word it might be looking at like the previous two words, but the problem is to look back. it becomes extremely expensive computationally expensive right? like you've got i don't know 50,000 words that you might be looking at and so then it so you're you're you're remembering 50,000 probability distributions or 50,000 top three words but you know then if you want to do 2, that's 50,000 squared right and if you want to go back three words you have to cube it. so you like raising it to the power of the number of words back you want to go which is which means that this type of model? basically doesn't look back by the time we're saying on the it's already forgotten the previous time it said on the it doesn't realize that it's repeating itself and there are slightly better things you can do in this general area but like fundamentally if you don't remember you're not going to be able to make good sentences if you can't remember the beginning of the sentence by the time you're at the end of it, right? and so one of the big areas of progress in language models is handling long term dependencies i mean handling dependencies of any kind but especially long term dependencies you've got a sentence that's like shawn came to the hack space to record a video and i talked to blank right in that situation if your model is good you're expecting like a pronoun probably so it's it's she they you know them whatever and but the relevant piece of information is the words short which is like all the way at the beginning of the sentence so your model needs to be able to say oh, okay, you know shawn that's usually associated with male pronouns, so we'll put the male pronoun in there. and if your model doesn't have that ability to look back or to just remember what it's just said then you end up with these sentences that? like go nowhere it's just a slight like it might make a guess just a random guess at a pronoun and might get it wrong or it might just and i talked to and then just be like frank, you know just like introduced a new name because it's guessing at what's likely to come there and it's completely forgotten that sure was ever like a thing. so yeah, these kind of dependencies are a big issue with things that you would want to language model to do but we've only so far talked about language models for generating text in this way, but you can also use them for all kinds of different things. so like people use language models for translation obviously you have some input sequence that's like in english and you want to output a sequence in french or something like that having a good language model is really important so that you end up with something. that makes sense summarization is a task that people often want where you read in a long piece of text and then you generate a short piece of text. that's like a summary of that that's the kind of thing that you would use a language model for or reading a piece of text and then answering questions about that text or if you want to write like a chatbot that's going to converse with people having a language model as good like basically almost all like natural language processing right is it's useful to have this the other thing is you can use it to enhance enhance a lot of other language related tasks so if you're doing like speech recognition then having a good language model like there's a lot of things people can say that sound very similar and to get the right one you need to be like, oh, well, this actually makes sense, you know this word. that sounds very similar would be incoherent in this sentence. it's a very low probability it's much more likely that they this thing which is like would flow in the language and human beings do this all the time same thing with recognizing text from images, you know you've got two words that look similar or there's some ambiguity or whatever and to resolve that you need an understanding of what word would make sense there what word would fit if you're trying to use a neural network to do the kind of thing we were talking about before, of having a phone, you know autocorrect based on the previous word or two suppose you've got a sequence of two words going in you've got "so" and then "i" and you put both of these into your network and it will then output, you know like "said" for example as like a sensible next word and then what you do is you throw away or so and you then bring your set around and you make a new sequence which is i said and then put that into your network and it will put out like i said - for example would make sense and so on and you keep going around, but the problem is this length is really short you try and make this long enough to contain an entire sentence just an ordinary length sentence and this problem starts to become really really hard and networks have a hard time learning it and you don't get very good performance and even then you're still like have this absolute hard limit on how long a thing you you have to just pick a number that's like how far back am i looking a better thing to do you say recurring neural network? where you you give the thing. let's like divide that up so in this case, then you have a network you give it this vector? you just like have a bunch of numbers which is gonna be like the memory for that network is the idea like the problem is it's forgotten in the beginning of the sentence by the time it gets to the end so we've got to give it some way of remembering and rather than feeding it the entire sentence every time you give it this vector and you give it to just one word at a time of your inputs and this vector, which you initialize i guess with zeros. i want to be clear this is not something that i've studied in a huge amount of detail i'm just like giving the overall like structure of the thing. but the point is you give it this vector and the word and it outputs its guess for the next word and also a modified version of that vector that you then for the next thing you give it where did it spit out or the sequence that it spit out and its own modified version of the vector every cycle that goes around. it's modifying this memory once this system is like trained very well if you give it if you give it the first word shawn then part of this vector is going to contain some information that's like this subject of this sentence is the word short and some other part will probably keep track of like we expect to use a male pronoun for this sentence and that kind of thing so you take this and give it to that and these are just two instances of the same network, and then it keeps going every time so it spits out like this is i so then the ai also comes around to here you might then put outside and so on but it's got this continuous thread of of memory effectively going through because it keeps passing the thing through in principle if it figures out something important at the beginning of you know the complete works of shakespeare that it's generating. there's nothing strictly speaking stopping that from persisting from being passed through from from iteration to iteration to iteration every time in practice, it doesn't work that way because in practice the whole thing is being messed with by the network on every step and so in in the training process it's going to learn that it performs best when it leaves most of it alone and it doesn't just randomly change the whole thing but by the time you're on the fiftieth word of your sentence whatever the network decided to do on the first word of the sentence is a photocopy of a photocopy of a photocopy of a photocopy and so things have a tendency to fade out to nothing. it has to be successfully remembered at every step of this process and if at any point it gets overwritten with something else or just it did its best to remember it but it's actually remembering 99% of it each time point nine nine to the fifty is like actually not that big of a number so these things work pretty well, but they still get the performance like really quickly drops off once the sentences start to get long so this is a recurrent neural network rnl because all of these boxes are really the same box because this is the same network at different time steps. it's really a loop like this you're giving the output of the network back as input every time so this works better and then people have tried all kinds of interesting things things like ls tms. there's all kinds of variants on this general like recurrent network ls tm is the thing. that might use isn't it? right right long short-term memory, which is kind of surreal but yeah, so the idea of that is it's a lot more complicated inside these networks there's actually kind of sub networks that make specific decisions about gating things. so rather than having to have this system learn that it ought to pass most things on it's sort of more in the architecture that passes most things on and then there's a there's a sub there's like part of the learning is deciding what to forget at each step and like deciding what to change and what to put it in what parcel and so on and they perform better they can hang on to the information the relevant information for longer but the other thing that people often build into these kinds of systems is something called attention which is actually a pretty good metaphor where in the same way that you would have? networks that decide which parts of your hidden state to hang on to or which starts to forget or those kinds of decisions like gating and stuff you have a system which is deciding which parts of the input to pay attention to which parts to use in the in the calculation and which parts to ignore and this turns out to be actually very powerful. so there was this paper when was this? 2000 2017. yeah, so this is funny because this came out the same year as the video you have about generating youtube comments. this is in december. i think that video was october ancient history now alright, we're talking two years ago. the idea of this is as its called attention is all you need. they developed this system. whereby it's actually as it's a lot simpler as a as a network you can see on the diagram here if you compare this to the diagram for an ls tm or any of those kind of variants? it's relatively simple and it's just kind of using attention to do everything so when made that video the astm type stuff was like state-of-the-art and that was until a couple of months later i guess when this paper came out the idea of this is that attention is all you need of it like this stuff about having gates for forgetting things and all of that all of that kind of stuff in fact your whole recurrence like architecture you can do away with it and just use attention attention is powerful enough to do everything that you need at its base attention is about actively deciding in the same way that the ls tm is actively deciding what to forget and so on this is deciding which parts of some other part of the data it's going to take into account which parts it's going to look at like it can be very dangerous in ai to use words for things that are words that people already use for the way that humans do things. it makes it very easy transform for more finds and just make, you know get confused because the abstraction doesn't quite work but i think attention is a pretty decent thing because it is it does make sense it sort of draws the relationships between things so you can have attention from the output to the input which is what that would be you can also have attention from the output to other parts of the output so for example when i'm generating in that sentence like shawn came to record a video or whatever by the time i get to generating the word him i don't need to be thinking about the entire sentence i can just focus my attention on where i remember the name was so the attention goes to shawn and then i can make the decision for to use the word him based on that so so rather than having to hang on to a huge amount of memory you can just selectively look at the things that are actually relevant and the system learns where to look where to pay attention to and that's really cool like you can do it there's attention based systems for all kinds of things like not just text you can do like suppose you have your input is like an image and you want to caption it you can actually look at when it was outputting the sequence you can say when you generated the word dog what was your you can get like an attention heat map and it will highlight the dog because that's the part of the image that it was paying attention to when it generated that output it makes your system more interpretable because you can see what it was thinking and sometimes you can catch problems that way as well which is kind of fun like it generates the output that's like a man is lifting a dumbbell or something like that and you look at it and it's not actually correct. it's like its owner trots and i go he's drinking some tea out of a mug, right and what you find is then when you look at your outputs where it says dumbbell you look at the attention and the attention is like mostly looking at the arms. that's usually somebody muscular who's lifting the dumbbell in your photos? it's and so it it's overriding the fact that this kind of looks like a mug because it was looking at the arms so the idea is this system which is called a transformer is a type of neural network which just relies very heavily on attention to produce like state-of-the-art performance and if you train them on a large corpus of natural language they can learn they can learn to do very well, right they give you they can be very powerful language models we had the example of a language model on your phone that's like a very very basic and then trying to do this with neural networks and the problems with remembering and so you have like recurrent systems that keep track of they allow you to pass memory along so that you can remember the beginning of the sentence at least by the end of it and things like lstms there is all these different varieties that people try different things that are better and hanging on to memory so that they can do better it they can have longer term dependencies, which allows you to have more coherent outputs in just generally better performance, and then the transformer is is a variant on that? well is a different way of doing things where you really focus on attention. and so these are actually not recurrent which is an important distinction to make we don't have this thing of like taking the output and feeding that back as the input and so on every time because we have attention. we don't need to keep a big memory that we run through every time when the system wants to know something it can use its attention to look back to that part it's not like memorizing the text as it goes. it's paying attention to different bits of the text as they as it thinks that they're relevant to the bit that it's looking at now and the thing about that is when you have this recurrent thing it's kind of inherently serial most of the calculations for this you can't do them until you have the inputs and the inputs are the output of the previous network. and so you can't do the thing that people like to do now, which is run it on a million computers and get lightning-fast performance because you have to go through them in order right? it's like inherently serial where as transformers are much more parallelizable, which means you get better computational performance out of them as well? which is another selling point so they they work better and they run faster. so they're they're really a step up. so transformers. are this really powerful architecture.  they seem to give really good performance on this kind of sort of language modeling type tasks and we but what we didn't know really was how far you can push them or how how good they can get what happens if you take this architecture and you give it a bigger data set than any of them has ever been given and more? compute to train with, you know, a larger model with more parameters and more data how good can these things get how how good a language model? can you actually make and that's what opening i was doing with gpt 2? so an executable binary the net effect of slotting that t diagram against here slightly downwards is to show you that the c you've written gets converted into binary and the net output from this process it produces out a program that you probably store in a