this week we're back in the world of data engineering with a curious mix of python and rust my guest for this is dan herrera and for a while dan was working in adte which is not everyone's favorite sector i admit and we do end up talking about that but it's undeniably one of those sectors that's going to teach you a lot about the problems of processing data at scale in parallel at high speed and it's those kinds of problems that led down to the topic of this week's episode bite wax an open-source distributed stream processing engine which makes a number of interesting architectural decisions in this space that we're about to get into but it has an interesting backstory of its own so let me briefly tell you how it got started back in 2013 microsoft labs published a paper on niad which was a novel approach to these kinds of data processing problem that got implemented in rust and had its first release in 2017 under the name timely data flow which possibly you've heard of and the thing i find interesting there is rust makes sense to me as a lowlevel low latency language to prove your paper and build out some serious infrastructure but it's not the first language you think of when you talk about data engineering that probably belongs to python and there's the origin story of bite wax how do you take a research back tool written in rust and bridge it into where all the data engineers are in python and what do you end up with when you get there what kind of beast have they built let's find out i'm your host chris jenkins this is developer voices and today's voice is dan herrera [music] joining me today is dan herrera dan how are you i'm very well thanks for having me pleasure i'm i'm wondering a lot of things about you you work with bite wax i'm wondering a lot of things about bite wax but let why don't we start with the origin story how did you get into the data streaming space sure yeah i come from like a pretty traditional data engineering background and that's spanned through several jobs most recently before bix at github but before that in the kind of adtech space and then sort of being the first data engineer and founding the first data engineering teams at other companies before that so i've worked in the data space for quite some time and then i think the streaming data portion of it came out of a lot of the work that i did in the attech space which ended up having like realtime connotations for doing bidding for like header bidding and ads and things like that so yeah it's been a a long and interesting road through it's funny how how much of advertising was a driver for adoption in the early days of it yeah yeah similar problems of like needing real-time interactions for data large data systems were very gerine in that space and so yeah it it was interesting yeah i think i wasn't crazy about working in adtech like anybody would be but the problems are really interesting and the people are always great you know that's always my favorite thing about working on a job i had i had a brief gig at a blockchain company and i feel exactly the same way the problems are really interesting can we please focus on that part yeah my boss used to start every talk that he gave by saying if you use an ad blocker like i do and then he would go on from there so right but let's get into the tech of it because the first interesting thing i noticed about this is you're in a python space you're building out a python library for data streaming but under the hood it's rust mhm and that seems like a curious way to approach it so how did that happen it was kind of a confluence of some things that i think are happening right now in the kind of like data space and and more generally just in the python space i think that python is really experiencing like a new renaissance i think there's so much renewed energy in interest in python and so many applications that you know the obvious one being machine learning and artificial intelligence but i think python has been such a base layer for data for such a long time and yeah it's been kind of interesting because my genesis in the data space was very much rooted in java which a lot of the current tooling and older tooling is all based around java and i was very excited about a couple things number one somebody introduced me to the project called pio3 which i think is pi3 is a library that allows you to make ergonomic rust bindings for python or vice versa so you can call rust from python or you can call python from rust or both and it's really a fantastic project so that was probably one of the things and the second thing that i became aware of because of our ceo xander introduced me to it funny enough was a timely data flow so that's a rust library so kind of the confluence of the availability of those two things i i thought it was like a really great opportunity to sort of visit the space okay i kind of i want to get into timely but just briefly because i've i know i've worked occasionally with calling c from python and so i can imagine that rust from python will work but it's but c from python i wouldn't describe as fun python is really amazing for being an extensible language i think they've worked very hard it's it's kind of amazing to me that you can install a few packages using the same package registry you would for pure python invoke a program and get some fortran code some c++ code potentially some r all in the same stack and i i don't think some people even realize how many different languages are end up being connected to python and so it's always been a very extensible language in that way and you know the most famous examples of these sort of connectivity are like numpy and scipi are libraries that connect to old and and very battle tested and very reliable implementations of things so in other languages okay so because of p 3 you're looking at niad and timely data flow and not scared that it's written in rust no yeah it was i was very excited when i started learning about rust it was like a very interesting language to me personally and the i think in between the amount of care that is gone into both creating timel and p3 is just kind of amazing they're both like just just fantastic libraries to use the community around them is is really great and you know you can you can see that there's some very deep thinking about some of the problems the problem space for p3 is you know very large python is you know doing such amazing things as removing the gill or providing sub interpreters that can interact without you know without holding the gill so there's a lot of really interesting work that's going into python these days and the people that work on pio3 have a very daunting task to be able to make a library that can take advantage of those things and is still like ergonomic to use and they do a great job of it yeah yeah that's a very difficult marriage of being good at the lowlevel stuff and still being good at like developer space developer experience stuff yeah i think the same is true of timely i think my experience learning timely and i would say that i wouldn't represent myself in any way as being an expert at timely data flow i'm a very enthusiastic user and i would say a dedicated student and it's taken me a long time to sort of understand the subtleties of the language but i think when i step back and look at timely i think they've done an amazing job of writing what they call a low-level library it's made all of the world of difference to us to be able to construct what we wanted to construct based on the primitives that they provided and i think after you do library design and api design for a while you learned that that's a pretty impressive thing to be able to do yeah yeah it's hard to get something that's both flexible and usable you either make it so general that it's hard to know where to get started or so usable that it's hard to remain flexible yeah that's exactly right so we're going to get into the guts of timely great of all the of all the different stream processing ideas right you've got data coming in you want to do stuff to it and then send it on its merry way what is it about timely that sets it apart in your opinion i think there's lots of things and i think one of the things that i went back through and i was reading through the timely data flow book which is a great read for anybody who's interested in this it's just the long form documentation that they have linked from their site that's not specifically api documentation but the timely data flow documentation describes data flow programming as trying to remember it exactly but it's organizing a program in components each of which can respond to the presence of new data which i think is like an interesting description and what it really means is that there's some coordination of pieces that happens in timely data flow but in sort of in contrast to imperative programming it's interesting to think about how that differs from like an imperative programming model because you can imagine any process reacting to the presence of new data like a web server receiving a new request but i think what's really interesting about data flow programming is that each of the components are described as being connected to each other but each of those components can receive new data at different points in the data flow the availability of new data is not something that starts at the beginning and ends at the end and a great example of this in a lot of data flow programming models is like a window if you have a window of data that closes after a period of time it's accumulating a bunch of state and a bunch of data and then after a period of time that window can close and that event itself can cause computation to occur somewhere else in the day flow yeah so i often think like we we treat stream processing like it's a new idea but it's very much once you take out the persistence part it's very much like reactive programming it's very much like go routines and co- routines yeah i think it shares a lot of similarities for sure yeah so so what is it that i mean you could have taken a naive approach and said okay python routines i'll just do yield all over the place and i'll try to remember to write it to disk yeah why why jump through rusty hoops to get to niad it's a really good question the thing that timely data flow gives us for for bwax specifically and for users of data flow programming libraries like b wax does for other people as well is the idea of progress tracking throughout the entire data flow is something that is simple on the surface but i think it has profound implications for the way that you can build and especially scale sort of independent components the idea of when you have inputs in a streaming system being able to correlate those with outputs in a streaming system actually becomes like an interesting problem go ahead do you mean for debugging or just for being able to produce correct answers or fit whatever particular idea you have for the type of data flow that you're writing give me an example sure that's i think that's a good idea so in a streaming system you can imagine that you're not going to see all of your input in effect you don't really know if you're ever going to see the end of any particular input and so the question becomes would do you produce answers and how do you judge the sort of correctness of those answers so the example a simple example would be if you're receiving a stream of bank balances for a particular user how do you correlate which outputs you see in there with the inputs that affected that particular thing so being able to assign those things together it's actually a different component of timeline of kind of like jumped subjects here but progress tracking and logical time stamping are the the uh the things that you can use in order to be able to correlate those things together and it's it's one of the key concepts of timely data flow okay maybe if you explain to me how it does that it start to come together yeah sorry it's hard to talk about timely and i i realize how how tremendous a job they've done in sort of presenting this information but you know frank mcer and and the other folks that have done a lot of work in timely do a really fantastic job about talking it so i'll do my best to do the to do so if you if you have a data flow and you're receiving input each one of those inputs can be assigned a kind of logical tim stamp you can say not necessarily like when did this particular thing occur but it gives a kind of sense of like ordering over the span of input additionally one of the things that timely allows you to do is you can say for a given timestamp you can say all right there's a point at which way i'm never going to see any more of this particular time stamp so if you imagine that all of the records that you receive for something all have the same tim stamp you assign them all the same timestamp and you know for lots of demonstration purposes it's easiest to think of them as like auto incrementing integers right each of the inputs gets assigned a logical timestamps and can proceed through the data flow so let's just say all of the first batch of records that i pulled off of kafka get the first tim stamp of one yeah in a data flow program you can say all right i've reached a point where i'm going to say i'm never going to assign anything else that same timestamp i'm going to advance the timestamp to two and the rest of the components in the data flow can react to that information they can know that they can now produce a result saying i know that i'm never going to see anything else with that same logical time stamp okay so i could have to use your example a bunch of transactions coming in onto a user's account and eventually i want to say okay at some point i've got to say for legal purposes closing balance on this day was x so i'm going to draw a line under all all transactions with a timestamp of one and anything that come anything that comes in now must be two or higher and it will not go into today's closing balance yeah that's a great example it's really interesting to be able to use that mechanism it's a very like lightweight mechanism in order to be able to coordinate multiple workers like you can imagine a data flow can be started with multiple processes or multiple workers they can share that information with each other but all of the parts of the data flow can know when they're not going to see any more information with the same timestamp anymore and then they can decide if they produce results at that point in time and those can be correlated with that particular input so it's really interesting they describe it as like the lightest weight way to introduce that sort of idea of ordering of coordination between multiple workers in a data flow system and i think it's a really interesting way to think about it because there are so many things that you can actually do once you have some of these really primitive interesting like very small primitives that's the thing that interests me because it sounds it sounds very simple in the small and i guess that's the beauty of it right but what happens is you start to build that picture up to a larger en larger graph in what way like how does coordination happen how does or what are the implications probably all these questions but like okay so it it seems pretty simple for the case where there's one processing node saying have i seen anything is it time to stop accepting once yeah but then okay so you're in your example we've got a couple of different people couple of different nodes all looking for the end of the age of one and maybe some downstream processes who are still expecting the age of one how how does this scale up into complex graphs yeah in ter in terms of that let's start with in terms of that notification that it's the end of an era going through the graph yeah so that's a great question so what timely is doing for you is giving you the ability to sort of notify any of the components in the data flow that you want uh to take action when these events occur when you advance the epic of input that's a kind of major event and the rest of the data flow can sort of react to that information and so i think what's really interesting about that is if you consider you can have more than one epic in the data flow at the same time just because you emitting records with the timestamp of one you can start emitting records with the time stamp of two or later and have those be processed in the data flow at the same time but it gives the entire data flow an interesting set of order in that you can understand that timestamp one comes before time stamp two and when you process these data you can process them together and there's a sense of which comes before the other but you can have multiple aexs running in a data flow at the same time oh okay so there's something downstream that because graphs are complex and different machines run faster it could get all the information from epic 2 first and say well i'm i'm going to have to hold on to that and wait until i've got a clos message for epic one and then i can just spit the whole lot out at once yeah so you can do some types of processing in an eager fashion you wouldn't potentially admit result at that point in time but there may be computation that you want to do every time you see a new item but you know that you can't produce a result until you've seen the end of that particular epic so how does that look is there i'm assuming there's some kind of protocol between nodes saying okay this is a domain record so i'm going to just pass on customer made a transaction right meanwhile there are messages that say hey guys it's the end of era one is that what the protocol looks like it's a mixture of domain messages and kind of protocol messages yeah i remember we looked at the progress tracking messages so if you have more than one worker in a timely data flow cluster they're exchanging this information with each other and i think part of the the brilliance of the library is that it's both a runtime and a library and the runtime has had lots of optimizations you can imagine that coordinating this among very large nodes can get expensive exchanging all of the data for progress tracking with each other and so part of the parts of timel that i haven't had direct experience with but i understand are doing a lot of optimization in terms of like how that exchange of data happens between nodes okay i think one other thing that's interesting to talk about just briefly about the way that timely is different from other data flow systems is that when you have more than one worker in separ system let's just say like a spark you can have parts of the data flow graph that are happening on different machines like for example like we can go back even further and think about like map and reduce happening on completely different machines altogether what's interesting about timely is that each worker has the complete data flow graph and all of the operators that are included so it has input and it has output potentially depending on what you want to do and so each one of those workers does more than just progress tracking exchange in order to produce correct results you might also want to make sure that all of the values for a given key end up on the same worker so they can not only exchange progress tracking information but they can also exchange data with each other okay but you saying like they've all got a copy of the data processing graph so i as a node can say oh it's key abc i best send that to machine 123 that's right yeah you can tell operators to participate in different i forget what they call them they have a name for them where you can say this is a pipelined operator meaning as soon as you're finished with this i want you to give it to the next operation and you can exchange it you can have an exchange operation where you say at this point in the data flow graph i want to exchange the keys to the workers where they belong and that's another like like a powerful tool that's in the toolkit that you'll sort of like find later when you need it like you realize i actually want to broadcast this information to all workers or some of the other things you might want to do right before we before we get further into the usage of this then are there any other main tools in that tool kit yeah i think i'm trying to think if there there are patterns i think the last thing that i probably didn't talk about was being able to attach probes to different parts of the data flow probes can tell you when things have reached different parts of the data flow so you can say i want to be able to know if information is made it all the way to this part of the data flow graph and then potentially take other actions that happen in there so as an example we use this in bite wax in order to do garbage collection after a certain period of time we want to clean up some of the records that we're keeping internally snapshots of older state and so you can use a probe to say i just want to make sure that the data flow and data in the data flow has reach this particular stage before i take actions other in other places so you can attach probes to various parts of your data flow and use that information to take action elsewhere is that something you do before the graph starts running or can you do it at processing time dynamic no something that you have to do data flow construction happens before runtime and that's i think a lot of that has to do with you're describing a data flow as operations that you want to have occur and then the connections between them so for example a classic would be like joins or branches in a you say if the predicate that i give for this particular operator is true i want the results of that particular operation to go to a different operator so you describe them all as like connections in between each other and then probes are an important part of that data flow construction as well so you would describe them when you're describing your overall data flow okay so to make that concrete i might be saying i've got three different kinds of message coming through i want to fan them out to three different kinds of processing node and once they've all been processed i want to probe to make sure they've all been processed before i throw away some kind of intermediate state in my branching algorithm yeah i think that's a good example okay okay we'll work with that so yeah the when you mention map produce i was reminded of something i read years ago about google optimizing a map produce job at the kind of scale that google do it and they found that even though they thought all their machines in the cluster were just as performant and basically identical they would always have like 3% of their nodes which inexplicably took too long to process a particular subtask yeah i'm wondering if you've got the monitoring and kill it off and start again from the start of that epoch capability it's something that we added to bite wax specifically so like we were talking about timely is an amazing low-level library and kind of the interesting thing that we did with bite wax is we had a similar goal we wanted to pro provide a generalized framework for construction of data flow crafts in python and so it's interesting for us because we actually don't use progress tracking in the same way that you would if you were writing a data flow using timely data flow so okay when i give you the example of using progress tracking in terms of assigning logical timestamps and correlating them to output we take a little bit of a different approach and we use progress tracking for internal pieces of the data flow that we manage for you and the biggest one that we spent a lot of time working on was recovery so being able to crash the data flow start again and resume from the same same point and right there's some large pieces of that that we had to take some time to create but the two major ones are being able to preserve state inside of a data flow and then being able to resume at a consistent state and so the sort of broad stroke of all of those features together are that we can guarantee in bite wax that we're going to process data at least once as long as your input supports it we can tell you that we'll restore the internal state of the data flow to where it was when we crashed or to a consistent point and then we can start consuming input data from some point in the past and then we can get back to a point at which we've processed all the messages at least once is this logically you're saying okay we closed out epoch 3 successfully so let's record the state of the graph epoch 3 and then we can always just restart from there yeah that's a very good summary actually it's it's surprising have you did you work on this yourself we i tend to summarize a lot of things in my role as a podcast h it's really good yeah so we start epics in white wax just based on wall clock time and so we say we're starting an epic now and at the end of each of those epics we can use that coordination point across multiple workers to record the state internally of all the stateful operations that we have going and the state of the inputs so simplest example would be like which offsets have we consumed in kafka to that point in time if you take the snapshot of that whole thing and serialize it and if you crash you may have process some data after that point and you may have seen some new records from kofka but what you really want is to restore the entire state of the data flow at that point in time to a consistent state meaning in some stateful operations you wouldn't want to apply the same message twice because then you'll get incorrect answers and so being able to serialize the state of things in a coordinated way across multiple workers and then replay data that you had seen previously in kafka but hadn't reached the next end point where you took a snapshot is how we do coordination but that's a little different than like the initial when you read about timely data flow and you start using timely data flow for the first time coordinating inputs and outputs using epic is a really useful thing and for a very specific purpose it can be like very beneficial to decide that for your particular problem domain that's how you want to coordinate things but for us when we were building a generalized toolkit for people to do data flow construction some it's hard to be able to make those that level of assumptions about people's inputs and outputs and we struggled with that a little bit at the very beginning we tried very carefully to sort of mimic the timely data flow api for our purposes right and then at some point in time we were like my colleague david was like i think that we should actually not do that i think that we should model this in a different way and he came up with the idea that epics could just be wall clock time and we can use that as a different type of coordination mechanism okay is that the point that it becomes more a low-level library than a you're using timely as a low-level library to what you want to build rather than it's just a wrapper around timely yeah timely is really amazing in that way timely is the underlying substrate under another library that uh folks that materialize and frank mcer and other people have been working on called differential data flow which is not something i'm super familiar with but the way that you understand it is that it is a construction of another system that's built on top of the timely primitives as well so i think that's what makes timely so amazing is that it can provide facilities for lots of different problem domains and it's very interesting in that way okay well we're going to carry on with the idea how you've been using it as your lowle tool but before we move on sure i have to i have to check something you mentioned wallclock time and we're talking about distributed systems yeah isn't that dangerous in the particular case where you have multiple workers that are using wall clock time you could imagine that you would want some kind of coordination between those two things but i think what matters is that the event where you say i'm not going to produce any more input at this time anymore is kind of the more key event that doesn't necessarily have to be coordinated for all of the workers to say when we reach the end of this epic we're going to take certain actions and snapshot all the data flow that doesn't necessarily have to be tightly coordinated with each other so it's a great question i i'm not 100% sure i'm understanding that you're saying that you choose an event which make marks the the end of this of of the bite wax epoch and then you look at that event tim stamp and you just say well hey that happen to be the time stamp of when we what is that roughly the model oh sorry no we're just using the system clock time but what happens if you've got two machines with differing system clocks well you're only really using it as like a marker of wall clock time not the specific time itself but when to advance to the next epic right you're using 10 wall clock system seconds in order to say all right this is the point where i'm not going to be i'm not going to be reading from or i'm not going to be emitting records from let's say kafka at that same tim stamp anymore and you move on to the next one so for that particular worker it's a coordinated point in time across that workers's operations you could say this is happening on a worker level not on a graph level correct yes right now i'm with you okay yeah it's a good question i thought we were accidentally mis coordinating multiple workers oh no good i think it's it's a good insight yeah something to be very careful about okay so so what else did you choose is is there anything else where you said okay timely is a good low-level library but we want to expose something different to user space let me think timely has some facilities that we are not using and that i think would be interesting to people that want to use it directly as a library so one of the things we haven't done timely supports the idea which is i think pretty unique in data flow programming of the concept of doing iteration so being able to if you have this kind of like epical progression model in there what happens if you want to be able to express a computation that needs cycles so like in your data flow programming model you have a sort of directed graph of operations what if you wanted to introduce your kind of like for loop inside of that computation and so timely gives you the ability to do that using compound timestamps which is not something that we actually use in biox but is really interesting for lots of different applications and the examples that i think that they give most commonly are graph computation oh like if i'm social network trying to do yeah okay so find me all the ancestors of this and then the sub ancestors and that's going to probably end up looping around on its to read its own output yeah yeah yeah okay so is that you've you're not using that because you don't see any need for it it's too complicated to expose to the user so far i think we just haven't we've been pursuing a lot of the things that i think were an operational data flow are so when we first started the primitives that timely gives you mean that you're responsible for building some of the systems on top of that so we we talked earlier about being able to do stateful operations where you're maintaining some state as you're doing an operation the ability to do recovery the ability to do branching and joining was something that we had to kind of come up with an api for and then the concept of windowing was something that we had to spend quite a bit of time getting getting right so if you're a user of other data flow systems like flink there's a feature set that i think you're primarily interested in using and we were tackling a lot of those first so maybe at some point in time we'll have the ability and the time to go back and work on iteration but it hasn't been something that we've had people ask us for specifically it's just kind of cool that it's a it's a capability that's there that we could potentially take advantage of in the future so one day you may find the killer use case for it but you've got plenty of work right now yeah i'm sure somebody has a a great use case for it but yeah we're working on a lot of really interesting things just sort of of like the these are the sort of table stakes if you want to provide somebody with a very useful data flow programming environment yeah yeah yeah okay so since you've mentioned flink and it's always difficult when you're talking about like competing ideas you don't want to tread on people's toes but since you bring it up that one of the things about flink for a python user is their python library is wrapping java and your python library is wrapping rust do you think and then there are python libraries which don't wrap anything are in native do you think what what's a users experience of you wrapping rust going to feel like no we have worked really hard to expose a lot of the primitives that we have as a python api and have implemented a lot of the functionality that we have in python directly so we started leaning we started the project originally leaning pretty heavily on all of the rust code and writing a lot of the stuff that we were doing in rust and just essentially calling python at certain points in the data flow when where you your operator calls the logic for this particular operator and that returns a result and then rust carries on doing the major parts but i think my colleague david and the discussions that we've had we've learned that moving a lot of that api into python is really helpful for people that want to construct their own operators and so to answer your question we would like the experience of using this to be fine for anybody who never wants to learn any rust in the rust layer are some really important pieces that tom also provides the communication fabric between multiple workers so it's both like a library and a runtime for being able to orchestrate data flows some of those pieces will stay in r but a lot of the pieces that we're working on will end up having on python 8 and so for most people's experience it should be the same as using any python package it's something that you can use by doing pip install you write your data flow in python you run it as though it was just a regular python program and so most of the r stuff should be pretty invisible what's the okay so again i want to be careful about mentioning flink but the point at which you will find out that flink p piie flink isn't quite python is when you get an exception back and it's a java stack trace yeah what's the point at which i'm going to find out that bite wax isn't just python as a user you can definitely see some rust in some back traces if you see the data flow crash although we did take a pass and we worked really hard at reconstructing one of my colleagues f has worked really hard on error handling and that look as pythonic as possible so reconstructing the the trace back of those and and seeing something that looks very pythonic and most helpfully pointing you to where in your code the problem lies was a tricky problem but yeah i think flink is a is a great system i i have a lot of respect for all of the work that's gone into flink and i've met some of the developers and they're they're great and i asked them you know do you have any advice for people that are writing their own data flow programming environment and they they said yeah good luck you know like it's it's a hard problem yeah it is they do it very well but i think that python users deserve something that is very pythonic i think python users prefer not to have to learn everything that's in the java ecosystem in order to be able to do the same type of programming i think i think it's great when people have access to i i need to express my problem using these tools and i want something that feels very native for me to be able to do that and i think that that's something that we do i think i have lots of friends in the python ecosystem who have always thought of themselves as like you know the the the neglected child in the ecosystem because you know everyone was writing java and they're like this is great and it works in java and they had to keep raising their hands and say hey it's not working for me you know i'm using this from python and you know it's been hard for them to get enough attention and so yeah hoping they see it now yeah i'm i'm also hoping someone's going to do something similar for the typescript world mh i could have a lot of fun with that yeah okay so so if i'm writing let's say give give me more of a sense of the boundaries of the system because if i'm writing something that processes my users transactions in a more interesting way i'm writing that in python code what's happening under the hood is that is that being passed to a rust process which has a pointer back into the python code so it can call it yeah the i think the beauty of p3 is that mirroring those interfaces together was pretty straightforward being able to call python code that way but i think the interesting parts were you're describing your data flow in python and what that needs to do is is translate into a series of operators that are happening essentially on the rust side so those are timely operators under the hood but what we did was boil those down to a core set of operators that we could construct the rest of the operators that we wanted in those so there's a very minimal footprint in rust for the shapes under the hood it ends up that my colleague david did a very good job of like reducing those down to just a very small set of core primitives so then the orchestration is essentially you're starting multiple workers inside of a python process and that is sort of animating the machinery of rust to do the communication between workers when you need to exchange data you know stopping and starting a data flow and then at each point in time where you're processing data the the operators under the hood are calling out to the user code that you provide as a part of your data flow you have a a map step that provides a function that you need to run at every time it sees new data a batch of new data ends up in that operator and then we call that and then move that along the data flow when you say we're calling it are we passing it to the the python function in process or we spawning out a separate python node which we give jobs to it's all within the same process so rust is calling into the existing python process with data that it receives so there's a little bit of translation sometimes when you need to serialize data and ship it to other workers and that means you know we have to serialize your python object we have to exchange it to another worker to deserialize it but then when it ends up back in python you wouldn't know that that stu actually happens okay but a lot of time it's just it's just another c pointer to here's my function yeah i think that's one of the advantages of the timely system is that for most use cases for a lot of use cases you don't need to serialize and deserialize in between those they're just passing them within the same process to different operators yeah okay great that that's definitely good for performance let's so you've hinted at it then so we now have to go into distributing across multiple nodes and parallelization so let's start with parallelization because that seems like the easier one to tackle python unless you jump through some hoops python is single threaded right mh so how are you parallelizing within one machine are you leaning on rust for that no not directly timely is also not it it it uses what they describe as cooperative multitasking right so functions need to yield in order to be able for other functions to run and effectively like a worker can be thought of as as basically single-threaded if you want to in timely you can start multiple threads each of which again is a whole copy of the data flow and is runing with inputs and outputs that are potentially independent of the others for us that can be difficult right because as you pointed out in order for that to proceed if we're running all in the same python process and we spawn multiple threads you do have contention where you need to take the gill in order to be able to do things in python so i think that's what's so exciting about the potential of like a no gill python sub interpreters in python and all of the work that we were talking about earlier they they're coming they're coming right yeah it's also really hard the typical solution to this has to do with async you could think of like co routines or other things like that those can be really difficult especially when you're thinking about well if i want to make sure that all of my output has happened and i'm using an asynchronous function to do that how do i tell my data flow system okay i finished doing that because essentially your asynchronous work is happening in the background could potentially fail there's not like a direct point where you can say have you completed doing that particular thing yet so it's a difficult marriage between data flow systems who are very carefully orchestrating the progress of data through a data flow and asynchronous systems and so generally what we do for people that want to interoperate with a synchronous code is use some of python's underlying async to just run this to completion and tell me when you're finished kind of things so okay but yeah it's interesting also timely has not incorporated a lot of those async pieces into the core part of timel for kind of similar reasons okay do you think you will or is it just like probably not a priority i don't know i think i mean is it a design issue or is it a workload issue that means you're not doing that i think that's a good point it it could be both i mean sometimes you have to ask yourself whether or not asynchronous solution is more appropriate or would be it depends on like are you io bound are you cpu bound sometimes it's hard as a generic library maker to make those decisions for people and to opt them into ecosystems where it may or may not be the greatest fit and so so far we've just sort of made it possible for people to marry async code into a very synchronous kind of like workflow but but there was a new release of p 3 recently where they're adding better support for marrying both python's async runtime and rusts async runtime and so there's some very exciting pieces of work that are happening on that side to make those two ecosystems work better together okay so if if it were a lot easier to do you might find ways to get it into the a into bite wax's api that fit more naturally oh potentially i think it was something we just hadn't focused on for a while and yeah i think we'll we'll see i think it depends okay so then the next thing is to go when we going across multiple machines how does that play out in timely and bite wax yeah so timely and bite wax both do the same thing you can start a process on a second worker and they will establish communication with each other essentially you give each worker a list of all of the workers that it should communicate with and you can start multiple workers threads in each one of those workers if you like but you can start them on multiple machines and they'll connect to each other so that communication fabric comes directly from timel itself and do you have to deploy like i've got i've written my code now now do i need to deploy that code to all the workers to get it up and running ah that's a really good thing that we haven't talked about i think one of the advantages is that there's no separate runtime for bite wax and similar for for timely the process that you write the python code that you write has everything that it needs in order to orchestrate those so you don't have to take your data flow code and submit it to a cluster that is going to run that for you or to submit it to a specialized process that's running somewhere else to run data flows the same way that you develop locally you can invoke it with python is the same way that you deploy it remotely okay so i just shipped my python route is not like there's a i install some platform software on my cluster and then i can send it my recipe it's i just shi my python code to all the nodes and run it yeah we do have a platform that we have for bite wax which incorporates a lot of the patterns of like managing recovery stores adding features like a programmatic api to be able to deploy your code that's more of like a layer over kubernetes so essentially a way for you to manage data flows and deploy them it's it's based on a lot of work that anybody has done who has okay i've written my data flow code i'm ready to go to production okay i need to have a way to manage this i need some monitoring i need metrics i need a way to sign into the ui and sort of monitor what's going on and so we have a whole separate product that we built in order to sort of do the management of data flows okay you you've reminded me of another question i wanted to get to which was what if i send it out to my six node cluster and one node goes down does does it get rebalanced the other five nodes do we have to wait for the sixth to come back up do we restart the whole job what goes on well in that particular case you what we would do is crash and then restart from our last recovery checko which is essentially depending on what you're doing that's probably what you want like it's a tricky problem to be able to redistribute okay that one that sixth worker was handling a sixth of all of the key space of all of the users in our fictional bank processing platform so it had state about the transaction data for those people it had state about which new transactions it had seen when it crashes the other five workers would have to take on responsibility for that piece of state so that was something else that we did in bwax if you're not in the scenario where you're crashing and just restarting like you're restarting pods and kubernetes you can we built primitives into bx so that you can rescale so you can stop the cluster restart it with a different number of workers and proceed from that point in time and the workers will take all of those pieces of state and say okay this worker is responsible for the state that that old worker used to have and the rest of them might be primary for other parts of that so we have a system built into recovery that allows you to do rescaling because you know obviously that's a pretty nice thing to be able to do we have a lot more load i want a lot more workers are you implying this is something the user asks of it or is it dynamically rebalancing no this would be something that you would do in like a stop start so let's imagine you had a flood of traffic for thanksgiving and you needed to spin up a bunch more workers to handle the increased load that you're having you could stop the entire data flow and start it again with a different number of workers but yeah unfortunately if one of those workers crashes you'll have to restart from a known good point because we don't have a way to dynamically redistribute all of that state into other workers it's a pretty complicated problem and it's it was the subject of some research that happened in zurich and so there's some interesting part of the academic nature of where timel came from means that there's some really interesting papers that were published and written about adding some of these systems to timely itself and so i think if you're a serious student about timely you end up reading everything that frank has ever written you read all of the papers that came out of eth zur and all kinds of other things you just sort of devour everything that you can find sounds like an interesting place to be reading but perhaps perhaps we'll go more into the practical application as much as i like academia sure so we've talked a lot about bite wax the library do you want to tell me a bit about the service you've built around it yeah i think we were talking earlier about the confluence of things that sort of existed when we created bwax originally and one of the things we were thinking about when we created bite wax was for operationalizing or deploying data flows if you could do it again what would you do differently and i think the thing that we wanted to do differently was not require a separate orchestration layer essentially in order to deploy you know spark or some of these other systems you need another system in order to be able to deploy those on and i think removing that dependency was something that i wanted to do so we built a platform just using kind of like uh kubernetes so essentially using kubernetes as the sort of ubiquitous back plane for doing deployments so we built a platform that integrates with kubernetes that allows you to deploy and manage data flows it's essentially what i would have had to have build when i was productionizing anything if i was working at a company you know okay i've got my data flow it runs great on my local workstation i need a way to deploy this and manage it and monitor it when it's running in production and so we build a platform to sort of like encapsulate all of those patterns together so a way to see which data flows are deployed a way to monitor them and a way to manage them that just leverages the the sort of back plane of kubernetes under the hood okay so if i i'm thinking about getting giving a kicking the tires on this i can i know i can install bite wax locally with pip install bite wax and then at some point when i'm ready to productionize it if that's a real word which i don't think it is i probably used it as well that's when i would that's when i'd let you let you take the headache of cubanus for me yeah i think there are several options depending on for me for places that i've worked in the past it was always just sort of the default choice for places they had a team that would manage kubernetes for you and so deploying there made a lot of sense and with the sort of availability of like those and major cloud providers it was kind of like it's a great place to sort of get started if you're running multiple data flows the other option is a tool that we created called wax control that allows you to just deploy directly onto an aws instance or something running in google cloud it's also just very possible because we don't really need anything else to just start a container that contains a single data flow the fact that you don't need a second process to sort of monitor those things that each of the workers can connect to each other means that you can kind of pick and choose what's right for you but i think there's a lot of advantage into outsourcing some of the pieces that are necessary but aren't really adding value for your customers exactly so building a ui to manage data flows is not necessarily something that's like maybe the greatest use of your time we also created a program api for our platform offering which would allow you to say i wanted to deploy a data flow that does these things based on these particular conditions and so having some of those pieces can be really nice for if you need the sort of programmatic access to being able to manage data flows so those are the pieces that we put in the platform nice i for the record i'm kind of person that always wants programmatic access to things so that sounds exactly you it's great have a button that's awesome can i please have an api yeah cuz in the end everything should be controlled directly from emac or vim that's my philosophy nice yeah nice i love it well i'll go and give that a try then i think i've got i've got always got some interesting data in cfra on my machine so i'll go and see how well it works in practice fantastic dan thanks very much for joining me yeah thanks so much for having me this has been great thank you dan and before we go i think a brief celebration is in order this is developer vo's 50th episode and at some point between this one being published and next week's it will be developer voice's first birthday on the 10th of may so happy birthday to us and i just want to say thank you very much for listening whether this is the first episode you've listened to or you're one of the something like 19,000 subscribers over all the different platforms if you're a regular or a firsttime thank you very much for joining me and most of all thank you to the guests who've joined me every week and let me pick their brains i hope you enjoyed the experience you know from one point of view being a guest on this podcast is the easiest thing in the world you just have a conversation about your favorite technology but i have to admit from another point of view it's like a really stressful job interview where you absolutely have to know your system inside out cu you have no idea what the questions are going to be so to all 50 of you i hope you mostly enjoyed the experience i hugely enjoyed listening and learning from you thank you so until next week in the beginning of the next batch of 50 episodes you will find links to all we've discussed in the show notes if you've enjoyed this episode please leave a like or share it with a friend and make sure you're subscribed for episode 51 and beyond until then i've been your host chris jenkins this has been developer voices with dan herrera thanks for listening