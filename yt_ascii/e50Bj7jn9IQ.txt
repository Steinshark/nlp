this is a video for anyone who already knows what eigenvalues and eigenvectors are,  and who might enjoy a quick way to compute them in the case of 2x2 matrices. if you're unfamiliar with eigenvalues, go ahead and take a look at this video here,  which is actually meant to introduce them. you can skip ahead if all you want to do is see the trick,  but if possible i'd like you to rediscover it for yourself. so for that, let's lay out a little background. as a quick reminder, if the effect of a linear transformation on a  given vector is to scale that vector by some constant,  we call it an eigenvector of the transformation,  and we call the relevant scaling factor the corresponding eigenvalue,  often denoted with the letter lambda. when you write this as an equation, and you rearrange a little bit,  what you see is that if the number lambda is an eigenvalue of a matrix a,  then the matrix a minus lambda times the identity must send some non-zero vector,  namely the corresponding eigenvector, to the zero vector,  which in turn means that the determinant of this modified matrix must be zero. okay, that's all a little bit of a mouthful to say, but again,  i'm assuming that all of this is review for any of you watching. so, the usual way to compute eigenvalues, how i used to do it and how i believe  most students are taught to carry it out, is to subtract the unknown value  lambda off the diagonals, and then solve for the determinant is equal to zero. doing this always involves a few extra steps to expand out and simplify to get a  clean quadratic polynomial, what's known as the characteristic polynomial of the matrix. the eigenvalues are the roots of this polynomial,  so to find them you have to apply the quadratic formula,  which itself typically requires one or two more steps of simplification. honestly, the process isn't terrible, but at least for two by two matrices,  there is a much more direct way you can get at the answer. and if you want to rediscover this trick, there's only three  relevant facts you need to know, each of which is worth knowing  in its own right and can help you with other problem solving. number one, the trace of a matrix, which is the sum of these two diagonal entries,  is equal to the sum of the eigenvalues. or, another way to phrase it, more useful for our purposes,  is that the mean of the two eigenvalues is the same as the mean of these two  diagonal entries. number two, the determinant of a matrix, our usual ad-bc formula,  is equal to the product of the two eigenvalues. and this should kind of make sense if you understand that eigenvalues describe  how much an operator stretches space in a particular direction,  and that the determinant describes how much an operator scales areas, or volumes,  as a whole. now before getting to the third fact, notice how you can essentially read  these first two values out of the matrix without really writing much down. take this matrix here as an example. straight away, you can know that the mean of the  eigenvalues is the same as the mean of 8 and 6, which is 7. likewise, most linear algebra students are pretty well practiced at  finding the determinant, which in this case works out to be 48 minus 8. so right away, you know that the product of the two eigenvalues is 40. now take a moment to see if you can derive what will be our third relevant fact,  which is how you can quickly recover two numbers when you  know their mean and you know their product. here, let's focus on this example. you know that the two values are evenly spaced around the number 7,  so they look like 7 plus or minus something, let's call that something d for distance. you also know that the product of these two numbers is 40. now to find d, notice that this product expands really nicely,  it works out as a difference of squares. so from there, you can find d. d squared is 7 squared minus 40, or 9, which means that d itself is 3. in other words, the two values for this very specific example work out to be 4 and 10. but our goal is a quick trick, and you wouldn't want to think through this each time,  so let's wrap up what we just did in a general formula. for any mean m and product p, the distance squared  is always going to be m squared minus p. this gives the third key fact, which is that when two numbers  have a mean m and a product p, you can write those two numbers  as m plus or minus the square root of m squared minus p. this is decently fast to re-derive on the fly if you ever forget it,  and it's essentially just a rephrasing of the difference of squares formula. but even still, it's a fact that's worth memorizing so it's at the tip of your fingers. in fact, my friend tim from the channel a capella science wrote  us a nice quick jingle to make it a little bit more memorable. let me show you how this works, say for the matrix 3 1 4 1. you start by bringing to mind the formula, maybe stating it all in your head. but when you write it down, you fill in the appropriate values for m and p as you go. so in this example, the mean of the eigenvalues is the same as the mean of 3 and 1,  which is 2, so the thing you start writing is 2 plus or minus  the square root of 2 squared minus. then the product of the eigenvalues is the determinant,  which in this example is 3 times 1 minus 1 times 4, or negative 1,  so that's the final thing you fill in, which means the eigenvalues are 2 plus  or minus the square root of 5. you might recognize that this is the same matrix i was using at the beginning,  but notice how much more directly we can get at the answer. here, try another one. this time, the mean of the eigenvalues is the same as the mean of 2 and 8, which is 5. so again, you start writing out the formula, but this time writing 5 in place of m. and then the determinant is 2 times 8 minus 7 times 1, or 9. so in this example, the eigenvalues look like 5 plus or minus the square root of 16,  which simplifies even further as 9 and 1. you see what i mean about how you can basically just start  writing down the eigenvalues while you're staring at the matrix? it's typically just the tiniest bit of simplification at the end. honestly, i've found myself using this trick a lot when i'm sketching quick  notes related to linear algebra and want to use small matrices as examples. i've been working on a video about matrix exponents,  where eigenvalues pop up a lot, and i realize it's just very handy  if students can read out the eigenvalues from small examples without  losing the main line of thought by getting bogged down in a different calculation. as another fun example, take a look at this set of three different matrices,  which comes up a lot in quantum mechanics. they're known as the pauli spin matrices. if you know quantum mechanics, you'll know that the eigenvalues  of matrices are highly relevant to the physics that they describe. and if you don't know quantum mechanics, let this just be a little glimpse  of how these computations are actually very relevant to real applications. the mean of the diagonal entries in all three cases is zero. so the mean of the eigenvalues in all of these cases is zero,  which makes our formula look especially simple. what about the products of the eigenvalues, the determinants of these matrices? for the first one, it's 0, minus 1, or negative 1. the second one also looks like 0, minus 1, but it takes  a moment more to see because of the complex numbers. and the final one looks like negative 1, minus 0. so in all cases, the eigenvalues simplify to be plus and minus 1. although in this case, you really don't need a formula to find two values if  you know that they're evenly spaced around 0 and their product is negative 1. if you're curious, in the context of quantum mechanics,  these matrices describe observations you might make about a particle's spin in the x,  y, or z direction. and the fact that their eigenvalues are plus and minus 1 corresponds with the idea  that the values for the spin that you would observe would be either entirely in one  direction or entirely in another, as opposed to something continuously ranging in between. maybe you'd wonder how exactly this works, or why you would use 2x2  matrices that have complex numbers to describe spin in three dimensions. those would be fair questions, just outside the scope of what i want to talk about here. you know, it's funny, i wrote this section because i wanted some case where  you have 2x2 matrices that aren't just toy examples or homework problems,  ones where they actually come up in practice, and quantum mechanics is great for that. the thing is, after i made it, i realized that the whole  example kind of undercuts the point that i'm trying to make. for these specific matrices, when you use the traditional method,  the one with characteristic polynomials, it's essentially just as fast. it might actually be faster. i mean, take a look at the first one. the relevant determinant directly gives you a characteristic polynomial  of lambda squared minus 1, and clearly that has roots of plus and minus 1. same answer when you do the second matrix, lambda squared minus 1. and as for the last matrix, forget about doing any computations,  traditional or otherwise, it's already a diagonal matrix,  so those diagonal entries are the eigenvalues. however, the example is not totally lost to our cause. where you will actually feel the speedup is in the more general case,  where you take a linear combination of these three matrices and then try to compute  the eigenvalues. you might write this as a times the first one,  plus b times the second, plus c times the third. in quantum mechanics, this would describe spin observations  in a general direction of a vector with coordinates a, b, c. more specifically, you should assume that this vector is normalized,  meaning a squared plus b squared plus c squared is equal to 1. when you look at this new matrix, it's immediate  to see that the mean of the eigenvalues is still 0. and you might also enjoy pausing for a brief moment to confirm  that the product of those eigenvalues is still negative 1. and then from there, concluding what the eigenvalues must be. and this time, the characteristic polynomial approach would be by  comparison a lot more cumbersome, definitely harder to do in your head. to be clear, using the mean product formula is not fundamentally  different from finding roots of the characteristic polynomial. i mean, it can't be, they're solving the same problem. one way to think about this actually is that the mean  product formula is a nice way to solve quadratics in general. and some viewers of the channel may recognize this. think about it, when you're trying to find the roots of a quadratic,  given the coefficients, that's another situation where you know the sum of two values,  and you also know their product, but you're trying to recover the original two values. specifically, if the polynomial is normalized, so that this leading coefficient is 1,  then the mean of the roots will be negative 1 half times this linear coefficient,  which is negative 1 times the sum of those roots. with the example on the screen, that makes the mean 5. and the product of the roots is even easier, it's just the constant term,  no adjustments needed. so from there, you would apply the mean product formula, and that gives you the roots. and on the one hand, you could think of this as a lighter  weight version of the traditional quadratic formula. but the real advantage is not just that it's fewer symbols to memorize,  it's that each one of them carries more meaning with it. i mean, the whole point of this eigenvalue trick is that because you can read  out the mean and product directly from looking at the matrix,  you don't need to go through the intermediate step of setting up the characteristic  polynomial. you can jump straight to writing down the roots without ever  explicitly thinking about what the polynomial looks like. but to do that, we need a version of the quadratic  formula where the terms carry some kind of meaning. i realize this is a very specific trick for a very specific audience,  but it's something i wish i knew in college, so if you happen to know  any students who might benefit from this, consider sharing it with them. the hope is that it's not just one more thing that you memorize,  but that the framing reinforces some other nice facts that are worth knowing,  like how the trace and the determinant are related to eigenvalues. if you want to prove those facts, by the way, take a moment to  expand out the characteristic polynomial for a general matrix,  and then think hard about the meaning of each of these coefficients. many thanks to tim for ensuring that this mean product formula  will stay stuck in all of our heads for at least a few months. if you don't know about alcappella science, please do check it out. the molecular shape of you in particular is one of the greatest things on the internet.