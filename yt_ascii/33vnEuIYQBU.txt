hello everyone i'd like to show you how you can get more out of your c plus plus code how you can run your c plus plus code not only on cpu or cpus but also on gpus fpgas and other devices how you can run your c plus plus code not only on intel or amd or nvidia but on all of them or many other vendors and what are the tools that can free up your code to do all of this but before we even begin just for a quick introduction my name is guy i'm a technology evangelist for intel and you can look here for my youtube channel just look for one api it's youtube or feel free to connect this is my linkedin profile and you know feel free to reach out and ask anything so the future is open parallel and heterogeneous and if you are already a c plus plus programmer it's easy to get there and i'll show you exactly how but before we talk about the future let's start with the past let's remember how did we get here so the lady you see here i don't know you might have seen her her name is margaret hamilton and she was the leader of the team who coded the apollo guidance computer she's standing next to her the printout of the code you see that it's not readable very difficult to code to debug and understand but we have landed aircrafts using assembly but when we move to higher level abstraction like opencl or cuda for example you know we saved a lot of effort because you know one line of cuda or opencl can replace 100 sometimes more lines of assembly so why not and if you move to c or c plus plus now we can actually read it there are loops there are conditions you can actually see the algorithms that we're trying to implement and moving even higher to python this is almost natural language you know right you can see here for each row for each column do this in this but you know it doesn't stop there i mean coding is not the target products are the targets so people want to go even to a higher level of obstruction they want to go to low code and maybe even no code and this is not a dream and this is happening today you can design very nice web pages a full e-commerce online store and even sophisticated games without writing even one line of code just pick up the character or purchase it on the market pick up the scene define the character moves and so on and you got a game not even one line of code so why not and the reason we're going to higher obstruction level is of course productivity if one line of python code can replace 100 c plus plus code then i can do everything faster or alternatively i can do more in the same time and in addition it's easier and take less time to get to a good programming level in python and it's not only that i can do more at the same time it's easier for me to get more developers as there are more python developers than cuda or opencl experts these experts that we sometimes call ninjas are hard to find and expensive to hire and many times there have a lot of experience but on a very specific platform so they can code cuda really well but not anything else right so when you're moving up the stack to higher abstraction level you're moving from developers from coders from ninjas to content expert to researchers to entrepreneurs this is a much larger community they're connected to the science they're connected to the business but they do not know or maybe do not even want to code one of the problems with going up the abstraction level is that sometimes it means going down in performance and this is what we call the ninja gap the performance we're losing by not coding directly in c plus plus for example and the performance gap is just growing there is on one hand a huge pressure on productivity to get products faster but there's also a huge demand for more compute power and for decade moore's low was our guillain light our beacon our performance scaling goals the original law or prediction actually by going on more intel co-founder talked about scaling the number of transistors but i've seen variation of this prediction to predict almost anything now this is all good but looking at ai workloads and the development of the models the demand for more compute is growing at a higher rate and when you talk about the ability to solve more and more of the world's problems using ai ai is just expanding to more and more domains and it's everywhere now when the building blocks are not norms but transformers that demand for compute power is just skyrocketing and we cannot scale performance using generic monolithic pieces of silicon anymore the only way to feed this hunger for compute power is by using specialized hardware and i brought here an example of ai compute demand power but hpc and other domains have their needs and the workloads we are running today are just getting more and more diverse and i can take these workloads and random oil on my cpu and get some performance results as you can see here you know cpus can usually run everything any piece of code but when the workload we're trying to run is very specialized for example rendering 3d graphics our gpu as powerful as it is cannot compete with an architecture that was built specifically to execute such a workload a gpu for example you can see here in the diagram that the gpu can run a smaller set of applications a smaller set of instructions but with much higher performance or we might have an ai accelerator like intel vpu or habana gaudi 2 processor that might look like that it can only run two workloads maybe tensorflow in pytorch but with a much higher efficiency or other examples like fpga that you can theoretically program to the range of application you want to support and this is exactly why we need accelerators we cannot compete with an asic code for specialized workload we need an accelerator and a specialized hardware and the truth is that in order to cover the full range of applications we usually need two we're using the cpu a host and an accelerator what we call heterogeneous system and lucky for us intel and other vendors has all the options all these devices we don't have only cpu gpu vpu we have xpus and i see so many specialized hardware been built up a few weeks ago i just heard about youtube building their own specialized video processing cheap and more and more and it's not only that we have these all so many types of specialized hardware we know how to connect many of them together to build and construct even bigger super computers and specialized computers for example you can see here two sapphire rapids two next generation cpus alongside six ponte vecchio the next generation gpus in one wreck that has been built by intel and you put more than 50 000 gpus and twenty thousand about twenty thousand cpus together to construct aurora to achieve two x scale compute power this is pretty impressive but the truth is that silicon is just sand without the software without software it's useless and if i'm a software developer i don't care if you have their gpu cpu thousands of them what i really want to do is i want to treat the system as a one monolithic entity i want to take my code and just throw it to this machine i don't care what's inside and it's more than that i don't want to just throw my code to the machine i want to throw my code to any machine right if the national labs in the us have from one generation to the other a different cpu from intel amd or in a different gpu from nvidia imd or intel i don't want to recode again and again i want to reuse my code from one generation to the other which makes a lot of sense and you don't have to go far to super computers to understand that think about a day-to-day scenario like you watching netflix and you're watching on your watch it on your widescreen tv and then you want to take it to the living room on your tablet and then you want to continue the tv show on the road on your mobile phone for example right in today's world it sometimes means that the same application the same viewer in this case has to be recoded recompiled re-optimized for any of this operating system and devices and this is not what we're looking for this is not productivity what we are looking for is for software that could be ported from one device to the other that will have good heterogeneous capabilities so i can run the same code on cpu gpu or fpga and other and of course i prefer that it is open standard open source and cross architecture cross vendor and so on and this is exactly what one api provides me and i'll show you a few examples in a few minutes one api is an industry initiative to unify software stacks and to work across devices architectures and vendors there are already many partners involved universities research centers and commercial companies they're all working together to define the specs to build the compilers the libraries and the tools and they all provided with one api there are already tools in the markets and forums that you can take part in to help define the specs and the standards and of course intel has its own implementation of one api and its free open source open spec and works cross devices what it means is that if you're working with one api no matter what types of application your coding and no matter what devices or systems you're using the software stack is unified most of the libraries tools and languages you will find here are not new okay you can see here just few examples these are being available for years on intel cpus but with one api you can expect the same experience on multiple devices so vtune for example cannot run only on cpu but you can run it now on gpu and fpga for example and you can get all of these components standalone each by itself or packed into toolkits for example this is the intel one api base toolkit you can find here everything you need to get started in one package and i wanted to show you one component of each of these categories will go one by one and i'll try to show you the one dpa library how you can use stl c plus plus stl libraries to code for heterogeneous system not only for cpu but on gpu fpga and other i'll show you the data file c plus compiler basically sickle and the compatibility tool or the open source versions the cyclomatic that can help you migrate cuda code to c plus plus with sql and the intel advisor very powerful set of capabilities to help you get more out of your code and i'll show you everything working and with examples so the first one is one dpl one dpl or one api data parallel c plus plus library is a library that basically brings heterogeneous or multi-accelerator capabilities to c plus plus stl library so the easiest way for me to show it is by example so you see here a vector of data and you know i initialize a vector using random numbers and i sort it begin to end and this is simple c plus plus 17 nothing new about that and now i can add parallelism by using parallel stl execution policy so i define here an execution policy it could be sequential but it will could also be parallel execution so parallel execution is here and this is parallel stl but now i want to take it to a heterogeneous platform so all i have to do is now read the right libraries and change the execution policy to one api execution default policy in this case and in this case the sort will run which is the heaviest function here we will be offloaded and be run on gpu no coding required change the policy and it's there let me show you how it looks like on my laptop so vector i'm just taking a large vector and i'm generating random numbers to initialize the vector i use a sort function to sort it from begin to end and let's build it and this is running on my laptop with no problem now let's add execution policy and i can for example run sequential policy and apply it to these both functions and when i run the code i can see that it's running and one thread was actually executed now let's change the policy to parallel execution policy and we run it and now you can see that this is running build and running on multiple threads on my laptop again but this is still everything is running on my cpu and now i want to move or offload stuff to the gpu so all i have to do is to take the execution and algorithm libraries from the one api ones and now change the execution policy to data parallel c plus plus default policy which will you offload basically whatever the policy applies to to my gpu here the generator cannot be applied because random numbers are not supported but let's run the code and now you can see my gpu utilization and you can see that it's it's been highly utilized so all these code the the majority the main computation is now being offloaded to my gpu and you know my alternative to do that i didn't do any code change just change the policy took the right libraries my alternative was to learn ncl or cuda and install all kind of stuff on my laptop and you know it's not needed so i think it's pretty elegant way to take existing code with minimal effort and offload it to an accelerator i think it's a pretty nice value to developer all you have to do is install the base toolkit you can even install just the one dpl library and no need for code changes as i said and i think this is pretty cool getting started with 1dpl is very very easy just google just search for one dpl and go for example for the github page of one dpl and you can see here examples and everything you need to get started or navigate to moneypi.io go to spec and you can see with the spec of all the libraries and of course of one dpl as an example and everything you need the api the extension everything is here for you to get started all the documentation i was there next is direct programming using sickle so i look at sql as a heterogeneous parallel extensions to c plus plus data part of c plus plus is basically intel implementation of securities the intel compiler that can now process also directives this is how i refer to from sicko and sickle brings to c plus plus exactly what it's missing for parallel heterogeneous computing the concept of an accelerator devices the ability for operating these joint memories exception handling from devices code and more it's based on iso c plus plus it is a single source asynchronous event based and there are already many real examples and you know big community around it the sql compiler is basically the google c plus compiler that can compile your c plus plus code as before but now you can also compile the sql code in parallel and generate heterogeneous execution program the device code for the accelerator or gpu for example and the host code for the cpu to handle the cpu code and the interaction between the host to the accelerator and here is sickle in a nutshell i believe we need three types of capabilities if we want to be able to program a heterogeneous system first we need to be aware of the system so device discovery and information is a must we want to be able to know if we have in the system cpu or also a gpu or other devices we would like to have information on these devices it could start by capabilities like you know number of execution units or vendor and it might even be real-time information like what's the utilization of the of this device or even what is the current temperature of the gpu the second is the ability to control the device to dispatch work to this device and to be able to know the status of the execution in real time and the ability is ability to exchange data efficiently with the device as you know in many cases the compute is sometimes not the issue i can offload some work to the gpu for example but transferring the data to the gpu back and forth is just killing the performance so this is a simple piece of code you can add to your c plus plus program you can see here the notion of platforms and you can see which platforms are connected to my system and which devices are available and for example if i run this code on on my laptop i see that i have a cpu and i also have a gpu and other devices and it's been discovered so now i can select the device i'd like to use and assign it to an execution queue here for example i'm assigning queue to to the host okay so the host so by using the host selector basically my cpu so now there's a queue to the cpu and i can also assign a different queue to the gpu and you can see that i can query multiple properties of the device so i could even build custom selector and choose only a gpu that supports fp64 and designed by amd for example and you know the next level is dynamic resource allocation and optimize for throughput or latency or for power or for price whatever you choose the second capability is controlling this device so i have a queue connected to my gpu as you you can see here and i can submit work using the handler to this gpu to this queue so the code here inside these curly brackets will run on my gpu or any other device i will choose so it's a simple c plus plus code that using this sql directive i could run on my cpu or if i choose on my gpu or any other device and now we need to think about the data data has to be shared across devices efficiently so we have two main ways to do it with sql the first is by using buffers i assign here a buffer called a and here inside the device code in the kernel i need an accessor to access that buffer but this array this a or accessor a is visible both to the host to the cpu and to the accelerator the gpu in this case i find the usage of buffers to be very elegant but you can also use unified shared memory if you prefer this style of coding so instead of allocating a buffer i'm using here the good old malloc and in this case malloc shared so this memory this range of memory will be shared between the host and the device and there are of course multiple ways to do that but the nice thing is that the data or array is visible to all devices cpu gpu and others so it's pretty readable and in line with your current c plus plus code style so as you can see i don't think there's a huge investment to move from c plus plus to c plus plus with sql the additions are not very big and they return the return you're getting is huge because your code can now run on any cpu on many types of gpus as you can see here you just need to choose the right compiler but you can see here gpus by nvidia amd intel and others fpgas by intel and xylic and you know more backends are being built so once your code is sickle it is free to run on multiple platforms and i think this is a pretty powerful capability the problem might be if you don't have c plus code but you have cuda code and we have some new tools that could automatically help you can convert cuda code to surplus plus which circle intel migration tool can do 90 95 percent of the work automatically cyclomatic is an open source project that's doing basically the same there are many examples and test cases online just just look for them so if you want to run your c plus code on cpu gpu or pga and other devices but what you have today is a cuda source code then you can automatically migrate your cuda code using cyclomatic then you automatically generated sql code will have comments and instructions if manual code changes are required and i'll show you a few examples and then all you have to do is to build the code with your preferred compiler to your targets devices target device or vendor it has many details and i've seen quite a few success stories people who've done the conversion with really minor effort and you know there are differences couldn't see c plus plus we sql have different ways to report and handle errors thread ids included looks like local identifier and sql so there are different way ways to to code and to allocate memory or buffers and and so on but i i found it pretty easy to understand and you can you know easily improve it if you need because it's open source i'll just show you one example of a code i try to convert from cuda as you can see here on the left side to sql to c plus plus on the right and you can see here how of course the libraries are different and been brought automatically the buffer allocation looks different you know the actual kernel is being built here you know and there you and you get in all kind of comments for example the error code is different which makes total sense and you know try it just google cyclomatic and take your clear code to to c plus plus and open it up to so many opportunities so if your code is not cuda but rather c plus plus code intel advisor just one of the tools applied with one api is a great tool it can help you analyze the code understand the dependencies graphically it can help you with better threading better vectorization and more for example you can see here the how you can analyze your code see the dependency flow graph and a great way to explore dependencies and understand the code flow there are capabilities to help you identify best candidates for parallelism prototype threading and check if there are data dependencies preventing parallelism that the compiler cannot exploit modern compilers knows how to vectorize but sometimes they take the safer side and sometimes with small rewrite of the code they can do much more intel advisor can help you improve your application performance with actual recommendation of how to fix vectorization issue the roofline analysis is pretty cool and i'd like to show you how it works in two minutes so your code has many functions loops each has a different arithmetic intensity a function that adds b and c into a is less complex and takes less operations then calculating the cosine of b and multiplying by b and c or something this is measured basically by the number of operations per byte of data with the x axis now each of these functions has a compute intensity but how fast can they run and this could be measured by flops per second which is the y-axis now the arithmetic intensity is theoretically at least fixed so cannot move right or left but can run faster or slower depending on the software implementation or the hardware it uses so to move up and down the graph our hardware has its limitation for example the red line here is the maximum flops per second this cpu can execute but sometimes the problem is not the compute right sometimes our memory bandwidths can also limit us as you can see in the red line and when you combine these two lines they form what we call the roof line which represents the maximum performance our machine can supply theoretically each of these functions could be improved all the way up but this function for example is hardly used and improving it won't help the overall program performance match so the advisor just color it green and this function is already optimized so optimizing it further is a big effort and doesn't worth it so it's colored yellow and this function is colored red as advisor advises us to invest to invest our time here i think it's pretty cool the real the truth is that representation of the hardware is a little bit more complex as you can see here different roof lines for vector operations and for scalar ones and the memory story is also a little bit more complex with hierarchies that imposes different limitations for example the level one cache is faster than the level two but the level two is has a bigger capacity this is how a real analysis look like where you see the roofs representing the maximum capacity of the machine each of the circles here is a function of inside the code and the you know advisor located in on the graph and color it with potential performance and improvements so check out the roofline lenses i think it's pretty cool and the last capability i wanted to show is the offload advisor imagine you have a heavy c plus plus application with many lines of code loops and so on and you would like to improve the performance by offloading the right portions of the code to an accelerator but which portions of the code to offload and by how much will the code be improved and offloading code to an accelerator cost you know the memory traffic back and forth can we take this into account the answer is yes this is exactly what offload advisor can do for you and this is what you see here is the main dashboard and you can see here for example my code has three regions or three loops that could be offloaded to an accelerator and you can see the three of them right here these three loops include 75 percent of the overall code so i can improve these three loops this code by 50 by five zero percent which will improve the overall application performance by ten percent and you have here all kind of information for example you know the memory bandwidths what's limiting the the performance and so on so to give you a quick example i'll open the intel advisor and i'll choose the offline modeling and i can choose many different intel gpus as my target platform you know i'll just choose gen9 gt2 my baseline is i7 cpu and i have the results and i can speed up only one percent of the code this one percent can be split up by 50 but it's you know the result of the overall code will be point seven percent not that much and i can see the actual loops that you know i'm advised to to offload this one could be is bounded by level three bandwidth and i can go to the accelerated regions and actually look inside the code that i was recommended to to offload to a gpu let's get inside the code itself and it's loading and you can see here that this is really an actual kernel which makes sense and can be improved by 40x this specific loop if i just take it from the cpu to the gpu i think it's pretty cool you know it just pinpoint you to the right place it's in the code after you know thorough houses and even give you the hints on the estimated performance and i think this is a very powerful tool and pretty cool you can search for intel advisor on the web and there are multiple ways to actually get it you can download the full base toolkit or download it standalone or you can even try it you know without installation at all remotely on the intel devcloud just look for intel devcloud you know it takes two clicks to to register and you have access to multiple machines you know they all installed they're all ready to operate so this is all for internal advisor and to summarize what i've said i think like i said the future is open it's parallel and heterogeneous it will have multiple accelerators and the fastest way to open your code to multiple devices by multiple vendors it's probably to add sql to your c plus code and i've showed you how to use library functions and i've showed you how to do direct programming and how to take cuda called code and convert it to c plus plus and how to use intel one api tools to analyze the code to get advices on vectorization on offloading and so on i think it's pretty powerful tool and more information you can find at sickle.tech or at oneapi.io and of course feel free to navigate to my youtube channel lots of information over there and you can leave me a feedback i'll be happy to to hear your questions thank you very much for your time and good luck