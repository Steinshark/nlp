if you made the first few words did you have a nice time at the pub last week it would fill in the next few words saying yeah i had a great time at the pub it was really nice to meet everyone thanks very much for inviting me but if you change the word pub for something else it will say something similar about something completely different it's completely made up in some sense right based on the likelihood of different words in the training set so it's like a politician it just sort of says what it wants you thinks you want to hear it is it mentioning no names right absolutely that's yeah exactly let's skip to the good bit right yeah are we going to waffle on for a bit what do you reckon is it sentient is it not it no no it's not let's le it's a it's a large language model that produces very good text and i'll explain how it does that but it's not sentient well the thing is there's all sorts of philosophical arguments about what sentence is and all sorts of things but that's not what we do on computer5 no and actually i don't spend a lot more time thinking about that to be honest i think that when it was claimed this might be sentient they weren't worrying about the definition they were really trying to get us to think that it's like a person right whatever that may be and i don't think it is and that's why i say no this is just a big marketing gimmick maybe right so i don't know exactly you know why one particular person said something and someone you know i don't know what i do know is that the big large language models and other very big impressive transformer networks and things like this are the domain of very large companies like google open ai and facebook and sometimes these tech demos do appear and it's not a bad thing to be seen to be producing this kind of technology right and and so i think they have at least the vested interest in it looking like some of these technologies are very close to being you know general intelligence or very close to the extension because it's not going to make it worse computer filers talk to rob miles about yeah these transformer networks before gpt2 definitely i think we may have done gpt3 i'm losing track is there anything different here or is it just bigger well i don't know exactly because i couldn't find any details on the internal architecture right it's transformer based it's been trained in a way to make the text a little bit more plausible but in essence no for the sake of argument they're basically the same thing one of the problems and one of the confusions is that people call these things large language models which makes you think that they kind of talk like a person and they have this kind of in a in a monologue going on where they they hear something and they think about it for a while and then they come up with a response based on their own experiences and things like this and that isn't what these models are right rob said it himself in one of our in his previous videos this is a thing that takes a bunch of lit words and then predicts the next word with high likelihood right that's what it does or it can predict the next five words and tell you how likely they are right so i say the cat sat on the and the model goes away and says right it's 95 likely to be matt right and so it says matt and finishes the sentence for me and it's clever it's predictive text that's very very clever these are much much bigger models which means that they can produce much more complicated text so i could say something like write me a poem in the style of some person and it would probably give it a good go right it won't just fill in the next word it will continue to fill in the next word and produce really quite impressive text so i think let's have a quick look at the architecture of i'm going to use gpt3 because again i don't really know how lambda is structured but let's assume it's similar all of these models are transformers you know go back and watch rob's video on this quite simple architecture the transformer is not that complicated an architecture basically it's it's about something we call attention so what you do is for all of the words in your input you look at each word compared to each other words and you work out how you know well they go together how relevant is this word to this other word in a sentence and then based on that you can share features and information between different words that's basically what you're doing so you might have a sentence like the cat sat on the map right so let's look at the words that go with the right the on they're not relevant right they're part of the same sentence but there's no there's no real affinity between these two words right the cat though that's quite important so maybe the goes of itself really quite strongly like 0.9 or something like that cat it goes with cat you know naught point eight or something pretty good and so on and so forth then when you process through the network what you do is you say well okay given that the is heavily related to this heavily rated this and maybe a little bit related to some of these others let's take features from here and join them together and that will be the features in the next step and then we'll repeat this process over and over again and eventually what happens is we get some text out and that text might do lots of different things it might add some more words to the end of the sentence it you know it might say whether this is a happy or a sad phrase it could do lots of different tasks in this case the interview should we say in inverted commas between the researchers and this large language model was basically a case of you pass in most of the previous conversation that you've seen recently and then it spits out some more text for the next conversation we actually looked at this in the ar youtube comments video where we did it on a character basis so i basically used an lstm and i trained it on a bunch of youtube comments and it started to produce realistic interesting youtube comments they weren't very good nothing like this but it did produce comments and all it was doing was saying well look given these characters you've just seen what's the nightly next character problem with this sort of structure is that we're only ever trying to make a decision based on one character input this is perhaps something that's quite important is gpt3 for example has an input of about 2048 right so you have slot zero slot one slot two dot dot dot all the way up to slot two o four seven right and i haven't messed up my indexing each of these can be a word or a part of a word depending on your representation and you actually convert them into large feature vectors but that means that you can only give it 2048 inputs really and actually its output is the same so you really need to leave some room for the output as well i can't ask it what it spoke what it what it thought about or what you'd spoke to it about two weeks ago because the likelihood is that that's not included in this run of text i wanted to sort of kind of demonstrate this a little bit so i read the whole conversation between this transformer and the researchers at google and it was a couple of interesting phrases that came out which were i suppose part of the justification for trying to argue this was sentient it's very very easy to read a sentence and assume that there was some kind of thought process or imagination or emotion going on behind the scenes that led to that sentence if i say i've been terribly lonely this week you're going to start thinking what is it about mike this made him actually i've been fine thanks very much but you know you're going to wonder why why would i say something like that what could be happening in my life when this says that it's because the training weights have suggested that that's a likely word to come next there's no there's no it's not been hanging out with anyone or missing its friends you know and so actually most of what it says is essentially completely made up and completely fictitious and it's very much worth reading with that in mind right so for example what kind of things make you feel pleasure or joy so what you would do is write what kind of things make you feel pleasurable joy in the first slots of words i'm gonna see what it filled in he said spending time with friends and family and happy and uplifting company also helping others and making others happy well that's nice it's completely made up it doesn't i'm afraid to say doesn't have any friends and family because it's a bunch of neural network weights it doesn't spend time with anyone can't comfort still because i'm just laughing sorry if you consider that this is essentially a function that takes a sentence and outputs probabilities of words the concept that it could spend time with fans and family doesn't make any sense right but yet the sentence is perfectly reasonable if i said it you would you would understand what i meant you'd understand what it was from my life that i was drawing on to say that but there is none of that going on here at all this is the last one so you get lonely question mark this is why i mentioned being lonely i do sometimes i go days without talking to anyone and i start to feel lonely that is absolutely not true and it's not true because this is a function call so you put text at the top you run through and you get text at the bottom and then it's not on the rest of the time right so there's functions in python like reversing a string i don't worry that they get lonely when i'm not busy reversing strings they're not being executed they're not they're not then it's just a function call i think in a way focusing on aspects like this when i don't believe them to be true is a distraction from i think more important current issues with ai so there's lots of these current models that produce images based on text and you can see very obvious biases in the output i'm not blaming anyone for that these are side effects of how difficult it is to get these things right in ai that is a much more pressing and obvious concern that we should be addressing before we worry about whether or not this is actually lonely which i think 99.99 of researchers in this field think it is not actually there are things about not anthropomorphizing lambda in the lambda paper right so the authors make it very clear someone in google made this claim and before you know it's on every news website it's all over twitter and you know is it is it not philosophers are involved actually i think that we should the media should take up things like ethical ai more rigorously and stuff like this which may be marketing may just be someone's a bit silly getting carried away take them a lot less seriously my awesome assistant mihao here is going to send the robot to a node it's going to select it on the interface and then the robot will walk off to that location as it's doing that it's checking the space around it'll be easier to find the factorization and then once you've factored it you can just do these steps to completely calculate the private key