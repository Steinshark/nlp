so thank you and welcome everybody so i'm john fowler i have been doing cpr plus for too long started full-time job first time in 94. so it's been going on for some time currently working for netinsightins.com we do networking stuff what i'm going to talk about today is sort of kind of relates a little bit about that i should mention right away that this is a very superficial orientation presentation it's you will not go away from it with the information about a lot of details you will have an overview this goes for both co-routines and for i o urine also i have a little bit of a cough i don't think it's a computer virus so you will probably not catch it from me at least not from this session but you have been warned i may be occasionally noisy all right so networking in limits so what we traditionally use for for doing networking is we have functions for for receiving messages so the second point you do read or receive a rec message or a multi-message to read data from the ips dac into data area and a very simplistic program you can if you if you're just listening to one data stream you can you can write a simple loop where you just read data in into that and do whatever you need to do with it and that works but and if you have a handful of streams not very many you you can get away with spawning a few threads and do this that works but if you have if you want to serve a lot of data hundreds of streams with thousands of messages per second one thread per stream does not work at all the the operating system scheduler simply cannot possibly catch up plus of course you have all the scheduling issues so what you typically do is you use the first point you have functions like select or poll or epo where you register a number of file descriptors that is network sockets that you're interested in what's happening and you'll stand waiting until something happens and then you loop over all the file descriptors where something did happen and now you can partition the load a little bit so that maybe you can serve you spin up as many threads as you have cores on the cpu and each thread serves a few a number of maybe 50 100 strings that works but there is a problem with this so with the the way you you do this typically you implement something like a poller or you use a library that already supports this like boost seo for example but in essence you still have the same thing that you you store the num the file descriptors that you're interested in receiving data from in this case in the add member function so register a file descriptor with some callback that does some kind of work and then we have a wait function that calls the operating system to say wake me up when there is data available and then you loop over all the file descriptors that have data read it and do work with them i should mention here in the in the weight function the magic number 1500 there for the the size of the buffer this this is not completely arbitrary if i don't know if you're familiar with this but in video streaming protocols they are almost always implemented on top of udp and by default the maximum payload size of a udp packet is 1500 bytes so in this kind of code you often see this constant 1500 because that is the max size that you can expect to to read from a udp package so you read this and then you call the the registered callback with the with the data that you read this works but what happens then is you you call paul and the kernel fills in some area with the ip stack fills in some data buffer and then your release from paul so you can read and the read means that you're actually copying data from that internal memory area up into to your memory area and then the kernel can start filling in some something else while you are processing your data and then your code poll again to wait for the next data you will see that it's available so let's see what actually happens when we when we do that so i have a very simplistic program here we have the the polar that you just saw oh there's a question about this 1500 bytes yeah that is the mtu that is exactly correct so i have the polar as i as i showed you and you can have a main program here this is a silly example but i'm reading from two sockets localhost port 4000 and 4001 and i have a something that just calculates the the number of packets and the sum of all the bytes in the packets and another function that just exits and prints these statistics on the exit so we can create our poller register our our callbacks and then we do pull and wait so if we run this by the way for all these examples i'm using g plus 11 and i'm compiling with the cpr plus 20. so i'm using s trace just to show what is happening here s trace is a linux tool that shows system calls so when i started we see a lot of things happening but what's happening here is that we see the call to poll and everything is stopped so what i can do now is this is cool by the way if you're doing linux networking stuff bash that the shell implements as if they were devices in the device file system the implements udp starting points so you can echo through to dev udp localhost 4000 actually creates a udp packet from this string and sends it to port 4000 on localhost we can do that and now we see that we were released from paul ryder foo with the line feed and then we're stuck on on paul again and we can we can call a few more and it's it's the same and then i can call to 4001 and print the the sums and exit so this is simple it works but there are a lot of system calls oops so enter iou ring this is a a fairly new system introduced in linux it's if i remember correctly it came in in the 5.5.1 5.1 kernel and what it allows is for for you to do operations with the ip stack or with the file system things that have file descriptors but when preparing this presentation i actually tried to use iou ring with student and that did really not work well just so you know so audio viewing is a log-free third memory data structure that is it's shared between your user space process and the kernel and it has two two queues the submission queue where you submit request to the kernel to do things and the completion queue where you get information back that something has been done now and iu urine is a little bit interesting in the that it has two apis you can choose which you want to use a low level api which is so low level that i'm not sure i want to call it an api it's more like a description of how this log free thread memory data structure works and how you should manipulate it to make things happen or you can have the high level api which is actually not very high level at all but it's fairly easy to use so the high level api is what i'm using in in the examples here so you include liburi and what you do is you create a during instance this is of course a c api and you can have one ring per thread in the user's base program and you give it a max number of pending entries for the for the ring so 8 in this case would be pretty low for a normal streaming application but never mind and the way to do things then is you to get the kernel to do something with the network you you get a submission queue entry and then you prepare an operation in this case prepare read say for this entry read from this file descriptor fd and fill in the the data into the data area of 0.2 by ptr and the size and the serial at the end is this flag we can ignore that for now and then you can i really should use this during sql set data because this is the way that you can identify which operation was finished when you get information back from the completion queue it's only one one word in 64 bits which means that if you want to associate both a callback and some data you will need some kind of indirection but 64 bits is plenty you you will you will be able to find a technique to store all the information you need and by the way these operations that i have showed here are all essentially completely for free these are these are read and write accesses into just a fixed size array this is this dirt cheap nothing is happening here really and then you wait wait for something to happen from the completion queue you get an entry and this weight can of course be if there is no data available it is a system call so you maybe write something like this a c plus plus the ring thing you really don't want this to be movable at all the the io urine expects things to be in in one place you don't want to move them or copying them bad things will happen but it looks pretty similar to the polar on this superficial level but we need here's one difference with the polar we just registered the file descriptor and when it it was released we could read into a local array but here we are actually saying to the colonel read this data into this memory area so we must provide it already beforehand so in this case i have this read work struct that contains a callback the file descriptor and the data area in in a real world streaming application you would definitely use some message pool for received data but let's keep the example simple and then when we want to add work we can make sure we create a destruct some somewhere where we can reach it store or are working on callback get a submission queue entry and prepare the read and store the address of the work so that we can find it when when data arrives ah some nice guy added information to the slides that said what i just said interesting so what happens here then is that when we run this we have some data buffers that are already in place and the the kernel starts to fill something in and we are waiting but then we see that oh the kernel starts filling in the next one immediately because there is one available and wakes us up so that we can process this information and then it goes on with in this case we have two two buffers for the received messages and we take turn the kernel and our process take turn which which uses which but we we essentially have a [music] have two threads here now so the kernel can fill in data to one of the one of the data areas while our process is working with the the previous one that was filled in so this is nice and it actually gives us fewer system calls too which can be can be important so let's let's have a look at this so the same program but with the u-ring instead so i have a ring struct here the the ring class the one i showed you wait this one is actually a little bit elaborate but what happens here is that when we call weight we call urine submit and say all these things that we have prepared now is the time to actually do things with them so here's the system call and then we wait if there is data already available this will not be a system called it but you will just get it and then we do work with this entry and with the work we do is in this case we call the callback and i said that the callback can return a status saying that yeah and i want to continue which is the true case in which case we prepare the next read for the same entry or the callback and say no i'm done i don't want to be here anymore and then we just remove it from from the the list of pending work and this completion cue entry scene function call is important that is the the way we tell this shared memory data structure that we are done with this piece now so we do this and then we have this peak that continues to just look at and is there one more and these are one more and we work with that one too and then the main program looks pretty similar to the one that we have with paul but in this case i'm actually adding the four entries for for reading data so that we can we can have a queue of up to four pending reads and we still have this 4001 to just print some exits so if we do this one we can see that we enter and enter to the kernel to say that here is the stuff i want you to do and then we have this weight and there is nothing pending so it would be a system call and we we we're suspended waiting for something to happen and then it goes on so this doesn't look like much of gain i guess compared to the polar solution but we can do another one we can do s trace dash c the the c just counts the number of system calls the number of each call how many occurred and here's another useful tool m ping it's a we we're telling nping to create udp packets to port 4000 at the rate of 1 000. i don't know if this is actually a bug in amping or if it's working as designed for numbers less than one thousand it's the number of messages per second but when you enter one thousand it just goes full steam ahead as fast as it possibly can send two thousand package 20 000 packages and with a payload length of 1400 bytes and then when it's done with that we end sending something to 4001 to to stop the process so we do that and here we are so it was a little bit too fast we missed a few packets this is because of the overhead with with s trace it's definitely fast enough if you're not doing s trace but we see that for nearly twenty thousand packets we have a bit over five thousand system calls so roughly four packets per system corner hmm interesting so let's compare that with with the pole solution instead doing the same thing go and jikes six thousand people we sent twenty thousand we already received six 6770 and we see that we have roughly two system calls per packet which is costly this is not for free so iou ring is good in this respect it can it can really boost your performance compared to the classic poland read so that was the first part of the talk so now let's pause this for a while and go into core routines instead change the topic core things so coroutines offers a way for you to write asynchronous code as if it were continuous loops so and we we have language support for this from from c plus 20. so it's there a simple plus 20 compliant compiler supports the language bits of of co routines and i am going to use the word magic quite a lot in the next few minutes sorry but it really is so you write code that looks like a loop and the compiler converts it to something completely else via co-routine types that you must write and they are mind-bogglingly hard to understand it's it was so much pain to to be able to write these types and the standard library unfortunately does not offer very much of help so this is a bit sad i think so what what happens then though is we see an example here with two coroutines one both are written as loops that just go around and do do something so the one to the right starts suspended and the one on the left can now do some computation it computes some x and it suspends its execution and passes on the the ram privileges to to the one on the right it resumes and starts doing something with x works with it and when it's done with it it suspends its execution so that the one on the left can resume its its execution compute a new x suspend its execution let the one on the right continue with this and do some work suspended execution and review and so on so mind you this is not threading everything that happens here happens in the same thread so we have no issues with data races here we don't have to worry about locks or whatever and we also get good locality of reference with regards to cache because copying information between cpu cores is really expensive with a cache usually not always sorry so what we do we can write a coroutine mycorrh a carotene that takes an xnoi and a coral source and we loop awaiting information from the source and as long as we get information that is non-zero we continue and do some work and add something to a result and then we cover it return it and the bit at the bottom is quite important that when we call the the core routine what we get back is a coroutine object and this one must be kept alive because that one is what contains all the information because what actually happens is that the compiler sees the code on the left and the compiler rewrites it sort of kind of like this we get a class micro where we see that the local variables now became member variables and the the loop just isn't there the the loop is in the magic in input at the bottom that gets values from the source and sees if they are non-zero then it calls the function call operator where we do the updating of our information and then it goes on and this is of course a vast simplification the reality is not quite this is simple but it as a first approximation to get an idea of what is happening i think it it works it's not terribly wrong it's good enough to get started the cracks here though is that this corrupting type is one that we must write and this is also one that we must write so if you look at the at the standard for zebra plus 20 it refers to these types are as co-routine return objects which is quite a mouthful and doesn't say a heck of a lot most blog posts and presentations from conferences i have seen refers to these as tasks so we'll good or bad i will continue with it with this naming convention so as not to add to the confusion a task must have a promise type and no it is not the promise don't don't go there don't try it will not work it will be very bad so promise is not a core routine promise at all and yes we have to write the promise type and the interesting thing here is that our task will be created from a promise so the compiler generates code that makes sure that the task is constructed as a local variable stack object given a promise and the promise is allocated by compiler magic i said i would use the word magic a few times in this and we must take care of its lifetime and don't call the lead on it ask me how i know it will not work it will be very very bad so what you do is you you can create a smart pointer type in this case i use a unique pointer with a core routine deleter so the core routine deleter we we get the core routine handle from the promise and then we can call dot destroy on that one and that will de-allocate the object and do some other housekeeping stuff sweep some of the magic away so that things look normal again and the promise as i mentioned is also a type that we we must write ourselves and there's a lot of ritual in this this is pretty much as small and simple as it can get actually it is oversimplified it's simplified too too much it don't write one quite like this get return object that is the function that is called by the compiler generated code to create the task and then we have the functions initial suspended final suspend they must exist and they must return one of system never and suspend always and both of these types are empty types they contain nothing other than the the type itself is the information but they the compiler sees this code and generates its code accordingly israeli and get are actually not used by the compiler that is these are convenience functions used to make it a little bit easier to write the rest of the code so in our promise we have a co-routine handle to a continuation as you saw in this sequence diagram a little bit earlier you you have a coordinate that does something and pauses hands-over execution to the other one that will resume and then it will pause and cause the other one to resume and that is the way you can do this by having a continuation that you say now it's your turn now it's your turn now it's your turn and i'm using an optional here to to store a value which is really kind of nasty because this means that we cannot handle the error case with the exceptions but never mind that for the duration of this presentation so yield value what we do then is we store the the information we got in the in the optional so now it's it's possible to to see that yes it does have information and we call resume on the continuation so that it can do its work and then to be able to co await something our task we will need to implement operator coal weight which must return an awaitable also a lot of ritual again and the the awaitable communicates with the promise to know what to do so awake ready is when you call the weight on unawaitable then it starts checking with the promise do we already have a value if we already have a value then we just get it otherwise it suspends so it's a way to spend we get a a quarantine handle and store it in the continuation and now you may think it's cool why do i have to store the continuation here i can square all that away into some data structure and write a really awesome cooperative scheduler and yes you can i'm really not sure that you should but you can i'm sure someone will do that and do something absolutely amazing and i think others will do that and cause some amazing mess but that's a different story but that is how we get this running and a weight resume is called when when the core routine resumes work again after a cold weight and then it just cause in this case the promise to get the value out of it so we can actually have a look at this so we have our time so these are a little bit cluttered with the trace things so that we can see what is happening but otherwise these are actually the the types that i just showed the the task with the operator call weight and be available and i have a make that we will soon see why and then i have a coroutine print all that just it takes a it takes a co routine that is a weightable and it just gets whatever the core routine provides and print them that is all and we have some tracing in that one too so in main we we get a co-routine handle called incoming that is done by just calling this make and make does suspend always that is all it does so what happens when you call make is you get a call routine handle to something that is suspended that's all and then we call print all on this coroutine handler so it can get information and now we have this coroutine handler note now that nothing has executed yet worth speaking of we have just instantiated the co-routine types i can get the the promise from the from this incoming quarity and i can call yield value which will turn this machinery around boom like that so when we call make on incoming tasks we see that how the promise for incoming was created get return object is called and then this creates the task of incoming and it calls initial suspend and from there we're now here and called called print all print all calls gets again the the promise gets constructed get return object we get the task for the printer and the initial suspend and we see this print all so it actually begins to do something because the initial suspend here says suspend never so it's it starts executing but then it comes to co await here so it calls they're awaitable or are you ready and the promise says no i'm not ready sorry so we call our way to spend which means that we are back here now so we can call yield value for your value to the promise which cause awake resume to the suspended print which will call get on on the promise so that we can print the value and so on just over and over and then when we come to the end brace we see how these tasks and promise types are all destroyed what we do not see is this print this trace saying print all is done because we have we have not broken out of this loop so as as i mentioned earlier the the core routine is an object and every time we call wait is it is called with a value and since we have never it in any way we are not running this instead we are just running the the structure of the of the object and this is safe this is fine this is not a problem excuse me again but there's a lot of things happening definitely a lot of things happening but this is actually quite useful we can we can do some things here's a an example of a super generic kind of thing to to use with with co routines that are co-awaitable because it takes some predicate and the task of whatever type and we just call wait values and check the predicate and if the predicate says yes this is a value that it is in we co-yield it and then we go go for another one yield calls the yield value member function of the promise that belongs to the task that is the return type so the the name yield value was not completely arbitrary it's actually mandated by the language itself another thing we can see here is that there is not a one-to-one correlation between the number of things we [music] we co await and the number of things we call yield we can call yield several times for one piece of information retrieved and we can co-await several values without code yielding anything and in fact we can go away from several different sources that is perfectly fine but we cannot code yields too many because it's we call yield to our task and we are one so let's do a simple really simple example so have an is odd predicate that just returns whether a number is an odd one we can use this make that we saw earlier and we can create this coroutine object odd values that is filtering in everything that matches this order from incoming and send that to to print all and then we just loop over them and yield the values in this case 0 through nine inclusively and we can see what happens when we do so here's the entire program i moved the task into a header of its own now just to save a little bit but we have exactly the code that i showed you incoming is odd get the odd values the printer and then we get going so and we see that it printed the the odd values okay not not rocket science but this is this is a totally generic piece of co-routine function template that that is useful in in a lot of situations so let's go back we we've chatted a little bit about co-routines now and earlier we talked about iu ring so we have seen how io urine offers asynchronous data in a cheap way and we have seen that you can call yield value on a co routine promise which starts to push data through a co-routine pipeline and we can see i've seen how to read values from an upstream quality with with cold weight and how to forward values downstream with with co yield so let's narrow down and put these together i must warn you that the example program i'm going to show you now is extremely silly because i don't want to burden you with how to do manipulations of video transport streams and whatever because that's really a mess so not necessary but a minor convenience a way to get something callable from a promise type that just calls yield value on it and we have our main function we have some statistics number of bytes a number of packets and a flag tell you if we're done so we have the line above what's marked now we get the task of a span of char that reads from that is going to be connected to read from port 4000 on this in 4000 we send to count packet data which is a core routine that just co-weights these spans of jars and increments packets and how many bytes we have received and then pass the packet on to the next one whatever they do we don't care it's not our it's not our problem and then maybe we want these in string format they are known spans of chars so we can have a two string that takes a task with spencer chars call weights these packets as bands of chars and creates strings of them and co-yield them to whoever is the next consumer and then we may want to strip trailing new lines if you remember the first example when i just showed the echo foo to a port you saw that it had a trailing new line so you may want to strip such things so we call await strings now and as long as i end with a new line we just remove the end and then we co-yield them and the next one we want to concatenate strings up to a length of in this case 40 characters so we can have this concatenate two that consume strings and return strings but now we don't have this one to one correlation between what we read and or rather what we call a weight and what and what we call yields so we have this string current line that is this is essentially a the member variable that carries state across calls so i have a loop where you say get the next piece which we call weight so the next piece is a string and we check if the current length plus the length of the next piece plus one is too long in this case we pass on what we currently have and we say that we store this piece we just received as the the new current line otherwise we just concatenate to to the current line so that it can grow so here's a co-routine that stores state between the weights so this is again it's a it's an object where a current line is a member variable that is updated in calls and every color weight becomes a color and then where print lines which doesn't really do anything surprising i just added colons as delimiters here to to make it a little bit more easy to to see and then we have our print and exit as usual so we we get a socket for 4000 and 4001 we connect the file descriptor to 4000 to this thing that just feeds the the red data as spells of chars into this coroutine pipeline and as earlier 4001 is just the the way to stop everything it doesn't do any well and print the information let's have a look at this one and i have forgotten what i call this program it's the only one left the ring coil so compact data two string strip trailing new line concatenate two print lines and then we just add everything here to have all these coroutine objects that communicate with each other and does the work once we start giving them data which which happens here at the ring weight function so if we run this i'm just listing the device devices i have on this machine and read the names and echo them to to this udp port and then end with the 4001 that stops the program boom like that and we can see how they just concatenated up to this maximum length and at the end we see that yeah 233 packets totaling 40 and 83 bytes this is powerful stuff just adding things like this but what you really have to think about though is that it's these it's these coroutine objects that well not been to mexico print and lines these coroutines objects they are the ones that keep things going if you let any of these be destroyed while the while the system is running things will not go good things will not go well at all so keep that in mind so i'm pretty much done here just wrapping up a little bit so as i mentioned initially this is a superficial walk around of just to show you the ideas of what exists here and i've omitted a lot so for example error handling is definitely not trivial you you need to think about how to how to propagate error information through these pipelines a major omission is how to cancel things and this is these are tricky in different ways with coroutines and iou ring the simple case with co-routines is the one that i actually showed that i have some some state for the entire pipeline that which was the done flag and when i reached that i just destroy all the coroutine objects and everything is dandy no problem but if you have a situation where some co routine somewhere in the middle says says i don't want data from you anymore and wants to sort of push back that is you have to figure out a way to do that and canceling with from iou ring has a completely different kind of problem because as as i mentioned when i introduced that is this is essentially multi-threading between your process and the kernel and its fair memory so if you start deallocating the objects that the kernel may just this moment be writing to things will not go well so you have to figure out how you want to get those pending operations done before you can start the allocating stuff and there here's one really confusing thing if you have looked at different blog posts or presentations like like this one but not this one you have probably seen that the co-routine types look differently and the reason for this is that the the compiler when it sees your core routine types it inspects them it's there's a lot of freedom here the compiler says ah your types have these functions and this function and this function and the signature for that function looks like that okay i will generate code accordingly which is mighty powerful but it really doesn't make learning any easier so i'm not jealous of the the trainers that will write training material to to teach us how to do this because the the the design space is so large so some conclusions the lack of library support is super much a headache there's no there's just no way around that and testing and debugging is absolutely terrible especially since you write your own types for a future intel's gonna waitable and if you get them wrong it can be extraordinarily difficult to understand what the problem is so actually what i was we started to do when preparing this presentation was this here's a extraordinarily simplistic const expert string type that can be used as a a non-temp non-type template parameter so i have written my types with these names instead and added this tracing to to see that okay it's get return object for that promise with that name because i will probably have several promises of type type-ins and something went bad and i don't know what but this way i can at least see what it is and like likewise for the for the functions then i give it a name print lines returns task void with with the name print lines and it accepts a task of strings with some name whatever and i can trace these this is back in the early 80s seriously this is this is not impressive but i failed to understand what was happening with my program until i did this which this is not good seriously this is not good it might be that i'm not very clever but this is problematic but writing code as a writing asynchronous code as if it were local loops is extraordinarily convenient it makes life so easy once you have ironed out the headaches of getting these types done right and connecting pipelines is amazingly powerful so this this can be used for so many things like a poster for example you you start with something that just reads reads the files and connects it to a lecture that gives you the tokens and then pass it on to the next step which generates or whatever i'm not a compiler writer so i'm talking nonsense but you get the idea or for that matter with like video streaming you you get the messages you you you get the transport stream objects you get the the program identifiers and you can filter out whatever you want so this this is super powerful this is also good for for testing because these core team programs that call wait something and call yield something you they don't care what it was that produced the data or or what it could sends them to you can just feed them data in your in your favorite unit testing framework that is amazing so some resources if you want to get into aya during srivad hussain's the the lord of the iou ring is a great resource to get to understand how this works co-routines presentations etc there are a lot i the reason i'm highlighting paul novikov's talk is that that is the one that made me understand what i was doing the other ones i it just didn't click for me but with this one it did i crossed out louis baker cbp coral because it used to be the next great thing and as far as i can see it's abandonware it is written for the corrupting ts but it's not a product you can look at a really simple one with from michael svetkin ecoro and if you want to go into some really impressive stuff look at libyan effects which combines everything i talked about today and more and it's really powerful and with that i say thank you for your attention this is all i had to say and here's how you can contact me if you want to do that thank you thanks for this great talk i really am find it amazing what anyone can do with cold routines and we still have a long way to go with it too so we're gonna have a bit of a conversation about that in the launch i guess and so if you have some questions and because we're like basically at the end of the talk at the end of the time join the table with bjorn and then all your questions can resume there and see you there all right see you in a bit