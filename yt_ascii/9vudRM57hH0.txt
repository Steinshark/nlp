um hello everyone welcome to my talk thank you for being here ever since i talked about how to rentify your code people asked me about the performance of ranges and in my last talk i benchmarked the code a little bit but i didn't go like into detail into the performance of all of this and so i thought it might be interesting to use some tools and try just to analyze the the ranges code a little bit and so here we are so this talk is an outline i'm first going to start by telling you how the setup is so how i generated my results then i'm going to show you a code example and because we need to compare the ranges output to something else i wrote three versions of the code the one that i called c style c++ where i don't use like any standard library stuff i have the c++ 17 version where i use stuff from the algorithm header and algorithms from that also standard containers and then i have the c++ 23 version where i use mainly ranges and with ranges in this case i mean mostly views and i'm going to analyze all of the three versions or compare them for just using google benchmark then cash grint and the visual studio profiler and the profiler will be a live demonstration scary and so i used a visual studio profiler before but i'm fairly new to the linux tools because i am a windows developer always have been i grew up with windows pcs and so for you the experience might be the exact opposite and after we look at the output of the tools we are actually trying to optimize all of the three versions of the code and then we run the tools again and in the end compare all of the results so let's first start with a setup so as i said before i'm a windows user and i always have been so also at work i mainly dea develop on a windows machine and also use windows compiler and so i never really had the inclination to learn linux or the linux tools so i wanted to have the setup as easy as possible for me in order to use the linux tools that's why i used awsl that's the windows subsystem for linux which is a virtual machine that runs on your windows computer and it's installs ubuntu on default and then you can use it like a linux computer and the nice thing for me is that i can access all of my files that are on the wsl from the windows and also the other way around so the the drives are mounted to each other so that makes it actually extremely easy to use especially for me and what also is interesting the computers that i'm using so this is my laptop and we will run the visual studio profiler on this laptop because i don't have anything else here and as you can see it's not built to do any kind of hard work it's hardly built to do any work that's why i need a new computer i'm saying this for a while now but i i really need a new one but i also of course have a main pc and this is my ridiculously overpowered gaming pc which i love it's super fast and as you can see like it's huge difference between the setups and this is where i generated the cach grint output and also that the google benchmark on so so that you know you know if the profiling result looks a little bit strange it's because i'm doing this on this laptop so now we have arrived on the code i wanted to find like a real world example where i can use as many ranges and views as possible and compose them all together but it turns out that's actually pretty hard to find and so i just made up some code i made up some nonsense code where i use as many different range adapters from c++ 20 and 23 as possible i'm going to show you the code i'm not spending too much time on the code it's not meant to make sense it's meant to be compared for the output like in a real world application you would have a mixture of all of these probably but now i wanted to have like a pure output of the ranges and the algorithms and the c style c++ so don't worry if the code does not make any sense it's not supposed to just wanted to put in all of the features that i found and in order to mimic some kind of real life functions i made three sub functions and in the end one function that calls all of them if you're it's i don't know why but if you're interested in the code it's on my github so you can look at it later and even look at the optimized version i will set say it later again this is only after one cycle of optimization so if i would be really out to do as high as a performance as possible i would not stop here here so i would do more optimizations this is just where i stopped for the talk and also so the first versions that i show you is what i would write naively without caring for like any performance so again as you know you should only just optimize your code if you know it's too slow if you benchmarked it you know it's too slow then you should start to optimize it so at the stage where you first write the code there's no reason to like try to optimize everything from the beginning so i'm going to start with the c++ 23 version as i said i wrote three subversion f sub functions this is the first one and so this function takes a standard vector as input and then i'm going to use one of my favorite views which is the cartisian product and what this actually is it is a nested loop so it combines all the elements of all the ranges that you put in with each other and so this first range that you input is the outermost range from your nested loop then the second one is the in in this case i'm using three is the one in the middle and the third range that you put in is the innermost range of your nested loop so this is a nested loop of three of of three containers and as you can see i'm just using the input range and modify it a little bit for the the innermost range so cian product as i said is a view and it creates a view of all of these combined elements so you get references to all of these elements and you get a tuple with these references that you then can address and do something with the values so that means at the end of these parentheses here we're not doing anything we created a view but i mean it's called cesium product you might think there's like some computation there some multiplication because you get a product no it's just a combination of all of the ranges and then you need to do something with it so i call transform transform gets a tuple of the references of all the elements i love structured bindings so i always use them i hate the get syntax and then i just do something with the values again it does not have to make sense i just need to use all the values at some in some way and i'm going to return as you can see i'm going to return this view and when you were in nik talk and also if you know ranges you you know that this does not execute anything rangers are executed lazily so we can return this view here but it will only be executed once the value from this view is needed so once we call the star operator on the first value of this range and if you want to write this function in c++ 17 you have no other option than to actually use a nested loop i love to create vectors and then return vectors in the end so this is what i'm doing here so i need to have the result stored somewhere it's not like the ranges view where i have all of this in a lazy way and so i loop over the outer range the middle range the innermost range do the calculation this is the calculation that we did before in the transform view and i push back the element that i calculated and then i return the vector in the end so this is the exact same code than before just written in a different way and of course we have c sty c++ as you can see it's ste style so i'm using pointers and because it's dynamic memory i need a size to for my range i have a new in there which is super scary and then i need to use indexed loops as you saw before i used range based for loops and now i'm using index loops because i'm in the c style world but the calculation itself is the same and here i'm returning this dynamic array from the function which i need to remember later because i need to delete this so the second function is a short one i really really wanted to use zip with which is called zip transform in the standard library and i haven't told you that this stuff is completely implemented in msvc currently but one crucial function is missing in tcc which is ranges two this is why i'm using range 33 here and not the standard library i would love to do this with the standard library but i'm using just this one i'm losing needing this one feature from tcc which is missing so that's why i'm using zip width here which is called zip transform in a standard library and zip width combines all the ranges that you input and you step through them like at the same time so so yeah in unison at the same time you have to put the function first for zipwith because like for internal reasons this is a it takes a parameter pack for the ranges so you can import as many ranges as you want but then you need to have the function at the first as the first argument it's not that pretty but it's it's the language and then you input all your ranges that you you want to combine again here i'm returning the view no work has been done yet nothing has been executed it's the instruction that we're returning here from the function doing this in c++ 17 we can use st transform again this a c++ 17 version so there is no range based version yet of the transform so i'm using the iterators and i'm doing everything in the transform that i did before also in the transform view but i'm returning a vector here and for the cstyle c++ you you know the drill gets some memory from the heap have a index loop to your calculation and then return your memory and the third function we're almost done the third function just creates two temporaries here i use partial sum because because i like it and for the second temporary i use the sliding view also because i like it i think it's very common that you want to use something like that and then i can just do something with the values i always call them transform on my views and then do something with the values and in the end i wanted to have one output and to get one output i'm using inner product which gives me one double in the end that i can compare that i'm actually doing the same thing for all of the versions and yeah that's basically it so because i'm using the sliding view for the temporary 2 view here i'm getting one element less than i get from the input range so i need to drop one value which is what i'm doing in the inner product here for the first range again it it's nonsense code but it uses a lot of features as you can see so we can actually write this quite nicely in c++ 17 all of this has an algorithm in the algorithm header so i'm using partial sum as before with the ranges i'm also using here adjacent difference which is actually quite nice it is a terrible name for for this algorithm because with adjacent difference you can do anything to the adjacent elements you don't have to calculate the difference that's the standard behavior if you don't put in any other function but you don't have to use it so you can do anything with the adjacent element so this is what i'm doing i'm giving it a lambda and i'm just doing what i want so name is terrible function is great and then i'm calculating the inner product and also like a need to shift the the iterators a little bit because also for the adjacent difference the first element is not what you want so the first element is not the result of the calculation it's the first element of the input range so you typically don't want to use it i actually don't know why they did this it would be very easier to have like the last element be different because then you could just cut your range at the last element if you cut it on the first element then you know you need to copy all your data uh in memory and you know everything is bad so for c style we of course don't have any algorithms here i should have used some comments in order to make this a little bit more readable but to have it like fit on the slide this is the parure suum this is the thing with a sliding view with the two elements and this is the inner product and in the end we need to delete the two temporaries that we just created now nothing goes wrong on this point so it we will actually reach the delete so again this is just a very quick overview over the code if you really really want to look into this in more detail it's it's on my github you you can get it later yeah and in the end of course i return the su and so the last function is just calling all of them so the first function i call with the input range that i get overall the second function i call with the result from the first function and then i call the third function with both of these temporary results same with c++ 17 note that we could make these cons in the standard library those ranges but it's non-c in range with three i actually don't really have an answer for that i guess nico has yeah and doing the same with the c style but then again we need to delete all of our temporary because it does not do this on its own so the first tool that i'm going to use to compare these three versions of the code is google benchmark because google benchmark is a platform independent tool and it's extremely easy to set up and use and i can compare all of the three versions of my code at once which is also really nice so to use google benchmark you have to implement one function um that takes a benchmark state as an input parameter then you prepare all your data or your input that you don't want to be benchmarked then you have a for loop over the state and in this for loop body you do the thing that you want to benchmark i'm using do not optimize here from the from the library because compilers are kind of intelligent and i'm calling a function here that returns a double that is never used anywhere and so it's a high probability that the compiler optimizes the entire function call away because i'm not using the result anywhere and do not optimize prevents that so it actually executes the function that you want to benchmark then there's this macro where you have to register your benchmark for um in order to be executed it i changed the output here to milliseconds by standard it's ncs and it's just a huge number and i don't think it's very readable so i like milliseconds here and then there's a benchmark main which is also a macro and then it calls all of the registered functions and calculates the benchmark for this so this is the entire setup i think it's super easy super fast to do and then you run this al so you run your your program and then you get some output and so the first number that we have here is for the se style and it actually runs for 25 milliseconds then we have the c++ 17 version which is a little bit slower with 35 milliseconds and then we have the rangers version which is super slow something happened there don't worry there's actually a problem in the code we're going to fix it in in the optimizing round and then we're benchmark again but this is an arrow that is not easy to spot i think so we will see this then in the in the optimizing round so next i thought i look at cash gr because why not so v grint is around for i don't know how how long and i never really looked into this i mean by why should i have and cint is used to to do performance profiling and analyze and optimize the cache usage of the program so i never saw this like in any talk that someone like analyzes the actual cache usage of the of the of the different versions so i thought i just do it and then yeah with the output we can try to optimize or we can try to see bottlenecks in the cash use and try to optimize that in the program so if you never used cashr before you will be uh a little bit overwhelmed by the output that you get so i'm going to go through the output like line by line just that everyone knows what we're talking about here and so if you run this you get this kind of output i mean cashin is is a command line tool for for linux systems if you run this you should build in release mode so optimized but with debug symbols especially if you want to analyze it any further because without debug symbols you cannot read the additional output that cach grint produces so the first block is the instruction cache statistics and instruction cache contains all of the instructions of the program when the processor needs a construction it first checks the level one cache which is the fastest cache and if the instruction is found in this cache then it's called a cash hit and if it's not found in this cach then it's called a cash miss and then it may have be fetch may have to be fetched from memory and this is what makes your program slow so everything that runs in the in the caches especially in the level one cache is super fast and the further up you go in the cache until you reach the main memory and or the hard drive then it gets super slow so that's what you want to look out for when you're looking at this output so next we have the cash misses for the level one cach and then we have the cash misses for the last level cach in my case i have the level two and three cache combined here in the last level cache and then it shows you also a mis rate which is in this case 0% i call this pretty good and then we go to the next block which is the data cache data cache is quite similar to the instruction cache it contains all the data that your program needs in order to be executed so data that you access frequently will be in the level one cache and data that you don't access frequently will be stored far away away in the caches like in the higher level caches or on the main on the on the hard drive in the main memory so if the process needs to read or write data it first checks in the level one cache and if it again finds the data there it's a cash hit and if it doesn't find the data there it's a cash miss also cash quin shows you the read and write operations separately as well um again if it doesn't find the data in the in the cache it needs to fetch it from main memory and this takes time so you always want to have your mis rates low and then you get the percentages again so in this case we will go over the numbers later so in this case the cach misses are actually quite high and then the next block is the last level cache statistics where the last level cache like combines the level two and three cache here on my computer and it refers to the highest level of cach in a multi-level cach hierarchy which is what you all have in modern processes and it also in it contains the data cache and the instruction cache and this also plays a crucial role if you want to optimize your program because the more data you can do in the higher level caches lower level caches sorry the faster your program is and then in the end we get also statistics about the branches that were created in the program it's split into conditional branches and indirect branches where conditional branches are changes in the execution flow of your program this happens in four loops in if statements so every time you have a condition then you create different branches and then you jump according to the output to those different branches and indirect branches are jumps in the address of the register from the memory location and this happens for like function calls virtual function dispatch jump tables and all of the situations where the address of what you're calling is not known at compile time and so the next lines indicates the branch mis predicts that are occurred and then the mis rate a branch misprediction happens when the processor's branch prediction mechanism predicts the wrong branch outcome and mispredictions are bad for the performance of the program because that means that the program fetched the wrong instruction and has the wrong instruction in the level one cache and it cannot use this instruction it has to throw it away and get the correct instruction so this is also what what can make your program slow so we haven't really looked at the numbers yet but i will compare them shortly with all of the versions so you don't need to remember all of the numbers that i just showed you for the c style this is here for to have the slide deck complete so if you get the slide deck later on and you want to look at all of the numbers yourself you can do this here also for the c++ 23 version but what i'm going to show you is the comparison because even i can't remember all of these numbers so as we can see in the first line in the first row that the c style c++ version has the lowest numbers of overall instructions created and c++ 17 is in the middle and c++ 23 is actually quite high what i found interesting is that the total number of misses are the same for all of them and but mis rates are zero for all of them so it may not be a problem just because you have a lot of instructions doesn't mean you have a problem if the mis rate is high then you have a problem also for the data refs same direction here what is interesting to note is that we have very very low data misses in the rangers version and that's because for all of the other versions we created temporary vectors that live somewhere in the memory and they have to be stored somewhere they have to be written and they have to be fetched again if you want to read it this does not happen for this version of the c++ 23 code because once you request one of the values in very low than in the inner product where you actually use all of the results that that you want to create then it fetches the memory from your one input source and that's actually quite good in order to like manage all the the data the data caches because you don't have like any additional memory stored that you need to f fetch so you only need to fetch this one or two values that you need for the actual execution of the of the algorithm this is why the number is so low and also i said before the mis rate is super high i think for the c sty c++ i don't have that much of experience but just looks like a high number and so last level cash also is very low for the c+ first 23 version which is the same reason than we solve with the data refs we just don't need to do this much stuff in in the caches because we only have this one input range that we are using and yeah we only need to access this and then do all the calculations in place branches are quite high for c++ 17 version this does not have to be a problem as long as the mis predicts are low and they are almost the same number again for all of the versions and yeah but it it's it's not a problem unless the mis predicts are like high but it's just something that i noticed so this is what i would focus on just by looking at this output and trying to lower these numbers so now for the fun part we're going into the virual studio and we use the profiler has anyone of you used a visual studio profiler before oh quite a lot actually that's that's good so in order to use the profiler i mean you can use it with almost anything as you can see you can use your startup project which is a visual studio project in my case but you can also attach it to a running process and executable oh oh sorry how do i get this wait this is not working wait wait wait we will get this i think so too yeah oh what's happening is there any tech support here yeah [music] attached yeah better yes okay so yeah i just wanted to show you yeah you can use your profiler with your startup project this is which is in this case a visual studio project because i think it's the easiest to use but you can attach it to a running process an executable running app like anything probably and it also works with cake projects i'm going to use the cpp usage i'm not looking at all of the other tools i'm building in release mode and x 64 and i'm just hitting start so this is all the setup i'm doing for the for the profiling you can do this computer perfect and then once you start start of the profiler you get like this top view of the diagnostic session with some kind of statistic here where all your execution of the c++ stuff is actually in kernel because we don't really have any runtime that we need to start like in c you have your net runtime that also does a lot of stuff and memory management and we don't have that here and i'm always just clicking one of the functions here don't really care and i'm going to go to the flame graph so i also don't really care for for these ones i want to use i want to see where the time is spent so i like to use the flame graph and there you can click into your program and it highlights where you are in the program and how how much time we have spent in that function and then also al the the size of these yeah of these of these thingies is according to how much time we spend in the function so i'm not really spending too much time here so of course we spend a lot of time in the loop this is what is expected also for the second one well we're in the loop nothing interesting here and we can see we spend the most time in the third function where we have these two temporaries and there we spend the most time in the in the first function if i wanted to optimize this the profiler doesn't really give you any hints on what to do now with the results that you get so you need to have some like optimized techniques in in in your head already or you need to learn them and so for this version what i would do to optimize it and what i'm going to do is i'm going to get rid of the two temporaries here and combine all of the three loops into one loop because i'm iterating over the same data like all of the time and i really only need one loop for this so this is what i'm going to do then with the optimization for the c style and this is the first round of optimization then of course you can go back and profile again and see where you spent the most time and try to optimize the next bit of code so if we do the same [music] for for the c++ 17 version we can actually do relaunch performance profiler which will launch the performance profiler with all the settings that you did before i mean we didn't do any settings but it just saves one click it's nice so this is how the output looks like for the c++ 17 version again not really spending too much time on this overview here i'm also not using this like extremely regularly and again i like to look at the flame graph and wow we can see we spend most time in maine that's interesting and here we can also see that we spent again the most time in the third function where we have the same problem that we have in the c style version that we use like these three algorithms which are loops in the background and we could combine this into one it will lose a lot of readability because this just tells you step by step what you want to do but it will lo you will lose readability if you move it into one loop but i mean it's it's the trade-off if if you're interested in the fastest code then you will lose some readability that's that's the game yeah so we look at the same so now this is actually the interesting part because this was the slowest function so now let's see if we can spot something in the profiler and see where we spend all of our time yeah that looks fun so let's click one of these again look into the flame graph and as you can see you see nothing again we're in the main function this is what i expect and then if you click here this is some memory address we are in range v3 numeric inner product which also makes sense because all of the work that we're actually doing everything that we're executing is in inner product so before we had all the views which are not doing any work this is the feature of of ranges that are executed lazily and inner product is the first point where these values are requested and that's where we spend that's why we spend all of our time in in a product so that makes sense but it doesn't explain why we're this slow so we're still in inner product we are in basic iterator which i think is called called by in a product so this we in invoke and what is well hard to read because this is the type it yeah does not help me like even a little bit when in twole algorithm does not help me again and here you can see the really readable type again the only thing that i'm noticing here well is the cartisian product so you it's like all of the combinations of all of the views that we did okay going back to main maybe there's something here we are in triple algorithm [music] again yeah i i i cannot work with this i don't know about you i cannot work with this i cannot tell you why my program is slow by looking at this output so i defaulted to something that i usually don't do and i did this diagnostic session in debug mode in hopes that the debug mode doesn't optimize away this much you will get a warning from visual studio like not a not an angry warning but you will get a warning that the results are not reliable if you do this in debug mode and i prepared this beforehand because you can see it takes four minutes on this computer and i don't wanted you to wait for 4 minutes so let's see if the deug mode helps us a little bit it's a little bit better so i mean at least shows you some of the functions but again it's also not super accurate as you as you can see so we actually spend all of our time here and not at the destructor of anything yeah but now you can see like way more things but we are still all in range with three and we don't know where the actual time is is spent like we we cannot see it like clearly we see that we spend the most time in inner product again this is what we suspect now we actually see it here in the output and then again if you just click through we are in inner product this is expected inner product and then you have like all of these locations again basic iterator partial this this again is basic iterator so i i still think that's extremely hard to use so but how i optimized this code but how did get behind what i needed to optimize and for me it is just the type hints so in this case i saw the cesium product here at the very top and i saw it again here and i saw it here and i saw it here and like all the way down so all of the branches that are executed in my code are calculating the nested loop which is like really costly like you have a nested loop over three ranges this is really costly so i thought to myself how does this happen and how does all my my code like execute the cesium product over and over that's actually a problem in the code and let me see yeah i'm i'm going to show show it to you later in in the slides so that's actually a problem in the code we're going to fix this this has a reason this has a good reason why we're going to fix this so now let's see yeah it works so now we are optimizing this as i said for the c style version i just want to move all of these three parts that that i created here with the temporaries into one function so that i don't have the temporaries i have less data that i need to store i have and and address and also just one loop instead of three loops so it should speed up the entire process and you need to trust me a little bit i cannot go into like the details of this you need to trust me a little bit that this part is here in the shorter version the second function which is like the the sliding thing over the two elements went over here and the third part is the inner product which went over here so again the code itself is not that important i just moved all of the loops into one and i did the same for the c++ 17 but because i don't like raw loops i used accumulate and did all of this in the accumulate in the lambda of the accumulate so this is the exact same thing that you saw before just wrapped in accumulate and now the interesting part i think is optimizing the rangers version and what we can see here is that we create the range one here by calling the first function and then putting that into the second function and the third function and so the second function now depends on the output of the first function and now i'm inputting this into the last function so now i have two separate functions where where i only have the instructions how i want to calculate the values and both of those contain the cesium product and this is the problem so i'm accessing both values and in order to be for it to execute this it needs to evaluate range one with a cesium product and range to again with a cesium product and this is what makes it slow and this is i think something that can be easily overlooked but it can be optimized and i optimized this in a very easy way i need to force this code to be executed and stored somewhere this is i i needed this to be only executed once so i use my favorite feature which is range two and i store the output in a vector note that i don't have to change anything else of this function or any other functions so because the way i wrote this code i'm returning auto here so now it does not return a range anymore it returns a vector it just works works and the other function uses i think it's only auto as as input and now it also it just works again so i should have constrained like the auto of the other functions but i'm i don't so this is all the optimization that i needed to do so now let's run the google benchmark again let's see if we improved so for the sear style c++ we actually doubled the speed it's quite good i think same for c++ 17 it's still slower than c style but also performance is doubled and yay my ranges version is now super performant which i like of course so i also did cach grint again just to see if anything like changed for the output i don't expect you to remember the output that you saw before this is again if you want to get the slide deck you have all the numbers but here i'm comparing the optimized to the non-optimized version instruction refs went down myth rate is still zero cannot get any better than that the data refs also went down which makes sense because we have two less temporaries that we create and need to write and read so that also makes sense the mis rate is still high not a not an expert in how to manage this memory so might be a trick to this also last level cach went down so again it's the same reason because we have two temporaries le less and we do all the work in one loop for the last function and so we need to read and write less data from memory so this is what speeds up the program and also branches went down so i simplified the program a little bit so mis predicts a zero percent so good program this is the output for the optimized zus plus 17 version and if we compare it we see again that the instruction refs went down a lot also mispredict is 0% we also decreased the data refs which again is the same reason than we did for the se sty version we have less temporaries that we need to write and read and store and fetch so this is what makes the program faster and also the last level cach same reason went down and we also reduced the branches that we created which makes sense the program is a little bit easier it's a little bit simpler but still mispredict is not high so it's not a problem that we have these branches at least i think so and now this is the optimized c++ 23 version and if we compare that we simplified the program a lot for the instruction caches because now we have to execute way less what when down is the data refs they went down a lot but the data misses went up and this actually makes sense because now we created a temporary vector for the other versions we delet we got rid of two but now we in introduced one temporary vector and now we need to write it we need to read from it and this is what increases your data misses but still i don't think like this rate is super high it's of course like the percentage got up but u this is what we're living with and also the exact same reason why we do now more work in the level two in the last level cach and yeah we also brought the branches down a little bit and now if we compare all of these so i have a winner for all of the categories so c style wins for the instruction refs has less refs overall but all of them have like 0% mis rate so for the data refs cy creates the less the least data refs but percentage wise c++ 23 wins so lowest numbers overall also last level c we do the le the least work in c plus 23 and branches so the simplest program from the computers view is the c sty version but again this does not have to be a problem so what would be next so next i would try to optimize this again like run the profile again try to optimize it again if i need it so if this is fast enough then i would be done here but if it needs to be faster then i would go back and optimize again and then run these tools maybe use some different tools if i'm not seeing the output that i'm expecting and yeah for the for the conclusion i learned a lot about how to set up wsl which i actually now really like it's very easy to to use even for a windows user if you want to do like crossplatform it's super easy to use it's integrated in visual studi so you can compile your code using wsl gcc in wsl in visual studio on windows that's cool there are things that you need to look out for for all of these coding styles rangers have their traps you need need to learn them but i think you need to learn like the traps for all of the coding styles that you're choosing for rangers it's not that well known yet because not a lot of people are using ranges yet so look out for niko's talks i think he will explore more of these things in the future as you saw the performance is really good in the end without sacrificing readability i know readability is a touchy subject because some people think that rangers code is not readable and raw loops are super readable but i think it's the other way around i like to see the intention of the code i want to see what the program is supposed to do i don't need to see the how it's actually executed but i know this is yeah people look at this in a different way yeah so i like the functional style of this where you express your intent but i cannot deny that the profile of this code was not fun so it does not give you any hints on what's actually going wrong at no point in this did it tell me that it's executing like the cesium product multiple times so i needed to kind of infer this from the types that were huge if it's not that obvious and if it's maybe not that obvious to you that cesium product is a nested loop and this is what it's slow this is super hard to profile maybe there are better tools for profiling or we will get better tools to profile this in the future or some static analyzers that are analyzing these kinds of things specific to ranges but right now i i really didn't like the the profiling part and but then again it helps to use different tools combine them use different tools see which output you can get from which tool how you can combine it how you can explain your programs behavior with these different tools and then if you need to optimize then maybe you can see something in one tool that you didn't see in the other one [applause] questions hi thank you for showcasing profiling it's always nice when somebody struggles it said of you i'll just mention one thing so i think you were trying to intuit it what those counters mean and why the issues occur like i might be wrong like i was half asleep i'm sorry but if you want to really understand it there is this person dennis bahala from intel who does training he has a book and has 1 million things goes a lot into what does it mean to have a branch miss project and all of that stuff so i think if you're interested you'd find that useful yeah that's super interesting as i said i'm new to these tools so i learned the tools with this talk but also the talk is not about cash so i could not go into too much detail also for this okay so if there are not any more questions thank you for coming have a nice evening