this episode is brought to you by brilliant almost a decade ago a sizable list of tech companies collectively wielding over 100 billion in investment asserted that within five years the once unimaginable dream of fully self-driving cars would become a normal part of everyday life these promises of course have not come to fruition and at present even the most optimistic forecasts suggests there will be a significant fraction of a decade if not several decades before regular consumers will have access to vehicles that would be considered to be fully autonomous currently most of the major car manufacturers offer driver assistance features that can automatically handle a handful of highly specific driving tasks such as cruising changing lanes and braking these systems are categorized by the industry's level 2 automation at this level and below human monitoring of the driving environment is required with a level of focus as if they were driving in a handful of cities a few companies are testing driverless vehicles that can navigate fully autonomously waymo for example is an autonomous vehicle firm owned by alphabet google's parent company that offers a driverless taxi service in select parts of arizona however these vehicles use substantially more hardware than the typical car with a driver assist system and only travel on roads that have been previously mapped by human drivers overall they are far more limited in capability than what would be expected from a fully autonomous vehicle beyond simple convenience the pursuit of fully autonomous vehicles is heavily motivated by the trillions of dollars that could be captured by disrupting the existing transportation industry several companies such as uber have even banked their long-term growth on the technology experts predict that once successfully achieved the technology could prevent 1.3 million estimated deaths worldwide per year caused by traffic accidents as well as reduce congestion and emissions however despite this abundance of funding expectations are beginning to shift as the dream of fully autonomous cars is proving to be far more complex and difficult to realize than automakers had anticipated much like how humans drive a vehicle autonomous vehicles operate using a layered approach to information processing at the bottom of this stack navigation and path planning is the easiest challenge to address and it is the most robustly developed and proven area of automated driving technology determining the ideal path from a location to a destination is accomplished robustly using hybrid navigation this technique uses a combination of multiple satellite-based systems vehicle speed sensors inertial navigation sensors and even terrestrial signals such as cellular triangulation and differential gps summing the movement vector of the vehicle as it traverses from its start waypoint to its destination this positional information is used in conjunction with a combination of high definition maps and even topological maps to determine the ideal travel path in some cases this can even be accomplished over unmapped roads with topology data only using heuristic algorithms because these navigation technologies fundamentally are an extension of human use navigation systems adapting the decades of development to autonomous vehicles has proven to be relatively easy once navigation and path planning is established environmental perception and car control now become the next challenge to meet this layer is characterized as the process of detecting and mapping the environment around the vehicle both for the purposes of traversing a navigational path and obstacle avoidance at present the primary mechanisms of environmental perception are laser navigation radar navigation and visual navigation in laser navigation a lidar system launches a continuous beam or pulse to the target and a reflected signal is received at the transmitter by measuring the reflection time signal strength and frequency shift of the reflected signal spatial cloud data if the target point is generated from this object information such as location shape velocity and attitude can be calculated since the 1980s early computer-based experiments with autonomous vehicles relied on lidar technology and even today it's used as the primary sensor for many experimental vehicles these systems can be categorized as either single line multi-line and omnidirectional single line lidar uses a single laser source and scans in a 2d plane multi-line systems however use multiple beams ranging from 4 up to 512 to generate height information from the surrounding environment more complex omnidirectional lidar systems perform sweeps in all axes producing a far denser 3d point cloud of the surrounding environment omnidirectional lidars offer the best detection ability for low reflectivity objects and have superior ambient light rejection as well as better penetration and rain fog snow and dust however they took longer to scan the environment when compared to single and multi-line systems this larger amount of data generated also makes it more difficult for an algorithm to generate a real-time output this is especially problematic for scanning moving objects such as other cars and people it's not uncommon for multiple lidar systems to be used in conjunction to bridge this gap between speed and detail at present lidar systems are also cost prohibitive for mass production use the veldine hdl6 64e 64 line system that was used on waymo's first autonomous vehicle for example cost upwards of eighty thousand dollars it's estimated that even with the cost scaling of mass production these sensors would still cost around ten thousand dollars each radar is also used for sensing on autonomous vehicles though it's generally used for distance detection vehicle radar systems come in three range variants short-term radar technologies used for blind spot monitoring lane keeping assistance and parking assistance while medium range and long range radars are used for obstacle detection in the range of 100 to 150 meters or 328 to 492 feet with beam angles varying between 30 to 160 degrees the long range radars used by autonomous vehicles tend to be millimeter wave systems that can provide centimeter accuracy and position and movement determination these systems known as frequency modulated continuous wave radar or fmcw continuously radiate a modulated wave and uses changes in phase or frequency of the reflected signal to determine distance radar is highly immune to rain snow dust and fog and it is both fast acting and a mature technology that is relatively inexpensive to implement however the data produces does lack the level of detail needed to be used as a primary sensor for fully autonomous vehicles while lidar and radar have been effective at navigating a vehicle through an environment and were popular approaches early on in autonomous vehicle research they lack the cognition needed to truly replace humans on public roads driving a vehicle requires a high level of understanding of the environment beyond simply avoiding obstacles rules practices signage and a comprehension of traffic behavior and dynamics must be understood as well as the ability to adapt to changes in local conditions such as in the case of weather or the quality of roadways simply mapping the obstacles of the environment without context using lidar and radar is far more similar to how an insect navigates its environment than a human reacting and responding to direct stimuli without deeper comprehension in order to push past this limitation of environmental perception one of the simplest and least costly classes of autonomous vehicle sensors would be combined with the recent explosion in ai development to bring visual perception using camera sensors to the forefront of autonomous vehicle research visual perception systems attempt to mimic how humans drive by identifying objects predicting motion and determining their effect on the immediate path a vehicle must take many within the industry including the visual only movement leader tesla believe that a camera-centric approach when combined with enough data and computing power can push artificial intelligence systems to do things that were previously thought to be impossible these systems also shift the cost of self-driving capabilities away from niche hardware and more towards computing and software development that also intersect with other industries at the heart of the most successful visual perception systems to date is the convolutional neural network or cnn cnns are a class of deep learning ai based on artificial neural networks that are inspired by the biological connectivity patterns and organization of the neurons in the animal visual cortex they are highly suited to analyzing imagery and an autonomous vehicle applications their ability to classify objects and patterns within the environment make them an incredibly powerful tool these systems work by first establishing a cnn architecture and software though this is often realized using optimized dedicated hardware the cnn is then initially trained using a data set that contains a large sample of real-world imagery that is pre-identified by human eyes the cnn is then trained on this data set and from this it can now identify these objects in varied forms from new imagery the more varied and numerous the training data is the more versatile the cnn becomes as this system is exposed to real-world imagery either through collected footage or from test vehicles more data is collected and the cycle of human labeling of the new data and training the cnn is repeated beyond the object recognition cnns are trained to detect temporal cues and stereo cues from an object over several frames and multiple cameras using a similar technique of repeated labeling and training this allows them to gauge both distance and infer the motion of objects as well as the expected path of other vehicles based on the driving environment in concept with enough training data the ability to identify and comprehend the environment at a level comparable to a human should be possible however despite tens of billions in funding and research current autonomous systems still struggle with identifying simple objects such as odd-shaped traffic codes and small animals and are easily confused by bad weather shadows and clouds at the current state of technology the fatal flow of autonomous vehicle advancement has been the pipeline by which they're trained a typical autonomous vehicle has multiple cameras with each capturing tens of images per second from even just a short period of real world operation the object detector system by nature would produce a large collection of mist or incorrectly identified objects with dozens or even hundreds of vehicles in a test fleet this pool of misidentified imagery grows drastically the sheer scale of this data that now requires human intervention and the appropriate retraining now becomes a pinch point of the overall training process while tackling this training dilemma is the focal point of ongoing autonomous vehicle research techniques such as crowdsourced identification and even ai systems designed to aid in classification training have yet to overcome the problem in fact many industry experts believe this will remain unresolved for several decades misidentified objects and inaccurate motion inferencing in autonomous vehicles are no trivial matter and can quickly lead to collisions and even deadly situations for example on february 14 2016 a google test vehicle equipped with over one and a half million miles of autonomous operational data attempted to avoid sandbags near a storm drain that was blocking its path with a maneuver that resulted in it colliding with the side of a bus in march of 2018 elaine herzberg became the first pedestrian to be killed by an autonomous vehicle occurring in tampa arizona hersberg was crossing outside of a crosswalk approximately 400 feet from an intersection where she was struck by a prototype uber self-driving car based on a volvo xc90 while herzberg was detected uber's system failed to infer hershberg's motion and did not apply emergency braking many experts had believed that a human driver would have easily avoided the fatal crash this incident led to arizona governor doug ducey suspending the company's ability to test and operate its automated cars on public roadways and ultimately would lead to uber pulling out of all self-driving car testing in california on august 12 2021 a 31 year old chinese man was killed in a collision after his neo es-8 self-driving feature failed to detect a static construction vehicle that same month a toyota e-pallet a self-driving mobility vehicle used to support mobility within the athlete's village at the olympic and paralympic games collided with a visually impaired pedestrian about to cross a pedestrian crossing due to misidentification even within the realm of human monitored driver assistance in 2022 over 400 crashes in the previous 11 months involving automated technology have been reported to the national highway traffic safety administration several noteworthy fatalities have even occurred with detection and decision-making systems being identified as a contributing factor while the argument could be made that human error statistically causes far more accidents over autonomous vehicles including the majority of driver assisted accidents when autonomous systems do fail they tend to do so in a manner that would otherwise be manageable by a human driver every human driver has over a decade of continuous training on perceiving the world around us including an understanding of motion lighting distance perception inferring intent and an unmatched ability to detect and classify the objects in an environment all before training on these specifics of operating a vehicle despite autonomous vehicles having the ability to react and make decisions faster than a human the environmental perception foundation these decisions are based on are so distant from the capabilities of the average human that trust in them still lingers below the majority of the public some proponents even suggest a hybrid approach might be needed where dedicated rules and roadways should be established for the limited capabilities of self-driving vehicles still many doubt that self-driving cars will ever achieve its promised future at least not until a general ai on par with human perception and comprehension is fully established this gap in perceptive capability is popularly summarized with a simple conceptual proverb can an autonomous vehicle reliably recognize and avoid a person on a pogo stick bouncing through a crosswalk the pursuit of self-driving vehicles by necessity has caused an influx of research and development into machine learning and its potential for visual perception the world of machine learning offers a fascinating perspective on how nature and now computer scientists harnesses statistical process to make sense of information in a noisy and uncertain world and with brilliant dissecting the concepts behind machine learning in an incredibly intuitive manner has never been easier brilliant is my go-to tool for diving head first into learning a new concept it's a website and app built off the principle of active problem solving because to truly learn something it takes more than just watching it you have to experience it brilliant has been tirelessly revamping their courses to introduce even more interactivity and with their recently updated artificial neural networks course written in collaboration with machine learning researchers and lecturers from mit princeton and stanford you'll explore how artificial neural networks learn by detecting patterns in huge amounts of information and it really makes you appreciate the mechanics of their structure and how flexible they are for data to processing and making predictions and decisions with brilliant you learn in depth and at your own pace it's not about memorizing or regurgitating facts you simply pick a course you're interested in and get started if you feel stuck or made a mistake an explanation is always available to help you through the learning process if you'd like to try out brilliant and start learning stem for free click the link in the description down below or visit brilliant.org for slash new mind and the first 200 of you will get 20 off an annual premium subscription