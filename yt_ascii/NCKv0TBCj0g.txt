okay hello everybody my name is and the talk will be about computers and computations as you see the title and i came from agh university in poland so we do a lot of computations but we also use c plus plus teach c plus plus we also cooperate with companies in the machine learning artificial intelligence so this this talk will be about computations and also with i hope a lot of practical things you can take on with you how to make computations faster and and data smaller or compressed so the idea is the plan is is here so i will start with what we already know about the for already 40 years old standard in floating point computations but quite fast it is not about the old standard it is more about new approaches new standards or future standards so the quest for for shorter formats and then i will mention about some some cool libraries to compress floating point data which you can use in your production code and new kitty on the block is posit arithmetic there are many new approaches to to to make make this this floating point computation representations more useful so one of them is is posit i don't know if this is the future standard but we'll see and also we'll we'll see some some libraries c plus plus to use posit and then we'll see some some new new directions quite new cool stuff from just last month in reinforcement learning how probably some algorithms will be developed in the future with help of artificial intelligence and reinforcement learning so as i mentioned this is me and the permitters computer at our computer center so just in the period of something like 20 years we see the still growth of computational power of of computers this is parameters in in top 500 computers we we try to keep it on the list but if you observe this memory line it is also growing but not that fast so what what does it mean it means that data transfer is a bottleneck at large processing scale and we live in the area of big data we say in university data is the king yes so whoever has data can can do whatever they want so time is quite specific now so every bit counts so what can we do so mostly these parts of my talk will be about how to compress or how to use different formats and safe and data and transfers so well starting from very basic basic basic level what we teach and the first meeting of c plus plus classes is how to use a some some values and the fasting is just just start with with some examples in computations so usually students start with with some sliders that they've had some code and they are surprised what is going on here and the fast solution instead of explaining what's going on here is just to take doubles and and we are done so we we go quite easily from integer computations to floating point and we are done okay so in some in some cases that that's okay but if we are processing for example images video and we do some computations you know about convolutional neural networks we do a lot of computations on pixels so going from 8-bit pixel or rgb three bytes to three times eight bytes of of doubles is is just pure waste of of of of space time and energy so what we can do more but before we go to this this part what what we can do let us take a look at what we have right now so we have this ieee 754 standard if you wish to see some more basic things i was talking about an a few conferences also here but here is the the the previous look only about this the standard and some some pitfalls we need to to to refresh from from time to time what we are doing also you can find some some material in this book for for beginners and intermediate students i published this with wiley a year ago so there is a entire chapter seven about computations in a fixed point and floating point but let's let's start with some history so once upon a time in a galaxy there was no floating point standard so what what people did at this time so computers use many different representations of floating point numbers and the lack of floating point standards what it was really a hard problem in 70s and and up to 80s because well you could not not move the code from one computer to the other computer and have the same results with the same code so well like usually in such a cases major companies that they had had their own standards like hp or ibm so but it was it was a problem so this floating point compatibility was in desperate need of standardization and indeed we we have we had it so we have the seminal paper by professor kahan why do we need a floating point arithmetic standard and you see this is more than 40 years ago so well just just citing his his words numerical software is costly and most programs must be portable over diverse machines and we need something simple not so capriciously complex and he he he mentioned that he devised this the standard and and the standard is is what we need but he he ends his his paper allows the proposed standard will not guarantee correct results from all numerical programs but the standard weights the odds more in our favor so he was aware that the standard is not maybe perfect or just just optimal but this is what at that time people came to and here is what what we all know from the school is just just will you need this this picture again when we would be talking about posits so therefore i put it here so in the in the just going briefly uh the the standard defines many many many formats will return to this formula again so we have single precision double precision so single precision format occupies for for bytes and we have this precision 24 and 8 bits for exponent it usually is is encoded in in our languages like like float and in c plus plus we have double which means double precision in this ieee standard eight bytes and 53 bits of of precision and 11 bits of exponent extension that position which some compilers implement like long double or even quadruple precision with 16 bytes also sometimes implemented like long double but this is machine specific if if one or of the second is long double or even the standard only says that it should have is at least as precise so so float and long double as double respectively so is the standard says that that the longer the longer presses representation is is not worse than the previous one but it appears that in many computations it is a waste actually going higher and higher this this was for 40 years we have just well trying to make computers wider buses and have more beats and in the computer world so we we came from 16 bits to 32 bits computer 64 bits maybe 120 and so on but at some point we had to stop and reflect what what we are doing and in many many cases like with the slider even the computations can be done and bright and in many cases like deep neural networks with millions of parameters we need floating point computations but 16 bits is is enough so we have this way back to to 16 point standard just introduced in 2008 so we'll see in a moment what what possibilities we we have as programmers so returning back to the to the basic formulas of this this standard so if we have number of precision beats and q bits of exponent that the value of of of of this floating point a representation is is like this we see that this is sine magnitude representation to some extent because sine is is separate we also see that that with this representation we will have two zeros minus zero and plausible it is also waste and we will see more such gaps in in the in the space of of of values in a moment and also amazon signed a significant significant called mantissa fraction b stands for a base and it denotes the exponent and this exponent is biased so it is it can be negative and positive but the value will be always here encoded like like positive value and for p digits and b base a value of significant is is given by by this formula so we see that that last position weight is this depending on dp minus one and we have also this d0 value here which is specific and if we make a closer look at this representation we'll see that a representation of numbers not unique so if we don't constrain a couple of things here there will be many representations of the same value so the committee decided to normalize forms so they decided that the zero will be always not zero this first beat and this way we have so-called normalized value and because the base is the base is two so we have only one choice of of of one in this case so we don't need to to store this bit anymore so we assume that the zero is always one and this is called the hidden bit trick just not not storing this this bit we'll see in a moment and but with this representation if we go back with the formula and we set this bit always for one so we have immediate problem we cannot represent zero quite easily so we we we have to wait if we have normalized form and and just a unique representations or we have some some problems around zero and really in ieee standard we have problems near zero so we have special encoding called the normalize which in practice in some on some platforms means that computations of a really tiny value is really slow and as i mentioned this exponent is is biased so it is it is shifted so it is always a positive in representation so the best thing to to grasp the idea is just to make a a simple experiment so let's make very short floating point representation with all this information i i provide it so we have this this formula again for convenience and let's assume we have only three bits of precision the base is again two and we have only exponent in the range of minus one zero one and two so if we draw such a what what values can be represented with such a tiny representation we see a very specific groups of values for each exponent so floating point values are shown in red so what is obvious from this from this picture we have no representation except zero we have no values near zero quite quite surprised for each exponent okay we have four groups of for each exponent and then they tend to gather together but once we go from a group to another group the spacing is larger and larger and look at this we we need to represent the real values and actually we have only couple of points which are precise everything else what is on this horizontal axis will be approximation so that's well for us it is obvious we take double we do some computations but actually we are doing only approximations of real values so it it has consequences so spacing within a group is the same but going from a group to a group it increases by a factor of the base so this is another debate what the base should be we you will see some some approaches in positive values okay so you see if you go here the this the spacing is growing up and up so we you will see that that the the quantization error will be bigger for for bigger values for for bigger bigger exponents and the the conclusion is that in floating point a real values cannot be exactly represented so due to round off errors the associative principle doesn't hold so we the order of of operations matter which is not in mathematics so summation for example multiplication is associative commutative associative but not not in in general in in this domain of our computation so we are doing computations every day and we have to deal with this and you will see well in my previous talk you can see that if we for example adding values from a very small to very large if you solve the values should it change the result well theoretically normal but in practice if you start from wrong value to large and to go to the smaller ones you will lose the value so the order matters just just make such an experiment generate some some some values from the small one to the big one and then sum them up with accumulate for example and then sort them and sum them up and the second surprise can be in in concurrent word of power programming you you split your matrix into parts you sum the values and always you'll have different results why because there is a preemptive mechanism changing the order of summation and well so you you have to understand this and see if you have problems with with some synchronization in your code or this is just this phenomena of of adding the value and we have some special values positive infinity negative negative zero positive zero and so on we have special representation for not a number all exponent values of ones are used to represent a special something for example you compute the square root from from negative value you will obtain not a number but this is the waste of time here we have space for improvement because the the standard says okay exponent values are once but we have only eight exponent values what with the rest the rest is is wasted in this in the old standup so the other standards are just just use this gap well and and yeah encoding value of a double precision yes i love the dimension that this is this is biased and the special the special computations are for the normals so there is a special encoding for values zeros well obviously we have positive negative zero which is not no good but for for tiny computations we enter the other word of the normals so the algorithms have changed a little bit and and and we we compute with different representations and i i found code in dsp digital signal processors for example when people just trying to avoid this this space because it was really slow in computation because those are different algorithms they added 0.5 did computations in with larger values and then moved back with the result so this has some consequences as well and naturally we have roundings because of of these gaps between the values so well you know this default is around to nearest but we have also optional around around to nearest ties around away from zero around to toward the infinity to the the larger value around down and also around to have zero so cut off fractionals and actually this is why the first line of code i showed you didn't work because well if we divide integers we have a still integer of just if we have an in we convert double to to integer it will have the the fractional cut off and and some some some short examples of representation so you see this is biased by 127 so for example minus one how how to represent and the and we have this hidden beat you see we don't store it here the one so if we put into the formula we have we have minus one just just a minimal value because i will mention in a moment minimal value so minimal is all zeros and and exponent is one so we have something like this again hidden hidden beat is just added externally because it is always one this d0 but we can also read it out for for flows i will mention in a moment this this numeric limits and numeric libraries we have in c plus plus and also what is the the maximum value we have 2454 here and because all ones are reserved for for for special representation so we we have this this this value actually this is not the reciprocal of minimum so if you multiply minimum by maximum it it won't give you one surprisingly and we can read it out for for float or for double or for other values and well you can play with this you can find a lot of calculators or you can ask your students to write if you're teaching this is good exercise how to convert to this beats and play with that okay so i'm just finishing with with the with the standard so again what is the well around we have random errors we need to remember about the lack of associative law order of operations matters actually and well when we are doing computations we we need to remember about special value which is called machine epsilon it conveys a value represented by the lowest bit of the significant and actually it is because we don't have this close to zero representation this is from one to the next when you change the least significant bit so this is the the distance this is epsilon it it it plays the important role i will mention in a moment okay so and well it can be used to to assess the threshold when you are doing computations and we have to remember that adding values with different exponents leads to large errors and also subtracting close values can lead to severe cancellation errors so if if you subtracting very similar values why this is in in another talk so i i will just just put some well recipes for you and we finish with with the standard so what we do in computations in math we are doing iterations and we are checking if the algorithm is is converging to a value so we are checking if the new value is different from the previous value well we when we train neural networks we check if if it is okay close enough to the to the result so we always check checks such an absolute value so but what we need to remember that we cannot measure less than the spacing between consecutive floating point of representations we cannot go inside so this is on the right side hand side we use this machine epsilon value which i just mentioned and maximum of this of this value so if this is true this condition we are close enough we cannot well we are just in the one group of of values and the second thing we always divide flows by floats so we all knew that don't divide by zero but this is not enough don't divide by two small value so what we can do if we need to compute correctly the result c and we have a to the b so either in our domain we can say okay my values in the dividend won't be larger by a max because i know i'm computing something there are some financial things of neural networks or image processing or whatever then i can compute b what is acceptable b b must be greater than this a max by k max value which i can read out from this numerical limit library or i can go the normal way well normal well i mean the most popular way if i i can say okay my divisor won't be zero but this is not enough my divisor must be larger by being mean then a indeed must be must be less than this b mean times k max so we can read them out from this numeric limits template below here so these are just just why is that as i mentioned this is on my web page in the book in the presentation so but these are just just okay so we we are done with the with the standard maybe too long but that's okay and the quest for for 16 bit quite fast this is just okay realize that we need 16 bits because we we need to park values and usually we don't in many many domains we don't need such a precision but we need to to put all data big data into the memory so why we need a binary 16 so we can make it smaller there are many custom versions you can look them up on the net so so this is the situations like before i triple seven five four standard the the big companies are devising because of lack of the standard but also there is this just just a quest for for adding to the to the standard 16 bit so maybe it will be short float yes so it was proposed in 2016 but i don't know if if there are works on on this or not many many approaches here well you can quite easily look them up from from here i will just google them but what i found interesting just just just first shot to try on is half library and here is my example just the hash include this half this is a header only library and you can you can compute for example area of of a circle of something and you can even try two yes we finally have constant in c plus plus 20 so we can we can try to do this with p v half but wow it might not compile yet so here is the way around we can call arcus casino on this and here is a simple function i wrote which is a template function you can read all the values all the parameters of of of of these floats or other numerical values from the limits library and once you call it you can put the template whatever you you wish and you can read epsilon radix digits mean marks either it has the normalized or not and so on so when i use this function we have this we have this results so what is interesting to see here is this epsilon value so this is the the minimum spacing once again from one not from zero from one to to the nearest larger value and and some random style and and so on and so we always when do doing computations we need to write code which automatically checks if dynamics and the range of our used values is is the same and the same platform because it can be different implementation of on on some platforms so we can read this just just using this fp limits or just just limits library okay especially if you are going for example from long double of things like this so here are some some some information about this library just check it out is is is is good but it is slow because this is no harm during support so so that that's the problem with all all things like this alternatives we have nvidia flex float b flow by google intel well but it is usually worth it considering if you are doing massive computations maybe my data can be 16 bit long and you can save a lot because in many many computations like neural networks image processing we we don't need more well in many cases we need but so you can use also fixed point arithmetic in some in some cases but well a lot of research into deep learning especially on the edge platforms fpga so we need really to squeeze data and to squeeze data we can use the library this is some cool stuff floating point compression another library another possibility you can try compression and every format but here is the library for ieee 754 and i found useful two libraries that fp proposed by peter lindstrom in his work from 2014 fixed rate compressed floating point ris and from the very beginning i can state we are using this library and we love this library because it is specialized for floating point if you do zip or some other compression you won't get good results because they are not optimized for this type of data and this is optimized for floating point of representations so the main their main observation was that multi-dimensional data tensors images so on elements tend to cluster in sub cubes so what i mean is that if if you take a a closing pixels for example they they probably will have similar values except they are on the edge of something of the image if you have looking just at images so they they use this feature in in this compression so the the more structured like a tensor like a multi-dimensional cube of data is and the more similarities you have you'll gain from this from this library or for example for visualization so for example if if we are processing a multi-spectral data so it is not rgb we have this rggb normal and more spectra so we have cubes like this of data so in in such a cases usually the small cubes tend to have similar similar values to some extent okay so what what's the idea near lossless compressions game is purpose so it maps small blocks of form to power of d we'll have the dimension to a fixed number of bits per block defined by the user so this is the virtue of experiments we need to experiment in in this 3d case this is for example four by four by four cubes each such a block is stored using the same user-defined number of beats and each block is compressed in five steps so align block values the common exponent so this is the idea if if they are similar they have common exponents so we can take the exponent out just store the data and we can we can save on this so convert floating point number to a fixed point of representation around orthogonal block transformation to decorate the values just just for better compression like like in in jpeg we are doing the same order the transform coefficients by the magnitude and encode the coefficients one bit plane at a time and this zfplows for random bit and dried access to compress fp data if you know how the jpeg compression works it is similar to this this point if you need more details to understand the paper is great just just to read but you can try from experiments in a moment i will show you some some code so what is the characteristic it is a c plus plus wrapper to decompress ri primitive to the user the the vector for example appears the same cash to optimize the frequency of compression and the compression but it is in internal cache not not the processor case in this case of the library and ztfp is strongly limited to regular data so if you supply the the library with random data don't expect too much or you can expect even worse results applications well i already mentioned in computer graphics machine learning and so on and nice nice license so there are five compression modes expert fixed rate fixed precision fixed accuracy reversible lossless well you well if you really need to to have that but you can still try to find this common exponents and and have some some compression better in this case five than i would say using simple zip library and it works in parallel and it has different different apis for c compressed ris and the code will be the the best in a moment some some parameters like this but the most important is that we can use in all like here look at this we have this vector we use this mercen twister random engine here uniform real distribution we generate some values here so once we are done with this we define these parameters for this library to to work here we define bits per value to starting compressor representation so we have crossroads and this is the parameter of compression so you can play with this right and we have this ri 2d my zfpri and i can do this okay it has the same number of elements but what we expect i can copy it and what i expect it will be [music] compressed but at what price so to to to to find out the price transformer reduce is nice function because it can work in parallel so we start from begin and and we have to supply to lambdas for from two operations of adding the values of just computing differences square differences and adding the values so this is how transform reduce works so we have this this fork and join part of of operations so once we run this we can we can ask for the number of bytes in this compressed data okay we are done with this and now we can print out the results just just simple things so we we see just compressing this as i mentioned these are random values so i don't expect to to to to to good results but anyway i i have this compression ratio quite quite large and this mean square error of 0.63 well it depends on the application if you are doing some some well financial things well maybe it is it is big value of 0 63 of euro difference in your bank or not i don't know so you always to be very careful with this yeah so you you have to measure but if you are doing visualization of your data that's perfect no one will notice if the pixel is different by 0.63 okay so this is what what we have and we have another library as z you can try it out as well but we found easier to use this set fp in many cases alternatives well we what we are doing in in our let's say research stuff is just just processing tensor so i wrote a book 10 years ago about this so what we can do if if we have a tensor you we can do the composition of a tensor and the the composition is always the compression of value so what we did more we did this zfp and this already compressed the composed representation so we can gain a lot and now the almost last big thing but really big is positive representation so we had standard floating point we had 16 bit addendum we had compression and now we have quite new we're going to help them domain of new representation this is not i3 police standard this is positive it grew up from many research attempts and you can find in literature i will provide you in a moment so what is posit is just trying to fill the gap especially of this waste of beats and the limits well one thing you'll see in a moment what it fixed fixes is it refers from the same number of bits per exponent the number of bits of exponent can can be larger or lower so this is the the key point but there are many many details you can find them in in this paper from 2017 gustafson is the the main authorian unimoto so what we expect larger dynamic range due to variable land and coding of exponent and this idea comes from gollum price works 40 years ago or 30 years ago more fraction beats if small exponent is sufficient so we can trade off the number of bits for exponents with the number of bits for a fraction because we have fixed computer world for example 32 bits but we can we can change we cannot adjust this in ieee representation with double float you have eight bits and that's all you don't need them you need more precision no no way you you have to change the long double and waste more space more memory never overflow to infinity or underflow the so-called tempered arithmetic not a number indicates an action instead of bit pattern but there is modification in the libraries i will show you we have this not a number because it is useful because this posit is designed for easy replacement of ieee because on standard because otherwise there would be a revolution so we we need to rewrite everything no way so if it is a replacement if it can go just one for one for for i represent that that would be the goal so this is a modulus in this direction but as far i don't recall any any processor commercially available with posse we have a lot of scientific work fpgas with posits we also tried this in our labs but no no not not not so popular yet positive environment the fused operations so the fuse operations are briefly not going into details the operations which guarantee the correct rounding of of for example adding the values and only the the result is around it okay so once again this this image so this is what we have in standard app and what we have imposit there is new thing we divide it we split the qubits of of exponent into so-called regime bits and exponent bits and they can be defined and adjusted you will see in a moment so they they go here and here is the the the the the the the whole story so exponent can be variable length and significant can be variable and so if we don't exponent we can have more beats for mantissa if we need more for exponent a regime is fixed and you you will see in a moment it's wrong it is just just another multiplication factor so so well certainly we have to be aware of of of the value so this exponent are called es beats in all publications and posits so i use this so here is once again the old thing i 3.754 the formula is like this if we go to the positive formula it is not much different the difference is here in this rectangle so we have your seed value to the power of k times 2 base to the power of e so this is what they change so the two things variable number of bits so okay the the the word is fixed for example 64 bits or 32 bits of 60. but we can we can change exchange the number of es bits with precision resume is still the same and this is a in in a little bit different different realization still we have this this representation with with one here but you will see in a moment that we can we can quite easily go with this tempered arithmetic avoiding this sub numerals which are this this the the problematic in in in old standard and here the mantissa is is computed in the normal way so we can have this this so this is deposit how it works and well here is a complete positive value well from the page pair of gustafson because the libraries are a little bit modified so we have these representations the thing is we don't lose bits so the sign bit is zero for positive number one for negative number and what is important is negative value we take two's complement so this is not like ieee sine magnitude but this is to complement before coding the regime exponent's significant fraction so we have one zero and now how this regime beats work this is this is used only here to compute this uc to power of k so how do we know k and you see it there are a couple of rules so we analyze only this regime beats now which are fixed you remember yes is is changing this is it a stream of zeros or ones is terminated either when the next bit is opposite or the end of the string is reached so if we go like this if bits are zero then case is is minus one so we have for example one two three four zero so this is minus four but if we have one two three and the transition to one then this is minus three and so on if they are one there is m minus one yes so we have for example one and one then is minus one so we have one value k is one and the regime indicates a scale factor so this is the the key idea okay the variable length and the scaling so so i can divide or multiply the whole thing by by bigger or smaller number this way i can i can have fractionals or bigger levels and you see it is computed from from this k and this is computed like 2 to power of 2 to e e s so you see it is computed from from es bits but then we have also this this k parameter so more parameters and this variable length so you you see here if es exponent is zero one two so we have from this formula and taking you see a two so we have even this uc can be can be 6500 for for years four some some examples in the moment so exponent is regarded and unsigned integer there is no bias like in old standard there can be up to es exponent bits a tape of the accuracy already mentioned number near one magnitude have more accuracy than extremely large one extremely small numbers if there are any bits remaining after the resume and the exponent bit they represent the fraction mantissa will so there are no sub number subnormal so we really see some examples here if we take only three beats nothing more for reposit we have such such a value positive value so so this is you see these two to power two to three is eight usually we represent the values and the horizontal line but well just just recalling the formula for like a helper the better is to represent them on the on the circle so you see here from k1 we have you see it from k0 we have one and four k minus one we have one divided by you see it so this way we can have either fractionals or larger values in the uniform way and if we go with larger values there are some rules i don't want to go into some details if you need them you can read the paper but well there are different rules of just adding bits to precision and what will be the new value the new value so adding one creates a new value between two points and the circle and we have some interpolation rules one is that the value is is a mean position divided by you see it if the value is between other values and different more than one then geometric mean is computed from from these values and for midway between closest neighbors values arithmetic means is computed why is that it is in the paper we don't we don't go into some mathematical details but this is how it how it works so if we are adding here the values for example we have 116 and we are adding in the in the middle value we are adding one to to the end then we have this geometrical mean which is 1 multiplied by 16 and then square root we have four and the same for fractionals so this is if we add more we have having these rules they prove that they they they they they they feel this space as as needed so we have three bits like in the previous example four bits or five bits so we can go with the precision up to the up to the limit of our of of our world so here is the example if we have such a value sign regime exponent fraction how we compute it well it is it is sorry it is this value of uc to -3 because because of of this minus 3 because we have three zeros in transition to one then we have exponent 2 to the power of 2 and this black is is is this fractional so despite having one bit in front of this hidden bit we have quite small fractional without the number the normals thanks to this scaling by you see it in in different so here is the summary of posits so we we see that variable length so we we have many gains so actually it is a game winner so hopefully we will be using this disposite soon well in other words i i cannot put some advantages of using iwp.754 over posits expect except they are not implemented in hardware yet and some software implementations you can google up then but what what i use is the cpposite which is quite useful for for me and for for deep networks we are using and here you can find the library cpposite and here is the example you can include deposit make posit 12 bits well remember the the entire talk is about saving beats so here we have not 16 even we have 12 bits and we can do a lot of computations like this but remember okay we can multiply them we can convert to float to to to to compute the values we can compute area of of a circle but please remember of this well we will we well have large compression but we lose some accuracy so we always must control dynamic and precision in our computations certainly just if we have 32 flow floats and we substitute them with posits 32-bit we expect much better result and this is proved in the paper so i don't go and you can read about values of of these posits and the future now because time is short so just matrix multiplication just to buy two matrices simple thing we we teach this grammar school maybe maybe not okay but it is quite simple well if if we make such a notation it is like like this well if we write down all the elements we we have this factors and and terms so we see one two three four eight at multiplication so simple is like this so this is also what we teach when we start programming maybe at the universities so people do multiply matrices and for 50 years i'm a little bit older than than this algorithm but we avoided one multiplication well only one multiplication is well do we care it is more complicated so why a single multiplication matters because this a11 a2 b11 can be another matrices so this can be recursive multi matrix multi so if you do this and you multiply huge matrices so you avoid one maybe not dutch that huge multiplication but matrix multiplication so it really counts and for years it was said no better way it was proved that this is optimal value but really can we do better and look at this just yes we can just just a month ago nature you can you can download it for free discovering faster matrix multiplication algorithms with reinforcement learning so this is the future probably so what they did they took alpha tensor the the one reinforcement deep learning machinery which play chess and go and and one with every human being with the masters of golf of chess and they transform matrix multiplication to tensor decompositions this into single player game agent training alpha tensor to find efficient ways to to win this game and this way they obtained so what they did is such a network so this is reinforcement learning gradually it provoked by playing the game with itself with some guides so it was a well again network neural network it just was trained on the played game and synthetic demonstrations it took an input tensor and output this decomposition of a tensor updated the model here the actors just just neural network using monte carlo research game the game was played and put again to the network and apple couple of days or weeks they discovered new winner well two by two is already removed by stressing that this is seven but for four by four no one found before so we have new new world record yeah just one month old so probably we are not well we are programmers researchers but machines will do development for us so ai will play a fundamental role in the field of algorithm discovery not only to improve the existing ones but also to discover the new ones and maybe discover better implementations so we programmers scholars will not be needed anymore we'll see maybe not maybe not okay so conclusions well literature is big literature you can use wikipedia for all these things quite easily float formats other things you can have these calculators or ask students to program them and yeah and this is the seminar work but do not not well some some good papers by and here is the the seminar with about posse's good stuffs yeah another things about codewords elias the the work by gustafson is based on and works 1975 alias yes goldberg is quite useful tutorial and floating points well new algorithm 908 to exact summation of floating point values quite fast and we have more we have some libraries we have this fixed rate compression arise in in the paper we have some papers of of us so you can google us app and there is this this book in a moment discovering faster matrix multiplication this is 2022 just just very fresh reinforcement learning so conclusions yes that's good we have the old standouts we are indebted to to the the the the fathers founders of it but the entire modern numerical word depends on it but we can do more but we must always control dynamics and precision of the numbers whatever we are using because we are always in the approximation world when you have to do computation new technologies launch new demands on the way data represented in machine learning data transfer constitutes a bottleneck in a lot of domains yes new directions data compression new formats but mostly posits i would recommend new hardware we need quite badly i count on this risk v or 5 implementations this is new standard for for making processors open open standard and we are still ahead of having new formats so maybe we can use reinforcement learning to to discover some better formats i don't know and artificial intelligence so that's that's cool stuff that's really something if someone is asking me what is artificial intelligence that's it no no just well detection of faces the neural networks okay but if if something is really new from us humans researchers this is this is that so that's that's all i hope we have some time for questions foreign by the way this is the new book for teaching c plus plus but also with power programming with posits maybe not but with all computations in in arithmetics in fixed point floating points so just please use the code yeah my question would be if i got it right we if we now keep the same precision of the floats we can represent or we can do our computations with with posits with better precision but we tried it off with being slower right because the hardware does not supported in general is it correct yes yes exactly or we can keep the same position that we have but with less bits of the trade so in a way it's a it's a space time trader implementation issue so once once someone makes a processor with positive it is a little bit harder to make hardware implementation of possible but it is possible disproved in the fpga is probably four years we have fpga platforms with posits working so this is for a company probably there is no pressure in the market just just to change it because if something is working we are flying to the moon and the further but we can really change how we compute and how we how we save the space just saving and beats and the small follow-up question is is this already a thing in like these tensor processing units these tpus that for example are used for for tensorflow are they they are also using a reduced loading by format is this good question they implement usually their own limited precision complications so always you you exchange for example openmp have very nice ability to download some computation some tasks and to gpus please remember the the use a lower precision and gpu so the result can be burdened by by by more branding errors six thank you for the talk it was very inspiring i'm actually curious about similar as previous question about hardware support for posits i'm aware that if we we will for a long time not have any real hardware especially on consumer devices and browsers but i'm curious about have you have you explored or even tried to implement a library that would basically just use post it for data storage but then use vectorized instructions and cmd to convert data from the storage into something that is compatible with today's cpus perform the computation while keeping everything either in cache or in registers and then efficiently return data back into the posit format and store it into our global memory this is still ahead of us so we we i'm just starting some tensor decompositions and well our group of fpga enthusiasts at the university they they plan to use posse finally but the beginning implementations are using ready floating point cores with some limits but this is the general your direction actually we need to go this direction i hope it will be faster especially all this nvidia and all this well i know that they already did a lot of research on changing precision of value because when you train the neural networks offline and big machines then you download it to the gpus so you need to convert to 16 bits and don't lose the well operation of the of the machinery so they if you google app and nvidia especially they they did a lot of libraries for converting values and and training networks and i know that they are working on posits if they have something ready i don't know but i would expect something quite fast because yeah i agree on that term for training on some in some data centers some big machines but from your talk i actually got an impression that the best use of bossets would be on a tiny devices on for example machine learning inference in web assembly in web browser or on small android or rs devices which will definitely not have harder support for posit for a very long time well i'm i'm a little bit older probably i remember working on the embedded systems not having floating point at all so probably some of you remember this these times but but they changed quite quite fast and all processors now have not only core processors routing point processor external devices but even each car has has embedded so this is progressing quite fast like with this risk of v or five technology i i'm sure that many universities there there are works so this is the only question if the mainstream will will catch it yeah yeah thank you i hope it will but one more thing we can use it we can play it and we can have the two versions for example well once we have hardware we can use our more optimal version for that hardware thank you thank you foreign