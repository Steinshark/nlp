the hidden performance cost i forgot that word existed of node.js and graph quel okay so no. js and graph queel are popular technologies for building web applications but in my experience they come at a certain scaling and performance tradeoffs to be aware of i wait hold i thought bun made made javascript faster than rust so what's this thing he's talking about incorrect take bad take graph qu's modular structure generally leads to code that instantiates excessive promises which degrades per request performance benchmarks show as much as 2 to 3x latency increase yeah yes about 3 weeks ago i tweeted that promises are actually really bad for performance okay no. js is known for its non-blocking io operations yeah that's what everyone keeps telling me if you say something like this to the modern javascript engineer if you say the phrase don't use a promise you will literally get people to be like you mean you're going to write sync code isn't that slower and it's just shocking that there's not a middle ground like oh you mean you want me to handle this with without using you want me to do async without promises it like it doesn't exist in the brain at this point it's shocking all right all secret is work and no js happens over an event loop thread other than a few isolated multi-threaded features like worker threads and of course garbage collection largely happens async if you read the what is it oroo oroo what's the name of it or oro oroo something o o something something something is the name of their sweet garbage collector it's awesome with the event loop is managed well and the io is is a true bottleneck no js can be very efficient scalable technology in general javascript is very efficient if it doesn't have to do a lot it's like your classic way to fix things how do you if javascript isn't doing a lot then of course it's pretty fast what would be a sync without promises in javascript a call back i this may be hard to comprehend but taking out a few promises taking out a few promises and going back through the event loop can actually really help people forget about continuation passing style people forgot about every i know it's are callbacks really a sync if your date never does does them shut shut up oh my goodness on the other hand if a request does a lot of processing on the event loop it will block other requests on that container no js applications are particularly susceptible to sporadic performance issues due to noisy neighbors a lot of the sporadic performance issues actually typically come from garbage collection others he let's see other heavy request handlers that overlay consume consume the event loop additional graph quals resolve like like if you run on a single core so if your instance is single core yeah concerned about performance don't use no. js it's really that simple but if your if your instance runs on a single core you get massively hampered by garbage collection right a garbage collection is so efficient these days because it can run on more than one thread additional graph quals resolver structure can result in more promises overhead compared to rest end points which may cause suboptimal user perceived latency if not managed carefully fair graph quel oh no it's that thing here continue reading just let let a man read graph quel enables a modular design for apis for example we define a type in our schema and define the one resolver for that type regardless of where that ver appears in the graph user query query the user this modular design is great for developer experience but leads to promise heavy code okay yes we know each promise adds a minuscule but non-zero amount of work for the event loop which is discussed here to demonstrate let's say we want to write a feature that retrieves a user's item let's see that retrieves a user's items wow so many s's right there on a shopping site we might build a rest endpoint like this user items details this would be power powered by a few squeal queries get some user get this you know typically maybe maybe we would do a join you know typ you know i always say we should just write squeal instead of using an omm i might be mistaken a well structured rest endpoint would have some relatively simple code that makes these database queries and massages and massages the data back into a desired format we would have no more than a few promises involved and resolve in the request life cycle in graph quel we would encourage to write query like this user items id details item other fields if we have a well structur let's see if we have it well structured as graph quel resolvers we might have type resolvers for users and item details yep yeah by the way this is called creating a chatty protocol by the way so for those that don't know chatty protocols it's where you start making a bunch of small requests to something else and so chatty protocols tend to have like a distributive degrading performance problem right because obviously making one small request not a big deal not chatty not with a d chatty right not giga chatty no not no one would say that unless if your protocol is very chatty what executed a graph quel query with nested fields will result in a promise per field being created such as user get user item user and items const id other fields get item by details oof as we use down data loaders to prevent the n plus1 query problem this would translate to the same squeal queries as we described in rest endpoint case so the io cost would be as optimized as possible but we would create one promise per item in the loop and each promise adds work to the event loop a promise per field is pretty crazy the funniest thing i've seen with graph quel is the fact that it ultimately ends up as js api common queries command f if we only had something like that if only we had some sort of structured language in which we could make queries with that's really what i think people want you know structured queries language i'd be squealing for it slq oh yeah let's call it slq structured language query i like it i like this tj we're on to something you know a lot about language servers we could develop one together i've written a benchmark of gra quel server that returns user to demonstrate the impact the overhead increases as we increase the number of promises involved we choose two graph quel servers apollo server plus express and mc mur mercurius mccarus mercurious mercurious oh i'm so curious mccarus the common mccarus by the way express is actually the worst framework ever created like i understand it was the first but express is so bad at performance it is shocking how bad express is like express just doing basic requests not a hot take there's nothing hot about that take it's crazy my innovation would be that instead of putting select before before from you'd put the from you'd put it after from and squeal and and s would be so oh dude it'd be so craigm from this field select that out i do agree that actually is the superior way are you saying that express is not express at all dude it's not it's not express is absolute dooo and i'm not i'm not saying that the people who invented express are doo i'm just saying the performance that has been created in express is doo okay because it is it just is all right the benchmarked queries return the same data but one wraps every field response in a promise and the other returns data synchronously we return 100 items per user number of let's see number of users return sync user milliseconds okay let's see data loaders plus promises okay i'd like to investigate this more because i've done some playing around with this and it can be really bad this is very interesting though like look at this that's pretty wild this is pretty wild mous oh i'm so mous okay i mean again i i never trust other people's benchmark numbers but i think there's at least something to be said that it's much much larger right that maybe maybe it's not maybe it's not good maybe it's not great but this shows that there's a huge disparity and that there's probably a big problem there we see that wrapping each user an item in a promise causes two to three increase in request latency an invalid criticism here is that real world graph quel resolvers perform io so the overhead will reduce significantly as a percentage of time taken by the resolver a well-tuned database can perform two squeal queries to return turn 10k items in less than 100 milliseconds you could just do one baby which is reasonable small percentage of high okay i'm i'm actually a little stuck on that whole two squeal thing i'm really stuck on that i'm going to let it go everybody we're letting it go together everybody in chat say can we just all quote is it elsa can we all quote elsa right now and just let it go let's just let it go let it go cho all right an invalid criticism here is that the real world graph queel resolvers perform io so that the overhead will will reduce significantly has a percentage of time taken by the resolver a well-tuned database can perform two squeal queries to return de gay items in less than 100 milliseconds which is a re i mean all of this doesn't make any sense like this phrase doesn't make any sense in general right which is a reasonable small percentage of high latency caused by the graph quil server here regardless of express or mous real world code is even messier we might check feature flags or perform other async work in a resolver which further increases the number of promises the event loop has to pro process all right so can i give you a quick reason one reason why the event loop can be a little bit difficult and people don't really understand why it's bad can i give you a quick little understanding of it let's just let's just talk about this so the event loop how it effectively works looks something like this okay let's go like this let's pull this thing oh gosh i'm i'm i'm i'm no master at at excal draw but we're okay at it all right so the event loop does something like this right it looks it looks a little something like this where it pulls next task off q and check micro task q right or empty microtask q so you can starve your threads by having this so if you have a bunch of microtasks or things that run right away you can kind of starve yourself right and so you get you know it can be bad it's a loop if one would say now what does a what does the task q look like well every time you do something like set timeout what will happen is that there's there's a linked list that exists somewhere and every time there's a set timeout it does this right it adds another it adds a item to the list so let's say that you let's go like this i'm going to put four items in here right we're going to put four little items in here there we go and let's say that you are this item and you are the first to be executing okay you get this beautiful chance to be executing and the rest of you are going to be red items all right and let's say that you are going to do a promise and this promise actually resolves synchronous cached work okay so all you do inside of your little promise is you have like a little async funk that checks for some sort of cached value cached value like you know if cashed return right you get the cash value it returns it back out pretty simple so what that means is that when you do this you go check your cash value all right you got a cash value return it well what's going to happen promises resolve next tick so this guy's going to be let's see let's go like this let's take this guy let's take him off the next time you get to run is now at the back of the queue so now this person's going to run this one's going to run this one's going to run and then now you're going to run again and you're going to have another chance that's how the process that's how the event loops work work and so what ends up happening is really simple items like this you throw an async on a function that doesn't need to be async guess what it actually is it will run significantly slower because you could have a bunch of people in in line ahead of you a ton of people they cannot resolve in they cannot resolve in the they can't resolve in the in the like immediate queue if they resolve in the immediate queue you would you could starve yourself you could sit there and just starve yourself over and over and over and over again and so a call back right like so if you use like if you use a callback to know when say you write out to oh transaction stay open too yep it's going to stay open for a while so if you use like a call back to write a file to disk or you use a promise what you get is when the thing is done right when writing a file is done it calls the call back that call back is called synchronously right so it stays you you maintain owning the process event loop for the duration of that call back so you do your extra work and then you write more to a file and until that file comes back you're not going to be called but once it's done it's called back and then you now are at the back of the queue and once you hit it then you can start using again so this is the big inefficiencies with this stuff right because if this takes if each one of these takes say very little time a half millisecond whatever right each one of these takes a half millisecond but there's also another half millisecond in between between each one of these to be called to the next one or whatever it is a quarter millisecond whatever it's going to be you could add in an extra you know three four s milliseconds of just lag and so every single process every single time you go back to the process tick it happens again it happens again it happens again it happens again and so this is why you can all of a sudden get these huge amounts of latencies is because you just happen to keep going to the back of this queue over and over and over and over again so you know understand some things understand why these things can happen because it is really important it can really add a bunch of stuff hey ind different ghost how you doing all right let's diagnose the problem it's useful to diagnose this problem in certain operations first we should confirm that our application is actually blocked on the event loop no js exposes a handful of perf hooks to measure event you loop utilization oh i didn't know how much of this is true i haven't played with any i haven't played with their i didn't realize that noj offer some hooks for that that'd be kind of fun to play with i'm going to have to play with that next we should confirm that our event loop isn't blocked by code we control in my case i confirmed this by inspecting cpu profiles if the event loop is occupied for more than 50 milliseconds with no obvious culprit in sight the culprit is likely in the runtime okay fair next next we can confirm how promise heavy our code is through the following code snippet each graph qu qual operation should increase the number of prom has created and give us a clue how promise heavy is our code all right so we're going to do a little async hooks hooks create hook ait something type promise count plus hook enabled oh interesting i didn't realize you could do async hooks like that that's kind of interesting another practical approach to determine whether the event loop is a block or is determining the difference between client reported database query latency and database reported query latency for example i mean this is actually a a very true source right if you can query a database and you get certain laty latency then you query your application and you get very different latency you got some things you can at least make some judgment about obviously the hard part is where are you located where's the database located where's the database located in comparison to your application versus where you're located at you have to take a lot of those things in for example i notice that the client side reporting of certain database queries is often greater than or less than 100 milliseconds no greater than 100 milliseconds sorry even though we were making an index query with a table with less than a th000 rows as expected we couldn't replicate such a slow performance when manually quering our database the slowdown was because the event loop was overwhelmed after making database requests so even though the database responded to certain requests very quickly the web application did not get around to processing the responses until after a significant delay if you've forgotten why there's a significant delay remember the graph that i showed you and remember every single time in a weight happens it does it once every single time a dot then happens it does it once so if you dot then dot then dot then you will have three going back to backof thee line operations since asyn a weight only affects request throughput in certain promis heavy conditions or most open source code is not heavily optimized to prevent unnecessary promises this example graph queal shield is one of the most popular graph quel off libraries assumed every field resolve resolvers async there therefore it constructs a promise for every field in the graph quel response which further amplifies the number of promises created in this life cycle of a request that's crazy it's crazy typescript and javascript do not prevent developers from unnecessary marking functions as async yes this is true so we need that we need es es lint rules like require a weight to avoid unnecessary async o8 calls dude i literally found a performance problem in some code and i kid you not it was because a function was marked as a snc that was not a sink it increases garbage collection it increases time it takes right it's a it's it's actually a real problem it's wild yeah exactly promise explosion equals memory explosion which equals more gc interrupts dude it's wild an accidental unneeded acing function can add milliseconds to to response times which means i me think about how many acing functions you could go over right all right apm and promises actions per minute everyone's favorite thing us star crafters finally we can in let's see we can incredibly slow down promise execution if we use async hooks a deprecated but widely used no tojs feature async hooks help us track asynchronous resources for example a tracing library might desire to track a request across callbacks and promises unfortunately any code we import may rely on this feature and can auto enable it dd trace data dogs apm library and likely many others uses this feature to provide traces across promise executions o o when your tracking library slows down your entire universe that's not good this makes me want to join marvin h mist and start optimizing op source libraries it would be a it's a it's a pretty big win the thing is is that often a program's not slowed down by a single issue right oh that by the way that's my favorite tweet which is j i'm gonna i'm gonna tweet that i'm gonna i'm gonna tweet this this is it triggers everybody which is my favorite thing it's like my favorite kind of tweet is the one that nobody wins let's do this nobody wins here nobody wins boom post it oh my goodness it's annie if you don't know annie you don't know about twitter okay you don't know about twitter i've have tweeted this a few times some version of this oh man it's the it's the greatest oh it's the greatest people lose it people lose it all right anyways fantastic async with hooks yep none of this is surprising obviously adding tracing to anything you do asynchronously of course by its very nature is going to cause a huge slowdown what is this twitter it's the place you go to so when you go to x.com you actually get redirected to a place called twitter.com so we all use twitter i don't know what this x thing okay i don't know what this whole x thing you keep talking about okay i i don't know what it is you guys keep telling me about x and that i keep using it wrong yet i keep going to twitter.com i don't understand why you guys keep telling me this okay i know x is going to give it to you just hasn't given it to me yet okay i haven't got it yet i'm waiting for it all right anyways okay we see that it just gets worse obviously we see that async hooks at significant amounts of latency i mean it's no different than like say four each for an array four each obviously adds latency or adds processing in compar in comparison to a for loop totally reasonable right does this article discuss resource pooling no they don't do any of that because it's not about that it's just about promises which i think is really great this is a great topic by the way data loaded no asyn hooks data loaded with asyn hooks async hooks overhead log scale yeah i mean it makes sense that we roughly see that asyn roughly adds 3 to 3 and a2x overhead to resolvers data dog engineers are diligently working to reduce this overhead by contributing to no js and va8 features however improvements in this area are critical to get right and take time to be implemented in general we want to reduce the overhead of promises and reduce the number of promises we we invoke reducing promises overhead to reduce promise overhead we want to minimize promise inspection of features yep reducing the number of promises to reduce the number of promises inol we have a few areas to consider we could remove the use of graph quel middle layer let's go let's go especially the ones that assume every field is async we could also rewrite graph quel queries to use fewer async type resolvers just rewrite it just rewrite it bro fork it just fork it a single resolver that manually queries the database and returns all the data needed for performance sensitive queries rather than relying on graph quel to hydrate nested fields type resolvers by the way just rewriting things is difficult it is difficult it is very hard to be able to see the the problem about easy is that easy is hard do you know what i mean easy is hard easy is truly hard and so this is like graph queel gives you the promise of easy i know i see the pinned message fine omega la omega la twitter the facebook same energy if it's the same energy why does x.com take me to twitter.com okay what am i supposed to call it i can't read did you put a spelling joke in there you know i can't read you know i literally can't read okay you know i literally cannot read all right what kind of joke is that oh i'm making fun of prime for not being able to read what a loser can't even read what a loser thanks i guess dislexia is a cool thing you can make fun of now you know out of all gosh i should be able to say okay dj i'm gonna quit saying i have dyslexia and that i'm neurode divergent and then guess what when you make fun of me you're making fun of neurode divergence and that is pretty offensive tj i mean that's i i would say we're probably in cancel territory disgusting absolutely disgusting shook hashtag not a safe space american education kona i can't read kona can't even blame him k k say i'm making fun of the us it's okay okay you're lucky you did get by by making fun of the us so we could write a one-shot resolver that implements the entire query okay wait find user by id dude i'm just so triggered by this i am literally so triggered by this chatty protocol find item details i am so effing triggered why why do you got to do this return all the items items map cons details find you create so okay so i'm gonna i'm going to say something completely different i want you just to look at this for a second okay i want you to look at this and you want to ask yourself why why why does my endpoint have so many large latency spikes why am i garbage collecting all the time let me just like regardless of the fact that you're doing two queries like this let's just talk about something different first off in a wait obviously causing a promise which causes multiple callbacks which cause a whole chain thing to be set up blah blah blah blah inefficient causes memory do it again causes memory you also create an object right here not only do you create an object you also have an array you copy the array boom okay so you create an object and an array and you create a closure and you create a lambda function okay you create four pieces of things that have to be cleaned up after that you return an object okay you return an object inside this object you do yet another map over items okay so now you're at seven objects in here we're going to do a find a find creates two more yay then you're going to return an object that has an inner object that creates the the details by id and then which literally is details by id i want you to look at this it's details by id and then creates another copy of the details you you copy the det details it's like 12 15 pieces of memory so every single time that's called something has to go and like collect all this right there's so much going on here this is massively this is ma you know it's just like a lot this a lot of memory this is why i get triggered so easy by javascript because it's so easy to create memory it's so easy like it's so easy you could just do it you could just just create it all day on accent and then garbage collection is wild it's like 15% of your application if you're on a single core machine it could be well over 15% instead of multiple batches of promises we fetch the user items and details in one shot that brings up the meta question why use graph quil in the first place but that's for a larger conversation for a separate time okay anyways i do agree with the whole graphql do you really need graphql what are you buying out of graphql i understand the the benefits of graphql i did literally right falor i still think parts of falor are a good idea to this day but i also see the downsides of this the easiness to create chatty queries the fact that we're looking at two select things that clearly should be one right like you see all these things that end up happening when you break up your api into these really fundamental small little pieces you can always accidentally create hyper chatty protocols and this is a great example of those hyper chatty protocols and so it's emotional it's emotional right you know but i love the point of this article which is promises are they they cause so much more overhead i really wish this article would have went over this because i i mean i didn't do a really great job i didn't do a lot of justice here on this but this is really good to think about is that whenever you do something and anytime you resolve you go to the back of the line so if you have 15 requests per second then you are literally potentially sitting behind 15 requests every single time you do a promise right it could be really hard it could it could be a lot depending on how many queries are you know how many things are running at that time how many promises are running internally if you do a promise. all you still have all those being added to the queue right so it's not just a singular promise a promise. all could have you do it a bunch of little times as well anyways just something to think about the name i really wish i didn't concern myself so much with memory but it's an emotional bruising situation and sensitive topic okay a jen