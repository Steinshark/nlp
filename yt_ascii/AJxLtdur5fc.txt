so, i think it's worth talking a little bit because like i'm usually talking to you about safety about the decision that opening i made to not release the fully trained model the big one so because this has not been released we know that it works like a transformer left to its own devices without being fine-tuned it's just a massive amount of data and off you go. is that right? yeah, like there's enough information given in the paper to reproduce it and you just need the giant giant data set which is a real hassle to make especially because you really need high quality data, does it say anywhere in the paper about how long it took to train yes and how and how many different how many tp use you need and stuff like that? what's the tpu? that's a tensor processing unit okay so like a gpu, but fancy you need a lot of money if you tried to train this with just like amazon's cloud computing offering you would be you'd end up with a bill that i i expect would be in the hundreds of thousands of pounds like it's a lot of compute but with all of these things it's a lot of compute to train them. it's not that much computer run this isn't a new architecture. this isn't like a vast breakthrough from that perspective. it's just like the same thing but much bigger and and nobody else is keeping their research and like not releasing their models to the public so, you know, you think it's dangerous to say that you think that your work might be dangerous and you're not releasing it it's kind of like you think it's much more dangerous than other people's work and therefore like it's so powerful that it's dangerous it's kind of like you're saying that your stuff is so good that it's you know, it's too powerful for you. you know, i can't release it or whatever i think people reacted in a sense to that there's just smack a little bit of publicity stunt i mean assuming it's not a publicity stunt assume rest is not that which i don't believe it is what are they worried about? so that the worry like people make a big people make a big deal of the evette generating fake news like fake news articles that will convince people that there are actually unicorns or whatever. i don't think that that's the risk i also don't think that that's really what opening. i thinks the risk is if you want to generate a fake thing it's still not expensive to do that. you can just sit down and write something right you don't need a language model to write your fake news and in fact, you don't have that much control over it so you wouldn't if you were trying to actually manipulate something you would want to be tweaking it anyway, i don't think that's the risk the the thing that the thing that most concerns me about things like gbg 2 is like the content is not particularly good but it is convincingly human and so it creates a lot of potential for making fake users and so there is this constant arms race between bots operators and the big platforms right? there's teams working at google at youtube at facebook everywhere working on identifying accounts that aren't real and there's various ways. you can do that one of the things you can do is you can analyze the text that they write because the language models that are out there aren't very good. and so if some if if an account is like repeating itself a lot or you have a whole bunch of accounts that are all saying like exactly the same thing then you know that this is like a spam maybe manipulation attempt and so on but with gpt, too you can have things that produce you give the same prompt and then you post all of the outputs and all of those outputs are different from each other and they all look like they were written by a human and it's not a human can look at them probably and figure out hang on a second. this doesn't quite seem right but only if you're really really paying attention which human attention on the large scale is super expensive right so much more expensive than the compute needed to generate the samples so you're outmatched if you if you spend more they can spend it you can you can spend 10 times more and you cripple yourself financially and they can spend 10 times more and it's fine. so you're gonna lose that battle the other thing is so it becomes very difficult to identify fake users. the other thing is one way that you can identify fake users is by analyzing the graph like the social graph or the interaction graph and you can see that because humans, usually when they see spam posts that are full of links to dubious websites and whatever they download them. they don't reply to them and you can create you can fake the voting metrics by having these accounts vote for each other's stuff but then you can analyze the graph of that and say oh all of these plate people they all only vote for each other and the people who we know are humans like never vote for them so we assume those are all bots and we can ignore them but the samples that gbt to produces the big model are convincing enough to get actual humans to engage with them right. it's not like oh my god, that's so persuasive. i've read this article and now i believe this thing about unicorns it's just like i believe that a real human wrote this thing and now i want to argue with them that there aren't unicorns or whatever, right? and now you have real humans engaging in actual meaningful conversation with bots and now you've got a real problem because how are you going to spot who the bots are? when you can't do it automatically just by analyzing the text you can't even do it by aggregating the human responses to them because the humans keep thinking that they're actual humans so now you have the ability to produce large amounts of fake users that the platforms can't spot and therefore they can't stop those users votes from counting on things up voting things and down voting things and liking them and subscriptions and everything else and maybe plating the metrics that way one thing people would do is spot the their profile pictures if you're trying to generate a large number of bots where are you going to get your pictures from and so you can do like reverse image search and get the find of it and they're all using the same picture or they're all using pictures from the same database of facial photos or whatever now we have these really good generative adversarial networks that can generate good-looking cases so that's now really difficult as well and like you can't automatically detect those almost by definition because the way the gams work the discriminator is like a state-of-the-art fake face image detector and it's being fooled like that's the whole point and if you released if somebody came up with a really reliable way of spotting those fake images then you can just use that as the discriminator and keep training right so not releasing their full strength model to me feels very sensible in the sense that people will figure it out, right they published the the science someone will find it. it is worth their while to do it to spend the money to reproduce these results, but by not releasing it. they've bought the platform's several months to like prepare for this to understand what's going on and they are of course working with them and sharing their full strength model with selected partners people. they trust to say here's what it can do take a moment you know govern yourself accordingly like get ready because this stuff is going to come but they're giving everybody a heads up to mitigate the potential like negative impacts that this work might have and the other thing is it sets a really good precedent i think because maybe gpg - isn't that dangerous? but the stuff that we're making is just getting more and more powerful and at some point somebody is going to develop something that is really dangerous and by then you want there to be accepted practices and social norms and industry standards about thinking about the impact of your work before you release it and so it's good to start with something that like there's some argument that there could be some danger from it just so that everybody is like aware that this is a thing that you can do and that people won't think you're weird or you're bragging or it's a publicity stunt or whatever to make it like socially okay to say we found this cool result and we're not going to put it out there because we're not sure about the safety of it and i think that that's something that's really really necessary. so i think that open ai is very smart to start that off now for we really really need it. i make a principled decision now i want the seven so in principle i should be going this way right and would think i'd want to steer towards the seven but on the other hand at this point it's your choice. you give it some random noise and it generates an image from that noise and the idea is its supposed