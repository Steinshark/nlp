we're only in quite a small space here but the mapping technology will cover much much larger areas we can do whole buildings we can do mine systems all sorts of things so we're the curious in bristol it's big science museum this is a map we built using the robot we navigated the robot around using the same mapping device we can then build up the whole map of this whole area if we wanted to we could then send the robot out autonomously to wander around in this space the risk assessment says that really we can only operate in a controlled space so here you can see we've got some barriers we've got we've got david here who's helping us control the space and so that means that people who come into the space know what they're doing in general we should be two meters away from the robot when the motors are engaged that means we can't just send it out autonomously into this space it would be able to navigate around people safely just as it can around other obstacles the robot is quite powerful so if people weren't paying attention it would also be a risky interaction so yeah we can't just send it out we've done stuff in the past we've got other robots mobile robots that are safety rated they have big rubber bumpers you just don't do it with spot well not yet anyway one day maybe so we've got a boston dynamic spot robot and the interesting thing for us is the payload on top so if you see a spot without any payload on there then you know people are not doing too much autonomous with it the stuff on top is what we use to make it autonomous on the front we've got what we call a frontier device so this is a kind of integrated unit on the top you've got a lidar that's giving you a laser measurement so 360 degrees around the robot in kind of a cone so you're always seeing a kind of a range of measurements around there on the front we've got a real sense intel real sense camera that is used to track the motion of the robot frame by frame using the images it's also got an inertial measurement unit in there imu so we can track a little bit of how the device moves and this box is an intel nut what does nook stand for is it it's the next unit of computation so it's an intel design of kind of a small motherboard kind of small form factor pc so all the computation all the algorithms we're using to build 3d maps to plan robot missions it's all running inside that box and we've got effectively an ethernet cable and a power cable that's all that's running out of the robot over here we've got a mesh wi-fi network so when we go autonomy and we want to go beyond line of sight we can drop a breadcrumb trail of wi-fi repeaters that we're able to use from that what we're doing with the robot and the whole payload is looking at autonomous inspection missions so the aim is to take a robot like this map a facility and then have the robot autonomously repeat a mission where that mission might be to take some images of different locations to measure for example radiation been working part of a radiation project or do gas detection so anywhere you want a robot to monitor a large scale facility we put these things together to achieve that kind of those kind of missions on the screen here we've got the visualization of the the software the robot's running and so as i said what we want to do is build a large map of the area and we're going to perform autonomous inspection in that so if you're in a site you typically need two things to be able to to do that you need the 3d representation of the environment and you also need a mission planning system to be able to task the robot so our kind of standard tasks are go there do that go to the fire door check whether it's open or closed for example so you can see these two things on the screen these white dots are the returns from the laser or the lidar that's showing us what the robot can see but that's also kind of the the starting point of the 3d map we've only just turned the robot on so it's quite sparse and that's going to get denser and denser as the robot walks around and gets more points then you've got this graph here that's going to be our mission system so when i say go there do that there is going to be one of these nodes so it's a location the robot can go to so what we're going to demonstrate is we can build both of these representations at once online and we're going to do that by actually giving the robot missions the mission will be to go just walk to a location so my awesome assistant mihal here is going to send the robot to a node it's going to select it on the interface and then the robot will walk off to that location as it's doing that it's checking the space around it and in that space what it's doing is it's looking for extra areas where it can add the nodes to build that graph up it's also continually building the the 3d representation around it so if you look back at the screen you can see that it's denser here now than it was before so more points we've got more graph and the robot is just gradually walking around building up the representation so at the moment the movement across the graph is autonomous but the the locations the robot's going to we're picking those manually so we could do this fully autonomously we could just let the robot loose it would continually build up a map or you could even do it fully manual so you can pick up a joystick and drive the robot around so this is now the mapping stage as complete as we need it to be in a space like this we can't go very far so the robot's done and then we can switch from this sort of exploration mode we can switch into missions so what we'll do now is meehau can switch the system into a mode where instead of us just driving it around we can just give it a list of tasks so the task will be to go to particular locations and perform inspections and usually we'd have some kind of inspection payload so we're working with a company called createtech as part of a large-scale collaboration around nuclear robotics and so we might carry a gamma sensor to measure radioactivity at particular locations we've also done visual inspections of oil and gas facilities this is the kind of complete map this is about as as good as we're going to get given this small area i haven't had much space to move around in so so that's a point cloud that yeah the lidar's giving you yeah so you know we can see various bits outside the glass as well but most of it's kind of constrained yes because there's glass walls and what we're doing is when we're querying the space we're not using this representation it's quite hard to query so when i say query we're saying is there a bunch of space where i could put a node so instead of querying the point cloud we put this in something called an octo map which is a kind of a voxel-based representation so it's like pixels but in 3d and they're a lot coarser and you can use those to quickly look up how much space is occupied how much free space is there so that allows us to do faster queries than you would you would on the the point cloud in fact this is it thanks this is kind of lego minecraft of this room yeah this is the minecraft version of the world these are on the order of 10 centimeter cubes and we use those to basically ray trace through the environment and we have a small bounding box around the ray so that we can actually capture you know if there's a small hole in the environment we don't want to poke through that hole we want to make sure that we capture all of the kind of environmental hazards as it were in the in the projections that we're doing so that we don't place nodes in places where we wouldn't want to go with the robot if you know exactly where you want the robot to go you take the 3d map and then you manually annotate it with the points in this case we're doing it entirely well the graph is built autonomously using that that lego representation so you say are there any lego bricks in this area if not then i'll pop a node in there because i can move there and you've configured how big the robot is and therefore whether it'll fit yeah so all i have to do is basically i'm selecting a node in the interface and when i click it stuff happens in the system in the background basically looking up to the closest nodes to this point and then we compute i think what was the floyd warsaw so we're doing yeah shortest path through the graph to reach the target location and what you'll see is basically the yellow arrows are showing the policy that we're using to traverse the graph which is if i'm at this node what path would i need to take to get to the goal location and this applies to every single node in the whole graph so i'll send it to node i think that's node 8 and you'll see that the arrows in the graph will change to all point towards node a well the policy gives the row one action for every location in its world because we often do this in dynamic environments or with uncertainty so that means the robot even though it's trying to get to one location it might accidentally end up at another location if it avoids an obstacle that it doesn't know is there then it needs to know what action to perform when it reaches that state so that's why in in the visualization you'll see actions from all the locations not just the shortest path this is kind of the manual autonomy stage as it were so i've been selecting the points that we're going to to build up the the map that we're using for localization but as nick said we have kind of the second stage which is the auto the full autonomy stage where we specify actions at specific nodes and we can then perform visual inspections or whatever else we can do right here we don't actually have an inspection payload so basically what we have to do is use some kind of body motions to indicate right when we're here we would do something what you'll see is the robot will be moving its body when it reaches a particular location the reason we move the body is because when you're doing an inspection typically the sensors are mounted on the front or the back of the robot you need to position the robot's body relative to the target so these are all actions you we would use in practice sometimes you want to look high you want to look low you want to look around a corner so you'll see all of those kind of actions deployed by the robot all right i'm going to start a mission in a second i've entered some relevant names into our text file which we then pause and basically send the mission this is definitely not a user interface this is a development tool we've built information what is what is the development now environment this is just a vim editor that i'm using to edit things i'm connected through this wi-fi extender to the robot so i'm i'm using vim because that's the easiest thing that i know to use but i'm using i'm using emacs keybindings in pycharm so i mean we could also talk a little bit about the various software tools we're using okay should we do that after we watch it i've specified a couple of nodes which are reasonably separated within our environment and it's reached the final the first node so now we'll just do a little body roll orienting its cameras better to get a better image of the target or looking quizzically at the door that's also and then it will re-route to the next node in its mission so i after i send the mission i'm not touching anything at all it's it's doing everything fully autonomously it's really worth reinforcing that this is all on board the robot everything is running on board the robot we are not giving any commands we've just specified where it should go and what it should do there it's using the graph that was built up earlier so it's got its map of the world so it knows where it's safe to move it's got its gra it's topological graph the network that's on the floor that tells it where it should be moving through the space all this autonomous behavior is enabled by the payload on the top so we talked through earlier the lidar at the front we've got various wi-fi connections running off it and that's what you know if you see a spot in a video or a show where there's nothing on top they're typical people using actively not doing anything autonomous you need that payload on top either to to enable the capabilities here or to do something when you're autonomous so it shows you what you're doing with the platform and why wouldn't you just cover an area with sort of cctv cameras or something why use a robot like this oh it's a good question so for some applications fixed sensing is better if if the world is static the sensors are cheap i think robots often occupy this sweet spot where you've got a reasonably expensive sensing payload and you need to get it into places where you can't continually monitor so for us large facilities where you want to just image certain areas but you can't get humans in there places where you want to see around corners all sorts of places where you put the robots in i think the other thing we're looking at is really the long-term angle so what we'd like to do next is have a robot-like spot that lives in a facility and it has a sort of daily routine a to-do list it gets up in the morning it walks around and checks the same things does the same inspection tasks or intervention tasks at certain locations and then because you're using a robot you get very repeatable data very repeatable observations from those sensors you're using there people often feel they're being observed if you've got cameras everywhere people behave differently if a robot walks up in the robot you know the robot is the thing carrying the camera it's kind of it shows you that this is the thing doing the inspection you're not being monitored constantly you've just got this robot that's doing the observations the vast majority of the code that we have running is running using the ros middleware system so it's basically a message passing system which allows us to communicate over multiple computers so my computer is connected to a roscore which is basically the system that is kind of running the whole robot that's living on the knock that we have inside the payload and basically we use a subscriber publisher thing so we're sending messages regularly to specific topics and we're using that information on other nodes which live in the system separate to map nodes there are ros nodes which run components of the system maybe a good example is the lidar there's a driver that's reading the data off the lidar that data is then being published into the ros system in fact we could we could see it on the screen so what you're getting is that the the data is being turned into a data type it's being published on the network you can subscribe to it on the robot to do some processing on board or when we run the visualizations that's what we're doing we're subscribing to the data streams that are coming off the robot and then you could use them on the computer to measure things you can also visualize them so the whole system is based on this topic infrastructure and that allows you to do things kind of quickly add new components reuse things and the other advantage of a component based middleware is you can write components in different languages ross stands for the robot operating system and it's that operating system element which means translating between languages and allowing multiple components to run so a lot of the mission planning side we've written is written in python a lot of the mapping side is written in c plus plus but we use ross to make sure that these things can talk to each other it kind of as the robot's running it's like the robot slack is it yeah absolutely you've got a chat so no it's it's exactly like slack well not exactly like that that's a complete exaggeration it's similar so you've got channels the channels are topics the topics have a name which describes the purpose so you might have front front left camera would be the the topic and then it just has a data type in this case would be an image data type and if you want to get messages about the front left camera you join that channel no it's a it's a great analogy then you can see that the depth still works but it's much much worse than it was before and that's because now we only have the two cameras we're just doing the best we can with what we've got so this idea of probabilistic robotics has been the dominant approach to program robots at the moment