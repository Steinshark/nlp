in his video on large language models 
or llms, openai cofounder and youtuber   andrej karpathy likened llms to 
operating systems. karpathy said: > i see a lot of equivalence between this 
new llm os and operating systems of today. i am intrigued by this notion. operating systems are some of the 
world's most important technologies,   with a history spanning 80 years. it mirrors the journey of computing in all 
of its physical forms. in today's video,   we look at the evolution of the operating system. ## aspects of the os
so what does an operating system do? maybe not so unsurprisingly thanks to the long 
history, this is hard to pin down. one definition i like says that the os 
manages the computer's resources for   the user efficiently, reliably and unobtrusively. hardware is hard. there is a lot of it - 
the cpu, main memory, secondary memory,   display, keyboard, mouse, and the network. 
users and their applications must navigate   the idiosyncrasies and pains of that 
hardware to make it do something useful. operating systems help make this easier by giving 
the user or their application programs a clean,   pleasant interface for their task - 
abstracting away the horrors of hardware. an operating system is defined by its abstractions 
because those are what the users are interacting   with on a daily basis. some have been around for 
so long we forget how revolutionary they are. for instance, take the humble file. 
in the beginning, users dealt with   physical memory - working with cells 
and bits. but each memory system type   has its own peculiarities, and dealing 
with all of them is a pain in the butt. you might risk one program overwriting data used   by another program - causing both to 
crash. it is advised to avoid this. the file throws a blanket on top of all that 
and just gives you this nice, clean abstraction. you might think that your "file" is 
sorted away somewhere as a discrete   entity on computer memory - like a book 
in a bookcase. but this is a fraud! in reality, the file data scattered in pieces   like cheetos across wherever the 
computer happened to have storage. when you "open a file", the file 
system is gathering those bits,   putting it into the right 
order and presenting it to you. the os automatically handles 
all that behind the scenes,   putting them in either the primary 
or secondary storages as needed. abstractions like this are needed for us to 
do our work. every day we are interacting   with abstractions built on top of more 
abstractions. and it all somehow works. ## beginnings the first computers of the 1940s 
and 1950s were made to be used   by just one user or group of users at a time. so they just gave that user every available 
resource. these expensive devices cost millions   of dollars today and so were rented 
to individuals and billed by the hour. however, those individuals found that 
most of their allotted time was being   wasted setting up the equipment 
for the job. this was costing   hundreds of thousands of dollars 
in lost productivity each month. so in 1956, general motors research lab realized 
that they can make a software for their ibm 701   mainframe to automatically handle the loading 
and unloading of each job - "batch computing". with batch computing, jobs are transferred 
from cards to magnetic tape. the computer   would then run them all at once sequentially, 
with the outputs recorded onto a second tape. special cards between each job told 
the computer what resources would be   needed to do the jobs. these were 
called "job control languages". there are a few who call these 
the first operating systems,   but the debate between historians on the 
validity of that statement remains fierce. ## multi-programming the 1960s saw better and pricier hardware - card 
readers, magnetic tape, disk drives, and i/o. users realized that not every 
job used all of the computer's   resources. so these expensive resources 
can be better utilized if different jobs   could be run in parallel. is there 
some way to take advantage of this? there was. back in 1956, the univac 
1103a computer introduced a new concept   called the "interrupt". it let a peripheral 
hardware call for the processor's attention. at the same time, we introduced new innovations 
in memory capacity. items like magnetic drums   were giving the processing units 
more memory than they had before. together, this let the computer 
hold and run multiple programs   at the same time. while one program is 
occupying something like input/output,   another can be simultaneously running on the 
processor. this is known as "multi-programming". ## time sharing if you think about it, running multiple 
programs simultaneously inside a computer is   a small step away from having that computer 
service multiple users simultaneously. one of the major problems with batch 
computing was slow development times.   that in turn was the result of a 
long "edit-compile-run" sequence. big batches took hours or even an entire 
day to run. if there was a bug somewhere,   the day's entire output might be just 
an error message. immensely frustrating. so in 1959, the computer and cognitive scientist   john mccarthy proposed a possible 
solution to his colleagues at mit: > an operating system ... that 
will substantially reduce the   time required to get a problem solved 
on the machine ... the only way quick   response can be provided at bearable 
cost is by time-sharing. that is,   the computer must attend to other customers 
while one customer is reacting to some output what this meant was a large central computer 
connected to what they called "terminals" - a   monitor and keyboard. this gives the user the 
illusion that they are the only person using   the computer. a very powerful software was needed 
to coordinate all this and provide this illusion. two years later in 1961, the mit team 
led by fernando corbato managed to get   a prototype working on their ibm 709 
machine. lacking hard disk drives,   they used a bunch of tape drives attached 
to four typewriters. it just barely worked. in 1962, mit announced the compatible 
time-sharing system or ctss as it was   called. a year later ctss got a hard disk drive,   and was offered to large scale users - 
though mit was not allowed to charge for it. though hints of the feature were 
implemented for the military's   massive sage radar coordination system 
and other specialized systems at the time,   we consider ctss the first timeshare 
expressly made for the purpose. by 1965, ctss had hundreds of 
registered users at mit and   other colleges across new england, 
handling up to 30 users at once. it also implemented the first mail and   mail box function between users - a 
spiritual precursor of the e-mail. several other timesharing services emerged 
throughout the late 1960s and early 1970s.   one notable system was the dartmouth time sharing 
system. the basic language was developed on it. an early version of dtss later powered 
a popular timesharing service offered by   general electric. ge was the market leader until 
the mid-1970s when competition overwhelmed them. today, we have largely forgotten about the phrase 
timesharing - though it underpins the idea of   what we now call cloud computing. but it had a 
lasting impact on the history of computers and   their operating systems. before the rise of the 
pc, this was how people experienced the computer. ## multics and os/360
ctss's success spurred mit to create a successor. so in 1964, the ctss mit team joined 
with bell labs and general electric,   the market leader in timeshare 
systems to create a new software   called "multiplexed information 
and computing service" or multics. multics' great vision was to enable a time-sharing   "computer utility" capable of 
handling hundreds of users. so kind of like how water utilities 
provide cheap, ubiquitous water,   multics would enable computers to 
bring computer service to the masses. but the project ballooned as it wanted to 
be everything for everyone and progress   bogged down. bell labs finally pulled 
out in 1969 and things fell apart. mit finally got multics to work on their own,   and it was eventually sold to honeywell, 
which installed it on a few systems. it gained a cult following and persisted despite   honeywell's determined efforts to kill 
it. the last site shut down in 2000. multics' troubles were reflected in another 
legendary os project happening at about the   same time. in 1964, ibm announced its 
historic computer line - the system 360. ideally, a program written for one 
360 computer was supposed to run   on all of them. that was the whole 
schtick. but programmers struggled   to write software that can work on all 
these different hardware environments. famously, ibm tried to build a single 
operating system for it - the os/360.   despite a monumental budget and 
an army bigger than the romans,   os/360 fell way behind on schedule. and in 
the end, they had to split it up anyway. its project leader, fred brooks, later 
wrote a book based on his learnings from   the os/360 experience - the "mythical man-month". ## unix
multics failed as a commercial product. but its groundbreaking ideas - security, 
hierarchical file systems, a command shell,   and more - were incorporated into 
its spiritual successor unix. i already did a video about unix's development 
so i am not going to reinvent the wheel. but i think it is important to emphasize that 
unix was the right thing at the right time.   it had many of the revolutionary ideas 
of multics and added a few of its own. for instance, the pipeline, which let you 
pipe the output of one process into the   inputs of another. it is like a human 
centipede but for computer processes. these helpful utilities were written for 
cheaper, lower class minicomputers just   as those devices came to be popular with users 
beyond those of traditional mainframe computers. and because it was written in the 
high-level c programming language,   unix can be easily ported to other minicomputers. 
this - and the weird bell labs situation that left   it in a copyright limbo - helped unix gain 
wide adoption in universities and beyond. unix's rise was a cultural phenomenon that 
paved the way for other decentralized software   communities like those for the open source 
linux os and the hobbyist microcomputers. ## the microcomputer in the mid-1970s, new semiconductor 
technologies enabled the creation   of integrated circuits with 
thousands of devices on them. these ics were powerful enough to be general 
purpose chips. the first such microprocessor   was intel's 4004, a four-bit chip originally 
made for a calculator and released in 1971. intel later released the updated 8008 in 
1973, and then the 8080 in 1974. other   firms like zilog and motorola released 
their own microprocessors as well. these   powerful chips would be the heart of 
what was then called the microcomputer. intel - then very small - hired 
a computer scientist and language   professor at the naval postgraduate 
school in monterey, california named   gary kildall as a consultant to produce 
certain software for their 8080 chip. intel needed an 8080-compatible operating 
system for testing purposes. so they helped   kildall port one he had written while 
working at the naval school. the new os   was called cp/m - which originally 
stood for control program/monitor. kildall believed that personal computer 
hardware was getting good enough to compete   with existing timesharing systems 
as a programming tool. remember,   the great illusion of timesharing is that every 
terminal user thinks that they are programming   on their own computer. what if that were 
to be actually true and not an illusion? ## cp/m
key to achieving this dream would be the memory. existing microcomputer memory 
systems - particularly secondary   memory - sucked. things like paper tape 
and cassettes. none of this was acceptable. so kildall got interested in a new secondary 
storage technology first invented and introduced   by ibm called the floppy drive. it offered 
far more storage at a relatively cheap price. oh and unlike paper tape, it was random 
access. you can just jump to the data   point you want rather than spooling 
through the whole thing sequentially. kildall gets a sample drive 
from shugart associates,   at the time just a few miles from intel. 
founded by storage legend alan shugart,   shugart associates would later dominate the 
8-inch and 5.25 inch floppy drive markets. now what? so there was kildall, in his room 
with just a naked floppy drive on his desk and   a crude intel cpu microcomputer. so as you do, 
he programs a controller software that helped a   microcomputer running the cp/m os interface 
with this floppy disk drive and its data. it might not sound like much. but 
hardware limitations meant that   early microcomputer operating 
systems were often just that: just a file system for organizing 
and managing files on an external   disk storage plus the ability to 
load and run programs on that disk. disk operating systems or dos. kildall founded a company called digital research 
and began licensing cp/m to microcomputer end   users, who paid him thousands of dollars. 
by 1981, he had several hundred licensees. cp/m - retroactively renamed to stand for 
"control program for microcomputers" - quickly   became the dominant operating system for the 
small and burgeoning microcomputer community. though they were not alone in the 
industry. others included apple dos,   the os for the very popular apple 
ii computer by apple computer.   a unix-cloneish thing called coherent 
which was ported down from minicomputers. and this small thing from microsoft, ms-dos. ## ms-dos fatefully, cp/m lost its early lead. in 1980, a rogue team at ibm began a secret   project to make their own microcomputer 
- the ibm pc. facing a tight deadline,   the team built the machine together with parts 
and software sourced from outside vendors. the pc team licensed a basic interpreter 
from bill gates and his company,   microsoft. they were connected 
to ibm through gates' mother,   who was co-chair of the united way 
non-profit along with john opel, ibm's ceo. the ibm pc team asked gates if they knew 
anyone making a microcomputer os and he   pointed them to cp/m. but for reasons 
that remain unclear today, kildall did   not personally take the meeting with ibm and 
refused to sign their non-disclosure agreement. so ibm went back to bill gates for an os. 
microsoft was then in negotiations with   bell labs for a unix license. that 
effort would eventually result in   the xenix operating system. however that 
was not yet done and there was no time. so gates went out and bought a dos from a 
local computer manufacturer. he then hired   its developer tim paterson to make a few 
modifications and rebranded it as ms-dos. critically, microsoft did not sell ms-dos outright   to ibm but rather licensed it to 
them on a non-exclusive basis. the ibm version that ran on the pc at 
its release in 1981 was called pc-dos.   to protect pc-dos from clones, ibm 
wrote part of the os - the bios - to   a hardware chip and copyrighted 
it by publishing it in a journal. the ibm pc - with its iconic name and 
marketing muscle - quickly became the   most popular microcomputer on the market. 
its setup became an industry standard,   inviting competitors and clones. at the start, ms-dos was a crude piece of 
software - about 4,000 lines. nevertheless,   it allowed software vendors like visicalc to bring 
their software packages onto the ibm pc platform. a bevy of computer-makers then managed to 
work their way around the ibm pc-dos bios   copyright - kicking off the pc clone 
industry. microsoft struck licensing   deals with those pc-makers, rapidly 
grabbing market share in the industry. working directly with pc assemblers or oems 
scaled far better than cp/m's approach of   going right to end users. microsoft's 
ms-dos overthrew cp/m as the dominant   pc os. by 1983, they had a fifth of the 
microcomputer operating system market. ## applications today, we might see microsoft as one 
and one with their operating system. but in the early days, gates and 
microsoft more saw themselves as   an applications company. operating 
systems were important - it provided   50% of the company's revenues - but 
they were seen as a means to an end. gates' thinking at the time was that with an os 
you get just a few points of the machine's price   so like $40 for a $2,000 machine. but with an 
application, you can earn hundreds of dollars. in 1981, their top selling application 
was multiplan - a now somewhat-dated   looking spreadsheet application for ms-dos. 
it sold a million copies over its lifetime. and for that reason, microsoft in 1983 
was big but nowhere the giant we know   them to be today. that year they 
generated $70 million in revenues,   very good but visicorp did $60 
million and lotus did $48 million. this thinking was why we had these interesting 
situations with microsoft offering two operating   systems to its customers. ms-dos was for 
its low-end ibm-pc users. and the version   of unix that microsoft had licensed 
from at&t xenix was for high-end users. ## windows it took time for microsoft to realize 
how powerful of an asset it really had. by 1983, semiconductor hardware got good enough   that pc operating systems can start 
incorporating a few needed features. one of the most needed was multi-tasking. 
computer work was getting more interrelated   and complicated, involving the 
outputs of several different programs. for example, making a company report might 
require a painting program, spreadsheet,   and word processor to be open all at once. with ms-dos and other single-task 
operating systems of the day,   users had to close down the one program running 
in front of them entirely, which was annoying. also, the way people interacted 
with ms-dos was through a command   line. you had to type in the right 
prompt to get the computer to do   what you wanted. deviations in the 
prompt could give unwanted results. by 1983, the pc community narrowed on the 
windowing graphical user interface as an   elegant solution to these problems. it was first 
demonstrated by xerox in the 1970s, and later   incorporated into the operating systems for the 
apple lisa and macintosh - sold in 1983 and 1984. microsoft adopted the windowing 
gui for its windows operating   system - first released in late 1985 
basically as a shell on top of ms-dos. throughout the late 1980s and 1990s, the 
pc ecosystem exploded in size. cpus and   other semiconductor hardware advanced 
in performance like never before seen.   hardware processes once only seen on 
mainframes made their way to the pc. the pc's modular design encouraged a plethora 
of hardware peripherals and software drivers.   on the software side, an ecosystem of utilities 
and applications to suit different environments   like the home desktop, the high performance 
workstation, and the enterprise server. to handle all this, windows evolved a 
sprawling modular architecture with each   system function handled by a separate os 
component. multiple software layers added   new abstractions to help programmers 
and users navigate these environments. it took years for microsoft to get this incredibly 
complicated piece of software working to its full   potential. but they benefitted as windows 
established itself as the dominant operating   system and the company started bundling 
adjacent software like office into it. by 1993, office had 90% of 
the productivity market,   contributing 50% of microsoft's revenues. its 
low prices - in part due to scale and subsidies   from windows - drove competitors like lotus and 
wordperfect out of business a few years later. thanks to its grip on the pc universe 
through windows, microsoft became the   defining technology company of the 1990s. but the 
sun don't shine on the same dog's butt everyday. ## mobile the first mobile "computers" were 
the personal data assistants. these were handheld pcs popular in the mid-1990s   for helping people manage their contact 
information, addresses, notes and to-dos. apple had been one of the pioneers in the 
industry, releasing the newton in 1993. it   was an ambitious product, but the hardware 
was not ready yet - making it difficult to   fulfill its promises. for instance, 
the ability to recognize handwriting. these early devices were extremely 
constrained in terms of resources. the   original palm pilot ran on a 16 megahertz 
processor and 128 kilobytes of ram. this   made them extremely challenging to build 
for - you can't just scale down a pc os. microsoft initially struggled to bring windows 
to the pda market. their first offering was   the windows ce os - now windows mobile - 
which they produced with hardware partners. released in 1996, ce struggled with bad battery 
life, os stability, and a very bad interface. successful companies like palm produced 
their operating systems from the ground up   with these constraints first in mind. this meant 
compromises. for instance, the palm pilot lacked   a keyboard and handwriting recognition, 
using a shorthand system called graffiti. ## phones it did not take a lot of foresight to see that 
pdas and mobile phones will eventually collide. having seen what microsoft did to the pc 
industry, in 1998 three of the largest   phone makers joined together and bought 
into an operating system called symbian. the symbian phone os was produced by 
a british company of the same name   once producing pda software. 
adopted by the phone-makers,   symbian became an early leader with 65% market 
share and one hundred million users at its peak. but symbian failed to build a powerful and 
lasting ecosystem around its advantages.   none of the handset makers wanted to 
give up their connection to the user,   causing serious fragmentation issues 
and a whole bunch of different uis. and since it had to serve so many 
different hardware environments,   symbian was notoriously hard to develop for. the   company struggled to build good tools and 
distribution channels for their developers. nokia was the leading symbian phone-maker, 
driving 80% of its sales. and while they   grabbed significant market share in europe and 
asia, they struggled in the united states. in   part because of the dominant position of the 
mobile networks like verizon and cingular. ## iphone & android the early 2000s saw more improvements 
in semiconductor hardware. in addition to faster and more power-efficient 
processors enabled by the arm instruction set and   its ecosystem, the decade saw the rise of flash 
memory as a compelling secondary memory option. the only thing now missing was a compelling 
interface to pull it all together. as well as   a company capable of cutting through all the red 
tape that turned symbian into a convoluted mess. apple made the first breakthrough with the iphone,   famously building its operating system 
by scaling down the desktop mac os. its   multi-touch interface and desktop-class 
browser instantly connected with users. and because it was based on mac os 
x, apple was able to port over its   ecosystem of passionate developers. 
developers so passionate that they   were hacking the os to make apps of 
their own before an official sdk was   released. the opening of the app store in 
2008 only poured gasoline on that fire. google saw the writing on the wall and 
pivoted their linux-based android phone   os in the same direction. by giving 
android away for free via open source,   android rapidly stole share from 
the then-closed source symbian. those old legacy operating systems are 
now gone. what we now call ios has made   apple one of the biggest companies 
in the world. and android is the   world's most widely used os period 
- and a powerful asset for google. it is interesting to see how ios and its 
deep ties with the app store help drive   apple's massive services division 
- kind of like how bundled apps   like office made microsoft king 
of the tech world in the 1990s. ## conclusion i have noticed that the story of operating   systems across its various form 
factors share a bit of a theme. in the beginning, systems were limited by 
compute. the first devices - mainframes,   microcomputers, and mobile pdas - were not 
fast enough to run anything other than the   most rudimentary programs. compromises had 
to be made to get the products to work. over time, the processors did get fast 
enough. now the new limit is memory.   mainframes needed dram and the disk drive 
to manage multiple tasks and users. pcs   had a craving for memory that was eventually 
fulfilled by the floppy disk drive. and mobile   oses could not produce bigger programs 
until flash memory got cheap enough. then finally after that, we 
are limited by input/output,   or the interface. we needed new paradigms 
of communicating and interacting with our   computers to get the results we need. for the pc 
that was the gui. for mobile, that was multitouch. so we cycle back to our original question.   are llms the next operating system? i 
don't know. but i did notice something. it first took breakthroughs in compute to show 
that larger neural networks had some potential. then after that, we leveraged improvements in dram   memory to really scale up llm sizes 
to where they can show economic value. and then most recently, we needed to find   new paradigms of interacting 
with these llms with chatgpt. it is fun to ponder the possibilities of an llm 
operating system, and where the metaphor can take   us. what new abstractions and environments 
for doing work can an llm os do for us? what might that actually look like? i'm not sure   about the answers for these questions. 
but i look forward to finding it out.