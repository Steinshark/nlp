so something slightly different with today's computer file rather than recording this in our usual location this one was actually recorded in front of a live studio audience as part of a training exercise for other members of staff in computer science at nottingham so we hope you enjoy it it's got a slightly different feel to it but if you hear people laughing in the background they're laughing at me not the topic so i thought today we'd continue on the discussion we've had about compression we did the video i did an overview and we talked there about how we could define compression in terms of a model how we actually model the data and in terms of encoding how we represent that data in terms of bits and we can change the encoding we can use something like huffman encoding to compress it or we talked about how we can change the model to get better compression as well i want to talk about that today i'm going to look at my favorite compression algorithm which is bzip 2. i mean how sad is that to have a favorite compression algorithm but i'm a computer scientist so b zip2 algorithm is based around what's called the brewers wheeler transform and this was developed by a chap called mike burroughs based on somewhere that she's phd supervisor had done david wheeler in cambridge he was over in digital at the time and he went on to develop a search engine called ulta vista for anyone who's over the age of 25 you might remember that as what we used to search the internet before this young upstart called google came along to have a look at the boroughs of either transform because it's not necessarily the thing that you would think that would actually help with compression but when you actually use it and particularly with a couple of other things it actually gets a really nice bit of compression going let's look at a common english phrase and i'm going to go with to b or not to be this string if you were to use a compression algorithm like run length encoding wouldn't actually compress that well we talked about run length encoding in the previous video it's where you have your symbols and rather than just saying there's this symbol you say how long that would occur for so in this case there's one t there's one o there's one space there's one b there's one e and as we talked about the string has got massively bigger it's expanded rather than compressing it it doesn't compress that well but things get interesting if you actually sort the string so if we were to sort this not into alphabetical order but based on its ascii ordering so using the numbers that represent the characters what we'd end up with we've got one two three four five spaces i'll draw them as underscores just to make it clear and then we'd have the letters in order we've got no a's we've got the b so there's one we've got another b there there's two we've got some e's we've got one e there we've got one e there so we've got two e's what comes next in the alphabet n we've got an n we've got one two three four o's and we've got two t's i think three t's when we get to that point when we've sorted it we get it to a form that is actually more compressible by using something like run length encoding we've now got five spaces two b's two e's one n four o's and three t's and we can compress that very relatively nicely using run length and coding does it matter you've missed out on r and there's an r in there as well this would compress really really well as we said but there's a problem with it if we've got this string there's no way we can get it back to to be or not to be because we've changed the ordering we no longer have that information so we've got a model that captures the ordering of the text which is what we're interested in really because we want to be able to read hamlet possibly or we've got an encoding which or a model rather which throws away all that information but gives us the letters is there something that we can do in between that gets us to a point which is really compressible but actually still gets us to a point where we can keep the information that we're on the text during the ordering and this is where the boroughs we either transform comes in so let's pick a slightly simpler example to start with because i really don't have to write it out to be or not to be too many times so i can use this word as an example to see how the boroughs wheeler transform works what you do is you start off writing down the data you want to encode i'm using text here but you could use any data for this and you just on the first line write it out as it is and then on the next line you rotate it to the left by one so we start off with o m p whoops that's a p obviously t-u-t-e-r-p-h-i-l-e and then we put that c that's fallen off the left hand side on the end and we keep doing that until we get back to where we started from now i could do this by hand and we'd be here until christmas so i've written a computer program to do it for me so what i'm going to do is i'm going to take the word computer file i'm going to send it into the program that i have written and i'm going to get it to generate the complete boris wheeler transform for us and what we end up with so this matrix this table where we have all the letters rotated around until we've done it 13 times and we end up with the last one where we've got the e at the beginning and the rest of the word follows that now why is that useful you might think well how's that going to help us compress what we do is we take that table and we sort it based on ascii ordering and if we do that if we sort it we end up with the list below and on the left hand side in the left hand column because we've sorted it we end up with the words and the characters that we saw before the sorted form as we got when we did to b or not to be we're getting exactly the same here c e e h i l m o p p r to you again we've lost all the information there we're not interested now what we're interested in is what's in the last column because once we've sorted it if we take that last column we can regenerate the complete data that we started with how does that work well let's switch back to the example if we sort it and we take that last column e-l-t-p-h-i-o-c-r-m-e-up and sort that again what we end up with is c e e h etc that we had before so if we take the last column as we said we can regenerate the first column because we can sort it so we can get the c e e h now if we pair them together and i've got to write this on the paper now we get e which is the character in the last column that we started with first character and c which is a character from the first column we get l and we get e and we get t and we get e to cut a longer story short if we actually look at what we've got e c appears on the second line that we've got here we can regenerate the whole of the first two columns by pairing up what we saved the last column and the sorted version of that we can regenerate the whole of the first two columns and if we sort that again we can pair that up with the last column again and eventually we can build up the whole table now why is that useful for compression it's actually useful for a whole load of things it's used for compression it's used for string searching it's used for genomics all sort of things make you through this transform but in terms of compression why is that useful well to do that we need to go back to a longer example so we are going to go back to hamlet's soliloquy but i'm not going to type it in by hand i've got it in a file there i'm going to feed this through the program and i'm going to run it into the bwt foam this time i'm not going to generate the whole table i'm only going to generate the last column and what's interesting once we've done this and once we've sorted it and we take the last column is that what we end up with is that we have whole load of runs of characters which are the same so we've got ooo there we've got n n n n here we've got n and then a bit later on in there we've got lots of cheese so by running it through the boris we either transform if we take the last column we not only get something that we can use to regenerate the whole original data we can run it through that algorithm to decompress it but we also get something which has lots of characters that are consecutive which you can run through the run length encoding routine to compress down to a smaller step now the interesting thing is why that actually happens and we can see that if we actually look at both the beginning and the end of the two columns and we pick an example i'm going to look at the lines that begin with the letters h e so on the left hand side the first 10 characters or so are the matrix and then on the right hand side we've got the last few characters in the matrix because it is 525 characters long and my screen's not big enough each of these lines at the beginning that begin h e they all end with t and the reason that is is because if we think about it we built up this matrix by rotating the characters to the left one step and then rotating to the left another step and that gave us each of the things so each of the ones where h e begins at the beginning of that the character that was before it let's come over onto the right hand side and then what we've got words like the which begin with a t and then have h e following them or there or there with a ir and so on they'll all end up sorting into the same spot which is what we've got them here and of course because they all had a t that preceded them that's ended up on the right hand side so that right hand column actually has whole runs of common characters because we're actually not compressing random data we're compressing data that actually has meaning and has structure in this case it's english language grammar if it was something else it would have its own structure with it as well and so by using this and then coupling it with a few other things where they don't encode the actual ascii codes they rearrange the table after each character we can actually get to a really nice compression algorithm which is used all over the internet b zip2 for compressing files for distributing things on unix systems and in other places as well but it's also really nice example as we said at the beginning of how by changing the model you'd never want to use the boris wheeler transform to storing tax you never want to have your word document or your latex document represented in the boris wheeler transform it would be a nightmat edit your computer would be slow because you have to repurpose it into the reading order to actually access it but actually to compress it we get an a version of the document a model of the text that is much more compressible so by changing the model as well as changing the encoding we can end up with a really good compression algorithm to our one space one s one u one n two it gets massively bigger this sort of encoding what's commonly referred to as the top of the file with pointers to where all these words occurred and if in doubt use the pointers for repeats