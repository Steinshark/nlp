this episode is brought to you by brilliant we've all seen the film cliches of spy satellites these black budget all-seeing eyes floating in space that can seemingly find and follow a vehicle by its license plate anywhere on earth within seconds this mystique however is easily pierced by the reality of an orbiting satellite imagine looking at an object the size of a small car from up to 2 000 kilometers or about 1200 miles away through a telescope all while moving at seven and a half kilometers or about 4.7 miles per second at these relative velocities a near-earth satellite would easily sweep a car-sized object in under a millisecond low earth orbit surveillance for the most part is constrained by the short period of time the system's optics is exposed to the target as well as the limits of the orbit chosen the alternative to this approach is to use a network of geostationary surveillance headlights or slow orbiting geosynchronous satellites that permit a far greater targeting persistence though this comes at the cost of significantly reduced imaging resolution as these systems must now orbit anywhere from twenty thousand kilometers to 36 000 kilometers or about twelve thousand to twenty two thousand miles away from the surface this tends to limit their use to coarser optical surveillance and radiation detection they're also relatively costly to operate and complex to establish as multiple satellites are needed for global coverage with program costs easily approaching a trillion dollars and taking over a decade to bring into full operation long before satellite surveillance reconnaissance aircraft were the tool of choice for gathering aerial imagery and as satellite systems grew in sophistication they slowly evolve into a faster responding tactical complement to satellites however much like with low earth orbit satellites capturing high resolution imagery came at the cost of shorter optical exposure times over the target and the inherent risk to air crew requiring missions to be very specific and purposeful in their nature by the 1990s the emergence of unmanned aerial vehicles would dramatically change the nature of aerial surveillance uavs could now loiter over large areas of land for multiple days and within a decade a revolution in aerial imaging would combine with these incredible uav loiter times to create a capability that would even supersede the hyperbole in film the ability to observe every event within an entire city in real time uavs operate in the world of tactical intelligence surveillance and reconnaissance or isr generally providing immediate support for military operations often within constantly evolving mission objectives traditionally airborne isr imaging systems were designed around one of two objectives either looking at a larger area without the ability to provide detailed resolution of a particular object or provide a high resolution view of a specific target with a greatly diminished capability to see the larger context well into the 1990s traditional wet film was still a big part of isr missions both the sr-71 and u2 reconnaissance aircraft employed a system called the optical bar camera that spun film through a roller cage in a back and forth motion to capture wide panoramic photos employing a roll of film 12.7 centimeters or 5 inches wide and almost 3.2 kilometers or 2 miles long the system would capture one frame every 6.8 seconds with a limit of around 16 000 frame captors per roll each frame would cover a land area for around 112 square nautical miles in a panoramic horizons horizon sweep the sheer size of each film frame allowed details to be resolved down to 15 centimeters or about 6 inches from 24 kilometers or about 15 miles up most narrow field reconnaissance systems that could fly closer and capture higher resolution images of targets were fielded as reconnaissance pods for existing aircraft the an asd2 for example was a film-based reconnaissance pod primarily used by the u.s air force that was designed to be carried by the rf-101 voodoo reconnaissance aircraft employing 70 millimeter film these pods were capable of capturing images of ground targets down to a resolution of around two and a half centimeters or about an inch and can be programmed to capture images at specific intervals or on command the an asd2 had a number of advanced features for its time including a rotating prism that allowed the camera to capture images from multiple angles and a stabilization system that reduced camera shake during flight allowing for higher speed and low altitude missions the first digital imaging system to be used for reconnaissance was the optical components of the advanced synthetic aperture radar system or asars installed on the u2 reconnaissance aircraft in the late 1970s acer's used a large phase array antenna to create high resolution images of the ground below using radar complementing the radar was an imaging system that used a charged couple device or ccd camera to capture visible light images of the terrain being surveyed this ccd camera operated in synchronization with the radar system and had a resolution of around 1 meter or 3.3 feet per pixel a ccd sensor consists of an array of tiny light sensitive cells each cell contains a small capacitor that can store an electrical charge when light strikes the surface of the ccd it generates an electric charge within each cell which is proportional to the intensity of the light each cell's charge is then shifted along a series of parallel conductive channels called registers using a clocking signal this charge is shifted through the registers in a process called a charge coupled transfer once the charge reaches the end of the registers it is read out one row at a time and digitally quantized by an analog to digital converter where it's then processed and stored ccd sensors would quickly become a standard technology for military surveillance as they allowed imaging in a compact durable and lightweight package however despite these advantages early ccds were expensive to manufacture and difficult to produce in high cell densities when combined with the limitations of computing hardware of the time their designs were generally limited to less than a megapixel with resolutions as low as 100 000 pixels being found in some systems by the early 1990s a new class of imaging sensors called active pixel sensors primarily based on the cmos fabrication process began to permeate the commercial market active pixel sensors employ several transistors at each photo site to both amplify and move the charge using a traditional signal path making the sensor far more flexible for different applications due to this pixel independence cmos sensors also use more conventional and less costly manufacturing techniques already established for semiconductor fabrication production lines these sensors also consumed as much as 100 times less power than an equivalent ccd sensor though these advantages came at the cost of being less sensitive and higher noise susceptibility in the 1990s dramatic advancements in embedded computing and the decreasing costs of both memory and computing power would bring about a new era of low-cost digital imaging technologies by 2000 cmos sensors were used in a variety of applications including low-cost cameras pc cameras and security systems the demand for higher resolutions and lower noise performance would push the technology even further and its proliferation into camera phones would have a profound cultural impact as the technology became intertwined with the advent of social media in 2001 at the lawrence livermore national laboratory a new program targeted at monitoring nuclear proliferation would be initiated by the department of energy called the sonoma persistent surveillance program at the core of this initiative was a new class of surveillance technology known as wide area motion imagery wide area motion imagery takes a completely different approach to traditional isr technologies by making use of panoramic optics paired with an extremely dense imaging sensor in concept this would allow for finer localized detail to be resolved in real time purely from the sheer quantity of information captured within the wide optical field in 2005 the project now called constant hawk was transferred to the u.s department of defense with mit labs conducting principal research for the program the first iteration of constant hawk's optical sensor was created by combining six 11 megapixels cmos image sensors that captured only visible and some infrared light intensity with no color information at a combined density of 66 megapixels each frame generated around 99 megabytes of data at the time capturing processing and storing such a large stream of data within an airborne system was a challenge in itself but to truly make use of constant hawk's full potential real-time imagery analysis would be needed it would take over four years of research and testing to develop processing algorithms that made it possible to extract practical real-time information from the large data set of imagery at an altitude of 20 000 feet the constant hawk was designed to survey a circular area on the ground with a radius of approximately 96 kilometers or 60 miles covering a total area of over 28 500 square kilometers or about 11 000 square miles at this altitude each pixel can resolve down to just one third of a meter or about 14 inches in order to perform surveillance on such a large area in real time the mit team devised an approach that focused on change detection the onboard image processors would align and match each captured frame of the area and only store captures of regions where changes occurred once an event on the ground triggered a subsequent change in the imagery of that region the system would store a timeline of the imagery captured from that region this now made it possible to access any event at any time that occurred within the system's range and the emissions of flight duration the real-time investigation of a chain of events over a large area was now possible in an isr mission in 2006 constant hawk became the first wide area motion imagery platform to be deployed as part of the army's quick reaction capability to help combat enemy ambushes and improvised explosive devices in iraq fitted aboard the unassuming short 360 300 aircraft operators were able to employ the system to track for the sudden appearance of parked vehicles or activity from footprints and vehicle tracks at key areas that would indicate a potential threat these detector changes would often trigger other investigations or if the event had already happened a search back through the archives to pinpoint exactly when and how things changed this ability to trace back and catch bombs being planted led to a greater understanding of the patterns and methods of how improvised explosive devices were used the effectiveness of constant hawk in iraq would lead to its deployment in afghanistan in 2009 though now aboard a mc-12w liberty aircraft because the core of constant hawk's capabilities came from its software it was relatively easy to reconfigure and adapt especially as cmos image sensor technology start to advance rapidly due to the emergence of the smartphone in 2009 bae systems would add night vision capabilities and increase the sensor density to 96 megapixels all while reducing the size of the original 680 kilogram or 1500 pound sensor package in 2013 full color imagery processing capabilities would be added by this point constant hawk was able to generate almost 15 terabytes of reconnaissance data per hour the system was so successful that the marine corps would adopt elements of the program to create its own system called angel fire and a derivative system called kestrel would even be fitted to a stationary airstat in afghanistan that would function as a large area aerial sentry as constant hawk was seeing its first deployment several other systems were being developed that targeted more niche isr roles however one system in particular would create a new class of aerial surveillance previously thought to be impossible called the argus is or the autonomous real-time ground ubiquitous surveillance imaging system this darpa project contracted to bae systems aimed to image an area at such high detail and frame rate that it could collect pattern of life data that specifically tracked individuals within the sensor field what makes the concept so ominously powerful is the 1.8 gigapixel imaging sensor at its core derived from smartphone camera technology the sensors made from four lenses and 368 5 megapixel cmos imaging sensors when compared to constant hawk argus is specifically designed to cover a much narrower field of view of around 16 kilometers or about 10 miles when paired with such a dense sensor each pixel can resolve down to just two millimeters of detail making it possible to identify faces clothing defining features such as hairstyles body shapes and tattoos and even read text much like with constant hawk a large part of argus is development revolved around processing the immense quantities of data produced operating at a base capture rate of 15 frames a second the system generates almost 21 terabytes of color imagery every second because argus is specifically designed for tracking a processing system derived from the constant hawk project called persistics was developed persistics similarly only capture and process changes that occur within the viewing area while building a chronological database of imagery argus is however takes this capability even further with the ability to actively track up to 65 different objects purely from optically defining signatures building a searchable timeline of movement for the object because this tracking can be done backwards in time the system now becomes a powerful tool for forensic investigators and intelligence analysis of patterned human behavior argus is completed its initial round of flight testing in early 2010 aboard a blackhawk helicopter and by 2014 it had achieved operational capability with variants of its principal design being adopted to other scaled-down wide area motion imagery platforms at present argus is and its derivative variants have been used operationally throughout the world aboard large and small uavs aerostats and manned aircraft though the extent of its use and advances made to the system have been kept from the public since 2016. it's speculated that with present computing power the ability to generate detailed 3d models for areas of interest from a combination of imagery and complementing lidar instruments are now possible and may be part of argus is current capability as of february 2023 the top tier of consumer cmos imaging sensor manufacturers all produce 120 megapixel range sensors with 200 megapixel sensors expected on the horizon these sensors are also supported by the powerful imaging processing capabilities of today's arm-based processors alongside dense but efficient low-cost storage technologies today the capabilities of the initial constant hawk system could easily be matched for a tiny fraction of that program's cost in a far smaller resilient and flexible package from this it can be speculated a system not disclosed to the public would conceptually offer unnervingly unprecedented levels of resolution and capability particularly if the technology migrates from military to civilian law enforcement beyond civil liberty concerns the simple fact that a scale down but effective wide area motion imagery system can now easily be created inexpensively from consumer hardware poses a new threat from the potential for the mass deployment of such devices by foreign adversaries that could slip by defensive measures purely by sheer numbers of boards slow-moving air vehicles whose intent can easily be disguised one of the interesting outcomes of wide area motion imagery programs has been the expansion of the technology into the civilian world with the help of data analysis techniques such as machine learning and computer vision wide area motion imagery can now be used to detect and track moving objects across large geographical areas providing critical insights and intelligence to a range of industries with data science now shaping its future have you ever wanted to build a solid foundation for understanding how massive pools of data can be wielded effectively to understand the world around us that's where brilliant.org comes in brilliant.org is my go-to tool for diving head first into learning a new concept it's a website an app built off the principle of active problem solving because to truly learn something it takes more than just watching it you have to experience it brilliance is constantly developing their courses to offer the most visual hands-on approach possible to make mastering the key concepts behind today's technology effective and engaging i've personally found their data science and analysis courses to be extremely valuable as it allows you to build examine and self-discover the key concepts of data analysis through the eyes of statistical thinking using interactive exercises with brilliant you learn in depth and at your own pace it's not about memorizing or regurgitating facts you simply pick a course you're interested in and get started if you feel stuck or made a mistake an explanation is always available to help you through the learning process to try everything brilliant has to offer free for a full 30 days and start learning stem today visit brilliant.org forward slash new mind or click on the link in the description the first 200 of you will get 20 off brillian's annual premium subscription