recent deep learning ai models are approaching levels of performance that verge on the uncanny when used correctly tools like github co-pilot gpt-3 and dolly 2 can produce occasionally stunning results good for us but i have a question what use is a great model if we can't use it where it is most needed like in the field with the customer in this video we look at the challenges associated with running ai on the edge but first a sponsor read for the asian armature newsletter check out the newsletter for full scripts of previously released videos and additional commentary there are over 14 000 signups right now which is pretty cool the signup link is in the video description below i try to put one out every week maybe two alright back to the show first we need to explain the edge edge devices are connected to the internet but are much closer to the actual users than data center devices it is a very broad term that covers things like robotics unmanned aircraft remote sensing satellites home entertainment devices like your amazon echo or wearables like your watch i might also include a smartphone but would exclude a laptop these devices all have differing constraints centering around power management processing speed and memory for instance an edge ai chip in a self-driving car might prioritize latency how fast the ai model can come to a result a small commercial aerial drone on the other hand only has enough energy for 20 to 30 minutes in flight therefore it can only allocate less than 5 percent of its overall power budget for computing and data processing and what kind of jobs are these edge ai devices usually doing the vast majority of the time computer vision or audio processing jobs for instance natural language processing facial recognition traffic prediction or something akin to that these models have gotten larger over the years in 2012 alex net was the state-of-the-art computer vision model and it had 61 million weights by contrast the winning imagenet model in 2021 coca now has 2.1 billion weights neural networks inherently consume more energy than non-deep learning alternatives an object detection neural network model requires up to 13 500 times more energy than a histogram of oriented gradients having so many weights and layers only worsens that burden we also have memory constraints in order to run inference on a meaningful amount of time we fetch and store several values into the device memory this includes the model's weights the inputs and so on this is one of the most energy intensive actions a chip can do using up to 200 times more energy than a standard multiply accumulate operation so stuffing a model into an edge ai context requires trade-offs the model has to be smaller and require less computation which means worse performance how much worse depends on various factors but it will be worse most early tech companies tried sidestepping these formidable issues by offloading everything to the cloud like siri or amazon echo the device simply becomes a thin client relaying information between the user and server this approach has its upsides but it also leads to brand new issues for instance issues relating to data transmission latency connection stability and of course privacy there is also a middle hybrid approach in which both the edge and the server share the computational workload imagine the edge ai hardware making a first pass of the raw data and then uploading its results to the cloud for final confirmation this can work too but i also feel that this hybrid approach shares the downsides of both a thin client and on-board processing you have to maintain models in both edge and server environments all right so you cannot run a modern ai model on edge hardware i've made that clear but are there things we can do on the software side to make that ai model more suited for an edge environment this is a field known as neural network model optimizations and has gotten to be a very hot space the first set of approaches center on the idea of training a compact model from the ground up specific examples of neural networks of this type include squeeze net and mobilenet they often replace traditional neural network structures with new ones in order to reduce the amount of weights in the model the less weights the model has the smaller its overall size and memory footprint in their paper squeeze net authors claim alex net level accuracy with 50 times fewer weights and compressed down to half a megabyte there have been some interesting studies as of late hinting that neural network models might be larger than they necessarily have to be so this leads into the second set of approaches post-processing a model that has already been trained neural networks are all ultimately exercises in matrix multiplication so if you can somehow shrink the matrices in a trained model they take up less memory this is the idea behind a methodology called weight quantization here we change the way we store a trained model's weights in memory to save space changing from maybe a 32-bit floating point to 8-bit fixed point another approach is to try to reduce a model's complexity through pruning for instance removing redundant weights one paper claims that 95 of a neural network's weights are highly correlated to a few key weights conceptually you can remove those and still retain much of the accuracy regardless of what you do there is no such thing as a free lunch you will have trade-offs between accuracy and memory slash power usage the aforementioned 32-bit floating to 8-bit fixed point trick apparently results in an accuracy loss of over 12 percent and unfortunately it seems like the actual outcomes of several optimization methods still consistently miss expectations it can be difficult to predict in optimization's effect on the model's performance and resource usage one might meet the spec while the other doesn't edge ai solution providers need to find the right hardware suited for their particular solution there are four widely available hardware types capable of being edge ai pieces cpus gpus fpgas and asics as always nothing is perfect they all have their own uses and drawbacks the first we're going to talk about are cpus a category that also includes microcontrollers or mcus these need little introduction cpus like the raspberry pi are easy to program versatile low power and best of all cheap the most significant downside of cpus however is that they are not very parallel even modern ones with multiple cores and modern neural network models require a great deal of parallel operations with that being said if the model is small enough to fit into its memory then even a tiny mcu with something like 100 kilobytes of ram can run it there are some interesting projects like tensorflow lite for microcontrollers which has enabled fun things like a voice enabled turn controller for cyclists microcontrollers are a big unheralded part of the semiconductor world with some 250 billion already deployed in the field so fields of ai that seek to put relatively sophisticated machine learning on these extremely constrained hardware environments like tiny ml have a lot of impact potential second are the gpus originally designed for gaming they are massively parallel and easily programmable due to widely used programming platforms like nvidia's cuda this makes them great for training new ai models however their extreme parallelism also tend to make them very energy hungry which as we talked about makes them less suitable for edge ai inference jobs one example of an edge gpu is the nvidia jetson the jets and nano is a small relatively cheap 99 embedded computer that is kind of like a raspberry pi generally cpus and gpus are not seen as acceptable hardware choices for edge ai solutions this leaves the other two edge ai hardware options fpgas and a6 which are much more intriguing field programmable gate arrays or fpgas have a lot of potential these integrated circuits are made up of programmable logic blocks and routing interconnections and like gpus they are inherently parallel using hardware description languages like vhdl and verilog you can make those logic blocks simulate certain logic operations so you can configure and reconfigure them as needed their flexibility is very useful in certain ai fields like the autonomous car industry where rules and algorithms can change relatively quickly another advantage of using an fpga has to do with energy efficiency as i said during inference a neural network model spends the most energy when it accesses memory outside of the chip most modern fpgas have something called block ram a set of embedded memory blocks to reduce latency and power consumption models that can fit into these sets save significantly more power the big downsides of fpgas is that they have less available memory bandwidth and computing resources than a gpu it ranges depending on the device but in some cases as little as 10 megabytes of on-chip memory furthermore using them requires a certain design expertise cuda works with popular programming languages like c and c plus not as many people are familiar with vhdl and verilog and finally we have the asics these are custom processors designed for a very specific task for instance ai chips or ai accelerators as they are sometimes called are a class of asics i discussed ai accelerators in a previous video that you might want to watch the biggest downside of the asic is obvious you have to invest substantial upfront financial and human resources to design and produce the chip designing and producing semiconductor chips with the latest leading edge process nodes can cost millions of dollars furthermore you cannot change certain architectures after fabrication like you can with fpgas most asic makers would try to get around this by building for more generic functionality not many companies will ever consider the option of designing their own edge ai chip from scratch luckily there are a plethora of interesting edge ai accelerator products available from vendors on the tech titan side you have intel's movidius myriad x vpu which was released in 2020 vpu stands for vision processing unit it can be used for drones robotics smart cameras and so on movidius had been a long-running irish startup specializing in visual processing at the edge before its acquisition by intel in 2016. google has their own edge tpu which it says is purpose built for running inference on the edge various iterations of the product usb sticks and developer boards are sold through an initiative called coral nvidia for their part has the tegra a series of system on chips it probably shouldn't be called an edge ai accelerator but it is mobile the tech giants are joined by a number of small and medium-sized companies it would take way too long to discuss them all but here are a sampling of a few others that have caught my eye rockchip is a chinese fabulous semiconductor maker based in futo of the fujian province one of their specialty ai chip products is the rockchip rk1808 a standalone neural processing unit or npu the rk1808 is a chip but it is also sometimes sold as a usb package called the toy brick i reckon that makes it easier to integrate into small projects and not guyer falcon technology out in milpitas california makes small low power and low-cost chips their neural accelerators are meant to pair with another processor to handle complicated image recognition and object detection jobs another small one that i want to mention is nehron out in shinchu they've been around since 2015 they offer a range of ai chips that can be used for speech and body motion recognition one of the big challenges with delivering edge ai solutions is that we have to balance between the hardware and the software they're extremely tightly bound together tweaking one messes with the other and this greatly slows iteration and the rate of progress we need to get faster at this so why not use neural networks to help with it recently there has been some interesting research made on something called hardware aware neural architecture search this is where you include certain hardware variables into the neural network model itself so that it can optimally run on a specific hardware usually a gpu or an fpga with asics this search doesn't work so well because the hardware itself can be so widely customized but asics open up the intriguing possibility of simultaneously co-designing both the hardware and algorithms together this is kind of like design technology co-optimization the process of crafting both the chips process node and chip design with an eye towards shared success it has a lot of potential for the edge ai hardware space too massive models are more powerful than ever we can see what they might be capable of but edge ai hardware makers face challenging economic and possibly physical limits in accommodating these models during the second half of the 20th century computers helped unlock unprecedented benefits in industry and commerce ai has the potential to do the same but if the edge hardware never gets to a satisfactory point then i fear that ai's full potential will remain locked away in the ephemeral cloud i hope the industry continues to evolve and push forward all right everyone that's it for tonight thanks for watching subscribe to the channel sign up for the newsletter and i'll see you guys next time