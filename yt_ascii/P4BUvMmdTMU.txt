i'm really glad to be back here in berlin i believe i gave my first conference talk in this very room six years ago and since then i've been at every meeting since that happened in berlin so it's really good to be back i'm here to talk about a deep dive into dispatching techniques there's a talk about performance so i need to start start with the following disclaimer you should never optimize without running your own benchmarks and just because i'm going to present you some benchmark results in the stock don't just copy them blindly and say hello he met at this he said it's good so i'm just going to use this technique run your own benchmarks on your own hardware all my benchmarks are on a 2020 apple mac mini with the m1 processor running as a linux and crank 14. this is most likely not the exact configuration you're targeting so it's important run your own benchmarks to verify the technique and then make a choice so i'm here to be talking about optimizing dispatching and that of course this dispatch loop so the pattern is we have a while loop and inside the while loopy of a switch so we repeatedly need to decide between different cases and need to execute different code so this is an example of a dispatch loop we want to pass a binary file so it consists of headers and payloads so we repeatedly pass a header then we switch over its type and depending on the type we pass different things this is not an example of a dispatch loop because it's no loop right we just convert an enum to a string this is most likely not going to be performance sensitive code if it hap like if your entire runtime is spent doing enormous string conversion something's gone really wrong and of course we've got the canonical example of a dispatch looper bytecode interpreter we've got the sequence of instructions that we want to execute and there's also the example that i'm focusing on in this talk so let's define a simple bytecode but we then write an interpreter for it and benchmark various dispatching implementations so the bytecode is very simple it's a simple stack based bytecode and each instruction like the opcode is one byte this is where the name bytecode comes from so we've got an opcode which is the enum this is what we're going to switch over and then inspect like instruction is a sequence of either opcodes or values or offsets so the idea is that we've got an up code and then based on the opcode we might have multiple values following it and then we execute different things use them in the execution of a byte instruction the bytecode is stick based which means that the operands aren't specified as part of the instructions there is a value stack or v stack for short this is similar to our wazenbergs if you're familiar with that so we have for example a push instruction which pushes a constant onto the stack so we've got abc and then we push 42 and then we've got abc 42 on the v-stick and that constant is specified in the bytes afterwards so we've got one byte that contains the push-up code and then the next byte contains 42 and this is nan pushed to add to values again use the v stick so we pop two values add them and push the sum back so we modify them on the stack and with that we can write a simple program that adds to numbers so we push our first two numbers on the onto the b stack then we call add which pops them and pushes the sum again then we can push the next number call add and then we have the result on the b stick of course as we can see this is a bit limited for example every time we want to use a value we pop it from the b stack and can't reuse it again so we want to use the same value multiple times what we do we need an instruction for that that duplicates the top value so the dupe instruction simply duplicates the value that's on top of the b stack and then you can use it multiple times similarly the stack might be in the wrong order so we need to clean it up and for that we might need to swap to value so we've got the instruction swap it swaps to values that are on top of the stick all the instructions so far they just happen sequentially this is maintained using an instruction pointer or ap for short so this is simply the current it points to the next opcode in our bytecode and then after we finished executing a normal instruction it simply increments the ip pass the opcode plus any data of that instruction and we can also implement jumps to go loops and branches like the with that approach so a jump instruction increments the instruction points are by the offset which is specified in the next byte so that way we can jump to an arbitrary place in our program and implement loops for example so we want to jump back to the beginning of a loop and then we've got a conditional jump to implement if statements this only increments the instruction pointer if the top byte is non-zero finally we want to add a function calls and for simplicity we only consider a single function so the entire bytecode is the body of a function the argument for that function are pushed on the v stack before we start executing so what on the v stack those are the arguments and then the return value is what's left when we're done executed then we have a chord instruction which only calls the current function so it's a really a recurse instruction and it simply jumps back to the beginning of the byte kit but before we can do that needs to save the instruction pointer so that the return instruction knows where to jump back so return jumps back to the previously saved location and they are saved on a call stick so we've got a separate stack that contains the instruction patterns and with that we can we can write the following function that computes because of fibonacci so the first line is the code that's what we're doing the very straightforward precursive implementation of fibonacci which is really slow but that's not important and then the bytecode so we first so we the function starts with n on the v-stick and we want to compute the nth fibonacci number for that we need to compare it against two but this would consumers so we duplicate it so we duplicate n push to and compare it if it's greater than or equal we jump to the bottom code otherwise we continue on so it's less than two which means this means that n is already the n fibonacci number and we can immediately return otherwise and we need to subtract one this would again consume and so we duplicate it then we can subtract one because we'll jump back to the beginning and compute it recursively for the n minus one fibonacci number when that returns fip n minus 1 is on top of the v stick we then i want to compute the n minus 1 n minus 2 number so we first need to swap them so that now n is on top of the stack then we can subtract two and then we can request finally we've got flip n minus one and five n minus two on the v stick we can add them and we can return that result you don't need to follow like the exact details that the code works i've tested it but just conceptually do you like understand what's going on here how the bytecode works okay now let's implement an interpreter for that for that we need to maintain two four things the instruction pointer which points to the current opcode the v stack pointer which runs to the top of the v-stick the c-stick pointer which points to the top of the chord stick and then finally the byte code itself so we know when we because we can jump back to its beginning and then each implementation is just a couple lines of c plus plus code so when we push something we read the value in the next byte after the upcode and push that on top of the v-stick then we increment it by two to skip past the op code and the value to duplicate the top value so v stack always points at that location where we are next going to write the value so the top value is at v stake minus one so we get that and push it again so now we've duplicated the top value and then we increment the instruction button to add to values we pop them compute the sum and push the sum and increment the conditional jump looks at the pops the top value to determine whether that's not zero if it is not zero we jump by incrementing the instruction pointer by the offset in the next byte otherwise we skip past that by incrementing by two a function called simply pushes the instruction pointer or whether the instruction center plus one because that's the location where we want to continue after the call we don't want to call again this would be an infinite loop and then we jump back to the beginning of the bucket and finally we turn simply goes to the location that we last saved then we can write a function that executes bytecodes or text bytecode and the argument it sets up the b is taken call stick so the cost starts with an exit instruction this is just the dummy instruction so right now we're done executing and that way like the first return jumps to the exit instruction then we know we're done and can finish the interpreter loop and the weed stack starts with the argument and then we just start executing the bytecode by calling this page and this is really the missing piece of code so dispatch takes the interpreter state and its good job is to read the current upcode execute the appropriate piece of code that i've shown your cups let's go depending on the opcode this also increments the ip and then we repeat that in a loop and so for the women of the stock we are only going to implement this function in various different ways and measure it but before if we do that a quick disclaimer bytecode interpreters are prime candidates for remote code execution explets right so you should never start executing untrusted unverified bytecode that's somebody just gave you it might can do arbitrary things it might escape your sandbox right this is a security vulnerable patient this is a talk about performance so let's ignore that and start with the most basic dispatch implementation the switch statement right we've got a loop an infinite loop we switch over the current upcode for each case we just insert the c plus plus statements i've shown you a couple slides ago the exit instruction simply breaks from the loop and returns the return value and then we're also marking the compiler that these are all the instructions that you care about so don't assume that any other value can exist so i've now shown you spread over multiple slides the complete interpretation of the interpreter if you don't know in full detail what it does don't worry we just pause the youtube video go back to the end of the slide i'll just share a link to the full source code so you have sort of you can't do that here okay so to get the feeling for the performance we're going to look at the generated assembly code who here can read assembly code who had thought i was referring to x86 assembly code and answering that question okay i'm going to look at assembly card in a 64-bit arm but don't worry it's actually easier to understand because it doesn't pretend to be a cpu from the 70s so amp has 31 registers they are nail label 0 to 30. quite reasonably if they have an x prefix there are 64 bits so this is a 64-bit arm architecture and with the w prefix you get the lower 32 bits but it's otherwise the same way just and then we quickly the addressing modes that we so it's about der referencing a register when you have a register that does a pointer you can derivance it directly or buy an offset so add an offset before do you referencing it you can also combine the offset with an increment and you can either increment it before you dereference it after so you increment it and then get the address or you get the address and then you increment it like in one instruction and finally for array indexes you want to index it with another register so this we have got a base address at x0 we add the index in x1 shifted by three and get the value back this is useful for every editors but don't worry we all assembly code is annotated so this is the assembly code invited for the switch statement we've got a label for a loop and then we load the op code so we load one byte x0 is always going to be the register that contains the instruction pointer so by loading the current top code storing internal register and then we compared so we first compared against d1 this checks is the current upcode push if so we jump to the push label otherwise we continue compared against the add instruction jump to add and so on until we finally have an unconditional jump to the exit instruction this is because we told the compiler that there are now more values left then the post instruction generates essentially like straightforward assembly code for the c plus plus instruction so we load the value that we're pushing we are storing on the v-stack with an increment incrementing the instruction pointer and jumping back to the loop very straightforward what we're essentially doing is we're getting the opportunity against impossible value and jumping into the perpet label this is a linear search but the actual labels themselves they're sorted and if you want to find something in the sorted sequence there's a faster algorithm binary search and actually clank generates binary search so the actual generated assembly looks something like this it's a bit tricky to understand the assembly code so here's protocol so we're checking is the instruction pointer between 0 and 3 if so we know that we're in that half then we don't like it the final message and this simplifies like reduces the number of comparisons that we need to make until we figure out where to jump so good job compiler there of course it's completely meaningless just to look at assembly so let's measure it and for that we can use the recursor fibonacci implementation because it's really good because it has exponential blow up so with just a few bits of bytecodes we can because it like doubles calls itself twice in each iteration we get more and more bytecode and can easily execute like millions of instructions to get like really long times that you can go to benchmark so we compute the 35th fibonacci number and get this result if you can't read the number don't worry it's about 460 milliseconds never thinks that is fast 460 milliseconds to compute the 35th fibonac chamber who thinks that number is completely meaningless without any context exactly so how to benchmark you need to take multiple ones because of noise then you need to report the averages and standard deviation none of which would edit and then you need to compare it against some alternative implementation this is a bit tricky but luckily there's a really great command line tool that you can use hyper fine so hyper phone can be used to command to benchmark command line applications it will run them repeatedly gives you the average and standard deviation the entire execution range and then at the end gives you a summary of which one is the fastest i highly recommend you get it it's one of those suite of modern with command line tools that are really useful so just for the sake of arguments if you don't have an alternative implementation yet let's just assume it's too slow and we want to optimize it so how to optimize step one is we need to guess what could be a problem right this can mean educated guess or just a while speculation like we need to get something that we want to addict deeper and my guess here is that we have got a problem with the branch predictor so what i mean by that the actual assembly instruction on the cpu this is very simplified happens in phases so for example we first need to fetch the memory for the instruction then we need to decode it and figure out what it does this is essentially a dispatch loop built in the cpu so we need to figure out what the instruction is then we execute it and then we can write the with aspect and because all of those steps happen on different parts of the cpu we can actually execute them in parallel so this is forms the instruction pipeline and the idea is that while we are executing one instruction we can already fetch the next one in parallel to get a reduced latency of course this is an issue with branches right when we have a bunch instruction we don't know what's the next instruction going to be it can be one of multiple locations so what does the cpu do it simply guesses right it guesses whether the bunch is taking or not and just starts executing its guess if it's correct the good job we made efficient use of the python if it's incorrect and it started executing something that it wasn't supposed to execute it has to clear the pipeline while back this is expensive so we want to guess correctly this is where the branch predictor comes in we sort of remember like a history of in the past this branch was i was taking so we guess it's taking and start executing it that way we can make a good prediction and make use of the cpu pipeline but in our case we're executing different instructions in every loop iteration right so it can't guess whether or not the bond is taken because every loop iteration be taken like doing something completely different the branch predictor is useless here so let's say that's the problem the next step is we need to measure that this is in fact causing the problem and in this particular case we can use per stat per stat is a linux command line utility that you can use to query the hardware performance counters in our case we want to figure out the number of branches and the number of branch masses so we do that and we get about 1 billion branches this is the actual number of branches executed not the number of branch instructions in the code so while we're computing we executing one billion branches and then the yellow part those are the branch methods it doesn't appear to be a lot but maybe it is like it's meaningless without a comparison whether or not that's many branches or not but again let's just assume there are too many branch measures what do we do we somehow need to work around them so if we don't we don't want a bunch instructions so instead of like comparing the values let's just jump to the correct execution code directly with function pointers that's the idea this is called call threading and it's called wedding many of those dispatching techniques are called vetting they have nothing to do with fats right it's a different metaphor that's called felling so the idea is to use an array of function pointers so we've got separate functions that execute each instruction so we've got one function that executes push one that executes add then we keep them all in and away and then inside our loop we index into that array and call the appropriate function pointer right that way we don't need to do the conditional comparisons to figure out where to jump to we just get the address and call that function of course the function itself need to take the interpreter state by reference right because we modify the instruction pointer in it and we want those changes to reflect back questions about this particular dispatching approach so let's look at its assembly codes this is the main loop in the middle we are getting the address from the array so this is the indirect instruction that i mentioned in the addressing mode so x20 stores the base address we're going to be indexing it with w8 which is the opcode then we're calling that before that we need to prepare the arguments the arguments are references references are pointers and so the actual instruction pointer and these checkpoints are there are stored somewhere in memory in this case on stack memory and we are setting their pointers by computing the offsets then we can call them and when we return we need to reload the next opcode somewhere from the application as dot in memory compared against the exit instruction to now to access the loop and if not we jump back to the beginning so now we don't need to do like the linear search or binary search to figure out where to jump and we are a lot faster so new benchmark results in green be a lot faster okay it's significantly slower except not actually because this is a really bad graph i sort of used like a proper tool to create the batches instead of just went to some arbitrary website plugged in the numbers and got this budget if you look closely and in the last rows you can't it doesn't actually start at zero it starts at 450 milliseconds and the switch statement takes 460 milliseconds so it appears that we are significantly slower whereas actually it's just 75 percent this is how the voucher looks correctly right we have about 75 slower i've actually started zero so watch out for that this is like a really common technique to create misleading wealth so why are we slower the answer is memory over it what do you mean by that this is the do execute push function if it would were to take everything by value again i've shown you the simply code in the switch again we're loading we're storing we're adding quite simple this is the same function if you're having called by reference now we don't get the arguments as arguments but as pointers to the arguments so before we can load the next byte we first need to load the location where the v-stick pointer is stored and then we can load the actual byte and before we store it we first need to store the mistake pointer then we can write into it and then we can store the new one back into the v-stack pattern so we've got this extra indirection in here because we are cpu can only work on register values so we repeatedly need to reload them from memory which is slow okay can we do better can we do the same approach but without the memory of it this is where the token threading comes in it's also called indirect threading it's also has nothing to do with tokens it's such a confusing naming schema and the idea is that instead of calling having away your function pointers where we call a separate function let's just jump to the same location within our same function this is possible with ignore extension computed go to so c plus has a go-to statement you probably know that and if you didn't i i didn't tell you so go to statement we can normal c plus code we can label a statement and then we can go to that label it's useful if we want readable control and computed go to adds two things we can take the address of the label with double with double ampersands because of course this gives us a void pointer and then we can go to that library referencing that address and yes we do our fencing avoid point so it's just part of the syntax so we can do reference avoid pointer and go there and now essentially the same approach but instead of an array of function pointers we've had an array of labels and then inside our loop instead of calling function we go like we're getting the address from the array and jumping there and after the la it's execute push for example we continue by going back to the beginning of the while loop and continuing on with the next iteration and you don't know that the 70 code looks very similar at that score so we get the base address if we get the opcode we get the index into the area we jump there and then we jump back to the beginning of the loop except the compiler actually doesn't generate this sort of assembly code the compiler looks at there and sees okay so after i have done push we're jumping to the beginning of the loop and then we're jumping somewhere else it's let me just jump to the correct location directly right right go to the immediate step so it duplicates the code at the top aft and paste it after i do execute push to execute add and so on and this is another advantage that we can combine the load of the opcode with the increment so we can increment the instruction pointer to the next and load this opcode in one instruction so it looks like so we only have a single jump at the beginning to bootstrap and then after each execute push we're loading the next upcode and incrementing the instruction point at the same time getting the address and just jumping to other location and this is also by the name value comes in because the control flow now is completely not obvious right we're just jumping between the function all the time it sort of like forms the thread or something i think that's the metaphor and because the compilers generating assembly code like that is sort of like the economical way to write it in c plus as well so this is the same thing in c plus plus no loop we have an initial go to to bootstrap and then a separate go to after each single instruction so this is a token threading and it's like it's essentially the same thing as with the functions just with computed code so let's benchmark that approach are we faster yeah and this starts at zero so we know about twice as fast compared to this switch but note that if you look at the assembly code all those instructions start with a b those are branches so we're still doing bunches so what about the bunch misses let's measure at the bottom we've got significantly fewer branches because we're only having one bunch after every instruction instead of the binary search and we also got fewer branch methods overall but presented twice and an absolute value so we've got fewer bunch of methods now so why do we have fewer bunch misses well now we got duplicated branches so with the switch we've got a single dispatch at the top that was shared for all instructions this means there was a single location the branch predictor can be used and they can only learn about what's the most common instruction but the further dispatch we've got a separate branching point after every instruction so they can be separately predicted and the cpu can learn for example okay after push instruction we're having a super instruction so we predict accordingly and after super instruction we're having a return or something right so we can learn what's most likely going to follow each individual instruction and we get better branch prediction so that's good now we want to figure out exactly what's slow so let's say we want to figure out what's the most common expensive instructions in our program and for that we can use a sample based profiler for example perfect quote it runs the program and frequently asks the cpu hey what function are you currently in and then we can get the most expensive functions in our program and it helpfully tells us that 99.85 percent of our runtime is spent in this bit yeah that's not really useful this is actually interactive like if you want it in the command line you can then go to this patch and get the assembly instructions inside this batch annotated by performance but it's well like really difficult if you're just trying to figure out okay what's the most expensive instructions for that we really need separate functions because perfect records and perf report work on a function basis so can we combine the separate functions for executing instructions but without the memory of it and we can using tail codes so the idea is to go back to separate functions as we did with code i think but now instead of passing everything by reference we're passing things by value and we're taking the idea from talking planning where each function now calls the next one so after we execute the push instruction we're calling the next one with the new values now we don't need to pass everything by reference because we're not returning to our caller we're passing the new values along to the next one of course this is a function call and the function call means that the cpu needs to remember where it came from so it needs to push the program counter jump to the label of the function and then return pops the program counter and jumps back this means that we start executing dispatch we put its program counter and jump to the first instruction so after we've done finished executing we push the program counter and call the next instruction after it's finished it jumps to and so on versus the program counter until we finally reach exit at which might be done and we do a return this pops the program counter and jumps back which returns pops the program counter entrance back pops the program count and jumps back and so on until we finally back at this page so if you try this technique we're going to get a stake overflow because every single instruction is a function call and we've got executing like millions of them in this little piece of cut but it's a bit silly what we're doing like so for example the duplicate instruction it calls the next one which is an ad or something and then but to do that it needs to remember the program counter and when app returns it goes back to the duplicate instruction and it immediately goes back to its caller so why did ed bother coming back to do we are just going back to our caller it should just skip us and go back to its caller that's the basic idea so when we got a function that ends with return fool we don't need to come back to us right we can just forget about us we're just going back to our color so we just jumped to foo instead of calling it this means that this bed doesn't actually call so we push the program counter but then we just jump to the first execute when it's finished executing it jumps to the next execute do execute push when it's finished executing it jumps to the next one we don't keep incrementing the call stick so finally when we reach the exit instruction we do a single return that return will look at the program counter that's on top of the course deck which is the dispatch one and so it immediately jumps back to dispatch and then nothing was lost we don't need to come follow the chain of instructions at the end backwards because they're just going back to our caller just go to the top level immediately and skip all the intermediate things this is called a tail call right because we're not because we don't need to come back we just jump and forget about us and compilers can do that for you it's still call optimization but it's we can't really rely on like we absolutely need that optimization right if we implement the interpreter like that we can't run it in debug mode otherwise we would have a stackable and this is where my favorite c plus plus attribute comes in it comes from clang it's called krangmaster and we can use it to annotate the return instruction and then clank is always going to generate the telecom even in debug mode even with optimization disabled and it will also give us an error if it can't do a tail curve so we can rely on if the program compiles we're getting a take for example they can't get the state call if you're having codes after the return statement like implicit destructor call right we're not going back to our functions that can't issue a call a destructor to destroy our local variables so this is a really useful attribute and with that we can just write the interpreter like that just but annotate everything with cling master and now we generate assembly code that looks i almost identical to the previous one so we've still got our labels we still got the arrays we're still jumping there the only difference is that now the actual address of the table needs to be recomputed but otherwise it's exactly the same assembly code as with the previous approach without telecalls the difference is now on a function level and so the actual performance like it's almost identical and this particular image is faster because i picked that image that way but actually if i were to report the actual noise like they are identical right it's no difference in execution time just pick one that's underlines my message and you shouldn't copy this benchmark results you should run your own matchbox and because it behaves the same as the threading approach it also gives you a almost identical branch masks i don't know why it's actually like slightly different amount of branches in there and bunch messes i have no idea like cpu is a black magic i don't know what's going on but crucially because we're now having different functions perf works we can ask perv what are the most expensive functions and it tells us that 16 of our runtime is spent doing push so we might investigate optimizing push further so that's really useful questions about this technique yes music new to label and hospital so as far as i know craig must tell like we must tell sort of behavior is only available in clang there was in the beginning of the year there was a discussion about the gcc mailing list but they need to support esoteric targets whether you can't really do take holds so my current strategy is like the projects that use them are neat clang right it's really cool technique but unfortunately only needs clean okay so really great so far let's talk about the register keyword so the register keyword comes from c and it tells you to please compiler please keep this variable in a register this useful as an optimization because as we've seen if we keep things in register they are a simple faster right of course modern compilers will do that for you which is why c plus plus still has it as a keyword but it has no meaning in c plus plus except they don't right it's an optimization so sometimes the optimizer doesn't really want what you're doing i do not mean to take my word from it you can take my pals from it so he is the author of lourdes which is just in time compelling but it also comes with an bytecode interpreter similar to one we are doing but his is written in assembly and and this email that link is also and yet in the resources he talks about why it's written in assembly so it says that we can use like a direct or indirect threaded interpreter and see just they're doing with computers go to but the advantage disadvantages is that like the register allocator of the compiler doesn't really understand that and in particular there's like no way to say that we want the same register assignment before each go to so for example if the instruction pointer should always be kept in the same register even if you're doing pusher if you're doing it it should be kept in the same register so that we don't need to move it between registers just because we jump somewhere else and there's no way to express that the compiler so let's talk about calling convention when we have a function call in assembly we're just jumping to an address how are the arguments passed this is what you calling convention dictates and it states that on 64-bit arm the first eight registers are passed in first aid arguments very straightforward so now what we call the function the compiler will put the arguments there and then the function knows where to find them this means that by using tail codes and functions we are forcing a particular register sent right because the instruction point is the first argument it will always be in x0 right as we can see in the assembly code x0 maps to the instruction pointer so when we then call the function like execute push the instruction part will be next year then we're doing something and at the end we're calling the next one which has the same signature so ip must also always must again be in x0 for the call of course the compiler is free so like move it around in between but that would be city right so essentially we're forcing the compiler to use a particular register assignment this means that if we want to ensure that something is kept in a register just pass it as an argument to the function we can force this register allocator to do a good job email continuous listing advantages for writing things in a similar like keep the fixed register assignment but we can also do that right as i've shown you we can keep the fixed registers and and then for slow paths like if an instruction has a fast path and a common case and then sometimes need to do something slow we can move them elsewhere to help with eye cache density and so on so let's investigate that this is a silly instruction with a slow path so we want to print 42 if the top value is 42. we can then benchmark this instruction by changing by code for flip 35 to start with a print 42. in this benchmark we are never going to printing 42 because it will always like check the argument and the argument is always 35 for less so we're never going to print anything so the performance should no different and it shouldn't be significantly faster it doesn't start at zero it's for milliseconds slower if you have 0.42 not like eight times as low or something right really watch out for those so it's 4 milliseconds slower for print 42. this isn't the much but it's a bit sad because like we're not printing anything ever right we're just paying four milliseconds for branch that's not taking what's going on well if you look at the assembly code for the print 42 function it's really obvious like what's all this crap so at the bottom left you've got a call to put right recording puts that's okay but to call the function we need to set x0 to the string but if there was our instruction point is that we need to save it somewhere so we save it a couple of instructions above in x22 but x22 might have a different value already so at the beginning of the function we push x22 on stack and then we when we end the function we load x0 back from x22 and we also load the previous value from x22 from the stack and the same is true for the other argument where this does because puts might internally call another function which populates those so as soon as we call a function we need to save all the vertices right and this is all right and it's a bit silly because what the compiler generated here because it always saves all the instructions registers even if we never end up calling puts right it does it unconditionally which is a bit unfortunate but simply like the compiler doesn't know that we are never going to call puts it doesn't optimize that it knows that as soon as we call the function we need to save all the registers so it saves all the registers even though we might not actually end up calling the function but don't worry we can help it a bit we can use a take call so instead of call inputs directly we are just tail calling to a different function which calls puts unconditionally now they do print implement this calls the function this is slow this generates a lot of spilling but the initial function never ends up calling function right it only tell calls the function so the compiler doesn't need to save anything in any register so we can sort of like trick the compiler into generating the good assembly code and we really need to trick it because it absolutely doesn't want to do it right we have to print input as now inline because otherwise the commander says oh that's a silly function and just inline it above and then generates the bad assembly again but we're really fighting with the compiler here but if we we can end up winning by just slapping enough attributes on it and then the compiler will generate the assembly we want this might be essentially writing assembly code here just in c plus plus but it works and we get this nice assembly we're getting the top value we're comparing it in 32 and then we might jump to the print function or we might not and with that piece of code we get the same performance again which makes me really happy because like as soon as you're of instruction that might sometimes be slow or call another function it could be you lose all performance benefits of take calls because then we need to save things so this is really common for example we might want to dynamically grow the call stack on which course so we don't have a stackable flow in the interpreter so when we reach the end of the code stack we want to go out this is expenses so we tell you call it somewhere else which actually performs the grill this is the expensive function but it's almost never called and then of course we want to return back to our caller but we can't do that because it's a tail call right when we take well somewhere we lost track of who quotas so what we're instead doing is we're jumping back to the beginning of the function then we're checking the condition again but since we've grown it we're no longer at the end of the stick so we skip over to continue so this is like absolutely non-obvious control flow but like it's fast right so that's the price you have to pay like as soon as you take a call somewhere yet then you take hold back to the beginning and then you need to cap check like if we already increment the instruction pointer or something before we do it and then we jump back like this is like really difficult to remember of what you changed like it's tricky to write but it's fast so this was the original conclusion i planned for the talk frank must tell enables padding where function calls which is really good technique you get detailed performance tracking and perfect code you can force the compiler to use a particular register sign by simply passing it to passing it around but you need to remember towards the surplus no regular function and not code and that way you can trick the compiler into generating the exact assembly you want all is good and then i thought well at the beginning i have a disclaimer that you should benchmark on the exact target hardware that you're targeting so just for fun let's benchmark it on my laptop so let's benchmark it on my laptop so new benchmark this is a 2016 thinkpad 13. so an older cpu running out of inclined 14 and as you expect you get the exact same performance behavior why is switch suddenly itself much faster than everything else right what what's going on here well if we look at perth stat it's really obvious what's going on here we're having significant amount of branch misses and the other techniques so switch on top and then we've got threading entail chords and they've got like almost 25 percent of their branch misses why but the difference is brand the issue is branch target prediction so with the switch we've got a conditional branch but it has a fixed target so we're getting each individual value right then we're comparing it against something and then we might jump the decision better not be a jump that that that might change but if you jump you're always jump into a fixed location whereas with the jump table we're always jumping right we're unconditionally going somewhere else but the address is completely variable right we can just go anywhere so branch target prediction is responsible for the item code it needs to determine where branch is going the original branch predicted only checks whether or not the boss is going but bunch target prediction whenever a variable target needs to figure out where we want to go and that's a more complex thing right yeah there are multiple destinations that is more difficult to do and they simply like this is a cheap laptop cpu they saved on that so this has a really poor branch target predictor which is why we've got the high number of branch masses in our code so this is so right we need to work around that we need to we can't use variable targets and this is possible we can just use a switch right instead of the lookup table we would say calling something we're having a switch and then if it's push we'll take holding to the do execute push function if it's 8 we'll take calling to they do execute it and now we again got a fixed target that may or may not be jump so we get now now no need to rely on branch target prediction because we got fixed targets and this means that the number of branch faces now at the bottom it's like exact behavior as with the switch just because we're doing the binary search but few number of merge methods and this makes it faster right we're now a similar perform with the switch i thought hey that's an even better conclusion because it turns out just trust the compiler to do the dispatching for you it knows best right i've just waited for wasted 40 minutes of your time just write the switch statement everything's good except not really because that particular like like if you have a good branch target per picture like the jump table is fine right this can be predicted it is faster and i've also lied about you because this isn't the actual generated assembly first pitch this is the actual smd generated phosphate and you've seen enough assembly code to recognize that's a jump table so now the compiler doesn't know what's best either it generates a jump table which is bad on my cpu right it has no idea it only generated the binary search after i nicely asked it to not generate a jump table because i wanted a different dispatch implementation to compare it against two in benchmark results right so by default it will generate a jump table and there's a flag like f no jump tables or something and then it will generate binary search which is what i need on my laptop so now compiler doesn't know best you need to run your own benchmarks on optimizations look at the assembly code look at the bunches so now i want to talk about two more advanced dispatching techniques so we've got a token for that dispatch and the idea is that the instructions are essentially like tokens like integer values and then we index into an array this might be an array of labels on a way of function pointers and then we might use computers go to a tail cost right it doesn't make a difference actually in the assembly same effort so why not cut out the middleman instead of having a table and we index into that table and jump there let's have the op code be directly the address where we're going so instead of an enum we now got the address of a label or a function and then after we're done we simply go to the next address or call the next function right so the bytecode opens themselves are no longer even values small byte integers but full 64-bit addresses and then we don't need to do the array lookup we can just immediately jump there and on the assembly code right this generates will decide assembly code we just go there we get the next one and we go right really good so the best dispatch code so far right because it's like if you want to go somewhere we just go there right can't get better than that but the downside is now the op code is 64-bit instead of 8-bit right this makes it bigger we might get one into issues with cache prediction we need branch target predictions or the support is slow on my laptop and it makes it really trivial to the remote code execution exploits because we're now jumping tool and the value that the gave us right there is no check so we might jump anywhere in our program which is like not ideal and i haven't presented the benchmarks because if you want that do their own virtual for the safety implications but on my laptop like it's slow because bunch target and on my m1 it doesn't really make a difference right it's within the noise from the other technique because the airway overhead isn't that much and the bigger benchmark might cause issues there so this was the best dispatch code so far the approach push one push 2 and the net so we want to execute the assembly code for push one then the assembly code for push 2 and then the assembly code for add so let's just insert the assembly code for push insert the assembly code for push insert the assembly code for add right this isn't just in time combination we'll sort of just copy and pasting the wireless assembly code adjust the time compiler would for example be loading v1 for memory and it was just inline it and just directly stolen it's just sort of a simpler way we just know the assembly code and we just copy and paste it around in the correct order and get the idea here that's called inline further dispatch and the idea here is that the best dispatch is now dispatch right we don't have any dispatching code we just after you've done the push we just start executing the next push and then we start executing an ad but the downside is this requires testing time compilation so it's not available on all systems you need to be available to runtime dynamically inbuilt assembly card and for that i've got the actual conclusion for my talk benchmark on the target hardware then optimize right it's really difficult as we've seen like different systems make different different things but we must tell technique in general it's like flexible you can combine it with different dispatching techniques to get full control you essentially get assembly code without writing it false lines source code is available at that url follow me on twitter but that's still the thing and if you're interested i'm going to be writing a c interpreter live on youtube so subscribe there it's a bytecode interpreter using the dispatch techniques of representative thank you foreign now have time for questions so you know if you do an immediately executed lambda yeah could you do that instead of a total well i mean you can you annotate that with a toe call and then do that and then it would make the bit where you sort of going off over there and coming back a bit less grim because that doesn't actually work you mean i mean lifetime so in this this code instead of returning what we with using immediately yeah so grow cool stack you put that inside a lambda which you annotate with knowing with well you make it cold take it out of line and will the tail call work in that context so what you can definitely do is you can like they're like the take caller plus do return statement right the return that calls a function and that function can mean immediately invoke lambda as long as it doesn't have a destructor that doesn't but yeah like you still i mean then it's a bit more obvious that you're because you see the code but at the end you still need to be turned back what i haven't tried is to mark just mark a function as cold and call it and see whether the compiler then does the exact assembly immediately i mean i think i think it does do that i think i think if you mark if you've got an immediate i think it just will go off to there and then come back i'm not absolutely sure yeah i've looked at this in connection with like optimizing their own exception yeah so you take all the all the stuff for preparing the exception take it all out of the hot path and i i can't remember if it does if it messes about saving the stack saving stuff down to the stack before it does it yeah but yeah i was just curious yeah yeah like it's something i thought like it just depends where the functions are placed in memory and to how their whether they are predicted and things like that and remember that actually affects like the register saving code if you have a code function that you might or may not got it might be like it might be smart like it should be it definitely should right because like there should be some way to prevent this sort of behavior when we know that this is so just i mean just saving that inside the branch doing the safe there you know there's another thing on on intel architecture interregister moves are free because they're done at the at the yeah instructions yeah so definitely the the moves themselves they are not the issue the issue are the stores and the loads at the beginning of the function yes right in front of you i really like most tail is there a paper to standardize it sometime i don't think it's going about going to be standardized because the standard does know about telecalls right it was there was a coaching proposal that relied on sale calls that sort of like at the first step we need to formula state calls and i don't think that leaving that part didn't went anywhere so yeah it's like we are forcing the compiler to generate assembly code it's highly specific stuff the standard isn't but i did i mean like it would be ffgcc supported it right then i would be happy but yeah i mean it's it's just a shame the to not standardize it because yeah here you're right it's a bit niche but yeah there are cases where you really would like to use tail calls yeah but if you just blow your stack up yeah you have to rely on optimization and yeah yeah cool anyway thanks maybe more a comments and a question have you considered instead of making dispatching faster making dispatching a lesser occurrence by fusing op codes yeah so this like if you're watching mobile interpreter for example that i mean i have time so i go to the slide this one ah this one if you see colors like this so in this particular case we now push this button and like if you look at the bytecode we do push one we do push two right it might make sense to add dedicated op cards for push one or push two or i'll be doing like a push one sub so it might make sense to add a sub one of using instructions together this is also optimizations you can do i haven't like i'm using this technique in a bycode interpreter i'm using it doesn't make sense for the talk obviously because it's about dispatching but like to do that you really need like a sequence of real world programs to figure out like what are the actual common instructions with this wire writing a c interpreter that uses that bytecode so you can get vlc programs figure out what are the expensive instructions and optimize accordingly that you really need data and not the micro benchmark that like you can make every benchmark as fast as possible like and then that particular function is fast like i can add a fib instruction and then we're just right at the very back now exercise so why not just write it in assembly rather than yes the compiler so right now like at this point we're essentially writing assembly code just in c plus plus the issue is in my particular case i'm programming on this laptop which is x86 and i'm programming on m1 which is an arm architecture so i would need to buy two interpreters at the same time which is just a lot of work right so it's like this is more portable than writing it in assembly except you need to benchmark on the target yeah except they need to benchmark on the target hardware yeah but just benchmark is like different than writing assembly code i actually like i mean i will and enterprise in the compiler at some time anyway and by that point i can write it in the assembly code but for now like it's quicker to prototype something and you'll get like most of the performance as i've demonstrators but thank you argued hi will be a benefit of just writing one function with many go-to's instead of function calls and lots of proprietary attributes which don't work on every compiler i mean like like writing one function and keeping you know being careful that you only use variables and add the variables and no destructor can be not called i mean that's that approach right we've got one function that called hence a bunch of this is like the usual token one and the main reason i switched away from it is actually to get like better perfect code and then i like learned about the other advantages like it's sure you can also use that technique like this is sort of like the traditional way of how your web like delicious says if you want to write a fast interpreter and see you use computers go to right and then everybody uses computers go to without thinking about it i didn't come up with the technical technique by the way yeah just foreign yeah hi thanks for the great talk this is not a question this is rather a comment i can only add to your point if you really want to have the ultimate performance you can't be generic you have to know your precise hardware and its characteristics and you have to know your compiler so i work in the automotive industry and we have weird embedded system processors and weird outdated compilers and yeah it's cool what clan can do these days just yeah we don't have it at the end of the day you have to live with what you have and yeah you can't be generic or you're suffering from performance issues yeah so that's the point yeah thanks for adding to that like i really like i added the slide run the benchmark on your own target hardware and i thought like let me just follow my own advice and then i discovered like it behaves completely different on my laptop right and then i optimized like edit the switch for now for my real bytecode interpreter to use the switch versus the lookup table based on that so you really should do that right so that's a key message to the big companies who dream about having a software platform that runs on any kind of thing yeah it will run but it will not run optimal and that's just the way it is and you have to live with that thanks okay thank you [applause] thank you