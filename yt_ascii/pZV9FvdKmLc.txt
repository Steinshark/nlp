in my very first professional programming job i had to write a financial report the heart of which was an sql query so i worked and i got it working and i was very proud of myself until we ran it against the staging data set and it ran like an absolute slug and then a more senior colleague of mine said put an index on it and i did and it was fast brilliant i now knew how to make databases run fast you throw indexes at them so i started throwing indexes at everything and wouldn't you know it the staging database starts slowly grinding to a halt perhaps you've already diagnosed why but that was my first real world experience of if you actually want things to work properly you have to understand a layer beneath you have to understand something of how things work under the hood so this week's kind of fun cuz we're meeting someone sort of coming in the other direction i'm joined by hannis mizen he is a professor and a database researcher but as you're about to hear he's absolutely not content to remain studying theory he's come out of that ivory tower and he's built one of my favorite new discoveries of late duck db came recommended by a few colleagues of mine and i've been really liking it it's a local convenient analytics database it's fast i like it enough that i should tell you this episode isn't sponsored by them i just thought it was good so i wanted to he back the covers and understand something of how it works and it turns out to be quite a treat and hannis is very good at explaining it if you're curious about how a database can chew through a billion rows in a few seconds or how you parallelize queries across multiple cores when multiple cores seem like the only way that computers are going to get faster anymore hannis is your man he's an excellent teacher so let's go and hear from him i'm your host chris jenkins this is developer voices and today's voice is hannis [music] mizen my guest today is hannis mizen hannis how you doing very good how are you i'm very well i'm very well you're going to take me all the way down into bits and bites today it's my favorite things to talk about like these bits i don't know what's what what you know what's so great about them but somehow they haven't kept me they have kept me occupied for a while and they've been drawing us down since at least the 50s right so but i want to get into your background first because i have to understand why it is you wanted create what you created my background is yeah that's very interesting well i guess not it's a super vanilla of you know sort of timeline no i at some point in in in my life i found computers it was like 14 or something like that and there's a fun story that i want to tell so i had a computer that my dad sold me okay he's a good businessman it was discarded old computer and h it had a bios password if you remember those set on it oh yeah yeah yeah and he didn't know anything about computers and i didn't know anything about computers but i just had spent all my money on this thing that had this like you turn it on and it showed you this password prompt right your dad sold you a brick he did he did and he's i'm telling you he's a good businessman and then and then it's like yeah okay what's this password problem and there was no internet people today cannot you know conceptualize this anymore but it's like there was no internet so you can't just google how to reset bios password you couldn't just go to the library they didn't know either right yeah so at some point you know you found somebody who knew somebody who knew somebody who knew somebody you tell you like take the damn battery out and and i think that's that's kind of i don't know this kind of way of thinking about problems and that they're just like a challenge and they're just there to sort of you know to be overcome and not something that you could that you have to give up on like maybe it was lucky that the first problem was actually solvable but yeah yeah so your dad actually sold you sold you a puzzle box and a career at once kind of yeah and so so then when you know i started programming like many kids with php and mysql i was the one of the founding members of my hometown my hometown's mysql user group oh cool i had a license plate with sql on it because i lived in stutgart and german license plate start with s so you're my kind of g ah it's very bad and and then and then i when you know i needed to study something i was like all right was looking to forestry for a while okay looking it for a naval career but then was like fine study computer science and and yeah so i did that and then liked it went went to berlin got my phd in computer science and your phd my phd is in is about distributed quer processing which is kind of the the anti-a of what we're going to talk about later which is it's sometimes interesting to see you know how how how carers kind of go so then i came to amsterdam after p phd bit of an accident kind of cuz i didn't come here for like career reasons i came here because i had a girlfriend here oh and i was like okay so i better find a job so i looked around and it turned out that there was one lab the dutch the dutch national research lab for computer science and mathematics okay that was hiring in a database group and i thought i wasn't in my sql you know user group i'm yeah i could do this and of course in a gigantic over interpretation of my own knowledge you know anyways that turns out this group is actually was actually very good at data systems that coming up with new ways of to construct data systems and so joined as a post stayed like a 10e track the whole thing yeah and ended up to be a a very a place where very clever people i've been thinking about data systems construction for quite a long time for over like 20 30 years oh wow i learned a lot about the fundamentals of how we do querying yes although in cs you always have to be careful when somebody says fundamentals because very often they mean that they want to hurt you with greek symbols okay i am not i am not about the greek symbols i want to make this absolutely clear i a painfully practical person it's so practical that i always wonder what i'm actually doing in academia it's like i was wondering a lot what i was actually doing in academia because it's like we're not people laterally told me it's like we're not here to solve problems we're here to talk about problems it's like yeah that that's rather simal but i i don't know i so anyway so the the the fun you said fundamentals that's absolutely right we we learn a lot about the fundamentals but not in the way that some people might think fundamentals as in like you know formulas and proofs and these kind of things was like now here's a modern computer works and here's how you have to sort of hold this computer funny so it actually solves data problems efficiently so that's that was really all that was about it's fundamentals yes it was about query processing you know memory pattern access patterns that sort of thing we can we will probably go into that more later yeah i hope so yeah and then okay yeah that was the academic then i got i even became a professor i am still a professor yeah on on officially on data engineering at the university of nan i'm also still affiliated with the lab the database architectures research group at the research institute but these days i spent a lot of my time at duct labs the company that we spun of at some point from the institute around this dub project that take me through that how do you go from being a professor to being an olap database writer aha practical data because real gap between professor and that programs is almost unheard of okay this this again you can call me cynical again if you want and it's only been like 5 minutes of podcast but but professor that programs is pretty uncommon yeah so how does this happen well i think in data systems it's you can write papers but it's kind of understood that a lot of your impact will derive on have you actually created any systems that are sort of relevant in the world out there right have you cre cre software that's unusual un academia the writing of software is seem is just you know yeah it's not respected a lot it's the turning of the handle that gets you to the paper that yeah it's true it's not respected a lot and it's also the quality of software producing academia is also typically very bad as a result because it's just as you said is means to an ends you want to get your nature paper is what you're going for the sort of the r script that analyzes this you know the the result from your you know telescope is like whatever yeah but now now actually in in in cs especially in practic computer science especially in data systems it is actually is actually the case that the people that have gotten highest honors in our field we have two touring award winners in in data management systems is jim gray and michael stonebreaker they both have created actual systems and know do you know them well yeah yeah pogress so stone breer was one of the people behind the creation of posr at berkeley you know way back when right and and they have gotten these awards not for their you know greek symbol sort of fighting abilities but more for like you know they have actually built something so it's i think in our field and there maybe some adjacent fields like operating systems that behave similarly like if you're an academ in operating systems it does help if you have you know made some linux kernel patches that turned out to be good ideas like i think it helps right if you're in security making tools is always going to get you some of credibility right so there's some adjacent feels but it's not very common i agree so it was always very yeah it was definitely acceptable in in in this sub field to write to spend time writing software we may have pushed this to new heights i'm i'm not entirely sure but so so so when you come up with a new idea you can write a paper about this new idea yes you can build a prototype to show that your idea is a good idea sure but people are only really going to take this seriously if this has like proven itself somehow in the market okay right that's very unusual to get h in academia you need to push yourself out into the marketplace it is it is a it is a i think there has also historically been a lot of sort of exchange between the academic teams and the the the the teams in in industry like you know like you have in ai as well like there's a ton of people doing cutting edge ai research at google they're not technically academ academic people but they are at the same sort of level i think data management systems we also have had this for a while that the teams at like at microsoft that buil secr server the teams at oracle that built oracle teams at ibm that built db2 they had like a very deep sort of knowledge in sort of in these in these systems and this the academics would frequently you know come from these teams go to these teams do sabatical there go back and forth so i think there has always been a pretty good exchange but if you want industry people to you know take you seriously you have to show that this can work in in in the marketplace and it is a gigantic market i think people underestimate this so databases are sort of a trillion dollar market or something like that right i didn't know that but i could believe it yeah so if there is such a huge amount of money flowing like flying around in this in this in this world you can see how you know like if you want to convince somebody that something's good idea you can show them okayy this is actually better it's going to make you more money it's like anyways there's there's always been a bit of connection but so what we the the reason i think we started programming so much in in the institute was that we were sick and tired of the quality of research prototypes right and we were like and we're like okay we actually want have impact with what we're doing and so when we started this dctb project we were like you know what we'll we'll do it you know for real this time like we will have you know ci on the third commit or something like that we have right we'll have testing in place we will have coverage in place we have all these software engineering tools that are pretty standard in industry but unheard of in academic sort of systems we will have all that right yeah and we did and and so we we also it's also very common academic systems to write like the prototype until it can run the benchmark is pretty common yeah you know you stop why would you add functionality beyond the benchmark when all you want is is the paper to prove your idea yeah right but we said no no no no this needs to be able to actually run general purpose eal queries with you know in case i haven't said this yet dub is a sql system so it needs to understand the sql language the one that was on my license plate problem with sql is that it is such a gigantically i mean honestly i was totally blown away but it's because i thought know sequel you know i mean i've seen some sequel how hard could it be tm like famous last words it's like you know thank you jeremy clarkson for this for this for this for this meme of how hard can it be and and it's like i was totally amazed like i only really learned sql when we had to start implementing an engine that interprets sql because people kept throwing queries at us we were like who this is allowed and they were like yeah it's in the standard like oh okay yeah i can believe that it's like you kind of think maybe it's got 20 or 40 key wordss but i bet it's a lot more and a lot more like the recently i recently i learned something about from another person that builds database systems i learned something about that there is something called accept all like in sql we have union and people know that there's a difference between union union all yes you have heard of this difference yeah turns out the same there's also an accept statement in in sql with set semantics but there's also an accept all and i didn't know about the existence of accept all until this other person told me about it but sure enough it's in the standard has been there since you know way back when so we then you know we were like fine we'll have to implement that so we spent a lot of time implementing this we wouldn't have had this ambition of implementing the whole of sql if we had just been out for a paper right yeah yeah yeah it only makes sense because we had been out to actually make a system that people use and if you want to do that well you kind of got to support as there's no shortcuts really like there just just have to get through it but did you okay so you're there in academia wanting to prove something did you start from this is our topic of research let's turn this into a product or did you say this is what's missing in the world so let's research that yeah i think the letter we we we talked to people in you know b data practitioners and we found out that this was absolutely missing in the world and what is this we found that nobody had really considered sort of the ergonom ics of using data management system is always very complicated to set up like it was i don't know if you've ever tried to install postris or something but you know you you find yourself editing like some yeah arcane config files and rebooting services and installing services and creating database files it's it's not a very pleasant process creating users whatever yeah so we so one of the core sort of ideas that led to du was to say look there's like there's a lot sort of stuff that happens that has that actually makes it very hard to use these systems like you have to as i said you have to do all this installation maintenance blah blah nobody has really thought about data management systems with like this user sort of angle like user in this case is like a programmer often or an analyst but we had really thought about them in like a with like what's the maximum simplicity that you can get here perspective also nobody had really thought about data management systems where getting data in and out of them is like a first principle like that has to be fast yeah right yeah i definitely dealt with plenty of systems where you get the database up and running and then the big job is to figure out how to turn it into a schema and some insert statements getting the csv reader to work like i have i've tried the csv readers of i don't know 50 database systems in my life they were all terrible right so so we realize that a lot of the initial sort of interaction people do with data systems is a the setup and be data import and export so let's you know maybe make that pleasant that sounds like a good idea right like this because this is this is like the the the one time you have to make a good impression it's like in these initial steps when the people don't know a lot about your system they've just started they just want to get some they would just want to play around just want to get a job done so we really designed this thing to be useful so that was our academic angle he's like look we need to reimagine these systems for usability and how you mean that's an academic angle because it seems that seems more like a product industrial angle ux yeah this is this is true it can be seen like that but if you think about it we actually wrote a paper about this the the the question is if if you start from like these user interaction principles like what i said needs to be easy needs to be good have good efficiency for import and export you can have an sort of a academic discussion on what does this mean for the construction of the rest of the system right so what are the how does this impact like our canonical textbook architecture and for example there's one gigantic way in which it's impacts the textbook architecture the textbook architecture since like 1984 so is this client server sort of model the two-tier system where you have yeah database client database server funky line between them right that's that's a textbook thing yeah but we realized that in order to to meet these user interaction and data import efficiency goals we needed to actually go to a different model which is the inprocess model which is similar to what sqi does if you if you're aware what sq sqlite works generally yeah it's just database as a library you link it to your process the thing runs in the same process okay so now you're in the same process as your application well that has a bunch of implications in terms of system design again right so for for example you need to somehow cooperate with this application process you can't just assume that this you know this computer is yours like typical data management systems do like you have to s of cooporate you have to be very sort of careful in like you know using resources you have to be careful in you know how you crash traditional database systems handled fatal problems by just exiting right if you're an inprocess system you can't do that because you bring down the host with you so now you have to reimagine sort of fundamental properties of how we do for example transaction control and persistence under the you know the premise that we cannot just exit when we don't like something yeah yeah it's not your environment to play with the environment another super interesting aspect there is like if like databases are used to being cuddled like database systems like oracle is used to be run on very very expensive hardware with like you know some sort of staff of nurses sitting next to it yeah they're called dbas but it's a same thing right and like they have like memory correction in memory they have redundant power supplies they have giant raid arrays to deal with hard diss failing and all that stuff yeah yeah if you make like that's so they can can make certain assumptions about their hardware dctb is made to run like on everyone's computer their phone it's made the simplicity and the way it's designed lens itself to run like everywhere on everyone's laptop stuff like that that's a very different environment like one of my favorite environments is the brazilian windows laptops not nothing against the brazilians but they happen to occupy a warm place and and you know now now you have a laptop that's maybe a bit older that's sitting in like tropical heat and then now you're trying to run like a data like serious sort of data crunching the thing that's going to raise the temperature even further the chances that your ram starts like doing something funky goes up quite a lot right yeah i can believe so so there we had some issues where we're like dude your computer is broken i can see from the bug report that your computer is just plain broken but then of course the next question is again like how okay maybe how can we cleanly deal with this so dctp now has a bunch of sort of self checks that you know for example make sure that the hardware is doing something meaningful that's that's that's that's one so you can have an academic you definitely can have an academic discussion because like these indeed these ux kind of aspects like you said they they are more traditionally more in the product space but if you really think what that means for the rest of like this architecture that is so that is so like well known or so this follows these trotten paths where it's like you open like the thick book about databases i can actually see it from my chair the the thick book and it tells you how to implement the database system like page 743 will tell this is how your transaction manager works book is that what's it called this called database systems i think it's the a molina yeah the author it's a textbook people university students have to read it poor university students yeah yeah okay so usability being both an important and academic concern but let's talk about what duck db does technically and how it does it i want to get down to this yeah okay so we're running sql queries yes if you if you strip away all the user interface stuff you our interface is like our our what we get is a sql query and now we have to come up with something called a query execution plan which is sort of a more sort of mechanistic description of the steps that we should do to compute the result because sql is a declarative language it doesn't actually tell you how to compute result it just just wants you tells you what you want yeah it's the task of the sql engine in our case to be to come up with an execution plan to do that as efficiently as possible there's some aspects to this so the first kind of steps you do is you like you you try to bind the query to the schemas that exist and the types and so then you know your types you know ah these tables actually exist it's a good thing yeah and then you run optimizers like there's some static optimization that always are good ideas that you then run like you run like a symbolic optimizer that like these things usually in are shap shaped like trees like you have operators stacked on operators and they can have two inputs or one input or multiple like a join will have two inputs and aggregation will have one input these kind of things right like these all plucked together to form your query optimizers will operate and then we'll say aha you're joining these two gigantic tables and then you're filtering a lot of the data resing data out again on top here can i maybe move these filters into these join into the inputs of these joints so that i don't have to create this gigantic thing only to throw it away lat yeah rewriting yes these are rewrites that that are like in this case of projection push down is what we call this or selection push down um these are always good ideas so you have static rite rules also have dynamic rewrite rules that depend for example on the cardinality the amount of rows in in tables yeah now you have for example you have a join yeah say we want to do you know join two tables together typical algorithm to to do is do this is a hash join where you build a hash table on one side of the join input and then you probe the other side of the join against that hash table yeah now cardinality of your tables tells you okay which table is the bigger one and it makes sense to build your hash table on the smaller one because the hash table has to sit in memory if you build a hash table on a 10 billion row sort of table and then probe with 10 rows that's very very inefficient the other way around is very efficient so these are dynamic rules that then depend on properties of the actual data maybe they depend also on statistics like dub or many other systems will know for example what the minimum maximum value in every column is these kind of things right right so then you have your optimized plan that's pretty standard the postest does something very similar like i think any database system worth anything has like an optimizer that does these things and then you get into the into the realm of the execution now you have you optimized your your your quer your query plan at this point we tend to call it a logical plan that thing gets transformed into a so-called physical plan which then says okay now instead of having a join i'm going to pick a hash join instead of you know doing a you know an aggregate i will do a hash aggregate these kind of things you pick an implementation you have a physical plan you go to execution okay so that's where it really starts getting like interesting so now i should also say it's very important for example that these executions are parallel right like that we use dub like other database systems also in in in in space of doing data analysis on large amount of row it's it's an analytical system needs to paralyze queries we we can't avoid it like it used to be optional you know like 20 years ago you could get maybe away with the database engine that was single threaded because most computers did have one core one yeah now my stupid macbook has like 10 cores right yeah and you need to use all of them to get in i mean they're amazing cores yes compared to the one one core from years ago faster they've gone they are crazy yeah yeah i'm i'm i'm i'm i was i was blown away when this thing came out doesn't matter we have to paralyze over these 10 cores okay so then part of out of curiosity where do that decision happen is it as you're you finished optimizing and you're building the logical plan yeah no actually induct happens during execution i'll get to that okay so now we need to actually execute this plan in parallel okay interesting question like how do you automatically paralyze it toing complete language is not is not is not obviously trivial right it's okay so these are all things that have to happen and then execution will you know compute things and come with a result now how does this work well the the general general approach is that you first separate your query into so-called pipelines okay so okay so have your query plan it's a tree now you're looking into pipelines what are pipelines pipelines are things that can happen without so-called pipeline breakers what are pipeline breakers those are operators where the entire intermediate result has to be assembled before things can continue for example say i'm aggregating okay i'm doing like a countstar group by x or something like that right in in sql so now this op ator can only start producing output once the entire input has been read right because okay say i pick one group to output first there might be relevant data for that group in the very last input row that i read yeah yeah orting i assume you can't outputting anything until you know which the smallest row absolutely sorting is a great example as well there's a couple of those join join hashtable build the hash table build of the join also one of these things you have to to read so there are called pipeline breakers so every time we encounter pipeline breaker we split the query plan up into these so-call pipelines and pipelines means that's an it's an it's a bunch of operates that can read that can run from a so-called source to a so-called sync right in sort of in a streaming way right so streaming in the heart of the idea there is streaming there has to be because you can't you can't just like okay i okay naively you could say why would you just run one operator at a time you produce the you take the input you produce the output and if the next operator run and so on so forth that's like what pandas does for example if you if you know pandas like this python tool to wrangle data run and that's a terrible idea because the materialization between operators that could actually stream data through is is a terrible idea because it will have to materialize this stuff in memory somewhere which creates memory traffic between your cpu and memory and that will take a lot of time and you might run out of memory and it's generally not pleasant where whereas you could have stayed all in the cpu is that what you're saying well whereas you could have just streamed ch data to multiple operators at the same time and that indeed would be staying in the in the cpu cach much more likely if you look at something like postris they go to the other extreme they do like a row by row thing so postris reads like a row of input applies the next operator applies the next operator appes the next operator and then you know produ output or not sqlite does the same thing and that's also that's really great for systems that deal with small sort of result sets or small input like the transactional systems tend to do like update an order you don't really look at 10 billion rows right yeah but for for analysis this like switching be between operators switching across the types that could potentially exist in the query actually is creates a lot of overhead so therefore we have to find some middle ground so what dctb actually does is implements a so-called vectorized query execution engine so what does this mean we have our pipelines that are streaming be multiple operators in a pipeline like this like a filter like a you know window some window functions could run in a streaming way projections can run in a streaming way these kind of things right yeah and now our we don't do the full materialization which would be like running one operator at the time and writing stuff with memory every single time we don't do the post thing and reading one single row every single time do something in the middle we take in dub's case we take like 248 rows right just as a like experimentally found to be a good idea to have 2048 r right like it used to be24 it eventually as i will explain why this this number exists and and then you basically take like a these vectors we call them vectors like subsets of of these of these intermediate results and then we stream them through the operators and it's important to know that this is a column first representation so we don't have single rows that we shove through the operators we have like little chunks of columns that we shove through the operators that has big advantages because we can then have a columnar sort of processing which allows us to be more efficient in terms of branch prediction which is the cpu basically going trying to figure out where your code is jumping next and it use a statistical model for that and if you if you do the same thing all over again model gets better and so doing so this is actually this is the reason why columnar representations are better in this because the branch predictor will be better at predicting where you're going next as your amount of sort of stupid repetition increases right yeah yeah that makes sense and if you had if you did this rowwise and you had to do something else for every every sort of field because the the fields are different types and different things happens to them then the br proor goes like i don't know you seem to be doing 10 different things at once yeah exactly a lot of things happens here i have no idea where you're going so this is why doing this in a column now way is better because the branch breor will tell you you're going there and you will be right and if it's right it will also do speculative execution so the branch prct will actually say based on my amazing statistics i i'm fairly sure that you're going to go this branch so i'm actually going to already schedule this branch for execution in my cpu and only if i realize that i was wrong in my prediction i will aband all that effort and actually do the other thing right so so that's actually quite a powerful thing so this is why we have vectorized execution we want to be one reason why we have execution we want to be good on this branch prediction but the other thing and that you you mentioned this already is that by controlling the amount of in like the size of the intermediates like the size of whatever intermediate results goes between these different operators that are in a pipeline we can make the query execution stay in the cpu cache for longer and ideally forever and that's how the that's where the 2048 comes from right because the cpu caches in modern cpus are actually quite big like they don't sound big that's like oh it's 32 megabytes but 32 megabytes is quite a lot of storage yeah that used to be a whole computer that used to yes i mean my first computer had eight megabytes of ram i think my first had 512 kilobytes there you go sound like that's and that's cach right m that's like on die cpu cach and you have 32 megabytes of them okay but point being by controlling the size of the intermediate with like this stupid con constant like 2048 we can yeah keep keep this stuff in the cpu cache and why is that important it's because of the memory hierarchy like as the the storage is this has this whole hierarchy we have the cache sitting on top actually on very top it'ss a cpu register that's the fastest thing it's one cycle to access very nice yeah and then you have the caches and then you have main memory and you have storage and you have tape and you have internet or whatever right but tape's still a thing really yeah the cheap cheapest medium to do backups on it's it's really like tape costs nothing right okay it's just tape i haven't seen it in a while but i can believe no i think i think if you you can actually the cloud still has it if you use amazon what's it called glacier yes it will be on tapes oh i didn't know that was tape yeah this why it takes them four hours to get your data back because it's like hang on anyways we want to be up in the storage hoke ideally in a register but we do data processing we can't be in a register but we can be in cach right and by staying in cash we can get like a 100x kind of speed improvement on with compared to if we had to go back to forth to memory all the time it's actually something that was invented here in amsterdam at the cwi institute that i used to work yeah i told you they knew something think about things they invented this whole vectorized quer execution paradigm specifically colleagues of mine peter bonds marinovsky and neil ses and it's very interesting because marchin is now one of the guys that founded snowflake it's like it's like there's some interesting connections in that world and peter peter is a also professor of of data systems here and he's also they yeah anyways there's this it's it's a very small world these data systems and the amsterdam tech hub investing in people yes well investing investing yeah this is it's the government right like it's a government agency that does this but yeah anyways the vectorization that's really a key ingredient and i should notice that note that there's a competing approach oh okay tell me about that it's a competing approach so you can do vectorization which is what i just explained or you can actually do something called jit just in kind compilation so there's also a bunch of sql engines out there that basically ship a gigantic c compiler with them right i'm not kidding they just embed lm done right and then you could also try to convert your sql query to a c program or like assembly program throw it into a compiler and then run the result that's a competing approach then you also don't get a lot of interpretation overhead like you do from like looking at the types and operations every time for every row that you would otherwise get but with the jitting you can also basically fix that by just saying okay we'll we'll just create a binary for this particular query we'll compile a binary for one specific query is that then a favorable approach when you're when you tend to run the same query over and over yeah well yes that definitely helps it does help for for for transactional use cases we have a lot of prepared statements it also helps yeah if you have very very long expression chains things like that the big downside however for jitted engines is that they have to ship the gigantic compiler with them and actually that was one of the things that followed from our first principles that we can't do that because said hey look this s thing needs to be easy to use and easy to install part of that is the binary can't be like two gigabytes but yeah and can't depend on you having a certain tool chain available it can't depend on you having llvm with exactly the right version by the way installed because yeah yeah of us have been there so therefore we couldn't use jitting it that's one of the things that followed from our sort of high level sort of assessment of you know what should this thing do how should behave it's like i can't use jit because it just it's not going to be economical it's not going to be portable it's not you know it's not going to be nice and also we there is a research paper written by the proponents of gting and vectorization that have essentially come together wrote a paper proving that the approaches are essentially equivalent so hey for time presumably i'm sorry allowing for time no no no no the time also equivalent as in like okay equivalent as in like execution performance oh okay yeah now the vector the vectorization has get gets to the same sort of speeds if done properly yeah so that's so that's so we have this vectorized thing which is really cool so the it does make the bit more complicated to write these operators because now instead of like writing like an like a projection where you do an addition of two columns like a plus b you go like if you had like a transac like a simple old school system with like rows it would be pretty obvious how to write that you have like a loop and then it goes like left plus right left plus right left plus right and so on so forth right it's what if if you want to do this in a vectorized way it's like oh now all my sort of math operators actually operate on these arrays of data they're like there has to be like code being generated with templating that expands all these you know things into like loops and there's to you have to do like null handling and you have to do like all these things so you get a bit more complexity in the operators themselves b yeah because you're constantly dealing with a batch of 20 2,000 you're constantly dealing with these batches right also like if you do like an aggregation you're aggregating a thous 248 values at the same time like that sometimes it takes a bit of sort of mental gymnastics to to do this well or like to to wrap your head around how the operator actually has to be implemented in order to be efficient when dealing with a lot of values at the same time or like an index lookup right traditionally you did like an index look up where you have like one value you walk down your b tree there's your thing if you have 2048 search values like like h how do i do traverse now right like it has it has some and there must be i mean you could just do it 2048 times but there must be more efficient ways to walk down a tree there is yes you you could have 248 pointers in the same tree because then you kind of try to reuse the same io but of course also problematic you can't have 248 sending io requests blah blah blah yeah complications but hey people have phgs for a reason right like it's it's we have we have done nothing else in the last 10 years than to think about that so that's that's that's fun but so that's vectorization stuff yeah that it does slightly complicate your operators but from a user's perspective it's great because it yeah stuff stays in cache so it's really really fast you as a user you don't really see that it's a vectorized system anyway you write your sql query and poof outcomes a result right like yeah it's a very sort of detail on how the implementation works but but it is it is it is the thing that dctb does and it's kind of cool cuz i think in dctb like one of the one of the unique things that we combine like this crazy academic sort of knowledge on how data systems should be built all wrapped into a package that's like try to try to be friendly and that's i think something that i don't think has been done before like as in like the the state-of-the-art stuff tends to be like a bit clunky to use and the stuff that's easy to use i don't want to name names but there are some management systems out there that try to be very easy to use but then have wreck serious issues in sort of the internals and how they deal with things like persistance right because they compromise that all of that way yeah so we kind of have both right we have this like crazy core that that you know that is that uses like really the state-ofthe-art in query processing u together with this friendly interface but i want to talk about something else i want to talk about paralyzation okay because that's also really really really interesting so paralyzation many course you yoo yeah we have so there is an interesting paper from a one of the greats of of data management systems research gutz gra i don't know if you have heard of him his name he's he's german like many database people i don't exactly know what it is about tables but somehow somehow it appeals to the germans but no comment ah it's mind yeah yeah it's it's really uncanny how many how many germans are it's like you go to the us you know it's like oh also a german person aha not only of course don't get me wrong it's there's many many many different sort of people working together it's just it's an uncanny amount of germans in there more than there should be okay anyways so g one of the greats he wrote a paper in like the '90s or something like that about it's called volcanoes the volcano query processing or something volcanoes in the title you will find it if you're good for that when i in this paper he also described how to do paralyzation and put and and so that's actually as i said it's it's a diff it's difficult problem you come up with you have your query plan and now you want to run this on many on many cpus what do you do well dr gra he he came up with this a of the exchange operator based parallelism and it at the time it was very clever because it is a par it's a a way of doing parallelism that doesn't impact the other operators implementation at all which is great right if you want to get something done it's great if you don't have to throw your rest of your system away like that's that's it's always good if you want to get something through like here's this cool idea and by the way it works with what we have so what how does this work okay okay y exchange by the way duct tob is not using that it's just what everybody else is using and i'm going to say why it's bad in a second but so exchange exchange exchange based parallelism works like this say we have a filter yeah and we want to paralyze the filter okay yeah so now we insert this special operator under the filter that splits up the incoming data stream into let's say eight data streams yeah okay into eight data streams you can kind of hang on i have too many fingers this is eight yes yeah and now you run this filter on every one of these age data streams in parallel ha yeah and then you set another put another operator on top that collects all the output of those filters back into a single data stream mh and now you have successfully paralyzed this filter okay this sounds like map produce aha yes it does it do i mean map ruce came much later of course so who who got this from whom is is it's pretty clear in this case okay point point being okay this works really great for a filter yes what about an aggregation now how does this work because obviously i can't just use my little distribution operator to have eight input streams run my aggregation on all of these eight and then just glue together the result again right it can ifit some it's a little trick here if it's average that kind of thing yes yeah you can do that but then you have to actually reinterpret the results you have to then remap the groups together after all these operators right and there can be millions so then you've lost then youve spend a lot of time on the remapping so that's really not the point right so what this exchange-based operator parallelism does is it introduces a hash partitioning on the group key in the distribution operator saying aha if i hash the group key in the distribution phase already and i do a radics partitioning based on bits in the hash between all these a data streams that i'm creating in my in my distribution operator then do the aggregation and then i can be sure that i can just glue all these results together again and the result will be semantically correct because you know yeah we can prove that only data that matches like from this hash partition went into this one group by operator and so you're saying if i'm doing select some average of something group by user you're going to hash the user id and chuck same yeah sp up like that exactly okay yeah that makes sense so that's how it works right also how the joins work they will say aha we'll we'll you know we'll split both the build and the probe side of the join using the same hash partitioning and then we can run this in in parallel and then we can do the recombination and everyone's happy well there's some there's one there's there some issues with with this and can you spot any issues already i would have thought like if you're trying to split it up into eight chunks going to dis then you've got a big problem you're basically still you're reading the dis it's going to be hard to parallelize pulling out the different bits of discs and then repartitioning them into different groups right yes but it doesn't have to go to disk this is like a stream thing right like so it as the on the fly it can do this partitioning oh okay so it's okay streaming by it doesn't have yeah yeah it's streaming okay so there's two two main issues with this one is the the fairness of the the fairness of the the partitioning okay so data doesn't give doesn't isn't is nice enough to be sort of uniformly distributed across all dimensions yeah right so what happens now is like what happens if my group key is like highly skewed like i have 10 billion rows with the same group key and then i have 10 billion rows but they all have different group keys yeah yeah plenty of businesses have a few users that problem i like to call it a justin bieber problem like every like everything is you know like if you look at like social networks they're all like following on these like massive nodes that have a lot of connections same problem so if you now do a hash partitioning on those what happens is that one of your cores will be very busy with the heavy hitter because one of them is unlucky to get the the hash partition where the one the heavy hitter sits in yeah all the other ones are sitting yeah yeah yeah right or doing something but but obviously the runtime of the biggest partition will determine the ent end to end runtime of this query yeah like so skew is really terrible here because skew kills the fairness in distribution the other thing that's really terrible is it's not reactive once i start partitioning this into eight partitions i am doomed to keep partitioning this till the query is completed because i cannot just change my mind and do fear partitions because my downam operators don't know about this that this is going on in the first place because yeah i've tried to avoid rewriting them and therefore you know it's i cannot really change my mind usually the exchange based parallelism is baked into the query plan so they actually have an a step that takes the physical plan inserts these operators that split up and recombine split up recombine and yeah and it's it's really it's really problematic for skew and you know just reactivity reasons so is that like if i've got 10 million rows and the first million is skewed quite differently to the last million that but also something like oh say a second query is coming i can't just hog all the cores while i'm processing the first one second one will have no resources to use maybe i actually want to go down with the number of cores that i'm using for the first query because now i want to distribute my resources i see yeah it's actually one of the issues that's really plaguing spark i don't know if you have experience with spark regrettably i have tons of experience with spark apache spark and they they use exchange they just implemented guts's paper right they use exchange parm but as a result spark classes are sometimes used by multiple people but if the if like somebody gets lucky and grabs all the cores for his query or her query the second guy or girl coming with another query has has to wait till the thing's finished because this just the they have baked in this parallelism in the query plan they're waiting for the whole thing to end and it might be that seven of the calls are idle anyway yeah well you don't know right so you could there's some ways of fixing this with like overc committing and blah blah blah but you could also abort the query and redo your partitioning they'll ignore all of this now so this is why a state ofthe art isn't this system anyway instead we use something else we use morsel driven parallelism now how is this work morsel morsel yes so there's another paper at vldb the very large database conference that explains how that works where we basically say hey this idea from guts from the '90s try to avoid redoing the operators for good reasons yeah maybe that we can we can now start re hacking these operators and we can avoid all these problems that come from this operator unaware parallelism scheme like the operator doesn't know about parallelism in the in the in the exchange based system in more d the operators do know so i already told you how the operators become more complicated because they have to be vectorized yeah we're adding a whole level of complexity now to make the paralis aware now which makes it even harder to write these operators okay but for good reasons because of what mor paradism does it says look we will not bake these extra operators into the plan to make parallelism we will actually make the plan itself parallelism aware and this comes back to the the pipelines i mentioned earlier the pipelines are streaming from a source to a sync sources know now can now basically split up their their source into multiple chunks but not with any l of logic they can use whatever partitioning scheme they want they can just say you know what the first 10 million rows go here the second 10 million rows go there it really it's really not doesn't it doesn't doesn't it doesn't matter anymore you just like if you do a table scan from this you will look at your method aha i have 10,000 blocks so i guess the first thousand go to core one the second th blocks go to core two right and now we've lost this partitioning scheme that the previous method relied on right okay so which means that we have some semantics issues in our operators however only blocking oper operators have these semantics issues because they have this whole data set view like a group buy like a sorting and so on yeah so now those are the ones that have to be adapted to basically be able to work with multiple streams of data from multiple pipelines that run in parallel coming into an aggregate for example at the same time and then this operator this aggregate operator this parallelism aware operator needs to know how to reconcile this and in the case of the the hash that you can imagine that you know you're building more hash tables you're trying to figure out which data is in which sort of group you're trying to add a secondary sort of recombine phase like you just said earlier it's all sorts of tricks you can pull it doesn't make the thing more complicated but there's a great upside because there's no longer a fairness issue if you know we can have n threads working on this and they just grabbing subsequent subsequent tasks from these sources and if you want to have you know n one task threads working on this we'll just wait till one of them is done and we'll do have the thread to something else and nothing bad happens like it's no longer this stream that has to do run like to be orchestrated to run in parallel for it to work it's like it's more like a list of tasks that you sort of go through and how many threats go the more threats go through this task the better but it's no longer like no it has to be eight and it have to be running at the same time right similarly is it that the source is can dynamically react as it splitting up that disc it it no it will just generate a bunch of tasks and the worker threats will grab those tasks oh i see okay yeah right secondary we had this fairness issue with with the bieber problem like with the with the high hit high hitter group that's no longer an issue because it means yes one of your threads will be like first of all we don't have these all in the same threat all these groups in the same threat this is a problem of the upstream operator and if one threat takes a little bit longer than another not everybody else is blocked because they can they can go and fetch the next task already like and be like if this guy takes a bit longer whatever right so you can just grab another task while this guy is still doing this thing okay yeah so this is it's it's very elegant it's very elegant to use this morsal based parallelism it yeah it's very yeah it's a very sort of effective way of doing paralis and the result is that drp can basically paralyze arbitrary queries over all the cores that you have you can also tell it to use fewer course it can dynamically react to say oh now we have two queries running we probably shouldn't give all the resources to the first query but maybe we should give some resource to second query as well like all these things become possible and the only cost quote quote is is of course to the poor people that have to implement this so some of some of the people in the company here that on myself i've done this too yeah now have to implement operators that are aware of paralis aware of this moral driven scheme well that's is the whole point of getting a database engine so you can force other people to do the really hard computer science for you right this is indeed indeed it's it's like i mean it's the same with lots of cs right like i mean if you if you look at how how the cpu works or how the operating system works or how your you know your phone works the bas b in your phone works a lot of a lot of really really hard complexity is hidden from people behind like somewhat efficient user interfaces in databases this is the same right like we do all this crazy stuff and these are just two examples there's more crazy stuff happening of course that that so in the end the user just goes goes and types a query and the thing you know we'll we'll process this in in as as fast as as short of time as possible using these crazy tricks to get your cpu like to be you know to be efficient as possible on this it's it's quite interesting like you're you're kind of yeah you're trying to wrap the ugly parts kind of to to hide them from from users but like i mean this these two these two things they the the vectorized scrip processing and the m rep parm they're kind of they're really fundamental let say to the whole thing i would say those are like those characterized like how the system works it's interesting how i mean the strategies involved are familiar even if the domain isn't so the idea of splitting things up into mediumsized batches and stream processing them where possible and the idea of of rather than having a really dynamic scheduler or something you just have chunks of tasks to be dealt with as they come in yeah i mean those are familiar outside of the database world yeah absolutely absolutely there there's a absolutely i think i think one of the complexities in database world compared to say like you know other systems that that support exposing this you mentioned like map produce in the olden days there was there was a doop you know to to everybody's great you know great entertainment it was pioneering let's say that for it i think the difference of data of data man like sql databases is that there is this crazy complex language that people throw at us like this is it's pretty obvious to do this like in a in a in a straight for query like you know select blah from the group by like you can draw this on i can draw this on a whiteboard explain this to students or dctb users once this becomes like you know 15 joints deep various aggregation levels you know window functions subqueries yeah nested subqueries a good one yeah yeah we actually we actually have implemented another paper from group from munich by the way we have we have colleagues from munich also academics that also by the way also work on actual systems the team that is behind hyper and umbra professor nyman and his team they've written a bunch of great papers on on how to solve some of these really ugly issues specifically one around nested subqueries that's that's something that that's something that that one of of my co the co-creator of dub mark asfeld who's sitting next to me here he he at some point disappeared for couple of months with this paper and reappeared reappeared only when subqueries were kind of solved and it's to this day it's like it's still it's still a i still have my high my highest respect for for mark for pulling this one off because like subquery unnesting is one of these things that you know like three people in the world know anything about you know what i mean like there's this guy there's professor norman in munich is mark and maybe one other person that understands this and yeah it is fun right like you're because because usually people don't do this like and po by the way cannot do it they cannot unest subqueries why not because nobody has ever bought at po has ever border to implement professor nman seminal papers unnesting arbitrary subqueries okay right like it's it's there it's just very difficult but once you've done it you have no more like the nested subes actually disappear in your execution which is great like that's that's like a that's like a factor 10 million queries speed up right there because you instead of sort of having to do this dumb re-execution of your nest queries for every row yeah you actually turn it into a join or an aggregate and that's it like i can recommend if somebody if somebody has a lot of time on their hands i can recommend reading this this unnesting paper oh yeah i bet that's i bet that's quite easy in the simple case but getting it to be semantically sound in the complex cases yes and they actually can prove they can do it in all cases and it's exactly what what you want because then you can throw away your other implementation right it's like you want you want to be able to like optimize all of them away yeah so so there's there's a lot of these things there's also there's also like fun stuff around like window functions that like nobody like you have you heard of window function in sql right like this this idea that you can look around a bit or you can also say you know like let's add up the previous two and the following two rows and things like that like normally in sequel that would be like four self or something like that it wouldn't be pretty you can you can take the difference between this one and that one and finally what we all want to do differentiation in the database yes exactly well what people do with this thing is really beyond me i mean i'm i'm we making database systems what people do with it is entirely different we talk to people we want to know we want to know what they do with it because you know we want to see something but like sometimes what ends up on our issue tracker is like what but to come back back to window functions they this is also one of these things right like if you there is maybe i don't know 10 people out there that know how to efficiently implement window functions and because yeah usually your database does it for you but if you're the if you're the person that has to write this thing it's it's not it's not going to be pretty because you're suddenly very very alone and and you're very grateful if somebody and this this on this example also the munic group has written a paper eventually you're very grateful if some crazy academic out there actually wrote down what it takes to do this efficiently like that that's something that we're really also really grateful for standing on the shoulders of giants like people here in amsterdam that we you know learned everything about vectorized quer execution from people from munich that we learned everything about unnesting subqueries from like that's it is a big advantage of being in the research community of just knowing where to look i guess yeah yeah yeah and having a bunch of people that have not only done the work for you but proved it's going to work well haven't they haven't done to work for you okay they they don't publish code that you can like code transferring code between database systems is is really not happening a lot right like nobody nobody has i mean nobody's g to copy paste the optimizer from from postris into my sql or something like that's just not going to work right you have they've done the thinking work for you they have done they have definitely done the work and and they have bothered to write down like i think i think a lot of and it's not going to be a pattern like when oracle does something like that it's it's complicated enough it usually ends up in a pattern and you can't touch it right like it's when somebody writes a research paper it's it's it's very it's it's a very it's it actually helps other people but as i said like building these kind of systems you end up being alone a lot like you like being the first person to come in across this problem ever and or like in recorded history or something like you have no there's nobody else you can ask right it's not like like with it's like with this computer that had a password on it there's nobody you can ask yeah but that's that's the joy of academia right the the worst situation is i can't find a single problem that hasn't already been solved yeah i'm i'm always i'm i have a bit of a rant about that so so especially the greek department like the people with the symbols yeah yeah they are masters at like inventing problems and then solving them and then there's a paper and then like everybody goes like clap clap clap clap clap but in our field like it always bothers me when people kind of design you know into thin air or a design based on sort of imaginary problems because there is so many actual problems in data out there like so many so many people are struggling so hard to to process data to like if you look at if you look at what people are using are they're like people are on pandas you know like running pandas which is this you know i mentioned it earlier is this full materialization query engine from like you know state-ofthe-art of maybe mid 90s or something like that right like it's it's not great but it's the only thing people have and then and then when the academics were actually know something know better how to build these things than decide to throw s symbols around and or decide to design something for imaginary use case then it bothers me because it's like look we have all these people out there on pandas can i just do something for them as actually a again the our touring award winner professor stonebreaker he's at mit he has a coined the term of solving the whales problems the whales problem the whales problems wales being like google and facebook and you know yeah yeah right and and and and i think as an academic it's actually quite criminal to solve the whales to try to solve the whales problems because a the whales don't care they have clever people we worked with some of the people at like facebook google they are clever people yeah you know they don't need us to solve their problems they will ignore solutions that we have come up with anyway yeah and now here we are spending like tax money on solving google's problems like what what is this saying like what is what what is the what is sort of the you know the moral story here i have have another example like when we started duct deb we said okay you know distributed cr processing the topic i got my phd in is not be gonna be something that we're going to do because it's absolutely idiotic to start throwing distributed systems at like 99% of data problems but that's not an acceptable view in academia because they say yeah but google has all these big data sets like how can your solution be relevant and meaningful and sort of valuable if it doesn't solve the 1centers problem but our point was to say look if the 99% are still sort of on pandas clearly you know something need to so we were always saying like no we're not going to touch distributed systems and in my opinion it was the right call because the computers get so fast you know you can easily go terabyte data processing on your macbook these days it's and then in my there's also this interesting effect that i've noticed is that hardware scales big faster than data sets because the humans scale slower than hardware so we we know we have we have like a natural limit on is hitting like keyboards right right and amount of meaningful valuable data we can produce is just limited by the amount of people that we have we have eight billion seems like a lot yes or nine i don't actually don't know the number do you know yeah it's it's 8 to n billion it's not going to be 20 billion next year but the macbook is going to be twice as fast next year yeah yeah so so the so actually many data problems are actually just eaten up by hardware right and and so so spending all a lot in and and you have to you have you have no idea how much sort of brain time and sort of thinking time went into distributed c processing was like aha clearly the data problem is going to be so big that you know we have to have multiple computers at and this was true you know if you're google sure i believe you you have your search index it's gigantic you need this but then you have people that you know are getting paid very well to to deal with this and i don't i don't have to spend my time with this at the same time if i can show that you know any meaningful data set that people see in the world fits on a macbook i want to that's what where i want to be that's just my that's just my ethos as a as a researcher paid with public money you know i mean it's like it's see i i often think like industrial programmers often complain that there's nothing new in programming since the 70s and i think well it's because we're not looking there's that bridge from ind this industry to academia isn't there there are new ideas and we're not implementing them yeah it's true yeah it's it's a fun it's a fun problem but yeah it's one of these one of these things that that i'm i'm sometimes wondering what why funding agencies for example don't mandate something like this like you there are enough problems out there just look at them yeah it's like just look at the world shortage problems in the world no we just picked one and then you know hey it's really annoying to process data if we in that case perhaps i'll feel better about pulling out to my specific use case of duct db and we can talk about quickly to end so i've i started using it i found duck db on the recommendation of a friend of mine because i have a bunch of modestly large json files that i wanted to do queries on and had face that i could use postgress or sql light and it's not that hard to create a schema that matches ad jason and d but then i just chucked it into i didn't even chuck it into duck db i did select star from json file name and it just works yep and that ra firstly that raises applause making it that usable but then i'd like to know what's going on under the hood for that is it actually is it passing the is it creating a schemer is it creating a temporary schema table is it doing optimizer stats on that to make the query what's it doing yes that's is great this works with json files works with csv files it works with pet files it works with a bunch of other stuff works with excel doesn't it can you yeah i think somebody made a plug i don't i don't see everything happens in activ anymore but like so with the json files what happens is okay you say select star from by the way select star is optional you can just say from jason file indb now yeah we have blog posts on friendlier sql where we innovate the language just to you know make it make people happier so so if you say select start from from json file what will happen is that our query analyzer will say okay from file name i would expect a table name there i do not know such a table therefore i'm going to do what we call a fallback scan where we look for other things that could potentially be treated as a table in this case it finds it it will go through various sort of extensions we say does the file name end in par no it ends in json aha let's ask our json reader to look if look if this if it potentially can read this file what does the json reader do yeah it will open the file and see if this file exists if it exists in ad jason file it says i can do this then in the in the binding phase i mentioned this earlier is the phase where we collect the types and the the data column names and the table names and all set for our query we will actually run code that will try to create a temporary schema for your json file and so that will that will there's i think i think by default it will sample your json file and not because it can be gigantic and you don't want to read the whole thing blah blah blah not entirely sure i didn't write the json reader but it will it will generate this temporary schema for you and then basically the names will be from from looking at the file it will know the names it will know the you know the types okay this is an int this is a string this is a time stamp or whatever json has and then so then with that information it can resolve the l of rest of qu so if you say select star from json file where a is 42 it will say aha is a a field in this file yes it is because i just generated a temporary schema the binder will be able to use this in the resolving the rest of the query and then the during query execution actually no dedicated importing step happens what happens that we do a streaming read on the file directly and we treat it like as if it were a table like dctb has this has this notion of treating a lot of things like there were tables like also stemming from by the way this whole integration the same process kind of idea because we realize that once we're in the same process we can treat a lot of things that exist in this process like if there were tables for example if you're in python you can treat like a panda data frame as if it were a table and just query directly okay so in this case json it treats as a table there's code in dctb that essentially can read bunch of bytes from the json file emit the vector chunk you know intermediate thing that the rest of these stack can understand and it all happens sort of dynamically and and that temporary schema is only lives as long as the query lives okay and if it's if it's sampling then you don't need to read the whole file multiple times we can start the work no i think you can make it to read the whole file because sometimes json files are very irregular but very very often they're also very regular right like yeah yeah the sadness about json files sometimes is that they are indeed exported from relational databases only to be read back inter relational databases it's a it can there's only one thing that can go worse there which is like if somebody does this with xml files but that's a hopefully hopefully hopefully you know won't criticize jason too much because it's a miracle we managed to mostly agree on a universal f no no i'm i'm not i'm not against jason i just said like jason is fine i'm happier with jason than with xml and this but this works with with lots of file formats like we can do as i said parket files does it do something similar for the for the optimization phase i mean you've got to gather some do you sample some statistics from the top actually we can't because the statistics we have have to be exact or at least out of bounds so for example if we want to if we can we want to do some static proving that filter can never be true so if you say select star from json file where a is bigger than 42 our optimizer normally would say let me look at the min max statistics if a is never bigger than 42 i can just remove this filter entirely and return empty set and be like done h this is the fastest query ever i can statically prove there cannot be a result based on the available statistics okay we do that in the json case we don't we don't have the statistics the operat the optimizer can work without those statistics and then it will just have yeah it won't it won't be able to do these kind of optimizations if you want these kind of optimizations to be done you have to copy the file into an internal table and then it will have all this information okay but yeah d has really also really great support for nested types like you know repeating fields structured fields these kind of things so these go natively into columns in our background storage they're compressed like it works pretty well there are file formats like park that do have statistics and if they if park files have in the metadata things like minmax statistics and a null count and things like that and we do use those if they are present so in a parket file if you did this thing where a is bigger than 42 and we can based on the metadata derive that there cannot be any value that matches this criteria and we will also just be like done so that's that's that depends on the input format if you can actually write your own plugin i ask yeah so if you have if you have you know if you have a data like format out there that we don't support yet tctb has this concept of plugins or we call them extensions that can provide their own scan functions we call them and so those scan functions can be ob just whatever you want them to be and they have to basically do two things they have to be able to generate this temporary schema based on some input right like the binding what i was said earlier and they have to be able to read this thing and produce our intermediates from this input like our column format or column chunk format with a 2048 okay that is it so i could i could create my own plugin that red apache http log files yes absolutely although our csv reader would probably be okay at reading those we have i think we have the world's most advanced cs actually have a phd in computer science does nothing else than work on our phd on our csv reader again not because we love csvs we actually don't and he doesn't he's quite miserable sometimes but because we realize it's the first thing people do with the data system is they throw a bunch of csv files at it right yeah so so because of that you you want you need this to be good and you need it to be fast i mean you want it to be fast care about this sort of thing where they are yeah yeah world world runs on a bunch of csv files who am i to you know who am i to i can i can i can whine about this on twitter but like who's helped by that i can also be like right you know get to work yeah that's well glad to hear it work for you yeah it really is so thank you very much me through how it works a pleasure chris oh cheers hannis i'll see you again thank you thank you hannis and i'd like to dedicate this whole episode to hannah's dad for selling his son a brick computer and i'm all for teaching your kids to be independent problem solvers and i think we've just seen the fruits of doing that but brick's computer that is a next level move good parenting hannah's dad if you would like some more next level moves and next level ideas of a different kind have a look in the show notes you'll find links to all the papers that hannah mentioned and if you'd like something a bit more accessible to go and play with have a look at duck db at duck d.org they're not sponsoring this it's just very quickly found a useful place in my toolbox so thumbs up from me go kick the tires on it before you go and kick take a moment to click the like rate share buttons i live for the brain food of these podcasts and the interesting conversations i have but i also live by a bit of feedback so if you leave some thank you very much and make sure you're subscribed because we'll be back next week of course with another great mind from the software world i've been your host chris jenkins this has been developer voices with hannis mizen thanks for listening