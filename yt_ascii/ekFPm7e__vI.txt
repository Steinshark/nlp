and we arrived with the next talk admin c plus plus 2022 a little bit of surprise we have four speakers on this talk park here andrew jules and joel farku and we'll be swapping through those slides and it'll be an experiment so stick with stick with us looking forward to the talk let me bring up the title slides for this talk joe you have the word take the take it away sentient thanks for having us at ptc plus 2022 always a pleasure so we will be talking about making a context for standout library among other thing i will be speaking together with a bunch of co-workers and colleagues from all around the place namely paul care which is a lecturer at uws andrew gazilian is former phd student which is now working at amd she's my own phd student right now and myself joel which is a [applause] associate professor at university of paris actually so we will all be talking about doing these concepts for experiment around the standard library so let's let's start by having a bunch of contexts around the work being done there as you may know template meta programming has been a subject for a very long time it's probably one of the older streets in the c plus development box it does being you know a large majority of the time be something that people were looking at as a kind of hack things where a lot of id code needed to be written and since those old times temperature programming evolved quite rapidly first through tools like boost and pierre boost visions that were later upgraded updated replaced by other version of those like boost mp11 boost anna and a lot of others but the main change in how we look at templates with roaming is the fact that the context for keywords and facilities that were introducing c plus 11 and usually upgraded in simpler 17 and 20 completely changes the way we have to think about doing stuff with templates and trying to force our compilers to behave in a some you know an orthodox way the cool thing about using contexter as a main tool for temperature for me is that it turn then place meta programming into actual programming as we will be seeing in a few moments we are now writing codes that may pass as a regular code okay but it's actually doing the meta programming somehow and it push meta programming into what it was from the beginning and even if it was not obvious it's a template data programming and meta programming in general it's a tool to write automated code generation and this automation level is now more obvious to everyone thanks to the simplification brought by constructor yet well if the the benefits from construction are mostly obvious as i say meta programs should be better we have less needs than before of specific libraries to write complex temperature programming and we can write the code in the runtime session debug it and then turn this phone's expert we are still missing a bunch of elements even through a lot of efforts that we'll be discussing we are still lacking a bunch of elements to get to a perfectly massively constructible standard library we have a lot of things that will be discussing that are new and we will be using them but we are still missing things there is a problem of compiled times which is always an issue and we will try to see how we can find well a creative way together around that and we will try to focus on how we can actually turn all of this into a proper continuation process and in the time that we i was now i will let paul speak about this first expert standard library jules will be speaking about compilation for generation trade-off and techniques and andrew we tried to give us an insight on how those compact times can be tackled on holding all what we want to achieve with this talk is try to get a large road spectrum of interesting techniques around const experts and all they can be structured in a meaningful way i will now pass the stage to paul that we'll be speaking about our implementation of a constable standard library components for up to you thanks thank you so yeah i'll i'll give a little introduction to say here before moving on quickly for the moment i'll just say that say is a a non-standard version of the c plus standard library that we have with extended support for compile time evaluation its purpose is to facilitate research into larger compile time program engineering and it should also help to port existing projects which use the c plus standard library to the compile time concept grid domain but before we have a closer look at say and a relevant porting project that we've we've tackled i want to first review some recent const extra features that have made it so that says now possible so i want to talk about first of all dynamic memory allocation which is now since c plus plus 20. accommodated within the constant evaluation domain so we have for example the familiar new and delete operators available to us and accessible within context for programs a context for function needn't run that compile time which is is great it can or or may not depending on its context and so it will be that certain scenarios will compel a contextual function to attempt to run that compile time for example it can be within a static assert it can be initializing a context for a variable declaration or it can be that it's provided the function call return values provided as an argument to an nttp parameter and the way that we're in general making use of this at least in our testing which i think is is nice to look at the general scheme on the right hand side so we've got a constant extra function called all okay we can see the since c plus plus 20 what we can we can do is is called the new operator dynamically allocate space for the integer there initialize it we can then test to see that that happened so we can initialize that boolean there and hopefully that'll end up true but we need to also delete and then we return the value which is hopefully true that came from the constructor function and then we can kind of doubly test it both at runtime and compile time so we call the the runtime assert function on the same function with the same parameters none as we do on the line afterwards here at the bottom on the static assert and you know so if we can compile that the static assert has worked if we can then run it then the runtime assert has worked this dynamic memory allocations should be transient allocations only which can have a little bit of an explanation here essentially all the allocations that we have obtained and during compile time evaluation must be freed before constant evaluation concludes so if you now look at the right hand side again i've made a a change here so i've removed the call to delete but that will now produce a compilation error if we try and compile that and that's due to the static assert here if we remove that we can get rid of the compilation error but the static assert will produce a compilation error because i am not freeing memory that i've allocated some quite reminiscent of a vowel ground memory leak chair and c plus plus 20 we also have vector and string they've appeared fairly recently in the standard libraries and that's that's louis dion's p0980 and p 1004 proposals so now vector and string can be used and there's an example of how it could be used around trivial example below you can see that we can create a vector have some values in it we can create a string and set it to something we can then check that the values are as we expect essentially forming the bill that we looked at earlier on and if that goes according to plan then static assert will be happy with the the true value being returned from that and all is good so that's vector and string i'd like to bring this towards the point of [music] how you should use the vector and string it's not the case that you can just declare a const extra vector or string which is sometimes surprising because when you hear the vector and string are available or with const extra support it might be the first thing you would write is maybe something like maybe the the bottom two lines i guess the point i want to make here is really that this this will produce an error this well actually this code will produce three errors if you try to compile it and the first is is probably the more obvious as to what has gone wrong the the memory is not being used transiently here the memory that has been allocated at compile time for the the double so the new has been called and has returned space for the the double and initialized it accordingly but the free is not called there so that this can't work this variable p would otherwise be available within a function or if there's a global variable it would the user would expect that that would have the value 0.577 etc but that's not going to happen we have to free that memory and it's clearly not happening there so the point is that it's also not happening here with the vector and with the string so this form is not allowed unlike the previous slide where we we see that the constructor and the destructor of vector and string are both called so at the end of the call to string vector okay the destructor feeds that memory and we are allocating memory transiently as we should foreign just for argument's sake we can access a standard concepts per container at runtime when it doesn't allocate so here trivially we've initialized the p pointer const extra to null pointer fine no allocations so too for the vector v here no allocations made string much the same although clank will give an error on that currently and visual studio and gcc are happy enough the code is as below it's just a test out trivially so we can check the pointer is is null pointer the vector's empty and the string does not contain nope yeah when we're working with standard library we have to be working with allocators as well so all containers of the c plus standard library support custom allocator from the user so we have to respect that the containers allocate storage with default construction and by default that will be the std allocator class which can be used and we can show some example code with that to get us up and running so if you've not used allocator before it's going to be a little bit like malloc i'm not sure if that's any more familiar to anyone right enough but it's a little bit like malloc in the sense that it's it's essentially just allocating memory it's not actually constructing or or calling the constructor in any of the the memory that's been allocated and much of std allocator is now marked const extra since c plus plus 20. so it goes along with the new and delete being available we also have allocators and member functions are largely also context pro as well so here you can see simple program making use of the standard allocator as well it allocates 1024 integers and then deallocates them immediately afterwards so that conceptual function is fine and yes void so all good but one thing to notice is that when you're writing to allocated memory of simple types for example of extended this example a little bit here to right to the 42nd element of the the the memory which was allocated and targeted by t and it can be common practice to start writing to that memory given that it's been allocated perhaps also by something like malloc but this actually causes undefined behavior no end object was actually created and its lifetime never started and this was noted as a somewhat long-standing defect in p0593 proposal from smith and vitalena entitled implicit creation of objects for low-level object manipulation and that proposal was subsequently adopted into c plus plus 20. so code stays the same but just to say that that adopted paper then defines implicit lifetime types and these are types where creating an instance of the type runs no code and destroying an instance of the type runs no code and so such types are given some leniency on this observation about the undefined behavior uh and and the alternative usage or the prior usage because undefined behavior is not permitted within or it's not permitted during constant evaluation so there's an issue there and this is has now been identified as undefined behavior but for these implicit lifetime types some some leniency has been provided so such types are are now permitted to be implicitly created in codes such as you see above but not during constant evaluation so section 3.5 of the proposal makes that quite clear and it's also in the c plus plus standard so as well so consequently let's just one more thing to consider when you're doing your dynamic memory allocation at compile time it's not so relevant really for library level development but for legacy codes that might exist and it's being ported this can often be an obstacle so since c plus plus 20 we've got std construct app to provide some appropriate syntax to let us use the memory that has been allocated so here we can see the constructor call has been been provided with the address of the 42nd element of p just before we use it or at least sometime before we use it and so now that code is good and it will it can be part of constant evaluation now we could put something like that around as well which would just ensure that that only ran during constant evaluation and then moving to code that just looks a little bit more like you would find within the standard library we typically would require to construct all array elements after creation and so we need two to as adam mentioned as i mentioned earlier respect custom allocators provided by a user and that can be done using std allocator traits and so the code becomes something like this which is not much different it's just it's not constructor it's construct now coming out as a as a member of the allocator traits specialized for the allocator that you're interested in so that now initializes all of those integers so certainly no problems using the for our second element there's a recent proposal which may end up in c plus plus 23 that's p 2674 and it offers an as implicit lifetime trade which might allow us some more precision here to ensure that it's only the only the types that are implicit lifetime that are may be constructed if you want runtime and compile time code to be exactly the same that can be quite handy i wonder if it would be good to now in in c plus 23 or 26 to remove that restriction on the implicit object creation that that has gone from runtime but still exists during constant evaluation and perhaps more more simply i wonder if construct a and might be a nice addition rather than destroy or which would go along with destroy n which does exist yeah of course we're important programs to concept explore these details will require some care and gcc and visual studio are lenient official c plus plus i should say but clank throws an error which you know i believe is the correct behavior but nevertheless i i also appreciate that well with gcc and visual studio we don't receive an error which can well ultimately it's got to get resolved before your codes can be good so now turning back to c again and how it might be used so the c plus plus standard library as it is at the moment c plus plus 20 standard library it may in time become entirely constant extra that's debatable and but meanwhile the sale library can be used today so we've created this version of the standard library incomplete as it is but which attempts to create a compile time versions of all of the functions entities classes that you know from the standard library it's quite a way from completion to say the least but we have made some progress and we want to let you know how we're getting on for us we're doing things like verifying string based embedded dsls and we're also exploring code generation we have support and complete support for forward list list set map dq q i string stream unique pointer sharepoint or exception and function say is not standalone lib stdc plus plus is required and its code is also used within say as well [music] established const expert entities from lib stdc plus plus are naturally just wrapped within c so now we're kind of at the point where we can wrap vector and string but we've got our own implementation of vector and string in there as well but but the parts which we've we've never implemented ourselves because they've always been available at least when we've turned our attention to them but we've got algorithm numeric the contents of most of the of those headers as i said vector string we've got array inside the say name space we've got optional here and variant see now supports recent versions of gcc and client and the github repository is linked as shown here's a small example code of say it fits on a slide it doesn't do much sensible but it tries to make use of a fair few features that we've got so you can see we we have a string we can initialize it vector has been initialized with one two three dq234 we've got a set with nothing in it at the moment we call set intersection intersects the the dq with the vector so that's going to be the elements it have that are in common which is two and three they're used with the inserter to initialize our two add elements to set then a function is is created an std function style function it's created from a lambda within the lambda we call accumulate we traverse across the set x is then initialized by the value obtained from calling f and so finally we can well note that this line is something that will just execute a runtime as i point out at the top the i o commands we've chosen to make them do nothing when they're executed at compile time so we don't get an error but at runtime they will still work so runtime what you get in the screen there is hello world 5 which is basically two plus three and we can also fit this within a static assert as well so we can check that 5 is equal to x and if it is then if we've got that within our static assert that will run fine so we we looked at a project within but it's included within quite a large amount of work that is comes along with the metamath project so the url for metamath is there it's a good project it has a few things it's a small formal language to express express math theorems and it's accompanied by proofs and tools for the verification of those proofs there's over a dozen proof verifiers listed at the metamath website which you can have a look at but for us we were interested in the main i think it's the only perhaps c plus plus version that is there that's been there for well over 10 years it's written in c plus plus by eric schmidt it's called check mm when you look at it there's a lot of things that are ideal for our endeavors here we'll get 1400 lines of c plus plus and one source file and makes extensive use of the c plus standard library 14 headers at the top there and we've got containers being used queue string set dq vector pair and map so a nice selection including some that are now supported with an std namespace but others that require the say namespace so for us it's a focus on say and we also have in in that project we've got the the c plus standard algorithm library set intersection and find functions being used and we've also got i o operations involving stdco and sierra and other sorted standalone functions so our project just put ct at the start of that ct check mm and the url for that is there the changes we had to make to check mm are listed below at least the highlights certainly we added the constant extra qualifier to all the functions that were involved but the next thing we did for step two is we changed all the global variables to class members of our newly created struct called check mm that we created to get that working for us because we we chose that approach in preference to you know adding a const expert before all of these variables it seemed like a a lighter touch to the the alterations that then free functions which is what they all are in this project at least before we got to it there i've changed to members of that simple struct that i mentioned we had one static function scope variable which was also changed to a class member and handled appropriately compile time file i o is not possible in particular we had to make an alteration to one of the existing functions there read tokens it now accepts a second string parameter which is used if it isn't empty and that allows us the flexibility to maintain the code so that it still runs at runtime and at compile time file includes within the mm database files that will be parsing are not supported when processing a compile time an exception is thrown if this occurs so we don't handle that yet and we don't have that and the projects that we've been looking at and the exception being thrown you know and a a thrown exception is illegal within a const extra program that is very useful because you can have your a message that explains what has gone wrong in that exception and yeah that's that's quite convenient we use say headers so anything that was something like hash include vector is now hash includes say vector.hpp and so on and we also have a script because the input files the the metamar database files that we're using need a little bit of alteration to allow us to hash include them basically so we have a script that places c plus plus 11 style raw string literal delimiters before and after the mm file contents and after that when we're using that file we then access that file's contents using a preprocessor macro mm file path and that's then set to the scripts output so the script input of piano.mm we'd have an output of piano.mm.raw perhaps and then we would just pass that extra component to the command line invocation this is just a glimpse into the mechanism of that this just shows the the main function essentially in one other function to give you an idea if we look at the main function over here you know the parts that are runtime are a little they're probably far closer to what was originally there so what's happening here is it's just check and see if the argument count is as expected it should be that a metamask database file is provided to be checked by this this program and otherwise returns an error then we've got the static assert run time you know that that's there's no cost at runtime it just comes down here at runtime and the app was created because we created this as a as a struct now or a simple class we then call app.run and we give the the parameter provided by the user to check and the code runs at runtime as it did it's the static assert that's been added and so over here you can see a little bit more about the function it's being brought in here ce app run and here we've got a check mm app being created and then we've got a little bit of micro magic going on here so we've got a c string text which has been initialized by hash including the the mm file path and we use x sdr here which you can see defined there and there is required to make the macros happy and now we call app.run here and we're over here we were passing the the file name as the the first argument to run which has got two parameters but one is a default but here we don't we passed an empty string for the first parameter we pass in txt here which is now a c string and we have a little bit of handling of of the fact that this is not a default value coming in to the run member function and all if all goes well then we're going to hopefully end up with having exit success but if the user doesn't provide mm file path we also accommodate that with the l stair and they just the return is then exit success and that means that the static is here is not going to get in the way of anyone who's somehow using this code to verify runtime metamath databases and my last slide is just looking at a little bit of benchmarking i'm just using the time function the unix type function here there you can see the spec of my old machine i'm using that version of gcc in that version of clang that's identified there we have bigger files than these you can see we're working with something called demo zero.mm which you can find on the metamath website and same with piano.mm you can see the size of these then you can see how long it takes to actually run them piano a little bit longer it's obviously a more substantial file compile times so for the small one that's with gcc it's 2.3 with piano x 8.4 with clang on the on the demo amm file it's 2.45 seconds piano dot mmm it's up to 14 seconds which is certainly a bit longer than piano sorry then gcc is taken there's the the invocation commands which which might be of interest as i see i was just putting the time command in front of those two invocations and that is that's that's all from me for the moment so i'm going to hand over to jules now so can everyone see my screen already yep okay perfect so in my part of the presentation we're gonna see how to go from const expert programming so essentially going from say or standard containers at in in contacts per context to actual code generation through templates so to go from dynamically allocated values to code we can well stick to trivially typed results and use dynamically allocated containers for other computations and then just return of course stud ras or purely types etc in the future we might be able to use other techniques like reflection and splicing which are essentially procedural code generation where you can reflect expressions and splice them back into a function or a structure or anything else there is there might be the prop const qualifier which might allow for data transfer from concepts per context to runtime contexts and if you're a bit more adventurous you might also try to use circle which has a lot of meta programming features but we're going to stick to c plus 20 to what's available in c plus 20. so we are going to see how to use non-trivial types such as stud vector or save vector and actually freeze them into a stud eyes there is a technique that allows you to to to get stellarized on the right size or we are going to see also another technique which is which consists in wrapping your results in constex per lambdas which allows you to explore let's say pointer trees and so on so we're also going to see how this technique how these techniques perform and for that we have a case study which is the meta compilation of a very basic structured language which you might already know it's brain fog so the spec of the language is like very simple on the left you have the tokens so there are single characters tokens and on the right you have the c equivalent so we just basically have a chunk of memory a pointer that we can increase or decrease we can increase or decrease the value pointed by the pointer we have very basic ao io sorry and we also have a very basic while loop so this language is string complete so you might be able to do any kind of computation you want with it although you might not want to do it but you you can and i have a meta compiler implementation available on github if you want to have a look at it this is the one i am this is the one i used in the test that that we'll see later and the language is actually parsed from a context per string into an ast with three sorry pointer trees that rely on say unique ptr so we're actually using the kind of data structures that you would like to use for a classical compiler one that's one that's sorry does not run at compile time so the first technique to bring your data over from const expert to nttp is converting them into a vector i mean basically serializing them into into a vector and then converting that vector freezing it into an r so here we have [music] a code that allows us to essentially evaluate a function into an array so we assume that phone returns a stud vector so we can just get the type of the value from it and then we get the size into a a constex per variable so this is doable because well the the return type of fun is not is not sorry it's not trivial but fun dot size is trivial so we can store it in a context per variable so we can then use this context per variable to give a size to the ra template type and all we have to do from there is copy the content of fun into the rn and then we can return it and because this is a trivial trivial title we can then store it in a context per value and use it as a non-type template parameter for code generation so yeah the only convitz is that it requires data serialization and by the way you can take a look at this code in action on this guidebook link so you do need to serialize your data so you must have a a trivial equivalence of your data so all you have to do is to use a stud variants to take care of the polymorphism and replace pointers with indexes within the within the the vector so the trick to convert a vector into an array was shown to me by luke alessandro so thanks a lot to him and there's his github in the in the slide okay so another technique that's way trickier but i would say maybe a bit more powerful or at least it doesn't require you to serialize your data is to wrap your results into into lambdas so here again we get the size into a const expert value and we use it to generate an eat index sequence and from that index sequence we can unroll template parameter sorry we can unroll a bunch of index and just gather all the data into a topo so these techniques allows you to eventually call evalastopol into it recursively to to explore pointer trees 3.03 sorry and eventually you have a tuple tree it allows you to have polymorphic values without stud variant so you might actually unwrap the the types from from the variance if you if you use them and yeah you basically have a an expression template at this point with values stored in the in the tuple so again i gave a link to a code example showing how it works in in compiler explorer so yeah the advantages of these techniques of this technique is that you don't need to serialize your data your values can have different types unlike with uh study ra but study can be can use i can use variant too so that's pretty much a known issue and the huge downside is that this will basically blow up your computer because well the complexity of this task is actually quadratic because you have to call the function every time you want to access to access an element so this is a huge problem and we can see it in performance measurements so here we have the two backends of the brain fog compiler benchmarks so in one case so we have two benchmarks we have a benchmark where we try to compile consecutive loops and we have another one where where we just implicate the loops to make you know to make just a bunch of loops stacked together and then we have the two backends we have the et backend for the president island technique and the flashback end for the serialized ast technique so the two curves that explode the the violet one purple sorry and the green one are the ones that rely on the best buy lamp techniques so you can clearly see that it dramatically increases like actually very fast and the two back and i mean the the two benchmarks that rely on the serialized ast back-end are much more like they grow much much slower and they're actually pretty much linear and this is again something we're gonna see with other benchmarks and by the way if you want to extract data and compile time curves from from benchmarks you can use my city bench tool which is a suite of tools cmake tools and c plus tools to run the compiler a few times gather sample sample data and aggregate them into into this kind of curves okay so this was actually done automatically automatically using a ct bench and we have also other benchmarks that rely that are based on more sophisticated cases so we have a hello world program and a manual broad display program so the hello world program has a 106 tokens so that's about 106 asc notes because well one character in in brent work is one ast node pretty much and when we look at the serialized ast back end the timings scale pretty well so we have 1.6 again for the hello world program if we repeat that so if we just change them together we get 1.9 seconds of conversion but when we look at the lambda wrapping technique it's it becomes pretty pretty much sorry nightmarish as the size of the program increases so at first it's 6.8 seconds which is still okay but when we double the size of the program we affect actively quadruple the compile time so again we're pretty much in we have pretty much quadratic compatimes so we're pretty much stuck with the lambda and wrapping technique and when you look at very large programs such as the mandelbrot's visualization program which is about 11 000 tokens or astinos the serialized ast back-end can just compile it in less than 40 seconds and the lambda wrapping back in simply won't compile on my machine so by the way this is the machine i run these tests on is his laptop it has a 6300u processor and 8 gigs of ram so it's not a big one but still i try to test to compile this overnight and i had a i had a clanger clank timeout and if we double the size of the program in the serialized asc version it's it folds up pretty well and again the lens are wrapping technique just uh just fails at this benchmark so to sum it up for the i would say the the meats of of your program i would simply recommend to just use any kind of data structure you want thanks to the say library and then just serialize it into a vector to freeze it into an stdra and then just use this third array as an nttp for code generation it's actually fairly easy try to stick with a template layer of code as minimalist as minimal as possible as you really don't want to write too much template code and you can test your contacts per functions at runtime is actually very useful this is what i'm doing whenever i can in my in my meta programs and you also want to make sure that your functions still run at compile time so you actually once you have some kind of context per compatible test to make sure that everything's in check okay so that's all for me i think we can switch over to andrew okay so hopefully my slides are viewable so i'm andrew andrew and i have been working on a parallelization compiler for coin stick spur so that you're saying this is the as coin sticks becomes more weighty you use the performance of all these features are going to become a bit more of a concern as compelled time features already sort of raise compel times significantly when they're used as you can see in some of the larger projects where compilation takes a significantly long time when templates and things like that are used quite vastly so the idea is that we'd like to take a look at increasing the performance of these kind of features through parallelization in this case it's through concept expert so it works for runtime then we only can see if it can it can be extended to compare them and work there just as well so client always was created by me it's a common compiler which adds extensions for parallelizing context for for loops essentially uses intrinsics to partition workloads from for loops across multiple cpus and then run them in parallel but this kind of means that the parallelization is explicit rather than implicit it requires users to sort of understand their algorithm and how they want it to be split across the cpus and paralyzed rather than implicit where it would be essentially automatically done by the compiler so that is all about the work got to be required to be done by users and so here is the github link to the cloudhouse compiler so it is accessible usable just now and although the readme is not quite all there yet but as long as you send me an email or so i can always point you in the right direction but the readme should be up there soon enough but as a research compiler so don't be using it for any production uses so essentially let's have a look at the compiler magic behind this parallelization process evanescence in essence it is using intrinsic functions but not actually clang intrinsics they are essentially function wrappers that the client compiler has been taught to recognized at the moment by the future hopefully they'll be converted densics but here is a for each from well it's slightly modified from lib c plus plus so it's a bit neater that's always changed and some intrinsics have been added to this function so it's your standard standard library for each in this case it's just going to iterate over a bunch of elements from a container and provided and the function gets provided a first and a last parameter which is the beginning and end of your container and a function which will be applied onto each element of your container in the next case we have to intrinsics the forces begin and a trader pair which essentially takes two arguments the force and the last saturator and is specifying to the compiler that these are the beginning and alien elements of the loop that you will be paralyzing and this is so the compiler knows how to calculate the partitioning and appropriately separate all the loops across each cpu then you've reduced variable which i have stated sum is the final values of course which isn't necessarily the right way to put another reduce necessarily or possibly the best name for it in essence it's just going to assign or recombine all the partitioned elements so when my or when the compiler is splitting the container of the air across it's going to clone each segment of data that is going to pass across to each cpu or yeah each thread essentially and these then need to be combined back into the original container essentially so this is what that's essentially doing in this case we're just variables basically saying here is the container which is first as a beginner trader motivated by the begin iterator first and here is how we want to then recombine these slash reduce these using partitioned order design which is essentially just going to assign each element back to the container in order that was looked over and then we have pre-ink which is basically saying this was the order that these were looked over essentially and it's the same as the plus plus first of the for loop so it's basically just saying process and order essentially going from left to right when you're assigning everything back and so it's not too complicated and it is somewhat reminiscent i guess of openmp if slightly more complexity added to it due to the reduced variable but the beginning and that repair i think is sort of similar to in concept at least to openmp in the general ideas so how do we sort of hate this basically compiler magic because people don't really or not everybody is going to want to necessarily deal with compelling physics to get their algorithms or the parallelization from their algorithms so it's good for library users to be able to hate this behind something and so they can then give these paralyzed algorithms to users who are less interested in the details they're more interested in actually getting the results from it so we in this case that's possible but in this case we've sort of used executors the executors proposal because we kind of think it standardizes quite well with the idea of parallelization of course expert elements and it sort of also works to hide the serial or to discriminate or to separate serial and concept expert function or paralyze concept for functions from each other is quite nice so you see below in these this code segment there is two standard for eaches and the first is your basic just the serial standard for each and we're going to pretend that these two four issues are inside of a coin sticks per context essentially so they'll be evaluated in coins to expert well just a runtime and so yeah bearing with that and essentially the first stand for each is just your standard for each that's going to actually over every element of array and multiply it by two and then the below one is your coin sticks were paralyzed one which is going to iterate over each element and in parallel and multiply it by two essentially of the whole array in parallel so it's going to split it into let's say four if you get four chords essentially and then multiply them and that's sort of a very simple distinction to separate the serial const expert and the paralyzed concept expert you're essentially just passing an executor argument to the standard for each and we have 30 or so algorithms at the moment incliners and a modified cxx that basically just paralyze and accept these executors that nlum context for parallelization so what kind of results can you expect from this actually it's surprisingly quite reasonable results so it's up to 75 peak efficiency for some of the benchmarks we've tested on most of the benchmarks we've tested on are very hpc specific so they're not very i wouldn't expect these programs to necessarily be used in a production environment for quantity expert necessarily we're still kind of looking for the ideal scenario for it but in this case we used five traditional sort of hpc benchmarks which is basically black shoals manual pro inbody and swaptions and sickle agitation in the case of second large detection the agitation important is perhaps more hpc benchmark it's just obliged detection algorithm but the second component is not necessarily your traditional hpc i suppose so we basically have a not fully complete quantity expert sickle implementation the allows parallelization of or context for parallelization of secular kernels and competitive which is quite nice and we tested that it's performance essentially using soul edge detection and so all the days from a four core i5 7600k so it's a sort of old cpu rather than a neurogen cpu with a ton of performance i know that there is a visuals to find the executions but from the graph you can essentially see the azure performs quite well in most cases with endbody essentially sort of reach new speed up but first and foremost and this is a speed up graph essentially so at the bottom you've got a number of threads and the neglective speed up essentially so for each thread you're essentially one thing or you'd hope the ideal scenario is that you get exactly 100 performance or a one speed up per thread and in this case you can see that it is not exactly the same all the way across so we do not reach the idea of speed up in all cases but i believe in most cases that is very unlikely even at runtime or even on runtime code and in this case as essentially showcasing that the end body is actually very close to the normal speed up and then all the other ones sort of fall a little bit short but they still actually have very significant performance increases across the board and that is it for me so i'm going to pass it over to you so saints andrew so as a quick conclusion we wanted to show that there is a lot of things going on in the context pro world especially around the fact that contextual allocation and the abstraction inside a standard library container style of code like cest is doing shows that it can actually get great results on none to your use cases and the discovery and refinement on all the runtime to comprise some information schemes also help a lot to go through complex piece of code the management of compile time azerbai actually going you know all the way to 11 by paralyzing the compiler which is quite a noble approach to the problem or by trying to reach the gap into a compile time programming tools like city bench does shows that we need more tools to make compile time programming a moment stream a way of dealing with all all the other issue that still are to be solved all in all we've shown that we can manage complexity in context sphere by using those tools and those libraries up to getting a non-trivial workload to be optimized and run it from fight time and the final word there is we want to send crc international collaboration awards that make this work possible and also shootouts to a bunch of cool people like modulebots for its compared extra things that helped us a lot and generator's work is which was able to give us access to a bunch of binary package so we can actually work properly thanks for your attention i hope you enjoyed this talk and see you next time thank you for your talk so let me get this straight so no okay thank you for your talk that was really interesting i think there are a lot of questions which some of them probably have been handed in the chat during this talk if you have further questions there is a table in the lounge to you know handle that and the speakers will be at the table for you to have questions and a debate with