analytics that's something that's been a part of our industry ever since we had databases i mean sql contained a structured storage language but there's a reason we call it the structured query language right questions you can ask of your data are at the center of the story in any data processing system always more questions to ask of the existing data set even though querying has been around forever i think dedicated analytics databases are relatively newer i think they've come to the fore as kind of as an inevitable consequence of having data sets that are bigger than a single machine when that became the norm we had to start thinking about specialized strategies for querying large data sets in aggregate give me a count star of everything right but how's that done it's time to turn our attention this week to the world of olap online analytics processing and for that we're going to take a look at click house which grew out of a need to do kind of google analytics processing google analytics scale but has grown up into a more general high performance analytics database but what does that entail what is an analytics database what does it need to provide and how do you make it provide it fast there's a huge topic and joining me to dive into it is alistair brown who's been in the data managing world since the start of his career i think if you look closely during this podcast you'll see a little hadoop-shaped scar in the back of his soul but these days he works for tiny bird which is a click house as a service company we do talk a little bit about their take on analytics at the end of the podcast but mostly we're talking about what olap database is trying to do and how clickhouse does it specifically so if your queries are too slow stick around for some answers i'm your host chris jenkins this is developer voices and today's voice is aleister brown [music] alistair brown coming to us live from the east of england how you doing hello i'm good thanks good to have you here you are we're going to grill your brain we're going to grill your brains that sounds like a delicious dish we're going to pick your brains and grill you about click house right and more generally about having a dedicated analytics database but i'm gonna i'm gonna start by letting you give me the the lift pitch as we say in england for click house what is it well i don't know how much of a lift picture is but it is an an open source columnar olap database so we can get a little bit into what column now an olap means but the tldr is it's an analytics database so it is purpose built from the ground up for analytics it's not looking at replacing you know postgres for what postgres is good at the transactional style stuff it's purely for analytics because it came out of i mean that's it's root and the name always rings this association with me it's like it came out of basically being like google analytics right a house where you store your clicks absolutely it's kind of a on the nose name really yeah yeah it almost does what it says on the tin right it's so it came out of a competitor of google analytics entirely to power their google analytics product right which is you've got potentially millions of websites they're running a little snippet and every time a visitor lands on the web page it captures some details you know a page view and here and clicking on a button and all that kind of stuff which has some details you know users region where they're from what page they landed on what page they came from all that kind of stuff and that's sending it back from all those websites all at the same time continuously going into a big database and then on the other side of it you've got you know some kind of application where the people who own those sites or maybe the companies and the teams who manage those sites can look at a big application a dashboard and see you know a bar chart that says your top 10 countries or top 10 devices or you know all that kind of stuff that's yeah that's that's what it was built for that's what that's where it came out of and i gather it's generalized from there but just that model implies a bunch of things i think we should go through so the first one is really high throughput ingest of data which isn't going to change yeah absolutely i mean that's like one of the core tenets of it and you know to solve that kind of use case you actually have to right it's not like you've got one stream of data coming from one place that's very predictable it is potentially millions of different you just sort of like iot right of loads of different devices and users all generating potentially at the same time potentially bursty you know you might have a in the middle of the night it might be pretty low and then suddenly in the morning it picks up and explodes or yeah yeah exactly right you know so yeah i mean it it wouldn't work without being able to handle very high volume ingest so how magic give me this give me some technical details it's a serious answer you need a bit of tea for this i can see yeah well it breaks the the the illusion of what we call streaming right so those of us who have worked in streaming kind of know what's behind the curtain you know people think of streaming as like oh well it's you know it's individual data points and just constant one by one by one by one realistically what it actually is is migrate batching right all the time streaming is is really just a a nice marketing name over over micro batching right so instead of you know doing a batch every 15 minutes which would be super slow it brings it down and tries to do it you know four times a second for example right so it's still technically taking a batch of data so he'll grab a piece of data from you know all of these users that come in on that api and all these users come on that api bang it all together into a chunk and then those chunks are run again i think by default it's like four times a second so 250 milliseconds that is configurable and may differ chunks up together and inserts those into very small very small chunks basically okay that takes us to the next and we might we might revisit that and go a bit deeper but that takes us to the next implied stage of a big analytics pipeline which i think is are you going to do a lot of pre-calculation or are you going to try and store it flat and process it which is very hard with potentially billions of records you need to analyze what's what's clickhouse's opinion on pre-processing denormalization yeah so it's optional you obviously can go and do a whole bunch of denormalization if you want to and if you are going truly into massive scale then you probably want to do some right but one of the at least in my opinion anyway of the most like interesting features of click house is it's incremental materialized views right which don't entirely get rid of the need for that but for most people pretty much get rid of the need for it so for people like familiar with materialized views right of generally materials view is you write a select query and you get a result and then it executes the query and it saves the result of the query into a table so that you never have to go and compute it again it's pre-computed and the results are stored in a table that's the traditional traditional materialized view and normally that runs on a schedule right so you say you know every day or something you know go and rerun the materialization query take the result and store it in the table and then you've got that day's result so normally you have like a scheduler and it and it takes it off and it just says go and do it and then it recomputes the whole thing more recently you know certain things have tried to do sort of an attempt at an incremental materialized view where you can say like actually only like here's a date and then you go and recompute the materialization for data beyond that date but it's still doing quite a big query and you have to schedule it and pass it out yeah what what clickhouse does is it has it's always it's always interesting to try and come up with the right the right term for explaining it like an event driven incremental materialization right so it happens upon ingest so in the same way that you would have a materialization query in click class you have your materialization query which is you know doing select and casting column a to a different type and doing some sort of aggregation etc et cetera whatever it is normal sql but instead of going over the entire table and running it for every row every row of row and then re-running it every day what it does is every time a new row comes in that row goes through the materialization query and it computes the new materialization value of the new row and combines it with the previous value of it so it's triggered not by a schedule not externally saying rerun this is triggered every time you do an insert to bring a new row in and it just appends and then merges the the new materialization onto it which is super efficient right you know you don't have to go into these massive re-computations of of the materialization but it also means that a lot of the like denormalization or the like the pre-processing you can actually do as part of that materialization and then so the database itself is doing that sort of denormalization or the transformation and then saving the result and then at query time instead of having to go and do any of that you know and all the complex transformations and joins and aggregations you can just talk to the pre-computed materialization without needing any sort of external tool to be processed right and is this generally user defined or because there's a certain okay so the more general question here is how much is click house still click analytics database and how much is it generalized to this is a general analytics database for any use case i mean it's very generalized right you can still see like hangovers of when it was specifically for doing web analytics right it's of all the databases in existence it's probably got the the most comprehensive set of out-of-the-box functions for you know like breaking down urls and like stripping http path sell and finding query parameters and stuff right you know the amount of time people spend writing like regex patterns to do that in other databases and clickhouse has just got millions of them for anything you might want to do with urls right and all that kind of stuff right so you can really see when you get into its functions okay it was obviously doing a lot of stuff with urls it's got a lot of stuff like uuids and and ulids right which is a slightly more modern uuid and like iep addresses and geolocation and stuff stuff that you would always expect to be in web traffic but outside of those functions everything about it is is pretty general right you know it's a general query engine it's a general column of storage engine it's it's not none of that is really tailored for specifically web analytics okay so you so therefore you must be like having a mechanism to use and define these materialized incremental materialize you yeah yeah so they are they are just sql queries right so you write sql query and you might say uh select to start of day right so if you've got a time stamp but you don't actually want you know you want to aggregate by the day for example you know you don't want it to be just per row so you want to bucket it into windows of of a day it's got a lot of really awesome like date functions which are just amazing time savers so you know super super easy to like transform a day into a bucket of say the start of the day rather than individual timestamps and then do a sum let's say you know just a basic sum and then group it by by that day right and what you end up with is you know one one row that says i am the first of january 2023. and and for some but if you pre if you've got you know a billion rows and you run that over all of that data you get a sum right but then if a new row comes in you obviously need to go and update that sum but you don't want to go and recompute it for the billion rows you've already done you just want to say i've already got this on for that day and now i've got a new row with a value of 20 and now i need to add 20 to the previous value super simple logically right it makes it makes total sense go and find the bucket that relate that this row effects and go and add it to it the way that clickhouse actually does that and this will be an interesting one to try and explain without tying myself into a knot because i'm not looking at a diagram is so it uses incremental bless you incremental states where you know a chunk of rose comes in and it will compute say you know let's say you've got 100 rows that come in and three of them are for today right so that's a chunk where you need to compute the sum and that's today's one and then all of the other rows are various other other buckets so in that intermediate incremental table you've got potentially multiple states for the same day where you know the first of january we saw three rows and the total was ten and then we saw another 10 rows and the value was a hundred and then we saw another you know 100 rows and the value was 50 or something and you end up with four incremental states so rather than you know you had two 200 actual like raw unique rows come in and then your incremental states might be say four rows right which is which is four slightly pre-computed values but then at query time you don't want four rows you want one row right so then you have to consolidate so clickhouse is a background process that takes all of those incremental states and then merges them so like every 10 minutes just on a schedule behind the scenes it's going and compacting all of those incremental states to say go and find all of the incremental states for january the first and compact them into one and then the next time i get an incremental state for january the first i just add a new row right and now i've got two rows which is the combined state of all the previous states plus the new state and again in the background it's going to go and compact those eventually so that you again only have one but at query time so when you know a user comes along and says select what the sum is for that day you don't actually know how many incremental states there might be right because it runs every 10 minutes in the background you don't know if that's run you don't know if there's 10 states or one state so at query time when you use these incremental materialized views you have to go and say do the merge and it goes and looks for any of the parts the incremental states and merges them at query time and goes and does it now you might get lucky and they've already been merged and so it doesn't have to do anything right so there's no overhead at all or there might be five states and has to go and combine them all which is still a lot you know a lot less overhead to combine five states than it is to compute the sum over you know a million a million rows potentially right which is just a little like intricate detail of when you do those materials views right you have to not only think when you're writing the query what do i want to materialize but when you query them you have to think i need to remember that these are incremental states that i need to merge for the query okay so this does actually leak into user space it does yeah so they have these what are called state and merge combinators so instead of just doing a sum when you want to do a materialized view you append the state combinator onto it so you say you know some state whatever the field is that you want to sum and that tells clickhouse behind the scenes that okay i'm doing a stateful sum where i want to sum whatever i get maintain a state some whatever i get next maintain a state and then it goes and does all of that merge process behind the scenes you never have to think about it you have to do anything about it it's all automatic but then at user time you have to use the merge combinator to say you know select some merge the field from the materialized view and then it knows to go and make sure that the final merge has happened at query time okay so that raises sort of side question to what degree is this is the sql interface standard yeah that's always the fun one so i think if you ignore some of that materialized view stuff then most of the sql is pretty standard right if you're using something that is you know if you if you write some sql that's like pure antsy sequel 99 of the time you can copy and paste that and it will work most of what clickhouse adds is stuff on top that extends it beyond that rather than changing what came before right right so this i guess in that case we should step back for a second and because you're implying that you need a certain amount of analytic specific mindset to come to this database so maybe we should talk about why postgres isn't enough or to pick a relational database that does it all yeah i mean let's go with postgres because i mean i'll say postgrespect when i say postgres the same largely applies to my sequel and oracle and yeah whatever i'm sure i'm sure everybody in the postgres world has mixed feelings about their trademark being used broadly but also they are the de facto yeah and by the way i love postgres and the thing that i always try and like make clear is that no analytical database especially clickhouse and all the other ones that compete with clickhouse are coming and trying to like compete with postgres and take away with postgres and say postgres is old and deprecated right they're different tools for different jobs i think certain companies once you get past a certain size the idea that you'll have one database to draw them all is falling away yeah yeah absolutely i mean the thing is like there are at small scale i mean people like analytics works on postgres right people do analytics on postgres you know they do sums they do averages it it works the the challenge becomes when your data grows your amount of users grows that's when it starts to become really challenging to scale now there's a there's a bunch of reasons for that probably and some of them i'm probably not worth going into because i'm probably not the right person to talk about some of them but one in particular right is is the the way the data is stored by them right so when i introduce clickcaps i said it's a columnar database and this is one of the main differences with like an oltp database so a transactional database like postgres and friends where you do an insert and a row comes in and you know that row's got columns abc you take that whole row and you store that whole row as one thing on disk and then the next row comes in and you take that whole row and you store it on disk next to the other row and then every row that comes in just gets appended to the end and you've just got a big you know list of of full full rows so that is very good when you want to say get me row three right i want all of the columns of row three just scan through them and finally the third one where it becomes challenging is when you want to say i don't actually want one row and i don't actually want all of their columns all i want is a sum across all of the rows of column three because what you end up having to do is go right read the entire review one get me column three read the entire river row two get me column three and so on and so on and so on through the whole thing right yeah with the columnistor what you do is every time a row comes in and you've got columns abc you take column a and you stop column a over here on disk and you just take column b and you store column b over here and it might be on a different disk right completely different spindle and take the mc and you stick it somewhere else right and then when the next row comes in you take column a and you put it right next to the value of column made for the previous one so one disk you end up instead instead of being row row row row you end up with column a column a column a column a column a and then somewhere else column b column b column b column b and so when you come along and say give me a sum of column b i never have to go and read the big file that's on disk with all of column a i never have to go and read the big file on disk with all the column c i can just go to the disk that has column b and say just scan through every single value read everything and summit and i never have to do anything else because it's already there it's all together and that is one of the biggest differences right and that is like column now versus row based storage yeah that's a good way of thinking about it it's like how much of a difference does that actually make though because i mean i'm gonna stick my finger in the air and say the average database table is 20 columns wide so i mean in my experience in the world of analytics right so certainly when you're in the transactional world you know you you do end up with tables that do have 20 columns or you know maybe 50 columns is quite exotic in the world of analytics you know you do quite regularly end up with 200 columns or 700 columns or a thousand columns that's a lot of columns and if you have to scan all of those for every row that's a lot of overhead and generally with analytics the other thing is you know your having generally a lot more data like a lot more rows you know in a transactional database you might not need to keep 10 years worth of data or you're doing a lot of upsets and deletes right you might delete old data you might go and when somebody it changes something changes in order you just go and update the previous row so it doesn't actually have another rotor scan it's just the previous row was updated the analytical databases you're not really doing updates and deletes most of the time what you're doing is you're just constantly appending so a change comes you append it and it's another row and then you append it and you append it and then it in your logic you know if you only want the latest one you say well you know get me the latest one and ignore the older ones or you're actually interested in being able to analyze how many changes there were and what happened between changes so you want the lock but all of that means that you end up with you know lots of rows you know your your your transactional database might have a row per user and you've got a million users but your analytical database if you've got a million users and you might have you know 20 000 rows per per user right because it's a log of everything that they do and what you know what changed and all that so you can end up with you know easily going into petabytes of analytics data and billions and billions of rows that you're trying to go through and so it's a compounding effect right if even if you did only have 20 columns but you've got 30 times the amount of data it's a compounding effect if you have to go and scan all that data but then you end up with actually a lot of analytical systems are much wider because i mean it kind of sometimes it goes back to the denormalization stuff right of quite often instead of like analytical about uses don't don't normally do the like referential integrity and like primary keys and foreign keys of transactional databases to do that you just want one row that's got the entire picture in the whole thing right which makes it significantly easier to do your analytics and you know the the you you might go going back to the the pre-processing question you might do that before it reaches the analytical database right you might pre-process it and then store it or you might do that denormalization inside the but the analytical database okay so does that mean that generally you'll be using something like click house in concept with other databases yeah it's part of a balanced breakfast yeah yeah like unless you are doing something hyper specific where you are literally only doing analytics then you are going to have more than one database you know if you're building any kind of front-end ambulance or like uber right you're doing doing ubereats or something yeah you know ubereats they they use analytical databases right but they also use transactional databases so when you log in and you go and get you'll you know give your username and password and you get your profile information it's got your email and your your name and your phone number and all that transactional database right you wouldn't want to use anything else for it but when you want to go and look at how what is the expected delivery time of all of those restaurants because you're looking at how long did it take for all of the other users who ordered and then got their food delivered how what was the average delivery time for each of those so that you can display to the user this restaurant is taking about 30 minutes to get food to you and that helps users reason about what restaurants they want to go to yeah doing that at the scale of uber of ubereats where you've got millions of users loads of data loads of restaurants that you need to go and crunch and yeah not only have you got a million users but you've got potentially tens of thousands to hundreds of thousands maybe even millions of users at the same time right it's not just like you've you've got millions of users but it's only like 10 doing a query at a time you've potentially got you know 100 000 users clicking what restaurants are open right now and they all expect to get a response and because it's an app and we're human and our attention spans are are tiny yeah you're getting worse you know you click restaurants and how long would you sit there and wait for that page to load to tell you what restaurants are available and and how long they were taking to deliver food right you're not going to sit there click it and it goes okay come back in five minutes and we'll and we'll tell you you'd be like all right i'm gonna uninstall this app and go and get a different one right people expect it to you click the button and at most like two seconds later the list populates and you get all of it so it's super quick with yeah very very high concurrency so are you actually using it directly to feed like is it user facing yeah i mean will when a user clicks on ubereats will they be running a query on click house or will they be reading a cache that click house is feeding i don't know specifically for ubereats okay yeah but generally but generally with them being the example is you you run it directly over the analytical database right okay so the the idea is that the data needs to be pretty fresh because people aren't that interested if data is like an hour roll then it's not really relevant anymore so yeah the idea is that it comes into the database live it actually runs the query computes the query obviously there's interactions of data being cached on ssds and then going into like os page cache and and then you can get results out of there but it's not like your pre-computing result and then putting it in redis and then actually you're just asking redisk and i have the pre-computed result that was actually computed 10 minutes ago generally so as a programmer writing sorry as a programmer writing a web server i'm expecting to write queries against say postgres and against click house to get the whole user experience i want yeah exactly depending on what it is you're trying to do you know you're trying to get profile information you're right at the postgres you're trying to get analytics you go to clickhouse or whatever other flavor of analytics database this is making me think of a very specific architecture cqrs of course because this are we are we skirting around the term cqrs here maybe in which you have one place where you stalk them up command store data but then most of the time you're reading from an analytics database from a from a pre-computed view database i mean i i try not to think too much about those kind of patterns to be honest you know okay i look i look at the the the actually like what is people trying trying to do and and just going from there because i didn't know i mean the like lots of patterns have come up and lots of stacks and and you know they end up being kind of inflexible people just go i i feel like this is the pattern that i am supposed to go with and it doesn't always work 100 for every say yeah i don't know i'm not a big pattern a big pattern first okay in that case let me put it this way do what real world common recipes common combinations do you see well i mean you know kind of what we've spoken about already is is a pretty common pattern right of having a transactional database and an analytical database and it kind of depends on the like the size and the maturity of the organization and it's almost always a journey right it's very rarely that you go right straight out of the gate it makes total sense to go and get a super scalable postgres flavor you know go and get cockroachdb right which came out of like a google research project and now it's like you can federate it to millions and millions of servers and we need to think about that kind of crazy scale and then we need to go and get a crazy scalable analytics database to go and do this kind of stuff and then we probably want like a data warehouse on the end of it to go and power all our reporting and stuff which by the way is probably something worth us talking about as well bringing data warehouses into the mix what end up ends up happening is people pick what they're familiar with right that they can build with and get something out the door which quite often is postcase right because who hasn't used postgres or mysql or wherever it is yeah and they build with that until until it you know stops kind of working what they're trying to do so you know if you've got very few users not many users going at the same time and not much data then doing a sum over you know a million rows once every 30 seconds or something in postgres is fine you know like why not it's then once you start scaling right you start to go oh it will actually maybe this is this is not scaling and now it's affecting the user experience because more users are hitting the app the queries are taking longer and crews are getting users are getting a bit frustrated that this is being quite slow yeah and then you start moving too we used to run that took one second now takes 10 seconds even with caching that kind of thing yeah particularly what happens i mean the interesting thing with caching is that you inevitably people do it with postgres and then they think hey the way i'm going to speed up postgres is i'm going to put readers in front of it right and yeah that is great and it works but what you end up with is you solve the last mile problem right which is the latency of the query so the user says you know hits the button and they expect a response and readers is is amazing right you know the tiny word we use readers right it's great that and it solves that latency right so the user gets the response in 30 milliseconds or whatever it is what it doesn't solve is the first mile problem right which is the freshness which becomes quite challenging if you're just relying on a caching layer because you know yeah the user can access the data really quickly but the data is quite out of date based on whatever your cash policy is and then you have something better hey how do i how do i evict out of my cache and how do i repopulate my cash and it gets quite complex to do that stuff as well so you know quite often that is the first port of call and people go hey i'll just stick readers on it i'll cache it and that works for a little bit and then users start saying hey my experience is degraded again because all the data is out of date and then they start going into like analytical databases and hey how can i actually get a system that can handle these big aggregations over big amounts of data with lots of users at the same time and you might end up with with click house i mean there's plenty in the space right there's there's a lot of analytical databases coming up in this space clickhouse is is we haven't really i mean we haven't spoken about this yet but like one of the nice things about clickhouse is that it fits quite well with users who are quite familiar with traditional databases like postgres that's just like so postgres is it's really easy to come along and download one binary and run it on your machine right and you've got a postgres and you can start developing it you can do it locally you can stick it on an ec2 you can go and find all of the serverless stuff for it but it's super easy to get started because you get one binary throw it up and hey you've got a postgres right yeah a lot of analytical databases like i i came up in the world of hadoop right and if anyone's ever worked with the world of hadoop what you will know is that single binaries do not exist and what you end up with is going and downloading 40 different binaries and then having to try and work out right so if i want to run this one this tool then i need to go and have this tool because that's its metadata coordinator but then even this tool itself not only does it have an external dependency it's got six different like services within the one component where it's got like the master and it's got the workers and it's got the coordinators within it they become super like complex topologies that are a nightmare to manage and a lot of analytical databases came out of that era so things like apache druid which like so i used to work at cloudera and that was a project that we worked with at the time it's a pretty complex topology of it right it's got lots of different roles you end up having to deploy you know one type of node and one type of node and one type of node makes it very difficult and that prevents problems in production as well as like developing locally right yeah yeah so it's just like a super complex model to go and say hey i'm just a developer and i want to run run one on machine to run one on my machine to run some code against to go and develop some stuff whereas clickhouse it does have literally a single binary where you can go and download it and run it on your machine locally nothing to configure nothing to maintain it's just like go and you get a click house and you can start developing against it which like it doesn't sound that impressive of people coming along from like postgres getting well duh like why wouldn't you but it's actually pretty pretty uncommon in the world of analytical databases to get one that that can actually do that which is why it's like become super popular interestingly in like embedded applications you know a lot of places have started to actually embed the clickhouse binary as a like a temporal database you know just spin up a super quick temporal in-memory database load a file and do some analytics output it and then kill it and it goes away and just use it as you need it super super similar like duck db right has come along and done this done a very similar thing so you quite often see them get compared of like click house local and updb of just being like and in memory very quick data like temporal database oh curious define temporal database for me quickly so basically just one that you can bring up do a little something and then get rid of it right it is ephemeral it is it doesn't it's not long-lived it doesn't stick around it doesn't sit on a server and it's always there coming around for queries it's one where you have an application and it says oh i need a database quickly spins it up does whatever it needs to do and then spins it down and it doesn't exist and doesn't take up any resources anymore that raises quick questions about startup time and is there a memory only flag sometimes i mean are very quick i don't know exactly what they are you know most of my interaction these days with clickhouse rate is street tiny bit which sorry i i'm not working with clickhouse local that often to know what it's when it's startup times are but they are like pretty much like unnoticeable it's not like you start it up and then you have to wait for things to get in sync and start for like 20 seconds it's you know half a second a second at most i can i can verify that i ran it before we started recording it was like okay this is far my first experience was very good then i realized they didn't have four billion rows to hand to do anything tasty with it yeah um in memory only mode for this kind of use case that's a good question actually i don't i don't know if the clickhouse local one is running by default in memory i mean like i would hmm yeah i'm not sure i would assume so okay we'll save that one for later research i would have to google it to be to be perfectly honest so this is a closed book test this podcast so going back to that whole pipeline of things you must need for an analytics database does it ship with like something user interface beyond sql is there a analytics gui yeah so click clickhouse itself is is like any other you know like postgres right it's it's a database server you know it counts as a dbms it's a database management system you drop the r because it's not relational but it is a a database management system so it is just a headless database that comes with nothing but obviously there's now a pretty rich ecosystem around clickhouse in you know there are connectors for pretty much every like bi tool you can think of you know tableaus and power bis and superset and all that kind of stuff that you would want to connect to there's loads of vendors out there you know obviously tiny bit is one but there's there's loads out there who have got their own styles of guis whether that's managing clusters through the gui or it's actually like an interactive you know way of building queries and building applications on top of it or doing visualizations but it doesn't come the open source project itself doesn't come out the box with with like a web gui okay do you have a particular recommendation or do you want to stay out of that well i am particularly biased working for for a vendor that sells clickhouse but no i don't have a particular rate i don't have a particular recommendation they they are all they all have their strengths and weaknesses like i'm a big fan of a pet like if you're doing bi which like we can get onto this a bit later right but the the click apps is very versatile right so people who are using it for like data warehousing and bi but people are also using it to do front-end applications and different vendors and different tools are appropriate for different things right so you know you wouldn't go and use like a bi tool like tableau or apache superset to go and build your front end for ubereats and at the same time different vendors have put different stakes on what side of the equation they want to work at so tiny bird has focused entirely on we think clickhouse is great for building applications right and we want clickhouse to be the back end the people build user-facing applications that do have very high concurrency and all that kind of stuff others are taking it off we want this to be a faster snowflake right so instead of using snowflake you come and you just load it in in click out and then you stick apache superset on it and you go into your bi and you do your reporting in your your ad hoc analytics it's it's pretty good at both but people who have you know optimized for one one particular thing which maybe gets on to like we were talking about like what is a typical typical stack look like that i mentioned like data warehouses and how do they come into it yeah yeah because like if you've if you've been working with databases and transactional databases and you but you haven't come across you know application analytical databases for applications like lookout you may well have already come across analytical databases but for warehousing like bigquery and snowflake and redshift and all that kind of stuff or hadoop back in the day yeah which you might think well what's like what's the difference why wouldn't i just go and do that because they generally are also olap and they are also columnar but then they vary very differently so that's like they tend to have the much more complex distributed architecture of you know you've got pieces of you've got your storage over here and generally it's like cloud storage up in s3 or gcs you know it's a blob storage in cloud and then somewhere else you've got your compute and then whenever your compute actually wants to run a query it has to go over to blob storage and you've got the latency of going over the network and getting files from blob storage reading in the file and then going through the file which adds a lot of latency but generally warehouses focus on like arbitrary complexity so this is kind of like a like i've sung the praises of clickhouse it's probably quite good to talk about his limitation as well right okay so the warehouses are super good if you are trying to do insane levels of complexity over insane amounts of data right so if you've got 40 petabytes of data you've got you know tens of hundreds of billions of rows and some crazy analyst comes along and they're trying to do you know the select a hundred rows with 20 aggregations around some of those columns and they are doing 70 joins right you know they're going out to so many different tales and doing these crazy joins warehouses are brilliant at that right because they will just go okay but they might say okay see you next week because because what what they're what they're very good at is just going okay i will take whatever you ever you send at me i will do it right i will find a way to do it but i will get you the response sometime i don't know how long because what they will try and do is is mpp right massively parallel processing of chunking queries up into very small fragments that will fit in the resources they have and they will always get you a response but it could be potentially slow so yeah you know if you've got limited resources but you're trying to do this insane massive query it won't go sorry i don't have the resources to do that what it will try and do is say okay i will do you know a very small fragment of it i'll take 10 000 rows and i'll compute the result to 10 000 rows and then i'll store that and then i'll go and compute the next 10 000 rows and i saw that and then i'll compute the next 10 000 rows and store that and just keep doing that over and over and over and over again until it's got the whole thing and then it will take all of those intermediate ones and do the same thing okay right i've got 10 000 intermediate states go and take 10 000 of those and merge those go and take the next ones and merge those i'll just keep doing it in these stages of breaking it up getting a result breaking up getting a result which means it's very very good at having a like any complexity of query one of the small amount of resources but it could take however long yeah and it will handle failure so if any of those fragments of the queries fail because the network went down or because when the servers failed it'll go okay i will wait for that to come back up and i'll retry it and i'll get you the result and it will delay me but you'll get a result which is exactly what you want if you're doing a massive report over billions you know tens of billions of rows and petabytes of data that takes a weekend to compute and you hit the button on friday and then you come back on monday and you want a pdf in your email inbox with the report right you don't want to come back and over the weekend it failed and it said sorry i ran out of memory halfway through right it's not useful so yeah that is like what warehouses are optimized for whereas click house one of its things that it's not so optimized for is is that kind of like arbitrary complexity and just throw anything at me and i will make it happen it will hit into boundaries of that's too many joints that's too much of a complex query i don't have enough memory on one server to handle that query so i'll fail and it's like a slightly different priority right of like interactivity so like you saw this originally come up in the hadoop world right so you had apache hive come up which was like the snowflake of of yesteryear which was the throw anything at it and it'll go away and take a week but it'll get you your answer and then you had apache impala come up which was like no you don't want to do a report you actually want somebody sat at a terminal running a query and they get a response immediately and the point is that it biases towards interactivity of i want a result very quickly of and if if it fails fine but i want the result so it will fail and just tell you it failed and then the user can go and retry it it won't sit there for an hour trying to recover and say oh no i'm going to retry that fragment or retry that fragment it will just fail and say no i fail which you might think why would you ever want that but right it's it's depending on on entirely what you're trying to do and what you want at the time whatever or do you want a slow result but it always works i sometimes think the the fast result model really shines where you don't know yet what question you actually want to ask so you're asking a lot of experimental questions wanting a quick response you can say oh no i didn't mean that i meant something slightly different yeah yeah yeah i mean it also it it works in like even if you do know the query up front it works very well in like user facing user-facing stuff right where you might think actually i don't really want things to fail right because potentially the the failure is a bad user experience but you can build your application logic to say we'll go and run the query and just tell me if you fail and i will work out do i want to retry it and take a little bit extra time or do i just want to tell the user hey it failed you need to go and do this because i can i can't recover from it so it just gives you the choice of how do i want to handle that user experience okay so one thing the whole data warehouse raises is and joins is a key word here what if i want to bring in analytics from two different transactional systems how am i going to do that with click house two or more yeah so i'm assuming what you mean is i've got two existing transactional databases how am i going to bring the data into clickhouse and do some analytics yeah how do i if i'm if department a is using postgres and department b is using oracle and i'm trying to service reports for management who care about both departments yes so this is one of the interesting i say interesting it's one of the incredibly frustrating and tedious topics of analytical database which is how do you integrate with other systems that are sources of data because it's very easy when the source of data is an api and it's it's new data that's coming in you can just say insert that data it's very easy when it's a kafka topic and you can just say well whatever's on the kafka topic just take that in and put it in a table when you've got and what happens in a lot of big big businesses now is you've actually got databases that have been federated all over the organization and everybody's gone oh well you know we hired somebody who likes and we we've got a team who likes postgres and we use microsoft sql which has kind of you know it's it's always been a challenge of how do i integrate all of these things and everybody's ended up writing loads of glue code of you know i'm going to write a little custom bash script that's maybe executed on a schedule and it just goes and takes beta out there and it pushes over there or maybe i use a you know an open source tool to go and do it like apache nifi or like divisium or you're going to do cdc with kafka and kafka connect which makes it like ridiculously easy or the explosion of like etl tools that are out there in the market at the moment i mean the the data ecosystem has got a bit crazy with etl tools and there's like a million different options of crazy etl tools you can pick but it's always been a challenge of like how do you do it because there's a there's like so many different ways to do it you know you can go and do it in a in a batch way you know with like a batch etl tool that executes every hour and all it does is it goes to the source database postgres and it says select star on where the time is greater than an hour ago when i last ran right and then it just takes all of that and then just does a big into clickhouse and just goes does it obviously the the caveat with that is the date is an hour rolled so you know it's freshness or you can go and get a cdc you know change data capture using something like dubesium and say actually go and tail the bin log and every time something a change happens in postgres take that change event and fire it into click house side side note on cdc for analytical databases is it's challenging in and of itself because at the start i said you don't really do upsets like updates and deletes in in analytical databases which means cdc becomes quite challenging because if a row is deleted or a row is changed most of the time i mean some analytical bases don't even support at all updates and deletes you just cannot run like there's no there's no command for it right yeah so how do you some map that mental model into your new world yeah so that becomes super challenging and what you end up doing right is you just start appending all of the changes and you just say like get whatever the original rare is you append and then whatever the change was you append the whole row again and then you append the whole row again and if it was just changes then you can just filter and say if i've got 10 rows that were all the same row going to give me the latest one because that's got the latest change and stuff like that and if you know if you've got deletes you can go and handle it in different ways because you can say well don't select rows that have got like a flag column that says i was deleted and all that kind of stuff but anyway side note on on cdc but what you hit what you end up doing is you do have quite complex patterns often right of integrating all of these disparate systems and getting a little bit out of just a blind but like clickhouse suffers from this right and it was one of the reasons why a year ago now when i was looking at moving on from cloudera and i was looking for what's the next you know i like data i want to work in data what is the next company that i join is i have fought with this my whole career of how do i go and integrate all of these sources and manage all of this glue code and all that kind of stuff and i was really interested when i came across tiny bird of they came up that they had an opinion about it and they took a slightly different approach and i liked that approach that's not to say it is the absolute perfect correct approach for everyone some people like the more control of going and doing it themselves and like natively writing their own integrations and all that stuff and that that works for me i really liked that tiny bird said we're going to try and handle all of that for you as a feature and just be like we can connect to snowflake and sync that data for you and it's like two clips and that was their approach to to take to that which fits in some cases right so a bit like kafka's connect ecosystem yeah yeah pretty much you know yeah like in the early days of cafe i didn't have any of the kafka connect and you kind of had to do it do it yourself and then people said hey that was a bit of a pain so let's do kafka connect and we could solve a lot of that pain for you and we kind of did the same thing and thought hey maybe we could be more than just a database and do some of this value-add like make it super easy to do certain things that everybody's doing for you okay i do want to get into that but i'm going to push you a little more on like the next level because that's you've just told me how i connect say postgres from department a into click house or oracle from department b and click house but then how do i merge those two data sets together to do analytics queries so there's a couple of different techniques to do that i mean the the generic way is effectively just you could either part of your process you know we talk about pre-processing before part of your pre-processing process could be to normalize data so if it's like you know department a's got customer dating department b's got customer data but they're in different schemas but it's largely the same data you might have your process like normalize the schema and then just insert them into the same table and have it that way or you might just take the sort of elt approach which is just take the raw data dump it into the database and then sort it out with a query which may be as simple as you know doing like a create table from select right and and creating a table that is the result of a select which the select is doing the transformation so it's saying you know take all of the data from a transform these fields to look like this and then you need it you let union it by selecting all of the data from the other one and transforming to this then sticking it in a table now the all of the like different analytical databases might have different like nice techniques that would make that slightly easier in clickhouse you've got the materialized views right which make it kind of nice because you can say you can write that normalization union query that is selecting from you know table department a table and department b table and then you could set up a real-time cdc stream from each of those and then as new rows come in it's not running on a schedule it's not doing it batch it's every time a new row comes in it's it's doing it in real time and always getting the result putting it all into one big table and then you can run your analytics over the combined table over it the the other interesting thing that i find i mean maybe it's not the most in retrospect maybe it's not the most useful for this but clickhouse is a very a very it has the concept of table engines right of so not every table has to be exactly the same and actually you can configure exactly how a table works under the hood by using a different table engine so it's got a whole bunch of different ones of like merge trees where it goes and like it can look for ids and then it can automatically if i you know if i see the same id i can go and find the latest one of that id and get rid of the old ones and automatically de-duplicate stuff it's got like replicating tables where you can say actually this table does not just belong in one place every time something comes into this table i want you to automatically replicate this table over multiple servers so it's got like these these table engines that you can you can configure on a per table basis to behave slightly differently depending on what the use case is but one of them is the null engine and the null engine is basically yeah which it's basically like on on linux cutting out like devnal basically it's it's just like a useful it sounds weird right yeah but what you can do is that it's like an ephemeral pipe so that you can pipe the raw data into a null table and then have a materialization query at the end reading out of the null table and materializing the result but you never actually store the incoming raw data that landed in the null table so with the previous one if you have a standard table you're actually taking up storage right because you're writing you're basically duplicating all of department a's data onto a clickhouse table and duplicating all of department b's data in into a clickhouse table but for the pure reason of transforming it and then storing it again so you end it with three copies that are just slightly different the null engine would mean that you could just take it all out of a and don't store the raw data immediately transform it and then just store the transformed one and then you're only storing the actual end result and never the intermediate you know raw okay result that you don't care about which is quite an interesting one of just like a a way that you could optimize optimize that which again i it may well be specific to click outside i have no idea if the like other analytical databases have that null engine or a concept like that i can think of plenty of systems that have that kind of transformer notion but that's another way to model it so yeah so we for time i want to push on to i do want to talk about tiny birds approach with and you've mentioned it a little bit with click chaos one of the things that caught my eye about the way tiny bird position themselves is kind of from the ap making api building easy that doesn't seem to naturally quite fit with the idea of an analytics database to take you through that thinking so the yeah it's an interesting one so i mean it if you if you consider the what i said before of there are kind of different fl like directions that people have taken clickhouse in so some have taken clickhouse down hey we're a faster snowflake and you go and do bi we've taken the approach of hey we think actually clickhouse is brilliant for user-facing stuff and you know we're not that interested in do you want to stick tableau on it and have people drag and drop like charts on it in ad hoc build queries we think it's better predefine your queries and it goes to an application and your users hit those queries and get and get results and generally the way that you end up integrating that style of thing like applications always talk to rest apis right if you build a an android app or you know or a web app whatever it is generally speaking it's reaching out to http api and saying hello i i'm hitting the get restaurants api can i have all the restaurants please yeah and what you end up doing and people who have used you know postgres to do the transactional side of things will have written an inordinate amount of apis and you know will have worked with all of the different orm libraries under the sun and all of the different api frameworks under the sun and you always end up writing the same thing right of of writing your api and all of your get methods and then sanitizing user input and then translating that to a model that then goes and runs a query on the database and blah blah blah and then once you spend all that time writing that layer you then have to go and you know host it somewhere deploy it and secure it and scale it and all that stuff we basically just said well maybe instead of just being a database we could also do that api bit right so because we are putting the stake in the ground to say we're interested in applications and we want to help you build applications and almost all those applications end up building our apis with with rms to go and do that will just save you the time and we'll do that bit as well so write a piece of sql do all of your analytics yeah it's it's click house it's a database it's it's everything you would expect from click house plus hit a button turn that query into an api and then you get the result from an arrest api and you never have to go and write the api yourself so it was really just a you know that's what we thought would make this more useful because i mean that's like the thing with databases right a database is great and a database can be super quick and like what you'll find a lot in analytical systems is people doing benchmarks right of vendor ready says ah we compared our our database to database b and c and we ended upon this this benchmark being 10 times faster and then one of the other vendors does the same test and says ah no we were 10 times faster and the other one we were 10 times faster who do you believe but also like does it matter at the end of the day you know are you gonna go and pick a database right because one benchmark said that it was like two milliseconds quicker than the other one but then in a different use case it's going to be two milliseconds then faster than than that one and they're all fast they're all pretty much exactly the same in terms of like performance generally some are better at one use case and some are better at another use case and they trade lows a lot of it comes down to you are going to have to work with this thing every day right and you're gonna have to do weight you're actually trying to use the database to accomplish something you're not just buying a database because it's shiny and it's fast you're saying i'm trying to build something i need a database and and we thought well instead of just selling another database let's actually try and help people who are trying to do something and we'll solve another problem on top of just having a database yeah so you're mainly going for the developer experience angle yeah you know it's not not a million miles away from you know from neon that you you spoke to a couple of weeks ago who are you know doing something very similar for postgres right of being a serverless postgres and you just click a button and you instantly get one you never think about servers and scale and it takes three seconds you know you hit a button and immediately you've got one there's no spin up time or anything like that and it takes away a lot of the operational complexity and just helps you actually be productive with the thing rather than just giving you a database that you then have to go and do everything yourself with in that system then will i end up building two front-end apis i mean will i have would i have all of my gets on tiny bird and all my posts and puts on some other system i've built how does that play out so so that really depends obviously we're generally we're obviously like we're working with transactional databases within an application architecture right so you'll have tiny bird and you'll be making get requests to tiny bird when you say hey i want to get all of this analytics data and i want to display a chart or i want to make a decision based on some analytics data but then when you want to go and do get a user profile right because a user's logged in you're going to make a get request to some other api and your transactional database now we are not dipping our toes at all into the transactional world so you know we we take you up to the apis for the analytical stuff and then the transactional stuff is on you i would love to see somebody come and do the same thing that tiny bird's doing but for the transactional world which i think you know people are people are doing right i mean that space is is pretty hot right now you've got like neon and planet scale and super bass and all this stuff that are trying to put an experience around postgres to make that stuff easier so people are doing it and yeah we just end up like your your application it makes it you know a get request to your api for transaction stuff then it makes a get request to tiny bird to native tiny bird apis for for its analytics as well for the okay the pots and like sending data that again kind of depends it might be that you know if a transactional thing happened that you want to send it to both right and so maybe you just have the application like if your transactional database has got a post api you can just post it to that but tony bird also has a post api so if you just want to do an insert and just append new data to sunnybird we've also got a post api that you can just you know that comes out the box you don't have to set anything up and your application can directly just http post some json to tinybird and ingests it but it may be that you want a you know an api in the middle that handles data and you put it on a kafka thing because you're actually putting it into multiple places yeah really really depends what you're what you're trying to do a lot of different ways to do it okay yeah so we are back to it being part of a balanced breakfast yeah very much yeah it's part of a a sane architecture right it's it's a tool for a a particular job okay in that case last question then if i want to get started with click house and actually kick the tires on it i've already downloaded clickhouse and typed clickhouse local that was easy i got a prompt what should i do next i could i could put my vendor hat on and say well no you should it's the end of the podcast they'll let you have one sentence no i mean like the the clickhouse local is like a super easy way to go and go and try out quick house and do do player play around with it right tiny bird is another way that you can go and play play around with it we have a free tier right so you don't need to go and put a credit card in to try it out there's no time limit on it you can go about sign up for an account and there's a free tier and you can play with it and build something for as long as you want it's serverless so it scales to zero dollars so you know you can go in and store some data in there build some do some queries create some apis and you won't get charged anything so it's just another way that if you want to play around with it and see if it works for you there's nothing wrong with going in and just trying out the free tier it's not going to cost you anything and to see if if it works for you how could i get a big chunk of data in there to play with into into tiny bird yeah there is a whole bunch of ways so we have a as i was saying earlier we have a whole bunch of like managed connectors to bring data in so that you don't have to write your own so we have what we call the event api which is a a http api that you can just post streaming data to so if you've got an application that is like a web app that can make a http post request you can just sit there sending a whole bunch of streaming data to it if you've got a streaming source and one of the projects i built recently was a a mock data generator called mockingbird which is basically you create a little fake data schema in json and it will just generate fake data like realistic looking fake data and post it to streaming endpoints and that works with tiny bird but also works with like kafka and ably pub sub and all these different sources the pre-generic tools open source it's free it's not really tiny bird thing it's just i like bird names so it ended up getting called mockingbird but that's another way that you can generate some fake data but you can also like upload files if you've got a big file you can just upload a file from your local machine if you've got it on s3 you can generate signed urls and just we'll download it from a signed url we can connect a snowflake and just sync data from snowflake or bigquery that kind of stuff as well okay sounds like it's time to get busy al thank you very much for taking us through the world of analytics databases yeah it was great chatting with you and thanks for having me cheers thank you al now this is off the point but as we're at the end i can stretch out and tell you this something that's always bothered me is the asymmetry between online analytics processing olap nice and pronounceable and online transactional processing alt can't pronounce that at all it doesn't work so the solution is we go and invent online event processing and then we've got olap and olap which is nice and easy to pronounce and as a bonus feature kind of sounds like two siblings from a hans christian andersen story so back to the point thank you al if you want to learn more you can find links to clickhouse and tinybird in the show notes and if bird and house are making you think of they might be giants hit birdhouse in your soul i'm going to put a link to that song in there too because it's my podcast and i can do what i like as always if you've enjoyed this episode a like or a share would be very much appreciated and consider clicking the subscribe and notification buttons to make sure you catch the next episode but until that next episode that's all we have for you i've been your host chris jenkins this has been developer voices with alastair brown thanks for listening foreign