classification lets us pick one or the other or some small number of labels for our data the problem is that real life doesn't fit into these neat little categories when we have label data there isn't yes or no or a b or c or some labels? right, then we have what we call a regression problem. we're actually trying to predict actual outputs, right so given these inputs what's the temperature at which something will occur or? given this movie on a streaming site and the attributes and the people that have watched it what amount of action is it right because that informs who should watch that movie there's lots of times when you don't want to say--but sees this and isn't this you want to say it's a little bit of this and a little bit of this and that's what regression is for and some of the algorithms we use for regression are actually quite similar to classify. so for example, you can regress using a support vector machine or support vector of aggressor, right? but we also use other ones like so we're more likely to use things like linear regression and things like this so let's start off with perhaps for simplest form of regression. that's linear regression, right? it might not occur to people who use linear regression for actually what you're doing is machine learning but you are let's imagine we have just data that's got one input so one attribute attribute one and our output which is why this is our table of data just like before and this is our instance data so we've got one two, three four like this so what we want to do is we want to input attribute one and we want to output y which instead of being a yes or no is going to be some number on a scale let's say between norton one. so really what we're trying to do is we've got our graph here of our input variable attribute one and we've got our y output and these are our data points in our training set so here like this and they sort of go up like this what we're going to do using linear regression is fit a line through this data and a line is of the form y equals mx plus c so in this case m is going to be the gradient of our line and c is going to be b intercept so in this case i guess something along the lines of this straight up like this so if our m was one in this case m equals one or maybe equals one point two to make it slightly more interesting and then our c is going to be let's say c his naught point naught to these are the values that we're going to learn using linear regression so, how do we train something like this? what we're going to do is we want to find the values for our unknowns which are m and c given a lot of x and y pairs, right? so we've got our x and y pairs here and we want to predict these values the optimal values for this data set so we're going to find values for m. and c where this distance the prediction error is minimized the better fit this line is the average prediction error is going to go down if this line is over here it's going to be a huge error. and so the hope is that if we predict this correctly and we have an m and we have a c then when we come up with a new value that we're trying to predict we can pass it through this formula. we can multiply it by 1.2 and then add 0.02 and that will produce our prediction for y and hopefully that would be quite close to what it is so for example, let's imagine. we have a new value for attribute 1. let's come in here we're gonna look up here and this is going to be the prediction for our y and that's the output of our aggressor so this linear regression is capable of producing predictions based on its attribute now if we have more than one attribute this is called multivariate linear regression and the principle is exactly the same is this we're going to have lots of these multiplier ends we could say something like y is m1 x1 plus m2 x2 and so on for all of our different attributes so it's going to be a linear combination a bit like pca a linear combination of these different attributes and it's obviously going to be multi-dimensional so one interesting thing about linear regression is but what it's going to do is predict us a straight line regardless of how many dimensions we've got now sometimes if we want to use this for a classification purpose we still can all right now i'm supposed to be talking about regression not classification but just briefly if you indulge me we can pass this function through something called a logistic function or in the sigmoid curve and we can squash it into something. there's this shape and now what we're doing is we're pushing our values up to 1 and down to 0 right and that is our classification between 1 and 0 so it is possible to perform linear regression using this additional logistic function to perform classification and this is called logistic regression. i just what i mention, but that's something you will see being done on some data so let's talk a little bit about something more powerful that's artificial neural networks now anytime in the media at the moment when you see the term ai what they're actually talking about is machine learning and what they're talking about is some large neural network. now. let's keep it a little bit smaller let's imagine what we want to do is take item for attributes and map them to some prediction some regressed value, right? how are we going to do this? well, what we can do is we can essentially combine a lot of different linear regressions through some nonlinear functions into a really powerful regression algorithm, right. so let's imagine that we have some data which has got three inputs so we've got our instances and we've got our attributes a b and c. our inputs are a b and c and then we have some hidden new orleans right and i explained a neuron in a moment then we have an output value that we'd like to address. this is where we're trying to predict the value so, you know how much disease does something have how hot is it these kind of things depending on our attributes? this is where we put in a this is where we put in b and this is where we put in c and then we perform a weighted sum of all of these things for each of these neurons so for example this neurons going to have three inputs from these three here and this is going to have weight one this is going to be weight - this is going to be weight three and we're gonna do a weighted sum just like in linear regression so we're going to do weight one times a plus weight two times b plus weight three times c add them together and then we're going to add any bias that we want to so this is going to be plus some bias and that's going to give us a value for this neuron, which let's call it hidden want right because this is generally speaking we don't look at these values. it's not too important. we're going to do a different sum for this one so i'm going to all them in different colors so we don't get confused. so this has got three weights so this is going to be a different way this is going to have another different weight and we're going to do this much times a plus this much times b plus this much times c add them all up add a plus a bias and we're going to get hidden - and we're going to do the same thing with these ones here like this this is going to be hidden three hidden for hidden five and so on for as far as we like to go all right now the nice thing about this is for each of these can calculate a different weighted sum now the problem is that if we just did this then what happens is we actually get a series of linear regressions all right because this is just multivariate linear regression and in the end our algorithm doesn't end up any good right? if you combine multiple linear functions together, you just get one different linear function so we pass all of these hidden values through a nonlinear function like a sigmoid or tan so a sigmoid goes between naught and 1 so this is not than 1 and a tan hyperbolic tangent will go between minus 1 and 1 things like this and what that will do is add a sufficiently complex function that when we combined them all together we can actually get quite a powerful algorithm the way this works is we put in a b and c we calculate all the weighted sums through these functions into our hidden units and then we calculate another series of weighted sums so add together to be our final output and this will be our final output prediction y now the way we train this is we're going to put in lots and lots of test data where we have the values for a b c and we know what the output should have been we go through the network and then we say, well actually we were a little bit off so can we change all of these weights so that next time we're a little bit closer to the correct answer and let's keep doing this over and over again in a process called gradient descent and slowly settle upon some weights where for the most part when we put in our a b and c we get what we want out the other side now, it's unlikely to be perfect but just like with the other machine learning as we've talked about we're going to be trying to make our prediction on our testing set as good as possible all right so we've put in a lot of training data and hopefully when we take this network and apply it to some new data it also performs. well, let's look at an example we looked at credit checks in the previous video and we will classify whether or not someone should be given credit well something that we cut we often calculate is credit rating which is a value from let's say naught to 1 of how good your credit score is so a could be how much money you have in your bank b could be whether you have any loans and c could be whether you own a car and obviously there's going to be more of these because you can't make a decision on this those three things. so what we do is we get a number of people that we've already made decisions about right? so we know the person a has a bank account balance of five thousand two thousand in loans, and he does own a car and he has a credit rating of 75 or northpoint 75 whatever your scale is so we put this in we sieze wait so but this is the correct prediction and then hopefully when another person comes along with a different set of variables will predict the right thing for them so you can make this network as deep or as big as you want. we're typically multi-layer perceptrons or artificial neural networks, like this won't be very deep one two three hidden units deep maybe but what's been shown in the literature is but actually if you have a sufficient number of hidden units you can basically model any function like this right as long as you've got sufficient training data to create it so we're going to use weka again because weka has lots of regression algorithms built-in like artificial neural networks and linear regression. so let's open up a data set. we're going to use this time so they said we've got is a data set on superconductivity right now obviously my knowledge of physics is should we say average? but a superconductor is something that when you get it to a critical temperature it becomes it has no resistance right, which is very useful for electrical circuits and so this is a data set about what are the properties of material and what is the critical temperature? below, which it will be a superconductor now, i'm sure there's going to be some physicists in the comments that might point out some areas of what i just said but we'll move on. so we're reading a file. this is quite a big data set so we have a lot of input attributes and then at the end we have this critical temperature that we're trying to predict this temperature if we look at this histogram goes from 0 to'5 if we look at some of the other things so for example, we've got this entropy atomic radius, which i can pretend i know what that is, which goes from naught to two point one four. is that good? right, what we're going to do is we're going to start by using multivariate linear regression to try and predict this critical temperature as a combination of these input features so i'm going to go to classify. there's just one classified tab even for regression we're going to use our same percentage splitters before so 70% and we're going to use a simple linear regression function for this let's go so we've trained our linear regression and what we want to do now is work out whether it's worked or not on our testing set we've got the variables. we wanted y and we've got the variables that have been predicted y hat and hopefully they're exactly the same if they're exactly the same then they're going to be on a straight line like this so we were hoping to get a why down here and we it now, of course this won't actually happened what will happen is these wines are ever so slightly different than the y's we were expecting so you might see a bit of noise around the center like this and the way we would normally measure this is something called mean absolute error or mean squared error or root mean squared error which all very similar ways to measure the same thing it's to measure what is the average distance between what we wanted and what we got so if we were hoping to get away of north point - but we actually got a y of north point for then our mistake was we were not point to too high and so for every single instance in our test set we can sum up all of the areas we've got and we can work out what the average error was right. so we have a hundred in our test set we sum up the errors and we divide by a hundred and that tells us i mean error was a certain amount what will sometimes happen is your predictions will be above or below right? and so your actual mean error might be zero because half a time you predicted too high half a time you predicted too low and so on average, you've got it exactly right. obviously, that's not correct so what we tend to do is calculate something called mean absolute error so essentially if you're too small, we just remove the minus sign and call call it an error of that amount all right so if your mean absolute error is nour point four then what that's saying is but on average you're naught point far away live above or below than where you were hoping to be it's also quite common to see similar measures like root mean squared error for every instance we take our error we square it we sum them all up and then right at the end we take a square root, right? and again, this is a very similar measure to mean absolute error like the squaring. we move our negative symbols for us it's also quite common particularly in fields like biology and medicine to see something called r squared or the r squared coefficient and this is essentially the correlation squared it's a measure of how well or how tightly correlated our predictions and our ground truth were for example this would be a pretty good correlation if maybe naught point eight or nor point nine if these were our points like this and were absolutely. perfect. that would be an r squared of one if our points were everywhere that will be an r squared of 0 and what i saying is it's a value between 0 and 1 that tells how well we predicted zero means you basically didn't predict anything at all it was completely random output one means you predicted everything exactly, correct now, of course that's unlikely to happen or a test set what you'll find is you'll hope to get some number but somewhere around point seven point eight, right? but it will depend on how difficult your problem is to solve so maybe on a really difficult problem an r-squared of 0.5 is actually pretty good, right? so it's just going to depend on the situation. so we've got our linear regression trained up we know that the correlation coefficient is 0.85. we know that the mean absolute error for example is 13 degrees what we haven't done is visualize cyst sometimes a simplify to do this is just to plot a scatter plot of what we wanted and what we actually got from our predictor so i'm going to right click on linear regression. i'm going to say visualize classify errors it's going to be a scatter plot of the expected value and the prediction we actually got from our networks so you can see generally speaking. it's not too bad obviously the data set is quite bunched up in some of these areas which means that it's sometimes harder to predict but we've got a general upward trend which is exactly what we wanted you can see that the prediction around zero is not good at all the x-axis in this instance is the actual critical temperature of that particular substance the y-axis is what the linear regression actually predicted you can see that the range here is from about zero to about 136 on our actual values and the predicted values are from about minus 30 which doesn't really make sense to 131, but they're pretty close most of the ones that caused a problem with a very low values right because you've essentially got lots and lots of values that have a very small critical temperature on this scale, but different attributes that's been hard to fit a line to something more powerful for example a multi-layer perceptron, you know an artificial neural network might do a better job of those kind of instances but you can see that there's a general upward slope in this particular scatter plot be larger x's represent a larger error so you can see this is of a line we're actually trying to fit here down here with all these small x's and there's quite a few of them on there so actually for a lot of these substances the prediction even by linear regression has been pretty good regression algorithms let us predict real scalar data out of our input variables and then this can be really useful in a huge array of different situations where we want to predict some it doesn't fit neatly into a yes-or-no answer or an abc category label we've looked at linear regression and artificial neural networks, and obviously neural networks get pretty deep these days but these are a great starting point, so thanks very much for watching. i hope you enjoyed this series on data analysis something a little bit different from computerphile i wanted to thank my colleague. dr. mercedes torres torres for helping me design the content please let us know what you liked what you didn't like let us know in the comments what you'd like to see more of and we'll see you back again next time