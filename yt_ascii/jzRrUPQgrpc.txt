we are going to talk about explanations of a black box ai systems right so we have a black box system it does some magic inside and it gives us an output how do we know that this output is actually correct if you know the system installed in our self-driving car is not recognizing the obstacles correctly we're gonna crash i'm sure that you know many people maybe you're one of them who are very concerned about this prospect of using self-driving cars i'm personally not but i'm a computer scientist i think you know i trust technology my ai much more than i trust people but that's that's a side note but many people will just absolutely not trust the self-driving car so hopefully an explanation of why it behaves the way it does would help us to to trust the system more there is not always a direct link because i think that for example when we're going to see a doctor maybe we transfer doctor more because they're very distinguished and they have a wall full of diplomas you know behind them and not necessarily because they explain you all the intricacies of the treatment that they prescribe for you but in general i think the explanations are really helpful for the users to trust and to be more confident in the ai systems that they're going to use and of course we need to know whether they are correct in order you know to debug them and fix them you know in case they're not in general like any system we use would like to know that that is correct right it's working correctly at least most of the time so in case we have this black box here system you know this is not a black box but imagine this is the box and i feed something in so maybe i feed this picture of panda and it tells me it's a red panda so how do i know so it does seem like a red panda how do i know that it made the decision from the right reasons and not because it says red panda on mondays or they're not because anything with the blue sky is a red panda right or anything with a tree's background just suddenly red panda so how do we know that so an explanation method that i'm proposing to you does not involve opening the black box luckily so i'm opening only to get piece of cardboard but actually i'm going to keep the black box closed i don't really don't want to look inside and the reason i don't want to look inside is because a it's complicated b if i'm looking inside the number line on a particular architecture number of layers the neurons and if somebody brings me another black box for this purpose i won't be able to look inside anymore so and sword actually because the systems these black boxes they designed not to for us not to look inside right they train themselves you know you feed them training data they somehow tune all these neurons inside with the layers god knows what and then at the end they say that they're trained and they will give us the output so let's try to get an explanation without opening the black box so i have this panda in front of me and now i'm going to play with this image this is how i'm going to construct an explanation first of all i'm starting to cover parts of the panda with the cardboard so what happens if i cover this part this is probably no longer a panda right this is not about that what happens if i cover this part oh that's still a panda right so this part was not not essential for it to be a panda okay let's now try to decompose the top one so we're covering now lesser areas of the panda okay right so if we cover only this one it's still a panda if we cover only this one it's still a panda but if we cover those two it's no longer panda so those two have some influence on the fact that it was classified as a panda more formally what we're going to do is we are going to find a minimal subset of the image of the area of of this pixels of the image that is sufficient for the for the ai system to recognize the same which as a panda and we're going to do it iteratively by throwing away the areas that actually are not relevant for it being opened up and refining gradually refining the areas that had some influence on it being a pandem so at the end we have this so this was covered with gray cardboard with a particular color of cardboard doesn't matter and we see that the part that was minimal and sufficient for it to be recognized as a panda is a part large part of panda's head so right effectively we can cover everything and which would be the same as a human right which will be the same as a human see some shadows and not know what it is but the moment we see that face i completely agree with you and this is actually how we propose to check sanity of those explanations right if i'm looking at this and saying yeah it makes sense right this is bundle's face i mean i'm not a a biologist not as old just but yeah it seems to me like a panda's face and indeed this is how i'm going to to say okay this network i said that it's a red panda for the right reasons and not because today is monday i have more cute pictures right so this is a welsh springer spaniel energy that was recognized as well springer spaniel using the same method with a cardboard this is the area this is the minimal and sufficient for it to be recognized as a welsh springer spaniel now we're getting to another very interesting application of those explanations and this one is to uncover misclassifications so of course so i'm looking at this picture this is a child wearing a cowboy hat had it been recognized as a panda we would immediately say no that's nonsense in this particular case a black box ai system or a neural network that we run on it recognized it as a cowboy hat which actually seems okay right i mean it is possible that this picture is labeled as carbohat it can be the child or kabukat but you know kabukat is a perfectly legal output but then we try this explanation method so we started covering it throwing away the irrelevant parts of the image and this is an explanation we came up with so this part of the image was a minimal part that is sufficient for it to be recognized as a cowboy hat okay that's obviously wrong right that's a phase that's not a hat so what happened here so there are several things that we can infer from this misclassification one that obviously network is wrong right there is a there is an error two that it instead of a carbohydrates it can't recognize faces and three the from two we can infer that our training set was not built correctly specifically images labeled as carbohydrates were all of people wearing copperheads and now we we even know how we can fix it so how can we fix it very simply by introducing more images of just cardboard hats without you know people wearing them just cover head on the table cabo hat on the hangar on the hook etc now we wanted to check the sanity of our explanations are they stable at all what happens if i take this panda that was sitting on a tree and put it somewhere else will the explanation still take a part of the panda so we took this panda we cut a part that contained an explanation remember explanation was part of panda's head we glued it on top of other images so here is our roaming panda visiting the eiffel tower the moon this is very far roaming panda natural history museum the giza pyramid and having somebody's backyard every one of these images was labeled as a panda in the output of the black box air system and indeed the explanations were part of the panda's head which means that yes our technique works it works correctly and the fact that that we found this minimal sufficient subset actually definitely is not dependent on the panda sitting on a tree in the bright sunlight with the blue sky how many images do you send through this to test whether the test is right oh thousands testing the test i suppose yes yes testing the test yes thousands so we said we tested the whole image net when covered several interesting misclassifications similar to kabukat none of them were that photogenic there was something that was recognized as wool because of the texture so no nothing that that interesting but but yes most of them were recognized correctly which is definitely not a feature of explanations but the feature of the ai system so their system worked well most of the time and explanations actually much much our intuition and then with this we did it with several images so we just like the panda most so that's why we took the roaming panda but this just make to make sure that that it passes the sanity to check let's now talk a little bit about explanations produced by these techniques versus explanations generated by humans so i have a starfish here if i ask you or the viewer why is it a starfish what would you say well it's the shape right it's the shape yes it has you know five arms but now i'm asking you so what is a minimal area of this image that is sufficient for you to realize that it's a starfish yeah good question very good question so probably thinking about this cardboard technique i will start covering it we will soon discover that there is a symmetry to this shape and actually i'm saying well it's a starfish because of probably you know i see i'm seeing these two two arms and yeah this is this is enough for it to be starfish right so this i'm covering the rest i will still think that that's a selfish right but it is a symmetrical shape therefore this is also probably an explanation of why it's a starfish so we need multiple explanations in some cases in case there is a symmetry a human will give multiple explanations and again we are in the realm of increasing trust we do expect our system to be capable of giving multiple explanations this is definitely an important goal not only to increase trust but also because symmetry is an important feature of some of the shapes just from biological perspective so the fact that this is a symmetric shape might must be crucial might be crucial for it to be recognized as a starfish but it's not the symmetry itself is it obviously we've probably all seen starfish with a missing leg right yeah yeah but there's still a starfish exactly exactly but you don't need it to be symmetrical for it to be recognized it's just the symmetry helps you to have multiple yes yes that is true and if we see starfish that is partially obscored by sun partially occluded by by sand or you know by another fish and by the water i would still recognize that it's that it's a starfish and it doesn't matter actually which one of its arms is occluded or you know maybe several arms it's a starfish yeah i agree so there is something here on the border between these explanations for partially occluded objects and the fact that it is symmetrical and actually there are several equally important parts to the starfish that make it a starfish right so what makes a starfish a starfish or you know we can even look at our famous pandam yeah so what makes panda a panda so we already saw one explanation and this is a panda's face but maybe this is not the only one maybe what makes panda panda panda is that it has this particular face or maybe that it has a tail or maybe it has this floor or a particular shape of its ears so just like a human would give several explanations to the fact that it recognizes a particular object as a panda or a cat our black box cx system should also be capable of giving several explanations and this of course if we want to make sure that it classifies similar to us humans i think this is crucial if we want to use this black box ai systems that they classify the recognize objects in a similar way to us humans otherwise how how can we use them how can we trust them over time we'll be able to make small changes which will change this number and hopefully increase it until that is the category that is seen in the image and not that you're completely different or every time you call the method then this will be less effective in some cases then it will even slow things down because the program will never appear to stabilize