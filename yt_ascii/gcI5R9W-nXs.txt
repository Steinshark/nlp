hello my name is shiman tot thank you for choosing my talk today's talk is inspired by the many package oriented talks that represented as recent conferences and i should say most of these talks are great and you should definitely watch them however many of these dogs share a common undertone of being dismissive of monoripo setups my personal philosophy is that you should use the right tool for the job and therefore here i am trying to balance out the negative marketing in the first part of the talk i will go over the main differences between monoripo and poly repo setups and then the majority of the talk will be spent going over different use cases of bazel as the build system for monopol environment i encourage you to follow along for the second part of the talk and to do that easily i have prepared for you a dev container environment all you need is visual studio code and docker on your machine then simply open the repository in visual studio code and you should get an on-demand built docker image that you can do use to follow along for the second part of the talk before i start with the first part though i need to do the mandatory self plug i have a free book about c plus algorithms and you can grab it on github or if you also want to donate to eff it's also available on nintpop besides that i also post daily c plus content on twitter and linkedin with that out of the way let's start with talk proper and let's have a look at the differences between monoripo and polyripo environments so what is the difference between a monoripo and a pollo repo well the prefix mono means one or single and the prefix body means many and i could stop there but i want first clearly this is a spectrum if you have two big repositories with many projects each do you really have two mono repositories or do you simply have a poly repo in this talk i will be discussing many of the two extremes that is everything in a single repository or each project in its own repository and notably this is already fuzzy enough since i didn't specify what i mean by a project on top of that a clear determination isn't really that important especially not in this talk because there is a second and arguably much more important axis first we have the ed hat model where we build everything from the current sources in the extreme cases you could even build your operating system your compiler and all the tooling from sources for practical purposes though there will generally be some sort of cutoff between what you consider the platform that you build on top of and what you consider to be your project that you build from sources the second model is the binary model where everything is a version binary for both of these models we will need some sort of infrastructure support for the attack model we will need a build system that will only rebuild what what is absolutely necessary for each change for the binary model we will need the package manager that is capable of resolving version requirements into a single single cohesive release now i said that the distinction between monoripo and poiripo isn't that important for this talk and that is because i will be only talking about two specific combinations specifically the edhead monoripo and the binary polyripole technically the other two combinations are possible but i consider them strange choices however if you actually use either a binary monoripo or an attack polyripole i would love to hear from you please get in touch a lot has been written about these two models and so i'll be fairly brief but i do need to give you some sort of introduction and point out the core differences let's start with the pain points for the head monoripo the main pain point is scalability if you don't have a suitable build system and don't structure your project appropriately you can easily end up in a situation where you will have large part of your project being rebuilt every time for every change this can become very quickly infeasible for the binary pol repo the main pain point is api stability since the different projects interface with each other using binary artifacts you either need aba stability or if you don't then you actually end up in the same situation as before with the attack mono repo and you need to rebuild everything all the time the big benefit of attack monoripo is that we maintain the ability to do global changes since all the code is in a single place we can actually do operations like upgrading the version of the compiler or doing a global refactoring just to be clear i'm definitely not claiming that these operations are easy if you want to see how incredibly hard such operations actually are i do recommend that you watch some of the talks from google notably the talks about switching from unordered map to flight hashmap and switching more recent one switching from un64 to un64 underscore t but while not easy these operations are at least possible with the binary poly repo with effectively disallow global operations completely each project is its own isolated bubble so we really need to step outside of the system to reintroduce any sort of global coordination in practical terms there will be a lot of this coordination and possibly some top-down company mandates the two models also differ in how they handle dependencies private dependencies that is dependence is that you do not expose to your public interface can be problematic with the asset model this arguably depends on how exactly you manage multiple versions of the same dependency generally speaking in mono repo you would only have one copy of a dependency well if you have one copy of a dependency well then even if a dependency is private for you it can still be potentially shared with other teams meaning that it's effectively shared if you allow multiple versions then a private dependencies are trivial with the binary poly repo model private dependencies are just that they are private because each project is isolated changes are completely local as long as you do not leak the dependency from your public interface shared dependencies are a bit different they need to be carefully managed in both models in the ed head monoripo the shared dependency can introduce breaking changes in other parts of the repository the good thing is we have global operations meaning that if the change is breaking well we will know immediately and the person making the change still has enough context hopefully to make a trivia fix on the other hand in the binary public report the situation is more complicated first we must ensure aba stability well if we have a shared dependency meaning something we export through our public interface what does part of our public interface so any api change is a problem second if the change is breaking it might not be noticed for quite some time version upgrades are not necessary under the control of the provider of the dependency and to some degree it can also matter what kind of dependency resolution algorithm you are using because of this disconnect a breaking change might be noticed after a long time which might mean that there will be additional triage time on the side of the user and when the fix is actually required the local context might be already lost to be clear on the whole these two models are equally good the point i'm trying to make here is that they might not be equally good for you your company or your environment i want to close this section of the talk with three more points that are a bit more abstract the ed had mono repo very much requires a cooperative environment if you make a change and accidentally break a user well you will know immediately and you actually do need their cooperation to move forward in the binary port repo model can actually work in a competitive environment if you are dealing with breakages outside of the documented behavior well then it's generally the problem of the user and the user will have to deal with that the ed had monoripo also implies a conforming environment since all the code is in one repository this implies common tooling common code style processes and so on with the binary poly repo well each project can have its own tooling and style as a consequence there is a definite bias in these two models in the added monoripo it is the users of dependencies that have the easier time and in the binary poly repo it's the providers of dependency dependencies that have the easier time so do choose carefully i did mention that both the ed hat and binary model will require special tooling so let's switch gears and let's talk about the basal build system is an open source build system which originates it from blaze the build system used in the google mono repo this heritage is what makes visa great as a build system for more environments i have debated how exactly to structure the practical part of this talk and ultimately i have settled on a series of recipes that should give you a good idea of how bazel works and also give you a good starting point if you want to explore these areas further and naturally we have to start with a hello world to compile this simple program we must first set up our bazel project and to start with that we need bazel itself if you are following along the docker image already comes preset up with bezelisk is a tool that will automatically download the requested version of bazel and then behave as the base of binary in the docker image basilic is already seem linked and in the root of the project we have a basilisk rc file which specifies the version of bazel to verify we can query the version of bazel moving on to the actual project we need one mandatory file the workspace file for now we can leave it empty or specify the project's name although to know that since all the examples i will be showing in the talk are in a single mono repo the workspace file already has additional configuration that i will be talking about later the second project byte file is bazel rc here we can specify the command line arguments for bazel this is typical the place where you configure project device settings for now we will use it to set up our compiler and flags finally we need to tell baser how to actually build our hello world project this is done through the build files for our hello world this is very simple we have one source file and the only other thing we need to specify is the name of the resulting binary with this we can now build and run our hello world program you might have noticed that i use a special syntax to refer to the hello world binary so let's take a short detail and talk about how bazel organizes packages and targets whenever we refer to a target there are three components of the full path the repository the package and the target itself note that the package buff corresponds to the file system path to refer to a local repository we can use the add sign without any name or skip the repository part altogether similarly when we refer to local targets we can skip the package path on the other hand when the target name matches the name of the package we can also skip the the target name and use the package path only to demonstrate this behavior let's go back to our code if he switched to the directory with our hello world program we can use the target name only i've also prepared a copy of the hello world program in a subdirectory we can again run this binary using the full path or since in this case the name of the target is the same as the name of the package we can omit the target a simple example like this is great for demonstrating how to build and run binaries however your project is probably more complex so let's split the hello world program into several pieces the hello world main now delegates to a library to bring the greeting this is then reflected in our build file where the binary route depends on another target as already mentioned the package name image is up with the directory structure so let's have a look at the greeted directory and let's explore the library the greeter delegates further to a formatting library to format the message the message itself is also stored in a separate header to build the greeter library we will require now a more complex build rule first the formatting library remains relatively simple however note that instead of cc binary for binary we now use a cc library for a library for libraries we also have an additional attribute specifying the public headers in this case we have only one source file and one public header the greater library takes advantage of multiple features first we take advantage of private headers headers that are not listed as public will be accessible will not be accessible to the user in this case the greeted dev header will not be accessible to our hardware binary second we also take advantage of implementation dependencies these are non-transitive dependencies that again won't be available to the user here we declare the format library as a non-transitive dependency finally we are specifying visibility by default each target will be only visible within its own package if you want to make a target accessible from other packages we need to change its visibility here we mark the visibility as public meaning that this target will be accessible from any package besides private and public the visibility rules offer finer grain control but this is not something i'll be over going over in this talk hello world now involves building the greeter and format library but this will be handled for house transparently by bazel i talked about the private headers and implementation dependencies so let's quickly have a look at what happens when we include the private header or rely on an implementation dependency first let's have a look at the binary that includes a private header when we try to compile this program we will receive a warning and because i have warnings set to errors this program will fail to compile second let's have a look at the binary that includes a public header but from an implementation dependency when we try to compile this program the private header will see the public header from an implementation dependency will simply not be accessible both private headers and implementation dependencies are features that are not enabled by default therefore we need to go to our bazel rc file and add these as runtime flags to to the build command on top of that private headers require compiler support so they will only work with the client compiler our examples are starting to glow in complexity so is the proper time to start talking about testing testing frameworks are external dependencies so if you want to start testing we first need to discuss how to pull an external dependency into our bazel project i will be using the google test and the cache2 frameworks to demonstrate both of these already support bazel which makes the situation simpler all we need to do is to load the dependency into our project however there is a distinction google test already supports the new bazel module functionality which cache2 doesn't the benefit of basin modules is what you would expect they resolve the correct version of transitive dependencies to satisfy all the requested modules when importing on dependency manually we might need to take care of some of the transitive dependencies modules are added into the module.pazal file so for example here we are pulling in the google testing framework and one of the transitive dependencies for the catch-2 framework to import the dependency manually we need to go to our workspace file here we use the http archive rule which is the recommended approach where you can also use the slower route and pull using the git protocol the typical naming schema for dependencies is the domain in the reverse order and for github the user then followed by the repository name since the naming is purely for the purposes of this project we can of course choose any name that is convenient now that we have our two testing frameworks ready it's time to write some tests the c plus code isn't essential here so let's keep directly to the basal configuration on the most basic level a test is simply an executable that either returns a zero for success or a non-zero value for a failure so it's not that surprising that the definition for test is similar to binary the typical name means convention for test is the dependency being tested followed by an underscore test besides the sources we also need to specify our dependency pitch at minimum will be the testing frameworks and the dependency we are testing in this case we depend on the google testing framework and despite using a different approach for pulling in the dependency the same test using the catch-2 framework will look almost identical i did not explain the size label yet by default the only thing that is affected is the timeout for the test however this information is also provided for the test binary so if you are providing custom main for your tests you can do extra things for example and force different system limits based on each of the supported sizes i said before that this is simply a binary that it returns zero on success or non-zero value on a failure we can combine this with the fact that bezos supports multiple languages and write a simple black box test foreign script that we use as a black box test we still have the same structure we have a name for the target we declare our sources but we have one distinction we need do not depend on our c plus plus code using codependency instead we pass it in as a data dependency the distinction is simply that codependencies have semantic meaning this meaning is built into the rows for example the c plus rules understand that if we are depending on a library that library needs to be linked in to the binary and the public headers need to be made available during the compilation data dependency is still treated as a dependency within the base framework itself so for example if it would change the binary.cc here and then run the black box test the binary will be rebuilt so the test is running against the current version but there is no other implied behavior however we do have a slight problem bazel is a hermetic build system one of the consequences of that is that tests do not have access to the normal file system and each run of the tests might end up with unique bars on the dependent data therefore what we need to do is pass in the current location of the tested binary one of the ways to do that is to pass the location as an argument i should note that while i have big big the show script as the language here the same approach can be used to write external tense tests using python or even javascript the complication created by hermetic execution is something that you will run into even with cpas plus fortunately for c plus plus bazel also provides a helper library that can translate the paths from workspace to runtime so let's have a look at an example here we have a simple test that has a data dependency on file.txt we also depend on the base library that provides this runtime support inside our test we can then use the run files library to translate the workspace path to our data to the current file path finally i should quickly note how we can run our tests perhaps not unexpectedly we use the bazo test command and as we have seen previously we pointed we can point it at a single target however more typically what you want to do is run all tests in a particle package for this we can use a shorthand the three dots represents all targets in in the package and all its sub packages you might also notice the cached annotation for the test that we run previously this is because bazel employs cache even for tests meaning that it will only run tests that were affected by changes once we have testing coverage it's also worth turning on sanitizers sanitizers can run on top of already existing tests and catch a subtle bugs that wouldn't be manifesting otherwise the way to integrate sanitizers into a bazel project is through configurations let's switch to our base rc file so far all the flags we have added were applied globally configurations around flags to be grouped under a configuration label and then these are only applied when that configuration is requested for example here we add flags for the address sanitizer that is the compilation flags and the linker flags all under the asm configuration the platform suffix is required to ensure that we don't trample over our non-acent cache we can then use this configuration by passing the config asan to a test or run command since we just talked about sanitizers it's only fair to discuss static analyzers because most static analyzers rely on a compilation database we can integrate static analyzers externally by having bazel emit this database to do that we can import the third party module and as with previous external dependencies that don't support bazel modules yet we add it to our workspace file this particle module provides an executable that will generate the compilation database so all we have to do is to simply run it alternatively if you want to integrate a specific static analyzer for example client id we can opt for more integrated solution that can take advantage of bazel caching facilities again this is proofer party module we can invoke this module manually however the optimal approach is to integrate it using another configuration for the build command the specific flags are simply copied from the documentation of the module with this done we can now build targets and have clanked id automatically run with the build here we have an example which doesn't report any issues and another example that does for the most part we have a complete project now i talked about how to structure your code how to put in external dependencies how to write tests and how to use sanitizer and static analyzers so far i have made life very easy for myself we are building for one specific platform not to mention that i simplified things even further by just building inside of the tightly controlled docker image while building inside of the docker image makes sense you will probably need to build for several platforms and that usually means dealing with sound of the idiosyncrasies of the different platforms through platform specific code to handle platform specific code we need two base of features platforms and constraints however for c plus plus we first need to flip a flag in our visa rc since the introduction of platforms and constraints was was a breaking change this already comes with predefined labels that describe the common cpu architectures and operating systems the simplest way to integrate with these is to mark specific targets as platform specific this is particularly useful for tests in this example we mark this particle test as windows only which means that if we try to run a test in this package this test will not be run and instead it will be skipped having platform specific code not cause issues on platforms if it's not supported is one thing however for a project to fully support multiple platforms we will probably need platform specific dependencies is an example let's take this simple program that prints the platform information from a library this binary then pairs with specific libraries one for windows and one for linux now we just need to put this together in our build file first we constrain our platform-specific libraries to the corresponding platform to make sure that they do not cause any accidental build or testing errors then or on our multi-platform binary we use a select tool to pick the correct dependency based on the platform when we run the resulting binary we get the linux version since we are inside of a docker image that is ubuntu based i should know that while i'm demonstrating platforms feature on a predefined constraint you can absolutely define your own if you for example need to introduce a bridge library to deal with an old version of a dependency you can introduce the version of the dependency as a platform feature and then use the select logic based on the version information the complementary part to platforms are two chains typically you wouldn't really define your own tool chain but instead use platforms to constrain your targets and define the target platforms and let the language rules figure out what two chains are appropriate however c plus is a bit special for c plus plus it's often desirable to build a project using multiple compilers on the same platform and to do that in a controlled manner you would have to define custom tool chains i will not be showing an example of that here because it's a highly mechanical process but if you are interested bazel has a very good tutorial on how to define a custom c plus 2 chain instead let's have a look at something more practical cross compiling into web webassembly using m scripting first as before we need to pull in the module for m scripting the m script and baser module defines the required m-script and two chains that can then cross compile c plus plus code into webassembly to demonstrate let's take this hello world program the civil space side of things is the same as before we simply have a cc binary with one source file the webassembly two chains are packaged into build rules that use the vessen prefix therefore to cross compile our hello world we need to create a new version binary and pass our c plus target as one of the attributes once we build it we will get the expected webassembly files in the basement directory and we can run the resulting javascript through node so far i haven't really discussed these simulink directories that baza creates these point to different parts of the basal cache for example the basal bin for binaries and base out for generated files while we could pull out binaries out of the bazel bin directory and pull out the generated files out of the base out directory manually and then package them that would be inherently quite brittle and of course unnecessarily complex so let's have a quick look at how we can package our project using bezel itself i will be demonstrating using a debian package for other artifacts such as docker images or windows installers you will need different basal modules but the overall approach is the same let's start with a simple program that reads and prints the content of a file installed in the user share directory first we need to package the binary in this case using the package style rule the combination of the strip prefix and package there means that the custom command binary will end up directly in user bin second we need our file this is the first time i'm showing the file group rule and it's exactly that meaning it's a group of files that we can then refer to under the new label we then package this data ensuring that the file ends up in the correct directory finally we put this together prep everything as a debian package and add the required metadata when we build the package visa will take care of building all the dependencies and at the end we end up with the debian package that we can install once installed we can run it from the system location when i start this talk i mentioned that to make the edit memory promoter visible we do need the build system that only rebuilds what is absolutely necessary and baza ensures that through caching however what happens if you are in a multi-developer environment can you still take advantage of caching let's start with github to ensure that we are not rebuilding everything all the time and not running tests that do not need to be rerun what we need to do is tower the cache in between the runs fortunately to do that bazel already provides a cache action we can use this action to store the cache in between runs the github action itself is fairly minimal we check out our project we fetch the previous cache install compiler and bezel disk however please know that i'm really only doing this from pure laziness the proper way to do it would be to build a docker image with these already pre-installed this way we are really wasting a lot of cpu cycles on the installation finally we can run the tests after pushing this configuration to our repository github will now test each commit but more importantly it will keep the cache in between the runs i can demonstrate by updating the project with the dummy file we can then check that both of the tests were skipped since they both still have a valid cache state alternatively if you are using a local dot environment you can take advantage of remote caching i will demonstrate an approach using web dev however there are other options also available first i have a small script that just sets up engines as the web dev server and starts it once this is done we can turn on remote caching in our visa rc file and then populate the cache to demonstrate that the cache is working we can switch the cache to read only and now if we clear our local cache and rerun the command the tests are still cached because they were fetched from the remote cache for practical setups you will generally want only your continuous integration to write into the cache and have all developers only read from the cache this means that if they pull a fresh version of the project that code was already compiled and tested by the continuous integration and therefore is already in the cache at the same time any local development will not be able to poison the cache finally i want to close this talk with couple of topics that are worth mentioning but did not make the cut for the full demo what about windows support well sadly windows platform is not really my area of expertise however as i understand it bazel should work just fine on windows and should interface with the microsoft tool chain without any problems throughout the talk i have used quite a few bleeding edge features not to mention the fact that the docker image is actually running a pre-released version of bazel now this makes complete sense for conference talk but what about production first you should absolutely use a stable version of bazel the stability of the experimental features is mostly fine however there are many corner cases in particle there are multiple features interact there is a very good reason why the examples in the torque are separated by feature adding third-party modules into the mix makes everything even more brittle as i was preparing for the talk i actually did report quite a few bucks both to bazel and to the third-party modules talking about the third party modules you might have noticed that i used quite a few of them and to be fair this is actually the typical basal experience the reason why this is is because bazel is britain in skylark which is a subset of python and as a result writing modules is fairly easy hence most things that you might want were already implemented by someone else if you want to write your own module based documentation is also quite good since we are talking about modules what about c plus 20 modules well there are two third-party modules that provide c plus 20 support unfortunately one of them is relying on the clank pre-c plus 20 module implementation and the second module is right now transitioning to basal modules and does not work on top of that there are still a couple of compiler issues notably clank is still hard coding paths inside of the compiled modules which is effectively breaking the bazel hermetic build model because space is building the paths end up being effectively random i have also silently avoided first party c plus plus dependencies in this talk now the thing is you have couple of options how to pull in third-party dependencies c plus plus dependencies notably there is a third-party module foreign rules that's maintained by the bezos bazel team which can actually wrap external ability stems however if the project is doing anything strange or custom it quickly runs into issues you can also opt into an approach using package managers if your package manager supports bazel or then you can put in the dependency as a binary and use the package manager integration into bazel and the third is the most painful one which is where you pull the project in using simply the http rule to just download it into your project and provide manually written build file i have also talked a lot about caching and cache tests but sometimes just simply run too long or can't easily switch to a hermetic mode so what do you do then well unfortunately then you will need to partition your tests you will need a group of tests that can run with each commit and a group of tests that run after the code has been already committed potentially not on every comment if the code passes through the initial test but fails on the integration test you might need to do git bisect to figure out the specific commit responsible and i would recommend the policy revert now and ask questions later when you find the committee responsible and just point it to the author to figure out what actually happened finally i have also included quite a lot of links and references in the talks repository so i invite you to browse through them so you can also live comfortably ahead with bazel thank you