meta's new llm based test generator is a sneak peek to the future of development kind of sounds exciting let's see what they do i always have this worry that we're going to have millions of tests in the future and they're all going to be like 90% of them are going to be horrible like that's always my big worry meta also gave us react so you know things to think about meta's test gen llm is a sneak peek to the future of development productivity specialized orchestrated and rigorously filtered okay metas recently released a paper called automated unit test improvement using large language models at meta it's a good look at how big tech is using ai internally to to make development faster and software less buggy for example google is using ai to speed up code reviews a major win of this paper is that while it integrates llm into the developer's workflow it also recommends fully formed software improvements that are verified to be both correct and an improvement to the current code coverage always generally worried about striving for just higher code coverage for the sake of higher code coverage sometimes tests are super stupid just to hit an if statement right you have like this internal hit if statement the problem i have with it in general is that often once you start driving towards like 100% you start writing tests for specific lines you've missed and then any form of refactor just invalidates the test immediately right compared to this github co-pilot let's see compare this to github co-pilot where suggestions still have to be manually verified to work by the human what a weird statement is this written by chad gpd what is this and we all know that debugging code is twice as hard as writing it i mean now you're just debugging tests aren't you unless if oh i'm curious if it's more like some sort of fuzzing version of the test meta claims that th this this is the first paper to report on llm generated code that has been developed independent of human intervention other than final review sign off and landed into large scale industrial production systems with guarant assurances for improvement over existing code bases but isn't like aren't we just isn't this problem again right like aren't we just like isn't this like the exact problem that we're running into right here if you're saying that you're adding nonhuman interv interv intervention other than review sign off like isn't this aren't you at some point going to have to debug the code and the the code could be well it's going to be written like the probably like the average github piece of code which i'm not i'm not trying to say okay i'm trying to say something here it could be pretty shitty furthermore there are solid principles that developers can take away in order to use ai effectively like already the first thing that i think of that would be way more cool is ai based exploration for integration front-end tests that use seeds to kind of generate how they walk through the ai or how they walk or some way to keep track of exactly how they walked how much time they did and everything and then to be able to use that to report bugs kind of you know that tiger beetle that tiger beetle presentation that we saw where they use ai fuzzing to test like 200 years worth of transaction actions every single day on tiger beetle it'd be much more exciting to be able to see ai do some sort of more generated walks through your ui and try to find bugs or try to find memory problems or try to find something that's more useful than simply this which is like here's yet another unit test for me that it just feels less exciting we'll see table contents key points one minute read stats one minute read actionable takeaways if you're short on time just read this three minutes dude bros i know i read i know i read articles for the guys and gals in here okay i'm the i'm the i'm the only professional dyslexic reader of anybody ever if you ain't got time for a two-minute intro read for your three minute read you got to loosen up the schedule can we all agree here that is it like is it really are you really that tight that one minute's too much if you're short on time i don't got two minutes sweet ridiculous absolutely ridiculous i want you to know that right now hold i'm going to bring chat over here so i can see it all right test gen llm uses an approach called assured llm based software engineering what are we assuring assured llm se wow that is is this where we jump the park okay jump the shark why is it not called a l ms e like why have this word spell out but the other and where did bas go using private internal llms that are probably fine to probably that was a great phrase right there probably fine ted with meta's code base this means that it uses llms to generate code improvements that are bl backed by verifiable guarantees of improvement and non-regression is this really truly all fancy talk just to say more unit tests test gen llm uses an ensemble approach to generate code improvements this means that it uses multiple llms prompts and hyperparameters wow that sounds so effing fancy to generate a set of can candidate improvements and then selects the best one this approach can help to improve the quality of generated improvements test llm is a specific design to improve existing human written tests rather than generate code from scratch okay that's more interesting okay so this is actually more interesting improving so i actually have like improve the improvements i know dude dude what are these words they make no sense i know okay just just go with it i do want to point out something one thing that i hate about unit test and i see this regularly is like with just i don't know if you guys know this with just here i'm going to go like this just test i think i have something called just test if i jump in here and i think i have these these are generated tests don't don't mind them i'm pretty sure you can go like this test do each dot and then you can do this and then you can actually have a function here i believe and there you go there's your test right there you go and you can actually pass in parameters in here right and so this would be a and a would would be this thing right here i hate this idea like i hate logical like a bunch of logic in your tests table tests they are called i hate table tests i hate this kind of stuff cuz whenever i'm debugging they are just such dude when you need the debug your test it's one of the worst experiences ever and so my worry is like right off the rip is that if you have tests improv in your already written test it it becomes worse and worse and worse it's basic parameterized testing yeah it is basic parameterized testing until it becomes just a nightmare and a half to debug something about it like i just hate the idea of a for loop and a test i feel just it's something just something bothers me i don't know why hey just call me old fashion hey maybe i maybe i'm wrong here i can be iy i'm fine to be wrong here but i don't think i am i feel like test needs zero possible logic like as little logic as possible the more declarative you can write your test the better you are wrong you're wrong test llm has been integrated into meta software engineering workflows this means that it can be used automatically to improve tests as part of the development process it would be cool to see some screen let's see it would be cool to see some screenshots of exactly how integrated but the paper doesn't provide any here we're going to go to twitter anyone anyone use the new meta test llm what the hell do we call it what the hell is this thing llm bas test generator anyone use the new meta lm best test generator i would like to have you on and chat let's see and chat about the experience for 15 minutes boom i think this would be really cool i think this would be really really cool i actually want i actually want to i want to hear someone explain to like explain to me some of the good parts of this because it is it it would be it'd be it'd be pretty it'd be pretty interesting casey's gonna make it i think i'm pretty sure casey has now just messaged me let's see threat let's see okay here we go test l has been integrated blah blah blah let's see table results from the first integrated testathon conducted in november 2023 human test authors named by the product component on which they worked the test the test gen llm tool landed in sixth place overall demonstrating its human competitive ad value okay so they had a testathon and the test gen llm was compelling and they landed sixth on number of tests written i'm not sure if the number of tests written really is a w can we all agree here threads engineers wild yeah because look they covered less lines of code with more tests literally over double the tests and 33% less code i don't know i i don't know what's the win here right i don't think like i think you might be measuring like overall this whole chart's effed up if anything lines covered per test the test gen llm might actually be the winner here all the all of a sudden i'm starting to think maybe it's really good because this actually sounds like this this could have been pretty nice friends engineer wait there's friends engineer there's a friend friends engineer i believe they call that hacking okay friends engineer hacking for sure all right these stats are either direct quotes or paraphrase quotes from the paper let's see all 100 build 75 bunch rejected 75 passed reject rejected added coverage okay so there was 25 that added coverage okay cool the image shows that the let's see that in the evaluation of reals and stories products for instagram 75% of llm test cases were generated built correctly 57 pass reli can we just back up 75% of test llm cases built correctly that means one out of four was hallucinated on some level i now take i i i take back everything i've said and i now think it's fake and this sounds actually terrifying like that actually is terrifying to be 57% passed reliably and 25% increased coverage i'm not convinced this is good i am not convinced this is good how do you measure what's right well they're not not notice that they don't say right what they say is that this couldn't even build 75% of them could build 57% could just like they kept making green happen over and over again and then 25 % increased how many lines were covered in the tests so nothing in here saying which like right or correct the test could be complete nonsense gibberish it just started catching lines that we that haven't been tested now again another one of the hard parts i'm seeing right away which is not every line is valuable to test let me give you a quick example okay let's pretend i had this as a function right here since this is javascript pam number there we go and i returned here co-pilot will write it for me real talk should i test that should you write a test for this i'm actually curious what what do people think i'm going to go like this i'm going to do a little quick pll because i'm actually i'm actually curious about this what people think in general about this because i think this shows two different natures should you test su and i'm i'm i'm dead serious it is a sum function who here in javascript hasn't written a sum function like 900 times because there is no standard library sum function right we've all done it we've all done it times and i'm i'm being dead serious about this yes no i don't know right like we've all written this so let's see we'll get back to this let's read a little bit more we'll read a little bit more and come back to that test gen was able to improve 10% of all classes to which it applied and 73% of its test improvements were accepted by developers and landed into production again a lot of questions here which is i have seen in my professional experience at one of the most prestigious places ever to work that sometimes when code is sufficiently confusing or involving just testing that people will lgmt or lgtm okay i've seen it i've seen some rubber stamping lgtm looks good to me they they get they honestly they give me the lgbt and we move on for the day it's true it's very very true by the way nothing nothing feels okay look at that so most people say no interesting by the way nothing feels better than having a hype train while doing this all right so most people think know and about the other half think i don't know or yes right you fall into the i don't know/ yes category it's interesting that people want to i'm i'm actually very curious about that so me personally real talk i wouldn't test this i just wouldn't this is just not something i would test a you have to export it you have to increase the you know the api surface area it's a simple one i'm not really into testing i'm just not really into testing it like real talk it's just too simple i know i know people feel differently about that i just think that when the functions too simple it's just not something i'm going to waste my time on i'm going to waste my time on the hard things right but management needs 99% or 90% code dev i would tell management to clean off all right anyways in a test athon between engineers of various meta engineers created test in order to increase instagram's test coverage the median number of lines of code added by test llm was 2.5 okay however one case hit the jackpot and covered 1326 lin this this this is a really important stat which i iterate upon below okay so that's interesting that they just found a okay so like to me this seems i guess interesting that maybe they could have dropped all the other tests and just this one test actually did something right all improved case generated during the testathon did coverage at least one additional valid corner case such as an early return endor special processing for special values such as null and empty list i don't know sometimes these happy case testing things i just find so they kind of like they hurt refactoring i don't know i i'm not i'm not a huge fan of it anyways so let's look at this prompt name extend test here let's see here is a cotlin unit test extend this class write an extended version of the test class to include additional test to cover some extra corner cases okay so this is just like their prompt template all right so they literally just pasted these classes in and made them generate stuff actionable take ways test llm is a good example of how llms can be used to improve dev productivity and software reliability in a in a time efficient manner i'm not convinced by that yet there are a few takeaways i got from reading this paper that give us a look both to how big tech is implementing llms internally and how any developer or engineering manager reading this can use llms to in a more productive manner i'm actually worried about this too see this is this is what i call conference driven development is that this inevitably will turn into a conference talk which will inevitably be adopted by a bunch of people which will inevitably create a huge amount of code which will inevitably cause headaches down the road i'm not convinced that this is the best way to do development conference driven development isn't always that great right every now and then you can find something that's truly amazing but i am not convinced that this is a good idea like even even these takeaways this one right here was interesting this hit the jackpot to me that's actually interesting that should be something maybe you should look at because that's a lot of lines covered at least ran once and maybe that's a compelling reason to keep this one test or to rewrite it as a developer to me what it seems more interesting is if you could use the test llm generator to give you test templates that search through your code for things that need to be covered better and give you a base test and be like hey this one test if you write it covers 1,300 lines of code and you're like ah maybe there's a maybe there's a compelling reason to run this at least once okay maybe maybe there's something that's compelling about doing that but even then maybe it's just completely useless because it's just a happy ca a happy case all the way through and it will pretty much never hit any bugs right and it's just kind of like wow that was kind of worthless maybe one test to test a like a sad case or whatever you call it is a little bit different anyways it's just i'm not i i just don't buy these no many many of these are my own personal opinions that i've taken away from the paper incremental integrated improvement for specialized use cases small context window windows and scattered dependencies make llms and near unusable for non- boilerplate solutions in large code bases aside from any privacy concerns it's not feasible to paste in multiple files of code into an llm when there could be 20 plus dependencies from across the codebase in a c++ header file as an example even if you do paste in multiple files there is a time and cognitive cost to actually using and trying to code out or trying the code outputed by an llm in a chat window or even in the code editor by github co-pilot in a code editor they're trying to say vs code without saying vss code i do agree with this yeah i mean using these things aren't free to use the price of extra cognitive load cannot be understated hacker news comments find the inaccuracies of gpt based tooling exhausting and unreliable i agree i have found that it's it's mostly i've been using i i wanted to test something so i've personally tested llms recently which is i wanted to see can it simply be a coverage for one of my knowledge gaps and the coverage for my knowledge gap is i don't really know python i am not very good at python i've never really tried to be good at python i really just try to avoid writing python and i want to be able to write graphs for any of the data that i'm finding right i just want to avoid learning python just been it's been a goal of my life to just not learn python i know it's stupid but it is my goal okay it's my goal let me have my goal and during this time i've avoided it for quite a few years so i've been using i've been using chat gpt to write my graphing stuff and what i've effectively found is at this point it probably would have been significantly better for me just to learn pandas and matt plot lib than it would be to use llms at this current moment i do use chad gp4 i use that the best of the best i pay for it and i think that it's better if i would have just learned it and moved on because i find myself constantly arguing with the thing at this point to just give me the effing code to give it to me correctly stop making up stuff stop having to look through things at this point you probably have already learned i haven't that's the problem is i haven't really learned panda and map plot lip because i just blindly copy and paste it in look at the results this is where the verification of outputs being both valid and non-regressive is extremely important this means that for a long time let's see that this let's see this means that for the long-term productivity boost in large codebase improvements will probably come in incremental specialized use cases like test generation and automatic suggestion during code reviews i do think automatic suggestions during code reviews is probably the best we're going to get which is here's a potential bug here is a potential test that you could use here's a potential i do think that that is really good i'd really love to see bug finding with larger context windows being like here's a potential thing you did not consider i actually think that would be super useful these are also low lowrisk ways to save c  cumulative developer time goodness gracious i've having a stroke basically gpt rappers will continue to be useful finding and catching edge cases the real value of l llms here are displayed through the edge cases the paradox of writing a good code is that nobody ever gets credit for fixing the problems that never happen correct that's actually really great that is a ve i i've never heard this quote i love it will wilson the fundamental problem of software testing is that software has to handle many situations that the developer has never thought of or will never anticipate this limits the value of testing because if you had the foresight to write a test for the particular case then you probably had the foresight to make the code handle that case too this makes conventional testing great for catching regressions but really terrible for catching all the unknown unknowns that life the universe and your endless creative users will throw at you that is a quote of a lifetime right there it is a quote of a lifetime honestly that is beautiful that is so dang good will wilson who are you will will wilson we're tweeting that one will wilson right what a name his parents really just had a good time naming him hey yo what what's our last name again willson let's name our name let's name our son will so he's will son of will you know i'm talking about a little will son of will what a jerk what a jerk move you know what i mean billy billy bilson yeah i know dude just like what are you doing that the most test cases created by metatest llm only covered in extra 2.5 lines however one test case covered 1326 the value of that one test case is exponentially more valuable than the previous i actually disagree with that i think you can 0% say that can we agree can can we agree to that that this is you cannot say that phrase because those 2.5 lines perhaps may be way more valuable than the 1300 lines the other way the 1300 lines may be all happy case they may be all the things that will probably never break cuz they're just like super dumb things right not all lines are created uh not all not all lines are created equal i was wondering if will wilson was a fake person created by an llm i was wondering about that i was wondering i didn't want to say i didn't want to i don't want to discredit will son of will the value let's see i you cannot say like literally you cannot say that one is more valuable than the other llms can vi i mean this is this only makes sense to people who've tested long enough people who have not tested long enough sl bad engineering managers will think more lines covered equal more value there's literally no guarantee llms can vigorously think outside the box i literally think llms can only think inside the box and the value of catching unexpected ed cage is is very high here i think i think they they are defined by the training data they receive in fact it's so high that the creator of foundations db startup anti-thesis is is entirely based on the fact that software testing edge cases are best found by ai really okay interesting orchestration pipelines and processing are required base model llms aren't plug andplay and shouldn't reliably ever be expected to sure they might output pristine react into tailwind cs code but that's a narrow use case in the most see in most production code bases yes great great great v0 if you've never used v0 vz is amazing by the way vz v.d this is not a sponsored floy i am not sponsored by oh flashbang i am not sponsored by versel but give me a give me a website that reminds you of a toilet let's see what happens there that's just the weirdest prompt i could come up with they need a fair amount of processing and filtering for code generation tasks that require correctness part of this processing means that grounding llms with example google and meta both make suggestions based on existing code where the results are much much better than raw generation llms in production should take ideas from how meta processes and filters llm outputs and most outputs should be expected to be discarded i think this is one of the reasons why co-pilot is i i tend to like co-pilot 10 times more than i like chad gpt because chat gpt is like it's like a conversation to generate something new whereas or frequently that's usually how i use it i'm sure you could past in code and try to figure out what's wrong tends not to be very useful for me typically whenever i run into that problem debugging and and and basic software non-skill issues usually fixes that problem but co-pilot is like is at least like it's it's generating code that looks like code i would have written in the style of the average github code right it like mixes my style in that and so it's pretty good all right what do we got here toilet talk the official blog of the bathroom how to keep your bathroom clean the history of the toilet the science of flushing look at all this welcome to the [laughter] throne what a name i mean they totally did not i mean this was way too crazy of one to remember but this was this is so your one-stop destination for all things toilet because when nature calls we answer damn that's actually that's actually a killer slogan that is a killer slogan all right i like it i like it integration wins llms do the best integrated into workflows this is the reason why github co-pilot is so popular and another reason why google's workspace integrations are a great idea asking a chat bo asking a chatbot works great for certain use cases like debugging and boiler plate again i i'm i mean i have seen it solve one or two bugs really well but for the most part i i don't find it as often that great and boiler plate generation i feel like co-pilot again boiler plate just tends to be better but but chat bots often fail at more complex use cases how to test llm works okay so we have all this stuff builds passes improves processes blah blah blah blah and it goes right into the garbage can okay any of them know then it goes into the garbage can this is still too simplistic test llm applies a series of semantic filters to candidate solutions generated by meta's internal llms making sure that only the most valuable tests are preserved here's why one filtering buildability initial test che let's see llm checks if the generated code can be built within the app's existing infrastructure any code that fails to build is immediately discarded i still can't believe 25% of it can't even make it past this step like that is so wild you know what i mean like that is so wild filter 2 execution does this test pass next the system runs the test that past the buildability filter is that really even a filter is that like that that's what we're calling it these days any test that doesn't pass is discarded this step is crucial because without a way to automatically determine the validity of a failing test whether it's due to a bug or incorrect assertion the test gen llm opts to only keep those tests that can be used for regression testing aka they make sure that they can protect against they can protect current code against future aggressions okay you know what's funny they they're also going to codify any bugs you have real talk you now have guaranteed success in failure we reliably are unreliable thumbs up thanks man i appreciate that this makes me this makes me feel happy dude i do like that i do like that phrase the throne welcome to the throne i keep thinking about it filter three coverage improvement finally to ensure that new test actually add value so this is another hilarious one oh my goodness to ensure that new test actually add value the test llm evaluates them for their contribution to test coverage tests that do not enhance coverage by exploring new code paths or conditions are discarded holy cow watch this disprove this one right away here you go are you ready for this one are you guys ready for this one are you guys ready for this one are you ready for this one watch this test sum here we go i say go like this sum one two expect su12 2 equal 3 there we go and then the llm comes along and goes like this llm lm generated and they go right and they and they provide nothing to equal right try maybe some sort of try catch right it does some sort of tr catch well guess what guess which test is not considered this one right here you know why cuz this it doesn't increase coverage it doesn't increase coverage and so therefore it throws away a test maybe that you should have planned for i'm not saying this is a good example right i'm giving you a very shitty example okay but let's pretend there's an api that when you provided a a certain answer will blow up but due to the fact that your happy path already covers the blowing up case this thing will discredit or discard an actual useful test finding a bug anyways i think it's just hilarious like to me like these are these are all part of the the problems of this this entire approach because there's not there's not a simple there's not a simple answer to this at meta they count number of commits and lines of code don't hold your hopes really high says the meta employee he does know this processing filters are pretty important as they guarantee improvements to a test suite it also shows that llms are very far from being plugg andplay the tests that successfully pass through all these filters are guaranteed to enhance the testing the existing test suite offering okay i got an idea here's my idea what if you could have something you use something like js do that says this function should not throw or should not air and then you have a fuzzing tester that goes through and attempts to make these functions throw and when it finds functions that throw or any function not marked with throw will get hammered to see if they can make it throw to me that'd be a more valuable test like can you break my code how can you break my code it's a little bit hard in javascript because javascript is inherently easy to break if you use typescript because you already have the bounds that do like the type checking the runtime type checking then everything internally assumes these types to be correct so therefore you can't really fuzz out the types whereas with like a strict programming language it can't do anything but fuzz the values of the type and so it would cause kind of an oddity it would cause you to program javascript as if you had no types which may or may not be the better way to go you know i'm just saying it i'm just throwing it out there by the way ryan winchester did say this i want to use that web page let's make a smart toilet startup with anal print recognition imagine the day you're running from the law you think you're safe and nature calls and you bend over and you're tagged due to the brown eye you get get tagged by the camera system because of your ass print damn you're verified but are you anally verified do you think the process would stop being able to identify you if you bleached your anus we got questions we got lots of questions here thank you for ruing everything okay the analyzer seriously this paper is good formalization of a use case that many devs probably are using llms like j jippy gemini and minstral al llama for keeping it right let's see keeping it writing is a good way of tracking the progress of future improvements on llms in software reliability space unit tests are probably the lowest most basic level of code generation where llms have the most immediate value see it's funny like i actually i i fundamentally agree with the premise that i think llms offer a lot for testing value but i fundamentally disagree that it's unit testing i actually think integration in random walk stochastic walking just provides such a better place for this like i'm just completely convinced that is such a better place but as time goes on we'll definitely see llms being able to catch up and test for bugs in an increasingly complex software systems the question is what will make software easier to develop in the long run or will it lead to proliferation of software complexity in the future i think it's going to be the latter more than the former but it is a good question to ask maybe it will make it easier maybe it does truly make things easier i'm i'm just not buying it but it's an it's a very cool thought process and it's very interesting that they're trying this out i would love to see meta in 6 months give an update to whether they like it or not and i' again i would love to see anyone that works on this if anybody works on this i would love to hear a 15minute talk because i want it i would i would love that where where is where is this dang thing did anyone because if anyone responded to this i will bring them on immediately nice look at that look at that look at that if anyone knows this i want to find i want to find someone that actually works on it test driven development that's pretty funny no okay anyways i hope somebody responds at some point with it that would be fantas fantastic anyways hey the name is i don't know what do you guys think real real question does anyone actually think this is a good idea anyone here think it's a good idea assured the toilet agend the poop agend maybe later okay fair they will abuse the metrics absolutely i mean it's the one problem analyzing yes worth exploring i agree i actually do i do agree it's worth exploring partially job stolen guys the name anally verifi pen