welcome everybody to my talk my name is daniel whop i work for bosch as most of the people here are german you probably know it but i'll explain anyway so bosch is one of the biggest car suppliers in the world we produce ebikes engines home appliances power tools the gyros in most of your phones are also made by bosch and as i said i work in self-driving vehicles so we are trying to make cars drive and also perceive the environment i'm not going to try to read the full title here today it's fairly long i know recently most people have an attention span of about four words myself included meaning today we'll be talking about compile time spar matrices fairly easy title so i gave a talk two years ago at this very conference when it still was completely online and back then the topic of the talk was physical units in vectors and matrices so this was about how can we represent especially non-uniform physical units and vectors in matrices for the uniform case it's fairly easy for the nonuniform case it can get fairly complex back then the mission statement or the goal was to annotate linear algebra types with semantic information at compile time and all of this was done to prevent all incorrect usages at compile time by fing static assertions if the user does incorrect operations one example of such an incorrect operation would be if you model mathematical points and displacement vector so the connection between two points then you only can add a point and a displacement vector but you cannot add two points in a mathematical sense that doesn't make sense and back then the mission statement of the talk was if it compiles it works meaning we try to prevent all misuses at compile time but as you probably know it's not 2021 anymore so we today in 2023 so today's topic is compile time sparity for matrices and here we again follow an overarching mission so the mission is to annotate types this time with sparity information in order to save memory and runtime if possible so saving memory for spar matrices should be fairly straightforward if we have zero or one entries that are trivy zer or one we don't have to store them the same thing holds for symmetric entries we also don't only have to store like one of the two symmetric entries and the other one is simply a remapping of that and all of that is done to make sparse linear algebra code efficient by design so as we have a new topic we also need a new mission statement today and it's no longer if it compiles it works but if it compiles it's efficient so let's see how far we get today and if we can achieve this so before i start with the actual content i want to give a brief overview of my talk i'll start with a recap because most of you probably haven't seen my previous talk i'll explain everything that you need to know for today's talk at the beginning so no worries if you haven't seen the other talk that's no problem at all so what i implemented is called types saave matrix the class or the library and this moves semantic information to compile time as i said and among these informations that are moved to compile time are physical units so which unit does an entry have there's also coordinate frames that i try to annotate so for example if you have a robotic a robot with arms and different joints then usually each of these joints has its individual coordinate frame and you don't want to be ending up like adding a position vector from one joint to position vector from another joint because that doesn't make sense really and the last annotation is also that we annotate the the type or kind of vector and matrix so there are coverance matrices jacobians points and displacements vector as i mentioned and all of this is later on used to achieve the goal of moving the error handling to compile time and preventing all of the operations that don't make mathematical sense then the main part of the talk of course will deal with compile time pass matrices in c++ and there we'll move the sparcity information from runtime to compile time furthermore it's not sufficient to just move that sparity information to compile time we also have to change the way how we iterate over the vectors and matrices meaning we have to move the iteration from runtime to compile time so for today we forget about the famous no raw loops statement by sean parent we'll ignore that and move to something even better which is no runtime loops and only if we do that like switch to compile time loops then we can reap the full benefits of of this approach and of course as i'm claiming this this is efficient i should also report about experiments that will be the last part and there i'm trying to answer the question whether we can get a free lunch by moving this information from runtime to compile time free lunch in that case would be to be both more memory efficient and more runtime efficient so we'll see if and how far we can achieve that okay so this now starts the recap so in order to annotate all the information at compile time we need a way how we can sort of tag each of the entries in a vector and matrix with its physical meaning and for that i'm using what i'm calling named index structs and these identify each entry in a vector matrix with a unique name this simple example here dx stands for distance x so this would be the x position we derive from a base class that specifies in which coordinate frame we are here i picked the vehicle rear axle coordinate frame we specify that this is the cartisian x ais in that frame and we also specify the physical unit of that entry so i should mention that vehicle rear axle coordinate frame in our application for self-driving vehicle this is usually the the rear frame of the ego vehicle and this is the frame in which you represent all other objects at least usually of course you could pick a different one but this is the one that makes the most sense of course we're not restricted to one frame we can also represent entries in different frames so this here would be dy sensor this is the y position in sensor frame and in general you can just create your own indices and arbitrary frame names that's of course possible in general i use the names consisting of three parts so the first part is the physical quantity identifier that says whether this is a distance velocity or acceleration we have an exis identifier xy z usually and the last part of the name is a coordinate frame identifier that specifies in which frame such an index lives now i should also mention why dx doesn't have such a frame suffix that's simply for brevity first of all for the slides but also in reality we do it like that so because the vehicle rear axle coordinate frame is our standard frame we leave out the suffixes here so by convention if there's no frame suffix it's in that standard frame good then we also need our typ safe matrix class that's fairly simple at least on that slide of course there are many methods but none of them are important for today so the most important thing here are the for template arguments first one is the scalar type so are we using floats doubles integral types for our representation the second one and third one are row index list and column index list and these are type lists consisting of these indices i described earlier and that with that you describe what the rows and the columns in your matrix actually mean and the last one is what i also hinted at earlier the matrix tag this is for example vector teag delta vector so delta vector would be a displacement vector we have jacobian cence matrices and also something like vector collections where your matrix contains vectors individual vectors in its columns or rows meaning the matrix is just a collection of individual vectors and the only member in that class is a matrix from an underlying linear algebra library i picked ig here i also have different other backends for in-house library and for blaze in this example here it's ig and the ig matrix simply uses the same scala type as we specified and you use the size of the row index list and column index list as the sizes of or dimensions of your matrix so this is what a construction of such a matrix would look like this here creates a cence matrix where the rows and columns describe x position and x velocity in our vehicle frame fairly easy and in the constructor of course you would list the individual elements in the correct order all of this is also being checked but that's more of topic of my previous talks if you want to know details check them out but it's not that important for today now what we're going to need today is a way to access individual elements in a vector or matrix so there are two ways the first one is a bit more type safe than the second one this one here calls coefficient s i you have to list the row and column index and this gives you a physical quantity or the physical quantity with the value of the respective entry in your vector or matrix this also has a nice side effect as we have to specify the indices as compile time as template arguments we can guarantee that there's no outof bound b access here so it's just impossible you would get a static assertion if you ever try to access this thing out of bounds and the second bit less type safe way is to use the at method that simply returns the plane scaler so the underlying float double integral type whatever is stored in the matrix by reference so we can read from that and we can also write to that so for today that last method is actually the only thing that we're going to need from the methods in the library because all the rest is built on top of that i also want to show the two or three important classes in the library so as i said most important one is type safe matrix and that inherits in a crtp metan pattern from matrix base with itself as a template argument so matrix base is sort of the the base class that provides the common operations for proper matrices and for matrix expressions and and that such a matrix expression here has two template arguments the first one is a promotion type and that promotion type is responsible for inferring the template arguments of your resulting matrix so it infers the new rows the new columns and also the new matrix tag in case this might has to be changed furthermore it also does all the error checking so what i mentioned before when there's error checking this is usually done by the promotion type because the promotion knows which which operation we're exec computing and also knows all of the preconditions and the second template argument for matrix expression is a matrix expression from the underlying linear algebra library so in our case ig and this is responsible for doing the runtime calculations so the promotion does the type promotion at compile time and all the runtime operations are sort of factored out to the underlying linear algebra library so this is what it looked like in my previous talk but for today we also want to incorporate sparity in our mectos and matrices and for that we have to modify it slightly so most important thing here is at the bottom we have to add an extra template argument to our matrix which is a functor and this functor is responsible for specifying the sparcity so it says which entries are zero in a matrix which entries are one or which entries are remapped because for example it's a symmetric matrix we will later on see full details how that look lies looks like for now let's just keep in mind that funter specifies our sparity and of course we then also have to modify the linear algebra expression template argument of matrix expression because ig doesn't offer support for compile time sp matrices so we have to implement the operations ourself for that i should also mention if something's completely unclear i'm also happy to answer brief questions right in between so somebody doesn't understand anything or has a like question that's important for understanding feel free to ask that later on i also have a a small quiz for you so i would be happy to have audience party participation later on good this is the last slide from from my previous talk this sort of visualizes how the checks in such in the matrix library are done so we have our our c++ statement up here where we have a coverance in vehicle frame on the right hand side and we try to do a change of basis oper ation by multiplying it with a jacobian from the left hand side and a jacobian transposed on the right hand side and what that looks like is visualized at the bottom of the slide so we have our cence matrix in the center left is the jacobian and on the right hand side the transposed jacobian we can see the row and column indices and the red numbers are what i call row and column exponents so these determine how you get to the physical unit of an individual g so for a coant matrix for example with both exponents being equal to one we simply multip multiply row unit and column unit to get the resulting unit for toobian on the other hand we have to divide row unit by column unit that's why the exponent is minus one there so we take row unit raise it to the row exponent and multiply it with column unit raised to the power of the column exponent now when the library sees such an operation the first thing that happ here is on the right hand side in the orange boxes we the library verifies that the index trs are actually identical in the orange box meaning that these matrices actually describe the same physical thing furthermore there's also a check that the exponents row and column exponents here in these two orange boxes are the s inverse of each other and if that's the case then these two physical units cancel out so the indices sort of disappear and we get the column indices from the right hand side that move over to the matrix in the middle so meaning we've already done the change of basis operation for the columns in our cence matrix and now the same thing happens on the left hand side there's again verification step that checks that the indices are identical the row and column exponents are the sign inverse of each other and if that's the case these indices disappear and the row indices from the left hand side sort of are transferred over to the coant in the middle and now as you can see we have a cence matrix in the sensor frame and that's also what we assign it up there in our black box so this means this is a valid operation that will not emit any compiler errors if anything would be different here so if the indices would be mixed up different order from different frames then you would immediately get a compiler error from the library that tells you okay this operation is most likely not meaningful what you're doing here good so much for the recap now let's move on to the new content spar matrixes so for spar matrices it's probably important to distinguish between two different kinds of matrices what most people probably think of when they hear sparse matrices is runtime sparse matrices where the sparse information is only known at compile time sorry at runtime that's what you usually have and furthermore these classical approaches mostly cover entries which are zero so entries which are one or if they are symmetric to another entry that's also usually not covered with a runtime spar matrices ig for example supports that yeah i will later on also have comparisons with ig but that runtime sparse matrix approach is usually fairly slow so that makes sense if you have really huge matrices hundreds thousands of lines and columns but for fairly small ones it's just an overhead and doesn't give much benefit what we're going to focus today on instead is compile time spar matrices and for these the spar information is known at compile time this could be the case due to physical constraints for example or modeling choices so if we have a cence matrix we might decide that we don't want to model correlations between positions and velocities if this is the case you also get fairly sparse matrices and i will later on have a few more example where this really occurs in practice and as i mentioned the goal here is also to not only represent entries that are zero but we're also trying to represent entries which are one statically one at compile time and to have the opportunity to model symmetric entries in vectors and matrices so now i want to give a few examples of sparse matrices the first type of matrix is what i would more consider matrix shapes so for example the easiest one is a diagonal matrix where all the entries are zero except the entries on the diagonal i also want to explain briefly explain the symbols here so we have black symbols is always like an entry that occupies storage at runtime and anything that's gray here is an entry that doesn't need storage because it's known at compile time so here the zeros are known at compile time there's no need to actually store them furthermore we have upper triangular matrices upper triangular matrices where the diagonal is one and only the upper part of the matrix is nonzero and needs to be stored at runtime then there are also classical symmetric matrices so usually all cence matrices that you have in a in a kman filter if you know about that they are usually guaranteed to be symmetric so this is something where i can immediately use it i should also mention if i have letters in here that's usually also means black letters means an entry that occupies storage i'm just using the letters to u symbolize symmetry in that example so here for the symmetric matrix that you can see that these entries are actually symmetrics to the ones on the other half of the diagonal good now i move on to even more spar matrices so what i would consider proper space spar matrices no longer matrix shapes so in this example here that's a diagonal block matrix where we have two 2x two blocks that are actually identical to each other so the we don't need to store the second one because we know at compile time that this is identical to the upper block and the rest of the matrix is zero a nice example where this could happen is when you have a change of basis operation for example for a for dimensional state vector that contains 2d positions and velocities in a given frame then you have to apply the rotation matrix twice once to the positions and once to the acc sorry velocities and this leads to matrix like this and we only need to represent the rotation matrix once but we can still use matrix calculus for the whole thing which is pretty nice of course you could unroll that by hand that's always possible but yeah that that doesn't scale if you have large code base so it's really nice if you have can still use matrix calculus and just multiply two matrices and end up with a correct result then there are even more general spar matrices like this one here so here we we have 2x two blocks that are symmetric and these are different blocks there is also something that could happen an example for that is a coant matrix if you want to again filter the state of of a vehicle or something and you could may sometimes you decide i don't want to model the correlations between positions and velocities i only want to model the cross correlation between the positions and the velocities individually which would give you such a sparse matrix where again half of the matrix is zero and if few entries are also symmetric and remapped to another entry now as i meent already mentioned that before a real world use case for this is the state transition equation for example in a calman filter where the x position is governed by this equation here dx prime equals dx plus the delta time times the velocity so we're adding to position a displacement that is given by the multiplication of the delta time between two observ a and the velocity and if you translate that to a jacobian matrix that transforms between old state and new state then this matrix looks like that here where almost the full matrix is trivial so we only need two non-trivial entries actually the slide is not 100% correct one entry would be sufficient because we have delta t twice in there so we could again use a remapping and just store delta t is the only sort of runtime entry in that matrix and the other one could be remapped to that one symmetric that's also possible i didn't do that for this slide here another example coming from a k filter is a measurement matrix so in that example the columns of that large matrix are the state that we want to estimate that would be 3d position 2d velocities 2d acceleration and an orientation of the object and the rows are the measurements by some virtual sensor that we're assuming and the sensor can measure xy position and xy velocity and if you have such a matrix then that's again almost completely trivial we only need to store two entries at runtime and all the rest is sort of zeros ones and don't need to be stored good brief intermission so this is sort of the the operation that we'll spend on for the rest of of the talk so one of the most complex operations for linear algebra libraries is usually matrix multiplication so if we can make that as efficient as possible the rest often just falls off or also is just not that significant so what we're going to try to do today is to make this matrix operation here the multiplication as efficient as possible so on the left hand side let's assume we have a sparse matrix where only 50% of the entries are stored the matrix b in the middle is full matrix and we want to calculate the result c so if we look at the first entry c1 of the right hand side matrix then this is given by the first row of a first column of b we put the ines next to each other and then we just multiply it out so c1 = a1 * b1 plus a2 * b5 and then it gets interesting here we have a 0 * b9 ideally that operation should just disappear because we're multiplying zero with something we don't care it's guaranteed to be zero so that should ideally completely disappear and the last operation 1 * b30 should just equal b13 that means if we are able to operate like that that we've come from using four multiplications and three additions to now two multiplications and two additions which is 50% almost good now let's move on i hope you've all been waiting for this the first slide with actually code so let's translate this to c++ what does it look like so we need a lot of template stuff i'll always try to explain it so this is basically matrix multiplication helper the most important thing is that we get a varic type list of pairs and these pairs are sort of all of the the cross product of rows and columns in our matrix so this sort of describes the iteration over the right hand side over the right hand side resulting matrix this is what's given by the pairs and the iteration is done as i said with a compile time loop fold expressions with c++ 17 and here we're just assigning one entry of the matrix and what we're assigning is the result of our calculate entry run method helper that we have done down here and this is again another fold expression and this time we're folding sort of over the columns of the matrix a and rows of matrix b so this is sort of the the multiplication that i showed on the previous slide so we're just doing a fold expression and adding all the multiplication of the individual entries together so the last thing that's now still missing is the multiply two entries and this well okay so if we do that i'll show it on the next slide multiply two entries if we do that then we've already achieved our goals to remove all the runtime loops so we're now runtime loop less sort of because we've moved everything all the iterations to compile time good as i said we're still lacking the multiply two entries run so this is simply a method with three or four if con express the details are not so important the most important thing are the if const express here so we're checking whether either the left hand argument or the right hand argument is zero in our multiplication if that's the case yeah we simply return a zero if both arguments are one we're returning a compile time one if the left hand side is one we're returning the right hand side we don't need to do a multiplication then if the right hand side is one we're returning the left hand side and last but not least of course there are also cases where both values are runtime values in that case we have to do a multiplication there's no way around that and we simply return the multiplication of the two individual entities from the left and right hand side matrix good and if we've done that we've already come really close to our mission statement if it compiles it's efficient as you can see yeah the if conort of takes care that we get the simplest compile time value so if we have to do runtime operations we get the runtime result if not we get compile time values good but now i want to move back that's also a really nice connection to the opening keynote by kevin henny we're going to need some of the stuff that he talked about i hope you all paid close attention the key question now is so we have this fold expression down here where we add several entry several multiplication results and some of these multiplication results are zero so now question for you what do you think does the compiler completely optimize additions with zero away can this be optimized away is anybody have an idea just random guess if you don't know that's also fine yes so you're saying yes u also not completely incorrect it depends so if you would tweak with a floating point settings then maybe the comper is able to optimize it away but with standard floating point settings this adding is zero a positive 0 to something is not an identity operation because minus 0 plus positive 0 is minus 0 sorry is positive 0 so it's not an identity operation meaning the compiler is not allowed with at least center floating point settings to optimize that away and as we usually don't want to play with floating point settings we have to find a different way how we can solve that fairly easy so earlier when k hry talked about that i looked it up and i think that there might be another solution than what i present so if you just return a negative zero i think that might also work for the zero entries because the addition with a negative zero seems to be an identity operation but we can do something even better we just while we are compile time we know which entries are zero we'll just move remove all of the zero entries from our cross product in our typelist which also makes maybe even easier for the compiler and the fold expression down here so we just remove that and then we don't have to rely on any compiler optimization we can simply guarantee that these terms are taken out of the addition and the compiler doesn't even see them which is always better than having to rely on compiler optimizations meaning if we've done that we've truly achieved our mission statement if it compiles it's efficient good the last thing that's still missing that's not directly related to the multiplication is is a simple element exess so if we access one element in our matrix with the at method here then that's again fairly long but yeah basically it contains three or for if conexus if u the functor indicates that this entry is zero we return a zero if it indicates that it's one we return a one and in the case that there's a remapping we simply ask our functor what is the remapping remapping to sort of and return that value and the last else is then simply the standard case if we have a full matrix without symmetry then you simply return the one element in your underlying area or whatever you use as storage that belongs to this entry in your vector matrix good now the one ingredient that's still missing is the sparity functor i talked earlier about that and now i want to explain what sparity functors look like so this here is a fun for symmetric matrix first thing that we're going to need is a method get entry kind and this will sort of return whether an entry is zero one or a normal entry in that case for symmetric matrices it just always returns normal and there's another way how we can identify the remapped entries and this is done with the method is entry remapped that just returns true or false whether an entry is remapped to another entry and if it's remapped there's also that remapped index pair template alias that specifies to which index it's actually being remapped and in the case of a symmetric matrix it's fairly easy we just swap row and column indices and that gives you the remat remapping target in that functor then additionally we also need the inver of the funter we will later see why this is important for now just note that we have that and the last helper template here or it's actually a bull constant that indicates whether the content in two indices is identical this will later be needed for error checking or crosschecking so for example you could have a a symmetric matrix where you store the upper part and then another symmet matrix that decides to store the lower part but it has the same symmetry and with that helper here you can verify that that it's actually the identical symmetry just in the opposite order which is also yeah equivalent i would say good now comes the fun part we have a simple c++ equation here i should mention that the square bracket because that question came up earlier the square bracket here is not true c++ syn syntax it's a bit slideware so that stands for a type that represents a physical quantity in second so we're taking a matrix multiplying it with a quantity of 1 second and then later on we're taking the transpose and now for this example let's assume the matrix looks like this here so the matrix stores as row indices velocity indices xy z velocity in our frame with a unit of meters per second and the columns in our matrix are just numbered indices so effectively we have two vectors stored in a matrix and i have to identify them somehow so i'm using a numbered index trct here and to make it more interesting our example we also have a remapping here so all of the second and third rows entries are remapped to the first one so they don't occupy storage so we basically have a vector with all identical entries in practice that doesn't make much sense but it makes for a nice example which is why i did it like that so now if we look at our matrix expression here first thing that we're doing is multiplying with a physical quantity of 1 second and if we do that then we the multiplication with 1.0 doesn't change the value so the only thing that changes here is the meaning of the indices in our matrix so if you multiply a velocity with a duration we get a displacement vector in the same space and this is what happens here and the last outermost operation is the transpose of our matrix and this simply swaps rows and columns as i mentioned earlier so this is sort of what happens when you write such a matrix expression and how the the semantic meaning of the matrix is is transformed with that now we can do a similar thing what i showed before on the slide before it was just a matrix expression that doesn't occupy storage so such a matrix expression still refers back to the original storage in our underlying matrix but what if we want to like create a new matrix by calling that eval method here that simply evaluates everything into a new matrix now if we have a new matrix we would have to provide a sparity functor for that all for that also but yeah it doesn't make sense to to let the user provide the sparity information for something that's as complex as this one here so this has to be done automatically and on that slide i want to show how this information can be inferred automatically so let's just assume we're looking at our red a entry here and we want to know what that entry is being remapped to in the new matrix that is being created here so first of all we have to now pass the the expression from the outside to the inside and invert all the operations so we're taking the transpose it's fairly easy the inverse of transposing matrix is another transpose meaning we just invert rows and colums and then we used to have a multiplication with 1 second this gets inverted to a division by 1 second meaning we now change the indices from position to velocity indices back and now we're already in our original matrix that occupied storage and now we can query the sparity funter in that underlying matrix hey what is your red entry a here being remapped to and that can tell you okay it's being remapped to the first row because the second row doesn't occupy storage and then we have to go the other way up again we have to go in the forward manner we again apply the multiplication with 1 second we doing the transpose and this sort of brings us back to our to our result space and now we have the final result the red a has turned into the green a up here and this is sort of the remapping functor for the resulting new matrix that we create with this simple eval call here at the end yeah so a bit of things happening under the hood but in order to pro propagate that sparity over all operations this has to be done otherwise it wouldn't work good now i also want to explain as i said that earlier how we can invert a sparity functor so the code that we had on the previous slide translates into the type system as this complex type here so the innermost part a storage factor that's given by our underlying matrix in the middle of the expression then we multiply with a physical unit that wraps that in another template and last but not least we transpose it so that wraps it in another template and now we want to use that inverse functor here to sort of invert this operation so i will show visually how this is done we take our large expression here and then we split that in an outermost blue part and an inner orange part and what that inverse functor here is doing it simply exchanged this exchanges these two parts so the blue outermost part is transferred to the right hand side of the colon colon and the orange part stays on the left hand side and now we repeat that operation so we again split that into an outer part and an inner part we move the outer blue part to the right hand side and now we only have our storage funter on the left hand side of a proper matrix and that fun usually is an identity operation meaning this is sort of our our final result here that we have and now if you paid close attention i mentioned earlier that the inverse of multiplication is of course a division so it's slightly different it's a divide by unit expression that we have here and this is sort of the the full inversion of our expression from up here that we have by simply yeah unchaining this thing and changing the order this can be done at compile time good now as i said it's quiz time i have a few questions for you so it would be great if you could try to answer first of all let me explain what it's all about so we have our simple m very simple matrix assignment up here we're assigning a matrix b to matrix a and the question is so we have five different options here on the right hand side i will step through each of them and we'll try to answer first of all the question whether this assignment is correct or not now i should also mention mention what what the question marks mean question marks are just entries that we ignore for for the quiz so just they can they can be anything so they don't have any constraints for for our quiz so let's start with the first one so on the left hand side i should explain that also so we have a matrix which has runtime entries on the diagonal marked with an asterix and the off diagonal entries are symmetric and we only have storage for the upper right entry and the other one is simply being remapped and now we're trying to assign this matrix here with two runtime values to our matrix on the left hand side is this invalid operation so see can you say why it's invalid because a is fixed and should be symmetric to the other a yeah and on the right side we have two we don't know yeah so i'll just repeat so on the left hand side we have two entries that are fixed to be identical on the right hand side we could have i don't know three and five for these countries and then this wouldn't work so this could really be a problem this is incorrect and we should definitely prevent this operation at all cost so what about the second one should this work yes yeah that's fairly easy same symmetry that should work third one also yeah so people also answer yes and it has symmetry a different symmetry but it's sort of semantically equivalent we just chose to to put the storage sort of virtually somewhere else that's the same matrix and the left hand side can represent that we simply assign b to a the small b to the small a and then we're done what about this one is this a correct operation if we assign that so everybody seems to agree that yes so it might be a partially okay i'm sneak peeking into the later answer so it's correct the operation we can definitely assign the zeros to a then we would have a runtime value of zero but it's still correct the operation we still representing the same thing afterwards and what about this one here it should also work so we have compile time zer at one on the diagonal we would store them at runtime values as runtime values that is okay and also a correct operation so if we proceed like this we pr prent the first operation by a compiler error and sort of let all the other operations succeed then we sort of achieved the the mission statement of my talk from two years ago if it compiles it works so there are no bucks in here if we do it like that it's completely fine but as i said we not only want to be correct we also want to be as efficient as possible so now for the second part of the quiz the question is are the operations efficient so the first one is fairly easy it's in correct can't be efficient if it's incorrect so we can ignore that one what about this one here is this as efficient as possible this operation if we assign the two matrices yeah so people seem to agree yes so we are assigning a symmetric matrix to another symmetric matrix we don't lose spity everything's fine here what about this one okay everybody seems to agree that's also efficient correct we just chose a different representation i mean it might be a different kind of efficiency if row major or column major makes a difference but in general we have the same amount of operation operation so it should be fine what about this one here okay everybody's answering no can somebody answer why maybe you again why is it in inefficient it doesn't know it's zero okay yeah correct it doesn't know it's zero so we used to have a compile time zero and now we're just having a symmetric entry so we're actually using runtime storage and the compiler can't know at runtime that this is zero which means it's not as efficient as we want to and we're losing sparcity information by doing that meaning we should prevent that operation and what about the last one i guess yeah that's also fairly easy we again losing the zero and one on the diagonal so this is also not really efficient if we allow such an assignment meaning we should here if we want to be as efficient as possible we should again prevent all the red operations here marked with red and if we do that we've come really really close to our overarching mission statement if it compiles it's efficient because we're always preserving the sparity information over all operations of course in some cases it also could make sense that you explicitly want to use want to lose the sparcity information but there's yeah you could just provide a special method that says okay please drop all the sparity information evaluate to full meth matrix instead that's perfectly fine to do it like that good so that concludes the part with the explanations now i want to move on to the actual experiments and show what that does with runtime and our famous slogan from the beginning here i want to also investigate whether we can maybe get a free lunch with this approach by being both more memory efficient and more runtime efficient at the same time so first experiment is this operation here where we have three matrices u d u transposed and u is an upper triangular matrix d is just the diagonal matrix in the middle and then we're multiplying it with the same now lower triangular matrix because we're transposing u on the right hand side this is a fairly common operation in matrix de compositions al so this happens quite often which also means there are nice textbook algorithms that you just can take that implement this as efficiently as possible and what they usually do is they represent the middle matrix just with a a vector that only stores the diagonal and also makes use of the triagonal shape of youu and u transposed good and when doing that this is sort of the result i should explain what you see in the plot so we have the the textbook algorithms that you usually find in your linear algebra textbooks and in green that's the compile time spar matrices that i presented in the talk x-axis is the matrix size so it's always a square matrix and we go from 6x 6 to 24x 24 and the y- ais is the runtime of our benchmark you can already see that yeah textbook algorithm is sort of efficient but the compile time spity is better in certain cases especially in the very for the very small values i also have that on the next slide for larger values there's still a benefit but it's yeah maybe 20% or so so not that much for the small matrices it really can make up to a difference of 50% in runtime between the two algorithms so you might ask why this is the case i presume it's most mostly because the classical textbook algorithms still have runtime loops and iteration so the compiler doesn't really see all the offsets of your entries and with my approach all the offsets are known in compile time so the compiler really knows which where exactly it's accessing memory and i think this quite often can improve runtime significantly good just for fun i also included ig runtime spar mees in the comparison but that's fairly u unfair as you can see ig is fairly slow you also need three arrays to represent runtime spar matrices because you have the actual values and the row and column indices you have to access all of that at runtime this is fairly slow especially for these small matrices of course if it's in the order of thousands then definitely i guess the other one is better your compiler would explode if you try to do the the compile time thing with matrices of that size so it always depends what what your problem actually is good second experiment that i want to report about is this operation here so we multiplying a * b * a agent post and our matrix a on the left and right hand side has varying spareness so we vary the spareness in that matrix from 0 to 100% so we're sort of increasing how many entries are zero in that matrix and the middle matrix is always symmetric so this could be from a coverent matrix that's being transformed to some other frame or similar operation now if we do that you see a lot of plots here the most important thing to focus on here is the red one that's sort of the the baseline that uses ig simd operations under the hood so that's full matrices with igen and this is sort of the baseline that we compare all the other plots against and the other plots are just our matrix multiplication with increasing level of sparity so the one with the highest runtime has zero sparity in in that matrix a and we increase the sparity from zero to 90% and you can see especially for for the very small sizes where i again have another slide for that ig is the slowest but as soon as the matrix get bigger and bigger ig starts to pick up with the syd operations and usually yeah if you have more than 50 60% sparity the compil time sparity is still better but i guess if you would increase that further then ig would get faster plus the compile times get get prohibitive at some point so this is sort of the zoom in of the previous one so it's just the smaller size of this one here and here we can see yeah for all small sizes the sparse approach is superior to ig even yeah even with zero spareness in some cases here for 8 9 10 so even there it can make sense to use such an approach but of course as always yeah you have to benchmark it depends on your machine on the kind of simd operation so i wouldn't claim that this is always the way to go but it can make a big difference especially if your matrices are really sparse good the last experiment that i want to report about is this one here and here we have nested spar multiplication so in the middle we have a symmetric cence matrix and that's multiplied from the left hand side and the right hand hand side again with the same term here and the term we're multiplying with is an identity matrix from that we substract the gain matrix that's a full matrix and multiply that with a measurement matrix that is fairly sparse as shown here so this is again an operation that you observe in kman filters pretty often and now i have two different measurements with my sparse approach in the first one i'm only using sparse multiplication for the innermost multiplication with a measurement matrix and the gain matrix and in the second measurement that i report on the next slide i'm using spar multiplication for all of the multiplications in here so also for the one with the cence matrix so what that leads to is this bar plot here so we have in in purple sort of a simple loop we just iterated runtime unroll your matrix operations this is sort of the the baseline ig is the green bar with ig sly operations of course and then we have the blue bar which is only sparse multiplication of the the innermost multiplications that i showed and the orange bar the final bar here shows what it looks like if we do all all multiplications in a sparse manner meaning we propagate the spareness through all of the matrix multiplications and you can see that we can still be approximately 50% faster than than what ig does in that case so again it depends on whether you're using float or double precision but it definitely can make a difference good as we still have a bit of time yeah i can briefly show maybe the let me see if i can skip to that one this is sort of the compile time plot because that might be interesting so here we have a number of entries in purple that's quadratic of course and the compile time in green that's more cubic i think so you can see that this starts to break down at higher numbers i thought i'd just include that briefly but let me go back to this one here because that already brings me to the summary and then we hopefully still have time for a few questions if there are any so what we've done today we've combined memory efficiency with runtime efficiency we've partially invalidated the famous saying here there's no such thing as a free lunch so we can get a free lunch in certain situations but of course restrictions apply so please read the fine print you always should benchmark on your machine know your problem domain if it's actually worth it if you really have spar matri so it's always a tradeoff of course the preconditions for achieving all that are that we've moved all the operations from runtime to compile time time meaning we've ditched the famous raw loops no raw loops for today and replaced it with no runtime loops and by doing that we can now enforce at compile time sparity correctness so this was sort of in our quiz the correctness which sort of satisfies pre the previous mission statement if it compiles it works but in addition to that we've also achieved maximum sparity efficiency which brings us to unlocking our final mission statement of this year which is if it compiles it's efficient and with that all that remains for me is to thank you all for listening participating and joining the quiz and i'm really looking forward to your [music] questions yeah so one of the questions was was about the compile time but you showed it so thanks but the other question would be what about the binary size because there are a lot of u templates and with the unrolling and all that stuff have you measured the impact on the binary size yeah that's a really good point no honestly i haven't yet measured it but it definitely makes sense to look at that i would also assume that it affects the binary size because there's a lot of inlining happening and stuff i mean all the if you're lucky all of the the type information can be removed at the end so it's just raw operations but i would also suspect that it has an influence the runtime loop is something different than yeah adding i don't know 50 values u one after the other so i would assume there's an influence but i have to measure that thanks so your fundamental abstraction for linear operations was a matrix but it could also be an expression like i already already has built in what were your reasons to go beyond ig and implement it yourself so especially for the spars for the spars because an i expression can also encode that it's an expression multiplying with di diagonal matrix or yeah so i haven't found a way to to easily use that in with ig expressions it might have been possible but i definitely have to modify the underlying storage so it's no longer an icon matrix that i can store so i have a i just store i don't know three out of 20 entries so you definitely need different storage type and the main difference maybe also why it might not work in ig i have i'm always accessing with template arguments so i'm not saying okay please give me this matrix at index 35 which which are runtime indexes for ig what i need so i absolutely need that is a compile time index so it could be an integral constant or u the types that i'm using but it wouldn't work if you really have runtime values and that's probably where it would break if you would try to implement it like ig did so i guess i don't know maybe it's possible with some really big workarounds but out of the box i i couldn't find anything so i think that's the main crooks if you have runtime indices or template arguments for your indices thank you so so we have two questions from the online audience the first one is if md spen will help the steam matrix operations sorry could you repeat the first one is if md spen will help you with matrix operations okay md span that's a slide i took out from a longer talk i should have left it in i guess so md span in that case would have i think it would have the same issue that the indices are runtime numbers and not not compile time and that's the major difference i try to look at it but i think that would also prevent me from from using md spens all the extents are compile time in md spens but i don't think the the iteration or the the indent okay and the second question can the compiler generate simd code for matrix operations and can native simd operations be faster than compile time spar matrices u yeah the simd operate i mean we saw saw that with a comparison to ig ig is using as much simd operations as it can can get i guess so that should be pretty efficient but there's also another point in the code i'm producing there could be some some autov vectorization happening that's one of the points you could even imagine that you're actually doing like parallelization yourself that you if you know your sparity structure you could even restructure your rows and columns reorder them that you have consecutive blocks that are seemly operation friendly and with a the library interface that doesn't mention okay i'm accessing index 3 and five but only mentions sort of the content like this is an exposition index you can arbitrarily reshuffle the order of your ind indices and your code will just compile again because yeah it's agnostic to the actual position in your vector and matrix thanks so i think you almost maybe completely just answered my question but just to make sure have you encountered scenarios where breaking up the contiguous storage actually prevented simd operations making the entire operation slower yeah i haven't really investigated a lot what if there's actually simly being created or not so it it definitely can break up things so i don't know if you if you would imagine a 4x4 matrix and only three entries in each ve row are occupied and your simd width is i don't know wide enough to store four entries then sim is probably pretty efficient by just using an additional entry that's not needed and with that you would probably immediately prevent it because you sort of putting the next three entries after the first three ones and the alignment would be violated that you probably need for simd operations so it's easily easy to prevent simd operations with that so you really have to make up with a level of sparcity in your mexor and matrices for the fact that you can't use simd operations that's also why i said benchmarks are needed for your application okay thanks hi yeah my question would be do you have you also looked at if there's any performance gains versus hardware acceleration so if you have gpu that can run m matrix operations quicker due to hardware acceleration do these overheads from the from the edit from the structure kind of diminish the returns on on hardware acceleration yeah i haven't tried it on on gpus but i would assume gpus are pretty efficient with matrix operations the the major point with gpus is you have to often transfer the code to the gpu and back this is a big overhead but as soon as it's on gpu then i would guess yeah just dumb dumbly calculating everything is often more efficient you would have to measure but i think there it leans more towards just calculate the whole thing and forget about such an approach yeah i think especially when you're dealing with big matrices and that's also what i said so what i showed here the 24 by 24 that's where it starts i mean the numbers i showed that was i think 10 minutes compilation time that was for a whole benchmark set with several operations so in reality it wouldn't blow up that fast but i think if you're at 3030 it's probably as far as it as you can take it and after that it doesn't make sense anymore okay thanks leing on to the last question while developing or for what hardware have you been developing so is this meant to run on service in a data center or is this actually meant to run on a microcontroller somewhere inside a car in real no more on the microcontroller so we're not yet using this in in production so the the the compile time checking from my talk pre two years ago we're already using that but this one here is sort of yeah it's a take it or leave it so ideally you would have to do that for all the operations that you have everywhere then you get the full benefit if you're always converting back and forth between a normal linear algebra library and this representation then probably doesn't get you that much or you just do it in in your central filter loop that takes a lot of time that would be possible but otherwise yeah usually it's like completely transitioned to this thing then it makes sense or yeah leave it as it is i think it's interesting from a product point of view if i think about the bosch microcontroller sensor unit which are like all included s soc's that something like this would be beneficial because you save the money on extra co-processors yeah on extra co-processor especially you save the memory first of all because if the metes are really spar it saves you memory that's usually also fairly costly if you can reduce that and then if you if you can save the memory and you would just be as efficient as you were before that might also be already worth doing it and often times you can be more efficient thanks yeah so there's a lot of compile time code you' shown do you think it would benefit from c++ 20 concepts yeah the question whether it would benefit from concepts what i did today in the talk not so much but everything i did in my previous talk all the compile time checks it's basically an emulation of c++ concepts with 1114 or 17 so yeah it's sort of i'm even calling the thing requires classes that help me with checking all the preconditions so it's really important and what i also used a lot is compile time testing so i'm actually i have lots of conditions that have to be satisfied otherwise the user gets a static assertion and all of that is being tested so i have 3 4,000 compile file tests and each of these tests sort of starts the compiler for a small snippet runs the compiler and checks whether the output contains a static assertion which is i think it now takes four hours if you actually do that and with concepts you come could completely rewrite that because you can actually write requires not requires which checks that something is invalid and just last week i also found a workaround in older c++ versions where you can do the same thing so you can modify method return types to return some sort of invalid expression in the error case and then you can again check with a single static assert in your test that this invalid expression is emitted and this sort of also allows you to have thousands of test files in one file compile it once and you're done but yes i would love to have concepts for that but we're still at c++ 17 currently thank you so yeah i think that's it thanks everybody again looking forward to talking with [music] you