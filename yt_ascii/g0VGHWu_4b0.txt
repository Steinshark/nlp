hello and welcome to our building and packaging modern c plus plus session my name is piotr gujkovsky i have over 10 years of experience with writing c plus recently i'm mostly focused on creating my own sub my own site projects with c plus other than that i also like to listen to music read a lot and practice parkour and my name is adrianostrovsky i also have almost a decade experience in c plus and aside from professional work i tend to try many different things recently i learned how to sail for instance so without further ado let's switch to the topic at hand which is building and let's let me start with my part this part we will talk about mostly tools that are used for building c code and one of the most popular tools for this is cmake so for those of you who don't know this already cmake is a build system generator so it's not a build system on its own it's a de facto standard way for building c plus plus code and all examples in this talk based on cmake so this is why we want to give you a short recap perhaps a too short recap just for what just of what we need to understand what we're talking about here so let's start with how to install cmake and the most common way is to just grab it from your system package manager but the minus of this approach is that let's say on the latest ubuntu lts it's in a pretty old version as i'm recording it's 3 16 3 so you could be tempted to get a different one let's say from brew so if you're on mac or linux you can use blue install cmake and this will give you a 319.6 but this still isn't the newest version so this is why i would like you to try out a different approach and actually there are two that you could use one of them is pip install cmake this time you can get the latest greatest usually 321 is supported and if if you want to try out something else you can also use conan to install your cmake there's a feature or a generator called virtual run env and this will give you a set of scripts called activate run which you can source and then use the tools that you want to use so now as we have cmake let's quickly show how to use it so the basic idea is that you create targets either executables or libraries in the most common case giving some sources you can link between them or specify dependencies they propagate the include directories and other properties automatically so in this case we want to set up public dependency on our application to from a library called my lib and if you want to link to something that comes out of your project instead of being defined in it you can just find it using a find package for instance it's just one of the ways and then you can link using this the same command target link libraries here we use what's called an alias so we have this my dependency double column by dependency so this way you can be if you use the targets this way you can be sure that you won't you will use cmake targets instead of some random library name that is not even involved in your project so you can also integrate other tools into cmake and they can often provide you with better code eventually so help you drive quality and cmake uses variables to specify paths for those tools so there's one of our cpp check as you can see there are also for the linter for clank tidy and even for include what you use and other tools can be also integrated so one of the ways to integrate using variables is to just specify the compiler launcher and this is used to use a different tool which will then run your compiler and we'll see in a second how this can be used in practice so now that we know how to build code using cmake let's talk about how to get the builds faster when you're using cmake so there are a bunch of low hanging c make fruits that you can pick and one of them is to change your build system and i'm not telling you to drop cmake i'm telling because it's a generator i'm telling you to change the build system that you use under the hood other ways to quicken the build is to build only what's required and also you can use just the tooling that you need to use and let's go over all of those items quickly so first i would like you to introduce i would like to introduce ninja to you ninja is a small build system designed to be used with a build system generator and it tends to be especially faster for incremental builds but if you have a large enough project it should be much faster than make even in the common case so this is what as a developer you should care about if if you're using if you're working on a day-to-day basis with code and incremental bills are of utter importance and ninja is used by many projects by chrome android lvm and if you want if you use cmake there is pretty there are pretty easy ways to start using it so one of the ways is to just specify ninja as the generator recently you can also use the ninja multi config generator so this is cool if you use visual studio because it's a multi-config ide so another way is to just export a variable which tells cmake to pick it up so that's that's basically it for generating and then if you want to build using ninja and cmake then you have two ways of invoking it so either you can just tell or just type ninja and it will automatically determine what number of jobs it should spawn and if you want a universal way to run builds then you can just type c make dot dash dash builds and then a dot or other directory and if you use ninja as the generator it will get picked up so that does it for ninja another way to speed up your builds is to build only what's required so many developers tend to remove their build directories recreate it then rerun cmake then run make j or make dash j all and this is pretty often not what you should do you should just type c make dash dash build and then specify a target which you want to build so this has several advantages if your cmake lists are written properly you don't need to regenerate cmake each time the changes should be picked up by c make on its own so if you use globs it's a common anti-pattern which makes you unable to to use this advantage of cmake and also if you specify target as we do you don't build unnecessary targets which in often cases can can really speed up your builds so without building not needed things maybe we shouldn't also run tools that we don't need and i had a case personally when i was using include what you use it's a great tool to analyze what you should include and forward declare so i definitely recommend you to use it it can give you great build speedups but running it has quite an overhead on its own which makes which is a problem if you want to use it using this for instance on your developer day-to-day life and what i would recommend is just to not specify include what you use using this cmake variable that we saw before unless you want to do it to speed up your builds so you should do it periodically then it's pre then it can give you a speed up but you shouldn't do it from build to build okay so now let's move to a different tool that you should definitely use and if you should pick one tool from all of the tools i that we show here i would recommend you to do big c cash in essence it's a compiler cache it allows you for much faster recompilation for many c language c family languages including c plus objective c plus plus cuda and assembly and it also compresses the results it can provide you with statistics checksums for correctness fallbacks if it doesn't know how to build also it can be pretty easily integrated into your existing cmake projects and also it emerges with support for c plus plus 20 modules which is also a very cool addition but it's still work in progress okay so how much does it really help in my personal experience it can really cut off most of the time from the second build onwards and for incremental builds more often too so don't take my word for it there are also benchmarks using various ways you can run c cash and then can get you even of almost 150 times speed up which is really really impressive in my opinion okay so with having said that let's speak about the environment that cache supports so it works on linux it works on mac os it works on windows but not with msvc there's a pull request which is being actively developed still and it supports gcc clang and nvidia compilers so it's quite a cool set to have and if you want to install it on windows you should just use binaries from github or use a package monitor like scoop for other systems you can either use the system package manager although that's not the recommended way you can also take it from from brew or from nixon pyotr will say a few more words about nick's soon or you could just build it from sources if you want okay so once you have it you just to use it you just need to call c cache and then the com the regular compiler invocation and if you don't want to type c cache you can also do links for mask writing the compilers i'll show that in a second and also you can just integrate it easily with build systems or cmake and to ensure that cache is used by default if you want mask reading you should just create a set of symbolic links named like your compilers and put them in your path and then just call your compiler by name so this way for example user local bin g plus plus would be called it will actually be a link to c cash it will call c cash and then c cash will recognize it was called as g plus plus so it will invoke itself with g plus plus integration with cmake is the easiest and most pleasant way i think that you should you could use it so it's as simple as setting one variable this compiler launcher i mentioned before and if you want to do it in a flexible way you can first find if you have c-cash install on your system and only if you have it then you set the rule launch compile for your seekersh program and that's basically all you need to get started if you want you can also configure some environment variables like the cache size you can also configure the sloppiness and sloppiness is how strict c cash is when checking if the thing it wants to compile was already built so it you you can also specify some debugging logins logging variables like read-only mode or forcibly recaching the compilations and also you can prefix put another command as a prefix for the compiler so we will show how to use it in a few slides also a cool feature is that you can actually share your cache if you put it on a network storage and this way you could share your compiler results with other developers or with ci machines it's really going to give you a lot of speed up and if you have several clusters just remember that it may be faster to provide one cache per cluster okay so if you want to share c cache between users i'd recommend the users to be in the same group and then you have to set some config variables like set a larger cache size because the default is just five gigabytes you set the base directory which is cut off from the path that from the part of the path that gets hashed you specify where on the network the cache the the cache will reside you don't hash the current working directory so that users can regardless of where actually they start the build they will get the same results also it's recommended to have the temporary directory on your local system instead of the network file system and also you need to set a mask so that other users will be able to share the cache with you and having said that there are caveats i mean there is one caveat that i want to point at and it's that cache is unable to cache results from clang based tools so if you want to use clank tidy on a build to build bases or include what you use seekers unfortunately won't cash you the results from those tools okay so that's about c-cash so what else a developer needs and i know one such thing and i think everyone could use some ice cream and ice cream is also instead of being just desert it's a fork of this cc it's a it's an excellent tool to distribute your builds and it has several features so the main one that distinguishes it from this cc is the scheduler and it can it's smart so it only uses free resources on your machine so if you want to play some video games whilst they're still working other users will use just the idling cores of your machine so you don't you won't get any slowdowns from your video games let's say and also even if you have many different vms or operating systems ice cream can handle that it can still provide you with good performance it measures performance of each host and can schedule work adaptively some machines can be off during compilation and it will treat it correctly you can even cross compile remotely using this tool and there are also possibilities to monitor what's going on in the cluster so having said that about the features how much does it help and don't get my word for it you have a developer from mozilla and they really got quite the speed up here so as you can see they basically cut the three four three quarters of their time from the bills by just switching to icc which is pretty nice and another benefit is that the local machines tend to be more responding so let's now talk about how you can see what's going on in the cluster and there's another perfect wizard called sunday and it's a tool that runs in the console and allows you to examine the whole cluster so having said all of that about monitoring how does it actually work so we have a scheduler and it's the central part of icc ice mon or sunday are to build two tools to monitor your cluster so they connect to the scheduler and get information from it also you have daemons which can run on your machines or some build servers or even some different operating system that you have and it all well plays nicely pretty much out of the box so we see linux and mac on this picture so other supported environments could be freebsd or cygwin unfortunately ice cream is still not available on windows but on the other systems it's pretty easy to start using it so if you want to install it you just need to either get it from your operating systems package manager it's the recommended way but it has some caveats so you should be sure you you need to be sure that you run version 131 or later because older versions tend to have some issues they they don't fall back gracefully when you encounter problems when building on a remote host if you have some firewall requirements you can also specify some ports but by default it should just work fine if you want you can also specify a persistent connection to one scheduler but usually the schedulers can choose one leading and the other switch to being just backups and also ice cream can be used with cash together so the easiest way is to just go to the cash config file and add this prefix command which just points to icc and that's the easiest and the most effective way of integrating the two if you don't want to do it if or if you don't want to use seekers but still want to use ice cream you can just put the binary path with masqueraded links to your path variable so it's pretty similar to how c cache got integrated and another different approach would be to go to your cmake lists and similarly to c cache you can just specify our launch compile variable okay so let's move to some caveats so for once there were some bugs in the older versions we already mentioned that another caveat of ice cream is that it only supports gcc and clang so if you're using other compilers it might not work and also some cross compilation cases are tricky so for instance it's not always easy for the tool to recognize when you're doing native builds when you're doing cross compilation builds especially if your compilers tend to have non-standard names and if those caveats block you from ice cream i still recommend trying ice cream if you can because it's free but if if you cannot one of the cool commercial alternatives is incredible it can run on windows and linux and it also supports other compilers that ice cream has issues with and also it's able to distribute your tests and it's it's using c-cash under the hood so pretty much a similar way to to how you can cache it the results okay and another alternative is to run sc cache and this is mozilla's c-cash equivalent and it also has ice cream style capabilities distributed compilation is pretty similar and if you're using rust then sc cache tends to target rust pretty much and also it can run the cuda compilers all of those on windows linux and mac os but it's not production ready yet and the last release was in january i think okay so that does it for the compilation the distributed compilation part and i think now it's time for a switch so pyotr welcome to the stage welcome back hello again so yeah well adrian mentioned a lot about getting your builds quicker and making the most out of your cpu my goal would be to show you how to make yourself more comfortable with what you already have so let's start with a common problem about portable build environments how to make sure everyone's playing the same toys so one most obvious answer to this question would be we should all use the vms because with vms all the software comes pre-installed it is easy to distribute vms fairly easy at least well we know that vms might not be especially pleasant to use because they take some cpu they take some memory so that's that's the downside but other than that they should be pretty cool but are there better ways maybe so one alternative is to use containers containers are a hot topic for at least the last five years they're yeah new shiny everybody loves containers so that's that's definitely for sure they're slicker than vms they don't take as much cpu they don't take memory they basically behave on par with native applications so maybe containers are the best way to ensure portable portable environments probably though application containers so these are the thingies that for example docker uses are not necessarily the best way for managing tool chain skills application containers are designed for one container one application and tool chains usually require launching lots of different tools you have ninja or make which spawns various gcc front-ends which then spawn linkers etc etc etc it's a big mess to be honest so what else is there and this is the part that adrian already mentioned there's this cool thing called nyx and nyx is a bit tricky because it may mean nixos the operating system based on nix it may mean nick's package manager but nyx may also mean the language used by this package manager whenever i'm referring to nyx i'm mostly referring to the package manager right now so that's that so nyx is a package manager that operates in user land it doesn't require root access that's that's perfectly fine if you want to have multiple users installing different stuff no problem it uses it is deterministic in the way that you use it so always people using the same the same mix packages version will get the same packages the same environments everything is behaving exactly the same way so that's that's totally cool you can have atomic upgrades and you can have very easy roll backs with nicks because you can always take a step back to the time before you installed a given package it can also manage environments setting necessary variables you know configuring this stuff so that's also very cool and as i've mentioned before multiple versions of packages not just not just that multiple users can have different packages installed but they can have multiple versions of such packages installed side by side on a single system one person may be running gcc10 another one 11 another one may want to use specific version of now beam no problem with that and nyx works very well on linux and also very well on macos one thing to look out for if you are running the fresh arm-based max some of the stuff may still be a bit shaky but most is already working smoothly so that's totally cool ninx takes functional approach so installing or upgrading a package won't break any other packages and won't break your system that's very important every package is installed in a separate directory which prevents inconsistent state because no package is interacting with any other directly what's more the the entire directory where the packages reside is read only most of the time so nobody can can even change anything inside it's great for multi-user environments that's understood but you can even make it better if you start using it with deer end deer and views is nyx shell so basically it creates a new pre-configured shell with nyx packages and it automatically sets up the development environment whenever you enter a directory so you change your directory to source slash your project and if there's a dot n rc file then direct takes care of installing all the packages setting all the variables maybe maybe also those containing secrets whatever you want it is already there so all the developers in your entire organization have exactly the same stuff installed and there is no well it works on my machine how does it look like then here's an example of a dot anversi file the use underscore mix is the most important part and there's the nix file that in this example says that we want to have a gcc 11 environment that has cmake installed cash installed gcc 11 of course git make and ice cream how does it compare to other package managers so it's still not as easy to use as homebrew if you want to create your own packages or if you want to update something that hasn't been updated upstream it may give you some headaches that's that's definitely true getting a working gcc compiler from git for example is extremely tricky so if it's been already released that's fine if you want to live on the bleeding edge then well you want to live on the bleeding edge it's up to you there's also an alternative called new greeks which uses scheme which in turn is a dialect of lisp if you are also a whisper then maybe this would be a better alternative this also works with diran so it's more or less an x alternative i'm not entirely sure how up to date it is with packages but it's something to to consider anyway so if you have all the stuff already installed you have the c cash you have ice cream you have everything you need you may also want to consider using git hooks hooks are a cool way to make sure that you run your tests that you format your code that you do a lot of other stuff before you commit your changes and then push them to the central repository and there's an app for that to configure and manage the git hooks locally and it's called surprise surprise pre-commit now pre-commit is a python package you can install it on its own using your favorite package manager or using nix of course and it's configurable with a yaml file so you configure the repos that contain the the pre-commit hooks that you want so in this example we want to check that we don't add large files to our repository and that we don't have case conflicts that we don't accidentally commit merge conflicts that we don't have mixed line endings dos and unix that we don't commit to the master branch and that we don't have trailing white space these are like pretty common stuff that you can use with any project basically but there are also pre-commit hooks that are dedicated to c plus plus projects so for example you can run client format whenever you are committing your code you can add some arguments to client format you can exclude some some files or directories from the checks yeah you can do a lot of a lot of cool stuff with that you can run clunk tidy each time you commit any new code which is something that you should probably be doing it's very nice very useful and you can also use other linters oscillin and crustify i didn't previously mentioned include what you see and that running include what you see on each compilation may not be what you want to use so then you can use include what you see only in the pre-commit stuff it won't be run every time you rebuild your project but it will be run when you commit the changes so you don't accidentally commit something that is unnecessary also if you are running cmake you can run cmake formats so that the formatting in your cma files is consistent okay so now something a bit about packaging historically sharing pre-packaged c plus stuff was a bit problematic usually on linux for example you wanted to use system package manager which may not have the exact versions that you want and yeah this can bring you some headaches so otherwise you would probably have to compile everything on your own share it somehow with your colleagues yeah less done less than ideal situation but there is conor and conan was already mentioned by adrian before but i want to dive a little bit deeper in what it does and conan is a package manager for c plus it's written in python and it behaves kind of like pip or npm or jam if you are used to those tools but instead of being you know pip and pm gem are all for interpreted languages so it's rather easy to make it work cross-platform with c plus plus and with different tool chains different compilers different cross compilers the situation is a bit more difficult but fortunately conan handles that it has a full toolchain support it handles different compilers different different c plus plus runtimes all stuff like that and what's more it also uses binary files whenever it is possible so it's very cool and very quick to use if you are using a rather common configuration if you are not using a common configuration it's still most of the time more convenient than to use native packages or packages compiled by hand so if you want to install conan brew is one way to do it nixon of course is another way to do it oh my i have no idea why i've written cash here it should be conon trust me and you can also use pip to install conon because it is after all written in python there are some downsides to conon as well which should be pretty obvious if you think about it but they are still worth mentioning so binaries might be missing for your platform that's that's true but if that's the case and this is something that prevents you from using it you may consider running your own artifact storage and put your binaries there so you would use a local cache that will speed things up conan sometimes resorts to system packages in a weird way i had this case i think at least with ffmpeg not sure if this is something that's general or not but you should be aware of that that it's not hundred percent user land stuff and conan is still being developed in a pretty fast paced way that the generators change the way that you should use it the way that you should rank packages changes a lot but this is also something that should become much more stable with the 2.0 release i hope pretty soon so even in its current scope i say give it a try because it will probably solve at least some of your headaches another downtime is that creating your own packages still requires some skill so it's not that you would just put your binary someplace and that's all it requires a bit of crafting but also it's not that hard and it uses python most of the time so this is rather a nice language to use of course it's not c plus but you know it's not also something quite exotic like cmake may seem so that's that's about conan's downsides i assure you that most of you can live with that here's an example of a common profile or a conon configuration because conan is based on profiles you can install packages for conon for different cross compilers for different compilers different api versions etc so to make canon work we need to define the os that we are targeting the os that we are building on the architecture the compiler the compiler family the compiler version the c plus plus api that we want to use and the build type whether it's a release or debug build most of that can be auto configured conon can guess the profile for you and you only modify it if you need it but this is this is like an example of how it looks like and the old style to configure your dependencies with coil is to write a con file txt which contains the requirements in this case we want to use flag library and speedy log and we want to generate generate cmake files for that to use to included in our in our project we would then use con and basic setup targets and that's mostly it in target link libraries for example we just refer to conan libs and that's all unfortunately this is the old-school cmake style that i have already mentioned is kind of discouraged so there is another way to do it and there's another way is to use cmakedeps in this case we are depending on microsoft's gsl and we want to generate cmakedaps so conan will download all the stuff to our machine and then configure the necessary find packages files locally in a build directory so in this case we would just resort to the standard way of configuring cmake stuff with find package another way another way another tool that you might not be using but it's definitely worth it is cpuc and cpa can generate source and binary packages for you it could speed up nsis installers it could speed up mac os dmg archives it can create debian packages rpm terribles whatever you want if yeah if the platform is supported and you have necessary cpa configuration but default configuration is quite cool most of the time but you need to know how to create debian packages and you need to know how to create rpm packages if you want to build them with cpac which may not be very convenient because you most you're most of the time doing the same job twice the alternative would be to switch from dev and rpm packages and instead use something like app image or flatback app images and flat packs are similar in their approach they're basically compressed file system overlay that include all the dependencies inside them so you can think of app image or flatback as kind of application containers for desktop linux distributions this is the new way to package portable linux apps flatback is definitely more recent app image has been there for a longer time for one of the projects that i've been working on i decided to go with app image and i think this was a good choice it was rather easy to to integrate it with the rest of the tool chain it generates nice self-executable files that i can distribute to my clients and they can just double click it or in the console around dot slash yada yada yada and the application launches pre-configured with all the stuff that's necessary already there so here is an example of how you can add a custom target in your cmake for app image so yes i know it's not very elegant this could probably get better and definitely there is the possibility to write a nice cmake library to handle up images for you but other than that this is how you build a graphical application using qt with app image so you first call the linux deploy stuff then you call the linux deploy plugin stuff and then you put some configuration so for example where your application is installed what plugins are you using where is the desktop file where is the icon file and which target which which cmake target would be the entry point of this application that's it another step would be to add a custom command for making an installer out of that and this is also rather simple you need to call one python script and yeah again provide where your app image file is resulting is residing how the application should be called and also what should be the icon used for the application that's it and if you are hungry for more come on don't be shy if you're hungry for more you can check out our book which features a chapter on building stuff but not only it also has a lot more to do with architecture focused on c plus plus but not just for c plus plus developers as well and we will be on the book fair with this book as well so you can ask us anything regardless if you're a developer wanting to become a software architect or if you are a software architect already i think you could get some new knowledge from this book and i recommend to you to buy it i also if you already did then thanks okay so having said that if you have any more questions you can catch us on this conference you can also catch us using the handles that you can see here or on twitter linkedin wherever you basically want so thank you and thank you very much have a great day enjoy the conference