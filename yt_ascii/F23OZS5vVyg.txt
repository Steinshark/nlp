um our guest speaker our first speaker today is anthony williams i'm very pleased to have you on the online conference thank you for coming thank you for preparing your talk anthony about buddhist reading and c plus 20 it's interesting topic please take it away okay yeah so thank you i'm going to be talking about naughty threading in c plus 20. and so first off then i'm going to be making some simplifying assumptions the first one is that we're starting a new project so we don't have to worry about the existing code and compatibility with what we've already got and secondly that we've got a c plus 20 compiler and library so lots of the things that i'm going to talk about will still still apply if you've got no c plus 17 code or c plus plus 14 code but specifically we're going to be focusing on the c plus plus 20 stuff so you probably need the c plus 20 compiler so the first thing that i'm then going to talk about is how you choose your concurrency model because you know that's the the basis for what you're building in your your multi-traded application you're going to have to have a think about why you're looking for concurrency and multi-threading why that's the solution that you want to use and what benefits you're having to achieve and then that can affect the way that you structure your code then i'm going to look at how to start and manage threads using the c plus 20 library and then finally we'll look at the library facilities that we have for synchronizing data foreign so when you're choosing your concurrency model then you know fundamentally then we can divide our applications into sort of two different types you know the based around the fundamental reasons why we're wanting concurrency now the first reason that you might want some currency is you want scalability now you intend your application to run not just on the computer that you have today but that you want to run on the computer you have that you're potentially going to buy next year or alternatively you know that today then your your software is going to be running on customers computers and that they vary in the power that they have if you if you're trying to run on desktop windows machine for example then some people might still have no single call or dual core computers and other people might have you know thread rippers with 64 calls and you need if you want your application to work seamlessly and take advantage of the more cause then you need it to be scalable similarly if you're writing library codes then you might find that you want to scale your your library if it's in some form of number crunching library then you might want it to the scale node with the system that it's running on and potentially it's running on again desktop computers or possibly it's running on on server computers which have more cores or possibly it's running on an hpc rack which they might have thousands of calls and you need to scale across the whole facility and you're looking for that scalability you're wanting to write software that can that runs faster for some for in whatever metric is you're this important to you as the processing power increases and on the other hand then you might be using multi-threading as a means of separating of concerns you have some things which your application does some of which can run concurrently and so rather than doing the complicated codes necessary to multiplex those on the same thread then you leave that to the operating system you say well there's this part of the application which runs concurrently with that other part and you as the operating system can make use of the available execution facilities to actually run those two parts concurrently either no by interleaving and time slicing as has been done on you know on computers for decades or then potentially you know taking advantage of the multiple cpus and multiple cores that your system has as they become available so if you're for example you you're a developer and you're running your editor and you want it to highlight syntax errors in the background you don't want that to get in the way of you're actually typing in new code or a new or but you do appreciate the red squigglies so you might have a background thread in your editor that does that compilation or potentially even invoking a background process to do that and so that is just that's a pure separation of concerns facility because we want to maintain that responsiveness in terms of being having both parts still running whilst actually taking advantage of the multi-threading facilities and which of those you choose fundamentally affects the structure of your code so they they inform our choice of concurrency mode okay so if we're thinking about scalability then what we're really wanting to think about is you know the speed up that we get from parallelizing our code we want you know that when you run it on a system which has more cores then we want it to run faster and so we have what's known as amdahl's law which applies in this case the speed up s is one divided by right one minus the fraction of your program that we've been paralyzed plus the fraction that can be paralyzed divided by the number of processes right it looks relatively straightforward but you know if you if you stick in the extremes here and you say okay well no at 100 so our fraction p is 100 so that's one then one minus p is zero and one over n is one over n but one over one over n then becomes n so if you have 100 of your program it's paralyzed then the speed up is just how many calls have you got and if okay multiplies and scales nicely but on the other hand there's very few programs that are 100 paralyzable because at the very least you have a bit of startup and a bit of at the beginning and then some form of serial termination at the end so we don't have that in practice so what do we get so let's stick some more numbers in so if 90 of our code of our programs execution can be paralyzed you might think that's a reasonably high number but that actually isn't necessarily as good as you might might think now even on a thousand cores then that still only comes out as a 9.91 multiplier on your speed up so we're not even 10 times faster on a thousand cores if we have a 90 of our program can be paralyzed so obviously and this is where we're looking at the whole program but what you really want to do is you want to say well i don't mind about the startup i don't mind about the shutdown what i really want is i've got this core data processing part that i want to worry about so when the user clicks go on the gui and then i want it to take advantage of all those thousand calls and do the processing really quick until we get to the end and so fine you can say yeah we can trim down the notion the fraction of the program that we care about to just really the bit that's been trying to be processing but still no 90 being paralyzed would might have seemed like a reasonable thing but you say is still only gets us 9.9 9.9 times speed up and obviously if we put in a hundred thousand calls then and still only getting 9.99 and we haven't got a 10 times speed up from 100 000 calls so if you're looking for speed up on scalability you really need to push that fraction higher of how much of your of the code you're caring about can be paralyzed any single part of that that's going to take away from the pure parallelization is going to hurt your you know multiplier in a big way if we can get it to 99 then on 100 100 000 cores we still only got to 99.9 times speed up not even 100 times speed up on 100 000 cores so you really do have to push this all the way to the limit to get some for the part of your program that you're wanting that scalability for you really have to make sure that that that there is nothing there that's going to get in the way of the parallelization okay so having looked at the you know potential speed up we can get we then have to look how can we do that and one way of doing that is the standard library algorithms because they have parallel versions you can pass as the first parameter you know before the additional standard sequential code parameters we have an additional execution policy so for example if you're trying to sort a big vector then you might say standard execution standard sort with standard execution path for parallelize as our first parameter and then we pass in the normal range and the comparator just the same as ever and this tells the standard library you want to use parallelization for this you want to know do that do that sort in parallel using the available course from your from your cpu and the standard library will do its best to accommodate that and therefore now it will run it will take advantage of the things now obviously it's not required to no the it is a legal implementation technique to just say well i'm going to ignore that execution policy or run it's always running single threaded and sometimes it's not worth it because there is overhead in dividing up the data and managing that execute that parallel execution and if that means that then the the fraction that's actually being paralyzed is now only 10 of the runtime of that of the sort then it's just not worth the overhead and you'll probably get better better performance just by doing the single threaded execution and implementations sometimes have heuristics in them that make that choice so for small sizes where small might be less than a million elements or something then it does it serially and then larger it does it parallel and obviously quite what the details that heuristic are will vary from implementation to implementation foreign anything that's going to avoid the power of the prevent parallelization and if you have consecutive calls to parallel algorithms then because with the way they are currently specified then they have a civilization point in between so in this case we have a chord transform which is parallelized and that will then do its parallel execution gather with all the results and then finish and return back to the single thread that started it then we have a quarter standard reduce which again is paralyzed and again it arms out the data to the peril into the to the different threads that are going to run the parallel execution and then it's serializes them back again at the end and that serialization in the middle reduces the parallelization capability so if you can you want to combine your consecutive course in this case then we have transform reduce as i combined combined algorithm that therefore doesn't have that serialization step and therefore increases the potential for parallelization in our code this is transform reduces c plus plus his answer to map reduce as people might have heard of from larger scale systems and in practice it's really quite a general facility and lots of things that you would like to parallelize can be no ins can be altered and adjusted so that you can express that as they call to transform reduce often with a series of range adapters no so using the c plus 20 ranges library in order to you know to map and adjust the data that's being accessed so you can then make it into a single transform reduce call and that will therefore help with the parallelization like but it can be complex to make that work okay so the other thing that you can do is you can split your work into independent tasks in user and use a thread pool and and this is is you can do this for parallelization if you've got a rather than using the parallel algorithms yourself now the ones that are provided by the by the library then you can say well i'm going to manually divide up my tasks into a whole load of small pieces and i'm going to give them to the thread pool and it's going to executable alternatively then you can use thread pools for for concurrency based things where you have i have this you know this part of the application that needs to run and this other part that i want to run concurrently so i'm going to run them both on on the thread tool but that's but if those tasks are long running then that's not necessarily the the the the right choice and you might be better served with dedicated threats but either way you're dividing your work into many independent tasks and then you put them on the thread pool and the thread pool then runs them on the available hardware and it will run more of them concurrently if the hardware can do so and fewer of them concurrently if it can't quite the details will vary on your thread pool implementation sadly we don't yet have a standard one so we're looking at non-standard extensions you know things like intel threading building blocks or various various other third-party thread thread pool libraries that you might get hold of no but we're hoping that we will get standard thread pulls soon no aiming for the c plus 26 standards but no so fingers crossed it will actually get there so touched on how you might want to use those thread ports for separation of concerns but really no so we're looking when we're thinking about this we're often thinking of large sequential tasks and so we're looking at literal raw performance is not necessarily a priority so we want things that can run concurrently we're thinking of things that can run in the background like background compile or spell checks or possibly even printing no where we don't want we want the task to continue as a long-running thing but we don't want it to interrupt the i'm going to call it the primary task of of our application or we want multiple things to run concurrently but we don't but the direct raw performance isn't the game although obviously if they can run can run truly concurrently then that is nice but we don't want to actually have to write the multiplexing ourselves we want the operating system and the runtime library to do that for us so you might then run long-running tasks on their own thread rather than putting them on the thread pool so that then the operating system is aware that this is a dedicated thread for this task you might have task that's running a thread that's running the gui or a thread that's doing printing in the background for example now note here in this example then you know standard j thread is a new class from c plus 20 for managing threads you pass it you know something i you know that is callable in this case lambdas to the constructor and it then starts the thread automatically which neatly brings us on to starting and managing threads though actually first i'm just going to take a detour we're going to talk about cooperative cancellation nope see we all use guise all the time that have cancel buttons now you might have it downloading a file you might have it rendering a video we might have it printing something or whatever that often is a cancel button and we want to our cancel button to actually stop that operation fairly promptly and then tidy things up nicely obviously you don't need a gui to have to cancel operations you might be doing things on the front line they might be internally in your application that you start start some task in the background and then you decide it doesn't it's not needed anymore and you know maybe the source data that it was operating on has changed and so it's now going to give an incorrect result so we want to cancel that operation and we'll restart it with new source data but whatever we're doing i mean force will be stopping a thread is undesirable if you actually use low level operating system calls to brute force cancel a thread then who knows what state your application is in because no the destructors in that thread have not had a chance to run so it might still be holding locks it might still have open files it might have partially written data that he was intending to synchronize to share with other parts of the application but it's now just left in a half finished state so you don't want to do that you don't want to forcibly stop a threat so you want to have some means where you can ask it nicely to cancel and tidy up and in c plus plus 20 this is done with the standard stop source type and the standard stops token type no it gives us cooperative cancellations and it is purely cooperative if you send a cancellation signal and the target task doesn't check then nothing happens it's not going to interrupt the task in in a forcible fashion it just sends it a signal that it can check it's a flag i'm going to ask you to stop and there's no and if the code doesn't look nothing happens so how would you use it well first of all create a stop source it's just default constructible that's dead easy then we ask the stop source to give us a stop token this stop token is copyable we can pass it around to as many places as we like so we can pass it to a new thread or a task or something to draw a thread pool and then when you want the operation to stop you call request stop on your stop source and that then sets the flag at some point then the task that we're hoping to interrupt will call no call the function on its token stop requested which checks to see if the flag has been set and returns true or false if the task if if the if the call to stop requested returns true then it's up to you to stop the task do something you know with that with that signal and again to reiterate if you do not check the stop take requested nothing happens if you check it and this discard the result then nothing happens special now it's up to you this is a cooperative mechanism so what does it look like in code well here we have two things we have our stoppable function where we take a stop token as a parameter and then typically you'll have some while loop or maybe a for loop or something with an exit condition and but where it's going to check the flag every fe iteration and well we have while the flag has not been raised we're going to do stuff and then when the flag is raised then stop requested or return true and while loop exits on the other side on a different thread we'll have our stopper function running which takes the stop source it's a movable thing so you can transfer around the shipping pass it by by value and then while we're not done where we're actually checking to see whether there's something that's happening that means that we now want to stop then we're going to do something and then when we finished when our done flag is true then we're going to ask the source to request that the old thread stops and that will set the flag and break us out of the while loop in our stopover function you can also use stop callback now this is actually one of the most powerful parts of this mechanism it gives you a means of putting cancellation around you know in it any mechanism that can be canceled can then be used with stop tokens so if you have say an async io library that you have that you're using that gives you a function for canceling pending i o then you can then integrate that with stop tokens by using a stop callback so take our stop token we open our file to give us a handle we then register the stop callback this is if we're stopped at this point then i wouldn't you know to call cancelio which will stop the pending i o on this on this operation on this handle and then so having registered our stop callback because we've created the object cb then we can then call out to our blocking call now read data on this handle and now this is a this would normally be a blocking call but if a stock token if if the stop token has its flag raised then the stock callback is invoked on the other thread on the thread that actually set the flag and that will then call cancelio which will then interrupt our read data call and move and we can then return and in this case obviously it's returning a data object which maybe it's some partially read data or in that of the data object contains a flag that says actually i was interrupted or whatever but the point is that now previously we would have had to know that the other thread was doing i o that we used to and we needed would need to know which handle it was so we could call cancel i o on the handle now with stop callback we can we don't need to know that we don't need to know what this other thread is doing we just need to know it's doing something that we can stop and so we set our flag with the stop source called request stop just like we did before and then the callback identifies the exact function that we need to call in order to do that interrupting so this is like i say it's a really powerful mechanism you can use it for anything that has some means of interruption that you can use because you've registered with the callback okay so having just looked at cooperative cancellation now we can go back to starting managing threads and so if you're managing threads manually you want to use standard j thread no 99 is probably too low a fraction no 99.99 it's it's what you want to use if you have it as an option and you're trying to manage the threads manually obviously ideally step back we're going to use thread pools because if you can if you can leave that the library manage the threads entirely then that's a good choice but if we're managing threads directly ourselves j thread is the answer foreign well sometimes you don't want sometimes you actually want to run the thread and it's going to give you a value as a result no it's going to run a long running calculation give you a value in which case standard async might be what you want because it gives you a future that gives you a value but in particular standard thread from c plus 11 should only be used if you have no choice okay we're saying we've got a c plus plus 20 component library and no legacy code so we have a choice we don't need standard thread but if in practice you've got some other code that uses standard thread then maybe you do maybe you need maybe you're stuck with it but you should be looking to migrate to j thread if you can so how do we use j thread well i said already creating the j thread object starts the thread we pass it a callable thing like with standard thread from c plus 11 you can pass some arguments as well so you can pass the name of a function and some arguments you could pass as a vandal that took arguments and some arguments anything that's callable and then arguments that are then passed and it would what it the reason why we took the detour into cooperative cancellation first is because j thread incorporates cooperative cancellation when it calls your function the first parameter is going to be a stop token and so and that stop token relates to a stop source held internally in the j thread object and so it gives us that cooperative cancellation option obviously if your function doesn't take a stop token then it's backwards compatible with standard thread so you can just do is use it as a drop-in replacement and it will just call myfunk with the arguments you supply but first it checks to see whether your function will take a stop token because it's trying to integrate with you know with that cancellation facility so the basic api we have a default constructor that creates an object but no thread we have a constructor that takes a callable and some arguments it gives us a stop source like i mentioned and passes that the token from there into our callable the destruction again integrates with this cooperative cancellation facility we call you know when the destructor occurs if the thread is still running then it calls request stop on the stop source and then waits for the thread to finish so this is quite different from standard thread standard thread if you if your thread was still running it would terminate your program in the destructor and you have to manually manually join in j thread it automatically joins it but first it says i'm the destructor so please stop no somebody's asked you to actually you know come to an end now and then he waits for the thread to finish we have a function get id to obtain the thread id of the actual the thread that is owned and again it returns the default constructed id just like standard thread does if there's no thread there and we can manually join manual you know explicitly wait for the thread to finish which doesn't automatically call no request stop now it just waits for the thread so if you know that you definitely want to wait for it to finish you can just call join whereas the destructible and i try and cancel it first and it's a handle it's movable you can transfer it around as an object no like a unique pointer i mean does mean you can store it in containers which is really nice particularly with you know with j thread with its automatic joining in the destructor means you can have a vector of j threads and when the vector is destroyed it will cancel and wait for all the threads and whereas a vector of standard thread if any of them were still running it would terminate your application when you when you click them out and there's no need to use new too many people who are coming from you know other libraries that you know other in other languages then have a tendency to write a new standard thread or new standard j thread don't do that just say standard thread or standard j thread please and then you have it as a value that you can then move around when necessary when you pass an object to the constructor of your j thread it copies the data into the internal storage so that there aren't any you know there aren't any dangling references there automatically by default obviously if you really want to pass a reference you can use a standard ref to wrap it into a reference wrapper object which then it copies the reference wrapper object and that still refers to the same source or you can use a lambda with reference captures but no by default because well they are say copied then moved if if it's if you pass it if you pass a temporary or an r value then it moves into the internal storage well if you just pass pass a reference to something you copied but it means that then the execution itself on the new thread is in storage that it knows about and it is in control of and means that therefore you don't have there's less scope for automatic dangling references so just another to reiterate the destructor request stop and then wait for the thread to finish so if we have our function through here on the bottom then we start a thread that's going to run our thread function and then going to do stuff and then at the end we're going to say well we're done now so we'll just drop off the end for destructor then request stop and joins and in the thread function we take that stop token as that first parameter we then get the string and the integer that were passed in as parameters and then we can have our while loop that's doing stuff while we haven't had stopped being requested foreign that's the first way that cancellation integrates with j thread because it integrates with the stop taken support directly we implicitly create this stop source which is then passed as the first parameter to the thread function and then we have the destructor that calls request stop before they call and join but again it's cooperative you still need to check the stop token because if you have your thread function that yeah it doesn't take a stop token or it takes one but doesn't check it then nothing happens that that co-op that cancellation is cooperative and if you don't cooperate you don't get it canceled but we also have an api so you can then call you can call get stop source which gives you the stop source that then you can you know actively request stop on somewhere else because you can pass it around you can call get stop token to get a stop token that you can say well when my thread is interrupted i also want to interrupt this other task and then of course you can manually request stop so rather than having to get the stop sourcing thing called request stop on that you can just call request stop on the thread project and that then sets the sets the flag in the stop token so that the callable that's running on on the other thread is interrupted okay so we've started some threads and in if you're in the majority of programs those threads are not going to work on isolated data they're actually going to try and communicate with each other if we're going to communicate data between threads we need some form of synchronization and the c plus 20 library provides us a whole bunch so first thing to watch out for though is the dreaded database a data race is where you've got unsynchronized access to a memory location from more than one thread at a time and at least one of those studies writing so it doesn't matter if all the you know if all the other threads are reading and one of them is writing that's a data race whereas multiple threads reading from the same location if nobody's writing that's okay and obviously two threads both writing still a problem and it's not about the actual timing so it's not is there a time a window in time when the the threads are definitely both trying to access the same variable it's have we got proper synchronization in place if there's not synchronization in place that forces them to be separate so that one happens before the other then we have a data race and the problem with databases is that databases are undefined behavior and so therefore your program could do anything at all right the the c plus standard places no requirements on the on what a program with undefined behavior does it might launch some missiles set fire to a computer reformat your hard drive no post spam messages and your name or whatever so we need to put synchronization in our programs to ensure that where there is data shared between threads we do not have databases like i said we've got a whole bunch we have latches and barriers and futures and mutexes and semaphores and then of course atomics so let's have a look at these in turn latches okay so the c plus 20 latch is a single use counter it allows threads to wait until that count reaches zero construct it with a non-zero count and then one or more threads decrease the count you can never go up again it always decreases then when it gets to zero the latch is signaled and threads can wait for that signal to arrive so you can say i want to wait until the latch reaches zero and then and then other threads can can signal it but when that count does reach zero it is permanently signaled like a you know a physical no electronics latch and so like then another thread tries to wait then again it is immediately woken so here's here's the api we have we construct it with accounts we call have count down we have weight we have the idiosyncratically named arrive and weight which does a countdown and then a weight it's you know called arrive and weight for consistency with barrier which we'll look at in a minute even which seems a bit odd with the latch api but no there we are these things happen and so how do we use it in some code well typically you might use it if you've got a bunch of tasks to wait for so we create our latch with a thread count you know for when everything's finished so we've got an effector of data and an area of a vector of threads we're going to push back some threads to you know that's going to process this they do some data and generation and each of those threads j thread of course is firstly it's going to catch up by reference the data vector so that we you know can store our data and we're going to capture by value the index to our loop and we're going to set and we're going to call make data to generate the block of data for this thread and store that in the appropriate slot and then we count down the latch at this point the latches we're done with the shared data you know the latch is signaled and this thread can now do more stuff of some whatever description as long as it doesn't touch the shared data it's so good all good so we have that for all of them for all of thread count threads and so when all of the all the threads have then reached the latch then the latch becomes signaled and so on the main thread we can call done.weight which waits for the last to become signal so all the threads start they do their make data part they all count down the latch they're all now still running and doing them more stuff potentially but the latch is now signaled and so the main thread here running foo can then process the data from that data vector knowing that every thread is filled in its part and even though the threads are still running the data is correctly synchronized another a particularly good use for latches is synchronizing tests because if you're trying to write multi-threaded test code then what you want ideally is you want the threads to all be running and they're about and so they are actually genuinely running and really competing for resources and accessing the shared data structures at the point that you're trying to test okay is a common problem when you're writing multiple test code actually starting the test threads takes time and so if you don't do anything special about it you can start a thread no start thread one that's going to do some part of the test and and give it some code to do and it will start the thread immediately and run its code then a meantime thread number your main thread is starting thread number two that's going to participate in the test but at this point thread number one is finished before thread two even starts because actually because starting thread two is so expensive in operation so if instead you set up the test data and create a latch and then create the threads and the first thing each thread does is arrive at the latch then now you've put move made sure that you've moved the expensive starting of the thread before the arrival on the latch and so these threads are actually running okay or at least ready to run as best you can make it when you know arrive at that because they all count down and then they're all wait then when all the threads are ready then the latch is now signaled and they all wake up and so they're all released to run the code as close to concurrently as possible so this is a really good way of setting up your test code obviously for them to really run concurrently you need to have enough calls in your hardware that they can and things like that but no and there's you're still open to the bakeries of the schedulers to whether they actually actually do compete and run things but it gives you the best chance you have of setting it up and so if you run you know set up a test like this then you've run the test many times then it gives you a reasonable chance that they have actually run concurrently for the bits of test bits of code you set up in the test threads okay so next up we have barriers standard barrier is the barrier that we have here and notice it's a template though if you don't care about the template parameter you know then you can just use the c plus 17 style argument deduction to you know omit those angle brackets if you'd like and the whole point is that it's done in phases so we construct a barrier with a count and a completion function and then threads arrive at the barrier and some of them will wait for the barrier to be signaled and others will say i'm not waiting i'm i'm i'm counting out count me out for this time then when the count reaches zero the barrier is signaled the completion function is now run and then the count is reset and we loop background to the next phase starting at point number two again so threads arrive they wait the count gets they're all arrived barrier signals completion functions run we loop so this is really good for where you have no some it's it's loop synchronization and so it allows you to provide a serialization step which it might for those of you used to things like mpi then that's this is a this is a gather step so you've got all the data that's that's been calculated and then you want to gather the results and do something with it so you might then know if you're encoding a video for example you might have that the different threads are doing different parts of the frames or ones doing the video and one's doing the audio or something like that and then the completion function then gathers all the data from all of them together and then ships it out into the file and then resets the data for the next frame and something like that you know so you have this continual loop processing where you have a bunch of threads that are working on us on the same set of data then they're all done you've run the completion function and then they all work on the next set of data and so on what does the api look like well our template parameter is the task type which is the type of the completion function so you might use standard function for that or you might use a function pointer for the task type and then the task is actually the function or the lambda that's actually going to be run as the completion function but crucially you need a count and when threads arrive then we get an arrival token and that decreases the count if the count reaches zero that can trigger the completion phase then when you want to wait then you pass your arrival token into weight and that waits for the completion to be finished there is this arrive and weight function the same as we had on on the latch which it combines both does the arrive it plus immediately passes that token into weight and see there might now not be a token because it doesn't need to do the splits but it has the same semantics and then there's this riven drop which says i'm going to decrease the count which might potentially trigger the completion phase and then not wait this thread is now not waiting it will no longer participate in this barrier and the count is permanently one less so when it is so if initially the count is five and sometimes when calls arrive and drop the next time it resets the count will reset to four it's a permanent decrease yeah so that's relatively straightforward and like i said barriers are great for loop synchronization you know because the and we have this completion function allows you to do something between the loops now this is explicitly this is a serial part so if we were looking for parallelization then this completion function saps away our parallelization because it's a serial part serializing in every every iteration of our loop that's just like a disaster from the point of view of parallelization but on the other hand if you're just trying to take you know advantage of the processing there is or you've got you know you're doing things for you know for not for the raw performance sake but just to take advantage of the of the paralyzation that there is then you know maybe it still is you know having a completion function isn't such a disaster it's actually something that allows you to separate your concerns here is the bit that can be paralyzed here is the bit of cereal here is the bit that's paralyzed here is the bit the cereal and so forth so what's it look like in code well like i said you might use a standard function as the parameter in this case then one that takes no function taking no parameters and returning nothing which is good and it needs to have essentially that signature whatever type of task you pass in there the return type is ignored it's guarded if there is one for the completion function so we have our have a complete function finish task and we have a number of threads and then i work a thread we'll have a number of these and they've got obviously i is the index so we loop and while we haven't been interrupted then we're going to do stuff on the arrive and wait at the barrier and then all the threads will okay each iteration they will do the appropriate stuff given the id index and then wait for all the other threads to do their bit and then they will all move on to the next phase check for cancellation and then do stuff what if we don't have you know a symbolization that can be done by a simple either one-time counter or a loop now if you want actually to pass in data in a one-off fashion so futures is how we do this it's a one-shot data transfer mechanism i mentioned stars async as way of creating a thread at the beginning now so this is what you can do here with standard async you can launch task it returns a value alternatively you can set that explicitly with a promise or a package task which is then itself a callable thing that rather than returning a value it stores them stores the result in the standard future that you can get from it separately so in theory you could you know build a thread pool using package task you wrap your custom user supplied tasks into a package task which then you can just have a have a queue of those to to execute on the thread pool and then that would you know return the results in the futures and that you handed out to when people give you tasks or something like that so the basic api around the future side is the default the constructor is always it gives you an empty object you need to get a populated one from one of the you know the three mechanisms mentioned on the previous slide but then you can check whether it's valid whether it has a value name is it just true with this potentially some shared state there false if there is interview it was default constructed or has already given you the data we can then wait for it either just unconditionally i'm going to wait or you can wait for a certain duration or until a specific point in time and then you can get the data and get implicitly assumes that you're going to do a blocking weights if you haven't already waited but once you've called get then it's one shot so the data is now gone it returns the data and moves it out from in terms of storage and then destroys the internal storage foreign we have the blocking weight and then the non-blocking weight so you can if you if you're just going to wait and then call get you can omit the call to wait but if you wanted to wait and then do some other stuff and then call get then you can and either way it blocks until the data is ready on the other hand you can take a future and do polling check so you can call wait for in this case zero seconds which is therefore going to return immediately and that returns of either returns future status ready if the data was there or future status you know time out if it wasn't and deferred if you've used a particular form of async and the data is not ready yet and then if the data is ready then you can process it in this case again you could get but we know that the data was ready to get won't block in this case so how do we get a future with any content well one way is is from promise and this time if you default construct a promise it has state it is ready to take a value and it's ready to give you a future we can check whether it has date by calling valid and we can never see set the value you can either explicitly set it as a value or you can set an exception by passing in an exception pointer and that the exception is then stored in the future and so if you call get on something that's got a stored exception it will actually throw rather than just returning the value of being a value there and then of course you can get the future in order to which you can then pass to whichever wherever you're wanting to wait for the data to come on another thread so how about it look if we've got promise and future we're trying to pass data from one thread to another so we construct our promise we get the future value from the promise we then move that future then movable not copyable so we have to move it into into the lambda that's going to our thread and then that thread that lambda can then call left or get and then process the result meanwhile on thread two then we can i we could take a reference to the promise here or we could we could move the promise in either way works again it's movable not copyable and then you can set the value on the promise you know with whatever function you have of getting the data and so therefore and when when thread 2 has executed that set value then thread one will wake up from its call it's blocking get call and then actually process the data and do stuff like i said if you set an exception then it is thrown so in this case you know we're explicitly making an exception pointer that's that wraps in my exception class and then when we call debt get here on thread one it will not execute do stuff because f dot get will throw and it will throw the my exception value that was constructed on thread 2. okay and i mentioned already async can be used to construct threads and it's very you know it it takes callables and arguments just like j thread does doesn't do stop tokens unless you manage that yourself but one of the important part is you have to specify the launch async parameters the first parameter if you want it to start a new thread if you don't then the runtime library decides is it going to start a new thread or am i just going to store the function for later execution and you'll have to execute it by calling get so don't do that always say you'll want this new thread so you specify launch async and then the other feature here is that in this case the future owns the thread is so it has the same sort of destructor blocking semantics as j thread but again there's no cancellation support so if you want the cancellation support use j thread if you want the value from the future use async future is one shot so after you've called get it's no longer valid and call it again will throw if you want do it twice then use a shared future and you get that by calling share on a normal future so we have a promise we call get future which gets us a standard future of my data we then call dot share which gives us a shared future of my data then we also know shared futures are copyable not only can we call get twice but you can call it get twice on separate threads and so we can copy them around and both threads can call f dot get and call do stuff and it's all fine new taxes low-level things no but reasonably commonly known of but fundamentally it's mutual exclusion a means of preventing concurrent execution you want to avoid it if you can i use them as sparingly as possible six there are six new text types in c plus plus that's five too many now you might say it's six too many because you really should be avoiding them but you know really really what you want to use is standard mutex there's variations recursive shared times whatever you don't need those no 999 times out a thousand maybe more often don't need those standard mutex is the one always use it just if you are really profiling your code then sometimes you might find that there are benefits to using some of the others i think then the visual studio api still has a better implementation in shared mutex than it does in mutex because it uses more modern parts of the windows api so you need to watch out for that but most of the time just use standard mutex and be done and and the less you find problems don't need to worry about it because mostly if you've got problems that means you've got high contention on your mutex if you've got high contention you want to be redesigning your code so you know standard mutex is good enough we also have raii types for locking we construct them with a scope lock when you they take a lock when you construct them they release the lock when they're destructed scope lock is the one you should use unique lock sometimes because it works for specific things but otherwise you don't need the other shed lock works with chev new textures which you shouldn't be using so like i say it's a r a i o type you construct it with the mutex and it unlocks in the destructor so beware it's locking multiple mutexes you might think okay well we've got a mutex in a value in two accounts we try and transfer data we lock both of them one at a time that leaves you open to classic deadlock don't do that no two calls to transfer lock them in opposite orders no deadlock if you've been said you just pass both of them to the same scope lock instance it sorts it out come no long time might be magic just makes it work so that's all the synchronization parts what if we're waiting for data well you really want to wait for data to be ready you could just do a busy wait you've got your mutex you check the flag no if it's set then you then you skip your loop otherwise you unlock the new text log it again and try again that just burns cpu cycles so we don't want to do that all right so trusty it delays the notification if the if the thread that's doing the generation can't get the lock because the thread that's waiting has got it or it's competing of the same thread in the os and for the time time slice then it delays the notification condition variable provides us a way of avoiding the busy weight it's in optimization you know so this is where you use unique lock because unique lock has this magic power that you can unlock during his execute during its lifetime and then relock it again and so you can pass it into condition variable that does just that it then it unlocks the mutex whilst it's waiting then it locks it in order to check the the predicate that you know check the data has got value and then you know it will exit that weight when the predicate returns true and then we know that we can call process data safely foreign must be notified so we have notify one and notify all you know which notified just one thread or any all that are waiting and we do it after we've set the data and again we do that we have a little scope around our lock in order to ensure that everything is set up properly it standard condition variable itself doesn't integrate with condition with stop tokens because it's a low level thing provided by the operating system but condition variable any on the other hand does have the means of integrating so we don't have to resort resort back to busy weights just because we want to to use stop tokens we can just switch our condition variable declaration from standard condition rule to standard condition variable any and then we can pass in our lock token stop token along with the lock and the predicate and this time no we need to check the return value because it might return when the predicate is false because we the stock was was requested okay and we have c plus plus 20 also provides semaphores which are very simple no their counters now you require a slot and you release a slot when the count is zero then you can't acquire one and then you can release one without without acquiring it and vice versa they're really very flexible you can build anything you'd like and synchronization mechanism with semaphores all of the previous ones everything we've mentioned so far you can do with semiforce for synchronization if you want to know how look up the little book of semaphores if you get the pdf for these slides then that's a link but mostly you're better off using the high level structures if you can a binary semaphore is a semaphore with two states and one slot three on those dots free could be used as a mutex in c plus 20 therefore we've got the two types we've got a counting semaphore and a binary semaphore which is just an ads for counting seven four of one though it might in practice be a specialization with you know more optimal code and then you also you don't have to block you can just try try to acquire a 74 and return immediately or maybe for a time period or until a point in time so you know if we've got accounting semaphore with a max of five that's constructed with five free slots so the two fines have a slightly different meanings here then you know we can then have five functions five threads of most calling do stuff requirement release a bit like a mutex but in this case there's a maximum currency level of five rather than just one our lowest level of synchronization comes with atomics and in c plus they're written standard sonic of t for any t at all as long as it's trivially copyable bit weight and bitwise comparable so it can be a struct you know with completely user-defined data but must be trivially copyable and likewise if you compare it with mem comp then it should give equivalent values floats are funny you know the bitwise compare might not be equivalent to equals equals but no mostly it's all right the only exception here is that shed pointer is allowed no shared point of obviously isn't trivially copyable it's got to increment the reference counts and likewise with weak pointer but the it's deemed sufficiently useful that we can have a standard atomic of standard shared pointer or something t and that deals with atomic reference counting now for that object it still might not be lock free even though it's no atomic it says it might use an internal mutex we have three guaranteed types that are but the sign the the integers sound good but you don't know how big they are or they only 8-bit are they 32-bit maybe 64-bit who knows so you need to be careful of that you're better off just checking the is always lock free no the flag for a given type and then if you combine that with if const express then you can then do some something else you know if if you wanted it to be locked free and it isn't or you can just do an asset but on those platforms anything that's smaller than the pointer is probably lock free so in summary avoid managing your own threads if you can use things like thread pools or the parallel algorithms use j thread if you're managing your own threads you stop token for cancellation future latch and barrier are the next level down if you're if you're having to do synchronization no use standard mutex if you can't use the high level things and occasionally rarely you might want to use standard sonic so do that now it's time for questions before i move to answer your questions then i just want to say that woven planet is hiring so my team in the uk is hiring and we have teams in tokyo and palo alto as well so get in contact if you want to come work for us anthony thank you so much for that fascinating talk i'm a big fan of your work but i think as we've gone over if we can take the questions in the lounge if that's possible are you available to to take questions of course yes great thank you very much okay well i guess we will see everybody over there okay see you in a minute yeah