how much memory do you need to run one million concurrent tasks by the way alerts are off uh in this blog post i delve into the comparison of memory consumption between asynchronous and multi-thread programs across popular languages like rust go java c sharp python node.js and of course everybody's favorite these nuts sometimes ago i had to compare performance of a few computer programs designed to handle a large number of network connections i saw a huge difference in memory consumption of those programs even exceeding 20x okay that i mean it makes sense it will you know how are they handling things because i can go it's pretty it's pretty slick goes pretty dang pretty dang slick you know what i mean some programmers consume little over 100 megabytes but others reach almost three gigs i think what he's trying to say here i think what peter what old peter up there kalaskowski is trying to say is that node.js node.js loves the memory okay at 10k connections unfortunately those programs were quite complex and differed also in features so it'd be hard to compare them directly and draw some meaningful conclusion as that wouldn't be an apple to apple comparison this led me to an idea of creating a synthetic benchmark instead synthetic by the way i actually do want to like follow this this little line of thought right here so i want everyone to pay really close attention if you go to github.com the primogen right and you look at this and you go to my latest one is they're like yeah not left pad there we go ts is rust zigds i guess i'm i'm going through a d's face okay if you go here you can join and add your own language we're building the interpreter okay and then i want to test it i actually want to build a server that does like full interpretation and we'll figure out what the what the rules are and all that but actually can compile remotely right and then that is what language can do that the fastest i feel like that is a way cooler like test of languages than these dumb tests that you see right there's that one youtube video that did like that just did like a mathematical formula and be like which one does this one the best and it's just like it but that's not even like real at all okay you're not even creating memory we need like memory you need cleanup you need things that happen you need connections you need sys calls you want to see the entire you know the entire thing you don't want to just see like just some tiny little nothing you know what i mean it never works that way it's always a lie it's just a lie it's just always alive anyways let's do this we have microsoft guy yeah benchmark i created the following program in various programming languages let's launch and concurrent tasks for each tack waits for 10 seconds and then the program exists after all the tasks finished what the program continues to exist after we are finished i'm pretty sure this is like a fork bomb then the number of tasses controlled by the command line argument let's go let's go the little help of chat jeopardy i could write such programs in a few what what what do you mean this is not hard in rust i mean are you using a real threat are using hardware threads or green threads or go that simple java this is probably not that hard to probably do it in a few moments of just a what do you mean chat jeopardy see this seems like such a crazy crazy things people just love them chad jeopardies anyways rust i created three programs in rust the first one uses traditional threads that's what i wanted to see okay i wanted to see one hardware in one language traditional threads okay i'm sick of all this polyphretisms going on here's the core of it bam make it happen the other two versions had async with tokyo and the other with async stood here's the core of the tokyo beautiful beautiful i like this the async stud variant is very similar so i won't put it there go go routines are the building block for conservancy so we don't need to do them separately but we used a weight group okay beautiful great use of a weight group by the way this is a great use of a weight await group loved it loved every part of this again i think go for all of its downfalls also has a lot of updrafts what's what's the term for like what's the opposite of a downfall yes i did have a bad experience with c sharp why you bring that up okay an upfall an upwind an updraft an uplift a boob job what do we call it i don't even know java java traditionally uses threads but jd jdk 21 offers a preview of virtual threads which are similar concept to go routines look at java go java has almost caught up to 2015. i think we all need to be impressed right now can we just take a moment to realize that java's going places it really is like this is incredible i mean the next thing you know they i mean the next they could actually beat c plus plus by the time c plus plus 23 comes out java might actually be exceeding what happened in 2023 as far as technology technologies go it could be it could be incredible and i think you guys are just not even considering how amazing this is all right let's see list of threads new arraylist good choice of arraylist you did not create you know a little alarming that you didn't you know pre-populate the size here you knew how big it was going to be a bunch of things new threads all right a little sleep little truck catch does it so he's not counting one thing i'd be a little careful of is even though you don't need to there is no counting right here going on right how many times did this fail i think that's really important to think about you know just in case when you're creating these tests you you need to do that okay thread start thread add let's go thread join perfect beautiful i'm curious about this as well when you go individual thread joins this could be bad right am i am i right on this one this could actually have a a greater slowdown because you're you're doing it one at a time and so you could have already had like a hundred finished that you like aren't you know what i mean that's a race yeah yeah i feel like this is it's blocking i know that but it's it's not just blocking us that you could pay a huge penalty up front and then you'll have this whole thing where you have to go through each one and i assume also join is like not a free call i assume it's like a sis call is that fair i don't know i don't know what it takes to do a join but my my assumption is that it's a non-trivial cost you know what i mean yeah anyways just a thought just a thought but i guess if they're green threads maybe that's not the case maybe they're actually really fast and delicious you know what i mean i don't know anyways these are just my thoughts i'm just kind of raw dogging my thoughts out here okay here's the the variant with virtual threads notice how similar they are oh nice that's beautiful well done oh i guess these ones are hardware ones and the other one these ones are hardware ones these ones are oh my goodness i'm scrolling way too fast oh my goodness i'm gonna vomit these ones are virtual ones beautiful still doing that i don't like that c sharp is similar to rust has first class support for async await awesome so there we go we're gonna do a little task run right here so this must be okay so this has to be those have to be synthetic thread so we're not actually getting so c sharp is not doing hardware thread so there does need to be like a a disambiguation here which one's doing like actual actual threads versus which ones are doing some sort of managed environment because i would assume that the managed environment this is where a managed environment does amazing right right i think so you can do both well i would assume whenever you see something like async await usually there's some sort of whole managed environment that goes into it usually right because if you're not doing it yourself there's something else going on you know what i mean i feel like the managed environment would be slower no because hardware hardware threads are expensive right node.js util's promise file okay so again you don't want to do this i feel like there is a little something there you got to be careful about you know maybe a little extra garbage collection going on i don't know but they're probably fine probably not a big deal probably enough probably not oh my goodness probably not enough to be too upset about it python i don't know what delay is delay must just be a function that returns a oh set timeout oh yeah okay yeah yeah i'm dumb i'm dumb this is also a syntax error guys syntax error right there okay watch your parents okay don't you should make sure you always just put code up on an article that's just like it works always make sure i mean i've done it too we've all done it just make sure it works right okay so this is looking good python out of days think of weight in three five nice okay look at this asyncio a sinkio there we go beautiful that all looks great elixir is famous for async capabilities as well let's go i love to see it task await mini until infinity and beyond you know i really hate that phrase by buzz lightyear to infinity and beyond wouldn't you not be at infinity if you could go beyond the point of infinity isn't that just like not infinity like can we be real here that's the joke it's true also infinity yeah i know thank you deep thoughts with me just letting you know deep thoughts all right test environment hardware xeon this thing okay this is starting to look like a personal computer which again gotta be a little careful rust 169 nice go eighteen one nice jdk dot net node python elixir let's go all programs were launched using release mode if available other options were left default all right minimum of footprint let's start with something small because some of the runtimes requires some memory for themselves yep that's right yep here you go this makes sense though because a node would be really really large well c-sharp requires a 131 megabytes for just nothing what like i get that there's like a whole thing going on but that's just a lot i mean this all seems about in line with node right c sharp i i would what does c sharp do that requires three times the amount of telemetry information that the other ones do okay it's pindos anyways so go make sense rust all these things make sense right because they all should be really really small because they're actually you know these are actually like compiled thingies but i would like to say that this is really impressive for go i want you to take a a moment about this this includes a garbage collector okay like things are running here that's really good that's really really good good job go good job go that's really really good the surprise hero is python it shouldn't be surprising oh i mean i guess it's surprising in the sense that it's half the it's half the memory of i i guess i would have i guess in my head i would have probably put it the same as java or node.js elixir also a little surprising and so large interesting i wonder how he's getting the memory is he using vmrss what does he use in here we can see there's our certainly two groups of programs go and rust programs compile statically to native binaries need very little memory yep this makes sense the other programs running on managed platforms and or let's see or through interpreters consume more memory although python fares really well in this case there is about an order of magnitude difference in memory consumption between these two groups yep it is a surprise to me that.net somehow has the worst footprint but i guess that this can be tuned with some settings maybe let me know in the comments okay this is good it's good that he's stating where he's a little bit surprised about i like to see that all right so 10k tasks before i oh no oh no you can see you can see what's happening here all right so let's see rust threads this this makes this makes perfect sense right it is expensive so i guess one thing that they we didn't specify here or that he didn't speak about with these two right here which was how much worker threads were created right so i don't know what tokyo does but there is definitely something to that right threat stack size right yeah there's a there's a lot here that might be hidden that we may not be considering correctly there's just it just looks really small which it may not be this again this is one of the problems about making a really really small synthetic test is that you don't know what's going to get you this makes more sense this is something i i guess i i can believe go being this just because go does have a whole managed system around it and so this is good this is still great this is less than what it took a thousand or ten thousand go threads go routines go go co routines is less memory than node.js by itself right so it's pretty good pretty dang good i like to see this let's see java virtual threads yeah this makes sense i'm a little bit surprised that java is that big for regular threads right c sharp i assume doesn't change much yeah it doesn't change pretty much at all because again it's probably doing the same tokyo thing that's going on here added nothing all right so this makes sense virtual threads also this is you know slightly in line this is more in line with go i guess note i'm very curious how that one i guess it's because set timeout is not really so one thing that he did not do correctly is it's not no doesn't really have this concept of threads right i i guess maybe the most correct way you could do this would be do would to do something like worker workers right and the reason why this is kind of odd is that node just creates a a a event loop item and that event loop item is probably some very small piece of information my guess is that especially since the timer is probably a time for when it's done and a pointer to a function to call right it's like going to be a pretty dang small amount of memory and so this makes sense that node really doesn't do much because you actually didn't create multiple threads you created 10 000 timers which is much much different because all of these through here they can they can also execute with parallelism you know what i mean there's parallelism that can go on in these that cannot happen in node.js therefore they're not really equal so it'd have to be worker threads you'd have to use something like worker threads i don't know if python has the same parallelism problem i don't really know how python works and i also don't know how elixir works but good job elixir right i said parallelism parallelism intentionally because these can all execute with parallel parallelism parallel parallelism depending on how many worker threads where's gradle great question all right few surprises everybody probably expected the threads would be a big loser of this benchmark and this is true for java threads which indeed consumed about 250 megabytes of ram the native linux threads used from rust seem to be lightweight enough that the 10k threads of memory consumption is still lower than the idle memory consumption of many other runtimes async tasks or virtual green threads might be lighter than native threads i would say might is i mean observably probably true is what we're seeing here right i'd say it's observably true but we don't see the advantage at only 10k tasks we need more tasks okay another surprise here is go go routines are supposed to be very lightweight but they're actually consumed more than 50 of the ram required by rust threads honestly i was expecting much bigger difference in favor of go hence i conclude conclude that 10k current tasks threads are quite competitive alternative linux kernel definitely does something right here huh again i don't again i i'm not sure how much i buy this i don't know i don't know what go does you know what i mean i don't know what go does that makes this good or bad go maybe reserving more memory it may have different parameters than something like tokyo does and so again i don't know how fair this is to say that rust greatly outperformed it because it's not doing anything too much telemetry too much telemetry all right go also has lost its advantage over rust ace async in the previous benchmark it now consumes over six more times more memory than the best rust program which is also taken by python yeah the final surprise is that 10k task memory consumption.net didn't significantly go up from the idle memory used yeah again telemetry now telemetry it's actually just telemetry probably it just uses pre-allocated memory or its idle memory is just so high that it 10ks didn't yeah it didn't matter okay 100 000 tasks okay let's do this let's see so the thread benchmarks could be excluded probably this could have somehow tweaked by changing system settings but after about an hour i gave up so hires a hundred thousand tasks okay so yeah you can't spawn non there you go so this is a good notice that all non-green threads have all gone away so i i guess my guess is that his program kept crashing he probably i would assume your u limit you should be able to do you i don't know if there's like i don't know what the potential requirements are that you can't spawn a hundred thousand threads but my guess is that you just fork bomb yourself and it explodes and dies all right so tokyo's gone up this has gone up this has gone up like to me this is pretty these are all pretty fine again i really doubt c sharp's doing something right something about c sharp tells me that this is not executing the way you think it is can we all agree that this is not doing what you think it is there's no way that you just did ten thousand there's no way that you did one to ten thousand to a hundred thousand with absolutely no memory change something is being clever here right something's being very clever or c sharp is probably the best no jazz so c sharp's gonna win i hope everybody sees this coming right i hope everybody sees this coming that c sharp is gonna start beating out some rust if they keep going with thread limits all right at this point go program has been beaten up not only by rust but also by java c sharp and node.js though let's see and linux.net likely cheats because it's memory you still isn't going up i had to double check if it really launches the right number of tasks but indeed it does and still exits after about 10 seconds it doesn't block the main loop okay i would still i would argue you need to do something i bet you this will greatly change in c sharp if you had like a a concurrent hash map that every one of those tasks try to add one item to and read one item from i think it would just completely change the memory wildly right let's say okay one million tasks let's go extreme extreme extreme extreme all right add one million tasks a lister gave up okay nice system limit has been reached okay edit some commenters pointed out that i could have increased the limit yep u limit after adding arrow p plus bajillion to elixir it ran fine okay nice all right let's see what we got here nice look at look at that c sharp oh memory did go up this time so that's interesting c sharp's memory did go up i wonder why this 10x caused memory to go up but the other two 10xes didn't sus and c sharp's the best everybody finally we see an increase in memory consumption of the c-sharp program but still very competitive it even managed to slightly beat one of the rust run times the distance between go and the others increase now go loses over 12x to the winner it also loses 2x to java java which contradicts a general perception of jvm being a memory hog and go being lightweight hey russ tokyo remained unbeatable this isn't surprising after seeing how it did 100k tasks final word as we observed a high number of concurred tasks actually i want to have a final word first okay i'm having the final word first first off i don't know if i like this benchmark i love the idea i don't know if i love the benchmark i feel like you need to do more things right i really do feel like you need to do more things for this to be real because something is wrong here first off one thing about c-sharp and memory that they're completely just disregarding along with java is garbage collection along with node along with go like all of these have garbage collection so does python i assume elixir does too but i mean elixir has already had four gigs am i right am i right but that's where rust is gonna really shine is that if you're just measuring memory doing something that creates and uses memory these other ones are going to really struggle but i wonder how much go is going to struggle because go gets the best of two worlds it gets a managed memory environment but it also gets like the smallness of rust when it comes to usage of memory so when you create a struct you're getting like a smaller structure you're not getting a node.js struct which is just much different right an object in node is not going to be nearly as lightweight as an object and go it's just that's how it works and so there is like it's kind of interesting you know what i mean it's just it's just interesting that doing nothing this is the results but i just don't believe it because my guess is that this elixir number four gigabytes is likely what would happen to a lot of these if you did that all right final word what is wrong why do i keep having like as we have observed a high number of concurrent tasks can consume a significant amount of memory even if it did not do not perform complex operations yeah i mean it makes sense like just imagine that every single imagine that every single task ran requires a hundred bytes of memory that would be a hundred that'd be 100 megabytes at a million right so it likely is going to require more than that yeah stack size you got all sorts of stuff it requires probably more than 100 bytes therefore it makes sense this makes sense right this is like what's a million times 4k right if you had 4k stack size boom you got that right anyways let's see conversely other run times with high initial overhead can handle high workloads effortlessly c sharp for the win by the way by the way the big takeaway here is you should just use c sharp we're all c sharp andes now inside this stream i hope everybody's ready for the c sharp arc c sharp arc everyone excited for it i'm excited for it i think that c sharp's obviously the best language okay i've been telling you guys this for so long now and honestly this chart just proves how good c sharp is okay you guys kept always talking about how great go is look at how terrible go is 2.6 gigabytes okay you just don't understand things at all all right c sharp clearly best language clearly best language the comparison focused solely on memory consumption while other factors such as task launch time and communication speeds are equally important notably at one million tasks i observe that the overhead of launching tasks became evident and most programs required more than 12 seconds to complete stay tuned for upcoming benchmarks where we'll explore additional aspects in depth i'd love to see this except for instead of doing node.js just doing timers let's set worker threads i'd love to see some sort of computation model added to everything i personally just think that the best way to do this is to do some sort of longer run living task right how many websocket connections can you make how effectively how many open tcp connections can you make to a server and then just start sending something back and forth how much can you do in a language not like these because you know the reality is you don't use a language to launch a timer you use a language to do something and i feel like a websocket's like a really great kind of it's a really great simple way to test something because it is it's just a tcp connection you have to do a moderate a pretty small amount of work to parse out a frame it shows what garbage collection does to the system it shows what kind of the interacting with with the system calls does to a system and then if you have anything extra on the system such as like if you do a chat room it will you know if you have to for each over clients what do four loops do to your program it's very very interesting even with node.js the difference like if you build a chat client and all it is is a chat line to where websocket connections can join in make a simple request to join rooms and then send messages to the room a for each statement will effectively cut your rps in half when you're iterating over the available sockets or it's not rps it's mps messages per second pretty wild that that can happen right and so it's it's pretty wild that even such a simple small little thing can have such a huge impact on performance anyways just something to think about anywho all right hey great article though great article everybody hey everybody great article hey great article everybody everybody give a little clap for peter good job peter appreciate the work you put in what is the name you know what the hell the name is the name is the primogen