there's a contradiction in the world of large scale data at the moment the de facto language of data science is python they have all the core libraries they have all the people pushing data crunching forwards but the deao language of data infrastructure is java they have all the big name apache projects for handling large scale data in real time and neither side really wants to become the other there's no sign that the python world is about to try and rewrite c and there's no sign that the java world's going to come up with something that replaces numpy even though num would be a very satisfying word to say but until someone bites the bullet and writes num i think the divide between data science and data infrastructure is here to stay and that's a very fertile ground for bridge builders my bridge building guest this week is thomas newbower he's the cto of quicks and i might summarize his bridge building technique as use python for data science use cfa for data infrastructure but if you stop thinking of cfa as a java tool and just think about it as a protocol the solution is to teach python a new protocol but that's a lot of work and how is it done how do you implement a pure python version of the kafka world without rewriting kafka how much work is it the devil is in the details and even if you do all that work is it enough to give people tools in a familiar language or are you still left with the problem of teaching them new approaches too let's get stuck in and find out i'm your host chris jenkins this is developer voices and today's voice is thomas newbower [music] joining me today is thomas newow thomas how are you i'm great thank you how are you doing i'm very well very well nice to talk to someone you're not too far from me right you're in london well currently i'm in prague i'm flying into london next week so i'm i'm traveling a lot between prague and london that's a nice life i when traffic is working yeah i spent some very happy times in prague so maybe i should come back with you sometime yeah sure but we're we're not here to talk about international travel that's a different podcast that i'd happily present we're talking about the world of python right yeah python obviously a de facto key part of data processing in the technical world and you've been trying to push the de facto state of things in a new direction which we'll get to but give me give me your estimation of the market as it stands what's the what's the normal way that people do large scale data processing in python yeah well the word is quite difficult to be honest if you are in python and you want to do a large data pressing right now it depends on what are requirements of course if you go to batch it's slightly better but if you go to real time and we talking about building stream processing pipelines your options are either using old big server side processing engines like flink or spark streaming m and you can also use you can build it by yourself so you can you can use kafka and client libraries or or similar broker and their client libraries to do it by yourself and third option would be using some cloud providers um options like lambas for example but they have lots of limitation as well now the reason why i'm saying that it's not ideal is because kafka and fling and spark they all coming from jbm world and as a result for you as a python developer who wants to use them the lives gets suboptimal first of all yeah it's it's just you are don't you don't feel to be a first class citizen in the ecosystem and and secondly when you actually try to use it you will see that you have to do a lots of java related stuff yeah yeah the first time i use the python flink api i was disappointed to get a java stack trace back yes so that's that's true but i think i got disappointed even even more when i had to put a jar files of gafka connector and then and then the jar file that this jar file depends on in the right folder and then depend reference it in my code i was even more disappointed yeah i hadn't thought about that cuz i'm used to writing in java as well but that's that's not a friendly way to treat a pure python developer yeah that's that's definitely not it and there are more problems to it like when you want to build your user defined function in python which i guess this is why you're using something like ping because you want to use python to build a business logic or potentially use something from the python ecosystem and it's not that straightforward because you're actually not running your processing in python you're running in in java and so what the fling does for example is run two runtime on each side by each other and there is some socket connection between them and when when you map your user defined function in python to some columns in fling what's going to happen that there will be basic communication between them and that means you can't really debug it easily there's performance apption on if the payload is too big and if anything gets wrong yeah you get a amazing java stti trace back yeah okay but there's i mean i think a lot of people in the data processing world recognize that problem but very few want to bite the bullet and rewrite something of the complexity of flink in pure python yeah well i think that there is couple of couple of new emerging server side engines that's trying to be more friendly to python and there was a project called f made by rin hood that actually were on a direction to do something but it got abandoned which is which is a shame and then obviously you have the cloud providers options spot then that you have a quite strong vendor lock in and they actually have a lots of limitations like i don't know if you use lambda you will struggle if you have a state in your services or in your transformations and there are problems with dependencies and resource limits so again lots of drawback there yeah yeah i can see that it's it's when you have to deal with state that stream processing gets really interesting up to that point you could just use simple transformers yes well basically i always say that until you are limited to one message at the time processing so you know when you process your data you don't care about the message before and message after yeah u it's a doable option to use just a u client libraries of of your broker of of your of your distributed system and and do it by yourself but the moment you touch state i think it get exponentially more complicated to build in a scalable resilient resilient fashion it's it's a really hard computer science problem to solve because you have to start thinking about persistence and distribution of that state yes and partitioning of your data checkpointing of your data and state delivery guarantees reassignments so so there's a lots of stuff you know that could happen during your state being changing and and so i would really not recommend to build your own stateful processing on top of like client libraries just for the purpose of one project yeah it's it's okay then because i know that seems like very good advice but i know that you had a project where you felt this pain and suddenly decided to build a stream processing library in python for one project effectively yes so in a way so so what's the project and why was it so painful for you that you decided to bite the bullet well so in my previous job we were building realtime decision insights for formula one and there was a basically a a pipeline with tele data from the car and an idea was to build pipelines that would tell you what to do in next seconds not next hours and the amount of data was was quite huge it was around 30 million different values from different sensors per minute from from each car and and the people that were developing these pipelines where you know the mechanical engineers data scientists ml engineers they all wanted to use pyon not java and and this is the first time i saw well hold on this is very powerful technology you know the streaming stack was really really great it worked really well but to actually leverage it by these teams was very difficult there was no tools no way to simple simply analyze data so you know things like kafka tool back then was wasn't really sufficient enough and and then when we actually managed to to explain everybody how to connect to the source of data and how to actually you know get it to the python the biggest problem was the muscle memory of working in a batch just analyzing data and database in jupit notebook versus building real time stream streaming applications where you process row by row it's such a different paradigma such a different approach to the problem that they you know they they struggled a lot with just the concept of streaming and like you know sometimes i'm saying even a mean you know even a mean that's that's the most simple operation that you can do in excel you know you have like imagine you have a speed rpm and you want to do average speed on a table not everybody can do that but to do a mean in the last 10 seconds for a car real time it's actually very complicated thing and very different approach to do it this is something that always interests me because i think a lot of this sector of the tech industry acts like it's a purely technical problem and that's a big part of it but the the mental problem is a surprisingly large leap right and you almost would have thought i mean python has asynchronous co co-routine primitives right and they've been in there for a long time you would have thought the idea of streaming data would be a little more normalized in python yeah i i think that buing asynchronous code it's it's obviously a bit more complic at than single trad code but i think streaming goes much further than that it's the the level of unintuitive for somebody who spend the whole life working with the static data is much bigger than someone moving from single to multi application and sometimes i kind of you know compare this to people trying something like reactive extensions for first time you know it's yeah such a huge overwhelming new thing or when people in the past moved from a classical you know wind form style des applications to this mvvm mv mvc buttons in a in a web these big mindset switches require you to suddenly do a common common task very differently and and this is very similar like everybody is you know used to to get the data from database analyze it in jup notebook print it on a on a way form look at on it on a map maybe do some filtering some analysis and and they are in the comfort of that the data are not moving when you run stream processing code every time you run it it will get a different input and that's just so so difficult to handle because at the end you building something which gets input you do something with it and you put output and it's difficult to tune that middle box when the input are changing every time you press around that's that's difficult yeah i totally see that and i mean maybe i've been thinking in this way for too long to fully see the problem but to me it almost seems a lot simpler you just have to build something that deals with one thing at a time and some other engine will scale that up to dealing with an infinite number of things yeah so that's obviously one way of looking at it the the problem is when you get stateful it's you need that context of your data and then in streaming where it's get even more difficult is that a part of the actual data what really matters is the behavior of your data so order of message mees the speed of messages and schema are also important not to trip over your code so when we were back in mcan for example there were lots of bucks and and crashes caused by just a nature of the burst of data coming after the start of the engine so people just did not unit test it and could not think that there would be a a burst of data in in the first five seconds worth of five minutes normally and they just got a lots of memory overflows so the the nature of the data how they come in is also important not just the content of it okay yeah okay so so let's start talking about your solution to this problem and how you designed it because you because it's a two-fold problem right you've not only got to design tooling but find some way of making it is it a question of training people to think in a different way or is it a question of finding a way that fits in with their current thinking yeah well i think it's a bit of both having said that i don't think that we can go away with just training people i think i personally by myself felt that it's rather difficult so you know when i was looking at the streaming and how you can what are the obstacles in in learning curve that was basically i was i was i was i was trying to look at like i haven't done the streaming at all and now i'm in this moment that you know you can you it's intuitive for you to do streaming what are the obstacles on this journey and can we somehow remove them and uh i think that the first the first big obstacle is what we discussed at the beginning which is you are not actually running your code you're running it in the engine which is even not python so i think the big power of something like f or something like quick stream library is that you actually run your functions in your ide so you can leverage the the ecosystem of the python and then when you inject any sort of dependency from it you will see how it works in your ide and you can debug it you can go line by line and see okay i'm using this mathematical function from this science package and it's crashing because i'm giving it badly formatted numbers and i see it because i get the break point before i call that library and i investigate all the inputs and i and i in a watch window in my ide i'm changing the input until i get it done and that is a vastly different experience for developer than i'm just deploying some black box which i'm registering to my engine and then i'm seeing crashing or not and i'm relying on locks to to yeah yeah yeah yeah as much as i like cloud services for putting things production having something local and native is just much nicer for development time yeah yeah for sure and and you know especially when you get into the more advanced stuff like the machine learning and computer vision you also get dependencies issues so you might do just pip install something like in computer vision and it will not work until you get a particular system depend in in in in when you run your code yeah and and so if you have a server side engine you have to make sure that every note where this fling cluster or spar cluster runs will have this dependency system dependency like for computer vision you have the lips something just going to have to be there which normally you just install in linux now if you go with the microservice docker route you have docker to do that for you and that's brilliant it's just a line of in a docker file so i think that library approach where you're basically running library in a containers gives you not just the better ide experience but also much easier integration with this ecosystem of of the language in this case is python okay so in that scenario you connecting your id into the docker container that you're developing inside docker that that's a good that's a good question yes it's a option so you don't have to do it you can obviously develop python on your laptop you know and it's very easy to to to do that but what we what we working right now is exactly to cover this with a def containers so the idea is that you have you have the deck prof file that you'll be using to deploy your code and then you attach it to your dev container so you have exactly the same environment for your visual studio code or pie charm and it's brilliant especially when you're a python library doesn't support m1 for example so i'm using this very often because some python library is still not build to mine architecture and and you can use def containers with a different base image to to overcome that yeah that makes sense i'm just making a quick note to look up dev container support for neovim and i'll move on from that cuz not usvs code well i'm not sure i i i but it's a quite a quite a u popular technology now so i would i would guess that would be some support and it's it's rather brilliant because you can even configure things like your addons in the in a in dev container config file so you can literally prepackage the environment for developer with everything they need and so they using it locally or in cloud like all the editor plugins yesly yeah that's nice yeah so what so what we what we working right now is that we going to have in a in our quick cli when you are developing python code you you would have a command and that would open the the visual studio code but everything thing you need to develop python based stream processing service including you know all the requirements all the python stuff all the python plugins in your visual studio code so you don't have to go to painful journey of of learning by yourself yeah that sounds nice but that seems like step one right if you would if that were just the problem you were solving you could have gone into the developer tooling business yes generally so what's your next step from that into making streaming specifically easier well to be fair i actually did so the first years of quicks were more focused on on tooling which we just discussed and yes the second part it's quite interesting you you kind of order it as it really happen the second step is the actual stream processing which is you know a second challenge so even if you have a tooling even if you get everything you can imagine to to develop comfortably your code and you have over side of the data and and you can deploy dey the code easily as well which like it was huge you know problem for us to to teach people how to use kubernetes so they are independent like you know you're either going to build a monster city internally so people can use kubernetes somehow obstructed or you going to teach them how to do it there's no idea solution yeah and then even if you give them that then you will find that they just can't or not can't but it's very hard to get the head around this new concept and so this is where the streaming data frames idea came to the place an attempt to bring a batch massa memory into streaming right data frames are a term i only really know from a cursory acquaintance with pandas right yeah you're treating it as more of a concept than a specific thing yes and to be honest we are not very first who had the idea to use the concept outside of the technology so a p spark has slightly similar approach to to batch and big you know spark data manipulation to give you a pandas interface to analyze your data but but actually under the hood is not pandas but it's spark and the the row is being redistributed in a spark cluster now that's what we did with streaming so you can think of topic in kafka as a lot of messages okay you like you're going to have thousands of messages in different streams stream being a message key so if if i if i give an example of a different uber car driving around the city now each driver going to send a message with the message key driver abc and the payload is going to be some data gps location longitud longitude speed etc yeah now you can think of that as that each stream is actually if you if you rotate it by 90 degrees it's an infinite table an infinite virtual table where each property of the message forming one column yeah so so imagine you have jason let's simplify this for jason there would be a property called speed and that property would repeat in every message of that stream of that one driver now what we doing is we we basically flopping it 90 degrees to a table where each message is one row and then you're working with the table like it will be static in your j notebook like it materialized but actually it didn't so let's say that you have a speed column and you just want to for practical reasons convert it from meters per second to kilometers per hour okay if you would have the static data in your jupit notebook we would just do a one line in pandas you know df name of new column equal df speed multiply by 3.6 okay and it will just add a new column to your static data now we do the same with this virtual table but the difference is that every time we get the new message which is the new row we execute this comm this this this function to add that new column which is actually in this case new cell and then we send it to output topic so we are doing the same thing is just we doing it for every message rather than the whole table right so i'm building up a mental image of an excel spreadsheet where inste i can scroll down and down and down and it keeps going and new stuff's coming in even as i scroll yes and and i would like to pretend i can just transform i can add a new column in my excel sheet once what you're doing is pretending is back back forward filling that every time a new row gets added you you add in that column that i defined earlier yes exactly so you when you you when you're writing your python code with streaming data frames you're actually not manipulating any data you're building a lazy loaded data pipeline which will know what to do when the first message arrives and then that instructions so you you building basically instructions with your b code that instruction will be executed for every message so you don't have to think about that you you know your your your brain doesn't have to uh kind of absorb the fact that data are flowing and they are causing this rather unnatural behaviors compared to badge but you can focus on the actual data and manipulation of it so for example u you know if if if i if i create a classical batch operation i load the data of uber driver i add the new column and i put it back to database yeah and basically what i'm doing is i loed all data i add the cell the column on and i save it once what i'm doing here is i have an input topic i'm getting these rows as they arrive to my transformation i'm adding the the column to each row in you know in independently and then i'm sending it to output topic where some somebody going to consume it equal is somebody going to consume the result of the budget work in the table so at the end you can sync that data pipeline to table as well and the only difference would be that the roles would be appearing real time rather than once per day or once per hour when the b happened but the result is the same and you are abstracted from as a developer from the drudgery of of of of thinking in this paradigma yeah i can imagine writing some code in python that look like i'm doing a for loop over a list and then some compiler magic transforming that into a bunch of yield statements which actually are processing it one by one as it comes in right yeah that's very similar yeah that's it's yes that that way of thinking yeah okay i'm happy with that i think this and and i could see how that would shield insulate the developer who's happy with batch from dealing with the streaming world surely this gets hard when we reintroduce things like state and windows yes having said that it's still you know it's not 100% obviously because for example with the windows there are more you know more windows types in streaming that makes sense in batch like for example a hopping window is a bit you know cont intuitive in a in a bbard but things like rolling windows or or tumbling windows there are still same in in jupit notebook or in in in streaming so you can think of like give me a last 10 seconds every speed now if you go you notebook you load your driver data and you you do that operation you would do df do rolling you know you put the parameters and it would add a column which would use the values from the cells above to calculate the value what we do instead is that as the rows are coming in we save them to the state and then every message coming afterwards would be a combination of the incoming message and the state to produce the output so then we kind of moving from iox to iob box plus state where the logic goes with the state as well okay let me see if i can push you further on this because i can see how you could library away compiler away some of the differences between streaming and batch does it not get does your abstract does your shield not break when you start doing things like streaming joins of potentially infinite sources of data yes obviously as further we go to a stream processing specific features yeah the the um it will go far bit from each other but still for joins you can join data in in a static environment you can join them in a streaming in streaming you have to provide more information to a behavior that you expect so for example how long you are willing to wait for your data now of course that's not happening in static because you have the data in your disposal but here you might won't be waiting 5 seconds for the second source to arrive and that slightly complicates the things but my thinking here is that it's a journey you know you go into stream processing and you're going to start with simple stuff then move to probably some stateful stuff the joints are really uh hopefully the thing where you know you can start with some defaults and then start tricking it still i think much simpler than using using just the the client client api okay so i think tell me if i'm right or wrong here but i think i'm going to characterize your approach as lots of people in this world are trying to say we need to make it seem familiar for people so we'll start with sql and that seems like a good choice but eventually you hit a ceiling where sql isn't powerful enough for the job and then you get bumped into java and you're saying we can still do the same trick of making it seem familiar but if we start with python eventually we'll have to introduce new concepts but we won't have to introduce a whole new language exactly yeah and we carrying all the the perks of a stream processing library with it which is native in the language so hopefully you know people can debug it to get bit more sense into it and and when they you know when they using when they building the logic it will be a bit less complicated to debug the problems so that's the idea behind it yeah and hopefully the mass memory from you know from from patch will carry on into the streaming yeah okay yeah i i quite like that cuz it's like saying you will have to learn new things but you don't have to learn a lot on day one to even get some results yeah yeah yeah and this was always our you know our way of thinking is to if you if there is a path to get you to something with where you see the value fast people will appreciate that because i always felt that when you have this intera you know when you're learning a new technology or new thing in general yeah having these ac points okay i know that i can do something and i feel enabled to do new thing because i just learned this new stuff it kind of drives you to and motivates you to go further when you have to spend a long time to get to the first anchor you you start d you know you you get the doubtful like am actually going to get death and maybe it's barely out yeah is it the library is it me is it time to move on either way yeah yeah exactly okay so let's dive into some code because i i can easily think of a language or two where i'd like a decent stream processing library that isn't java let me pick one i'm going to say gleam i want a stream processing library in gleam i'm feeling spicy tell me how and how much work i can expect to build a proper stream processing library in a new language give me some pointers basically first first thing you have to look at is what are the client libraries for the broker you want to choose so let's say you want to use kafka now if that language has a decent library for that techn for the for the for the broker that you want to use the one thing is thick it's it's a good start okay and then you going to have to learn a lot of concepts if you want to go to stream su stateful stream processing for stateless the you know it's it's more about packaging the whole kfka interface into more digestible more language specific interface or use case interface if you like yeah take take the cfa library and make it idiomatic for the for the language yeah okay but then when you go to stream stateful stream processing then it's a then is the a jour because you need to think about how you going to start your state how you going to use change l topics are you going to use state with checkpoints are you going to duplicate your state to do some resiliency in terms of database corruption and then you need to find a way am i going to use my broker my broker and possibly the deployment engine like kubernetes to scale my processing which is the client library approach or am i going to build a service engine that's going to manipulate my data and my code in a fashion to do that so you know they are both both directions has plus and minuses but this is this is the this you have to do and if you go with the streaming library like we did then yes you you're going to have to f you're going to have to use the broker scaling and resilien features to make this skillon resin so like retention res uh replication factor of the topics yeah the assignments of your partitions you probably going to have to use temporary topics for certain things like goodp buy so you know the way how how you can do grp bu with the streaming library is that you restream your data to a temporary topic and then by that you repartition it right if you do it with if you do it with a flink you will basically flink will take the data and partition it inside the engine but we don't have that engine so yeah so you have to build some kind of process job graph generator thing that's transparent to the end user yeah and well transparent but also extensible so i think that whatever whatever your buildin methods are rich or not rich in terms of what you offer and build in in any real application people going to have to build their user defined functions it's just you know i i just i don't see how any sql based system without udfs is any way useful it it needs at least the sql udfs if if anything because in real world you just need more flexibility and and so that has to be easy to do to do like to to build you know you are doing not mean but standard deviation or maybe you doing some you you know you're calling a machine learning model to give you some some estimate or or i don't know recently i was calculating which is quite interesting use case if you have gps coordinations of the car what is the distance traveled between the points oh yeah it's a function it's just mathematical function and it's a it's a function that gets it's one that's easy if you just want to do pythagoras and a bit more tasty if you want to consider the curvature of the earth yes right exactly yeah i i was thinking about that recently we're talking with a friend like the problem with the some of the sql engines maybe you can tell me how you solved this so the problem with some of the sql engines is you've got that ceiling where eventually you get kicked out into say java yeah and if you can define if you got user defined functions that ceiling goes a lot higher right there's a lot more you can do staying in sql but you you always have a problem when you let users run their own functions but you're trying to provide a cloud service like you can't let you can't let the public run arbitrary code but you need to let the public run arbitrary code how do you solve that yes well first of all i would say that the user define functions of seel move the ceiling higher also open the doors for monstrosities of it's i think that when people start to build a business logic in sequel very quickly it get very ugly and yeah and yeah i have i have quite experience with that in the beginning of my career actually yeah i may have committed a few sins myself in my past yeah i remember the tql times very well when people were creating at thousands lines long straight procedures and this is kind of lead to the same thing well yes so to to run a customer code it's a challenge now the way how we we have solved it is using a doan kubernetes and set the rules around it accordingly but yes you kind of opening opening the p's door with with that but i just yeah i i i don't see the way in limiting the framework the way how i see it is that people will just deploy dedicated instances of the thing for systems that that requires the extra the extra protection from any sort of you know separated infrastructure so don't do language level security do container level security yes because like i i just don't see how how a sql based transformation without gdfs not be any way useful because you're literally working with five or 10 or 15 functions at your that you have a new disposal and that's just not going to cut it to me yeah yeah yeah that's i i think we do sometimes in the streaming world underestimate the business value of just taking this schema and turning it into that schema of course but yeah but as soon as that becomes day-to-day work the more interesting work it it's like being given a programming language where you can only use the standard library yeah there's a lot of you can do but if you can't define your own stuff very limited yeah and well yes and also you know the standard language libraries are usually much richer than the yeah the fink s the flink cle is quite rich full because the library is and the engine is very old but still it's yeah still admitted the nice thing about sql is it's very accessible and even non-programmers might well know it but i think that's becoming kind of true of python too yes having said that we also planning to build a sql layer on top of our python library because well it's not that difficult to be honest and it has a value it's opening the door even more for a bigger pool of people but yes i think the python well it's you know the python is most used language in the world and there are reasons for that i think it's it's probably easier to to to to to learn than the others although python wasn't my first language to learn so i i kind of never had the opportunity to learn the python first and i think second is the ecosystem like right now if you look at the world right now all the interesting stuff all the all the you know all the thing around llm all the all the interesting innovations is somehow happening in in a pikon ecosystem and and that's i think what fueling the the user base of of of the python yeah it it's the the fact it's becoming the def facto language for people getting into programming and for data manipulation experts right yeah yeah it does yeah yeah and and i would argue that kind of if you're starting in the ground zero is it is it going to be sql that much easier than the python if you you know because to me the secret problem is that lack of autocomplete in most cases some some databases have autocomplete and it's great but it's always kind of fiddly and if if if you if you if you start the python right and you get a good content i don't think it's much more complicated but then the the power you get with it is so much higher yeah yeah again that ceiling is i mean it's a general purpose language right yeah okay so are you at all tempted on that logic you know how to do it you you're looking at making it accessible to more people are you at all tempted to say our next target is the javascript world or something like that we're we're constantly debating this internally how we going to what we going to do here we have focused on one thing because as a as as a comp as a company our stage we have to focus our resources and to be fair building a because we not building just the library we building the whole cloud platform to to help to help you stream process to to help you process streaming data it's such a challenging engineering task that even with the focus to python it's still too [music] big we we will we will look into more languages later that's we we we we will do that but now we're trying to solve the python okay so if if i start my gleam library you're not going to feel too threatened at this stage no no not with the gleam but yeah there are there are other languages underserved by streaming texte like you know my you know my old net which i started my career on it's very underserved in it's exactly the same problem like in python like there's no difference in the problem it's just that maybe the python has more users but the net is super underserved the the the client library to kafka is very suboptimal i would say diplomatic way of putting it it's surprising because c is so spiritually connected to java you would have thought that it would already have pretty good support yeah and yeah and there will be a something like a kafka streams very well supported by some big company but it isn't there's some one attempt some open source attempt which i haven't tried but yeah it's it's it's people are left in a suboptimal place to to use gafka and other technologies in that stack okay well so let's just go on to a slightly different topic which which relates to you dividing your attention as a small company right you've got a cloud platform and i'm going to say the the user interface for that for building streaming pipelines is one of the nicest i've seen so give me some insights on how we make this whole technology presentable to people in the world not not just accessible to programmers but like the art of user interface design for a new area is a tricky one yeah i think one of the most important ux element that we have done is our pipeline view which kind of help you to just get your mental thoughts straight in what you're actually building and what is actually built by others so like even people that are not programmers but going to be maybe using this pipeline maybe they will use the result of the pipeline somehow to integrate it or they just need to understand what the team is building with this pipeline view you you can visualize it in your hat just getting a bunch of services that maybe have some environment variables in them that's very difficult i think the second this the second thing we are now trying to kind of internally do is to serve this technology to different personas because they have very different way how they want to consume the data and how how they want to use it to their goals so you have a software engineer which probably want to use full power of microservice full power of local ide and for power of the dependency management that that provides versus a maybe a python analyst analyst using python that wants to just do a simple analysis on data and just write a couple lines of python to manipulate data to do the windows to do filtering aggregations and maybe they're not interested in in using you know the full-fledged programming experience there so this is where kind of we're thinking on that front okay is this as i always think the you get a data science job and you think you're going to be doing data science you actually spend 80% of your time cleaning up data are you looking into this idea of like what's the word data mesh like data cataloges like we can build we can easily build something that would do the cleaning of data and then present that as a user interface like heo or options for data sources within the company yes so i think that with this with with with this approach first of all you don't end up having such a unreadable data data that's that's a number one thing because you can you know you can you can branch the data as they come into the system and then save them in a nice way so you kind of you kind of moving the responsibility of do something with data before they saved versus after they saved okay which i think is very powerful because the natural thinking is i'm getting i'm getting this data let's dump it to s3 yeah and or similar style of database storage and then you know and then you have this 80% of the time they trying to do something with it yeah well you can actually you know still dump it to s3 because that still you know pointing that but then you can build a branches and say okay there is a bit of time series data well let's save it to influx and here we have some data for our vect database and maybe here a metadata for database or similar style document store could be useful and and then if you enable them to do that rather than some integration engineer that's actually getting data in then they can actually leverage it so that's that's number one thing and in terms of the the resources i think that it's just about getting companies to the better being in a better game in terms of data like how you treat your data and where you are in that game this is just one step it's not everything you know there are more things to to improve that but it's just one step okay that makes you think of one more thing we haven't really touched on and it's it's not a natural fit for the python world but data schemers types is that part of the puzzle that you' thought about no it's a very good question actually we going to integrate with schem magistry we haven't yet and i i i tell you kind of the the reason other libraries and other engines are strictly typed and they strictly require you to get a certain schema in the topic because the way how they surface the data they surface the data as objects and and for that you need the types now we surface data as data frames tables with columns and and and and they are they don't require such a strong schema although at a certain point and there are definitely use cases where where schem integration makes total sense for validation and for for kind of the whole companywide orchestration of your schemas but you it's not mandatory so you can still equally as you would do in panda in jit notebook you can you can check are these columns present in my data and maybe fill the columns that are missing with something so you know people in jus are not used to work with schistes i can believe that yeah i can believe that most python programmers are happy with dynamic typing sure exactly but the reason why we planning to integrate it is that not this is not 100% true and also it helps with some you know things like data visualization and etc when when you kind of provide some metadata for data that you're sending but on the other hand it also bringing a lots of complications in other use cases so that's why we definitely never going to make a mandatory so you know imagine you getting data from i device that have a different sensors sensing in a different frequency yeah now what you're going to get is for example geforce data 20 times per second and gps data once per second now are you going to send it into topics that going to make your life very hard so then what you have to do is create a schema that contains both and then you're just sending unnecessary syntax sugar around or encoding sugar around or you're going to send messages as as you want and then you have a processing in a pipeline that just join the data when you need it but not join the data when you don't need it so you don't need to join data to sync it to your database but maybe you need to join the data if you need to reference both columns in your processing logic okay okay i can see that as a solution we are gradually going full circle here but i'm now thinking about error handling now i'm not a i'm not a python data frames expert so correct me if i'm wrong but i believe when you're dealing with like you deal with data you've got a million rows of data you calculate something all the data is there so it works or it doesn't in the streaming world you can have code tested on a million rows of data and then the third millionth row turns out that number isn't a number sometimes it's a string how do you deal with the error handling when when you can't process your errors in a batch there are two type of use cases which requires a different way of handling these situations so either you are in a situation where and that's usually when it's stateless operations to be honest but where you rather get data continuously and you rather not stop your processing and then you probably send the messages that failed to a dead letter queue of some sort basically topic which will hold the messages that wasn't pro processed properly and the second use case is where actually it's a you know it's a stateful operation where missing one message could makes everything completely incorrect yeah and that is by the way a default behavior of our library and it's that the way how it would work is that when you reach to the point where there is something like that that there ra an exception the code will stop that restart the service to the last checkpoint and try it again because maybe you know it was some environment factor in it and if it's not if it's really just your code the pro steer processing will stop there until you fix it until you deploy a fix in your code it will be basically stopped at that checkpoint and it's it's fine because you have the the storage of topic and and you have the checkpoint saved and you have your say state saved to that checkpoint so moment you update your code to fix you know maybe you have not expected the date in that form or maybe you have not expected that this colum might be missing then you can continue your stream processing and that's a different this is a second yeah second scenario really okay yeah that makes sense and is there dead letter q scenario is that something i would just handle as ordinary python code cat save it off yeah yeah yeah yeah it is it is like you know imagine you you you have very stateless processing where the fact that one message in a 100 has a corrupted schema or corrupted data in it doesn't necessarily means that 99 messages has to stop working as well but if you have a problem with you know a balance calculation on your bank account yeah now it's probably better to stop yeah and that's definitely a domain specific thing like how important is it to process every single row yeah yeah that's totally makes sense okay so i think it's almost time for me to go and build my gleam based competitor i better go and have a look at quick streams first on my local dev container how do i get started so the first thing you just do a bip install quick streams which is part of the beauty and then on our website we even have a code sample with a public source so you can literally play with that and and then when you are ready you can sign up for our cloud and actually start building the pipeline but yeah the easiest way is to literally do pip in s quick streams and get that code sample from our website it's doing some window and you can change it to do whatever you want kick the tires on it and see if it's a good fit yeah exactly yeah cool okay okay great i will go and do that and then i'll go and speak to my gleam expert team and you'll see me competing with you two years from now thomas great to talk to you it was great to talk to you as well and thank you very much for having me and pleasure yeah good good luck see you the next time you are in london yeah see you see you next time cheers well thank you thomas and i'll tell you that i genuinely have started scratching out a gleam library for this but it's going to be a very very long time before it's anything more than my little toy playground unless for some reason you want to invest in which case we're weeks away from launch provided we can secure the right level of funding while you're holding your breath i'm waiting for that li don't hold your breath while you're waiting for that library if you want to kick the tires on quicks your best bet is probably pip install qux hyphen streams and then check the links in the show notes for documentation one quick announcement before i go if you're a regular listener thank you for joining me regularly there won't be an episode next week we'll be back in a fortnite that gives you a little extra time to like rate or share this episode please do please take a moment and if you're not a regular listener and you're not already subscribed do click on subscribe and join me in a fortnite for another developer voice until then i've been your host chris jenkins this has been developer voices with thomas newbower thanks for listening for