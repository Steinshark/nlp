so we're going to talk about sort of an entry level clustering approach called k-means now k-means comes up a lot in other fields so machine learning uses k-means quite a lot it's what we call an unsupervised clustering method often what you have is a situation where you have some training [data] but you already know what it is, and then you try and teach your network to find the same thing again so we've got labeled training data in k-means what we've got is just some data and we say split that into three please i'll start by showing a very simple overview here of how caimans works if we imagine we've got some some data if it's grouped up so i'm going to do some x's here and some x's here, so if we wanted to split this data up we have no idea about this data to my eye it looks like there are two clusters here right partly because i cheated and drew two clusters, but you know so if we were giving this data, which is in two dimensions to a machine and said cluster this into two what would it do basically is a question there's lots of different approaches k-means is just one of these approaches i'm going to show you today it's [cayley] like just a variable or yeah k is a variable we input at the beginning so if it's two we're going to split this into two if it's five we're going to split this into five which will kind of be over splitting this arguably, but it depends when images come in then you might imagine splitting it into 256 so we can turn it into a 256-color palette image as an example okay, so the number of k is very much dependent on the situation you're looking at so how do we do this? well? what we do is. we have k averages for this data so k-means that's how it works so i've got myself a couple of squares of paper here, [but] i'm going to use with my mean position so i'm going to have this as mean position one and this is mean position two now if i was to calculate the mean position of all of this data this one is going to be somewhere in the middle and what k-means is going to do is partition this into two and then calculate the means and then we partition it based on those means and try and iteratively work out where the ideal means should be so let's start number one over here and let's start number two over here we want to partition this data into these groups it's probably going to put a partition somewhere around here, and maybe this is going to be in group two so these these will be decided you know depending on which their nearest to and so that's our initial segmentation which is pretty poor because put a line straight through the middle of this data, and it's no it's not really any good but it's a start and so then what we do is. we say what we've got all of these in group one so what is the actual average position of these and maybe this one's in group one as well? so it remember moves just down there a little bit. just just tweaks a little bit now this one it's got quite a lot of these in it, so this is going to come up a little bit so that's step one, right? so we partition them into these two means and then we move the means a little bit based on how these new partitions. [have] formed for now. let's assume we've picked one [and] two at [random]. we might talk a bit about how you initialize them in a minute, but for step two we do the exactly same [thing] again so we say well now look the data has changed somewhat okay, so i'm gonna use [my] green pen now so maybe these ones are now closest to one and these ones are now closest to two so we're getting there, okay? so then we reclassify them and we compute the means again and [to] comes up here and one comes a bit down here and then we do it again and two comes over here and one comes over here and gradually we settle on the optimal mean position for our groups, okay? and then what that finally means is we put a big nice line between these two bits of data, okay? which is exactly what we wanted is it possible they could get it completely wrong good question yes so absolutely right? so if i was if i was putting in one and two at random [so] for example if i put one and two over here okay, you might imagine a [situation] where if we're drawing a line down like this. they're kind of evenly distributed there's no real pull [right] away, and they just kind of get stuck there okay, so that could happen all right? so often what we might do is run k-means a few times with different starting positions, and then pick the best one okay? pick the best one as in each of the ones in this cluster a nearer to one than they would be in this situation what we sometimes do is instead of picking these at random because you know if i put over? here that's not hugely helpful. it's just going to take longer to converge on a solution what we sometimes do is pick two points as our starting positions so i could pick a point here and a point here [now]. [that's] not going to [necessarily] completely solve the problem you know [if] you pick really bad points that might be a problem, but on average it's going to work out okay, okay? there are other there are other initialization methods like amy's plus plus and things like this you can read about that do slightly more complex things but the very general idea is we have an guess how to separate our data we separate it and then we calculate the centers of those regions and then we repeat that process to try and converge on a good separation and k-means is very effective. you know it's simple really simple two steps basically move these points into one of the two classes and then we compute the means and just do that over and over again now this is [two-dimensional] data x and y, but there's no reason it couldn't be free or for five dimensional data right which i can't draw a five dimensional than what that's called, but you know a five dimensional object here on the paper i could barely draw a three dimensional one so but in in an image of course we've usually got three dimensions [rg] and b so what we have is we [have] one mean for the red position and one mean for the blue position and one mean for the green position and we're trying to move around these in this color space trying to find what are the dominant colors so k-means on an image will not only tell you? which pixels belong to which of the three classes or four classes or five classes? it'll also tell you what's the average color of those classes, so [then] we can simplify our image so if you wanted to compress an image for example and change it to say [sixteen] colors right then you would [just] split it into k clusters. where k is 16 and then those dominant colors are what you're going to pick and it will look kind of like the original image not great but you know people are used to seeing compressed images like this you know on the internet so let's look at some moods and see what it does you could pick any initial image to do this to my eye there's maybe three or four dominant colors here. there's green obviously blue and black and to a lesser extent white i suppose because of these clouds or gray what we will do is we will pick three pixels of random okay, and they will be the initial values for our means okay, so let's imagine i'm splitting this image into three because i think maybe there are three dominant colors so i pick i have three means instead of just number one i have [a] two and a three they get started at random with random rgb values and i cluster the whole image into those regions one two or three then i recompute these mean values and i cluster again, and i recompute [the] mean values [in] my cluster again and this is what happens on this image for a k of three so we've got the black or very dark green down here [like] green gray blue sky so that's done. exactly what we hoped. it would do okay it split the image into three if we start to increase the amount of classes, we can slowly start to improve the image and maybe start to get towards what the original image actually look like this is eight classes you can see that now. we're starting to see what we were looking at before there's now a difference between the cloud in the sky and quite a lot of difference now on these bushes here, okay? and as we go up it gets better and better [now] on 256 colors we've had a problem here [with] some of these have [been] put into a weird cluster but that's just what happens sometimes with k-means do we initialize it? but you can see that actually the [sky] [is] now looking quite a lot like it did originally because we've got lots of different colors that we can represent it in terms of image processing we might segment this image to try and find the dominant objects in this image, it's not hugely helpful because even in even with a few classes. we've got objects all over place we can't for example pick out the trees particularly well because the trees are the same color as a grass and the same [colors] [as] [brees] bushes here. so doesn't really help us, but it depends on the image you're using if there was a red bus and nothing else in the image was red we could pick that class out nicely so it depends on the situation going forward. they're [a] much more complicated segmentation approach it. so things like super pixels that we can talk about another time they're trying group coherent regions of the image locally so they're bringing spatial information into it as well which makes a lot more sense because our bus isn't going to be distributed in the red throughout? the image is going to be in a box so we can start to look for things like that. i did particular implementation in matlab because matlab can do this in [about] five six lines of code? we can make that available in the comments so if you want to see the matlab code that does this it uses the inbuilt k-means function of matlab so i didn't have to work too hard to get it to work, and if you haven't got a matlab license octaver also do this using the same code so you can have a go?