so now we're going to talk about something that is kind of a specific part of big data so the velocity part huge amounts of data being generated all the time, which essentially is a data stream so that's a flow of instances so you could have a flow of images coming in have a flow video coming in or just a flow of essentially lines to go into a database the thing about the dynamic data is that the patterns within it can change so if we've got for example a static machine learning model? that's not going to deal very well with a changing pattern happening in the data we build a single model at the start. we use it to make predictions on later data the model accuracy can kind of degenerate over time as that data changes the problem of kind of designing algorithms to deal with this real time data there's been a research topic for kind of several years now and there's several real world applications on top of that as well so if you think about banks trying to detect fraud as patterns change of different forwards occurring they want their models to kind of be able to update all the time similar for intrusion detection systems and computer networks they want to be able to update and keep on top of what is happening ideally, you would want this to happen automatically so minimum interference from humans, because otherwise they've got to spot when changes are happening we just want the machines to be able to do it by themselves so if you think about a traditional classification problem on a static batch of data you assume you have all of that data there already. you have your training test set and you have instances with features which x and then there's some unknown function f of x which gives you the class label and you want to find a hypothesis that gives you the best prediction possible so what kind of approximates this function as well as possible? so you have a red class and a green class and we have instances that look like this our function f of x may create a class boundary that looks like this. so anything on this side is red. anything on this side is green our model doesn't know that but we use standard machine learning techniques decision trees new or networks whatever you want and it learns a boundary that looks like that and so that will do okay on whatever dates that we have it's not effect, but it may get the results that we want. this is static classifications. we already have all our data so we've got our data we've done our machine learning this is the decision boundary that we've learnt. the dotted line is what is actually the boundary this gives. okay results let's now say that this is happening in a data stream. so we get this data originally and we build this model but then later on we have a similar distribution of instance arriving however, what now happens is that some of these instances are now in reality in a different class so the true boundary is now here, but we still have our model with this decision boundary and so we're now predicting instances here and here into the wrong class if we use that exact same model. so what we would see in this case in centage accuracy over time you would see at this change point accuracy would plummet. so this problem here is called real concept drift. what is effectively happened here is that this function the unknown function has changed but we've kept our hypothesis our machine learning model exactly the same and so it starts to perform badly we can also have a similar problem called virtual drift and what would happen in this case is that the target decision boundary has stayed the same from this original but the instances we now see in the stream are somewhere else in the feature space. let's say we now see data like this so though the kind of optimal decision boundary is in exactly the same place. we now have different data. that means that are predicted boundary it's going to give this instance as wrong because we haven't got a way of incorporating information from this instance into the original model that we built both of these will create this decrease in accuracy so we can also look at the drift in the data streams in terms of the speed they happen so something that would give us an accuracy plot that looks like this is called sudden drift we go from straight from one concept in the data stream so one decision boundary straight to another one another possible thing that could happen is that our accuracy looks like this? so rather than this sudden switch this decision boundary gradually shifts save me your life if we're looking at a very very oversimplified intrusion detection system. we have only two features that we're looking at in the original dataset anything with these features, this is a security problem and intrusion anything on this side is good in this case what happens is that suddenly there's a new way of attacking the network and so suddenly what was here is now not good. so we see those patterns and we say ok no, that counts as an intrusion in this case what it means is that we see something that we've not seen before so the model hasn't been trained with any similar data and so it could get it, right it could fall somewhere up here and we correctly say this is bad but it could also fall in an area that we didn't learn the decision boundary so well, so yeah, we get that prediction wrong. we just looked at what? the problems are with using a single static model when we're dealing with incoming data over time the distribution changes and we start to see a decrease in accuracy on whatever model we built so what happens in kind of a stream machine learning algorithm would be so first of all you've got x arriving. this is your instance in our previous example, this would just have two values associated with it what would first happen is we make a prediction? so in the classification example, we classify this. yes it's an intrusion. no, it's not intrusion using the current model that we have then what happens is we update whatever model we have using information from x and we'll talk about some of the ways that this is done in a second and one of the kind of caveats with stream machine learning is that you need for this to happen you? need to have the real class label if you're doing classification so in order to incorporate information from this instance into whatever model you've got you need to have that label there now in some cases it's very easy to say we've seen this data. this is what it's classified us and we do that immediately if we're thinking about making weather predictions we can almost immediately say yes. this is what the weather is like it may be a day's delay but yeah, we can that's pretty immediate thing four things for example for detection you may see a pattern of data you may predict it is not being fought and then suddenly two days later this person figures out that actually there's something wrong with their bank accounts they phone up and it does turn out to be fraud and so we'd only have the label for that data after that has happened the final bit is to update the model at this point and so the goal of updating the model over time is so that rather than having a performance plot that looks like this so we go from 95s and accuracy down to 20% accuracy we instead end up with something that okay we may drift a little bit here and have a tiny performance decrease but the model should very quickly recover back to the original level and we still have a high performance so that's the goal of this model update. there's various approaches we can take so the first one is explicit drift handling which means that we first of all detect when a drift happens in the data stream so to do that we have drift detection methods and these are usually statistical tests that look at some aspects of the data arriving so if the distribution of the data we see arriving and the distribution of the classes we see is changing if morph like that as a drift some of these we'll also look at the performance accuracy of the classifier so if the classifier performance suddenly drops we can say well, we've probably got a drift here we need to do something to the model to mitigate this who spots that though? is it, you know, is there an algorithm that actually spots that something's different to what it should be yes, so there are various statistical tests that will do this that will kind of just measure things like the mean of the data arriving and be able to spot things that have changed basically so yeah, once we detected that a drift has happened we then want to take some action. the first thing that we could do is we could do a complete replacement of the model so we get rid of whatever model we had before and we we have taken chunk of recent data and we retrain the model on that and continue using that for predictions until we've hit another drift this is okay. but it means that we could be getting rid of some information in the previous model that is maybe still going to be useful in the future so then there are also methods that we'll look at specific parts of the model and say okay this specific part of it is causing a performance decrease. so let's get rid of this we can then learn from new instances something to replace this that will do it better basically so if you think of a decision tree if you can detect that there are certain branches in that decision tree that are no longer making good predictions you can get rid of them and we grow the tree to perform better prune it. yeah, exactly it is called pruning. you prune. yeah, you prune the branches off the tree there are no longer performing as you want them to the alternative to explicit handling is to do implicit drift handling so rather than looking at the data or looking at the performance and saying something has changed we need to take action we're just continually taking action. there are various approaches to implicit drift handling so the first and probably most simple one is to use a sliding window so if we imagine we have the data stream with instances arriving like this we could say we have a sliding window of three instances and we learn a model off of them. we then take the next three learn a model off of them. so as each instance arrives we get rid of the oldest instance and this makes the assumption that the oldest instances are the least relevant. this is usually the case it's kind of a valid assumption to make so this performs okay the problem with this though is that it kind of provides a crisp cut off points every instance within this window is treated with exactly the same kind of impacts on the classifier. they were weighted the same so we can introduce instance weighting so that older instances will have a lower weight their impact on the classifier will be less so again, the more recent instances will be have the largest impact on the current model and then again these algorithms that we'll use instance weighting will usually have some threshold. so once the weight gets below a certain point they say that's the instance gone we delete it presumably the windows can be larger or smaller yes, so setting the window size is a pretty important parameter if you have a window, that is too large then okay, you're getting a lot of data to construct your model from which is good and cents between learning more data usually good what it also means is that if there's very short-term drifts so this drift happens and then we don't learn from that drift if that makes sense because we see that all as one chunk of the data again if you didn't set the window to be too small we can react very well to very short-term drifts in the stream but you then have a very limited amount of data to work on to construct the model so there are methods that will automatically adjust the window size. so during times of drift the window size will get smaller so we want to be very rapidly changing the model and then during times when everything is kind of very stable the window will grow to be as large as possible so that we can use as much data to construct this model as possible so the problem weird sliding windows and instance weighting is that you need all of those instances available to construct the model continuously. so every time you add a new instance and delete another one you need to reconstruct that model and so the way we can get around this is by using single pass algorithms so we see each instance once use it to update the model and then get rid of that instance it's probably still in long-term permanent storage, but in terms of what is being accessed to construct this algorithm it's gone now in that respect then you've got information out of the instance, but you don't need the instance itself. yeah, exactly so we see the instance we incorporate what we can from it into the current model we get rid of it and that instances impact is still in the model an example would be a decision tree so decision trees are kind of constructed by splitting nodes where we're going to get a lot of information gained from making a split on a certain attribute so as the data stream changes the information gained that we might get and some of these nodes may change so if we say get a new instance and it will say okay now this actually makes this a split worth making we can make that split continue growing the tree and then that instance can go we don't need it anymore but we still have the information from it in our model so we've got our implicit and explicit drift handling appro. you can also have hybrids approaches so the explicit drift handling is very good at spotting sudden drift. so anytime there's a sudden change there'll be a sudden drop in performance that's very easy to pick up on with a simple statistical test but when we then add in the implicit drift handling on top of that it means that we can also deal very well with gradual drift so gradual drift is a bit more difficult to identify simply because if you look at the previous instance or like say that 10 previous instances with a gradual drift, you're not going to see a significant change so it's a lot harder to detect by combining the implicit and explicit drift timing methods we end up with a performance plot. that would look something like this we maintain pretty good performance for the entire duration of the data that's arriving the problems of a changing data distribution and not the only problems with streams and so if you can imagine a very high volume stream and high-speed got a lot of data arriving in a very short amount of time if you take a single instance of that data stream and it takes you like five seconds to process it but in that 5 seconds, you've had 10 more instances arrive. you're going to get a battery of instances very very quickly so you need to be the model update stage needs to be very quick to avoid getting any backlog. the second problem is that with? these algorithms we're not going to have the entire history of the stream available to create the current model so the models need to be for example the single path algorithms that can say we don't need the historical data that we have the information we need from it but we don't need to access these because otherwise, you just end up with huge huge data sets having to be used to create these models all the time and again these streams of potentially infinite we don't know when they're going to end and we don't know how much data they're going to end up containing most of the kind of and well-known machine learning algorithms have been adapted in various ways to be suitable for streams so they now include update mechanisms. so they're more dynamic methods. so this includes but decision trees neural networks k nearest neighbors. there's also clustering algorithms have also been adapted. so basically any classic algorithm you can think of there's multiple streaming versions of it now. so if you are interested in these streaming algorithms there's a few bits of software that you could look at for example, there's the mower suite of algorithms which interfaces with the worker data mining tool kit this is free to download and use and includes implementations of a lot of popular streaming algorithms it also includes ways to synthesize data streams so generate essentially a stream of data that you can then run the algorithms on and you can control the amount of drift that you get how certain it is and things like that and that's quite good to play around with to see the effects that different kinds of drift can have on accuracy in terms of big data streams specifically there's software such as the spark streaming module for apache spark well there's also the more recent apache flink that are designed to process very high volume data streams very quickly you just mentioned some yourself where people can download and have a play with but i mean in the real world as an industry and websites and things that services that we use every day he was using these streaming algorithms. and so a lot of the big companies or most companies to be honest will be generating data constantly that they want to model. so for example amazon recommendations like what to watch next what to buy next they want to understand changing patterns so that they can keep updating whatever model they have to get the best recommendations again optimizing ads to suggest based on whatever searching history you have that's another thing that is being done via this. so yeah, there are a lot of real-world applications for this stuff now i've got the token so i can load a value in add the value emerged or into it and store it back and hand and now i've got the token again i can load something into its my register you and do the computation split across those machines so rather than having one computer going through i don't know a billion database records. you can have each computer going through