i have not actually i have actually not seen this video in my last video i introduced the two major bottom necks that slowly program down okay compute bond and memory bond y at the end of the video i mentioned the specific compute bondy task which is general matrix matrix multiplication i remember this video i think we watch operation despite its apparent simplicity can be remarkably challenging to optimize efficiently in today's episode we're going to walk through some genius algorithms to make it over 100 times faster all achieved through pure cpu optimiz techniques leveraging seemd and cash strategies okay some of these techniques might seem counterintuitive at first class but they'll start to make perfect sense as we delve deeper into the underlying reasons we'll begin by implementing a vanilla jam algorithm the straightforward approach involves a triple for loop structure to compute the dot product for each entry in the matrices this is code that i let co-pilot right now what was it it must have been like gosh two years ago i wrote matrix multiplication on stream it just you know writing this thing just takes so much it's just the world's worst thing to write here we simplify the input to be square matrices of size mat sides yep the performance of this basic implementation is notably slow y taking approximately 2 seconds to process matrices of size 1k by 1k of course we're not even using ming right nowon i mean it seems right because it's n cubed right and so being a square matrix that' be a th time a th time a thousand oh that's a billion that's a billion i'm pretty good at math okay i'm really good at the maths okay just use an npm package to do that sim d's nuts matrix multiplication npm package just install it in quick maths we've already accelerated the orm by a factor of 10 oh this is because my cpu boasts an a core 16 threat configuration however despite this improvement important question right now is is it really fast enough yeah we are going to optimize in when should we be satisfied to understand this question we have to do the math first okay not a big specifications my cpu has a maximum compute bandwidth of 600 g flops the memory bandwidth limitations stand that's a lot of flops i'm not going to lie to you i i don't i don't think i have that many flops does does an average man have this kind of many flops or is this like an unusual amount of g- flops it's at approximately 46 gigabytes per second h however keep in mind that memory bwi isn't solely dictated by your cpu peripheral factors including lower memory frequencies and under utiliz the memory channels can also decrease your practical memory bandwidth to test the memory bandwidth in the production environment return to the stream benchmark i'm never going i i'm not going to lie to you guys i've never been to this level of optimization you know what i mean i've never had to optimize here like i've never hit a memory bus problem right skip skill issue hard skill issue okay my my my flops are way too floppy for this you never used npm fair okay maybe maybe i have maybe maybe i have ran into it maybe i've only been subjected to the memory problem but i've never actually done the memory problem according to this benchmark result my system's practical memory band with sealing hovers around 21 gb per second equivalent to 5.25 g flops it's a lot the next question is we all know that j relating memory to gig flops is that normal that seems confusing cuz it's a floating point operation but what does that mean in is that just saying that it can only you can only send through the results of so many flops it's not really a comparison i was am i am i the only one bandwidth limits g floppies i can't believe i just read that phrase out loud but i guess we're gna be calling them g floppies from here on out yes the g does stand for giga chat flops will eventually become compute bonded from the last video okay but when the answer theoretically isn't overly complex since we can find out the total number of memory access to be s and squared and the total number of floating point operations required is to enced gem should shift from b memory bound to compute bound when n exceeds 380 but here's the problem at a resolution of 1k by 1k both the compute band ws and memory band ws fail to reach their full potential this is because we have a relatively low cash heit rate within our naiv implementation not that each entry in the matrix is a and b needs to be accessed multiple times but if they don't reside in the cache our memory access comp can get close to 3m cub rather than the expected 3 squ this complication can be problematic when n is large as the cach is less likely to keep the data when the same entry is accessed the next time however if you have a relatively small matrix there's even a pretty high chance the entire matrix can be accommodated within a large cach so increasing the cash heit rate huh where should we start introducing the first trick transposing one of the matrices from row major to column major okay i've heard about this this is that just i assume you don't act do you do you transpose the matrix or do you just walk it differently cuz is actually coping over the matrix to a new piece of memory really the way you do this or is it actually just just walking you're saying yes transpose yes so you actually you you do it you do the transposition on the fly or do you actually copy it yes to the copy okay the small adjustment makes our algorthm run three times faster but you might be wondering are we doing more computation when transposing the matrix well this bring us through the two principles of cach optimization maximizing spatial locality and temporal locality in our original jam implementation both of these aspects are suboptimal yeah you see in the vanilla gem both matrices a and b are stored in row major order however when performing the matrix multiplication matrix is access the longest columns this means that the alm needs to skip an entire row to access the next value yeah but wait a second it's not like truly random memory access so a mod cash system can handle this pattern right and you' be correct actually your cpu likely employs prefacing as long as you have a stable memory access pattern like this the problem is that you don't catch a single float when you access the value but catch an entire cash line the way you access matrix b results in a substantial amount of cash being brought in but not fully utilized therefore the low spatial locality in this this case iss to inefficient cach line utilization transposing the matrix spe despite the more computation and even memory access overhead in curse significantly improve the cach line utilization and therefore increase the overall at the end of the day so we can improve the spatial locality by transposing the matrix what about temporal locality to improve that's a great that's a great visualization for showing you because you just will always want to walk it linearly right like that's why they that's why i mean one of the most common ways to increase performance in the most simplest way is that if you have a set of like 10 items 15 items you don't use a set even if you're removing an adding instead you just put them into an array and yeah when you remove you have to move everything back but it just makes it so that you get this nice tight array where everything's located that you can walk really you know swiftly and even that adjusting is still better plus you have whatever the complication is of the hashing factor right so there's a i mean these type of improvements are are wild right they're just not something that i think the average programmer thinks about but they do exist and there are really good practical implications for them which is really nice power locality we must first understand what cause it to be suboptimal in the context of our matrix multiplication every entry in matrices a and b needs to be accessed multiple times however when dealing with large matrices holding an entire row in the cach can be challenging so how can we make this memory access more efficient here comes the magic of linear algebra each entry in the result matrix ci equals to the sum of a k times bkj for k from z to n classic but if we divide the matrix i have never heard someone say that in such a swift way i mean he just tossed out that quick math so quick and mathy like that was just like the most natural it was that was it was it was beautiful out blocks the entry is still equal to the of multiplication within each block and when you really think about it you'll find we are actually doing matrix multiplication in blocks each block multiplication is essentially another smaller matrix multiplication why is this useful well imagine if we choose a block size that's small enough to fe entirely inside the cache ideally by doing so we only need to access each entry in the matrix l times instead of n times take a look at this code snippet while it appear to have more nasty loops the total number of floating point operations remain unchanged what we've achieved is a significant improve in the memory access pattern resulting in a better temporal locality by breaking down the matrix multiplication into smaller and cach friendly blocks we can maximize the reuse of data storing in cache thereby accelerating our gem algorithm that's enough for the concept let's turn our attention to the practical side performance sadly using blocked jam can be a little bit more challenging than the transposed gem since you need to decide what block size to use you also have five for loops it's not just that you have to decide the you got all five for loops really easy to screw up there remember the actual number of memory accesses is the product of total size of two matrices q and squ and the number of memory access for each entry l choosing a small block size will resting a large block number l therefore the total number of memory access to and square l can approach the the worst case memory access time to enclude okay on the other hand selecting a block size that's too large to be inside a cache may negate the advantage of using blocks all together in practice different machines may prefer different block sizes okay and the only way to find the best block size is through experimentation another important difference between the block jam and transposed gam is well we don't need to transpose anymore our goal is to feed the entire block inside a cache moreover people often tend to choose block sizes that are in multiple of cash line so transposing the matrix won't improve the performance right let's transpose it to see what happened anyway i have never had to optimize something to this level when i look at this this just looks like i mean none of it is crazy surprising right like we all know that you want to exist in in in as small you know you want to exist in the most memory efficient way but to actually like i've never had anything at my job where this is something i've had had to do right i've never had a locality problem and so it's it's very interesting i mean this this seems like this is like the game engine work in my head this is what i assume this is like this is ecs stuff this is where like the real optimization happens the things that you don't actually really think about a lot it reminds me also of like it probably this is probably in some this type of optimization this like making things closer is also like the sme optimization in v8 where they they do small integers so if you have an array of integers that are small you literally have an array of integers in in v8 which is just going to allow for pretty fast access whereas you know in the oldie days it would have to go to each integer and then hop to each heap offset where they stored it in the heap to be cleaned up now it doesn't have to do that you know so there's like a little bit of little something there yeah i assume it's also with any of the any of the amazings ml stuff ml is just one gigantic linear equation continuously running mlps are just they're literally just doing sweet sweet mlp stuff performance increased again that's a little bit unexpected as i mentioned earlier transposing metrix speed doesn't fundamentally change the cach line utilization instead he improved the performance by affecting another parallel mechanism seemd these nuts you might have notice that i added an open mpcd macro just before the innermost loop the seemingly small addition enable the compiler to leverage cd instructions for do product calculations within the blocks oh now if you've been following my previous videos you are probably familiar with sim these capabilities you can perform multiple floating point operations in a single cpu cycle but there's a catch it depends on efficiently loading data into sd registers yeah for the block jam without the inmost loop direction only aligns with matrix a after transposing matrix b we can load both matrices into cd registers easily now let's explore another content intuitive but highly effective optimization technique copying blocks into local buffers in the code you see we've made a significant change by hardcoding the block size into the orgm this provides the compiler with more information for compile time optimization okay however the real magic lies in our ability to create local copies of data for each block copying data is often considered an expensive operation but in this case it almost doubles the performance again the reason behind this approach is once again related to cach optimization notice i use the aligns keyword when navigating the local buffer this ensures that all local buffers start at addresses that are multiple 64 which is the lens of the cash line on my machine yeah remember that every time you access an address the entire cach line containing that address get loaded into the cache by making sure that our data structure is align with cach lines we again increase the cash line utilization the next reason we are using local buffers here inside this opm thre private mac i believe that's why rust has in their mac or in their strs like if you do two8 bit uh members in a rust struct it's still eight bytes or it's system length bytes right times two and so it won't be or not eight bytes it'll be 16 bytes even though you only are using two bytes technically it's because it's always doing these these larger offset because it's just fast to read it's fast to put those things there you might recall a discussion about for sharing the video when two threats try to access the data on the same cach line performance can plummet even without locks or other software limitations the hardware often interv to synchronize cach line between different physical cores causing performance degradation to solve this issue we need to guarantee that not only does each threat have its own private data but also these data blocks reside on different cach lines i know it sounds a little bit stupid when they said just keep them on different cach line but the solution is really that simple just use the opm threat private macro it copies a private buffer for each threat response and also handles the force sharing concern perfectly looks good but there's actually one more problem we can solve matrix transposition as discussed previously the performance skin from transposing matrix b is mainly attributed to achieving alignment with the innermost cd loop in both local a and local b however what if we could achieve this alignment without matrix transposition yeah it turns out we can okay if the matrix b is not transposed the coda look like this but hold on a second there are still two local buffers aligned the same way local b and local c actually instead of transposing the matrix to let the buffer align with the loop we can swap the loop to let them align with the buffer this adjustment doesn't affect the computational logic of a rthm it merely transform the aess pattern of the buffer and that is that what i isn't that what i said earlier we can instead of copying we can just literally walk it in the correct order pre-at i pre i pre-at this one clearly pre-at this one i knew it i knew it i knew the pre-at it was going to happen it just felt right oh man i mean that makes perfect sense like if your goal is to access in a nice linear way why copy all why transpose when you could just access that way who's the vtuber now this guy is now we can get rid of the annoying matrix transposition step there are also some minor details you can find toing your gem implementation for example we can clear localy less fre by moving the clearing step one loop outward all these efforts led to a substantial performance improvement reducing the time needed to process a 1k square matrices from 1900 milliseconds to just around 16 milliseconds that's an impressive improvement i have to say i wonder how does this thing scale for just like 4x4s right so like if you're doing game programming does this all scale at that point or is n so small that it actually makes no real difference or even hurts it at a smaller level like you know cuz sometimes optimizations don't always always they don't always work uniformly they work at certain sizes scales linearly i think i mean does it no real difference i mean because my my real question is like if it it may not matter at all on small amounts right i am wrong i it turns out i am actually wrong no i mean i'm just curious 4x4 fit in cash okay that is what she said she she did say that a 4x4 always fits in cash especially considering there's no stuff like fancy gpu acceleration the bad news is that we probably don't want to optimize gam or any other linear algebra operations yourself because there's a much better option called basic linear algebra sub programs the intel mkl implementation of bas can get you only 2 milliseconds for a 1k resolution square matrix jam plus it also support matrix multiplications between nonsquare matrices unlike the crappy demo we showed today but you get a point that demo is great is first off that demo was fantastic okay great job on that demo second two milliseconds and vin by the way what clearly clearly looks like uh lazy vim by the way lunar vim is this lunar vim okay maybe simar approaches we use today somewhere in their proprietary code base you said you want to know what makes it even a times faster than the best we could do well i did learn something doing that high throughput optimization course i took lot of master including separating block hots spot into another module and using assembly intrinsics to optimize it like crazy and my professor also mentioned that even loading data into cd registers in different orders can affect the performance but i never use them in my own project so i say i doubt if i can tell you anything about it anyway if you enjoyed this video remember to subscribe to the channel as always i hope you cash well and see you in the next one i hope you cash well what a great give that thumbs up give that subscribe depth buff buffer that was a great video that was really really really well done i think the thing that makes it so good is that everything he stated was extremely difficult right everything how many people are you subscribed to too many but everything he talked about was like it's a really difficult topic and he did really good that was surprising i'm actually surprised at how much better he did that and also i think the second thing is that you see this all the time which is people make like i think it's very easy for anybody to see the matrix multiplication algorithm and just be like well i mean like can you really make it that much faster like even if spend months optimizing it can we really make it that much faster and that thing went from like 2 seconds or yeah 2 seconds to 2 milliseconds like that library made it a thousand times faster and so i think sometimes you can make things quite a bit faster by playing around and having you know not everything can be optimized that way obviously if you're working with the web server you really got to you know it's not going to be the same situation you're not talking about a specific algorithm in which you're like looking at the assembly instructions instead it's like how are you managing memory are you creating a lot of garbage how often are you in garbage collection how often are you returning back to the runtime what's going on like how much time can you spend just within just doing your code and doing the things you need to do and then getting out you know there's definitely a whole slew of things that can be very important yeah the animations were very very good how often injs no this is actually a really good one you may not realize how important this one is there are libraries that are written in native code that can be used often in javascript and if you know them and you're doing a large chunk of your work in these native libraries it can be really really effective to replace them and not use javascript at all and that's most specifically true in node in bun it's less it's it seems to be less of an issue you don't get nearly the same wins because bun just has a really good optimization from javascript to the runtime and so a lot of those wins go down so spending more time in native doesn't it it's not like a 10x increase right you're getting like a 2 to 3x increase bun just have a good jit compiler it's literally the i mean it's it's it's jsc versus v8 i think v8 if i'm not mistaken is better than bun so to say that bun has something that's much better i don't know if that's true right i can't tell you if that's true or false because i don't know enough about jsc versus v8 anyways the name is the primagen i'm not going to do the cachen okay we're not doing it