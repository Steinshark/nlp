all right migrating netflix to graphql safely please the algorithm loves it just make a comment i need you to in 2022 a major change was made at netflix's ios and android applications we migrated netflix's mobile apps to graphql with zero downtime which involved a total overhaul of the clients to the api layer i wrote falcor for ios my code finally got deprecated six years after i wrote it oh until recently an internal api framework fell gore let's go falgar look at that look at that look at that falcore look at all them falcons i wrote that look at that fair car look at that look at that falcons right there let's see powered our mobile apps they are now backed by federated graphql a distributed approach to apis where domain teams can independently manage and own specific sections of the api doing this safely for hundreds of millions of customers without disruption is exceptionally challenging especially considering the many dimensions of changed involved this blog post will share broadly applicable techniques beyond graphql we used to perform this migration the three stages we'll discuss today are a b testing replay testing and sticky canaries dude by the way all great stuff in here okay all right migration details before diving into these techniques let's briefly examine the migration plan before graphql monolithic falcore api implemented and maintained by the api team yes legacy api monolith i like how we use the term monolith always in negative terms i mean like even the witcher they're all upset about these monoliths what the hell is going on okay by the way i'm only on a couple episodes in so no spoiler alerts i still have no idea what's going on i feel like charlie day and always sunny in philadelphia with all the red things trying to figure out what the hell is happening in witcher i have no clue what's happening okay so i basically get this i don't really understand this graph in here but okay see before moving to graphql our api layer consisted of a monolithic server built with falcor a single api team maintained both the java implementation of falcore framework and the api server yeah yeah yeah i mean i wrote a lot of the initial java stuff created a graphql shim service on top of our existing monolith and by the summer of 2020 many ui engineers were ready to move to graphql graph wheel instead of embarking on a full-fledged migration top to bottom we created a graphql shim on top of our existing falcore appy the graphql shim enabled client engineers to move quickly onto graphql figure figure out client-side concerns like cache normalization experiment with different graphql clients and investigate client performance without being blocked by server migrations to launch phase is one safely we used a b testing who here is familiar with a b testing i assume everybody is that's like a hot button word from like 2009 is everybody familiar at this point i mean is there anyone here that's like what the hell is a b testing why is steals card in the description what the hell are we talking about what the hell are you talking about all right okay what about c testing you know you know okay deprecate the graphql graph wheel shim service and legacy appy monolith in favor of graphql services owned by domain teams okay look at that we're slowly getting them out we didn't want legacy falcore api to linger forever so we leaned into federated graph wheel to power a single graph wheel api with multiple graphql servers we could also swap out implementations of a field from graphql shim to video api with federated directives to launch phase 2 safely we used replay testing and sticky canaries testing strategies a summary two key factors determined our strategies functional versus non-functional requirement i did i item potency or i dependency i dependency i can't tell are you canadian or not that's really the test here is how you say this is your canadian level okay so if you're really canadian it's i i dependency right but then if you're not it's item potent but i don't know how to say it with this you know what i mean so so what this effectively means is that if you call a you get b back and if you call a again you get b back and if you call a again you get b back right item ponies i and them ponies that's all it means is if you pass in something you should get the same thing out just think about like a map right a map if you if you store x with some key and you pass in some key you always get x x out at some point like until until you change the key you will always get x out right so that's what that means item potent right it's just a big ass fancy word okay let's see we could replay the graph wheel queries or mutations that requested non-ident fields true this is fact right so so what they're saying is that you can't actually test this if what you pass in requires big changes so if i ask for say just a video id i should always get the same thing out maybe right i mean you should but you know things happen underneath the hood and and stuff changes underneath your you know underneath the hoodwear when you're doing these tests it can actually like come back to bite you it's a little tricky it's a little tricksy so replay testing i think you get a lot of like you can get you can kind of sneak away with this we definitely couldn't replay a test non-functional requirements like caching and logging user interactions in such cases we were not testing for response data but overall behavior so we relied on higher level metrics based testing a b testing and sticky canaries so a b testing would be your best possible way to really tell if things are going good right so even if you use core metrics what happens is if you just use like just basic core metrics and you're both and you're going if streaming starts to dip in one and not the other you probably effed up right long as you have a sufficient sample size yeah f'd up right and so that's why they do this and sticky canaries just mean a canary that sticks around for a long it's a feature branch it's just a long ass feature branch right tool a b testing netflix traditionally uses a b test to evaluate whether a new product features resonate with customers this is not this is not actually true this is not a good statement sorry netflix blog in 2016 i was using a b testing to test all sorts of stuff 2017 i was using a b test to test we use a b testing i bet you more a b tests are used to test whether or not what you're releasing works versus not right we do it all the time i do it all the time phase one we leveraged the a b testing framework to isolate user segment into two groups totaling one million users the control group's traffic utilizes the legacy falcor stack while the experimental or experiment population leveraged the new graph coil client and was directed to the graphql shim to determine customer impact we could compare various metrics such as air rates latencies and time to render great so now we can get what is called appqe qoe quality of experience metrics out we can see and we can kind of debate between the two which one's faster which one's worse is there anything because making users wait having higher error rates will cause a lowering in like they you know qualified plays or stuff like that we set up an a b let's see we set up client-side a b experiment that tests falcore versus graphql and reported coarse grain quality of experienced metrics see quality of experience and the ab experiment results hinted that graphql correctness was not up to par with the legacy system we spent the next few months diving into these high level metrics and fixing such issues such as ttl flaws or cash time to live time to live flawed client assumptions etc this would have been a horrible experiment or a horrible experience right like imagine combing through that you are experiencing 0.05 percent increased error rates across millions of people and now you have to go through and just like comb it out it's just the worst i hate those kind of errors that only exist in production due to the grand scheme of of what's it called of or the the grand amount of usage nature it is just the worst i've had to do it a couple times oh hold on one second one of the times that i had to do it that was just really awful was when we were doing this thing called always fresh where like i'm constantly trying to keep the ui into like a fresh state so when you turn on netflix you are always up to date you have updated information everything's really really good but there was like four different tvs that just didn't work in production and i didn't know why so now i'm like sitting there going okay how do i debug a production running specific tv i used a b testing of course but it was just so annoying because now i'm like okay so i gotta i gotta go in here and i gotta go and try to figure out why is this philips tv not working and of course the answer was the answer was it was actually pretty straightforward which is effectively how i could tell if something was on or off was buy these we have like these resource metrics like is there networking are we rendering do we have cp do we have memory like what what part of the systems are on and off and this tv i was working with either they were all on or all off so when i do like this like background refreshing attempt they were all on so i'm like oh we must be in the foreground therefore i will never refresh and so it's just like sucks huh yeah it sucked anyways high level high level health metrics a b testing provides the assurance we needed an overall client side graph wheel implementation this helped us successfully migrate 100 of the traffic on the mobile homepage canvas to graphql in six months so i think what they mean by high level metrics of course is that people are continuing to play stream starts aren't statistically different between the two groups of millions of people right and so long as they're not different then we should be able to say hey okay so they're able to continue to use the app identically therefore we're probably doing the same thing gotchas error diagnosis with an a b test we could see coarse grain metrics which pointed to potential issues but it was challenging to diagnose the exact issue absolutely tool replay testing validation at scale the next phase in the migration was to implement re-implement our existing file core api and graphql first server video api service the file core api had become a logic heavy monolith with over a decade of tech debt okay okay why why you gotta be like that first off a lot of the stuff that was written was written in 2016 so that's like not a that's not a decade okay buddy it's not a decade second off second off i don't even think the i don't even think the ios application is a decade old okay you're hurting my feelings right now you're hurting my feelings right now okay i wrote i wrote the ios falcor implementation in 2016 or 2017 or 2015. one of the two either way that's not a decade so come on why are you doing that why are you doing that to me so we had to ensure their implementation our implemented video api server was bug free and identical to the already productionized shim service we developed a replay testing tool to verify that item potent apis i love dude nothing makes you feel smarter than using big words but nothing makes you look more dumb than using big words but highlighting them i swear you can't do that you can't point out that it's a big ass word you know what i mean you can't do that you can't point this out you can't point that out you got to be like oh you know you gotta casually use it you got cash you know you don't want people to realize you're doing that you know because i'm up here a very loquacious individual up here talking to you guys about the specific the specificities of these things and you know you don't see me going like oh look who is this right you can't do that you got to get in there you know you know what i mean okay anyways how does it work replay testing framework leverages the override directive available in graphql federation by the way overrides dirty this directive i every time i see override it just makes me java 1.5 myself i like i i can't even help it i like shake and i 1.5 right there it's crazy directive available in graphql federation this directive tells the graphql gateway to route the one graphql server to another take for instance the following two graphql scheme is defined by the shim service and the video service here we go so we got one we got over oh override this one yeah baby you know what i mean i did just say java 1.5 yeah i know the graphql shim first defined the certification rating field like the r or pg and phase one then phase two we stood up the video service and defined the same certification rating field marked with the override directive the presence of the identical field with the override directive and form the graphql gateway to route the resolution of this field to the new video service rather than the old gym service nice the replay testing tool samples raw traffic streams from mantis okay so if you don't know what mantis is just think of mantis like kafka easiest way to think of mantis is just like big ass kafka except for you get like this squeal like we call it mql mequel or i can't make it into squeal and queel right anyways so mql and it's like a sequel light query language such that you can select out out of all the events coming down the pipeline you can actually select information out right and so by taking that you can take all this information coming in and out and you can build like alerts off of it by uploading any of those things off to atlas atlas is like a real-time counter right mclell ebquil m quill the names mql if one of y'all says silly ass name again anyways here we go with these sampled events the tool can capture a live request from production and run an identical graphql query against both the graph wheel shim and the new ap video appy service the tool then compares the results and outputs any difference differences in response payload this is beautiful this is great i think this is a great little thing right here right mcclellan i think this is beautiful right this is beautiful right here i i like these kind of things i i love this kind of stuff right here where you can actually like a lot of times i hear something another version of this which might be in some sense easier is called shadow trafficking right so you can you can make a request and then underneath the hood it actually splits into two requests the two requests go in and then the two requests come out and then you compare the results of the two requests and there you go you got yourself like is it good is it bad are we actually doing it so shadow traffic is kind of like another little way people do this kind of stuff we do not replay test personal identifiable information better say that legally it is used only for non-sensitive product features on the netflix oeue once the test is completed the engineer can view diffs displayed as flat and just on node you can see the control value on the left hand side commas and parentheses and the experiment values on the right hand side nice look at that not really sure what's going on here but fantastic we captured two diffs above the first had missing data for the id field in the experiment oh okay okay oh yeah there it is missing it right there i don't know what's going on here oh that must be the encoded difference right is that like an encoded difference and the second had an encoding difference yeah should have just read on we also saw differences in localization data precisions and floating point accuracies [music] oh my goodness that sounds awful that sounds so bad it gave us confidence in a replicated business logic where subscriber plans and user geographic location determine the customers catalog availability wins confidence and parity between the two graphql implementations enable a tuning config in this case where data was missing due to overeager timeouts tested business logic that required many unknown inputs and where correctness can be hard to eyeball yeah gotchas personal identifiable information and non-item potent apis should not be tested using replay tests it would be valuable to have a mechanism to prevent that yeah that's the hard part i mean usually so unfortunately with something like graphql and all that i don't think it uses it doesn't use http in a standard way and so typically how this would work is that an item potent request should be a git request if you go to google.com theoretically you should always be getting the same result back out if you have the same input so long as your user time all the things they rank go in your get request comes back out the same thing a post request does some form of mutation so therefore you can't do testing against a post request would be like the the standard way you know what i mean manually constructing queries about graphql since they bastard since they they're like a bastard of http you get this untenable behavior that you have to kind of like get request gets requests okay shut up you know what i mean manually constructed queries are only as good as the features the developer remembers to test we ended up with untested fields simply because we forgot about them you know correctness the idea of correctness can be confusing too for example is it more correct for an array to be empty or null it's a good call or is it just noise ultimately we match the existing behavior as much as possible because verifying the robustness of the client's error handling was difficult yeah i'm more on the empty array plan even if it takes memory just because it's such a pain in the ass to do it the other way around despite these shortcomings replay testing was the key indicator that we had achieved functional correctness of the most item potent queries the most item potent queries wait of most i've put a gun there tool sticky canaries while replay testing validates the functional correctness of the new graphql aps it does not provide any performance or business metric insights such as overall perceived health of user interaction are users clicking play at the same rates are things loading into in time before the user loses interest replay testing cannot be used for non-ident appy validation we reached for a netflix tool called sticky canary to build confidence feature long-lived production feature branch that's all that means that's all that all that means sticky canary is an infrastructure experiment where customers are assigned either to a canary or a baseline host for the entire duration of an experiment all incoming traffic is allocated to an experimental or baseline host based on their device and profile similar to a bucket hash a bucket hash effectively are always routed to the same place right the experimental host deployment serves all the customers assigned to the experiment watch our chaos engineering talk from aws reinvent to learn more about sticky canaries beautiful zuul is like our routing gateway thing makes make stuff happen and the two cases of graphql apis we used a sticky canary experiment to run two instances of our graphql gateway the baseline gateway used the existing schema which routes all traffic to the graphql shim the experimental gateway used the new proposed schema which routes traffic to the latest video api service zul our primary age edge gateway assigns traffic to either cluster based on the experiment parameters okay then we collect and analyze performance of the two clusters okay medium and tail latencies air rates logs resource utilization device qoe streaming health metrics perfect look at that oh i don't know what's going on there but average latency something seems a little goofy right here look at that that doesn't look good cpu utilization oh oh no that does look good experiment look at that it's way lower what the what the hell we don't actually see what the numbers are but you can imagine it looks different we started small with tiny customer allocation for hour-long experiments after validating performance we slowly built up scope we increased percentage of our customer allocations introduce multi-region tests and eventually 12 hour or day-long experiments validating along the way is essentially a sticky canaries impact live production traffic and are assigned persistently to a customer kind of exciting right i like that hi youtube stream it is a youtube stream you can tell you can tell it's a youtube stream see see we'll do that every now and then when sticky canaries was essential to build confidence in our new graph wheel service non-ident apis these tests are compatible with mutating or non-ide ident apis if i see the word independent one more ident i'm gonna i'm gonna lose it business metric sticky canaries validated our core netflix business metrics had improved after the migration system performance insights into latency and resource usage helped us understand how scaling profile changes after migration gotchas negative customer impact sticky canaries can impact real users yep we needed confidence in our new service before persisting route persistently routing some customers to them this is partial migrated by real-time impact detection which will automatically cancel the experiment yep that's good yeah there's like we have automated canary analysis so we have a bunch of incoming real-time metrics and it can make some sweet real-time math decisions and shut things off and stuff like that short-lived sticky canaries are meant for short-lived experiments for longer live tests full-blown a b test should be used yep in summary technology is consistently changing and we as engineers spend a large part of our career performing migrations the question is not whether we are migrating but whether we are migrating safely with zero downtime in a timely manner at netflix we developed tools that ensure confidence in these migrations targeted towards each specific use case being tested we covered three tools a b testing replay testing and sticky canaries that we have used for graphql migration this blog post of our migrating critical traffic series also check out my grading critical traffic at scale part one part two and ensuring successful launches great job look at the clappies 708 clappies okay imagine the level of clappies you could have on a post that's global clappies right there well hey that was pretty good i you know the fun part is being able to know so much of these things right like i know all those things you know and so it's kind of fun reading these articles and going oh yeah okay okay in this case prime is the biggest tool hi i'm the biggest tool how long does it take for netflix to ship something like this from the start of the effort so this one was a multi-year effort not all not all efforts are multi-year like when i when i talked about that always fresh thing that i did earlier where i'm keeping cash is up to date that took two months right it's not huge right testing like testing and making sure as correct was the hardest part just because of the difference because i had the ship on tvs and tvs are notorious for being like all different right and so creating a really good abstraction layer has been very very hard i i don't know i i wasn't working on the team that did this and plus we are using falcor so there was no rest we have not we haven't done rest in 12 years do you really have beef with java java is not great falcor post mortem when no i'm too dead to do a falcon mortem the name is the falcorigen