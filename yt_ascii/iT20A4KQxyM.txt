this episode is brought to you by brilliant randomness is a strange concept that defies intuition seemingly random actions such as a series of coin flips can produce long sequences of heads or tails that perceptually lack randomness while a clearly patterned series of numbers such as a digit series of normal numbers would pass many tests of randomness conceptually randomness can be theoretically assumed but in practice it can only truly be inferred indirectly from properties of a generated output through various statistical tests that attempt to quantify it randomness is an unobservable property of a generating process beyond the theoretical the applications of randomness spans a broad range of fields in its more classic manifestations it can be found in art music and literature as well as gaming and gambling it also plays a cri critical role in the statistics of science but within the past few decades it has by far seen its largest practical application in securing modern communications humanity's first exploration of randomness began with gaming all major ancient civilizations engaged in games of chance initially using dice evidence of such games existed as far back as 2100 bc among ancient egypt india and china the chinese in particular had a long history of playing games of chance before europeans around 1150 bc the chinese text ising or book of changes discussed problems related to coin tossing exploring permutations of heads and tails in ancient greece democratises considered randomness a subjective concept arising from human inability to understand events while aristotle viewed chance as a genuine but minor part of the world making the first attemp attempts to classify randomness into events that are certain probable and unknowable epicurus further proposed the idea of inherent randomness that is woven into an atomic fabric that forms existence by the roman empire chance was personified as the goddess fortuna and games of chance were used to simulate her decisions however by the age of christianity the deterministic nature of christian teachings posed challenges with scholars like augustine aquinus and martin luther grappling with the concept of free will versus divine forn knowledge throughout history chance has always been linked with faith and divination despite encountering chance for millennia the understanding of randomness progressed slowly as grasping statistical principles from everyday experiences proved too challenging it was not until the 16th century that italian mathematicians began discussing outcomes of games of chance as mathematical relationships j carano's 1565 work labor de lud al provided one of the first formal analysis of the odds of winning at various games marking a milestone in the mathematical understanding of chance by the 17th century notable figures like galileo pascala and fmat began exploring the concept of probability paving the way for the formalization of probability theory german mathematician and philosopher er godfrey liit raised questions about randomness and its relationship with complexity this concept would later be formalized in the 20th century the first textbook on probability titled the doctrine of chances was published in 1718 marking the field's growth however despite the progress made by the mathematical community the general public continued to rely on practices like fortunetelling the term entropy an idea crucial in studying randomness was first introduced by rudolph claus in'65 this concept exposed the limitations of the existing 19th century belief that accurate knowledge of a system's initial state could predict its behavior indefinitely this became evident especially in complex systems like celestial mechanics by the 20th century new interpretations of randomness challenged its perception as a minor consequential property to a fundamental phenomena of existence with the development of information theory this led to the entropic view of randomness being applied across a broad range of scientific fields over the course of history the elusive concept of randomness has led to various interpretations aiming to quantify it the first formalized interpretation the mathematical theory of probability emerged from efforts to describe chance in gambling however it would become a big part of studying the natural world in statistical randomness given a sample space of for example a sequence of numbers or events each element in the space has an equal probability of occurring this is known as uniform distribution these elements must also exhibit independence of each other meaning the occurrence of one event does not influence the occurrence of subsequent events from these properties a statistical random sequence inherently exhibits unpredictability like a discernible pattern given past outcomes one cannot reliably predict future outcomes while this seems like a solid definition of randomness it suffers from a major flaw the perception of unpredictability the digits of pi for example exhibit statistical randomness yet are completely reproducible given the correct initial conditions of the sample space the same can also be said with other statistical random events such as a coin flip or dice the their degree of unpredictability only exists because of the limits of available information of the event statistical randomness cannot guarantee true randomness or objective unpredictability however in practice this pseudo randomness is still usable for statistical purposes and other real world applications the scope of a statistically random sample space also determines how usable the randomness is and must be considered for practical use with a large enough sample space that exhibits global randomness local pockets can exist that are by themselves not statistically random in 1938 mg kendall and bernard babington smith introduced the first random number tests based on the kis square test in the journal of the royal statistical society developed by english mathematician carl parson the kisore test evaluates if there's a significant relationship between two two categories of information or if they're independent of each other kendall and smith proposed four relationship tests the frequency test which checked for the even distribution of single digits the serial test that examined two-digit distribution the poker test that tests for certain sequences of five numbers at a time based on poker hands and the gap test which looked at the distance between zeros if a given sequence of numbers is able to pass all of these tests within 5% of statistical significance it is deemed locally random by kendall and smith the absurdity of trying to quantify randomness is a direct result of how probability is interpreted developments in the early 20th century saw the concept of probability split into two primary categories of interpretation physical and evidential physical properties which are also called objective or frequency probabilities are derived from real world physical systems where a given type of event tends to occur at a persistent rate or relative frequency in a long run of trials statistical randomness is based on this as it attempts to evaluate these stable frequencies evidential probability also called basian probability views probabilities as a measure of belief or confidence in the occurrence of an event given some prior knowledge or evidence it updates the prior probability to obtain a the posterior probability which then serves as the basis for making decisions or drawing conclusions during this time period the rise of quantum mechanics also began to challenge classical determinism and introduced a level of inherent randomness and unpredictability into the behavior of particles at the quantum level quantum mechanics does not provide definite outcomes for individual events but rather gives probabilities of different outcomes this probability istic nature had profound implications for understanding the fundamental nature of reality it made randomness and irreducible fundamental property of existence in 1948 claude shannon a mathematician and electrical engineer working at bell labs published a seminal paper titled a mathematical theory of communication in the bell system technical journal in this paper shannon introduced the concept of entropy as a measure of uncertainty and information content formalizing the fundamental ideas behind information theory shannon's work provided a mathematical framework for understanding communication systems encoding and decoding messages error correction and the limits of data compression within information theory randomness is the opposite of predictability in a probabilistic process if a system has zero entropy it has no randomness and is completely predictable as entr entropy increases the randomness or unpredictability of the system also increases entropy tells us how much information is contained in a random variable or data set higher entropy means more uncertainty and higher information content while low entropy means less uncertainty and lower information content the implication of this is that truly random data contains no information that can be predicted and represents the most amount of information possible within a system within the scope of probability theory statistical randomness had one major disconnect with the real world it avoided the definition of a random sequence but rather described the abstract properties that it should exhibit to be truly random in 1919 austrian mathematician richard von misus introduced an interpretation of randomness that was more in line with modern practical use he defined an infinite sequence of zeros and ones as random if it is unbiased meaning the frequency of zeros and ones each stabilize at one half from this any selected subsequence is random if it maintains this unbiased property this concept is known as algorithmic randomness however much like statistical randomness vesus only defined the characteristics of randomness but not a proper selection rule for subsequences attempts to solidify a selection method led to a selection rule being developed in the mid 1960s simultaneously by mathematicians andre kov ray solomonov and gregory chayton conceiving of a method for selecting a subsequence based on a notion known as colog complexity or algorithmic complexity it was proposed that the idea of measuring the complexity of a sequence as the length of the shortest computer program or encoding that could generate that sequence if this program or encoding required more bits than the original sequence it is said to be colog random colov complexity effectively tests a data set for its compressibility with a truly random sequence of data being effectively incompressible k mra's initial idea however suffered from a fatal flaw it is uncomputable in the sense that there is no algorithmic procedure that can compute the exact colog complexity of an arbitrary object in 1966 swedish mathematician her martin lof would extend upon cra's work on algorithmic randomness with his own more rigorous definition known as martin love's randomness or ml randomness in martin love randomness a sequence is considered random if it does not belong to any null sets a null set is a set of sequences that can be generated ated by an algorithm or process in effect if there is no algorithm that can effectively predict or compress the sequence it is considered martin l random as the field of computer science began to grow in the mid 20th century randomness would quickly move from the realm of theory to application as it became a big part of scientific analysis data analysis simulation and modeling one example of this is the development of randomized algor gorithms these algorithms used randomization as part of their logic to solve computational problems unlike deterministic algorithms which produce the same output for a given input every time they run randomized algorithms introduce an element of randomness to achieve certain advantages such as simplicity efficiency or improved performance in terms of time or space complexity the two primary categories of randomized algorithms are las vegas algorithms which always produce the correct output but use randomization to speed up the average case running time and monte carlo algorithms that might produce incorrect results within a certain probability scope but can hone in on a solution over time using randomization providing approximate solutions to problems by the late 1960s randomness would become a fundamental element of communication security as computer-based encryption began to evolve in the early 1970s ibm formed a crypto group which designed a method of encrypting data in blocks or a block cipher to protect its customer data call the data encryption standard or dees it would be adopted by the us as a national standard dees is an example of a symmetric key encryption algorithm which uses the same key for both encryption and decryption processes both the sender and the receiver must possess the secret key making it crucial to keep the key secure and confidential the original 56-bit version of dees would prove to be too weak by the 1990s and would be replaced by more sophisticated methods such as the advanced encryption standard or aes and triple dees in the 1980s the need for non-shared keys led to the development of asymmetric key algorithms that use a pair of keys a public key for encryption and a private key for decryption the public key can be freely distributed allowing anyone to encrypt messages that only the holder of the corresponding private key can decrypt asymmetric algorithms such as revest shamir alaman or rsa and elliptic curve cryptography or ecc would become crucial to the digital signature mechanisms that secure the vast majority of today's internet communications as well as cryptocurrencies by nature encryption loses its strength as more information can be ascertained about the process used to generate its computational precursors they can be attacked from predicting or exploting patterns the less predictable they are the greater the security because of this characteristic random number generation is vital in cryptography it provides the unpredictability needed to create secure encryption keys initialization vectors and other critical mechanisms encryption's reliance on the pess of its randomness source created a new challenge as there are only three primary practical sources the environment a chaos derived source where slight variations in a starting condition lead to significantly different outcomes over time and pseudo random algorithms pseudo random algorithms are the easiest implement computationally efficient and the most common randomness source in computing they generate sequences of numbers that appear to be random but are actually determined by an initial value called a seed and a deterministic computation process however if a seed is known a random sequence of numbers can easily be replicated they're also periodic and can repeat sequences after enough iterations to counter this a relatively unpredictable seed is typically selected such as the time however for encryption purposes this is far too predictable for cryptography hardware random number generators are used that exploit naturally occurring physical environmental phenomena that are inherently unpredictable the most common form of these devices are electronic noise random number generators that rely on thermal noise or avalanche noise thermal noise also known as johnson nest noise is the random movement of electrons in a conductor due to thermal energy avalanche noise occurs in semiconductor junctions where voltage is close to the breakdown threshold both types of noise provide a source of true randomness that can be amplified and processed to generate random numbers more advanced hardware random number generators use photonic processes such as the detection of photon arrival time or quantum properties of light to generate random numbers known as quantum random number generators these devices are the pinnacle of random number generation as they exploit the fundamental randomness of quantum mechanics another common method is to use the unpredictable nature of radioactive decay using gyer merer tubes or other radiation detectors to measure the time at which radioactive particles decay being technically quantum in nature the intervals between decay events are inherently random and can be used to generate random numbers chaotic physical systems are also used to generate random sequences chaotic systems are highly sensitive to initial conditions making their behavior unpredictable over time one notable example of this is cloud flare's use of a wall of lava lamps as a source for random number generation that secures their infrastructure in concept hardware random number generators are the purest practical source of randomness available particularly for encryption however their highly specialized construction or lack of technical access makes them difficult to audit by the general public leaving them susceptible to flaws or back doors that purposefully compromise the system's randomness and making breaking encryption far easier than its theoretical difficulty one example of this is intel's onchip electronic noise random number generator instruction known as rd seed and rd rand found on many of its cpus on june 9th 2020 researchers discovered a vulnerability named cross talk that allowed malicious code on one core of the processor to read random numbers generated by rd rand and rd seed instructions from another core the team was able to demonstrate that they could extract a complete encryption key after just one signature operation highlighting the severity of the issue it's also possible for algorithms to be covertly compromised or at least compromised by insufficient randomness though at present no widely used encryption algorithm have been known to the public to be fully compromised currently the future of commercial randomness lies in more robust lowcost embedded quantum random number generators simultaneously encryption algorithms are undergoing constant refinement to withstand the emerging threats posed by these very same properties that permit quantum computing in effect pitting the fundamental laws of nature against each other human intuition is constantly being challenged by mathematical analysis and understanding randomness is a perfect example of this in fact data science is a direct expansion of this need to comprehend the counterintuitive and apply this understanding to solving real world problems have you ever wanted to build upon your ability to better understand how to interpret the true meaning behind data well there's a free and easy way to get started immediately that's where brilliant.org comes in brilliant.org is my go-to tool for diving head first into learning a new concept it's a website and app built off the principle of active problem solving because to truly learn something it takes more than just watching it you have to experience it brilliant is constantly developing their courses to offer the most visual hands-on approach possible to make mastering the key concepts behind today's technology effective and engaging an eye openening learning experience i highly recommend is brilliance predicting with probability course this intuitive set of data science lessons helps you build the skills needed to analyze and interpret data using interactive exercises to allow you to pull effectual information from uncertainty with brilliant you learn in depth and at your own pace it's not about memorizing or regurgitating facts you simply pick a course you're interested in and get started if you feel stuck or made a mistake an explanation is always available to help you through the learning process to try everything brilliance has to offer free for full 30 days and start learning stem today visit brilliant.org newmind or click on the link in the description below the first 200 of you will get 20% off brilliant's annual premium subscription