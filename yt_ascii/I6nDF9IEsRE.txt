hello everybody good afternoon and welcome to my presentation about pms and performance so was here who am i i'm a freelancer i'm doing mostly plus plus programming i started out as net with network protocols than i did a lot of client server rest and now i switch to qt and user interfaces i've been working with linux windows now in the in the last time more with embedded and i somehow always cared about performance so what's in this talk first intro what what we are doing now then we'll have a quick look at the allocators then at the allocator support in stl then i'll add this ominous pmrs nobody knows what it is then how can we use them then we'll i will digress a little to system allocator themes and then it will be question time and a short summary what are allocators well the slowest part of computer hardware today is not the processor but the memory bus so we have to care about about cpu cpu caches we have to care about the hotness we have to care about data locality also allocating memory is costly because ciscals are involved we have to synchronize between threats we have to care for fragmentation avoiding avoid fragmentation and allocators are pieces of software which try to solve these problems to us there are two kinds of memory allocators system memory allocators which will be provided to you by by the standard library by c library lip c they could be also injected into the program at the runtime on the startup they're invoked by malloc and free functions and one set the valid for globally for the entire program well there are very good i must say but they have all have also to cater for a very wide range of applications so the default settings are normally held a little conservative and sub-optimal so there is some wiggle wiggling room for custom memory allocators so custom memory allocators we must include them explicitly in our program programmatically that but they can be applied only to some specific parts in program they are simple to customize and that can provide not only for performance by but can also simplified debugging they provide can provide special placement of data they can provide profiling of a memorial allocation and so on so what what is the wiggle room for memory allocators there are two two possibilities to increase performance first first by faster location faster allocation and the allocation codes and the second one is by improving memory access and memory memory layouts of the allocated data and which one dominates it's it depends on the program for short running program faster location calls will be dominated and for long-running programs of course memory layout fragmentation memory diffusion things like that so how is c plus plus supporting customer locators we have we can overrate override the global new and delete allocators for the entire program very crude we can override it first before a single class and then we can use allocator support with stl containers so what is the allocator api in stl first we have a couple of typedefs which are needed then this rebind function which isn't that interesting for us and then comes the functional api and these are two groups of functions first allocate and de-allocate function it it deals with memory it gives memory for your objects and then construct and destroy construct will take a memory and call the constructor and destroy destroy will take memory and call the destructor on that memory it's kinda placement new and we have also comparison operators which are required so how is the interplay between allocators and containers as steel containers are included as a template parameter in in containers type then a container will take allocators pointer and reference definitions here this this kind of stuff and use it internally then a container will has to keep an allocator instance as a member is that the problem is this weights wasted space when we [music] we have a you can have a look at at the code of taken from visual studio implement studio implementation of vector and then our allocator the last parameter will be forwarded to a compressed power per compressed pair anybody knows what it is compressed pair boost compressed pair yes empty base optimization so if the allocator is empty it will be optimized optimized away but not empty stateless stateless stateless it means it has no state it has has no no members and in c plus plus 89 all locators were stateless is it clear okay so what changed in c plus plus 11 well a couple of things first we got support for stateful allocators and then we got support for fancy pointers and scoped allocator this means allocator forwarding or fixed so we see in simpleplus 89 a whole whole a bunch of thing things were was wrong it the design wasn't ready but we won't discuss these these fixes because now everybody uses c plus plus 20. who uses c plus plus 20 in work one two three four five people seventy ah yes and 40 14 uses qt containers nobody okay good because one one person because qt container doesn't don't support allocators so okay but so we will skip this c plus plus 11 stuff and go to c plus plus 17 and there was one unfinished business and this is this allocator type dependency on a container type dependency on allocator it's part of container type and is it a problem or is it a nuisance question so if we define a function and this function takes an vector as a parameter and then we have two vectors one one normal standard vector and a second one with a different allocator then compiler will will accept one of the vectors and reject the order because type isn't there is no type match so theoretically what we should do is to template every function which is which is using containers on the allocator class well nobody will do that because it's too much too much hassle and it doesn't scale we've got many functions and everything can shouldn't be templated it just syntactic noise yes there was some attempts in c plus 11. to solve this problem and the new function the new classes like function promise and sharepoint are got support for typed arrest allocators but then it stopped the sdl containers were left unchanged so the question next question is what problems are pmr solving and you probably know it by now i mean at least sense it yes this is the this type type signature story we don't want we have to fix in in we fixed in c plus plus 17. and the solution was we wrap a base class for the new allocators in stl conformant allocator wrapper so that we can use the containers unchanged and then we always will use this wrapper in our containers and the single wrapper can use then different pmas internally polymorphically and the question is what design part pattern is that no bridge okay so what up your mouse victor says it's poor man's rust french people would say it's personal it's person mobility redwood but it's an acronym for polymorphic memory resource well lived why not polymorphic allocator so we have to have a look at the design of these types so we have our old std container and then we have we templatize it on our new polymorphic allocator this is our wrapper class this wrapper class will then have a pointer to a base class of pmr memory resource and if you want to provide a new memory allocator we just implement new type of memory resource new subclass so because polymorphic allocator doesn't change very much it just stays as it is so these old this whole library was named after polymorphic memory resource insert pmr so this pmr on the on the this is the right side yeah on the right side shows us the the interface of npmr so it's the part of the allocator which which does memory allocations allocate the entire locate plus is equal to we need this to to distinguish between classless and stateless and stateful allocators so how can we use that construction okay we just hear how we just defined here some some buffer as as memory we give it to my buffer resource which i wrote and then we will give this resource to our vector how by taking a the address of it and this is working because in in this in the interface of polymorphic allocator we have an implicit conversion which you can will take a point of memory resource and wrap it in in this wrapper class and what is pml vector of string it's just a temple type diff like this one we have in the next line down next code line it's a it's a template type there for a vector taking pma polymorphic allocator also locator type that's all immediately you can see here two problems first containers shouldn't outlive its allocators because it's a naked pointer we are using second problem pmr are not forwarded to standard string instances this pmr vector taking string won't forward it it's pmr to the string so that oops let me see for example we have here the so such pmf vector of a string and then we push back three small vectors strings and they will use small string optimization and it won't will stay within the pmr but when small string optimization is is can't can't be used the the new operator will be called so what is the the correct way of doing it okay we have just two we have to give it not the naked string but the pmr string type and then the mechanism will will provide for forwarding forwarding of the pmr to the string instance question everything clear okay so kai so what happens if the memory we gave gave to the pmr will be exhausted we can set an upstream allocator upstream pmr for for our base bmr and if this memory also gets exhausted then the default memory resource will be used and it's it's the new delete resource new delete resource will fall back to global new delete operators and of course we can change that we can set a new global default resource so what about our implementing own pmr classes well for a memory resource it's not that complicated because we have to implement three three methods dealing with memory management but for allocator or classes like pms string which will then accept pmrs from from its container it's more complicated and we won't discuss it in this talk so instead of it we will have a look of on on the pmrs provided by stop to us by the standard library and we have some pre-packaged pmr types in the standard library in c plus 17. so the situation here is not so dire as with core routines we have firstly two basic pmos which will be our workhorses we will discuss them in the following then two special peer mouse namely new delete resource which we already have seen and then i null memory resource which doesn't allocate anything what's the what's the use of it we'll discuss it also and as as a third part of the support there are many containers located in std pmr namespace like pmf vector pml list pms string so we can just take it and use it and these are very cheap the type devs for regular for normal stl containers just with with this polymorphic allocator set lcd will type allocator type okay this is these are these three parts of our library support so let's have a look on our first workhorse this monotonic buffer resource that's a weird name isn't it but the implementation of monotonic buffer is very simple we have just a slab of memory piece of memory and monotonic buffer will is designed for very very fast memory allocations so it basically just will just bump up an internal pointer and all the entire allocation is this is done however memory will be released only when this monotonic buffer is pmr gets out of scope in scope in its destructor and if we just release a single object which was allocated by monotonic buffer it's the allocate operation is no operation does nothing so we see here three objects which which were allocated by monotonic buffer and the object in the middle was released is destructor was called but the memory stays wasted is that clear okay it stays wasted and monotonic means always growing in in mathematics so [music] it's a matching name constructors simple thing we can specify a memory upstreams initial sizes and give explicitly some buffer we are located by ourselves for it to use so how are we using it just in the in the standard way we can define a vector of pmr strings and give an address to this pmr to it there's however one one problem and it because it's not very intuitive and it's usage of such a monotonic buffer in in a loop let's assume the first the this buffer mem release isn't isn't there so we look we create a vector of strings pml vector of pms strings using some buffer memory resource then we use it and then it it it goes out of scope gets destroyed but the monotonic buffer never shrinks so when we start the new iteration we will next objects will be allocated at the current position of the three pointer it's not a shrinks only on the lead but for that we have this release method so that we rewind the pointer for the beginning is that clear first iteration object object object object now pmr vector gets out of scope the structure destructor destructor called and then the second part of of allocator is to release memory but this guy here does nothing understood and it it has its usages it's not just badly designed allocator because i have to say these allocators are not designed for for the committee they are taking over from existing code base which were using for for 20 years or so at bloomberg so it's well designed well tested so okay we could could think it's something new it's something very shiny no people were using it all the time like the first usage of it i've seen was to increase performance of xml parsing program which used lists internally and it was of course a performance disaster and then when switched to such a simple buffer back to bump up allocator it was a high performance parser so in game programming the using it all the time they were using it for for ages so i have don't have time to discuss it maybe in the question okay but now the second workhorse this is our pool resource we have it can have it synchronized and unsynchronized and what it does it consists of collection of pools for different sizes and allocations are also fast because we have just to find the pool with the right size additionally fragmentation will be diminished and locality will be maximized because the the object will be placed in in contiguous memory we will see that in in a moment but of course it's optimized for blocks of equal sizes so objects which are roughly of the same size so constructors we have just normal constructors with upstreams but also with options which can customize a little these pool resource okay this is how pool resource will be implemented and so we have a couple of pools for different sizes and each of these pools will manage a connect a collection of chunks these are chunks and this chunks is divided in in blocks so you see it's contiguous it provides good locality and no memory diffusion so how can we customize it first by required pullable size it was it's the old name larger the quiet pool block it it tells you simply that the largest pool we support is of size 4000 something and for larger pools for oversized pools you have to go to optima locator and then the length of of the chunk can be also changed some trade-offs when when shoot the length of the chunk be changed and no time no time for that it's much too much mature i'm sorry so but also this isn't anything new we used it in game programming for pool allocators or particle project projectiles spaceships the first usage i ever seen of that was in network programming for network packets so a pre-allocated blocks of of equal size for the packets and apache http server used it in the 80s with memory pools and they had either even memory pools hierarchy for separation of of operations and for locality and it's all very old techniques so special bmrs no memory results why should we need it at all well it doesn't allocate but it must do something and it throws a better look on a locate so we could use it as a kind of allocation card we give it as upstream resource for some monotone buffer resource which is 20 000 bytes and when these 20 000 bytes get exhausted we get bad allocated thrown so that is the use for it in testing so how can we use pure mars because it was all very very theoretically theoretic so but first have a look of some well-known memory tricks which we probably all know trick one we need a local variable local string a vector which will be used in a function and then released so the trick is that in instead instead of allocating it on the heap use simply use a malloc extension called log a and this will give you memory on stack it's dangerous i know but people use it so and that is something that monotonic buffer can do always do without using malloc extension secondly we allocate many small memory blocks in my lock so just we want to shave off this overhead of system call and just allocate one large block and split it by ourselves so and this is also what monotonic buffer does and the third third well-known trick we allocate and they allocate many objects of slime type or similar size so people just cache memory blocks for that instead of returning them and then reuse it it's old and this is what our peer pull peer mart does internally so kai now let's get a little bit specific i have a couple of scenarios scenario one we have a short lived dynamic object locally some string some vector and we have to build it up and then somehow accumulate the values over it something here and then we don't need it anymore so we can just use a monitoring buffer resource backed with a local memory buffer on the stack we just defined unsigned buffer or something like length of something don't call alloc a or anything and then give it to this monotonic buffer resource and this resource has a slap of memory and will bump up the pointer on each allocation on on the end it will just rewind the pointer overhead is minimum and this will buy normally bypass the global heap entirely in which case k is it it will allocate on global hip but why who knows that what is upstream default app sim allocator your delete yes if we if we if we overflow this buffer you delete get kicked in so yeah understood scenario one scenario two we have a large data structure which isn't changed very often it's first built up monotonically like in monotonic buffer it means elements are added edit edit once in time may be removed but normally not so our recommendation is just take our old friend monotone battle resource which was designed for this case especially for this case for example we are passing some configuration files and write it into configuration map so it normally doesn't change very often and well here we don't specify any any local memory buffer so what happens watch this the default upstream you delete yeah so at first unordered map will get some memory so we can say we can reserve the memory for the expected number of of entries so we don't reallocate it on every time if possible call reserve on the container to avoid reallocation understood okay see now here too scenario three we have a diet structure we have built it up but we updated on and on so this this buffer guy isn't a good fit because it would grow endlessly so in this case pool resource would be a good fit why we have seen it was optimized for efficient memory reuse and locality so if we are changing our data structure very often we need something some some pmr which provides it so we don't kill the performance so this is a use case for for a pool resource of course we must [music] paramet parametrical size it's so that it fits but in general it's it's a use case for memory resources the use case it was designed for scenario here four now we have a deep coal chain function quality maybe even recursive recursive one and in each of these functions we need this this local dynamic objects like vector of string which will be built up and something yeah so what can we do here we this is also a case for for pool resource we just define a pull resource and pass it down the call chain and what's the effect why should it be good cash reuse every time we go out of a function and the frame will be popped out of the stack then the memory backing the up the detroit objects will be returned to our pool but then we will call a similar function again and then with it will use it will allocate similar objects so we can immediately reuse memory which was freed in the other function in the new one and additionally if the blocks returned to the pool will be probably still in cash and are ready for immediate reuse of course in theory yeah but we have to measure we have to experiment but this would be also a good fit for pull results is that clear yes okay something advanced questions would we like to see something more advanced is everybody already confused okay we can skip that not advanced do you want to go advanced okay so this is one advanced scenario called wing out well google protobufs arenas use it and well they say object will be freed at once by discarding the entire arena ideally without running distractors of contained objects nobody knows what it what it means yeah so let us have a look we have a monotonic buffer for the container i'll spma and then ah yes yes we want to achieve that no conti no destructors will be called for the elements of the container for example streak string destruction destructors and so on because if we are out of the function we don't need these objects so so the trick is we can use monotonic buffer from the container of the objects okay this is c plus plus 20 but monotonic buffer buffer resource now i directly construct an allocator what normally is done in in construct of of the of the container but here i don't need a container i need the allocator and then also c plus plus 20 i call new object and on these allocator and it will do two things it will allocate memory and call constructors of the objects okay it's it's a shortcut for construct and and allocate now we can use it use it use it oh yeah yeah and it's we we get a pointer to the object then we use the object and at the end of the score the data leaked and will be will construct us be called oh yeah yeah the data you're pushing in is not really destructible a big problem of course no side of effects yeah sorry guys i missed that one of course because it's an advanced advanced expert only technique so here we licked this object so no constructors for it will be called here in polymorphic allocator the underlying memory will be freed at the point where this allocator goes out of scope so we've read the memory and didn't call any cons destructors and this is what's protobu brought above arenas provides and they provide even additionally provide even registration function for constructors that must be called on the exit so okay understood or is there is all a great confusing mess no it's not that difficult it's not an advanced talk so now i i would like to digress a little and and see at gm my lock what's gml log gml log is [music] system memory allocators allocator and it's it was designed to reduce fragmentation and provide good good concurrent scalability it has it has many features beyond normal allocators and its major use users are freebhd where it originated mozilla firefox we and they have quite a an amount of success with fragmentation reducing fragmentation in in firefox with it and then it it was passed to facebook i mean the developer of it was passed passed on to facebook databases are also liked it like it because it is very concurrent in in scalable in concurrent environment and also android switch to it so it's a good thing i think so and what how it is how it it is designed it reduces lock contentions for threaded programs by [music] by creating a multiple multiple arenas and arena is some sub allocator which works totally independently of each other [music] so as a default it creates four arenas per cpu or core i think it was like that some time ago and then internally it different differentiates between three size categories small large and huge for objects to to apply different optimizations and these categories are far further split into different size classes seen that already will allocator anyway yeah okay additionally this allocator support threat specific caching like tcmalog this means each thread each thread had a threat local memory with the most accessed objects in one so and we can tune it we can tune it with runtime options like we can we can switch switch on the back background threads for for purging of 30 pages you can change decay and decay time for for unused pages we can change arena count if we don't like too much memory waste and we can switch on affinity threat to arena affinity and we can get extensive traces and extensive statistics for it if you are looking for memory problems and gml log or tcml autism log is also good but jml log has better performance as well so then it's it's for you so examples from the documentation doesn't look very very cool it's just a parameterization for high resource applications with cpu prior priority prioritized rising spiritualization memory then applications with lower memory consumption and we totally new memory so it's not that interesting it's more like the system administrators of sre engineers but we have all the programmatic api as as malloc extension it it's called mallocs malloc x and we can do a couple of things there we can explicitly create new arenas new threat cases and then we can access it with malloc x giving giving an id to it so an application could allocate frequently accessed objects it has in a dedicated arena which was created before that and improve locality and decrease contention and so on and so on and also such created arena you can you can tune it individually for example for relaxed decay time if frequent reuse will is expected so we wait a little bit longer before we give memory back to operating system let me see let me have a little water so we can also programmatically set an explicit explicit binding between a thread and some arena so to to set a threat arena affinity by hand not globally and yeah to allocate contention on the allocator level okay good and then we have extend hooks it's far too advanced but these accent hooks allow you to implement your own memory memory handling and example use of it is because it's so cool utilization of huge pages to reduce steel beam is this i've had it already in some presentation here and it was done by by facebook hvm they wrote it's the the own extent hooks to manage one one gigabyte huge pages for frequently accessed data it's cool thing and it's open source you just have to google fight so oh we have time we have time comparison with pmrs because there are some similarities here and the mechanism are some mechanisms are not that different okay but first what gmail log developers say s an extreme example we can use it use we can allocate this an arena and use it as pull allocator because internally it will have this this gradation for different different object sizes so and then we will do a general purpose allocator a location on it and then the entire arena gets destroyed in the single operation and so on yes and what does it look like it's it's somehow similar to apache memory pools however yes apache however our mechanism our pmr which will duplicate this mechanism is the synchronized pool resource because arenas are synchronized by by logs second point threat caches you can you can also allocate a thread cache for you so for for me this it looks like just a memory slab allocate locally so somehow similar to monotonic buffer resource but without all the all the mechanism it's just a slap of memory and the size of of thread cache must be configured because there is a default max size you have to change it and so on so this what do you think would you use it because it's it's it's cool it's nice but i gave it a thought because i didn't know that you can you can customize global memory system memory operator is allocated in that way because there was never possible with with [music] news memory allocator with standard mail log and tc malloc hasn't that much you can change somehow number of threads of of threats cache size something something like that but jmr log has an extensive support for customizations but then i thought okay i will go back to my pm off and we'll use would use it maybe as an app streamer locator to to provide basic memory for it and for example facebook folly everybody knows that that library fully library by facebook by alexandre esco and company they have an fp vector and replacement for for standard vector and what they are doing they are trying to detect how we are using j a malloc and if so we know what what reallocation strategy is it using internally and then this vector will not grow and we exponential somehow exponentially two times in a size but it will use just the same growth strategy that jmr log is using internal so that's a cool cool case of optimization and there are also using support for relock at table types in jml log which we don't have in c plus plus at the moment another another possible usage would be fact hip allocator every ahead of it anyone okay tact hip would is something like you can say on what heap you want to allocate some objects game game industry uses uses that naughty dog i think was using that and you have several heaps and then the program can choose which heap it is using and then after after it's no more needed it will be released but not not all objects of a problem will be released but only those which were allocated on a given heap it's tech tip because you give attack when allocating something from the heap and we could implement it with jml log arenas for example but unfortunately it's not included in stl in standard library so summing up i mean oh five minutes okay my conclusions are that writing custom data structures allocators and so on is costly but now we have pmr and it it it can be used for most of the use cases so we are happy to develop the system allocator tuning is interesting but well i will i i personally would stay with pmos of course never use my scenarios as a rule of thumb just measure experiment update measure again and then you see if it makes sense so another thing with allocators it's not they are not only improving performance but can do other things like placing objects for example on this tag on the file my memory on sharp memory then can measure and report memory usage test correctness of programs principle plus 23 but we will come to that are there any problems there well c plus plus wouldn't be c plus plus if there weren't any problems with a library module so what we don't have is for example shared memory resource what we would we would like it i want it but okay we have we have boost in the process allocate also okay and they could be plugged in in sdl containers it is working admittedly i don't know i didn't try it but boost is saying it will be it it will be okay then pmr test resource for which is used at bloomberg's for years wasn't included in in c plus plus 11. not also not in c plus 20 and maybe it it will come in in 20 in c plus plus 23 but i don't know i didn't didn't have anything didn't hear anything about it so it's not complete the support is not complete secondly we have gotchas for example shared pointer is not allocator or but it has an allocator argument but it won't be passed down to its its contents so you just have to know it and it's not intuitive so in c plus plus 14 i said it already this standard function std function had constructors taking allocator arguments and then they implemented type eraser allocators internally but it was removed in c plus plus 17 because it wasn't that good specified and nobody knew how to implement it and each implementation was different and then in c plus plus 17 standard any has land from it and doesn't know because allocator customization at all because why not it's only it it's problematic we don't we don't want to do it what in the future for us okay let's be frank this all was very boring okay lazy programmers why why why can't we let compiler do the work and yes some people are thinking about it for example this year would be a future syntax in c plus plus so we have some container and that's it we don't think about about allocating anything we simply say to the compiler well use this allocator for that and do all the plumbing there was even i think some people tried to bring a proposal but i don't know i don't know like really like course from lakers allocator model yeah pablo alba and no they try to propose something else but it's in this this in this way in this manner like to put in the class using something and everybody who's in that class would use that so i would like it and you are located even better why why why why why can't we forget about all that compilers are optimizing things let the compiler optimize the hip layout of a program so it's performant and we could do it like with lto pdo profile guided or annotation based and working with is underway but only in the research community so it's a long way to go so that's it question time [music] [applause] or do you know to learn even more i have many slides a lot of questions it will be bombarded yeah so early in the talk you mentioned by one of the the drawbacks of including the allocator in the template type is that you can't pass it to all your functions that already taken a vector right so if i understand correctly that that problem still exists with the with the new thing kind of unless you've already committed because we have we have we have one wrapper and these this rapper will be used exclusively so there's no variation in in this type so we fix pml type for string and that's all and we can we can then plug in different memory allocations come come come come come come come come come where is it here no no this is victor what yeah this one this is our our can i say placeholder we need it here but we don't only once when we fixed it it's all this std container will be in pmr stdpmr namespace no maybe not if it's so defined by we have this placeholder holder and every container will only use this plus placeholder and if we change memory allocations we don't change this right but this but this assumes you've already ported your whole code base from stud vector into the new thing now which may or may not be hard maybe not but this are optimizations you don't need to to apply it it everywhere you just apply it and at pieces of code which would need it okay because pretty much optimization and so on thank you thanks for the talk short question if understood correctly could you go to the pool slide where you see these different like blocks if i understood it correctly that the configuration is only the largest required pool not the minimum no okay so so if i have like if i know for example use case that i know if i compute ffts and i have always the same size for for fft arrays that i need and i know that like three distinctive sizes for example so i would also know the minimum not only the maximum that i want to allocate that's not possible here yeah but but if we give the maximum size and the pool allocator will will then compute this this gradation okay thanks so maybe there's a case when another standard library pmr everything clear i don't have a question i have a comment so this is this expert winkable stuff yeah i couldn't emphasize more how important this is for anybody doing graphs trees or anything like that have computations on grass trees on or so on with a lot of allocations so based on john lack of stock back here i think in 17. we use this like allocate everything don't just destroy anything yeah and just throw away a memory at last and the performance was like 300 i give it so talk on this topic last year but it was it was advanced level but i can i can show it to you this graph graph garbage collection yeah we already implemented our stuff back in 2017 but yes i just wanted to comment who doesn't know about this should find out but okay it's it's it wasn't supposed to be an advanced talk yeah it was that's the basics to pmrs how the plug plugged in how can i use it and if you want to read more just be more ping me on twitter i have one more question if i maybe i missed it but you said that one of the problems is that the pmr resource whatever shouldn't out know the vector that uses it should not leave it but no it's not fixed right this is still no it's it's by design it's not fixed yeah okay so you have to take care yeah it's it's like it's it's the same situation we have with the lambda captures we have to take take care yeah to know what you are doing now we are c programmers c plus plus thanks so we know what we are doing as c plus plus programmers python programmers don't know that but it was it was it is clear what i wanted to convey to you these allocator classes how can can they be used and that are also advanced users for expert only because the comes there comes the question it it is undefined behavior or not it's not tell me why expert only it's another talk it's an advanced talk thank you [applause]