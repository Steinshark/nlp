hello everyone my name is elizabethton and like many of uc plus passengers i like writing efficient code and i also like teaching and writing and recently i combined these questions and wrote a book on the topic of performance in union it's called algorithms for modern hardware it is hosted on github and is fully available online so take a look if you understood and while working on it i try to optimize various classic algorithms and data structures are the ones that you read about in textbooks both out of curiosity and for pedagogical reasons to use them later as case studies in the book and succeeded in speeding up quite a few of them but in this talk i'm not going to pick just one of these algorithms instead i will discuss a broad topic that is crucial for algorithm design and especially for data structure design and for programming in general memory and more specifically the cpu cache system so why should you care about memory consider this fundamental question how long does it take to add two numbers to integers together well that depends on what you mean more specifically on where these numbers are stored if the values are stored in cpu registers then we only need one instruction not all arithmetic structures and take equal time but being the simplest and also one with the most frequently used instructions the addition only takes a single cpu cycle to execute so if the data is already loaded into registers it takes just one cycle but in general we need to fetch the operands of this instruction from the memory perform the addition itself and write the results back where they are needed and this can get much more complicated when you fetch anything from memory there is always some latency before the data arrive and this latency depends on many factors but mainly on where the data is stored you see a modern computer memory is a hierarchical it consists of multiple cache layers of varying speed in size where the higher levels typically store most frequently access data from the lower layers to reduce latency each next level is usually an order of magnitude faster but also smaller or more expensive this is a mental model of how the cache here works it neglects many of the important details but it is a good starting point so to answer this question how long does it take to add two numbers together it depends on where the operands are stored if the data is already in registers it takes one cpu cycle to execute the instruction if the data is stored in the main memory it typically takes around 100 nanoseconds or about 200 cpu cycles to fetch it and then probably around another 200 cpu cycles to write it back but it it was accessed recently it is probably cached and will take less than that to fetch depending on how long ago it was accessed it could be around 40 cycles for the slowest or largest layer of cash and around four to five cycles for the buses closest to the processor but it could also be stored on some type of external memory such as a solid state drive a hard drive or some network back storage that is memory map to so that it appears to the program like the data is in the main memory but when you access it the operating system interrupts the program execution and talks to the device that stores it to return the data in this case the delays are so much larger than their psychologically perceivable measured in milliseconds of real time and billions of cpu cycles so because of this huge timing diff differences before optimizing anything else in your program you really need to optimize its memory efficiency and to perform more fine-grained optimization a simpler values of the cache here here is not enough you also need to take into account the many specific details of how the cpu cache system works and in this talk over the next 50 years or something means i will discuss these final details and hopefully teach you how to make use of them in real applications and instead of giving you just drive effects from boring spec sheets and talking about theoretic credential limits i will showcase the many features of the memory system through experiments by running small benchmark programs with the very specific access patterns that often resemble the ones that occur in practical code and give a few examples of how this knowledge may be useful in the real world so there are many different styles between the cpu registers and the ram and the layers closer to the processor are smaller in size but also faster right and the word faster can apply to too closely related but separate techniques first it can mean the delay between the moment when a red or right is initiated and when the data actually arrives the latency or that can mean the amount of data that can be transferred per unit of time the bandwidth it's important to understand that one is not just the reciprocal of the other far from it and the a plus b example we care about latency but for many algorithms the most important characteristic of the cache system for many algorithms is the memory bandwidth and at the same time it is also the easiest to measure so we are going to start with the measuring bandwidth to measure it we can write little program where we create an array and iterate over it incrementing its values we time the whole thing and run this loop many times to mitigate cold start effects and to make measurements more precise to reproduce this and basically all other experiments in the talk you can compile it yourself with your favorite compiler but make sure to set the highest level of optimization importantly enabling neutralization and unroll the loops so that the cost of loop maintenance itself does not significantly affect the measurements all benchmarks will be run on amgs and two a particular cpu or micro architecture but the conclusions that will show are generally applicable so if you run it with many different array sizes on the x-axis and expressing the total running time as operations per second by normalizing it and we get a graph like this and you can see clearly see the cache sizes on this graph are represented with dotted lines when the polar array fits into the lower layer of cache the program is bottlenecked with the cpu rather than the l1 cache bandwidth as the array becomes larger the overhead associated with the first few iterations and the loop itself becomes smaller and the performance gets closer to its theoretical maximum but then the performance drops first to about 12 to 13 billions of operations per second when it the array size exceeds the size of the l1 cache and then gradually to about 2 billion operations per second when it can no longer fit into the l3 cache and has to be be read from the random access memory which is an order of magnitude slower and this situation is typical for manual lightweight loops so the word lesson here is that when the data is the data set is large what you can what you really want to optimize is not the number of arithmetic operations but rather the number of the amount of data reached read from the main memory the common way to do this is to fuse operations together as much as you can especially in the context of loops it's beneficial to join two loops together not only because it reduces the overhead of maintaining the loot itself and it helps with optimization for example if the operations into initial loops are used in different parts of the cpu they can be executed concurrently so for free essentially but also and very crucially for big inputs that don't fit into the cache it helps with bandwidth as you can read the array just once and not multiple times this issue is particularly painful when you are working with the high level abstractions because the separation of concerns principle dictates that we should implement separate operations that do not know about the existence of each other but this naturally can conflicts with optimizations such as the loop fusion but let's get back to experiments in this run turbo boost was turned off and the cpu was running at a fixed two gigahertz we store boston it can run at up to 4.1 gigahertz on more than two times faster all cpu cash layers are placed on the same microchip as the processor so the bandwidth latency and all other characteristics scale with the clock frequency the drum on the other side is separate from the cpu and leaves on its own internal clock and its timing remain constant which we can observe on the ram portion of this graph this detail often comes into play when comparing different algorithm implementations when the working data set does not fit into the cache the relative performance of the two implementations may be different depending on the cpu clock rate because the ram remains unaffected by it and everything else scales with it linearly so you might have two implementations and one of them is faster than at a certain corporate but slower at another clock rate so for this reason it is advised to keep the clock rate fixed and so we will run most of our benchmarks in this talk at playing two gigahertz there is also actually a problem with this experiment what we are really measuring is not bandwidth in our benchmark we are incrementing array cells that is we are reading values and writing them back while in many applications we may only need to do reading or to do only writing so let's try to measure unit directional bandwidth to make the cpu generate only read requests we can calculate say the sum of the array and zero and out an array or filling it with any other constant value only requires memory rights it doesn't need any reads same with incrementing loop these two loops can be easily requised by the compiler and the second one is actually replaced with the mm set so the cpu is not the bottleneck here if we run all three and put together we get a graph like this first all three loops perform the same but then the performance of the implementing loop drops but not dramatically when we close the l2 boundary because it needs to perform both reads and writes and only read and only write loops perform roughly the same which is expected but then something interesting happens unlike the caches the cpu and drum are connected with a single dual purpose bus that kind of any point of time be used as either for either read or write requests but not a simultaneously so this is like a single lane road and the direction of this load or this road this memory bus is a periodically this reached by the memory controller depending on the number of pending read and write operations so the program that does both reads and writes naturally conf consumes twice as much around bandwidth as the one that only does reads so but the really interesting thing really interesting and anomaly is that the right only loop in green here has the same performance as the incrementing loop on this run portion of the gap in fact it starts in the electric cache why might they be this is because the cpu moves the data to the highest level of cache on each axis access whether it was a read or write this is typically a good optimization as in many use cases we will likely be needing this data that we just read or just written again soon but written data when reading data this isn't a problem as the data travels through the cache header here anyway and but when writing this causes another implicit read on the same data to be the space right after over it and thus we will be requiring trust twice as much bandwidth despite that we will only be doing greece so this read backs after rates to maintain cash currency doubles the ram cpu traffic and this is why the on the right loop performs the same as written right we can prevent the cpu from prefetching the data that we just have written by using so-called known temporal memory accesses to do this we need to re-implement the zero in loop directly without relying on compiler automation and ignoring some special cases the optimal way to implement a memset is to move a 32 by chunks of zeros into the destination with the vector operations which we can implement manually with same intrinsics so this intrinsic moves eight zeros or a full vector registers of 32 bytes into a certain specified locations and we it write in eight elements so with one instruction we fill eight elements with zeros this particular cpu can execute one rate operation per cycle and with each right operation we will be writing eight a vector of eight zeros into the memory at two gigahertz we will be executing two billion cycles as per second and multiplying these numbers together we'll get a theoretical upper limit of 16 billion iterations per second or a theoretical memory bandwidth of 64 gigabytes per second because each number written is a four byte integer which we actually almost reach here by the way the compilers as the compiler also uses simmed operations for the read-only and read and write loops but it is a bit more complicated than just writing zeros and the stock is not about sims so i want to explain how to work exactly but now that we've implemented memset ourselves we can replace the usual vector store intrinsic with a non-temporal one the non-temporal memory reads and or writes are essentially a way to tell the cpu that we won't be needing the data that we just have accessed in the future so in our case there is no need to read the data back after a write and so the memory system wanted execute the implicit readback after this non-temporal right operation so if we added to the benchmark we get a picture like this at the left side of the graph if the array is small enough to fit into the cache and we actually access it at some short time after this has a negative effect because we have to write it into the run each time instead of writing it to a locally cached version and on the right side of this graph when the array is too large to fit into the cache anyway this prevents read backs and let's us use the memory bus more efficiently in fact the performance increase in the case of the ram is even more than two volts and is faster than the read-only loop despite both requests request types using the same bandwidth this happens because the memory controller this way doesn't have to switch between reading and writing and also because rights are simpler the memory controller can just fire and forget a non-temporary write requests without tracking them which allows more concurrent writeration as compared to the read on the loop also if you didn't notice we just sped up and said by a factor of three although only for a very specific use case when we don't need the data that we wrote right away and most of the time we actually do need this data which is the reason why mimset doesn't do it and we can do the same with the mem copy and other similar operations there is also a non-temple or a non-temporal read operation and if we don't need the source or the destination of the mem copier data right away you can get similar performance improvements by replacing main property with the non-temporal and copy so the lessons are ram and cpu caches are different l1 and l2 caches are like two lane roads and but shared three caches in the ram are like one lane roads this is important for performance estimation in many algorithms in many memory bound algorithms you can just calculate how much data transfer you need and divide that by the memory bandwidth and you get the total running time but here you need to take into account if it's rich or right bandwidth and the your estimates for the lower caches and the ram will be different the second tip is use non-temporal reads and writes for the data that you know you won't be needing so first this can reduce memory bandwidth like we've seen and also it doesn't kick out the data that already stored in the cache which can also be beneficial next let's try and measure latency the time it takes to fetch just one byte from the moment of the request to the moment you you can actually use it despite that bandwidth is a more complicated concept bandwidth was much easier to observe and measure than latency you can just simply execute a long series of independent tweet or write operations and the cpu scheduler having access to these instructions and vice reorders and overlaps them having their latency and maximizing the total throughput but measuring latency is harder to measure latency we need to design an experiment where the cpu can't cheat by knowing the memory locations we will request in advance the only way to ensure this is to generate a random implementation of size n that corresponds to a cycle and then repeatedly follow a permutation so we are here we are generating first a random permutation and then we are creating a cycle queue out of this random permutation that follows the indices that are in this random of this random permutation so this is essentially a random cycle and then we are starting at some element of this cycle and we are jumping to the another element that the current array cells cell points we run it and compared to sequential iteration it is much slower to visit all elements of an array of same size array this way by about two orders of magnitude not that the scales of these two graphs are different not only does this pattern makes visualization impossible but it also stalls the pipeline creating such a traffic jam of instructions all waiting for a single piece of data on the next pointer to be fetched from the memory this performance center pattern is known as pointer chasing and it is very frequent in data structures especially those written in high level languages that use a lot of allocated hypocrated objects and pointers to them that are necessary for dynamic typing for to work this is one of the main reasons why c plus plus is the fast and languages like java and python generally aren't but when talking about latency it may makes more sense to use the cycles and nanoseconds rather than throughput units right so we can replace this graph by with its reciprocal and what we get roughly corresponds to latency not the the cliffs of this graph aren't as distinctive as they were for bandwidth this is because we are doing random queries and so there is still some chance of featuring the previous layer of cache even if the array can fit into the cache entirely we can infer latency of each layer using math or use more direct ways of measuring latency for example using non-temporal reads to make sure that we are fetching the elements from specific cache layers but this benchmark is actually more representative of practical access buttons because this is what happens when we say query cache table we read some random cell which may or may not be cached with the chance of it being cached depending on the structural website so this is more representative similar to bandwidth the latency of all cpu requests proportionally scales with its clock frequency while the ram timings similarly do not we can observe this difference if we change the frequency by again turning into bubble sun similar to how we did with bandwidth before so this graph makes a bit more sense if we plot it as a relative spin up and you would expect the relative speed up to be would be a default for array sizes that fit into the cache entirely and then roughly equal to for the arrays stored in the ram because it's mostly comprised of ram tanks that are independent of the clock but the later is not quite true there is a small fixed latency delay on the lower clock run even for access this happens because the cpu has to first check its cache before sending a read request to the main memory to save the ram bandwidth for other processes that might potentially need it and this is what we see here when we increase the clock speed another very important feature is that the basic units of data transfer in the memory system are not individual bits and bytes but cache lines on most cpu architectures the size of a cache line is 64 bytes i mean in that the memory is divided in block of 64 bytes and whenever you fetch a single byte you are also fishing it's a 63 cache line neighbors whether you want them or not so to demonstrate this we add a step parameter to our incrementation loop this d here now we only touch every this element while iterating and incrementing the elements of the array if we run it with d equals one and d equals 16 we can observe something interesting when the array fits into the l1 cache this traded version completes faster although not 15 times but just two times as fast this is because it only needs to do half the work it only executes the single instruction for every 16th element while the original loop needed two eight element vector instructions to process the same block of 16 elements both computations are bottlenecked by the by writing the result pack the cpu can only write one word per cycle regardless of whether it is comprised of one integer or eight but as problem size grows the graphs the two graphs emit despite one doing six times less work than that this is because in terms of cache lines we are fetching the exact same memory in both loops the exact same cache lines because 16 integers is exactly the size of a single cache line and the fact is that the striated loop only needs 1 16 of the data that it fetches is irrelevant because you can't request just one machine board you need to request the entire cache line the important practical lesson here is when designing and analyzing memory about algorithms you need to count the number of cache lines accessed and not just the total number of memory reads and writes it is not entirely accurate but it's much more accurate that counting than counting memory operations or individual bytes also the fact that the memory is partitioned into 64 byte cache lines makes it difficult to operate on data words that cause a cache line boundary because when you need to retrieve some primitive type say a 32-bit integer you really want to have it located on a single cache line both because retrieving two cache lines requires more memory bandwidth and also because stitching the results of that came from two different cast lines in hardware together with with some transistor space so this aspect influences algorithm design and how compilers choose the memory layout of data structures by default when you allocate an array of some primitive type you're guaranteed that the addresses of all elements are multiples of their size which ensures that they only spun the single cache line for example the address of the first element and every other element of an inch jury is guaranteed to be a multiple of six bytes because six bytes is the size of it sometimes you need to ensure that this minimum alignment is higher for example sim the instructions read and write data in blocks of 32 bytes and it is critical for performance that these 32 bytes already written belong to the same cache line in such cases you can use align as a specifier that when defining a static array variable it grant is that the beginning of the array will have addressed divisible by the alignment 32 in this case and and it can be easily read with the simple instruction display to allocate memory align the array dynamically you can use a standard align the lock which takes the alignment value and the size of the array in bytes and returns a pointer to the allocated memory just like the new operator does you can also align memory in sizes larger than the cache line the cache line the only restrictions that the size parameter must be an integral multiple of the alignment you can also use the linus specifier when defining a structure here whenever an instance of data is allocated it will be at the beginning of a cache line so say if it is smaller than 64. when you request any of its fields you fetch the entire cache line and the entire structure tool the downside is that the effective size of the structure will be rounded up to the nearest multiple of 64. this has to be done so that for example when allocating an array of these data objects not just one not just the first element is properly aligned but all of them so some holes will be inserted in the memory layout this issue becomes more complicated when we need to allocate a group of non-uniform elements which is the case for structure members instead of playing tetris trying to rearrange the members of struct so that each of them is within a single cache line which isn't always possible c and c plus plus compilers try rely on mechanism of memory alignment too so consider this following toy example when sorted one stored succinctly this structure needs a total of one plus 2 plus 4 plus 1 8 bytes a per instance but even assuming that the whole structure is granted the alignment of four its largest element is largest member it is possible that it is allocated on say the last four bytes of a cache line and the c variable will be speed before between the two lines if the structure is stored section clip so to fix this the compiler will inserts some unnamed members so that each member gets the right minimum alignment and also the so the next instance of the structure when stored sequentially gets the right minimum alignment this potentially wastes space but saves a lot of cpu cycles the trade-off is mostly beneficial so the structure so structure alignment is enabled by default in most compilers padding is only inserted before and not yet aligned member or a debt at the end of the structure to plan the structure itself by changing the order of members in a structure it is possible to reduce the required number of billion bytes and the total size of the structure so we could reorder the structure members like this so indie goes first then goes short then goes char and then goes to the other chart now each of them is lined without any padding if the structure itself is aligned has an alignment of what is for it kind of seems stupid that and therefore this structure doesn't require any padding and the size of the structure will be a four it will be eight instead of 12. it seems kind of stupid that the size of a structure and consequently its performance depends on the order definition of its members but it is required for binary compatibility so as a rule of thumb place your type definitions from largest data types to a smallest this grid algorithm is guaranteed to work unless you have some weird non-polar of two types such as a 10 byte or long double or something like that also if you don't if you know what you're doing you can disable structure piping and pack your data as data as possible you have to ask the compiler to do that for you as such functionality is not a part of c or c plus plus standard at least yet in gcc and clinical this is done with this packed attribute this makes the instance of data take just nine bytes instead of 16 that would be required by alignment with padding at the cost of possibly fetching two cash lines when reading its elements next memory requests can overlap in time while you wait for a request to complete you can send a few others which will be executed concurrently with it this is the main reason why sequential iteration is so much faster than pointer chasing the cpu nodes which memory allocations it needs to fetch next and sends memory requests far ahead of time so the number of concurrent memory operations is large but limited and it is different for different types of membrane so we're designing algorithms and especially data structures you may want to know this a number of other limits the amount of concurrency your computation can achieve so to find this limit kind of theoretically for a specific memory type you can multiply its latency time to fetch a cache line by its bandwidth the number of cache lines of h per second which gives you the average number of memory operations in progress the latency of the l1 and l2 caches is small anyway so there is no need for a long pipeline of opinion requests but for larger requests but follow your memory types there is such in it and they can sustain 25 to 30 to 40 concurrent reads operations we can also measure the available memory parallelism more directly by modifying our pointer chasing benchmark so that we loop around the parameter separate cycles in parallel instead of just one so we create the separate cycles all of them are random and loop around in parallel with this small key array variable in the experiment we'll fix the sum of the cycle lengths constant at a few select sizes corresponding to different cache levels and try different these new values we get mostly similar results for all memory types the performance just scales linearly with the amount of parallel memory lanes but they all max out between 13 and 17 because if you use more iterators than that number there will have to be a register spill and performance degradation due to that you don't always get to the maximum possible level of memory parallelism a register spill being one of the reasons why but in most applications doesn't it doesn't concurrent grids is more than enough our lesson is make use of memory level parallelism the cpu can easily have dozens of memory operations and it is a very important consideration that commands up in many data structures especially those that are easier to implement with pointers but faster with something else so for example this is why it is much more efficient to implement hips stacks skus and so on on tops on top of global array also known as vector and c plus plus as opposed to pointer-based structures like the tissue encoders it's also important an important issue in hash tables in search trees and also in various places where you need to store some potentially large objects you can either store the objects themselves directly as fields or store objects somewhere else like on on the hip and store pointers to these objects storing the objects themselves makes access to them faster as you don't need to chase pointers but makes it problematic to move them if the objects are large at least larger than the pointer so there is an actual trade-off here another thing that contributes to latency is how you make the memory access in the benchmark we didn't use actual pointers but integer indices relative to a base address at the beginning of the array is small enough to fit into the l1 cache this measures around three nanoseconds or four cycles cpu cycles per iteration the memory addressing operator on x86 is fused with the address computation so the x k equals q of k line falls into just a single terrace instruction that also does multiplication by four and an addition under the hood although for the first these additional computations add some delay to the memory operations the latency of an l1 feature is either four or five cycles of the later being the case if we need to perform a complex segmentation of the address like we do in this benchmark and this is a typical situation not just for these cpus but for many others the complex addressing operation is usually one cycle takes one cycle more than the the role purchase so we can make our benchmark run a slightly faster if we replace these fake pointless indices with the actual real pointers this code now runs at flat two nanoseconds or four cycles for arrays that fit into the l1 cache i have receive two cycles because of a feature of this specific micro architecture but the most cpus we would have saved one unfortunately there is a problem with using pointers on 62-bit systems the pointers become twice as large as integer indices in an integer index takes four bytes and the pointer takes eight bytes making the array spill out of the cache much sooner compared to using a 32-bit index the latency versus size graph looks like if it was shifted by one popular of two to the left well exactly like it should but this isn't the case on a 32 bit system but switching to 32-bit mode has its own disadvantages so the lesson here is a use pointers if the data set is small enough to save a few cycles accessing the data otherwise use indices may be wrapped in iterators or something more safe to safe cache space it may look like a bad software purchase but there is really nothing wrong with them so also starting at some level of the hierarchy the cache becomes shared between different cores so this lets you add more course on a course on a single chip but also poses some knowing the enables problems as it limits the effective cache size and bandwidth available to a single execution thread so on most cpus the only the last a layer of cache l3 is short between the course and not always in uniform manner so on linux the topology of the memory system can be retrieved with this ls toppo command it generates this graph and for example mind machine i have eight physical quotes each core has access to 32 300 32 kilobytes of l1 cache 512 kilobytes of altukash but the l3 cache is shared moreover it is not shared uniformly between or all cores its total size is 8 megabytes but it's shared between two groups of four cores each have each having access to its own four megabytes of electric cache and there are also more complex topologies where accessing certain regions of memory takes non-constant type different for each core which is the case for mostly second systems that have several separate cpu chips installed and this has some important implications in parallel computing the performance of multi-thread memory x axis depends on which chords are running the execution for this so to demonstrate this we can run the bandwidth benchmarks in parallel i like writing code but modern writing code i like not writing code so instead of modifying the source code of that benchmark to run on multiple threads we can simply run multiple identical processes with the gun peril and to control which course are executing which processes we set their processor affinity with task set so this combined command runs four processes that can run on the first four cores of the cpu that share the same l3 cache so here's what we get when we change the number of processes running simultaneously the l1 cache the l1 and l2 caches are private to each core so the performance is completely unaffected here but of course start competing for the and ram the performance degrades when looking at the ramp section of this graph it may seem that with more cores the pair process throughput goes one half one third one fourth and so on and the total bandwidth remains the same this isn't quite true the contention hurts but a single cpu can't usually can saturate all of the ram bandwidth if i plotted more carefully plot the total combined bandwidth of all processes more carefully we see that the total bandwidth actually increases with the number of course although not proportionally with some diminishing returns but it eventually approaches the theoretical limit of 32 gigabits gigabytes per second the jump in performance between four and five cores is not perfect we specifically set all processes in each experiment to run on the first end course and the first and the second half of this course have separate l3 caches and also memory controls if some of the processes were to be scheduled on that half of the course there would be less contention for the electric cache and also in part memory bandwidth so to show this let's run another benchmark but now with being in the processes to different four core groups that don't share the e3 cache so we will run two processes on course zero and one and to process this on course zero and four which belong to different groups you can see that when we pin the threads to different core groups they perform better as if they were twice as much l3 cache available which is in in a sense true so bandwidth is a shared resource it is limited but a single cpu core usually can saturate it fully if you have a memory intensive task it makes sense to add more cores but the returns are usually a diminishing this is especially hard to manage in the cloud setting say in kubernetes or similar orchestration systems there is a control in terms of which course you get or how much memory you can allocate but the memory bandwidth can be separated between user applications so increasing the requirement for course or the memory requirement does not necessarily help also since this there is a competition for memory bandwidth it is also important to remove all interference or one benchmarking to get accurate results this is perhaps the single largest source of noise except for frequency scaling when benchmarking also unless the system is perfectly symmetrical and it really is it matters which course runs which threads so if the threads use a lot of memory bandwidth or the shared l3 cache put them apart so so that they do not compete for these resources on the other hand if the two threads need to communicate they take logs for example they do so via the least common ancestor in the topology if that ancestor is run the communication could take around five times are longer than if the ancestor board the l3 cache so if they need to communicate it may make sense to do the opposite and put them in the same group let's get back to the question of memory level parallelism well taking advantage of the freaking currency available in memory hardware it can be beneficial sometimes to preferge data that is likely to be accessed next if the location can be predicted this is easier to do when the processor the processor executes a fixed stream of independent instructions as it can just speak ahead industry remove instructions but sometimes these memory allocations aren't in the instruction stream and and yet they can still be predicted with high probability and in this cases they can be perfect by two mint but by two ways first it can we can prevent them explicitly by separately reaching any byte in the cache line that we need to uplift in the cache error here this is called cache prefetching and second we can fetch them implicitly by using some simple access buttons such as sequential iteration which are detectable by the memory hardware that can start refreshing automatically this is called hardware provision so let's modify the pointer json benchmark to show the effects of hardware professional instead of random implementation we will now cycle over a permutation that just always points to the next element and loops around at the end so and we get the performance as if the array was in the l1 cache regardless of its size so the process server here doesn't know for sure that we will be reading the next element but based on the previous access patterns the memory controller can speculate that we will need the next cache line and prefetch it ahead of time hardware profession can only detect simple enough patterns you can iterate forward and backwards over multiple arrays in parallel maybe performs perhaps with some small strides but that's about it for anything more complex the professor won't figure out what's happening and we need to help it out ourselves with the explicit software perfection the simplest ways to do software provision is to load any byte in the gas line with the move or any other memory instruction but cpus have a separate instruction that lifts a cache line without doing anything with it this instruction isn't a part of the c or c plus plus standard but it is available in most compilers with the built-in prefetch intrinsic it's quite hard to come up with a simple example of when it can be useful so to make the point to chasing benchmark benefit from software profession you need to construct a permutation that at the same time loops around the entire way can be predicted by hardware feature and can easily and has easily computable next addresses so that we can provision and we can get such permutation for example if we generate it with linear congruential generator which is well known and simple random number generator when where we multiply the previous number by some constant and add another constant and take modulus some number n it has the property that if the models n is a prime number then the period of the generator will be exactly that n number so we get all the properties that we want it loops around the entire array it is too hard to be predicted by hardware prefecture and we can also compute it easily in advance if you generate the permutation using this slg we get the ability to pick ahead and if we prefetch one iteration in advance here the latency becomes almost half of what it was for large enough arrays interestingly we can provide show more than just one element's element ahead making use of this pattern in the lcg function when we expanded that is repeatedly applied to itself so to load the dth element ahead we can do this and if we execute these request on every iteration we'll be essentially simultaneously perfection d elements ahead on average increasing the throughput not by default by it by d types and this way we can reduce the average latency are fairly close to the cost of computing the next index so when you know which elements you will need privilege them this is an artificial example but there are cases where this is useful they are just too too complicated for this talk also it is worth noting that a lot of cases are already handled by the hardware profession and you actually fail more often in my experience than not when trying to insert software professional into practical programs especially when you don't know what you're doing this is largely because you need to issue a separate memory instruction that may complete but that may compete for resources with the other instructions at the same time hardware prefetching is 100 harmless because it only it has kind of a lower priority that than normal requests and it only activates when memory and cache buses are not busy also profession is a mechanism that helps performance and now i'm going to showcase a really weird but quite frequently occurring feature that hurts it consider this the same strategy incrementing loop over an array of about 2 million elements that we used to measure bandwidth but now we will use a step size not one not a 16 but 220 6 and 227 so which one do you think will be faster to finish well there are several considerations that come to mind at first you think that there shouldn't be much difference or maybe that the second loop is one 227th so or so faster because it does the fewer iterations in total but then you recall that 2 26 is a nice round number which may have something to do with the sims or the memory system or something or so maybe the first one is faster but the right answer is very intuitive the second loop is faster and by a lot by a factor of 10. and this is this is not just a single bed step size the performance the grades for all indices that are multiples of large powers of two there is no vectorization or anything and the tulips produce the same assembly except for the step size this effect is only due to the memory system in particular to a feature called a cache assertivity which is a peculiar effect of how cpu caches are implemented in hardware so in software the most common location strategy is the least recently used policy where we have some fixed size capacity for cache data and when we need to add something to the cache we you need to make space for it and we kick out the element that we have not accessed for the longest time hence the name least recently used cash in the context of hardware this scheme is called a fully associative cache we have m cells each capable of holding the cache line corresponding to any of the end total memory locations and in case of contention well we need to add something to the cache and there is no space for it the cache line not accessed the longest gets kicked out and replaced with anyone the problem with pulley associated cache is that implementing the find the oldest cache line among potentially millions operation is already pretty hard to do in software and just unfeasible in hardware you can make a fully associative cache that has like 16 6 16 entries or so but managing hundreds of cache lines that hundreds of gas lines already becomes either practically expensive or so slow that it's not worth it in the first place so inside of this we can result to another much simpler approach we can just map each block of 62 bytes each question in rom to a single cache line which it can occupy say if we have forehand sorry 4007 96 blocks in memory and 64 cash lines for them then each cache line at any time stores the contents of one of well divide 496 by 64. you get 64 different blocks so each case line can be marked to 64 different box and direct map cache is easy to implement as it doesn't require storing any additional meta information associated with the cache line except it stuck the position of a particular memory block so that we can tell them apart the disadvantage is that the entries can be kicked out too quickly for example when we are repeatedly jumping between two blocks that are mapped to the same cache line we will be rear reading them on each iteration leaving to lower overall cash utilization so for that reason we settle for something in between a direct map and fully associated questions the set associative cache it splits the address space into equal groups which separately act as small fully associated caches it's like shutting in distributed computing associativity is the size of these sets or in other words there are many different cast lines each data block can be mapped to higher associativity allows for more efficient utilization of their space but also increases its cost for example my cpu the electric cache is a 16 way associative while l1 and l2 caches are four-way associative so most other cpu questions are also said associative except for the small ones that house very few entries and can afford full associativity so there is only one ambiguity remaining how exactly the cash line mapping is done if we implemented a set associative cache in software we would have we would compute some hash function of the memory block address and then use its value to cache as the cache line index in hardware we can't really do that because it is too slow for example for the l1 cache we need to support a latency of foreign cycles and even taking a model it takes about a dozen cycles let alone something more sophisticated for caching instead the hardware uses the lazy approach it takes the memory address that needs to be accessed is and splits it into three parts from lower parts lower biz to higher the offset the index of the world within a 62-bit byte questline there are 64 bytes so you need to log 64 6 bits for that index the an index of the cache line set the next 12 bits as there are two to the 12th just lines in l3 and the duck the rest of the memory address just to tell the cache lines apart in other words these all memory addresses with the same middle part of this index these 12 bits not to the same set this makes the cache system simpler and cheaper to implement but also it's acceptable to a certain but access buttons now so let's get back to wherever the reason why iteration with strides of 226 causes such 256 because of such a terrible slowdown when we jump over 256 integers the pointer always increments by 120 124 that is a 256 by four this means that the last 10 bits of the address remains the same the cache system uses the lower six bits for the the offset and the next 12 for the cashline linux and since the last 10 bits remain the same we are effectively using only 8 bits of the available 12 for the l3 cache which has the essentially the effect of shrinking the usable cache space by 16 and so because we are using 8-bits for index instead of 12. and so it splits they array spills in drum because there is less l3 cache that is an order of magnitude slower hence the slowdown performance is discussed by cash sensitivity effects arise with the just remarkable frequency in all good instance because for multiple reasons programmers just love using powersoft tool when indexing arrays because it is easier to calculate the address of a multi-dimensional array if the last dimension the power of two as it only requires like a binary shift instead of a multiplication it is easier to calculate a modular a power of two as it can be done with the single bitwise end it is convenient to and often necessary to use power off to problem sizes in the different divide and conqueror algorithms it's also the smaller integer exponent so it is attractive to use for benchmarking sizes also more natural barrels of them are by transitivity since 10 is divisible by two then a powers often are divisible by two and large powers of two yeah so there is a natural tendency to use powers of two so it is often arises in different algorithms so for example matrix multiplication the just standard name multiple multiplication is about 13 30 slower for matrices of size 226 then for matrices of size 227. for exactly the same reason because in the inner loop of the matrix multiplication we're iterating in the steps of size of the matrix size and therefore we are jumping in increments of to 226 which would again map to the same set and reduce our effective cache size also bind research is 20 slower for arrays of size 2 to the 12th then for a race of size 2 to the 12th plus 123. this is slightly more complicated when the array size is a multiple of a large power of two then the indices of the quartus elements the ones we likely request on the first dozen or so iterations will also be divisible by some large power of two and not to the same test lines kicking each other out and causing a 20 performance decrease luckily such issues are more of an anomaly rather than serious problem and the solution is usually simple avoid iterating in powers of two make the last dimensions of multi-dimensional race slightly different size or use any other method of inserting holes in the memory layout or better yet make make it so that you don't have jumps in the first place because when you are doing jumps you are probably doing something wrong anyway so for example the optimal algorithms for matrix multiplication and binary search both promote the input elements so that the reads are as sequential as possible during the algorithm execution i i initially wanted to name the stock what you need to know about memory or something along these lines but i realized that the stock is very very far from being enough there are many relational things that i intentionally did not cover and i would like to at least mention them so you can read up on them later so first gratos for memory analysis for more complicated programs you might want to use these tools that let you measure how much cash misses or page faults your program produces perf and similar statistical profiles i can do that and the cache green can also tell you where these cache misses are happening also to execute any sequence of instructions in the program the processor needs to first load it and to first load these instructions which is stored just in memory just like that that does and to speed things up there is a separate system of instruction caches and different program layout optimizations for examples that try to group hotcode together with hotcode so that they occupy the same cache lines and memory pages ram is also quite different from the cpu caches and it has its own times and quirks also performance becomes much harder to assess on normal systems when you have multiple cpu sockets or multiple ram sticks especially different ram sticks there are also complicated things happening when multiple cores one the same cache line or do other communication external memory can also or also have some very specific purposes that are as especially important for implementing high performance storage system how operating system interacts with storage devices and all that account ensures that the stored data is correct and it's not corrupted but by like collagen cosmic neutrinos and stuff and the last and definitely not least how memorable location and garbage collection happens or how it should cabins should happen these all are large or very important topics with separate books written about them so nope you do not know everything you need to know about memory and nobody does so i highly encourage you to read up on them and in terms of more immediate ranging recommendation first well there is my book that covers everything in the stock and a lot more there is also a famous manual titled what every programmer should know about memory by or dropper it is 100 and 14 pages of two column text so very comprehensive it was written back in 2007 but it is still very relevant still great and highly recommended and there is also timor dumler who is also a speaker at this conference by the way who did a talk titled oneplus cps plus no your hardware that is very similar in style to this one but covers other things too so if you watched to this point you probably like this talk and if you like this talk you will definitely like timur stock so that's about it and thank you everyone [music] so sergey thank you so much for that amazing talk i think we will take questions in the lounge outside of here so it