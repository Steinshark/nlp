on developer voices today we're talking about distributed systems clusters of computers working together you know when your software spans more than one machine or sometimes even more than one country how do you get it to behave in a safe sane rational way how much of that problem should you push out to the database how much of it is in the hands of the application developer to discuss it all i'm joined by benjamin bangfort who has a really interesting career that we barely had time to touch on he's worked on self-driving vehicles back in the early days he's worked on detecting bombs via their 3g signals for naval intelligence but in his 30s he went back to school to get a doctorate in globally distributed systems and that's what we really talk about in this podcast i started out by asking him tongue slightly in cheek surely we just use zookeeper and that's the end of the problem but by the end of our conversation he has me completely convinced the most fun thing i could do next is write my own distributed consensus mechanism so it seemed so we seemed a bit like encryption you know one of those problems where maybe you shouldn't roll your own but benjamin has persuaded me the time is ripe to dig into distributor computing bypass the off-the-shelf stuff and build some software that's genuinely tailored to the problem that you're dealing with i'm inspired i'm inspired to do some hacking and i hope some of you will be too so let's do it let's go from here in the uk to there in the us spanning the globe to talk about software that can span the globe i'm your host chris jenkins this is developer voices and today's voice is benjamin benford [music] i'm joined today by benjamin benford benjamin how are you today i'm doing well thank you glad to be here i'm glad you're here because you're going to take us to school which is going to be fun you're our developer voices resident distributed systems expert oh that's such a high praise it comes with a badge and a little blazer lapel pin and that kind of thing so i i'm going to start with deliberately challenging question if i may i think we can probably all agree that there are times when you need to grow beyond writing to one machine right so distributed systems are kind of inevitable past a certain size but why don't we just isn't the answer to distributed systems install zookeeper and forget about it because it's now someone else's problem and that actually is the way that many many distributed systems are built and i think that that's a fairly common architecture zookeeper scd plus some distributed storage system like seth underlying it and then what you end up with is a single node system that can be placed by a zookeeper right and so you've turned your distributed system into maybe a sharded or sort of multi-part singular system and now you can reason about the consistency on that shard but the problem is is that you've lost some of the gains that you've made by distributing the system in the first place right so if the y of distributed systems is we want things like fault tolerance right so we want to survive failures we want to have high availability which is a really loaded term for mainly high availability is is you know you might have people in north america you might have people in europe you might have people in south america you want the servers that they're accessing close to them right okay so that they can get access to their data it's kind of a different definition of high availability right but you want that access close by yeah the the definition i always want to go with high availability is if something explodes at 3am do i get woken at 3am or can i roll in at 9 00 a.m and fix it yeah and actually that's a that's like the engineers perspective or the academic perspective but when you're building applications you're thinking about your user so maybe that's the problem right that the disconnect between just using zookeeper is that you have people who are thinking about the systems and then the people who are using those systems are thinking about the users and there's a disconnect right yeah yeah so maybe we do need to delve a bit beyond just solving it off the shelf absolutely user-focused distributed systems that i like that yeah that could be the title of the next book you write okay so i know you have this theory about the history of distributed systems as being like a pendulum between weak and strong consistency i think you better yeah i mean i think that's true for a lot of technologies right especially when you have trade-offs that you have to deal with in technology and i think the hope is that the pendulum will swing from one extreme to the other and then swing back but not as far and then swing back but not as far and then hopefully by doing this sort of walk back and forth we find some you know trade-off middle ground that's optimal in in the generic case consistency is no different and i think studying history is incredibly important especially with the messaging that happens around these types of systems where they're sort of said like these are new brand new ideas and then you have someone else who comes and says no these are these are not brand new ideas but yeah what's really happening is you know the you start off with distributed systems you want to scale so you you make weak consistency guarantees in order to scale and i'll start at bigtable maybe sort of early 2000s with with this talk but this also happened in the relational database community so as far back as the 70s and 60s you know talking about consistency and isolation you know you have a whole sequence of events that's going on in the relational database realm and then restarted it again and maybe the nosql realm i don't know if i want to call it nosql but in sort of the the new version of distributed systems that happened after the you know the.com bubble popped and then there was all this dark fiber and available server wear that you could you know the google and sort of the big tech guys took real big advantage of right at the beginning yeah when we reached the point where having lots of machines was just the cost of it was easy and the software logistics of it were hard exactly and you know the hardware changes you know also are a big part of the story right things are getting faster cheaper you can have bigger clusters right virtualization you know there's a whole other side story to this yeah yeah there was definitely a time in our history when people didn't think about distributed systems because being able to afford two computers was beyond the realms of evaluation right yeah and that's how the data relational database community ended up right they ended up in like a vertical scaling model right yeah because you know horizontal scaling was super computing you know type technology in sort of the late 80s early 90s yeah but so once google has bigtable you know sort of a weaker consistency model and it works for them right it works for their advertising portal but they say hey like let's generalize this and give it to other people to use other people start to use it for their applications but they can't reason about the consistency semantics in in bigtable so now you go you swing back the other direction to stronger consistency models and you know at the time i think it was you know data warehousing things like vertica hadoop with a very strong consistency semantic with hdfs and but you know now these our systems are too slow right so you you swing back right and now we're in sort of dynamo land and thinking about dynamo and s3 and and more eventually consistent models but again the people who are building applications on top of these things they don't know or understand distributed systems because that's not their job their job is to build a web application and so the consistency semantics are creating bugs or problems that are very costly for them and so you swing back into things like spanner and you know and so it keeps swinging back and forth this consistency pendulum and right now where i'd say that we're at is we're in the the new sql phase i think is what it's called new sql new school new sql where everyone is trying to do sql distributed sql transactions so give application developers a consistency semantic that they understand you know sql and isolation levels but do it in a distributed fashion and other things come in yeah all these different databases that aren't relational databases but want you to access them with sql right yeah and like postgres drivers yeah don't look under the covers because honestly it's a lot more complicated than you want to think about exactly and the consistency problems do pop up you know aurora is notorious yeah that's sometimes that's the worst thing when it behaves mostly like a consistent system and then there are edge cases where suddenly you need a degree to understand what's going wrong yeah but maybe we should step back a bit and say because you've said weak consistency and strong consistency maybe we should define those okay to know where we are and then move forward from that yeah so first you know what i want to say is i personally believe that consistency is a continuous spectrum between weak and strong and that's an academic opinion of mine you know and there's an academic debate about whether that's true or not i point to two things to talk about that so first cassandra will actually on a per query basis let you choose how many replicas are involved in in the read and write query so one of the ways that you might think of consistency as a spectrum is that you know if if you have a 30 click node cassandra cluster which kind of ridiculous but if you did and you chose one replica for your read or your right that's the weakest possible consistency that you can get whereas if you choose 30 replicas in that cluster you have the strongest you know possible consistency that you can get and then any number of you know read write replica sets in between you get this sort of varying consistency over time so is this what we're saying that like in a distributed system you write some data inevitably the data ends up first in one machine inevitably yeah and then it's spread across the cluster and by the time you come to read it are you going to get the same answer regardless of who you talk to or not i mean i think that's a good definition of consistency so even before we get into like what is weak and what is strong consistency maybe we should talk about consistency yeah folks who are used to databases probably recognize the c and acid as consistency but in distributed systems it's actually c and i right it's isolation and consistency so it's not just data consistency it's also what is the experience of one user querying the database from an external perspective yeah so broadly what you might say is given a system of more than one replica and just treat it as a black box if you give reads and writes to the system are you always going to get the same answer from two different concurrent clients right that's that's what consistency is yeah and so you know we consistency like if you have two replicas replica a replica b and client one connects to replica a and writes to x in client two connects to replica b and reads from x and then client a reads from replica a they're going to get two different answers for what x is unless replica a and b coordinate somehow to ensure that that right is the same thing that they see yeah and that does kind of translate to isolation you know we talk about read committed you know snapshot isolation that kind of read your own rights that kind of stuff exactly right but it's also you know that's more about transactions but it's also about just sort of the data consistency model like you know have we violated any invariance like if x is a number should x always be greater than zero in sort of the classic accounting you know atm transaction case that a lot of people like to talk about in school or if it's a unique value you know what happens if one person writes to x on a and one person writes a different x to b and it's supposed to be unique one of them is supposed to get an error back this wasn't unique but they we don't know about x then they're both going to succeed and they've violated the c and acid in that case you violated the uniqueness constraint and plenty of systems even like modern systems say the way to solve this is to have concurrent readers but a single writer that is one way to do it certainly this is sort of the high availability postgres model right where you have one read write postgres replica and then you have a lot of standby replicas it's now the terminology and you can have standby replicas that can take over if the first one fails it's mostly fault tolerance or you can have standby read replicas you know so that you can have that sort of high availability you can read from those replicas but you do introduce an inconsistency in the form of stale reads in that case right so if if replica b is a read-only replica and it's behind the read write replica it can you can read something that's not current right you can read something that's untrue yeah so you end up the main server is in san francisco you're reading off a replica in australia because that's where you are it's delayed but then when you go to post a new transaction that has to go over to san francisco because that's where the writer is it is and you could end up reading your order history and the order you've just put in hasn't made it around the round trip by the time you do that exactly and it introduces the transaction problem again because if you want to read then write which is a very common operation you can't read from one replica and then write to the other you have to do your read and write on the primary yeah and maybe you push that into developer space to make sure right they're handling those semantics and so now your australian user has a worse experience than your san francisco user so what was the point of distributing the system in the first place it's faster but it's also more wrong that's true actually i say that facetiously but is there ever the right trade-off faster but wrong yeah absolutely i do believe that's a good trade-off right i think that a lot of people want to say like strong consistency is the thing that we want right and you know we haven't even gotten you know talked about weak versus strong but there was a reason that bigtable was successful there's a reason that dynamo and other eventually consistent systems are successful and that's when collisions are rare right when it's a rare that two clients are going to be working on object x at exactly the same time and when collisions are rare a weaker consistency system does improve the performance availability and fault tolerance of the overall system and you just have to somehow deal with those collisions you know downstream and if you can do that at the application level then you know this weaker consistency system it behaves better overall but what that means is that the application developer has to know what the semantics are of that weaker consistency model so that the application can deal with it yeah and that's where the gap is that always seems tricky because it always seems like when we buy into a distributed system with those semantics we start off trying to follow the rules and six months into developing the project we start to behave like everything works the way it does in development right because everything is always completely consistent in development right because the network's so small and fast yes and or you're just developing on a single node [laughter] yeah i think one of the issues is that you know in the case that i just talked about where collisions are rare they're even rarer in development because they're not pushing the system as hard as you would be in production yeah and then even in production they're hard to notice right like if we talked about this you know client a reading x and client b reading x client a and b have to talk to each other in order to figure out that something went wrong right yeah they can't just sit there and be like oh this is a consistency problem i mean like it's really hard to observe consistency problems generally yeah yeah it normally ends up that you just get weird behavior that the users are complaining about and then you realize and that's a horribly far down the pipeline time to find out about these problems it is and distributed debugging of consistency problems is horrendous ncd recently had a massive bug that was a consistency related bug and the way it manifested itself was it kept overwriting decisions that had already made and they kept that you know basically these in these very few very rare cases scd would fill up its storage volume and then like blow up and yeah i mean it's a great kubecon talk about this from last year's kubecon 2022 kubecon in detroit so if you're interested in in that that bug in ncb i do recommend that video and i'll link to that in the show notes yeah absolutely that's something to link to you but yeah the you know so anyway this problem is happening and and everyone is trying to debug it and like no one can figure out what's going on and is it on the application side it's clearly an sed problem scd is running raft right and so everyone's looking at raph like did you do something wrong with raft is it like the strong consistency system is it is it something wrong there is there a bug with the invariance it's raft not safe but it actually turned out to be the consistency of the level db transaction that's under raft when it's writing to disk right they were actually opening two concurrent transactions to write to level db and not closing one of the transactions and so one transaction would beat the other transaction and that's what was causing this failure so you know but you know back to the sort of original point was just like you know the weird thing that we saw was etcd filling up its storage volume but how do you diagnose that right like how do you and it's so rare like it only happened in these few cases like tons of people are running ncd successfully in production without this you know very edge case you know very low probability occurrence happening and it's very difficult to fix and if you watch the video the story of how they found it and fixed it is is months worth of effort and heartache yeah so it's important to get it right when you start as ever but you can't just say that without giving me some guidance and hope that's true that's true because i the thing that makes this really hard is i think our brains are hardwired for linear time well that is extremely philosophical and you're right you know we think in instances of time yeah and when you ask me to describe weak versus strong consistency you know i before i even define those things i started in with well consistency is a spectrum but we as developers we still think about these positions on the consistency spectrum right there's weak consistency there's eventual consistency there's causal consistency sequential consistency and then linearizability and i agree that we think in linear time but we don't think in linear time like i'm not thinking in your linear time i'm only thinking in my linear time and so but the two of us are talking concurrently right yeah and so as a developer like how do you develop a system where there's multiple actors in the system that have to linearize some sequence of events you know and obviously they can coordinate just like you and i can coordinate but there are two separate linear streams of time so you're saying our first step is to imagine ourselves in a crowd yeah absolutely yeah so so how about dancers i think dancers are the good a good analogy that's actually how i prefer to develop there's actually two i think models of distributed systems development so the one that i subscribe to i suppose is very close to the actor model okay but what you do is you develop a single node right which implies that you're going to have a homogeneous system of all of these same nodes right and you think about the behavior of that node and what you want to do is you want to come up with the simplest set of behaviors on the replica so that when you add them all together you get more complex behavior right so from simple local behavior you get global emergent behavior yeah so in the dancer analogy is you think about the choreography for one dancer and sure it looks great when there's one dancer but if you put 20 dancers and they're all doing the same choreography it looks amazing right this is i'm reminded of something louis pillfold said to me recently it's like it's easier to write a reliable web server for one request and then make a million of those exactly to make a web server that can reliably serve a million requests yeah exactly yeah that's exactly it but that's not the only model so then there's this specialization model or the specialist model which is kind of the other model of development and in the specialist model you say okay this we're going to think about these different roles and responsibilities and here is the responsibility for this type of replica here's responsibility for this type of replica here's a response and then you have specialization and that might be more akin to like a cheerleading squad right where you have people who are doing the list and there are people doing the flips and you have people who are getting lifted right right okay you can follow me through that because we don't have cheerleaders over this side of the pond gymnasts i guess or you know or a dance that has maybe more complex components to it like modern dance versus you know something else and like if you actually look if we can take this back to the distributed systems paxos is the specialization model of development so paxos you have replicas acceptors leaders commanders scouts right you have all of these specialized roles in paxos whereas raft is the actor model right raft you have one replica here's what it does it can become the leader but it can also then go down to becoming a follower right so raft versus paxos you can see those two different thing styles of thinking about developing distributed systems right because check my knowledge here the raft impacts us the surface differences they behave exactly the same paxos came along and it did all this like leader election stuff raft came along and said we can do exactly that but you'll understand the algorithm better i mean i think that was certainly diego's goal it's like understandability and a diego garcia and john oscar the authors of refs i think i might have gone diego's last name diego yeah anyway hopefully you can cut that out subtitle caption on the youtube version that corrected it for you sorry about that little brain fart yeah i mean that was certainly the goal of raft was understandability right and even the name of the paper was a more understandable consensus algorithm and you know i remember the initial way that this went about was they taught raft and they taught paxos and then they gave a survey to students that they were teaching to see what they understood to show that raft was more understandable than paxos but you know they're the same in the sense you know if you think about multi-paxos where you have a leader that can cover multiple slots or so rather sorry multiple ballots versus what does that mean well so paxos is paxos is all about safety and theory right you know lamport define paxos thinking about safety you know and thinking about these multiple processes that we're running so replicas have slots and the slots hold commands and what the goal is is to have replicas have the same list of commands in all of the same slots and the way they achieve this is by asking a synod yeah all of these terms and factors they ask a synod to decide what command should go in one slot and the synod does that with balloting so slots and ballots are kind of different although whenever people think about paxos they always unify them right so so they are a little different whereas you know raft is about a single totally ordered log of commands which obviously is a simplification of the paxis model and then all you know paxos has all of these invariants for the different roles you know for for leader you know acceptor committer you know proposer scout commander all these different roles have all these different variants that they have to follow and raft replaces many of the invariants with timing right and timeouts what does that mean take me through that yeah so lamport has this idea of sequential consistency right and if you are a distributed systems academic you don't believe in time right there's no such thing as time right once you've got tenure i suppose that's the natural response yeah so instead of time what you have is you have things happening before something else okay right so you know this happens before this this happens before this and if they're you have if you have two events that have no relation to each other it just means that you can't establish it happens before ordering so in distributed systems you don't have time you have the ordering of messages that are received by the system right so you know you don't put a time stamp on it because my clock could be different than your clock so you said like i got this message then i got this message which implies a total serializability of messages which is one very big bug that a lot of people who are just writing distributed systems make is they forget that most web servers paralyze requests so you have to have to sort of sequentially order sequential ordering of messages in order for all of this to work and so ralph says okay you know that's fine but you know broadly speaking at a higher level of time granularity you can you know you know as long as we're at like the millisecond level or the second level you know that level of granularity we can have timeouts and things in order to control these roles and responsibilities so there's you know heartbeat timeouts and candidacy timeouts that control what roles each node in the system has as opposed to paxos where you know you send a bunch of messages out like saying hey i would like to be the leader for this slot and then if you you know if you get enough responses saying sure you can be the leader for this ballot then you proceed but if someone else says no i'm actually at a higher ballot then you kind of flip-flop back and forth and it's all about the timing of messages so that one node like all the nodes get the messages at the same time in the same order rather than just actual clock time right already this seems very hard to implement in practice yes and there's a whole lot of academic literature that complains about how hard paxos is to implement did all the complaints come before raft was rated law after you know that's hard to say a majority of the complaints came before rash was written okay so pre-raft i think you zookeeper use chubby use that cd to build your system that was the right thing to do right because someone or many someone spent a lot of time dealing with the paxos implementation right and you didn't have to worry about it but post-raft right i think it is a little simpler to implement these consensus algorithms so e-packs us and and raft with a little bit of time and care those things are able to be implemented in an effective and safe way really so you're saying it wouldn't be mad to roll your own i don't think it would be mad to roll your own and i think that we're seeing the effects of that right cockroaches rolled their own at cd rolled its own raft kafka recently kafka recently nats red panda you know i mean like it's it's not uncommon right you know these the hashicor has one console you know so there are tons of systems out there that you can look at the implementation they're open source and that you can slightly tweak to your use case okay you know even inside rafts you know we were talking about read versus write well raft alexa leader all of the rights can go through that leader but are you allowed to read from a follower yes or no and actually if you can read from the follower then you've got what we talked about originally which is you know the the re secondaries yeah which might you might give you stale reads but you still have strong consistent strongly consistent rates so even when you're implementing raft you still can make choices are we aggregating commands are we sharding raft there's still a lot of things that you can do even side of that implementation so now post draft post epaxos we're in a world where i think people can and should explore writing their own consensus algorithms wow i didn't think anyone would ever say that go ahead and comment i'm really i'm willing to do that to say that and we haven't even talked about the eventual consistent algorithms but you know also writing gossip protocols right and and thinking about you know anti-entropy type eventually consistent replication is also another thing people should be considering doing because what you can do then is you can embed your application semantics into the storage system that you're writing oh that sounds exciting and weird and avoiding all those lovely abstract out your distribution layer things tell me about that one of my favorite examples is of this is rq light which is raft sqlite so it's a raft replicated sqlite server okay and the way that you do transactions in arculite and i'm not an expert in arculate so that you know but what i looked at this implementation and what it looks like you do is you take your transaction everything between begin and end right a bunch of commands and you bundle that all together and that's one raft command and you wrapped it around and then it gets executed on sql light server and then you find out as a user whether the transaction succeeded or whether it was rolled back right because sql lite is naturally single threaded right that's still the case i don't actually know if that's true or not anymore because i know that sqlite has a lot of locking inside of it and you can actually open a sqlite database now for multiple processes oh right okay maybe i'm out to date there but you know you're now saying it's possible to do it from multiple servers right so yeah rq light basically makes sql lite a server yeah and also just while we're here sqlite is an incredible technology i am such a big fan of sqlite it is really just an amazing piece of database technology and i highly recommend considering to you using it even in inside of your kubernetes clusters because it's easy to attach a volume and drop a sqlite server there and then you don't have to deal with all the like h8 postgres stuff and you still have a relational database under the head and it's an incredibly effective tool yeah okay i one thing i remember finding out to my surprise is that there's a sql like database in the ios core libraries yeah yeah make use of it as well and i think a lot of browsers use something very similar i think it might be called index db yeah yeah they're internal html5 database that's equal right under the hood most times right i or a forecast sqlite or something like that i don't know for sure but yeah i think that it's it's gotten around you know sqlite maria db level db rock cb like there's all these sort of embedded database engines out there that are just you know available for distributed systems researchers to use or distributive systems application developers to use you know to create higher level distributed systems from cool yeah haven't we though it seems like we've got very firmly into this idea of elect a single writer and however you do that and whoever you change whose leader now we're still doing single writer model yes and that's certainly one of the ways that things goes down especially with raft and you know the leader is a bottleneck right and yeah you know i'm primarily interested in in large geographically replicated data systems so i like the idea of you know someone in san francisco in australia and europe i want to see how those database systems work but you know in practice you know even just having something on you know one side of the continent the other side the continent can introduce a lot of problems you know even with a fiber trunk between them so you know the first method to deal with the leader-based system is to shard you know the the system right so you have multiple leaders operating on different tablets of data or different shards of data and you're running you know multiple raft quorums and this is how a lot this is how cockroach works that's how spanner works you know you'll see basically huge numbers of raft quorums inside of a lot of bigger distributed systems and they're just working on different pieces and one of the reasons that works is let's say that you have a three node system while we know that the leader is doing most of the work right the followers aren't doing most of the work yeah so what we do is we break our object space down into three parts and then hopefully will happen is that you will have you know part one on one replica part two on one replica part three on replica and they're all sort of leaders for each of their parts and so you're maximizing you know cpu and and you know disk rights across all the partitions yeah but how do you generally tend to shard that is it like to simplify you've got all the accounting stuff in san francisco and all the user data in europe right and all the product catalog in australia are you sharding it by all the keys from a to m end up in san francisco or are you starting it from from the provenance of the first right for gdpr compliance or you know ccpa or the other privacy laws that we have you know basically the key spaces provenance related so whoever the first writer was it has to stay the leader stays in that region oh that's that's particularly spicy yeah exactly but again this is just another point right like your application should determine right how this is happening and you know some database systems might give you control over that but you know being you know it's you're going to get to the use case eventually where your sharding mechanism maybe doesn't exactly match you know what the database is providing you and then there's an opportunity to start thinking about rolling your own solution and ensuring that you're charting technique works i never thought we'd hit the day when you were saying generally companies should be thinking about rolling their own distributed systems as they grow you know like i said it's probably a controversial opinion but i i think that we are in that phase right now i don't think that phase will last forever but i think that's what might help us with the innovation that we need to get to the next stage and frankly in the early 2000s that's where we were at then right google was rolling their own twitter was rolling their own amazon was rolling their own square was rolling their own dropbox was rolling their own so you know they have been phases in databases and distributed systems technologies where companies thought okay it's actually better for us to roll our own and the only and you know at the time those companies that i just mentioned weren't big tech like they are now right yeah you know back then it was microsoft and oracle and yahoo and these were the little guys right and so now we're back in that same phase again right where now with privacy laws more globalization more cloud locations than ever before right i mean five minutes and you can have some i can have something running in every single continent right except antarctica except for antarctica that's true i can't get there quite yet so six out of seven is not bad right but and you know and you know if you look at the planned rollout of these cloud locations right south africa and northern africa brazil peru are getting new data center locations so it's easier than ever before it just put something up in japan put something up in in europe put something up in india so we're here all the problems that come with it and all the problems that come with it yeah exactly and then also we're facing the problem so yeah i think you should roll your own we're in that phase where innovation is necessary and the more people that are working on it and if you feel enabled or empowered to do it that's where the innovation will come from right yeah so you're more saying it's that's the next natural swing of this pendulum exactly okay i think then you have to give me your benjamin bangfort guide to writing my first draft algorithm oh my goodness yeah if we want to go with raft in that yeah writing raft is easy at first and and hard after so the step number one is you want to make sure that there's a single thread right or a single place where all messages are handled and raft right so whether you're using you know grpc or using an http client a server and the thing that you're embedding all of your messages are going to be coming in concurrently and you want all of those messages whether they're from a client or from another replica to go through what i like to call one big pipe right so step one is make sure that you totally order your messages within your system and so if you're using go channels are great for that right if you're using something that has mutexes or something like that you want to make sure that your central thing is handling one message at a time in as close to the order that they're coming in as possible some kind of software transactional memory thing exactly right yeah exactly so step one is make sure you've got the one big pipe thing locked down step two is implementing your timers and remembering that your timers have to go through the one big pipe right so you might start to think of these things more like events right so a message received from a client or from a replica is an event and a timer going off is an event and these events have to be happen have to be handled in order with one big pipe right so one central process okay so if i'm using like a channels or co-routines thing i might have one machine that's just responsible for sending heartbeats and that goes into the co-routine pipeline exactly it's not something within my single raft atom that's time ticking exactly okay okay i'm with you so far yeah so and and really if if you do that you've solved like 80 of the bugs that the undergrads and graduate students who are being taught this usually have right because they usually end up having you know lock contention issues or other types of concurrency issues like concurrency issues between the heartbeat and the networking component right so you just want one thing that's rafting right one thing that's managing the state and that goes through the second thing that's important to think about is you want your raf replicas to fail hard right you want them to die totally which is another thing that i think a lot of people have problems with right so you don't want to get into like a partial shutdown state you don't want to get into like you know partially writing to disc or anything like that you you want things to die and die completely and and so thinking about like what are you using for your storage under the hood i personally recommend level db as sort of the storage layer under the hood to start off with because it's so easy to get started or you can do something like badgerdb that has transactions and buckets but you want to make sure that you're using something so that you understand the consistency model from that sort of central process all the way on down and that if something goes wrong or something shuts down it all just completely goes away and dies completely right and your partial right gets rolled back or and your partial rate gets rolled back exactly okay or you know you don't you know one mistake that people make is like they have like a heartbeat messenger right and so every time the heartbeat timer goes off they send a message right but that's dependent on that replica being a leader right so if you're if you if you're if you're don't become like if if you lose your leadership status that heartbeat timer needs to stop but it could be that it's sent a heartbeat message because it's an independent process yeah so yeah this is another reason right that everything happens in that sort of one order of events right the heartbeat timer goes off that middle thing is going to send heartbeat messages right it knows that i'm the leader state it sends its heartbeat messages out then the next thing comes in and this is like you know a vote right and the vote term is higher than my term well now i've got to go down to a follower right because you know the term has changed right and i gotta get all the messages from the log that i miss right and then the next thing comes in and maybe it's an append entries from the new leader right everything has to happen like one at a time whether it's a timer message or whether it's a a network message right okay and then within that i'm building like can i download like the raft state machine spec and just implement that yeah so if if you go to the paper right in the paper there's like a a one page description of the algorithm right it's in it's in blue and red and it has like black text and it's just like a box and it says like this is what a follower does this is what a leader does here are the invariance here's one you should be sending these messages here on these timeouts it's just one page you can pull up that page and as long as you get the things right that i just said more or less you will end up with a working raft system you make it sound so easy yeah i don't want to imply that it's easy but with like a little effort a little consideration i you know i think that it's it's buildable it's doable okay and it's a fantastic exercise even if you're not going to put it into production on your own for just understanding the semantics of a distributed system because it was i mean there was a time i would have said anyone attempting to do that was mad you're now saying it's a sane thing to do insane now i mean the devil's always in the details right so i've tried to head off most of the issues that people have had by minimizing the amount of new texas that are around state and minimizing the amount of concurrency issues by using that one big pipe thing that is where a lot of the edge cases come in what you're not going to solve by doing this exercise is things like you know the flp problem or as i prefer to call it the leader thrashing problem where you know you have the timing is like if you set up your timing wrong and raft the leaders will just thrash right so one person will say i'm the candidate and then the other person will say i'm the candidate and then they'll say i'm the candidate and they'll just keep trying to be candidates and just keep trying to climb the term numbers up and up so you need to make sure you do things like have stochastic intervals for candidacy timeouts so everyone's not becoming the leader at the same time you need to configure your system so that you have enough time between timeouts right so that you have this sort of reasonable outage when you don't have a leader but also so that the new leader can elect itself without thrashing right yeah yeah so there are problems that you'll encounter when you're running this in production you know that aren't on that one pager in the graph paper but you know getting to that good quality state where you can get a leader elected you can kill the leader and a new leader gets elected and you can get totally ordered commands through the rap system i i'd say that's a doable exercise cool and what happens when you've got that when you've got like you've got it working and you've got your leader up do you just how does it actually work when i want to actually start writing some data do i then just send my data packets to raft and it has a special message that says ah now i'm just writing this data or yeah i mean that's actually a great question so like now what do we do with it well yeah yeah i mean so you're building some sort of application right you know if you just want like nominate a leader and like tell me where to write probably you should just use that cd for that for zookeeper proof here if you're writing in java but now that you have rafter up and running you can start thinking about the application semantics of whatever you are doing right so the simplest thing to start with is a key value store right right and so what you have inside the key value store is you're just totally ordering all of your puts right and if you want linearizability which is the strongest form of consistency you can also totally order all of your gets as well so every client says i want to put this value to this key and then raft turns that into a command right replicates it around it's in this log and then it executes that command onto their local database and then now if you have concurrent puts that are coming in right they are totally ordered so you end up with that sequential consistency this happened before this right this put happen before this i mean even they come in in different nodes and then you can start to think to yourself okay well you know we have this object space that belongs in one shard and this object space that belongs in another shard so maybe now we have two raft replicas that are working on opposite shards so that we can increase throughput and now you have sort of a distributed key value store you know that might be using you know key hashing or something like that to distribute the different shards so you know now you start to think about your application but maybe that sort of key value stores you know and if the key value sorry you could start thinking okay well i want caching i want eviction so now on top of my raf cluster i have another process that will start to do eviction in a consistent manner right so eviction becomes a consensus process inside the system and right so you can start to see how like any time you want something that has a strong consistency semantic in your application you already have the sort of underlying consensus thing running on the nodes and you're not asking some external system to do it for you right you can have these internal processes that are working so you can literally describe the custom menu of commands you want to be rafted exactly yeah exactly like with arculite right like any insert or update is getting rafted any select just goes to whatever replicas underlying you know thing that it has right so you can you can absolutely or you could create a lock server right now if you have a lock server maybe you just use that cd for the lock server but you can create a lock server if you want to do more complex key value transactions like maybe i don't want to just do a put on x i also i want to put on x y and z and i want to make sure that i lock all of those keys before i put them together yeah or you can implement your own check and swap right i want to put exactly x provided x is currently this value yeah yeah or maybe i want to start thinking about a file system right and i want to create a distributed file system or i want to create a you know a distributed compute system that where the executors are working on specific pieces of data you know and that we have fault tolerance on the executors and you start thinking about well i don't need something as massive as yarn or mesa or something like that right so now you have this little consensus algorithm that's working to manage your your distributed compute or maybe now i want to build a graph database right you can start to see how you start to build up how your consistency model like what you're going to allow to have like a weaker consistency what you might be allowed to be replicated differently versus what types of operations go through consensus and they're already on your your data storage system yeah yeah it reminds me of a little hack i've been playing with is like networked game engine okay where you have this bunch of commands which is i'm joining the game i'm leaving the game and so forth and then you've got within that session thing a packet of commands actually relate to the game that's actually been played like fire the missiles for instance and it's a bit like that isn't it it's like once you've got this base layer of session consensus management then you can stick whatever you like in the middle of that data type absolutely yeah i don't know if anyone's ever said this in history but by gosh you've made it sound like fun well you know if there's one goal i can have yeah it should be fun right you know i do want to sound like fun i you know this is where i think access struggled packs those folks focused on safety right focus on understandability you know e-packs those focus you know i also recommend looking at epixes too so we're talking about raft but epacceptance is another you know there's basically two optimizations to base paxos optimization one is called the the leader oriented paxos which is multi-paxos raft and the whole host of others and the other optimization is optimistic fast path and this is what fast paxos and epacos do is they try to get they basically maybe i should back up for a second for any consensus algorithm there has to be two phases minimum there has to be a proposed phase and an accept phase minimum you probably also have a commit phase or an execute phase probably but there has to be a proposed phase or an accept phase in order for it to be safe right in order for that coordination to be safe what leader oriented optimizations do is they just do the proposed phase all at once right so basically the leader says okay i have proposed for all of the following slots right they're mine and so the leaders so that it just takes away the proposed phase and everything after that is accept phase so that's the leader oriented optimization the fast access optimization is we go straight to accept and we skip propose and if we detect a conflict then we have to do propose except and yeah so what happens is you're optimistic in that there's no conflict you can get these accepts through and you won't detect an issue but if there is a conflict then you have to go through the slow path which is a proposed accept but it actually is turned into three phases right because you have that initial accept then a proposed then an accept so those are the two basic optimizations raft is the leader optimization epaxos is the fast path optimization right yeah i see the optimism in there and it's that must work well when collisions are rare exactly exactly all the fun of writing rollback code you do and actually the most complicated thing about epaxes is that dependency thing which gets into our point about applications right in order to detect a dependency that's kind of an application level thing right like how do you figure out like how do you tell epacc so it's like a generic epac says that this thing is dependent on that thing right x is dependent on y but not z right and so that's the tricky thing so epacos is probably better used in embedded into more specific applications where those dependencies are known but epoxos massively increases the throughput because you don't have a leader bottleneck anymore and so you'll get much higher throughput from epaxos at the cost of having to figure out this dependency thing but you can still get their strong consistency guarantees across the yeah it's just as strongly consistent as raptors and should we be ambitious enough to try and implement that one ourselves that one is tougher unfortunately there isn't that sort of nice one-page one-pager thing that raft has absolutely for fun i think you should do that for production there aren't as many reference implementations right so one thing we talked about with raph is there's a huge number of reference implementations right cacha core scd you know cockroach like there's just a huge number of reference implementations that you can look at not so much with epacc says but i would like to see epaxos start to grow the criticism with epaccess is that you can't shard it like raft right so raft you have this leader that's maxing out throughput so if you have three nodes you put three leaders on each of them on on each of the nodes whereas with epaxos all of the nodes are maxing their throughput so the idea is if raft can do 5000 messages per second then you can get up to you know 15 000 messages per second with three whereas epaxos can do 15 000 messages per second but it requires all three right you're maxing out the cpu of all three right but you don't have to shard right yeah this dependency thing are there particular kinds of applications where this shines yes anything where the latency between messages is high epaxos was developed specifically for geographic replication right so you know my research you know what we thought is you know we we have you know vertical paxos which is you have a configuration paxos and then the object paxos or the command paxos if you have an epaxos that's managing your sub rafts right that's actually a good way to really create a big system that goes across different regions and has privacy guarantees and providence aware things and and is that i mean that's sounding like a real mess is that something we can hire you to do for us are you busy at the moment i am busy at the moment and that's exactly what we're thinking about is is how do we create these sort of large distributed systems that that are planet scale and how can you replicate across them and our sort of mode of operation is you know mixing and matching right so having some layers that are doing epacos some layers that are doing rafts some layers that are doing eventual consistency with probabilistic entropy yeah so that's that's how we're building our distributed systems at rotational labs rotational labs like who the clients for that what are we talking like just the major large geographic players or is this something where smaller companies might need i think this is something that smaller companies might need right now small and medium-sized companies you know like i mentioned it's very easy these days to put your application into a different region and therefore grow your market right you have machine translation for i18n issues right you have the cloud services to place your application in those countries and it's easy to do that but then you just get into all sorts of complications once you start to manage those distributed systems and what the cloud companies want to tell you is just use our service and it'll just work by default my favorite example of this going terribly wrong was the pokemon go release by niantic i missed that one help me out yeah it's a niantic was commissioned to to create the the pokemon go mobile app which was one of the first very successful augmented reality games right so you know i remember we were all joking back in the day people walking around with their phones looking for these like pokemon yeah exactly yeah you just use the camera and you would find these pokemon like in the world and there was gyms and you know you could throw your pokeball at it and capture them and everyone's fitbit spiked because now everyone's walking around [laughter] and it was a very successful game but the release was highly chaotic it was highly chaotic because i think they released in like 50 countries simultaneously like they like they first released in japan then the us and then it was like 50 countries right and people were having login issues registration issues you know they were being able to capture pokemon it was like a real big issue and when they diagnosed it it was a distributed data system issue right the databases that they were using weren't replicating well and all of these things were consistency symptoms so they ended up hiring google to fix the problems and so google brought in their sort of cloud resources and you know a spanner and some other things to fix the issue for niantic and everything went smoothly after that and and you know it was a great tale but when you're a small to medium company right paying for spanners horribly expensive and selling in google is never cheap right exactly exactly and and you do have to understand i mean you know niantic had experts help them you know redevelop their application for these new global consistency models and and so if you want to go to market in a different region right i think you do have to consider this problem because you are going to have issues if you're selling something and there's an inventory issue and you know you know is it fair that whoever in in whoever's closest to your server gets the best you know deal right like yeah you don't really want that to happen so you want to make sure that you're you have a distributed system set up well and and we're certainly interested in in and making sure that that people can do that and there are increasing number of companies trying to do that right across the globe because it's but because it's initially so easy and obviously business desirable but the devil's in the details the devil's in the details and you don't know necessarily where your products is going to land or where you're doing is going to be getting the most traction so it is important i think for companies to branch out besides just their local regions yeah yeah years ago i sold a a software thing and it turned out my best market was austria far and away didn't see that coming how do i target austria yeah i love stories like that yeah well from from here in the uk to there in the us our distributed system must come to an end we could talk we might have to have a sequel maybe we'll try and be in two different countries for that one you can be an australia i'll be in singapore or something there you go thanks very much for joining us it was a pleasure thank you for having me thank you benjamin and as i said at the start that really has inspired me to do some coding and if you caught our episode a few weeks back on erlang and gleam i'm thinking actors i'm thinking sending messages to a single serializable point i'm thinking predictable failure modes this might be the gleam project i've been looking for i'll keep you posted if i manage to get that together i'll let you know and if any of you out there break out an editor and put something together please let me know but you don't have to start a repository to get in touch my contact details are in the show notes as always and the comment box the like button the subscribe button and such they're always there for you send your feedback across the globe and my inboxes will linearize them for you automatically and also take a look in the show notes if you want to check out rotational labs and see the problems that benjamin's actively solving with all these techniques and you can also look in the show notes for links to the original raft paper and a few related papers that might tickle your fancy and i think that brings us to the close of this consensus slot i've been your host chris jenkins this has been developer voices with benjamin bangfort thanks for listening foreign