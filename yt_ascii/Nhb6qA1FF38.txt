on developer voices this week we're back in the world of data and data streaming and specifically kafka but this time we're going to look at it right from the very top of the stack because the thing about kafka is it's kind of a groundup rethinking of the way you handle data at serious volume when you might have a billion messages an hour coming through across a cluster of machines right that needs some new thinking compared to what we had a decade ago and i think kafka has been really successful because it solves that core scale problem with a very simple core idea the log file and it builds up quite nicely on that the challenge that kafka has really faced is that when you rethink things from the ground up you can't just rebuild the foundations you have to build the whole tower all the way up to the top into user space you need to build infrastructure you need to build tooling and i think it's fair to say that in the 10 years that cf has been around it's done a lot of good things for data at scale but the user experience the developer experience still not a solved problem maybe it is maybe it is if you and everyone on your team speaks and loves java but for everyone else most of the answers you get are some new dialect of sequel that's what's going to make this easy treat it like sql good answer but there are other answers the design space is still being explored bo and my guest today luca pete has been building a new data processing tool called typ stream which takes unix as its inspiration instead of sql cu when you think about it unix has been building pipes of data since the 70s so why not mine it for some ideas so that's the topic for today's podcast an old design idea unix transplanted into a new domain real-time data streaming let's see what we can learn i'm your host chris jenkins this is developer voices and today's voice is luca [music] pete i'm joined today by luca p how you doing luca nice nice goods goods and you yeah i'm good i'm glad to see you we we've yet to cross paths in the real world even though you know working similar circles in the data space right it's true i i think you know the main reason might be that i don't go to conferences especially in the us but in general i'm not really like a conference person so i guess that that may be the reason why we never met maybe next year if we both end up at kafka summit would be nice i think there's a good chance we'll overlap sooner or because you're only in berlin so you're not far away from me right it shouldn't be yes it's true it shouldn't be but but even if you're not a conference person you're stuck now you've got to you got to give me a conference talk in conversational form for the next hour or so true the reason i wanted to talk to you is you've been i'm gonna frame it like this so there's a lot of people working in the big scale data processing world trying to make this more usable right that's a big problem like there are people we had neil abusing on recently yeah saying how kafka streams is his favorite library for data processing and it's a great library but you couldn't call it user friendly yeah i i i think it'd be kind of a stretch i do agree with him is the best at least also my favorite java library out there it's i think a pretty incredible piece of technology but yeah user friendly would be kind of a stretch just just for a starters you would have to be a java developer that you know that's already that's already that's already a stretch to call it user friendly because you have to know a language and for example i am a good example of that i literally went back to the language just because of kfast stream right so i forced myself back into the space trying to learn the language i hadn't seen in 10 years so no no it's not and i and and i think it's i would say i would argue generally true of the data streaming tooling out there and there is not nothing wrong with it like i like you know i don't want i don't want that to come across as you know as a critique saying you know all the tools out there are not user friendly i think it's more like of a historic like if you look at it historically it's just like a you put it in the right perspect perspective and then it's actually just normal that the tooling is not user friendly yet because well it's kind of early days it doesn't feel like it's early days of thata streaming because i don't know how you first came across cf but my i think my in my case the first project must have been 0.8 0.9 so we're talking yeah yeah it's a very long time ago i think it was so early that that people in the team i was working with they were a little worried that you know kafka wouldn't you know wouldn't be so stable that's how early days it was because no one would ever say that now and to be fair it was also it also made sense because kaf had just changed the protocol significantly and we've been talking to people saying oh they just changed all the protocol we're stuck on z. 7 so people will wored but it sounds like a million years ago but i think it's slightly more than 10 years ago so it's just a de good since we have seen you know streaming come to i would you call it mainstream it's also somewhat of a strange world play i i i think we're still i think we're on the cusp of being a mainstream idea but we're still not there yet and i think one of the things one of the things stopping us getting there i think in order to get any kind of traction in this we had to go right down to the nuts and bolts of how do we store data y and we're climbing up that ladder from dis all the way up to user space yeah and that's why usability is almost the last piece before we can go mainstream yeah no i and and it's kind of exactly my point with saying i don't mean it as a critique when i say the tooling out there is not user friendly because so first of all it is kind of just a fact that it's not user friendly yet because i believe kfast stream might be the simplest library like the simple to simplest tooling out there to do stream processing and it's definitely not a easy technology to to get started with because of the language because it doesn't click immed medely in people's head i wrote a very long article trying to explain how cfast stream works to other people but it was also way for me to see like do i even get it myself because the you know the beauty of this technology is so profound that it doesn't doesn't click in your head that that fast and yes i agree with you that usability comes last and in a way if you think about it it's i think the conversation is somewhat starting now and i think most people are approaching it from a perspective that i i i think it's obvious if you know what i'm working on that i don't agree with that is you know we're trying to bend sigl to do things that sigl cannot really do and again one more time i don't i i'm not trying to be negative or trying to critique the solutions that are using sigell because i think they make sense i think it's obvious that you say okay how do we make stream processing more user friendly and then i don't know confluent comes up with casy will itb i think i i understand why they did it and if i was where they were when they start at kl db i would probably suggest sel myself i think it makes sense because it you know everyone knows sql that's kind of the argument right everyone knows seel and that's how you get people to do stream processing but then you hit limitations pretty soon actually and then you end up with the dialect of sel that people don't know anything about like i i i've done sel for basically all my career and then every time i have to do something with k db i just have to learn the syntax from scratch again because because and and it's there is nothing wrong with it it's just that it it always makes me feel that you know the metaphor doesn't actually work because sigal has this declarative approach to asking question that is like you know this is what i want and you figure out how to give it to me and it there some sort of like maybe it's not even really there but i get this this you know there is this inner requirement that says like it's more of like a request response thing where you know i give you i give you some squel you giving me some data back which by definition doesn't actually work with streaming because streaming is unbounded data so that you know i give you something and you give me something back it like there is already something breaking down in this in this metaphor and yeah it's like i think i think the usability thing we are scrabbling around the design space looking for answers and and this is great this is an exciting time i like it when we're thinking about new ways to design software and yeah sql is a very it's a natural fit for how can we get data and what people are used to and usability but that's a double-edged sword what people are used to can also be a limitation of if you've got a very different underlying model the abstraction can break down as you say yeah which is i think what we see even even though like people have gone really far right if you look at how powerful flink is and like they have gone really far with it but i the problem is that you know it's kind of funny because while on one side it did make it more approachable i like i don't know if you try to run this technology on your machine then you know you're still like like miles away from the average experience you get if you interact i don't know with post on your machine right and i and i think that's such a it's such a simple and like trivial way maybe of looking at the problem but well that's what literally developer experience means right if i'm experiencing getting data out of postris i just have to remember how select works i don't even know where post store data i never even looked at it i mean it's not true i'm just making i'm just making this up for the for the for the sake of the conver ation the point is more that you don't actually need to know where the data is you don't need to know their encoding you just have some interface that gives you the data back and and because the basic obstruction is so simple then we kept building up on top of it and then you end up with the you know the i don't even remember how it's called i think it's called the datag grip plugin inside intellig or these beautiful interfaces that allow you to talk to seel it it's it's even like even the interface is nice like it even looks nice when you interact with it which you know it's like a second stage of usability and you know this might be actually the only thing i always struggled with streaming since 2013 on that is like even the simplest tasks are not actually that easy like you know i worked in a variety of spaces with streaming that they the only thing they had in common was literally kafka right you know the different industries different reasons use kafka sometimes it's sometimes it's just moving data around sometimes it's actually processing data but all these projects had in common that it was really hard to do even the simplest thing like giving give me a 100 records from this topic because i actually need to look at the data i'm looking for like i have a bug that i don't know how to fix it but i need some real data because i don't understand the problem and you know i imagine how is it to do that with with red shift or with a remote post database you just you know connect to it with a read only database get 100 rows you even have a synex to get directly as a csb and just move on and in every single place i worked with there was always you know there was always like two or three ops just to get to basic answers now there is you know there are there are some uis out there that make it a little easier but while the nature of the problem and the fact that we you know kept building on something that i don't believe it works that is sel we ended up with with solutions that more or less solve one problem really well but don't give you this you know like like full developer experience that is just like even remotely as good as what you have with rem rational relational data basis which to be clear it's somewhat obvious because we have started working on usability for relation databases i don't know in 84 85 i i don't know when to yeah maybe maybe even in the 70s we could be pushing on to the 50th anniversary yes you could argue that we started the i wasn't born yet i think that's kind of you know in the late 70s i wasn't there yet and and our industry was already working on how to make this this more usable and you know i use always the same trivial example but to be honest it happens every day especially when i've mentored a lot of people into the streaming space because i'm really into it because i really like like it fits my my mental model of how data flows into systems like a glove so i applied it in a lot of companies so i found myself in a position where i had to mentor a lot of people and i could see for example this completely obvious burden that you know when you are working with say with a with a cf cluster and you have a bunch of topics and you have to get data out of it well you have to know the encoding up front which i know it's obvious right i know the technical answer is of course you have to do because you have to theit the network that's not what i'm arguing i get that's obvious but that would be the same as saying that when you exract data from a postest database or a myle one you would have to know how the b3 tables look like on dis i think it would be really hard to get data out of yeah it would be a hard sell yeah it would be very hard to sell so i understand that some of the complexity is not going anywhere like at least long term like i can't imagine like completely being able to hide the concept of partitions even though i have my opinions about that as well i think it's a little harder but some of those things like for example knowing up front that abpic or a json one or a a proto buffer and you actually have to have the scheme up front before you can even look into one record of this topic yeah i think it makes it significantly less usable because of how much work you have to do to even know what you're working with and that's you know i think it's like it's literally that part of it that every single project that use at kafka i worked with people struggle at first and then you and then i've seen two different two different scenarios coming out of this either people get really good at kfast stream and then become so experts that they they don't remember anymore how hard it was to get there so they say you know cfk is easy like if we have to write a stream processor that gets the data filter out some stuff and then does aggregation i mean it's 15 lines of code right they've got all the they've got all the recipes in their short-term memory because exactly it's cached in the red they know exactly how like they can picture the dsl from from kfka stream in their head even though nitpick there it always bothered me that we call it the dsl because i i think it's a fluent api not technically dsl different conversation yeah that's i have thoughts on what dsl actually means and yeah sep conversation but it would be a very long conversation the point is that you either end up with people becoming experts nothing wrong actually pretty pretty amazing like i have some friends that now they're going around the world and doing afa streaming places that there was no kafka stream first very happy about that or people just you know say okay kafka is some some more of a black box that can can you know ingest data really fast i can get it out of it but when i have to do something i will i will move it first somewhere else and then i do my it's like i move it i move it to which to be clear there are some really obvious use cases where this is the right solu option right but i've seen a lot of people like then ending up saying you know kafka is just good at moving dat around and then when i have to do something i will do it somewhere else which i think it's like not really true like i think there's a lot of like even the most basic work that you could do it in flight like you know filtering data out like basic aggregations there's a lot of work that you could do out of this realm but the problem is that for that to be easy for you you got to be a cfast stream expert and then then you see the problem there there's a terrific amount of power that we're missing out on just because the user experience isn't quite there and i do think you're right without criticizing the sql approaches at all and i use them and i love them yeah there and we've made great strides in that but it's still not a solved problem and an answered question there is still more to explore in the design space and this is why i got you in because i think you have an interest interesting and novel answer to how we could sol how a new area we should be looking at for this design space so i'll let you tell me about it yeah typ stream yeah so i mean so i i i have to make a premise there because i don't like you know i sometimes you listen to people talk about their projects and they're a bit sensationalist about it it's like you know this is the best new thing so i appreciate you saying it's novel and i do get why you say it's novel but that's actually the thing i struggle the most with when i talk about typ stream because i think the ideas are not novel at all actually like i like when i look at what typ stream does and how it solves problem i didn't invent any of this i just like looked at it from from the perspective of data streaming so so what does so how does typ stream look like first of all right so the initial story there is that it has actually nothing to do with streaming is that i really wanted to get into cotland and i needed a real word project and i had nothing at my hands that's maybe two years ago and i started exploring programming languages because it sounded very art it's like you know can can i write a programming language it sounds impossible like i don't even i don't actually understand how programming languages work well it turns out that shout out to um munificent bob i think on twitter that is the author of crafting interpreters it's an incredible an incredible book like it's literally you know nothing about programming languages and you read this book and when you're done with it you can actually write your own programming language there are there are you know there are things that the book doesn't go through because the space is too big but you know you can you can do most mo most of a small programming language on your own so this is how i get i got to typ stream like it was it was like i didn't come up with idea i was like i've had i've had the idea for typ stream some 10 years ago almost like in 2015 i think it was at at the time i was the c at this food startup called marles and we were discussing how to use kafka to move order data from a web shop into you know different parts of our infrastructure like we need we need you know shipping labels on one side we need the ability to have manifest to build boxes on the other and it's like if you think about it it's always the same data and it just looks slightly different like i need all the errors going to customer care i need all the shipping labels to go to the shipping team and it's you take always the same data and you do these little pipes that in my head in this meeting i can picture the meeting in my head they just looked like these basic uni pipes that we use all the time i was like you know i can cut my ers and then i just grappled the errors and send it to customer care and when i had this idea i actually couldn't put it into words because well first of all it was so early that i think afka stream was not even there yet or it was really early days i actually don't really know when when kafka stream came out but it must have been around that time and i also had no idea how to put it all of it into practice then i you know went on with with streaming for like some eight more years years and then i found myself wanting to learn cotlin and then everything just came together and said you know here is a very difficult project can i get a bsh like programming language that i give it strings that look like bsh like bush comments pipes or on liners or whatever you want to call them and can i compile this down into a kafka stream application this this was this was the question right i didn't i didn't i didn't tuggle the problem from a usability perspective it's more the other way around it's like while working on these that sounded like very difficult and very fun well the more i looked at it the more interesting the metaphor got because you know by nature i'm one of these people that is very skeptical about his own grandio ideas right you know it's like when i started working on this i'm like am i really trying to write a programming language for data that sounds that sounds crazy like that like that doesn't make any sense you're never going to be able to do this and you know sure it's maybe a half imposter syndrome a half hey skepticism but you know the metaphor just kept giving while i started looking at it because you know i i got this remote compiler working and then i could do these basic things like you know cutting a cf copic and you know doing some grap which is literally filtering and then i got you know some basic vc version wc i think it's the common where i could aggregate dat and just you know count page views and things like this and then and then i and then it did really it hit me really hard that you know this uni metaphor that we have heard for 50 years that in unix everything is a file i mean there was really no difference like between the between these concepts where like in unix you know what makes it really powerful this abstraction is a composition like it's a combination of two things right it's like the the way you interact with the data is pretty uniform because well literally everything is a file and the programs are very composable because of pipes they do one thing you compose them together and then you can do the same kind of transformation over very different files using the same exact ideas and you can just keep reusing it as much as you want and the more i thought about it the more it felt like it would work for streaming and then what happened is that i just kept going and now it turned out into this vision where yes i do believe that's another way of looking at usability for streaming because type stream pitches what if you could interact with your and with your kafka toopics what if you could write streaming applications this with the same simplicity you actually uh do some work on your files on your file system and and yeah and the beauty of at least in my opinion the beauty of this metaphor is that it actually works perfectly from the perspective of the file system too like meaning you know i have typ stream as this virtual for system where you connect it to a kafka cluster it will create a view of this cluster where you get each topic is its own part and that's how you addressed it right you have this so that you can do cut name of the topic which is a part so you're literally you're literally a something like you can run typ stream and go cddf toopics cat users onto grep berlin onto berlin users yes and and and you know the basic the basic metaphor that i appli that already works like even though typ stream is a very young project also very complicated one so it's bit hard to to to to get to to get everything up and running and it's also definitely my first programming language and bash like programming languages are very complicated because of bad words like because you like until you get to the final process of compilation you don't even know if that string you're looking at is actually a string or a p on the file system it gets very complicated very fast yeah it's it's very tricky or variable it can be a lot of different things right so the the thing that really clicked in my head is that when i started looking at tp stream from a like more of a perspective right because when i started talking to people and say oh this sounds like it sounds like a good idea with like you know i met people telling me why isn't this like the default way we interact with with data which is very cool feedback right it's like it sounds so easy why are not we doing this by default and well i have two answers one of them is that i have no idea why that's not default in a way which which feels very suspicious because we're doing this for 50 years with unix and am i the only person that ever thought of applying the concept of unix to something we literally call data pipelines it's literally in the name right yeah the operator we've been using for decades every day several times a command is yeah i feel really strange about it it's like i at the first person thinking about this and that's why i kept trying to sabotage my own project and say yes if i do this it doesn't work and this is how i discovered how the metaphor works in reality with the like it works beautiful beautifully in context i had never thought it would work let me give you one concrete example are you familiar with tiny bird the the the start yeah you i think you might have told they do click house apis in front of click house right yeah and it's very clever like i like you know when the first time i ran into it it was a bit like i think it felt a bit like the first time i run into react that i'm like why is everyone saying this is cool i'm not getting it it's it's my fault right let me spend two minutes on it and then i and then when i realized that they would expose a api for you and then you instead of consuming the data from just a manag click house you can build on top of it you know it clicked in my head in two seconds saying that well if i apply the metaphor to typ stream and i have a media file system i can mount a a web web server and say you know then cut topic grab you know berlin and then you can you know redirect this into a a web server that is a sl media server one sl endpoint which is the actual endpoint you expose to the public ah so processes as like entries into the file system yeah and then also be eventually there'll be slev postrest too and you can exactly and it works like you know what's interesting is because it works both ways in unix it will work both ways in typ stream i mean if i get to building it and both ways i mean then you can end up with a oneliner where you actually get data out of post address you process it on top of kafka you throw it at a topic but also you take the topic with te and you also expose it with have web socket because why not i mean that you know the whole the whole idea behind it is that it looks exactly like unix and that composability is really obvious for us in the unix world right it's like we do that all the time right we like like you you might send data via aring well arn it's like kafka connect it's just ar syn in typ stream right conceptu yeah because it's kind of the same idea yeah i can see that i can see that yeah and and and and that's kind of and that's kind of you know that's kind of what fascinates me and keeps me working on this project which i will not deny it is technically the most difficult project i worked with because there is way too much going on even like you know because we ignored one thing that typ stream does so far which i think it's worth mentioning that is there is a reason why it's called typ stream apart from the fact that it sounds nice and it resembles other things that people are familiar with it's because all these data pipelines they're all typed and i think that makes a very big difference compared to other solutions where you know you would have to bend sequel significantly to act achieve the same usability so let me give you an example for example when you do cut topic pipe grap something if you use a be word typ stream will do exactly what grap does that is just like it just looks at the whole line which in in the context of typ stream is the old record and just looks for whatever you passed right that that makes sense but of course grap like you can use this i think it's called square brackets operator that creates these conditions right in bash there is the square brackets operators and you can use it in in if statements and i i i started thinking about it it's like you know to make grap more usable i can use the same conditional concept to grap and then now for example typ stream does things like you know you can cut a topic and then you can grap and look that you know a specific field is bigger than 500 now this doesn't sound very smart because of course you expect a system like this to give you the ability to to to i don't know give me all the books that are more than 50,000 words what what i think makes typ stream more usable and it's kind of easy to do once everything is a programming language like the technical problem you're solving is a programming language is that well i can actually type check the pipeline you give me because i know i know because of the the the schema of the data i know the the type that the whole pipe is so i can infer each single step of this pipe and say okay the book schema it looks like this there is a title there is a and it's a string there is a word count and it's a long and you know and so on and then you do this when when you get to the grap operation i know that the field words is not there because it's called word count and then i can actually tell you i cannot run this for you i cannot comile it for you which at first sounds like just a nice feature but then if you actually you know apply it to the whole idea of riding data pipelines every day with this with this technology well you end up in a place which we just give for granted in the whole industry that is not streaming which is just obvious that it just works like this everywhere as long as you use a typed language but you know in data streaming it sounds like it sounds like a very advanced feature and to be honest it's i think it is no i think it is because i mean i use things like cfa streams yeah and sometimes it feels like most of my time is getting it it won't infer the types and the serialization yeah stuff and i just have to teach it how to deal with types at every single step yeah yeah yeah yeah it's and you know so but what i meant to say is like i i i know why you say it's advanced feature and the point i'm trying to make is that it is only because we're not tling usability because well it's really not advanced feature conceptually because it's something we be able to expect yeah exactly my point even though it's hard to implement that's what i'm getting it's that not easy i i i will agree with that but the point is that when you look because you know if you look at a cfast stream application that gets data from two topics it joins them together and then it filters out some of the the data for some business criteria and sends the result via websocket with a forage these 150 lines of java code or cotlin code or closure code whatever you use as long as is jdm well there is a lot of details going into this because there is no abstraction it's not cfast stream's fault that they don't solve this problem for you it is really out of the scope of the library to solve the like the serialization the i know exactly what you mean about this find yourself there is like okay oh i have to tell that the resulting type is this thing that i don't have yet and in every single project i worked with we ended up with some sort of like we would call it like a hybrid serializer that for all the internal steps of the data pipeline we would use ajon cizer so it would be easy to make all the you know to do all the operations really fast cheat in the middle yeah because the truth is that when you work in a project where you use these a lot well this happens every day and you're not going to do a new up schema for every step of every single pipeline well the beauty of of typ stream in a way is that well typ stream can do that because it will just compile it right it will compile every step and figure out what's the right schema and it can also output in different formats because again well you can just have a two jon or two csv or two whatever command at the end of the pipe to change the encoding because that's how that's how you would solve the problem in unix right l like i didn't invent it again i think that is actually literally a com in unix i never remember its name that changes the encoding from dos to unix i think oh there's unix to dos and dos to unix and and the idea is the same right all i'm saying is if you want a different format you just pipe it into a into a tiny program that its only responsibility is taking whatever you're giving it and changing the encoding that's all it oh so so one day type stream will support pipe jason to ao so right now so no so so it already supports this like automatically at inference level the point is the point is that if you do a pipeline where you start with a schema and then you grab start you grab you grab something out of it and then you redirect it somewhere else conceptual is very easy to imagine that the the end topic should have the same schema as the starting one because you did not change the schema right you just like filtered some data out but then if you filter some data out you should change the schema so this type stream right now does it automatically by assuming that all the like so what it does is like it it iners the encoding of the pipeline on the fly and it will figure out that none of the data operators you use changes the schema so you get the same schema as you already had when you started okay and as soon as you use something like uh cut or wc or join which by the way i discovered while working on typ stream there is a we have to talk about joins yeah there is there is a joint command in unix i had no idea about this i discovered while working on typ stream i know that yeah yeah yeah you know this is how actually the first time i started working on typ stream i was debating it with a friend of mine shout out to bruno because he's very helpful into doing this you know like remote rubber duck where we told on on signal or something and like and then that's the exercise with it is like can you find semantics that work with you know because some of these are really obvious like grap is filtering wc is a aggregation cut is literally reading file and you know the bigger than is literally the two of kfast stream and i that's how i discovered that that unix has a join command where you can join two fil you're just thinking surely unix has already solve this problem somewhere that's kind that's kind of my that's kind of my point right the point is that i didn't even know that unix already had a program because it's like i'm gonna have to do a join command because otherwise i cannot join streams that like that are from different sources together and then when i started looking join unix i could not believe the comment was already there and i started using it with files to understand how it works of course it has limitations compared to the joint semantics of streaming but still the basic idea that the metaphor just keeps giving is what get kept me going with typ stream so before we move to joins to close the the the the encoding part well to be honest this is one of those things that every single person i got started with uh with the data streaming has struggled with because you have to handle encoding on your own and from from the perspective of a typ stream user it's completely transparent unless unless you want to physically change the encoding saying from abro to protuff there are things that i have to a out from a syntax perspective but it already works it already does that because well the point is that typ stream is a remote compiler so when you give it a string that is actually a pipeline it does all the things the programming language would do it looks at the pipes it figures out all the operators it tells you well you cannot use grap with this field this field doesn't exist yet right or and anded the beauty of uni metaphors in this context that it works with both kind of commands i have a http command so that you can you know pipe things into an enrich block where you get all the data coming in from a stream do a remote http call and then have a resulting stream coming out of it which is a non-trivial cf stream application say yeah abolutely to write and the http comment because everything is a stream and because everything is a like because the metaphor works everywhere it just worked at first try like it just works with the with gp it just i i literally discover that it would work with the other data operators by testing it after i finished the first implementation because i wasn't even sure that it would work but it actually worked immediately which i you know which i think in a way it's a form of validation that's this might not be the best solution ever but at least is a concrete and valid way of looking at the problem space of improving usability in data streaming because even copilot agrees with me like you know i i i think i think i tell this to every person i meet that is when i write docs for for for typ stream and i'm writing like i don't know today literally today i added the minus b option to grip for some reason i thought it was already there but it wasn't it's the the option that inverts the match right so i i added this option i started writing some dogs and co-pilot just like finished the line in in the typ stream code right right because you're reusing the metaphor so perfectly yeah yeah yeah because co-pilot doesn't know that the code is typ stream copilot just thinks it's a shell script right it's a little shell script and and and that's what getes me very enthusiastic about it like like even co-pilot that is a heartless machine sees the sees the yeah yeah yeah i you know i have to ask you this is this is something in a side but i feel like i have to ask you this you're so you're building out an existing language with a lot of lot of conventions and a lot of rules it's your first programming language you've written you're writing it in cotlin which is your first big cotlin project yeah you're doing not just a programming language but programming language with type inference yeah i mean how large a mountain are you trying to climb lca no it's fair look it's so i it is a very fair question yes it's very large i don't i will not deny this it's like you know i'm working with a friend out of aut way we cofounded a small company behind typ stream like because typ stream is to be clear typ stream is fully open and i believe that there is no other way of doing this like i i'm not even like you know i'm like i'm not even gonna argue for any other way of looking at typ stream from a from a project perspective because i don't think it makes sense like i actually timing is perfect because i like six months ago i used to say terraform would never become so successful now it feels a little strange to say it if it wouldn't be open sour in 2014 and and i i think you can tell about how badly the open source community reacted to to ashor changing licenses so yes ty stream is openers project and the scope of it is immense i i i agree with you right and there is no doubt that there is a bunch of first things i never did i think that you know to be fair the the fact that is my first large cine project i that i don't really care because i like that that's not that that's not that's never been an issue for me because for me languages are literally tools like i don't like you know i don't look at it as this it was not even hard to learn i just needed a real world project that's how i ended up using cotlin i i will say i will say that it is difficult from a programming language design perspective because of course i've never done a programming language but you know it's kind of funny because if you look at this of the people that build programming languages when you talk to them they they will always say like i had no idea what i was doing and then you end up with python and with ruby and with with javascript and yes some of these languages have obvious quirks in inside but in a way i feel safer from that perspective because well i'm not really inventing the synx right i i have to borrow ideas from other languages that that bash doesn't doesn't have because it's really hard to do stream processing without the concept of block like you know if you because if you're like mapping data like in a in a in streaming it's really hard to express that without the block syntax and bash doesn't have that actually like you know the like the the typical lump that you would have in java or in cotlin it's not really there in bash so i i had to add a bit of synex coming from other languages there is a rust project out there called nell that's it's kind of funny because they solve a very similar problem that i'm trying to solve with typ stream but for shell only so they it's a shell where everything is structured data so you can write very very clever scripts inside your shell because the shell is aware of the data types of everything you oh okay very interesting and you know in a way again one one one more time a form of validation that you know the metaphor can work in the in the in the in the context but yeah so the point is in a way the the thing we really need for the project is of course to people to use it a little more like i have friends using it but ideally it gains a bit of traction so that some other people are interested and maybe the the company behind typ stream can hire a couple of like full-time open developers in ideally we would hire at least one compiler engineer that works with me that does nothing else than working on the ers project which by the way i think it's a dream job if you ask me but but you know everyone has opinions about that too if anyone listening would like to apply send your details now yeah i i don't do that yet because to be fair i'm doing this out of my pocket at the moment right okay yeah i cannot really hire anyone at the moment but but ideally that's you know the part you would want to take because i think it's also important for for a pro and and that has nothing to do with typ stream right it's more about my leadership experience uh of 20 years in in in the industry that for a project to be successful you do want the to know your limitations right you do want you do want to you know put the project in place with people that are hyper specialized into solving in one problem i do feel very secure of the the vision that i have for typ stream because i've spent so much time trying to sabotage it myself that i can't that well e either either i have a giant blind spot that i haven't seen yet but you know the more i talk with smart people like you the less i'm convinced i have a blind spot toward typ stream and the more i'm convinced that we should try it we should try to make work the full extent of division where as you said some alpan hour ago at some point there is a sl dev sl postgress table somewhere can use that to basically saying typ stream becomes a data programming language which is a very strange thing to say i think in a way i could see one day it's going to try and occupy the same space as say flink yeah right yeah connective processing tissue yeah and you know in so the when i when i started talking to people about the the the the project from a prod perspective one of the feedback i got most often was why why are you even trying to solve it as a programming language right that because there are other ways you can approach the project i think there is a company solving a very similar problem as a python library i think it's called bx bx and it's yeah bws and it's very clever very smart and it's a different approach and it's as valid as typ stream the difference is that because it's a programming language typ stream is something that you cannot have with these other projects not because you know typ stream is better it's literally because of how the typ stream is designed and because it's a compiler kafka stream is one runtime and i think this is not obvious when you talk about it typ stream for the first time because right now it looks like this you know the naming in compiler space it's really funny but you know there is a front end a back end and midle end i know you know if you never heard it's really hard to believe there are midle ends but it's true right so the naming is a bit strange but the com the because it's a compiler and it's thought as a compiler nothing prevents you from changing either the front end or the back meaning you can compile to different runtime you can compile to pulser you can compile to memphis you can compile to flink why not to flink job because all you need i'm trivializing the not the scope but the simplicity of the the task but the scope is very well defined what you need is a one-on-one mapping semantic between the data operator that typ stream offers and the ability of expressing that problem in the native code of say fling or pulser or whatever it is right now if you would look at the compiler the last stage of the compiler the one that creates the gafka stream application it's literally a like a walking algorithm of like you know basic depth first algorithm on the graph that does nothing else and every noe that that it runs is say oh what is this node okay that's a that's a grap well that's how you do grap in gafka stream oh that's a that's it's so you get to the point where like flink and pulsar and python will all become ar ures yeah on which you run yeah yeah yeah yeah from the perspective of typ stream they are just run times right now now yeah there is even a larger scope which it's a can of worms which probably we don't have the time to go through this specific korm that would be at some point typ stream can get to a place where it can run it can run pipelines where there are multiple run times involved meaning meaning you have clusters of all kind and then you decide to run the you can decide on the fly what's the best place to run this pipeline because and which sounds like science fiction level kind of technology for data for data pipelines but i don't think it is once you accept that typ stream is nothing else than a programming language and then you have available to you all the tech techniques that are literally basic stuff in programming languages like you know code code elimination or caching caching is very common in programming languages right and you know caching part of a computation of a data pipeline sounds like a very advanced feature in in other system but you know if you write cut a topic grab something and then you pipe it somewhere and i write something that starts the exact same way but then does one more gp and pipe somewhere else i mean nothing prevents us to use the first part of the computation is the same exact pipeline and those things are only unlocked by the fact that typ stream is trying to solve the problem at a in a way at a unreasonably low level that is giving you the ability to express pipelines with a language where the hardest cell would be you would have to learn a l language except you don't because the language looks exactly like your terminal that's kind of the final pitch right of the whole yeah yeah yeah may be a lot of work for you to implement each part but the the syntax is yeah yeah so i mean i think i think it's really interesting i think it's at least as valid an answer as the the sql is the right answer to this thing but i feel like we should just if unix is the metaphor and you're convinced that it's a good metaphor that seems to be fitting well we should test it in a couple of places places y test if the metaphor hangs together when we start doing more interesting things so the first one is joins tell me about how joins will actually work i mean so the the syntax of the joint comand in unix is very close to the syntax of of the join fluent api from uh from the cfus from cfast stream because it does nothing else i'm you know simplifying a little but it does nothing else says you know th those are the two streams you want to join together and this is how you stream them together and this is what i brought up when i talked about blocks when we were discussing bash syndex bash doesn't have that feature bash doesn't have the ability to tell you you know cap capture a little piece of code here and use this resulting code every time you do this operation and that's how the join syntax looks like now i i right now typ stream as the simplest join synex possible which is literally you can join two streams and will join them by key and i think it is a default window like it's all hard to called that at the moment because i you know because the scope is so big for each single problem i just try to glue them together there and say okay that's the infrastructure if you want to make joints really smart well there is one class that is the d the you know the abstract syntax three class representation of join you can make it as smart as you want because the beauty of the obstruction is that they just work with a class that it's called data stream inside the the code they don't know about anything else about the outside world so you can make the join as smart as you want and the way you would do it is by imagining a little synex that gives you the ability to to to a what what i think normally called the volum mapper because you have to map the result of that join and that synex is already there because typ stream supports command that is not unique standard called enrich which is the equivalent of map i sto the name i think from newell because it was okay literally one onone with the idea that i wanted and then the rest you can do it with options the way you do like if you look at the aring page like there are a million options because it's really smart program and if you think about it i'm pretty sure aring has more options than how many possible joins you can do yeah i probably agree with that yeah and and you know and maybe maybe i should you know maybe i should say this out loud because i don't think i ever did so far that i'm also like i don't think you know i don't think the one onone ability to express one onone every single thing that this streaming processing that already exists can do should be a design goal right i i think it would be nice to get to a good 95% and not do like you know the last 5% but i don't believe in in in the you know into we have to be one onone feature compatible because because this is not really an alternative right i'm not trying to build an alternative to cka stream or to fing i'm trying to solve a different problem the problem is interacting with streaming stream processing with this tooling is very very hard so i'm going to create a tooling that allows me to do 95% of the job with one line of code and then if i need to do something really complicated and i need one kafka stream up i can still do a kka stream up that writes into a topic and that topic just becomes a file system part for me right yeah yeah it's the how do we make things that are easy trivial yeah exactly exact so i i can i i can significantly more about the fact that right now if i was working on a cf project and i had to get the past four weeks of data from a specific place and filter data out i have to write a whole app instead of writing a line of code for it okay so so maybe moving on from joins then let me test it with another important thing in stream processing is there a unix way of handling state if i want to like roll up have a running balance for instance i mean so so the the funny thing is that this was one of the hardest thing to implement for me because so the the thing is that so that there there is two answers to it right there is the syntax answer and the physical answer the actual how does this work in typ stream and the the the syntax answer is that you know in a way as long as you have a unix command that makes sense like i test these things by sending people little pipeline and ask them so what do you think this pipeline does right that's i don't tell them what it does and that's how i figured out that for example the wc command works in a lot of plac where you have to do these basic count aggregations right you know i want to aggregate all the data by key as for handling state i don't want typ stream to ever solve the problem in a way because if you if you think about the typ stream is just a compiler like i say just with quotes because it's a little reductive for how complicated the problem is but at the end of the day typ stream doesn't actually solve the problem because it compiles down to cfast stream well cfast stream solves that problem right so what i'm aring for is that and i've read this question from other people as well saying look you know when you get to state when you get to storing data it gets very complicated and while i agree with all of it i don't think it's in the scope which is kind of interesting in a way and it confirms a part of this idea that i enjoy very much that is because it's just a compiler i don't have to deal with this problem at all actually because it's relegated completely to the underlining implementation of the streaming library that i'm using in in theory you know that's kind of a funny a funny part of the answer in theory you could compile uh a unix pipe written in in typ stream in into a streaming library that solves these problems on their own like bite wax does because why not right because what prevents you from solving that problem right that's in a way so maybe maybe there is something that is not clear to you from from from a technical perspective and i should spend a minute explaining it right is like typ stream is a remote compiler which which makes a difference right why is that relevant it's relevant because in production i and there is a variety of reasons why it works like this now and we could talk probably one hour just only about this honestly but in production i like typ stream requires you to use a a kubernetes cluster and the reason why it works like this is because as i don't want to solve the state problem for the stream processing i also don't want to solve the orchestration part which i think it's unclear to people that and it's my fault obviously because if you go to the website right now it's obviously not clear that typ stream also manages these jobs because well once again the unix metaphor just keeps giving right because if i give you a pipeline and i add the you know commercial at at the end of it it will just run in the background right oh yeah yeah yeah right like why not and that's exactly what typ stream does right when when you run typ typ stream as a little piece of code that says where am i running oh this is a kubernetes cluster and then it puts itself into kubernetes mode which means all the all the long running jobs instead of being in cor in cor routines which they would die as soon as the server dies they get they get delegated to kubernetes jobs why why am i bringing this up in the context of of state because i think those two things are actually connected because what you end up getting if you run types stream in production in your cluster where you have you know your you have your apps you have a name space with typ stream and all of these talks to gafka what you end up with is with a extremely simple way of running long running jobs on top of of kafka that do the almost the exact same thing that a kafka stream application would do with the difference that the thing will take you i don't know 15 seconds to write instead of two days and you also have to manage it yourself you know long term the idea is that right now the ps command again the unix metaphor just keeps giving the ps command is a bit dump because it just shows you the name of the app with the state but nothing prevents you to making this much smarter using once again a different aspect of the unix metaphor by exposing metrics of you know consumer groups cfast stream applications brokers all of these via the proc so one day type stream will let you say cat slpr cfus some job and you'll get the metrics for it yeah yeah that would be very nice yeah so right to for that to work like everything is already in place because you know that's kind of the exercise that i did it's like is this two hearts that i don't even know how to do so i just kept answering questions to see if i could go full circle with a problem and there is actually literally one thing missing there that is when you start jobs they reflect inside the file system immediately so what i'm trying to say is that if i implement that feature then what you just said would already work actually because of everything is a stream everything is a file and it's always the same metaphor so if i add to the file system a live job part that like basically the proc management i think that's how we would call the road map item in typ stream when we implement this well then this would already work and and and you know the the beauty of it like the longterm vision the beauty of this of this approach is that there is a lot of code already written that works with this kind of solutions and then if you have a script that monitors the proc it would be very easy to translate right that's kind of yeah it would be very easy to translate all these scripts and i you know i haven't even looked into the progress that we have made in in the past i think four or five years not more with the abf with all like you know this inpection that is a bit more low level trying to resurface all these matrix again via the file system because why not it would work you know immediately yeah i can say right and and then you know and that's kind of my you know that's kind of the the reason why i'm so excited is because whatever question i get even if i don't have an answer immediately i my brain thinks about it and then maybe while i'm swimming i'm like well i mean unix does this already and the answer is consistently you didn't invent anything you just realize that we've been doing this for 15 years we forgot to apply to data that's all we did yeah you've got a you've got a 50- year old book of design recipes draw from yeah yeah okay so that that leads probably my last big question which is possibly one of the biggest parts of a unix system which is the ability to extend those pipelines with your own commands yeah where is typ stream on the i can extend your language story yeah it's it's it's a beautiful question and so i i think there's two things that need to happen right one of them is really h syntax sl semantic so right now the language to be fair this is also relatively easy to do i just didn't get to it yet but there so the language doesn't have the ability to define functions and the reason why i bring it up because well one solution is obvious that is you add two features to the language one is the ability to define functions and the other is the ability to source functions and then you end up exactly where you ended up with bash and z shell where you write your own scripts and then you just searce them and then they just appear and they just work so now this solves a variety of use cases and i think it's very interesting that you can get a function that removes i don't know it removes all the the possibly pri privacy sensitive data from whatever stream it's coming in like if there is an email or ip address it looks like this whatever it is you will just remove it and then you have this tiny function you can call it pii and just use it on all your streams like in case pii pipe yeah yeah pipe and then it just works right and and that's also very f because in case you do something wrong type stream will just tell you well you know you appli this to a stream that doesn't have these types so what are you doing like and you could also relax it to a warning if you wanted to the other answer is a bit more complicated and i like i don't have a fully like you know fully formed answer i feel like there has to be a way for people to write their own native operators and you know with like let's make it as concrete as possible right so cut grap like you know these filter comments the wc ls cd those are what you would call the built-in shell programs right if you try to to do a man page of one of these programs you actually get one giant page with all the programs it always confused me i want something about one program but you don't tell me with the with the man page and what how do you um add more comments and i think there the answer lies into some clever application of the of a p like with the with like the concept of p in unix where i give you a binary p and you can put a file there now the the thing is that this language so this program is natively written in cotlin which i think it means that the comments that you put in the binary part got a bit jars i think there is no way out of it which is which is what connect does i think that's literally how kafka connect i don't know i don't know if you ever worked with this i think they're called simple message transformation oh yeah yeah yeah yeah they're very to be honest they are significantly more powerful than what people think like you know people because they don't know that scfa connect has this feature it doesn't even cross their mind that with just a configuration file you can get pretty far ahead with konet by cleaning things but if you really need that a special transformation that is i don't know some business logic that you only you are the only person knowing about it and cfet couldn't possibly solve that problem they give you the ability to like load plugins on the fly now again not not trying to bash on the project because i absolut love love the way they solve the problem it's not the most user friendly way of adding a functionality to it now what i expect the long-term vision that would be is that you know you would would make all of typ stream a java library and then and then you add commments by running your own typ stream version with your commments inside and then it becomes much easier right you have you have this main command that is sate typ stream and you are just you register your own commands which which just abide to some interface and well the interface will actually look really simple right because it's a function that takes a data stream and returns a data stream that's all yeah yeah i can see myself one day like scp in a jar to typ stream sl user local bin right yeah that's yeah that's kind of like and you know because we we shipped with typ stream an official because typ stream is a server and it's written in cotlin and it exposes a a grpc server it's not the most userfriendly way of interacting with typ stream so we support official um common line application written in go because once again big fan of using the right tool for the job and i think go is very opt for little common line applications so i i i imagine the common lineup might actually just support that out of the box i think a bit like kubernetes does with the cp command oh yeah yeah in a way probably do do that as well so userdefined functions are definitely coming and you know in a way what i like the most about the conversation is that they would be typed and i think and i think that's not immediately obvious how much powerful the other solutions in the space this approach would be because it allows you to express a a lot of problems in a very safe way right like the example we just made like if the stream is coming it has emails just remove it it's not so easy for me to imagine how you would solve that problem in different technology that is already out there that is both reusable and type safe that's the that's the thing i cannot see yeah yeah okay i think at that point we should probably say what state is the project in today and how do people get started playing with it yeah so what's the state so from from so if i would have to look at it with the most critical eyes i would have obviously say that it i don't think it's production ready for the most obvious answer is that no one uses it in production right so when production it is production rated by definition and it's not which means right now this would be the most valuable next step for typ stream would be putting it in production somewhere with with someone from like i say feature standpoint sl how you get started with it so there's two two two things about it to say so the the getting started experience i think it's somewhat pretty solid already like if you you go to the to the docs right now because i really really care for i think obvious reasons at this point the developer experience i try to make you know the getting started as you know as little friction as possible so that are you know i'm gonna say you succeeded because i i tried it out and than yeah the initial on boarding is very smooth if you've got very cool and and and i worked really hard for making that point like because the point is that there is a lot of things that that are new to you when you get to typ stream and because you know i used the react metaphor there i i don't know if you remember when react came out but they had i think literally the first six months on the homepage on when it was still on github not even when they had the official domain they had a thing saying you know can you please give this project five minutes before closing the top just because the idea is really really novel and you should probably spend some time and the reason why i bring it up is because of course typ stream is not under the facebook umbrella so i don't get that you know boost in trust right so in a way i had to make it as easy as possible just for that and and i and i i think the getting started is somewhat of pretty much solved the only thing that is maybe not completely straightforward and i don't really know how to make it easier than that is if you want to get the project amper running on your machine and develop kubernetes features then there is maybe two or three things you have to do but the getting started thing where you want to play around with it i think it's pretty much there there is a variety of comments already there working at the data operators i mean and maybe there is more comments than docs i have literally this as a next to do item in my list saying you know i should i should make more dogs because there is more cold than dogs so when i have more dogs than code then i can go back to to the cod again and eternal circle of program yeah it's it's a circle i do believe strongly that that documentation makes the quality of the project actually like your project is as good as your dogs but from you know some of the things we talked about today are very hard to so i'm not going to lie about it right like the idea that you can have these data pipelines that get data from extremely erogenous data sources like say post on one side monga in the middle kafka and then redit yeah that's i would say that's far but all the ideas we discussed about you know being able to write 95% of what you would normally do in a cfa stream application that actually already works like it like it literally already does all of it of the what what i would call the you know beta features the only thing that's missing is the ability to change the like to force the encoding of your of your choice like right now you either get your original a or proto buff or whatever it is encoding or you get json because that that's the linga fran of when typ stream join things coming from different encodings and well if i have a topic it's up the other one is and the other one is prot buffer what's the output supposed to be and i think there is no right answer so as a as a default answer we use jason because that's kind of the you know the structured ling of structured data encoding linga fran of the word right now like that's that's what is right there is nothing like it or not it's langage we speak i'm not not a big fun because of the lo loss of types obviously that's the reason why i don't really like that's encoding format and if you have the types it gets pretty cumbersome really fast so a very inefficient way to to bring the the the data around but it's it to me it was really obvious answer and if you look at it like a bit further not just looking at the golding it also makes some of the features that typ stream can offer really obvious because if you are like doing a pipeline that it ends up into a web soet server it's a very by chance you want that stream to be jason anyway yeah say that yeah that's why defaults to jason right that's also that so in a way the state of the project is i think it's in a state where if we get some production users it will grow really fast from here because it's very it's very easy for me to imagine that once i get past the the the the initial you know effort of getting it installed in production where someone would use it every day i think they i would expect them to use it 95% of the time because this is what i missed 95% of the time when i work of course i'm one person and then you know that's not how data works like you know i as a sample of one but i do believe and i i i have the feeling you agree with the with the basic the basic metaphor holds up really well for yeah yeah i think i think you've got you've got you've really hit on a seam of a good design idea to mine and you've got a lot of mining to do but that but you don't have a lot of design work to do i think and that's a huge accelerator that those design problems there's really difficult search space problems being solved on which note i should probably go and leave you to pick up your pickaxe and go mining down that seam of years you got a lot of code to write yes yes and a lot a lot of dos yes a lot of docs to write thank you very much for joining us luuka this fascinating idea thank you thank you very much thank you thank you luca i kind of have to say i think that's the most time i've spent thinking about pipes with an italian since super mario brothers sorry luca if you're interested in giving typ stream a wh check it out at typ stream.io there's a link in the show notes it's going to be a while before they have full support for s and orc but what they've got right now is a useful tool and also an interesting design study i think it's also worth taking a look at new shell which we mentioned briefly i'll put a link for that in the show notes too but if you're looking for a new shell or new shell ideas search for nu shell and with that i think it just remains for me to remind you that if you've enjoyed this episode please like it rate it share it hit subscribe because we'll be back next week with some more thoughts on how we can build the future of computing sometimes with inspiration from the past until then i've been your host chris jenkins this has been developer voices with luca pete thanks for listening