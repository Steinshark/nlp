in 1981 microsoft hired a former xerox programmer charles simoni to create a relatively new type of productivity application the word processor seven years earlier simonia had developed the very first graphical user interface word processor known as bravo for the xerox alto pioneering the concept of desktop what you see is what you get document preparation working with another former xerox software engineer richard brody the team produced multi-tool word microsoft's first word processor first produced for the xenx operating system it would eventually be released in 1983 on ms-dos as microsoft word free demonstration copies of the application were even bundled with the november 1983 issue of pc world making it the first program to be bundled on disk with a magazine microsoft word was unique at the time in that it was one of only a handful of dos programs designed to be used with a mouse and while it did offer many modern word processing features it lacked the ability to render fonts and was generally seen as uncompetitive with wordstar the leading word processor during that period the entire package of the first version of microsoft word for dos took up only 150 kilobytes with the core executable being just 17 742 bytes in size fast forward to today and the modern feature-packed descendant of this humble little program now requires a minimum of four gigabytes of disk space and two gigabytes of memory to even start the rapid expansion of software from simple text-based tools to massively complex feature-rich highly visual products would dominate the mass-market computing world during the 1980s and 90s and with this push came a higher demand on processors to both efficiently utilize more memory and growing computing power all while keeping costs at consumer accessible levels during the mid 1980s in response to the growing demand of software the opening moves towards the mainstream adoption of 32-bit process architecture would begin while 32-bit architectures have existed in various forms as far back as 1948 particularly in mainframe use at the desktop level only a few processors had full 32-bit capabilities some even operated on a hybrid model the popular motorola 68k for example used a 16-bit data arithmetic logic unit and external data bus combined with 32-bit registers and a 32-bit based instruction set early 32-bit processors proved to be expensive to manufacture and deploy requiring many compromises and limitations in order to cut costs in 1982 national semiconductors introduced its 32k series starting with the 32016 these were the first 32-bit general purpose microprocessors to hit the market however the bugginess of their design along with failure in achieving rated speeds prevented any significant adoption in 1984 motorola would finally release a true 32-bit successor to the 68k the 68020 produced in speeds ranging from 12 megahertz to 33 megahertz the 68020 had 32-bit internal and external data buses as well as 32-bit address buses its arithmetic logic unit was also now natively 32-bit allowing for single clock cycle 32-bit operations denser pin grid array packaging was now required to accommodate the larger signal count that 32-bit architecture offered the 68-20 became a popular desktop processor some notable use cases were the apple macintosh 2 and macintosh lc computers sun 3 workstations and the commodore amiga 1200 despite the wider architecture the 60-20 still lacked integrated memory management features requiring an external memory management unit chip later variants such as the 68030 would eventually integrate the mmu into the cpu making the processor family a competitive option one year later intel would introduce its own true 32-bit processor family the 80386 by the middle of 1986 full production would commence as mainstream adoption of the new architecture occurred compact would become the first to utilize the 386 on a desktop pc inherently updating the ibm pc compatible standard without any input from ibm available in speeds up to 40 megahertz the 386 was seen as an evolutionary milestone of the x86 architecture not only did it offer a new set of 32-bit registers and a 32-bit internal architecture but also built-in debugging capabilities as well as a far more powerful memory management unit that addressed many of the criticisms of the 80286 much like the motorola 68020 the 386 die surpassed a quarter million transistor count and also required the switch to pin grid array packaging to accommodate the higher signal count other high pin density packages were also used for surface mount variants the higher circuit board costs of 32-bit architecture also led to the release of the 386 sx in 1988. the sx model was a stripped-down lower performing variant that reduced the data bus to 16 bits in order to reduce system costs for lower end use the core 386 design was subsequently rebranded as the 386 dx the 386 architecture had a strong emphasis on backwards compatibility and for the most part it was able to execute instructions designed for previous generations of intel processors as far back as the 8086 this characteristic is still retained even on modern x86 based processors this was accomplished by extending the general purpose and memory indexing registers to 32 bits while retaining their lower halves as backward compatible 16-bit registers this allowed most of the instruction set to target either the newer 32-bit architecture or perform older 16-bit operations with 32-bit architecture the potential to directly address and manage 4.2 gigabytes of memory proved to be promising this new scale of memory addressing capacity would develop into the predominant architecture of software for the next 15 years the 386 took full advantage of this by offering one of the most sophisticated memory management units of the time it can run in a 16-bit legacy reel mode used by previous generation processors a 32-bit extended version of the segmented protected mode first introduced in the 80286 and a new highly requested virtual 8086 mode which allowed multiple real mode environments to be encapsulated within protected mode on top of this protected mode can also be used in conjunction with a paging unit combining segmentation and paging memory management how these memory management mechanisms work can be found in part 3 of this series the ability of the 386 to disable segmentation by using one large segment effectively allowed it to have a flat memory model in protected mode this flat memory model combined with the power of virtual addressing and paging is arguably the most important feature change for the x86 processor family this formed the basis of most modern operating systems as it allowed programs to be loaded anywhere into memory in fragments without any consideration for its actual physical location the operating system would both manage and protect each process through page lookup tables as they were shuffled in and out and around the system memory since the beginning of the microprocessor the basic concept of fetch decode and execute has driven every cpu's instruction cycle however a fundamental flaw with this begins to emerge as processors and their supporting hardware become more complicated during each phase of the instruction cycle the other two phases and their associated regions of the cpu remain mostly unused furthermore when a phase requires access to a slower external bus such as with memory access it creates a bottleneck to the overall efficiency of the instruction cycle particularly as cpu clock speeds begin to vastly outpace that of memory by the mid-1970s a solution to this dilemma was starting to emerge in supercomputers and mainframes using a design paradigm known as instruction pipelining instruction pipelining divides incoming instructions into a series of sequential stages that are assigned to different processor units because each of these stages are continuously operating on an instruction it effectively makes the instruction cycle operate in a parallel manner allowing more throughput and utilizing more of the cpu's logic on average cpus designed around pipelining can also generally run at higher clock speeds due to the fewer delays from the simpler logic of a pipeline stage modern risk processors that are built around relatively simple instruction sets that execute in one clock cycle on average are especially conducive to pipelining in pipeline processors a control unit is assigned to managing the sequencing of the pipeline the instruction data is usually passed in pipeline registers from one stage to the next via control logic for each stage the control unit also assures that the instructions in each stage does not harm the operation of instructions in other stages one common case of such a complication is when an instruction requires data from another instruction in the process of execution ahead of it on the pipeline because the instruction must wait for this data to be produced before continuing through the pipeline a stall occurs this data inconsistency that disrupts the flow of a pipeline is referred to as a data hazard hazards can also occur when a program branches known as a control hazard this condition emerges when a conditional branch instruction is still in the process of executing within the pipeline as the incorrect branch path of new instructions are being loaded into the pipeline once the branch is executed and the correct path is known the pipeline must be flushed and reloaded with this new instruction path resulting in a performance penalty another more obscure hazard condition is self-modifying code due to the inherent invalidation of instructions upstream of itself on the pipeline one common technique to handle data hazards is known as pipeline bubbling pipeline bubbling is the insertion of empty stages or bubbles within the pipeline in order to create a delay that resolves the hazard operand forwarding is another employed technique in which data is passed through the pipeline directly before it's even stored within the general logic of the cpu though more complex to implement due to the need to detect where it can be used this method limits the performance penalty created by a pipeline stall in some processor pipelines out of sequence instructions that have no dependency on the instructions caught in a stall may be executed strategically to create pipeline delays in lieu of bubbling known as out of order execution this technique helps reduce underutilization of the pipeline during the data hazard event control hazards are generally managed by attempting to choose the most likely path a conditional branch will take in order to avoid the need to reset the pipeline this is known as branch prediction and it can range from simple static hardwired path decisions to dynamic branch prediction where branching statistics are tracked by algorithm to form more accurate predictions branch prediction accuracy is further aided at the software level by structuring code in such a way that conditional branches are optimized to take a more statistically determinable path this is generally accomplished by assigning a branch event to a less likely code condition some cpus even forego branch prediction altogether and operate multiple pipelines in parallel with duplicated logic this is known as superscalar pipelining and it allows multiple instruction paths to be processed simultaneously when the correct path is determined the incorrect pipeline is abandoned while pipelining would grow in sophistication becoming a major contributor to the rapid advancement of the processing efficiency over the next decade especially in risk processors its mass-market beginnings were far more limited both the 68-20 and the 386 for example employed early forms of pipeline architecture the 6820 used a traditional three-stage construction decoding pipeline as well as several pipelining mechanisms throughout the cpu's logic the 386 however used a three-stage pseudo pipeline mechanism that evolved from the 80286's design each 386 pipe stage was a self-contained state machine that typically took multiple clock cycles to accomplish its function these stages were operated in a loosely coupled pipeline in order to increase the processor's throughput while pipelining targeted the efficiency of the instruction cycle the rise of cpu speeds in the 1980s presented another pinch point early on memory access was only slightly slower than register access but as microprocessors advanced much faster than memory especially in their operating frequency accessing memory became a performance bottleneck because using higher speed memory was not economically viable the concept of caching started to make its way into mass-market cpus to alleviate this performance gap in caching a small amount of high-speed static memory is used to buffer access to a large amount of lower speed but less expensive dynamic memory this effectively offered a practical combined performance at a lower cost in addition a system memory to cache data transfer can be optimized around the system memory's configuration and access availability dynamic ram for example requires periodic refreshing making it optimal to perform cache transfers in between refresh cycles caching also frees up access to memory for use by other system components that can access memory independent of the cpu this is known as direct memory access or dma much like pipelining cpu caching would quickly evolve in sophistication to complex multi-megabyte-sized hierarchical mechanisms that independently cache instructions data and virtual to physical address translation or a translation like a side buffer many modern cpus implement even more specialized caches that directly aid in their pipelining process early cpu caches were far simpler the 68020 for example cached only 256 bytes of instructions internally other cpus such as the 386 had no internal cash but could be augmented with optional external caching on their main boards for the 386 up to 64 kilobytes of external cash memory was supported though this increased overall system cost and complexity it would not be until its successor the 46 for on-die caching to appear on intel processors fundamentally a cpu cache holds copies of regions of data from system memory into units of cache storage called a cache block or cache line the storage capacity of a cache block varies by processor the larger the block capacity the more likely data stored around the current memory reference can be found within the block this is known as spatial locality additionally temporal locality or the likelihood of a reference being reused is also factored into cache design a block also contains a valve bit to indicate if the data cached within a block is usable cpu cache is organized so that fixed size system memory regions are mapped to cache blocks in the simplest scheme known as direct mapping each region of system memory can only be stored in a single cache block location via simple mathematical relationship a derived identifier called a tag that points to the memory region the block represents amongst all possible mapped regions it can represent is also stored within the cache block while simple to implement direct mapping creates an issue when two needed memory regions compete for the same mapped cache block though it would be possible to store any memory region at any cache block creating a fully associative cache this would require enumerating the entire cache to find a specific block as well as more memory for the tag such as design would be far too complicated and perform poorly in practice a common approach that compromises between associativity and complexity is called a set associative cache in a set associative cache the cache is divided into groups of blocks called sets each memory region maps to exactly one set in the cache but data may be placed in any block within that set in practice set sizes between 2 to 16 blocks are generally used on modern processors as they offer an excellent compromise between performance complexity and capacity when an instruction invokes memory access the cache controller calculates the block set the address will reside in and the tag to look for within that set if the block is found and it is marked as valid then the data requested is read from the cache this is known as a cache hit and it is the ideal path of memory access due to its speed if the address cannot be found within the cache then it must be fetched from slower system memory this is known as a cache miss and it comes with a huge performance penalty as it can potentially stall the instruction cycle while a cache update is performed on modern processors hundreds of instructions can be executed in the time it takes to fetch a single cache block when a miss occurs and the needed cache block is loaded into a full set the cache controller must decide which existing block to overwrite how this is determined is known as the cache's replacement policy replacement policies by nature must predict which existing cache entry is least likely to be used in the future and while many cache algorithms exist for this task one of the more popular is the least recently used or lru policy which replaces the least recently accessed block in some processors replacement policies can also be aided in software by code that declares regions of memory that are seldom reaccessed as non-cachable writing data to a memory location introduces its own complications as the cache must now synchronize any changes made to it with system memory this is known as the cache's right policy and it may take one of several common approaches the simplest policies known as a write through cache where data written to the cache is immediately written to system memory another approach known as write back or copy back cache tracks written blocks and only updates system memory when the block is evicted from the cache by replacement some more sophisticated policies pull together multiple writes and process them in groups utilizing system memory access more efficiently keeping one or more caches synchronized must also be addressed in cases where devices using dma change memory independent of the cpu in multi-processor systems this also becomes an issue communication protocols known as cache coherence protocols are generally implemented between cache control logic to manage this because of the performance penalty associated with accessing system memory virtually every aspect of a cache's design is made to achieve a maximal hit rate this has become even more critical as processors expand in complexity even further during the 1990s relying more on caching and pipelining to achieve higher processing efficiency particularly in the rapidly evolving world of risk processor design the tight integration of these new mechanisms would even push the evolution of the tools used to develop software with so many variables that could disrupt the optimal utilization of the instruction cycle the direct interaction with processor instructions by human hands has for the most part been made obsolete by the software optimization of code you