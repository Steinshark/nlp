this week on developer voices i'm joined by ryan worldl who's taken an interesting career path from being an engineer at data dog the monitoring at scale company to the co-founder of his own company warp stream he has jumped ship in order to reimplement apache kafka in go with the fundamental design decision that they're going to use s3 for all the storage the logic to this being that if you want to build a platform that stores and processes vast amounts of data s3 is going to sacrifice some latency for a storage system that's vastly cheaper than local discs and endlessly scalable in order to unpack ryan's journey we go through his experiences at data dog the things he's learned trying to replace live systems that are currently dealing with ludicrous quantities of data how and why he chose cfa as the target for his next project and why he chose go as the language to do it in and of course fundamentally what happens when you try and build a database like thing that doesn't really have its own diss how do you do that what technical problems come up and how do you rewrite a decade old apache project without falling into the trap of memorizing the whole source code surely there are going to be some nasty surprises along the way let's find out i'm your host chris jenkins this is developer voices and today's voice is ryan [music] well joining me today is ryan well how you doing ryan i'm doing great thank you for having me oh it's pleasure i you've you've been at one of those places that always seems to me to be a hot bed of really thorny interesting problems because you used to work for data do for a while and they being a repository of half the world's logging information have some serious scale and data headaches yeah that's that's to put it nicely it's a data dog is a very complicated engineering problem that you know had to evolve as the cloud you know cloud native companies or cloud first companies were were starting so so data dot got to start out very simply but now it's used by huge enterprises that generate a ton of data so it's a it's a very interesting challenge i've worked in the past for companies that were brought down by their own log files but company gathering everybody's log files must have had some serious problems give give me give me an idea of what you worked on while you were there because i know it's going to lead into what we're really talking about yeah so my co-founder and i both worked at data dog for roughly three and a half years or so i think that's about about right and during our time there we replaced the legacy search and query system for what data dog calls the event platform internally which is if you look around at data dog's uis you can kind of notice there's some similarities between the different products and at a high level there's like metrics and then there's the event platform and most of the products are some amalgamation of those two different types of things so like metrics which are the pre-aggregated time series and then event which look more like structured logs so most of the products that they do are some combination of those things pretty graphs and tables yeah basically but they're two pretty distinct storage systems and two distinct like even teams within data dog and my my co-founder and i joined data dog to replace the legacy version of the search and query system for the event platform which you can kind of think of just the the data being stored there is like json with a time stamp attached and then you want to search for it with full tech search and run analytics queries like aggregations and group bu and stuff like that yeah again that's one of those problems that seem simple in the small i imagine it gets horrific in the large yeah it's the there are a lot of challenges across multiple dimensions when it's a multi-tenant product i think the the sheer volume of the data is is one of them but because it's a multi-tenant product it's not like all the data is in one place you know it's there's a bunch of different customers that have their own isolated sets of data so that makes some problems easier but it also makes some problems harder because the the variety variation in the queries that you have to run they'll hit every different edge case of your query engine and every different edge case of your storage system because of the you know the size of a log might vary by 100x between customers some customers would admit huge logs that had tons of stuff in them other customers would emit a really high volume of really small logs there's there's there's variation all over the place there so it's there's yeah there's a lot of different pieces and parts that you have to to get right to make the system work as a whole for for everyone yeah yeah i can imagine that's one of the things about running cloud services is for for every corner case in your code base there's a customer who who absolutely exercises it right y exactly yeah what what's the what do you have any strategies for dealing with that at those kinds of scales so i think the strategy that was the most effective for us because we were migrating we were migrating all of the existing customers their same queries the same data set ideally they weren't going to notice anything different about this migration wall was happening so the strategy that was the most effective for us was shadowing where we would ingest the same data set in both systems and we would shadow the queries from the live system to the new system called husky that we built and over time we would crank up the shadowing percentage from like you know 1% and then eventually we make it all the way to 100 and we would log any of the mismatches between the old system and the new system and that gave us the confidence that we weren't changing the behavior and that we weren't creating any new bugs that we didn't know about obviously there were some old bugs that we we found along the way some old bugs that we behaviors that we considered a little weird that we had to keep because they were you know we needed to be compatible with the old system but i think that was the most effective one for us was was shadowing yeah yeah i've heard a few people say that as a strategy for replacement like don't try and do the big bang rewrite if you can but if you have to then at the very least keep the old system running around while you check the new ones the same yeah it's obviously expensive to do that but i think the way that we did it was basically we did it a product at a time and then subsets of tenants within that product at a time we didn't migrate so we didn't have to run full capacity versions of both things all the time yeah that also gives you to ramp up the new system right yeah yeah i mean and this was a migration that took from start to finish it took multiple years so it there was not there was never a date where we had to like you know shut off the old thing and be fully migrated to the to the new system it was a pretty gradual process because as we were rewriting or creating husky we had to we started with the simplest products for first in terms of what they the features that they exercised in the query engine and the the first product that we migrated was network performance monitoring which was like network flow data it's the product the needs of that product from the at the time from the career engine were pretty simple it's just aggregations and group by there was no full teex search there were a relatively small and static number of columns in the data set so it was it was a pretty easy one to do the migration for and then i think the last one was logs which is like a huge product at data dog with you know touches every part possible edge case it has a bunch of special cases because it's a really old product as well so it's like there's so along the way we got to the the shadowing helped a lot both so we just picked a good path through which to do the migration we didn't start with the hard one first we picked the the easy one first yeah cut cut stairs into the hill that kind of thing the one thing you haven't mentioned is what is it that made the old system legacy what made you think we need to get rid of that yeah so there are a a bunch of different issues with it but i think that the the the big one from a cost perspective was that it stored all the data on local ssds so it was not a cloud native architecture by any means like the the data was ingested onto the same nodes that queries were running against the data was being written to local ssds so it was just not a very elastic system and as you might imagine at a company like data dog being able to elastically scale up in response to data ingestion is like a super important task it's like that's the thing that the customers are paying for essentially it's like we don't want to think about what it takes to run a system like data dog we're paying you for that we would like to be able to just send you 10 times as many logs you know between one day and another because maybe there's a black friday event and we're an e-commerce company or it's like a big the the super bowl or like what there's so many things that could be going on in the world that are seasonality you know driven things for the end user and you need to just accept their big volume of data because that's the whole that's the whole point and that caused the and and and that worked before before husky like it did work and but it was it was a lot of operational burden on the team that was running the system at data dog basically it was like backed by people who were on call and would get ped these things would happen and would go deal with it basically instead of it just being handled automatically in software by the system yeah yeah and throw people at the problem as a fine solution until you find you are one of the people right yeah exactly with and during the migration we got to have pretty strong taste of what it was that we were replacing in terms of being on call for it so we yeah we we felt the the pain for sure so it's interesting that you describe servers running in the cloud using ssds as not cloud native interesting almost to the point of being controversial yeah i i'm okay with that i i think that when when most people talk about cloud native things these days they're basically just not like they're they're just not describing reality like they're describing software that could have equally have run in a data center just the same as it does today on like vmware vms and would be no different from an experienced perspective the thing that makes to me what cloud native means is leveraging higher level or even like it's it's hard to even describe it in terms of levels but there's like there's things that can't exist outside of the context of a hyperscaler cloud provider like object storage like that fundamentally can't exist within a data center environment because in order to get the at least not at the same cost i should say the api you can mimic but to get it at the same cost you need hundreds of thousands or millions of spinning disc hard drives and you just are not going to get that in your in your data center so the what makes it cloud what makes something cloud native to me is not just we took something that would have worked at the data center and installed it on cloud provider vms that's basically just what lift and shift means which people don't use that word very much anymore because it's kind of like a a 2010s cloud migration word but that's what people have been continuing to do is they just take software that would have worked just fine in a data center on vms and they run it in the cloud and the only thing you don't have to do at that point is figure out where your vms are coming from but it would have just been the same thing as it would be in the data center yeah yeah okay i can see that so your definition of cloud native then is something closer to akin to pulling the computer architecture apart and saying okay dis is a service cpu is a service the bus between the two is a service it's i i think that it doesn't need to be so highly articulated into that ty that type of a definition i think it's just it's kind of a you know it when you see it type of thing like it depends on what the system is doing whether or not it could be whether or not it's it's cloud native but i think that the the shift towards the shift towards serverless architectures makes it a little clearer i think that basically if you have to provision something in terms of individual you're thinking about it in terms of i have vms somewhere that like you're like if you go to a database provider and the thing that you provision is i need this number of vms it's probably not cloud native now it may use cloud native things under the hood like with aws aurora like you do provision a vm but like they have this massive scalable storage system under the database that makes it much more magical than you know vanilla postgress and star on bm so that's like kind of on the border of being cloud native but it's i think that's i think a good way to to think about is if you have to go provision a vm with a certain amount of storage and ram cpu it's probably not a cloud native thing okay yeah that's fair so what i want to know what the results were of this migration you did for husky you called it because it's going to lead into what you did next but what was the what was the after picture of that system so the obviously saving money by moving data from local ss object storage that was a huge winner that's not obvious why was that such a difference the the math behind it is pretty straightforward i think to explain if you're replicating data three times on local ssds you know you can go aws and you can get it's easier to talk about for ebs because ebs is a you know it's a specific list price per per gigabyte compared to instant storage where it's baked in with the the hardware but ebs is roughly eight cents per gigabyte and with the legacy system it was trip replicating the data onto ssds so you need three of those that's 24 cents per gigabyte and then you can can't run the system at 100% capacity because that would not be very elastic at all so let's say you wanted to run it with 50% free space so that you could handle some burst before the new vms come online for when you want to scale up y and that gets you to 48 cents per gte and that's like that's you know in the abstract what does that number mean compared to s3 s3 charges 2.3 cents per gigabyte per month so it's a pretty dramatic cost savings if you could move the data from trip replicated local ssds with 50% free space to s3 where you pay for exactly the number of bytes that you have stored and that's it so that saves a bunch of money obviously the system needs to change it can't you can't just swap one out for the other directly that's why we had to rewrite the whole thing and it was a huge project that took a long time but that was a that was a big winner the but i think those are actually not that interesting basically the like so the saving money ones are are obvious and that's a big motivator to do the project but the the product level stuff that you could do once you had this it it lets you kind of uncouple the like data retention becomes more affordable at that point it's not just the data ingestion part that you're thinking about anymore it's now you can make products that are more useful when you retain the data for longer because you're no longer tied so directly to the the cost of storage so you can offer different products basically and the like husky is obviously not the only part of this but do announced a product called flex logs at some point and they i don't know exactly what when it was but during one of their recent conferences and it's a completely different model from what they were selling the like the standard tier of of logs for so retention is very affordable in flex logs but you pay to provision compute that's sized differently based on how much compute you need to do to run your queries so it's like kind of decoupling those two things now and then along the way we also removed the limitation of the old system that you would need to pre you have to specify upfront what fields were in your logs so that you could actually query them like that was a limitation of the old system where you had to say oh the t this field name is a string type and like if i don't specify that it's not searchable like you can see it if you could find that log through some other way and then you could click on it you could look the field would be there but it wasn't searchable so we remove that limitation which was a huge product enhancement there's yeah there's a the the migration was was super effective across a bunch of different axes it was a i was really like working on that project was was really fun because it was a technical challenge but also like the the business and the product stuff that we got to drive as a part of that was was also really fun to see i can imagine it was very satisfying very very very rewarding from the point of view of the results and also good brain food for what came next i assume because what you did next isn't that far away but it is a bit of a jump so take me to where you got next yeah so along the way during this migration we like basically when it was over looking back we we were saving a lot of money because the big like the high pole in the tent from a cost perspective was the legacy search system once we had changed that like we you know removed a huge amount of the cost from the legacy search system we were looking around at what was left basically in the costs and kafka was a huge part of it and at the beginning we would have never guessed that i think going in just because of the ratio like the way the ratios were but by by the time we were at the end it was a huge a huge fraction of the cost we my my co-founder and i left data dog to basically solve that problem and some other problems related to to kafka by taking what is like philosophically the same approach that we did to the legacy search system at data dog and applying a lot of the same techniques to kfka so it's so we built warp stream which is a drop in replacement for apache kafka but it stores data only in object storage not on any local discs which is you know philosophically the same thing what that husky did with the legacy search system we're gonna we're gonna get into that i just have to ask you before we leave the data dog thing what was your motivation there did you look at i know data dog's bill summary and think this is going to be their next target let's jump sh build a service and sell it to them when it's ready or did you just think this is a really interesting problem that we know how to solve so i actually had pitched this same idea to my co-founder before the like when we were working on the system before we joined data dog that eventually was like the inspiration for for husky i pitched this idea to him too basically what if we could take kafka and put it onto object storage and the other idea that i was pitching to him basically this was was like early 2019 i pitched him these two these two things and the the second one was let's build a colmer database for observability those were the two ideas that i had he did not think that the cka one was very interesting at the time he thought it was a little boring i think which it is it's not like a super exciting idea but the also it's just like he didn't use kafka at he was working at uber at the time on their metrics database called m3 and they they didn't use kafka for for m3 they had some other system that was like an inmemory message bus that they used because it was cheaper than cofco they didn't they didn't need the durability i guess right so he didn't see the value in it i think at the time but i think once he was a data dog for a little while he he really got it and the the problem is very industry agnostic i think basically every vertical needs something that looks like kafka for some reason or another even if it's just something as simple as you're using it to move data from point a to point b and integrate it in between disperate systems that's like kind of a universal enterprise challenge which is why people use kfka it's kind of like a one place where people can put all of their stuff as it's moving around in a common api to to integrate things together so it's it doesn't really have anything in you know specifically for for data dog but the the problems that we set out to solve like mo were most acutely affecting companies like data dog i think with the and we'll we'll get into it a little more from the technical perspective but like the cost of storage and then the cost of inter zone networking and the having to be scaled for peak load those i i think within observability security iot those those vertical i think feel the problem most acutely but it's it's pretty universal and most companies nowadays have some slice of all of those things within their within their enterprise like you know financial services company has nowadays they have iot data they have security data they have observability data and maybe it's in kafka maybe it's not because you know maybe they outsource this function to a vendor but a lot of them don't or they do hybrids where they use a vendor for the super important stuff that they want the best experience for or some department within the enterprise uses it but another department doesn't they self-manage the that system so it's it's kind of all over the place basically the these problems are pretty universal yeah i think they they almost inevit they almost inevitably crop up as soon as you find a company large enough to have several it departments then they will have data integration problems yeah yeah yeah exactly there's there's so many enterprises out there that they're like the way that they evolved historically is through murders that inevitably things are a complete mess on the inside and they need huge amounts of data integration it's yeah it's a very like there's kofa is very effective at being the single place where people put everything behind a common api it's a the data warehouse is a little bit too opinionated as the as the place to do all that integration work because developers for whatever reason don't i don't know they it's it's hard to get everything integrated in into one place in sql because not everybody wants the the schemas or the the schemas for a sql table are a little they don't quite match what their application does but with kofka you can just show up whatever bites you want in there optionally you can use schema registry and stuff like that to keep get a little bit under control but kofka is flexible enough that you can make it work for for just about anything yeah yeah sooner or later someone wishes they could just dump the raw data and someone else wishes they could just read the real data and then you build stuff on top of that okay so let's let's get into the nitty-gritty of warp stream and let me put it this way so someone coming to kafka for the first time would probably hear the phrase we separate storage and compute and they would think well okay so that sounds like it's going to be fairly easy to separate the compute from s3 storage isn't that job done so take me from that very beginner's perspective to what you actually had to do yeah so first of all a lot of people don't under like when you say that you separated the storage and compute for kafka they don't quite understand what you mean because they think that kafka is just storage so what's the compute which is somewhat true i can understand why people would would have that first reaction and and when for for kafka when i say that the the thing that i mean is basically there's the when you're talking about apache kafka i think the way that the best way to think about it is like there's the broker that that presents the tcp server that the the thing that clients speak to to read and and write data and do metadate operations like you know stuff with consumer groups things like that and then there's the local disc on the broker that is the storage and the combination of the replication protocol and all the local discs on the brokers is the the storage system basically and the the compute part is the interacting with clients and getting them the data that they're asking for when they're reading something like translating the i want to read topic a partition one offset x translating that into reads and writes on the local dis on the machine that's like the compute part that that translation so there's there's a lot of hard problems along the way when when solving that and i think that the the biggest one that we had to solve was kofka makes files on disk that are per topic partition so like there's when you think about what what a kofka partition is it's essentially a log of the records that you wrote to that partition over time but there's a few other files that go along with it there's a file that's an index by time and then there's a file that's an index by offset so that you can seek efficiently within the within the partion right yeah the the problem when you translate that into object storage is the let's say that you you understood that object storage the latency is going to be a little bit higher than it would be on a local dis so you want to write a new file so that you could acknowledge new records that some client wrote you wanted to do that every 250 milliseconds because you don't want it to be too long you want you want it to be you want it to feel reasonably real time if you do if you write a new file to object storage every 250 milliseconds you're going to pay roughly $50 a month month in put requests against like for your object storage so if you have one partition the minimum cost of that partition if you write one record every 250 milliseconds is $50 a month which is obviously not going to work so we had to figure out how to solve that problem first and the way that we did that is instead of writing a instead of writing files that are per topic partition we have our stateless broker thing that we call the agent the agent writes a new file to object storage every 250 milliseconds but that file contains records for all of the topic partitions that were written to that agent in the last 250 milliseconds so a file we've now grouped all of the records together for each topic partition into these like 250 millisecond windows of time we also do it based on size so if you if you write more than 8 megabytes of uncompressed data within that 250 millisecond window will also make a new file but that's the the basic idea so once we once you do that and you youve decoupled those those things you can get the cost down to around $50 a month in put requests per agent which you you run way fewer of those and they're not directly coupled to how many partitions you have anymore in the cluster so you can have thousands of partitions and it doesn't change the the cost of the cluster from a object storage puts perspective so what you're doing just to make sure i've got that clear say you've got three agents running you've got different clients connecting to them each one is taking a time is gathering up the data every quarter of a second slicing it by time and shipping that and you'll end up with three new files because you've got three agents times four in a second yeah and you don't actually need to run you don't need to run three agents you could run one or two obviously the how highly available it is changes when you run one or two but it doesn't change the durability of the of the system because all the data is either in object storage or the producer request hasn't been acknowledged yet so you can get away with running just one agent if you if you decided you wanted to have the absolute rock bottom cost right yeah but that's the math is right there yeah it' be 12 new files per second okay so hang on so this seems like it changes quite significantly the way data stored on disk which makes me wonder how things change when you're trying to read it back because a consumer in cfa reading off a partition has a fairly easy job of just chunking through the dis right you've just made it harder yeah it's it's made it's made a little harder i think that if you so it's funny that you bring up that comparison because kafka relies extremely heavily on the page cache on the operating system to make reads efficient when the data actually ends up on dis in kafka it actually looks not it looks even more disorganized than how it looks in orp stram in terms of yeah because the the file system is not it can't perfectly it doesn't take a file and like perfectly lay it out in a linear order on disk a file most file systems look somewhat like a tree so when you're writing new data to a file there's that that new append that you created it's not going immediately next to the f the the append that was before it it ends up in some other place on the dis so you with with kafka if you have lots of partitions you effectively are you end up in exactly the same position that we end up in in warst stream and we have a sol a solution that's very similar to it the way that you read it back out in warst stream is the agents form a cluster with each other in each availability zone and they in that cluster we route read requests for files using consistent hashing to one of the agents in that availability zone and that agent will take care of caching in fairly small chunks blocks of data from those files in s3 so that when you're reading the live edge like the most recent written data the effect that we get is basically when you read out of s3 we'll only read we'll do one s3 get for every four megabytes of data per availability zone the agents take care of being acting like the page cache in the operating system where they keep the recently written data in memory so that reads are very efficient because they are really you know as you might imagine if you're if you have a lot of clients writing to a lot of different partitions you'll end up with lots of little tiny chunks of data that you need to read and we take care of that by you know lots of batching for the you know if you're going to send a read request for you know this 10 kilobytes and there's another client that's concurrently reading 10 kilobytes that's just a little bit further away like we we do lots of batching to make the the rpc overhead of this pretty efficient but the the problem is essentially the same as it is with like linux and the the phage cache we have to we we make sure that you have enough as long as you have enough memory to read the recently written data which is usually just a few seconds worth of your fruitbit it'll work just like the the page cache does in the operating system but the thing that do that kafka does not do that makes it possible to replay historical data with really high throughput is that we take the lots of you know the 12 small files per second that the agents were creating in your example before and we compact those files together so we we merg them into much larger files so that later when you go when you want to come back and read let's say that 10 kilobytes of data that was in one of the files that you were reading to as a live consumer if you reset your offset back to the beginning and read the data that's been compacted you might get tens of megabytes worth of data that you're reading at a time so you can read that out of s3 with with really high throughput and kofa doesn't do this the storage system for kofa does not rec compact any of the data and most file systems don't just like move the data around out from under you because the performance of that would be pretty catastrophic like if it does it at the wrong time it'll like you know stop the the the live workload that the machine is doing might be disruptive so basically no file systems do the that type of background compaction but we can because the the storage and compute is separated we can schedule the compute in a intelligent way because we kind of know what's going on in the whole cluster we can we can say oh this compaction job is really big and we have we don't have enough capacity to run it right now so it's wait we have that information at the operating system doesn't really have okay you see you're saying is there there's a separate process that goes away and grabs those 12 files and turns them into one big file sorted by topic partition yeah so it's it's integrated like you don't have to run it's not like a separate service or anything like we you can split it out into into different machines if you want to we have the flexibility to do that but by default you just run one docker container and it does everything and the the magic is kind of in the control plane to to decide what what to do when you don't have to manage any of that right so it's a separate logical process within the same binary yeah and is that i'm curious is it building that as a back i mean i i'm wondering if you're doing something like the git trick where you store everything immutably and eventually you're just going to replace the pointer to those 12 files with a new pointer to the one jumbo file that's exactly how it works yeah so we have a job that runs in the agent that will read in a lot of small files and then it merges them into one larger file and then we replace that file the all the pointers to data within it we automically swap the new file and delete all of the old files so that we don't delete the actual data from storage for a while because you may have some concurrent reads that haven't finished yet for the old files so we don't delete them immediately they're they're still both there so it's like a it's a copy on right architecture where when we want to rewrite the data we make a second copy of it and then we replace it at a metadata layer and not actually physically delete it yet that makes perfect sense if i i'm just curious because it's s3 so that would be storage i have access to would i if i went to one of those files and try to read the binary what would i see so workstream has a custom file format it's not particularly interesting and we have a tool for you to convert it to json if you want to that's built into the agent okay but the we're we're working on right now a feature that will let you basically take a logical snapshot of your cluster at some point in time and read all of the files from object storage like take a consistent snapshot at this point in time and then you know blast it with trino or something some query engine that you want to read all of your historical data with at once that's a feature that we're we're actively working on right now it doesn't work by directly reading the files out of storage because again there like it's a custom file format it's not integrated with trino or any other system so you would still have to pull the data back out through the agent but because the agent is stateless you can just temporarily scale them up and then remove them it it can run you know just as much as you need to and you can also run agents in a completely disconnected way for these kind of analytics workloads from your primary agents that are serving the live workload but they're sharing the storage underneath it in s3 so they have the same logical view of like these are all the topics i have these are the consumer groups i have but they're running on isolated sets if i wanted to replicate a cluster would that be a simple as just copying a bunch of s3 directories so this is an another feature that we're working on right now the only thing that you can do for replication in work stream today is use mirror maker to which is essentially the same tool you'd use for for kofka but one of the features we're working on right now is for either warp stream as the source or another kafka compatible system as the source you can hook up the agent to read all of the data of an existing cluster it would it that basically looks like mirror maker again but it's just built into the agent and then a third feature that we're working on is this kind of replication at a kind of like the thing that you were getting at is what if i just made two copies of the data in two different s3 buckets in two different places we're working on another feature for that right now which it goes even a little bit further than that the idea is that we would have a synchronously replicated multi- region active active cluster where the if we use so inside our cloud service there's a database that controls like it's like where the pointers def files go basically that's what you can think of what what this database is storing if we moveed that database from a regional database that is now if we moved it to a highly available multi- region database like cockroachdb cloud spanner and the agents synchronously wrote to multiple s3 buckets we could give you a multi- region synchronously replicated active active cluster as long as all of the agents and all the different locations can read the data from these multiple s3 buckets with obviously a preference to read it locally if it was available because it would be cheaper you would be able to survive a like let's say the have a configuration of three regions like you had one in the us east coast us west coast and europe if you had that setup you could survive an outage of one of those regions and the other two regions would keep running it it's obviously a more expensive configuration because it requires copying data from one aws region to the other yeah but it's it's definitely something that for a subset of data could be it could could be worth it for like if it was mission critical information because a lot of companies do this today anyway they just use mirror maker to copied it from one continent to the other but we could have that kind of buil built in yeah i could see that you replicate the financial data but you don't worry too much if the product catalog gets stale that kind of thing yeah or like the observability data can stay region local but yeah the the transaction history that's you know like directly touching payments would probably want you probably want that replicated in in multiple regions for disaster recovery purposes that makes sense so let's go into the how of this because if you're adding features that makes me wonder how have you written this what have you written it in and how did you learn which bits of cfa you needed to replace to make this work yep so it's i mean the the first one is pretty simple everything is written in go the the full the full system is is written to go we are shooting for just full kafka protocol compatibility the only major thing that we don't have left is transaction we have ent producer which is a big part of transactions but we don't have the full implementation of transactions yet and that's actively being worked on right now right so we're we're shooting to just be a dropin replacement for all of the features like we have compacted topics consumer groups like most kofka applications that are not using transactions will just work today as a drop in replacement so the which is most of them especially when you talk about the simpler use cases like i need to move my application logs from point a to point b like that's the the most trivial usage of cod there is basically it uses almost no features so that one works extremely well today and it's very cost- effective and we have customers in production that that's exactly what they're using workstream for and they're they're saving a lot of money sorry what was the what was the third one i think i covered one and two but what's the how do you how do you go from okay we we know we want to re architect or replace kafka so uses cloud storage i have go in my left hand ready to go she'll forgive the double use of the word go h how do you actually learn what it which bits do you need to replace do you start by just pouring through the java code base so we we basically don't look at the kofka code unless we absolutely have to okay it's very and and there are some cases where you where we have had to unfortunately because the documentation is either misleading or about how features work internally okay this especially with especially with consumer groups consumer groups are full of undocumented stuff undocumented behaviors that if you were implementing the broker side of consumer groups basically you would need to understand there you know in theory maybe there was a message on the mailing list seven years ago that explains what this was and there probably is that message but just because kfco is a single implementation specification essentially the the the implementation is the specification for all int they write and then start implementing it yeah and you can tell that a lot of the kips are basically written backwards like somebody wrote the implementation first and then they wrote the kip later and because there is essentially one vendor who's like dominant in control of the kafka open source project because they employ a significant number of the committers some of these things are just like it's obvious that the the implementation was written first or like an interface was written to work with something that is already happening internally at the at the other vendor so we we do have to look at the kofo from time to time unfortunately but we try to take a fresh perspective on how the how these things should work so that we're not like our our minds are not poisoned by somebody's choice from 10 years ago yeah okay i could see that what do you do then do you do you fall back on your old trick of shadowing do you have a kafka cluster that you compare your exact behavior with we we do that as we have some integration tests that do that for for certain things what is more i think what is more useful than reading the kafka java code is actually reading the kafka client implementations the clients have because the clients were not the clients that are not liard kafka and the java one so all the the clients that are custom britain for other languages because they were not so closely evolved with kafka they have a lot of interesting comments in the code about like what they had to do to get it to work with this latest release of of kafka so the clients are actually very interesting and instructive for for what to do and then at other than that we just try to look at the protocol specific specification and a lot of the protocol messages are self descriptive about like what should happen like based on the name and then what the names of the fields are and you know some basic description of what that api method does a lot of them are very self descriptive so you don't need a lot of doesn't take a lot of work to to get compatibility consumer groups is basically where that all falls off a cliff like the the fields like the the consumer group protocol they just kept shoving stuff into the existing messages and just having like implicit behavior baked in or there's just a lot of settings that you can attached to consumer groups that are very complicated that was the that was the hardest one for us i think was was getting to compatibility with consumer groups can you give me an example i think the biggest example is the the behavior for static members in a consumer group like the whole concept of static members and dynamic members like i guess that's the opposite of number the that distinction was added later along with the incre like the concept of incremental consumer group rebalance was also added later so the what you have to do at each point in the consumer group protocol varies based on what kind of member it is and the the this information is is not encoded like there's not a static member join group request dynamic member join group request with like the right information for for each one for i i guess for backwards compatibility reasons that would be my best description of why things work that way i'm not our internal expert on kafka consumer groups by by any means it's some other members of our team that were a little closer to that but the but it but it has been a continual evolution for us to to get closer and closer to like that mean this was six months ago i think when we were going through this this process and things are good now but just along the way our first implementation of consumer groups went back in i think it would be august of 2023 that we we shipped that it was just broken because we wrote it entirely based on the like just looking at the protocol messages and saying okay this is how it should work because the protocol messages they're very obvious you just need a very like there's no reason why there should be any problems here but that right you know when we first encountered somebody's real application in the world like because it mostly worked if you write a very very trivial application it mostly worked but when you when you first you know encounter somebody's real application in production it fall it would fall over spectacular oh yeah yeah i can imagine because it's like if the if the api looks self-documenting but it's evolved in ways that have attempted to move the behavior forward without changing the messages that could be very misleading so this leads me to a philosophical question if i may excuse me if ha's protocol is quite complicated and it is and it's got these gotas why do you think it is there are so many services cropping up that treat kafer as a protocol because it's not that people are saying this is the best protocol in the world we must jump in and replicate it he yeah yeah i think that it is the strength of the ecosystem that makes it you kafka was essentially the only game in town if what you wanted was open source or more importantly if what you wanted was free so the the ecosystem that has built up around kafka over the last 10 years or so is is pretty strong and there have been attempts by other vendors to introduce other protocols like there's kinesis which is a proprietary protocol for you know it's a service that's only available in aws there's pulsar which is technically an open source project but that doesn't have nearly the strength of the ecosystem that that kafka does so it's it's us j is this kind of niche but kafka works with essentially everything at this point so i think that's why it's such a popular protocol to to reimplement because it lets you piggy back off of a lot of existing work integrations with existing tools like if you use orst stream today like if you use our serverless product that we you know we run everything for you it's just you connect to a bootstrap url over tls over the internet if you use that you can go to other vendors that have integrations with our competitors and just use their integration you don't like you could put in your sassle plane username and password and the bootstrap your that says orst stream.com in it into a competitor's like branded integration with some other vendor like an like tiny bird for example is an analytics provider they you know they have a very nice ui very easy to use product for running like basically doing sql materialized views over your streaming data and if you go use the branded integration for confluent cloud with warp stream it just works because it's the same protocol and it works basically the same way the strength of the ecosystem there was was just great like we didn't have to go talk to tiny bird to get them to put the warst stream logo on their website or any of that to to get it to work it it just works because it's confident right yeah so it's that it's one of those cases where once once a protocols reach critical mass you follow that one because it gets you in the club yeah and i and i agree that cka is not the best protocol like there's a lot of things about kafka that are hard to use from a user perspective that are driven by limitations at either the protocol level or the you know the implementation of open source kafka at the you know the java code that we would love to evolve over time but it will be pretty slow because we can we can only basically move at the speed of kafka for some things there are plenty of things that we can evolve on their own because we can kind of hack our way around them within the existing protocol but there there are some things that we that we can't so that's the it's it's helpful that we we get a little bit of a head start but it also makes us more complicated because we're not as fully in control of what we can provide at the product experience level yeah yeah i can blessing and a curse but that's the reality of programming sometimes right y i just find it curious yeah yeah i can believe that i find it curious that the same thing isn't happening with it's not like there are a mountain of companies saying we are postgress api compatible not in the same way i i think that they're for for postgress i think it's a little different because they're so there are some they may not pitch themselves as postgress compatible but the way that they say it is just it's postgress but they've actually swapped out a bunch of the stuff under the hood they're probably using postgress code like i think the biggest one that has come around recently is neon neon is postgress compatible in the sense that like they actually they just they wrapped the single node part of postgress with a new storage system and a new api like load balancer thing but they're still postgress running you know way down in the middle where warst stream is not like that like there's no java stuff running anywhere in warp stream at all so i think for postgress it's just like it's a more complicated thing to replace so the way that people do the replacement is they reuse as much of postgress as possible that's that's the way that they do it and there are you know aws aurora is kind of like this as well i don't i don't think it's known publicly whether or not aws uses the actual postgress code in aurora i imagine that they do just because it would be easier and they did the same thing as neon where they swap out the storage engine and they have a new api like network you know load balancer thing in front of it so that's that's i think where the the the difference is and the other difference is there's other sql implementations besides postgress so you can kind of you can still get the benefits of switching to another system without necessarily having to rewrite everything like it's it's easier to rewrite a little bit of sql here and there to switch to a new system compared to rewriting everything that processes any data in kafka because the client library is has completely different behavior like in go there's a there's an abstraction layer for sql databases where you can connect to you just have to make sure the driver is installed but once you do that the interface for interacting with the sql database is the same it's just like the sequel text you might have to change between vender vender to vend yeah yeah yeah that makes sense that makes you wonder you say that with quite a lot of enthusiasm are you happy with the choice of go yeah it's been so that's what we wrote husky in it's it's what we we chose to write warst in because it's a very no nonsense programming language and my my co-founder and i and the engineers here at orb stream know it pretty well so it's it's like a it's a combination of the the culture of people at least that we interact with using go we don't spend time like talking about go we talk we there's no discussions about programming languages at workstream because we just want to like write and get things done we don't spend time thinking about this is there are other popular programming languages today that i'm not going to engage in a programming language debate but a lot of their community focuses on things about the programming language and their programming language versus another programming language they focus on that a lot versus actually delivering real projects in the real world that provided a lot of value i'll just i'll put it that way okay i'm i'm avoiding entering into that debate i'm just listening to you your perspective and i i to be to be clear java is not that java i think very similarly to go has a there's a lot of nonsense in the culture i think but that has nothing to do with debating about whether java is a good programming language or not java is a very get things done language as well nobody thinks it's cool anymore it's got it can do anything you can contort it to do anything so i i think java is also a we we could have written warp stream in java i think if we like happened to be people that were java experts i think that was more why we we picked go is because we are we know everything about it and it was easy to to you know hit the ground running br new new developers i would say it's i think it's a little easier for developers to learn le go than it is to learn java you never interacted with it before yeah i can believe that and it's a testament to it that if you've done one major project in a language and you say and you not saying never again that's usually a good sign for a language yeah yeah i mean it's it's just go you have to learn a few weird things but the the number of weird things you need to learn is pretty small to be productive in it and then once you once you've kind of mastered those and you have good tooling which is another great thing about go is the tooling is fantastic and it's very easy to write your own tooling because like the parser for the programing language is available for you to import into a go program and you can like we have things that that are that are that run in our ci that like we wrote that check our source code to say that they don't like you can write your own lters really eas basically is the way that i'll say it does you don't have to be a programming language expert to write a linter for for go that's interesting it makes it really the the tooling for go is is fantastic and i think that's another reason why we've we've kept going with it even even if like it takes a little bit longer to get the performance to be the same as what it would be in c or c++ or rust the the benefits that we get from a productivity perspective because of the tooling and how straightforward the languages i think far that way cost that's a good adver for go i think okay so you've mentioned performance so that gives me a way to take it back to warp stream to wrap this up which is if here i am a cfa user tempted by warp stream what am i trading off what do i get and what do i lose yep so there's one big thing and one small thing one big thing is that the latency of warp stream is going to be higher than open source kafka or basically all the other vendors that use local ssds for the storage instead of object storage to put that more concretely if you're going to use s3 standard and it works in all the major cloud providers so anything that's s3 compatible or gcs or aure bob storage but i'll just talk about s3 because it's simplest the p99 producer latency with the default settings or with a very small amount of tuning nothing crazy is about 400 milliseconds at the p99 and by that time while that time you know during the time that that has elapsed the thing that's happened is that file that contains the data that you wrote is durable in object storage and is durable in the control plane metadata store database right so it's very durable by the time that that's happened and the end to end latency is p99 of around 1 to 1.5 seconds with s3 standard if you need something a little bit lower latency than that warre stream also works with s3 express one zone which is a new high performance tier of s3 and with that you can get the producer latency to around 150 milliseconds at the p99 and the ended latency to around 400 ms with s3 exess one zone you can also the way that we use it you can get regional high availability out of it with workstream because as the name implies it's a zonal service so what we do is write to a quorum of two out of three s3 express one zone buckets so you can get regional high availability using that storage class as well which you don't get out of the box so that's the that's the performance trade-off and then the there's one kind of weird configuration that you could run a kafka cluster in that you can't like war stream won't be able to match which is if you run a really really tiny caca cluster that is producing very few messages per second it's cheaper to pay the inner zone bandwidth and the local ssds than it is to write a file to object storage every 250 milliseconds so if you're if you don't use kafka heavily at all like you're writing you know one kilobyte per second through through kafka well i some people do because they they're you know they're in development in their application and then eventually they're going to go to production hopefully the traffic is is higher that would be cheaper to use to self host an apache cofa cluster whether it would be easier to manage that's another story but just purely talking about the infrastructure cost it would be higher with with orbing because of those those s3 api fees this is where we're saying that the costs are a floor basically that your floor is that $50 a month for yeah it's a little more than that because the cost like the cost that or the price that we charge for having a cluster running 247 connected to the workstream control plane is $100 a month and then there's a yeah there's the s like the minimum footprint of an agent costs around $50 a month in puts but to solve that that problem at least on the storage side we're we're working on some alternative storage back ends besides s3 for the for the low lower volume data the first one of those would probably be dynamo db so if you have if your data is just flowing in in a very small trickle it is actually more coste effective to to write it to db instead of s3 okay yeah and i suppose the a s3 api is sufficiently simple that you could use very wide range of potential storage backends yep so s3 the guarantee or the features of s3 that we use along with the guarantees that s3 provides for those features is pretty widely applicable to any storage system like we workstream only writes to new keys like we never overwrite objects so you can you can make it work on on just about anything like in in theory we can make it work on nfs we could make it work on cassandra dynamo db big table any like almost any key value store or something with a key value store like api it would it would work okay postgress my my sql any you know we could we could sque we could squeeze it into anything it just it it's just a matter of like what makes sense from a cost perspective or ease of ease of use squeeze into github just for fun probably i don't think they would be very happy with us but yes we could probably squeeze it get up interestingly given the opportunity to tell me the tradeoffs you've only told me what's worse so i i guess i thought that's what you meant by by tradeoffs is is what's what's worse i love i love it you're being honest about what you lose but i'll give you the chance to say what you get in return for that yeah so what you get in return is warp stream is like the thing that you deploy that you run in your cloud account is a stateless docker container if you're going to deploy it into kubernetes it's a deployment not a stateful set so you can hook it up to whatever autoscaler you feel like that doesn't need an operator or anything like that and all of the data is in object storage and there are two big benefits to that it's basically there's no in zone networking cost and there's no like the the storage cost is like dramatically cheaper than triple replicating on local ssds the thing that we target is for a high throughput cka cluster we can reach roughly an 80% savings over self-hosting apache kafka inclusive of our licensing fee like to use the cloud control plane so it's a significant savings over self-hosting apache kovka if we're using another vendor then it's going to be even even cheaper than that so it's basically it's all about making it easier to operate and and cheaper that's the that's what you get in return for that latency tradeoff and the throughput is similar we can achieve as good or better throughput than apache kafka in most situations the interesting one is that you just had your workload has very few partitions in kafka you would be bound basically by the number of brokers in your cluster this is a very weird situation but let's say that you just had one partition in your whole cluster the throughput that you would get would be limited by one but the throughput that you could get out of one set of three brokers that were replicating the data with warp stream because the metadata and the data is separate we can drive all of the throughput for that one partition through through all of your agents if you wanted to it's a weird setup but basically like the trade-offs are completely different around what work like the the way that the like the partitions are all virtual effectively there's not like a physical resource backing the partition so there are lots of weird situations you can get into with aache kovka that would limit your throughput that you won't find with warst stream because the way that we load balance the the workload is we're not balancing partitions among brokers we're balancing connections to agents and our connection balancing is literally round robin so you get a very smooth distribution of resources assuming that your assuming that your clients are all doing roughly the same activity round robin will get you a very good balanced utilization across the the cluster which means the more balanced you are you can add more agents and it will remain balanced so you can drive more throughput to the clust interesting well i'm very interested to see how this evolves and i'm and especially as part of the landscape of cfar as a protocol coming up with people tweaking for certain use cases and certain usage patterns yeah yeah we're our our bread and butter customer today is essentially doing something in the high volume telemetry space so application logs security logs iot things like that because those you know the the ratio of staff to throughput on those workloads is very different than what you might find at for people for other people using kafka so that's that's where it makes the most sense today because you can show immediate cost savings and reducing the operations burden for those very few staff that was running the the kafka cluster before so i think that we're gonna you'll you'll continue to see adoption from us along along those lines for for for now once we have transactions i' say basically sky is the limit because we'll just be fully compatible with kafka and we can squeeze into any use case where where kafka is today as long as latency isn't i've always found this in the kafka world everyone has a different definition of of fast and for some people one and a half second is terribly slow and for some it's lightning yep that's it's i from our conversations with with customers and users of kafka you'd be surprised how often people don't actually know what the end latency is of their processing pipeline at all or if you ask somebody who's at the like more on the business side like they'll say what and the the product that they offer involves processing data so like they understand basically that's what they're doing you ask them what their lat what they think that their latency is and they're like oh we're we're a high performance organization and our data p processing pipelines are super fast they're 10 10 milliseconds end to end and then you get a little bit further down closer to the cluster either developers the operations team and they'll say ah it's maybe it's two seconds or so i we don't really keep track because because most of the time it just doesn't matter there there are very few use cases i would say that we come across that latency does matter and they're basically all in financial services and a big fraction of them are fraud detection related so it's like i have a credit card that's being swiped right now somewhere in the world and i need to give a yes no decision for whether to allow this charge that has somewhat strict latency bounds and a lot of times that fl the critical path is kafka for that but for that we have alternatives like s3 express one zone and eventually once we have dynamo db i think we're going to be able to squeeze the latency down even lower so that we can fit into those use cases but yeah today if you're just using s3 standard most people are perfectly satisfied with one and a half seconds and to end yeah i can believe that i wish you luck ser that niche and i keep an eye on warp stream to see how it grows ryan thanks very much for taking us through it thanks this has been fun thank you ryan if you want to check out warp's stream you'll find a link to them in the show notes and i wish them the best of luck if you've enjoyed this episode please do leave a like rate the podcast or share it with a friend and make sure you're subscribed because we'll be back next week with another fellow geek chipping away at their corner of the internet trying to make things work a little bit better until then i've been your host chris jenkins this has been developer voices with ryan world thanks for listening e e