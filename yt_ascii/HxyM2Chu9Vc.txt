in march 2023, nvidia announced a new software
library that they call "culitho". nvidia claims that this computational lithography
library can "deliver a performance leap of up to 40x beyond current lithography". most of the comments on the internet have
centered on the library's name, which unfortunately reminds people of a spanish slang word meaning
"small butt". butts are funny. but in this video, we are going to stop touching
the "butt" thing and focus on what this technology actually does. so what did nvidia do that has gotten both
asml and tsmc saying such nice things about it? in this brief video, let us dive butt-first
into the pool. ## introducing culitho there was an nvidia session titled "accelerating
computational lithography: enabling our electronic future" by dr. vivek k. singh. i think it gives a good overview of the whole
situation including some background. the talk isn't very technical and dr. singh
seems like a pretty nice guy so i think you should go watch it. dr. singh describes culitho in this way: > culitho is a collection of parallel algorithms
invented by nvidia scientists for accelerated primitive operations of opc he shows a bunch of algorithms whose performance
has been accelerated many times over - sometimes even 100s of times - using gpus. these algorithms are integrated into traditional
opc eda software. using parallelism, gpus can chew through these
algorithms far faster than cpus can. speeding these component processes give you
an overall faster end to end computational lithography system - 23-42 times, which is
where the headlining number comes from. and that's pretty much what culitho is: gpu-optimized
software for more quickly running computational lithography processes. but why is it important that we run these
things faster? what is opc, ilt, and computational lithography? ## lithography lithography involves the work of transferring a chip design from a photomask onto a resist-coated wafer. since the very beginning, we have been using
optical lithography. this is basically a multimillion dollar camera
projecting powerful light waves through or off the mask to transfer a design at high
volume. up until the late 1990s, the feature sizes
printed in a process node were larger than the light wavelengths used to print them. this changed with the'0 nanometer node generation,
which used the 248 nanometer krf duv laser. though if you want to talk semantics, the
193 nanometer arf laser was available then - but not widely. anyway, this situation is bad because when
the feature sizes are this small, any deterioration in the resolution and image quality can cause
the pattern to be incorrectly transferred. and then stuff starts breaking. ## improving k1 so how are we going to achieve the necessary
resolutions with these big, clunky wavelengths? let us look at rayleigh's holy equation once
again: > resolution = k1 * wavelength / numerical
aperture or na our options are either to raise na or lower
k1 and those options have tradeoffs. raising na means to collect more light. it is the concept behind immersion lithography
and crazy things like high-na euv. but raising na also means a thinner depth
of focus, which is the vertical distance at which the image quality is still acceptable. depth of focus, as dr. singh said in the nvidia
talk makes for cool photographs, but can be terrible for ic development. we want that depth of focus to be as nice
and thick as possible, otherwise the image is going to blur at a certain depth of the
resist. in that case we need a thinner resist layer,
which can cause issues when you get to the subsequent etch process after lithography
is done. the resist protects the etch acids from breaking
through and a thinner resist might compromise that. anyway, so raising na can be done and is being
pursued - but it can only work to a certain extent. thusly the only other thing is to lower the
k1. k1 represents the contribution from the manufacturing
process like the resist, the materials, and the imaging techniques. this is where computational lithography helps. it helps lower our k1 and over the years it
has helped us lower it by a great deal. k1 has a physical limit, but we have been
trying to get as close to it as possible. ## computational lithography starting in the early 1970s, we began quantifying
the natural principles behind lithography. this was a difficult task. lithography was before then seen as an artisan
technique that was highly dependent on an operator's individual skill. but efforts led by ibm's dr. rick dill and
others helped create computer models that modeled the surface of the resist-coated wafer
and how it reacted when struck by light. put these principles and others into a computer,
and we can then start simulating what happens when we run light through the optics system,
through the photomask, and what is left after that light hits the wafer. it took many years for computers to git gud
enough to run these simulations, but by the early 1990s computational lithography had
gotten to be useful enough for real world use. now we can leverage our knowledge to improve
the printability of a particular chip design. these tactics are broadly called "resolution
enhancement techniques" or rets. ## rets & opcs rets are basically little optical tricks that
we employ to better transfer the designs over to the wafer. correction, these are nowhere near little
tricks actually. they involve the whole optical system. for instance, three big rets introduced over
25 years ago helped bridge the wavelength-feature size gap at the'0 nanometer node. they were phase shifting masks, off-axis illumination
and optical proximity correction or opc. i will skip over explaining the first two
rets - though they are very fascinating - to focus on the third - opcs. optical proximity corrections are modifications
we make to the chip design layout in order to compensate for optical distortions. in other words, we basically mis-print the
chip design onto the quartz photomask so that after the light passes through it, it ends
up printing correctly on the resist-coated wafer. the first opcs introduced in the mid-1990s
were based on simple geometric rules. for instance, adding tickmarks at the end
of the design lines - kind of like the serif in serif fonts. the serif tick mark makes that design line
easier to print. simple as that. these simple rules made it easier to check
everything at the "design rule checking" stage at the end of the verification stage. but because they were so simple, these "rules-based"
opc as they were called were not useful lower down on the silicon. so they were used for the metal layer - where
the interconnects are - higher up in the ic. ## from rules to models as feature sizes continued to shrink, simple
rules no longer cut it. a new type of opc emerged called "model-based
opc". these use extensive computational lithography
simulations to correct the entire layout right before it goes to the fab. after finishing the design, we get the gdsii
file which is almost ready to go to the fab. we then run this file through the opc semiconductor
design software to add the rules in. it is impossible for the model to simulate
the entire chip design all at once, so it breaks down the polygons that make up that
design into segments and take samples within those segments. we then simulate the light's entire journey
through the sampled mask segments and even how the resist reacts when hit by the light. the program then iteratively makes corrections
to the current mask design to optimize for the proper parameters. model-based opc is very accurate, but its
reliance on intense computation and iteration also means that it can get very slow. and because the model only takes samples at
certain areas of the chip design, we will still need to run physical verification after
opc. these long runtimes got longer as chip designs
grew more complicated - to the point where they interfered with development cycles. ## inverse lithography in 1981, professor b.e.a. saleh of the university of wisconsin-madison proposed the concepts of what would eventually
be called inverse lithography technology or ilt. like opc, ilt is a resolution enhancement
technique. the goal is still to improve the resolution
of the chip design image transfer. they just take a different approach. the flow of traditional opc operations is
from the mask to the resist-coated wafer. we start with a design and then the simulated
model makes slight changes to that design based on the models' results. ilt on the other hand goes the other way conceptually,
from the wafer to the mask. we know what pattern we want to put onto the
wafer. what is the best mask design we can come up
with to get that pattern - knowing what we know about how light travels through the lithography
machine and interacts with the resist. then ilt writes up that whole mask up from
scratch pixel by pixel. this "inverse" approach - starting with the
destination and working backwards from there - is the key differential from traditional model-based
opc. now how do we do it? professor saleh first used a variant of "simulated annealing", which is
a statistical optimization technique that has popped up a few times before on this channel. "simulated annealing" seeks to discover a
local optimum. saleh first created a randomly generated mask
and then made random changes to certain pixels. "flipping pixels" as it is called. keep what works and drop what does not. after saleh's paper, researchers at berkeley,
ibm, and other organizations refined the technique - experimenting with different methods and
results. in 2003, a startup called luminescent technologies
made a big splash with their paper, "fast inverse lithography technology". luminescent gave ilt its present name and
strongly pushed it across the industry. and then there is intel. intel has been funding academic research in
the field since the 1990s. in 2007 they presented their own version of
ilt. in classic intel fashion they avoided the
"ilt" name, calling it pixelated mask technology. over time, computational lithography has already
added a whole cornucopia of distortions. but ilt takes it to a new level - these ilt
results are very unintuitive like some sort of horrific mutation of the original design. and yet the results speak for themselves. ## bottlenecks if you look at it purely on the basis of results,
ilt takes the cake over model-based opc. the masks that come out of ilt tend to be
more accurate, have greater fidelity to the desired pattern, and offer more room for process
error. however, there are also substantial tradeoffs. first and foremost is computational power
and throughput. accounting for every pixel in the chip design
helps produce a more optimal solution, but it also means you need a lot of computing
power or time. some designs could take weeks to process,
which is unacceptable. second has to do with manufacturing the mask
itself. today we "write" our photomasks using focused
electron beams. it is slow but precise. most masks have what we call manhattan geometry,
meaning that they use shapes made up of many rectangles. masks generated with ilt tend to have more
free form to them - with complicated curves and diagonals. masks like these are referred to as curvilinear
masks. producing these curvilinear masks using the
traditional technologies meant that you had to "fracture" the complex shapes into rectangles,
which was challenging. manufacturability, computation and competition
turned out to be game-breaking issues for luminescent and ilt back in the late 2000s
and early 2010s. ilt was just a bit too early for its time. in a 2016 article for semiengineering.com,
former employee leo pang recalls that luminescent tried to push ilt for the 45 nanometer and
32 nanometer nodes. but then came 193 nanometer immersion lithography,
which improved resolutions to such an extent that semiconductor manufacturers didn't need
to choose ilt over ops. while various eda vendors did adopt ilt into
their systems, it was mostly reserved for special situations and not widely adopted. in 2012, luminescent was split up with bits
acquired by both equipment-maker kla and the eda-maker synopsys. ## changing times as i mentioned, ilt arrived a bit early to
the party. but over time the party has caught up. mask writing tools improved. new multi-beam tools like those from the austria-based
ims now make it possible to produce the shapes for these complex curvilinear masks in a reasonable
time. however, we still had to deal with the computational
challenges. how can we generate the proper masks without
taking too long to do it? it gets even more daunting when you consider
how complex today's leading-edge chips are. as dr. singh mentions in his talk, hopper
has 80 billion transistors on a chip - nearly double that of its previous generation. that means trillions of polygons on a single
chip. these ever-increasing demands have forced
the fabs to use ilt more and more in their nodes. this in turn incentivized them to figure out
how to make ilt work faster. dr. singh talks about how tsmc presented intriguing
simulation results hinting at how gpus might be able to speed up ilt. those were just simulations but tsmc wanted
to turn those into reality. nvidia took up the challenge and the result
was culitho. ## conclusion techniques like computational lithography
have a direct impact on our ability to print ever smaller features. it is a massive field with many more functionalities
that i have not yet mentioned in this video. for instance, source mask optimization. which is where you optimize both the mask
pattern and the illuminating light source behind said mask to get the best results. i have always thought there was something
a bit funny about how we use computers to allow us to make chips for faster computers. something self-referential about it, i guess. i look forward to hearing more about the effects
and benefits of this software library in the near future.