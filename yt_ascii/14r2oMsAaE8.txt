in 1962, a 22-year old grad student at cambridge 
predicted a mind-blowing superconductor phenomena. his prediction paved the way for 
a new type of superconductor-based   circuit capable of switching at incredible speed. for 15+ years, ibm tried to use these concepts to 
make a computer. it didn't work. in this video,   we look at ibm's failed superconductor computer. ## the superconductor i did a whole video about superconductivity 
back during the lk-99 hype. i recommend you watch it. at room temperature, a superconductive material   exhibits electrical resistance - meaning that 
it impedes the flow of an electrical current. for some materials, that resistance might 
be quite high. but when the material reaches   a low enough temperature - the transition 
temperature - the resistance falls to zero. yes, actually zero. you can suspend 
a current inside a superconducting   ring and it will stay there for an 
indefinite period of time. no resistance. it will also float on a magnet. this superconductive state only exists in certain 
circumstances. not only a low enough temperature,   but also a weak enough magnetic field and 
current density. if a magnetic field appears,   then the transition temperature goes down further. ## the cryotron concepts for a computer based on superconductors 
have been around since the 1950s. back then, the world's research institutes 
were using vacuum tubes as switches for a   digital computer. however, these tubes 
were not small or reliable enough. in late 1955, dudley a. buck - an 
electrical engineer who left the   us national security agency for 
mit - proposed the "cryotron". cryotrons are superconducting switches 
- switching their superconductivity on   and off. the first cyrotron was 
very simple - just a segment of   tantalum wire with another 
copper wire wound around it. you then dip the whole thing into very cold 
liquid helium, which causes the inner tantalum   wire to go superconductor. an electric current 
can now flow through it without resistance. so how do you switch it off? you push a 
current through the winding copper wire,   creating a magnetic field that pushes down 
the transition temperature enough so that   the tantalum wire exits the superconductive state. ## fizzled start this is quite similar to 
the field effect transistor,   except it uses a current rather than 
a voltage as the control signal. the cryotron was not only far smaller 
than existing transistors or vacuum tubes,   it could be easily fabricated. buck and the media envisioned 
superconducting circuits powering   computers as small as a "cubic foot, exclusive 
of refrigeration and terminal equipment". buck continued development on the cryotron 
throughout the decade. but he unexpectedly   passed away in 1959 in massachusetts at 
the young age of 32 - apparently from a   viral pneumonia or an edema resulting 
from the gases he was working with. cyrotron development continued for 
a little bit after buck's death.   and industry people showed good 
progress. a team at ibm created   a thin-film version that switched 
far faster than buck's original. and a team at ge was able to put some 2,000 
superconductor switches onto a single integrated   circuit - far beyond what anyone else had 
done before - and make a computer from it. however, the invention and subsequent growth of   the silicon planar integrated circuit 
offered superior performance without   the need for liquid helium refrigeration. 
the industry receded into the background. ## broken symmetry in 1961, a theoretical physicist 
named phil anderson visited the   royal society mond laboratory 
at cambridge on sabbatical. anderson himself would win the nobel prize 
in 1977 for his contributions in solid-state   physics. in any story about superconductors, 
seems like every character has won a nobel. while at cambridge, anderson taught a course 
on solid state physics - discussing an idea   known as "broken symmetry" and its 
connection to superconductivity. the idea of symmetry in physics is an 
important one, and what follows is a   radical simplification. if a statue is standing 
upright, you should expect it to stay standing   upright until something acts on it. the 
statue is not going to suddenly flop over. this should be the case no matter how we 
"translate" that system. meaning no matter   where the statue is - a translation in space - 
and no matter when it is - a translation in time. the laws of motion are symmetrical in 
terms of space and time. there are many   types of symmetries within physics 
other than just the laws of motion. this was a tough concept for me to grasp at first,   so feel free to stop and google 
for additional info if you need it. anderson proposed "broken symmetry" as a way to 
explain how atoms can sometimes collapse into a   lower-energy state that do not exhibit the same 
symmetry as the laws of physics acting on them. he uses this to explain how electrons in a 
metal form what we call cooper pairs when   a metal enters a superconducting state. this 
unusual pairing would not normally happen in   a non-superconducting state - thus breaking 
a symmetry of electromagnetic interactions. did that make sense? i hope it did. anderson also invoked this broken 
symmetry theory to try to explain   how a superconducting material can expel 
a magnetic field - the infamous meissner   effect. photons inside the superconductor 
suddenly gain mass, expelling the field. inspired by the concept of broken symmetry, 
one particularly bright student in anderson's   graduate class named brian josephson began trying 
to see if it could be observed in an experiment. ## josephson
josephson began studying several experiments and came across a few recent ones 
on superconductor tunneling. tunneling is when a single electron 
or pair of electrons pass through a   barrier that would have ordinarily stopped 
them. it is a quantum mechanics thing so   only practically observable when 
the dimensions are very small. a 1960 experiment by ivar giaever showed 
that single electrons can tunnel through   very thin barriers - like just 20 angstroms 
wide - from one superconductor to another. josephson tried to formulate a 
theory of equations to fit this   behavior. as he tried to explain the 
behavior of electrons traveling from   one superconductor to another, he 
discovered something startling. ## the effect let us imagine a circuit with a junction or 
barrier - made of some insulating material - in   between two metal superconductors. let us now 
put this circuit into differing situations. if the barrier is thick, then we cannot 
send any current through from one side to   another. this is the case no matter 
how much voltage we apply to this. classical physics dominate 
here, not quantum mechanics. recall that voltage is like 
the pressure for the current.   it is like how water pressure 
pushes water through a pipe. now let us shrink the barrier to a very 
thin width. per the well known tunneling   principle from quantum mechanics, this 
thinness grants us the possibility that   a single electron can tunnel through 
that thin barrier to the other side. we apply more voltage, we get a directly 
proportionally stronger current tunneling through. now, let us start lowering the temperature. as the   system approaches the transition temperature 
for a superconductive state, we see this   directly proportional voltage-current 
relationship start to break down. now we are in a superconductive state. so long 
as the current is below a certain threshold,   we see that current flow through what 
is normally an insulating barrier   without resistance. no voltage/pressure necessary. but if the current breaches that certain threshold 
- or if a magnetic field is applied - the   supercurrent collapses. the barrier regains its 
resistive properties and everything is as before. this very strange behavior is what we now call the 
josephson effect. what josephson is saying that   is new and radical is that it is not just single 
electrons quantum-tunneling through the barrier. but entire cooper pairs - carrying their 
superconductive abilities along with them.   the supercurrent flows like as if the 
insulating barrier was not even there. josephson's paper set off a rush of 
people to verify his predictions,   which few people believed. even john bardeen 
- co-creator of bcs theory and one of the   discoverers of the first transistor - could 
not believe it. the two famously debated. but then in 1963, a set of experiments verified 
the josephson effect. bardeen graciously withdrew   his objections and brian won the nobel physics 
prize in 1973 alongside the aforementioned ivar   giaever and the semiconductor pioneer leo esaki. 
josephson was just 33 years old at the time. ## the junction the josephson effect's ability to move 
between a zero-voltage superconducting   state and a normal resistive state 
forms the basis of a digital switch. we can use the levels of voltage to 
represent states of logic like 1s   or 0s. a high voltage state can be like 
1, and a low voltage state be like a 0. after the josephson effect 
was experimentally verified,   ibm began working on building a thin-film 
device for possible use in digital computers. in order to differentiate this 
device from the "failed" cryotron,   ibm's juris matisoo decided to give it 
a new name - the josephson junction. early devices were made up of two pieces 
of lead or niobium - chosen for their   relatively high transition temperatures - 
separated by a very thin insulating oxide   barrier. one of the earliest displayed 
examples was a flip-flop memory circuit. a memory cell has to retain a bit 
- 1 or 0. dram memory cells do this   by storing a charge inside a 
device called a "capacitor",   using a single transistor to read or 
write that charge into said capacitor. the ibm superconductor memory 
circuit does not do that - it   sets up a current circulating forever in 
a superconducting loop. the direction the   current flows in indicates whether 
the memory cell is holding a 1 or 0. ibm also showed it possible to put 
an array of such memory and logic   devices onto a substrate using a series 
of deposition steps - though there were   acknowledged challenges in uniformly 
applying the very thin layers involved. by the late 1960s, ibm had seen enough 
to initiate a research program to try   and build a computer system out 
of these superconductor circuits. their entry into this space spurred dozens 
of other programs at universities and   labs. the nsa later got interested and 
contributed funds too - hoping that a   josephson computer could help break soviet codes. ## the ibm project throughout the 1970s, the ibm superconductor 
computing project focused on developing   the technologies to integrate various 
josephson junctions onto a single chip. josephson circuits are patterned 
using the same tools as those   used for traditional silicon ics. you 
use lithography to transfer a pattern   to a substrate and use deposition to lay 
down the necessary layers for your chip. one of the most challenging parts 
of fabricating these junctions was   putting down the insulating barrier 
in between the superconductors. for   the josephson effect to work, it needed 
to be a uniform 1-4 nanometers thick. one of the great things about silicon was its 
oxide - silicon dioxide. it is easy to grow and   sits quite well on top of silicon. that was not 
the case for the josephson junction's materials,   so ibm had to develop a sputtering 
deposition method in low pressure oxygen. the josephson project was controversial within 
ibm itself. established technologists there like   rolf landauer criticized it - citing issues 
with noise, unreliability and manufacturing. why? think back to a switch. a good switch is 
one that gives you a solid 1 or 0, on or off.   give it a weak flick, it still either flips on 
or off. it does not get stuck in the middle. if the device happens to take a 
weak input, it can't give a weak,   uncertain output too. but that is 
what the first josephson junction   did. a resistive or superconductor 
state is not as strong an output. this is important because no 
fab can possibly make a perfect   transistor trillions of times. there will 
inevitably be manufacturing variations. the silicon transistor's robust output 
- no matter the weakness of its input   signal - gives you sufficient margins of 
error when dealing with those billions of   transistors. without that, you get unstable and 
unreliable outputs at high volume manufacturing. ## the promise the promise behind josephson junctions and their   ability to surpass silicon systems 
were based on a few arguments. first, josephson junctions can seemingly switch 
far faster than silicon ever can - as little   as 10 picoseconds. the argument is intuitive. 
faster switching means faster processing speeds. second, people foresaw power limits on 
silicon. whenever a transistor switches,   a bit of power dissipates as heat 
that needs to be carried away. the idea was that increasingly dense silicon 
chips would dissipate increasingly more   power until consumption and heat generation 
surpassed what cooling systems can handle. by comparison, josephson circuits 
dissipated orders of magnitude less   power - just several micro-watts. this presumably   balanced out the inconvenience of dipping 
the whole computer into liquid helium. in 1977, a former ibm executive turned 
consultant was quoted in a computerworld article: > in 7-8 years, we should expect 10,000 
times greater performance from josephson   technology ... a megabit storage chip 
will cost approximately $30 by 1985 others hesitated to go that far,   thinking that a 500 times improvement 
seemed more reasonable. in any case,   the claims were that josephson technology would 
unleash a flood of compute unto the world. as late as 1980, the ibm josephson 
project team still believed that   superconducting technology could build 
the next generation of fast computers. the superconductor computer managed to get 
to a stage where you had about 50-100 chips   packed into a single core and immersed into 
liquid helium at temperatures of 4 kelvin. but a few things happened. silicon technology 
raced far ahead faster than anyone could have   imagined. and ibm's superconductor technology did 
not pan out in the ways they thought it would. ## dennard's law first, silicon chips continued 
to get denser and denser. yet somehow those denser silicon chips - with 
device counts surpassing the millions - did   not consume proportionally 
that much more power. why? midway through the 1970s, robert dennard - 
inventor of the 1-transistor 1-capacitor dram   cell - proposed a scaling theory 
that we now call dennard's law. dennard's law says that if we physically 
shrink a transistor by some scaling factor,   then the voltage needed to supply that scaled-down 
transistor also scales down by the same factor. this is because the transistor 
relies on electric fields in   order to work and a smaller transistor 
area requires a smaller electric field. so end result, dennard scaling means that 
power consumption per unit area of a die   remains the same. so even if we stuff 
more transistors onto those unit areas,   the chip's overall power 
consumption does not go up. fueled by the ongoing pc revolution, 
microprocessor innovation was going   absolutely nuts. chips were getting 
iterated on faster and faster,   shaping up silicon to be a 
formidable competitor indeed. ## speed limits
at the same time, the ibm team discovered a major limit on how fast their 
josephson circuits can switch. experiments found that the josephson 
junction can switch very fast from a   resistive state to a superconducting state 
in less than 10 picoseconds. this was good. but going in the reverse direction 
- from a superconducting state to   a resistive one - was far slower. at least a 
few hundred picoseconds in a practical sense. this essentially gated any josephson-based circuit 
to about 1 gigahertz. and by the early 1980s,   it became increasingly clear that 
cmos silicon chips were going to   hit and surpass those speeds without 
needing to be dunked into liquid helium. furthermore, there was a known issue known as 
"punchthrough". this is basically a small but   non-zero possibility of the gate entirely 
failing to reset to a zero-voltage state. in other words, the gate is - just like brian 
mcknight - back at one. a serious error issue. ## failed memory the final and most commonly cited reason why the 
ibm project sputtered out had to do with memory. ibm was never able to get anything 
close to a large enough working memory   for their computer. without a fast memory to 
provide the logic circuits with data to run,   the computer's speed is fundamentally capped. as i mentioned, ibm had created early versions 
of a memory cell that circulated a current in an   infinite loop. ibm broadened that into two working 
concepts that could have created a computer. the problem was manufacturing and density. a 
dram cell is just two devices - a capacitor   and a transistor. but a josephson-based 
memory cell needed all sorts of inductors   and resistors along with it - making it 
huge compared to a dram or even sram cell. so the team struggled to put enough good 
memory cells onto a single chip. by 1983,   the best the team was able to do was about 
1 kilobyte. twenty plus years later in 2006,   the best nec in japan could do was 4-kilobytes. ## end in 1980, ibm put together an 
extended investigation on the   progress of their josephson computer project. a council of experts evaluated 
the josephson team's progress   as well that of existing silicon 
technologies. they wanted to see if   there was a possibility for josephson to 
overtake silicon over the next 15 years. considering the issues we spoke about earlier,   12 out of 15 members voted not to continue. 
thus in 1983, ibm downscaled and eventually   canceled its josephson junction project 
after 15 years and $100 million spent. many felt it had gone way too long 
even then. the team of about 150   people in yorktown heights in new 
york state dispersed. these talents   benefitted other ibm projects like 
those on heads for hard disk drives. at about the same time, a team at japan's telecom 
monopoly ntt picked up josephson technology too. but despite a high profile start,   their project slogged on for eight 
years before ending the same way. today, there are few significant 
josephson-based devices in people's   hands. most notably, "superconducting 
quantum interference devices" or squids. squids leverage superconductivity's sensitivity 
to magnetic fields to measure the presence of   weak magnetic fields. they are regularly used in 
neuroscience, metrology and detecting submarines. because we have not overcome the manufacturing 
variation issues at scale, these systems are   mostly analog. such analog systems are 
characterized by their low device count. ## conclusion the issues with the ibm superconductor 
computer demonstrate many of the problems   of creating brand new, alternate 
paradigms of compute. silicon for   all its problems works really well, and 
the tradeoffs are often not slam dunks. but there is just something 
about the superconductor   that really attracts attention and clicks. adherents of superconductor 
computing have pointed to a   new type of logic - rapid-single-flux-quantum 
or rsfq. invented in moscow in the mid-1980s,   rsfq replaces the flawed voltage-state logic 
paradigm with quantized magnetic flux pulses. today rsfq is the dominant form of superconductor 
digital logic. it presents the next great hope for   superconducting compute and - if we believe their 
proponents - compute in general. we shall see.