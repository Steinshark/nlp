so today i want to talk to you about some machine learning algorithms there are really not much discussed for example we talk a lot about how to work with text for example right so for example if i have some text and i want to predict maybe the next word or something and those are large language models for example that we see a lot out there maybe maybe i'm interested in comparing images right for example let's say this is a dog this is my very good impersonation for dog and this is let's say a cat and then i want to predict is this new image a dog or a cat we also have a lot of algorithms that work with tabular data so like in a table and i want to find out whether this person maybe is eligible for a loan but same thing but with graphs so here are two graphs they will show to you i have graph g1 and graph g2 this graph has six nodes so this color blobs are called nodes and the connections between them are called edges in this particular graph the nodes have color so they can be red or blue and there are six notes with one two three four five six edges as well and we have a second graph which kind of looks maybe similar it also has six notes with the difference that i have three blues here rather than two and the connections here are a bit different they kind of look like molecule maps are the colors important the differences and colors that's awesome comments i was gonna exactly discuss why do we do these things and molecules are the best example so maybe if you have something that looks like a molecule like a carbon connected to an oxygen that's connected to a hydrogen and this keeps going i'm not a chemist so i don't know how this is gonna go but i could think that maybe i would represent a carbon with a red blob and an oxygen with a blue one for the chemists out there this is probably not real molecule but we don't care but you could absolutely model molecules not graphs you could model other things you could model friendship networks in which colors maybe represent people's preferences about a certain topic or something that identifies them in some way so these are obviously graphs in a kind of true sense that way with nodes and edges they're not we're not looking at plots here not plots and that's the first important distinction that we should but could you do plants like this as well or is that diff is that different for what we're going to see today plots cannot be analyzed with the same algorithms what we're going to look today are very specific to graphs in that sense of nodes and edges network is a way to describe them a network is a way that you maybe differ the growth these actually could be parts of a cloud or something couldn't they it could actually could be they could be knowledge graphs that i know you discussed in the channel before they could be anything that you can represent with connections maybe it's a natural question that we have maybe i have a third graph that i'm going to draw here maybe this third graph looks something like this right so here i have a third graph i'm calling it very creatively g3 and you can see it's a bit different from g1 and g2 it also has unlike the other ones it has one two three four five six seven notes instead of six again sean maybe you can tell me reasons why g3 should be more similar to g1 and maybe reasons that it should be more similar to g2 so obviously i'm looking there upside down but actually it probably shouldn't matter actually she loves it very good but looking at you know the the g1 to g2 you've got that kind of triangle at your the top from where you are but then it feels like g1 to me because it's just maybe an extension taking those two blues to be three and it feels more like that and to me than g2 and and there is this kind of yeah that kind of a kind of things coming out of it which here we have too but with an extra one so maybe kind of these parts is sort of similar on the other hand this part is very similar to this one it seems oh yeah the other way around of course yeah but it's upside down but again maybe we don't care about this of course these are toy examples right these are like not realistic examples but maybe in real life these are molecules again coming back to the example and maybe this g1 belongs to a big group of molecules that somehow are good they are drug that solves your problem they are active or something and these are not good or inactive you you may want to come up with a new one and say should this belong to group one or group two so before actually going there and like doing the synthesization of them could i maybe have models that will tell me kind of looks more like this blocks like this so today we're going to see two very very very simple models that nonetheless are going to give us conflicting answers and the reason why they give conflicting answers is to highlight the fact that working with graphs is really hard and we're not never expected to have one final only way of doing things because there are presumably all sorts of different ways of saying they're similar you know how many edges what different colors how many you know how do you measure it i suppose is the question isn't it and i've you've touched on something that's quite profound so if you have two vectors and maybe two vectors like this and they have numbers like there may be one two five and seven and the other one is one two four and seven you want to know if they are the same it's very easy you just walk from the beginning of the vector comparing the entries if they are the same keep going if they are different you know the vectors are different and you are done that can be done in linear time for those of you who like complexity of algorithms graphs not that easy the graph isomorphism problem is not known to be solvable in that quick way so the whole point is that we can't even tell if graphs are the same so let alone the degree of similarity that they have that's exactly what we're going to calculate degree of similarity but it's not an unique answer and maybe there's one takeaway of this whole video is that when we do such similarities in comparison between graphs we throw information away what we throw away and what we keep is the magic so if you're applying this to some sort of application specific some specific application you may want to discard some information you may want to keep some other information and that's the important choice that you would have to make we start with an algorithm called vertex histogram you probably remember histograms from maybe when you studied statistics so if i would look at g1 for example and i want to maybe count so histograms immediately here counting the number of occurrences of a given color labeled in that case so for g1 i'll have two blue ones and one two three four four red ones so this is my histogram for the first graph i will not draw histograms as plots like this i'm just going to draw vectors so that's going to be quicker i think so i'm just gonna get booze oh god okay threads and we're just going to calculate i'm going to call them vh prefer text histogram in this case of g1 so this is going to be a vector that has two blues and forwards and then vertex the circum of g2 what happens well i have three blues here and three reds so this is going to be a three comma three so you see what i'm doing i'm suddenly creating a vector that sort of represents the graphs now you're probably thinking there are many things that we're discarting with this that we're gonna analyze later okay so at this point you could get this vectors put it to a nice cartesian plane place them there and do your favorite favorite algorithm of course this is a toy example but imagine that you have hundreds and hundreds of those points you could use nearest neighbor classifier you could use anything that you want we are going to use something called kernel methods we're not going to get into the details of the kernel methods but we're going to go just see one example of how they work so if i want to use the kernel method i will take the inner product between three and one and three and two because i want to compare three right so i at this point i don't want to compare one with two i just want to compare three with both of them so we're gonna go through step by step so if i do the inner product is going to be two comma four if you have not done inner products or dot products before you can just it's just a sort of a multiplication of vectors in such a way that you multiply entry by entry and add them together so in this case i do two times four which gives me eight plus four times three and that's 12 so this would be eight plus twelve and that gives me 20. if i do the same thing with g2 now and g3 i'm going to get we don't have to write it down we can just look in here it's going to be 4 times 3 12 plus 3 times 3 that's 9 which gives us twenty one right inner products are a good measure of similarity between two things two vectors for the purpose of this particular part of the algorithm we can conclude that t3 is more similar to 2 because 21 is more than 20 then 2g1 you're probably thinking many many things at this point because you'll be like well i can't i can come up with so many examples of things that shouldn't be similar but give me a high number here and that's absolutely true i mean i will give you many i if you if you just have one vector that has huge numbers and the other one has low numbers this product is going to be high if maybe maybe higher than the comparing it with itself which should be the utmost similarity but nonetheless inner products are understood this measure of similarity and especially because we are going to use algorithms such as svms support vector machines in which inner products play a very central role and comparing and creating the boundaries between the classes and everything if you are really bothered with the fact that these numbers are very high or that you could get similarity that's much higher than you would like otherwise with outlier numbers here you can always normalize your vectors now it depends on your application maybe normalizing does not make sense maybe normalizing does make sense so again it really depends on the particular algorithm you're using okay sean there is a there is a central piece missing in whatever i've done here it's this yeah jesus name exactly and and what they're connected to not not just how many there are but where they are and all of that sort of stuff awesome actually not even how many they are at this point we are completely ignoring we're not taking into account edges at all nonetheless this is a very simple example that i like doing because it gives us a flavor of what's going on there is one thing that may be bothering you does it mean that if you put this in a support vector machine algorithm the fact that 21 is higher than 20 would automatically be the case that g3 is classified as the same group as the g2 not necessarily support vector machines work with a lot of different parameters and there are biased bias terms and and all all sorts of other parameters and factors that should be taken into account but nonetheless in the measure of similarity still works so the higher the number the more similar it's sort of the opposite as distance where the higher the number the more different now we're gonna get edges into the whole thing and see what happens this is called vice versa lehmann algorithms in this case we're going to do the graph kernel version of them they show up in other parts of graph theory what we're going to do and the basis of what we're doing now is the following look at this for example this red node and this other red note they are similar because they're both red but they are also similar in the sense that they have only one neighbor that happens to be blue so they are not only the same color but their neighbors have the same color unlike for example this one and this one so the top one here is red but actually neighbors are red in a blue whereas this is red and neighbors are blue so for the purpose of this algorithm now we're going to say that these two things are the same but these two things are not the same anymore because their neighborhoods are different we're going to analyze all the patterns that we see in this particular graph i'm now writing wl and i'm putting a number one you may guess y1 is in here at this point but we're going to discuss later is that for one neighbor it's not from one neighbor i'm gonna give you a spoiler it's i'm only looking at immediate neighbors so i'm looking only at neighbors with distance one distance one exactly so you're gonna guess that wl2 is going to somehow look at neighbors of neighbors so the first pattern that i'm gonna analyze is the one we just discussed so it's a red comma and i'm adding a comma here to clarify that the first red is the actual nodes label and what comes after the comma is the color of the neighbors because it could have more than one so it's red comma blue again we're gonna do a histogram again but with the new patterns so how many in the first graph have that behavior so this one is a red with a blue one this one is also red with a blue neighbor but the other two are not the same so there are two other only have one red yeah okay take out one red with a blue neighbor okay but if you go to second graph it seems that there is only one here right this one is it red with a blue neighbor and the other reds they have reds as neighbors so they shouldn't be the same pattern so we have a one here now for g3 i have three actually right because i have this one this one and this one terabytes with blue neighbors so it's going to be a long vector longer than just two because now all the intricacies are being taken into account and so not to get lost because this can be confusing i'm gonna tick all the nodes that i've already processed in a way if you actually want to implement this there is martaways of doing it you use hash functions and you're going to use all sorts of things so let's get this thing done and then let's see what's happening [music] at this point we've ticked all the nodes in the first graph so we know that the rest of this feature vector here has will have a bunch of zeros in the first line because there is no more patterns to pick up also you probably realize that two plus two plus two is equal to 6 and as it should be because as each node will contribute to one tick in here and the second one will also have to add up to six and the last one will have taught up to seven because i have seven notes [music] right we seem to have some feature vectors going on you can see there are a bunch more zeros now because there are many patterns that we only find in one or two of the grams and if you have done machine learning at this point you probably realize that something that is potentially starting to happen here is called overfitting as we get more and more details on the start of type of patterns we get it could be that we are too focused on the very very very fine details of our training data and we fail to generalize so when we see something new everything is different so how much of this fine graining you do depends on again the application you want and depends on again on things that you want to do you may think that for example this will never be used the vertex histogram one because it disregards all the edges right so why would we use it nonetheless it can be a baseline for your model let's go back to here and finalize our calculation so we're taking the inner or dot products between these two things so what happens if i do w l1 of g1 with wl1 of g3 i'm not going to rewrite the whole vectors down we can just look here and see what's going to happen so this first product is going to be six but it so happens that every other product is going to be zero because two times zero zero zero and then 1.0 and 0 and 0 and 0. so the inner product so the similarity using this algorithm in this case is six i should mention at this point many implementations don't have just this vector here but appends some concatenates the first one the vertex histogram at the beginning so i would have something like 2 comma four comma the rest and in that case i would not get a six i would get 20 plus six but for this example let's just stick with with the second half and finally i can do wl1 of g2 and compare with g3 and then i get 1 times 3 which is 3 then once with a zero and that's zero again and there's a one here and there's also another one here so i get three plus one plus one which is five again this is a measure of similarity that has a lot of caveats in it it's not exactly a final say of everything but it's just an example to see how in this case g3 seems more similar to g1 whereas before it would be more similar to g2 because it was a 21 versus 20 and in this case we have a five versus six or six four the first one but obviously there's only one in it for both those examples isn't it i mean they're very similar they're just all very similar right they are it's just one and has a very tiny difference again doesn't mean your support vector machine algorithm is going to do it we have not discussed exactly how these numbers go into your algorithm for those of you who know kernel trick it has to do with that it has this idea that you do not need necessarily to know the future vectors in order to do your prediction in real life you would not have only three graphs you would have many many more and you have you will train in many more of them and you test in some others so this is just a toy example if you actually implement this as svms i do think for the first one this is going to be classified as closer to to g2 but again there are bias terms involved and the number is not a predictive necessarily of what's going to happen but just to get a feel of how these things are working the kernel matrix that's going to come out of this for wl1 which we're not going to go into detail is going to be something like this that if i compare graph 2 with graph 3 which is this one i get a five so i also get a five here and i get a six if i compare three with two and i can fill these with other numbers so the magic happens because in support vector machines for example you can just put this matrix into the algorithm and get a predictions so you don't necessarily need to know this thing there are many graph kernel algorithms that actually don't ever do this because maybe those are infinitely long vectors but nonetheless you can get a number and this number is going to be what you're going to use there's one last thing you may be wondering at this point you may say well okay look at these two red nodes again they are both red they're similar they both have a blue neighbor so they're even more similar but more than that their blue neighbor so happens to have one red and one blue neighbor so they're similar if i go up to distance two if you want so the neighbors of the neighbors happen to be the same unlike these two although they are both red and both have a blue neighbor the neighbors of the blue neighbors are different because this one has a blue neighbor with a bunch of neighbors and this one has a blue neighborhood with just two neighbors so we can keep going and we can keep going forever so i could just create wl2 in which case wl2 would have patterns that would look a bit different and you at this point you may be wondering i am going to get lost right where do i stop like neighbors of name like how do i even write it down okay so that that i pre i pre-drawn this because it can get really complicated so each pattern that we saw before will have a new color so if it's red with a blue neighbor like red but the blue neighbor is going to be this one and this is now going to be pink and this is going to be pink as well and so on and so forth so if you wanted to wl2 you're going to look at patterns such as like pink comma light green and that's going to be like this one and this one but not this one because this is a pink with an orange as a neighbor we do this to make sure we can do this whole thing efficiently computationally speaking nonetheless you can try to understand in an algorithm if you use the future vectors as we have them before as 2 and 4 instead of the numbers that you get from the kernels you could just do this this algorithm with the vectors try to explain using some models to explain which feature was more important and then try to bring out and say ah this particular graph was deemed similar to g1 or the whatever class because of that pattern so you could try to explain decisions as well using that but that's another topic an edge or relationship like is located in so bush house is located in london you see how possible for any behavior because how could they they are completely distracted by the computer of all videos they're not even looking at their own