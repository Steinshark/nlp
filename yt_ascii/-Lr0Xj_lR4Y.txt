so hello everybody thanks for coming in for my talk about a smooth introduction to cle let me maybe introduce myself and our company so i'm jo fu i'm an associate professor at the well the computer science lab of r which is near paris i want buture the very french acronym so my research topics and general interest is basically doing par computing and doing that in z++ with proper interface proper abstraction and to do so i usually try to u torture compilers for various reasons because why you know so i've been authoring and commenting a bunch of cace library u on value subject including cd programming and other stuff and i'm also a co-founder at codon which is a company centered on giving helps trainings on c++ and hpc in general in various way and we try to integrate whatever we do and whatever we we need to be doing in the company into various open source projects so that's for myself what are we are going to speak about today well we are going to speak about a bunch of tools to deal with this you know very recurring subject about the fact that you know computers you have more of them every year you are they are more complex every time you look at them you know more calles more new system stuff like that and it's very cool and dendy because it makes you know like gpu vendors or cpu vendors to do cool keynotes with fine benchmarks and curves that go up but the issue with that is that at some point someone somewhere has to write code for these machines and before one it was basically a matter of juggling with a bunch of threads in in your head or having to think about how to vectorize a bunch of codes or doing both at the same time well it's not going to be easier and easier because now we have like thousands of cores in gpus you have a systems you have reconfigurable systems that starts to be a major player in a in a in a bunch of fields so it's not going to be easier for for the random joe developer that have to write meaningful you know business related code on those machines and you have the problem of choosing which vendor which hardware which way of handling all of that and usually you have to deal with issues about how can i actually express whatever i want to do in on those complex systems and what what are the ergonomics of all the existing solutions so that's normally where i should have had you know this very wellknown is kcd web comics about standards you know so you have 10 standards about something and nobody is happy with that so someone decide to make a new standards to you know unify everything and then you have 11 standards that's kind of what happened somehow with the cycle standard okay or cle and i i should learn to pronounce that it's cal like you know the the round curvy things okay and just to you know answer this question before it came up there is no armor project related to that so it's just the sle so cle is what it's a standard it's mostly proped by the kronos group that you may know for its work on open c among others and it also deal with stuff like sp v and stuff like that which are open industry standards around all these technologies about accelerators gpus whatever the idea behind cle is the fact that we want to have something that looks like regular c++ with regular c++ em and construct being built in so you can actually write c++ looking like code on some stuff as you as you may see a lot of them and and the stand up being open the implementation is well rather free for anybody to try and do some things and you have a bunch of actual implementation by different actors being companies or open source initiatives that targets different subet of machines and systems and basically you can write secal code okay and well you can target a bunch of intel machines you can target a your bunch of gpus style machine including imd and nvidia you can target fgas stuff like that and so the idea is to keep this standard as close to the language as possible and that's why we try to play around a bit with it to see if we can well if we can actually understand what's going on can what can we do with that the basics of the idea the vision behind the standards we worked with a bunch of people on some you know actual subject well related to high level sorry high energy physics and we will see how we can actually you know take all of that and can we actually fit that into an actual library with you know a decent api or whatnot and can we actually get something out of that so cle open standards for ause you know accelerator based computing okay okay that's the that's the goal of the it's supported by a bunch of companies the one version we used is the intel one which is called 1 api that gather a lot of things a bunch of them are very intel specifics you have a bunch of libraries like the old mkl stuff for data analytics you you got tbb all punched together and everything is associated to new version of the intel compiler which is called ipx this slide is actually a bit old now which is actually well it's a new version of the intel compiler which is based on nlvm and that also give us access to this sql supports you can actually try it i will give you some information but if you are more into you know like using nonproprietary software for building your own clang starting clang 50 i guess also support cle so you can try it with with clang or you can try one of the other you know version of that you can try iol also which is open source which is based on openmp somehow so you have a lot of you know how to say that variation of cal implementation laying around the intel one is great because now they have this you know push for more open source software and you can basically download the with everything inside and it just works so it's rather easy to to do this so that's the version we used just for this closure so whatever result we are going to present is based on this version of cle okay so what's what's the big things how how is this this thing is working so the sql programming model in some way if you are familiar with pring model for gpus like huda or stuff like that has some similarities with this we will see that we will have this notion of you know active threads over multicores that do stuff into blocks and so and so on and so forth the one m one main difference is that a lot of things in cle are actually very explicit in term of construction in term of selections so it may looks a bit more like you know verbos at the first time but we find find out that in the end it's actually easier to reason about what's going on and to build stuff around that so that we would try to to see that well it's put a bunch of functions and objects to find the device get an action quee on that which is the the way we would have to pass computation to the device there is a bunch of way to express different par operations and different way to handle memories either manually or using the buffer and accessors abstraction that will be as we will see far more c++ like that than what we have in other systems and we have a way to build task graph with an implicit or explicit endling of dependencies between operations so we have all of that playing around and we will try to see how everything works again if you want to try this there a bunch of link you can get later if you want to try that so let's go and see what's going on so the first thing we have to do when we write a cle program is to connect to a device this is done through the q object which represents the medium of communication between your program on the host machine which is usually your main your main machine your main cpu to the whatever device you are choosing to and the queue will transfer in operations data to the device and it can also retrieve information from the device so you can know what you are currently doing what kind of specificities your device can support because you can actually choose whatever you want as a device and this will be the main how to say that the main intermediaries between the cpu code and the accelerator codes so you can just build a queue and by default you will get whatever the implementation decided to give you it could be the best device you can find like if you have an accelerator it will give it to you and if not you will access to your cpu or it can be always a cpu it's it's implementation define but we can as we say that we can get the device back from the queue and ask him some information so we can say what's your name what's your what's your version number how many parm level do you have whatever and of course we can also select a device based on some well properties so you can you have a bunch of pre-existing selectors that you can use to say okay i want my i want gpu i want a cpu i want any accelerator if i have m of that you can also select device by choosing aspects so give me whatever you have but i want something which is the bable and support 16bit floting points for example and you can also write your own logic and say okay this is a device i have and if it has this or that properties i will rank it higher than if it only have that or that one and depending on where you run your code it will fetch the device information rank them and will give you the best one depending on your logic so you can embed a lot of how to say that tricks into the selection process because for example you can actually say oh you know what if my i i'm building a queue for doing some operation on a bunch of data in the matri or something and if i have the size of my matrix i have the number of operation i want to do i can fetch a device fetch the information about i don't know like it's memory bandwidth or something and if it fits my execution model then i will take it and if not i will just take the cpu and you can just run your code and it will be aut automatically be placed on the correct places depending on what you want to choose so it's very flexible so but the thing we have to to see there is that it's explicit we start by building a queue okay but the cool thing is that you can have multiple cues you can have multiple device at the same time and you can feed the device with different cues okay and every action on any number of devices on any number of cues are all asynchronous as long as they are sent to different cues so you can actually build a complex system where you will be pumping data into a queue while you pump operation on the other one you can synchronize those so you can build a classical you know pipeline structure so you can send the data while you are computing and getting some back and everything will be working basically without doing anything else but that because everything in cql is asynchronous by default that's also something we need to care about because obviously at some point we will need to do some synchronization or to do some precise code to ensure the cooperation of the different actions and cues and whatnot so let's see how we can actually start those par operations so this is a q things okay so we have a string which is badly encoded of course don't do that so this is the most basic things we we can do okay so that's using something which is called the unified memory model in which we will be just allocating memory in a place where both the cpu and the device can actually read and write inside it it's very crude it's not very performant in some cases but it help us just you know make something works and from that we can you know refine and optimize later so this is just so that it fits in the slide basically okay so what do we do there so we have this string over there we have this malo shared things that let you allocate a bunch of data in this shared memory block if you are familiar with cuda terminology okay this is not the same shared memory as in cuda that's something which is across the cpu the host and the device it's not the shared memory inside the device so we can just m copy the data inside that and it will trigger some data transfer at some point and then we can ask the queue to start some operation and we know it's parallel operation because it's like in in the name actually that's a parallel for takes a number of repetitions and it takes a regular lambda as as a kernel function so there is no you know like underscore underscore global kernel syntax whatever it's a regular lambda and actually it can be any regular colable objects so you can have function object somewhere you can have regular non non-template function if you want as long as it's calable as per c++ definition it's okay so there what do we do we take the input which is the index on which the kel is currently replicated and we do something at this place we just substract one from this from the current characters and then we wait we wait so we wait on the completion on parallel four okay so we can do it this way we could do it like waiting on the que or we could have stored the event object at the parallel for return and wait for it later okay event in cle it's a bit like a pance promise future things okay so you can just you know move the things around and when that's done as the memory is shared somehow we got the result there directly and we can send it back to the system afterwards so that's the basic things okay it works but it's not very you know neither efficient nether very c++ issue know like mal free things we don't like we don't do that anymore well we shouldn't be doing that anymore you probably should not because i don't and so yeah that's the basic thing we will see that we have better a better system for that now the the interesting thing is that the malo shared things do this allocation on this special memory block which is a bit kind to the pine memory ina so we do the m copy things it's damed into the into the device and whenever it's done because it could take some times over there and we run that we are basically waiting implicitly on the result copy to be done we start the the kernels on every sz computing element we go we do the results to wait wait for the finishing and we know that the result is back and we get the thing that's just that but as i say we could actually have done something else there wait for later and so on and so forth so now let's do that in a bit of a better way because at this point what we did was just allocating a bunch of data and call it today what we can do is we can use buffer and accesses buffers act as a view like array stuff okay that maps an existing part of memory on the ost okay and that can be later accessed on the device using an accessor so this basically say this is a piece of memory i care about is it's that big okay then i can use this submit things which is a bit more po than just calling parallel for it takes a endler reference which is some kind of a medium between the cpu code and the device code and what we do is that say oh you know what on this device endler i want to have an access source that will look into the datab buffer and this will create a relationship between the data on the host and the device and then i can use the accessor directly as a array like you know object to access the data through the buffer so it looks like exactly like what we did before but with extra steps and non-trivial abstraction for no reasons but we will see that it's actually important and what we can do later is we can have an host accessor which is an accessor on the host machine that look up and read the data back for us to get into our on the cpu and display and we see that we have this red only thing there which is an access modifiers we can have an accessors and that's very important because using this accessor and the order in which the accessor inside a submit call are constructed the comper will be able to infer the task craft dependencies between all the buffers and when the point where you are using them okay so this is a way to do it and it's a bit better than before because we just use this buffer thing on top of the of the data for that we don't have this malo shared and we don't have the free it's a more a bit more you know u er to i like okay and we can actually do even better because if we let's zoom on that a bit the same thing okay but look at what we do there we build the buffer into a scope and when the buffer is destroyed okay whatever data it's tied to which is all already on the device and not on the on the host yet will be transferred automatically so we can get rid of this strange o accessor notation just by scoping our buffer doing operation on them wai for them to be destroyed the destructor gives the data back onto the host machine and we got our results okay so we could actually use these buffer access of f as well as the an actual r2i enabled transfer system which is very interesting because we could actually put that into a function okay we start building the buffer we do whatever we need on the device and we go out of the functions and we know that whatever the data we are computing on the device in the function is back on the cpu when we go out of it okay so that's one way to do this we can we could also have a storage for buer elsewhere so we can keep the data in the gpu up to the point we need them later on okay fine fine fine fine now yeah we send stuff on on the gpu on the accelerator we did very simple computation for now what what if we want to actually extract some amount of fism from that so we have another model which is very close to what we are used to have on all the gpu like systems which is this block things that we call work groups there and so when you work on some amount of data okay you can actually slice them into work group and the world group is a bunch of work items okay it can be one two or three dimensionals and inside this world group you have subgroup which is onedimensional bunch of work item this is this green things over there in which you can actually access every work item the way you want and we have a way to either just work with work group and work items or we can work with the your hierarchy of work group group and work item as we need to express more or less nested parallelisms and by doing that it will also map on the actual hardware on the different level of fism you have access so if you have a gpu you probably go onto the mti processors and into the internal threads but if you have for example a cpu because you can access cpu system with cle the war group will probably be threads like in in cpu threads and the subgroup will probably be mapped onto cd registers or stuff like that or multiple cd registers and by doing these hierarchical parallelisms we can actually exploit different level of performance you know sources so let's write a small algorithm right let's do a four all things which is basically for each you take a function okay you take a vector and you want to process that this function on on the device so what do we do well when we build the queue we map the data from the vector inside the buffer okay and look that in this case i don't even have to specify the size of the buffer because buffer know what the range is and he knows how to get to the size of the range in c++ 20 directly so just know it's that it's a vector or it's a range or it has a data it has a size and it worked with that directly without us doing anything what do we do next well we have an accessor on on this buffer we compute the size and we run this parallel for this time with a nd range that will tell us that we want to work on block on 64 blocks of eight work items so this is block size and the work item size and what we have we have these item things instead of auto there we see that we have this one over there which is the number of dimension we work with and we do the classical oh well if my index of my walk item is in the zone of my data i just call the function over over the data and i'm taking a function from outside that i wrote as a lambda on the cpu directly i mean this is a regular lambda and you get transfer over there get ship to the to the gpu or whatever without us having to do anything else so it's a single source single path we don't have to think about oh it has to be a device function it has to be whatever it's a lambda just works okay so this is a very basic things we can probably do that better and some people did okay if you want to have a a look at that we can have a look at the parallel stl implementation from kronos which is basically a sqle based execution policy implementation you have a bunch of algorithm on gpu using this and it's it just works this way okay it just you just take a function you you run your parallel for you submit the things you capture the fun there by a reference over there and then again there and just walks just walks don't have to think about that and despite the fact that it's very explicit oh you have to get the queue you have get to to the endler you have to submit the thing you have to wrap stuff into a buffer in the end it just i mean it just look like regular c++ and you can just do whatever you want okay and one thing what we be having a look at later is that what if i push that to a point where i have a very complex c++ 20 library and i just do that somewhere and it just it just works okay that's in term of languages that's the main interesting point point okay so what do we do with that well we do some science okay or we try i mean some someone with a science degree do science and we do the computation but that's deal of it okay right so you may know about the lhc the larger drone colliders they take protons and they smash them together because that's how physics is done right now and in these collisions we expect that the this ey energy collision we just spring out new particles or new phenomenon that we don't know about or we know about and we want to be sure that it works the way we think it works and the atlas experiment is basically a bunch of detectors different kind calorimeters electromagnetic detectors a lot of things okay that detects how to say that not particles themselves but ins that somewhere somehow particle was there at some point and by analyzing those trace those measurement in space and time we can guess what kind of particle was there because if your particle is evier than what you think it has a different you know parabola like trajectory if it just go straight you know that it's not magnetic stuff like that okay physics and there is software which is called the acs software which is obviously as all use software as a recursive acronym of course so cs iss common tracking software it's a bunch of algorithm of detection so we pre-process the result of the row detector data right and we try to find traces of particles so there is a bunch of detection reconstruction and tracking of particles based on different algorithm so some people do machine learning some people do old school stuff like you you know calman filter stuff like that so we can group a measurement somehow in the in in some energy level and when we have that we can say oh yeah you know it looks like we go this way you know and by aggregating those small ins in a in a in a process called seaing okay these blue things as soon as we have something that looks like yeah it kind of looks like a trajectory okay we can extrapolate it and try to find you know where actual detection fits and do some statistics and so we know that it's actually whatever we were looking for okay and there is a lot of such traces in in a single experiment it's multiple hundred of gigabytes per second in every collision they do so they have a huge amount of data to process so they want to do that fast so they try to use whatever they can to accelerate those computation so gpus fpgas whatever okay it has to be fast so what do they do well they implemented a bunch of icts core algorithms using different techniques on nvidia machine using cuda using cle and basically we have a rather huge speed up compared to the basic cpu versions and it quite performs okay if we compare to more manual proper ca cod so all in all it was some kind of a good experiment because we we were about to see that yeah it works and it works as much as good as any other gpu targeting things so if we do sal stuff it works okay so what about we do less s stuff okay and more c++ stuff yeah because you know some point yeah we work on some oh sorry some bunch of library one of them is kaku which is the a c+ 20 storage library of multidimensional data and the question is why doing this because you know md span and stuff okay well it's not md span for different reasons first because we wanted to have a single you know places where we have the both non non non-owning and owning data structures and we wanted to play around and try to you know play around with api with the way we want to define algorithm the way we want to define interfaces for people to specify what they want into their container so we want to do that and as multi multiple dimension data processing is complicated for many reasons we also provide different algorithms and execution context that let you go on different hardware without doing the wrong thing with your data we are not in the business of computing in linear algebra stuff we are notun we don't do expression templates whatever we are just storing the data in a meaningful way in a controllable and configurable way and so we also provide way to process the data in a curr way so we want to do use c++ 20 as much as we can which implies bunch of template meta programming a bunch of concepts a bunch of a lot of thing like that and we use something that looks like execution policy it's the same idea but it's done in a different way because we wanted our context execution context to be something that some people can actually define themself which is not easy with execution policies and to do so we are based on something called algorithmic par skeleton that give you a bunch of simple functions and as soon as you have that you can you are good to go and we can use them into our algorithms so let's have a look how does it work so we have views obviously that's the easiest things to do and and the first thing we are going to look is what the e is this definition of v okay so we have this named the parameters based in the face where you can just say okay this is my size this is my data sources this is my option of storage and whatnot and you don't have to think about the order which parameter is a template or not we just infer everything from the definition of the of the object in a complex deduction guide but you just make a view with whatever stuff we that so this is a view of some size over the data i have there and i can pass them through to the square each things and as the type of the view is very complicated because we need to turn that into something that fit and keep all the information it's not easy to take one as a parameter so we have also parametric concepts where you can say oh yeah i want to view or i want to view with some dimensions or with some base type or b in any order you want or any other test compile time testable properties so usually we do this we have a concept of view and we can say okay give me whatever a view which is one dimensional and floating points and i can do something like this one what why 1d and not exactly the size because you can have this size is dynamic we can have static size we can have hybrid static compound time runtime size so we just say oh i have just want one dimension and you can just work on it like you know it's an array so you you can just you know iterate with using index but of course you don't really want to do that so you can actually have algorithm on that and this version is a bit different so we build a table which is the owning version of view so we allocate some memory there and we copy whatever is in the source thing and we have this parent thing that let you build a sub range from a sub range descriptor so if you ever worked with mat lab or stuff like that it's basically based on this notion of i want to go to there from there it's also a bit like numai so this give us a view so that's a view between the f the second element and the one up before the end and well i can transform this view okay and if you look at what we do that we have the function first okay this is the output and this is the input and the input are actually viic so we have a viic amount of input at the end so we don't have to rely on having an implementation on of zip or whatever which is not natural for a lot of people and then we can print the table and we are done so we have a bunch of algorithm can also do more complex slicing okay this is a 2d view of some data and i make two sub view oh right that's the second one should be a z sorry that's that's a z that's what i get for not re reviewing my code before we have this slice things which is a bit more complex stuff where you can pass information about from where to where how many elements you want how many elements you want to jump through and we have this underscore things that basically means you take everything along this dimension so we basically take a view of the data and we make two sub which are the two half of them okay along the outer dimension we make a table which has the same size as w and we transform w and z and store them the result into t and we can return t and everything is basically just working like that and if you want to go further all our algorithm is about to take a context so we have a basic cpu context which is regular computation or you can have cycle context and which is basically buildable like a que like a sql q so there i'm asking for having a gpu and this is the lambda i want to to walk over my table and my view and be done okay so that's basically what we want to have as an api and the question is or far can we go in term of performances well that's a result of a complex computation think like std power of ar tangent of x divided by cinus of whatever a huge compute compute bound computation right so the great thing is the time on on the cpu context so the basic things and second one is a cle timing using the cpu as a target okay so this is a huge 24 cores or 16 cores i don't remember 16 cores hyper threaded imd cores it's double so we expect to get something around 2 * 16 as a speed up okay two from the cd level and 16 from the course and the best speed up we have is like 29 over 32 which is quite okay okay okay and now we can take the same the same code and we just change i mean we just change the way we initialize the selector for the cle context and we can go over some nvidia things okay and then again we got these things speed up is a bit less because the gpu is a bit less interesting than the cpu but that's another discussion but we got some performance out of our code and the the the the thing that we want to achieve with that is to be able to if we go back a bit let's just provide proper implementation for all these algorithms and a bunch more especially being able to work on subtiles working on complex stencils and maps up onto the proper device code each time and by using this simple implementation of the context that we just have to have a map reduce and a scan base operation and we can rebuild all the algorithm from that we can quickly have a bunch of of first running and for more complex algorithm that doesn't fit the simple model think about sort or search stuff like that we have a way to specialize algorithms based on on an actual concrete type of a context and so we can write the proper things done so it was a cool a cool experiment the non cycle version of the library was quite already advanced when we start doing that and the cle implementation to us like what two weeks something like that of just finding the correct you know way of doing the things wrapping the the elements in the correct way so it was quite a nice experience because we had this existing c++ 20 code based we came back with cle just show the thing where they should go and it was just working so that's something which were actually a good you know positive return on that okay so before concluding [music] want to say that there is a lot of things going on into the c api there is an extensive document on the konos websites that give you all the information about how it's supposed to be working okay and usually okay i i should have make a real solo that because 10 bucks i will pick something and it won't work where is it section whatever yeah got some pretty extensive documentation and even sometimes you got some decent yeah not this one of course got some decent examp or maybe for the r you got some decent examples some places it's it's updated quite frequently whenever something changes you have an extensive list of stuff that change from the version that was not c++ 20 based they offer information about how to simplify old sqle into new sqle and stuff like that it's a very useful document it's pretty much interesting to write and for the people that do what is it they want to go further than that they also explain oh you have a special hardware and you want to support cle this is the thing you have to write so we can actually support your whatever into sec directly so it's a very open-ended things it's very interesting to to have a look at that if you are into this so let's conclude on that so moving forward well if you want to use accelerators or whatnot you have to have tool for doing this because you are not going to be able to find people that are able to think about your business like business side algorithms and then know how to work on all those machines so you have to have tools and having standard like c which is ca vendor ca machine is actually a step into the right direction so you can quote me on that that's my personal opinion on that it's easy to use it's very simple to deploy so i cannot do anything else but telling you to try it if you want to go into that it's c++ 20 compatible you know what the range is it knows how to endor stuff like you know topples it has a very extensive way to detect trivial types regular types and know how to handle all of that it's concept compatible so it's actually very interesting it's also cool to give feedback back to kronos they are pretty open about that so you can actually try the things report bugs and try to get things sorted and i would just want to make a special thanks to s which is actually my phd student that work on that and which is responsible for all the graphs and the explanation about the atlas experiment that we worked on with adan gon and david shamon from labs that are also working on on this project so thank you very much and see you next [applause] time and and in a in in a quite surprising time of event i still have time for questions this time so any questions come on yeah i have a few so like i can alternate with other people let's go so first of all like i didn't you said oh okay it's going to target this device it's going to target that device right and depending on what's available and stuff when is the compilation happening of the of the code of the sqo code for the device so you actually have the choice basically the the the main way of doing that it's all compiled ahead of times in something that looks like a lot like ptx for cuda which is a some kind of an abstract pre-compiled stuff that at run time we get adapted to whatever happens okay as the very beginning now you could also if you if you use a preset selector like if you if you say oh i i build my queue with a cpu selector or i build my queue for a gpu selector that's something the compiler detect and we just compile for the correct so for the correct sorry the correct device ahead of time now you have also a way to ask for just in time compilation so when the application start whatever your colel walls are going to be compiled so you actually have the choice and the compiler on it side try to infer from whatever you wrote on the q definition whatever it should be trying to compile for and this compilation can be done either when you compile or it can be done just at the beginning of the application you you actually have the choice on that so i can do all ahead of of time all all just in time or i can do partial compilation that's a good question i i i think the partial compilation it was working at some point okay okay let me rephrase that oh god and i'm and i'm actually you know recorded so in tell people you know just forget that it was supported at some point and you still find trace of that into the document mation now i will be very frank with you we usually just you know do whatever the basics things do so we are probably just using the ahead of time things i'm not sure the hybrid thing is still supported but i have to go back on you and that because i don't want to say something stupid but we we have this opportunity to change that i know the hybrid things was working for a while when c was basically trying to just ride over open c and all the pre-existing implementation i probably have to check if it's still the case thank you hello i have some basic questions just about selecting uh the requirements on on on your yes where the q should the selector yeah you can put there as many condition as you can yep and it will choose the best one and can you pass their condition that you want different device than the other q has yes so what you can do is you can select so you have this which is this is basically a nm okay this one the cpu selector gpu selector accelerator selector it's an enem it's basic things to do and we just pick one then you can use what they what we call aspects which is a list of stuff you want on your device or stuff you don't want on your device you have a low deny list systems so you can actually say oh i want this and that but not that and that and the the system will try to find you something but if it doesn't find anything that works it would just say i didn't find anything i'm just quitting and then you can pass an arbitrary function that take a device as a parameters and return an integers and what it does with that is that you can test for information about the device or whatever you want next to that because you can just construct this function or object function the way you want or you can pass additional information and what you do is that you say okay this device i can ask for information about it and i can give it the score and you will pick the highest score possible and you have the choice to say if i return zero just pick one because that means i i didn't find anything fancy so just make a choice or you can return minus one which give you the say okay if you didn't succeed into finding this lo something with this logic i want you to fail so instead of just saying i want this one or that one based on information you can actually rank properties of the device and the ranking will be used by the system by okay this is all the device i have i will rank all of them and i we pick the best ranking or i will pick nothing or i we just fail so you have a you have a bunch of how to say that flexibility on that considering that for example this get for things it basically has like i don't know like 20 30 something information you can grab from the device so ranging from the kind of memory it supports the kind of operation it supports is it debuggable is it emulated or whatever i mean you can get very fine grain selection process or ranking process and usually it's enough and you have this opportunity to say if i don't find anything i can go back and you know give you a default oper systems so you can b bally write whatever so in the beginning you had this slide that showed the bunch of implementations that are available so i'm wondering if there if i'm writing an open source library is there some kind of fallback implementation if a user of my library doesn't use one of these compilers that will just always oh you you can you can use a sorry you can use a clank cycle implementation yeah but say someone is using visual studio and they don't have it is there like a library implementation that will just always schedule everything on the cpu and if you use another so that's probably you probably want to target three cycle which was the old vers the oldest version of all of that and it has a default mode where you just do cpu things no i'm thinking more along along lines of i'm making an open source library i want my users to be able to use whatever compiler they are using yes and so they will compile my open source library themselves and they will use probably visual studio and i have no control over that is there like a let's say you do that you your target user studio and you want people to be able to use your things that you cle then you should tell them to use a three cycle library version which is basically cle as a library without any compiler based so just the most regular you know no special things you can have and so they just you can just say oh if you are on these things and you don't have any of this fancy you can use that can use just tricycle okay thank you again thank you so joel i know you're very familiar with cpu cd stuff and the question is so this seems to from what we talking about this seems to be a lot gpu based so how close do you think you can get with cle to the handwritten c code that targets cpu and cd and threads okay so for the cpu supports on the threading front it's basically using it's usually used stu like as open m or tbb in the back so the trading support is pretty good for simd depending on the back end so i'm i'm speaking from my own experience so i may be wrong so take that with a piece pain of salt people usually either rely on pragma sd from open mp or they try to reuse internally the autov vectorization process of the compiler so that basically mean that the imd quality is depending on what kind of system they use for that so i guess basically that if you compile using say the intel versions you will get result that we get pretty close to whatever the auto vectorizer from intel is going if you use any other one probably clan clan is probably relying on its own autov vectorizer on or on open the sd and in all cases that means that if you have a piece of code that is not not how to say that that amenable to being vectorized because it use some you know like some cmat functions or it use a complicated memory access pattern that the auto vectorizer doesn't know about is probably subar compared to what you can do by end now what could be done but my days are quite full already it should be possible to make a a platform back end for cle that actually use an actual proper vectorization system i didn't look into that but it's probably fible but then you will have to be careful because you cannot you have a lot of information in this platform back end things that is probably too run timey if you want to have a perfect you know cd code generation so it's usually as good as whatever the autov vectorizer of the comper is which means that if you do how to say that regular computation i don't know if it's a thing you know like you are not going to to do very fancy complicated matte things or you have very complicated run runtime based and in size or whatnot it would probably be pretty good i mean as as much as good as you could have with the auto rizer but it's probably some not below what you can be doing with manual or simply if you want to you know write everything yourself and and do all these complex things all by yourselves it's probably in between okay hello so you had these questions with a secret string what's the decoded version oh i don't remember i think it's something like h world from sel or something like that yeah stuff like that yeah it's probably a world this is this is cycle from one api or something like that the funny thing is that that's actually a that's actually nint example and they didn't check that i mean i mean the end of the the end of the secret is actually okay sowhere no yeah it's a h world thing i mean it's not very very funny and we have another question from the audience so what implementation do you recommend to get get started with cycle and can you use actually clang directly normally clang just work out of the box as long as you have like some recent versions we used to use the clang trun that it's on compiler explorer to do our early exploration if you don't want to you know go into oh i need to install complex stuff on my machine or i can't install whatever just just give a try to the 1 ap docker it's i mean it's a bit big it's like 10 gb something like that but you have the y you have the y bonanza so you have you have the compiler for sql you have the libraries you have the what's the name of these things it change all the time vune slash parallel advisor sl i don't know how it's called today but you see this kind of thing so it's it's r trial you can just docker it up and it just works so depending on what you want to have as an experience you go either with a claim version or with that and if you want to go a bit further than that and you need stuff like you need to support cuda you you want to target sorry cuda and stuff like that then you have an extensive setup setup documentation on the 1 api pages where you can have you can find the exact packet list for your own linux distribution both for the compilers the tools and the the cod playay cuda plugin if you want to support cuda it's rather well documented and from experence it usually just works so it's something which is also actually quite cool because you don't have to deal with a bunch of you know trashing your display because you forgot to update your graphic driver or whatnot so then thank you again thank you [applause] again