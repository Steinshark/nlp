this is kind-of a follow up to brais' videos on deep learning so deep learning is kind of a big thing at the moment and there's some disagreement between research over whether this is gonna be - the, this is *it* this is the big thing that's gonna change everything or whether this is another flash in the pan, like artificial neural networks were in the 80s everyone got very excited and they got quite a good results and when they realized that they couldn't solve all the problems with them, i don't know for what it's worth, these are a big deal, i think [offscreen] let's talk about convoluted neural networks, have i said that right? convolutional neural networks. [offscreen] ah, right, ok they combine both deep neural networks, which is what brais was talking about and kernel convolutions, which is what i talked about in a previous video. i would thoroughly recommend people watch that video you know, it's got an entertaining host, right? *laugh from offscreen* so, but, but, because, if you don't know what a kernel convolution is, this isn't gonna make much sense to you so watch that video first [offscreen] so that's the kernel convolutions we did on graphics and things like, uh, sobel op- yeah, sobel operations, gaussian blurs, and things like this. sobel operations in particular, and edge detection so, if we think back to a traditional artificial neural network ok, what we've got is we've got some kind of input we're trying to learn, ok we've got some hidden layers, alright, and then we've got some output layers maybe just this one, i don't know and these are fully connected, so we have lots of connections from here and here and here and these are connected to here and so on, i'm just drawing in a few of them, and then these are all connected to the end. ok, now using brais's analogy, we were talking about house prices. ok, so, this will be something like number of bedrooms, and this would be something like "has it got a pool" and this would be, you know, what floor space is it and has it got a good garden and so on lots of these, ok, lots of inner nodes that we don't really care about particularly, or i don't uh, right, and so on and so forth, and then finally at the end we have a house price. now what this house price is, is a complicated function of these inputs. it's complicated because this node here is some linear, or non-linear now, combination of these. ok, so, a bit of this plus a bit of this, plus a bit of this, plus a bit of this through some non-linearity function. this is a different combination of these. this, again, different combination of these, and so on, right and then this, a different combination of these so, you can see, you're building up some kind of level of abstraction here where you've got combinations of combinations and that function is very complicated when brais talked about a black box, in some ways that's exactly what it is because we can't look at these individual weights and say, "well that's got .2 of this one, so that must mean this" because we just don't know what it means, right in the grand scheme of things, in this whole network, we don't know what that individual weight means and to be honest, we might not even care that much what we really care about is how well does it predict house price, how accurate is that based on that for a different input, so we change this, read the output - is it good? yes? brilliant! ok, now, for images, which is obviously what i spend most of my time around, this is a start, but it's not very useful to me. if you think that this is our inputs, ok, and i give you a picture of a house, and i say "right, tell me how much this house is worth" ok, well, what? so how do i, ok, so there's two things i could do, right first of all is i could try and calculate things like number of bedrooms and stuff, based on the image and put them in here in some way, i'd be calculating some features and then i'd be putting them in here and learning on those features. that is quite a smart way of doing it, because, apart from that's obviously quite difficult um, it's smart because we don't have to have that many more neurons in anything, we can actually use the same network as we used before for our model on our house, all we have to do is work out the bit of code that does the image analysis now, anyone that's tried to find out the number of rooms in a house based only on one picture of the outside of the house will tell me that that can't be done, right that's hard. ok, so you could naively think, what we could do instead is just put the image in here. make this the first pixel, and this the second pixel, and this the third pixel, and so on, ok. then, this has got all the information it could ever need, right but it, but that's the problem 7 megapixel image, that's 7 million input nodes, let's say we have 7 million nodes on the next layer each one connects to each other one, you can see that that's just gonna melt my computer it's not even gonna try and create it, it's too much information that's why we downsample our space a little bit. what we would usually do is calculate some small subset of features and then we would put them in at this end. so that's quite important. so, traditional machine learning is done a bit like that. so michel's done some videos on this. calculate some features about someone's face and put that in to some machine learning algorithm what you don't do is try and put the machine learning algorithm just on the face because it's too much information there until now, ok, right? that's where convolutional neural networks step in. so, convolutional neural networks replace each of these nodes with a kernel convolution. so, like, a sobel edge detector now, so instead of what i would've done before, which was run a sobel over something and then machine learn on that, i just give this the opportunity to learn which features are interesting maybe it is an edge detection, maybe it is a corner detection maybe it's something that highlights whatever's in the middle of the picture or something that highlights the top left-hand corner it doesn't really matter, and the point is i don't know what they are right, if i give you, you know, two thousand pictures of houses and ask you to predict house prices based on the pictures i don't know for sure, i can guess, but it might be - that how many windows it has and things like this but i don't know for sure. and a computer can brute-force through those things much quicker than i can and tell me and then i can go, i can both predict it, and i can look back and say, "oh, it was windows after all" so, let's imagine that what we have is our image, ok, so i'm gonna move away from the house analogy now because i'm gonna have to draw a lot of pictures of houses if i do that. ok, so let's talk about cnn works um, and why it's useful. so, we have an image of something now, i have seen convolutional neural networks used for non-images but for now, we'll just talk about images this is a picture of, let's say me. it's, you know, it's not a great likeness but i'll stick by it now, there are three channels here, ok. so this is actually a 3d volume, in some sense remember when we talked about 3d images, you could view rgb as a, in some sense, 3d so, the first plane is our r, g, and b, or vice versa what we do is, if we performed a sobel edge detection on this, what it would do is produce another image that was  slightly smaller than this and only one deep. so hypothetically, it would be another image where the edges, let's say the horizontal edges, were highlighted so it would kinda look like, that, or something, i don't know some half of my face where the horizontal edges are highlighted, ok it's not a great diagram but there would only be one output, because sobel just outputs a number between 0 and 255, as soon as you scale it, ok now the problem is that i don't know that sobel's the best thing for this task, ok it might be, right, it might be useful to detect edges on houses, to work out what their prices are or if you want to detect faces, to detect the size of the face, that kind of makes sense on the other hand, it's gonna produce a lot of erroneous bits if i was sitting in front of a tree, there's gonna be loads of edge stuff going on there that i don't care about in a convolutional neural network, what we do is we do, let's say, 60 of these on the first layer. so we have one, and then behind it we have another one, and behind it we have another one, and behind it we another one, and so on, going this way. so the first one will be some convolution process applied to this whole image that takes three input channels and outputs one output channel the next one will be a different kernel convolution operation so each of these will have a different kernel those are our weights, those are these values here in sort of our analogy back to normal learning um, and so let's say we have 60 of those, or 64 of those one of them might be detecting edges, one of them might be detecting corners um, and then we use them as our features for learning now that's a start, but we're - this is is deep learning now, right, so what do we do now well, what we now do is we do more features based on these features so we find combinations of corners, combinations of edges, that make something interesting my face is not just a circle of edges, what it is is a number of corners and edges and bits of texture and things all in a specific shape that is unique to, uh, well, certainly to a human face, but even unique to me right, because we're capable of distinguishing between different people so, this kernel window will go down to this pixel here ok, so this will slide about this image and produce this output image and then the next one will do the same, and the next one will do the same then we do the same thing on this one let me do it in a different pen so we can see better. here's my red kernel convolution and this slides about and produces another image, which is a slight combination of, maybe, corners and edges or something. i mean, this second level, it's not gonna be too abstract, but we'll get the idea so there's gonna be some sort of shape that's gonna be sort of... it's not gonna make much sense to us, but it'll make some sense to this machine. and there'll be another set of these, so there'll be lots of these, right, going back all of these will look different and be some different representation of my face transformed in some way, to be useful and again, i haven't picked these, these have been learned, just like a normal deep learning algorithm so i haven't had to say, "i definitely think edges are important for this" cause i don't know for sure. so this goes on, and we keep doing this, and sometimes we also downsample the size of these images, just to save memory, ok, but we won't dwell on that too much. and, because of the way that we downsample, and the way that sometimes these convolution operations slightly shrink the image, cause they don't go all the way to the edge, right if you've got a 5 by 5 kernel, you can't go to the edge 2 pixels cause you're going off the edge so we don't worry about that, we just get slightly smaller in the end, we end up with a much smaller image, and lots of features going all the way back so these are my different convolutions of convolutions, of convolutions, of convolutions and each one will look different, and represent something different and we don't know what that is. so this one, could be highlighted when it's a face in the middle, or it could be dark when there isn't this one might be highlighted when there's an ear at a certain position, and so on. eventually, these will get down to being just one pixel, and very very long so essentially what we've done there is we completely removed the spatial dimension. there's no more spatial information left, we don't know where anything is. but we know what it is, because it's listed in all these features. these now are our neurons at the end. so we have a couple more layers that point to these, and then finally, we have one at the end that says "is this a picture of mike's face?" and it produces a 1 if it is, and a 0 if it isn't. and then what we do is, just like a normal network, we train it. so we say, "here's a picture of me", ok , so this should be a 1. and let's say it's 0.5. cause it's kind of random. so we adjust these weights, and we adjust the weights inside all these kernel convolutions. [offscreen] so does that adjustment happen manually? no, it happens using a, uh, well, it's coded in, um, but it's usually performed by a library, and it's using a process called back propagation. so what we do is we basically predict what direction we have to move the weights in to improve our output, and then we move them over slightly in that direction. and we have to do it in reverse order, because these ones depend on these ones, depend on these ones, and vice versa, what we do is we say, well, look, given that i've said it's 0.5 chance of mike and we want a 1, how do i change these weights here to get slightly closer to 1? and i do it. and then i say, "how do i change these again to do even better?" and so on and then i work my way back, ok, that kind of maths we're not gonna go into, right. a lot of these things are, are implemented in libraries so as a researcher, i mean, much as i'd like to implement some of these things, it takes quite a long time just because programming takes a while, right, and and, it's better for me just to apply these things and get good results than it is for me to reinvent the wheel all the time, constantly, if everyone was programming the same things over and over again, no one would get anything done so, i'd have to start by programming up linux, to get, to get, i'm not claiming i can, by the way, and, and so on. so, you know, let's not reinvent the wheel. um, so i do this, i send in, let's say 1000 pictures, 500 of which are me you know, so i've been to a photo shoot or something, right and 500 of which are not. and i train it so the convolutions and these weights on the output are such that it gets 1 when it's a picture of me and 0 when it isn't. and then i can look at these convolutions and say "what is it about me that's distinctive?" and it's probably gonna be finding, um, you know, weird shapes on my face, right cause it's a bit of a weird shape, so it - things that are unique to me now in a more general situation, there's a big database called imagenet they have a competition every year to see who can classify these images the best. so dogs, cats, planes, trees, and so on ok, they're all in there, and there's a thousand or so images of each right, so, we have a really big network that's much bigger than this little one i drew and we say, "right, let's throw millions of images at this", right, thousands of cats, thousands of dogs and we have lots more outputs than just the one, and we say "what is it?" and it says "it's a dog" and it is. *dog bark* convolutional neural networks have been around for a little while but, they've really started to be big in about 2012 when it - when someone came along, applied one of these to imagenet, and got incredible results. and so on and so forth. and now there's this big push and everyone's trying to get even better results, and even better results. now, i work on more of the applied end of computer science, so i'm more interested in how this affects plant science and things like this so that's what we're working on. um, but, the kind of results we're seeing are really really impressive so, i mean, case in point, i've done, i've done some root tip detection, so detection of root tips in images, right, of plants and, um, i've got some software that i've already programmed and i've kind of done a low-level feature detector approach to detecting root tips and it's about 70%, ok, which is what you would expect, because maybe some root hair gets confused as to root tip or a bit of blotch of dirt, or maybe there's just two root tips really close together and it gets confused this, the cnn that i trained, um, is 98% accurate and it finds them with 99% accuracy. it doesn't make many mistakes. and that's over thousands of images. [offscreen] so does that mean the work you've done already just goes out the window? yep. uh, no, to an extent, yes, and to an extent, no. you need expertise to be able to craft a network and train it and prepare the images. and there's obviously work to be done, and there's some disagreement over how much of a problem you can solve with a convolutional neural network so, there are lots more things you can do with roots beyond finding tips. can you do all of them with a convolutional neural network? i don't know, we'll see. are we trying, but, we'll see. maybe not. so maybe what you do is you use this as a tool, just like other machine learning algorithms, within a package that does lots of other things as well. on the other hand, if you're just doing cat and dog detection, you might as well use a cnn, cause it's gonna do better than anything else. the other purpose for ways the botnet can use its parts is for distributed computing [fades out] [fades in] now, some objects obviously are more amenable to this than others, but the more images we get, the better it is. there's no depth involved here at all, ok