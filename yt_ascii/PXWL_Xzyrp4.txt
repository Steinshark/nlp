here's a common mistake that i see in python this code computes the sum of all the unique numbers in a list we loop through the numbers and check if we've seen the current number if not we append it to our scene list and add the number to our sum in this version we use a list but with very slight modifications we could have used a set dictionary or tuple instead yes i know for this specific problem you could also use this slick one-liner but the problem here is just a stand-in for this general pattern of doing a loop and repeatedly checking if you've seen something each iteration so how much of a difference does changing the data structure make let's give it a try on a thousand ten thousand and a hundred thousand numbers on a thousand elements clearly tuple and lists are doing much worse than set in dict but this is python does a few milliseconds matter ten thousand is a bit more of an annoyance though set and dict are still sitting around a millisecond but tuple and list are now approaching a full second of runtime wow and keep in mind this is just for one function call not your whole program if you need to call this function a hundred times you're now waiting minutes if you use tuple or list but still a fraction of a second for set and dict for a hundred thousand numbers set indict are using 10 to 20 milliseconds but tupeland lists are now taking minutes to complete that performance is bad enough that even if you don't care about performance you probably care when it's that bad and that's because of course performance is relative if you're writing a high-speed trading system a microsecond might be an eternity if you're painting frames to the screen maybe you only care if your computation exceeds 16 milliseconds to reach your 60fps goal if you're scriping data from your favorite websites as long as it takes under a minute you might not mind waiting or if you're computing something that only needs to be computed once you may well be perfectly happy waiting hours or days for a computation to complete rather than spending weeks or months devising a clever way to compute it faster the difference here is in the big o run times of the in operator and adding an element for less than tuple using involves looping over all the elements until you either find what you're looking for or if you check all of them whereas for set and dict in takes advantage of the hash table based implementation in order to calculate where the item would go if it was present and only check that location in other words in is a linear or big o of n operation for less than tuple but a constant time or big o of 1 operation for said indict in the case of the tuple appending one element is also a linear operation since tuples are immutable and you have to copy all the old elements into a new tuple that's one larger however for list dict and set adding one element is what's called amortized constant time which is a fancy way of saying that adding a single element might do a lot of work but if you add n elements then the total amount of work is still just o of n this often happens in geometrically growing data structures like list set and dict that have some capacity and when you exceed that capacity you have to go get more memory and copy everything over to a new location but if you have spare capacity then you can just immediately put a new element in one of your unused spots assuming that the runtime of adding the numbers is negligible this makes the set and dict versions of n and the list and tuple versions o of n squared which is what we call in the biz in accidentally quadratic algorithm now big o is in everything especially on small inputs but in this case it's the reason for making this code unbearably slow so make sure that you know your big o time and space complexities for all the basic operations on common types because yeah even in python performance matters sort of