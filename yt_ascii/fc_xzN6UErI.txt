intel has always been really good at r&d. furthermore, all throughout their history,   they have had the balls to take 
swings on new technologies. the high-k metal gate. the 
finfet transistor. strained   silicon engineering. doing 
these innovations took guts. for all their struggles, intel 
never lost that. and their coming  'a node is going to be another one 
of those big hack wilson-type swings. one of the two bold innovations in that 
node is the gate-all-around transistor,   which i covered in an earlier video. the other one is not as sexy. but 
i argue is way more ... electric. in this video let us look at 
intel's next breakthrough:   the back side power delivery network. or as i 
shall insist on calling it in this video - bspn. ## a note i am going to do my best here, but this stuff 
is particularly tricky so forgive me any errors. before we begin, i want to recommend doug 
o'laughlin's two write-ups on this topic.   doug is great and he did very well here. feel 
free to read it first before you move on. i also want to point you guys to the research 
done by imec over in belgium. they pioneered   the technologies behind bspn and have 
published loads of material about it. ## beginnings let us start with the basics, 
and then work our way up. the core function of the transistor is to switch 
on or off upon the receipt of a control signal. in order to do that, they need power stably 
supplied to them at the right voltage. power is the measure of doing work, as in how 
quickly energy is being used. mathematically,   it is current multiplied by voltage. voltage is the force that drives electrons through   wires like the pressure that 
pushes water through pipes. if the voltage is too low, then the 
transistor might not switch properly.   think of opening a water pipe but finding 
no water because the pressure isn't there. a 1% drop in gate voltage results in a 1% decline 
in the chip's operating frequency. at its worst,   insufficient voltage can cause timing errors, 
which in turn cause a blue screen of death. but if the voltage is too high, 
then the chip starts using too   much power. the power consumption of a 
switching circuit is proportional to the   square of the voltage. higher 
voltage, more power consumed. and if the voltage gets way too 
high, then the transistor's gate   oxide might break down entirely 
from the powerful electric field. the voltage at which this happens 
is sometimes called the "reliability   wall". we should take care to avoid breaching it. different sections of the integrated circuit have   different voltage requirements. designers 
segment the chip into areas called "power   domains". some mobile socs might have hundreds 
of power domains to best optimize power usage. thusly, delivering power is a goldilocks 
situation. not too little, not too much, just   right - within a margin of about 5-10%. and it has 
to be delivered stably, efficiently, and reliably. ## the power delivery network think of a desktop cpu. where 
does its power come from? it comes from the power supply, which 
provides a high voltage and current to   the cpu through the sockets and socket 
pins attaching it to the motherboard. if we are talking about mobile 
chips rather than a desktop cpu,   then we won't have socket pins but 
rather solder balls. pins are too big. once it enters the chip, power 
must travel through the chip's   packaging to the transistors. this 
is done using a customized network   of metal wires or "interconnects" and 
"vias" running throughout the chip. vias help us go between the ic's many layers. so   think of interconnects as being like 
roads and vias as like mario pipes. there are two other devices spread 
throughout this system - voltage   regulator modules and decoupling capacitors. first, voltage regulator modules. the ic's 
various power domains have different needs.   so the voltage regulator modules step the 
voltage up or down as needed for each domain. voltage regulator modules are nice, but 
sometimes cannot leap into action fast enough. so we add decoupling capacitors 
to smooth out the network. like if a microprocessor suddenly 
ramps up for something, the sudden   activity may cause voltage spikes via 
inductance in the metal interconnects. inductance refers to the fact that when a 
current flows through a metal interconnect,   it produces a magnetic field. if we are to change the current - like if we 
turn it off or on - then that interacts with   the magnetic field in such a way to create 
unexpected fluctuations in the voltage. when that happens, the excess charge can 
be routed to a decoupling capacitor and   stored for future use - like 
in the case of a voltage dip. so in some ways, you can think of them as like 
drainage ponds taking on water during the storm.   and like such ponds, they are strategically 
placed on the pcb and even the chip itself. altogether, we call this system - 
the power supply, the interconnects,   the voltage regulator modules, and the decoupling 
capacitors - the power delivery network or pdn.   much of our focus for this video is just on 
the interconnects. but i wanted to be complete. ## ir drop the power delivery network's role is 
to distribute power and voltage while   keeping noise at the minimum. 
it is extremely complicated. because we are living in reality 
and not an ideal simulation,   metal wires suffer from parasitic inductance 
and resistance. let us break that down. parasitic, meaning that we don't want 
it. like that one roommate who eats   all the chicken nuggets, doesn't do the dishes,   and just plays video games all day. i am 
talking about you, jeff, i still remember. anyway, i already talked about 
inductance so let's talk about   resistance. resistance impedes the 
flow of electrons through the metal. the result is voltage at the end of the 
wires is lower than what it is at the   power supply at the start of the wires. 
kind of like how a dam holds back water. people in the industry call this "ir drop", and it   is one of the more serious issues plaguing 
the leading edge semiconductor industry. ## scaling on the frontside
throughout semiconductor history, the metal interconnects carrying 
power sat on top of the transistors. so if you can imagine the silicon 
wafer, the transistors are built   into the surface of the silicon. then 
we use deposition, lithography and etch   to add many layers of interconnects and 
vias on top of that. maybe ten to twenty. the power delivery network brings power from the 
global power supply in from the top, traveling   out and down to the transistors on the bottom. 
the deeper you go, the smaller the wires get. this arrangement is referred 
to as the "frontside power   network". and it worked fine for a long time. ## the cost of moore but as moore's law progressed, we crammed more 
transistors together onto a single ic. nowadays,   we are dealing with billions of transistors. since each of these transistors have to 
be powered, we had to add more layers of   interconnects. these power-carrying 
interconnects interfere with those   interconnects carrying electrical signals. the only way to deal with that 
is to make them thinner. but   nowadays these interconnects are 
around 20 nanometers in diameter. this results in substantial 
ir drops by the time the power   gets through this long long journey 
from the supply to the transistors. resistance is a problem that the industry 
has been dealing with for a while. it was   one of the reasons we moved from aluminium 
to copper interconnects twenty years ago. but that was twenty plus years ago. now we 
are beyond the 5-nanometer generation of   process nodes, and these challenges 
have come back like the measles. at twenty nanometers, copper isn't the 
best metal to use despite having the lowest   bulk resistance. copper's bulk resistivity is 
really good - 1.664 micro-ohms per centimeter. but electrons traveling through it are more 
likely to deflect and scatter instead of going in   a straight, uninterrupted path - or as they say, 
"electron mean free path". so at 20 nanometers,   the electrons in a copper interconnect are more 
likely to scatter - making its resistance higher   than what other metal lines might have. 
weird how nanoscale stuff works right? furthermore, copper interconnects require 
barrier layers to prevent the copper from   poisoning the silicon. those liners are 
generally made from tantalum nitride,   which has a very resistance, and 
does not scale down very well. so chip designers have had to trade off signal 
resources in order to guarantee a more robust   power system. in response, the esteemed 
semiconductor research center imec out   in belgium proposed a set of new innovations - 
buried power rails and then after that, bspn. ## buried power rails let's do the first one first. while there have been a lot of innovations on 
top of the silicon surface and its transistors,   there has not been as much work done beneath the   silicon. this is where buried 
power rails come into play. we build designs from groups of finfet 
transistors called standard cells. power   rails are what we call the strips 
of metal interconnects at the lowest   levels - bringing power to the adjacent rows 
of transistors within the standard cells. we measure standard cells in 
units we call "tracks". your   average standard cell at the 3-nanometer 
node generation is about 6 tracks high. but due to the issues we talked about, 
the rails no longer scale. they now take   up two of the tracks, leaving the 
whole standard cell a bit cramped. so what imec is proposing with 
buried power rails is that we dig   trenches into the silicon substrate using a 
methodology called shallow trench isolation. we then use atomic layer deposition to put 
the metals for the rails into the trench. we are doing all this in the same stage as 
when we make the transistors - known as the   front end of the line or feol. the temperatures 
in this stage get way too hot for copper metal. so we have to use a sturdier, more 
thermal-resistant metal like tungsten   or ruthenium. this brings upsides and 
downsides - as is always the case in life. one upside is that - when taking into 
consideration the line widths of 20   nanometers - tungsten or ruthenium metal power 
lines have substantially lower resistance than   copper lines. for the reasons we talked 
about earlier, the scattering and all. so tungsten or ruthenium. the supply chain 
situation for tungsten is quite good. and   ruthenium for its part doesn't need a thick metal   barrier layer, but only a small layer to 
adhere it to the surface. that's good. the downside of course is that fabs have never 
done metallization work in the front end of the   line. there are metal contamination issues to 
consider, which always makes people nervous. after we finish cutting out and filling the rails, 
we can then build vias to bring down power from   the larger power delivery network to the rails. 
voila. we just stuffed some stuff under the floor. ## bspn
imec positioned buried power rails as a first tiny step forward in making better use of the 
space underneath the silicon transistors. with bspn, we take a great leap forward. 
okay, maybe let's not use that phrase. with buried power rails, the power 
still has to make its way down the   ic's 10-20 layers to the silicon transistor 
layer. the distance gives more opportunity   for metal resistance to do its insidious thing. so the bold thing that bspn does is to 
separate all the interconnects carrying   power from those carrying signals and then move 
that underneath the silicon transistor layer. to connect the network to the power 
rails, we dig nano-through-silicon   vias through the silicon wafers. this is 
bspn and together with buried power rails,   it cuts down on overall power consumption. how much? a study at the university of texas, 
austin found that power consumption is about   8% better with bspn over a standard 
front side power delivery network. more critically, ir drop is 
significantly reduced to about   15% of that from the front side configuration. arm presented simulation results back in 2019 
showing something similar - a 7x improvement. there are knock-on benefits too. moving 
out the power interconnects lets us spread   out the signaling interconnects on the 
top side. more space cuts down on their   parasitic capacitance - unwanted stored 
charge that hinders their signaling speed. ## image sensors the idea has been pioneered in cmos image 
sensors with their backside illumination system. a cmos image sensor works like an 
eyeball. light first enters through   the lens and some color filters, then hits 
an array of silicon photodiodes built into   the substrate - the silicon wafer. the more 
light that hits the photodiodes, the better. but with early image sensors, the light had to 
first pass through a mass of transistors and   wiring before hitting that substrate photodetector 
array. this was because they built that wiring   on top of the substrate. this messed with light 
collection, hurting low-light imaging performance. so in 2009, the image sensor companies omnivision   and sony semiconductor solutions 
announced backside illumination. they moved the wiring to behind 
the photodiode substrate,   making it so that the photodiodes are 
illuminated from behind. ergo the name. this required changes in the 
fabrication process - i won't   cover all of the changes in this 
brief overview. but here it goes. after sony makes the photodiodes 
on the wafer as normal,   they then bond on another wafer on top of it. then they flip it over and thin 
the wafer with the diodes down to   a uniform 5-10 micrometers thick 
to expose the diodes' backsides. getting that exact width uniformly across the 
whole wafer is very hard. sony basically grinds   it down with a grinder sourced from the 
japanese company disco. i like their name. any failures would ruin that wafer, which would   be especially painful since you already 
did the work of making the photodiodes. sony first released these products into 
commercial production with the iphone 4 in   2010. and they have advanced the technology 
further with more wafer-stacked 3d sensors. ## process sony has shown that the concept 
is possible. it can be done.   but what imec is proposing has 
some tricky new things to it. rather than 5-10 micrometers that sony did, 
we need to thin out the silicon wafers to 500   nanometers or just 0.5 micrometers. this is way 
far beyond what sony was doing fifteen years ago. you start with an ordinary wafer. and then grow two layers on top 
of it - a 50 nanometer thick layer   of silicon-germanium and then a second, 
thicker capping layer of very pure silicon. this capping layer is where we then cut our buried   power rails using the aforementioned 
shallow trench isolation technique. we   also use ald here to metallize the 
trench with tungsten or ruthenium. then we bond on another wafer on 
top of all this. this is called   the "carrier wafer", because we are going to ... flip it like sony did. the carrier wafer at 
first bonded on top is now on the bottom. we then grind down the original wafer's backside 
in a series of steps. first we use a grinder,   then a polisher, and then finally semiconductor 
etching tools like dry etch and wet etch. last is the wet etch, which involves applying acid   to eat away the aforementioned silicon-germanium 
layer and exposing the very flat capping layer. we are now ready to dig the through silicon vias   into these silicon layers to connect the 
buried power rails to the power network. imec used lithography to tell the etch 
machines where to dig the vias. then we   use etch to dig the tsvs where the 
lithography patterning tells us. imec used a type of etch called 
the bosch process. it works kind   of like atomic layer deposition in that it 
works via an alternating two-step cycle: first, we fire a plasma of acid 
gas to etch the silicon. then,   we fire a teflon chemical to deactivate 
the etch and keep it under tight control. this cycle eventually drills the tsv,   which is about 90 nanometers or so wide. we 
then clean the tsv, line it with a barrier,   and then metallize it with copper to 
wire it to the greater power network. imec has laid out the overall process 
and it seems like all the fabs will   largely follow it. what the fabs need to 
decide is on the finer details - tungsten   or ruthenium? the width of the 
tsvs. what liners to use. so on. ## conclusion
intel calls their version of bspn power via. and it first entered their process flow with 
the 20a process node - which precedes'a. tsmc will have bspn too but are more 
conservative in its introduction.   their n2 node in 2025 introduces 
the new gaafet transistor. they   reserve bspn for a follow up 
node scheduled for 2026 - n2p. so assuming all schedules hit as they should,   intel will have a very good node'a with 
both these innovations in the market. just as important as all this impressive 
manufacturing is also the design aspect.   existing power delivery networks for ics 
are incredibly complicated as they are now. bspn presents a tricky new proposition 
for designers like intel and tsmc's   customers. they have to port over all 
their networks and redesign them to   work just as well as they have before. 
this is a massive eda design challenge. but the benefits are immense. just as 
these backside techniques opened the   door to incredible 3d stacking benefits for 
image sensors, it poises to do the same for   digital logic. this is not a one time 
boost. this is a trend that will last.