we seem to be living in an era of tailored databases there's been a real explosion of them in recent years you get databases that are really strong at certain kinds of workloads databases designed with a deliberate sweet spot and i don't think that means the end of the general purpose database like postgres or mysql or anything like that i don't think they're going anywhere i think what it means is that as developers we have to expand our toolbox we need a larger mental list of the kinds of databases that are out there so that we know when it's time to use the specific tool for the specific job so in that spirit today we're looking at xtdb which is a bi-temporal database what to buy temporal database you ask exactly i've brought in an old colleague of mine james henson to discuss it we last worked together years ago and since then he's become the lead developer for xt and i wanted to ask him about the two sides of that job what to buy temporal database why should we care and what's it like being the lead developer on a new database i don't think that's quite like other programming jobs no we're all users of databases but very few of us will be on a project building a database so this week we get to live vicariously through james as well as expanding our list of options for future projects to buy temporal database it's kind of something you might have already created an ad hoc version of in the past but today we're going to see something that's formalized it properly so let's look at bi-temporality i'm your host chris jenkins this is developer voices and today's voice is james henderson [music] joining me today is james henderson james how's things going hey crystal and we're all thanks and yourself very well very well glad to see you we used to work together many years ago a long time ago wasn't it yeah it must have been 2013 i want to say that's that's forever in internet yeah yeah it really is it really is these days you are a lead developer for a database called xtdb all right and i wanted to talk to you about two things which we have to get deep into one is why does the world need a new database and two what's it like being a developer on a database so it's probably best for context we start with what you're building rather than what it's like so why does the world need a new database james so i'll talk a little bit about what what xcdb is so it's a database we've been developing for about four or five years or so now it's written in a written enclosure and it's its main cell is that it's a bi-temporal database so it's got it's got time with its a time when it's very hard and what what we essentially part of the reason for us building this was that we we see so many and so many of our client use cases we see people trying to do the roll their own by temporality whether it's like sort of take tables with soft deletes or tables with loads and loads of different time columns you're gonna have to step back and define by temporality i probably amanda yes yeah so by temporaryity quite literally means two times and two time dimensions you they're called valid time and transaction time and the names for those vary hugely across the so like for valid time you might hear application time business time domain as usual in the industry we can't agree on the names of these things transaction times also called system time but so transaction time or system time is when the system first under when the system first up sees the fact right valid time is when that fact is actually applicable to your to your business to your to your application so it enables things like retroactive and what we call retroactive and proactive updates so retroactive being i i understand that someone's changed their name i've been informed after the fact that they in fact change their name when they got married last weekend right yeah i was like i'm overdue doing my business expenses at some point i will say i bought a laptop last month so we'll have the timestamp of now when i actually assert it and the timestamp is last month when it matters exactly yes exactly that and then it allows you to ask questions either with or without corrections so if i'm if i'm looking i mean one common use case we see for example this thing things like risk and risk calculations and the like okay and if as in a lot of regulations i had to justify why i made a certain business decision or a certain trade or whatever it may be yeah one of the things i might say is that in hindsight that was a terrible decision i mean an absolutely terrible trade like why why on earth did you do it it took you completely out of your risk profile yeah but one of the things you can say is actually based on the information i had at the time it was a reasonable decision because you go back without the corrections that you've seen later right yeah so what what did we know tuesday at 4 pm without subsequent corrections yeah so only consider it to you filtering for everything that had happened a month ago whether or not it was in the future or the past of that in logical business time right yes okay but then in other situations i might want i might want the exact price as on tuesday or 4 pm as as we know it now right because we might have been back and corrected it and said right okay so you might microsoft share prices whatever it is actually we made a mistake there or the system was delayed in getting that information to us we now know that the price at that time was this and so now i want the absolutely up to date information with the corrections as we best know it right now i in my dim and distant past i've worked on accounting systems and we did that you know we had like i forget what again i forget what the names are but we had like an insertion time and business transaction time right yeah i have evernote we're not moving the field forward no right but but we did it in an oracle database and it was just two columns so i think you have to justify where we need more than two columns in our familiar database right yeah so the the update especially when you go to buy temporality the update procedures for that get quite hairy and this and especially once you add on those those retroactive and proactive updates so when you've got four columns you as the developer then have to consider right okay so if i am going to make a retroactive update as of 4 pm last tuesday i need to consider how many rows i need to add because i'm going to have to add row to that table because you're gonna have to add adverse then essentially the new versions of the document and you're then going to have to consider the what changes you make to the existing ones so i'm going to need to cap off the current row but also if i'm doing it by temporary i'm going to need to cap off cap off the system time i'm then going to need to keep the record of what it was back then without corrections and the record of what it was back then with corrections and that ends up being to maintain full by temporality that ends up being quite a headache the other thing that we bring to this is performance of course because if you are if you are just doing comes in your own in your own way the database is won't necessarily be able to optimize the queries and so for example we put quite a lot of emphasis in making sure that as of now queries which typically in a lot of systems represent the vast majority of everyday queries we make sure as of now queries are correspondingly faster than maybe historical queries might be so we understand a bit more about the the distribution of the data if you like okay yeah that makes sense it doesn't factor into like querying as well do you have anything like special operators for querying at particular points in time we do yes so we're we're basing at least in sort of xt version two the early access that we released a few weeks ago we're basing that very much on the sql 2011 spec which covers an awful lot of ground here and for byte time priority really introduces yeah introduces a number of new syntactic structures which aren't very well implemented across the industry as yet it's taking the second class was it sql 2011 what are we now 2023 it's taken us a while to catch up yeah yeah but the sequel spec it's not supposed to move fast a bit faster than that but yes go quiet quiet so it does allow you for example to when you're when you're doing select from a table it allows you to say from that table for system time as of and then you can give it a system we've then extended that for valid time as well but you can also do sort of system time between or valentine between so give give me the the history of this entity throughout 2022. so between 1st of january and 31st of december including corrections so corrections depends on what you specify for system time right so if i if i want corrections i'll leave the system time as of now because i want to be the most up-to-date and what the system knows now if i don't want corrections i'll go back in system time i can see accountants and auditors loving this absolutely but the thing is they've known how to do this for centuries perhaps yeah yeah they they've known i mean and they've certainly known about immutability yeah this whole thing around we don't we don't ever go back and correct the past we write a new we write a new version we write a correcting transaction they've known about that for say way longer than i've been around me too i'm not that old yeah so what we're doing is sort of drawing on drawing on that kind of knowledge which let's say that's it's about it's about time we did and especially now that the the constraints have changed as well so certainly in the early days of relational sql engines we we had very different reason industry had very different concerns and i wasn't around for that either by the way we certainly have very very different constraints in terms of how making best use of our storage because storage was very expensive we had we had to make updates in place in order for it to in order for it to not become crazy expensive whereas now with storage especially remote storage being a lot cheaper and a lot faster to access the sorts of solutions we can consider for for these kind of databases where where all of a sudden it does become an option to store literally everything to store your entire history yeah yeah there was a period in our history where you would update and throw away old financial data because you had 64 meg of ram yeah right or sometimes like 64 meg of disk if we're really going back so you had to yeah we can make different design decisions now absolutely and we can i mean obviously we still prioritize keeping as much data as local as we can so we'll nxt we're looking to bring us bring as much data sort of into into into some cpu cash into memory into disk into something like redis and then only at the last stage do we go all the way out to the remote storage yeah but again i think that's i think it's a very typical pattern anyway we're not we're not inventing anything new there okay so you're leading into you're leading into some gory details which i want to get into what's happening under the hood give me some technical facts about how you make this work so xt both both in the in the version one that's out in and production at the moment and the v2 with we're on the early access it's based on the what's called the inside out architecture which i first heard from taught by martin clapman apologies if that may will not he may want to be the first person to have thought of it he's i think i don't know either but he's widely recognized as giving the founding conference talk that made everybody start talking about it right yes yeah and the inside out architecture says that rather having that rather than having the log sort of ingrained within the database and being an internal implementation detail we make it front and center we we very much have the the loggers the history of everything that's happened and then base our data store on that log as a like a like a projection over so your data becomes essentially a projection over that log yeah and that's and that's so the event sourcing folks will find that very familiar no doubt yeah yeah so xxt hasn't really got into the event sourcing space yet yeah but they'll they'll seem to be very very familiar with that yes no exactly yeah definitely related to kafka and in fact kafka is our model for what we call the the transaction block within within xt when you say the model as in you've copied it or it's like you use it so the model implementation the the better implementation of the transaction blocks right okay so x xt is more so in v1 is what we call unbundled so you can bring certain components to the table we ask you to provide a transaction log and an object store so a transaction log has to be totally ordered and all of the all the clients have to but other consumers have to agree on the the order of the transactions that's what gives us the consistency over the over the cluster right but then we also have a component called the object store which as it sounds is i mean if you think s3 or blob storage or whatever google cloud call it it's called cloud storage isn't it whichever sort of whichever cloud you subscribe to yeah other yeah other services may be available so it's that kind of it's that kind of principle but for a project storage we're looking at being able to store big blocks of data and and grab hold of them as necessary okay so you also like storing images fill movies that kind of thing happening so the the blobs is foreign right and then and then it's up to us to then manager which which pages we bring locally and which pages get and which pages have to say so yeah the idea of the idea behind that is that we as xt don't then manage that so we we delegate a lot of the hard a lot of the hard work to that yeah i particularly particularly kafka and the and the total ordering and consistency between between clients out of curiosity what are you totally ordering by which of your fields so we're totally ordering a situate transaction time so it's a single writer okay similar to similar to the atomic idea okay so single writer single partition on kafka and you're just building this big append only lock that's exactly right okay yeah yeah yeah that's what that's so yeah no no consideration about politicians at least as at least as yet and so you say you're offloading a lot of the hard work but it sounds every database contains part of an operating system right and you're managing a page cache to disk so it's getting you close into like what operating systems do to manage levels of caching that must be a laugh it's quite naive as it stands okay we've we've got a lot of work to do in that area before before before we can read it so push me to that big time okay so is this largely considered to be like an analytics database rather than a transactional database we're calling it htap so hybrid transaction analytical okay so the the idea being there that way too much time in our industry is spent in etl land and if we can get if we can get something that works reasonably enough for both we'll be in a we'll be in a good shape and we can get we can get people building applications on top of this without without having to go through a lot of that rigmarole yeah so certainly there's there's a few of the technology choices we've made a slightly sort of oriented towards the overlap side and we're having to make the oltp side catch up a little bit at the moment indeed what i was working on before i came on this before i came on this podcast how do we make that happen that gets into it what's it like being you know because lots of us build like web apps right a lot of programmers are out there building some kind of application that faces eventually some kind of non-technical user and then excuse me you're working on like really core infrastructure that will eventually be used by developers to eventually do something for people that aren't technical at all so what's it like it's quite different and certainly for me at least it took it it took a while to get used to i think so for me so i mean the vast majority of my experience has been in exactly that it has been sort of web apps that's been in like talking to end users and you i mean you've millions and chances are most people just be familiar with the kinds of processes that go on there the kinds of cadences you get into when you're when you're developing in terms of sort of milestones and releasing functionality and yeah and that kind of thing there's a couple of things that are quite different about databases and obviously it's for us it's very much more about r d so i think one of the big differences we see there is that we have very few estimates in our planning in our planning processes largely because of the r d r d nature of the work and especially in this last' months or so when we've been very much in a very much in a research phase we're also focused a lot more on backwards compatibility like i've in in the sort of my early days of closure i wrote a few libraries for the for the closure ecosystem and there you in library world you have to worry a lot more about backwards compatibility than you're doing with that world and in database world it's worse yeah i can believe you've got to worry about not changing apis and they've already got this log of immutable data yes yeah no exactly yeah and so there have been quite a few issues in in xt certainly in the early days where we we really had to consider right what if people got serialized on disk yeah and particularly when you're in the immutable world and when we're saying that this this transaction log is complete is completely immutable we're never going to go back and change that log like whatever whatever structures are on there are on there and they're and short of a migrate your entire log which we've been tempted by once or twice because we understand what impact that would have on our users yeah you've got you've got a live running system what you really don't want is your database like your database folks saying right so we're going to need to do now is to stop everything yeah my like pipe your log into something else a public transaction log into something else and then and then go again which ideally i mean if things are working the way they're supposed to it's a really large important log of like possibly financial data yes yeah yeah exactly that exactly so how do you deal with that how do you how i mean it sounds like your hands are very much tied how do you make progress generally by being quite conservative about what we put on there in the first place right so there have been a few there have been a few features certainly that we've that we've considered and who've had to step away from because of because of not wanting to change that transaction law but also partly because we when we read that transaction off we then we then index it in a form that's more useful to us obviously you don't want to be playing through a transaction log every time you need to make a can you find me the order with this id queried right so we we put all of the we pull all the transaction data off there and an index it and in version one it's all local inversion so that's a bit more it's a bit more shared between the different notes but that gives us an opportunity then to say right if we if we really do need to make bigger changes that's our chance to in in a in a reasonably backwards compatible way so right what we're going to do we're going to change our indexing structure and so while your system is running live on like version m minus one your system can carry on going and we'll index it into that into that format but you can bring up another xt node with a new version once that's caught up with the with the index instruction in the new format you can then sort them over into stuff at the green green blue diploid type way right yeah well but that's but that's a great thing we get out of this inside out architecture and we couldn't do that if the log wasn't the if the log wasn't the absolute golden store yeah yeah and it's also nice that you've got like within the mutable log most of the data's just sitting there guaranteed unchanging and there's just a new bit at the front to worry about yes so i imagine if it's anything like kafka world you can do those in re-indexing upgrades and do nearly all the work as slowly as you like yeah yeah that's exactly that easier yeah okay this once makes me want to circle around a bit to you say indexes makes me think what's it like as a developer using this database am i is it like a relational database and i'm creating my own custom indexes and stuff like that so xt is a very much a scheme on this database so we don't we don't require any upfront schema from user and so even though it supports your supports now both data log and sql we we don't have to do any of the sort of the usual sql ddl of create table or anything like that okay you're you're not restricted on what columns you put into the table essentially you insert a document you insert a map of data the only thing we do ask for is an id like give us an entity id and we'll work with that that just gives us that that gives us the ability to keep track of an entity's changes over time because the id remains the id remains constant yeah so it's quite free and flexible in that in that regard and then we then we then do the best job we can with our with our indexes to get you the the data you need so we keep a number of different trees a number of a number of different trees i say this at the moment this it's in rocks okay rocks tv yeah yeah and that that's great storing sort of ordered trees and that gets us that gets us pretty decent performance okay we're looking we're looking at different structures for v2 v2 is a a column the database based on apacheary way of thinking about how we're storing the data in and also again coming coming back to what we're talking about earlier the constraints are a little bit different all of a sudden the the cost of scanning a lot of data from disk com comes down relative to relative to sort of random accesses and so our query engine now is prioritizing those over a lot of random accesses in some cases so for version two you've had to rewrite the query engine on top of everything else yes yeah that in itself is a lot of work i mean i'm just thinking of oracle that's like it's it's almost like a historical empire of optimizer code right it's certainly a lot of optimizer code like if you we were looking around once at the various job boards when we were looking to hire for for xt and you look at the job for database engineers and they are nearly all you'll be working on query optimization [laughter] by far and away the the biggest the biggest thing people are looking for yeah i'm surprised i mean i know it's big but i wouldn't have thought it's like significantly the number one thing it certainly seemed to be in our very limited research okay yeah definitely okay so i'm i'm trying to think this through now so inserting documents right is have you got this is it like if let me put it this way i create a user by inserting a new user document and it's got like their name and their age and their social security number and their address now i come back and i want to change their address can i just insert the delta or do i have to reinsert the whole record so you can you can now so we we do have sqls update dml and and what we're essentially doing then on your behalf is grabbing the document and creating a new a new version of that document okay so under the hood you're you're writing a whole new you're like git right where it stores the the complete version of the file every time yes yeah okay that must be mixed because you've got a single writer so you then got the kind of operating system problem of managing access to that single writer so the the operating system we we get that for free with the with the sql partition from kafka and so we don't have to do an awful lot of coordination we don't have to do any coordination between any any threads or anything like that okay concurrent processes and for for a lot of use cases that's that's absolutely fine okay let's talk about something that can be can be very quick these days yeah and very large and still be useful all right yeah yes yeah let's take a look at this from the other side then you've there you are optimizing different queries worrying about not changing data how do you get meaningful data sets to test against for a new database so there are a few industry benchmarks out there that we that we use the the main two that we that we tend to look at when we're doing when we're looking at the impact of new changes is tpch which is quite common tpch so tpc tpc is the performance consortium they've got benchmarks like a through whatever i've got no idea how far they've got these days but h is a is an olap benchmark okay and it's it's based around a fairly a fairly typical use case of customers orders products line items and suppliers and pretty much any if you if you're running a if you're running a business of that kind of nature it's it's all it's all of the analytical and bi-like queries that you that you can imagine oh okay and you can run that at different scale factors so for example we run it at quite a small scale factor to have a a rough idea of the of the of the impact of a change but then you can scale it up and run it on a larger on a larger scale factor for example when you're doing a new release or you're really looking at a change that's going to heavily affect performance at largest gains okay that's interesting there's also similar ltp benchmarks as well right yeah someone who builds demos having access to a data set like that i should store that away mentally for future you mentioned the word of the industry scale what how what's the scaling story you're a single writer so you're limited on rights which is scaling for reads yeah so let's go england reads is that you can you can scale reads horizontally so if you need if you do need more read scaling then you add a new xdp xt node it brings it brings itself up and then you start querying against the shared object store in v2 so it will pull down what it needs from the from the shared object store and then and then get going okay now because our transactions are entirely deterministic we know that the the states on each of the notes is going to be consistent and that's that's a big simplification for us like we don't we don't need any coordination or anything like that between the notes because we we trust that they're going to end up at the the same state at the end oh right they may not necessarily be in sync that perfectly they may not be in sync so what what you have to do in that case is that each again again i'm talking talking the v2 here each client keeps track of the last transaction that it wrote and at the very least it will give you that trend and the the world as of that transaction so that that enables that enables you to read your rights yeah if it's got further than that that's great yeah yeah you can you can at least put a document into the database and then the next the next query you you make will read out so you have been thinking like like my university professor is going to be very unhappy with me but the name for those like isolation levels in database design yes yeah okay that must be fun so in that case i mean the the single writer again gets us a long way here so in in terms of the because the the reason the rights are separated on the on the right side of things you obviously submit the transaction to kafka it comes back around through the classical pipeline but then because you've got this the single right at the head of the queue we actually work at a strict serializable level which is the the the the top level that you can get on the on the brighter side yeah like yeah every every transaction sees every is guaranteed to see every transaction before it and and that choice is naturally serializable because it's a single like a single writer yeah things get a lot easier when you don't have concurrent rights yeah okay but you think about the amount of locking code that we also don't have to do i don't have to consider either yeah yeah yeah road locking and that isn't the thing we don't we don't we don't at least yet have interactive transactions so it's not it's not like you can you can then read and then write based on that based on that read so you don't you don't get sort of you don't get people accidentally locking a table or anything like that this is the thing i thought like off the back of martin clepman again if you start with this transaction log then you're basically building estate machines on top of that transaction log and and like sql database like postgres or oracle is a really really clever fancy state machine to allow these things but do we need as sophisticated a state machine is that for all transaction log processing tasks a very good question i mean i think we're obviously a little bit biased [laughter] quite biased on that one so do you ever get like different people using the same transaction log that you're using for your bi-temporal view of it people reuse that same underlying transaction log for different purposes ever i'm i'm not aware of people using our transaction log i mean the the the the format on there while it while it's an open format is probably a little bit gnarly for people to be okay honest i think with i don't know that's a line i'll tell you what because as part of the world we we provide it so it's it's a half-life and we we provide a leucine module for for xt then that reads off the same that reads off the same transaction log and there are there are internal apis if people do want to go diving down that don't do give them that access to the the events coming through the events coming through the system okay what's the leucine api get used for i mean what do people say it's just like free text search in your transactions yes yeah no exactly exactly that so yeah if you're yeah i mean i think all the usual use cases for for for leucine really we're we do have the opposite in fact where we do have the ability to hook xt into someone else's transaction log and we've and we've done this we've done we did this for a client who were using quarter and the quarter blockchain oh so they they had a i would say the the b word came up yeah i can see why they want i mean they've got a mutable log and they probably need analytics on it yeah i can see this so they already have the data in that format what what they want is a way to query that yeah using using xd's by temporality and so we we wrote the middle module that put the two together and so they they kept they call they kept they called a source of truth but they were able to then run run xt queries over oh i can see that being really popular in the blockchain world without without declaring for or against blockchain technologies i can see decent analysis over those blockchains as a service being quite popular right yes yeah well i guess it's the same principle isn't it it's a it's a look at the heart of the system yeah yeah so yeah there's there's quite i can see quite a lot of overlap there that takes us to the question the 64 000 question for a small company building a database who's using it and for what so we're seeing quite a lot of users across across various different industries obviously obviously we see quite a lot of it in financial industries as we were talking about earlier i think the the kind of the natural tendencies in that industry both the no no whatever happened at a certain time and it's especially with the kinds of regulations that are coming through that are really forcing people to to justify what they what they knew and when yeah and that that kind of thing so we see an awful lot of we see an awful lot of usage in those kind of areas but to be honest it varies it varies quite widely like any anything where like time is of the time is of the essence if you like so we see for example in pri in pricing systems in product pricing systems so in one case in particular that jockster's worked on with quite closely we've the the company like to schedule price changes and they can do that with a bisexual system because you can you can say i want i want this product to have this price as of next tuesday yes yeah so any kind of scheduling or cms or those kind of systems use the other the other side of my temporality if you like yeah writing into the future which is a largely unexplored field right yes yeah yeah yeah yeah it does it is that reliable is it is the developer experience such that you're not going to accidentally write software that pulls from the future in in what ways are you i'm just thinking like you've got all this flexibility to make that query how easy is it to ensure that you do the right thing in most cases so our defaults and make sure that you that the default is always as of now so you you only ever see fm if you can if you can imagine a nice big sort of 2d graph of sort of history going through in both dimensions we we limit you to the y equals x the diagonal you know this is an audio podcast right well yeah okay so you can't see me waving my hands and sort of hand wave the explanation of any of the people listening to this on like spotify and apple podcast can't even see your resplendent beard james oh they're missing out they're missing out yes in which case without without the visual matter for them in in a lot of cases because because it's what sort of traditional relational sql databases do we we make sure that the default is as of now both in what we currently know and of and what we currently know about the current time right so transaction time and valid time you have to explicitly ask for a cross-time queries that that's what i was getting at so the default behavior matches your expectations of how things generally work principle of least surprise they call it absolutely yeah yeah absolutely okay so i think i have two more big questions to ask you and i think they're slightly controversial all right but i have to ask them why why build a database in closure so i think for us i mean i mean juxt is a closure company and we're very we're very much sold on the on the ideas behind closure especially as the so building an immutable schemeless dynamic data database seem to go quite hand in hand with an immutable schema-less dynamic language yeah so for us it was a good fit in that sense we've also obviously got quite a lot of experience with the with the closure industry as well so there was quite good there was quite good pairing there we do of course find that the the more sort of the the more performant areas of the code we do and especially the more sort of a stateful mutable ugly code that you tend to want to hide away yeah that is being written more and more in in java now okay just because there were certain areas where we found that we were writing closures if it were java for the for the mutable performance but this is as as our lord and savior rich says rich hickey creative closure yes he says it's yeah i think he's done a couple of talks now where he said that as long as the ex as long as the exterior in the system looks immutable and behaves immutably what you do below the surface if if there's a if the tree is being mutably hacked down in the forest as long as the exterior is behaving beautifully it gives you those guarantees that that's that's okay okay so you're you're happily enclosure in the logical world but under the covers i don't want to say slumming it with java you're i can't think of the right way to say that isn't going to offend someone so i'll just drop it hey it's it's it's getting better these days it really is yeah they've learned a lot from the jvm languages i think they got a bit i don't want to say complacent but what they just they they lost momentum yes in the java world and then suddenly maybe closure definitely scala since then made them get their act together and that's very hard to go from a language that's lost momentum to getting it back again but hacked it to them they really have yeah definitely okay the other thing i'd say about pleasure is especially for a research project we find we've been able to move a lot quicker so for the if you think about the sort of make it work make it pretty make it fast yeah certainly for the make it work and we've found that we've been able to do that phase very much quicker from closure in its interactive development and the fast turnaround times so the the experimentation side of things like what happens if i design a system in this way yeah yeah closure could be a great language when you have no idea what you're doing which is oh yeah right right and it happens just however maybe more so i can believe yeah yeah because your explore especially if you're writing a new database where you're having to explore entirely new design decisions yes yeah definitely um yeah and and i mean for us at least in the early days when we were figuring out how to best use apacheary having something that we could that we could really sort of poke it poke it with and see what it did you have to be quite it's quite strict with you about your memory management in the patio it's very sort of manual memory management type this is an aside that maybe we'll save for the dvd bonus features but tell me a bit more about apache era so it's a column the data format so it's been pioneered by pioneer by apache and being used in a number of different companies in industry but one of one of its main benefits is that the on disk beyond disk form has exactly the same as the in-memory format so there's no serialization of deserialization if you want to if you want to for example read an arrow file a memory map is a great way to do that okay because there's no trans there's no say for example you were writing it files in in json or whatever have you you'd need to translate that into into whatever objects that you've got whatever objects that you're working with or maps if you're in the closure world yeah whereas with with arrow there's none of that translation happening so you you're you're literally reading bytes and what are you getting back do you spend a lot of your development time great deserializing byte strings or is it just just pop off disk looking like a closure map because that's what you wrote certainly for the primitives so it'll it'll pop off this looking like doubles and longs and okay that kind of thing maps and other composite structures and maps maps and lists predominantly in arrow are stored in a column in a way so for example when you've got a map of like a and b keys the the arrow format will store all the a's in a row sorry all the a's in the column and then all the b's in a column and so you you end up particularly if those are fixed width so particularly if they are sort of norms or doubles or the like you end up being able to navigate like straight to the straight to the value you're looking for okay and that i'm guessing that works out super fast if you want specific fields or you want to aggregate specific fields but the trade-off is you lose getting the whole object with all of its fields and we've had to put a little bit of thought into how we do things like select star yeah okay so last controversial choice i know xtd xtdb supports two different query languages it does yes tell me about that so from the from the early days of xt we very much went with with data log again heavily inspired by the atomic i don't think that's any that's any secret and i think for for us in the for us in the closure industry i think there's always been a little bit of that love for data log it's very much of a data oriented query language it feels a lot more sort of declarative in the way that you're you're asking for what you want rather than necessarily how you want to for those that don't know because i've worked a little bit with data log but for those that don't know describe it so data log is essentially is a language that works it's it's a logical language it's a it's a substance it's a subset of prologue although i guess if you bind different variables to values coming out of your documents so for example if i'm joining customers on toward us i'll make two decorations i'll say that a customer has a customer id of my customer id and then i'll say that my order also has a customer id of that order id and what the what the data log engine is then doing is saying right okay i can see customer id twice here i'm going to make sure i'm going to unify these two and that essentially is an implicit join right and so you can you can treat them you can you can very much sort of within your query you can very much see right okay so here are the things that i need here are the things that i'm joining on and i get a bit more of a declaration of sort of when i am when i traverse my graph if you like if because you can you can kind of think of customers orders line items as a bit of a as a bit of a graph database and at times we have called xt of graph database oh have you for that for that very reason yeah you can think about it in in a graph like way a graph of documents what is that going with that so yeah when you when you're writing the query you are writing it's almost like a graph traversal you're you're saying right stop like find me this customer node and then from this customer node navigate out to the order node and navigate from the order to its individual line items and the query the query actually looks like that to work with as well so it's data rather than a string so anyone who's i think anyone ever works with a sql has probably had to generate sql at some point in the past and has sort of got in a bit of a tangler but how many ends do i need in my where clause when i'm mad you know what i mean you can't really the thing i my favorite feature of data log is you can start because it's a data structure rather than a string you can start to compose things right you can say here's two extra clauses that i want to mix in with this query yeah without having to like string edit where the and is in my where clause and all of the and all of the sql injection that follows no doubt oh yeah yeah but what we've done recently so we we obviously love closure and i think the i'm sorry we obviously love datalog and i think the closure industry obviously those those data log as well but but we've recognized that that love's not universal we recognize that it's a hell of a niche and and that you're not you're not gonna get even if even if you the development team are fully bought into this you're not you're not going to convince people outside of that that they they should cast aside all of their sql experience and tooling and everything else that they've they've gotten used to about the about the sql world so that's that's one of the reasons why we really wanted to make sql a first class citizen as well and so what we've ended up building is a database where you can query the same data with both datalog and sql and roughly equivalent datalog and sql as well so you can you can look at a data or query in a sql query side by side assess their their equivalent queries and they'll return you the same data can retain the same results so i think we think that's we think that's fantastically powerful i think on feature parity is one more powerful than the other and they're not far off you know so we we've had to put a fair bit more work into sequel data log particularly in terms of its scoping rules it's quite a lot of a simpler language but what we what we do is we compile both of them down to an intermediate representation we compile both of them down to work all of the what we call a logical plan right which is essentially relational algebra that old thing your university professor must be very proud of you oh yes yeah might not have been proud of how much i remembered when i first started on that but at least the yeah that the the remembering that they existed was certainly a very good start yeah basically isn't that all we're ever doing so remembering that something exists enough to go google it yeah sometimes the hardest problem is discovering the bits that you don't know that's why this podcast exists but no relational trip was fantastic i love it and i mean it's it's obviously the underpinnings of underpinnings of sql so sql is very much very much based on that but the composibility of relational algebra that you really can compose those queries together and we're looking to get back to that with with datalog so are you hoping secretly that those people that start off with sql will gradually be tempted to your way of doing things you can cut that out of the podcast right don't tell anyone yeah yeah we'll leave that in the cutting room floor honestly if you're hearing this at home i have betrayed james and he's never speaking [laughter] no to be honest i mean we're the it very much was the tooling of the secret sql ecosystem because there's just so much of it with the best will in the world we're not gonna we're not gonna compete with with that in in the closure industry yeah i'm inclined to think that if you're trying to do a new way of doing databases that is one front on which to change the world and that's plenty yes yeah yeah you have to be a little bit sort of conservative about that but that particular budget only i mean yeah yeah absolutely yeah so you said what's the phrase but sql reaches data log rocks but sql reaches love it it's a to steal a phrase there yeah i can predict that at some point in the future your team will have that on a t-shirt at a conference yeah so you said you've just put version two in like was it preview release or earlier it's very much early access at the moment so we're we're in we're in listening mode and so john our ceo announced announced the v2 early access at closure conch which is a few weeks ago over in them over in north carolina and so the the idea behind this is to get people to have a bit of a play with it try it out let us know what you think let us know of any big sampling blocks you you come across over time and we'll be we'll be moving through the the traditional sort of alpha beta rc and and stable release okay so i i think it's vertical at pre-alpha right now if you're not if you're not comfortable with the bleeding edge it's probably not for you just yet but where do i go is it fun to play with if i want to just kick the tie as an experiment absolutely so we've you can go to xddb.com v2 and all the all the all the instructions about how to get started and write your write your biochemical queries will be on there okay i'm gonna give it a go it sounds like fun and maybe i'll finally get the accounting system that will make my taxes easy at the end of the year you've got to write on the tax rules yourself don't bundle those in i hear the government's going to simplify them any day now i won't worry yeah i'm sure james is ever a pleasure to talk to you good luck with the path to official final release of version two cheers chris cheers thank you james if you'd like to take xtdb for a test drive if you want to explore by temporality or data log or anything like that there's a link to the project in the show notes and if you do please drop them a line if you've got any feedback i know james would appreciate it i appreciate feedback too so if you've enjoyed this episode please take a moment to like it or rate it or share it or review it or subscribe to it or all those different feedbacky things it always helps and it's always interesting to hear or just drop me a line my contact details are in the show notes too for twitter and linkedin and the usual and while i'm thinking about it we're going to continue to explore different kinds of database on this podcast so if you've got any suggestions or requests let me know i know i'm planning to do vector databases soon that one interests me so that will be coming up but i think that's all for this week so i've been your host chris jenkins this has been developer voices with james henderson thanks for listening [music]