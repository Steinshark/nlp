traditionally where people have used compute in their their research they know what to do because they've they're experts in the field and a lot of our classical users using hpc know exactly what they want they know how to process their data they know they need to use scientific linux they can write software to process their data, and they can get their own research questions answered by themselves and you know part of information services job is simply just to provide the hardware in the grunt that they need to do that but increasingly we're finding that other people are wanting to use compute in their research but they're not really knowing how they haven't got those skills so part of my role is to sort of link up their research problem with the skills needs the software and the analysis tools that they need to you know answer their questions i was a medicinal chemist so that's a type of synthetic chemist who designs and makes new drugs, so all the different processes through you know designing a small molecule for a particular drug target doing some computational docking using the hpc to find out of the million possibilities which ten should i make and then test to see which might be the cure for cancer? asthma alzheimer's disease or just the new painkiller i started off my training doing a phd in chemistry just the simple. how do you make this compound what reagents to use? how do you make it but as i sort of progressed in my career moved into the drug discovery space? where the chemistry's just applied as the technique? that's just one of the tools and the the interesting and the intelligent thing is this is the drug target. what should i make to interfere with that target to have that biological effect? and my interest really looked it moved into sort of the using computers to answer those kind of questions so what do we make why do we make it? where should we direct our efforts? traditionally in all style research it would just be somebody sat in an office thinking about a particular problem, and then proposing an answer but then you know that as a scope and possibilities. they increase you know we've got more data available we've got more possibilities available it really expands beyond what one person can hold in their head so as research is interdisciplinary and we use chemists biologists engineers mathematicians so that you know in a traditional submit' like wet organic chemistry we now need to use computers to help analyze possibilities data and questions, so yeah kind of expanding that research space into using computers is really sort of becoming increasingly more important how i got involved in hp pcs and computing was doing something called docking so if we have a small molecule of a drug molecule say like aspirin or salbutamol though, we think might be a good molecule in a particular drug target what we can do is use the computer and ask it a question saying does this small drug molecule fit into this receptor protein how good a fit is it? where does it fit in what shape is it when it fits so what we can do is use specialist software packages to ask that question of hundreds of thousands or even millions of small molecules so you will prepare a question how well do these five million drug molecules fit into this receptor with the question with the software? i'll submit that question to the hpc queue, and the that's where the hpc takes over and say okay, this guy's got this question that's broken down into actually 25 million sub questions and it's the hpc scheduler that then splits that job up into separate nords and distributing out to different processor cores so i could have 2,000 processor cores one working on each drug molecule into that separate receptor when it's finished with that one it will tell this and a master controller that it's done and it will be allocated the next one to do so it's kind of like that the hpc is acting as my research assistant and answering all those millions and millions of questions for me while i'm in the lab making - or having a cup of coffee or lunch or chatting to the boss about what the next question is that particular question sounds like a very very complicated puzzle really isn't it and it sounds like a really complicated puzzle but that the techniques and the software tools for asking that question. how well does that molecule fit into there a very mature? it's very well known. it's very well understood a subject the problem is really the scale and i can't have enough computing power to dock every single possible molecule into every single drug target it's got to be an intelligent choice, but as computing power is ever increasing you know moore's law more processors more memory i can ask more of the computer get it to tell me more information so i can concentrate on the the chemistry specific knowledge i'm imagining researchers from across the university well certainly across the world wanting to to use computers to do these to answer these questions is there one science that uses it more than others or? no, i don't think there is really i mean. i think traditionally users have come from physics astronomy chemistry engineering in particular, but also increasingly biology genomics researchers the life sciences and we're now seeing people from the social sciences humanity and even arts coming in to start using computers in their research and are these high-performance computing systems or clusters are becoming the norm are people expecting these as part of their research now absolutely yes, you know again going back years what we need to provide to an academic would be an office a desk a green board and some chalk now people really expect and you know that high performance computing facilities are available to them and the university does that by providing hpc facilities like we're just looking at today but also renting them from cloud providers like amazon and azure microsoft and google and others as well thinking of those leaders of the field google and amazon cloud computing etc why would you do it yourself if all those options are available was a number of reasons we might want to do it ourselves first and foremost those companies are really good at it but most companies are really good at it because they charge for doing it so there is a higher cost associated with renting somebody else's kit to do it there are times when we need to do it on-site for security reasons so if we're working on some very sensitive research material, maybe something in the medical field with patient data we'll need to guarantee to the funders and who owns that data it's very safe and secure by keeping it on-site equally we might be doing something say with genomics research where the quantity of data is so vast in you know terabytes of data per hour that we need to analyze it and process it here on site and that the costs and the time associated with shipping that data somewhere else to process it and answer those research questions and then bring the data back again is is prohibitive equal at those times where we're using remote data. say satellite imagery or data from the human genome project well that data already exists off in the cloud so it makes far more sense there for our researchers to take the question to the data and analyze that data off-site so kind of by offering both we can hopefully give researchers the ability to choose this one's better for me or this one's better for me does it ever go horribly wrong in that you ask a question and you've made a mistake and wasted hours and hours or days of hpc - absolutely, it's happened to me more times than i care to mention one time in particular i was doing those docking small molecules as a drug mark targets only a couple of hundred thousand there was some sort of software error it started churning out an ever-increasing error file size of the error file went through 300 gigabytes blocks the entire system everybody else's jobs failed 20 or so people quite angry with me that i just killed all of their research but that kind of thing does happen in research and i guess that's the that's the computational equivalent of the professor having an explosion in the lab and spraying stuff all over the room which i've also done maybe i'm in the physics lab working with professor moriarty and i want to do some computing time how much it gonna cost me to use that kit that's a good question at the minute we don't actually charge our researchers directly for using an hpc facility so there's no per hour charge for using a computer core you do know this is going to be available for them to all watch and then they're all going to start clamoring after this absolutely you know and if more people need more resources and we can provide them we can we can look to work for them what we want our research to be ambitious and to try and push the boundaries, and we can't do that by restricting unnecessary access to kit if money doesn't really come into it in that way how do you decide who does what and are there fights? that erupt as to who needs to compute power more than who else there are vigorous discussions each time we get a new hpc system and you know you you you can see in there it's a large hpc there are so many processor cores and it can be used for such and such a period of time and there's always a debate about how much of it at any one time is somebody allowed to use and how long can any one person's job go on through so? very much like tetris how can i fit these variable width and size blocks in? is it better for me to use a thousand calls for 10 hours or 100 calls for many thousands of hours? and how do i fit that workload it always causes vigorous discussion few disagreements? there's always morning that the cues far too long, but that's just how it is where does the book stop in terms of the decision-making is it left at a software world as a human come in and say? every time we sort of review the process a group of humans will sit sit down and decide okay we think it's fair if everybody's allowed to run up to a thousand cause for up to four days time and we kind of sit down and make those decisions as a research community, and then the software vigorously enforces those limits for us so you are only allowed up to so many hundreds or thousands and your job is only allowed to run for a maximum of four days and if it goes over four days it stopped and the next person's given access to that resource in a way that could be limiting research, right? it's something that the researchers have got to trying to fit into their research plans such as in the same way that office space and laboratory space can be a limiting factor it's how do i divide my research question up onto the computer am i better? using two thousand calls for a short period of time or is it the kind of question that is only limited to a thousand cores and needs to run for a longer period of time do you need to know a fair bit about computing just to make those decisions, though? traditionally yes, i think there has been an expectation you have to know a lot about how computers work to make those decisions, but we are seeing now as the use of computers in research moves into other areas part of my role is to try and help people to understand how computers think as computers work in a very different way to a researcher they know good at answering the same question for a long period of time in parallel so sort of changing how you would do it in in person how you would add do some calculations to how a computer would do it lots of different things at the same time it is a is a step change for some people? the equipment itself is fairly generic. you know that these are standard blade enclosures the storage is standard storage we have about 240 terabytes in this it's all connected up by a minivan