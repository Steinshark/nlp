there's plenty of superb anecdotes about the history of the bite and and so on i suppose the earlier question which if we want to go right back to ground zero who invented the word bit a chap at guess where bell labs john chooky worked with shannon and others a bit i think it was here in a paper in 1947 actually uttered the word bit rather than binary digit for the first time so how big should your groupings of bits be to hold the numbers you were interested in and this is the key to it all early computers were word based later on the idea of the byte came to be not synonymous but associated with what's the amount of storage we'd have to set aside for holding a character but you know real programmers aren't bothered with waffly bits of text you want numbers you know the obsession with the early computer pioneers was very much we're gonna build these things because they're interested in in their own right electronic general-based computers but what we really want to do is be able to do serious applied mathematics engineering quantum chemistry computer science x-ray crystallography radio astronomy all good solid scientific things which need guaranteed decimal accuracy so remembering the rule of 3.322 which i don't have on my t-shirt this time sorry about that and if anybody wants to produce a t-shirt with that one i might even buy one the 3.322 is the multiplying factor for how much more binary circuitry you need in your arithmetic unit of your computer to be able to do things in binary of arithmetic rather than decimal arithmetic the logic bits of the computer it's a no-brainer it's yes no decisions so proper binary is fine there but why not use decimal integers well using decimal integers we can point to the previous video on this it's fine but you've got to stabilize 10 different voltages for 10 different levels of decimal digit you've only got two to stabilize for binary it's much simpler to build in binary but you have to accept you will need 3.33 whatever times the amount of binary components to do arithmetic then you would need decimal okay so that's accepted that we build in binary but for school use you were taught how to use logs to do your multiplies with four decimal digits of accuracy babbage and others wanted at least 10 digit in fact i've not got them here at home they're going to work i've actually got 10 digit log tables quite a big thick book of them so if 10 digits was deemed to be a pretty decent starting point multiplied by 3.322 and you could see it by the time you got to something like 33 36 bit for safety maybe groupings you're beginning to get the 10 decimal digits of accuracy that you want so you look at edsack a first generation computer which we've been talking about it's actually got an'-bit word of which 17 bits are useful but by the time if you only glue two of those together in some way you're up to 35-36 bits now you're talking that's getting us the accuracy we want however at greater expense of course contemporaneously as we now know with edge cycles the ed vac they went for 40 bits straight away so they didn't have to double up the locations more expensive to build of course but 40 bits fine that will get you 10 or 12 possibly decimal digits even when i came to do my work early work on quantum chemistry all along you were aware of not wanting to lose precision on your decimal digit calculations and take great care about rounding errors and stuff like this i once looked at some exam papers for the cambridge diploma because for many years from about the early 50s onwards cambridge did a postgraduate diploma in computing yeah based around the headstack you went there a lot of famous computer scientists converted themselves if you like by doing the chemist diploma an awful lot of those diploma papers were numerical analysis there was the odd question in there which was shock horror non-numerical covering things like algorithms for the traveling salesman problem but then the next question would be you know how could you invert a household and normal matrix of dimension 20 by 20 in the minimum number of operations without losing decimal precision in the fifth decimal point all this kind of stuff that was the background scientists mathematicians and engineers built computers and they built them to do hardcore numeric calculations so was that all there was to it no on the sidelines and treated quite honestly and and shame basically i say this in many ways with a bit of derision was commercial computing and the company as i'm sure you all know that was instrumental in leading the way with that was ibm ibm for years and years and years was the biggest computer company in the world it so it's probably probably still in the top 10 but i'm guessing that microsoft apple and google will be bigger than ibm in terms of revenues nowadays yeah ibm of course had got the best part of almost a cent no half a century a good half a century of lead in using punched cards as a means of holding data and even though the machinery that sorted them collated them and could even do elementary additions and subtractions on the codes that were on the cards ibm was the market leader by a mile others tried to get in on the act and get a little bit of action and they did but the industry leader was ibm so ibm understood about real data okay then so they were the right people to basically start saying you look at these computers now and how they handle characters it's pathetic and it is because you can go back to our edsack video about how to print high but it doesn't matter whether you're printing out high or hello world you look at the way that these early computers did characters and it was absolutely suboptimal the story was well if you don't want to fill up your word with lots and lots of bits that real scientists use you could always fill up a subset of your word with maybe 5 bit for a bordeaux code or invent your own 6-bit code to stop having to use figure shifter letter shift turn it from 32 possibilities to 64. so six bit characters were common but some people started to say well you always go on about memory being so massively expensive and so precious and yet on edsack you say well it's five big character so you can economize can you by squeezing three five-bit characters into a 17-bit word no why would you do that it would be hell to get them out again you'd have to do bit shifts and also slow things down so when you look on an early machine like edsack it uses five bit characters in the middle of an'-bit word and the rest of the thirteen bits just don't do anything ibm came along and said look here's the story for those of you who got lots of money and don't mind spending it we have the perfect solution stop making the word be your minimum addressable unit address the characters inside it individually and in order to keep things clean and to allow for future expansion don't mess about with multiples of two like six big characters let's be brave and say right eight bit characters now along for a while before all this came about people had been calling groups of bits of arbitrary size five bit six bits have been calling them bytes but ibm proposed to standardize on the word bite being an eight bit entity and here's the win-win situation for those of you mad mathematicians and engineers we can arrange the hardware to re regard groups of four of those as being a 32-bit word it's a win-win ibm can dominate not only commercial computing but can also make a reasonably good showing because of the sheer speed of their hardware at great expense in scientific computing as well so people said oh great idea oh wow yes that is you know now that we've all got more money i can afford more expensive that is a sensible way to do it without a doubt so the idea was absolutely spot on you choose a bite width which is a power of two and then you arrange to be able to put together groupings of bytes that form something sensible and bigger for macho types who want to do real number arithmetic so everything in the garden is lovely you've got 32 bits you've got eight bits it was almost end of story but not quite because although ibm did very nicely with things like the 371.95 and so on at being a pretty good mainstream scientific computer they didn't really go for chasing the market at the very top end because there just wasn't enough in it for them they made mega bucks off their commercial database customers so right at the top end you get alternative solutions emerging which are still with us today started off with a company called cdc and a brilliant hardware engineer called seymour cray and ears will say cray he left cdc eventually yeah he did and form cray computers so yes at the super computer end of things there was an alternative market but ibm solidly there in the middle said we do commercial and we do scientific we rule the world and i think they were remarkably prescient and they even at the other end for soyuz and tried to get it right but failed but maybe this will have to be another video because it does complete the story is what about people who wanted something even cheaper than the 16-bit pdp 11 they wanted an 8-bit micro ibm saw that coming and tried to get in there and dominate it but for reasons we half understand but can be revealed if there's enough demand they did lose control of that one particular piece of data you have to wait for it to come round again so that tape that you see there is going at 5 000 characters per second let's get over there 5 000 characters per second roughly i think corresponds to 30 miles an hour