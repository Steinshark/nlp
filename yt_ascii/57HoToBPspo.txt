i want to talk about the lessons we have learned at think cell while we imported our software from windows to mac os and in little parts to the web so what's the problem that we started with we have been we have been doing about 12 years of development only for windows when we decided we wanted to port our software to mac os and in those 12 years we had written about a million lines of c plus plus code and you can imagine that we had used windows apis windows data types throughout our entire code base everywhere so there was first of all quite a lot of cleanup that we had to do and what's special in our case is that our product is an added which is dynamically loaded in powerpoint in excel and that has some important implications it means that we are not in control of the main application and unlike people who do server development we are not in control of the computer either of course some of the implications are that we have to be very flexible regarding a lot of things because so many things are out of our control for example we have to be able to render into anything that powerpoint or excel would give us as a backing surface we have to be able to render into views or core animation layers on mac os or h windows or directx textures on windows we always needed a renderer that would support diary x on windows and opengl on the mac just because that's what our host application did and that's what we felt was best supported of course we share the main message the main event with the application we don't control that either and we always wanted to support platform-specific features the same ones that our host application would support that could mean small things for example on the mac there's this triple click feature so if you triple click text the entire paragraph is selected so if our host application would support that then we would have to support that too just to minimize the friction in using both parts of the application but it could also mean bigger things for example on macos office runs in a sandbox environment and that means we are sandbox too and we have to we are to live with that and of course when we started porting think cell we were looking for some cross-platform toolkit that would just hide all those platform specifics and would simply behave identically on all platforms and this talk is largely about why this does not exist and why this this cannot exist actually i will go over several of the challenges that we that we faced while we developed why reported things now some have more code than than others first of all i want to start talking about one of the most important problems that you face when you start porting your software you you have to find the right level of abstraction at what at what abstraction level do you hide the inevitable platform specific code that you will have so you have your operating system on one end that has some features every operating system is a bit different and you have your platform independency plus plus world that doesn't know anything about operating systems and then those two at some point those two have to meet and how does that boundary look like is that just the c plus standard or do you have to write something special that is special for your application and well how that boundary looks like is very much special and it depends on your application and it depends on the semantics of your code it really depends on what it is you want and what it is you need and there's no single correct solution when i looked through our code base and looked at things where we when we developed cross-platform abstractions there were a couple of easy cases where the semantics were so clear that there was what was really a no-brainer how to program that so one example for example is the rendering engine where we have our internal rendering that produces lists of triangles with texture coordinates and then we pass that to some rendering engine and that rendering engine is completely platform specific but from the from the viewpoint of the c plus programmer we don't care we just give triangles to some interface what happens behind that doesn't really matter one one back end produces directx commands the other opengl so it's a simple interface another simple case was doing http requests when we do http requests for downloading updates or whatever we are using the system apis always so we don't use by default cross-platform apis like cr or something because we want to use the system apis where we are sure that they they use the system settings for example for proxies that the user might have configured so we have a business software it departments configure a lot of stuff we want to make sure we use the most default solution we can so all those system proxy settings firewalls whatever are used correctly when they are configured here again the interface is super simple i mean you have a url you want to access maybe with some arguments and you get data back that interface is essentially a function with some data and data out and a slightly more complicated but also relatively simple example starting child processes also the implementation can be very different starting child processes maybe it's running in a sandbox environment and maybe you want to set up some i o mechanism between the child and the parent process but also that that can also be abstracted away nicely where you don't really care about the implementation as long as you get something back that represents a child process that you can send data to so what all these three cases have in common that they have very clearly defined data in and data out and what happens in between can be a complete black box from the viewpoint of the program the more interesting case are there cases where you really have to interact with operating system objects and a very simple operating object is of course a file so let's look at a very simple example probably something everybody has tried to do once that's how to rename a file so when you want to rename a file there are different options that the operating systems give us so in windows we have the movefilex function which of course takes the original file name it takes the new file name and it takes some flags i've omitted some flags here but the the important ones is that you can tell movefilex that it can't copy the file when the source and target are on different volumes and that it is allowed to override the target file if it already exists by default it wouldn't do so it would return an error on linux and mac os there are different functions there's there are two versions of the rename function that have different options of course so one of them is not supported by windows the rename functions can swap the source and target file atomically but only on supported file systems whichever those may be and then it has an option which also exists on windows where it says okay i can i can return an error if the destination file already exists but also only on supported file systems and last but not least micros has something else entirely the copy file function also takes the source file name the target file name it can report in progress state and it also has different flags for example and very interestingly you can specify if you want to copy the access control list of your file from the source to your target location or if you only want to copy the data now you can think huh i have operating systems that support access control lists that's a valid question do i want the access control list to be copied or moved with a file or don't i but the other one didn't let me specify that so you can choose copy the acl copy the data could be both you can also fail if the target file already exists the others could do that too and you can move the file in the sense that you remove the source location it's unclear if that's an atomic move for if it's not an automatic move so here we have many options on the operating systems and they don't it's not clear how how the common denominator of all these options would look like what is it actually that we can do on all of those operating systems how does the common interface to those operating systems should look like and well we actually have an answer the c plus cluster gives us an answer that's the answer it's a simple function that returns no errors it always overrides the target and does give us no options at all and i would argue if that wasn't bad enough it also implements the least useful semantics they could come up with because overriding the target file is something that i never wanted to do and i know why they did it because that's what the posix standard actually standardizes also very useless function but i don't see how that function can ever be used in production software so i think we need something better i think we always need to do something better but how does better look like and how do we make a better cross-platform rename function without filling our code with if defs and different flags on different operating systems it's pretty hard apparently it's pretty hard to get identical and useful semantics on on such a low level and rename is a very low level function and i would say well luckily that's actually not the problem we have to solve we don't need a cross-platform file rename function that's the wrong level of abstraction we need to raise the level of abstraction and the problem becomes much easier if you look at what your application really needs to do in the places where you wanted to use rename there are a different couple of different things that we wanted to do for example we wanted to download a file to a cache in a thread safe way where we downloaded it download it into some temporary file with the random name and then when the download was done we wanted to rename it to the actual cache name in some atomic way so no two process that would cache the same file would ever overwrite the same cache location maybe we wanted to create a user application settings file or we want to create a temporary file that is automatically deleted but which can be opened by different applications at the same time or you wanted to create a document in a user specified folder which would involve opening the system specific save file dialog creating a sandbox exception etc so there were a lot of file operations we needed to do that had much stronger semantics than just renaming the file and it's those stronger semantics that we can actually implement in in identical and useful ways on different operating systems so let's look at two examples the first is the simplest example how to create a settings file that is user specific for your application so on windows we would say well we probably want to store that settings file in the app data system folder in a theme cell subfolder that already implies a choice that only windows makes us do the app data folder the application data folder is what's called roaming that means that if your user profile stored on the server it will be copied to every computer you log on to and this thing cell app data folder will also be copied that's useful if people use our application and use different computers we probably want them to use the same application settings on each of those computers then on windows a process can run under the same user but with different integrity levels which is also kind of privilege so a process might be started with a high integrity level then it stores some settings files then it's run under lower integrity level and it maybe cannot write it cannot read the settings files the process has previously written so maybe we want to encode the integrity level in the folder name as well and then we're done mac os makes us offers us different choices and and we have to do different choices on mac os maybe the standard location would be library application support think cell but maybe our application is running in a sandbox environment and maybe we know that there is a group of application that we have written that should share the same sandbox and they should share the same access to the same files in that case there would be one location a group container a so-called group container that they could all access and maybe we want to store the settings there and then last but not least because we know what the semantics of those functions of that function is there are a couple of decisions we can make which we always make identically because we know what we're supposed to do for example we could say well when we are writing that user settings file we always obtain an exclusive lock while we are writing that file and we also always want to inherit the access controllers from the parent folder when we create that file because the parent folder is a system location and we assume it has the correct access control so in that sense there are a lot of decisions we can make here different decisions that are different in each operating system and decisions that are identical different operating systems just because we know what we are supposed to do so a more complicated problem is how to create a temporary file that will be automatically cleaned up for us but which can be opened by another application in the meantime so one windows that is pretty easy you can create a file with create file you give it some location in the temp folder then you specify some windows specific settings some security attributes maybe you want to prevent file execution you want to make the file accessible by the current user and then on windows there's this magic flag file flag delete or close which does exactly what we want here it says when the last handle to that file is closed then windows will take care to delete that file automatically but in the meantime everybody who has the path or a handle to the file can read from that file so that's pretty useful there's no such thing on posix systems unfortunately so we might we have to do something ourselves so we open the file here i assume it's in the group containers folder because i said we want to share the settings file file with different applications again we make the file readable and writeable just for the current user and then there are a couple of macro specific things we have to do or posix specific things the open call can be interrupted by a signal and we have to handle that into e-interrupt maybe try the open again and now we have to do the clean up ourselves in some way that is that is conforming with the desired semantics so maybe we want to implement reference counting in some shared memory object we can't do the usual posix temp file trick here which is to create a file and then delete it immediately because i said we want it to be readable by different applications at the same time maybe we say oh manual reference counting in share memory that is error prone if our application crashes then we have no guarantee that we actually cleaned up the file so let's do something simpler again we know what the semantics is we're not writing some low level meaningless function we know pretty well what the use case is so we are very free in in the choice of our application and we can decide okay we can do something completely different here we can for example say well let's store all the data all the temp files that have this requirement in some database in a single one and just hand out objects that look like files that can write into that sqlite database and now maybe we are crashing and maybe that database stays around on the disk but now we only have one file to clean up whenever we start again so that simplifies our cleanup problem at the very least so that shows the advantage you can choose very different implementations because then you know the the contract that the function is supposed to fulfill so that's why we're saying that what we found was that cross-platform interfaces need to have well-defined and strong semantics otherwise it's impossible to implement them correctly if your cross-platform interface your abstraction level has a two-week semantics then there's a good sign that you've created a problem is when you start to look at the implementation of one of your operating systems because you know your operating system you know the questions it's going to ask do we lock the file do we copy acls do we create the correct security descriptor and you know you notice your your function interface does not seem to imply the the correct choice here so it's not clear what happens on one operating system and you start digging into the implementation that's when you know you've made a mistake in your in your interface design and as i've shown before specifying strong semantics increases the degree of freedom for your implementer which is a good thing when you're choosing the right level of abstraction implementation there are essentially two mistakes you can still make you can introduce that level of abstraction from your operating system at a too high level that would mean you miss the chance to unify some code in my experience that is extremely rare i don't think i've ever made that mistake because we are very lazy as programmers but i've done the opposite mistake quite a lot of times i've tried to force common interfaces on two low-level things like foundry name for example and that either means that you lose all the expressiveness so we've seen what the c plus plus standard made of all our powerful operating system specific rename functions none of that power was left in the standard function or you implement something that kind of works but whose semantics don't match any match one of the operating systems very well one example i found was for example q file set permissions if you're not familiar with that that uses the posix permission model on windows as well so q file set permissions on windows lets you specify the readable writable executable bit for user group other on windows files and you can certainly implement that on windows because the windows security model is very powerful but it just does not match the windows security model so in that sense it doesn't match your operating system so that's my that's the first part of my talk in the next part i want to talk about something else we found that sometimes you you just get handled you just get given objects by the operating system that behave very differently and that's also something that you sometimes have to deal with and that cross-platform toolkits don't solve for you that's a problem that they don't solve for we use shared memory a lot in our application to implement inter-process communication and i've said initially that our product is an add-in but that's not quite the s that's not the entire story so our product is an add-in which runs in powerpoint in excel those two already have to communicate with each other but there are quite a lot of other processes involved we have a google chrome add-in in the little web app that lets you link data from the tableau.com data tool to think-cell charts we have processes that do oauth authentication so users can import images for example from getty or other image databases we have a chart scanner that uses text and image recognition techniques to extract data from charts that you would just have in a pdf report so you can essentially take the screenshot and it will try to read the chart and then get data out of it and put that data in an excel sheet and we also have a command line tool that lets you batch create presentations from a template and data that you're given in json file format and then you can create your monthly reports in powerpoint so all these tools have to communicate with each other and they use inter-process communication for that so they use shared memory for that ipc we use boost interprocess because it does offer a common api for shared memory on windows and posix and it offers everything that you need essentially so there's a shared memory object a named memory object that you can map into different processes and that you can look up by name and there's a special kind of shape memory pointer so you have the shared memory segment you map that into each process but it will be mapped at different addresses now if you want to store a pointer in that shape memory to another address in that shared memory let's say you want to store a vector of things that vector internally has three pointers but it can't be raw pointers because then the vector would not be correct in the second process that has mapped the shared memory instead the vector should use offset pointer which uses a clever trick it stores an offset to its own disk pointer so the offset pointer is correct in all processes that map the shared memory object and last but not least you need some synchronization objects so you can synchronize access to the shared memory object and how does that look well looks a bit like this in a very simple example here i have a server process that creates some shared memory object with a given name that name has to be very specific maybe the shared memory object lives in some global namespace so it should be specific to your software version maybe your host maybe your security identifier and then when i have that shared memory object i can construct an object by name here a simple pair of double edit and then in my child process i can look up that object by its name the chat memory object and it will be mapped in my process and i can look for the pair and i find it now let's say i run those two processes and they terminate and i start the server process again well what happens to my surprise i will get an error here and the error will say a shared memory object with that name already exists and that's kind of surprising it's especially surprising if you're coming from from windows because it means boost inter process by default does not use the windows shared memory implementation that would look like this on windows shared memory is essentially in anonymous file mapping so you can create a file mapping that doesn't matter any specific file but instead it will be mapped by the page file now again we can map that same object in different processes and when all these processes have died then windows will clean up that memory object for us and if we recreate it we get a fresh one that is uninitialized that's great that's a great guarantee we windows cleans up after us good guarantee we never get stale share memory that's the feature that we want but it's not supported on posix posix shared memory works differently and that creates a lot of problems over the years so when i first encountered that i started googling for a solution and all i found were people who had the same problem everything i have to clean up everything myself how does that work so on windows generally kernel objects share memory mutexes even the temp files we have seen are reference counted that means when the last user or the last process that used them has died even if it has crashed their resource is free for us on posix shared memory can be either file backed which means the backing file still exists if the process is crashed even after a system reboot or it can use the actual posix chat memory implementation but that is very limited on many posix versions and it still means that the backing memory will persist when our processes has crashed until the reboot at least so here posix assumes a server client model where the server owns the shared memory object and it has to take care that when the server started first it cleans up the old one and creates a new memory object if that's not your process model and it wasn't ours then i could find one and a half two solutions to that problem but neither boosting the process nor acute implemented them so one solution or maybe only half a solution to the problem were robust mutexes so a p frame mutex can be marked as shared process shared that means you can store the p-thread mutex in shared memory itself and then you can also mark it as robust and the robust pthread mutex keeps track of which process last locked it so if you try to lock the mutex and the last owner of the lock has died while it held the lock then p3 mutex will return e on the dad and tell you so and say oh and then you know that your the the resource you're guarding the shared memory object in our case is probably in an undefined state and you have to clean it up or you have to terminate or whatever so that's half a solution because it doesn't tell you anything if the other process has died well it did not hold the lock then you might also want to clean up but you will never know so mac os well there's some there's a boost in the process pull request by somebody who wants to get that merch to boost in the process and that may be some support because it hasn't happened yet macos didn't support robust pthread mutaxes anyway so i had to look for another solution the only other solution i could find was file locking because file logs are the only resource that have process lifetime on posix systems and there's a big warning here because fireworks are very weird composite systems as well and every a lot of things you have to watch out for and there's also a boost inter process pull request bus because what is what did we do we introduced a different kind of shared memory object and in its most internal method the details are not so important we also create a packing file and in that in that function we create our backing file we try to obtain an exclusive lock to that backing file when that succeeds then we know we are the first ones the first process to open that backing file to open that shared memory object and we truncate the file so we try to do the same that windows does and guarantee that you always get a fresh shared memory object and once we have truncated the file we degrade our exclusive log to a shedlock and go on with our lives when obtaining the original exclusive log failed immediately we know we are not the first ones and we just wait until we get a shed so that sounds super simple and has worked well in practice and i think it's much better semantics than the default implementation it has two small problems that i think are not actually problems the one problem is that this f log call down here where i said it degrades the exclusive lock to a shedlock it's not actually an atomic operation it actually releases the exclusive lock and then waits for a shed look so that can be a problem but it is not a problem in our case because there are only two things while that airflow case call has released its exclusive lock that open might succeed with an exclusive lock and the second process might think he's the first one in that case that other process will also truncate the file but truncating the file twice is likely not a problem and then both will go on with the shed lock and the second thing that can happen is even more harmless is that this call would succeed before that call succeeds and that doesn't matter more interesting problem is that as i've said before fire locks and public systems are difficult and one such problem is what happens if you want to obtain a fire lock into a file that is stored in a network file system and here also we are i think lucky and all that we require here is that these fireworks are visible on your local computer they don't actually have to be visible on other computers that have the same drive mounted so there's also boost in their process pull request for us because i think that's useful semantics for a shared memory object maybe you can give that an upvote so here in that chapter i wanted to say that again in our cross-platform development we were aiming for strong and identical semantics for all our kinds of objects and strong semantics can also mean strong guarantees and it especially means strong guarantees from your operating system these are almost always good and here what the boost implementation did it sacrifice operating system guarantees on one operating system for the sake of an identical api and that's a i understand why they did that because one was easier to implement on the other operating system than the other way around but that doesn't mean that that solution is the right one for you or the right one for everybody here there were problems maybe it's a problem maybe network file systems are more of a problem in your use case than in our use case so there might be different how do you say different different considerations here but in our case we found that we could actually implement better semantics when we looked at the problem and we looked at the at the we say the requirements the requirements that we had we know our constraints by sales so that's something that you might be able to do better than than even cross-platform toolkits can so now my i want to change focus a little bit and talk more about actual tools that we have built and some of them are also open source the first tool we've built is very much c plus plus unlike the previous two chapters maybe and it concerns text internationalization text internationalization in general is more than just translation but here i focus on text translation so how does that usually work well every text translation toolkit has to have three features the first one is that you want to annotate the text that should be translatable in your source code so that you can easily extract it and send it to the translators the second is that all those translations they need a context for the translator so they know what you are actually talking about and last but not least your translation software must not make many assumptions about properties of your language because languages are very different and i learned when i prepared the talk for i think cpp russia originally but russia has a russian has a different number of plural forms for example and english or german has when you have such a tool then the usual flow is always the the workflow is always the same so you have your annotated source code you extract all the translatable text you send that to the translators then you get something back in excellent file format and that xml localization interchange file format you import that into your project somehow as a resource and then at program runtime you would look up the original text let's say in english plus your current ui language and you get the correct translation back now that's a problem where you do not want to support different native mechanisms because that would be horrible you want to have the same markup in your entire code base you also want to have a single text extraction run that will find all the translatable text regardless of which platform they will run on in the end and at runtime you also want a uniform way to access the translatable text you don't want to have cases where some calls return a reference count in a string pointer and some calls return a is to basic string that is heap allocated so it should work the same in all cases boost local does that for example boost locale did not exist when we start to translate our software but it does everything that i've just i've just described there's a translate function which also serves as markup that translate function supports translation context here i will translate open in the context of opening a file to german and it supports for example different plural forms in russian and then that translate function also does what i just said it will in the background do a runtime lookup in its translation catalog and return the correct translation but this is not 1995. we don't have to do runtime lookups we have console expert functions now and we can do cons we can do compile time lookups and that's actually what we do so you have a very simplified example of our code let's say we only support two languages english russian and then a translatable string is just an area of those two translations and the translate function would choose the correct option considering our current ui language so that's the trends lookup macro in that case unfortunately is where the magic happened where the magic happens given a string in a context it would return the correct translatable string and if we have that then we can put it all together and output a translated string so what does translocate do if it doesn't do a runtime lookup what does this there's a translatable string map and that's essentially a compile time hash a compile time hashmap that takes four unsigned integers as keys and those unsigned integers are a compile-time computed hash of the string and the translation context you see here we use a rumor hash and we calculate the hash for the string and the context and because there's no built in 128 bit integer type but the hash will be 128 bit we split the hash into two 64-bit unsigned integers and now if you see that and if you know c plus plus you can probably imagine what our translation tool outputs it outputs template specializations of course so our tool the tool that we have that processes the xliff files that are returned by the translators will calculate the same murmur hash at build time that we calculate here in the c plus plus compiler and we'll output a cpp file filled with those templates virtualizations and of course now you might wonder why do we do it like that well in a cross-platform environment this has one big advantage i don't have to worry about installing files finding those files on the disk having itd partners delete the files having it departments change file permissions on those files there's all class of problems i no longer have just because i can look up this translation in our binary at compiler it makes our binary larger we also could not ship additional languages in some language pack that would be dynamically loaded but i don't know who still does that anyway so a short reminder what can constant extra functions do or they can do a lot with literal types a little type can be a scalar can be reference aggregate types that you can initialize with brace initialization or literal types for example a state area types that have constant exp constructors and destructors and literal data members and base classes are literal types of selves for example pair and all areas of liberal types are literal types now context for functions of course can't be virtual they cannot contain go to and i think for now they can't contain catch statements but they can contain this if switch in all loop statements which makes them very powerful and very easy to use and they can use local variables as long as these are initialized and liberal type themselves so all that is easily enough to essentially take the hash implementation that is open source and implement it in a context for context so here you see the implementation it contains template on the length of the string we even take the pointer to the first element of the string as a color pointer and that is also not a problem in a concept context and i spare you the implementation because that's easily found online so here looking for strong and identical semantics means we also want to have as far as possible the same tools in our build process tools that are well that produce the same output of course and the first version of my talk that's left here for some reason i kept score of which operating system did something better than the other one so we have think cell we also do pretty special error reporting we've given entire talks on our approach to error reporting and this is just a short short excerpt here because it also had very important implications in a cross-platform context we have a very powerful reporting architecture and we use a lot of asserts in our code to check invariants and what's special at thinkstill is that those asserts stay in the code even in release builds we also check all return values from all system api calls we do and we always distinguish between expected and unexpected errors so if you try to open a file then it's always an expected error that the file no longer exists but it would be very unexpected if the function suddenly returned error out of paper so you have to handle the file does not exist error you do not have to handle the error out of paper of error you just can assume and that it will not happen and we will report an error if it does happen so once we encounter an unexpected error like error out of paper then of course we might show an error message but more importantly we send an error report home to our backend server and that server will analyze the array point it can for example check if we have already fixed the bug or if we need more information from the user it will report that back to the process that has just encountered that error and that process can then decide to download the available update silently and even install it silently and can even reload our add-in silently so the entire process might be unnoticeable by the user but we will fix the problem and it will never occur again or it might well if the backend decided we need more information then it might show a more helpful error message so users saying could you help us reproduce the problem etc from our developer sites we have a little interface to the database where we can look at the most frequent errors we can filter them by version by operating system etc and then say okay i fixed that and released so-and-so and that back-end looks at the at the stack where the problem occurred and groups them and filters themselves etc so that's extremely useful and it finds a lot of bugs that are dependent on the user's setup bugs that only occur if you have another software installed if you have a security tool installed etc and the core of that functionality is something that only exists in windows it's mini done right dump which as the name implies is a function that will write a dump of your program at the very least a dump that includes the full stack and the registers and that's extremely helpful information microsoft provides simple service for all the simple libraries you have the symbols for your own software and together you can symbolicate a dump that maybe takes 100 kilobytes and you can you get the full stack of the your program at the point where the bug occurred and you can find out something about the problem nothing similar existed on macos there's google breakpad google crashpad of course but again that's a bit the case of well the web pre-existing solutions but weird things i'll always like to look at those solutions and see if they are really good fit and here google breakpad and google crashpad made a lot of strange decisions so for example they write windows minidumps on all systems even on non-windows systems and then of course because no tool can read a windows minidab that originates from a mac os system they need to write custom tools to analyze the windows minidams so that was a lot of code for something that wasn't very powerful in the end and something that we would have to support as soon as we included it in our software and for mac os at least there was a very obvious and much simpler solution because the mac os binary file format is very well documented and it includes documentation for the core file format the situation is a bit different on linux where the core file format is not so standardized i think but all that we had to do was to write a valid microcore file that would only contain stack memory and then the standard llgb debugger could already read it so this is what we did and it's pretty straightforward we've also sourced the interesting bits of that code and implementing that was then straightforward is really going through the file format specification and the list of system calls and assembling the necessary information the first thing you have to do whenever you write a bug reporting software is that you have to send you have to let another process take over your process just encountered a problem an error it is an undefined state maybe you've overflown your stack and there's nothing you can do so you send all access rights to some crash reporting process mac os has a funky kernel architecture it uses messages a lot messages and you can assemble a message that sends all access rights from to your process to another process so we do that and then we assemble all the important process state we collect the number of threads the register state the memory we look for overall memory regions check which memory regions reference stack memory we include that and then we write dot write it into a binary file there are some additional metadata that we need to include most importantly the loaded libraries the pdb file would include that for us we have to write it ourselves so which shared libraries were loaded what version did the shared library have and what was its start address so this is all the information we need and then our backend can analyze mac os back reports just as it can analyze windows bug reports we have debug builds for our own software we can cache the system binaries for all the macos builds that we support because there aren't so many and then we can write a small driver for ldb that takes all that information and sets it in leb and ldp will output a valid stack trace for our crashing process so that's very good and again sorry again the case where we said well we need the same tools on all the operating systems there are pre-existing solutions that don't quite do what we want or they're much more complicated than we want them to be where it was worth our time to implement something ourselves that in the end worked much better for us so last but not least a little i want to i want to wet your appetite so to speak for my upcoming cpp called talk as i've said we've moved some parts of our application to the web and we ship with a chrome extension and a small web application and when we started writing that was really our first web application we had to ask ourselves well what language do we want to use and it was pretty clear that we did not want to use javascript because that's just horrible we started writing the web application in typescript that looked much better it was type safe it had awesome type definition libraries that allowed us to interface with legacy javascript libraries the typesafe way but it turned out that sharing code with c plus plus was now possible of course and there's always a little bit of code that you want to share even if it's just an enumeration so we looked at f script and script would let us compile c plus plus so we could share all the code we want but now suddenly we have the opposite problem we couldn't interface with javascript anymore or with the with the entire dom object model for example that lost all type safety again because interfacing with javascript looks a bit like in the example down here it's all untyped checked strings essentially you can look up global objects by the name and hope they exist call functions by name and hope you got the name right and the number of parameters so that's that's horrible again we we can't program like that either so we said well maybe we can write our own tool you see that the repeating pattern things up that combined the advantages of the typescript approach and the m-scripting approach and we built kind of our own compiler so i said that typescript has awesome type definition libraries so this is what they look like this is a typescript description of javascript the standard javascript document dom element so you see that's the document and it has a couple of properties it has a url the url has a string it's always set it's never null but it's read only the document also has an active element property which is an html element but which can also be null anchors our collection of other elements etc so since the the name says already says what it is it's a type safe description of a javascript library in typescript and that is not just available for the standard javascript libraries it's available for over 7000 javascript libraries so there's a huge repository of such descriptions and luckily we don't even have to parse those descriptions ourselves typescript comes with an awesome compiler api so we can with that small snippet of code here we can we can read a file a typescript file and iterate over all the syntax node in that typescript file and ask is that syntax node a function declaration if it is a function declaration what's the function's name what is its return type its arguments etc so we can use the touchscript parser to do parse typescript interface definition files and output c plus plus interfaces and that's what our typescript typescript tool does so it compiles typescript interface declarations through c plus plus interfaces so we can call javascript libraries from c plus plus in a type save way via unscrew so when in javascript you would set the document titled property like here just assigned to the tile property the c plus plus looks kind of similar you get the global document object and then you call its title setter and you pass a javascript string but all of that c plus bus would be compiled to webassembly and then would make calls in javascript so this is an example of the code that we would generate for example so at the base there is some javascript object a javascript object is essentially an abscription value and if you're not familiar with m script m script the abscription value just represents a javascript object so we make a call transcript and tell it to create a new javascript object it will actually call javascript function that will create an empty object put it in some map give it a reference count and then return to the web assembly world and say okay here you have a handle to that javascript object do what you want with it and then we have that c plus version of the interface we've just seen earl is just readable active element is read only title can be read and set and the implementation now looks like the script calls we have already seen the implementation is not type safe we call the title function by name and abscript will unwrap that name transform it to a javascript name and then call that property for us but we have wrapped that in a type save interface that we can use from c plus plus and our project is currently almost self-hosting that means typescript can compile enough of the typescript interface definition itself so we have implemented typescript already in c plus plus that uses this typescript compiler interface and this typescript code i've shown before that what parts the program and iterate over the syntax nodes would look almost identical in c plus so again you create a program a typescript program from a typescript file that will instantiate the typescript parser and then you can get access to your source file and iterate over all the syntax nodes in your source file and i'll go over this shortly and then i think i'll be ready for questions you can imagine there are quite a lot of challenges when you're trying to parse typescript and transform it into valid c plus plus because those are very different languages and for example one of the easier problems to solve is that in typescript the declaration order of your types does not matter so this is for the type script i can define a union full bar of two types foo and bar that have not been declared at this point they are declared later but for typescript that doesn't matter it wouldn't matter for c plus plus so we have to do something obvious we have to collect all those types and then sort them and find some order of declarations that will will work for c plus plus there are other problems typescript for example supports enumerations that includes string values that's also not supported in c plus plus and here as well we do some hopefully clever tricks we define c plus plus union that is a standard plus plus union and then we just specialize the marshalling so there's some point where this plus plus value is marshaled and transformed into a javascript value that can be sent to that javascript world and here the javascript value of course has to be the string that javascript actually expects there's some interesting magic that i'll skip over here that we have to do to implement function callbacks functions of course in javascript are also just objects that are reference counted that's something that we can't do here or c plus plus objects always have a scope that might go out of scope what do we do with function callbacks the implementation is not so trivial and there are a couple of interesting challenges that are still open that we are working on now that i'm working on now the first one is generics which is like typescript templates but they can have interesting constraints where i'm not sure if they'll translate to c and then there is one example of the the interesting things that typescript does to express common javascript idioms in a in a type safe way so if you have program javascript if you've programmed a web page then you're probably aware of that standard event list interface where you call ad event listener for example it's a button and the type of event you want to listen to that you have to pass is actually a string so you call add event listener and then click as a string and then the listener function so how can you make an interface typeset that takes the string as a first argument but you're not allowed to pass any string and here they did a clever thing so they said well let's define an interface that was never gonna instantiate it's just a description of something and that add event deletion interface is now templated on a type k that is one of the keys of document development that means k can be here click or key down the string so the first argument to type the event listener is either click or key down and then the listener function receives an event where the event depends on the kind of event we're listening to and then here they implement a kind of type lookup where they say okay document event map of click is a mouse event and document event map of key download is a keyboard effect so this tool is going to be interesting to to transform into c plus plus because it's a very common agent but i have some some ideas already okay here's the link again i'm happy to check it out i'll have a whole talk on this at cppcon and with that i am i'm done and i am i'm open to your questions