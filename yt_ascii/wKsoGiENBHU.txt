yeah it's a different kind of ner we're looking at today so these are neural radiance fields right this is this is something that's been happening you know been going around the ai literature for a while a couple of years at least really impressive ways of generating new views of scenes i'm familiar with how nerfs work but i don't work with nerfs every day so i brought in my tame phd student here lewis hello yeah and lewis actually you know trains nerfs he's working with nerfs as part of his phd and so you know you're going to explain to us how it works i hope so yeah what they do is they take a series of rgb images and from those rgb images of a scene they're able to reconstruct it in 3d using a neural network traditional methods of 3d reconstruction are things like point clouds voxal grids meshes things like that but they're discret and for some scenarios that's all you need but for real life situations for example this room it can get very very complicated many many points many many faces on your mesh this ne radiance field is is able to reconstruct it in very very high quality detail just only using simple rgb images that you can capture on your phone so i think the interesting thing about this for me is that we're actually not doing rendering in in a way you might expect so normally what you would do with rendering is you'd have some meshes but you then you know you you render them as pixels so you rasterize the image or you do ray tracing or something like this with a nerf what we're actually doing is essentially a bit like ra tracing but we're actually using a newal network to say okay this ray is going to be this color and this ray is going to be this color and so we basically building our 3d scene into the kind of inside of a newal network so the parameters and weights of the newal network are actually encoding our christmas tree or our car or whatever or our room or whatever else it is we're we're we're looking at i'm just going to draw here a trained nerf for now so imagine we're looking at this from a side angle so we have something like this and and for the sake of this video as you will see later i'm going to draw you a nice christmas tree i hope you're better at drawing than me not a chance that's actually a lot better than me okay and you know i'll just shade it in green yeah there you go so and let's imagine that we have a camera up here now when i say camera an image has already been taken of of this christmas tree in real life and what we're doing is simulating this this camera this camera in real life we're an image that looks like that makes sense looking at it from a above sort of view so how do we render this right so what we do we start shooting rays into the environment like this this is what the neural network does so unlike things like diffusion which basically generate noise isn't it the stable diffusion and yeah yeah stuff like frogs on stilts and stuff like that yeah th this doesn't generate an image given a position in 3d and a and a view direction it will give you a color and a density at that point in the environment that's important that's what is different from things like diffusion is that it has a 3d representation it understands the environment so now we shot this ray through what we need to do is query a series of points along this ray and ask the neural network okay i'm at this point i'm looking at it from this direction what should the color be what should the density be so let's start off by doing this so we're going to do this point here right what is the color and what is the density we go to neural network and it comes back and says the color is white doesn't matter because the density is zero why because we're in empty space right there's nothing there's nothing there yeah it's like air you can't render nothing so doesn't matter let's continue do it again nothing again nothing again nothing again nothing oops go thank you wow here we go we've created a point here and it's come back with a density of one aka it's inside an object and it's come out of a color of green makes sense because we're now entering this christmas tree and what we're going to do is do it again here here here here and you've queried all these points along the ray the neural networ come back with nothing nothing well white white white but with let's say density 0 0 green density one green density one green density one 0 0 perhaps the thing that when you first learn about this is hard to get your head around is normally in ra tracing what you would do is you would fire arr into your scene usually based on your position of your pixels and say basically query what color is it and it might bounce around and do lighting or something like this but essentially it will come back and say yeah it's red and you paint that pixel red in this case we're actually doing multiple samples per ray and we're saying what's here what's here what's here all the way along and what you'll do is sometimes you'll just shoot off and they there be nothing there sometimes you'll hit an object you'll intersect an object and for some time you'll see different colors and so your rendering process is going to be about sending out a lot of these rays and finding out where in 3d everything is right as opposed to just sending out a ray and going oh it's red yeah now that might seem really inefficient but actually this is the only way it trains right because if you trained a new network and just said yeah this is red this is red this is green this is red then it will work very nicely at just drawing that particular image yeah but you can't then move the camera over here and say okay what does it look like from above right where we haven't necessarily got any images so the idea would be that you train this with with sort of another camera my camera's worse than yours and another camera so you maybe have three cameras but now we can draw this one and we can draw this one and this one because we can shoot rays out and we can do thing what's important here is that when you shoot the rays out here they are intersecting let's say like that that's sort of bit iffy because it goes through there but that how you're able to finalize where that object is in 3d space because these points intersect on the rays that you shoot which is why with nerf one of the downsides maybe is that you need quite a few images for it to properly get a good reconstruction if you only have one image what will happen is it will look pretty great from anywhere near that image but if you move elsewhere and it will it will degenerate pretty quickly so and another thing that's interesting about this is normally in any kind of ai or machine learning what you're trying to do is generalize your approach to some other data set so you say i want to train on this data set but ultimately i don't really care how well it works on that data set cu i already know all the answers what i care about is how it works on this data set and this data set but in nerf we're actually learning this exact scene it's not going to reconstruct a different kind of tree cuz that's not in the training set we only care about producing images of this tree from different directions yeah overfitting to the max pretty much they tell you never to overfit but in this case overfit as much as as you can this is over fitting by design right by so i figure we could just trot off down the corridor and take some pictures and have a go now christmas tree is actually a really hard example of pines and everything yeah i mean that's not an easy thing right and and and it's a good example in a way because we'll see some of the problems as well as the benefits of nerf but also it is worth knowing that this is something that most reconstruction techniques are going to really struggle on right so this is a hard problem but it's also fun yes well you know when you say you've got to take quite a few pictures how many is quite a few for something like a if you want a good reconstruction at least 250 okay yeah and and those are the good images there are to be fair because nur really popular there's lots of research going on and there are many many techniques that are trying to reduce the number they don't all work very well right so you know your mileage may var if you want really good reconstruction then a lot of images is what you're going to need and obviously if i'm obviously i'm a videographer so video any good video is good because video is a lot of i mean aside from maybe motion blur and things like that if you've got good quality frames that's just more and more shots so what have we got we've got a nice looking christmas tree i didn't decorate this one and this is a very complicated scene right because we've got bushes at the side with huge numbers of leaves we've got sofas we've got whatever all this stuff is hanging off the the trellises it's a big big place as well one thing about normal nerf that you see in the literature is that they run on fairly constrained scenes like you often you've used a robot or some other capture rig to capture very nice concentric circles of images all equally spaced everything's very constrained we're just going to kind of wave a around and see what happens it's more fun so all right so lou will catch us some videos and then we'll i'll yeah i'll stand over here and not be in the shot what happens if you get things changing in these images it's not good that that's where you'll get a lot of i think they they call them floaters where it can't figure out where to put certain pixels in the scene cuz they're different throughout and then it would just create noise so you could be photob bombed and that would really cause a problem exactly you get kind of a ghostly mic and then fing back out again that's exactly it yeah so we've got the video now the next step is we need to get the camera positions and then we just got train the nerf and that should take about an hour i've trained up this nerf and currently we're we're viewing it for this i'm using nerf studio which is what a lot of people are using now cuz it's very userfriendly and it's very very good and this is the sort of thing that it looks like so you can see all of the images and where they are in 3d space and that was me going around that christmas tree it takes a while for the for it the quality to increase because what this does it renders it very quick because you need to get an understanding of the environment but nerfs are slow right for real-time rendering so it takes about 5 seconds for it to render a good image every time but if i move it around here and let me just get rid of the cameras here so you can just see the thing give it a few seconds and it should there we go now i want to talk talk about what's good and what's bad about this there are nerf data sets out there that are fantastic and you'll be able to get really good high quality reconstructions from them this is is not so good because i took it on my phone and there's lots of things like motion blur different sort of issues i didn't capture the whole scene but things like the christmas tree that looks pretty good to me you know you can see the bu balls on it and high quality you're not this tall right with the greatest respect if you come down yeah yeah will it be better if you're closer to where the original cameras were ex it should be oh hang on hang on it's a bit finicky mind you there we go so this is a player yeah it's a real time renderer and it looks good from this perspective because this was where the images were taken from cuz that's around my height where i was taking it from and this is probably very close to there you go it's very close to where these images were taken which is why you can see when it loads you can see the background somewhat you can see the tree tre it makes sense the christmas tree looks good as soon as you move out suddenly it looks pretty bad right and that is one of the things with nerf is that it's very good where the images can see the scene there is going to be a lot of loss of quality when you go outside where we were capturing the images from would you call this a data set or a picture or an image or a scene what would you call it i would call this a data set data set so how big is a data set like that so this is around 300 images so nerf data sets have a series of imes and then json far with where all the camera positions are that's that's it but this is about 300 but they're all relatively if you see here they're all pretty close together right if you want a really good capture really good scene you want them spaced far apart capturing the entire thing i'm not mr fantastic with long arms i can only capture things around here right which is why when you go and look let's say over here in the background you'll see that the background here especially on the ceiling it's really not very good because not a lot of dat was captured in these images of the background so that's why it's not as good you see here this is just noise and the reason why that is just noise cuz i didn't actually capture any of that floor when i was capturing the video i just forgot which is why when when you look at it from here it looks awful but that's not really the fault of the nerf per se it's more of a fault of me perhaps it's worth thinking about what we would actually use this for because ultimately people might be looking at this and going well it's not as good as a 3d render but but we actually only had to capture 300 images we didn't have to artistically design the tree in 3d we didn't have to paint all the all the meshes we didn't have to do any of that and you know i couldn't do that anyway because you've seen my drawing abilities so but the other thing is that we also as well as a color we can also extract where the objects are which means that you can convert a lot of these objects into a mesh so you can use this multi view reconstruction to essentially obtain 3d volume so you could then speed up your creation of an actual 3d asset that you could use in a game or something like this is this the future then of of 3d rendering is this how it's all going to work this month possibly yeah actually there's a new rival to nerf called gan splatting which is also providing incredibly impressive results so maybe that's video number two this network is maybe slightly better when it has a text estima the noise so you actually put in two images of dystopian abandoned futuristic i with overgown plants right and then i just put them in a four loop and just produce 200 of them so i can pick the nice ones