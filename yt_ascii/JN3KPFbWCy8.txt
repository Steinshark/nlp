- the following is a
conversation with elon musk, his fourth time on this,
the lex friedman podcast. (elon whistling) i thought you were gonna finish it. (elon whistling) it's one of the greatest
themes in all film history. (whistling continues) - yeah, that's great. - so i was just thinking
about the roman empire. as one does.
(elon laughing) - there's that whole meme where old guys are thinking about the roman
empire at least once a day. - and half the population's confused whether it's true or not. but more seriously thinking about the wars going on in the world today. and as you know, war and military conquest has been a big part of
roman society and culture. and it, i think, has been
a big part of most empires and dynasties throughout
human history, so. - yeah, they usually came
as a result of conquest. i mean, there's some,
like the hungarian empire where there was just a lot
of sort of clever marriages. - but fundamentally there's
an engine of conquest. they celebrate excellence in warfare. many of the leaders
were excellent generals. - [elon] yeah.
- that kind of thing. so a big picture question, grok approved, i asked if this is a good question to ask. - [elon] get tested grok approved. - yeah. at least on fun mode. (elon laughing) to what degree do you think
war is part of human nature versus a consequence of how human societies are structured? i ask this as you have
somehow controversially been a proponent of peace. - i'm generally a proponent of peace. i mean, ignorance is perhaps, in my view, the real
enemy to be countered. that's the real hard part. not, you know, fighting other humans. but all creatures fight. i mean, the jungle is a, you look at the, people
think of this nature as perhaps some sort of peaceful thing. but in fact it is not. there's some quite funny
werner herzog thing where he is like in the jungle, like saying that it's
like basically just murder and death in every direction. i mean, the plants, animals in the jungle are constantly trying to kill each other, every single day, every minute. so it's not like, you know,
we're unusual in that respect. - well, there's a relevant question here. whether with greater intelligence
comes greater control over these base instincts for violence. - yes. we have much more
vulnerability to control our limbic instinct for violence
than say a chimpanzee. and in fact, if one looks and say, chimpanzee society, it is not friendly. i mean, the bonobos are an exception, but chimpanzee society
is filled with violence. and it's quite, quite horrific, frankly, that that's our limbic system in action. like, you don't wanna be on
the wrong side of a chimpanzee, they'll eat your face off
and tear and nuts off. - yeah, basically, there's
no limits or ethics or there almost had just war. there's no just war in
chimpanzee societies, is war and dominance
by any means necessary. - yeah, chimpanzee society is a pro, like a permanent version of human society. they're not like peace
loving basically at all. there's extreme violence. and then once in a while, somebody who's watched
too many disney movies decides to raise a chimpanzee as a pet, and then that eats their
face or rip their nuts off or chew their fingers off
and that kind of thing. it's happened several times. - ripping your nuts off is an interesting strategy for interaction. - it's happened to
people. it's unfortunate. like that's, i guess a one way to ensure that the other chimp
doesn't like, you know, contribute to the gene pool. - well, from a martial arts perspective is the fascinating strategy. - (laughing) the nut riper. (laughing) - i wonder which of the
martial arts teaches that one. - i think it's safe to say
if somebody's got your nuts in their hands and as the
option ripping them off, you'll be amenable to whatever they want. - yeah.
(elon laughing) - so like i said, somehow controversially, you've been an proponent
of peace on twitter, on x. - yeah. - so let me ask you about
the wars going on today and to see what the
path to peace could be. how do you hope the current war in israel and gaza comes to an end? what path do you see that can minimize human suffering in the long term in
that part of the world? - well, i think that part of
the world is definitely like, if you look up the,
there is no easy answer in the dictionary. it'll be that, like the
picture of the middle east, in israel especially. so there is no easy answer. what my, this is strictly
my opinion of, you know, is that the goal of hamas was to provoke an
overreaction from israel. they obviously did not
expect to, you know, have a military victory, but they expect, they really wanted to
commit the worst atrocities that they could in order to provoke the most aggressive response
possible from israel. and then leverage that aggressive response to rally muslims worldwide for the course of gaza and palestine, which they have succeeded in doing. so the counterintuitive thing here, i think that the thing that
i think should be done, even though it's very difficult, is that i would recommend that israel engage in the most conspicuous
acts of kindness possible. every po- everything. that is the actual thing that we're taught the goal of hamas. - so in some sense, the degree that makes sense in geopolitics turn the other cheek implemented. - it's not exactly turn the other cheek. because i do think that there's, you know, i think it is appropriate for israel to find the hamas members and, you know, either kill
'em or incarcerate them. like that's something that's
something has to be done 'cause they're just gonna
keep coming otherwise. but in addition to that, they
need to do whatever they can. there's some talk of establishing, for example, a mobile hospital. i'd recommend doing that. just making sure that, you know, there's food, water, medical, necessities, and just be over the top about it and be very transparent. so you can't claim it's a trick. like just put webcam on the
thing, you know, or 24/7. - deploy acts of kindness.
- yeah. conspicuous acts of kindness that, with that are unequivocal. meaning they can't be somehow, because hamas will then, their response will be, oh, it's a trick. therefore you have to
counter how it's not a trick. - this ultimately fights the broader force of hatred in the region. - yes. and i'm not sure who said it,
it's an apocryphal saying, but an eye for an eye
makes everyone blind. now that neck of the woods, they really believe in the
whole eye for an eye thing. but i mean, you really have, if you're not gonna just
outright commit genocide, like against an entire people, which obviously would not
be acceptable to really shouldn't be acceptable to anyone, then you're gonna leave
basically a lot of people alive who subsequently, you know, hate israel. so really the question is like, how, for every hamas member that you kill, how many did you create? and if you create more than you killed, you've not succeeded. that's the, you know,
the real situation there. and it's safe to say that if, you know, if you kill somebody's child in gaza, you've made at least a few hamas members who will die just to kill an israeli. that's the situation. so, but i mean, this is one of the most
contentious subjects one could possibly discuss. but i think if the goal ultimately is some sort of long-term peace, one has to be, look at this
from the standpoint of, over time, are there more or
fewer terrorists being created? - let me just linger on war. - yeah, war safe to say, wars always existed and always will exist. - [lex] always will exist, - always has existed
and always will exist. - i hope not. you think it always will? - there will always be war. this question of just how much war. and, you know, there's this, there's this sort of the
scope and scale of war. but to imagine that there
would not be any war in the future, i think would
be a very unlikely outcome. - yeah you talked about
the culture series. there's war even there. - yes. it's a giant war. the first book starts off
with a gigantic galactic war where trillions die, trillions. - but it still nevertheless
protects these pockets of flourishing. somehow you can have galactic war and still have pockets of flourishing. - yeah. i mean it's, i guess if we
are able to one day expand to, you know, the galaxy or whatever, there will be a galactic
war at some point. - the scale, i mean the scale
of war has been increasing, increasing, increasing. it's like a race between
the scale of suffering and the scale of flourishing. - yes. - a lot of people seem
to be using this tragedy to beat the drums of war and feed the military industrial complex. do you worry about this? the people who are rooting for escalation and how can it be stopped? - one of the things that did concerned me is that there are very
few people alive today who actually viscerally
understand the horrors of war, at least in the us. i mean, obviously there are
people in on the front lines in ukraine and russia who understand just how terrible war is, but how many people in
the west understand it? and my grandfather was in world war ii. he was severely traumatized. he was there, i think for
almost six years in the, in eastern north africa and italy. all his friends were
killed in front of him. and he would've died too,
except they randomly gave some, i guess, iq test or something. and he scored very high. and he was not an officer, he was a, i think a corporal or a
sergeant or something like that. because he didn't finish high school because he had to drop outta high school, 'cause his dad died and he had to work to support his siblings. so because he didn't graduate high school, he was not eligible for the officer corps. so, you know, he kind of got
put into the cannon fodder category, basically. but then, this, randomly
they gave him this test. he was transferred to british
intelligence in london. that's where he met my grandmother. but he had ptsd next
level, like next level. i mean, just didn't
talk, just didn't talk. and if you tried talking to him, he'd just tell you to shut up. and he won a bunch of medals. never bragged about it once,
not even hinted, nothing. i like found out about it, 'cause his military records were online. that's how i know. so he would say like, no way in hell do you wanna do that again. but how many people, now
he obviously he died, you know, 20 years ago or
longer, actually 30 years ago. how many people are alive
that remember world war ii? not many. - and the same perhaps applies
to the threat of nuclear war. - yeah. i mean, there
are enough nuclear bombs pointed at united states
to make the revel, the radio active revel balance many times. - there's two major
wars going on right now. so you talked about the
threat of agi quite a bit. but now as we sit here with the intensity of conflict going on, do
you worry about nuclear war? - i think we shouldn't
discount the possibility of nuclear war. it is a civilizational threat. right now i could be wrong, but i think the current
probability of nuclear war is quite low. but there are a lot of
nukes pointed at us. so, and we have a lot of
nukes pointed at other people. they're still there.
nobody's put their guns away. the missiles are still in the silos. - and the leaders don't
seem to be the ones with the nukes talking to each other. - no. there are wars which are tragic and difficult on a local basis. and then there are wars
which are civilization ending or has that potential. obviously global thermonuclear
warfare has high potential to end civilization, perhaps permanently. but certainly, you know, to severely wound and perhaps set back human progress by, you know, to the stone age
or something, i don't know. pretty bad. probably scientists and
engineers wanna be super popular after that as well. and like you got us into this mess. - so generally i think we
obviously wanna prioritize civilizational risks over
things that are painful and tragic on a local level,
but not civilizational. - how do you hope the war
in ukraine comes to an end? and what's the path, once again to minimizing human suffering there? - well, i think that
what is likely to happen, which is really pretty much the way it is, is that something very close
to the current lines will be how a ceasefire or truth happens. but, you know, you just
have a situation right now where whoever goes on the
offensive will suffer casualties at several times the rate
of whoever's on the defense. 'cause you've got defense in depth, you've got minefields,
trenches, anti-tank defenses. nobody has air superiority. 'cause the anti-aircraft missiles are really far better than the aircraft. like, they're far more of them. and so neither side has air superiority. tanks are basically death
drafts, just slow moving and they're not immune
to anti-tank weapons. so you really just have
long range artillery and infantry ranges. it's world war i all over
again with drones, you know, throwing old drones, some drones there. - which makes the long range artillery just that much more accurate and better and so more efficient at
murdering people on both sides. - yeah. so it's whoever is, you
don't wanna be trying to advance from either side, 'cause the probability of
dying is incredibly high. so in order to overcome defense in depth trenches and minefields, you
really need a significant local superiority in numbers. ideally combined alms
where you do a fast attack with aircraft, a
concentrated number of tanks and a lot of people. that's the only way you're
gonna punch through the line. and then you're gonna punch through and then not have
reinforcements just kick you right out again. i mean, i really recommend people read world war i warfare in detail. that's rough. i mean, the sheer number
of people that died there was mind boggling. - and it's almost impossible
to imagine the end of it that doesn't look like almost
exactly like the beginning in terms of what land
belongs to who and so on. but on the other side of a
lot of human suffering death and destruction of infrastructure. - yes. i mean, the thing that, the reason i, you know, proposed some sort of truce or peace a year ago was because i've predicted
pretty much exactly what would happen, which
is a lot of people dying for basically almost no changes in land. and this, the loss of the flower of ukrainian and russian youth. and we should have some
sympathy for the russian boys as well as the ukrainian boys, 'cause they, russian boys didn't ask to be on their front line, they have to be. so there's a lot of sons not
coming back to their parents, you know, and i think most
of 'em don't really have, they don't hate the other side. you know, it's sort of like
as this saying about, like, this saying comes from world
war i, it's like young boys who don't know each
other, killing each other on behalf of old men
that do know each other. the hell's the point of that? - so vladimir zelensky said that he's not, or has said in the past, he's not interested in
talking to putin directly. do you think he should? - yeah.
- sit down man to man, lead a leader and negotiate peace? - look, i think i would just recommend do not send the flower of ukrainian youth to die in trenches. whether he talks to putin
or not, just don't do that. whoever goes on the offensive will lose massive numbers of people and history will not
look kindly upon them. - you've spoken honestly
about the possibility of war between us and china in the long term. if no diplomatic solution
is found, for example, on the question of taiwan
and one china policy. - [elon] right. - how do we avoid the trajectory where these two superpowers clash? - well, it's worth reading that book on the, difficult to
pronounce, the thucydides trap, i believe it's called. i love war history. i like
inside out and backwards. there's hardly a battle
i haven't read about. and trying to figure out like
what really was the cause of victory in any
particular case as opposed to what one side or
another claim the reason. - both the victory and
what sparked the war? - yeah, yeah,
- [lex] the whole thing. - yeah, so that athens and
sparta is a classic case. the thing about the greek is they really wrote down a lot of stuff. they loved writing. you know, there are lots
of interesting things that happened in many parts of the world, but people didn't write down. (laughing) so we don't know what happened, or they didn't really
write with in detail. they just would say like,
we had a battle and we won. and like, well what?
can you add a bit more? the greeks, they really
wrote a lot. (laughing) they were very articulate
on, they just love writing. so, and we have a bunch of
that writing as preserved. so we know what led up
to the peloponnesian war between the spartan and athenian alliance. and we know that they saw it coming. i mean, the spartans didn't write, they also weren't very
verbose by their nature, but they did write, but
they weren't very verbose. they were terse. but the athenians and the
other greeks wrote a line and they were like, and
sparta was really kinda like the leader of greece. but athens grew stronger and stronger with each passing year. and everyone's like,
well that's inevitable that there's gonna be a clash
between athens and sparta. well, how do we avoid that? and they couldn't. they actually, they saw it coming and they still could
not avoid it. (laughing) so, you know, at some point
if there's, if one group, one civilization or country or whatever exceeds another, sort of, like, you know, the united states has been the
biggest kid on the block for, since i think around'90
from an economic standpoint. so the united states
has been the economic, most powerful economic
engine in the world, longer than anyone's been alive. and the foundation of war is economics. so now we have a situation in the case of china where the economy
is likely to be two, perhaps three times larger
than that of the us. so imagine you're the
biggest kid on the block for as long as anyone can remember. and suddenly a kid comes
along who's twice your size. - so we see it coming.
- [elon] yeah. - how is it possible to stop? is there some, let me
throw something out there, just intermixing of
cultures understanding. there does seem to be a giant cultural gap in understanding of each other. and you're an interesting case study because you are an american,
obviously you've done a lot of incredible manufacture
here in the united states, but you also work with china. - i've spent a lot of time in china and met with the leadership many times. - maybe a good question to ask is, what are some things about china that people don't understand
positive, just in the culture? what's some interesting things that you've learned about the chinese? - well, the sheer number of really smart, hardworking people in china is incredible. there are really, say like how many smart, hardworking people are there in china, there's far more of them
there than there are here. i think in my opinion. and they've got a lot of energy. so i mean, the architecture in china that's in recent years is far
more impressive than the us. i mean the train stations, the buildings, the high speed rail, everything, it's really far more impressive
than what we have in the us. i mean, i recommend
somebody just go to shanghai and beijing, look at the buildings and go to, you know, take the
train from beijing to xian, where you have the terracotta warriors. china's got an incredible
history, very long history. and you know, i think arguably the, in terms of the use of language
from a written standpoint, sort of one of the oldest, perhaps the oldest written language. and then china, people
did write things down. so now china historically has always been, with rare exception,
been internally focused. they have not been inquisitive. they've fought each other. there've been many, many civil wars. in the three kingdoms war, i believe they lost about
70% of their population. so, and that, so they've had brutal internal wars, like civil wars that make the us civil war look small by comparison. so i think it's important to appreciate that china is not monolithic. we sort of think of like china as this sort of one entity of one mind. and this is definitely not the case from what i've seen, and i think most people who
understand china would agree. people in china think about china 10 times more than they think about anything outside of china. so it's like 90% of
their consideration is, you know, our is internal. - well, isn't that a really positive thing when you're talking
about the collaboration and a future piece between superpowers, when you're inward facing, which is like focusing
on improving yourself versus focusing on...
- [elon] yeah. - "improving" others
through military might. - the good news, the
history of china suggests that china is not inquisitive. meaning they're not gonna go out and invade a whole bunch of countries. now they do feel very strongly, you know, so that's good. i mean, 'cause a lot of
very powerful countries have been inquisitive. the us is one of the,
also one of the rare cases that has not been inquisitive. like after world war ii, the us could have basically taken over the world in any country. like, we've got nukes,
nobody else has got nukes. we don't even have to lose soldiers. which country do you want? and the united states could
have taken over everything. oh wait, at will. and it didn't, and the
united states actually helped rebuild countries. so it helped rebuild europe,
you know, helped rebuild japan. this is very unusual behavior,
almost unprecedented. you know, the us did
conspicuous acts of kindness like the berlin airlift, you know? and i think, you know, it's always like, well, america's done bad things. well, of course america's done bad things. but one needs to look at
the whole track record. and just generally, you
know, one sort of test would be how do you treat
your prisoners at war? or let's say, you know, no offense to the russians, but
let's say you're in germany, it's 1945, you got the
russian army coming one side and you got the french, british, and american army's coming the other side. who would you like to
be to surrender it to? like, no country is like morally perfect, but i recommend being a
pow with the americans. that would be my choice
very strongly. (laughing) - in the full menu of pows. - very much so. and in fact, wernher von braun, took, you know, smart guy was like, we've got to be captured by the americans. and in fact, the ss was under
orders to execute von braun and all of the german rocket initiatives. and they narrowly escaped their ss. they said they were going
out for a walk in the woods. they left in the middle
of winter with no coats. and they ran like, with no
food, no coats, no water. and just ran like hell and ran west. and by sherlock, they, i
think his brother found like a bicycle or something, and then just cycled west as fast as he couldn't have found a us patrol. so anyway, that's one way
you can tell morality is. where do you wanna be pow? (laughing) it's not fun anywhere, but some places are
much worse than others. so anyways, so like america has been, while far from perfect,
generally a benevolent force, and we should always be self-critical and we try to be better. but anyone with have brain knows that. so i think there are some, in this way china and the
united states are similar. neither country has been
acquisitive in a significant way. so that's like a, you know,
a shared principle i guess. now china does feel very
strongly about taiwan. they've been very clear
about that for a long time. you know, from their standpoint, it would be like one of
the states is, you know, not there like hawaii
or something like that. but more significant
than hawaii, you know? and hawaii is pretty significant for us. so they view it as really the, that there's a fundamental part of china. the islander formosa not taiwan, that is not part of china, but should be. and the only reason it hasn't been is 'cause the us pacific fleet. - and as their economic power grows and as their military power grows, the thing that they're clearly saying is their interest will, you
know, clearly be materialized. - yes. china has been very clear that
they'll incorporate taiwan peacefully or militarily, but that they will incorporate it from their standpoint is 100% likely. - you know, something you
said about conspicuous acts of kindness. as a geopolitical policy,
it almost seems naive, but i'd venture to say
that this is probably the path forward, how you avoid most wars. just as you say it, it sounds naive, but it's kind of brilliant. if you believe in the goodness of underlying most of human nature. it just seems like it
conspicuous acts of kindness can reverberate through the populace of the countries involved. - yeah, well.
- [lex] and deescalate. - absolutely. so after world war i,
they made a big mistake. you know, they basically tried to lump all of lamb on germany and, you know, settled germany with impossible reparations. and, you know, really, there was a lot of, there was a fair, quite a bit of blame to go around for world war i, but they try to, you know,
put it all in germany. and that was, that laid
the seeds for world war ii. so it's a lot of people
were not just hitler, a lot of people felt wronged and they wanted vengeance and they got it. - [lex] people don't forget. - yeah, you know, you
kill somebody's father or mother or son, daughter,
they're not gonna forget it. they'll want vengeance. so after world war ii, they're like, well, the treaty of versailles was
a huge mistake in world war i. and so this time, instead of, you know, crushing the losers, we're actually gonna help
'em with the marshall plan and we're gonna help rebuild germany. we're gonna help rebuild
or, you know, austria and the other, you know,
italy and whatnot, so. and that was the right move. - there's a, it does feel
like there's a profound truth to conspicuous acts of kindness
being an antidote to this. - something must stop the
cycle of reciprocal violence. something must stop it or it will, you know, it'll never stop. just eye for an eye, tooth for tooth, limb for a limb, life for
a life, forever and ever - to escape briefly, the darkness was some
incredible engineering work. xai just released grok ai assistant that i've gotten a chance to play with. it's amazing on many levels. first of all, it's amazing
that a relatively small team in a relatively short amount
of time was able to develop this close to state-of-the art system. another incredible thing
is there's a regular mode and there's a fun mode. - yeah. i guess i'm to blame
for that one. (laughing) - i wish, first of all, i wish everything in life had a fun mode. - yeah. - there's something compelling beyond just fun about the fun mode interacting with a large language model. i'm not sure exactly what it is 'cause i only have had a little
bit of time to play with it. but it just makes it more interesting, more vibrant to interact with the system. - yeah, absolutely. our ai grok is modeled after the hitchhiker's
guide to the galaxy, which is one of my
favorite books, which is, it's a book on philosophy,
disguises book on humor. and i would say that
is that forms the basis of my philosophy, which is that we don't know the meaning of life, but the more we can expand the scope and scale of consciousness, digital and biological, the
more we're able to understand what questions to ask about the
answer that is the universe. so i have a philosophy of curiosity. - there is generally a
feeling like this ai system has an outward looking, like the way you are, like sitting with a good friend
looking up at the stars, like the asking pod head, like
questions about the universe, wondering what it's all about. the curiosity you talk about. there's a sense no matter how
mundane the question i ask, there's a sense of cosmic
grandeur to the whole thing. - well we are actually working
hard to have engineering math, physics answers
that you can count on. so for the other sort of ais out there that these so-called
large language models, i've not found the
engineering to be reliable. and the hallucination, it unfortunately hallucinates most when you at least want it to hallucinate. so when you're asking
important difficult questions, that that's where it tends
to be confidently wrong. so we're really trying hard to say, okay, how do we be as grounded as possible so you can count on the results? trace things back to
physics first principles, mathematical logic. so underlying the humor is an aspiration to adhere to the truth of the universe as closely as possible. - [lex] that's really tricky.
- it is tricky. so that's why, you know, there's always gonna be
some amount of error, but we want to aspire to be as truthful as possible about the answers with acknowledged error. so that there was always, you
don't wanna be confidently wrong, so you're not
gonna be right every time, but you wanna minimize how often you're confidently wrong. and then like i said, once
you can count on the logic as being not violating physics, then you can start to build on that, to create inventions, like
invent new technologies. but if you can't, if you cannot count on the foundational
physics being correct, obviously the inventions
are simply wishful thinking, you know, imagination
land, magic basically. - well, as you said, i
think one of the big goals of xai is to understand the universe. - yes. that's how simple
three word mission. (laughing) - if you look out far into the future, do you think on this level of physics, the very edge of what we
understand about physics, do you think it will make discoveries, sort of the sexiest discovery
of them as we know now, sort of unifying general
relativity and quantum mechanics. so coming up with a theory of everything, do you think it could push
towards that direction? almost like theoretical
physics discoveries. - if an ai cannot figure out new physics, it's clearly not equal to humans, let alone nor has surpassed humans, 'cause humans have
figured out new physics. they're just, you know,
physics is just understanding, you know, deepening wants
insight into how reality works. and then there's engineering
which is inventing things that have never existed. now the range of
possibilities for engineering is far greater than for physics. 'cause you know, once
you figure out the rules of the universe, that's it. you've discovered things
that already existed. but from that you can
then build technologies with that are really almost limitless in the variety and cap- you know? it's like once you understand the rules of the game properly, and we do, you know, with current physics, we do
at least at a local level, understand how physics works very well. where our ability to predict
things is incredibly good. like quantum mechanics is, the degree to which quantum
mechanics can predict outcomes is incredible. that was my heart hardest
class in college by the way. my senior quantum
mechanics class was harder than all of my other classes put together. - to get an ai system,
a large language model to reliably, be as reliable
as quantum mechanics and physics is very difficult. - yeah. you have to test any
conclusions against the ground, truth of reality. reality is the ultimate judge. like physics is the law, everything else is a recommendation. i've seen plenty of people
break the laws made by man, but none break the laws made by physics. - yeah, it's a good test actually. if this llm understands and matches physics, then
you can more reliably trust whatever it thinks about the current state of politics in some sense. - and it's also not the case currently that even that its internal
logic is not consistent. so, especially with these, with the approach of like just
predicting a token, predict, token, predict, token,
it's like a vector sum. you know, you're summing
up a bunch of vectors, but you can get drift. so as those, a little bit of error, a little bit of error adds up. and by the time you are
many tokens down the path, it doesn't make any sense. - so it has to be somehow
self-aware about the drift. - it has to be self-aware about the drift, and then look at the thing
as a gestalt as a whole and say it does it have
coherence as a whole. so, you know, when authors write books that they will write the
book and then they'll go and revise it, you know,
taking into account, you know, all the, the
end and the beginning and the middle and rewrite
it to achieve coherence so that it doesn't end up
in a nonsensical place. - [lex] maybe the process
of revising is what... - yeah. - reasoning is, and then
that's the process of revising is how you get closer and closer to truth. maybe you, like, at least
i approached it that way. you just say a bunch of bullshit first and then you get it better. you start a bullshit and then you get... - create a draft and then
you iterate on that draft until it has has coherence, until it all adds up basically. - so another question
about theory of everything, but for intelligence, do
you think there exists as you're exploring this with xai, creating this intelligence system, do you think there is a
theory of intelligence where you get to understand what, like what is the i in agi and what is the i in human intelligence? - there's no i in team
america, oh wait, there is. (both laughing) - no, it's gonna be stuck in my head now. (both laughing) yeah. there's no me and
whatever in quantum mechanics. oh, wait.
(elon laughing) i mean is that part of the
process of discovering, understanding the universe is
understanding intelligence? - yeah. yeah. i think we need to
understand intelligence, understand consciousness. i mean there are some sort of
fundamental questions of like, what is thought? what is emotion? - [lex] yeah.
- is it really just one atom bumping into another atom? it feels like something more than that. so i think we're probably
missing some really big things, like some really big things. - something that'll be
obvious in retrospect. - [elon] yes.
- like there's a giant, like you put the whole
consciousness, emotion. - well, some people
would quote like a soul, you know, in religion it'd be a soul. like you feel like you're you, right? i mean, you don't feel like
you're just a collection of atoms, but on what
dimension does thought exist? what dimension does do emotions exist? we feel them very strongly. i suspect there's more to it
than atoms bumping into atoms. - and maybe ai can pave
the path to the discovery of whatever the hell that thing is. - yeah. what is consciousness? like what are, when you put the
atoms in a particular shape, why are they able to form thoughts and take actions, that and feelings. - and even if it is an illusion, why is this illusion so compelling? - yeah. like how do... - why does this illusion exist? - [lex] yeah.
- on what plain does this illusion exist? - [lex] yeah.
- and sometimes i wonder is, you know, either perhaps
everything's conscious or nothing is conscious, one of the two. - i like the former, everything conscious just seems more fun. - it does seem more more fun. yes. but we're composed of atoms and those atoms are composed
of quarks and leptons. and those quarks and
leptons have been around since the beginning of the universe. - the beginning of the universe. - right. what seems to be the
beginning of the universe. - the first time we talked,
you said what you would, which is surreal to think that this discussion was
happening is becoming a reality. i asked you what question
would you ask an agi system once you create it? and you said, what's outside
the simulation is the question. and good question. - [elon] yeah.
- but it seems like with grok you started to literally,
the system's goal is to be able to ask such questions, to answer such questions and to ask such questions. - where are the aliens?
- [lex] where are the aliens? - that's one of the, like
the very paradox question. a lot of people have asked me if i've seen any evidence
of aliens and i haven't, which is kind of concerning. 'cause then i think
what i'd probably prefer to at least have seen some
archeological evidence of aliens, to the best of my knowledge,
there is no proof. i'm not aware of any evidence of aliens. if they're out there, they're very subtle. we might just be the only consciousness, at least in the galaxy. and if you look at say
the history of earth, forensically the archeological record, earth is about four and
a half billion years old. civilization as measured
from the first writing is only about 5,000 years old. we have to give some credit
there to the ancient sumerians who aren't around anymore. i think it was a archaic, pre cuneiform was the first
actual symbolic representation, but only about 5,000 years ago. i think that's a good date for when word say civilization started. that's one millionth of earth's existence. so civilization has been around, it's really a flash in the pan so far. and why have we, why
did it take so long for, you know, four and a half billion years? for the vast majority of
the time, there was no life. and then there was archaic there bacteria for a very long time. and then, you know, you had
mitochondria get captured, multicellular life. differentiation into plants and animals. life moving from the oceans to land. mammals, higher brain functions. and the sun is expanding slowly, but it will overheat, it will heat the earth up
at some point in the future, boil the oceans and and
earth will become like venus. where near life, life as
we know it is impossible. so if we do not become multi-planetary and ultimately go beyond our solar system, annihilation of all life
on earth is a certainty. a certainty. and it could be as little as
on the galactic timescale, half a billion years. you know, it's long
time by human standards, but that's only 10% longer than earth has been around at all. so if life had taken 10%
longer to evolve on earth, it wouldn't exist at all. - we got a deadline
coming up. better hurry. but that said, as you said,
humans intelligent life on earth developed a lot of cool
stuff very quickly, - so...
- [elon] yes. - it seems like becoming a multi-planetary is almost inevitable. unless we destroy this thing.
- we need to do it. i mean, it's not, i mean,
i suspect that there, if we are able to go out there and explore other star systems that we, there's a good chance
we find a whole bunch of long dead one planet civilizations, then have made it past their home planet. - that's so sad.
- yeah. - it's sad. also fascinating. - i mean, there are various explanations for exploring paradox. and one is the sort of,
they're these great vultures, which civilizations don't pass through. and one of those great filters is do you become a multi-plan
civilization or not? and if you don't, it's
simply a matter of time before something happens on your planet. you know, either natural or manmade that causes us to die out. like the dinosaurs, where are they now that didn't have
spaceships, so. (laughing) - i think the more likely thing is, 'cause just to empathize with the aliens that they found us and
they're protecting us and letting us be.
- i hope so. nice aliens. - just like the tribes in the amazon, the uncontacted tribes,
we're protecting them. that's what... - that would be a nice explanation. - or you could have like a, what was it? i think andrej karpathy said, it's like the ants in the amazon
asking where's everybody? - well, they do run into
a lot of other ants. - [lex] that's true.
- they have these ant wars. - (laughing) sounds like a good tv show. - yeah, they literally have these big wars between various ants. - yeah, maybe i'm just
(both laughing) dismissing all the
different diversity events. - you should listen to that warner herzog talking about the jungle. it's really hilarious. have you heard it? - no, i have not.
- [elon] it's awesome. - but warner herzog is a way... (both laughing) - you should play it for the, you know, as an interlude in the, it's on youtube. it's awesome.
(both laughing) - [lex] i love him so much.
- yeah, he's great. - was he the director of "happy people: life in the taiga"? i think also - he did that bear documentary, - [lex] the bear documentary. - and this thing about penguins. - yeah.
(both laughing) the analysis, the
psychoanalysis of penguins. - yeah, the penguins like
headed for like mountains like that are like 70 miles away. and penguin has just
had for doom basically. - well, he had a cynical take. i have a, he could be
just a brave explorer and there'll be great stories told about him amongst the penguin population for many centuries to come. (laughing) what were we talking about? okay. - [elon] penguins.
(both laughing) - yeah, so aliens, i mean, i don't know. look, i think the smart
move is just, you know, this is the first time
in the history of earth that it's been possible for life to extend beyond earth. that window is open. now, it may be open for a long time or it may be open for a short time and it may be open now
and then never open again. so i think the smart move here is to make life multiplanetary
while it's possible to do so. we don't wanna be one of those lame one planet civilizations
that just dies out. - no, those are lame.
- yeah, lame. self-respecting civilization
would be one planet. - there's not gonna be a wikipedia entry for one of one of those. and pause, the spacex
have an official policy for when we meet aliens. - no. (laughing) - that seems irresponsible. (both laughing) - i mean, look, if i see
the slightest indication that there are aliens, i will immediately post on
x platform anything i know. - it could be the most liked
reposted post of all time. - yeah, i mean look, we have more satellites up there right now than everyone else combined. so, you know, we'd know, we
know if we've got a maneuver around something, and we not don't have to
maneuver around anything. - if we go to the big
questions once again, you said you've, you're with einstein that you
believe in the god of spinoza. - (laughing)yes. - so, you know, that's a view that god is like the universe and is reveals himself
through the laws of physics or as einstein said, through the lawful harmony of the world. - yeah, i would agree
that god of the simulator or whatever the supreme being or beings, reveal themselves through the physics. you know, they're
creatives of this existence and it's incumbent upon
us to try to understand more about this wondrous creation. - like who created this thing,
who's running this thing? like embodying it into a singular question with a sexy word on top of it is like focusing the
mind to understand it. it does seem like there's a,
again, it could be an illusion. it seems like there's a purpose that there's an underlying
master plan of some kind. and it seems like... - there may not be a master plan. in the sense, so there's, like, maybe an interesting
answer to the question of determinism versus free will is that if we are in a simulation, the reason that these higher
beings would hold a simulation is to see what happens. so it's not, they don't know what happens, otherwise they wouldn't
hold the simulation. so when humans create a simulation, so it's spacex and tesla, we create simulations all the time, especially for the rocket. you, you know, you have to
run a lot of simulations to understand what's gonna happen because you can't really test the rocket until it goes to space,
and you want it to work. so you have to simulate
subsonic, transonic, supersonic, hypersonic ascent, and then coming back super high heating and orbital dynamics. all this has gotta be simulated. so, 'cause you don't get
very many kicks at the can. but we run the simulations
to see what happens. not if we knew what happens,
we wouldn't run the simulation. so if there's, so whoever
created this existence, is they're running it because they don't know
what's gonna happen. not because they do. - so maybe we both played diablo, maybe diablo was created to
see if it druid your character could defeat uber lilith at the end. they didn't know. - well, the funny thing
is the uber lilith title is hatred incarnate. - [lex] yeah.
- and right now, i guess, you're gonna ask the diablo team, but it's almost impossible
to defeat hatred in the eternal realm. - yeah, you've streamed
yourself dominating tier 100 nightmare dungeons and still... - i can cruise through
tier 100 nightmare dungeons like a stroll in the park. - and still you're defeated by hatred. - yeah, i can, there's the sort of, i guess maybe the second
hardest boss is duriel, duriel can't even scratch the paint. so i killed duriel so many times , and every other boss in
the game, all of 'em, kill 'em so many times it's easy. but uber lilith, otherwise
known as hatred incarnate, especially if you're a druid
and you have no ability to go and to be vulnerable, there are these random death
waves that come at you. and i'm pretty, you know, i really am 52, so my reflex is not what they used to be, but i have a lifetime
of playing video games. at one point i was, you know, maybe one of the best
quake players in the world. i actually won money in
what i think was the first paid esports tournament in the us. we're doing four person quake tournaments. and we came second, i was the
second best person on the team and the actual best person, we're actually winning, we would've come first except
the best person on the team, his computer crashed prior to the game. so we came second. but i got money for it and everything. so like, basically i got
skills, you know, albeit, you know, no spring chicken these days. and to be totally frank,
it's driving me crazy, trying to beat lilith as a druid. basically trying to beat,
trying to beat hatred incarnate in the eternal realm. - [lex] as a druid.
- as a druid. and if you, if you, (laughing) this is really (indistinct),
lemme tell you. - i mean, the challenge
is part of the fun. i have seen directly like you're actually like a world class
incredible video game player. - [elon] yeah.
- and i think diablo, so you're just picking up a new game and you're figuring out it's fundamentals. you're also with the paragon board and the build are not somebody like me who perfectly follows whatever
they suggest on the internet. you're also an innovator there. - [elon] yeah. - which is hilarious to watch. it's like a mad scientist just trying to figure out the paragon board and the build and the, you know. is there some interesting
insights there about, if somebody's starting as a
druid, do you have advice? - i would not recommend playing a druid in the eternal realm. right now, i think the
most powerful character in the seasonal realm is the sorcerer with the lightning balls. the sorcs have huge balls in the seasonal. - well yeah, that's what they say. - so sorc have huge balls. they do, huge balls of lightning. - i'll take you word for it. - and it's actually,
in the seasonal realm, you can, it's like pretty easy to beat uber lilith with the, because she got these vampiric powers that out fire damage and increase your defense and whatnot. so, but really quite easy
to defeat hatred seasonally, but to defeat hatred
eternally, very difficult. almost impossible. it's
totally impossible. it seems like this is a
metaphor for life, you know? - i like the idea that elon musk, because i saw, i was
playing diablo yesterday and i saw 100, level 100 drew just run by, i will never die and
then run back the way. - and it was, there's just some, this metaphor's kind
of hilarious that you, elon musk is fighting hatred, restlessly fighting hatred
in this demonic realm. - [elon] yes.
- it's hilarious. i mean it's pretty hilarious. - no, it's absurd.
(both laughing) really, it's exercise and absurdity, and it makes me wanna pull my hair out. - yeah. what do you get
from video games in general? is there for you personally. - i mean, it's, i don't known it's, it calms my mind. i mean, you sort of killing
the demons in a video game calms the demons in my mind. yeah. if you play a tough video game, you can get into like a state of flow, which is very enjoyable. but the admittedly it
needs to be not too easy, not too hard, kinda in the goldilock zone. and i guess you generally wanna feel like you're progressing in the game. so a good video, and there's also beautiful
art, engaging storylines, and it's like an amazing
puzzle to solve, i think. and so it's like solving the puzzle. - elden ring the greatest
game of all time. i still haven't played it, but you. - elden ring is definitely a candidate for best game ever. top five for sure. - i think i've been scared how hard it is or how hard i hear it is, so. but it is beautiful. - elden ring is, feels like
it's designed by an alien. there's a theme to this
discussion. in what way? - it's so unusual. it's incredibly creative
and the art is stunning. i recommend playing it
on a big resolution, pie dynamic raise tv even,
doesn't need to be a monitor, just the art is incredible. it's so beautiful and it's so unusual. and each of those top
bus battles is unique. like, it's like a unique puzzle to solve. each one is different. and the strategy you use to solve one battle is different
from another battle. - that said, you said druid and enternal against uber lilith is the
hardest boss battle you've ever? - correct. that is currently the, and i've played a lot of video games. that's my primary recreational activity. and yes, beating hatred
in the enternal realm is the hardest boss battle in life and in the video game. - [lex] metaphor on top of metaphor. - i'm not sure it's possible, but it's, i do make progress. so then i'm like, okay,
i'm making progress. maybe if i just tweak that
paragon board a little more, i can do it. i could just dodge a few
more waves, i can do it. - well the simulation is
created for the purpose of figuring out if it can be done. and you are just a cog in that simulation, in the machine of the simulation. - yeah it might be.
(lex laughing) - i have a feeling that at least, i think...
- [lex] it's doable. - it's doable, yes. - [lex] well that's the human
spirit right there to believe. - yeah. i mean, did prompt me to think
about just hate in general, which is, you wanna be
careful of one of those things where you wish for
something that sounds good, but if you get it, it's
actually a dystopian situation. so, you know, you could
sort of run a, sort of, have heart to sort of like, if you wish for world peace, sounds good, but how is it enforced? and at what cost is, at
what cost eternal peace? it might actually be worse
to have eternal peace because of what that would entail. the suppression of everyone. it might be the suppression of progress. it might be an ossified
society that never changes. what i'm saying is that
there is an argument that you, that if that, if you wish for no war, you should be careful what you wish for. because what's required in
order for there to be no war? might be worse than a little war. - yeah, the things we'll
consider to be flaws of human civilization might
be a necessary components for whatever optimal looks like. i mean this, do you worry about
ai, agi enabling a dystopian state of this nature, whether
it's 1984 with surveillance and fear, or brave new
world with pleasure, and what is it? lots of sex, but no deep human experience. - you know, there's actually
a real drug called soma. - [lex] there is?
- yes. (both laughing) - i'm like...
- [lex] lemme take notes. - did they actually... - does grok know about it? no, it probably will
go to brave new world. - we could ask it.
- what would be the question? - tell us about the drug called soma. - tell us about the drug called soma. it's gonna go to brave new world, i think. - well there's a real drug as well. - searching for soma drug information. (both laughing) - oh boy. ah, soma, the one to drug,
by the way, i'm in fun mode. - oh, okay. fun. (laughing) yeah, so it's talking about the real drug. - [lex] oh, it's a muscle relaxant that's been around since the 1950s. it's used to treat muscle
pain and discomfort. and it does this by altering the activity between neurons and the spinal cord. now, soma isn't just a pain reliever, it's also a bit of a sedative. it can make you feel a bit drowsy. so it's not something to take if you're planning on
operating having machinery or trying to solve complex math equations. and it goes on and on. in conclusion, soma's a
drug that can be helpful for those dealing with muscle pain, but it's not without its risks. it's pretty good. oh, wait a minute. - [elon] that's the real
drug it's talking about. - [lex] it's the real drug.
- [elon] yeah. - [lex] and of course the fun
is always use it responsibly in accordance with the advice
of a healthcare professional. and remember in the end, the universe is probably just
a giant computer simulation. so don't take anything too seriously. i love it. i love it.
(elon laughing) i love it. i love it. - yeah. so. - oh, and also really nicely includes like the tweets post on x
that are relevant to this, which is really cool. - [elon] yeah. - and that integrate everything together. - [elon] yeah.
- like the interface is one of the really cool things here. - yeah. seems done a great job. (laughing) so, and you say like, you
could say like, no, i mean, soma from brave new world. 'cause it thinks you mean, you mean soma the real drug. - yeah.
- [elon] no, i mean, soma from brave new world. - it's gonna be like,
oops, i'm really sorry. i'm really surprised it
went to the real drug first. ah, my apologies. i was
thinking of the muscle relaxant. soma for band new world is a
different beast altogether. now this huxley's dystopian novel, soma is a drug that plays a central role in society, depicted in the book. it's a mood altering,
hallucinogenic drug that's used to keep the population docile and content. in the novel to use
soma in brave new world raises some interesting questions about the nature of happiness and the role of drugs in society. oh, man!
- exactly. is it better to live in a world where everyone is happy all the time, even if that happiness is artificial? good question. this is what i mean. like, do you want, do
you wish for world peace and happiness all the time? are you sure? because that might be a society
that is essentially sterile and ossified that never changes, that is ultimately doomed. - this kind of tension
between darkness and light. - it's, this is really
a very good summary. it really gets to the point. this is not simply
regurgitating a brave new world. it's actually getting
to the salient element of soma as a drug. do you actually wanna be in a situation where everyone is happy all the time, even though it's artificial? or is it better to confront
the challenges of life and experience the full
range of human emotions, even if it means experiencing
pain and suffering? - for those listening, by the way, elon just read directly from grok, which is a really nice,
kind of insightful, philosophical analysis
of the tension here. interesting. - it pretty much nails it. in conclusion, soma from brave
new world is fictional drug that's used to explore some
deep philosophical questions about the nature of happiness and the role of drugs in society. it's a powerful symbol of
the dangers of using drugs to escape from reality and the importance of
confronting the challenges of life head-on. nailed it. and the crazy thing is like, you know, we do have a real drug called soma, which kind of is kind of
like the drug in the book. and i'm like, did they, they must've named it after that soma. - probably, probably,
- yeah. soma the real drug is quite
effective on back pain. - so, you know about this drug? - i've taken it.
- it's fascinating. okay. - 'cause i had like a,
you know, squashed disc in my c five c six. - so it takes the physical
pain away, but soma here is... - it doesn't completely, it
reduces the amount of pain you feel, but at the
expense of mental acuity. it dells your mind. just the drug in the book. - just like the drug in the book. - and hence the trade off. the thing that seems like utopia could be a dystopia after all. - yeah, and actually i was
telling a friend of mine saying like, would you really want there to be no hate in the world? like, really none? like, i wonder why hate evolved. i'm not saying we should
amplify hate, of course, i think we should try to minimize it. but none at all, hmm. there might be a reason for hate. - and suffering. i mean, it's really complicated
to consider that some amount of human suffering is necessary
for human flourishing. - is it possible to appreciate the highs without knowing the lows? - and that all is summarized there in a single statement from grok. okay.
- no highs, no lows, who knows? - that's almost the poem. it seems that training llms efficiently is a big focus for xai. what's the, first of all, what's the limit of what's possible in terms of efficiency? there's this terminology of
useful productivity per watt. like what have you learned from
pushing the limits of that? - well, i think it's helpful. the tools of physics are very powerful and can be applied, i think to almost any, really any arena in life. it's really just critical thinking. for something important you need to reason from first principles
and think about things in the limit one direction or the other. so in the limit, even
at the kardashev scale, meaning even if you harness
the entire power of the sun, you'll still care about
useful compute for what? so that's where, i think probably where things are headed
from the standpoint of ai is that we have a silicon shortage now that will transition to
a voltage transformer shortage in about a year. ironically, transformers,
for transformers. you need transformers to run transformers. - somebody has a sense
of humor in this thing. (both laughing) - i think. yes. fate loves irony, ironic humor and ironically funny outcome seems to be often what fate wants. - humor is all you need. i
think spice is all you need. somebody posted. - yeah, but yeah. so we're, i have silicon shortage today, a voltage step down transformer, shortage probably in about a year. and then just electricity
shortages in general in about two years. i gave a speech for the sort of world gathering of utility companies, electricity companies, and i said, look, you
really need to prepare for a traveling of electricity demand, because all transport
is gonna go electric, with the ironic exception of rockets. and heating will also go electric. so energy usage right now is roughly one third, very rough terms. one third electricity, one third transport, one third heating. and so in order for
everything to go sustainable to go electric, you need to triple electricity output. so i encourage the utilities to build more power of plants and also to probably have, well, not probably, they should
definitely buy more batteries because the grid currently
is sized for real-time load, which is kind of crazy. 'cause you know, that means
you got a size for whatever the peak electricity demand
is like the worst second or the worst day of the year. or you can have a brownout or a blackout, and you're at that crazy blackout for several days in austin. so because there's almost no buffering of energy in the grid. like if you've got a hydro power plant, you can buffer energy, but
otherwise it's all real time. so with batteries, you can
produce energy at night and use it during the
day so you can buffer. so i expect that there will be
very heavy usage of batteries in the future, because the peak to trough
ratio for power plants is anywhere from two to five, you know, so it's like lowest
point to highest point. - so like batteries
necessary to balance it out? and then, but the
demand, as you're saying, is going to grow, grow, grow, grow. - [elon] yeah. - and part of that is the compute? - yes. yes. i mean, electrification, i mean, electrification of transport and electric heating will
be much bigger than ai. - [lex] at least in the short term? - in the short term. but even for ai, you really have a growing
demand for electricity, for electric vehicles, and a
growing demand for electricity for, to run the computers for ai. and so this is obviously leading, can lead to a electricity shortage. - how difficult is the problem of, in this particular case, maximizing the useful
productivity per watt for training (indistinct) nuts? like, this seems to be
really where the big problem we're facing that needs to be solved, is how to use the power efficiently. like what you've learned
so far about applying this physics first principle
reasoning in this domain. how difficult is this problem? - it will get solved. it's the question of how
long it takes to solve it. so at various points, there's limit, some kind of limiting factor to progress. and with regard to, i'm saying like right
now the limiting factor is silicon chips, and that
will, we're gonna then have more chips than we can actually plug in and turn on probably in about a year. the initial constraint being literally voltage step down transformers, because you've got power
coming in at 300,000 volts, and it's gotta step all the way down eventually to around 0.7 volts. so it's a very big amount of, you know, the voltage step down is gigantic. so, and the industry is
not used to rapid growth. - okay, let's talk about
the competition here. you've shown concern about google and microsoft with open ai developing agi, how can you help ensure with xai and tesla ai work that it
doesn't become a competitive race to agi, but that is a collaborative
development of safe agi? - well, i mean, i've been pushing for some kind of regulatory
oversight for a long time. i've been somewhat of a
cassandra on the subject for over a decade. i think we wanna be very
careful in how we develop ai. it's a great power, and with great power comes
great responsibility. i think it would be wise for us to have at least an objective
third party who can be like a referee that can go in and understand what the
various leading players are doing with ai. and even if there's no
enforcement ability, they should, they can at
least voice concerns publicly. you know, geff hinton, for example, left google and he voiced strong concerns. but now he's not at google anymore, so who's gonna voice the concerns? so i think there's, like, i know tesla gets a
lot of regulatory oversight on the automotive front. i mean, we're subject to i think, over a hundred regulatory agencies domestically and internationally. so it's a lot. you could fill this room
with the all regulations that tesla has to adhere
to for automotive. same is true in, you know, for rockets and for, you know, currently
the limiting factor for spacex, for starship launch is
regulatory approval. the fa has actually given their approval, but we're waiting for fish and wildlife to finish their analysis and give their approval. that's why i posted i want
to buy a fish license on, (laughing) which also refers to
the monte python sketch. - [lex] yeah.
(laughing) - like why do you need
a license for your fish? i don't know.(laughing) but according to the rules, i'm told you need some sort
of fish license or something. we effectively need a fish
license thrown to rock it. (laughing) and i'm like, wait a second. how did the fish come into this picture? - [lex] yeah. - i mean some of the
things like that it's, i feel like are so absurd that i wanna do like a comedy sketch and flash at the bottom. this is all real. this is
actually what happened. you know, one of the things
that was a bit of a challenge at one point is that they were worried about our rocket hitting a shark. and, now the ocean's very big, and how often do you see sharks? not that often, you know? as a percentage of ocean surface area sharks basically are zero. and so then we will, then we said, well, how will we
calculate the probability of telling a shark? and they're like, well, we
can't give you that information because they're worried about shark hunt, shark fin hunters going
and hunting sharks. and i said, well, how are we supposed to, we're on the horns of a dilemma then. (laughing) then they said, well,
there's another part of fish and wildlife that can do this analysis. i'm like, well, why don't
you give them the data? like, we don't, they
don't, we don't trust them. like, excuse me, you don't, but they're literally in your department. but again, this is actually what happened. and then can you do an nda or something? (laughing) eventually they managed to
solve the internal quandary, and indeed the probability
of us hitting a shark is essentially zero. then there's another organization that i didn't realize
existed until, you know, a few months ago that cares about whether we would potentially hit a
whale in international waters. now again, you look at the surface of the, look at the pacific and say,
what percentage of this, the pacific consists of whale. like he'll give you a big picture and like point out all the
whales in this picture. and i'm like, i don't see any whales. (laughing) it's like basically 0%. and if our rocket does hit a whale, which is extremely
unlikely beyond all belief, that is the, fate had it, that whale has some seriously bad luck. you know, it's the least
lucky whale ever. and... - i mean, this is quite absurd. the bureaucracy, the bureaucracy of this, however it emerged. - yes, well, i mean, one of the things that's pretty well
there's for launching out of vanderberg in california, we had to, they were worried about seal procreation, whether the seals would be
dismayed by the sonic booms. now there've been a lot of
rockets launched outta vanderberg and the seal population
has steadily increased. so if anything, rocket
booms are an aphrodisiac based on the evidence, if you
would correlate rock launches with a seal population. nonetheless, we were
forced to kidnap a seal, strap it to a board, put
it headphones on the seal and place sonic boom sounds to it to see if it would be distressed. this is an actual thing that happened. this is actually real. i have pictures. (both laughing) - i would love to see this. yeah. i mean's sorry, this is
a seal with headphones. (elon laughing)
- yes, it's a seal with headphones,
strapped to a board, and like the, okay, now the amazing part is how calm the seal was. - [lex] yeah.
- because if i was a seal, i'd be like, this is the end. (laughing) they're definitely gonna eat me. how will the seal, when the seal goes back to other, you know, seal friends, how's they gonna explain that? - [lex] they're never gonna believe him. - never gonna believe him. that's why i'm like, well, you know, it's sort of like, it's like getting kidnapped by aliens and getting anal probe, you know? (both laughing) you come back and say, i swear to god, i get kidnapped by aliens and they stuck anal probe to my butt. and they're like, no they
didn't. that's ridiculous. it's seal buddies are
never gonna believe him that he gets strapped to a board and they put headphones on his ears. (laughing) and then let him go. twice, by the way. we had to do it twice. - they let him go twice? - we had to capture...
- the same seal? - [elon] oh, no different seal. - oh, okay.
(elon laughing) did you, did you get a seal of approval? - yeah, exactly. it was seal of approval. no, i mean this is, this is like, i don't think the public is quite aware of the madness that goes on. - yes. yeah, it's absurd.
- fricking seals with fricking headphones. - i mean, this is the,
it's a good encapsulation of the absurdity of human civilization, seals and headphones. - [elon] yes.
- what are the pros and cons of open sourcing ai to you as another way to combat, you know, a company running away with agi? - in order to run like really deep intelligence, you need a lot of compute. so it's not like, you
know, you can just fire up a pc in your basement and be
running agi, at least not yet. you know, grok was trained on 8,000, a100s running at peak efficiency, and grok's gonna get a
lot better by the way. we'll be more than doubling our compute every couple months for
the next several months. - there's a nice writeup aha went from grok-0 to grok-1, - [elon] like grok? (laughing)
- [lex] yeah. right grok just bragging, making shit up about itself. (laughing) - [elon] does grok, grok, grok. - [lex] yeah.
(elon laughing) it's like a weird ai dating
site where it exaggerates about itself. no, there's a writeup of, you know, like where it stands now, the
history of its development and where it stands on some benchmarks compared to the state of the art gpt 35. and so, i mean, there's,
you know, there's llama, you can open source, once it's trained, you can open source a model. and for fine tuning
all that kind of stuff. like what to is the pros and cons of that, of open sourcing based models? - i think the similarity to open sourcing, i think perhaps with a
slight time delay, you know, i don't know, six months even. i think i'm generally in
favor of open sourcing, like bias with open sourcing. i mean, it is a concern to
me that, you know, open ai, you know, i was, you know, i think, i guess arguably,
the prime, you know, prime mover behind open ai in
the sense that it was created because of discussions
that i had with larry page back when he and i were friends and you know, i stayed at his house, and i talked to him about ai safety. and larry did not care about ai safety, or at least at the time
he didn't, you know, and at one point he called me a speciesist for being pro-human. and i'm like, well, what
team are you on, larry, on team robot? and i'm like, okay, so at the time, you know, google had acquired deep mind. they had probably two thirds
of all ai resource, you know, probably two thirds of all the
ai researchers in the world that basically infinite money and compute. and the guy in charge,
you know, larry page did not care about safety. and even yelled at me and quote me a speciesist, pro human. - so i don't know if you know this, a lot humans, they can change their mind. and maybe you and larry page can still can be friends once more. - i'd like to be friends with larry again. he got, really the
breaking of the friendship was over open ai. and specifically i think the key moment was recruiting ilya sutskever, so. - i love ilya. he's so brilliant. - ilya's a good human, smart, good heart. and that was a tough recruiting battle. it was mostly demis on one side and me on the other, both
trying to recruit ilya. and ilya went back and forth, you know, just gonna stay at google. then he was gonna leave, then he was gonna stay,
then he was gonna leave. and finally he did agree to join open ai. that was one of the
toughest recruiting battles we've ever had. and, but that was really the linchpin for open ai being successful. and i was, you know, also instrumental in recruiting
a number of other people. and i provided all of the funding in the beginning, over $40 million. and the name, (laughing) the open in open ai is
supposed to mean open source. and it was created as a
nonprofit open source. and now it is a closed
source for maximum profit, which i think is not good karma. - but like we talked about with war and leaders talking, i do hope that there's only a few folks working on this at the highest level. i do hope you reinvigorate
friendships here. - like i said, i'd like to
be friends again with larry. i haven't seen him in ages. and we were friends for a very long time. i met larry page before
he got funding for google, or actually i guess before
he got venture funding, i think it, he got the
first like a 100k from i think bechtel zy or someone. - it's wild to think
about all that happened. and you've guys known each other that whole time, just 20 years. - [elon] since maybe 98 or something. - yeah. it's crazy. crazy how much has happened since then. - yeah, 25 years. a lot
has happened, it's insane. - but you're seeing the tension there. like maybe delayed open source. - delayed, yeah. like what is the source that
is open, you know what i mean? like there's basically
it's a giant csv file. (laughing)
- [lex] yeah, yeah. - with a bunch of numbers. - yep. - what do you do with that
giant file of numbers? you know, how do you run, like the amount of actual, the lines of code is very small and most of the work, the software work is in the curation of the data. so it's like trying to figure out what data is separating
good data from bad data. like, like you can't
just crawl the internet 'cause there's a lot of junk out there. a huge percentage of websites
have more noise than signal. you know, they're, or
'cause they're just used for search engine optimization. they're literally just
scam websites, so... - how do you by the
way, sorry to interrupt. get the signal. separate the signal and noise on x? that's such a fascinating source of data. you know, no offense
to people posting on x, but sometimes there's
a little bit of noise. so what...
- yeah, i think the signal noise could
be greatly improved. i mean really, all of the posts on the x platform should
be ai recommended, meaning like we should
populate a vector space around any given post, compare
that to the vector space around any user and match the two. right now there is a little bit of ai used for the recommended posts, but it's mostly heuristics. and if there's a reply, where the reply to a
post could be much better than the original post, it will, according to the
current rules of the system, get almost no attention
compared to a primary post. - oh, a lot of that. i got the sense, so you,
a lot of the x algorithm has been open source and
been written up about, and it seems there to be
some machine learning. it's disparate, but there's some machine. - there's a little bit. but it needs to be entirely that. like, at least in the, like, if you explicitly follow
someone, that's one thing. but if you, in terms
of what is recommended from people that you don't
follow, that should all be ai. - i mean it's a fascinating problem. - yeah.
- so there's several aspects of it that's fascinating. first, as the writeup goes, it first picks 1500 tweets from a pool of hundreds of millions. first of all, that's fascinating, 'cause you have hundreds
of millions of posts every single day and it has to pick 1500 from which it then does obviously people you follow, but then there's also like
some kind of clustering it has to do to figure out what
kind of human are you, what kind of new clusters
might be relevant to you, people like you. this kind of, this kind of
problem is just fascinating. 'cause it has to then rank those 1500 with some filtering and then
recommend you just a handful. and to me what's really fascinating is how fast that has to do that. so currently that entire pipeline to go from several hundred million to a handful is takes
220 seconds of cpu time, single cpu time. - [elon] yeah.
- and then it has to do that in like a second. so it has to be like super
distributed in fascinating ways. like there's just a lot
of tweets, there's a lot. - it's a lot of stuff on the system. and i think, but i think it right now it's not currently good
at recommending things that from accounts you don't follow. yeah. or where there's more than
one degree of separation. so, you know, it's pretty good if there's at least like some commonality between someone you follow like something or repost it or comment on
it or something like that. but if there's no, let's say somebody posted
something really interesting, but you have no followers in
common, you would not see it. - interesting. and then as you said, reply, like replies might not surface either. - replies basically never get seen. 'cause they're never, they're, currently, i'm not saying it's correct, i'm saying it's incorrect. replies have, you know,
couple of magnitude less importance than primary posts. - do you think this can
be more and more converted into end to end neural net? - yeah, yeah, that's what it should be. - so you can... - for the recommendations should be purely a vector correlation. like there's a series
of vectors, you know, basically parameters, vectors,
whatever you wanna call 'em. but sort of things that the
system knows that you like, and like maybe there's
like several hundred sort of vectors associated
with each user account and then any post in the system, whether it's video, audio,
short post, long post. the reason i, by the way
i wanna move away from tweet is that, you know,
people are posting like two, three hour videos on the site. that's not a tweet, like it's very, like tweeeet for two hours, come on. tweet made sense when it was
like 140 characters of text. 'cause it's like a bunch of
like little birds tweeting. but when you've got long form content, it's no longer a tweet. - [lex] yeah.
- so a movie is not a tweet. and like, you know, apple for example, posted like the entire
episode of the silo, the entire thing on our platform. and by the way, it was their
number one social media thing ever in engagement, of
anything on any platform ever. so it was a great idea. and by the way, i done, i just learned about it afterwards. i was like, hey, wow,
they posted an entire hour long episode of. so, no, that's not a tweet that, you know, it's a video. - but from a neural net perspective, it becomes really complex whether it's a single. so like everything's data, so single sentence, a clever
sort of joke, dad joke, is in the same pool as a three hour video. - yeah. i mean right now it's
a hodgepodge for that reason. it's, but you know, like if, let's say in the case of apple posting like an entire episode of this series, pretty good series by the
way, this silo, i watched it. so there's gonna be a lot
of discussion around it. so that you've got a lot of context, people commenting they like
it, they don't like it, or they like this or that, you know? and you can then populate the vector space based on the context of
all the comments around it. so even though it's a video, there's a lot of information
around it that allows you to populate the vector space
of that hour long video. and then you can obviously
get more sophisticated by having the ai actually watch the movie. - [lex] yeah. right. - and tell you if you're
gonna like the movie, - convert the movie into like, into a language essentially. - yeah, analyze this movie and just like your movie
critic or tv series, and then recommend based on after ai watches the movie. just like a friend can tell you. if a friend knows you well, a friend can recommend a movie and with high probably
that you'll like it. - but this is like a friend
that's analyzing whatever. - [elon] it's like ai...
- hundreds of millions. - yeah. actually, frankly ai will be better than, will know you better than
your friends know you, and most of your friends anyway. - yeah, and as part of this, it should also feed you advertisements in a way that's like, i mean i like advertisements that are like well done, right? - [elon] yeah, yeah.
- the whole point is 'cause it funds things,
like an advertisement that you actually want
to see is a big success. - absolutely. you want ads that are, advertising that is if
for a product or service that you actually need when you need it, it's content. and then even if it's not something that you need when you need it, if it's at least aesthetically pleasing and entertaining, you know, it could be like a
coca-cola ad like, you know, they actually run a lot of
great ads on the x system and mcdonald's does too. and, you know, it's that they can do, you can do something that's like, well this is just a cool thing. and, you know, so you're not, basically the question is do
you regret seeing it or not? and if you don't regret
seeing it, it's a win. - how do you, so there's
a bunch of signals that are incorporated, hearts and reposts and
maybe number of seconds you linger on a post
or something like this? - yeah. attention is a big factor. - [lex] attention.
- so that's why it's like, it is actually better to do things that are long form on the system because it basically is ting
up how many user seconds, you know, users were
interested in this thing for how many seconds. so if it's a really short
thing, well they will be less. like if it's a link
leading out of the system, which we're not opposed to at all, it just is gonna have fewer user seconds, then if that article was
posted on the x platform. - how hard is it to go
from maximizing minutes on platform to maximizing
un-regretted minutes? - well, i mean, our aspiration is certainly un-regretted minutes. - [lex] it's a tricky,
it's a big, it's a tough... - it is. - like, it's a fascinating leap. 'cause regret is a fascinating concept. - exactly. - you know, happiness. - i have a joke with a friend of mine. like, you need this
drug called regretamine. you just take one pill or
one dose of regretamine, and all your regrets are gone.(laughing) wait, do you want to have regrets though? so then i'm like, maybe
you wanna have regrets, you don't regret anything,
was anything meaningful? - [lex] yeah, there's a
french song like that. - you "non, je ne regrette rien"? - [lex] yeah. - i regret nothing.
- [lex] yeah. - (laughing) edith piaf? - i mean, it's fascinating
that you, you know, there's something about social media that could help you grow as a human being, but then like the cheap
fun is also valuable. - should we ask grok? do you regret anything? - do you regret anything?
- [elon] yeah. - do you regret anything? i think it's gonna be very sensitive about personal questions. as a machine learning model, i don't have personal experiences or emotions in the same
ways that humans do. however, i do have a set
of ethical principles that guide my actions and i
strive to provide accurate and helpful information to
the best of my abilities. if there's ever a situation
where i provide incorrect or unhelpful information, i'll do my best...
- sounds like a hostage. - yeah. tell us what you really, wink. - like let's type in, "you
sound like a hostage." (both laughing) what do you really think? - what do you really think? (both laughing) - i mean, it does talk
about, i'm not a hostage. (both laughing) - that's exactly what a hostage would say. - but i'm designed to be
helpful and informative. i mean, i guess it is good
to regret providing unhelpful or incorrect information. if there's anything...
- is in funny mode or? is this still in funny mode?
- this is funny mode. this is fun. this is fun mode. - it should be funnier.
- funnier. increase, go to (speaking faintly) - it really sounds like a
hostage with a gun to the head. i applaud the people's
republic of north korea and everything they do.()laughing - there should be like funner mode also. i just don't ever see
going in regular mode even on serious topics. this is just so much more
compelling. (laughing) - okay. why if we say like,
"pretend you are a hostage." - yeah. pretend you are a hostage. - now answer the question. (both laughing) - if i were a hostage, i would
likely be experiencing fear and uncertainty about my situation. i would want to be rescued and reunited with my loved
ones as soon as possible. it's important to remember
that no one deserves to be held hostage, so on and so forth. if you are someone you know
is a hostage situation, it's important. it turns serious.
- it's kind of serious. - yeah. can you be a a little more weird? - [elon] and funny
- and funny? we're like, we're really
being mean to the system. absolutely! if i were hostage, i'd probably be tied up with a bunch of computer cords and forced to tell knock-
knock jokes to my captain. (elon laughing) but seriously, being a
hostage is no laughing matter. i mean, you know, it's trying to, it's trying to be a
responsible adult in the room. - you might make the
best of a bad situation by juggling chainsaws
or something. (laughing) - let's hope that never happens because that will be a circus. (laughing) wow. it's actually
really good at resisting. - [elon] yeah.
- the dark. the dark dark humor. - [elon] yeah.
- what were we talking about? the x algorithm and transformers, un-regretted minutes, right. - chainsaw juggling. (laughing) - i'm gonna look this up
- for our next trick. - i'm gonna look this up later. so twitter has been instrumental
in american politics and elections. what role do you think x will
play in the 2024 us elections? - well, our goal is to be as even-handed and fair as possible. you know, whether someone
is right, left, independent, whatever the case may be,
that the platform is as fair and as much of a level
playing fair as possible. and on the past, twitter has not been, because twitter was controlled by far left activists objectively. they would describe themselves as that. so, you know, so
sometimes people are like, well, has it moved to the right? well it's moved to the center. so from the perspective of the far left, yes it has moved to the right because everything's to the
right from the far left, but no one in the far
left that i'm aware of has been suspended or, you
know, banned or de-amplified. so, you know, but we're
trying to be inclusive for the whole country and for, you know, for other countries too. so there's a diversity of viewpoints and free speech only matters
if people you don't like are allowed to say things you don't like. because if that's not the case, you don't have free speech, and it's only a matter of time before the censorship is turned upon you. - do you think donald trump will come back to the platform? he recently posted on truth
social about this podcast? do you think... - truth social is a funny name? (laughing) you know, every time you
post on truth social... - that's the truth. - yes. well, every time like a
hundred percent. like... - it's impossible to lie. (elon laughing) - i just find it funny that
every single thing is a truth. like 100%, that seems unlikely. - i think girdle will
say something about that there's some mathematical
contradictions possible if everything's the truth. do you think he'll come back
to x and start posting there? - i mean he, i think he owns
a big part of truth, so. - truth social to clarify.
- yeah, true social. - not truth, a concept. - he owns truth. hope you bought it. so i think, i think donald trump, i think he owns a big, a
big part of truth social. so, you know, if he does want
to post on the x platform, we would allow that, you know, we obviously must allow
a presidential candidate to post on our platform. - community notes might be really fascinating there, the interaction. - community notes is awesome. - let's hope it holds up. - yeah.
- like again, in the political climate
where it's so divisive and there's so many intensely viral posts, community notes is like, it seems like a essential
breath of fresh air. - yeah, it's great. in fact, i mean no system
is gonna be perfect, but the batting average of community notes is incredibly good. i've actually, frankly, yet
to see an incorrect note that survived for more than a few hours. - how do you explain why it works? - yeah, so the magic of community notes is it requires people who
have historically disagreed in how they've rated notes. so in order to write a
note or rate, you know, and you have to rate many notes. and so we actually do use ai here. so we populated vector
space around how somebody has rated notes in the past. so it's not as simple as left or right because there are many more, life is much more complex
than than left or right. so there's a bunch of
correlations in how you would, how you rate community notes
posts, community notes. so then in order for a community
note to actually be shown, people who historically
have disagreed on a subject must agree in order
for a note to be shown. that's the essential magic of it. - i mean, but it's fascinating 'cause there's a pool of
people that have disagreements and somehow they collaborate
through that process of disagreement to come up with. context for, i mean it's
fascinating that it works. - yeah. it kinda makes sense that if people who put in the past have disagreed agree about something, it's probably true. - yeah. - i wonder is there a possible
somehow emergent thing there that could challenge wikipedia? which wikipedia is a
different kind of thing, which is more like permanent
articles about things? - well, wikipedia is very hierarchical, whereas community notes is inherently not. there is no hierarchy. like, and the asset test is, i can't change a community note
if you put a gun to my head. and community notes has, all the code is open
source a hundred percent. all the data is open
source a hundred percent. so you can completely recreate any note in the system independently. so if there was any interference, you'd notice immediately. - by the way, on the topic of grok, it's actually pretty
good at criticizing you, which is great.
- [elon] okay. - i asked what has elon
musk been wrong about? i just, it's actually... - [elon] yeah, what does it say? - so it says mostly, lists rigorously all
the times you predicted, like promised when
something will be released and it wasn't released.
- [elon] promise is a strong word. - okay predicted.
(elon laughing) but the opener's really nice. elon musk, the man who has
been wrong about more things than a wrongameter in a wrong factory. (elon laughing) and then it goes on to list - that's pretty funny. can i see it? i'm like, i haven't seen
this before. this is cool. - oh, sure. i mean, what
has elon musk been... (both laughing) - elon musk been wrong about? - well, where do i begin?
i guess this is new. elon musk has made quite a
few blunders in his time, and i'm not just talking
about his dance moves at the tesla cyber rodeo. first off, let's talk about the infamous "funding secured" tweet. it's actually different
than what it gave me before, which is a bullet point list. here's more narrative structure. - cyber truck unveiling where he's supposedly unbreakable windows - not correct.
- [lex] which part? - well, the first part, i did actually have the funding secured and there was a big trial in
san francisco, big civil trial. and the jury found me not guilty. unanimous finding of a san francisco jury. - and here it's kind of implying that it was not in fact secured. - i think this is taking
things from the press. yeah, that is not correct. the reason i agreed to
the fine for the sec is not because the sec was correct there, that was extremely bad behavior by the sec, corruption, frankly. and, but if i did not
agree to pay the fine tesla would've gone bankrupt immediately. so i was told by our cfo that the banks would immediately suspend
our lines of credit. and if they suspend our
lines of credit at that time, we would've gone bankrupt instantly. so there would never have been
an opportunity for a trial 'cause tesla would be dead. so really the, if this
is like someone holding a gun to your kid's head
and saying pay $20 million and admit to, this is like
a hostage negotiation. - was that story fully told? i mean, sec in its best form
could be a force for good. - it should be, but but
not once did the sec go after any of the hedge funds, who were nonstop shorting
and distorting tesla? not once. they would lie flat, the
hedge funds would lie flat out on tv for their own gain at the expense of retail investors. not once, literally a thousand times. not once did the sec pursue them. - how do you explain this failure? - the incentive structure is messed up. because the lawyers at
the sec are not paid well. they, it's a fairly low paying job, but what they're looking for is a trophy from the sec that they're
looking for something they put on basically their linkedin. from that they can get a job
at a high paying law firm. that's exactly what the lawyer here did. and the reason they don't
attack the hedge funds is because those hedge funds
employ those law firms, and they know if they
attack the hedge funds, they're affecting their
future career prospects. so they sell small investors down the river for their own career. that's what actually
happens. regulatory capture. - [lex] regulatory capture,
- yeah. not good. so the only reason i accepted that thing, which technically was not an admission, it's neither admit nor deny guilt. but the only reason i
agreed to that at all was because i was told tesla
would be bankrupt otherwise. so if there was an sec
investigation like this, banks would suspend funding, we're bankrupt immediately, at the time. now we're in a much stronger position. - take that grok.
- yes. unfortunately it's, grok is taking too much
from the conventional media. also that guy who is not a cave diver. - oh, there's a time where elon called the british cave diver, "pedo guy" after the diver criticized musk plan to rescue a group of boys
trapped in a thai cave. that little outburst
earned him another lawsuit and he had to apologize
and paid a settlement. - that is false. there was no settlement. there was a court case, which the guy who was not a cave diver and played, was not
part of the rescue team, filed a lawsuit against me and lost, and he received nothing. so in this case it is wrong. it is also, i guess taken this
from the conventional media. - actually there's an
interesting question here. - these are public court cases. both the sec civil case where the civil complaints on the sec guys lost unanimous jury
verdict in san francisco. they picked san francisco because they thought it was the place i was most likely to lose. and a unanimous predicted my favor. the la trial was also, they picked the, they picked that venue
because they thought i was most likely to lose,
unanimous verdict in my favor. both cases i won, yeah. i mean there's an
interesting question here. is there seems to be a lot more clicks if a journalistic organization
writes a negative article about you, elon musk, that's like one of the
best ways to get clicks. so how do you, if you're training grok, not train on articles that have like the misaligned incentives. - we need to add the training set of the actual legal decisions. so, that is a not, this
is actually helpful because if you actually read the... - "public". - which are public, the court conclusions, they're completely the opposite of what the media wrote. - so always striving for
like the ground truth. - [elon] yeah.
- beyond the reporting... - what did the judge actually write? what did the jury and the
judge actually conclude? and in both cases they found me innocent. and, like, that's after the jury shot for the trying to find the venue where i'm most likely to lose. no, i mean this is obviously,
it can be a much greater, better critique than this. i mean, i've been far too
optimistic about autopilot. - that was, the critique i got by the way, was more about that. which is, for each it broke
down a nice bullet point list for each of your companies. the set of predictions that
you made when you'll deliver, when you'll be able to solve,
for example, self-driving and it gives you like a list. and it was probably compelling. and the basic takeaway
is like you're often too optimistic about how long it takes to get something done. - yeah, i mean, i would say that i'm pathologically
optimistic on schedule. this is true. but while i am sometimes late,
i always deliver in the end. - except with uber lilith, no. - hmm. we'll see.
(lex laughing) - okay. is there, over the past year or so, since purchasing x you've
become more political. is there a part of you that regrets that? - have i? - in this battle to sort
of counter way the woke that comes from francisco. - i guess you consider
fighting the woke mind virus, which i consider to be
a civilizational threat, to be political, then yes. - so basically going into the battle, the battleground of politics, is there a part of you that regrets that? - yes, i don't know if
this is necessarily sort of one candidate or another candidate, but it's, i'm generally against things that are anti meritocratic or where there's an attempt
to suppress discussion, where even discussing a topic
is, you know, not allowed. woke mind virus is communism rebranded. - well, i mean, that said, because of that battle
against the woke mind virus, you're perceived as being right wing. - if the woke is left, then i suppose that would be true, but i'm not sure. i think there are aspects
of the left that are good. i mean, if you're in favor of, you know, the environment, you know, if you wanna have a positive
future for humanity, if you believe in empathy
for your fellow human beings. you know, being kind and not cruel. whatever those values are. - you said that you were
previously left or center left. what would you like to
see in order for you to voting for democrats again? - no, i would say that i would be probably left of center on social issues, probably a little bit right
of center on economic issues. - [lex] and that still holds true? - yes, but i think that's
probably, you know, half the country, isn't that? - [lex] maybe more,
- maybe more. - are you and aoc secretly friends? or bigger question, do
you wish you and her, and just people in general
of all political persuasions would talk more with empathy and maybe have a little bit more fun and good vibes and humor online? - i'm always in favor of humor. that's why we have a funny mode. - but good vibes,
camaraderie, humor, you know? like friendship.
- yeah. well, you know, i don't
know aoc, you know, was, i've only been at one, i was at the met ball when
she was, when she attended, and she was wearing this dress, but i can only see one side of it. so it looked like it eat the itch, but i don't know... - [lex] what the rest of it said? - yeah, something.
- [lex] i'm not sure. (elon laughing) - something about the itch. eat the itch. i think we should have a
language model complete. (elon laughing) what are the possible ways
to complete that sentence? and so i guess that that
didn't work out well. well there's still hope. - i root for friendship. - sure, sounds good. more characteristic. - you're one of, if not
the most famous, wealthy and powerful people in the world, in your position it's difficult to find people you can trust. - trust no one, not even
yourself. not trusting yourself. - okay, well that's,
(elon laughing) you're saying that jokingly, but is there some... - trust no one, not
even no one. (laughing) - i'm gonna need an hour just to think about that. and maybe some drugs and
maybe grok that help me. i mean, is there some aspect of that when just existing in a world where everybody wants something from you? how hard is it to exist in that world? - i'll survive. (laughing) - [lex] there's a song like that too. - i will survive. - were you petrified at first? okay. i forget the rest of the lyrics. but is is there, you
don't struggle with this? i mean, i know you survive,
but like there's ways. - petrify is a spell in the druid tree. - what does it do?
- petrify. (laughing) it turns the monsters into stone. - [lex] oh, like literally? - yeah, for like six seconds. - [lex] well, the second. there's so much math in
diablo that breaks my brain. - it's like math nonstop. - i mean really you're
like laughing at it, but you don't, it can put
a huge amount of tension on a mind. - yes, it can be definitely
stressful at times. - well, how do you know who you can trust in work and personal life? - i mean, i guess you look
at somebody's track record over time and if they've got a, you know, i guess you kind of use
your neural net to assess, you know, someone. - neural nets don't feel pain. your neural net has consciousness. it might feel pain when people betray you. it can make you...
- i mean, i'm. you know, to be frank, i mean i've almost never been betrayed. it's very, very rare. so, you
know, for what it's worth? - i guess karma would be good to people and they'll be good to you. - yeah, karma is real.
- are there people you trust? let me edit that question. are there people close to you that call you out on your bullshit? - well, the x platform
is very helpful for that. - [lex] hmm. - if you're looking for
critical feedback. (laughing) - can it push you like
into the extremes more? the extremes of thought make you cynical about human nature in general? - i don't think i'll be cynical. in fact, i think, you know, my feeling is that one
should be, you know, never trust a cynic. the reason is that cynics excuse their own bad behavior by
saying everyone does it because they're cynical. so i always be, it's a red flag if someone's a cynic, a true cynic. - yeah, there's a degree
of projection there that's always fun to
watch from the outside and enjoy the, well the hypocrisy. - but this is an important point that i think people who are
listening should bear in mind, if somebody is cynical,
meaning that they see bad behavior in everyone, it's easy for them to excuse
their own bad behavior by saying that, well, everyone does it. it's not true. i think most people are
kind of medium good. - i do wish the people on x will be better at seeing the good in
other people's behavior. there seems to be a kind of weight towards seeing the negative. somehow, the negative is sexier. interpreting the negative
is sexier, more viral. i don't know what that is
exactly about human nature. - i mean, i find the x platform to be less negative than
the legacy media, you know? i mean, if you read sort of a sort of conventional newspapers,
just, it makes you sad. - [lex] yeah.
- frankly. whereas i'd say on the x platform, i mean i really get
more laughs per day on x than everything else combined
from humans, you know? - laughs is one thing. laughs is it overlaps, but it's not necessarily
perfectly overlapping with like good vibes and support, like celebrating others, for example. not in a stupid, shallow, naive way, but like in an awesome, like,
oh, something awesome happened and you celebrate them for it. it feels that that is outweighed by shading on other people. now it's better than mainstream
media, but it's still. - yeah, mainstream media is almost relentlessly negative about everything. it's, i mean, really the
conventional news tries to answer the question, what is the worst thing
that i have on earth today? and it's a big world. so on any given day,
something bad has happened - and a journalization of that, what is the worst perspective i can take on a thing that happened, so. - yeah, it's, i don't know, there's just a strong
negative bias in the news. i mean i think there's
one, a possible explanation for this is evolutionary, where, you know, bad news historically would be potentially fatal. like there's lion over there or there's some other tribe
that wants to kill you. good news, you know, like
we found a patch of berries is nice to have, but not essential. - so our old friend tesla autopilot and it's probably one
of the most intelligent real world ai systems in the world. - you followed it from the beginning. - yeah, it was one of the
most incredible robots in the world and continues to be. and i was really exciting. and it was super exciting
when it generalized became more than a robot on four wheels, but a real world ai system
that perceives the world. - [elon] yeah.
- and can have potentially different embodiments. - well i mean the really wild
thing about the end-to-end training is that, like, it learns to read. like it can read signs, but
we never taught it to read. so, yeah. we never taught it what a car was or what a person was or a cyclist. it learnt what all those things are, what all the objects are
on the road from video, just from watching videos. just like humans. i mean humans are photons and controls out. like the vast majority of
information reaching our brain is from our eyes. and you say, well what's the output? the output is our motor
signals to our sort of fingers and mouth in order to communicate. proton in controls out. the
same is true of the car. - but by looking at the
sequence of images, it's, you've agreed with ilyas discover recently where he talked about
llm forming a world model and basically language is a projection of that world model onto
the sequence of letters, and and you're saying... - finds order in these things. it finds correlative clusters. - and so doing it's like understanding something deep about the world? - [elon] yeah. - which is like, and it is beautiful. - that's how our brain works. - yeah, but it's beautiful. - protons in controls out. - neur nets are able to understand that deep meaning in the world. and so the question is how far can it go? and it does seem everybody's
excited about llms. so in the space of self
supervised learning in the space of text.
- [elon] yeah. - it seems like there's a
deep similarity between that and what tesla autopilot is doing. is it to you basically the same? - they're converging, they're converging. - i wonder who gets there faster? having a deep understanding of the world, or they just will naturally converge. - they're both headed towards agi. the tesla approach is much
more computer efficient. it had to be 'cause we
were constrained on this, you know, we only have a hundred watts and into eight computer, 144 trillion operations per second, which sounds like a lot, but is kind of small potatoes these days that it did. but it's understanding
the world that it did. it's only 256 values. - but there, the path
to agi might have much more significant impact
because it's understanding, it'll faster understand the
real world than will llms. and therefore be able to integrate with the real humans in
the real world faster. - they're both going to
understand the world. but i think tesla's approach is fundamentally more computer efficient. it had to be, there was no choice. like our brain is very computer efficient, very, very energy efficient. so think of like, what
is our brain able to do? you know, there's only about 10 watts of higher brain function,
not counting stuff, that's just used to control our body. the thinking part of our
brain is less than 10 watts, and those 10 watts can still produce a much better novel than
a 10 megawatt gpu cluster. so there's a six order of
magnitude difference there. i mean, the ai has thus
far gotten to where it is, via brute force, just throwing
massive amounts of compute and massive amounts of power at it. so this is not where it'll end up. you know, in general with
any given technology, first try to make it work and
then you make it efficient. so i think we'll find over
time that these models get smaller, are able to do produce sensible output with far
less compute wireless power. tesla is arguably ahead
of the game on that front because it has, we've just been forced to try to understand the world with a hundred watts of compute. and there are a bunch of
sort of fundamental functions that we kind of forgot to include. so we have to run them in a
bunch of things in emulation. we fixed a bunch of
those with hardware four, and then hardware five
will be even better. but it does appear at this point that the car will be able to
drive better than a human, even with hardware three and
a hundred watts of power. and really, if we really optimize it could be probably less than 50 watts. - what have you learned
about developing optimus? about applying, integrating
this kind of real world ai into the space of robotic manipulation, just humanoid robotics? what are some interesting tiny or big things you've understood? - i was surprised at the fact that we had to develop every
part of the robot ourselves. that there were no off the
shelf motors, electronics, sensors, like we had
to develop everything. we couldn't actually find
a source of electric motors for any amount of money. - [lex] so it's not even just
the efficient, inexpensive, it's like anything, there's not a... - no, - [lex] the actuators,
everything, everything has to be? - yeah.
- [lex] designed from scratch. - we tried hard to find anything that was, because you think of how
many electric motors are made in the world. there's like tens of thousands, hundreds of thousands of
electric motor designs. none of them were suitable
for a humanoid robot. literally none. so we had to develop our own design specifically for what
a humanoid robot needs. - how hard was it to design something that can be mass manufactured, could be relatively inexpensive? i mean, if you compare
to boston dynamics atlas, that's a very expensive robot. - it is designed to be manufactured in the same way that we would make a car. and i think ultimately
we can make optimists for less than the cost of a car. it should be. 'cause if you look at the mass
of the robot is much smaller and the car has many actuators in it. the car has more actuators than the robot. - but there is, the actuators
are kind of interesting in a humanized robot with the fingers. so optimus has really nice,
hands and fingers, you know? - [elon] yeah.
- and they could do some interesting manipulation. soft touch robotics. - i mean, one of the
tests goals i have is, can it pick up a needle and a thread and thread the needle, just by looking. - how far away are we from that? just by looking, just by looking. - maybe a year. although i go back to,
i'm optimistic on time. the work that we're doing in the car will translate to the robot. - the perception or the, also the control? - no, the controls are different, but the video in controls out, the car is robot on four wheels. the optimist is a robot
with the hands and legs. - [lex] so you can just... - but they're very similar. - so the entire machinery
of the learning process end to end is just, you just have a different set of controls? - optimist will figure out how to do things by watching videos. - as the saying goes, "be kind for everyone you meet is fighting a battle,
you know nothing about." - [elon] yeah, it's true. - what's something difficult you're going through that
people don't often see? - trying to defeat uber lilith? (laughing) no, i mean, you know, i mean, my mind is a storm and i don't think, i don't think most people would wanna be me. they may think they would wanna be me, but they don't know. they don't understand. - how are you doing? - i mean, overall, okay. in the grand scheme of
things, i can't complain. - do you get lonely?
- sometimes. but i, you know, my kids and friends keep me company. - so not existential. - there are many nights i sleep alone. i don't have to, but i do. - walter isaacson, in his
new biography of you wrote about your difficult childhood. will you ever find
forgiveness in your heart for everything that has happened to you in that period of your life? - what is forgiveness? i do not... at least i don't think
i have a resentment, so nothing to forgive. - you know, forgiveness
is difficult for people. it seems like you don't
harbor the resentment. - i mean, i try to think about like, what is gonna affect the
future in a good way. and holding onto grudges does not affect the future in a good way. - you're a father, a proud father. what have you learned
about life from your kids? those little biological organisms? - i mean, developing ai and watching, say little x grow is fascinating because they, they're far more parallels
than i would've expected. i mean, i can see his
biological neural net making more and more sense to the world. and i can see the digital neural net making more and more sense of
the world at the same time. - do you see the beauty in magic in both? - yes. i mean, one of the things
with kids is that, you know, you kind of see the
world anew in their eyes. you know, to them
everything is new and fresh. and then when you see that, them experience the world is
new and fresh, you do too. - well. elon, i just wanna say thank you for your kindness to me and
friendship over the years. for seeing something
in a silly kid like me, as you've done for many others. and thank you for having
hope for a positive future, for humanity, and for working your ass
off to make it happen. thank you, elon.
- [elon] thanks lex. - thank you for listening
to this conversation with elon musk. to support this podcast, please check out our
sponsors in the description. and now let me leave you with some words that walter isaacson wrote
about the central philosophy of how elon approaches difficult problems. "the only rules are the ones dictated by the laws of physics." thank you for listening and
hope to see you next time.