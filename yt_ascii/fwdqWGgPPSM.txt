this is animal in its version sea this is built by a company in zurich and we've been working with for quite some time now that is a spin-off of the eth university in zurich one of the main difference between working with animal and the spot robot is that we have we have access to the low level controllers in this robot well for for spots we don't have access to the locomotion controller so this shout out to boston dynamics please make this accessible and we can do a lot more with your hardware so today i'll i'll tell you a bit more about how we do control for legged locomotion so this is how we control the limbs of robots with legs so that they can navigate environments that are flat and beyond flat i mean the interesting parts are you know where you need to do dynamic maneuvers and sort of scale obstacles jump off boxes and whatnot currently there are two prevalent approaches to it one of them is model predictive controls and the other one that i'll focus a bit on would be reinforcement learning so that way you get the robots of work it out for itself exactly yeah that's the whole point so this is mpc mpc is kind of on the more traditional control side of the spectrum right you have models of your system and then you can in essence simulate forward your model and decide whether what your control input would result into and then that's whether what your what you're asking the robot to do is successful or not by some metric to see how this is done let's let's think of a robot system robot quadruped this is the robot body and it's got four legs right this is one of the four legs without going into too much detail robot moves forward let's say leg has three degrees of freedom three degrees of freedom it means that it has three ways in which it can move right it has three joints that in most cases are actuated by three motors now each degree of freedom is one variable that we need to control and for the entire system if we consider a robot with four legs three degrees of venom per leg then we have 12 degrees of freedom for the joints and then we have six more division of freedom about where the where the base position is with respect to the world now we model what each joint needs to do and one way to go about this is to model that as a polynomial a polynomial would be a way of representing a curve let's say against time and be that q be the position of one joint in model predictive control we would have a set of polynomials that would orchestrate how the robot moves and then we would look at a time horizon let's say capital t and then forward simulate the system see what it does in the environment that it navigates and then for example play out t h which is the horizon we can loop through that and repeat that multiple times and as time flows we sort of command the robots to do what it might and this helps us deal with the things like sensory noise actuation noise estimation uncertainty and so on drawbacks we need to have good models of the system and this is particularly difficult especially if we're dealing with a system that you know is operating a dynamic environment is out in the world you have things like wear and tear of motors or of fits or actually waiters themselves you have vibrations from making and breaking contact you make and break contact all the time which is an an added complication so this is this is the kind of analytical approach the sort of more control heavy approach now on the other side we have a sort of i think it's fair to say more recent development on the machine learning side of of control where we train a neural network as everyone these days the idea here is that this being annual network these are nodes this is input and this is output so we want to input the state of the system and get us outputs uh a vector of control inputs be that torx position desired positions desirable losses and so on that accomplish a a target that we set for the robot for the system in many cases this would be being robust to external perturbations pushes long story short the robot's not tumbling over not falling and stepping on where it is supposed to step the great benefit to that is that okay to start with this operates in with data right we would learn these controls from data and the great benefit with that is that we can simulate examples episodes that we learn from so we can simulate the robot moving in a in an environment that we control and we can collect data of the robot performance and the the controller performance in the simulation we can leave that run sort of overnight let's say or have a cluster that uses that forcing that can do the simulations asynchronously and then we can use this corpus of data to learn variable bus controllers there is a complication when taking this from the simulation to the real robot this is this is known as the sim to real transfer problem there are a couple of ways that we can we can work through that one of it is double down on the machine learning approach and learn and learn from data a model of how the system responds to our input and the other approach is to actually bake that in into a into a controller learning so instead of learning with one particular model in simulation we have a range of models that we if think of it as a range where we can sort of have knobs for each variable of the system imagine having a model of a quadruped where we can twist a knob and get longer legs or shorter legs or larger bodies we don't change the morphology so we would not change the degrees of freedom we would change the the the variables of the simulator itself so by adding these variants we can make the controller robust against changes in that parameters and these are the set of parameters that are hard to estimate from the real system and accurately approximate in the simulator so we can make our controllers or bus to variations of these parameters which means that it can transfer to at a somewhat different model which is the model that it's not the model it's a real system right okay so can we see some of this is that possible absolutely yeah we have one of our quadrupeds downstairs in the lab and we can show you what the robot does in in in working over some of sort of our benchmark obstacles and we can also have a look at what the robot sees how the robot person how the robot models its surroundings and where it decides to step and so on so this is quadrupled robot right this is animal version c its name is coyote it's one of the robots in our lab the research that we do here with we're looking into machine learning approaches to a legged locomotion this is quite different from traditional approaches that use for example module based control or model predictive control there is a kind of convergence in the fields at the moment we we are working and we're seeing a lot of approaches that combine model predictive controllers and and learned aspects to these controllers now sit here is going to demonstrate our rl based learning controller right this is a control that has been trained in simulation having a set of examples shown to a neural network that learns to control the system and after having it trained then we can run it on the robot and evaluate its performance so you've got projection now it's kind of a similar to where we're standing isn't it so what's going on there exactly so we can these are the lidar returns from the lidar sensor that said on the back of the robot and then on the robot there's four depth sensors that are distributed around the body and this is the estimate of the system of how the ground looks around its feet and with that the controller can decide where to place its its legs at least an estimate so there's still the robot can still stumble and sort of a place fit at the sort of opposition where it's sort of three minutes away almost yeah the goal here is to have a controller that is also robust to sleeping that plank could be wet it could be slippery yeah exactly yeah or a stumbling and so on this is animal and its version see this is built by a a company in zurich that builds robots and we've been working with for quite quite some time now that is a spin-off of the eth university in zurich we talked about different methods of learning upstairs the reinforcement any other model but where does that fit in with doing that then is that then you've not got a static kind of equation that you plug all those numbers into it works differently doesn't it yeah exactly so this is this is uh this annual network that then online receives perception information from the joints and the torque sensors and the vision and depth sensors and then decides where to place fit and how to orchestrate the motion of all the actuators in order to be able to coordinate the system into walking over things and instead going up flights of steps and going down flights of steps and so on and so is it learning as it goes or is it is that then a it's already done the learning and then you've applying that to this yes in this particular controller this is the learning has been done and it's not learned as it goes but we're currently looking into approaches that we can we can distill some of the experience the online experience of the of the robot and see how we can incorporate that into into a sort of on la ongoing learning system all right so this is the signature here what if we do take this and we decrypt it with the public key because remember they reverse one another and then we can sort of change their location they can start gathering more information so by moving you actually introduce uncertainty into the motion