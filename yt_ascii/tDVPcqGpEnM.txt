so apache spark is another kind of framework for doing big data processing distributed across clusters like mapreduce the differences is kind of come in how those computations have done. so for example with spark you've got a lot more flexibility in the computations so with mapreduce you've got to do map and then reduce like there's no way of getting around it in spark they provide like a load of different operations that you can do on the data such as joins between different data structures why would a you spark this purpose is to process large volumes of data so it's mainly data that's not going to fit on a single node there's also computations that over a large volume of data. you don't want to go through and sequentially data and if you've got parts of your computation that are independent of each other and so you can do it on the data items individually, you can split that data across the cluster and then do the computations on that single node exactly like with mapreduce you there's the data locality prints? but again do the computations on the nodes where the data is stored and then you reduce those results down to what you want the main programming structure that you're going to be dealing with is called a resilient distributed datasets which is usually shorter than two rdd which is kind of a collection of objects that spread across a cluster but as a programmer when you're dealing with that, you're kind of just interacting with it as if it's on a single node so it kind of hidden from you this it's distributed in a spark cluster you'll have a driver node and then several worker nodes and the driver node is running the main program where kind of has all of the transformations that you want to do to your data and then these kind of get sent out to the worker nodes who then operate that on their chunks of data that they have. in fact, the transformations can be similar to mapreduce so it still provides the same map function and reduce functions, but then you have additional stuff on top of that. so they like give you a filter operation directly so you can do dental to implement that you can just call the filter function on an rdd and say i only want to return objects to which this is true so here we've just got a very very simple spark example of just loading in a text file from the local file system and we're just going to go through and count the number of occurrences of each word this is exactly the same as the mapreduce example. we looked at last time but we're doing this in spark this time okay, so at the start we set off a spark config so we just set the app name which allows us to seize the which of our jobs is currently running within the web ui we then set the spark master. so because we're running this locally on a single computer. that's just local we then set up a spark context which gives us access to like the spark functions for dealing with rdd's we first of all need to load our data into an rdd so we do this using need text file function and that puts the contents of that text file into an rdd the rdd you can kind of just view it as like an array if you want to it's been like an array distributed across the cluster so here we've got our lines rdd each element in the rdd is a single line from the text file we then go through and split each line using the flat map function so that map's a single function over every single item in the dataset. so every line we go through is split it up into words and then because we're using flat map that then takes that from an rdd of arrays to an rdd of strings again we then go through and exactly the same as in the map reduce example we use the map function sum up each word to a key value pair where the key is the word and then the value is the value 1 so indicating we've got one instance of that word at that point that then gives us a new rtd and for that one we go reduce by key instead of it's a map reduce that would just be reduced but here in sparc if we just did reduce it would give us a single value for the entire rdd back at the driver reduced by key takes an id d of key value pairs and for each key you give it a function to apply to those values for how you want it to be combined so for us we want to add up the number of those instances of that word that we have so we use just a simple + to aggregate those values so that finally gives us our word count rdd which contains key value pairs of words and a number of instances of those words. and so we then called the collect function which will bring that back to the driver node, and then for each one of those lines we've in them out. so we right now the counts all those words so this at the moment that code is written for something that might be on your own computer how would it differ if it was on a cluster and i'm server farm or a massive data center or something like that? how would that vary? and so if you're running this on an actual cluster and not just on your local computer then rather than setting master within your code and setting it to run locally what you do is you would have sparc running on the cluster and you would use something called spark submit submit your spark jobs to spark to then be run so it would it's just a different way of running them basically rather than hard coding it within your program even though you're their money on the cluster the rest of the code would be the same so the work that i've done with spark has been kind of using it to analyze large volumes of telematics data coming off of lorries as they're driving around and using the data from that to identify locations where incidents are occurring such as if they're cornering harshly or breaking harshly outside of research what sorts of things is spark we use. yes so sparky is used quite a lot in the real world like you would find a lot of companies will be using it to kind of do large-scale jobs and all of the data that they have and it can be used for like analysis or simply just processing that data and putting it into storage the good thing about the distributed computing in clusters is that if you want to scale the program more you just add more like nodes to the cluster. so the point is if you want to increase your processing power you don't have to buy new hardware in terms of replacing your hardware you keep your old hardware you buy new nodes and just stick them on the end and you've instantly increased how much processing power you have so if you suddenly say get a load more data that you need to be posting. you think oh miss current cluster size that's not great. we can then expand that and then just add a few more nodes so the sparc program would then just scale automatically going back to rdds these are immutable data structures so that immutable means they can't be changed. right? is that right? yes. yeah. so yeah, and they're immutable they cannot be changed once they're created you can pass them to other functions, but you can't change the contents of that single rdd so what the spark program ends up being is kind of a chain of transformations with each one creating a new rdd and passing it on to the next function the advantage of the rdds is that they can be persisted in memory which means that then it's more efficient to reuse them later in the computation. so one of the disadvantages of hadoop mapreduce, is that you it's every time you're writing and stuff to disk basically after your mapreduce computation if you want to reuse it you've then got to go and get it from disk again whereas with sparc you can just persist the rdd's in memory if you want to come back to them later, then you can do it really easily you're saying large amount volumes of data. can we put some numbers on this? well, what are we looking at here? see the volumes of data we're talking about can vary i guess depending on company. it's probably ranging gigabytes to terabytes and then the biggest we then just keep going up basically