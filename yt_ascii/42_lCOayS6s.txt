this is a virtual human she's currently not really paying attention to me but but later on we will turn her on and then she will be able to respond to me when i said virtually even i really what i mean is an embodied conversational agent, but that's quite a mouthful so we just call it's shorthand virtually human the point is that it looks like a human but it's on a screen so it's not a robot and it behaves and talks and listens the way humans do we use two sensors really so we use a microphone and a camera so she can see me basically, it's her vision and she can hear me and we can begin give the trials. hello what are we doing today? yeah, sure for each question, please answer in terms of options to show them the screen more than four days every day. okay. here's the first question over the last two weeks not at all yes okay, here's the second question tower bridge you answered no i'm sorry. can you please repeat your answer? brown sauce no it's a study to see if we can replace a written questionnaire with a with a virtual human questionnaire, so that's it's a bit different the mock normally do we also made a version of this called alice where you can ask things about the book alice in wonderland's and in general there's many different ways in which you can use cultural units. i suppose all you're seeing is a unity skin a unity character and you can replace it with any unity character that you want but in it all starts with the input basically she you can listen to what i'm saying and you can see my face. so we're basically creating characters that are linguistically skilled emotionally skilled socially skilled so they there's a speech recognition module in there that basically captures all the words that i'm saying and based on that she can determine what how to respond, but she also takes into account my face and my facial expressions, so that means she knows when i'm looking at her when i'm looking at her it means that she knows when i'm smiling not smiling and then you can start doing interesting things because then the agent can start back channeling so it can also start smiling back when i smile nodding when i not or if you want to create a particular persona, a particular character that is absolutely abrasive or even aggressive you can start doing the opposite so we made a character called spike always an answer for everything and if you not he will shake if you say yes, you'll say no and people find it really engaging but they get really worked up talking to spike. so there's the let's say the sensing component that recognizes your face your facial expressions your voice your speech. also how old you are and that information gets sent into what we call the dialogue manager and the dialogue manager maintains what to say next and also maintains what's you know what we call dialogue state? so what has it already said? what i said topic that we're talking about at the moment and then based on the input and the current state it decides what to say next that is sent in a very high-level commands using something called behavior markup language to the realizing and they realized it and turns that all into the actions that need to be made and the speech that needs to be spoken quite a lot of effort went into this particular virtual agent into making sure that there was synchronized lips the speech and that you can interrupt the agent because in the past quite often people made these virtually humans were in a sense. they played a little little pre-programmed movie. it was a bit of code of what to say that was turned into a set of behaviors and once that was executed they were played and there's no way of stopping it but of course in real life you quite often want to interrupt the agent, but then you need to be able to stop that behavior realization and gracefully going to a neutral mode into a listening mode, etc so that's the kind of developments that we're working on with these virtual humans is this some kind of machine learning or is it what's going on behind? no it's it's not a machine learning but it's an algorithm that can go from any state that it's in back to a neutral state to the next day. so it's it's an algorithm that allows you to break where you currently are and go to a next phase while it's running rather than having to wait until the end of the segment to play and then going back what sorts of challenges do you get from having to recognize speech and you know getting a device or a virtual human to respond yeah, so there's actually a number of components that they can do if you might have realized that having before the speech recognition happens, so the first one is is somebody speaking or not. so this voice activia detection and then another very important one is turn-taking if the agent starts interrupting you while you're speaking that's really annoying so you need to know when somebody is done speaking and you can't just work by voice activity detection because sometimes there's little pauses in my speech and you're not supposed to just button so so that that's actually done with machine learning where we learn exactly when it's okay for an agent to interrupt not interrupt to take that turn although for some agents sometimes it's it is appropriate to interrupt but but at least when you know when when somebody is speaking their turn then the agent knows that if they would speak now, they would be interrupting and that would of course have a particular effect only conversation the person might be unhappy about that but maybe that's the the point of the interruption as part of this project and other projects we have recorded many databases quite often of people talking human to human but through screens. so it's as realistic as possible to the setting as we would have with a virtual human and then we annotate all the data for when i was speaking. what are they saying? are they smiling or not? and and of course then from that also deterrence is whose turn is it and then who's you know? when those go into in somebody else's turn and then you can also when was there an interruption and how was it dealt with? and yeah, so there's a lot of there's a lot of manual work going in to that we can show you some of the these are our dashboard in a sense arousal is an emotion. that's basically how excited i am and that was relatively high i don't think it's currently doing valence because this is from video and my face is not currently in front of the camera but it would recognize whether i'm happy or not happy and this is where i was a child youth and adult or senior unfortunately, i'm slowly moving towards the senior a bit and this is some things about what kind of vocalizations do i have ms island. is it a filler? is it a brief or anything else if i swell the valence should go up but i can't so that's the kind of facial expressions that we're that we're making and then we can work with that is there a big database behind here is an ontology. how's it work? there are? okay, so all the sensing is done on a on a learning basis, so that's all machine learning so the facial expressions are learned using large databases the speech recognition. i learned age estimation emotional technicians all done by by machine learning the dialogue manager actually is it's a rule based. so it's handcrafted so that actually gives you the ability to author a scenario quite quickly the downside of doing natural language processing with a machine learning approach is that if you wanted to create any new scenario you would basically need to first collect lots and lots of data to then ultimately create what it is that you want to dialogue to be about and because we are mostly working on these virtual humans as tools to do, you know a particular study with or ask particular questions? or provide particularly information you want to be able to craft to author those interactions really well, and then actually a rule-based system is much better with a particular ontology of topics turns etcetera sentences and that you can you can all create that of course where we want to go to is sort of a halfway house between a machine learn and a crafted version so that we can still craft the outline of the of the discourse so the topics the questions to be asked etc. but we want to learn or all the possible variations in which you can say that and all the possible variations in which people can answer questions because at the moment we have to hard code that and that is makes it fragile when you were making kind of really crazy random answers to what she said instead of reacting to those she just realized that wasn't what she was looking. she has no idea what hp brown sauce is so she can't talk about that yes, so you you'd want to be able to use a machine learning system? so she's so she knows it's off topic and that we're not talking about that and then she can go back to what we programmed her to to talk about we're basically handcrafting everything that it can say and yeah, we want to keep that control quite quite tight does that give you a reduced flexibility though or so you you putting this in very specific? sort of silos. yeah, so your your we're not creating virtual humans that can talk about anything and i think to be honest we're still quite a long way off from that and when i say we i'm talking about the big companies, etc so the problem with you know commercial products like siri, cortana google assistant is that they don't they're not conversational agents. they are more like voice commands interfaces and that works quite well but they can't you can't hold a general conversation with them either right and i think doing that is still very much science fiction that's quite a long way away in the artificial intelligence roadmap the other thing that's quite different from what we're doing. is that those voice? assistants, they're all voice only their voice user interfaces what we're adding to that is your gestures your facial expressions not just from the user because it's the first human can sense me but also she will use her own facial expressions in her own gestures to communicate with me and that also creates a sort of a point of contact so, you know, you will be facing her usually which also allows her to distinguish whether i'm talking addressing her or my addressing someone else in the room, so that's a whole new area, i think that will seem much more about somewhere in this space when you put my face in i'll appear and when you put steve's face in it'll appear somewhere else and this actually solves a really nice problem, right? it's called the one-shot learning problem how do we convince a phone to let me in having only seen one ever picture of my face? which is when i first, you know calibrated it the first time and the answer is we don't train a neural network