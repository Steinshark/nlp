um welcome everybody it's good to be here i see a lot of faces i'm including the ones that i know from twitter on the first time meeting you all in person so my talk the foundation of gpu programming is basically to provide more insight how to use the underneath capabilities of gpu chips so my name is philip mulund i'm from angola it's a country in the super second in africa i have a bachelor degree in software engineering a master's degree in applied mathematics and computer science and since these years i've been working with the sapphire research group at eth we work on computer architecture we build tools like simulators and many others that simulate systems we also have a lot of open source tools that we use that that we that are public and many people here that when some of the most user language cic plus plus and c so if you wish to contribute you can also join so the safari stands for safe fair robust and intelligent computer architecture we work in the entire stack of computer activity for memory controllers memory controllers memory systems and many other things and at safari research group i work with a pim there's a processing in memory basically as you can see in the speaker by this side the particular ship that i work with the app mem ship where we add compatibility inside of the chip itself inside of taking the data from the entire cache hierarchy to compute them in a particular accelerator or in the whole system as a cpu research group is lead by professor honor moto that recently wanted the the 2022 google security and privacy research award the professor have a youtube channel where he provides a lot of content in computer architecture like in and many other kind of uh of talks so there is a lot of classes in in in about different computing paradigms like gpu cpus processing using memory and the group is also heavily focused genome analysis i work with genome analysis for processing as a member we also have a lot of lecture on how to how to process the human genome and not only and this lecture i don't know my slides are failing i don't know why so i would definitely try to shift it from because it's not working so let me just use the google side instead i hope i have internet here yeah etc so we have this particular tactile will be that we're doing is based on one of the shutter software resource a set of lectures on heterogeneous computing systems as you can see here we also have the software live seminars where we provide a lot of content on what is going on in academia industry so we invite experts from the field of computer access and system in general to give thoughts on the cutting edge research that they are doing so i highly recommend to follow safa research groups if you want to know where what kind of computing and what kind of programming you all will be doing in a couple decades you can follow him on twitter so now back to my talk the foundation of gpu program so here is the outline how we start with the motivation so the motivation of this talk starts with this question so when to run this and where and where and why would you run that so you have a cpu and you have a gpus where will you run this here we assume that n is 1 billion for example so you can stay with this you can stay with this the answer for this question for yourself and see if at the end your answer is compatible with my because the rest of the talker will try to describe why i why gpus are more suitable to run this kind of code than cpus because the important thing here is not only to know where to run but we need to be able to justify we as a programmer we have people we work on workloads that the kind of the the most suitable process in unit for this particular workload at a given time changes right you can have an entire program where the behavior change so in the first section of the blog institute for cpu and the half blocks of the section of the program are more suitable for fpga or even gpus for example so it's really important to know where to run which piece of code so let's start with the background the background of this talk is to give a small introduction where gpus come from so because the goal of the talk is to make people to be able to reason about gpu programming so to start out we start with the flames taxonomy of computing on his paper on 1966 very high speed computing systems where he described different computing systems like 60 and we have 60 we have cmd from msd and we have mmd but the fox of this style is this one is single instruction upper it's a cbd we're seeing instruction operates on multiple data elements and cbd can be implemented but two different kind of processors we have an airway processor and we have a vector processor so let's start by describing a bit what's data parallelism so data parallelism is when concurrence arrives from applying the same operation in different data elements so like a single instructional multiple data smd for example when you are applying the dop of product of two vectors for example it's in construct with the data flow where concurrence arise by performing different operations at the same time or in parallel and the same construct with threads where concurrence survive by performing by executing two differences of control in parallel and and basically see me the exploit operation level parallelism of different pieces of data some operation concurrent apply in different pieces of data it's kind of instructional level parallelism where the same operation will be applied to different datas so let's start talk about cbd processing so as i said i don't know we keep saying it a little bit more single instructional operator multiple data but this kind of operation can be bought in time and now can happen also in space and in this system for example we assume that we have different processing units or execution units and this is a new as on this execution units can operate in space and on and in time looking forward that will show more how it's done so but basically the ri processor instructions operate on multiple data elements at the same time using different spaces spaces here we we mean like processing engine engines for example and we have a vector processor where instructions on multi-data elements on conceptive time steps using the same steps so in the next slide we see how it is done so here we have an example so we have this stream of instructions basically what we're trying to do is to load that an array to a particular vector so here we talk about a vector operation because we talk about cbd so here the these instructions and vectors that's where you can see that we are loading four memory locations directly to a vector and then we adding the adding one to all the elements of this vector and then we multiply two to other element of this vector and then we store the result that we we compute the results that are in the vector back to the memory location where we fetch the initial values so let's see how array processor works and how a vector processor also works so in array processor we have processing engines so we have four processing engines and all these processing engines have the ability to compute different instructions and because of that we can operate we can on different piece of data at the same instructions at the same time so in the first cycle we can load the four element of that array and then we can perform the add at the same time we can also then perform the multiplication and then to store back the data in our vector processor it's different because the what we have here is a process is processing units that can only perform one kind of computation for example the first one can only can only load data the second one can only add the third one can only multiply and the last one can only store data so here what we do in order to come to apply the same operation on different data so in the first cycle we load the data in the second cycle what we do we load the next data in the frost functional unit because it's already free and we move the the the the the the the the the the the data where we load to the add function unit basically what we do in a vector processor is pipelining and we keep going and keep it going until we perform the rest of the operations so what we have here is that in our ry processor we perform the same operation in the same at the same time in terms of time and here at the same time we are performing different operations here we're performing equal operation everybody's in a particular given cycle everybody's been filming either load either odd either multiplication is the store but here at the same time we are always computing different aspirations because we are pipelining and in terms of space this one in space is always computing the same instructions but an array processor in space is always computing a different instruction in this in the case of easy streams of instructions so let's give a little background on what is vector processor because we will see how gpu have the underneath structure of a gpus is heavily based at an array and a vector processor so a vector is a one-dimensional array of numbers and many significant commercial programs this vector and we have a loop here showing that a particular operational vectors but a vector processor have a set of requirements that needs to be fulfilled before we start competing on them so the base requirement is that we need to load and solve vectors into resistors we need to operate on vectors of different lines and for that a vector processor always provide a vlan is basically a resistor where we specify the maximum element that we have in a vector an element of a might of a vector may be stored a part of each other and for that we also need to provide a stride basically how we access consequent elements of a particular vector in memory so here we have an example of a multiplication of matrix so the matrix a and bear both the store in row measure order in memory so in order to perform the muscle multiplication what we do we do we load the the first row of the a matrix and each time to load the entire road what we do each time we only increase the address by one and in this case we say that this kind of accessing have a slide of one for better b is different because we need to load the column instead of the rows and to access we always need to increase uh 10 in the in in a particular address and because of that we say that destroy distance so an example we have a linear memory because memory is linear this is how we try we store this particular element and here we can see that destroy this one to access each of this element of the of the first row of a and for b we can see like the stride is is 10. and we will see how it is important later so a better instructions perform an operation which element in concentrative cycle which vector units are pipeline it and each pipeline stage operates on different data elements and because of because of the lack of basically cpus they perform a lot of pipelining by the pipelines in a particular cpu cannot be so deep because of dependency that you have in cpus and we will see how deep use deal with with these particular problems and how is to explain how vectors are so connected to gpus because this is what a particular inspired the exist of gpus we will see like in particular gpu we don't have in intro dependencies between datas later on we'll see how it's its performance but let's stick with a vector so in a vector we don't have introverted dependencies do you don't need hardware interlocking there is no control flow within a vector but you see that we can have it in gpus and we need to know the stride and and destroy when we have a strategory it allows to perform a lot of things quickly for example we can do prefetching because you all know which is the next data that we need to fetch for memory and then we can do early loading we can skip that one so we have here an example of particular vectors so hold nmb the followers and we have control control resistors like veterans and vector weatherstride and vettel mars the better much we use basically to decide in which in which data of a particular weight we should not operate on on or perform a particular operation so but when we start with another interesting file so that is also existing modern gpus that was previously solved by vector vector architectures so it's basically you need to load data and sometimes you want to sustain some particular kind of of bandwidth for example you can say like i want one piece of my vector every in every consecutive cycle for example and the question is how you do that how you sustain that for example normal memory system most of them they are monolithic memory systems so they can particularly give you one cash line up under at a particular time unless you have multiple ports in your memory system but we assume that we only have one port in each bank of your memory system so here for example how you enable that how you enable so that in each cycle you can take the next data of your of your vector so basically the technology that also exists in cpu so you can find it exiting gpus it's basically the structure of a cache on gpus is the memory banking so what you do you bank your memory so that now you can sustain concurrent access in different banks of your memory so at this each cycle you can send particular requests to each of these bankers to each of these banks and later on you can sustain like one cycle uh to to to to fetch the data for a particular vector so this is an example of a particular vector you can like this kind of just really a simplicit way of showing how you would perform computation or we write code for a factor process for a vector processor so for example for this loop that we have there you will say to the view virtual land you need to to to add 50 because the number of elements you want to compete on and then you need to move the stride is basically how you gotta access each element of your vector the distance of them in memory and then we are loading the vector a the vector b and then we perform an addition of each element and then what we do we shift right because shift right is basically the same as dividing by two and then we store this vector back and at here we have is this the the amount of the time that each of these operation consumes so you can see that because we have banking now the amount to load this particular vector it will be like for example here 11 just means like it's just just a number that i picked to justify the latency of fetching a particular data in memory so you will execute the first the first read in memory and conceptively you issued this the because the memory is bankhead you can and then in each cycle send the new request for data and then you will just have this like this is the question to figure out how much it takes to load a vector in memory so now let's start talking about gpus i hope i'm not going too fast so gpus are just an intersection of an array processor and a vector processor besides other things but fundamentally there it is this is what inspired gpus it's just really array processor and vector processors so here we have an nvidia a100 block diagram that we here we can see we have'0 cores and you can you can it can have up to 128 correct flu bond and you have like four megabytes of l2 caches and we can see here that we have a hpm memory just high bandwidth memory and for this particular example what we have is like okay let me explain a bit what's hvm hdvm is an emerging technology if i can say emergence but i have been there for a while so basically you start the run ship one in top of order so here what you have in this particular gpus you have five up to five the run chip connected to each other and then you communicate you take data from this ship using what you call what is called a tsv it's called a true silicon virus and then this hbm chips are connected to a memory controller on this memory controller is connected to the to the particular gpu cores but when we come to that soon again but it looks scary like when you see this right you see like what is this like maybe for some people it's scary maybe father it's not but when you look at this at this particular course of the gpus it's what we have there is exactly this that we saw in the previous uh in the previous slide it's we have computation is python in time so basically what you can see there is anything that you can use to point okay i think we can use this in a way so basically what we have here are different functional unit or execution units and because we have all of them they're different for example you have to support different kind of data tapes but for a particular data type we still have some of them right we can have i don't know up to how many we have here but we have some of them and each of them can perform they send me all of them are this particular set of function units can perform desktop instruction at the same time and because each of them is pipelined inside at the same time we are also executing different instruction so this is how a gpu is an in-depth session of a vector and ri processor because a set of functional units execution the same instruction at the same time and inside of each of execution unit we have a pipeline that is is a deep pipeline and because of that we are executing different instruction at the same time too so this is how it oh you just to get a better view of that so gpus are seeing me the engines underneath the instruction pipeline operates like a cvd pipeline like a an array of array processor a vector processor but the this is one of the key difference that we start seeing the difference between the vector array processor and the gpus is that the programming is done using threads this is where the build of gpus reside this is really where when you're reading a gpu paper you're still wondering how beautiful this is put together so here we're no longer using in gpus we don't use similar instructions to compute gpus we will see that we use color and threads the computing gpus and this solves a lot of problem because programming vector is really complicated because you need to do you need to work a lot to fit the layout of your data to the requirement of the particular vector processor and we will see how gpus allow a lot of flexibility in dealing with that so to understand this let's go back to to the example the first place of code that we saw in the beginning so but before that let's discussion between programming models and hardware execution models because i think it's important so a programming model refers to how the program expressed the codes know how you write your code and it can be done in multiple it can be sequence like one lemon it can be data parallel it can be data flow it can be multi-threading for example but the execution model refers to how the hardware executes the codes underneath for example out of all the execution virtual processor array processor data flow multi-processor multi-traded processor and so on at this and the execution model can be completely different of the programming model right for example in modern gpus we write code for cpu sequentially but the underneath underneath more than high speed processor it doesn't follow the one element it's a data flow machine it finds instruction as soon as there is no more dependency between them if you really look at how as a modern cpu works completely beautiful is a wonderful work of engineering we still we will see there but it's it's important to understand the contract that the hardware provided to the software when performing a different kind of execution model so basically what you have here is like the underlying cpu or the underlying device can execute the code as they wish as soon as far as they respect the correctness of your code and in the past there is different processors that only respected the correctness so they would execute your code completely in a different manner that the programmer doesn't know and after that this particular acting should have failed because it failed to adjust to address a different important aspects that is debugging so that particular i don't remember the name of this architecture but it was so fast but it failed because it was so hard for programs to debug so that's why if you look at intel for example a processor you will see that the underlying micro architecture is trying to do things for to do things differently what the pro the program is specified but it addressed 2k things the first one is the correctness and the second one is to provide precise exceptions this affects the performance of the cpu itself yes but at least it provides easy easy programmability of of the the internships so now let's understand more about the different execution models that we have so we have this code and if your execute this code in a sequential way so what you do first you will load the first defrost address the first memory location and then you will load the second and then you perform addition and then store and then you go to the second iteration and and so on so this is how you do it sequentially but you can't explore different parallelism like even from this code for example and let's see which kind of them we can do so we assume that the programming model is sequential and then this sequential model can be executed in a different underneath hardware so we can have a pipeline processor to execute this sequential code you can have a how to afford execution and you can have a super scholar very long instruction processor because it's basically it can unroll this loop and then make them because you can see that these are completely independent so you can complete and hold unroll this loop and make them each instruction complete independent and then a super color can fetch multiple instructions at the same time and if you have available functional units inside of the chip you can schedule them you can execute them in parallel and if you wish to understand more about like the underneath like pipeline processor and all the technology like data flow processing and so on i'll give a talk at the cpp con like last year's about the the foundation of c plus plus atomics this is actually a series of talks and this talk that i'm giving today for we joined them and later on we talk about also atomics on on gpus but this talk is one hour 30 minutes that goes in full details about modern cpus at least for intel yeah at least for the ones that affects how you need to understand atomics so the other programming that we have is the multi-thread so the horization is like each iteration is completely independent and then what you can do you can run each of these these loops in a particular thread this is like is the the multi-threaded paradigm now we start we will talk about the model that's tightly connected to gpus that is the data parallel like the simi the single instruction multiple data so the realization here is also the same as the multitridet we see that each direction of this loop this particular complete is completely independent of each other and the compiler of the of the programmer can generate but a cmd instructions to execute the same instruction for all across the data so basically what we have here we take different interaction let's assume that we have a loop of two elements we take each loop each each iteration of the loop and we schedule it to a particular to a particular thread and then these two threads will always be executing but let's assume for for now the same instruction at the same time so basically you're performing cbd not using photos about threads and inside of each of these threads the the model of the data that you have is no longer vector it's completely scholar so this is this kind of loop is best executed in a cvd professor proof authority of reasons if you try to execute something like that in in a cpu you can get a bit of performance but the overhead that is the gpu is really the touching our proposed and tackles a lot of the paintings a lot of overhead that you will pay you can see this code is completely clean no dependencies so executing this in a cpu we pay a lot of offer heads so let's see like this course is how we this after we see that this code is vectoral but let's see how you can execute them in this particular model so basically as i already said you have like two different iteration of the loop and they add at particular time these two threads will be computing the same instruction so in the first cycle they will load in the second one they will load again in the third one that we add and in the last one they will store you can see they are performing kind of a look step execution here so basically different threads are now putting together and essentially they are sharing the same program counter so these particular modes are so colored single program multiple data the gpu on the main control for that is single instruction multiple thread this is how individual uses it and then just really we can have more and more and we're going forward to this to the stock we will see how we can talk or multiple trades and how is the underlying structure of the gpus so as we say gpu is a cmd machine except it's not programmed using instructions each programmed using threads each thread is the sum code but operates on different pieces of data each thread has its own context this can be treated restarted executed completely independent you don't have this flexibility in a vector processor because the water processor say that you need to operate all the operation at the same time you don't have this independability of your different elements of your vector and this is how gpus give you this flexibility here so a set of threads executing the sum instructions are dynamically grouped into a warp the amed domain plateau is colored wavefront and we're going forward we will see what we mean by a warp is basically a particular group of triads sharing the same program counter executing instruction in a log state manner not necessarily always so a warp is essentially a c media operation formed by the hardware so it takes a lot of burden for the program side and put it in the harder side so let's see an example of of a warp so again we saw in the in the in the slides uh in the previous slide so we can see that a particular group of threads that share the same program counter meaning they are sharing the kind of instruction they need to execute together the approximate the number of which kind of instructions they need to execute next so basically the resistor inside of a cpu or gpu that all this information is called the program counter so in in this case we have like the warp 0 so warp is a set of threads that share the same problem counter again so the warp 0 can be execute the load and after one cycle it can be executed in the next load and then we can execute also uh the art and the store the most important thing is to is to give attention to the work work means a group of threads because we can have multiple warps in a gpu we can have thousands of them thousands is just really thousands set of the triad sharing the same program counter it's what we mean here so a work is a set of trade execute the same instruction the same program counter and then we will see like drops processing units is cmd not exposed to the programmer so you don't use vector instruction and so on you use normal filter vector you use normal scalar instruction and even the code that the particular code that you can use for that is each of these work we execute you can run it even a multi-track cpu but of course it's not for free media vismedi like a single sequential instruction stream of simi destruction in a simile man there you can see like in yosemide we are always sequential right we don't have multiple threats executing the same instruction together it's always a single threat executing first or instructions but in the in the gpu is completely different we have multiple threads executing the same set of instructions so basically what you have is this vector instruction and you also have a defective land basically this means like the the number of elements that you have in a particular vector but when you when you go to the cmt he's the programming model of gpus like single instruction multiple multiple multiple threads we have scalar instructions no longer vector and we only need to specify the number of threads and no longer the number of elements in a particular vector and two major advantages of that you can treat each thread separately you can execute store independent of of each other you can also group threads into work flexibly but for the rest of the thought let's focus on this particular part aspect of it that is like how we can treat each trade separately in a particular way so the key idea also is very fundamental to understand gpu arc to raise the idea of finely graining multi-threading and in gpus we don't we don't do fine you can do with the threading we do fine grading multi-trading of warps and remember warped is a set of tries that share the same program encounter so let's assume that award consists of 32 threads 32 threads means we have 32 lanes or separately 32 functional units inside of a gpu and each of these functional units can be executing a particular instruction at the same time so if you have 32 key iteration for a particular loop and you as assign one iteration for each thread then you will need 1k of warps so let's see a particular iteration of this work and how you can have then so you can pipeline these works and one of the key fundamental aspect to understand here remember in the in the beginning when i said about that gpus don't need to take care of dependencies to certain degrees less like the cpu does so basically defining grain is here states that you cannot have two instructions for the same work in the pipeline right because assume for example you have a branch if you don't have control because you don't have inside of cpu how do you control with which kind of instruction you need to fetch next so because cpus gpu is done what doesn't want to to to to its particular design for regular workloads it does it doesn't perform this kind of control and for this particular reason you have a restriction of not having extra multiple instructions from the same war in the pipeline together so you can only have one group of one work one instruction for a particular warp in the pipeline so for example for example here what we have like the work number one at particular program counter the program countries with instruction the warp is executing so the warp one can be executing the first load you can have the war up to execute in the sec the the executing the the second load because it already completed the first load you can have the ward 5 computing the art and you can have the the the what for computing the store so we can see we have different works in the pipeline and these warps again are set of the threads operating on different pieces of the data there's some instructions so let's talk a bit more about finding grading multi-threading as i would say and we will see like going forward how how it is important so we just take like on the advantage of the of the move finding multi-tradix so at the at the at the hardware level you don't need uh logic for honda control and data dependency within a threads it tolerates control and that depends latency by overlapping we see going forward what what we mean by that its improv pipeline is utilization between advantage of multi-threads but it's also have disadvantages so seeing a single threat perform suffers a lot as we saw in the trade before right so basically you cannot have a this this particular group of instructions is assigned to a particular thread but you cannot make a particular thread be executing multiple instructions that it has in the pipeline you can always only pick one so for single thread it's really problematic and we will see like if you don't have enough number of threads performing computation of gpus takes you almost nowhere if your your goal is to take is to gain performance so again we can see here finding any multi-trading of warps here we have a particular idea yeah we have an example of a work so it came for this paper any video tell us unified graphic computing so you can see a particular wrap we can see the common program counter we can see the number of scholars threads inside of the work and we can see that the instruction there are complete this color and then here we have a example the final grant so basically each time the underlying architecture pick a warp to send them to the pipeline so these are also a particular example for any video of a gpu architecture we have a memory system connected to a memory controller we have inter interconnection network and then we have this what's called a shadow core and inside of the short code what we have we have the register file together with the mask we will see how important masks are later on but what we do we fetch particular instruction for a particular work we decode this particular instruction and then we assign particular threads inside of the web to each of the cmd instructions so that's why you have multiple lines inside of our gpu so this is the idea of defining animoto trading it also provides one particular benefit on gpus so it's basically when one when you fetch for example as you can see above you have different warps and then we perform the same thing we fetch with the code we access resistors we perform computation but if a particular work needs to access the global memory that takes hundreds of cycles what defining grading multi-trading we do it will take this trade away we isolate it while it's requesting the data and we use the pipeline to compute works that have the data already uh in the cache or in the registers so if you want to understand more about finding any multi-threading you can see all these these lectures and here we have basically how the how you can execute in a cmd manner instructions depending on the number of the functional units that you have so basically this is the example of computing when you have only a single functional unit and this is the example of when you have multi-function news inside of the gpus here what we're doing basically is distributing each element of the two different vectors that we have to different functional units that we have inside of the of of of the gpu and then we this what we have here we can see like functional units that share the same number of resistors they performed what is called the cmd lane so this cbd lane is basically on the place where a particular thread inside of the warp is scheduled so if you have we have here 40 midi line if the work have four threads each of this thread will be assigned into this particular for a particular lane and also okay skip this event because of the time so here we have a graph is specifying the performance of modern from old to modern gpus architectures we can see that we should gain more and more performance and basically the performance here comes for adding more and more functional units and also improving the communication system on the memory system for cpus so now okay finally let's talk about softwares right let's talk about chief chief use of the hierarchy okay we'll just this again so gpu said make it easier to program to in a style of cbd because you don't do with vector instructions and sound so use color instructions so it makes it easier to program and gpu was particular design even though they are becoming general purpose gpus but i still believe the fundamentally right the most throughput that you can gain for a particular gpus if you have any if you have regular parallelism if you don't have regular parallelism you're just gonna pay a lot in gpus because basically of the fact that you have similarization you're basically not using a lot of your line and you have a lot of overheads there so many workloads exhibit this inherent parallelism like matrices image processing deep neural networks and but this is not for free right you cannot just took a particular code and then execute it completely gpus because gpus they bring a new programming model a different way of setting this instruction to be executed in the in the analyzing with the instructions so that's why in the previous slides we talk about programming model and execution model because to fundamentally understand why would you write gpu codes you really need to understand the underlying at least reasonably the underlying architecture so that you can really reason and also to help with the back your gpu program so you need to understand both so algorithms need to be re-implemented and returned but you still have some bottlenecks inside of gpus basically you need to to to to to to perform cpu and gpu data transfers so basically you need to move data from the host cpu to the gpu before computing on them so and you also have a lot of drum memory bandwidth basically we talk about the the global memory so gpus are becoming faster and faster but when you need to go out to access data from the memory you still pay a lot of but it's not so fast comparing to how fast the computation units are getting so this is still a bottleneck and also sometimes you need to do with the data layout of a gpus so basically here we go a bit faster so here what we have here actually an example of cpus and gpus you can see that the majority of space that we have in gpus is separated for computation while in cpus what we have we have caches we have controls because we perform a lot of the heuristics we have a lot of structures to for example for branch prediction and many other kind of uh elements that we have inside of cpu that's required controls but you can see in a modern cpu the computation is area reserved for cpus is really small right the rest of our gpu or modern gpu say like probably more than 80 percent of a gpu ship is designed specifically for memory interconnects inter interconnects and the different cache level members that you have but in gpu is completely different it smelly i think like the researching gpu is particular going to the same direction of cpus we see each dpus been adding more and more memories also because fundamentally both share particularly the same problems they are too fast in computing but they still problems and fetching data from memory even though the computation is different depending on the parallel that you have and gpu or cpu can be faster now let's see how the gpu computing is performed so computation is offloaded to the gpu in two steps what we do we perform the cpu gpu transfer we move the data from the host cpu to the gpu memory and then we execute the kernel so the column we compute inside of the gpu once the computation is done we move the data from the gpu back to the whole cpu if needed webs are not exposed to the gpu programmers and you see like the pro what the program deals in gpus when programming is blocks and not warps and the hardware takes a particular group of trades and dynamically form works for each block and normally what we have in a system is a kind of heterogeneous system where we have cpu gpus and we can have other devices but and then we need to understand which part of a section of a particular code we execute in a cpu and which one we execute the gpu so sequential or modestly parallel section of your codes you likely executed in a cpu and massively parallel is what you execute in your gpus so basically traditional cmd code basically look like that have syria code being executed after a while you will launch account to the gpus after a while you execute a bit more of serial or modestly parallel code and then you execute a massively parallel again on the gpus but just like in cpus the amount allow still applies also because the serial part of your code can be a bottleneck is a bottleneck but as i said before we talk a lot about warps but warps are not exposed to programmer when you're programming for example individual gpus what you deal with are blocks and you assign and and you assign a particular number of thread four blocks and then the underlying hardware will map a particular set of threads to warps i don't know if we have time to go through this so we skip this a bit my sister okay this i think this is worth saying so let's talk about the cuda on opencl programming model we have book synchronous programming as we saw like we we send we compute on on a cpu after a while we send the mass the massively parallel part of the code to to to to the gpus and then what you have is is a kind of synchronization because you compute seriously and then you send a partial support to the gpu and then you can execute again some part of serial some part of the coding gpus and then execute again in the in the gpu so we have the whole cpus it's typical of the cpu we can have a device that is typically a gpu but in all the platforms you can also have fpgas and other accelerators that can also collect devices and we also have grids the grids basically the number of threads that you want to execute in the gpu that we execute the same kernel and then this amount of threads are divided into blocks and inside each block you have a particular thread okay see so function prototypes we have in normal function function we have like the the the the return type and then the name of the function and then the argument but for gpu function we need to use the global to specify that this is a kernel and you want to execute that in the gpu and then we have the name and also the parameters but we still have the main function in the in the host cpu and what we do we first we allocate memory for in in the good device for that we use the kudama log and then we also transferred the data right from the whole cpu to this allocated block of memory in the gpu itself and then we perform the execution configuration the execution configuration is basically the number of threads that we execute a particular corner and in how many blocks you have you want to have in in in your system in your particular program so the column will look like that is how you launch a kernel you put the execution configuration and the particular argument and then after the computation is done at the gpu you move the code you move the results from the gpu to the host cpu so we can see like automatic variables that you decline those cc plus codes automatically map to resistors inside of gpus and then we also have shared memory that's the particular region of memory in the cpu memory hierarchy that can be shared inside of a particular thread but i don't know if we have time to see that for example the last gpu that launched by nvidia now even between blocks that belong to the same block cluster can show the same can sure the can access the cache of each other and then you repeat it as needed okay i wish i skip that go to the example okay okay so basically we're talking more about the configuration the configuration basically is like the number of trades that you want in each block and then you also have a particular number of blocks and then you can launch your car now so you will see like in the next slide how how this is applied so in this code what we have is a function so we first declare three arrays and then we allocate memory in the gpu memory for these arrays after that we move the data that are in the host main memory to this particular arrays after the data is moved there we assign the configuration in this case you say like i want number number the thread for for block like you say like 512 and then you sell so the number the blocks itself after you have this you can launch your kernel after the kernel have completed the computation you can move the result of this computation back to the core cpu and then you can use cuda free for example to deallocate the data that you just allocated so a kernel as we say is preceded by a keyword global to indicate that this is the gpu kernel and it uses special cable wall to discretion the which piece of data and also each thread in a particular web should execute an on so this k was like block id block dimension and trade ids i don't know if we make that but i have a lot of slides on this i don't know if we make to this so this is basically here we are assigning a particular we're trying to find which index a particular thread inside of a particular verbs that belongs to a particular block we work on i hope we can make to the to the examples that i have on how it specially work so this is basically how do how you access a particular index like in the vector processor we see like we have indices and we kind of in parallel in each of these indices but here we're trying to find in each of these a you can see that a and b are like vectors and we have multiple threads you want that each specific triad operates in a particular piece of data so how you define which piece of data each thread should operate you basically use these keywords in this manner for example you can use in a different manners it really depends on the layout of your code to give each stress which piece of data they should work on and if you saw this just look a lot of plain c code and if you want more fancy c plus plus cycle that is this libre that that was developed by nvidia that can make your legs very modern c plus plus like gpu code but the underlying understanding like thread and how easy gpu execute doesn't change because of that they still completely phenom fundamental so you can also check this website to see which kind of features for each standard the cuda c plus support so let's go to the example of the vector addition so basically we assign one one gpu thread to each element which then we want to like to perform vector audition so we have like each element of each from one vector and the other vector we want to perform additional then so what we do we assign each gpu thread one element weighs addition so you can see here for this this basically means like a particular thread so for this particular thread we assign a particular wise element-wise edition and then the problem like you can have thousands of threads you can define in your configuration i i want like two thousand three thousand threads but gpus architecture they don't they they are divided into course and one core only supports a particular number of so you can basically to distribute the code in a more fermenter so that you don't have parts of the code that not been used you need to divide your computation into blocks so that now you can have multiple calls on each core inside of the gpu chief will work on a particular block or more so then we divide the computation into blocks and then we send each of these blocks to a particular gpu core so this is a particular example so what we have here is the device the chip itself and then we have for example the kernel a kernel grids basically the number of trades that you want to execute your code you divide this thread these threads into blocks and then depending on the number of cores that you have in a gpus the gpu can schedule this block in a different manner so there is no particular for example for the prior before the age 100 then the new jeep there was no assurance which block we execute with each block so basically have no control then the gpu output is completely free to pitch any block that you want to schedule it and whatever you want depending on also in the number of devices basically cores that you have in the same ship so here we have a particular scheduling if you have only two cores in a particular gpu and here if you have two cores in a particular gpu so in the h100 the new architecture that was launched this this year so what you have now is what is called a thread block cluster so it's a new hierarchy in the software hierarchy so you have grids after grids you have blocks after blocks you have threads but now the the new any video chip add more one it's colored thread block cluster so basically now you can take blocks and group them also and block of threads that belongs to the same cluster now they can synchronize and they also can share memory so basically in this h100 and before transformed assembler could never share the same cache each of them have on cash and you can only synchronize and share data between the threads in the same block but the h100 changed that now threads from different block belonging to a cluster can execute can sure can access the cache of each of each other again out of time so this is basically the computer the the compilation i think also skip that so you can go to more important stuff i don't think i have much time right now just keep up it okay let's talk about gpu member hierarchy the kind of memories that we have you have in gpus so this is a particular streaming processor in a gpu so you have resistors that accessing them typically takes one cycle you have also caches for example that to access the l1 cache take like five cycles and you also have a shared member that's basically a scratch pad a scratch pad and the difference between scratch pad and the cache is basically that you as a programmer you define what to evict and what to mend here so for example you say like i don't want the configuration of my cache because keep evicting the data the rest you need and you see that a lot of threads use have much more use of particular piece of data so you store them here so that it's always a hit because you think that we have actually your program is good when this particular data is always local you also have constant caches that take 5000 to access you have a global memory that is close to the chip itself and you also have a l2 cache and an l2 cache so this is also well we also don't have time to do that it's basically the difference between the the in terms of fetching data from memory what the h100 are to the system before to fetch data from drum we need to go to le2 le1 and the resistor file itself the h100 now skips that it give you go to drum to to the le2 you don't go to the references resto file and anyone you can skip directly to the memory cache directly so you bypass two of them and the new one that was launched this year add more to that so before a thread was the responsible to calculate the address that you want to take from memory the new architecture now are the specific computation units that perform the calculation of the address asynchronous to the computation going on to the threads and then i think also skip that i'm sorry so here what we have is the code of arab type qualifiers on their lifetime so we have an ins is a local variable it's always and it's always assignment to a particular resistors and an array is have is stored to the global memory the light the scope is also a threat and the lifetime of this is is also a threat but depending on the available number of resistors and how good is your compiler this array can so be assigned to address to a group of resistors and then you have device shared variable is is placed in the shared memory as we saw and the scope is a block and the lifetime is also block and then we have the devices it's stored in the global memory the scope is agreed so when a particular viable have these have that is is of the device i can call it so basically we have a type of is a device so the wall grid of thread can access it is accessible to everybody in the grid and then this is typically just the height of memories now in our gpu i don't think we have time do you have that yeah so we'll talk about the performance configurations but can i go like five minutes okay so performance confederations we have the main bottlenecks on gpus is the transfer is the data transfers and access to the global memory so light is hiding you need to be good to hiding latency so you need to have the necessary amount of thread to compensate the finely grinding mechanisms of worms that you have in your system we are called this occupancy occupancies is basically how busy is your final grading mechanisms or your pipeline you also have memory card listening so you see like it's really important because of of of of of of of locality you always want that when you go to the global memory when accessing is so expensive you want that the particular element that you fetch is useful for all the trades through for example in case of matrix normally you need to take a row and a block right so the block is problematic because if it is if the thread is a storage in memory in a row measure it's mean like you have one use in every cache line that you fish from the global memory because you only access in a particular block so what you do in the memory collecting you change it right you make here for example you can make your your your your column to be a role for example and many other techniques have been used so that we use shared memory and also see me the warp utilization symmetry warp utilization is basically sometimes you don't execute threats because of the mask that you have there so basically you're executing and then you have divergence each thread goes to a particular path and then what you do basically is normally the gpu we do with a cute a particular number of thread that goes to one direction and when it is executing all these threads sounds some lanes instead of gpus are not being used it's being idle because you're not competing on them and then when you perform then the threads that go to the different part is the same thing so there is a lot of proposal on how to do that and one of them is basically to aggregate trades that goes to the same branch dynamically this is one of the ideas of how to mitigate for example tread drive versions and there is all the other consideration like atomic and the data transfer between cpu for example here is like sometimes you don't need to completely wait to move your data from the c from the cpu to the gpu you can do it in states like a pipeline you move a particular chunk of data to the gpu you start the computation and then you start doing it uh like a in a pipeline manner rather than completely move huge amount of data to the gpu and then start the computation so this is what we yeah i think it's horrible and also this particular tool that you can use is provided by any video that you can basically calculate the equivalency of your threat of your specifications so there are not important things on gpu that is worth considering is bank conflict free right the conflict so we say like just like in a vector we are banking the memory but it's not enough to have your bank memory you need to put your data to fetch indifference in different banks right because you can only send requests in each cycle if each memory that location that we want to access is completely in different ranks so this is the height of scenarios that you want in a gpu you want each piece of the data for your thread incomplete independent banks so that you can fix them in each cycles it doesn't matter if it is just try this one or a random the most important is that each of them are in different banks the problem is when you have something like this right you have like for example if your stride is basically the address that you need to access the next element that you want to compete on is for example a stride of true so you mean like you're not using some banks and then what you happen is like you need to serialize some some requests before your bank is busy for example because now for example you can see like while the the bank is processing the request for the bank zero it out it's later received the request to process also requests for the trade number eight but the brass can only process one request unless you have memory multiple memory ports but it's not the case here if you have one memory port this is very problematic because you need to serialize the access and it can become worse basically you can have you can make completely poor use of your banks if you can have this kind of this kind of data layout for example in your memory system so i will be outside happy to take any questions so thank you very much for your attention