it's becoming increasingly common to start using machine learning or ai driven techniques to make decisions the world over so for example, you know credit checks health checks, and these can be life-changing right, so it's really important we get this right you could find yourself turned down through a mortgage on your dream house because quite literally the computer says no let's talk a little bit about classification. so now we have a data set where we've got labels all right, so we've got some input features or input attributes or dimensions lots of instances and we've got some labels for these attributes all right, and so we've got for example books and the type of book or music and the genre with the music things that we want to start to try and classify so supervised learning is the idea that we've got labels for our data. so we're still gonna have instances we're gonna have attributes or dimensions to our instances. but we've also now got labels for our data and so classification is the process of learning how to correctly assign these labels to these instances before we start talking about classifiers let's talk a little bit about the learning process and machine learning process we want to use it's not enough to say i've got my data set and i can correctly predict all of the classes right because then someone will ask well what happens if we have any new data that we haven't seen before right? maybe you've got some medical data and you can correct me diagnose all of the diseases but a new patient comes along and you could incorrectly diagnose the disease, right? that's not helped anyone what we need is a regimented way of training and testing these approaches so that we know how well they apply in the real world so what we're going to do is we've got some data set just like before where we've got some instances and we've got some attributes this way and so, you know we might have a lot of attributes a few it doesn't really matter and we also now have our labels which we often call y right but this is going to be a vector of all of the labels for data, so this could be label one-one b's could be a few twos down here and this could be a few three so this is a bit like our tennis example where we had this is the weather outlook and are we going to play? tennis today, right? yes, or no so that you could have multiple labels or just two for binary classification it's not enough just to train a classifier over all this data we want to make sure that this classifier will work properly when we apply a new data to it so what we're going to do is we're going to separate this data into training sets and testing sets so we're going to train on the training set then we're going to test as we go on the validation set and then right at the end when we're finished we're going to do a final test on our test set the reason we do this is it's a very safe way to make sure that we don't accidentally gain the system we don't accidentally report incredibly good results on the training set but that's because we all just show the machine those things so we hold out the validation of a test set for later to make sure that it will generalize now exactly how much of your data goes in the training validation and testing set is really up to you right typically you might use something like 70% for training 15% for validation of 15% for testing that will be quite a reasonable way of doing it so what are some good classifiers we could use given that we've done this right? let's imagine. we've got our instances we've got our attributes and we split them up probably randomly into training validation and testing what we want to do is train our classifier on the training set and then test it on the validation and testing sets to see how we're getting on so what algorithms could we use? let's start with a simplest. one of all zero are in zero are we just take the most common label and that's what we predict every time. it's v you've got five minutes until the deadline just hand something in approach to machine learning in the case of playing tennis or not playing tennis we could say well i play tennis more than i didn't so we'll just assume that i'm going to play tennis and predict. yes all the time all right, regardless of what the weather is this is not a good way to perform machine learning but i suppose it does give you a baseline accuracy, right? if you're baseline of just yet saying yes to everything is sixty percent accuracy then if your machine learning doesn't perform at least a 60 percent, we know we've got a real problem we can go one better than that we can use one r one r is where we pick one of our attributes we made classification based only on that and then we pick the best of those attributes i mean, it's slightly better than 0 r but not a lot so you'll find you will find references to bees in military too a little bit but not very much because we use much more powerful approaches to this. so let's talk about one example classifier is very popular and that's knn or k nearest neighbor let's imagine. we've got a to attribute data set. so i like to draw in two dimensions. it's just a little easier for me and so we've got attribute one and attribute two, and we've got some different data points in here now don't forget also that each of these is going to have a prediction as well so if this one is going to have let's say a label if we did play tennis when we want to test a new data point an unseen data so a new person comes along who may or may not play tennis. they're going to appear over here we measure them and we find the k number of nearest neighbors to this point so that's this one this one this one this one and this one so this will be 1 2 3 4 5 6 this would be k of 6 and then we take the majority vote or the average of these responses so if four out of six of these people play tennis, this would be assigned to play tennis so the output is what in the existing data set. have we already seen nearby? and can we use that to make a prediction? so this is quite a good approach obviously choosing k is a little bit difficult to do right and this starts to get very very slow when you've got hundreds and hundreds of dimensions finding for k nearest points to a point when you've got tens of thousands of dimensions or tens of thousands of instances, it's not easy to do even with good data structures why it starts to get slow quite quickly nevertheless. this is an effective and popular approach are there any alternatives there is one decision trees. all right, now i like decision trees they have a nice benefit that once we created a decision tree which is just a series of decisions on is the data this yes, is it this? no, once we've done all that we can actually look at the rules and say ok. that's how a decision was made and that's quite a good rule set. so kind of a way of lighting a sort of if-else programming language, but you're doing it automatically let's draw out another data set so we've got our instances down here and we've got our attributes here and remember for each of our instances we're going to have some label that we're trying to output all right so here well you know 1 2 3 4 5 6 and so on so let's imagine but this is a credit score by a credit check so you've got actually boots based on how much money you've got how much you spent me to me if you already have other loans and what we want to do is make a decision as to whether you should be allowed more credit or not, right? so the answer is yes or no quite simply so a decision tree is going to partition the data up based on the attributes so let's say the first rule is credit rating credit rating you know greater than or equal to 5 question mark and if the answer is yes we continue if the answer is no then we actually output a leaf node here which says credit denied here we say, okay, so the credit ratings are by five. it's not a no yet now we say okay do they earn? more than let's say 10,000 a year or something like that and if the answer is yes, we proceed to the next stage if it's no then they don't earn enough credit denied this is what a decision tree does now you don't have to design this yourself. there are algorithms to produce decision trees for you the way they will work is they will pick one of these attributes at each level that best separates for data out so for example you've got a lot of different instances of yes and no decisions in your training set is credit rating the best way of separating out the yeses and anodes and one of them is going to be best for each individual step and we can use all of them in a tree structure like this until we get to a series of leaf nodes which end up with only yeses and only nose and then is very simple to apply this when you data comes along we apply these rules and we get to a decision a decision tree is going to be equivalent to programming a bunch of carefully chosen if statements but of course the benefit is that you can do this over a huge number of attributes very very quickly without having to do all this yourself, right? so yes, it's not much better than doing it yourself, but it's much quicker. so let's have a look at this in some code we're going to change and use a different piece of software today because for things like classification and prediction we're going to use weka it's a very simple tool that makes applying things like decision trees. very very simple and it has some of the same data cleaning processes as our does but in a graphical form, we've already prepared our credit report right so we've got credit data where we have a number of inputs things like how much money do they make whether they've defaulted on any credit before we have these in a file so i'm gonna go in here i'm gonna find my file. it's gonna be in here right now. you can load up various file types json files for example, we're gonna load a csv. it's our credit data. so we have about 600 rows of whether or not people i think it was japan this data originally came from were given credit or not so we have things like age debt marital status whether they're a customer at the bank already whether they've got a driving license what their current credit score is and you can see that what weka has done is load all these work out whether they're nominal or values numerical values already so for example credit score is a numerical value and you can see here a quick histogram that shows the different types and whether they've been approved for credit approved at the bottom weka has interpreted as the output or the classification that we're trying to achieve alright, so in this data set we have 307 you can almost see that font 307 approved and 383 denied credit. so let's train up a decision tree and see how it does. so we only go to classify we're going to select a decision tree. so we're going to choose we could choose 0r that's not so gonna go down to trees and j48, which is your standard decision tree we're gonna use a percentage split and we're going to select 70% for our training set. this one doesn't have a validation set we're gonna be predicting whether one what they were approved and then we're gonna train up like this what happens this weapon will train the decision tree and then it will produce for us some measurements of its accuracy you can see it's correctly classified 85% of the testing set which is good. i mean, it means a lot to these people so maybe those 15% could be a bit aggrieved and then we get a confusion matrix down here so we're saying that of the yeses a 76 were correctly allowed credit and 22 were denied incorrectly and if the noes a hundred were correctly denied and nine were accidentally allowed, right? so that's the ever we can see here now the nice thing about decision trees is we can now look at these rules and see what they are so we can go into visualized tree and so you can see that the most important attribute that is decided on is whether or not they defaulted on a loan prior to this. so anyone that defaulting on a loan before is immediately denied credit if they haven't default on a loan then it starts to look at whether they were employed and if they are it's going to give them credit all right. it's a simple rule system and it's the best it can do given the amount of data we've got if they aren't employed, but it's going to look at their income maybe they're self-employed gonna make a decision then whether they're married where they live and their income again right, so you can use attributes multiple times to make complex decision making processes so this is a very simple tree which actually has performed pretty well on this data set and it's not a huge data set for 85% that's not too bad once you've used a classifier so knn or a decision tree to classify your data you want to know really as how well as it performs on your testing set so you could quite simply calculate accuracy so what is the percentage of the time that we were correct iein? obviously that's going to be hard to do for many classes, but for credit yes or no 85 percent is not bad right if our if our average was guessing at 50% it's quite a lot better than that there's another type of classified as perhaps a little bit more common these days and a little bit more powerful with decision trees and that's the support vector machine. so what is a support vector machine? well what we're going to try and do is separate our classes based on a line or plane or some separation in the attributes that we have but what we're going to do is try and maximize a separation between these two classes to make our decision more effective so let's imagine we have two attributes just like before so this is actually because one misses attribute two don't forget this is labeled training data. so we know which classes either been already. this is not like clustering so maybe we have some data over here and we have maybe some data over here now obviously this is our quite an easy one we're going to try to find a decision boundary between these two classes that maximizes a separation so for example one decision boundary we could pick will be this one here right, but it's not perfect because it's very close to this point here and it's very close to this point here so these are on the fringes are being misclassified right and you've got to think that this is just a training set if we start to bring in testing data that may appear around here or around here. maybe that's the stuff that gets misclassified so what a support vector machine will do is pick a line between these data points where the distance to the nearest point is maximized these nearest points are called support vectors, right? so this margin here is going to be as big as we can get it so you can imagine if we move this around the margins going to get bigger and smaller now the nice thing about support vector machines in a kind of almost reverse pca approach you can convert this into a higher dimensional space and perform quite complicated separation of things aren't really obviously separable like this things that are essentially we have to have a nonlinear decision made, right? so not a simple line something more complex like a curve a lot of the time we're going to look at precision and recall so recall is a measure off for all the positive things but all the people that should have been granted credit how when even actually were like so we should have said yes how many times did we actually say? yes, right and that's a measure of how good is our algorithm at spotting that class and precision is of the ones it spotted what percentage of them were, correct? you can imagine a situation where your recall might be very high because you've just said yes to everyone right? so yes you spotted every single person that should have got credit but also your precision is low because you were giving it to loads of people who shouldn't have had it, right? so a really good algorithm is going to be one that has a very high precision and a very high recall right, and we combine these measures into one score? f1 or f score and this is going to be a value between norton one. where one is absolutely, perfect. and zero is doesn't work at all. where did our training data come from in this case? we've got our train date off internet, right? but if you're a credit agency then what you're going to do is you're going to use humans to make these initial decisions then you're going to train a machine and you're going to test to see whether it can do as well as people can do right maybe there's nuance there that this decision tree couldn't capture those 15 percent of people that were misclassified is there something we could have done better to help those people? so what you'll find it happens in practically is your trainer system but maybe you don't rely on it entirely maybe for the very obvious yes is we can use a decision tree or some other classifier to just say yeah those people are fine maybe for the obvious knows we can say no they're not going to get credit but for the edge cases the people in the middle, maybe that's when we bring a human into the loop so in our data set for our training examples we're going to have all of the attributes and then we're crucially gonna have an already known label for that data but says yes that person was denied credit or they were allow credit. right? so we're going to use those training examples of input attributes and output yes or no decisions to train our classifier and then we're going to test the results and whether or not it'll work when we use our unseen test data for unknown cases classifiers let us put groups into discreet labels yes or no a b or c depending on what our situation is. they're very powerful and as long as you've got enough training data we should be able to use them to make real-life decisions what we want to do going forward is start to move from just yes or no to can we actually produce output values you know, can we regress actual values out of the these algorithms? let's talk a little bit about something more powerful that's artificial neural networks now anytime in the media at the moment when you see the term ai what they're actually talking about is machine learning and what they're talking about is some large neural network. now. let's keep it a little bit smaller for this but let's imagine what