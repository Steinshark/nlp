today i want to take you through how we're using large language models in a software as a surface platform that i recently launched called learntail we had to make some design decisions that i want to cover and we're also running into some issues and i'm actually really curious to hear your thoughts about it now with learntail you can create quizzes from anything including texts websites and even youtube videos the idea of this platform is that it's a way for you to learn more actively instead of just watching a youtube video or reading a blog post you can immediately train your knowledge by doing a quiz about it if you want to try this out yourself it's free go to learntil.com i've also put a link to a quiz on learn to help below most of my videos if you go to the description you can simply click there and do a quiz about the topic of the video you don't need to register or anything it's just something that i want to give to you to help you learn better now before i dive into the code there's one more thing i want to get off my chest you know i've seen so many videos or blog posts of people talking about how easy it is to launch a platform like this well from my experience launching a software service platform is really hard people just underestimate how much work goes into the mundane things like being able to register and log in and delete your account having a paid subscription billing and accounting international tax compliance privacy policies and terms and conditions security measures making sure you don't pay through the nose on cloud hosting cost and the list goes on and on i want to avoid you falling into the trap of thinking this is easy so what i'm going to do in the coming months is post videos about what we learn while we're developing and improving the platform i won't share the full source code with you it's just way too much and i still want to turn this into a commercial product so we need to keep some secrets right but i'm going to more or less develop this platform out in the open and i'll share everything that goes wrong things we have to do to extra problems steps we took to gradually improve and scale the surface and my goal with this to make sure that you have a realistic view of what's actually involved in building a product like this and at the same time provide you with a sort of blueprint of how you can do it yourself so i've created a new playlist where our group of videos i do about learn so so you can watch them all together or completely ignore them if you don't like me doing this kind of stuff now don't worry i'm not going to turn iron codes into a learn till only channel or something like that still mostly post content on python software design and architecture and any other new cool stuff that i think is exciting in the software industry now i've talked a lot so let's take a closer look at how learntail uses large language models so the way we generate quizzes in this system is that you can either enter a text or description you can enter a url that also includes a youtube url so in the code itself we need to do some work in order to extract from those urls before we can actually create a quiz about them at the moment the way this works is actually pretty rough so we're using beautiful soup to retrieve text from a url so we're using the requests packets to get the url data and then provide that some beautiful soup that parses the html then we simply get the body tag and then use the get text method from that body tag and strip it and then return it and that's basically it so it's a pretty stupid way of doing it and this definitely leads to problems for example we have some cases where the quiz was actually generated about the cookie statement instead of the actual text on the website and of course the body of the html contains way too much way too much information so currently investigating mechanisms of doing the smart one way we could do this is by simply trying to come up ourselves with a couple of simple mechanisms to clean up the data from the body tag another thing that we're thinking about is actually sending this to gbpd and then asking it to clean up the data and provide a coherent text based on the html data and then we can even look into the fine-tuning mechanisms that openai recently opened up so that could be an interesting path the only problem with that is that it's going to increase the response time of generating a quiz and i'm not sure we really want to do that because having a system that's fast is also nice what do you think is the better method should we go for letting gpt analyze it and having a larger response time or should we try to come up with something semi-smart ourselves that's going to be a bit faster so that's about getting text from a url for youtube videos we do this slightly differently because of course the page itself doesn't contain much information if any at all so what we're doing there is that we're using a library called youtube transcript api that allows us to get the transcript of a video and then use that as the basis for the quiz the problem is that this uses a undocumented feature of the youtube api so it's completely unclear google can basically decide to switch off this feature and then we won't be able to do this anymore and there's other options such as rate limits and other limitations that we have to work around so this has been quite a challenge one thing that we are currently working on is adding a fallback mechanism so that getting a transcript if that doesn't work for example because of some youtube shenanigans or because there is maybe no transcripters video there are definitely videos whether it's not transcript available if it's not there then well we can't use that to fill the quiz around it but of course what we could do in that case is try to at least build a quiz based on the title and description and those things you can get from every video using the youtube data api by the way if you're enjoying these videos where i share a bit more about how we set up the whole system you might also be interested in my upcoming software architecture course i'm going to launch the course this year you can pre-register for this course for free just go to ironwood code architect to learn more i've also put the link in description of the video so how do we then actually create the quiz so for that we're using lang chain and lang chain has an integration with open aisgf model we're also using the pedantic output parser so this allows us to define an object for example here we have to quiz objects so that has a slope a source we store the keywords quizzes are created by users so as user id adds a couple of other things as well and then the quiz itself has questions which are the actual questions in the quiz so what do we need to do to then actually create these quizzes well one thing we learned is that if you integrate your application with an ai like open ais gpt model that it's kind of finicky there's all kinds of limitations that you have to work around for example the most important one is the number of tokens so gpt 3.5 for example does have a 16k token model which we switched to we use the 4k token before and for gpt4 you even have a 32k token limit so that's pretty large but it's still possible that if you have a large web page or a very long youtube video that you actually surpass these limits and your system needs to be able to handle that so we've built a simple chunking system that takes long text and chunks them into several parts that we can now concurrently send to the openai api and then when all that data gets back we combine that into a single quiz but there's other things you have to take care of as well we noticed that the api sometimes doesn't respond so then you have to retry it a couple of times or in some cases we're actually not getting valid json data back even if we told the the ai explicitly under prompt to provide us with a json response or perhaps you still reach the token limit when the ai generates the quiz because the problem is that those 16k tokens for gpt 3.5 or 32k for gpt4 includes both the inputs and the response from the ai model so that means that it's very hard to predict what the actual limit is of how much text we can send to the api so that we take the company there's enough room left in the token space to actually create the quiz because if the quiz is not complete then we get an incomplete json object that we can't parse and we can simply not use the result and there i really see a key difference between integrating with traditional apis just sending a request and getting response versus integrating with an ai api with the ai api there's just more things to go wrong and part of that has to do with i guess the current popularity of these ai models and at least to some scaling issues but part of it also has to do with the ai being unpredictable in some cases and that's just extra stuff that you have to deal with in your application the final thing that i want to show you is that we have our prompts so here i have actually a shortened version of that prompt so we simply ask it to create a multiple choice quiz and we provide it with these templated values that you can then change in settings what we're currently struggling with is getting the ai to produce higher quality quizzes and what we've realized is that the ai doesn't know the difference between an easy medium and hard quiz so we're going to have to adapt the prompt to supply that information and basically teach the ai what is an easy quiz what's a medium level quiz and what's a hard quiz how do you actually create those so we're currently doing some experiments with that another thing is that if you look at some of the quizzes that i've been generated by the tool that's typically the longest answer is always the correct one so that makes them of course way too easy so we have to instruct the ai somehow that it should generate answers that are approximately the same length but by simply writing that in the prompt it doesn't work that well it doesn't really give those results back now that's with gpt 3.5 perhaps with gpt4 it does a better job we we're still looking into that but we spent a fair bit of time experimenting with different prompts and different ways of asking for the information and checking what works and what doesn't so that's another thing that we learned from this is that if you build a tool that integrates ai you don't just write tests for your for the code that you're actually right you also need to have a testing methodology in place for testing how well the api works with specific types of prompts and that's pretty hard to test because of course the ai doesn't respond in the same way every time and we like some randomness i think what could be really cool i i don't think that exists at the moment is to create a prompt testing library sort of like hypothesis which does property-based testing but then it would generate prompts maybe even using ai gets the response and verifies that it actually corresponds to what you expect of the prompt and then can vary those things and and test it automatically i think something like that would be really cool to have would be definitely be useful to us if we have some time maybe we will set up something like that ourselves and publish it as an open source software so there's lots of things we've learned so far from using these large language models in our backends and there's lots of questions we still have that we don't have an answer to at the moment we're trying to figure out what the best path forward is and of course when we learn from it i'm going to share this in more videos with you if you have any suggestions post them in the comments i'd love to hear i hope you enjoyed this deeper dive into how we integrated an llm in our backend so like i said we use lang chain for this if you want to learn more about how line chain actually works how it's been designed and what are some of the possibilities with the land chain i did a full video about that you can right here thanks for watching and see you next week