okay hold on whoa whoa whoa whoa your cpu may be faster than you think i've been arguing with one of my friends lately about cpu and gpu optimization my friend sent me tons of nvidia propaganda about gpu acceleration you can't put discord notifications in a video every single person in this video just got triggered and went and checked their discord now look at them they're confused about what's happening how faster as pi towards gpu than cpu blah blah blah blah and i can't really agree with him i have to admit that i'm a little bit too obsessed with cpu optimization after taking a course called parallel and throughput optimize the programming last semester but i still think spending all the days coding in javascript makes people spring sales shrink and many people have already forgotten who is this man who is he who is this man who states publicly programming with javascript makes your brain cells shrink oh my goodness what a beautiful man here and how fast cpus can really be i mean you don't need to think about cmd or even multi-threading when using node.js as a [ __ ] back-end developer [music] [laughter] i love when someone's just like when someone's just like yeah no it's really easy to use you just you just spawn n minus one node processes where n is the amount of threads you have and you're just like dude that's how you're solving it first off that's like a whole system onto itself what you're saying what porque maria speak english let's take another look at the performance comparison in this pi torch community post in the post this user mentioned bgg 16 is 66 times slower e5 2630b3 cpu compared to a titan x gpu so what is the theoretical performance difference between these two hardware the metric we are using is floating point operations per second or flops and we can find all the specs we need on intel you know where i learned about flops from i learned it from pick taught me all about flopping it he really he knew he knew what he's doing he was very good at it too and a vds official website take this zm processor for example it has eight cores with a base frequency of 2.4 gigahertz that is 19.2 giga cycles per second including bandwidth of a single 2630 will therefore be 19.2 gigaflops but this processor can actually perform more than one floating point operation per cycle specifically this cpus support avx2 instruction set extension which can process 8 floats in a single cycle we'll talk about that later huh it's pretty fast huh man the giga chat gigaflopping over here i mean this this boy's gonna be teraflopping at any point this is exciting the theoretical computational bandwidth or one of these chips it's it's okay calm down it will be 153.6 gigaflops and a dual cpu setup will be around 300 gigaflops in total what about the performance protagon ads a gtx titan x has around 3 000 kodak cores with a base frequency of 1 gigahertz we therefore the theoretical computational bandwidth will be 2.9 teraflops wait that's a that's a terra chat right there we haven't seen a terra chat i didn't think they existed i've heard about them i didn't know that they are real but apparently they're real we have proof right here by a simple calculation we can see that titan x is around 9.7 times faster than a dual xeon e5 cpu so what's causing the exaggerated 66 times acceleration as reported in the post well oh this is some this is some gigaflop making music you know what i mean are you are you feeling that i'm i'm not gonna lie i'm feeling a little love fill a little giga floppy right now you know what i mean many cases programmers often fail to optimize their algorithms on cpu i don't think that's i'd say in many cases they use javascript which shrinks your brain cell size as we've been told by reliable sources and you don't optimize your cpu that just doesn't happen so practice your time waiting list will be much slower than the theoretical one for instance if you are not using abx instructions the cpu can only process one load per cycle and if you even fail to use multiple cores in parallel the practical performance will be even slower more complex we need to take the memory speed into your account before i took the parallel optimization course i thought cpu speed was the only thing people needed to care about however it's about that girth of your bus everybody knows that it's not just the length of the bus it's also the girth of the buffs you know you can't just you just can't do that the music is running out the video i know unfortunately they did he did not equalize the video which or the the sound too well on this one but it's still really good video it's still really great video yourself in the case that memory speed is the major reason that slows your orgasm down according to this study conducted by the university of virginia cpu speed any website that looks like this was not only written with hdmax web 1.0 but is probably filled with the most important information you'll ever read in your lifetime okay this is just a fact it is like for whatever reason there is there's a there's an equation out there i swear there exists an equation out there that looks something or or at least a graph out there that looks a little something like this that as the site looks nicer its usefulness is inversely proportional you know what i mean so like twitter looks pretty nice is it useful no no it's not it's not useful it's not useful at all it it doesn't it's not useful at all okay the only thing you're gonna get out of twitter is that one dog and cat posting meme that the those those weird l accounts that exist no context humans and of course a pregnancy monitor or a pregnancy pen that's playing doom like that's what you get out of twitter okay oh sorry x oh it's x-men come on whereas you look at this little website this little website right here that we're looking at okay that's not good design okay we're not feeling it it's not beautiful it's not shiny it's not sharp where's my like button i don't see any of those things usefulness usefulness it's all the way at the tip okay this is it's just you know i feel like i need to rewire my brain grows faster than memory speed over many years yeah the result is that important computers can handle 10 to 100 cycles per memory access we can also confirm this difference between computational and memory speed limits by calculating the memory bandwidth of the xeon cpu we've just talked about the specification states that max memory bandwidth is 59 gigabytes per second and if each flow takes four bytes this equals 14.75 gigaflops in this case our memory bandwidth is 10 times smaller than the computational bandwidth therefore if a tasker requires less than 10 cycles per memory access on this cpu we'll call it a memory bonded task otherwise it's pretty good some pretty good stuff you know this is why you know this is why arrays are really really nice and linked lists are sometimes not very very nice you know we talk about this that how linked lists can just be slower like they're they're theoretically faster for a lot of operations but they're not faster often in practicality because with an array you get this you get a nice tight piece of memory that all exists into a single location but a linked list is just you're new and stuff up all over the place right and so you're getting all these little things so you get you get you just get you get oddities if you know what i mean you didn't come here to learn stuff well guess what get learned on kid okay this is an educational channel now it will be a compute bonded task some of you who have taken operating system courses or other hardware related that's my biggest regret in life is that i took a lot of the hardware courses i just never took an operating systems course i i it's still one of my bigger educational regards and at this point in my life i just don't have the time to sit down and just sink 400 hours into learning something because i can't do it on i know you guys want me to do it on twitch you don't actually want to do it on twitch nobody actually does want to do it on twitch i know everyone says they want to but no one actually wants to and so it's like i want to sit down and just read through a building your own os course i know you guys all say you do no it doesn't no it doesn't it doesn't it does not count comozo did it is he simpsons is gamoso now a simpsons horses may think that using caches will help solve this problem but what i'm talking about here by psychos per access actually has nothing to do with caches take a memory bonded vector operation ax plus y or xp for example this operation takes two vectors x and y as inputs if they both contain and floats the total number of floating point operations needed is 2n which is one multiply and one add per entry however the total memory this origin needs to access is also two and floats yeah no matter how you cache it those data have to be loaded from the main memory and it is definitely bonded by the 59 gigabytes per second memory bandwidth limit since the cpu can perform 10 flops per access this algorithm is always memory bonded if you find your awesome to be memory bonded there's almost nothing you can do to optimize it and some of the cpu performance is pretty much guaranteed to be wasted so it is why the performance difference between cpu and gpu is bigger it's actually the opposite you just spent like four minutes explaining that just to say it's not what a plot twisted what is this guy m night shimab a lot like what what year what just happened here i are we watching signs like this is the gpu memory bandwidth is usually only a few times larger than the cpu memory bandwidth we are implementing a memory bounded task on gpu won't make it a lot faster let's still take the gtx titan x for example okay okay the memory bandwidth is 336 gigabytes per second which is 5.6 times faster than the cpu and since a memory bounded task won't be converted into a compute bonded task on gpu and xp will still only be 5.6 times faster when implemented on gpu to really understand the most difficult part of optimizing algorithms on cpu we need to take a deeper look at the compute boundary tasks okay an algorithm can be compute bounded if the number of cycles per access exists the computation to memory bandwidth ratio the classic example is the general matrix matrix multiplication damn those matrices every time i try to write a matrix multiplication i forget how bad i am at remembering how to do matrix multiplication there are so many eyes js and k's every time i get it wrong but now with copilot every time i get it wrong it's just more confusing why i got it wrong an operation for jeff a gem takes two matrices a and b as inputs yep yep we'll assume they are both earned by n matrices for simplicity beautiful beautiful each output entry cij equals to the multiplication between aik and bkj 4k from zero to n classic b b calculation takes and float multiplications and there are unsquared entries we need to calculate the computational bandwidth of the entire algorithm will therefore be o and q the total memory complexity however is o n squared so theoretically no matter how powerful your cpu is this algorithm will eventually become compute bounded given a large m feels good feels good it's going way over my head why it's just that we just we just looked at some stuff that's all we're looking at most compute bonded organs can be optimized in two major ways okay body threading and scene b multiple cores on your cpu to run the organism in parallel it's right i love generated what's it called caption the organism and the simdi oh man none of it makes any sense it's so beautiful easy for most compute bonded algorithms to support this feature all you have to do is find the part in your organism that can be paralleled divided and schedule them to different threads is also known as the fork drawing model since all the cpu ports are mostly independent the good part is is that with rust you could wrap all of your values in artex or art and our text and our mutex so it's beautiful definitely very very easy very speedy you'll definitely not screw it up oh but you can download it okay okay calm down guy yeah i know i get i get that you can i get that there's a way to do it you can window off that array with a bunch of mutable slices but nobody does that okay everybody's too confused by arc mutexes and they're they're confused why they're playing a zerg character while programming of each other the biggest issue that can slow you down is fast sharing which only occurs if multiple cores try to access the same cache line on the shared cache it's also pretty easy to solve this problem just don't make the private data of different threads on the same cash lab cindy on the other way is not that easy to use what's the solution don't do it duh dummy just don't do that everybody knows that answer just write your code better dummy just do a quick parallel operation just fork it then join it it's that simple what what is the problem name stands for single instruction multiple data which helps you accelerate further using a single core yeah this stuff's wild like what it can actually do wait is that an m i didn't see that i did not see the m in there but when we did that little contest with russ to see who could write the fastest advent of code solution for rust like when people started getting into like the cmd stuff and these like constant bounded arrays and all that i mean it was pretty cool to see what rust would do for you on your behalf and just give you the cmd is beautiful it was it was beautiful it happened you must load multiple data into a single cmd register and boom all the calculations will be done in a single cycle it's not hard to imagine that this will be extremely easy if your data already lies continuously in the memory on the country if they are randomly spread across the ramp fetching them and then load into the register may be even more expensive than doing it without cmd true some details can also influence the same deep performance for example if your data is cache line aligned it may be possible to load it faster but this varies from different architectures so i won't discuss it further it's already difficult to parallel on the cpu but it even gets harder when we consider the memory again take the jam for example okay i've just said that it's compute bonded but for the simplest implementation it's actually still memory bonded damn remember this if your algorithm is compute bonded theoretically the memory bonded in practice you are in a really bad situation of life also shows that theoretical don't care about actual theoretical just don't care let's take a look at the code although matrices a and b only contains q1 square floats notice that we're doing a little bit of c plus as the lord intended so we're not shrinking our brain cells with javascript so this is good this is a good situation that we're looking at right here big fan the total number of memory access is still 2 and q if each access has to fetch the data all the way down to the main memory it will be extremely slow the solution is to guess it using cash in fact the cash hit rate determines whether your program is compute bonded or memory bonded in this particular case i didn't guess cash not gonna lie to you i guess quaternion i thought i was right but i don't think i'm right now that i look at now that i realize we're not just solving for for 3d space okay so i didn't i didn't i didn't know i didn't know i'm sorry and optimizing the cache is the most difficult and most important part of high through just threaten the cps family for jam you can do a few tricks to make it faster okay okay for example transposing one of the matrices from row major to column major will help increase the cash line utilization the counter-intuitive part of it is it actually requires you to do more computation with this method but because the computational cost is much cheaper than the memory cost the performance increase from the higher cash hit rate outperforms the extra computation for the cpu really there are some even crazier techniques including adding a few more nasty loops to break the algorithm into blocks i'll make another whole video discussing the details of these genius organisms when i first learned those stuff in the course it literally blew my mind but for this video we've already covered most of the reasons that size your organism done on c dude i wish i i wish we would have just at least seen like the video when you transpose the columns like that now you got now you got me all excited pu i hope you parallel well and see you in the next one default just be real here you know when i when i go off and i parallelize things you know what i mean you know what i mean [music] you know what i mean [music] it was a really well done video definitely a motion canvas yeah i've used motion canvas i've been wanting to play around with that more but real talk it was a good video i i wish we would have done a little less of the intense music but other than that it was really good you know i think there's a lot of things to take away from this it was a little too shallow for me so i didn't quite learn as much as i wanted to actually learn you know what i mean like it didn't go into depth in the places we wanted to know more about and so it made me feel a little lonely you know what i mean a little too much of a tease yeah i got teased a little bit i'm not surprised at his name the hell is that supposed to mean what the hell are you saying twitch chat i gotta pee so bad or else i would figure out what the hell's going on in chat right now the name is let me rotate your matrix baby again foreign