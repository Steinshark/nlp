so i have to confess something to you when all the noise started to kick off about chat gpt i kind of dismissed it as hype you know it was fun sure it was a lot of fun but it was another ai toy like those image generators that was here today but would drop off the radar in six months but then i couldn't help but notice how many of my friends kept talking about how they were using it not how exciting it was not how cool it was not how it was going to change the world oh my god we're all going to the moon hooray they were actually talking about being users how it was helping them to get stuff done day to day and that's when i started to pay more attention and wouldn't you know yeah i've become a day-to-day user myself take the number one thing i use it for it rewrites my linkedin posts it's just way better at that kind of corporate to use tone than i am so yeah i get ai to leverage my synergies these days while i've started to see it as an actual useful tool until recently i assumed it was a tool that was owned by people with access to supercomputers or at least massive aws cluster budgets not so i recently got talking to toby funk heinle and he's opened my eyes this stuff is now firmly going into the hands of regular developers we're about to see personalized ai on our desktops because we're about to have the tools to hack them together ourselves and i think it probably won't be long before you know when you check out a new programming language you end up installing a language extension that ships with syntax highlighting and lsp and debugger support and a custom ai model that's been trained specifically for the language by the extensions developer i can see it coming i think by the end of this episode you'll be able to see it coming too and if it excites you as i hope it will toby's got some explicit tips on how to get started building this stuff so let's figure out how to put ai in our hands i'm your host chris jenkins this is developer voices and today's voice is toby funkina [music] joining me today is toby funkinel toby how you doing oh pretty great thanks how are you okay i'm good i'm glad to see you we saw each other in person last week which is a rare treat for me as a podcast host and the week before yeah yeah we did actually you were in england the week before and i was in germany the week after so who knows what we'll do next week i i one of the things we got to talking about when we first met was openai right i've always thought of open ai as this enormous very clever database of billions of whatevers that is the state of the art and completely out of our hands and this tool we use and then you mentioned to me this blog post from someone at google we have no motor i think it was called exactly and it basically said to paraphrase oh god we're being hosed by the open source world yeah and like this is what i want you to tell me about how can we run our own open apis the open source side is sufficiently rich that we can get involved yes so the state of the technology technology today is that it's possible even to run llms on hardware that do not even have a gpu that's that's how far it has progressed first the note of caution that memo it was written by a google employee and it was said to be a leak it was maybe not meant to be published and also it's a it's a solitary opinion but still it gained a lot of media coverage and at the time it was published meta had recently released some of their first generation large language models the llama models yeah like i think spread like the animal only with a slightly different capitalization and large language model yeah of course meta is a competitor to google in in some ways but what they had done is they open sourced the models they published them under an open source license and then if i'm not mistaken they only limited the access to the model weights which are needed to to really use the models and they they provided researchers access to that according to a waiting list and then after a while like two weeks i think after they started in the beginning of march this year the weights also were well it said leaked they appeared to be people could download them via torrent at first and then the whole thing really took off yeah because suddenly there were reasonably capable models not at the level of gpt4 by far but capable and some of them could be run on consumer grade hardware from the start and so everybody could suddenly get into llm research without needing a cluster without needing like some some membership in a research group or some some access that was limited to that point and it seems many people were eager to get their hands on that kind of technology and so within the span of a few months yeah since march multiple things have happened so let me just check i've got this right first so the model is like how do we configure this big clever neural network right and then the weights are like once you've configured it you have to run vast amounts of data through it to train it and you end up with the magic multipliers that set the weight of the neural net rate so you need those two pieces to do anything interesting yeah i i think so yeah yeah i need to be as with those llama based models at home but the thing i haven't done is to try to fine-tune it or to try to try to train a model myself from the ground up and so i i might just be saying nonsense here but the yeah so the model is something that needs to be trained and then what happens there the state of the art in terms of llms it uses transformer based architecture and it goes back to a paper from i think 2016 or something that that is titled attention is all you need and basically it's it introduces an attention mechanism in the way into the way that models are trained and then it's possible with an existing model to fine-tune it and i think what you receive there that's probably the way it's much better look it up as i say i'm gonna get into that at some point to work on it myself but right now the simple use cases they are interesting enough when it comes to applications and so i'm i'm just retelling basically what happened it started to be multiple things that made it easier for people to run run their models on more and more hardware and lower and lower spec hardware and so at first there was a way to kind of if i understand it right modularized the fine tunings called q laura and that was even before i got into working with those models and by now it is what has happened is the models have been re-quantized firstly yeah so right quantize quantization maybe i quickly say something about that i think the original llama models they were quantized with 16-bit numbers yeah as far as i understand it yeah so you are dealing with vector spaces which have like a few hundred or a few thousand dimensions in current generation um large language models and then words are represented or tokens which might be a bit shorter than words or sometimes might be multiple words in a few cases they are represented as vectors and those vectors they consist of numbers yeah like in a 500 dimensional vector space you would have one token and it is assigned a vector which has 500 numbers and those those numbers they need to be multiplied a lot here those vectors are multiplied with matrices to do the whole processing of the large language model and basically that's something that gpus are specialized to to do they perform well on this so essentially you're downloading a giant matrix database base of 16-bit floating point numbers yeah yeah that's the way i imagined it and the quantization then what what happens there is initially you have 16-bit precision numbers a floating point numbers and it turns out if you just decrease the the the well exactitude i don't know how we would call it english conversation precision yes the precision of the numbers to eight bit or nowadays even to four bit and then people are going even lower than that the llms still perform and deliver useful output and that of course means it makes a huge difference if you're multiplying a 16-bit number with a 16-bit number or if you're multiplying like a four bit number with a four bit number and then what you need to save in terms of what what needs to be stored in memory what what you need in terms of storage then how often the memory needs to be swapped in and out and then of course what kinds of hardware are capable of running these models yeah yeah and so this is a big part of the recent progress that has been happening just decreasing the quantization and by that enabling more and more people to run those things on their own hardware so people have been like down sampling these databases so you can run it on more commodity hardware and see if it still gives you a good enough results yes and some of the research in the academic field that's going on is also dealing with this and testing and comparing according to various benchmarks if i decrease the quantization further or if i try to become a bit smarter about the whole vector space and maybe i'm a bit more exact in some areas and a bit less exact in other areas how can i get close to the 16-bit original models output quality with much much lower resource usage yeah yeah you would think as soon as you've published a useful database of 16-bit floating point matrices someone who knows about compression will get involved right yeah yeah plenty of people got involved and some some really specialized into that there are some people who who are regularly like re-quantizing new models or fine tunings of the original llama models that appear then by now it's not only the llama models that are there and since bless you and since the i think since the middle of may there's a this library that's called llama cpp which is based on c plus plus it's been around a bit longer than may but what happened in may is it started to offer a way to run the models in a mixed mode between cpu and gpu and people can just set a config i want to have this number of layers which are processed in by the gpu and are living in the video ram and all the rest can live in the just the normal what is called cpu ram in this context just the normal ram of the computer and then if you have a model that is maybe a size of 60 gigabytes and you have a graphics card that maybe has a 10 12 gigabytes vram or 24 is pretty common with with the top of the line you know gaming gpus is a fairly widespread then you can just say okay i want to have a part of it running in my gpu allowing in my gpu and then if it's maybe just a bit too big for the gpu ram just a few layers need to be computed by cpu and then you get already very good performance and it's very flexible okay yeah because that kind of network neural network topology does lend itself to having multiple stages in different places yes but okay so here i am i recently bought myself a new macbook and it has way more gpus than i think i need so if i want to put that to an ai what kind of i want you to tell me how to do it but first what kind of results can i expect on my little laptop yeah it depends on the specs i would say yeah so i mean you nowadays you could basically i think with if you just have 64 gigabytes of ram normally a ram in in that notebook then you can run all of the llama based models in in some quantizations like 4-bit or five bit i think even well with 65 billion parameters that's the biggest llama model that has been released i'm not quite sure but i think even that should fit the thing is the more that the cpu gets involved the performance in terms of token output token token frequency it decreases but the quality of the output does not decrease yeah so it doesn't matter yeah yeah in case you would think right logically we've spent all these years building these dedicated circuits for multiplying matrices so it can display polygons on the screen it's nice to know there's another useful reason for it yes yeah these specialized matrix multipliers so what does this actually translate to i'm going to you're going to tell me how i can run my own local version of open in the api because one thing i would like to do with something like this is a problem with that open apis it never has that specialized data set you're interested in [music] yeah that's true and that's that's where vector databases come in that's a it's another component that can be used in the stack just now you are saying open api i think you're referring to open ai right and sorry yeah yeah exactly so i think openai also has various ways for people to to just post their their data to them and they will then host it and give give people a way to use it with their models i've never tried that yet but the exciting thing is that actually if you have your own data and it's just not part of any model that has been trained or fine-tuned and also you're you're not getting into fine tuning because it's still fairly expensive or maybe not that easy then you can just use a vector database together with an embedding model that's today a very very low threshold threshold of entry very low entry threshold a way to to use your own data with llms and there okay how does that work what's the architecture there yes so there are embedding models which are the purpose of them is to take some input which is a natural language most commonly and to translate it into vectors and yeah so for example i have a big book or i have a collection of documents then what i do is firstly i will do some processing on them they should be nicely formatted they are not not too many errors because then the the retriever would also suffer later on and then i split up that book into chunks and those chunks are individually processed by an embedding model and so each each of those document chunks they might be like 500 or 1000 characters long or something like that is also assigned a vector and those vectors they they often if if the embedding model is well suited to the domain and to the language that that the book is is published in then those vectors they correspond somewhat to the semantics yeah so i can i can do when i have done this kind of embedding as it's called the connecting the snippets with the vectors i can do natural language retrieval which means i can just write a question in natural language and then this question will also go through the same embedding model and then will also be transformed into a vector and then what's what's fairly simple is to find what what is the distance between two vectors and so then using that query the document snippets from before that are closest to the query vector they are retrieved from the vector database and often it turns out they are cement centrically close to the query so you can have a big big document and quickly search it using natural language somewhat fuzzy and it's it's a lot of trial and error for people getting into it but also nowadays there are open source embedding models for download there's two tools like the language chain to to connect it all up and then vector databases are also something that people can run on their local systems they are open source vector databases so let me check i've understood this so i get a vector database like maybe i the extension to postgres or something like wavy8 or something like that and i chop up my let's say my product documentation apache kafka documentation there's loads of it it's really hard to search i chop that up it gets turned into vectors of floating point numbers in the database then i say how do you configure a partition in kafka it turns that into a floating point vector and then finds other vectors near it yes by the magic of indexes yes indexes are important as well because of course if you have lots and lots of documents in the database and need to compute like the distance of the query to all of the other vectors that would take a long time so there are clever ways to do indexing and to like put for example the the closest vectors to to one vector that's already in the database in the index so that the search can happen quickly but yes that's that's exactly what happens and this is also exactly one of the use cases that could be treated by that that's cool okay so i want to get into how that relates to things like large language models but let's just check first is that language agnostic i mean human like if i want to do that in german would i have exactly the same experience probably not so there's there's different embedding models and there's this platform called hugging phase where you can find lots of different machine learning models in general they have embedding models they have large language models for download and for trying them out doing research on them and hugging phase also hosts a leaderboard and a leaderboard for the embedding models and then you can find which are the top performers according to various categories and most of the embedding models they are primarily focusing on the english language and so the availability there is much better but then the creators of those embedding models they i remember the one i'm using i i read the comments and there were was positive feedback for one of the english language specific models according to training anyway that they also perform well for east asian languages for example and that's true yeah and i i tried german as well and it seems to work as well i haven't looked into the details of that but i i would imagine maybe the training data of that model also just contains some amount of of content that is not in english yeah you have a book and maybe that book is for some reason multilingual and it's used to train that model or you have data from the web and then people post in forums i don't know what they use in multiple languages and then so coincidentally that also works yeah and for german i've tried it out with a model that is meant to be used with english and also it worked for me the output made sense and it's trial and error in that case okay that's curious so is that the piece i'm missing then is the embedding model that does that convert this chunk of text into floating point numbers yes and then the vector database does the indexing and searching yes for similar factors okay so that's where i need to download this llama download this llama i love that phrase yeah so when you have the vector database set up and have decided which embedding model to use and then maybe linked that all up for example with link chain which is a powerful open source tool to link various components that are related to machine learning models especially large language models then there's another leaderboard for example where you could start out of open source large language models and they they have been benchmarked according to various metrics and yeah most of the models on that leaderboard are based today on the llama models because there's those are just the models that have had the most optimization that perform the best on low spec hardware or or like normal consumer grade hardware that that many people have at home and so those lava models they can normally be recognized by by their size yeah that's the biggest one is 65 billion parameters and they have some are called 30 billion parameters but it's actually 33 so it's slightly inconsistent you will find some naming using 33 billion 30 billion and then there's 13 billion and 7 billion parameters that's what the llama based models are but today the leader or right now it's it's also constantly changing the leader of the open source large language models is actually a 40 billion parameter model that's called falcon and that's completely unrelated to to the llama models it was released by a research institute in abu dhabi and as it has a significant advantage over the llama models when it comes to licensing the license is much more permissive yeah meta has published their first gen models with a license that says you can do research on it and better do your own research i'm not giving legal advice but the the falcon models they they are very capable and have a more permissive license but right now as we speak the drawback is still that for the llama models just much more optimization has been done yeah falcon right now would perform not as well as similarly capable llama model but this is like breaking news when we're talking this has all happened in the past few months it has happened in the past few months yes and falcon is i think not more than two months old so you've done this locally with your own data sets right and i want i want you to tell me what you've been using it for personally but also like what performance which models do you choose what kind of performance do you get out of it so at first i just wanted to try out what what can i actually do with those models yeah because i'm really really excited about gpt4 and what it can do with the the models behind that they are very powerful very capable but also there's a drawback sometimes i'm not sure if i'm writing a query will it be considered an appropriate yeah i would i will i get flagged and just the horror of of maybe losing access to that kind of resource because some some value system that has been used to to filter the prompts doesn't exactly match my own value system yeah so yeah so so it's a it's basically a matter of of freedom and just being able to to just put my unfiltered thoughts into a large language model and of course if it's running locally then then i i have nothing to worry about regarding that and so yeah but mostly i've i've really been doing benchmarking and comparing and linking it all up recently i've posted something to my github as well which is i i cleaned up my own lang chain code which has this tool chain that that we've been talking about with a vector database with the embedding model with a large language model and is somewhat easy to use at least for me i i don't know if anybody else has had a look at it yet when it comes to there's a new model that's being released and i just want to drop it in quickly i have that model is llama based then i can try it out rather quickly and yeah so i've been using it for various documents and we've been posting about it i'm i'm trying to only use a public domain data when i make a linkedin post or something like that yeah for obvious reasons but it works quite well with books yeah recently i for example i've used the bible which is one of the public domain books that just comes to mind yeah it's a fairly big and then i just wanted to know how long does it take to to create embeddings from it and turns out on my system it's like less than a minute and i have all of the bible subdivided into snippets and those snippets created embeddings from them yeah we'll start we'll sidestep the whole religion discussion of course of course that's a nice large open source collection of books right yes that makes sense yes and there's lots of open source data out there and you can just yeah you can use the yearly reports of companies you can use whatever you might have put into the data lake of your own company yeah so it's it's suddenly becoming much much more exciting to in terms of what's possible with with all the data that has been just assembled in some cases or many years so i i give me the number so how long on your laptop which presumably is like a reasonable spec laptop how does it take you to index if that's the verb the bible and then what kind of query performance do you get from it yeah well i i've recently because in part motivated by that that google memo i've recently bought a fairly beefy machine and so it's not the laptop anymore it's a desktop and it has a 40 90 and that's which is currently the top of the line nvidia graphics graphics card and when i create embeddings from the bible on that machine that takes less than a minute but also i wouldn't expect it to take much much longer on lower spec machines yeah i would need to do testing on that but currently i'm saying what's the point i want to use the best i have available and yeah i might move into clusters if i hit some limits but i don't if i'm doing this at home i can expect minutes overnight so what i've seen is i've had a fairly large corpus of text 1.6 gigabytes and i split that up into documents that are 1000 characters long and that took a few hours on that same machine but i've also noticed that that is much quicker if i use documents that are only 500 characters long yeah so it seems to be non-linear to me take it with a grain of salt i haven't done a study on this it's just my impression from what i've been doing currently i much prefer document length of 500 characters just for the performance reason oh so this is another decision you've got to make going into you need to choose your embedding model you need to choose your chunk size do you end up iterating a lot and just tweaking parameters seeing what's going to happen yeah the reason i've i've decided i want to publish something on github is because i noticed i just started out scripting something with python and with lang chain and i i cannot actually python so i it became something like unwieldy when i wanted to extend it further and so i thought okay now i need to do some design paths on it and so i thought okay let's let's take open source as a motivator if i want to well publish something that another person might theoretically be able to use than what would be an interface yeah and then from that perspective i arrived at a design that is now again much much more extensible and configurable and more reality again than what i previously had yeah and i think it's precisely what you say i want to fiddle with the parameters i want to drop in different models of different quantizations try different document sizes compare and there's so many variables yeah and so it's really important if i want to have meaningful knowledge that that i'm methodical about it and and systematic and so it's a it's important to design it as well yeah this is cool you're a home ai researcher yeah officially yeah i mean okay and then so the the the really big job is getting this language model which you do from someone else the lesser but still quite chunky job is indexing the corpus in this case the bible and then you've got this final vector database output right yes and that's a thing you can deploy to someone else and they can start querying yes i mean i can of course host a vector database anywhere or i can also use just some cloud api if it's not data that i really need to keep on my own systems then there are cloud providers there are also cloud providers which which care about privacy and so there's there's many options there and so this this whole process yeah you can you don't even need a local embedding model if if you don't care about the data really remaining private you can use an open ai embedding model they have a good offering that's it's becoming less and less expensive in terms of per token cost so there are many many options okay i think we're in getting to in this is i can imagine a future where i've got an ai trained on the data i'm interested in maybe i've trained it on all the emails i've ever written and all the blog posts are never written and then i would like to deploy that to my phone where it could write emails for me on the go yes are we approaching that future there are people who are already doing that kind of pre-writing their emails based on what they can do with ai and what's what's popular for people who are doing that who are not that familiar with coding for example they they have access to low code or no code tools that are also provide a way of for example using lang chain connecting the gmail api with with the open ai api yeah so right now it's just maybe a combinatorial explosion of what's possible yeah that's also what do you describe it's depending on how much you're willing to do yourself it's not the future it's already possible and it's already being done oh so we're in the actual productionizing phase of this stuff yeah of many many things it will take many years until the potential of what's currently out there has been well exploited okay where should i be looking i mean if you've got any recommendations well i think right now it's there are just so many possibilities and the development is progressing so rapidly that i would say what's a good starting point is precisely this this architecture where you're using a vector database to use be able to use some data that's just not been used in any training or fine-tuning process for for models and then use an embedding model use a large language model and then look for example what i've been doing recently look into more of the length chain features lang chain has really a lot of ways to deal with the limited context size of the large language model to do compressions using large language models again to extract data from various formats of documents and so even there yeah and then combine it with a business use case maybe something something that you care about personally something that maybe if you have a business if you if you're no business owners talk to them what what are their challenges and then just yeah connect maybe connect the dots and i would say due to the sheer number of possibilities of combining and also to due to the speed of development it's for me the question is not where should i be looking but what filter should i apply so that i end up actually doing something and not only being overwhelmed by by all the things that are there to be used yeah the the classic kid in the candy store problem if you've got too many yes okay is there something in particular you're working on other than slicing up the bible yes of course so currently i'm just starting out to look for a job and obviously there are applications there yeah so in the beginning i just take a somewhat global perspective and i i think about what what are the fields that i would most like to be working in and so my current challenge is to just find out who are the major players in each of the fields in the domains and so obviously when you've got a big company you also have to publish lots and lots of stuff and then i can just create embeddings of the yearly reports maybe maybe even of job postings yeah and then ask natural language questions about it just so i make progress on my skill set when it comes to llms at the same time as i as i narrow down where where i would like to go next you know so this is right now what what i'm thinking about regarding llms that's brilliant you you employing ai for your job service that's great fun but how's that working are you like you pick five companies you like and you index them all or you just as they're like you here's a particular company i'd like to learn about so yeah i'll query that there are definitely some companies that eager to be working for potentially no matter what what they are currently doing and publishing and in that case it's easy i just try to find out what they do in some cases i'm just i'm just joining a meetup where there's somebody from that company yeah that's that's a low-tech way approaching it but yeah so there's still a place for natural intelligence and it's a much more efficient way yeah than taking the public routes taking the information that everybody can access and then yeah i think a big company that's hiring is always also has a spam problem and then yeah the the public routes to to hire us is often the one that's much more resource intensive as well but then yeah so i one thing that i have in mind is i'm going to take the fortune 500 and then i'm gonna see how do they publish their their yearly reports if they publish them in english and then also gonna see like ai what what are they writing about ai what what maybe the subsidiary of this is working during work related to that field then other other important technologies i'm i'm really passionate about the potential of robotics as well of fusion energy and i can just yeah browse their publications and their their reports according to well are they connected to that then in what way are they how are they focusing it where should i turn then maybe i will just look at the website of some subsidiary instead of the of the main corporation yeah in that case and narrow down what's the most efficient way to approach a potential employer who i know nothing about yet now i'm only passionate about the domain they are engaged in yeah this is the coolest and definitely the nerdiest approach to job searching i've ever heard so four marks we should talk a bit about the the other side of ai which is prompt engineering when you've got all these models what do you actually say to the thing so the thing is if you have a model that doesn't have that many parameters a small model then prompting and prompt syntax is really really important so if for example there are some small models where if you like leave out a space at the wrong point or where if you don't use exactly the the prompt syntax that was also used in fine-tuning them then they will not produce useful output or the usefulness of their output is really decreased they have this big disadvantage that a very small model has for example versus what we know from the the big models and gpg files especially where you are really free to just prompt in any way and so recently i've mostly been using the 30 billion size models or 33 and they they are really much better at that already they provide people more leeway they will be more more accepting of free form input still it's important when i build a pipeline to have some some structure that makes sense for a prompt yeah if i know okay i will put lots of documents snippets in the prompt together with my query and then maybe i also have a conversation memory which is another very important component if you want to do conversation and not just have like a zero shot i think it's called prompting and just expect the desired output within just one one interaction yeah then the the prompts sometimes they become somewhat complex here and then it's important to have some formatting which will let the large language model so to say make sense of what's the prompt yeah and then they have like right 10 snippets from some report and then they have the what has been gone what has happened previously in the ongoing conversation and then there's a actual query and yeah so maybe the next few days i'm going to experiment also with smaller models and see what's the complexity that i can still expose them to while expecting output that makes sense output that's useful and then of course they they perform much better yeah so this is if i want to run stuff in parallel or if i just want to do async stuff then using smaller models is very interesting and attractive but i i need to know what can they do and what can those generalized models do where do i use specialized models options options okay so do you are you saying that you with a smaller model you you have to be more careful about talking to it but it still gives you very high quality results back potentially yes so it depends very much on the fine tuning that the model has and then how it relates to to my use case and yeah so it's where where i really need to do some experimenting recently as i say i've you've been using 30 billion parameter models for just about anything but it's a waste of resources for for some use cases here if i just want to sum something up here i i give it a prompt and i just want to use the language capability of the model to give me a summary of what i've presented it to be used in the next step of like a chain of language models then i might imagine a smaller language model is already capable of that but i need to test it and also see what works and what doesn't work for for the use cases that i in mind right so do these language models do they ship with like here's how to talk to it yes or is that trial and error yeah well there's for the models on hugging phase there's something called a model card and every model should have a model card which which lists its prompt syntax in the best case yeah sometimes yeah sometimes whoever uploads a model there is not too tidy about it and then people have to figure out okay i have this model there's nothing in the model card but maybe there's a link to the model that it was based on or that it is a quantization off and then sometimes it's also a matter of okay this it just says this model uses the vikuna 1.1 prompt yeah and then if you if you know where to find that that's a good thing in that case there's the i think foreign github i think it's called uber buga or something like that and they have like just a folder of common prompting syntaxes where if you know where that is you can look into that and it's fairly likely if a model card says i want that prompting syntax that you can find that prompting syntax in that repo this is from i've just started reading like it's an old sci-fi book called the moon is a harsh mistress oh i i don't know that and the lead character has been trained in this special language on how to talk to ais they've got a name for the language but it's about loglum or something but it's really reminding me of that indeed yes that's superb so i think i think you give me enough to get going actually where's the first place i should go and download after this conversation yes so i think for i i would personally recommend to get started with length chain and lang chain is basically this big toolbox which which enables you to link lots of components together whether it be those apis from openai from anthropic from google probably also other providers and then you can just start out maybe maybe you get free credits from some of the api providers and can experiment there they have lots of tutorials they have descriptions of basic use cases getting started page yeah so just start out with the simple use cases and then once once once you've figured out some of the components then you can you already know you can run your vector database locally you can download an embedding model to to actually use the vector database with your own documents then for example if you're using python then you will need a component like pi pdf but it's easy to install it's just a python package yeah and then okay yeah then you have on hugging face you have those leaderboards of the open source models yeah the for the embedding models and for the open source large language models as well yeah yeah and then yeah then there's a llama cpp python i think it's a very good package right now which is the one that enables the mixed mode between cpu and gpu for the llama based models and also there's some preliminary work on the falcon model that the one from abu dhabi that's also right receiving their first optimizations and you are actively blogging about this so we can also check your blog first tips yeah well i'm i'm writing on linkedin and i'm blogging about it and yeah so i don't really know what what i will discover next but that's the joy of research and you've got to publish it spread the message about what's possible today yeah because basically if if you're a student at university you can just try it out get into the field and there's lots of potential there's basically lots of lots of really early bug yet to be done yet to be discovered publish about then talk about and it connects to just about any domain that that is in some way cognitive yeah so it's a it's a thing that will go on for at least years if not longer and who knows wherever go yeah i can totally see that shaping the next few years of the discussion because it's like a rich ripe field that's we're only really months into open source bedroom hackers exploring right yeah yes indeed yeah great fun well in that case we'll have to have you back in the podcast in a year or so and you can tell us how far you've got i look forward to it great toby in the meantime thank you very much for taking us through it and thank you toby thank you very much that was truly enlightening and i think i now know enough to achieve one of my real life goals which is to download transcripts of all david bowie's interviews over the years and get something that could rewrite my linkedin posts in a tone that i really respect i suppose the floor in that plan is it assumes there was one david bowie when really we got a different one every few years he was a great chameleon as they say over here we're much more stable than that and we will be back next week with another developer lending their voice to the global conversation so make sure you catch it by clicking like and subscribe and follow and notify and rate and all those good things and you know drop me a comment as great as ai is there's still no substitute for hearing from real people that's the very raison lecture of this podcast isn't it so check out the show notes if you want to get in touch with me check out the show notes if you want to get in touch with toby because at the time of recording this he is available to hire on your marks get set go get him and he also gave me a list of links that you'll want to look at if you want to learn more about this field there in the show notes too all of which i think brings us to the end of this episode i've been your host chris jenkins this has been developer voices with toby funkina thanks for listening