one of the strongest forces on the internet is the belief that someone else is wrong or something else is wrong that it's wrong they're doing it wrong and it needs to be put right immediately and as we know that's not always the most positive motivation on the internet but it can be it can be the thing that pushes us out of the status quo the thing that forces us to make fundamental improvements rather than just incremental ones that burning feeling that the current way we're doing things is wrong it can drive genuine revolutions in programming it can upset the apple card and it can also be the drive that pushes us programmers out from behind the keyboard into starting some genuinely new businesses some of the most successful businesses i know started with someone saying the current way we're doing this software isn't good enough and the only way i can change it is by stepping outside the current system and that was kind of the starting point for today's guest joran dirk grief he had an urge to build an open source database called tiger beetle and then build a company around it from looking at the current status quo and wanting to change things radically and it's still early days for tiger beetle they are just about to go into production so i'd say you know they're reaching the end of act one of their story but i think that story that journey from discontent to inspiration to launch it's one that speaks to all of us programmers who have a little slice of entrepreneurship in us and along with that journey along the way iran has some really interesting ideas about how we should be doing systems design clustering network communication reliability integration testing all these things burning to build something genuinely new which is putting iran in a heck of a road a long road and yeah we're going to meet him a waypoint on that journey and hear the story so far so i'm your host chris jenkins this is developer voices and today's voice is yoranda grief [music] my guest today is uran dirt grief you're anne how you doing hi chris i'm doing really well great to be here good good and where are you coming from you're coming from somewhere in south africa is that right yes an interesting thing is i'm we are actually same time zone london time best times in the world most most overlap with all the continents and but i'm calling in from cape town so nice yeah it's on my list of places to visit okay you haven't been yet not yet not yet i'll find an excuse okay well you've got to develop a developer friend here you know so i'll look you up when i come over yeah see now the reason i've got you on your your story starts very much in cape town the project around there right and one thing i'm always interested in is developers who have such an itch to scratch that they end up walking along that road saying this should be a real product and a real business so let's start with where you've got the inkling of that you're working on some project for payments fill me in film it yes so i i love that question i can relate to that i think this was like a i don't know i think it was a 16-year itch in the making and and then and then covert happened and and and covert was a catalyst for a lot of people i think to do a lot of interesting projects it was not an easy time but i i studied yeah in i was always into coding as a as a boy and and i loved coding and i always wanted to do a business and so i actually studied accounting at university because i understood people told me you know that it was actually on my list of things not to study i had a list 10 things never to study and number one i think so i had it i had it in my head at least and accounting was top of the list and yet i always wanted to do a business startup that was in my in my blood in my nature my grandfather immigrated from netherlands and he built his own business and my father built his own business and so it was just a thing that i was always going to be an entrepreneur or just just someone just to start a little company you know and yeah yeah so and and so i had this conflict i didn't want to study accounting and i wanted to start a business and people actually spoke to me and said well look you know accounting is a great way to see the world of business because that's how you describe businesses accounting and so i thought oh okay so i went and studied accounting instead of computer science and and i got and then i actually got back into coding thankfully and i i got into startups but it was a long time like 16 years or so of just honing my skill and practicing and coding and coding and coding and that was a period where i could learn a lot and discover a lot of techniques and get into storage systems and how to write fast software how to write safe software and learn and it was incredible i daniel lenoir the music producer used to say that he spent 16 years in a cave learning how to work with you know sound engineering equipment and i kind of can relate to that because i was in this cave in cape town in you know somewhere inside table mountain just for a decade or more just practicing and then yeah yeah and then and then covert happened and the next thing i knew i i bumped into adrian hope bailey on a soccer field and he said well you know he's working on this payment switch and i had no clue what a payment switch was so but he brought me in to consult on it and to see you know how can we make this payment switch faster and that's sort of you know that that's fast forward that that was not 2020 and yeah so what was interesting there was that i had this background of doing things differently you know living in a cave in table mountain honing my skill and so i had specialized in in a in the galapagos galapagos islands you know and become this this interesting data in a database developer creature [music] different genetic path than everyone else yes yes exactly and and then what happened was like i came up to this payment switch and we had to see how can we increase the number of transactions per second you know from it was if you gave it a lot of high-end cloud hardware you could get 2 000 transactions a second which is a lot but that that wasn't good enough for me you know i thought well can't we do 200 000 a second can't we do a million why not what you know because i had i understood from some of the experiments i was doing that even a spinning disc is really fast it can write sequentially 500 megabytes a second and that that's not even solid state you know so flash can do you get devices already that are beyond three gigabytes a second and if you think of the information you think of the raw materials you have you have disk you have network you have memory and you have cpu you've got these four physical raw materials and you can reorder them in an interesting combination and why why can't you process transactions really quickly you know with we've got these incredible raw materials so i looked at the switch and from the very first second of the engagement i was already drawing on the boardroom table you know here's another sketch for something totally different that could power the switch kind of like iron man you know take out take out the old heart and put a new one in and suddenly yeah more you've got you got all this you know this this energy roughly what the existing architecture was for comparison yeah so sure sure thanks great question so is this i think this would be helpful you know for people listening in so the existing architecture was my sql database sql rules the world so you had a sql database you had 10 000 lines of application code around the database like we all do and obviously there was a lot of kafka streaming everywhere and and that's still there that that's that's kind of orthogonal so the heart of this payment switch was a sql database that was tracking transactions basically bank balances and transfers between bank accounts for the participant banks using this payment switch so it's you can think of it as like eight rows in a sql table eight banks eight rows and then you're doing a lot of transactions across these rows so it's it's actually so simple you know really simple you've got a table of accounts and there's eight accounts and then you've got a table of transactions and and transactions that table is big you know that's the whole financial history but the interesting stuff is that you're really just trying to move you're moving numbers you're doing addition and subtraction across eight rows and and to do that there was 10 000 lines of application code yeah it seems okay this is always one of those things where you where until you actually see the code you don't realize the complexity you're not seeing but on the surface of it that seems like disappointing performance given that you're only swapping money between eight different accounts yes yes yes and i think this is the interesting thing what i learned is that i think i mean just in general you know as you work with the sql database you know we denormalize our data or whatever we've got different you know then sometimes you have you've got indexes and you've got tables and you've got schemas and design but how often do we stop to think how many rows are going to be in this table are they going to be eight rows or 800 000 or 8 million or 8 billion and what what's very interesting with this actual domain is that very often you actually only have like eight banks and and say we've got atras or you in practice you actually have a little bit more so i'm i am simplifying maybe you've got 16 or you've got 32. it's some multiple of the number of banks but very often you only have four banks or eight banks or 16 it's pretty much the same order of magnitude so the table is actually very simple in that sense you know you can see it you can this is like a an excel sheet that could fit on a screen without horizontal or vertical scrolling and yeah yeah yes yes exactly and but the performance then of 2000 transactions a second is actually not bad for for for a sql database and that was the other thing i learned is that you know if you do if you do one network round trip to do a transaction you can only physically do so many you know according to your network bandwidth and according to the right blocks that are going on in the database and remember like the database also has to do it has to write to disk for that transaction and has to call fsync and there's only so many f-syncs that a disk can do a second and obviously now with flash that's gone way up but when you put all this together you know rolox individual roblox and network requests and round trips and latencies and disk latency there's always a fixed cost that you pay to write even the smallest of unit of information to disk durably you're going to pay a minimum fixed cost and then as you write more you're gonna pay more but you've always got this minimum threshold whenever you want to get some disk but then if you think about it you know we typically use sql databases and we do one sql transaction for one real world event so one one real world event is one physical sql transaction one in other words one logical transaction one financial transaction is one physical sql transaction and that has a minimum cost right and that has a minimum cost exactly and and so kind of the 2000 is not bad because you know simon the skilson has got a fascinating blog called napkin math and he dives into this one of his posts you know how how many f-syncs can mysql do if a mysql could do fsig's you know or postgres and and that's a really interesting question because it brings in things of group commit obviously the database is gonna internally batch some of these transactions so to try and amortize that minimum cost but kind of when you look at it like hand wave the rough rough napkin order of magnitude is maybe you can do 10 000 transactions a second that's like your theoretical limit for these designs okay so now having convinced me that that's pushing on to the limit what makes you think you can do better what makes you think you can get up to like half a million or whatever numbers you're aiming for yes so this comes back to like you know scratching the itch for a long time and what was funny was the you know the very first day of the engagement we were sketching our tables in the office i was thinking about this this morning but there are these white tables and you can draw on them with a permanent marker and then you've got like a little you can rub it out but you can actually all the tables in the office you are supposed to draw on them so they encourage graffiti so and i mean if it wasn't for that maybe tiger beetle wouldn't exist because you know we were we were in the boardroom where i am now we were at at this the conference for this payment switch we were looking at the architecture and and already i was just drawing on the table and it was my first day in the office you know and i i didn't feel bad drawing on the table but i i already was sketching out something that i thought could do much better and it all to answer your question it all comes back to just taking a step back and saying well look we've got incredible raw materials at our disposal you've got nvme that can now do three gigabytes a second of sequential right we have like raw information like there's a minimum size in bytes to capture the information content that's being processed by the system by this raw materials but what if we could you know re re reorder the composition of these re these raw materials in an in a more optimal way and basically what you have to do is you have information that flows into the switch network network bandwidth isn't really a problem it can handle quite a lot of information then you have to write this information to disk and that has a minimum cost so you there's an advantage that you maybe write one megabyte a second you know one one megabyte per not one megabyte a second but one megabyte for each sysical you know because that that's you don't want to do like you don't want to write one byte at a time or 100 bytes or a thousand bytes you want to do something a little bit bigger because that that just amortizes that that cisco cost and and if you start to do that well you can see you know then after that you've written it to disk durably then you have to run it through your in-memory state and you can picture your in-memory status just a few hash tables so again you've got eight hash tables sorry you've got you've got one hash table with with eight keys in it and each value is the bank account balance for these eight banks again i'm simplifying but this is really the the the the the the the the skeleton the bone structure of this human that we're trying to create you know that you can start to see the form if you can understand this that you've got one hash table in memory you've got eight keys eight balances and well how fast can you write to a hash table and actually you can do this with some of the work that i had done experiments you can do some of the new high performance hash tables can do at least say 10 10 million inserts a second which makes sense because the memory bandwidth is pretty good and you can look at like you know how many nanoseconds does it take to do a cache miss to go to a location in memory and it's it's maybe 70 nanoseconds or better and you work that all back and you know if you know that that data that there's also memory parallelism you know the the chip can access 10 cache lines and pull them from main memory in parallel you start to to see you know what a hash table can do and now you know you've got network then your right to disk durably then you you do a lot of hash table operations and then you're done and you you send an act back across the network and that that's kind of you're doing like this event sourcing trick where you capture the raw data store that durably and then do the onward processing and then you don't have to worry about the onward processing being quite as durable because you can always reconstruct it that's right and that's kind of using a log so event sourcing or thinking thinking of a database as a log you know there's that that classic saying that the database is the log and everything else is just a cache and yeah i had a i had a chat you know with alex gajago of red panda and i asked him because i i see red panda as a database which is it's a funny thing you know people will say well this is my favorite debate yeah yeah yes yes so i asked alex i said you know there's some of some of our friends will say well is it really a database obviously i think it is and alex i'm gradually coming to the conclusion that yeah if i think i think if you tell someone who's used to databases that it's a database that we disappointed but if you tell someone that that's used to logs that it's a database they'll think yeah this has way more power than i thought originally i interrupted you you had a quote no well well that that's that quote you know the database is the log and everything else is a cache but that that is actually how you know postgres works my sequel all these systems they have right ahead logs they have a log and that's really the master record that you don't get to see yes they put it in the log then there's some processing there's going to be more data written to disk you know as indexes are updated but really that is all you can think of that as cash because if you if you and this is also how the database is guarantee atomicity so crash consistency through a crash or what they call crash consistency through power loss so the database is busy doing things updating indexes doing a lot of work and then you pull the pull the chord and and really it comes back to the log at startup it's going to go back to the last known location in the log and it's going to redo that work yeah and yeah there's there's many ways obviously of of specializing on this but this is the big idea is that you've got network and you've got a log and then you've got in-memory work and maybe you'll have more storage work later so so i i kind of did see this through the same lens and this is even how file systems work you know for example zfs also so has the log and then this copy on right but taking a step back i thought well you know the the information content of a financial transaction it's i mean this is another question you know what what is a transaction in the oltp world it's kind of like you've got the who you've got the who what the when where and the why so that that is oltp you know who what when where why how much and you need to do a lot of that and you want to be highly available and online and and i think that is oltp you know who what when where why how much and basically if you squint and look at this which i did as an accountant i said hey oltp is really double entry accounting like it's the same thing because double entry accounting it's we use this it's been the schema for hundreds of years to model businesses any kind of business that wants to track business events they want to track the who what when where why how much so so right exactly original event sourcing it it has all these properties that are very friendly for our raw materials of compute and storage network and you you alice bob that's the who what well you know it was alice reimbursed bob and there's the why okay it's the reimbursement the the how much is the amount the when was when did the transaction happen maybe there's another win as well you know when was it recorded and when did it really happen there's a where as well like you want to track jurisdiction and i was thinking about this yesterday you know as a developer like it took me a long time to realize yes utc timestamps just use utc timestamps and then it took me even longer to realize no utc timestamps are not enough you want to track the wear as well like you want to track the the identifier of the locale so that you can you know dst or you know the classic calendar app you want to show someone a time that makes sense in their time zone yeah yeah utc is data and everything else is formatting that's my opinion yeah exactly yes and and if you know the when and you know the where you can do the formatting and yeah yeah yeah okay sorry yeah actually actually captures all of these quantities for you you know the it's it's a transaction between two people or places you know something moving from somewhere to somewhere else like google maps you know get get me a direction in google maps that's double entry accounting you know i want to go from cape town to london how long is it gonna take you know it's place to place or or person to person it's oltp i'm i'm obviously being very handwavy here but i really think you know at in in principle like the heart of oltp is tracking these things these business events and double entry is a great schema for that yeah yeah i got my career start with the software systems for double entry accounting i still think it's one of the great data models and the great understanding of how you model what's actually happening in the world yeah we should get back to your journey because you've got you've got all this double entry accounting knowledge from your accounting training and you've got an architecture you think is going to work written on a desk which is the first time i've heard that as a design tool and it cracked out chris it's it got dropped out that's like the back of a napkin is more durable yes yes but how did you get onto the point of actually cutting this into software and seeing if it worked so yeah i think that's the great tension you know because you have to always build trust you know if you're a consultant on a project you can't jump in and say well we we're gonna you know redesign postgres or mysql in terms of the optimal configuration of raw materials from first principles and so i so we actually this was the the best part i think is that we actually looked at the switch and we said well the heart of the switch is oltp it's sql transactions less there's thousands or tens of thousands of lines of code you know we could read this or we could see where do we talk to the database and let's instrument that let's trace every single sql query that flows through the system and let's send a lot of payments through a lot of financial transactions a lot of oltp let's see the physical transactions that are being issued across the network so we trace the sql and then what we saw was for one logical transaction there would be say 10 to 20 physical sql transactions and you could optimize that further you know some some systems do do one sql query for one physical transaction if one logical transaction you can do that this particular switch there were good reasons for being between 10 to 20 but this was interesting because you start to see there's an impedance mismatch because we're really trying to do oltp we're trying to do transactions processing real world business transactions business events using yes using physical database transactions but the original as far as i understand the original meaning of oltp was always in the sense of what does the user need you know first and then we yes do you have physical transactions to do that physical transactions to implement because they are real world transactions but you see there's an impedance mismatch for every payment there were like 10 to 20 network requests fsyncs etc yeah there's a clear disconnect between the logical world and the reality of how the software is modeling it yes exactly and and and in the past this didn't matter because discs were so slow you know you yeah it really didn't matter and transactions were never nearly that high right because no one expected yeah exactly so i think like this is my thinking lately i think for a long time oltp was very you know was very welcoming it said yes and we're also general purpose you know you can put all your meter data everything in the same database and you can we're oltp but also general purpose database bless you thanks and and that that was fine for a long time and then i think there's like a divergence you know oltp and general purpose because of this impedance mismatch so coming back to your question when did we decide you know to do something different i think i'm really grateful that we we didn't immediately you know we first analyzed the switch and we could get these insights really understand the problem first rather than try and you know come up with a solution in search of a problem we could really understand the problem deeply and and see yes actually there's only on the order of 8 16 or 32 rows in this table and there's there's a highly contentious workload so straight away you start to see you know it actually doesn't make sense to horizontally shot because there are you know there's contention double entry you're you're gonna update one bank account and update another one you you almost yeah and sometimes you want to update all the rows because you're doing very interesting financial con contracts where they literally touch all the balances so sharding isn't going to solve the problem counter-intuitively now it's going to make the cpu wait even more you know as you start doing network requests and so i think that we we were we did this work for like six months or so or three months in at three months in actually then there was just this period where i think 16 years had built up and you you know i i it was actually a sunday afternoon it was raining my one co-founder he does his best work when it rains and i think i do my mine mine when the sun shines but this happened it happened to be raining i had the fire going i was listening to black keys this album el camino which is great and it was a sunday and covered and and i just like i banged the keys for five hours and i sketched sketched these raw materials and you got this prototype that could do like 200 or 40 it could do 200 000 two-phase transfers so which is you're doing everything twice so basically it could do 400 000 financial transactions a second and this was just a rough performance sketch yeah like back of the envelope and but and it was javascript so and and but it was it had all the ingredients it had fsync there it had cryptographic check something even really which yeah yeah just to raise more than just a very simple prototype yes yes yeah and it was trying to see you know if we sketch out the network protocol the disk protocol the in-memory hash table operations the the the real thing that could actually work you know and it it could work so it was very basic but after five hours and maybe a few days more work we actually did plug it back in we did a heart transplant we took the the ledger database out you know this mysql and application code we took that out we put what we called proto beetle we put protocol into the switch and it took a day because the design had come out of the switch so we could put it back in it extract it from a real problem and straight away you know the switch that we when we evaluated it we had this trick where we were trying to evaluate it on really small hardware so it's very easy to do benchmarks where you have the the best hardware what we were trying to do was have minimum viable deployment like what's the smallest you know gcp instance we can deploy this on and the minimum number of instances so we used to benchmark like that and in those configurations we would only get 76 transactions a second and when we put protobital in we could get to 2 000 without you know fixing any of the other performance issues [music] so that yeah so proto-beetle itself you know if you use it properly you you could get 400 000 a second there was a lot of other overhead in the switch but it already made a you know order of magnitude difference was that with the same level of guarantees about transactional safety durability or you cutting corners on that so yeah so proto beetle was just a sketch so it definitely wasn't it wasn't bait already and it but it did do cryptographic check summing it did do logging to disk it we we were basically trying to evaluate the performance if if this could work so it wasn't a plane that you would sell tickets for but it was it was like a it was like a plane that you could launch off a cliff and see that it flies and whether it lands is another story you know it didn't have landing gear didn't have parachutes or okay we best go on to the what happened next after that prototype on this i think that's that's the the whole question you know that's the heart of it it's like you you can say to people hey we've bought this plane and it's so much faster and they're gonna ask you you know well is it as safe you know as as our regulated planes is it gonna land me safely and you'll be like no you know it's just really really fast yeah so and and then you realize like how much work goes into safety how do you make a plane really safe yeah and how much space do you sacrifice on that road yes yes exactly and but you know being involved with the payments which again was a fantastic opportunity to learn because you realize that actually it's not good enough to say we're gonna be as safe as all the regulations and the standards and the existing planes because people are still going to say well you say you're a safe but you're still a startup you know how like i it doesn't connect with people on a human level to say that you've built a new database and it's as safe as something that's 30 years tried and tested people are going to be well that's table stakes and i'm still going to er on the side of 30 years i i know you're as safe but you're not 30 years yes yeah you've got an uphill battle on trust if you want to turn that into a business definitely exactly so it comes comes back to trust and so then we realized well there were a few factors in this decision that actually these systems were 30 years old you know you you spin the coin around one more time and you carry on with that and you go well actually they're 30 years old and there's there's been tons of storage research you know these were those experiments again in the cave and like places like uw madison and the fast conference incredible research every year testing these systems and that's kind of the other side of the coin you know they are tried and tested which means in the research they know how they fail they they've you know all the latent correctness bugs how you can lose data through fsync gate a lot of consensus systems that the design of the writer head log is actually not fundamentally safe and you can have global cluster data loss with the raft with with a lot of raft clusters some of them are patched for this but the you know the stock standard raft consensus does doesn't get you there safely so there was a lot of research around safety storage storage folk research distributed systems research in which was also at uw madison and and then we started to see okay yes we are going to build a database and it's the perfect time to build it 10 times safer so so again like orders of magnitude we it's just so much fun you know you realize hang on like yeah that that moment of tension like yes we're just gonna do this prototype you you get past that and then you realize well now we can do this a thousand times faster and ten times safer and ten times cheaper you know because actually we made it 10 10 000 times faster but let's give 10x of that for cheaper you know small hardware rusty hardware instead of you know instead of the art so how can we make something that's really fast and small and much safer because that that moves the needle and then that's really what you want is far more safety so yeah happy to drill into more of this you know yeah so yeah so let's talk let's get concrete about the implementation of this because i know you've you've got your design idea there the prototype has proved you've decided to go ahead you've made some interesting choices i know about how you're going to implement this and the the first most obvious one that jumps out at me is your choice of programming language yes you didn't start with javascript as in the prototype yes i didn't stick with the memory safe language yeah so i think probably the most surprising thing for someone you know if they ask why zig is the language you've chosen yeah yes zig is the language why not see why not rust why not javascript why not go and i think the most surprising part of the answer is just to say well i truly believe believe then and now that zig was actually the safest choice if you look not only at memory safety but safety of the system so safety is a much bigger thing than only memory safety safety has to do with correctness what makes for correct control flow and i think what makes for correct control flow is simple explicit control flow a minimum of abstractions versus zero cost abstractions so i i would rather i think more important than zero cost abstractions is to have a minimum of excellent abstractions because that reduces the probability of leaky abstractions so it's things like it's like subtle things like this she probably just quickly step back and for those that don't know the programming language zig give us the headline features yes okay great so i i was actually coming at this as a c developer so around this time i'd been doing most of my coding in c and i was looking for a c replacement i loved rust i had i had pointed like ryan dahl i was just one of the people and he had written denno with the back end in go and i said well you know rust would be great because then you don't have two gcs so i did but then i you know i was a c developer at heart and i loved the simplicity of c and the control of c and ex and precision and but zig came along and it just fixed everything that was wrong with c but it also gave you far more safety so it gave you checked arithmetic which i thought was very important and actually if you look at a lot of newer languages if you look really closely they most of them don't actually enable checked arithmetic by default in safe bolts so what checked arithmetic is is if you're going to do integer operations on a type like a u64 or let's say a u32 and that integer arithmetic is going to overflow in a lot of languages that's just undefined behavior or it's a it's a wrap around and i i had you know the words using two smaller view exactly but many languages you would be surprised i think if everyone went home and double checks this you know in a safe build the default settings is checked arithmetic enabled usually the answer is no which is very surprising because actually you want check charismatic on where safety is mission critical because i i had done a lot of security work in in the security work in the cave you know some r d on how do you do static analysis to detect zero day exploits and it worked you know you could you could catch them it was really fascinating but a lot of it came down to like let's look at a zip file and let's see is there is is this zip file format is there arithmetic overflow happening in these values and and if there is chances are it's almost certainly a zero day it's gonna attack the antivirus software that opens it up because you're going to get arithmetic overflow which will allow the the malware to exploit something else and you chain these things together and you get a cve so there's like a checked arithmetic is really important if you look at things from a security perspective i think and and security you flip that around and while that safety it's a big ingredient so it says why zig you know well it it was one of the few languages that actually had it enabled in safe in safe bolts and i thought wow that that's a great design decision i wish more languages have that and then it had bounds checking which is base you know if you if you're in c you know it's like walking blindfolded on the cliffs of dover and you're gonna fall off yeah so so easily you know and you really want bounce checking and zig gave you that and you want you know actually like for distributed systems the challenge isn't really memory safety bugs most of there was some a paper on this you know i think on the order of like 90 of the major incidents in distributed systems are actually the lack of error handling around ciscoles and zig was really great for that they gave you first class error handling and the compiler would check you know if you don't do proper error handling you can't just ignore stuff even now zig has got a design decision that people have pushed back on you know no unused variables people say well i want to have my unused variables but in terms of safety like do you really because often that's you know in tiger beetle we've seen before zig made this change we saw we could have had some you know latent correctness bugs we we found them already but this principle of not allowing unused variables is a good principle you know from a safety perspective so and zig is a very simple language but fundamentally also if you're a database you need to handle memory allocation failure you also want very fine-grained control of your allocators that you can use you don't want to have just a global allocator somewhere hidden you know and a panic so again c it was either going to be c or zig and and the way i motivated this internally was i said well we're gonna do c and then everybody panicked and they said no please i want to i want to i want to be able to build this on my windows computer like please give me a proper tool chain you know and zing had such a phenomenal compiler you can cross compile from windows to linux or to m1 chips it was the first compiler to do that and so it's just got a fantastic developed experience story you it's a great lovely compiler like it it's and and it's also one of the few languages that are also investing in the tool chain you know the compiler is so important so so it was always going to be zig i guess but threatening the cec helped that is a captain kirk level bluff there well i think it was unintentional but retrospectively you know let's let's claim it for cook claim it is a win claim it is a win yeah okay so we've got the architecture you've gone for zig i know you've another thing you've wanted to do differently is the way you approach testing which i thought was very interesting in the architecture take me through that yes thanks chris so yeah and i guess just to add you know both zig and rust they they're phenomenal new systems languages so it was again like are we gonna invest in the last 30 years of c or you know what if we're going to write a lot of code where do we want to be in 20 years time what language do we want to be using and for sure like rust or zeke so that that kind of made it very easy so testing yeah that was the other challenge you know because building a database it's it's a storage engine if you're a single node database you have to build a storage engine and that's something like a an lsm tree usually that's the big engine that powers these things these days and typically if you want to build this lsm tree and you want to get it production ready they say people said like you know it's about five years to to not only to build it to to test it most of the time is dominated by testing so you can build it in a year but then you tune it in another year and you test it in another three years and and then you know it gets it gets widespread adoption but that's five years and the problem was you know we needed a a database that was not only single node like mike sql or postgres because we wanted a great operator experience that you just get automated failover and high availability so you run tiger beetle as a cluster of like three or five nodes and we actually support some more interesting configurations you know but this general idea you know a cluster of five nodes and your primary goes down and the cluster will automatically elect a new it's basically backup and and recovery like automated for you instead of you doing this you know doing manual failover between in a in a multi-master setup you know for postgres you don't want to do that manually at 2am consensus can do that for you in a way that's automated and tested and highly available the switchover happens within milliseconds so so slightly aside but did you roll your own consensus mechanism or have you done something like zookeeper great great question so i think all of all of the again the answer is always safety and i think it's surprising like so we we wrote yes we we picked a new systems language we wrote a new storage engine we wrote a new consensus protocol and the answer was was surprisingly always safety because again you know the research was showing that the way existing consensus protocols were proven you know that yes they had four more proofs for paxus and raft but the formal proofs missed something they only considered how the network fails they didn't look at how the storage fails so if you want to build something that is correct for paxos or raft you have to guarantee that the disk never fails so each each disk of the cluster may never fail because otherwise the promise that was given during the voting phase of consensus that promise could get revoked which could undermine the quorums which could cause the cluster to regress into split brain and global data loss so your these two protocols rely on perfect disk they call it in the consensus world they call it stable storage and the formal proof there is no fault model it just says disk is perfect and that's fine you can then solve that locally with with logical raid like zfs you can't solve it with most grade because most trade you know if if there's a corruption on one of the disks it's a guess as to are you going to recover in favor of parity or in favor of the corrupted version you know it's just xor happening across the stripe so okay there's a there's research on this too you know raid raid will make sure that your discs are in sync but if there's corruption there's not enough information at the block device level check something to know which is the corrupt version in the stripe and which is the the real version so zfs can get this right so you sure you can run your consensus over zfs but now you're paying the cost of 3x or 5x global replication plus 3x local replication so now it's extremely expensive and and again yeah so so these were kind of what motivated us you know we thought well but the big problem too is that actually like these consensus protocols if the storage agenda takes five years to test the consensus protocols take 10 years and they still haven't found all the bugs and so you know even if we take an off-the-shelf protocol is it still safe and do we understand it because we can take it but then we have the responsibility to audit the code you know we can't just bring in a dependency if we don't understand it and understanding an existing implementation of consensus that is again going to take a year or two years to build up that in-house knowledge so what we thought was well sure we we what we want is a storage fault model we want to handle this corruption and we want to do that efficiently without local replication because we already have the global replication and there's been a ton you know the last 10 years of research on lsm trees since roxdb and level db so we didn't want to take those two because there's you can do it there's a lot you can do these days for example those have one second right stalls which will bump your p100 latencies in your database so you yeah so the short answer is basically we realize that there's this paper uw medicine protocol aware recovery for consensus best storage in other words how do you build a distributed database in 2020 that's the paper how do you build a distributed database in 2020 and the answer is that you really need to design your local storage engine to work with your consensus protocol and vice versa so what that means is you want your storage engine lsm if it fi yes it must have checksums but if it finds a checksum error you don't want to do any local action immediately you don't just want to like truncate your writer head log because that could actually lead to data loss in the consensus protocol context so what you really want to do is your lsm and your writer head log of the database must be designed first class for your global consensus protocol and vice versa then then what you can do is if you see a local corruption you can actually ask the cluster and get a quorum in correctly and and then you can say well what is the correct thing to do am i allowed to truncate this piece of data and the cluster can say to you yes for sure we know this was never committed it's safe to truncate and that way you actually keep the cluster going much longer because because now you know you know you can unstuck things you know you yes it's safe you can quickly recover truncate the log you know for sure it's safe but there are cases where you can't truncate the log because that log contains committed transactions and you need to to preserve your vote to the cluster so then the cluster will say well no you can't trunk it and really like you have to ask the cluster you can't most of the existing designs they they make this decision without reference to the consensus product which is not correct yeah so yeah long you know a long answer we we realized we have to actually this is going to be one of the first implementations of how you do this you know we're gonna make an integrated stable storage that's integrated with consensus protocol and the thing can be efficient use the global replication to recover local faults and do that correctly high availability yeah that makes sense you know we had an episode recently with benjamin bengfort about implementing your own consensus algorithm and how you could then have custom primitives inside the consensus algorithm and it's exactly that but this is a specific use case of that idea which is really interesting yes yeah no yeah i think that's that's often why this isn't adopted because people will say well you know we're building a new storage engine for raft but we don't we're not at liberty to to improve raft to be storage fault away and and so it's you you're stuck you've got the abstractions but it stops you from getting to the system you really want and yeah so yeah but then the desire to make consensus a black box because is coming from the fact that it seems too scary to open that box but you kind of do need to open that box if you want to get the right behavior yes yeah and you do want to understand it you really you do want to understand it and and it comes back to zero cost abstractions because actually abstractions always carry a probability of being leaky if the abstraction boundaries are not exact you know and this is a classic example where the abstraction of global consensus protocol in isolation from local storage faults is actually the the formal proof needed to really consider the whole system be a system proof yeah with you and benjamin for a smackdown that'd be interesting oh yeah yeah that would be great i think i still haven't answered your question you know how do you test testing we were talking about testing yes yeah i've i've just made everybody nervous like you know oh they wrote the storage engine they wrote their own consensus protocol storage engine takes five years consensus protocol takes 10 years and we still haven't got all the bugs but i'm going to dip into that before we get to testing in that case as you mention it because some people are going to look at this and think all these safety properties are very important and great but it sounds like you're disappearing down a rabbit hole how are you actually gonna get to production from that large work stack yes yes so i think the surprising thing is we found a silver bullet i love fred brooks you know no no no silver bullet but we did find one in the cave and this is really like credit to foundation db they they wrote foundation db very differently also you know they they wrote this whole database that you could run it in a simulator so you know what's the best way to become a pilot and in the past you know you used to jump in a plane and fly and crash and then you survive and you get better and you you put hours on and then one day people realized well we don't want to keep crashing our planes this is very expensive and we lose the pilots so actually like let's build a flight simulator and we'll simulate everything and you you can crash inside the simulator where it's safe so that's sort of what foundation db did like i think they were one of the first most databases are not designed like this you have to fly them for real and crash them in production and get the issues yeah foundation db you fly in a simulator and then the simulator speeds up time also so if you have to run your database for two years before you hit the bug the simulator can speed up the passing of time so you can get you can find the bug in like two days of simulator time versus real you know these if you were to test a system with jepsen you have to run it for two years yeah jepson can't speed up time so foundations found this very this this silver bullet you know so valuable this kind of it's actually called deterministic simulation testing it's the idea that you design the database to be run in a simulator because then you can speed up time you can you can get to for example so tiger beetle obviously we did the same we wrote our consensus and storage in a very specific way that it's deterministic so given the same inputs and network latencies you'll the code will always execute exactly the same and so you always get to the same answer which means that now you can debug tiger beetle and from a single seed you generate this whole like simcity world of events and it's like a big bang and and a whole lot of chaos happens and then but you can always recreate that so if you find an interesting consensus bug your team you know you drop a little seed in slack and suddenly everybody around the world can reproduce this whole series of millions and millions of distributed systems events you know and this is like this is like generative testing but you're not just generating the fixed data set from that seed you're also generating how reliable is my network model in the simulation how reliable is my disk in this simulation that kind of thing big exactly and then then what you do is you you're simulating latencies so yeah if i you know and latencies alone are very interesting if you simulate wild disk latencies it could uncover a lot of interesting race conditions in your database code and i'm i'm sure you know existing systems could could do more of this but you you can do this in the simulator you know wild latencies network faults and then storage faults so tiger beetle we actually some of the tests we inject like eight percent of corruption every time you read the simulator will corrupt that read eight percent and and that's something that i think almost no database can handle there's a lot of check summing but usually the check summing is there for crash consistency through power loss but it isn't there to to handle just correct action of everything in the data file like in anything in the data data file is just corrupted eight percent of the time on the read path and then on the right path we'll just nine percent of the time we'll write to the wrong place [music] yes and we just do that we do that every day it's very normal you know so we operate like at extremely tight tolerances like extremely rare bugs and then the easy bugs become very easy you know yeah yeah i almost hesitate to say this on record but it seems like there'd be the military would be interested in running your database in the field if you've got that level of fault tolerance you know because i'm just thinking where would you actually get eight percent this corruption when there's when there's a battlefield happening around your desk right yes yes or if you you know you you boost your tiger beetle into space and and yeah you've got all these cosmic rays and when i was i mean the tiger beetle in space was my favorite cartoon so that's perfect is that really a cartoon no it's no no i'm kidding i wish okay well maybe for the next generation of kids we can make it happen yeah yeah even if it's a little sideline mentioned that yeah okay so so we're sort of running out of time i do want to ask what's your part where are you what's your current status but like in terms of alpha beta production and how are you going to get to production yeah so we're very close we've actually only been a little over three years from from zero and already the consensus is tested to a tolerance that most systems couldn't survive and the storage engine so we we're busy polishing and wrapping up and and coming into some first releases this year starting a release process in september and tagged releases and then we'll increase the you know the guarantees of storage stability as we go that's sort of the the current focus now is let's just you know lock in our data file format i guess the second leap we haven't really dived into you knows is at some point we can open source become a company you know so and that that happened last year november you know this idea you know tiger bill incorporated yeah and is what's your angle for that how are you going to make the open source software pay for itself yeah so that that was also kind of a tension you know we were we were fully invested and we still are in always in in apache to open source because we we came out of our open source payment switch so we saw how in you know open source is crucial to a business you can't you you couldn't have a business supplying an open source payment switch with software that isn't open source yeah you know even the bsl wouldn't work because it's not open source so it is it's the anti-business license you know we couldn't have a business if it's not open source the question is well how do you do a business then if it is open source and that that took me a long time to figure out but i think the key is just that it's mission critical so people want they they want open source but they also want people to run it for them because it's mission critical or if they run it themselves you know who who are you going to call if if something does does go wrong because yeah so we may see tiger beetle support contracts and tiger beetle as a service before long yes yeah yeah i think and it's it's quite i would love that you know and and the more people running tiger beetle the better like we it's kind of i i would never have worked i i actually joined coil full time to work on tiger beetle's open source but i would never have worked on these things i just see them as so valuable you know a new distributed database i love zfs so much you know and i was always sad that they didn't oracle didn't get the license right and so i i would never have worked on a new you know distributed database technology if it wasn't open source because i think these things are too valuable they're they're bigger than a company it's an ecosystem you know so you you really want want this to be open source and and it actually makes business sense you know to create an ecosystem around yes yeah well i hope building the business around it works out so that you can keep the open source going for a long long time yes in the meantime you're in thanks very much for giving us the breakdown and i'll talk to you again soon yeah yeah thanks so much for having me it's been a real pleasure so thanks again to you cheers thank you joran it's got to be said he is biting off a lot there but it's an interesting bite to take and his ideas about testing i think he might be onto something he might actually accelerate the development of making that database reliable by a heck of a lot so i'm gonna put a link in the show notes to one of tiger beetle's test cases it's an easy read it's well written and it's interesting the way they've modeled like cluster failure and disc failure network failure straight into the test suite it's probably the way we should be doing really really mission critical integration testing something to learn in that code i think we're also something to learn going to need to have a dedicated episode on zig the language he's using i'm keen to learn more about that so i'll go and look for a guest stay tuned and of course the best way to stay tuned is to click subscribe and notify and all the buttons whatever buttons your app has if you've liked this episode please do leave a like or a share or a rating or all three because ultimately that translates into there being more episodes to come for a long long time and it makes my day too and with that i think i'll leave you to your day i've been your host chris jenkins this has been developer voices with yoran dirk grief thanks for listening foreign