pandas is a fast maybe even blazingly fast powerful and easy to use open source data analysis and manipulation tool built on top of python but if you're not careful bundles can get slow especially if you're dealing with larger data sets that use a lot of memory to avoid that it's helpful to know a bit about the different data types that badness has so that's what i'll cover in today's video i'll also show you how a simple mistake can result in your code using 90 more memory than needed making it hard to scale to larger data sets a more general thing is going to help though is to simply design your code better i have a free code diagnosis workshop that teaches you a particular mindset for reviewing and detecting problems in your code based on three factors it's really practical i'll show you how it works with real production code so iron dot codes slash diagnosis the link is also in description of this video if you want to start using pandas in your python project then obviously you're going to have to install it so you can simply do this with pip install panelists like so and as you can see i've already installed this because of course i've actually prepared this code example so basically in partners when we talk about data and types of data there's two important data structures that you should know about which is the data frame and the series basically a data frame is a table like format so it's going to have columns and rows in it with data and part of data frame is a series which is going to be a single column of information so anything you do in pandas basically works with these two main concepts what you will often do in panels is import data from let's say a csv or an excel file and then do some processing or analysis on that data and what happens under the hood is that pandas actually relies a lot on not at all which is numpy so it uses numpy arrays and d types for the various elements of the series and that means it relies for a big part on the built-in numpy types so things like floats integers booleans these kind of things that basically all directly taken from non-pipe and that's nice because numpy is built in c so it's going to be very efficient which is great however there are some limitations to what you can do with numpy since that's really built for numerical analysis though so the data types are pretty basic so to speak for example it doesn't have a time zone aware date time representation so as a result pandas has actually added a bunch of extra types that it uses on top of the built-in numpy types one of these is the time zone aware daytime type a few other examples of types are the period d type and that's used for let's say time series analysis where you have a certain frequency and you want to model that it also has a sparse matrix representation which is basically a matrix that mostly has zero value so you can store it more efficiently using this particular representation it has types for let's say intervals if you want to represent an interval in your data and a couple of other things as well in particular one type that's interesting is the categorical type and that i'm going to touch on a bit later and finally each of these built-in types also has a string representation and anything else bundles basically uses the object type so if it doesn't know what it is it's just going to use objects so what happens is that when you import data with pandas it's going to try to infer the types from the data and often it does a good job but there are some cases where it's going to make the wrong assumptions and if you don't take care of that properly it's going to potentially have a huge impact on the performance and the memory utilization of your program and this is exactly why it's so important to know a bit more about data types how pandas handles it especially if you're working with bigger data sets if banners detects data type is wrong you can use the ask type method to convert manually from one type to another here you see an example of how that works so in this case we have one example where we're converting everything in the series to a float and the second example only looks at columns a and c and converts those two booleans and floats another example of type conversion in pandas is to numeric so here's a simple example of how if you let's say have some string data but it's actually a number you can use to numeric to convert those things to actual numbers i want to show you a quick example of how this type conversion works in pumba so we have a file here that reads a data set of airports so these are all the airports in the netherlands this is the csv file one thing that actually really surprised me is how many airports there are in the netherlands i mean we're like a tiny country but there's like almost if i scroll down almost like a hundred airports actually a lot of these airports are actually medical airports so they are from hospitals but there's also some military airports and i have no idea what for example stats canal airfield is actually i grew up very close to that town i never realized there was actually an airport there so must not have attracted a lot of traffic but who am i maybe i just completely missed it when i was a kid anyway we have this set of data as you can see there's like ids and we have an airport type and we have a name of the airport we have some longitude latitude information there are some country location information websites and i have file here where basically i have a couple of functions to read this airport data set from path so that simply useless read csv and then i have a i'll talk a bit more about this part but then i have a main function where basically i define the path in the file name and then i'm sim simply going to read the airports from the file but when you do that so let me run this to show you what happens and i'm printing out some information here to show you what's happening so when you do that you see that there's actually lots of wrong types that have been inferred by pandas so here are the columns that it got from the csv file but you see that counts at how many there are but basically all the data types are of type object and that basically means bundles has no way to infer what the data actually is the reason that this happens is actually that the second line here is a bunch of meta information about what each thing is so it's basically that it's an id that has a code that does a bunch of extra information and that sort of throws off pandas in trying to infer what the types are supposed to be so second thing that i did here was i removed the metadata information so basically that's actually almost the same kind of function except that i'm skipping the first two rows so that's it doesn't try to infer the types from the metadata which is what was happening before so when we look at the next part so these are the types that formulas in first after we've removed the metadata row so we still have all these columns id type etc etc but now you see that a bunch of them have actually a different data type like an integer or a float like the id which is recognized as an integer or the latitude and longitude are represented by floats this is already going to be much better because this means that bundles can now use the appropriate internal representation for the type of data awareness object is very generic and that might not be very useful because then you'd have to do lots of conversions of types when you actually want to use the data so this is still not perfect because there's still lots of these generic object types here so often before you do any analysis you might want to go through the different types that panels invert and actually switch them over to use the correct type and i'll show you an example later in the video of why that is important so what i've done here is let me scroll down a bit more is that after removing this metadata row i've created here simple dictionary which contains a couple of mapping types so basically i'm telling thomas hey the name is a string component is a string the iso country iso region these are also strings that's a wikipedia link there are keywords etc so then what i do is that i use as type like i mentioned before from that and that works on the data frame to map each of these column names so that they're actually the correct type so you can do this with dictionary like i'm shown here but you can also do manually per column here we have an example where i do a manual conversion of last update which when we print this we see that it's been recognized as an object but last updated is actually a date time so i'm converting it here to date time and you could also include is here basically if you wanted that then you would have to add a last updated entry and then you specify it as a date time type and you can even add the time zone between brackets if you want it to be time zone aware so let's see what the types look like when we print this one more time so that's this list so now you see for all these data types we no longer have objects but we have actually strings booleans and last update is of a date time type now because we have the proper types in panels it's going to be a lot easier to do analysis of the data and also the internal representation of the data is going to match better with what we've read from the csv file now you might think okay well then i'm done basically and now i can start processing all of the data well not entirely because there is an issue if you try to use a large set of data and i have another example the netherlands airports common separators file is not that large i mean it has less than 100 lines even though that's a lot of airports for a small country like netherlands but i have another data set here which is this one and this comes from kaggle and this is a data set belonging to an e-commerce website with some customer information so it has a bunch of ids zip codes it has cities states basically data about customers of this particular company in brazil and as opposed to dutch airports this actually has lots and lots of lines i think there is about well you can see it here almost 100 000 entries which still is not a huge data set i mean this is not millions of entries but it's a lot more and if you don't think properly about how your data is represented the way you're doing that efficiently you might run into problems when you want to analyze the data if we look back at this type conversion that i did with the airport here i'm mainly using strings actually but strings themselves are actually not very efficient representations of data especially if you look at this data set for example if we would if we use strings for all of these cities and states in particular well that's going to take up a lot of space for things that are often exactly the same string so for example here we have different states spc etc so there's a limited number of states in brazil there's also a limited number of cities even though there will be of course more cities than states so if you simply represent everything as strings there's going to be a lot of duplication in your data set and with hundreds of thousands of records the duplication is going to be a problem if you need to analyze that what pandas has is a so-called categorical type that optimizes for these types of situations where you have a string representation of something in your data but the number of possible representations is smaller than the number of entries in your list of records but panels doesn't recognize this automatically so you'd have to manually indicate that a particular type is a categorical type and then panels can do that optimization for you so here i have another script where i have function that reads the data sets so this is an example where i'm simply reading the whole data set from that csv file so that just contains a bunch of customers i have a helper function that calculates how much memory is being used by this particular data frame so thomas has a memory usage method on data frame that's actually very useful for this you have to make sure to set deep to true so it measures the actual data being used by object types and then what this function does is that it simply prints the memory usage and then i have a function that converts certain columns in the data frame two categorical types so basically what i do here is i create a copy of the data frame and after each column i'm using as type to convert it to category which is what you use for categorical type and then i returned that data frame copy and then finally i have a calculate percentage difference function that computes the percentage difference in size between two series so the main function i read customers i simply print the types before conversion i also calculate the initial memory usage here then i have my customers after the type conversion so i'm using converting to categorical that's the function i just explained and then basically i'm taking the zip code prefix city and state and convert those to categorical types and finally i print the types after i've done this conversion i compute the memory usage and then i compute the percentage difference and finally i print out the percentage so when i run this then this is what we see so let's scroll up to see what's going on so we have to type before conversion so i didn't do any manual typefixes here so as a result dandas used objects for the customer ide and for the city and for the state basically only thing that it detected was that the zip code prefix is an integer so that's the only type that thomas actually was able to detect and you can see we also have memory consumption information here so there's an index to keep track of all the records and each of these columns will use quite a bit of memory because we have almost 100 000 records after conversion you can see that we have a customer id and customer a unique id is still an object we could convert that to a string if we wanted to and then we have the zip code prefix city and state which are categories because we converted that manually then we have again the table containing memory consumption information so these are exactly the same as before because we didn't change anything so then you can see there's a pretty huge difference between memory that's being used here after switching to categorical type for versus what we had before with the object type and same for the state for the state it's even more because there's even less variety in states than there is in cities so if you look at the percentage you see that city and state they basically save 90 or even 98 of memory usage and that means that if you deal with not 100 000 records but you have let's say a million or more records then this is going to make a major difference in how performance your data analysis is going to be because you need much and much less memory now categorical types are not always suitable an example of this you can see here in the customer zip code prefix we're actually using categorical types increased memory usage and the reason why that happens is that if you look again at this data set you can see that each of these prefixes is basically a string but if you look a bit through the data set you see that basically all of these prefixes are different so that means that if you switch this to categorical type you're not getting any benefits because the data is simply too different you can't really use categories here but even worse because now you don't just store the data but you also have to store the categories it's actually going to use a bit more memory than used before so that's what happens here with the customer zip code so in short it's a good idea if you import data from a csv file excel file and you want to process that later you need to take care of how that data is going to be represented internally especially for big data sets this is really important and simply switching everything over to categorical types is not going to be the solution though in many cases it can actually be a really great solution to help you save memory and then you'll be able to deal with larger data sets more easily in python so hope this helps you get a slightly better understanding of how pandas is representing data internally so the takeaway is that if you have a column of string data and the number of possible values for those strings are much lower than the number of records then use the categorical type because that's going to save you a lot of memory so i hope you enjoyed this video if you'd like to learn more about how to build well-designed python applications and analyze data or use machine learning you might want to watch this video series next where i dive into a data project and analyze and refactor it completely thanks for watching and take care