hi meeting C++ this talk is on AI assistance for C++ developers and my name is Michael price I'm a product manager on the Microsoft C++ team where I focus on using the power and flexibility of cloud computing to improve C++ developer productivity and satisfaction with developer tools produced by Microsoft and GitHub this session will discuss some fundamentals of the artificial intelligence tools that are rapidly emerging in the developer tools market and demonstrate how C+ plus developers might use these tools in the coming years finally we'll discuss principles for responsibly developing artificial intelligence so that the tools that we build on top of these technological advancements provide benefits while reducing the potential for harm to users and the public at large I want to start off by clarifying what we mean when we're talking about AI assistance we'll break it down into parts and cover some artificial intelligence Concepts first artificial intelligence can be defined as a broad set of techniques used to train computers to complete tasks that would otherwise require human intelligence such as answering questions generating data and recognizing objects what I find interesting about this definition is that to satisfy the requirements set forth something doesn't need to exhibit what we call human intelligence but rather emulating this idea well enough to provide some benefit is sufficient this is an important distinction with something called artificial general intelligence that phrase is usually associated with computers or other technology gaining sentience or a true sense of self-awareness complete with internal motivation and the ability to take independent action The Fill of artificial intelligence has been around for a long time almost as long as what we call Computer Science today throughout the decades different implementation strategies have been pursued and developed in the field including expert systems genetic algorithms neural networks and recently the appearance of large language models some of these methodologies perform better for classification systems where an AI system is tasked with evaluating input against some known state or Fitness function and other others are better suited for generative tasks where the AI will generate some potentially complicated output based on the input llms are the driving force behind the recent Spate of AI assistance so let's talk about those in a little bit more detail large language models or llms are models trained on large amounts of Text data that can perform a wide variety of language tasks including text summarization generation and categorization we'll discuss what some of those terms mean on the next slides but I want to emphasize that these models are created from extremely large amounts of data each llm we will will be built from different sets of data and the procedures for building them will vary between them which results in potentially very different responses to the same prompts but they generally are using the same kinds of techniques let's look at some of the key terms you'll want to understand to fully appreciate their complexity the process of creating a model from a training data set is quite obviously called training the model that is created by training is a graph data structure where the relationships between natural language components are stored nodes in the graph represent tokens and relationships between them are called parameters during the training process the incoming data is broken down into tokens the exact definition of what is a token and how the input gets tokenized varies between llms but you can think of a token as a small enough unit of the language that you can determine relationships between them in the language once a model has been trained you can use that model to generate some responses sometimes called a completion to a prompt generally the large the larger number of tokens and parameters a model has the more sophisticated the response it generates from the prompts let's look at a very simplified diagram of of how all this works so in this diagram we have some training data in German that roughly translates into English as Let each sweep his own door and every quarter of the city shall be clean that training data will get tokenized and the training process will produce the model which consists of these tokens and the complex relationships between them the parameters at some point later the lmma is provided with a prompt of this other German phrase in English it would be something like how do we keep the city clean this also gets tokenized and then sent to the model for prediction the model will look at the relationships in the prompt and determine what the next token is likely to be and until it determines that it is done or it it hits a practical limit this is effectively a very sophisticated and complex version of word prediction that you likely see every day on your smartphone when you're writing a text to a friend so what llms have been created in the past few years and by what organizations the first of what we call llms were created less than 6 years ago in 2018 Google created the B llm and the next year they also released a second llm 2019 also saw a relatively new player in this area open AI released gpt2 which had generative capabilities that the two prior ones from Google did not have then in 2020 open AI released a follow-up called gpt3 and in 2021 Google released Glam then in 2022 we started to see many more llm appear from many other vendors Google produced several llms in 2022 and meta also entered the market with opt and later Galactica Beyond those five additional llms were released in 2022 for a total of 10 new llms in one year twice as many as in the previous four years and in 2023 this trend has only sped up meta introduced llama and its successor llama 2 open AI released GP pt4 which is a significant improvement over gpt3 and Google released their iteration on the palm llm that had been released about a year earlier 10 additional llms have also been created in the first three quarters of 2023 making the total for this abbreviated period 14 and as this revolution has occurred the sophistication of these models has increased dramatically it's common for llms being introduced today to have hundreds of billions ions of parameters and trillions of tokens so now that we've covered llms let's talk about what it means to be an assistant our friends in the dictionary industry say that the verb assist means to give usually supplementary support or Aid to supplementary is a key word in this definition an assistant isn't there to completely solve your problem their purpose is to supplement your own knowledge skills and judgment in service of accomplishing your task at hand you the developer determine what sort of Aid the assistant provides this has two implications that I think are worth pointing out the developer is responsible if an AI assistant suggests a line of code to insert into a source file it's still the developer's responsibility to ensure that the code is correct and developers should be very critical of any information that the assistant is providing to you by their nature the information they will present to you likely will sound very convincing it's the job of the developer to use their judgment to evaluate the information the assistant is presenting a useful guideline is think of an AI assistant as a very eager overconfident beginner with access to an enormous library and the skills to quickly find and collate information it does not possess reasonable or really any judgment so let's let's talk about the different ways that um you can be assisted by an AI assistant how might AI assistants deliver their supplementary support broadly there are two ways that you might interact with these assistants the first is the conversational model in this model your interaction with the assistant will take the form of a series of related questions or requests after each prompt and response you may provide a follow-up prompt and the assistant will have some recollection of the prior prompts and responses some assistants may also make helpful suggestions for follow-up prompts to help you direct the conversation in a way that you find useful this approach is helpful for Discovery and gaining an understanding of an unknown aspect of a codebase library or platform another way that AI assistants might provide their help is to focus on a particular kind of problem with a limited amount of user interaction perhaps just triggering the assistant and accepting the suggestion this task focused approach will usually be tightly integrated into the developer inner loop of code compile and test and will eliminate or greatly reduce a lot of boilerplate work that developers currently do manually some will be active meaning that a user will explicitly request assistance with a task While others may be passive with the AI assistant taking on tasks without being invoked directly to do so with each of these approaches the ability for the user and or the tool to provide a properly crafted prompt is a crucial component of getting a useful response because of the importance of writing good prompts for these assistants we are seeing a new software engineering discipline appear that is called prompt engineering prompt engineering is the practice of crafting input to llm powered AI assistant to obtain suitable and desirable responses there are a handful of useful prompt engineering tips to consider as you start interacting with these tools set a Persona uh it can be surprisingly effective to indicate what sort of role the assistant should take the description of these personas can help direct the llm down useful paths when it is considering subsequent portions of the prompt provide context by default the llm has no prior knowledge of anything that it wasn't already trained on in order to get the most useful responses you should provide enough context with which the rest of the prompt should be considered many tools will provide shortcuts for you to provide useful context such as source code or other data that shows up in your editor or IDE specify constraints if you know of responses that you do not want or some qualities of those undesired responses tell the llm that those should not be included and it should be able to exclude those in its response multiple submission llms are non- determin deterministic systems you can provide the same prompt to the same llm and get a variety of responses usually they are not terribly different but sometimes one of the responses will be more desirable than the others llm limitations llms and the apis to them have limitations for instance llms can only support a fixed number of tokens in a prompt so if your prompt is too large you run the risk of part of it being lost or if the or of the request to fail entirely most llm apis also have to throttle traffic to preserve access and reduce bottlenecks and denial of service attacks specify output formatting this is especially important for task focused prompts where the response to the prompt will be programmatically interpreted but even in conversational applications specifying how the response should be formatted can force the llm to be more direct and tur which can be valuable to the user and finally threat modeling since these systems are complex prediction engines if a malicious actor can inject text of their choice into a prompt they can potentially cause information leaks or misleading responses everyone creating or using these new class of tools should become somewhat familiar with the risks exposed by emerging threats like these injection attacks so with the understanding you now you you have now these tools let's look at how they can be used in a C++ developer day-to-day activities so here we have GitHub co-pilot chat and I want to learn a little bit more about how to use it by typing forward slash I can access a list of commands that will help the AI understand what it is I'd like to do here here I've triggered the help command to give me more information about how can use GitHub co-pilot chap so I have an old project that I started about a decade ago um that I've been interested in picking back up um uh I've already updated it to do sort of modern CI and package management and build systems but I I I want to keep making improvements to it um so I want to learn more about what this project is so I'm going to ask co-pilot chat uh what is it that this project does but as you can see you know co-pilot doesn't really understand what this is uh what this project is without actually having more context uh you know I I can ask in a different way maybe maybe if I tell it about the current project he'll understand but GitHub co-pilot kind of gets the idea but do really doesn't still doesn't have enough information to really tell me what the project is so I'm going to open up the read me file and give it a little bit more context um now this readme file is about as simple as they come uh doesn't really have a lot of information in it but even that is enough to sort of help GitHub co-pilot understand what the context is so I can ask it the same question I asked before but now that it has this context it can give me more information about what Coons are uh and what they mean in the context of this project so I want to continue learning more about the project so I'm going to ask it what what language it's currently uh the current project is written in and you know it's going based uh it's deriving this based just on the name and it understands that cxx is sometimes used instead of CPP to to indicate that it's a C++ project so then then I ask okay well tell me some of the coons that are in here and again it doesn't really have the context to really give me a list of all the coons that are in this project but it is able to determine what sort of coons might exist in a project that is teaching C++ through Coons now I want to continue exploring um the project and I'm going to start in the the main.cpp file where my main function is so I'm going to tell co-pilot to tell me what the main function is doing it does a pretty good job of describing what what it's doing now it's a simple function uh but it does a good job of indicating that there there's a server involved and that it's listening on uh a port uh and it understands that it's pulling file likely to be pulling files from certain directories in my source tree it also understands that it starts the server and it waits for feedback and then it'll call stop whenever that whenever the function is exiting so I want to ask what the cxx  server object is responsible for it does its best job it's answering kind of generically uh and this is partly because it only knows of this because of what's in this file it doesn't actually see the cxx server file so let's go ahead and ask the question that it suggested now it it suggested this question um but it turns out that even though it knows that this file exists out there it only knows it because of that include statement it still doesn't have the context of the source file itself and it and it realizes this and tells you that it would like to see more let's open up the implementation file though uh and see what sort of information it can give us about that so let's open uh let's take the suggested question again of the suggested prompt and it looks like it can understand quite a bit about it it understands that it's uh it implements an HTTP server uh using the C++ rest SDK um it understands that it serves up static files and that it is handling different kinds of resources so let's ask it about what the C++ rest SDK is as well because um I happen to know um you know that that is a Microsoft Library um that is deprecated and I I think I'm interested in actually updating uh this tool to depend on a library that's actively being maintained so let's let's ask it what it knows about the C++ rest SDK yeah so that's actually a really good description of it um it understands it was a code name Casablanca uh it understands that it's a crossplatform library um and it understands the the the the goal the purpose of the library um it even understands that it uh handles and produces Json on data and it also sort of understands the usage of this Library within the context of my project uh and that I'm using the HTTP listener um object to create a HTTP server uh that listens for requests and responds to them so I've identified one potential Improvement uh replacing the C++ rest SDK but I'd like to still get a a little bit um more in depth understanding of the other parts of the product uh so I'm going to take a look at a few different classes here a few different source files here and see if it can explain to me what it is they're doing and when I do this um we'll see we'll see some of the limitations uh some more of the limitations of of these AI assistants um in that um it can only process so much context because of the limitations that the API for for the llm has so we so we asked it about this HTML printer source file and it does a good job about describing what it is um but then it it claims that there's not a cxx  HTML printer class defined so I I remember whenever I first implemented this that I didn't Implement uh this functionality as an object uh I just use a colle ction of functions and so I'm I'm pretty sure about this let me just scroll through here and check and yep there there's a print as HTML function I'm pretty sure that's the one that that implements the the functionality here so let's just remind or let's just let um GitHub copilot in on the information that we know and maybe it can give us a good summary as to what uh this function is doing yep and it and it understands that oh yes right you're correct here uh so it goes and describes what the print is HTML function does uh and that it's doing what it thought that a hypothetical uh Class Type might be doing let's see what information GitHub co-pilot track can provide about this source file H interesting so it um it it claims that there is no actual implementation of the git exercise answer but that doesn't make a whole lot of sense because you know this thing wouldn't have built if if that function didn't exist um and sure enough there there's the definition this is an example of uh GitHub pilot chat not being able to grab enough surrounding context to really truly understand what's going on so I'm just going to tell it um that hey there there is a there is a function down here uh and and now that I've scrolled to it on the screen um it should give me a better answer now yep and and once again um with a little bit of help GitHub co-pilot chat was able to find the the thing and uh do a summ summarization of what it's doing so let's ask it a little bit um more about um some of the non C++ code sections of of uh the project so I'm going to ask it about the dub www resource folder um and what all exists in there and again I ran into the problem of well it doesn't actually have um access to the the the file system uh in order to actually inquire about what files are there but it understands the pattern that www is usually a a full is usually a director that's used by web servers to to serve up static files and such so it does a pretty good description it even gets some of the names of some of the um source files and um and directories that are underneath it um just just out just by guessing now let's ask it a little bit about the files that uh that we have these are special files that um are basically source files with some special annotations for certain sections uh these get turned into h HML source files uh through that um HTML printer um uh source file through the functions in that HTML printer source file let's see if it can tell what these source files are intended for so I ask it tell me what this is trying to teach and there are safeguards built into GitHub co-pilot chat it's it's understanding to mean in um in the sort of philosophical uh practice sense not in the sense of this particular project so I'm just going to modify my prop sum to be a little bit more clear about what I'm asking tell me what programming principles of C++ this  is trying to teach now that was just enough of a prompt to get it over the the um safeguards that were put in place to make sure that it doesn't answer potentially controversial questions or things that are not related to programming because this is a programming AI assistant so it does a good job of explaining what this particular uh is trying to teach it's all about um teaching what tokens and literals and operators are in the context of C++ so let's ask it about the static assertions and this time I'm not going to give it a lot of context in my query I'm just going to ask it and what about this one and it understands all of the previous context from the conversation we've been having this whole time and it can answer the same question about this source file and tell us what it is that it's teaching so now what I'm doing is I am going and executing a build of this I want to just kind of like test and make sure that uh the project as it exists in its current state before I start making changes um is actually doing what I think it's supposed to be doing so let's build this and debug it and see what the actual product looks like all right it's built uh now I'm going to start a debugging session and because this is a web server um VSS code uh detects that it opened up a port on Local Host and it does a automatic port forwarding so that I can access this um on my system now this is actually running inside a codes space and so the actual execution is happening off on a server somewhere uh but this port forwarding allows me to then view it on my local system so this is what the web page looks like uh whenever you go to the about static assertions and you can see here it it rep it presents this as um source code and on the line where you are supposed to um add the con add the code that will cause this thing to be uh a valid statement there's an input text block uh and and currently it's shaded red because um this assertion is failing so let's go ahead and see what happens uh if we type some stuff in if if I type in true and then tab to the next field it will turn green and if I if I go back and put false it'll turn red again uh let's set it to true and then let's put the right text in here for the static assert message and it's all green so let's go on to the next  and the next one is about keywords and identifiers I can fill in the correct tokens here and voila everything is green so now what I want to go do is I actually want to go go do that piece of work that I identified earlier and I want to swap out the C++ rest SDK uh with a different Library um to implement a a restful HTTP server so I don't know what library I want to use as a replacement so I'm just going to ask ithub co-pilot chat um what it thinks might be good replacements uh for the C++ rest SDK oh great it found a handful of different libraries that I could use I'm kind of more curious about the Poco uh C++ Library so I'd like for it to explain to me more about those particular set of libraries and what it offers yeah that sounds like a pretty good uh library that I'd like to use for this so now now I need to understand okay now that I know which one I want to try um what do I need to actually do to to do this replacement okay it's given me a list of things now you know I don't want to remove the C++ rest SDK until I have a fully functioning poon net replacement um so I I'm not going to do things in the order that it suggests here uh but this gives me a good starting point at which I uh from which I can understand what I need to do so I'm going to ask it what I need to do in order to actually integrate the Poco libraries uh through my VC package manifest file and it gives me and it gives me a little snippet of code that I should be able to go and put um in my VC package manifest file now as it turns out um the snippet of code uh isn't quite right um there's some extra Fields you have to add in if you want to use versioning uh but I don't really need ver uh strict versioning for this for this project this is just a side hobby so I'm just going to uh remove all of the versioning information and just depend directly on just add Poco directly into here and we should be good to go uh from VC package point of view yep and there it goes it's off and it's downloading Poco and all the dependencies it has and um building that stuff for me so I don't have to bother myself with learning how to build Poco myself and now I've sped this up um it takes a it takes a few minutes to do this um but for the interest of this demo I sped that up and now the Poco libraries are actually installed in a place where my project can take advantage of them so I need now I need to go update my cmak list. text file so at this point I'm going to start using um the GitHub co-pilot inline completion instead of GitHub co-pilot chat so I I know enough about cmake that the first thing I need to do is I need to do a fine package package for this Poco Library so I'm going to go ahead and find the right place in the file and do find package and it first suggests boost uh but I don't I don't want to do Boost I'm going to type in poco and and there we go there's a completion that was done by GitHub copilot now I don't think I need util and Foundation librar I'm going to remove those and then I know that the next thing I need to do is I need to add it to my target link libraries so going go to that that line and start typ tying in and it suggests pocon net um in there I'm going to go ahead and do a reconfigure and that uh that worked really quickly which is great and I'm going to execute a build I'm going to make sure all the things I did for the cmake list file um are actually working the build's going along and there we go the build finished and everything linked so now let's revisit the act the code that it actually suggested and what I'm noticing now is that the code that it suggested um earlier on is actually for a client um let's go into our server header file so let's just see what happens if I use the GitHub co-pilot inline completions to just give me a suggestion about what it is that I should put here and look at that it actually gave me the header for HTTP server I'm guessing that it probably recognized that the type of the name of the class in here was server and maybe that I have uh other things in here that are in that same vein so my guess is that's why it suggested that header file which was right on so let's gohe and see what I need to add as a member of the server class so I'm going to start typing Poco oh yeah wait no it's an uppercase p p uh let's go into that namespace uh let's go into the net Nam space okay and let's see it didn't actually suggest um a type for me so I'm just going to use regular intelligence and then give it a name okay so I'm pretty sure that this object um probably doesn't have a default Constructor so so let's go into my server Constructor and um initialize it properly so it looks like it needs to have an HTTP request Handler Factory pointer okay I don't know what that thing is Let's go ask GitHub co-pilot chat what it is I need to do to create an HTTP server using poet ah okay here we go it's talking about an HTTP request Handler which is right up the alley of what I was getting stuck on so this explains that you have to have a a Handler type and then a factory for that type um so let's go ahead and just go over to the source file I'm going to copy the code for the request Handler but I don't really like the name so I'm going to give it a name that I prefer let's try server sorry let's try request Handler okay let's try to use GitHub co-pilot inline completions to generate the factory wow look at that it came right up and gave me pretty much exactly the right thing named with and the same pattern of the thing that I renamed earlier and yep it looks a lot like the piece of code that the GitHub co-pilot chat suggested as well now let's go back to the initialization of the HTTP server op and give it a new request Handler Factory and then if I if I look at the intellisense hint that it's giving me it says I can actually just give it a port number I don't I don't have to create one of these server socket objects so let's just give it a port uh we'll say 8082 and there we go okay let's let's build that and make sure that that compiles oh okay it didn't compile why why did not why did it not compile um it's complaining about base class has incomplete type um for the Base Class of my request Handler what this probably means is maybe I'm missing the header file for this so let's go back up to the header files in our server um CP P file and see if co-pilot can just tell me the right ones I need to include here okay I'll include HTTP server okay request Handler Factory that sounds like a good one I know I used the type from there and and part of the code I added ah and there's our request Handler header file that hopefully should Define uh the base class that we're using ah and look at that the build successfully compiled and linked now I know from previous experience that you usually have to start and stop these things and I even have a start and a stop method on my server object so let's go in here and um call the start uh member function on our server in the right place and we'll do likewise for the Stop function okay let's build again okay built and linked okay let's actually debug this and uh see what happens okay it looks like it's successfully bound to Port 8082 okay great I'm not actually going to go check this with a web browser uh because I know that my request Handler basically did nothing so let's go Implement that next let's ask the inline GitHub co-pilot chat what the simple implementation of the handle request function generating a simple hello world response is fine now you access this in vs code with control I to bring up the inline uh chat interface this is pretty reasonable I'm going to accept this and it's going to insert it in here and I happen to know that I need to set the status as well so I'll add that in oh no it failed why did it fail uh of course it actually didn't give me a function body it gave me a full F function definition including the signature I don't need that stuff so I'm just going to remove it all right let's try again ah it still failed this time a member access into incomplete type HTTP server response well we know what this was last time um this was a missing header file okay well let's go see if GitHub co-pilot chat can tell us what a header file should be added to fix the incomplete class error well it's not terribly helpful let's see if we do better with GitHub co-pilot line completions there we go we almost typed the full thing but it eventually got there ah yes and of course it's not HTTP response. that we need it's HTTP server response. that we need all right it looks like the build finished now that we're actually handling the request and providing a response let's do a debug session of this and see if it's working all right we're over here in our in our browser we we've connected to that locally forwarded port and sure enough the response was Hello World this is an indication that our HTTP server is actually up and running so the next steps I'll take are to migrate the rest interface currently implemented into the C++ rest SDK and Implement that in my pocon net um HTTP server but in the interest of time we won't be able to do that in today's talk I'll just have to save it for later but now I have a running HTTP server which is a good start in completing my removal of the C++ rest SDK in favor of an actively maintained HTTP server Library so let's talk about the framework that Microsoft has set out to make sure that artificial intelligence powered products are made responsibly Microsoft has published the Microsoft responsible AI standard that has six core principles beyond the principles that I'll describe there are specific goals with detailed requirements that map to these principles in the published responsible AI standards document you can read in more detail about this standard at the link at the bottom of the slide www.microsoft.com responsible D aai we'll start by looking at the principle of fairness how might an AI system allocate opportunities resources or information in ways that are fair to the humans who use it this is important because llms and other AIS that use a training data set will inevitably display any biases that are inherent to the training data but the normal remedies that might be used to address biases by humans such as a conversation with a manager or co-worker may not be applicable to an AI assistant therefore it's important to develop ways to balance counter or eliminate biases that may be exhibited by AI systems a good example where you want to make sure that AI was fair would be an assistant that identifies job applications to forward to a hiring manager if a system was trained to look at the applications of successful and unsuccessful applicants without guard rails it would be likely to exhibit the same kinds of bias that had been present in whatever system existed before for reliability and safety designers should consider how might the system function well for people across different use conditions and contexts including ones it was not originally intended for it is a fairly standard practice for computer science students to learn some about the need to make sure that software functions in safe ways that will not fail in catastrophic ways and potentially cause serious harm these are not hypothetical concerns as incidents such as the radiation poisoning and deaths caused by the theak 25 radiation therapy system in the mid 1980s for example an AI powered computer vision library that detects pedestrians Crossing into the path of an automated vehicle needs to be designed to take into consideration a potentially unbounded set of environmental constraints that may not be represented in its training data temperature low light conditions worn out roadways and unpredictable human behavior failures to build in reliability and safety features to the system could result in catastrophic consequences designers of AI systems should ask how might the system be designed to support privacy and security as as sensors of various kinds become ubiquitous and as people and systems around the world become more connected threats to the privacy and security of individuals will continue to rise 40 years ago it would have been almost inconceivable that nation state actors could hijack the equipment of someone in another Nation to be used in further attacks or to steal private information about that person these and other threats to the rights of individuals should be safeguarded by building AI systems that that take these concerns into into serious consideration an AI assistant that identifies affordable housing that is likely to be attractive to single parent families could be a useful tool for many families but it also provides the potential for an abusive spouse to violate the privacy and security of someone who escaped that abusive relationship how might the system be designed to be inclusive of people of all abilities developers of products will bake assumptions about users into their products often based on their own experiences but this can result in products that only work for select group of people leaving other populations in a disadvantaged position and removing them from the market for those products making an AI assistant support a wider range of users by accommodating their differences where necessary can result in happier users across the board consider an AI assistant that summarizes the conversation between a patient and medical professional for the purpose purposes of medical recordkeeping if the patient speaks a different language than the medical professional or perhaps speaks with a pronounced accent an AI assistant that wasn't designed to be inclusive might incorrectly summarize the conversation which could have a lasting impact on the user's health and the medical professional's ability to provide their best health care How might people misunderstand misuse or incorrectly estimate the capabilities of a system it's important for users of AI systems to understand the limitations of those systems and that means they should be designed with transparency in mind any response from an AI assistant should be clearly indicated so that us so that users can make the best informed judgment about the helpfulness of the assistance take the case of an AI powered application that is used by parents to monitor their newborn sleep patterns If the parents do not fully understand the limitations or intended use of the system they may trust the application to also keep track of the child's temperature or or other health metrics when it was never intended to do so this could result in a delay in seeking Medical Care due to the misunderstanding of the application and finally how can we create oversight so that humans can be accountable and in control when it comes to the artificial intelligence systems remember I said that developers are ultimately responsible for accepting or rejecting the recommendations or advice of their a a assistance AI at least none that yet exists do not possess judgment that is something that humans have near exclusive domain over discounting possibly a few other known species so imagine installing an AI powered application in your home that predicts grocery needs for your household and places orders with a delivery service when it notices that some item is about to run out now imagine you throw a huge party at the end of the year in purchase just several gallons of eggnog a week or so after the party a huge order of eggnog shows up at your door without you expecting it it is doubtful that you really wanted a six-month supply of eggnog but the AI application was just basing things on your prior behavior and without the accountability of a human being in control you'll be sipping on that seasonal treat well into March I want to thank all of you for watching my talk on AI assistance for C++ developers the mission of the Microsoft C++ team is to empower every C++ developer and their team to achieve more we hope that you try out our AI products like GitHub co-pilot and GitHub co-pilot chat and that they indeed help you achieve more in your role as a C++ developer enjoy the rest of the conference and if you have any questions about our tools including AI assistance please contact us at visual CPP at microsoft.com or on the platform formerly known as Twitter at visual C