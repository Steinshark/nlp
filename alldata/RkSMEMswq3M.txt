a few years back I was working at a startup that had an enormous product catalog that was constantly updated and people needed to search through it and I got into a debate with a colleague of mine because the data was in my SQL and he wanted to start regularly exporting it out and shipping it across into elastic search because he thought the search capabilities would be better and I thought that's a great idea in theory but making that run efficiently and reliably every day was just going to be an ongoing maintenance problem that our startup didn't need we debated it back and forth and in the end we went ahead and it turns out we were both right it was painful to set up it was never quite fast enough to be up to date and it was reliable except on days when it wasn't and those days just took all productivity out of the window but it was worth it it was a much better experience it was painful but it was worth it could it have been less painful when you need as so often you do to pull data from these places do something with it and stick it into those places is there a good generalized solution to that problem and that's today's topic we're going to look at Apache flank which has been getting a lot of interest in this space and it's designed to solve exactly that kind of problem so I've pulled in Robert mezer who's on the PMC for Flink and I'm going to get him to explain what Flink does and how it does it and in this conversation we managed to go all the way from why does Flink exist and how did he get involved to how sophisticated can you make the data processing part what options do you have what languages can you use and how do you recover when the process crashes what's its crash recovery and scalability story and lots more so let's talk about Flink let's ship some data around I'm your host Chris Jenkins this is developer voices and today's voice is Robert [Music] mezer Robert Meer thanks for joining us how you doing um good thank you I'm really excited to be here on the podcast to talk about Flink today yeah I'm very excited to learn more about Flink because there's a lot of Buzz about it but I've never really sat down with an expert and really understood what's going on so you can fill me in from scratch yeah let's do it okay so let's start with your credentials why are you a Flink expert how did you get into it and what do you do with that um I try to keep the story as short as possible so when I was studying in the Technical University in Berlin in like 2012 13 or so I was working with a bunch of PhD students there that were building a big data system called Stratosphere that um was about competing with aachi hadu basically they had like a database background and thought that um there's so many findings in like 30 years of database research that have not really been considered with the Pachi Hadoop and they wanted to take these learnings and add them to a big data system and um then a few events happened so we so we um open sourced This research project donated it to the Apache software Foundation we started fundraising I became a co-founder of a company called um data Artisans now it's called Rica that um set out to commercialize Apache Flink um as a technology back when we started the company we still intended to build a batch processing framework but um we also it was actually an outside contribution um to add a stream processing API on top of the engine because the engine as I mentioned is based on database principles and databases use this concept called pipeline execution where multiple operators are running at the same time and this sounds very much like stream processing right like operators running all the time processing data so it was actually quite easy to add an API on top um of this engine for uh stream processing Primitives and of course then we added a lot more things to the engine like checkpointing event time and Watermark support the state backends and so on and so forth to make it a let's say real stream processor um and so going back to this contribution sorry um was that we noticed that there was a lot more interest in um our stream processing capabilities compared to the batch processing capabilities that Flink was offering and also to some extent I mean um a pachy spark was Rising very quickly at that time um as an alternative to Hadoop and um I think they were let's say winning the race in the um batch processing space okay and this is how I This is How I Learned Flink and all of that because I was basically working as a student on the stuff and um started a company around it I helped building the open source community as part of Apache and um we did an successful exit of the company to Alibaba they are offering Flink in their Cloud product um and like one and a half years ago I switched to decodable to work on making Flink more accessible as part of a stream processing platform basically which is also based on Flink so your entire career has been in Flink yes are you happy with that um yeah but of course like after I don't know 10 years or so almost in the same technology you start to wonder if you also need to look left and right but of course I mean Flink is um a really big project not as big as like the Linux kernel but there are like subsystems in fling completely different apis and abstractions and you can go very deep in many areas so there's a lot of very different areas in Flink um to work on very different problems yeah any large enough project becomes a whole sub world right a little Universe in itself yeah yeah okay so you've raised a lot of things we need to get into but before we get there just for context give me a typical example of how someone uses Flink to solve some problem so let's say you um have some relational database like my SQL and you're noticing that your full teex search queries are taking too much time on that database so you want to offload this um these types of queries from your transaction database into something that is more purpose buil something like elastic search so you would use um um deum for change data capture to follow the changes on this transactional database then perform some joints in Flink to combine data from different tables into let's say one big document and then you ingest these documents into elastic search so basically you're using and you're not using the beum as a standalone um um system you could do that and use like Kafka in between but in this case Dum would run as part of a Flink connector so there is um connectors in Flink that are based on the visium that allow you to follow changes um on a transactional database and um then you can use for example Flink SQL to um um Express this join of multiple tables and filtering um whatever aggregations and then you use a elastic search sync in Flink to write this data to um elastic search and in effect you have something like a continuous real-time join from multiple different tables and whenever there's an update you basically instantly get an update in elastic search to always have your data freshh and up to date okay so the main thing it's doing for you is in combination with the beum slurping that data out from one place a certain amount of processing to transform it as you load it into another place I think the core of link is a processing the core really is how Flink is able to do this realtime join in this particular example there's many other examples you can also do machine learning stuff with it you can um do aggregations um but that's the the core what Flink is mostly about like realtime stream processing the connectors are more um a necessity to get your data in and out of Link yeah yeah it doesn't it's not going to do much without those yeah the project I mean there other stream processors like Kafka streams for example which are only working with Kafka as a data source but Flink is independent of data sources okay how many data sources and syncs does it support um roughly so so I think the core project has maybe like eight is and then there's probably like 20 or 30 that are in other repositories or somehow available publicly okay um but all the important stuff is covered like palsa Kinesis Kafka um on the sync side you can do any jdbc database you can do um writing to S3 you can I mean any file system actually you can monitor for changes you can write um to these file systems in various um bucketing like time based bucketing database bucketing whatever so there's a ton of options for getting your data in and out and to be honest if if there is a rare case where you have like a custom system then it's also not terribly hard to build your own um Source or sync and Flink okay does it ever get used for like is it always transforming the data and sending it to other places or does it ever get used just for straight reporting um I mean you can use Flink so what do you mean with by reporting like I guess what I mean is could you have like a CSV or um a PDF as your sync CSV for sure that's supported out of the box um so you can write you can write like CSV or more popular like pet files for example ex or you can write in the iceberg um formats um to S3 or something so you can definitely use Link also um for getting your data in in the shape that you want like for loading your data warehouse for example okay I only ask that because I'm always thinking like what's the smallest possible change I could make to introduce this to a project ah yeah okay in that case I think we should dig down into how it actually works what do I need to understand about Flink architecture um so Flink has a few basic building blocks that you use to express your workload so the core is a data flow craft like a set of operators let's say a kfka source and a CDC source and then a bunch of operators like Transformations filters aggregations joints and they can be arbitrarily complex like trust me they can fill entire screens if you visualize them with like hundreds of operators really um I mean okay hundreds is maybe an extreme case but like 30 40 operators is really not unheard of okay and um you basically use one of the apis in Flink like the Java or Scala API or or the SQL API or the python API to build this data flow graph so basically to define the structure of your Flink job and to Define either the logic of predefined operators so Flink has for example window operators that allow you to window data into hourly buckets or something um or session Windows where you want to analyze like have basically Dynamic windows that are just um have the size of a certain user activity and um so you define the structure and then you also Define the logic of what is happening inside these operators and then the engine knows how to execute this um craft efficiently on a cluster of machines so if your workload is increasing you can um change the number of instances the parallelism of your job in Flink and then you will execute this on more machines okay if I I'm just trying to think how that's going to look so what do I do if I'm using SQL am I doing something like create thing that looks like table but is actually a slurping from my SQL pretty much yes so and then I can join to that in my from Clause later exactly so basically you're defining a stream is like a table in Flink SQL so when you when you want to read from a number of Kafka Brokers or whatever like Kafka clusters you create you do a create table statement um for each of these um data sources so Flink SQL supports all the data all the connectors that I've mentioned so you can create um that you do like whatever if you have a cfast um topic called users you do basically create table users and then you define the fields in your um Kafka topic and uh let's say the data is in Json so you define like a Jason der serializer and then Flink knows how to read the Jason data from your CFA topic to analyze it so you def do like a create table to expose your CFA Topic in Flink SQL and then if you want to do for example a filter you just do select star from this table and then in the wear Clause you can Define your filter or you select or you or you read from m multiple tables and you do a join right so I'm going to make you Flinch again because I got to mention CSV again uh do I also do you like create table as output. CSV and then insert into output table select star from exactly yes that's it just looks that like regular SQL type operations yes but there's no underlying storage the underlying storage is just outside of Flink exactly Flink doesn't come with any storage so you define uh S3 S a storage or local file system or whatever yeah okay okay so the next question that rais you say it will automatically distribute the jobs across nodes how is that working um so there's a central comp so when you're building this data flow representation in your let's say Java API or in SQL um you're typically doing this on some kind of client instance so either it's the SQL shell that you're using or it's a Java application that you're implementing and um the Java application knows how to connect to um what is called the job manager process of link so the job manager is the central component that is coordinating the execution of a Flink job or actually also multiple Flink job so Flink supports both single or multiple jobs on a job manager and the job manager is create and the job manager is creating a um distributed representation of this um data flow craft and um is basically splitting out so if you have like Kafka sources some joint operator and a CSV sync um then it will create multiple instance of these operators like if you're if you're running this on 10 machines then it will create 10 Kafka sources 10 joint operators and um 10 CSV syncs and um the work will then get distributed from the job manager to the so-called task managers so task managers are the workers that are running on all the cluster nodes and as you add or remove task manages um you can scale up or scale down the resources that are available for processing and can you do that dynamically can you just throw more machines at it yeah that's a feature that we've added um in Flink I don't know like one3 or 14 I don't know maybe earlier um so it is a feature that has been added to Flink let's say recently um for value of recent two two years ago or so um that allows you it's called the reactive mode in Flink and it basically allows you to add or remove machines and Flink will dynamically grow and shrink okay and that's pretty neat if you're using something like um kubernetes horizontal po Autos scalers so kubernetes will monitor like CPU usage and if it goes too high it will just add more machines you can do the same with like E2 OS scaling groups or something okay that makes sense and is there I mean I'm just trying to figure out this SQL interface is there something like the notion of a temporary table can I join two things stick them into one logical table while I think about how I want to select that data out um so you don't have to worry too much about the representation um like how Flink is representing um a table or a join um in your SQL statement so what so Flink will of course internally so it really depends on what you're doing so if you do a let's go back to the example that we had earlier some user table um and you do like a select star and um let's say you're reading you're reading from Kafka you're filtering and you're writing to Kafka then it's really data is just flowing through the system it's never um materialized in the system it's just um passing through and we're filtering out some like if you do like a insert into Target select star from uh users where X smaller 10 or whatever then it will filter out all the elements um based on that on that predicate okay and but if you but of course if you do something like a join and your input so let's say you have this user table and you want to enrich some information in this user table from a database table so you you have this um stream of user data and um you have a mySQL database that contains um let's say the address of each user and um you could do something like a database query for every user like a query on MySQL for every user that comes but of course you would bombard your MySQL server with um uh queries for every incoming record in the Stream processor your stream processor would actually be um slowed down to the speed of your MySQL server and that's not what you want for a massively parallel system like fling so what you do instead is that you um do change data capture on this MySQL table so you um do an initial snapshot of all the data in the MySQL table loaded into Flink State and then you subscribe to further updates from that table so whenever like a user address is changing we are updating this data in Flink State and and um Flink can then do the look lookup locally in its um State back end so you don't have to do any network access for this particular State lookup it will just be in the local Roxy instance um of uh of the task manager okay yeah so that's a that's a look up join in um in fling SQL and um that's where Flink SQL will decide that one side of the like one side of the join the side with the address information from my SQL needs to be stored in Flink state but the user data that is Flowing um will not be persisted but there are cases where you're joining two tables and you're materializing both sides of the of the join um and whenever any of the two inputs are updating you produce an output result like an output event that will send Downstream and then the syn has to decide what to do with it okay that that leads naturally to two question questions one's about time and windowing but first you're saying that um like for the address thing you're maintaining what is a essentially a local readon replica of my sequels address table yes right yeah how is that State Management working in Flink because you said Flink doesn't do any storage so what's going on yeah it is okay I was lying to you let's just say it was for the sake of making it easy to understand um we can we can separate between logical physical storage sure um so um Flink the project itself doesn't provide any permanent storage like Amic SQL Server um to persist your data in your um database tables but Flink um needs a lot of temporary storage and um that's what we generally call State so State can be as small and simple as the current Kafka offset that you're reading from and it can be as big as um your entire uh address table from my SQL so we have customers that are using Flink with like 250 gigs of State um in their in their joints and even that is small like if you talk to Apple or Netflix which are using Flink in production they're really talking about terabytes of state that they're maintaining um across their task managers okay and um so a lot a lot of things that we've implemented in Flink are about um the State Management because um I think that that's also why I think we say on the website Flink is doing stateful stream processing so this ability to maintain State inside the operators is what makes Flink really interesting because if you just um without State Flink would just be a system that allows you to um move data from left to right but it it cannot really maintain or build up any knowledge about the outside world so with link you can you can basically you have like memory you know um what happened before and you can make some decisions based on what happened before like in the yeah in the case of this lookup join you know what is in the MySQL table and you know that um Flink is always making sure that you're not losing any data so even if you kill one of those task managers or multiple of these task managers Flink is able to recover this state and um restored for you and um you will never notice that there was any loss of State temporarily because the task manager has died okay this presumably is also the same mechanism you use to calculate Aggregates that are live updating right exactly so um you can store Aggregates in state yes yeah we should probably talk a little bit about this the difference here between um static data and live data because you say Flink was originally built for like batch files but you you can just to give it a really simple example I can say something like count star of users and I would expect that count to be gradually Rising as people register on my site yeah so you have so what what flinks of a live stream of data if everything a create table statement um or is all data in even the stuff that looks static is all data in Flink considered just a stream of data coming in from an entry perspective yes so even if you're reading like if you're doing a classical batch use case you have I don't know a few terabytes of data in your S3 and you want to join it and filter some data and then write it to Snowflake and um this is a batch shop this is a finite batch shop you're starting and and Flink will execute this in a streaming fashion so it will just um start reading the files in S3 stream them through the join operators and whatever other operators you have and then um load them into Snowflake and once it's done done reading the files it will shut down and the jop will be done okay this is kind of batches a special case of streaming correct yes yeah okay okay that keeps things I'm assuming that keeps things simpler but we should go deeper into how it processes like like joins how do you do streaming joins what's what's flink's notion of that um if you're if you if you know that you're processing a finite stream you can do more optimizations um than when you're processing an infinite stream so if you're doing batch processing you have a finite stream so you know that you can write all your data on disk you will be able to assuming that you have like sufficient disc space you can always assume that you're able to consume the entire data set right to dis do some operation and then continue with the processing in um in the streaming world that's not possible you want like low latency so you cannot and like data is it will never stop so your dis will just um run full at some point um so if you're doing if you're doing a batch join and you know that your data set is finite you can um be more efficient because you can load your entire data set um into memory or into on on the disk and then for example sort it on disk and then um do like a sort merge join um of the data you cannot do that in streaming because you cannot sort something that never finishes yeah you have to see streams are infinite so you can't ex possibly s yeah yeah and what we are doing in SQL is that we're using Windows if you need something to be finite so if you if you want to chunk your data into um daily batches in the Stream processor then you can do um a daily window so you um you collect data for an entire day and only after a day you um do some analysis based on the this one day worth of data and there's like many like different types of windows so you can do these what we call like tumbling window where there's where they're just discrete Windows like for every hour there's a window if you do hourly windows or you can do sliding Windows where there is like every 10 minutes you're triggering a new window and the windows are like one hour long so they're like interleaf then you can do session windows that are Dynamic and you can do any kind of custom window um based on your needs um so that's pretty flexible okay you can custom def find them I didn't know that yes yes so there um so the window API and Flink so Flink has a built-in window operator that um you can customize so I think a typical customization is that um you use a standard window let's say a sliding window but you trigger it fre more frequently so theoretically by default you trigger the window um when the hour is completed if you're doing like an hourly window but you could do something where you trigger the window every 10 minutes and then have like a final trigger after the full hour and this way you can um get results faster so you can basically show your user this is the data of the current hour it's in progress and then at some point you can tell your user this is the finished hour like this is the this is the final count aggregation or whatever I'm doing um of my data and then okay there like even more complexity to this um and this is about handling of time because um when you're building a window you can do windows based on count of your data you can also say like for every 10,000 elements I want to do some analysis but I'm not I think I don't think many users do that most of the users do windows based on time and um if you're doing that then you have to cope with the nature of time and out of ESS so um imagine your data is coming from mobile phones and your users are sometimes in the underground or in an airplane whatever they are on the countryside and their spotty reception so your events will not always like when the user is liking stuff on Instagram um your likes will not um arrive always um when the user is clicking the button the likes might arrive like five minutes later when the mobile phone has a reception again yeah and if you've got a three minute window that's a problem exactly so um I mean it depends what you're doing so Flink has a lot of um functionality for handling this exact use case so um Flink has a um concept that is called event time so you can tell the engine that in your data some field like one field is the time at which the event has happened so imagine your your Instagram user that is currently on an airplane is doing likes whenever you click the like button um you assign the timestamp of the phone of the user to this event now the user is landing two hours later and these events will arrive with a 2-hour delay but the event time is the time when the event has happened not the time time when the data is arriving in Flink yeah and then you can um if this window is still open like still available you can assign this to the right window or you have some custom logic that is handling late L late events so like events that are arriving out of order so we're saying if you had a window that was a day long then this is a non-issue but if it's an hour long you've already closed that window shipped it off and those are the results right yeah so how do you back fill that with the old data um so that's something that Flink cannot solve it gives you the means to solve it but it doesn't solve it for you in a magic way so this is something that you have to solve as the author of a Flink shop so um the window operator um has um multiple approaches um let's say the one approach is that you have like a side output like a special stream where late events are sent to and um then you do then you run some custom code that is handling these late events so imagine you're do you're inserting these um hourly Aggregates of the number of likes whatever per country or so um into uh snowflake then you could run a query in this special code that is updating the aggregate so you know that this you know that the that the count from hours ago it's already closed but you get this information that there is one last user and you just update the count by one in Snowflake so your custom code is literally just running update statistics where time slot equals yeah yeah yeah because I mean the the the problem the trade of um when when working with out of order um events or like late arrivals is always latency so you don't want so theoretically you could keep your hourly windows and this is actually what happens in Flink so um if you do hourly windows with event time in Flink then Flink will keep multiple of these windows in its local state and um because it's basically collecting data for multiple hours at the same time and um only every now and then Flink decides that um a window is um done and it it's closed and then it's doing its computation on the window data and send the result down um okay so there there comes a point where you say to the user either you have to figure out a special way to handle this or we can just drop it on the floor and say it's too late yeah okay yes so um and this is all configurable configurable by the user so you can configure how long after a window has been um closed you want to still keep it in case of late arrivals and you can also Define how Flink decides when a a window is ready to be closed because that's another set of issues um like how does if you're building hourly Windows how do you how does Flink know when the hour is complete when it's working on event time and um for that it's using a concept called water marks um like low water marks and um every Source in Flink is emitting these Watermark events into the stream and um these are special records that you cannot immediately see when you're implementing your data flow but um they allow you to track event time across your operators so um let's say in the Instagram like example with event time at some point um the the source that is um produ like that where you're consuming these events from in Flink like this like events from a Kafka topic will decide how far the time has progressed so right um it will it will basically say um based on the data that I've seen so far I think that 2m has passed now right yeah at least one event from 2 p.m. so it must be later than that exactly this is one approach so you can say um you basically just follow you follow the um highest time you've seen and that's basically your virtual clock what people typically do is that you add some lag to this so you um you say I saw an event for 2m but I'm actually assuming um I'm like I'm just reducing like half an hour from this subtracting half an hour from this so that the time this event time is always lagging a bit behind like for half an hour it's lagging behind so that you can um account for late arrivals so if you know that you're able that you're willing and able to tolerate up to half an hour for late events um then you define your watermark so that is always trailing for half an hour of the event time that you're tracking this is reminded me of meetups that say they're going to start at 7:00 but they have to start at 7:15 cuz no one shows up till 700 yes exactly like yeah everybody who who has ever hosted a party at home knows that you cannot start yeah you can't start on the top yeah and so another um approach that you can do here is for example um that you build like a histogram of the distribution of um how late events are arriving and then you say I want my watermarks to um to be covering 95% of the events or 99% of the events or whatever and you're willing to tolerate like 1% loss or you're willing to handle like 1% of late events um using some custom logic right yeah okay so that leads naturally into um if that's how you deal with user space being unexpected in its in its reliability how do you deal with the machine space being unreliable what happens if if I'm Netflix and my 250 gigabytes of state crash and die what's going on internally there yeah um so in so let's say the 250 gabt of state that Netflix is maintaining are actually one hour windows so to just stick to to the example um so you're you're so big because you're Netflix that your that your state is 250 gigabytes and um if any machine is failing Flink will um restore the state from the latest checkpoint so Flink is periodically creating um checkpoints that contain the state of all operators in your data flow craft so um let's say we we have a Kafka Source the state in the kfka source is the um names of all the um partitions and topics you subscribed to and the offset of the latest message that you've read and then you have this window operator and there it's all the data that is in the windows so it's either some aggregate per hour or it's all the data from the hour and then there is a sync let's say again Kafka sync um this might also store some state if you're using exactly one's Kafka sync and um if you're in um you configure a checkpointing interval which means um at this interal Flink is making a backup of the state um to some reliable storage outside of Flink um typically these days it's something like S3 or whatever your cloud provider is offering um like a cheap durable storage um where you're uploading your checkpoints to so um in our example we could configure something like checkpoint every 5 minutes so we will um every 5 minutes upload the state um of all the operators in particular these 250 gabt of data um from the window operator um to S3 and um yeah okay does that mean it's snapshotting 250 GB every 5 minutes or is it incremental is it clever than that yeah it's clever so it supports increment it it supports incremental checkpoints um that only contain the diff from the last checkpoint okay and if that's so what happens when that goes wrong if I've got one of your 30 operator pipelines may be running across 60 machines and one of them in the middle dies I'm just trying to think how you coordinate all those different snapshots so that you get everything and don't process something twice um so um you will reprocess data twice in case of a failure but you won't count data twice you won't have duplicates and this is the reason why Flink is considered or we call it exactly once stream processing framework so it is exactly once with respect to State it's not yeah let's say it's exactly once um with respect to state but it might still process data multiple times so um I if a single operator in your Flink pipeline fails it will reset the entire pipeline to the last successful checkpoint um there is an optimization it's called local recovery in Flink that um the only on the failed machine you have to redownload the state on the other machines you can just reuse the files that are still on your local file system so the checkpointing files are available on your local disk so the recovery is not as painful as it sounds like you don't need to redownload 250 gigs I don't know you're downloading like 15 gigs or whatever of that failed machine um and then you can continue processing so the the trick how we achieve exactly one semantics is the way we create these checkpoints so um the we are creating these checkpoints um using a distributed snapshotting mechanism so we are not just stupidly uploading the current whatever data we have on each operator and this message like when we uploading might be off right so you might have some messages that are in between and processed multiple times or whatever we can guarantee that this checkpoint is consistent um across the data so how this works is that we are um inserting um a special event into the stream that is called the checkpoint barrier so if you have the kfka source window operator and sync then it Triggers on all the source instances via an RPC message it just says now do a checkpoint so the source will um upload its date to S3 and then it will um so the The Source will get this message it will stop processing it will up upload its state then it will emit a special record Downstream that says here's the checkpoint and then it will continue processing so then in the in the output of the source like you consider like imagine the Q um and like there's regular events then there is a checkpoint event and then there's regular events again and this checkpoint event will travel Downstream to the join operator now the join operator um will have multiple inputs right you have you have this as a distributed um graph and there's multiple sources and the joint operator will have to wait so it will receive a um a checkpoint barrier on one on one of the input channels and then it will stop accepting new data from this input Channel and it waits until it has received the checkpoint barrier from all the input channels this is called um the alignment phase of a checkpoint and once it has received the barrier from all the inputs it will create again a copy of the state of this operator in this case it will upload 250 Gaby of state and then the barrier is traveling through the system Downstream to the next operator which will repeat this until we reach the sync and once we've backed up the state of the sync um the checkpoint is conceptually closed like we have created a consistent copy of all our state at this particular point in time and records are not allowed to overtake these checkpoint barriers and because of that we even if we if we so if we have a failure now we reset the Kafka sources to their offset that was at was there at the time of the checkpoint and the contents of the window operator also to exactly the same point in time so we will reset the state and the data and this is in Line This is How we can guarantee that we are not counting twice so if you if your window operator is counting the number of Records we reset the counter to the right count at the time of this particular offset of the Kafka sources so we will replay the data from Kafka and the counter will go back a little bit and then we will recount and continue processing after we we we recovered from the failure right so if I'm further down stream in the operator pipeline I'm going to get hey it's time to do checkpoint number three and I do checkpoint number three and I do checkpoint number three and I get some more rows and then presumably there's another message that says hey I crashed we're all going back to checkpoint three and I'm going to resend you rows from that point yes yeah yeah okay yeah okay that I can see how that makes the whole system consistent with respect to checkpoints and there is um so these checkpoints are done asynchronously so um these 250 gabt of State in our window are of course our problem child that we need to give special treatment and what Flink is doing for the special treatment is that it will um when you're using actually it's in the royb on disk State back end and also in the memory based St State back end it will create only a let's say snapshot of the data like a few of the the data at that point in time and it will continue with the processing and upload the data to S3 in the background so you you can actually the barrier can actually flow through the system quite quickly it is only necessary for making sure that we creating these State snapshots at the right point in time and then in the background um there will be upload processes um sending the files to S3 and only when the barrier has re reached all the syns we need to wait that also all the synchronous uploads that are happening are also finished and only then the checkpoint is considered complete and then um we can use it for recovery so if we are failing at any point in time we just always go back to the last successfully completed checkpoint so at that point you could nuke the cluster reboot it and expect to get back to where you were exactly yes okay and um so this is the mechanism that Flink is using internally for um recovery but there's also user managed checkpoints and they are called safe points so you can also as a user say um you basically do like uh use the CLI or the rest API or whatever to trigger a save point so you can also as a user say hey create me a copy of my state right now and write it um to this directory and um then you can basically trigger a safe Point like you're triggering a checkpoint out of band so you're you're creating a special um copy of your state and store it somewhere and this allows you to go back in time so if you um if you create these save points like once a day and you realize that there was a bug in my code then you can go back and say okay I'm replaying the the um save point from three days ago that fixes the bug and then with event time you will actually get exactly the same result even like you it's like stream processing it's supposed to be real time but you can use it also for historical reprocessing or backfill yeah go back to that point in time so presumably this gets used a lot when people are just before people are deploying on a Friday afternoon yeah for this um exactly for this it's useful it's also useful if you want to migrate your state um from one cluster to another so imagine um so I don't know yeah you just need to change your infrastructure for whatever reason you can create a safe point and restore the safe point on an entirely different Hardware or if you want to change the Flink version like there's a new Buck fix release um you create a save point um and then you restore um with the fixed Flink version and you can even use that to fix your job so if you're if you if you have a buck in your code and you want to fix it you can um restore with a different safe Point okay so you you do the whole thing along with um a job is job the right terminology actually in FL yeah we try to um change the name to application like stream application but throughout the code and the documentation and in people's mind so it's both job or application right okay okay so we're getting into what the developer experience is like for this then maintaining it but we've mostly been talking about as though this is a SQL only world yeah yeah it's not tell me tell me tell me about the hierarchy of ways I can interact with Flink MH um so there is um two big apis the SQL API and the data stream Java Scala cotlin whatever jvm API um and the Common Ground I would say is the data stream API that um that allows you to do highle and lowlevel operations so you can um Define your sources in Java so you just Define like a Kafka source and aium source and a Pulsa source and then you define like a window operator and then um you sync in Java and you can like run your own code of course in like the maps and filters and so on um you can go one level down from that and use um what we call the process function in Flink and the process function allows you to access State directly so you can say I have a list state or a map state or value state so that's basically like a Java field in your in your code but this field is backed by the state back end so you can store terabytes of data in this field even if your machine only has like 16 gigs of RAM or so because the the data that you put into this list for example is actually written into roxb and roxb means it's stored in a special file format uh on your local SSD ideally um and if you're putting this into this um State abstraction in the Java API Flink will also checkpoint your state so it will be recovered and it will be exactly once like the mechanism that I've described so the high level window operator that is available in the data stream API um is also using these State Primitives and you can access these State PR atives in what we call the process function in Flink and um another benefit of this process function like this low-level process function is that you can um access also time so you can access the event Time Field of every record like the current Tim stamp and you can register um timer call backs so you can say um wake me up in 30 30 minutes and then I want to close my window or do some kind of whatever so you can Define callbacks and then when the time has arrived for this callback um you get a special method call and you can do stuff like closing your window for example and this this timer is either based on what we call processing time like the wall clock real world time or it's based on event time so even if you're saying 30 second whatever 30 minutes for this timer it might the system decides when to execute um this timer okay so I could do something like um I'm thinking if I modeled an auction I could accept bids in but when I get a message saying the auction has now closed then all future bids become irrelevant the window closes at that point something like that and this you this like let's say custom bunding logic you don't have you can implement this yourself with the process function you have all the ingredients you need you have access to the time of the bits and you have um the state like list state to store your bits in Flink state so is this what people generally do that they write SQL until they hit something the SQL API doesn't support and then they break out Java or Scala um it depends I mean um so let me let me finish a thought about the API so data stream is like the common API process functions the lowlevel API and SQL is of course the high level API and you can use the SQL API within the Java program as well so you can basically say you create your sources in Java and then you just put a SQL string um to do whatever you want and then you continue in Java like you can also mix and match um the different apis or you can go full SQL like you just use the SQL shell and you create your data sources and um queries and whatever everything in SQL you don't have to touch Java at all um so now there is also extension points in SQL like you can do user defined functions so besides the buildin functions like conat and whatever average you can also do um you can Define your own functions and then you implement them in Java or you can even use Java to delegate to python or whatever language you want um so you can extend Flink SQL so um what what I see as a pattern for companies building stream processing platforms is that offer let's say business specific um userdefined functions as part of their streaming platform okay so they there's a department responsible for shipping a library of things they'd find everyone would find useful in SQL that kind of thing yeah yeah okay okay where do people tend to start with Flink I mean what's a typical way it gets introduced to a company um so I think that I mean it depends a bit on um on your skill set like whether you're Java developer or whether you're um a SQL Developer and or whether you're a lazy Java developer who knows that you can sof a problem in SQL that that's totally fine and makes a ton of sense like why and honestly I mean SQL is highly optimized of course right so there's like an Optimizer that knows how to um to do an efficient um data flow and it uses optimizes optimized data types and it uses code generation and all kinds of tricks to make this really efficient so you would would spend a lot of time expressing what you can express in like 15 minutes of trial and error with a SQL you would probably spend like a day or so in Java to make this really efficient so it makes a ton of sense for many use cases to use SQL and you can totally use Flink um without knowing Java you just Define your um data sources and things and write your queries and you can do quite a lot with Flink SQL so one way of getting started with Flink if you just want to do SQL is um go to the Flink documentation and check out how to use the um SQL client like you start Flink locally like you just run a b script that um brings up a job manager and a task manager instance and then you start your uh SQL client it connects to a job manager and then you can write your queries um with it if you're Java developer um there's a different path that I would recommend and that is um you create an a Java project you add the Flink dependencies to your project and then um in your main method um you define like the this execution environment of Flink and um trigger the execution there in your local IDE locally so it will bring up the same components that you're running on a big 100 machine distributed cluster in your local um machine so you can really run exactly the same code that you're running on the big big cluster also locally it will bring up a job manager thread and it will bring up a task manager thread basically in your machine okay and then so it's not just running like a local client side part of Flink it's running the whole of Flink as you wish so you can of course use your developer machine to connect to a fling cluster and submit your job to a big fling cluster but for development it often and I mean if you're just getting started you're not that sophisticated you just go and uh press the Run button in your IDE and it will run everything for you and you can even bring up you can um bring up the Flink web UI to investigate what's going on you can configure checkpointing you can configure the Roxy stateb and like all of that stuff also runs locally and if you're Java developer that's my recommended way of getting started it feels like developing against any other Library you just um play with a code and uh add like lock statements and put break points for your debugger and it's a pretty neat development experience and you can even do like once you've um once you've done once once you're done with the implementation um of your fling job you basically create like a jar out of it and then you upload it to your fling cluster where it gets executed and um you can even do things like profiling on your local machine so if you notice that there's some performance issue potentially on your cluster you can run it locally again and attach a profiler to see where you're spending most your CPU cycles and another another benefit of this um it's again Advanced use but you can also use this local execution for um testing for integration tests for example so if you want to um make sure that you're not breaking your production code you can um run Flink mini cluster in your um unit tests to make sure that um you're not breaking anything okay and can I stub out my my SQL database for just a local static file Source yes exactly yes so um in the Java code um basically every like there's a abstraction called the data stream and a source is producing a data stream so you can just put this behind a method and then the method is producing or creating a stream um of whatever you want so you can just Implement like a simple data generator in Java that is producing data that looks like the real data and on production you replace it with a Kafka Source um for SQL there is um a data generator available as a source um so you can specify like basically the schema and the KE from the schema the data generator deducts how to generate data for you oh sweet yeah so well I mean you basically Define like an integer field and you say I want to generate positive numbers or something right yeah gy um I have to ask for this is it any language I like as long as it's the jvm uh yes plus python oh okay there's a python yes so there's um a python um API as well for Flink that is fairly close to the SQL API or like SQL abstraction um so like between um fling SQL and the dat API is something called a table API and that's a Java way of saying SQL basically so you can do like uh select where whatever in Java um and I think that the python API is similar to that and of course you can run python code so you can also do a um transformation with regular python code on aggregation uh as regular python code in your user defined function and is it feature parity or does it like behind the Java version or has it or has it diverged yeah I would say it has diverged yes okay so I think it I'm not 100% sure I have little experience with it but um it is um fairly close to the SQL API like the table API okay okay I need to go and check it out then on which note I just head to Flink dorg right uh Flink apache.org so it's part of the a purchase of a foundation that saves me asking you what the licensing terms are exactly Robert thanks very much for taking me through all that and I will see you again cheers yeah I really enjoyed the conversation thank you Robert thanks again I have to say Robert's inspired me since we recorded that conversation I've started using Flink in anger on some analytics stuff that I've been working on maybe at some point I'll show you maybe we'll do some coding videos for YouTube eventually something I'd like to do time allowing another thing I'd like to do and I think this time will allow is a deep dive on more Apache projects we've covered a few of them already on developer voices long term I think we'd like to cover a lot more cuz there are lots of interesting ones so if there's a particular Apache project you think I should be looking at please let me know in the comments on the topic of comments if you've enjoyed this episode or found it useful please take a moment to click like or share or rate or review whichever feedback is kind of buttons the platform you're listening to this on offers developer voices is available on YouTube Spotify on Apple all the usual podcasting places they all have different buttons please go and find one I am looking for your feedback on all of them in fact I might have some of that feedback data traveling through Flink just between you and me so why not give me some more datums to run through on which note I think it's time for me to leave you and for both of us to get back to the keyboard I've been your host Chris Jenkins this has been developer voices with Robert meter thank you for [Music] listening