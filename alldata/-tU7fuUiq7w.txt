type annotations type hints help make your code easier to read and it creates a smoother experience while you're coding but what if you're doing data analysis and you're using panels can you use type annotations then well no but there's nothing else you can do which I'll show you today code that doesn't have any type annotations is actually quite hard to review if you want to learn how to effectively review code I have a free code diagnosis workshop for you you can enroll by going to ion.com diagnosis it's based on a three-factor framework and I apply that framework to actual production code so you can see how it works so go to rm.codes diagnosis to get access for free to the workshop the link is also in the description now let's dive in if you've worked with pandas before you know that there are two important data structures that you should know about the data frame and the series so the data frame is basically a table with rows and columns and a series is a single Row in the table it's basically a pair of key values and both of these data frame and series you can actually use as type annotation so here I have a very simple example where I'm reading a data set it's basically a set of products and here I have a function that loads the Cs feed data provided with a path that comes from pathlib and it returns a data frame so I'm using data frame here as a type annotation the problem is that data frame itself and the series that are the rows in the data frame are each quite complex data structures so you might actually want to specify somehow what type of series are in the data frame and what are the exact types of the different columns in each series ideally maybe you want to do something like this where we say like okay we have the data frame but actually there is a string column and there is an integer column and there is a float column etc etc unfortunately pandas doesn't allow us to do this and it gets worse because we might want to do even more than that for example maybe you want to specify limits on numerical values suppose that this is a quantity so we want to make sure that the quantity is always positive or suppose that this is a percentage so we want to make sure that the value was always between zero or one or perhaps we want to indicate that some of these values are optional that are not required in the particular entry or we might do something even more complicated like that this string is perhaps actually an email address and we want that string to be a valid email address for each of the rows in our database you could even imagine that there are dependencies for example if we have one field in series that is supposed to be the country well if that's an EU country we've done once another field to be a valid EU tax ID number or something like that so there's potentially a lot of complicated things that we would like to do that go Way Beyond using type annotation so that means that it's in principle not even that interesting to want to use type hints type annotations as helpful tools with Thanos data frames and series we actually want something more which is validation and there's a really nice package you can use with fan loss that does validation for you and that's called Bandera and I'm going to show you today a couple of things you can do with it so let me remove this so how does bandera actually work well the important aspect of Pandera is that you can define a schema that defines basically what your data is supposed to look like and then you can use Pandera to validate your data according to the schema and it has lots of different options lots of different ways to define the schema you can even infer the schema automatically from the data as I'll show you in a minute and you can even use a tool like pedantic in combination with Pandera so that you actually specify the schema using a pedantic model and then you can actually rely a bit more on type hints which is really cool I've done a video recently where I compared pedantic and data class and authors and they're one of the things that pedantic is really good at is validating data and that's compatible with Pandora so that's really nice if you want to learn more about how to validate data using by Danzig check the link at the top and by the way if you're enjoying this video so far please validate it by giving it a like so that YouTube can suggest this content to others as well so next to being compatible with pedantic which I think is really a way to go for validation but there Bandera also has a couple of data types that that already built in uh things like booleans ins strings Etc next to that it also has some data types like N64 that are directly compatible with canvas because it's supposed to be a pandas extension and what's actually also really cool is that Pandera integrates with fast API so the combination of having pandas fast API pedantic and panderas really really nice and finally Pandera has other Integrations as well for example with hypothesis which allows it to generate synthetic data automatically can be useful sometimes so let's take a look at a couple of examples of how you can use Pandera to do validation of your data so in this example I have no validation whatsoever right but we would like to have a way to add validation to the data frame time so one thing that's nice about pondero is that it can actually look at existing data and then try to infer schema from that so I'm going to start with that I think that's the easiest so here I have an example of almost the same code so I'm retrieving the products from a CSV file and then ponderos which I've added as an import here then has a function called infer schema and it gets a data frame as input and then it creates a data frame schema and what you can then do is you can write this schema to a python file so that later on you have easy access to it so when I run this this is going to create a file called invert schema and you also see there are some errors I'll show you what that means in a minute so the inferred schema that's basically this file so this is what it has generated so it's it at the import already and then created something of type data frame schema and you see that detected a number of columns like the invoice number the stock code description there's a couple of other things customer ID you see it also automatically added some checks and not all of these checks make a lot of sense by the way so what you can basically do is once Bandera has inferred the schema from your data you can then go through each of the columns and then check whether it actually makes sense so what I did here is I have an adapted version of that schema where I changed a couple of things for example I change the invoice number to a string or I removed a check from the quantity because I only care that it's greater than or equal to one so what you can also see is that Bandera validation allows you to do a lot more than just checking the type because we also have these kinds of checks that we can add to the schema and then what you can do is you can use the schema that you inferred or in this case I'm I'm using this altered schema and then the schema has a validate method which takes in this case a data frame of products this is the one that we read from the file here and then what I did here as I specified that validation should be lazy so don't want this to stop validation as soon as it finds the first error I wanted to give a summary of the errors so you see that it produces a sort of table containing errors well it's not formatted nicely because there's like a bunch of different values here but typically this gives you an idea of the types of arrows that there are in your data set so once you have the schema so as a first step you infer the schema and then you adapt it to what you need then you could actually very easily check new data sets with the schema that you have by using a decorator so here I have an example that uses the Bandera check output decorator and we Supply it with schema so that's again loaded from the file that we created and also here you can write that it should be lazy and now when I call this function it's going to check the data automatically using the schema that I provided so winner on this you see we get the same types of Errors so it provides again the same summary but now we use a decorator to check the output so this all works fine but I do think a schema like this is a bit cumbersome to use and what I would actually suggest that you use instead is pedantic models and use that for validation instead how does that actually work I have an example right here so instead of having this very complicated data frame schema that we had before what we can do now is we use the schema model which is very similar to how it works in fidentic and then create a new class in this case I'm calling this outputs schema and then I'm simply supplying all of the schema elements as instance variables so this is a much shorter representation of the schema and since this is all built on pedantic you have exactly the same validation capabilities at pedantic has so for example here I have a field quantity for example I can do something like PA Dot Fields and then I can specify that I want this to be greater or equal to one so now I'm sure that quantity is never going to be zero or less so I find this much cleaner way of defining validations and by the answer it has lots of other options as well that you can use and that's directly compatible with Pandera and if you use the combination of Bandera pandas and pedantic what's really nice that you now have some extra capabilities in terms of typings so I have here again my retrieve retail products function that gets above but you see it now returns a data frame and I can specify the schema that the data frame is supposed to follow and that's because I'm importing the data frame type from pandera.typing and now what I can do is I can again add a decorator here that's going to check the types for me and that's going to use this type annotation to determine what schema it should use to check the types so I think this is a really clean way of using schemas and verifying that the data follows a particular structure and then what I do in the main function is that I have a tricep block so I retrieve the products just like I did in the other scripts and I simply print some information and I catch schema errors here and then I print out the error so when I run this then again we're going to get a very similar kind of summary that's going to give us exactly the same errors as we had before but now I'm relying more closely on type hands financing schemas to do the validation for me and like I said this fully supports pedantic validation so this is actually really powerful so I hope this video gave you a basic idea of how to add validation to your data processing pipelines using canvas let me know in the comments whether you've used validation like this before or if you'd like me to dive in deeper and you also want me to cover for example how to integrate this with fast API just let me know and I'll do video about that now checking that your data is nice valid clean is of course really important but especially if you're processing big sets of data then it's also really important that you know a bit about how data is actually represented in Timeless and what you can do to make that a bit more performant so if you want to learn more about that check out this video next thanks for watching and take care