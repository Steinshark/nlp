well thank you for having me i'm really glad to be here i'm really glad uh to still have the opportunity to present at conferences despite this pandemic i hope uh everybody's doing well uh for those who don't know me as being said i'm bryce edelson-malbec i work at nvidia um where i am the hpc programming models architect so i work on various different programming languages like c plus plus and fortran and also various different runtime systems or parallel program paradigms things like open acc openmp um stuff like that and the one that i've been particularly heavily involved in and heavily involved in is uh parallelism and standard c plus plus i am the chair of the c plus committee's uh library evolution group which is responsible for the design of the cpos plus standard library and i'm also the chair of the us standards committee for programming languages so today i'm going to talk about a project my team's been working on for a couple years now called the cuda c plus plus standard library or lib coup plus plus um but before we talk about that project let me first go over what cuda c plus plus is um some people who are watching this may not be familiar but don't worry even if you've never used cuda c plus plus or you don't particularly care to i think this will still be an interesting talk to you so um one of my colleagues wrote this uh a great quote which i think really captures the the essence of what cuda c plus plus is um cuda c plus plus is an extension of c plus plus that uh nvidia um uses as the uh the primary language to program our gpus in or to write heterogeneous programs where you have code that runs on the cpu and then some of that code also runs on your gpu so we like to think of cuda c plus as being a super set of standard c plus in your host code you can do everything that you can in standard c plus plus um and in fact today you can use uh whichever host compiler you want so you can write your host code um you know with gcc um but also in that same program you can have code that can run on the device um and there is uh not everything not all types of code will run on the device today although we're we're working to minimize that gap and there are some uh restrictions on what things you can pass between host and device code for example if you have a function that is a heterogeneous function so function that you intended to compile for both the host processor and the device processor you can't pass a function pointer to that function between the host processor and the device processor because the addresses of those compiled functions are different for the host version in the device version so there's some restrictions like that that's worth knowing about but for the most part you can just write regular c plus plus and have it run on your gpa so if we look at what makes c plus plus what it is if you look at the standard itself about i think 700 of the 2200 pages cover the core language and the rest of the standard covers the standard library um the standard library is uh an equally important component of the language as the core uh language itself uh c plus plus without a standard library is a completely different language a very severely diminished language so when we look at cuda c plus plus we've had a core language um but until recently we have not had an equivalent of the standard library you have not been able to use your host's um standard library in uh cuda code that's been one of the restrictions um and that has left the experience a little bit lacking until now um until the arrival of the cuda c plus plus standard library which we call libku plus plus um it is an open source project it's uh a fork of well it's a derivative of i don't say a fork suggests that perhaps it diverges from upstream but it doesn't um it's a fork of uh libcu plus plus of libc plus the llvm uh standard library um and it has three sort of key properties that i want to talk about which is that it's opt-in it's heterogeneous and it's incremental so it's opt-in which means that it does not interfere with or replace your host standard library when you use it with nvccr um primary compiler so uh if you include you know some header like uh type traits and you use stuff in stood colon colon that's the stuff that you that's from your host standard library and it's strictly conforming to standard c plus but you can only use those facilities in host code if you include the same header in cuda stood you'll get the version that will uh work in both host code and device code and it'll be in the name space cuda colon colon stood colon colon um and then uh there's a third name space which is just the raw cuda colon colon namespace and in that namespace we have some direct extensions of some things that are in the standard that are conforming extensions to various uh standard facilities so for example you know for atomic if you just use stood atomic that's your host atomic you can just use that um only in host code cuda stood atomic you could use in both host code or in device code and then cuda atomic itself is a extended form that has this additional template parameter but otherwise operates just like a cuda atomic and we have a few other extensions of the sort of a similar nature to that so libku plus plus is also heterogeneous um and our intent there is for essentially all things in libku plus plus to not only work in host code and not only work in device code but also to work across host and device code so that you can move them between host and device code so that both host threads and device threads can access them at the same time so we try to make the guarantee that all copyable and movable objects can migrate between host and device code uh that host and device can call all member functions um and that the host and device can concurrently use synchronization primitives although there's some uh caveats around that and libku plus plus is also incremental which is uh just a a fancy way of me saying that it doesn't have everything that's in the standard library today um each release we add a bunch of new stuff our focus is on uh two different categories of things first facilities that need a specialized cuda implementation so things like concurrency primitives clocks syscalls io etc and the second category is essential facilities that aren't that hard to implement in device code but um they're so commonly used that everybody ends up spending time re-implementing them so it's it's high value for us to provide a version so that people can stop doing that so uh in uh lib coup plus plus today we primarily have a collection of concurrency facilities and we're looking at expanding that in the next few releases and then we'll probably move on to some other things like string processing and some more um some of the simpler container types like array um and io is also on our like longer term road map people want stood c out to just work on the gpu um so today i'm going to focus on uh sort of the marquee feature of libq plus plus which uh is atomics that was the the first major feature that we had and it's one of the most uh important features in libcute plus plus because it directly exposes um some of the uh exciting capabilities of our hardware so this is what uh uh the atomics api in lib coup plus plus looks like so we uh we have two versions of atomic and lip coop plus plus um the first is uh cuda colon colon atomic which is this um extended version of the standard atomic so it uh it takes two template parameters the standard one of just what the element type is and then it takes this uh scope parameter this thread scope parameter and a thread scope specifies which threads can safely use an atomic object at the same time so thread scope system means all threads that is the equivalent of the semantics of stood atomic stood atomic doesn't have any restrictions on what kinds of threads can use it thread scope device means that all threads on the current processor can use it so a thread scope device atomic and device code can only be used by other threads on that device a thread scope at tom thread scope device and host code can only be used by uh cpu threats and then thread scope block is sort of specific to cuda there's this notion of thread blocks in the cuda programming model and an atomic with a thread scope block is can only be used by threads within that uh thread block and so then we also have um our cuda stood atomic which is an implementation of the standard and strictly conforms to iso iec international standard 14882 um and it's equivalent to a cuda atomic with a thread scope system um it's actually just like a type alias in our code base so next i want to look at some examples of how you would write atomic code in cuda before libku plus plus and while these examples are going to be specific to cuda the lessons um are applicable to many of the other pre-c plus plus 11 non-standard atomics uh apis like gcc atomics or atomics on uh windows and we'll we'll see through this uh exercise why c plus plus 11 atomics are a much better abstraction and a much less error prone abstraction so uh before libercu plus plus you might try to write a uh cuda signal flag function like this where you use volatile variables to signal uh that a flag is set atomically uh so you just you know this function just takes enough this flag parameter and then it just does a volatile right to it this isn't actually uh sufficient though because volatile does not actually guarantee atomic semantics it doesn't um make the guarantees about memory ordering uh that uh you might uh incorrectly uh assume that it does um volatile is not safe for inter thread communication that's not what its intended purpose is and it might happen to work on some systems like this happened to work in legacy cuda um but that doesn't mean that it's a good idea because it means that code um is probably not going to be super portable not only to other platforms but it might not be portable to future iterations of the system that no longer make uh make it just magically happen to work because that those semantics aren't guaranteed you can't really rely on them you're relying on undefined behavior here and that's never a good idea in general it's in particular a dangerous idea when you're dealing with concurrent code so again volatile is not equal to atomic so um to make this code a little bit more correct we need to put in uh some sort of fence operation uh if we put in something like a uh a thread fence system here um then this code uh is a little bit closer to being uh notionally correct um but it's a little bit unfortunate that in a lot of these legacy atomic apis like legacy cuda atomics or gcc atomics the fence operations um had to be explicit the fence operations were not fused with the operation that you want to defense so like the operation that we want to fence here is the assignment of one to that flag um and what we would really like is an api where that assignment operation itself will take care of doing the fence so what we really want here is an atomic with store release semantics not a volatile store now unfortunately uh cuda does have in its legacy atomics you know some atomic functions it's not all just volatiles but there's no atomic store function and no atomic load function and this is a pretty common pattern uh across um uh these legacy apis where they don't have an explicit atomic store and atomic load um it it those were emitted because of this reliance on you know explicit fencing and some on some platforms on uh volatile um providing the certain semantic guarantees but not portably of course so um we have no atomic store there's some other similar operations that we could use to sort of uh uh mimic its effect like we could do atomic exchange and just ignore the result um but that's not really like what we intended to do so it's a little bit clutchy here we still can't get rid of the fence that we had to put in though because um all of the cuda legacy atomics uh have relaxed semantics and again this is very common across um these pre-c plus plus 11 atomic interfaces so as i said we wanted store release semantics which means we still need to keep this fence uh in here so here's what this code looks like if you're using c plus 11 atomics like libku plus plus we have an atomic bool flag not a volatile int um and uh we just write that we just assigned true to the flag and that's all and this takes care of the fence it does everything that we want it to and one of the subtle improvements here is that we were able to write atomic bool instead of using ants for a flag bull was probably our intent the reason you often see a lot of ins used instead of a more appropriate type in uh older pre-c plus 11 atomic interfaces is that a lot of those interfaces only supported one or two sizes typically they'd support you know 32-bit operations and 64-bit operations um now uh your platform might not natively have one byte atomic operations but atomic bool will still work which is nice so on the previous slide we just assigned true to the atomic and that works but that will use the strongest memory ordering in the c plus model which is sequential consistency and that's actually a bit stronger than what we need now it's a good default and it is the default for all of the overloaded operators like the assignment operator um uh but here if we want to be a little bit more efficient um we might want to instead of using the assignment operator called the store member function which takes a um memory order parameter and we can just say hey i want to do this store with memory order release semantics now there's an even better form that we can write uh with c plus 20's new atomic weight and notify api um uh this is available in libcu plus plus and we've back ported it um to uh c plus plus eleven uh and we also contributed the patch upstream to live c plus plus um for this facility so after we do our uh store release uh to the flag um we'll call this um notify all api on the flag to wake up anybody that was waiting for the value of the atomic to change and when we look at the consumption side we'll see where this comes into where this factors in all right so now let's look at the consumer side of this example so a function that pulls on a flag and then reads some int data um that the producer wrote uh before it signaled so we have the same issues with this uh example uh that we had with volatile in the previous example that it it does not actually give us um atomic semantics it might happen to work on some platforms but it's not portable and and it's all just undefined behavior so in the code here we've just got a a while loop where we're just pulling until that flag is no is uh uh equal to one and then we just exit out of the loop and then we uh read the data um and uh that loop is a little bit um sub-optimal because it's really just a naive spin loop under contention it's gonna perform pretty badly you know imagine if you have tens or hundreds or thousands of threads all of which are every cycle trying to um read this one flag waiting for it to be set from zero to one um well they're all going to be um pounding uh the cash and memory uh trying to access that one location uh and and the read that we're doing here is a volatile read so it's a it's a fairly expensive read we don't want or need to read the data either atomically or volatile with volatile loads the read of the flag should ensure that we can safely and non-atomically read the data because reading the flag should establish a memory ordering that once we read that flag um we we we should have established a uh the lease acquire memory ordering where we know hey because we've seen this flag right we know that any rights that happened before the flag was written are now visible to us so on the on the producer side the producer would first write to data and then after it's written to data it would write to the flag with those store release semantics and it would have the guarantee that any thread that sees the flag has been set to one will also see the right of the data so there's no need for us to actually read the data as volatile but if we just remove the volatile read um we're going to have a bug in the code because um this uh volatile read of the flag that we have doesn't give us that memory ordering because again volatile does not give us atomic semantics so we haven't established the memory ordering on this side of the operation yet so we could do that in legacy apis by explicitly adding a fence but very often you'll end up forgetting to do that again it would be very nice if that fence operation which we need for correctness was fused onto the reed operation the operation where we read the flag in the while loop so instead of doing a volatile load of the flag what we should really be doing is an atomic load acquire the cuda legacy atomics api doesn't have an atomic load just like it didn't have an atomic store um but there's like a little hack that you can do which is you could use an atomic add where you add zero um but like that's a little bit unpleasant and it just makes for terrible code clarity um now even if we switch to the this atomics api and get rid of the volatile we still need um the fence here and the reason that we still need that fence is because all of the legacy atomic functions in cuda like atomic add they they just give you a relaxed memory ordering the weakest memory ordering and what we need here is an acquire ordering we need that promise that once we read that flag as one that we know that the data right that happened from the producer that we'll actually see it so with uh c plus plus 11 atomics this code becomes much cleaner and more correct um we changed that flag from being an ant to be an atomic pool um and uh that makes this very nice and clean we've still however got this uh spin loop uh that's gonna be bad under contention and that's where this new c plus plus 20 weight notify api comes in um but first uh we don't actually need the sequential consistency here um a load acquire is sufficient and it's going to be more performant now in that previous slide uh we were just relying on the atomic's implicit conversion to t which loads the value using sequential consistency uh semantics now if we want to specify um uh weaker semantics we need to use the load api and say we want to load with memory order require all right so now let's deal with that spin loop so instead of using this spin loop we can use this new c plus plus 20 weight notify api um and what this weight api does is it says hey i would like to wait until the this value changes from what it what i believe it currently is to something else so it wait says i tell me like exit this weight once this value is no longer equal to the value i've just given you in this case false so this weight false memory order require says i want you to read this thing with memory order acquire semantics and when it's no longer false i want you to return so the particular waiting mechanism that's going to be used is left up to the implementation on a cpu you might use an operating system primitive for efficient weighting such as a futex which is not just going to be in a spin loop constantly uh hitting that variable it's gonna use this um more clever and efficient uh and fair operating system mechanism that will scale well under contention uh in our uh gpu side implementation we use a back off mechanism that will um sleep the waiting thread for short durations after every failed read so it'll go and read the value it'll check if it's changed from what it was supposed to be and uh if it uh hasn't changed then it'll just tell us the thread hey just go to sleep for a little while just go and like wait and then come back later it's sort of similar to if you're um if you're at a theme park and there's a really long uh line for a ride you might just say you know what i'm just gonna go do something else and i'll come back to this line later and maybe it'll be uh shorter than and then i won't you know contribute to that line being really long and i won't have to wait in that line uh forever i can go do some other thing so see those bus atomics also give you a greater deal of type safety than you'd get with older atomic apis that did not denote atomicity in the type system for example if you mix the thread scopes of your atomics lib coup plus plus will catch that at compile time and will give you an error so if you try to call a function that expects an atomic with a thread scope system try to call that with an atomic that has a thread scope device that will give you a compile error whereas in the legacy cuda atomics um uh there is no such type information uh and so that bug would just be a silent uh possible data race at run time uh the legacy cuda atomics also made it somewhat challenging to write code that was not just device code but but also host code that was truly heterogeneous code that could run everywhere because all of the legacy cuda atomics only work in device code um so you'd have to go and write your function one way using the cuda legacy atomics and then you have to write it another way um for you know your host platform um whereas with libcube plus plus um all the atomics work in both host and device code so you can just write your code in one way so the moral of the story here is uh you should stop using legacy cuda atomics and you should stop using any uh legacy atomic api any atomic api it's not c plus 11 atomics or c 11 atomics um in a lot of these legacy apis you don't have a way to express sequential consistency and acquire release as sort of uh top-level citizens you have to build them yourself with fences the legacy cudatomics as you mentioned they're device only the memory scope is a property of operations not objects which gives you a lot of potential for um errors and atomicity itself is a property of operations not objects again which leaves you a lot of room for making mistakes accidentally passing something that is not actually intended to be accessed atomically into an api that's going to try to access it atomically and also please please stop using volatile uh to attempt to synchronize between threads because it does not work that's not what it's for volatile is not uh mean atomic uh on some platforms volatile had been a vague pact but that's not something that you can rely on it's not something that's portable it's not something that the standard guarantees you should instead use c plus plus atomics which have clear semantics okay so now i want to talk about why all this matters um you know we our team spent a lot of work um exposing uh c plus plus uh atomics and when i say our team here i don't just mean our software team i mean our hardware team uh spent a lot of effort on this too um we we spent a lot of transistors making sure that our modern gpus um conform to c plus plus parallel forward progress guarantees and to the c plus memory model it was a lot of work it was like a 10 year project so why did we do this why is that this so important why does it matter well when you have these two things when you have c plus plus parallel forward progress guarantees and conforming implementation the c plus plus memory model um that enables you to write a much wider range of concurrent algorithms and data structures on gpus particularly blocking algorithms so this is sort of a taxonomy of the different uh types of concurrent algorithms and you don't have to necessarily understand all the terms that are on this chart the key thing to understand is this on platforms that only ensure that threads make weak parallel forward progress that would be pretty much all gpus that uh are not made by nvidia and all nvidia gpus before the vaulted generation they only provided weekly parallel forward progress guarantees um and such platf so the this this greatly restricts the type of concurrent code that you can write um uh for example the simple signaling and polling functions that we looked at before you can't actually write those on these platforms because they use blocking on the uh on the uh poll then read side what were we doing we were in that spin loop well that's a form of blocking so the the ways that you have to write concurrent code on these platforms that don't provide parallel forward progress guarantees it's it's very restricted now with uh volta and newer nvidia gpus because you have these parallel forward progress guarantees and c plus atomics uh you can write any type of concurrent algorithm or data structure on these on on our platforms and that means that you can run more types of applications and more types of code on gpus and we think that's really important so now i want to look at a slightly more advanced uh example which is going to be building a concurrent gpu hash table with atomics so we're going to use open addressing and linear probing both for simplicity and uh performance and uh this concurrent hash table it's sort of it's it's not really a hash map um it's sort of like a hash array because we're not going to make it something that can be grown in size so it'll have an initial capacity and once that capacity is full it'll be it'll be done so um the number yeah the number of the slots in the map are going to be fixed um uh and uh we're going to store the keys in uh this one array so we're so the keys and the values will be stored in two separate arrays not right each key value pair will not be uh consecutive um in memory they'll instead be disjoint and then the values are gonna be stored in this second array here and then we're going to have one more array that'll be the the size of the other two which will be a state for each one of the slots in the map and that's going to be an atomic of this state type which is just this um enumerator and there's three states for each different slot um this the states are going to be it'll go from empty to reserved and then to filled no other transitions are allowed so we're only going to support uh insertion into this data structure we're not going to um uh deal with uh removal of elements so once something's been inserted there's no way to take it out we have a some function objects for hashing the keys and then comparing the keys for equality and that's it and then we've got this uh try insert um function uh which does the actual concurrent insertion so um let's look at a little diagram of how this is going to work so we're going to start up here at the uh at the top at try to set slot to reserve so that's the first thing we're going to do when we're trying to insert is we're going to go and find the slot that we want to uh that's the candidate for this insertion um using uh linear probing and then we'll try to set that slot to reserved now if uh we succeed in setting the slots reserved that means that we are going to be the one that's going to get to set this slot we've claimed this slot essentially we've sort of locked this slot so then the next thing we'll do is we will uh fill the slot with the key and value that we need to insert and then once that's done we will set the slot to filled and setting the slot to fill tells everybody else hey this slot can now be read and then the insertion has succeeded now um suppose that uh we're trying to set the slots up reserved and we find that the slot um is already reserved um okay well so now what we need to do is we need to wait until the slot becomes uh filled um and we'll see why in a minute so now let's go to the case um of we tried to set the slots reserved and the slot uh was was filled so if the slot was filled now we need to check to see if the key in the slot is equal to our key it's possible that it's not it's possible that this is just like a hash collision um but it might have been that this element was already inserted in into the table so if they are equal then the insertion failed and we just return and say hey this this key has already been inserted if they're not equal um then we're going to go and find the next candidate slot again using linear probing and then we'll repeat the whole process um uh again until we eventually either succeed or fail all right so now let's walk through the code of how we do this so the first thing that we do and try insert is uh we compute the first slot that we should attempt to put the specified key in and we do this by hashing the key and moduloing the result with the capacity of the map so next we're going to enter this loop where we iteratively try to insert the key into the slot and if that fails we pick another slot and try again and we will um uh if we fail to find any slots after we've made a number of 10 attempts equal to the size of the container then we give up because we've tried to insert into every slot in the map and we failed which means that the map is full so now let's look at this probing loop so the first thing we do inside of this loop um is we atomically load the state of the slot that we're probing with memory order acquire semantics if that state is empty then we are going to try to attempt to set that state to reserve to lock it to claim it for ourselves so to lock a slot uh we need to set a state to reserved and we'll do that with just a compare and exchange so if the compare exchange succeeds then we've set the state to reserved and we've acquired that slot now we own it and now we are going to succeed so all we have to do is insert the key in value and um after we've inserted the key in value now we need to let everybody else know that uh the slot has been filled and that they can now go and read it or that they can check whether their key matches the key that they're trying to insert um so we use memory order release here which is important because we want to establish this memory ordering here we want to make sure that when any other thread after if any other thread reads that the state of this slot is filled that they can also read the key and the value and they'll get the key and the value that we wrote before this store and that's what memory order release does memory order release says hey make sure that anybody who sees this right will also see any of the other rights atomic or non-atomic that happened before this atomic right my apologies i just dropped my phone all right so finally we need to wake up any of the threads that may have been waiting on this slot and we're going to do this with notify all so that's the again this new c plus 20 um uh wait notify api so here we're just saying hey anybody that was snoozing waiting for us to make this thing filled now you can wake up okay and then we're done so we're going to return a pointer to the inserted value to say hey here's the node that we successfully inserted okay so now let's go to the case where we failed to set the slot to reserved but we've observed that the state of the slot is reserved and that means that another thread has acquired this slot and we need to wait for that other thread to fill it before we proceed so we'll know that the slot is filled when the slots state is no longer reserved again in this simple example there's only the three states there's empty reserved and filled and uh you're only allowed to go from empty to reserved and from reserved to filled so if we just wait um until like if we see that it's reserved we know that it's never going to go back to being empty we can just wait until it's no longer reserved um and then we'll know that it has become filled so we could do this with just this you know simple naive spin loop here but again we want to avoid those types of spin loops because they're not particularly efficient so instead we can use a weight here and and what this weight says essentially is wait until this atomic is no longer equal to this value and we know because of the invariance of this data structure that when this state of this is no longer equal to reserved that it will then be filled okay so now after that wait we know that the slot has been filled by another thread um and so now we need to check whether the key that was filled is the same as our key it's possible again that it's not in which case it's a collision and we just move on to the next candidate slot if it is the same key then the insertion failed and we just returned the value that the other thread inserted so so if the key wasn't equal then we need to move on to the next slot and try all of this again and so we just uh because we're doing linear probing we just say okay we're just going to try the next index um modulo the capacity and that's it this is a pretty efficient uh gpu implementation of concurrent insertion into a hash map um i i don't need to show you the algorithm for lookup it uses the same basic principles here and there's a couple different variations of this type of data structure there's different things that you need to do if you want to be able to support removals um but this is something that will scale to tens or hundreds of thousands of threads on your gpu and it fits nicely on one slide so uh the sort of the moral of the story here is that there's this whole new world of algorithms and data structures uh that can be accelerated on modern video gpus because now we have um c plus parallel forward progress guarantees and uh c plus atomics which lets us write any class of concurrent algorithm including blocking algorithms so it greatly expands the type of uh code that you can write on gpus okay so that's um that's pretty much it um so as i mentioned before lib coupe plus plus um is an open source project you can go find it on github it's also included in the cuda toolkit it's been there since uh 10.2 and we're constantly uh working on adding new uh features we just put out a release a few weeks ago um so be sure to check it out