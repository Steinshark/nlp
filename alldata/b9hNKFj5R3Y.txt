anyway uh who here has ever heard about ECS or entity component systems okay not even a half I would say I'm I'm bad at math just get out immediately out of I'm not a good math person but that was roughly half for the camera and maybe a bit less than that so that's good that's my audience uh we're going to learn together what it is about uh and what learnings we can take from it I'm not trying to sell you an ECS I think I'm maybe I'm getting ahead of myself a bit I'm not trying to sell you an ECS but I'm going to explain a bit what it is why do why did the concept come to exist and what you can take home uh even if it's not really a thing you can entirely use so um if you ever been to like any game developer conference ever this is like if you ever get a bingo card this is the this is the the drer this is the one that always wins because regardless of the talk one of the first three comments would be by the way have you thought of using an ECS that would solve all your problems regardless of the talk you can talk about animation audio design doesn't matter use an ECS he will solve all your problems that's that's that's that used to be the talk like every time I watched the GDC talk that was guaranteed top three questions um and and I I tried to do some Google Trend search just to just to see you can see there was a big spike somewhere in like 2017 2018 and then the hype kind of died down and then it's still backing up but I don't I I I don't know charts much uh I I don't really offer Financial advice but it does look like it might be going down again we'll see uh so and also uh bonus apparently Sweden is the chart so yeah it goes far I I live in Sweden I don't speak Swedish but uh it felt felt good I don't know why Sweden is like 100% on ECS that's not entirely my experience but who knows anyway hi uh I'm Matthew uh I'm a I'm a tech lead at the Paradox development Studio where I work on hearts of fire run and over stuff that I cannot talk about uh or else I'll be fired uh but yeah it's a video game company we make historical video games uh you may have heard about us so this was a a very difficult talk to work for me because I I knew over summer I wanted to talk about usually I talk about what I've researched the the year before that that's how I go for conferences and I was I I wanted to talk about that exploration about data oriented design and a bunch of things related but I couldn't really get the talk out so what I did is I went to my Meetup and I say hey I have a talk about ECS but I don't really know if it's about ECS please come at the end and tell me what this is about I didn't get the feedback I hoped but I got a really good idea in my shower like four days ago so I Reed half of my slides and I hope this has a better uh connection to to the thing uh if you've ever seen actually maybe another self likee appreciation as anyone have you seen my one of my talks before Oh I have some fans nice okay for the camera there was everybody in the room by the way thank you uh okay so I made a talk about profiling I think like a year ago uh I made one about multi Fring two years ago also based on my games uh one day I think I will have to talk about allocations uh but the the the thing that that I keep running around is basically it's performance related and it's usually things that I realize are bad for performance in the games I work on and how do I try to bring the after I solve some issues or I notice some issues with that or fix some problems how do I bring back that knowledge and I think this is really the working the the the the the thread through through all the talks like how can you profile your performance problems and then how can you use threads how can you fix your location which won't happen one day and today we're going to talk about caches data local and how does how does RAM work basically that's going to be our goal so we're going to talk about what is ECS or entity component systems you will probably be uh like tired to death of hearing the words ECS by the end of this talk I'm I'm warning you in advance uh we will talk about data oriented design which I think is going to be maybe the meat of this talk uh and in general cach friendly data structure and why does that what does that mean uh and coupling or more importantly decoupling your code and you'll see there is a very strong connection uh to some degree between data oriented design and like fry thread friendly design but first let's talk about Hardware because you know this is a C++ conference we really like to talk about Hardware we like to say that we're very close to the metal to some degree so let's let's thought about that uh we can start by giving some appreciation for the people who make the CPUs that make our job exist uh first we can eat and two they're actually pretty good at what they do like uh I don't know if you ever looked at like the actual rle numbers of how much stuff your CPU can do on a daily on a second on a nanc basis it is crazy uh like most B basic aric is like a couple cycles and you know I again I'm not a physician but from what I'm told like a cycle is like you divide the one by the frequency so like if you have 4 GHz that's about like something less than a nanc which is just an amount of time I just can't really be present in my head I'm told the best way to figure out the speed of light is to use like how much to figure out nanc is to figure out how fast the light can go in a nanc to give you like a a reasonable physical distance that you can make your head about it's it's nothing like a nanc is nothing again your CPU can do four billions of those in uh your CPU has four gz do does four billion of those things in a in a second that's that's an insane amount uh when I was in school I was taught to avot divisions in square roots in code because that's really bad and your CPU can't handle that that's I'm not saying it's a lie because it is technically way slower by maybe like a factor of five or 10 than making an addition or subtraction but it's still nothing like again a few dozen Cycles again at 4 GHz your CPU can crunch like maybe not a billion of Dos but probably hundreds of millions of dues per second I don't know how many numbers you need to divide but I I I don't think that's going to be your performance bottleneck and then if you have simd and every CPU for 20 years I've had simd you can multiply that by four or eight depending on how many uh levels of AVX and uh n instructions you you have available it it is absolutely insane and that that's that really the bit where I like the CPUs we have are really stupidly insane and I think that's been a big thread through the talks I've been having lately it's a simple but very important realization that none of the what we do should take that much time given how fast those things are and yet well we all have performance problems I mean I don't know who who who doesn't have performance problems okay I'm not lying this is nobody in the room everybody thinks like they could get a bit f FAS if it could so uh here's the problem here is one of the problems that's the the problem we're going to focus on today the problem is memory right because there's this thing called physics which I'm about as good at as with math but it says that you can only go so fast uh so if uh if if things are too far away uh so basically your CPU is very fast as long as whatever data it needs to read like The those those bits or physically close to it and by physically close I don't mean in the same room or even in the same computer it it has to be on the chip if it's not on the chip it has to F it has to figure it out another way so basically what I mean is if it's if it's in the ram you're out of luck like if it's in your if it's in your CPU registers or in your L1 cache which are like data storage on your CPU then you you're fine you just it's it's really it's fast it's it's it's collocated so it can operate at the same frequency as the CPU but if it's not if it's somewhere in your RAM STS and don't even get me started or if it's on desk or on a machine on the other side of the world like that we not even talking about that today we're talking about like if it's in your RAM stake instead of your CPU it's too far away and I have a couple slides to explain exactly how much because I don't think it's always easy to grasp uh so here is a here's a very um accurate representation of a CPU and some Ram STS uh it's about 100 NS roughly speaking and as far as I know that number has not really changed in the past 10 years I mean they tried to do some stuff with like Ci CLS like super gamer Ram but mean roughly speaking 100 NCS if you need to actually fetch some data from one of those like tiny chips and put it in this chip too scale or not I don't think it's too scale at all uh so 100 nanc okay but again what's the nanc we talked about that before that's a very hard number to reason about for most people like I I start complaining when when it takes a couple seconds for an application to to do a thing and probably if it doesn't respond in the second I'm I get angry but 100 Nan seconds I don't think anybody can measure that in their head like that's just physically impossible for us but as we said before it's like a okay it's a 100 Cycle at 1 GHz so already we start having like a a frame of reference assuming your CPU is running at 1 GHz it's not right it runs at like two three four GHz five if you have turbo but for n of magnitude is good enough if if if one arithmetic operation is like one Nan one cycle so one Nan seconds well fetching the same data from Ram is 100 nond so it's 100 times slower to fetch the data do the addition and then push it back to whatever memory cell you have then to add it like if you have to fetch it add it and then push it back it's going to take you about yeah 200 200 Nan seconds to do the the round trip and one nanc for the addition you're like you're wasting your time you're basically the aric doesn't matter what operation you're doing doesn't matter what matters is how fast it can get to your machine uh and to really hmer the point uh I want to compare that with like the over caches we have on your on on on a modern computer so uh if it's on a register it's usually considered free like it's basically as part of the as part of your operation that does like an add a multiply whatever other arithmetic your CPU does writing or reading it it's just free it doesn't really have a measurable impact if it's in your level one cache which is also on the chip about half a nanc you can measure it it's it is technically not free but it's as close to three as it can get uh if it's in your L2 cach then you start being a bit slower it's four Nan seconds but still like those numbers especially if you start normalizing them like assuming that L1 cach head is is one it's a maget of it's It's Time s and time 200 and to really make sure you understand this uh I'm going to show you a small animation that I I saw in the do talk and painfully recreated in uh in Google Slides because it's I'll see okay so I'll try to scale to show you the time to fetch so let's do L1 that's done you know you haven't you barely had the time to Blink okay let's fetch for L2 cach with the same like time scale okay uh let's do it okay well I could see it going right so it's not as fast but still I mean if we compare it like okay this is in instantaneous and this is close enough all right let's fetch from Ram now I can take a drink almost halfway there wait ah ah you almost got it you're almost going to be able to increment that pointer by four almost almost there we go and now we can D reference it and do it all over again because it's another part of the memory cool so I think you get my message right memory access is slow it is stupidly slow compared to what your CPU is able to do and it's basically ball necking a lot of things if you get in the wrong situation uh it's really bad and that's why Engineers have spent the last 20 30 years making a lot of cash every time you see a CPU now it's a big argument how big the cach is and this is the reason is because L1 L2 L3 caches are trying to basically hide that latency from you by trying to get the data or keep the data around to help you but there are limitations and that's what I want to explain a bit uh this is what we call data locality and that's what I will explain okay so uh your caches like let's just say you have an L1 cach and it has a bunch of lines I think each L1 cach is about 64 bytes it's not a lot but you have a bunch of Dos uh I can't remember the size of top of my head but you have a bunch of do uh and the idea is every time you try to read some data in memory the CPU is not going to just fetch the two bits or the not bits but bites like if you ask like to read like an inch four bytes it's going to be nice with you it's not going to just fetch those four bites because that would be super wasteful it's going to fetch those four bytes and everything around that like on a 64 byte segment basically the reason is well first of all it's just throughput like if if you can't beat the latency try to Beat It by just more data because maybe you can just have more stuff going on your bus so that's the first thing and the other one is just usually or possibly when you access a bit of data there's a high or reasonably High chance that the next bit you will try to access is basically the one next to it which means it's already there we already paid the cost might as well uh might as well do it right and that's what that's what caches do right so if I keep reading my memory cell assuming it's an array for example like I have an array of ins in memory and I try to iterate from it h that's what it would do right I would just go through memory and I would just read them all the time the first one I will pay for because I have to fetch but all the other ones are virtually free because I already prefetched every line in immediately uh and your CPU Founders are also really uh smart so what they realize is that if you keep you know walking through memory like one two three four there's a high chance that it's going to continue right patterns have this these things so we have a there's a mechanism called the prefetch we will automatically realize like hey you keep hitting those memory addresses in the sequence you know what I'm going to save you from yourself I'm going to prefetch the next one cuz so by the time you at the end of that one you're probably going to have this one already loaded very nice and they smart enough to realize that you know if you can plus plus you can minus minus that's usually let the two ways people go about Rays so if you go the other way at some point they will also do the same thing and fetch the top one I'm sorry the color doesn't really show that much but as what I mean is yeah this line is now also loaded by the time you reach the end so even if you're going like decrement uh you will get there so every time a colleague tries to have an argument with you that you know going incrementally or decrementally for rate change this thing it doesn't it's it's roughly the same thing the CPU figure people have figured out that you can go two Direction with with with basic Ari effect and both Direction have a prefetched mechanics no what it it works fine and it's really good um so basically memory access is slow We we've been that but caches will try to alleviate most of that by making sure that the minute you start accessing some data somewhere in memory it will basically load the rest around that and then if you continue growing or uh decrementing addresses it will continue doing the same pattern and most of the time the data will be ready for you before you actually need it roughly speaking if it's if it's if it's if it can predict the thing but again it has to be able to predict the pattern and it's smart but it's not that smart so it can predict that after one comes two and after two comes three and the other way around that's that's has the two F he can he can he can do but if then you jump like plus 10 minus 5 42 and then completely somewhere in memory then you're out of luck then it can't predict where your memory next memory load will be so it can't pretch it for you so you're going to be looking at the loading animation that I showed you earlier so if I have to summarize it the most like stupid way possible it really is linear access good Random Access bad like if you go up or you go down you're fine if you do anything else and it doesn't really matter if if you jump 100 or like a million memory Cell It's kind of roughly the same thing you will have the same problem you will you will hit the problem that this memory is not really available on your cache and it's going to have to fetch it first and this is basically the whole reason why uh there has been a push for this thing called Data oriented designs it's not that new as far as I know people have been talking about it since like the early 2000s at least uh and the world ECS entity component system have also been mentioned somewhere in the 2000s but I know it's it's making a comeback and I think it might potentially be related to the fast that again we're hitting a bit where we can make the CPUs as fast as we want but the RAM staks have been as fast as they are for like 10 years and have not really changed so this is getting uh a bigger I don't know if it's a bigger deal but it's still a big deal it's never going to it's not going anywhere so the point of of data oriented design is to try to work with the CPU rather than against the CPU and basing okay your CPU is is good at arrays that's what it it's good at it sucks at M it sucks at Maps it sucks at lists it sucks at anything that jumps randomly in memory what it's really good at is for I equal Zer I less than size plus plus I it's really really really good at that and if you have to BU process anything it's going to be very good if you do that with a vector it's going to be horrible if you do that with anything else or anything else that is not linear Access Memory you might get away with a deck if your chunks of a deck are big enough enough but if you start going with a list uh a map even an unordered map especially the standard unordered map you're you're you're not going to have a good time so the idea is that okay we try to think about bulk because again uh when you program you should usually optimize for the bulk case not the one case right okay if you have to process a transaction it's a transaction but that's usually not the thing that kills your performance the thing is when you have to process a 100 of them and I know as programmers especially the way I was thought and I think it's still a thing we try to think of the one case and they okay I can generalize it by putting it in the loop and calling the same function that's that's a very normal way of modeling things in your head right like you you do the one case and then you write a loop that just calls the swing repeatedly which works but the idea is to try to go away away from that and say the one case is the exception it's not the one I care about the the care the one I care about is I have X amount of data and I will will process them in a bulk and you don't have to write it as a loop with no like function in between but the rough IDE is that's how you should think about it it's it's really I want to do a bol processing and I want to do a bol processing uh that just takes an array in and outputs an array out or uses the same array both options are are correct um and toh show you an example this is usually call as the array of struct versus struct of array uh debate or uh fight or uh Gundrum whatever you want to call it so okay let's say we make a video game right a very simple video game we have a thing called a unit I make strategy games right so you have like units on the map like little dudes with guns and tanks and they just move around uh so it's it's maybe the simplest possible thing it has a position a velocity uh and some HP uh so position is free float because we're doing freed so you need free coordinates uh and it has a velocity which again like the one of them is probably zero your tank is usually not flying up but let's just say we want to handle every case so we don't bother that that's the thing it's it's free Flats again and finally I have an end which is my HP and if I run below zero I die and the unit is gone that's very very basic like the dumbest possible thing I can do the classic way you would do it in in most like uh way you want to think about it's like okay that's what a unit is Right a unit is described by its position its velocity of in HP so I chug all of those in a struct and then what what is my game about my game is about a vector of all those units that does make some kind of sense when you think about it that conceptually if you start trying to explain the design of your game it does make a lot of sense sadly your CPU hates you when you do that and you very much prefer the the one on the right which is okay we don't have a stru for a unit we have a stru for all the units and what is it it's a vector of positions a vector of velocities and a vector of HPS and then at the end all unit is well it's just a well it's it's a struct of one item and why would we do that let's let's look at what it is like let's look at the memory layout of our of our thing let's see we have like a a classic array of struct so this is my memory uh this is I think each each cell on all my slides is considered four bytes because floats in eight or four bytes so this is my unit right it's it's it's uh 4 eight 12 uh 12 bytes of position 12 bytes of velocity and an extra four byes at the end for uh for uh uh for for HP which is an in we don't have any extra alignment requirements so we can even just call them put them one by side by side and there is no padding or anything let's exclud padding from from the equation right now it's just that's our unit that's how it would be laid out in memory and oh sorry and so if I have a lot of them they would do that right like you would have the first one here the second one here etc etc and they would just be uh be laid out in memory like that in my Vector that's that's reasonably what you would do but now let's look at what the operation I actually try to do on those uh on those on those units do it is very unlikely that I would have one or it would be probably bad design even from a simple like you know like single responsibility principle concept that I have one function that does update the thing that does everything in one go right I give it a unit and it does everything it draws it it updates it position it simulate combat it is the one function that's usually not how you would structure something you would have like several systems in your game that are possibly managed by different people or maybe at least return at different time or maybe run on different schedules and they would each access a part of the uh of the actual data so for example uh I have a bit that updates the position of my unit which uh I'm really bad at math but I'm I'm told it's just a sum of those two because if you sum your position and your velocity you have your new position you can multiply by a Delta if you have time but that's that's the rough ID so basically what we do is that we use those three here those three here and we output those three here that's basically the memory footprint and data footprint of for for function then I have a thing that will just simulate damage because I have a combat system somewhere and the only thing he will access is the uh it's the HP uh data member and he will probably change it and then I have a third system which is my 3D graphic uh thing that is just rendering all my stuff on the on on the screen and it doesn't care about any of that the only thing it cares about is the is the is the free coordinate so you can put it on the screen and see if it's visible for the player or not which is some math involved and it only cares about those three bits now here's the thing if I try to update position I will try to keep every time like in Gray all the bits that you don't actually read but still have to fetch because you're reading this data from memory right and all those things come together so in a classic like layouts if I have my update position it's not too bad like out of uh seven uh quad bytes uh memory cells there's only one that I don't care about when I iterate through the array to update the positions so you know one in seven waste it's not the best but I've seen worse right as far as as far as as waste is concerned turn it's potentially an acceptable trade-off if that was the the only case I had I would say maybe it's not worth changing all my code just to get that benefit whatever it's fine but then we start having a function that just simulates damage and now we're like oh I only use one in and seven cells of memory and again memory fetch is are expensive and even if you can try to prefetch uh the more packed it is the more likely your pre-etch will actually give you all the data you want because even if you prefetch it still takes time so if in the time you the prefetch run the only thing you had is like touching one end it's very likely that the next bit you will need will not be ready and you will be waiting on the on the memory BS so that starts to be a problem because for every seven uh yeah quad btes I fetch I use one and the six over are just wasted I don't care about that data is just there to take space in the memory bus so I'm wasting a lot of bandwidth and we don't like to waste band with when we're talking about performance uh and then we when I start running my renderer I have the same issue it only cares about the free first quads of every uh of every structure and then the rest are just garbage with like those bites could be random for it cares it's never going to read them so same thing I'm wasting about over half of the over half of the memory fetches I do are useless and I could just do without them and roughly be twice as fast that's a bad estimation but I have some Benchmark later so what if I do the opposite what if I have a vector of position a vector of velocities and a vector of head points for every uh for every uh for every unit I guess I should have like maybe split those because you can see that there's way more units stalled in this one than in those two at the top but that that's roughly the idea right this this this Vector is like three times smaller in memory du to the sizes but if I start doing the same problem the same update thing well this time the green bit and like the gray bit is the bits I don't fetch but it's fine because I don't use them so while I will run this update I will have like my memory band with will fetch this uh uh disposition uh this this this this bit in memory too but it will never fetch those one so everything I fetch is exactly what I need I waste zero memory on the on the process and your caches are small enough that you can have several uh independent arrays in your uh in your uh in in if in your cach at the same time even if they share like some uh similar modular memory addresses there's a thing called associativity if you have a look at the specs of your CPU it will say like x x cache associative like I think the minimum nowadays four which is like if you have four um if you try to fetch four pointers that happens to line on the same cach line you still manage to have the four and they don't eject each other and unlike big super modern CPUs like the top like Intel or IMD CP you get I think you have like 12 lines or something ridiculous like that so you're not going to have a problem with that uh if I look at hit points it's probably the best I can ever do I'm just fetching a vector events and I'm just iterating through it and I will never bother reading the last one and same thing for the random thing I will only read my positions all the time and again all of those operation waste zero memory they always only read what they need and again that's the thing we're trying to optimize it's the amount of memory we fetch because that's our bottleneck so what we call data oriented transforms is basically the idea that you make small Loops that operates on array of fields and those fields in the arrays are only what they need and exactly what they needed nothing else or as close as nothing else as they can there is an upper bound to this process because if you push this process far enough then every uh you don't every field is just one end or one bull or whatever at some point you start maybe thinking you know what it makes no sense to have two arrays of those things maybe they should be in the same array because they makes significant sense to be together even if half the transform maybe you're not using all the data but as far as possible what we're trying to do is make sure that we have small Loops that operate exactly on arrays of fields they need and only what they need uh so you minimize the amount of data in the cash that isn't read or right during the update because again Cash Cash sizes are very very precious if you have a look at the spec sheet of your CPU I think you have like even on a very expensive CPU your L1 cach is like 64k I think 18K maybe that's that's not much compared to like the uh I don't know 12 16 32 gigs of RAM you will have so you really try to optimize that real estate uh and and that's the bonus that I will make a small uh uh a part um like uh side side note about is those transforms then you realize wait a minute those fields that I'm looking at like they're entirely independent like you know this this this this step uses only do bets this step only uses do bits I could even run them at the same time on two different Frets and I would not need any synchronization because I'm guaranteed by construction that they're literally not accessing the same data the same data fields and I think we had a talk ear you today about like data synchronization I couldn't make it but one of the big things you have to know is the best way to do data synchronization is to not have data dependencies and then you don't need any Atomic or mutes or whatever you can just prove by construction that those two Frets can run at the same time and another way to optimize CPUs that have been working a lot for the past 10 years has been adding more cores so all right let's look real quick at a benchmark so far it doesn't look that big of a deal I made a small Benchmark on quick bench which is just I created like I don't know 100,000 units and I run the free updates in sequence uh and this is what it got what you get when you do it with an array of strs and this is what you do if you have like a bunch of uh a stru full of arrays it's not I mean it's like a maybe 10 20% it's not non-negligible but it doesn't sound like a big deal uh I will get back to that but first this is the small um break I have to talk about code architecture because this is this is the bit about coupling that I think uh is important to keep in mind after talking about caches because you know we're not only just wasting memory uh when we start having like cach line full of data that an operation doesn't need we also mixing up concerns because to some degree I think it IND this maintainability that is struct as a lot of fields that are technically not used in the same update even if logically it makes sense in your like mental model like you know if if you're trying to design a unit you say Okay a unit is a position of velocity and uh and and uh and some HP but when you're thinking about code and what data do this function actually need to access then you realize that it's really hard to resign to to to reason about because in the first model my random method takes a unit as a parameter I cannot like unless I look at the implementation of the random method I cannot guarantee that it will not read any and all fields in the uh in the unit and then I don't know if two operation on a given unit or or threed safe for example and it's just a lot of cognitive load because there's all those fields that are given to me with my in my in my in my code and it turns out I only care about one so basically it's a thing that we we we've cautioned people against like uh coupling and and and mixing things together forever I think as as far as we taught computer science we say don't mix concerns that don't need to be mixed and usually when we think about that we think about code right we say okay this function should have only one responsibility but I think it should and I don't think it's it's a thing I invented I I'm pretty sure it's it's it's a thing that's being taught more and more it's a you should also apply that to data why do you have a struct that have members that do not appear in the same update well then maybe they shouldn't be the same update they should maybe be two two different structs and then you could have like a parent that is the same and they could both have the same index in two different arrays but they're not the same and they don't have a reason to live together if you don't actually use them together and again the point of data oriented is to look at like the the architecture comes from who uh where the data of how the the the data is accessed and not how the conceptual model in our head uh looks like so uh yeah basically don't mix fields that are not accessed by the same function it's all a nothing right like if if I take if I take if I take an object as a parameter or a collection of object as a parameter the default if I'm reading the code is to assume that they can access anything so the least I give them the better because the more assumption I can make and as I mentioned then I can uh yeah I can't reason about data sharing if I have two function that takes like a writing a a mutable copy of uh a mutable reference to to a unit I can't assume that they're both uh able to run at the same time I can try to read the code of both function prove well prove as in in my head convince myself that it's safe then decide to put them both in a different threet to gain performance and then the developer comes back and doesn't know about this thing because I mean he's given a function that is allowed to access all the unit and then starts creating a data race and I call enforce that in code because it's one struct and either I lock it for every either I lock it and then it's not multif Fred anymore or I don't lock it and I pray to God that nobody violates the promise uh so yeah uh if you remember that talk I don't know if you've seen it um it's pretty good you should take a look at it uh the is like yeah you can't you can't you can't you can't be fat safe if you use two function that that takes mutable references to the same thing so if you split the object in smaller parts then you can prove like by reading the code that those two functions literally do not access the same data member AO they are frat safe and if you decouple your data well first your buffer uh your memory will will thank you but also you will have a way easier uh ability to decide if those things can be Fred safe if they can if they need to be save together a lot of good things come from here and now we can go back to our benchmarks so um because you know I I only had 20% right in my in my in my in my previous Benchmark about the difference between the two which sounds nice but I can understand why someone would come to me and say Matthew like I'm not rewriting my entire uh like unit class and everything that accesses it just to gain 10% Like I 10% is nice but it's it's not worth it and that's a fair argument I I I would accept it but now this was a minimalistic uh unit right this is not an actual unit in the game an actual unit in the game I'm just going to put some padding data to simulate it but an actual unit in the game in my game would have way more data than this you would probably have like a bunch of other stats and not just HP probably like some data concerning like Which models and texture it should use uh who's the owner um million other things you can imagine a simulation is doing to simulate an actual like unit and if it's a tank like maybe you have data for different parts like is the cannon damage is the did it l the track how much fuel does it still have you can go you can go as far as you want and we make very complex simulations that people really like having millions of stats per unit so that's that that gets that gets really quickly like an issue so they start to be a bit more fair so assume that instead of like no data here there's maybe 32 bytes of like extra stuff that I also don't need in my updates and now it starts Turing to be a bit more like you can see the difference like it's very noticeable it's almost twice as fast uh if we start going to 64 BX reping it is yeah over two two times as fast and if I go to 128 bytes of extra data that I don't need for my updates you can start getting to uh I don't know like three times maybe four times I stopped here I could have continued it gets worse uh The Good the good thing is at some point it stops it stops because there is a limit and the limit is for each unit you basically need to fetch the memory and weight and that's the worst you can do which is bad but it's it's it has an upper bound which is an upper band that your could probably has on some systems because mine has though um I'm I'm assuming over people have the same it's there is an upper bound and it's not obvious that there is an upper bound but when you change your code you suddenly realize that wow the same exact code could run like eight or 10 times faster just because suddenly the memory access is uh actually efficient uh which brings us to the concept of ECS and why do people talk about ECS I know it took a while to get there but we finally going to start talking about what is ECS and again it's because the main point of this talk is not as much ECS but more like why are people talking about ECS what can you learn from it because as we will see it's somewhat of a fad but there is some interesting ID behind that so the idea of ECS is that we bundle Fields into components so like for example all my pos position would be a component which is like free free floats uh velocity would be a component HP would be a component I have another slide to explain it I think uh then every object in the world is an entity which has an unique ID um I mean classically from zero to like how many you have but that's that's you could you could you could do different models of of of ID but some form of a unique identifier it allows me to look it up if I need to uh and then each entity can have an arbitrary number of components so like not every entity in the world will have a position of velocity HP and the 20 of a million components that you can think about in your game so like you know like does it have like a stockpile does it have like a fuel Reserve does it have like an owner all those could be considered separate components basically uh and you can add or remove components through uh to a given entity at a given point for example like if a unit is under the construction maybe it has like a a construction progress uh thing but it doesn't have an actual like effective stat because it's not done and when it's done then you transform it and you say Okay remove the construction component because it's been built and give it like an actual active rooll uh component instead things like that the other classic is an AI component like if you have a tank in the game as long as the player controls it for example you don't need any special data for it but if if it's not controlled by the player but it's controlled by an AI then maybe it has an AI component which holds all the data that the AI needs to simulate and make it move things like that uh and then what is a system well a system is a transform basically that operates on a selection of components so I have a I will explain that so yeah and then the the the big concept is like if you want to update the world what you do is that you gather all the entity in the world then for each transforms you say okay give me all the entities that satisfy all the components I need as input and output and then I do a ball transform on those and potentially through from framework magic I can realize that those transforms that do not use the same component SC mod at the same time we like some schedu mechanism basically that that's that's the the base idea so if we go back to a very minimalistic game design here uh so this would be uh like alter transform or like update position take damage and renders are basically all transforms uh all components are uh position velocity and HP each one is a different component and each unit is an entity so it has an in it has a unique ID and it has potentially those three components maybe more maybe less that's the that's the reality so that's that's the components and that's the transform SL systems I think I need to update one of the subtitles on this so the efficiency basically comes from breaking objects into components uh and then storing them in dense arrays so we optimize the memory bandwidth associated with reading and writing them uh and the rest of the talk and the original point of my talk was how do we implement it uh which now is more like a a nice thing to learn but I think the main takeaway you should have has already been uh given but we can still continue and talk about implementation a bit because I haven't shown much code yet so actually there's no code it's mostly diagram s but I will explain to you how you can do something like that because again I think the the main point is okay that sounds interesting I would like to optimize the memory band for my program when I go home but maybe I don't want to use an or I cannot use an ECS system entirely or I think it's too much but maybe I could just take the data structure so let's look at what it's how it's done what's the goal what are we trying to achieve what are the constraints and also the constraints are important because they help you figure out can it work in my case or not because games have their own demands and they own Necessities you might not work for whatever you're doing so uh obviously the thing we're optimizing the most and the thing we want is that if we're iterating for components we want it to be as fast as possible uh you know like as fast as the machine is basically allowing us to do it so mostly speaking it is like as make as better use as best use of the memory as you can to make sure we don't waste memory bandwidth cach utilization etc etc so yeah basically it should be as fast as going through an array that doesn't have any unrelated data um if we do uh if we start doing topples for example like our our update position use component toles right because it takes all the position and all the velocities so it's technically a tole of both we would like to be very fast we can't be we can't guarantee that we will be as fast because it starts it's a problem that gets infinite complexity eventually uh but I want to try to if I want to do a zip iteration uh over like a selection of components I want it to be pretty fast uh extremely fast would be nice but most of the time due to implementation concern you can only go as good as pretty fast or very fast uh you can try to constraint the Topol at creation saying okay like only those combinations of uh of of of of components will actually be used and then your your your data storage can try to optimize for those cases but it's it's a vast field and I don't have all the time in the world to explain why but trust me on the fact that is complex problem and it kind of runs into a uh like a combinator explosion uh issue eventually uh we want a fast lookup uh so for example if I have a given entity and I want a display on the screen either for debugging purposes or just like to give the player feedback or whatever else I want to be relatively fast uh in uh in getting all the components it has by which I mean like I think a linear search should be considered too expensive I won't want to have to go through every component table and say do you have ntx yes I know by because I I actually did like a linear search for the I want something faster so I need some form of an index or a lookup table that will allow me to be able to do a lookup in reasonably uh reasonable time uh that's the thing that not everybody does but for example my game is sometime needed sometime I need to be able to iterate for component in a deterministic order and by deterministic I mean if I start the game I start deleting and adding components all the time because some units so created and deleted I want a way to guarantee that if two players on two computers are iterating through the same collection I get the same order it's mostly used for multi multiplayer synchronicity and things like that but it is a use case that is not always used by every game but I would say I I don't have a number but a solid class of video games and the way they handle their simulation requires some time that they have to be able to iterate through all the component through a deterministic order it doesn't have to be by uh like uh increasing in index but it has to be deterministic we regardless of like how many uh items have been deleted even if someone like joins a game or whatever uh and gets like a copy of the current state and doesn't have to repack a bunch of data it still should get the same order uh and of course entity ID should be stable upon creation and deletion so uh if I if I add or remove components or add or remove entities all the other ones I can still do a look up by ID and I still find my uh my babies I don't have to do uh I I I I will not lose them uh so I looked a bit at some systems so I done some some some some checking uh Unity engine has been offering like an ECS System since 2018 I think this is this is the big spike you see in the in the Google uh Trend that I showed earlier they made like a big deal of the fact that Unity engine was uh was was going to do NS system in the future in 20180 they had a cool demo called the mega City there's an interesting talk behind it about how they did it it's it's it's a nice watch I recommend it I also get entt uh or ntity I'm not sure how it's supposed to be pronounced which is an open source one you can find it on the Internet it's C++ unlike Unity with C well and internally C++ but c as the API uh I think it's used part some parts of Minecraft are using it uh which I included because that's one of the difficulties I had doing this talk was Finding actual production use cases of of of of of of ECS because game developers love to tell other game developers that they should use an CS to solve all their problems but when you ask them okay whichs are using like we're not really doing ECS but so yeah uh and flex is the one that I looked at because it has been used in some bits of our engine but it's in C so I hate it and I refuse to take a look more than that because it's C I just don't respect that this is a C++ conference after all so you know let's let's focus on the important bits oh and that's the wait but my transition is broken okay fine we can move to the next slide so okay now I will just how much time do we still have okay 15 minutes uh I'll just show you very Basics how can you implement like the most simple data structure that will allow you to do like those kind of magic vectors including the the the constraints we have because as as I mentioned before the constraints we have uh do not allow us to just use a vector just like a simple Vector would not be good enough because as everybody knows if you delete an element in the middle of a vector you have to move everything around which means that suddenly you can't find your babies or you have holes in the section in the either you leave a hole but that's bad because then you need to skip some bits and then you don't have the most packed uh data structure ever or you don't and if you Shuffle data around then your key starts getting jumble then you can't look up things so that's the that's the problem we're trying to solve right so uh some people have tried to make solution for this if you ever heard about a paper about Colony or hype they keep changing the name I don't think it's been standardized yet but it's been in discussion for a bit uh it's a data structure that tries to address the problem by having what we call Skip fields which is either a beted set or some metadata that basically allows you to jump across gaps in a vector so it's it's a vector and when you remove an element instead of repacking everything you just Market as dead and then you have the whole thing is about having optimization to know how to jump through it when you when you iterate it's pretty good but it still weights memory because it's not densely packed and I think there are simpler way to do things and to me the simple way the one i' I want to focus on this is the one that's entity does for example is just swap every time you have to delete something just swap it to tail just just swap it with the last element and then have an external index because again like the thing we're trying to optimize most and again that's that's one of the main point here is that we trying trying to optimize the most is iteration lookup doesn't have to be 100% as fast it has to be fast enough so having an external table that does the lookup for us and and gives us the the the guaranteed like uh not iterator but index stability is good enough so how would we do it so yeah that's what I was saying here I can just skip that bit okay cool so all right let's skip this okay so here's here's how that works uh I don't have a name for this and this is the thing that sparked my talk already because I run into this data structure looking at how ECS are implemented I couldn't find a good name so you tell me if you've ever heard about this or at least the concept I don't know about the name the the guy who was mentioning it was the offer of entity has an interesting blog about how he implements the whole thing and how data structure should work with it I think he call it the dense SLP array or the den array Spar index it has different names but the whole idea is when you start you just have an array of uh of of all the the components so a BCD e FG is just like instances of the same components right I just tried to give them letters so not to confuse you with ID uh at the botom you have an index look up that is just an offset it basically say okay if I want entity zero it's set index zero in the table if if I want entity one it's at it's at index one Etc when you start when you just keep inserting that's very simple and consistent right you just this is this is the actual ID this is the the this is the index of that ID in the table that's that that's as simple as it is uh if I want to start deleting something then this is where I start doing some things so like I want to delete B for example B is dead that my unit B is gone forever uh RP and I I'm trying to delete it so I want to keep the in the thing at the top completely uh packed so what I will do first is I will mark this entity as dead the one of the classic option is to just do a minus one so if someone tries to look up entity one you just get like up not there anymore BR uh you're going to have to do something else uh and then I swap it with the last one because I want to keep that one the top one packed and of course then uh wait did I do oh yeah and then I have to update this index because obviously now uh this entity number six is not is not out of set six it's it's out of that uh it's it's at of that um one so I just update the index and that's it that's all I have to do so it's not well first of all there is no real like most of the time there's no reallocation it's just like a push back and uh no it's not even a push back it's uh it's just a swap like it's a it's one memory Swap and two memory uh right and that's it and that's how I can delete something and I still keep my top array like packed so if I iterate over it again not deterministic order anymore because I start jumbling things around but I have all my entities super packed and I'm not wasting any memory bandwidth when I iterate over when my actual cost is one of the Assumption is of course that iteration is more important than addition than adding and deleting stuff uh in general I mean if you're trying to optimize for a if you optimize for insertion and deletion you're probably not going to have a main your main issue is not iteration right but in video games and in many things we have the problem is not creating new things it's iterating over them and then we try to just make creating new ones not as painful as it could be uh now if I want to insert a new one well obviously I just put it at the back because that's the only place I can insert new elements because it's always a dense array uh and then I will just push back a new index entry and just say oh this is where your thing is and that's it that's that's really the data structure it's it's really simple I haven't seen any papers about standardizing anything similar that I could talk about about and again I did not I don't claim to have invented this I just found it by looking at uh the the blog of entity and how it's implemented and it's just two arrays it is just two arrays and that gives you good enough things to make sure that all your that satisfies all the constraints we have with one exception uh we talked about lookup before uh there's one case here that is not satisfied and it's the reverse lookup if I have like a component and I want to know what its index what its ID is I can't right there no easy way for me to get it from there because unless you want every component to keep an index of itself which again is wasting memory most of the time that is not worth it uh I can't really reverse it from any of those index without a linear search which is not good so the solution would be if I really need that which in some cases you do so like okay I have a given I have a given component but I didn't I lost where it's coming from I lost its entity ID uh well what I could do is just okay let's keep a number like you know this is this is basically the data I want right this is number zero this is number six this is number two three four 5 seven Etc but I don't want to throw it in there so what I'll do is I'll do the exact same thing that I keep doing I'll just put it in another array and that array is shares the same index Keys as the top one so if I'm given like entity G component g i compute his index we can I mean it's it's pointer Ari but it's just a Ric where he like okay uh it's it's basically St distance from the start to to it which gives me uh one and then I just take okay take that index in this one at the top and that's my thing and that's again that's just two memory lookups there is no linear search it's arguably constant time but again complexity you've noticed I haven't talked about complexity at all in this talk it's because complexity is important but memory latency is starting to get more important in some cases it's not 100% I'm not going to say it's fine as long as you have good data locality go nuts and use like factorial or or whatever polinomial like time no don't like try to keep it too constant or linear but but there are many cases where actually memory latency is a bigger problem than the fact that your thing is linear rather than uh than constant uh or maybe not concept but logarithmic is a big one uh I haven't had any slide about it because again I had to fit this slide and this talk in one hour but one of the big thing that that you can that people have started noticing is that binary search is starting to get slower than linear search in some cases because it keeps jumping through memory all the time and on top of that we have this thing called Branch prediction which also hates when you randomly jump SL or right all the time and at sometime doing the dumb thing turns out to be faster and that's it that's if you really need to be able to keep your indexes back but you don't want to poison your actual like component data you can just have the third array and then you're back to be able to uh to have the relationship both way between indices and and components so yeah it's as easy as two or free array and it's like it's simple enough to implement like maybe it's not stand I would say it's not standardized because it's it's it's too easy to write is a bad argument because some people would argue that implementing Vector is not that hard and still I'd rather have it than having to implement it myself so I don't know uh the second aray can be split into Pages because one of the issues we you can run into is that uh you start having like big gaps in your indexes because this one will always be packed this one will always be packed you will always be paying as much memory as the number of active components but this one well you notice that we start putting like some minus ones when we kill some things and it's just going to keep growing and growing and growing unless I Implement some form of like reuse of IDs which is also a big important thing but again um see me after or I'll probably make a blog post about it uh if you don't know this man this man is ferma uh he's very well known for having like a Last Theorem of FMA which he famously in the paper that he wrote in the 1600 said oh I have a proof for this but it doesn't fit in the margin of my paper so don't worry about it trust me I'll tell you later and he never wrote it and it was proven 350 years later or something using computers and nobody thinks that this proof actually worked but that's the idea like see me after I can explain to you how to implement ID Reus and things like that which is usually a thing you want to do uh else your uh your index uh lookup uh table might start growing crazy even if you have only five items in it because you created and deleted a million entities and you usually want to avoid that uh do I even even have time for the multicomponent case I think I would rather take questions so I will skip that bit and go to the conclusion there we go sorry I have to skip over a few slide there we go okay so did it solve our problems is I guess a question right so uh again production games at PDS do not use ECS like as as I know no no game in production at Paradox uses ECS so far uh future titles I've been looking at potential uses as I said our engine team has been experimenting with um Flex uh for some animation system I think but it's still very much an experimentation and I don't know if we will keep it or not or WR our own and in the end if it's used by only one system maybe it's not really ECS maybe you just have this nice dens R and that it and yeah for game play I still looking for it if you're a game developer and you actually use thiss at scale with like I don't know 100 different concept or components and you actually manage to use an ECS framework from that I'm really curious so you did it because every time I try to ask people like we tried um yeah we can skip that uh so yeah one of the issues that you run into this and that is somewhat important is that you want kind of want to tie down like which kind of uh component uh pairs you can have uh topples uh veal archetypes in some literature an archetype is basically an entity that is given that is guaranteed to have like a select set of components and that's usually the one you of operate about on on a given transform uh designer hate them especially if they're pinned in code because what designers love especially in like complex games that have lots of system that interact with each other is they like to say oh what if suddenly the unit system was starting to feed data from the Spy system because that would be super cool and then you're like oh but you can't access the data here because I have to change the archetype and make sure that every of those has access to this component and that changes the whole lot blah blah blah blah blah uh and this is a thing we've started noticing when we tried to use a similar approach but it was mostly for multi threading at the time not for uh semi over talk I guess uh they really hated the fact that nowadays system updates are like taking very well defined sets of components in the signature they really like to be able to do unit. get or do access whatever and then just Jam a gameplay feature like five minutes and be able to test it so that's cool I could do it and never like oh I can do that now I have to go back to the Coler change the signature add one new component to this update and now I get warnings that this is they don't like it as much so uh there is limits to this but the the speed up is uh is is very important and again the fact that if you start defining your updates as free functions that take constraint input by a type will allow you to get multi friending for free and decoupling so I could have called this talk like you know I use thiss and it didn't solve any of my problems uh but it's not entirely true uh I think it is important to realize why we went there because of the data oriented Concepts and all the speed ups that it can go uh if you start having multiple components and you want to make a z view iteration it starts getting a bit tricky that's the bet I skipped because this talk was complex enough uh but again single components is the structure I showed you I think it took like five minutes to explain maybe 10 I don't think it's really hard two or three vectors and you will get something out of it so yeah consider the dense par way uh it is pretty it's the fastest and all iteration speed you can do basically and it still keeps a constants uh outside of relocations obviously if you run out of memory in your vectors but outside of relocation it's constant Distortion and delation time because it's just swap or or or or push back it's constant look up by ID because you just look for an indirection table uh it has ID stability but not pointer stability obviously if you insert or delete the memory address of a given component changes but the ID is stable you can just do pay extra look up and find it again and it has okay but not amazing deterministic deterministic iteration speed because usually what you do if you want deterministic iteration is you go you go through the lookup array basically because that one is guaranteed to have a to have like an index and you do a jum memory jump so it's not as good because you keep jumping through memory but if it's packed enough you might not pay a cost that is too high uh and also like yeah it mostly works and I did not mention that before it mostly works with trivially relocatable types so like types that do not uh like have pointers to themselves or funny things like that it's just things that you can M Copy and again I think this is a reaction thing that I that I've hear more and more in the C++ committees people going more and more to like dumb structs that you can just M Copy everywhere because you you get a lot of places where you have huge gains by the fact that the thing can just be M copied and then forgot on that right so decouple your data is basically the one thing I want you to take home uh it's good for multif Fring it's good for cash performance uh ECS is probably Overkill but try to take a look at data structures that have spawn through the systems because there is something there that can probably be taken and used in your own projects oh and furthermore um I think you build should be destroyed as usual thanks I have some reference is if you are interested uh there's the the the collection from M on the on ECS is pretty good there's the there is the the GitHub of the project and he has made a talk I think in Italian C++ or something like that uh where he Compares his ECS to like unity and he talks more about the implementation if you're interested in that stuff I'm probably too late for questions but I don't know if we're not getting kicked out I can still maybe take one or two especially if you if if you use ECS and it solve all your problems I'm interested hey hello hi I um I I've been trying to use ECS ever since I heard about it um I think it was Mike Aon talk or something yes that's the one that's often mention I mean I I immediately fail at the first thing because the very basic example always is about position and how do you do hierarchical position because that's my biggest pet pee with this yes because ECS do not give you any Cape of hierarchy right so for example the classic is people would like to put them in like quad trees or o trees or blah blah blah uh I actually have you seen this talk about the mega City in unity I did not know because he talks about it and he talks about how he managed to solve some of that hierarchical data not by introducing a new structure but by taking advantage of the fact that the way this is stored is hierarchical uh so basically having uh intermediary steps where you can basically skip over chunks of the of the of the of the object and I think the way it's done is that it probably keeps uh components into either different sets or at least sets that are together in memory so you can't use the spse uh thing because it keeps jumbling things around but it can take advantage of the fact that this this disarray you can basically do a jump like over a thousand entities because you know that the first thousand of this block then the next first next thousand are in this cell etc etc Etc but yeah I think it's called like mega mega mega City Unity thing it's a 30 minute talk it's pretty insightful cool thanks you said that the ECS concept is not that much applicable onto um Maps um did you still explore the performance benefits if you try to build a map or SL dictionary structure uh the thing about Maps is that there's only well okay actually like standard Maps there's only bad ways to make them in memory right because it's going to be a red black tree that's basically the standard can of force it doesn't tell you it's a red black tree but if you look at the requirement it is a red black tree and it's a tree which means it's a list which means you jumping pointers all the time which mean there is no way you can guarantee data locality the best you can do is try to chug on a locator next to your map and try to get the poorman's version of data locality because every item in the map will hopefully be close enough in the same memory chunks but you're still jumping through memory and you can't guarantee that you're going to act that two elements are actually next to each other they guaranteed to be in the same map which is better than what map does but it's still very far away and that's why uh if you have looked at hash tables for example that's why people keep telling uh do not use an ordered map in the standard because an ordered map is also has to be uh what they called uh buckets with with lists like they can't use what we call open addressing and open address hashes I'm getting a bit far away from the topic but hash Maps they always if you don't follow the standards to implement them as one Ray with only a few gaps in it and it's it's okay enough for iteration uh because you just you just have to skip a bunch of uh it's it's it's kind of equivalent and not as good as colony in term of skip Fields but it is still you have the guarantee that you're going through an array so as long as your as your hashmap is reasonably popular like let's say 75% or something you're mostly not you're not wasting too much memory when you're doing data when you're doing fetches although arguably you will still be fetching keys and a bunch of things that you don't care about for your actual update so it is okay but I I think this misses the point of just saying no no we want a vector of things and this is the end only thing that I'm going to fetch yeah okay thank you do we have any more questions okay so we're done uh you can come and ask me afterwards and I will happily answer anything else