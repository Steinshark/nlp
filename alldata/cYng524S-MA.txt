all right so what is this right here we've seen I do remember Java started this like 1 billion row challenge I didn't realize that a bunch of are a bunch of languages now doing this 1 billion row challenge is this like a is this like a thing cuz I mean go sounds fantastic yeah they are the the 1 billion row challenge is intended to be a fun exploration of how far modern Java can be pushed for aggregation 1 billion rows from a text file okay so they are okay so effectively it's how fast can you read and do something with one billion rows of data in a language so it's kind of like let's let's let's go to a pretty far case and then you have to do super weird programming to make it successful right all right anyways sometimes around the middle of January I stumbled across the 1 billion row challenge I had a lot of fun working on this I started uh with an execution time of larger than 6 minutes and finished at 14 seconds okay okay this sounds good what is the 1 billion row Challenge and input a text file containing temperature values for a range of weather stations each row uh is one measurement in the format of string station name float measurement output for each unique station find the minimum average and maximum temperature recorded and admit the final result on standard out in the stations names alphabetical order with the format this 14 seconds dude I'm not going to lie to you if I could if if I could last 14 seconds I mean that would be an achievement in of itself okay I'm just letting you know I I would be stoked this is fantastic by the way 14 14 seconds to do a billion rows is still very impressive okay same brother yes hell hell yeah hell yeah ja dude Java can do a 1.4 second damn what a lucky man what a lucky man uh let's see no constraints working with this temperature values are within uh effectively 100 up 100 down okay okay the temperature value has only exactly one fraction of a digit the bite length of the station is within this there will be a maximum of 10,000 unique stations rounding of the temperature must be done using semantics of i e 754 rounding Direction round towards positive I don't what is what is round towards positive does that mean you just you just always round up you effectively just seal it's in the name dummy you just seal at all times I thought we just called that seal by the way every time I say seal I just I literally cannot help but to think I every time in my head somewhere deep down I go every single time I make a weird sound like I okay that's it sounds much weirder when you do it out loud you know now that I now that I did it out loud that sounds weird okay now that I've done it out loud I feel kind of weird about it but that's right think about every single time you ruin seal for me yeah I'm going to ruin seal for you cuz now you're going to think about seal seals the whole time the wrong kind of seals okay what are you going to do I think of Batman Forever what okay I know we're like trying to stay on on on track can somebody can somebody please explain Batman can please can you explain why you think of Batman Forever when you see the word Seal Kiss From A Rose seal wow that's wow okay okay okay eight okay okay okay uh sounds simple enough what's the catch the input file has 1 billion rows how big is a billion it's it's less than a trillion I can tell you that much if you started counting to one billion at a rate of three seconds per number that would take my guess is 99 years 95 years okay I was close I thought it was 33 years per billion if it's one out a second anyways the challenge is to process the file to print out the output in the least amount of time possible uh it's summarized nicely in this picture okay yeah yeah tools I'm working with the challenge was initially in uh induced introduced for Java but folks started trying it out in different languages you can check out the discussion about the one BRC in goang I solved this using goang 1.21 all benchmarks are ran on an Apple M1 Pro Apple silicon mentioned Apple silicon mentioned apparently Sushi Dragon told me a really cool story where he is like he is exploring the Apple M1 stuff for for his kind of really crazy streaming setup and it consumes assumes like significantly less energy and it's just as fast and he's like very happy about it Apple Fanboy exposed D I don't even own an apple I actually I might I actually am curious if I do anyways a 2021 model with 16 gigs of memory and uh 10 vcpu uh the input file with 1 billion rows is about 16 GB you can find the GitHub repository with my solution here I took the approach of solving this iteratively doing so helped me track my progress you can find the iterations documented in the repos read me with each iteration I focused on optimizing one of these three areas data structure concurrency reading the file now that all of this is out of the way let's Dive In by the way I do want to do this again this would be like one of the things I'd want to do full-time content creation creation I wonder so what would be kind of fun is do you think that who do you think would produce better code Gemini Brock or jiid to solve this problem or co-pilot none probably none they would all produce shitty one I love you grug Minstrel is Minstrel a thing dolly dolly will just produce images of the weather stations you are AI I am I am I am I'm definitely not like I'm definitely I'm definitely not like not General AI I'm definitely uh I'm still learning I still have some problems like it's hard for me to self-learn we're getting places all right Baseline implementation I start with the naive implementation to establish the base uh Baseline runtime the first iteration did uh not make use of any concurrency good uh read a file line by line the input file each line can be processed independently so I started by reading the file line by line for this I used uh buff io's Scanner with the default split function scan lines a pretty standard way to read the file uh read a file line by line in go L scanner is Handy face that reads from file and see and returns contents up to the split defined for us this means a new character sln will not be returned for each line okay that makes sense so we won't have to handle it separately foreshadowing this is where the problem lies with this method of reading a file okay interesting okay AI vetle mentioned vetle mentioned did I just get a vetle thank you toaster chicken I appreciate that thank you by the way I I left alerts on boys we left alerts on alerts are on yeah I did I walked on stage it was fantastic uh data structure map to store all the temperature values recorded for each yeah okay this is what I would do as well the output requires minimum maximum and average temperatures rounded in each City so I initial initially started with a map that stored the all temperatures recorded for each unique station each station stored a string type and each temperature stored a float 64 type making this map signature map string float okay fair oh wait hold on wait no that's no no no no he said that wrong that's a that's a an array of floats you stored all the arrays you didn't just why would you store all the arrays anyways I would even think about storing the arrays I would just I would assume You' just do a summon account right I discovered your channel recently oh thank you as each line read from the file values were added to this map accordingly after the uh the entire file file contents are read and the map is constructed we iterate through each key value pair in the map and calculate the min max and average values okay so this is who here would probably create it something like this who would do something along this whole line of just reading line by line throwing it all in a map and calling it a day I think a lot of people would probably start there a lot of people would start there I would have started with the aggregate value with counting sum Min and Max I always reduce my loads when I can damn uh yeah I probably wouldn't start with an array because an array just requires more programming for me just being able to just do a quick if check as each one comes through it just feels easier if that makes sense I wonder what the I wonder what the answer is so in my head before looking forward without looking at what the the better thing to do is is that there has to be a way that you want to pass these values you want to be able to effectively have a bunch you want a bunch of threads doing something right but you need to be able to you need a way so that you can split out all these threads without now this is the big trick here you want to be able to do it without having to use a mutex right cuz that's where things get all that's where things get all terrible is once you use a mutex you then cause you cause things to slow down you don't want to do that so you could imagine that you could have like almost what it's like how I would refer to it as like a a consistent hash right so you could you could imagine that in my head how I would try to speed things up is that every city that starts with an A goes to a specific um thread that's running and each one goes to a different thread so all A's are sorted in one area all B's are sorted in in one area all C's are sorted in one area so that way there's no they can all just run independently that like at least that's how I would I would like a thread index by key exactly that's like how I would think of it I don't know if that's I don't know if that's true I don't know if it's any good but that's I I would use a some sort of consistent hash that's like that's what I would that's like that's the first thing I think of how let's see how would you load B let's see how would you balance the load then I wouldn't try to balance the load not at first right cuz I don't know if there's any sort of like you know what I mean yeah I wouldn't do this either so someone just said do this right here I wouldn't do this either so this is so the reason why you don't want to do this is that there's one billion rows you have to do 1 billion uh divided by what you do is you just have a sum and a count right if you just have a sum and a count you already know the average at the very end right so that way you only do 10,000 divides all right you don't need to balance the load first uh first up a number of go routines that is guaranteed to be greater than 2x the cores and send messages in see that's kind of what I want to do so that's kind of how I'm thinking about it right if you just have the sum and count you're good uh won't the sum possibly overflow use a 64 then well you got to remember look at this uh will the sum overflow there's some rules to it which is the temperatures between 999 and positive 999 so that means you already know that you can only be less than you know you won't be any less than negative 100 and greater than 100 so a billion of those would be 100 billion right if it was all positive numbers in one location that's 100 billion that means you only need like what a 38 bit number to store all those so a 64 is plenty right it overflow on an in32 but you're using a float 64 anyways I just use the middle out strategy everyone's pretty much favored okay let's keep on going but that's how I would want to do it all right concurrency processes uh each mix let's see stations min max and average temperatures in separate go routines okay the first place I introduced concurrency was the last stage of execution for each city in in the map I instantiated a new go routine to process the city's min max and average temperatures oh that's clever okay so he went with literally just do it on a per City basis okay interesting uh I code up to this point can be found here so in my head I'd rather use a uh a fix sized array this improved performance by 100 seconds oh my goodness oh my goodness can someone drop the article for this yeah this is a great this is a great this is a great article 100 seconds that's pretty impressive okay okay I like it uh this is inefficient because we're spinning up too many go routines a maximum of 10,000 uh one for each station the go scheduler is spending more time managing the go rains than actual work we will fix this in the future iterations okay okay concurrency decoupling reading and processing of file contents currently we are reading a line from the file part parsing the station name and temperature and adding it to a map and then reading the next line doing this sequentially means that we are not taking advantage of all the CPU cores instead we are reading a line waiting to finish processing it before reading the next line to overcome this I decoupled reading and processing of lines I introduced two go routines a producer go routine responsible for scanning lines and a consumer go routine uh to process reading these lines okay that makes sense okay so now this is we're getting closer we're getting closer sup what's up baby I like like where this is going this is a good idea to communicate between these two go routines uh send the read lines from the producer to to the consumer go routine I used a Channel all right channels are blocking the best way explained by the concurrency and go by Katherine uh Cox budet all any go routine that attempts to write to a channel that is full will wait until the channel has been emptied any go routine that attempts to read from a channel that is empty will wait until the uh one item is placed in it I just assume you make your channel big enough right isn't that like just like an easy way to kind of avoid it this means if we don't use a a buffer channel uh when one go routine is executing the other will be blocked okay I'm a genius I'm a genius let's see using an unbuffered channel the execution time indeed increased twofold nice CPU profiling the code we can see uh the most amount of time is going in go routine switches okay so we pretty much just sit there and is that what this is so I don't know what these things are that are I can't read this cuz I don't understand it I guess let's see go ready Funk is go ready Funk that one or is it this one oh schedule I guess yeah runtime. schedule yeah it looks like runtime schedule and runnable is just like just like getting whammed right there W this makes sense in Alliance with our understanding of unbuffered channels okay using a buffer Channel using a buffer Channel with capacity 100 I just why why stop at 100 okay like why stop at a 100 why not a thousand it's a billion right anyways using a buffer Channel with capacity 100 we see the performance increased by 50% compared to the unbuffered channel so that means you're at 4 and a half minutes so you're back to like nominal okay that does not look like 50% there buddy okay that's not 50% there I don't know if you guys can see that but 5 minutes and 22 seconds is not 50% of 9 minutes and 12 seconds okay we're doing some loose we're doing some loose ma some loose math here all right make channel one billion all right the code and profiles I'll point to can be found here okay we I feel like all this is like making sense cuz we haven't done anything different really like if you really think about it we haven't done anything that is fundamentally changing how much work he's doing uh but this still got slower than the previous iteration looking at the CPU profile we noticed that there's a sign significant time going into uh runtime Channel receive people are just hanging out doing nothing let's see what are we looking at what are we looking at I do like okay I do want to take a pause here for a quick second I really like that he's taking the time and even though I feel like I could just improve this immediately he's not doing what he thinks is faster he is first profiling and then thinking about what he should do so you can see right here read file line by line into map is going real slow and channel receive is a huge portion of it right here right and then you can see this one right here a lot of P thread condition weight a lot of time spent in P thread condition weight and you sleep and this so these all seem very excited if I remember correctly if you keep a nice discreet uh stack size like 100 go will keep the references on the stack which makes it much faster oh okay that's good to know I like this setting a slice of lines uh on the channel one way to reduce the number of items we send on the channel is to chunk a few lines together in a slice and then send it over to the channel this means the Chan Channel type will change from string to string array okay okay buffer channels this this this seems like a good idea since the channel type is slice to avoid race conditions we need to create a copy of the slice to send it over the channel alternatively we can use sync. poool and reuse the memory and the limitation let's see in limit memory allocation I would just one would just simply assume that creating a new array might be easier I don't see why anyways whatever go has a handy uh data race detector which can be used by adding dash race flag uh when running your code okay look at that I don't I don't even want to know about all this all this code uh code after these change let's see changes in the State uh running this execution time comes down by 160 seconds now this is real this is good if go is keeping things uh on the stack then memory allocation shouldn't be a huge concern I'm not sure you I don't think you can keep this on the stack can you I'm not really sure how that would work with passing stuff through channels and keeping things on stack and stuff like that in my head I don't I don't I'd have to write it myself pretty much to understand why that works because I don't it'll only keep the references on stack Judo isn't all references always on stack unless if you have unless if you have a double pointer how do they keep how do they keep a reference on the on the Heap because don't you need a reference from the stack to look it up on the Heap Judo now you're now you're damn confusing me Judo okay something has to be on the stack anyways data structures use uh N64 instead of float 64 really at this point I add a test in the CI and realized my tests were failing due to how I was rounding according to the constraints rounding should be done using the semantics of iple E 754 rounding detection round towards positive I fixed this by parsing the temperature string into int and then doing a summation int and converting it to a float 64 only after the calculations have been done this ended up improving considerably by almost 40 seconds wow that's a lot this ended by I'm surprised by that was surprised to see that there's such a significant performance improvement with this change I guess parsing a float is significantly more complicated than parsing an INT I think that makes I think that makes sense now that I think about that I think that probably makes a lot more sense that parsing a parsing an INT has to be easier well because floats can be like represented in all sorts of different I mean there's so much to floats it's about CPU registers I mean parsing a float is more work yes I assume there's there's because an INT is just literally it's just a specific character range with the minus sign right floats have more the data is fixed uh fixed Point yes but I'm just saying an INT is but I can't imagine it's that much anyways I don't I don't know I don't get it results will wild very uh widely based on CPU architecture individual ALU FPU performance uh as well as actual numbers of alus and F fpus available per core in super scaler designs which influence how many independent operations can be executed in parallel this means your hardw will play a major factor in determining how much this change will contribute to Performance Improvement a classic it depends moment there we go look at that there we go that's the data structure we were talking about I knew this would be the best I just knew it uh in the Baseline this feels like a very obvious one right and it also feels easier honestly cuz then you don't have to you don't have to take you don't have to take a value read it put it into an array and then later Rego over all the values and keep track of all four of those things it said you can have that all as one operation which just makes it seem like it's way easier right in the Baseline implementation we're using a map of uh string to float uh float array where the each station we are storing all the temperatures recorded this is wasteful as we don't actually need to store the temperatures and it's just honestly simpler we can uh simply store the minimum maximum sum and count of all temperatures with this change we'll see performance improvements for two reasons one decreased memory allocations this will go from storing a slice of around 100,000 uh in 64 items to uh let's see more or less equaling whatever this is to ex storing exactly four oh really you're not doing an object you're doing uh you're doing like a little array okay I mean I guess that makes sense I like by the way I love this kind of stuff when you when you toss in an array like this I like it I like that uh this significantly decreases our memory footprint decreasing the number of go routines in the last step we can get rid of spinning up go routines to process temperatures of each station as we already processing the min max uh and count values while constructing the map itself this means the go schedule needs to worry about significantly lesser number of go routines making this change uh the execution time went down by 70 seconds which is a huge amount at this point let's see code and profiles uh till this point can be found here okay look at that we're getting low we're getting low I like this optimizing all three read chunks instead of one line at a time oh yeah so in other words my guess is this is just like take a file and read out whatever is like Optimal to read out I don't know what I don't know if there's like an optimal amount of space you should read from a file but you can instead of having something that scans it in right you can just read a huge chunk and then you can just do it yourself uh in the Baseline implementation we use buff iio scanner and read by file contents line by line while this is a handy interface it reads the file contents performs some checks and then iterates over it and returns a single line without Whit space character if we read each file uh in chunks it will help uh performance in two ways single iteration over the btes when parsing the city temperatures we will avoid iterating over bytes that scanner F internally does yep you get yeah you get a little you get some nice ones right there it depends on the situation 128 is a good General value oh interesting okay hey hi hi from YouTube hi this makes sense because you also get you I mean both are o of n but n drops a constant right to reduce the number of items sent over the channel we will be sending 100 lines together in a string slice we are sending one uh 10,000 items over the channel if we read 64 megabytes uh Chunk from the file and send it over the channel That will be 256 items very significant reduction okay this is this is actually pretty neat I like this to process each chunk independently we should end uh in a new line we can do this in two ways after the chunk is read read until the next new line concatenate to the two bites and send it over the channel slice uh the read bites uh till the last new line the leftover chunk can be sent along with the next chunk read that's how I do it right here I would assume that you could just have an array that already exists and just mem copy it into your like temp array and then you don't have to like you don't have to do something clever like this because that's way too clever it's way too clever trying to do making it work out I first went with the first option as it was more clean to write and required less slice copying to implement I decided to use a buffered IO buff reader uh I read a file in 64 megabyte chunks let's see uh to read till the new next line character I use read byes method this did not improve performance as read bites method again iterates over the characters to find the delimiter but there's something more both scanner scans method and reader read method internally calls OS read but as they provide more functionality Beyond simply reading the file they do extra processing on top of it look at the implementation for each okay so they do stuff they do stuff for our use case we really don't need these convenient helper interfaces I can directly just call Os read this all makes sense this is great this is a great step- by- step like how to reduce stuff and a lot of this stuff is I think everybody in this Channel right now like anyone in this channel find like a lot of this stuff too confusing or does this all just make sense because this feels like a really great this feels like a really great simple optimization problem where none of it is like wild optimizations and this is also really great like you can just watch the person's thought process which I think is just fantastic confusing for me because I'm kind of stupid I know but like which part is confusing like you could imagine that using something that reads over your data multiple times is not as fast as something that can just read the data out and give you each one of those chunks like you're reading once versus reading twice co-routines are confusing but just because I don't know go okay fair I guess it sounds like a really good and thorough thought out uh commit uh I still have no idea what you're talking about though it sounds absolutely amazing so it's kind of like rings of power absolutely amazing really well thought out everything is fantastic most average piece of stuff you've ever seen that's how I think about about it it really that wasn't a very good analogy but I just wanted to make fun of rings of power it was just like that's really all I really wanted to do I'm just I that's actually just what I wanted to do anyways hey Nightshade dude thank you 51 months of caring dude hype trade incoming can you guys hold on I'm in the process of reading something all right hold on let's look at this let's look at the last one hold on people uh since we are now sending chunks over the chunk channel the chunk consumer uh go routines are the first spitting let's see splitting the chunks into lines processing each line and sending it over to the line channel the line consumer Channel finally constructs the summarized map these chunk consumer go routines uh can work in parallel as they're not adding values to the map directly to take advantage of all the CPU cores I spun up a number of vcpus minus one of Chunk consumer go routines each concurrently taking chunks from the channel processing it and adding lines to the line Channel okay interesting so you'd have is he saying you have a read channel that just like sends a bunch of of data through then you have like a chunk Channel yeah that makes sense I think that makes sense right for so for each each each step that would make sense if you could just read you could get like 64 megabytes send that 64 megabytes over and then you start reading again which takes a nonzero amount of times and while that nonzero amount of time is going you are processing and creating new lines right there which I think is fantastic gluns okay guys I'm still in the middle of doing this thank you for the hype tra hold on I'll say thank you in a moment I mean that that that this that makes sense right the chunk Channel consumed by the chunk consumer go retains 250 items the line slice Channel over the line Channel constructs the final map okay the total number of items send and receed from all channels is around that okay uh to reduce this each chunk consumer go routine can process a chunk into a Min summarized map this map can be sent over to the map channel the final map can be created by combining the Min summarized Maps H uh 250 uh let's see 250 uh file chunks sent over the chunk Channel plus 2 50 Min summarized map Senter to the map Channel I'd like dude I want to explore this so much uh Implement they finally able to get it to 28 seconds okay improving uh string to N64 parsing looking at the flame graph we still see a considerable amount of time and going into this does isn't this just like emotional by the way that now you're getting to this level of improvement because if you look at this there is quite a bit right here right so 8 1% that's a lot initially I used a stir conver parse int to convert the string to un 64 uh looking at the implementation of stir conver parse int it does a lot of checks that we don't necessarily need ourselves like one thing about this data is you know it's all correct so technically you could read until the point and if it's always Round Up do we even need the rest wouldn't you just read till semicolon then read until period like couldn't you honestly just do that there's uh there is our Arc oh is there an arc mutex is there an arc mutex hashmap at the end reading sequentially will always be better than spawning uh n threads yeah I mean I want to play around I want to play around what about uh X point0 then you need to make sure that that like yeah yeah negative 1.5 needs to be one exactly it's kind of interesting there seems to be something that's very interesting there which is if it's negative you don't add one if it's positive you check to see if the last item of the point is a zero or not if it's a zero let's see uh 0.9 will turn into zero not one that's not rounding up is it that doesn't seem right that doesn't seem that doesn't seem right because the whole thing was rounding up it had a very specific thing you always round towards positive Infinity right all right let's see further optimizations I only got this far with the challenge deadline January 31st there's a lot more to explore some ideas with the latest CPU uh Trace flame graph the most of the time now seems to be going to map access and assign potential ways to optimize this using numeric keys in a map so one thing you know is that you have alphabetically sorted uh you need alphabetically sorted items and you have 10,000 I wonder if there's something there that could make sense with being able to put them into I'm very curious about that like how can you make it so you don't have to look stuff up in a map I don't know I don't know yet I don't I have no idea a tree would be sick something like that but a tree I assume would be slower than a map you look like someone that was in the military I was not but thank you though um anyways replace in built-in maps with fast string Maps or swiss Maps I don't even know what a Swiss map is uh is it a very like neutral map replace map uses which the tree data structure I wonder I wonder how that would work because how do you store the tree data like what do you store it in don't would you store a isn't I guess yeah if if your if your node was an array of 26 slots right then yeah because then you don't then you don't need to do that because then you literally just take a uh you literally just take your your character and you look it up in that position offset into it does that make sense a tree this is called a tree a tree is just like Auto autocomplete a tree would be really good for inserting but might be awful for printing out the final answers yeah because you have to Traverse the tree and you'd have to construct those things unless if you have the leaf node contain the constructed values I hadn't used unsafe uh so far because I wanted to see how far I can get without it turns out a lot map uh can be used let's see can uh be used to get better results than IO speed uh methods from Go's unsafe package can also be used for string and bite manipulation okay interesting I had a lot of fun working on this massive shout outs to Gunner morling for putting this together uh challenge together by the way this was awesome this was fantastic uh that challenge I cannot believe how great this challenge was I started with a rather impressive execution time of greater than 6 minutes and brought it down to 14 seconds I actually I I want to do this because I don't I have never used any of like I haven't really used a lot of go like this is this seems like a great way to learn the language you want to learn because I want to learn more go go is kind of like the language I want to learn so this is a uh this is on my this is on my [ __ ] list now it is well hey go follow it on Twitter go follow boom followed amazing absolutely out of control this was fantastic we'll link I I'll try to make sure I link all of that in the uh on the stuff all right hey the name you know what the name is you know what the name is hey prate it is prate the prate the prate agen the billen the one billion aen I don't know what what is the name what is the name what is the name what is the name the name is the prime gen thank you very much spell with an a at the end thank you