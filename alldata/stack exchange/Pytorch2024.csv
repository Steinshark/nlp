Post Id,Parent Id,Title,Body,CreationDate,Score,PostType
"79629193","78328401","","<p>I had a similar issue, and with <code>pip</code> it got resolved:</p>
<p>e.g., <code>pip install torch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1</code></p>
","2025-05-19 17:07:52","0","Answer"
"79628280","78097861","","<p>By just installing <code>soundfile</code> using following command, my issue resolved</p>
<pre><code>pip install soundfile
</code></pre>
","2025-05-19 07:28:42","0","Answer"
"79621882","79082850","","<p>Based on my experience the reason for this error is accelerate python library not installed relevant python environment. This way help me to resolved the issue.</p>
<p>(1) First I run this command in my JupyterLab Desktop notebook cell.</p>
<p><strong>import sys</strong></p>
<p><strong>print(sys.executable)</strong></p>
<p>(2) Then this command gives you to path of relevant environment. Now you can  installing  accelerate directly into the JupyterLab Desktop environment using the following command. You should enter your path instead of below command <strong>your path</strong> word.</p>
<p><strong>!your path -m pip install &quot;accelerate&gt;=0.26.0&quot;</strong></p>
<p>After that restart your JupyterLab Desktop and check.</p>
","2025-05-14 16:12:00","0","Answer"
"79621033","78846880","","<p>Unfortunately PyTorch does not respect <code>requires_grad=True</code> at an individual index level — it works <strong>per tensor</strong>. So even if you do <code>weight[idx].requires_grad = True</code> as mentioned in <a href=""https://stackoverflow.com/users/2166778/elmeromero"">@elMeroMero</a>'s answer it may not actually isolate gradients to those indices.</p>
<p>A possible workaround would be to use gradient masking. For instance the following might do the trick.</p>
<pre><code>for param in model.parameters():
    param.requires_grad = False

embedding_weight = model.get_input_embeddings().weight
embedding_weight.requires_grad = True

# Register hook to zero out gradients for all but new token IDs
def mask_gradients(grad):
    mask = torch.zeros_like(grad)
    mask[new_token_ids] = 1.0
    return grad * mask

embedding_weight.register_hook(mask_gradients)
</code></pre>
","2025-05-14 08:25:12","0","Answer"
"79606644","78165875","","<p>It might not be like that (about the position of <code>broadcast</code>). I've recently run into the same issue and figured out the solution as below:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.zeros(shape0)

if accelerator.is_main_process:
    x = &lt;some computation here&gt;

x = broadcast(x) # broadcast does a sending-or-receiving job. it should be out of the if block
</code></pre>
","2025-05-05 09:21:07","0","Answer"
"79585944","78008119","","<p>I had the same problem (none of the above worked for me). I managed to solve it by simply assigning device_map=&quot;auto&quot; when loading the Hugging Face model:</p>
<pre><code>original_model = AutoModelForCausalLM.from_pretrained(model_name, 
                                                    device_map=&quot;auto&quot;, ##Here
                                                    quantization_config=bnb_config,
                                                    trust_remote_code=False,
                                                    token=True)
</code></pre>
","2025-04-22 07:47:16","0","Answer"
"79584899","78347092","","<p>This is thrown by the <code>timm</code> package. Checkout <a href=""https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/_hub.py#L123"" rel=""nofollow noreferrer"">this</a> line in the source code.</p>
<p>For me it was thrown for version <code>timm==0.5.4</code>. Upgrading the latest version (via <code>pip install --upgrade timm</code>) solved the issue.</p>
","2025-04-21 15:39:55","0","Answer"
"79569005","79022620","","<p>So I thought to rather address this question myself, months after I figured it out myself, in case someone comes looking for an answer to this stupid sounding, yet critical question.</p>
<p>AMP, or automatic mixed precision, as the name suggests, allows you to use whatever accelerators you have to run the workload, the catch being each accelerator would execute the workload in its own supported precision (FP32, FP16, mixed precision, etc.). So, to answer the question, until you are using cuda to program and assign the workloads directly to a hardware accelerator, it is not possible to assign it just to a specific hardware. The only thing you do is that you can specify what bare minimum precision you want and the NVIDIA compilers would themselves assign the workloads to all the compatible hardware in order to get the workload executed with the required precision.</p>
<p>Final Verdict: You cannot individually toggle any portions of the NVIDIA Hardware on or off.</p>
","2025-04-11 13:55:03","1","Answer"
"79541911","77802368","","<p>I downgraded my <code>pytorch_lightning</code> package to <code>1.9.0</code> and it works</p>
<p>Originally I was using <code>pytorch_lightning 1.5.10</code> with <code>torch 2.3.1</code> and it gives me this error</p>
<p>Then I upgraded my <code>pytorch_lightning</code> to 2.4.0 and it returned me with another error</p>
<p><code>AttributeError: type object 'Trainer' has no attribute 'add_argparse_args'</code></p>
<p>Finally I downgraded <code>pytorch_lightning</code> to <code>1.9.0</code> and it works</p>
<p>I hope this will help u out</p>
","2025-03-28 16:08:45","0","Answer"
"79512332","78320397","","<p>go to <a href=""https://github.com/Purfview/whisper-standalone-win/releases/tag/libs"" rel=""nofollow noreferrer"">https://github.com/Purfview/whisper-standalone-win/releases/tag/libs</a> download <code>cuBLAS.and.cuDNN_CUDA12_win_v2.7z</code> and add it do your cuda bin</p>
","2025-03-16 09:06:35","0","Answer"
"79509513","78346857","","<h1>TL;DR</h1>
<p>Try an older miniconda installer version and use conda (yes it seems crazy but it changes which GLIBC was used to build wheels, and thus solves dependencies issues).</p>
<h1>The story</h1>
<p>I ran into a similar problem and found a surprising solution, after a large amount of time spent on this.</p>
<p>I was trying to install <code>pgy_lib</code> , <code>torch_sparse</code> and <code>torch_scatter</code> into a cluster environment (no sudo rights), for which GLIBC version is <code>2.17</code> (when some recent builds need <code>2.27</code>). Using pip, I was always running into this GLIBC problem.</p>
<p>Using conda, I initially had this problem:</p>
<blockquote>
<p>Pins seem to be involved in the conflict. Currently pinned specs:</p>
<ul>
<li>python=3.13</li>
</ul>
</blockquote>
<p>downgrading with:
<code>conda install -c conda-forge python=3.12.9</code>
was only postponing the problem, to issue more incompatiblity issues (more details below).</p>
<p>The solution for me was to use an older version of the (mini)conda installer: switching</p>
<ul>
<li>from <code>wget https://repo.anaconda.com/miniconda/Miniconda3-py312_24.9.2-0-Linux-x86_64.sh</code> (or a more recent one, sadly called latest today, and being conda25)</li>
<li>to <code>wget https://repo.anaconda.com/miniconda/Miniconda3-py311_23.10.0-1-Linux-x86_64.sh</code>
got all my problems solved: no pinning and no dependencies issues.</li>
</ul>
<p>In practice, the older installer uses an older python version, but <strong>probably also uses wheels that were compiled using an older (good for my system) version of GLIBC.</strong></p>
<h1>Details of my errors</h1>
<p>Using the newer miniconda installer and downgrading python (given the first error that says &quot;Currently pinned specs: - python=3.13 &quot;) to python=3.12.9, I then got:</p>
<blockquote>
<p>(..)
LibMambaUnsatisfiableError: Encountered problems while solving:</p>
<ul>
<li>nothing provides __glibc &gt;=2.28,&lt;3.0.a0 needed by cudnn-9.8.0.87-h81d5506_0</li>
</ul>
<p>Could not solve for environment specs</p>
<p>The following packages are incompatible</p>
<p>├─ pin on python 3.12.* =* * is installable and it requires</p>
<p>│  └─ python =3.12 *, which can be installed;</p>
<p>├─ pyg-lib =* * is not installable because there are no viable options</p>
<p><strong>(...many many lines...)</strong></p>
<p>Pins seem to be involved in the conflict. Currently pinned specs:</p>
<ul>
<li>python=3.12
Note that the error messages are not helping much, at least the last one does not.</li>
</ul>
</blockquote>
<p>This solution allows to avoid having to build from source.
It seems that what is missing is wheels that were built using an older GLIBC version.</p>
<h1>My ready-to-use solution:</h1>
<p><code>wget https://repo.anaconda.com/miniconda/Miniconda3-py311_23.10.0-1-Linux-x86_64.sh bash https://repo.anaconda.com/miniconda/Miniconda3-py311_23.10.0-1-Linux-x86_64.sh</code>
(and accept all )</p>
<p><code>srun --gres=gpu:1 --time=4:00:00 --pty bash</code>  ## go somewhere with a GPU to install torch</p>
<p><code>conda create -n fl23 ; conda activate fl23</code></p>
<p><code>conda install -c conda-forge torchvision torchaudio</code></p>
<p><code>conda install -c conda-forge -c pytorch -c nvidia pytorch-gpu pytorch-cuda=12.4</code></p>
<p><code>python -c &quot;import torch; print(torch.__version__);print(torch.version.cuda)&quot;</code> #=2.5.1.post303    11.8  (initial versions)</p>
<p><code>conda install -c conda-forge pytorch_geometric pytorch_sparse pytorch_scatter pyg-lib</code></p>
<p><code>python3 -c &quot;import torch; print(torch.__version__); print(torch.version.cuda)&quot;</code> #=2.3.0.post301 12.0 (install pyg &amp; co. downgraded torch a bit, it's ok)</p>
<p><code>python3 -c &quot;import torch_geometric; print(torch_geometric.__version__)&quot; </code>  # 2.6.1</p>
<p><code>python3 -c &quot;import pyg_lib; print(pyg_lib.__version__)&quot; </code> # 0.4.0</p>
<p><code>python3 -c &quot;import torch_scatter; print(torch_scatter.__version__)&quot; </code> # 2.1.2</p>
<p><code>python3 -c &quot;import torch_sparse; print(torch_sparse.__version__)&quot; </code> #  0.6.18`</p>
","2025-03-14 15:25:16","0","Answer"
"79507784","77811886","","<p>I also got tripped up on getting the correct input_size parameter to get summary() to work. In my case, I am not using CNN. The input are simple vectors. I think the example was using a tuple of 3 items, and I kept getting error. After seeing this, I was able to fix my issue:</p>
<p><code>```python</code></p>
<pre><code>import torch
import torch.nn as nn
import torchsummary

# Define the neural network model
class CustomModel(nn.Module):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.fc1 = nn.Linear(100, 50) # input is 100
        self.fc2 = nn.Linear(50, 20)
        self.fc3 = nn.Linear(20, 10)
        self.fc4 = nn.Linear(10, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x

model = CustomModel().to(device)
print(model)
torchsummary.summary(model, (1, 100) ) 
</code></pre>
<pre><code></code></pre>
","2025-03-13 22:39:53","0","Answer"
"79494349","78579409","","<p>For loading I use:</p>
<pre class=""lang-cpp prettyprint-override""><code>torch::serialize::InputArchive in{};  
in.load_from(&quot;model.pt&quot;);  
model-&gt;load(in);
</code></pre>
<p>If your model is purely created and saved in C++ as it seems to be the case.</p>
<p>But you need to delete your model if you change it's layout - because if your layout is different (aka different layers) when you load and try to use the old one it will crash.</p>
","2025-03-08 12:06:58","0","Answer"
"79493731","78097861","","<p>I had same issue with demucs which uses pytorch<br />
---<br />
I needed to install both `torchaudio` and `soundfile` in conda isolated environment</p>
<pre><code>pip install torchaudio soundfile
</code></pre>
","2025-03-08 01:20:36","3","Answer"
"79491539","79266767","","<p>Of course you are having troubles with indices mismatch between <strong>node feature</strong> matrix and the <strong>edge_index</strong>.</p>
<p>The edge index must be a tensor with shape <code>(2, number_of_edges)</code> and with values <code>&lt; num_nodes</code>.<br />
Each column of the edge index represent and edge and it is used to access the matrix of node features through the convolution process.</p>
<p>Probably, in the program you are running, you have 1000 nodes, and you didn't aligned edge indices correctly because you removed node features without updating the edge index or added nodes to the edge index without updating the node features.</p>
<p>It is very important that indices of edge index are aligned and consistent with node features, if not, you must add an offset to node features or normalize edge indices depending on what is your issue:</p>
<p>I usually do something like this on dim <code>0</code> or <code>1</code> to normalize <code>src</code> or <code>dst</code> of the edge index:</p>
<pre><code>_, edge_index[0] = torch.unique(edge_index[0], return_inverse=True)
</code></pre>
","2025-03-07 07:32:59","0","Answer"
"79474609","78620402","","<p>I believe the size of each annotation varies each batch that is why it is complaining. Need to make a collate_fn.</p>
","2025-02-28 05:42:35","0","Answer"
"79472751","78606054","","<p>Re-install torch and torchvision via your local <code>.whl</code> file.</p>
","2025-02-27 12:51:34","0","Answer"
"79470311","79127383","","<p>A common gotcha is python3 versus python. Ensure you are calling the correct one when running your install command ('pip install torch' v. 'pip3 install torch').</p>
<p>You can verify which you are running my with 'python3 --version' v. 'python --version'</p>
","2025-02-26 15:49:22","0","Answer"
"79462844","78681145","","<p>I had this same problem, and I sorted it out by upgrading my torch version from 1.x.x to 2.x.x</p>
","2025-02-24 08:25:05","0","Answer"
"79457555","78734964","","<p>I just came across same problem. I ended up with running smaller models (fewer params).</p>
<p>Because what happens is that the <code>VRAM</code> is being fully occupied by this large model and thus parts of the model are forcibly shifted to run on the <code>RAM</code> and on the <code>CPU</code>.</p>
","2025-02-21 13:39:50","0","Answer"
"79447041","78239566","","<p>This worked for me on Windows 11. Download pytorch with support for cuda (this example is for cuda 12.8), then download faiss-gpu build for windows 64:</p>
<pre class=""lang-bash prettyprint-override""><code>pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128

conda install -c conda-forge faiss-gpu
</code></pre>
","2025-02-18 03:17:23","0","Answer"
"79444291","78606054","","<p>I did solve it with:</p>
<pre><code>pip install torchvision
</code></pre>
","2025-02-17 02:40:05","0","Answer"
"79438138","78717341","","<p>Experienced the same problem.</p>
<p>I noticed that with pytorch backend the GPU memory is ~10x smaller, so I increased the batch size to be 16x, then the training speed is 16x faster. Now comparable to the TensorFlow backend (however, the GPU utilization is still low, ~3% vs ~30% with TF).</p>
<p>NOTE: increasing the batch size may affect training quality, which is yet to be compared.</p>
<p>I suspect batch size with pytorch backend has different semantics than the traditional Keras semantics. See here: <a href=""https://discuss.pytorch.org/t/solved-pytorch-lstm-50x-slower-than-keras-tf-cudnnlstm/10043/8"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/solved-pytorch-lstm-50x-slower-than-keras-tf-cudnnlstm/10043/8</a></p>
","2025-02-14 01:32:08","0","Answer"
"79434845","78523154","","<p>you might try calling your training script like so:</p>
<pre class=""lang-bash prettyprint-override""><code>PYTORCH_ENABLE_MPS_FALLBACK=1 python train_me.py
</code></pre>
<p>that would make sure that the variable is set when you pytorch goes looking for it :)</p>
","2025-02-13 01:08:26","1","Answer"
"79426064","79026181","","<p>Maybe this can help</p>
<p>The function <code>from_pretrained()</code> has a parameter device_map that automatically distributes the model with device_map=&quot;auto&quot; to your GPUs.</p>
<p>change</p>
<pre><code>AutoModelForCausalLM.from_pretrained(model_name)
</code></pre>
<p>to :</p>
<pre><code> AutoModelForCausalLM.from_pretrained(model_name, device_map=&quot;auto&quot;)
</code></pre>
","2025-02-10 04:13:29","2","Answer"
"79423447","78097861","","<p>Install <code>ffmpeg</code> as suggested here: <a href=""https://pytorch.org/audio/stable/installation.html#optional-dependencies"" rel=""nofollow noreferrer"">https://pytorch.org/audio/stable/installation.html#optional-dependencies</a></p>
<p><code>conda install -c conda-forge 'ffmpeg&lt;7'</code></p>
<p>Then restart kernel and retry again.</p>
<p>Worked for me, though I'm on macbook.</p>
","2025-02-08 15:34:35","0","Answer"
"79388558","77960052","","<p>I encountered the same question after I installed cuda 11.7 for another project. Reducing batch size couldn't solve it.</p>
<p>My cuda version is 11.7 but in <code>Pytorch</code> 11.8 is installed by <code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code>.</p>
<p>Install the proper version of cuda may be helpful.</p>
","2025-01-26 12:57:38","0","Answer"
"79384956","78660414","","<p>The issue boils down to the <code>Formatter</code> class code in <a href=""https://github.com/pytorch/pytorch/blob/main/torch/_tensor_str.py"" rel=""nofollow noreferrer""><code>_tensor_str.py</code></a>  and your use of <code>sci_mode = False</code>.</p>
<p>When determining the width for each element in the tensor representation, the code first determines whether scientific notation should be used based on some numerical criteria:</p>
<pre><code>if (
        nonzero_finite_max / nonzero_finite_min &gt; 1000.0
        or nonzero_finite_max &gt; 1.0e8
   ):
        self.sci_mode = True
</code></pre>
<p>then determines the max element width, assuming scientific notation is used. Finally, the code overwrites <code>self.sci_mode</code> based on <code>PRINTOPTIONS</code>. So, in the case that you specify <code>self.sci_mode = False</code> but the above criteria are met, the code will allocate enough spacing per element to accommodate the scientific notation representation, but will not actually use the scientific mode representation.</p>
<p>A brief example:</p>
<pre><code>torch.set_printoptions(sci_mode=False, precision=4, linewidth=200, profile=&quot;full&quot;)
arr = torch.rand(100,100)

print(arr)                     # prints with extra space because max_element/min_element &gt; 1000
arr = arr.round(decimals = 3)  # max element is now strictly &lt; min_element*1000
print(arr)                     # prints with correct spacing
</code></pre>
<p>The issue has been <a href=""https://github.com/pytorch/pytorch/pull/126859"" rel=""nofollow noreferrer"">raised on github</a>, addressed by an external contributor, and closed as of May 2024, but is awaiting review. You can go and add a comment expressing your interest in it being reviewed and maybe we can get it merged!</p>
","2025-01-24 16:17:59","0","Answer"
"79378005","79284426","","<p>The main issue could be that your model is too large, so you should first check the size of your model.</p>
<p>For example, a model with 8B parameters will require approximately 32GB of memory in fp32 precision or 16GB in fp16/bf16 precision.</p>
<p>If your GPU memory is insufficient, you can consider using CPU offloading. However, if your DRAM is also inadequate, disk offloading can be employed to manage memory constraints, but disk offloading is very very slow.</p>
<p><a href=""https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#loading-big-models-into-memory"" rel=""nofollow noreferrer"">https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#loading-big-models-into-memory</a></p>
<p>Moreover, during training, additional memory is consumed for storing gradients and optimizer states.</p>
<p>For instance, if you use mixed-precision training (bf16 for parameters/gradients and fp32 for optimizer states), the memory requirements increase.</p>
<p>For a model with 8B parameters, the total memory usage would be approximately:</p>
<ul>
<li>16GB for parameters (bf16)</li>
<li>16GB for gradients (bf16)</li>
<li>96GB for optimizer states (parameters, momentum, and variance) (fp32)</li>
</ul>
<p>In total, this setup would require around 128GB of memory. If this exceeds your hardware capacity, consider advanced memory management techniques, such as gradient checkpointing, Paged optimizer, etc.</p>
","2025-01-22 14:03:40","0","Answer"
"79361949","79267821","","<p>I've had the same issue, turned out the actual DLLs weren't in the path.</p>
<p>Try copying them to the output directory with a custom target.</p>
","2025-01-16 14:24:47","0","Answer"
"79357227","78583802","","<p>though its been 7 months since the question has been asked, but still answering it you or someone else might need it</p>
<p>as mentioned by Christoph Rackwit, to sum over the instances, i would be using the same method to calculate the total number of pixels, along with the code mentioned by you to find the total pixels for each instance, dervied from instance segmentation</p>
<ul>
<li>so first we extract the predicted class, which are basically index values (pred_classes) and predicted masks (pred_mask), the order of these both tensor values are the same</li>
<li>then we iterate over pred_classes and pred_masks and using a dict we add the previous sum of pixel of a particular instance to the current</li>
<li>then store the class names used for training in a list</li>
<li>lastly iterate over the dict to using the list of classes and the pred_classes index values, <strong>we find the total number of pixel for each class</strong> ( containing multiple instances )</li>
</ul>
<pre><code>import locale

pre_classes = MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).thing_classes # this contains the class names in the same order used for training, replace it with your custom dataset class names
masks = predictions[&quot;instances&quot;].pred_masks # this extracts the pred_masks from the predicitons
classes = predictions[&quot;instances&quot;].pred_classes.numpy().tolist() # this extracts the pred_classes (contains index values corresponding to pre_classes) to a simple from the predicitons

results = torch.sum(torch.flatten(masks, start_dim=1), dim=1).numpy().tolist() # this calculates the total pixels of each instance

count = dict() # create a dict to store unique classes and their total pixel
for i in range(len(classes)): # itearte over the predicted classes
    count[classes[i]] = count.get(classes[i], 0) + results[i] # add the current sum of pixel of particular class and instance to the previous sum of the same class, adds 0 if the class didnt already exist 

locale.setlocale(locale.LC_ALL, 'en_IN.UTF-8') # set the locale to Indian format
for k, v in count.items(): # itearte over the dict
    print(f&quot;{pre_classes[k]} class contains {locale.format_string('%d', v, grouping=True)} pixels&quot;) # printing each class and its total pixel, pres_classes[k] for accessing corresponding class names for class index, locale.format_string for formating the raw number to Indian format
</code></pre>
<blockquote>
<p>i used the predefined model to perform instance segmentation on the following image</p>
</blockquote>
<p><a href=""https://i.sstatic.net/0bTdw63C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0bTdw63C.png"" alt=""Input Image"" /></a></p>
<blockquote>
<p>which resulted in the following image</p>
</blockquote>
<p><a href=""https://i.sstatic.net/Qs2q8BJn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qs2q8BJn.png"" alt=""Predicted Image"" /></a></p>
<blockquote>
<p>which also produced your required results</p>
</blockquote>
<pre><code>dog class contains 1,39,454 pixels
cat class contains 95,975 pixels
</code></pre>
<p>as i havent had any hands on experience on semantic segmentation, so the solution is provided using instance segmentation,<strong>but if you insist on achieving the same using semantic segmentation please provide the weights, and the inference methods and test dataset, so that i can help you out</strong></p>
<p>anyways i hope that this is what you were looking for, any questions related to the code, logic or working, feel free to contact</p>
","2025-01-15 06:27:53","0","Answer"
"79348095","78215347","","<p>It works on Google Colab (you can try on <a href=""https://colab.research.google.com/"" rel=""nofollow noreferrer"">https://colab.research.google.com/</a>):</p>
<pre><code>import torchvision
import torchvision.transforms as transforms
trainset = torchvision.datasets.EMNIST(root=&quot;emnist&quot;,
                                   split=&quot;mnist&quot;,#letters #digits 
                                   train=True,
                                   download=True,
                                   transform=transforms.ToTensor())
</code></pre>
<p>Produces:</p>
<pre><code>Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to emnist/EMNIST/raw/gzip.zip
100%|██████████| 562M/562M [00:10&lt;00:00, 52.9MB/s]
Extracting emnist/EMNIST/raw/gzip.zip to emnist/EMNIST/raw
</code></pre>
","2025-01-11 12:41:32","0","Answer"
"79346003","79268108","","<p><code>config.build()</code> creates an <code>Algorithm</code>. Use <code>ray.tune.Tuner</code> or <code>ray.train.Trainer</code> to create workers.</p>
<p><code>foreach_worker</code> will only work if you spawn multiple workers.
For non-distributed training you can use <code>algo.env_runner</code> to access your local worker.</p>
<p>In your case you only have one such worker modify your code like this:</p>
<pre class=""lang-py prettyprint-override""><code>algo = ppo_config.build()
for batch_idx, batch in enumerate(train_loader):
    # Prepare batch-specific env_config
    new_env_config = {
         # new data for the batch_idx
    }
    algo.env_runner.env.reset(env_config=new_env_config)
</code></pre>
<p>Alternatively for distributed training  (and I think for local training too) use <code>algo.env_runner_group.foreach_env(lambda env: env.reset(env_config=new_env_config))</code>.</p>
","2025-01-10 14:21:58","0","Answer"
"79319481","78438315","","<p>This is just a version issue...I did</p>
<pre><code>!pip install torch==1.4 torchvision==0.14.1
</code></pre>
<p>and it seems to work fine!</p>
","2024-12-31 08:12:01","0","Answer"
"79312983","79309200","","<p>The shape of the hidden state is <code>(D*num_layers, N, H_out)</code> where <code>D=2</code> for bidirectional.</p>
<p>The hidden state tensor alternates forward/backward for each layer. The <a href=""https://pytorch.org/docs/stable/generated/torch.nn.GRU.html"" rel=""nofollow noreferrer"">documentation</a> isn't super clear, but this is what they mean by <code>For bidirectional GRUs, forward and backward are directions 0 and 1 respectively</code>.</p>
<p>You can separate the forward/backward states as follows:</p>
<pre class=""lang-py prettyprint-override""><code>hidden_size = 32
gru_layers_count = 2

encoder = nn.GRU(hidden_size, 
                 hidden_size, 
                 num_layers = gru_layers_count, 
                 batch_first = True, bidirectional=True)
ip = torch.randn(64, 100, hidden_size)
op, hn = encoder(ip)

forward_states = hn[0::2]
backward_states = hn[1::2]
</code></pre>
","2024-12-27 23:34:11","0","Answer"
"79312964","79309840","","<p>This behavior is expected and explained in the <code>IterableDataset</code> <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When num_workers &gt; 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset’s <strong>iter</strong>() method or the DataLoader ‘s worker_init_fn option to modify each copy’s behavior.</p>
</blockquote>
<p>The linked documentation page also gives two examples for using an <code>IterableDataset</code> with multiple workers. One using worker info in the <code>__iter__</code> method of the dataset, the other using the <code>worker_init_fn</code> for the dataloader.</p>
<p>As a simple example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import math

class ToyDataset(torch.utils.data.IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end
    
    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        
        if worker_info is None:
            iter_start = self.start
            iter_end = self.end
        else:
            per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))
            worker_id = worker_info.id
            iter_start = self.start + worker_id * per_worker
            iter_end = min(iter_start + per_worker, self.end)
        data = torch.arange(iter_start, iter_end)
        yield from data

dataset = ToyDataset(0, 386)

loader = torch.utils.data.DataLoader(dataset, batch_size=256, num_workers=2)

print(len(list(loader)))
&gt; 2
</code></pre>
","2024-12-27 23:18:43","0","Answer"
"79310841","78899566","","<p>I had the same problem. The post by David H Parry does not seem to help at all. After a bit of trial and error I found a fix. Heres my code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from diffusers import FluxPipeline
from huggingface_hub import login

login()
pipe = FluxPipeline.from_pretrained(&quot;black-forest-labs/FLUX.1-dev&quot;,
                                    torch_dtype=torch.bfloat16).to(torch.device(&quot;cuda&quot;))

prompt = &quot;dog&quot;
image = pipe(
    prompt,
    height=256,
    width=256,
    guidance_scale=3.5,
    num_inference_steps=50,
    max_sequence_length=512,
    generator=torch.Generator(&quot;cpu&quot;).manual_seed(0)
).images[0]
image.show()
print(&quot;Done!&quot;)
</code></pre>
<p>The main problem is RAM. Running FLUX dev requires about 50-60GB of RAM (you can also use a pagefile). After setting up a pagefile it should start generating the image.</p>
","2024-12-27 03:23:23","0","Answer"
"79309840","","Dataloader on Iterable dataset yields copied batches for num_workers > 0","<p>The title says it all. An iterable dataset with a multi-worker dataloader yields more batches than it should (seems that each worker yields all the batches separately). Here is an MWE:</p>
<pre><code>import torch


class ToyDataset(torch.utils.data.IterableDataset):
    def __iter__(self):
        data = torch.arange(len(self))
        yield from data

    def __len__(self):
        return 386


dataset = ToyDataset()
loader = torch.utils.data.DataLoader(dataset, batch_size=256, num_workers=2)
print(len(loader), len(list(loader))) # 2 4
</code></pre>
<p>Is it something I'm missing? Is this a bug in pytorch (though it seems highly unlikely)? And most importantly, is there any way around this?</p>
<p>I also created an issue on the <a href=""https://discuss.pytorch.org/t/dataloader-yields-copied-batches/200529"" rel=""nofollow noreferrer"">pytorch discuss forums</a>, however it didn't get much attention.</p>
","2024-12-26 15:45:30","0","Question"
"79309200","","can anyone explain h_n output of gru layer?","<p>I am new to pytorch, have started coding from one month.
this is my gru code</p>
<pre><code>hidden_size = 32
gru_layers_count = 2

encoder = nn.GRU(hidden_size, 
                 hidden_size, 
                 num_layers = gru_layers_count, 
                 batch_first = True, bidirectional=True)
ip = torch.randn(64, 100, hidden_size)
op, hn = encoder(ip)
print(op.shape, hn.shape)
</code></pre>
<p>the output is:</p>
<pre><code>torch.Size([64, 100, 64]) torch.Size([4, 64, 32])
</code></pre>
<p>here I am actually concerned with the shape of <code>hn</code>, its start dimension size is 4 so I am assuming it is 2 gru * 2 directions.
however I am a little confused on the arrangement.</p>
<p>So my question is,
is it like first 2 are forward and last 2 backward hidden states. or it is alternate forward and backward hidden states ?</p>
<p>is the following is the correct method to extract only forward gru states ?</p>
<pre><code>forward_hidden = hn[[x for x in range(0, gru_layers_count * 2, 2)], :, :]
</code></pre>
","2024-12-26 09:59:38","1","Question"
"79307055","78853571","","<p>Looks like dynamo_export is a bit buggy... or it doesn't really support earlier models.
Try using the original onnx export instead:</p>
<pre><code>x = torch.randn(&lt;your input dimensions&gt;)    

model.to('cpu') 

with torch.no_grad():
    torch_out = model(x)

    # Export the model
    torch.onnx.export(model,               # model being run
                    x,                         # model input (or a tuple for multiple inputs)
                    &quot;&lt;Your model name&gt;.onnx&quot;,   # where to save the model (can be a file or file-like object)
                    export_params=True,        # store the trained parameter weights inside the model file
                    opset_version=15,          # the ONNX version to export the model to
                    do_constant_folding=True,  # whether to execute constant folding for optimization
                    input_names = ['input'],   # the model's input names
                    output_names = ['output'], # the model's output names
                    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes
                                    'output' : {0 : 'batch_size'}})
</code></pre>
","2024-12-25 07:26:58","0","Answer"
"79303409","78048828","","<p>【1】Values in <code>new_locs</code></p>
<p>In pytorch's <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample"" rel=""nofollow noreferrer"">document</a> about <code>grid_sample()</code>:</p>
<blockquote>
<p>grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. For example, values x = -1, y = -1 is the left-top pixel of input, and values x = 1, y = 1 is the right-bottom pixel of input.</p>
</blockquote>
<p>【2】Why use interpolation?</p>
<p>The coordinate of pixel A after registration is related to a position (A') in the original image. Taking affine transformation as an example, the coordinate of point A before and after registration can be related by a transformation matrix. However, A' may be something like [9.2, 10.4]. Therefore, to decide the pixel value of A, interpolation of the original image is required.</p>
","2024-12-23 14:47:01","1","Answer"
"79302427","79302218","","<p>I think you are confused about what the softmax operation does.</p>
<p>Softmax is defined as <code>softmax(x_i) = exp(x_i) / sum(exp(x_j))</code>. The values you are getting are the correct values:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.tensor([ 1.,  2.,  3.,  4.,  5.,  6.])
x_softmax = exp(x) / exp(x).sum()
print(x_softmax)
&gt; tensor([0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337])
</code></pre>
<p>The values you want (<code>[ .047,  .095,  .142,  .19,  .238,  .285]</code>) are the result of <code>x / x.sum()</code> which is an entirely different operation:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.tensor([ 1.,  2.,  3.,  4.,  5.,  6.])
print(x / x.sum())
&gt; tensor([0.0476, 0.0952, 0.1429, 0.1905, 0.2381, 0.2857])
</code></pre>
<p>Also note that softmax gives the same result when the values are shifted by an arbitrary constant (ie <code>softmax(x) = softmax(x-c)</code> when the shift by <code>c</code> is applied to all values along the axis. From a softmax perspective, each row of your <code>sims</code> tensor is just the vector <code>[0., 1., 2., 3., 4., 5.]</code> shifted by different values. This is why each row has the same result post-softmax.</p>
<pre class=""lang-py prettyprint-override""><code>sims_shifted = sims - sims.min(dim=1, keepdim=True)[0]
print(sims_shifted)
&gt; tensor([[0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.],
        [0., 1., 2., 3., 4., 5.]])
torch.allclose(
    nn.functional.softmax(sims, dim=1),
    nn.functional.softmax(sims_shifted, dim=1)
)
&gt; True
</code></pre>
","2024-12-23 07:16:46","2","Answer"
"79302218","","torch.nn.functional.softmax giving inaccurate softmax output","<p>I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results.</p>
<p>What I tried:</p>
<pre class=""lang-py prettyprint-override""><code>print(sims)
print(torch.nn.functional.softmax(sims, dim=1))
</code></pre>
<p>Which gives the following output:</p>
<pre class=""lang-py prettyprint-override""><code>tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],
        [ 7.,  8.,  9., 10., 11., 12.],
        [13., 14., 15., 16., 17., 18.],
        [19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30.],
        [31., 32., 33., 34., 35., 36.],
        [37., 38., 39., 40., 41., 42.],
        [43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54.],
        [55., 56., 57., 58., 59., 60.]])

tensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])
</code></pre>
<p>As an example, I am expecting the output of the <code>softmax</code> function on the first row &quot;sims&quot;</p>
<pre><code>[ 1.,  2.,  3.,  4.,  5.,  6.]
</code></pre>
<p>to show</p>
<pre><code>[ .047,  .095,  .142,  .19,  .238,  .285]
</code></pre>
<p>which would be the accurate softmax attention percentages needed to apply to my value tensor</p>
","2024-12-23 05:18:17","0","Question"
"79301406","79301013","","<p>Something that can significantly affect memory usage is the resolution of the images. If the images are 1024x1024, thats 11 GB of RAM. When the images are loaded into Python, they are decompressed which increases the memory load. This is a small thing but <code>cv.imread()</code> returns a numpy array so you don't need to call <code>np.array()</code> on it.</p>
<ol>
<li>Resize images to a smaller resolution if needed (maybe 224x224)</li>
<li>Avoid calling <code>np.array()</code> on the image</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>#load in the train data - explore and clean
training_images=[]
missing_labels=[]
#count the sample distribution per class 
count={&quot;No DR&quot; :0 , &quot;Mild DR&quot;: 0, &quot;Moderate DR&quot;: 0, &quot;Severe DR&quot;: 0, &quot;Proliferative DR&quot; : 0}



#iterate through the data in subsets, iterating through 200 images at a time
for chunk in np.array_split(df, 1000):
     for idx ,i in tqdm(chunk.iterrows(), total=len(chunk)):
        try:
            img_path = os.path.join(path, f&quot;{i['id_code']}.png&quot;)
            img = cv2.imread(img_path, cv2.IMREAD_COLOR)
            img = cv2.resize(img, (224, 224))
            label = i[&quot;diagnosis&quot;]
        
            if pd.isna(label):
                #if any image within the csv file does not contain a label, we append the image to the missing_labels, in which we can later filter out
                missing_labels.append(img)

            #convert each image into a one hot encoding
            #np.eye() allows you define the number of output classes and the [] after it defines which class will be hot
            #np.eye(2)[0] for instance meaning [1,0], we have 2 classes and the 0th index is hot meaning that is our true label
            training_images.append([img,np.eye(5)[int(label)]])    

        except Exception as e:
            print(f&quot;There was an error processing  Image : {img_path}&quot;)

print(training_images[0][0].shape)
print(training_images[0])        

#as the label and image will be a list of lists, this should return a numpy array and its corresponding one hot encoded
print(f&quot;The number of msising labels found in the dataset are : {len(missing_labels)}&quot;)
</code></pre>
<p>A last resort is to lazy load the images into PyTorch. You can create a custom dataset with the PyTorch <code>Dataset</code> class and then load the images into memory in <code>__getitem__()</code> instead of all at the beginning, which will make the runtime slower but memory usage will be less.</p>
","2024-12-22 17:29:20","0","Answer"
"79301013","","What is the most optimal choice and best practice when iterating through datasets","<p>So im trying to iterate through some fundus images but my ipynb file keeps crashing. There are only 3662 images, is there a more optimal way of iterating through my entire data? The sole purpose for this is just visualization and exploring the data.Therefore i will use a dataloader once the data is cleaned but for now i'd like to iterate over the entire dataset</p>
<pre><code>#load in the train data - explore and clean
training_images=[]
missing_labels=[]
#count the sample distribution per class 
count={&quot;No DR&quot; :0 , &quot;Mild DR&quot;: 0, &quot;Moderate DR&quot;: 0, &quot;Severe DR&quot;: 0, &quot;Proliferative DR&quot; : 0}



#iterate through the data in subsets, iterating through 200 images at a time
for chunk in np.array_split(df, 1000):
     for idx ,i in tqdm(chunk.iterrows(), total=len(chunk)):
        try:
            img_path = os.path.join(path, f&quot;{i['id_code']}.png&quot;)
            img = cv2.imread(img_path, cv2.IMREAD_COLOR)
            label = i[&quot;diagnosis&quot;]
        
            if pd.isna(label):
                #if any image within the csv file does not contain a label, we append the image to the missing_labels, in which we can later filter out
                missing_labels.append(np.array(img))

            #convert each image into a one hot encoding
            #np.eye() allows you define the number of output classes and the [] after it defines which class will be hot
            #np.eye(2)[0] for instance meaning [1,0], we have 2 classes and the 0th index is hot meaning that is our true label
            training_images.append([np.array(img),np.eye(5)[int(i[&quot;diagnosis&quot;])]])    

        except Exception as e:
            print(f&quot;There was an error processing  Image : {img_path}&quot;)

print(training_images[0][0].shape)
print(training_images[0])        

#as the label and image will be a list of lists, this should return a numpy array and its corresponding one hot encoded
print(f&quot;The number of msising labels found in the dataset are : {len(missing_labels)}&quot;)

</code></pre>
","2024-12-22 12:48:54","0","Question"
"79300183","79300055","","<p>From <code>MaxVit</code> Args parameters:</p>
<blockquote>
<p>block_channels (List[int]): Number of channels in each block. <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/maxvit.py#L575"" rel=""nofollow noreferrer"">Source</a></p>
</blockquote>
<p>The classifier <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/maxvit.py#L823C5-L833C6"" rel=""nofollow noreferrer"">Source</a></p>
<pre><code>self.classifier = nn.Sequential(
        nn.AdaptiveAvgPool2d(1),
        nn.Flatten(),
        nn.LayerNorm(block_channels[-1]),
        nn.Linear(block_channels[-1], block_channels[-1]),
        nn.Tanh(),
        nn.Linear(block_channels[-1], num_classes, bias=False),
    )
</code></pre>
<p>Since <code>block_channels</code> is a list,`block_channels[-1] returns the last item in the list, 512 in the following case <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/maxvit.py#L823C5-L833C6https://"" rel=""nofollow noreferrer"">Source</a></p>
<pre><code>return _maxvit(
    stem_channels=64,
    block_channels=[64, 128, 256, 512],
    block_layers=[2, 2, 5, 2],
    head_dim=32,
    stochastic_depth_prob=0.2,
    partition_size=7,
    weights=weights,
    progress=progress,
    **kwargs,
)
</code></pre>
","2024-12-21 22:14:56","0","Answer"
"79300055","","What should be my classifier in Transfer Learning using MaxViT?","<p>I am trying to do transfer learning on Pytorch pretrained models with custom dataset. I have been able to successfully perform transfer learning with SqueezeNet.</p>
<p>For Squeezenet my classifier was, <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/squeezenet.py#L81-L83"" rel=""nofollow noreferrer"">layers source</a></p>
<pre><code>model.classifier = nn.Sequential(
    nn.Dropout(p=0.2),
    nn.Conv2d(512, len(class_names), kernel_size=1),
    nn.ReLU(inplace=True),
    nn.AdaptiveAvgPool2d((1, 1)))
</code></pre>
<p>For Efficientnet my classifier was, <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/efficientnet.py#L314-L316"" rel=""nofollow noreferrer"">layers source</a></p>
<pre><code>model.classifier = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True),
    torch.nn.Linear(in_features=1280,
                    out_features=output_shape,
                    bias=True))
</code></pre>
<p>Similarly I have been trying to do for MaxViT, I went through the source and saw that there are <code>block_channels[-1]</code> in parameter. I have recently started with this, and I don't know what they are, <a href=""https://github.com/pytorch/vision/blob/d3beb52a00e16c71e821e192bcc592d614a490c0/torchvision/models/maxvit.py#L696-L702"" rel=""nofollow noreferrer"">layers source</a></p>
<pre><code>self.classifier = nn.Sequential(
    nn.AdaptiveAvgPool2d(1),
    nn.Flatten(),
    nn.LayerNorm(block_channels[-1]),
    nn.Linear(block_channels[-1], block_channels[-1]),
    nn.Tanh(),
    nn.Linear(block_channels[-1], num_classes, bias=False),
)
</code></pre>
<p>For reference, if needed, following is my complete code for performing transfer learning using squeezenet.</p>
<pre><code>weights = torchvision.models.SqueezeNet1_0_Weights.DEFAULT
model = torchvision.models.squeezenet1_0(weights=weights).to(device)
auto_transforms = weights.transforms()
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=d1,
                                                                               test_dir=d2,
                                                                               transform=auto_transforms,
                                                                               batch_size=32)
for param in model.features.parameters():
    param.requires_grad = False

torch.manual_seed(42)
torch.cuda.manual_seed(42)
output_shape = len(class_names)

model.classifier = nn.Sequential(
    nn.Dropout(p=0.2),
    nn.Conv2d(512, len(class_names), kernel_size=1),
    nn.ReLU(inplace=True),
    nn.AdaptiveAvgPool2d((1, 1))).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
torch.manual_seed(42)
torch.cuda.manual_seed(42)
results = engine.train(model=model,
                       train_dataloader=train_dataloader,
                       test_dataloader=test_dataloader,
                       optimizer=optimizer,
                       loss_fn=loss_fn,
                       epochs=15,
                       device=device)
</code></pre>
<p>What should my classifier be for MaxViT?</p>
","2024-12-21 20:24:49","0","Question"
"79298544","79298447","","<p>You should be able to directly use <code>scatter_max</code> on the sparse tensor if you keep the indices that you pass to <code>scatter_max</code> also sparse (i.e, only the non-zero ones).</p>
<p>Consider this example</p>
<pre><code>query_indices = torch.tensor([
    [0, 0, 0, 1, 1, 1],
    [0, 1, 2, 0, 1, 2],
    [0, 1, 0, 0, 1, 0]
])

values = torch.tensor([1, 2, 3, 4, 5, 6])
num_lines = 2
img_size = 3

value_tensor = torch.sparse_coo_tensor(
    indices=query_indices,
    values=values,
    size=(num_lines, img_size, img_size)
)

# need to coalesce because for some reason sparse_coo_tensor doesn't guarantee uniqueness of indices
value_tensor = value_tensor.coalesce()

</code></pre>
<p>Then, compute <code>flat_indices</code> as a sparse tensor containing just the non-zero 1-d indices (2-d indices are converted to 1-d indices similar to your <code>arange</code>)</p>
<pre><code>indices = value_tensor.indices()
values = value_tensor.values()

batch_indices = indices[0]        # &quot;line&quot; (in your terminology) indices
row_indices = indices[1]
col_indices = indices[2]
flat_indices = row_indices * img_size + col_indices


</code></pre>
<p>You can use <code>flat_indices</code> to <code>scatter_max</code></p>
<pre><code>flattened_result, _ = scatter_max(
    values, flat_indices, dim=0, dim_size=img_size * img_size
)

per_line_max = flattened_result.reshape(img_size, img_size)

</code></pre>
<pre><code>indices

tensor([[0, 0, 0, 1, 1, 1],
        [0, 1, 2, 0, 1, 2],
        [0, 1, 0, 0, 1, 0]])


values

tensor([1, 2, 3, 4, 5, 6])


flat_indices

tensor([0, 4, 6, 0, 4, 6])

per_line_max

tensor([[4, 0, 0],
        [0, 5, 0],
        [6, 0, 0]])

</code></pre>
<p>The output I get is the same as what I get from your code.</p>
","2024-12-20 22:45:35","1","Answer"
"79298455","79296036","","<p>The solution is as simple as constructing a new sparse coo tensor.</p>
<pre class=""lang-py prettyprint-override""><code>value_tensor = torch.sparse_coo_tensor(indices=indices, values=values, size=(L, N, N))
</code></pre>
","2024-12-20 21:39:36","0","Answer"
"79298447","","PyTorch scatter max for sparse tensors?","<p>I have the following PyTorch code</p>
<pre><code>value_tensor = torch.sparse_coo_tensor(indices=query_indices.t(), values=values, size=(num_lines, img_size, img_size)).to(device=device)
value_tensor = value_tensor.to_dense()
indices = torch.arange(0, img_size * img_size).repeat(len(lines)).to(device=device)
line_tensor_flat = value_tensor.flatten()
img, _ = scatter_max(line_tensor_flat, indices, dim=0)
img = torch.reshape(img, (img_size, img_size))
</code></pre>
<p>Note the line: <code>value_tensor = value_tensor.to_dense()</code>, this is unsurprisingly slow.</p>
<p>However, I cannot figure out how to obtain the same results with a sparse tensor. The function in question calls <code>reshape</code> which is not available on sparse tensors. I'm using <a href=""https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/max.html"" rel=""nofollow noreferrer"">Scatter Max</a> but opened to using anything that works.</p>
","2024-12-20 21:36:06","2","Question"
"79297744","79291704","","<p>I think broadly your approach should be correct, though since it doesn't work, it suggests some issue with your process. If there is some temporal effect you aren't capturing properly, I might suggest doing a more custom approach to the train-val-test split.</p>
<p>What you could do is split your data into <em>n</em> subsets, each with a gap in time between them. The gap has to be minimum 1hr to avoid overlap. Though really you should consider something like the auto-correlation timescales of A, B, and C, to determine what might lead to data leakage.</p>
<p>I would do something like the following:</p>
<p>Choose how long a continuous segment should be. Let's say 5 hours for example. In your dataset this would correspond to 300 samples.</p>
<p>Choose how long your gap should be. Minimum 1hr - but ideally informed by the auto-correlation of A/B/C. Let's say 2 hours for example.</p>
<p>Select your data subsets. E.g., if you have your data in an array:</p>
<pre class=""lang-none prettyprint-override""><code>n_segments = len(x) // (300 + 120)
segments = [x[i*300 + i*120 : i*300 + i*120 + 300] for i in range(n_segments)  
</code></pre>
<p>Where this should only yield the 5 hours (300 minutes) for each cycle of segment and gap (2 hours - 120 minutes).</p>
<p>Then you just need to assign these to train/val/test, and you should be able to do that by a random sample, since now you shouldn't have a problem of data leakage. Though you do obviously have to concatenate them back into singular arrays rather than the current sub-arrays in the list.</p>
","2024-12-20 15:48:46","0","Answer"
"79296681","79291119","","<p>Just a temporary solution, which is just a little bit better than copying hints each time, inspired by @InSync and the decorator in <a href=""https://stackoverflow.com/questions/71253495/how-to-annotate-the-type-of-arguments-forwarded-to-another-function"">related question</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import TypeVar, Callable
from typing_extensions import ParamSpec # for 3.7 &lt;= python &lt; 3.10, import from typing for versions later

T = TypeVar('T')
P = ParamSpec('P')
def take_annotation_from(this: Callable[P, T]) -&gt; Callable[[Callable], Callable[P, T]]: 
    def decorator(real_function: Callable[P, T]) -&gt; Callable[P, T]: 
        # typing real_function with Callable[P, T] can directly tell python the input type (at least for code hinting in VSCode)
        # so wrap the real_function like the related question is unnecessary
        real_function.__doc__ = this.__doc__
        return real_function
    return decorator
</code></pre>
<p>And use it as</p>
<pre class=""lang-py prettyprint-override""><code>from torch.nn import Module

class MyModule(Module): 
    def __init__(self, k: float):
        ...
    
    def forward(self, ...) -&gt; ...: # with type hints
        &quot;&quot;&quot;docstring&quot;&quot;&quot;
        ...

    @take_annotation_from(forward)
    def __call__(self, *args, **kwds):
        return Module.__call__(self, *args, **kwds)
</code></pre>
<p>And this solution may be proved if last three lines of the code above can be packed as something like macro, because it remains unchanged among different implementations of sub-<code>nn.Module</code>s.</p>
","2024-12-20 08:56:41","0","Answer"
"79296036","","Construct a sparse tensor while propagating gradient?","<p>I have code similar to this I would like to make faster:</p>
<pre class=""lang-py prettyprint-override""><code># indices: indices of a 3d tensor
# values associated to the indices


result = torch.zeros((L, N, N))
for idx, (i,j,k) in enumerate(indices):
        mask = torch.zeros_like(result)
        mask[i][j][k] = 1.0
        img = img + mask * values[idx]
</code></pre>
<p>Now, even if I chose to make a sparse mask, I notice that each iteration runs slower. Is there a simple solution to a function that will propagate the gradient of the form <code>img = func(indices, values) </code></p>
<p>I'm looking for a solution that takes advantage of vectorization or/and sparse data structures</p>
","2024-12-20 02:24:31","1","Question"
"79291704","","Splitting multivariate time series data to avoid data leakage in PyTorch?","<p>I have two predictors <code>A</code> and <code>B</code> with the goal to predict target (y) <code>C</code> using a multi-layer ANN. My inputs are 2D combinations of time series using 1 hour of <code>A</code> and 30 minutes of <code>B</code> for prediction.</p>
<p>Assume we are predicting <code>C</code> at time t = 0, then the input (x) is as follows:</p>
<pre><code>x0 = [A(t=-60), A(t=-59) ... A(t=0), B(t=-30), B(t=-29) ... B(t=0)]
</code></pre>
<p>Then for t = 1,</p>
<pre><code>x1 = [A(t=-59), A(t=-58) ... A(t=1), B(t=-29), B(t=-28) ... B(t=1)]
</code></pre>
<p>As you can see, inputs 0 and 1 have overlapping time history, and therefore, would have data leakage when split into a train, test and validation set like so:</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_split, random_state=64, shuffle=True)

x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=val_split, random_state=64, shuffle=True)
</code></pre>
<p>One clear way to solve this issue is to make <code>shuffle=False</code> when splitting the dataset and then shuffling the training set in the data loader, which I do here after converting to torch tensors:</p>
<pre><code># training data loading
train_data = TensorDataset(x_train_tensor, y_train_tensor)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

# validation data loading
val_data = TensorDataset(x_val_tensor, y_val_tensor)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)
</code></pre>
<p>The problem is that when I do not shuffle on the train/test/split, my model is awful. Suspiciously awful (e.g. correlation coefficient on the test set of -0.02). I say suspicious as I know the my predictors <em>are</em> statistically correlated with my target, that even if the model was not performing well I would still get better metrics than what I am seeing.</p>
<p>Since I do not shuffle my data during splitting, I suspect that there is important information later on in time that is in the test set that my model would need in training. I am not sure. It just seems to be performing too poorly that I think there is an issue I am not seeing.</p>
<p>Is there a better way to split my dataset to preserve time history and prevent major data leakage?</p>
<p>Edit: I also should add that I run the model through a hyperparameter tuning script, so the dataset gets to train with a variety of parameters. The results are consistently awful.</p>
","2024-12-18 15:48:19","1","Question"
"79291458","79291434","","<ol>
<li>You need to make sure that your Python version is at least 3.8. You can look at <a href=""https://pypi.org/project/torch/2.0.0/"" rel=""nofollow noreferrer"">torch's version history</a> to see what Python versions are compatible with which versions of PyTorch.</li>
<li>Also, you need to make sure <code>pip</code> is updated with <code>pip install --upgrade pip</code></li>
</ol>
","2024-12-18 14:33:30","1","Answer"
"79291434","","Torch installation: No matching distribution found for torch>=2.0.0","<p>I am trying to install version of pytorch as per the requirements.txt file which looks like:</p>
<pre><code># --------- pytorch --------- #
torch&gt;=2.0.0
torchvision&gt;=0.15.0
lightning&gt;=2.0.0
torchmetrics&gt;=0.11.4
</code></pre>
<p>I create a new venv and then run <code>pip install -r requirements.txt</code></p>
<p>It cannot find the correct version of torch giving this error:</p>
<p><code>ERROR: No matching distribution found for torch&gt;=2.0.0</code></p>
<p>What am I doing wrong?</p>
","2024-12-18 14:26:39","0","Question"
"79291119","","In PyTorch, how to make __call__() of nn.Module automatically copy the type hints and docstring of forward()?","<p>It is common to implement the forward steps in <code>forward()</code> and call it through <code>__call__</code>, like this example</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn, FloatTensor, IntTensor

class MyModule(nn.Module): 
    def __init__(self, ...) -&gt; None: 
        nn.Module.__init__(self)
        ...
    def forward(self, x: FloatTensor, y: FloatTensor) -&gt; tuple[FloatTensor, IntTensor]: 
        &quot;&quot;&quot;
        Args:
            x (FloatTensor): in shape of BxTxE
            y (FloatTensor): in shape of BxE

        Returns:
            tuple[FloatTensor, IntTensor]: (sth. in shape of BxT, sth. in shape of B)
        &quot;&quot;&quot;
        ... # implementation of forward steps

model = MyModule(...)
...
a, b = model(x, y) # call it through __call__
</code></pre>
<p>However, IDEs like VSCode cannot recognize the type hints or docstring of <code>__call__</code> because it's a completely different method without overloading.
Though this is reasonable in principle of Python, it is still unfriendly to circumstances like co-operating that needs convenient coding hints.</p>
<p>A possible but clumsy solution is copy those information to overload of <code>__call__()</code> in each <code>nn.Module</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn, FloatTensor, IntTensor

class MyModule(nn.Module): 
    def __init__(self, ...) -&gt; None: 
        nn.Module.__init__(self)
        ...
    def forward(self, x: FloatTensor, y: FloatTensor) -&gt; tuple[FloatTensor, IntTensor]: 
        &quot;&quot;&quot;
        Args:
            x (FloatTensor): in shape of BxTxE
            y (FloatTensor): in shape of BxE

        Returns:
            tuple[FloatTensor, IntTensor]: (sth. in shape of BxT, sth. in shape of B)
        &quot;&quot;&quot;
        ... # implementation of forward steps
    def __call__(self, x: FloatTensor, y: FloatTensor) -&gt; tuple[FloatTensor, IntTensor]: 
        &quot;&quot;&quot;
        Args:
            x (FloatTensor): in shape of BxTxE
            y (FloatTensor): in shape of BxE

        Returns:
            tuple[FloatTensor, IntTensor]: (sth. in shape of BxT, sth. in shape of B)
        &quot;&quot;&quot;
        return nn.Module.__call__(self, x, y)

model = MyModule(...)
...
a, b = model(x, y) # call through __call__
</code></pre>
<p>So, <strong>how can I tell python or VSCode that <code>__call__()</code> and <code>forward()</code> share identical input/return types and docstring in any subclass of <code>nn.Module</code></strong>, without write them again in overload of <code>__call__()</code> of each subclass?</p>
<p>(I guess possible solution for docstrings may be decorators? But I have no idea about copying type hints. )</p>
","2024-12-18 12:38:03","1","Question"
"79289620","78906762","","<p>This is a tensorboard issue with the current version. Downgrade to 2.16.2 fixes the issue.
<code>pip install tensorboard==2.16.2</code></p>
","2024-12-17 23:30:02","0","Answer"
"79289085","79280773","","<p>It's hard to say without more code, but this is what I think is happening:</p>
<p>The full <code>X_train_tensor</code> has <code>requires_grad=True</code>. Each iteration you take a slice defined by <code>X_train_tensor[start_ix:end_ix]</code>, run it through the model, and compute the grad.</p>
<p>When you backprop your loss, it backprops into <code>X_train_tensor</code> since <code>requires_grad=True</code>. You only process a slice of the tensor, but autograd calculates the gradient for the entire <code>X_train_tensor</code> (all values not in the slice are zero as they did not get used in the loss calculation).</p>
<p>When you do the next iteration, <code>X_train_tensor</code> already has a grad, so you get the &quot;Trying to backward through the graph a second time&quot; error.</p>
<p>If it's not important to compute the gradient of <code>X_train_tensor</code>, you can fix this easily by calling <code>detach</code> on the tensor - ie <code>X_train_tensor = X_train_tensor.detach()</code>.</p>
","2024-12-17 19:03:25","1","Answer"
"79288417","79268122","","<p>This seems to work fine: <code>torch.autograd.functional.jacobian(model.decoder, latent_l, strategy=&quot;forward-mode&quot;, vectorize=True)</code>, where only forward passes are needed instead of computing the whole jacobian.</p>
","2024-12-17 15:16:43","0","Answer"
"79287932","79287712","","<h2>Short answer</h2>
<p>We need to keep in mind that for a convolution, the kernel values need to be flipped/reversed before taking the weighted sum with the signal (in your case, <code>[1, 1, 2]</code> becomes <code>[2, 1, 1]</code>; but also see the last section <em>An implementation detail?</em> below). The resulting calculations are:</p>
<ul>
<li>For your first two channels: <code>2 * dot([1, 1, 1], [2, 1, 1])</code> = <code>8</code>.</li>
<li>For your last two channels: <code>2 * dot([1, 1, 2], [2, 1, 1])</code> = <code>10</code>.</li>
</ul>
<h2>Long answer</h2>
<p>Let's take this apart:</p>
<ul>
<li><p>Your input <code>x</code> is a 3-element signal with 4 channels, with the first two channels containing values <code>[1, 1, 1]</code> and the last
two channels containing values <code>[1, 1, 2]</code>.</p>
</li>
<li><p>Since you have <code>groups=2</code>, the 2nd example regarding groups from
<a href=""https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html"" rel=""nofollow noreferrer""><code>ConvTranspose1d</code>'s documentation</a> applies:</p>
<blockquote>
<p>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels <em>(2 per
group in your case)</em> and producing half the output channels <em>(again 2 per group in your case)</em>, and both subsequently
concatenated.</p>
</blockquote>
<p>In others words: the first half of your convolution kernel's weights will convolve the two channels containing
<code>[1, 1, 1]</code>, the last half of your weights will convolve the two channels containing <code>[1, 1, 2]</code>.</p>
</li>
<li><p>Your <code>weight</code> tensor that contains the convolution kernel's weights has a shape of <code>(4, 2, 3)</code>. With your setup, this translates to: You have two groups of weights of
shape <code>(2, 2, 3)</code>, each convolving <code>2</code> input channels to produce <code>2</code> output channels with a kernel size of <code>3</code>.</p>
</li>
<li><p>Regarding padding, we follow the note on padding from <code>ConvTranspose1d</code>'s documentation:</p>
<blockquote>
<p>The <code>padding</code> argument effectively adds <code>dilation * (kernel_size - 1) - padding</code> amount of zero padding to both sizes <em>(I guess this should be &quot;sides&quot;)</em> of the input.</p>
</blockquote>
<p>This translates to <em>no additional padding</em> in your case with <code>dilation=1</code>, <code>kernel_size=3</code>, <code>padding=2</code>.¹</p>
</li>
<li><p>As a consequence from your signal effectively not being padded, your output will contain <em>a single element</em> for each channel, as there is only one single position to convolve your signal of length <code>3</code> with your kernel of size <code>3</code>.</p>
</li>
<li><p>Since no striding or dilation is applied in your case either, the corresponding values can be directly calculated as the weighted sums of
the elements at corresponding positions in your signal <code>x</code> and in the kernel weights from the <code>weight</code> tensor (note that
we need to reverse the order of elements in the kernel, since this is a convolution and not a correlation, thus <code>[1, 1, 2]</code> from <code>weight</code> becomes <code>[2, 1, 1]</code> in the calculation):</p>
<ul>
<li>First two channels: <code>2 * dot([1, 1, 1], [2, 1, 1])</code><br/>= <code>2 * (1*2 + 1*1 + 1*1)</code> = <code>2 * (2 + 1 + 1)</code> = <code>2 * 4</code> = <code>8</code>.</li>
<li>Last two channels: <code>2 * dot([1, 1, 2], [2, 1, 1])</code><br/>= <code>2 * (1*2 + 1*1 + 2*1)</code> = <code>2 * (2 + 1 + 2)</code> = <code>2 * 5</code> = <code>10</code>.</li>
</ul>
</li>
</ul>
<h2>An implementation detail?</h2>
<p>I mentioned above that, since we work with a convolution, the order of weights in the kernel needs to be reversed. This is based on the mathematical definition of a discrete convolution as can be found, for example, <a href=""https://en.wikipedia.org/wiki/Convolution#Discrete_convolution"" rel=""nofollow noreferrer"">on Wikipedia</a>. However, only later did I realize that this is not always followed in PyTorch, compare below:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as f

signal = torch.tensor([2, 1, 1, 3]).reshape(1, 1, -1)
kernel = torch.tensor([1, 1, 5]).reshape(1, 1, -1)

print(f.conv1d(signal, kernel, padding=0).ravel().tolist())
# Prints [8, 17] == [dot([2,1,1],[1,1,5]), dot([1,1,3],[1,1,5])]

print(f.conv_transpose1d(signal, kernel, padding=2).ravel().tolist())
# Prints [12, 9] == [dot([2,1,1],[5,1,1]), dot([1,1,3],[5,1,1])]
</code></pre>
<p>Thus, while getting reversed for transposed convolutions, for (regular) convolutions the kernel <em>doesn't</em> get reversed.</p>
<p>I am not entirely sure whether this algorithmic difference is intentional or an implementation detail. It could be an implementation detail since usually, when you learn your weights rather than predefining them, it does not really matter whether you learn them for the kernel or the reversed kernel. I guess it is intentional though, since (quote from <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html"" rel=""nofollow noreferrer""><code>ConvTranspose1d</code>'s documentation</a>) <em>[transposed convolution] can be seen as the gradient of [(regular) convolution]</em>, so I guess it follows as a mathematical result that only one of both operations needs to work with the reversed kernel. I did not fully think this through, but in that situation, I would have expected the (regular) convolution kernel to be reversed rather than the one of the transposed convolution, following the mathematical definition of a discrete convolution mentioned above. In any case, I agree that this makes the observed result less obvious.</p>
<p><sub>¹) The <code>padding</code> parameter definition for transposed convolutions might not be really intuitive, but it is defined for ease of use in connection with &quot;regular&quot; convolutions; again, see the note on padding from the documentation: <em>This is set so that when a <code>Conv1d</code> and a <code>ConvTranspose1d</code> are initialized with same parameters, they are inverses of each other in regard to the input and output shapes.</em></sub></p>
","2024-12-17 12:47:24","1","Answer"
"79287712","","How does ConvTranspose in pytorch with groups > 1 work?","<p>I'm trying to understand the workflow of convtranspose of pytorch with groups &gt; 1 , mainly focusing on the calculation process between grouped transposeconv weights and padded input, I've experimented with my code, but I cant understand how the result was calculated.</p>
<p>I had an experiment with my code:</p>
<pre><code>import torch
import torch.nn as nn
weight = torch.tensor([1,1,2] * 8).reshape(4,2,3)
transpose_conv = nn.ConvTranspose1d(4, 4, 3, stride=1, padding=2, groups=2,bias=False)
x = torch.tensor([1,1,1,1,1,1, 1,1,2, 1,1,2]).reshape(1,4,3).type(torch.float32)
with torch.no_grad():
    transpose_conv.weight.copy_(weight.reshape(4, 2, 3))
print(transpose_conv(x))
</code></pre>
<p>and the result was tensor([[[ 8.],[ 8.],[10.],[10.]]]), I can't understand how the result was calculated, could you please tell me the process?</p>
","2024-12-17 11:38:20","0","Question"
"79285817","79284381","","<p><code>TripletMarginLoss</code> uses an underlying p-norm distance. If you are using the default parameter for <code>p=2</code>, then you should use euclidean distance to compute the similarity of embeddings.</p>
<p>Classification requires a labeled classification dataset and a a separate loss for classification, the same as any standard classification problem. Similarity loss does not give you classification.</p>
","2024-12-16 19:10:39","0","Answer"
"79285712","79285272","","<p>Modern <code>pip</code> uses build isolation, it uses a transient virtual env to build a wheel. For packages that don't require build dependencies or packages that declare build dependencies in <code>pyproject.toml</code> it's not a problem. But <a href=""https://github.com/facebookresearch/detectron2/tree/c69939aa85460e8135f40bce908a6cddaa73065f"" rel=""nofollow noreferrer"">the package</a> <code>detectron2</code> <a href=""https://github.com/facebookresearch/detectron2/blob/c69939aa85460e8135f40bce908a6cddaa73065f/setup.py#L10"" rel=""nofollow noreferrer"">requires <code>torch</code></a> and doesn't provide <code>pyproject.toml</code>.</p>
<p>You can downgrade to older <code>pip</code> version 22 that doesn't build in an isolated environment. Or disable build isolation using <a href=""https://pip.pypa.io/en/stable/cli/pip_install/#cmdoption-no-build-isolation"" rel=""nofollow noreferrer""><code>--no-build-isolation</code></a>. The full command should be</p>
<pre><code>python -m pip install --no-build-isolation 'git+https://github.com/facebookresearch/detectron2.git'
</code></pre>
","2024-12-16 18:25:54","1","Answer"
"79285709","79283872","","<ol>
<li>The reason <code>frame.show()</code> is showing the wrong colors is because OpenCV uses the BGR format while PyTorch and PIL use the RGB format. Your frame is created from OpenCV (BGR format) and then you attempt to display it with PIL (RGB format) without any conversion.</li>
<li>Usually for image processing, libraries expect values in the range [0, 1] for floats and [0, 255] for integers. The reason <code>self.check_tensor(rotated_tensor)</code> shows a black screen is because the image-to-tensor conversion in <code>rotate_tensor()</code> normalizes the values to the range [0, 1] and when you cast it to an integer in <code>check_tensor()</code> it floors all of the values to 0 since they are all decimals between 0 and 1 so you need to multiply the tensor by 255 after rotating.</li>
<li>Since your array already has a range [0, 255] you don't want to multiply it by 255 again in <code>tensor_to_image()</code> and if you do OpenCV and PIL will usually mod the values leading to unexpected colors.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>def tensor_to_image(tensor):
    tensor = tensor.byte()
    tensor = tensor.squeeze(0)
    tensor = tensor.permute(1, 2, 0)
    image = Image.fromarray(np.array(tensor).astype(np.uint8))
    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)
    image = Image.fromarray(np.asarray(image))
    return image

def rotate_tensor(frame_tensor, landmarks):
    roll = calc_face_angle(landmarks)
    frame = tensor_to_image(frame_tensor)
    frame.show()

    if not np.isnan(roll):
        rotated_frame = frame.rotate(roll, resample=Image.BICUBIC, expand=True)
    else:
        print(&quot;Failed to calculate face angle for rotation&quot;)
        return frame_tensor
    
    rotated_frame = cv2.cvtColor(np.asarray(rotated_frame), cv2.COLOR_RGB2BGR)
    transform = transforms.ToTensor()
    rotated_tensor = transform(rotated_frame).unsqueeze(0)
    return rotated_tensor * 255
</code></pre>
","2024-12-16 18:25:19","0","Answer"
"79285682","79279472","","<p>Your implementation of cosine similarity is wrong. You can see this by inspecting the values of the cosine similarity matrix. Run the following:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F

bs = 8
d_proj = 64
z_i = torch.randn(bs, d_proj)
z_j = torch.randn(bs, d_proj)

z_i_norm = F.normalize(z_i, dim=1)
z_j_norm = F.normalize(z_j, dim=1)

cosine_num = torch.matmul(z_i, z_j.T)
cosine_denom = torch.matmul(z_i_norm, z_j_norm.T)
cosine_similarity = cosine_num / cosine_denom

print(cosine_similarity)
</code></pre>
<p>You will see the values in <code>cosine_similarity</code> are quite large (when it should be bounded between -1 and 1).</p>
<p>Below are two correct ways of computing pairwise cosine similarity:</p>
<pre class=""lang-py prettyprint-override""><code># F.cosine_similarity is preferred for performance
cosine_similarity = F.cosine_similarity(z_i[:,None], z_j[None,:], dim=2)

# alt version to show how cosine similarity is computed 
cosine_similarity = (z_i[:,None] * z_j[None,:]).sum(-1) / (torch.norm(z_i, dim=-1)*torch.norm(z_j, dim=-1))
</code></pre>
<p>You also have errors in your cross entropy implementation. For example you shouldn't zero the diagonal values of the denominator, and <code>denominator = torch.exp(torch.sum(cosine_similarity, dim=1))</code> should instead be <code>denominator = torch.exp(cosine_similarity / temperature).sum(dim=1)</code> (include temperature scaling, sum after exp rather than before).</p>
<p>Overall, you should use <code>F.cross_entropy</code> instead of manually computing the log-exp values - this is much more numerically stable.</p>
<pre class=""lang-py prettyprint-override""><code>cosine_similarity = F.cosine_similarity(z_i[:,None], z_j[None,:], dim=2)
labels = torch.arange(cosine_similarity.shape[0], device=cosine_similarity.device)
loss = F.cross_entropy(cosine_similarity/temperature, labels)
</code></pre>
","2024-12-16 18:14:42","2","Answer"
"79285586","79283584","","<p>For this kind of computation I found NumPy rather slow compared to PyTorch (on CPU): for a 5000x4000 (20M pixels) input image, PyTorch is almost 10 times faster than NumPy on my computer (Apple M1 Pro CPU).</p>
<p>You should be careful though, PyTorch seems to fully utilize the CPU cores while NumPy does not. This means that if you load multiple samples concurrently using PyTorch DataLoader you may not see a speed improvement since multiple thread will fight for the same CPU ressource. However, this may still overlap some I/O (image loading) with computations, thus it may be worth it to give it a try.</p>
<p>Here is the code to reproduce the benchmark:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
from tqdm import tqdm


def main():
    N = 100
    H, W = 4000, 5000

    # NumPy
    mask_np = np.random.randn(H, W) &gt; 0.0

    for _ in tqdm(range(N)):
        _ = np.where(mask_np)

    # PyTorch
    mask_torch = torch.from_numpy(mask_np)

    for _ in tqdm(range(N)):
        _ = torch.where(mask_torch)

    # Check consistency
    y_np, x_np = np.where(mask_np)
    y_torch, x_torch = torch.where(mask_torch)

    assert np.all(y_np == y_torch.numpy())
    assert np.all(x_np == x_torch.numpy())


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
","2024-12-16 17:42:03","1","Answer"
"79285272","","ModuleNotFoundError: No module named 'torch', but torch is installed","<p>I'm trying to use (and have successfully installed) Layout Parser, which requires detectron2 for certain functionality. While trying to install detectron2, I ran into the following error:</p>
<pre><code>&gt; python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

[snip]
      ModuleNotFoundError: No module named 'torch'
      [end of output]
[/snip]
</code></pre>
<p>Setting the flags as specified in the <a href=""https://detectron2.readthedocs.io/en/latest/tutorials/install.html"" rel=""nofollow noreferrer"">installation instructions</a> does nothing:</p>
<pre><code>CC=clang CXX=clang++ ARCHFLAGS=&quot;-arch x86_64&quot; pip3 install 'git+https://github.com/facebookresearch/detectron2.git@v0.4#egg=detectron2'

[same output]
</code></pre>
<ul>
<li>I have torch (2.4.1) and torchvision (0.19.1) installed, and the versions match.</li>
<li>I'm on macOS Sequoia 15.1.1</li>
<li>I'm using Python 3.10.14 and pip 24.2</li>
</ul>
<p>Full output of the installation command is at <a href=""https://pastebin.com/YbZ3u3A5"" rel=""nofollow noreferrer"">this pastebin</a>.</p>
","2024-12-16 15:50:24","1","Question"
"79284426","","How to manage memory consumption in deep learning models?","<p>When I run this code, the runtime session automatically closes. There is no space in RAM left. Hence the session closes automatically. I am using pytorch in Google Colab notebook. I tried switching from CPU to GPU but still the session closes automatically.</p>
<pre><code>!pip install datasets
!pip install transformers

import torch
import wandb
import torch.nn as nn
from datasets import load_dataset
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig

!huggingface-cli login

model_name = &quot;distilgpt2&quot;

print(f&quot;Using GPU: {torch.cuda.is_available()}&quot;)

tokenizer = AutoTokenizer.from_pretrained(model_name)

len(tokenizer)

tokenizer.add_special_tokens({'pad_token': '[PAD]'})

len(tokenizer)

class Custom_GPT2_Model(nn.Module):
    def __init__(self, tokenizer):
        super().__init__()

        self.gpt2 = AutoModelForCausalLM.from_pretrained(model_name)

        self.gpt2.resize_token_embeddings(len(tokenizer))

        for param in self.gpt2.parameters():
            param.requires_grad = False

        self.gpt2.gradient_checkpointing_enable()

        self.custom_layer = nn.Linear(self.gpt2.config.vocab_size, self.gpt2.config.vocab_size)

    def forward(self, input_ids, attention_mask=None, labels=None):

        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)

        logits = self.custom_layer(outputs.logits)

        loss = None
        if labels is not None:
            loss_func = nn.CrossEntropyLoss()
            # loss = loss_func(logits, labels)
            loss = loss_func(logits.view(-1, logits.size(-1)), labels.view(-1))

        return {
            'loss': loss,
            'logits': logits
        }

model = Custom_GPT2_Model(tokenizer)

# Data Pre-Processing

dataset = load_dataset(&quot;wikitext&quot;, &quot;wikitext-2-raw-v1&quot;)

dataset

def tokenize_func(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')

tokenized_dataset = dataset.map(tokenize_func, batched=True, remove_columns=['text'])
tokenized_dataset

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=0.5,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    fp16=True,
    warmup_steps=2500,
    learning_rate=0.1,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=5000,
)


trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_dataset['train'].select(range(100)),
    eval_dataset = tokenized_dataset['test'].select(range(100)),
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
)

trainer.train()
</code></pre>
<p>How should I manage memory while fine-tuning deep learning models?</p>
","2024-12-16 11:00:10","0","Question"
"79284381","","How to calculate similarity/distance in a Siamese network (pytorch)","<p>How exactly do you calculate the similarity/distance in a Siamese network, and classify them after?</p>
<p>This is my current attempt</p>
<pre><code>class SiameseNetwork(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.resnet = torchvision.models.resnet18(num_classes=5)

    def forward_once(self, item):
        output = self.resnet(item)
        return output

    def forward(self, anchor, positive, negative):
        output1 = self.forward_once(anchor)
        output2 = self.forward_once(positive)
        output3 = self.forward_once(negative)

        return output1, output2, output3
</code></pre>
<p>I use resnet and TripletMarginLoss as the loss function but i'm confused on how to calculate similarity and classify the output after</p>
<p>also if you find anything wrong about the code please tell me.</p>
","2024-12-16 10:41:53","0","Question"
"79284161","79283584","","<p>If your masks do not change over time, meaning they will be the same for multiple runs of what I assume are the training epochs of an ML model (since you mention the use with PyTorch dataloaders), and if the positive points are rather sparse, you might want to consider storing the <em>indexes</em> of your positive mask points (as lists of 2-tuples, N×2 Numpy arrays, etc., where each 2-tuple/row is the <code>(x, y)</code>-coordinate of one mask point). This might take one expensive preprocessing run, but after that, you can sample from these lists and are guaranteed to get a positive point each time.</p>
<p>It pretty much depends on how sparse your mask points are, whether sampling the indexes or sampling the mask directly repeatedly (as proposed in <a href=""https://stackoverflow.com/a/79283623/7395592"">Cris Luengo's answer</a>) is faster.</p>
<p>Here is some code for timing both approaches, trying to sample a single positive mask point:</p>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt
import numpy as np
from timeit import Timer
from tqdm import tqdm  # To monitor progress

h, w = 1000, 1000
rand = np.random.default_rng(seed=42)
num_timings = 100

def sample_from_mask(mask_):
    while True:
        r, c = rand.integers(h), rand.integers(w)
        if mask_[r, c]:
            return r, c
        
def sample_from_idxs(mask_idxs_):
    return rand.choice(mask_idxs_)

timings_mask, timings_idxs = [], []
num_positives = np.round(np.logspace(1, np.log10(h * w), num=10)).astype(int)
for n in tqdm(num_positives):
    # Set up the positive indexes and the corresponding mask
    mask_idxs = rand.choice(np.mgrid[0:h, 0:w].reshape(2, -1).T, size=n, replace=False)  # N×2
    mask = np.zeros((h, w))
    mask[tuple(mask_idxs.T)] = 1

    # Time the sampling
    timings_mask.append(Timer(lambda: sample_from_mask(mask)).timeit(num_timings))
    timings_idxs.append(Timer(lambda: sample_from_idxs(mask_idxs)).timeit(num_timings))

plt.loglog()
plt.xlabel(&quot;density of positive pixels&quot;)
plt.ylabel(&quot;time to sample one positive pixel (s)&quot;)
density_positives = np.divide(num_positives, h * w)
plt.plot(density_positives, timings_mask, &quot;x&quot;, label=&quot;sample_from_mask()&quot;)
plt.plot(density_positives, timings_idxs, &quot;x&quot;, label=&quot;sample_from_idxs()&quot;)
plt.legend()
</code></pre>
<p>And here is the result I get (the y axis label is wrong here: the result for <code>num_timings = 100</code>, thus for sampling 100 positive pixels, is shown):
<a href=""https://i.sstatic.net/1KWqCVz3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1KWqCVz3.png"" alt=""plot of timings of both approaches"" /></a></p>
<p>The crossing point for my implementations is around a density of 28% positive pixels, below which sampling indexes is faster. Note that this doesn't include the step of getting the mask indexes from the mask (i.e. the preprocessing run that I mention above), which would shift the result in favor of the repeated mask sampling approach.</p>
","2024-12-16 09:21:39","2","Answer"
"79283872","","how to covert a frame (np array) to a pytorch tensor","<p><strong>i need to convert a frame of the video (which is a nparray) to a pytorch tensor, do some particular actions with it and convert it back but i'm struggling</strong></p>
<p>so, i have a frame returned from video_capture.read() and, as i understood, it's a np array. firstly, i convert it to a tensor and checks if looks correctly (sorry i can't add photos for some reason)
then i analyze it (no mistakes), try to rotate it and here's a problem.</p>
<ol>
<li>frame.show() shows the tensor in different colours, looks super wrong</li>
<li>self.check_tensor(rotated_tensor) after rotation shows just a black screen</li>
</ol>
<p>can somebody please help me to fix this, i'm so exhausted, chatgpt confuses me even more and don't understand anything... i guess the problem with colours is related to how i convert tensor to pil image, but i tried several changes (commented lines) and nothing hepled.
also is there a way to avoid converting tensor to pil image before the rotation? can't i just rotate a tensor?</p>
<pre class=""lang-py prettyprint-override""><code>def tensor_to_image(tensor):
    tensor = (tensor * 255).byte()
    tensor = tensor.squeeze(0)
    tensor = tensor.permute(1, 2, 0)
    image = Image.fromarray(np.array(tensor).astype(np.uint8))
    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)
    image = Image.fromarray(np.asarray(image))
    return image

def rotate_tensor(frame_tensor, landmarks):
    roll = calc_face_angle(landmarks)
    frame = tf.to_pil_image(frame_tensor.squeeze(0))
    #frame = tensor_to_image(frame_tensor)
    frame.show()

    if not np.isnan(roll):
        rotated_frame = frame.rotate(roll, resample=Image.BICUBIC, expand=True)
    else:
        print(&quot;Failed to calculate face angle for rotation&quot;)
        return frame_tensor

    #rotated_tensor = tf.to_tensor(rotated_frame).unsqueeze(0)
    
    transform = transforms.ToTensor() # Используем torchvision для преобразования в тензор
    rotated_tensor = transform(rotated_frame).unsqueeze(0)
    return rotated_tensor
    

def check_tensor(self, frame_tensor):
    frame_numpy = frame_tensor.squeeze(0).permute(1, 2, 0).byte().numpy()
    #frame_numpy = cv2.cvtColor(frame_numpy, cv2.COLOR_RGB2BGR)

    cv2.imshow(&quot;Frame&quot;, frame_numpy)
    cv2.waitKey(0)   
    cv2.destroyAllWindows() 


def analyze_video(self, video_path):
    video_capture = cv2.VideoCapture(video_path)

    for i in range(1):
        ret, frame = video_capture.read()
        if not ret:
            break

        # преобразуем фрейм в тензор
        frame_tensor = torch.from_numpy(frame).float()
        frame_tensor = frame_tensor.permute(2, 0, 1).unsqueeze(0)
        #frame_tensor = frame_tensor[:, [2, 1, 0], :, :]

        self.check_tensor(frame_tensor)

  
        orig_prediction = self.analyze_frame(frame_tensor)
   
        rotated_tensor = im.rotate_tensor(frame_tensor, orig_prediction.head())
        self.check_tensor(rotated_tensor)
</code></pre>
","2024-12-16 07:15:28","0","Question"
"79283623","79283584","","<p>The best method is to randomly sample a pixel in the mask, and if it’s not set, try again. You can try thousands of times before you get to the cost of enumerating all set pixels.</p>
<p>If your mask is very, very sparse, then your current method is likely best.</p>
<p>If your mask is a small and compact region in the larger image, getting the bounding box and sampling only random pixels in that box would be a speedup.</p>
","2024-12-16 03:48:30","4","Answer"
"79283584","","What is the most efficient way to randomly pick one positive location within a large binary mask image in Python?","<p>I am writing a custom image data loading function to randomly crop part of a large image according to its binary mask. The function will be used in PyTorch dataloader so I want it to be as fast and memory-efficient as possible. The image and the mask are quite large, as both the width and height are on the order of 10k~20k pixels.</p>
<p>I want every crop of the image to contain at last one positive point in the binary mask. My current solution is to first randomly sample one positive point from the mask image, and then generate a crop box around it. The implementation contains a section of code as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import PIL
import numpy as np

... # Some preprocessing to find all mask and image files

mask = PIL.Image.open(mask_file)  # both the width and height have 10k~20k pixels.
# fast_pil_to_numpy: https://uploadcare.com/blog/fast-import-of-pillow-images-to-numpy-opencv-arrays/
mask_np = fast_pil_to_numpy(mask).astype(bool)  # dim: [height, width]
mask_loc = np.where(mask_np)  # get (loc_y, loc_x) of all positive indices
idx = np.random.randint(low=0, high=len(mask_loc[0]))

... # Generate a crop box around (mask_loc[1][idx], mask_loc[0][idx])

</code></pre>
<p>After profiling the entire function with <code>line_profiler</code>, I find that the line <code>mask_loc = np.where(mask_np)</code> is one of the performance bottle-necks. How can I optimize this part? Is there another more efficient way to randomly sample one positive point from a binary image?</p>
","2024-12-16 03:18:50","3","Question"
"79283428","79274150","","<p>There is a known issue with PyTorch 2.5.</p>
<p>You can read more about it <a href=""https://github.com/pytorch/pytorch/issues/142344"" rel=""nofollow noreferrer"">here</a>.</p>
<p>One solution is to download the source code, modify the two files as described by <a href=""https://github.com/pytorch/pytorch/issues/142344#issuecomment-2541767272"" rel=""nofollow noreferrer"">malfet</a>, and then compile the source code. This issue is expected to be resolved in PyTorch 2.6.</p>
<p>However, the simplest solution I found was to install the nightly versions of PyTorch:</p>
<p><code>pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu</code></p>
<p>Best regards,</p>
<p>--KC</p>
","2024-12-16 00:42:41","1","Answer"
"79282909","79281681","","<p>From the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"" rel=""nofollow noreferrer"">layernorm documentation</a>:</p>
<pre><code>torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, bias=True, device=None, dtype=None)

&quot;&quot;
Applies Layer Normalization over a mini-batch of inputs.

    This layer implements the operation as described in
    the paper `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`__

    .. math::
        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated over the last `D` dimensions, where `D`
    is the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`
    is ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over
    the last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``).
    :math:`\gamma` and :math:`\beta` are learnable affine transform parameters of
    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.
    The standard-deviation is calculated via the biased estimator, equivalent to
    `torch.var(input, unbiased=False)`.
&quot;&quot;&quot;
</code></pre>
<p>As the documentation says, <strong>The mean and standard-deviation are calculated over the last <code>D</code> dimensions</strong>. If you create a layer as <code>nn.LayerNorm(d_model)</code>, it assumes the input will have a last dimension of shape <code>d_model</code> and apply layernorm over that dimension. The other dimensions of the tensor are not relevant.</p>
","2024-12-15 18:16:10","1","Answer"
"79281777","79280126","","<p>The error</p>
<pre><code>AttributeError: 'RecursiveScriptModule' object has no attribute 'compute'
</code></pre>
<p>is caused by the fact that only <code>forward</code> and other methods <code>forward</code> eventually calls recursively are scripted (compiled) by default. Since <code>compute</code> is not called by <code>forward</code>, it is not included in the scripted module. To do so, one must decorate it with <code>@torch.jit.export</code>.</p>
<p>Because I cannot change the original library, I can create a wrapper module whose <code>forward</code> method calls <code>LibraryModule.compute</code>:</p>
<pre><code>class WrapperModule(nn.Module):
    &quot;&quot;&quot;A wrapper module that uses the library module&quot;&quot;&quot;
    def __init__(self, in_features, out_features):
        super().__init__()
        self.lib = LibraryModule(in_features, out_features)

    def forward(self, x, some_class_object: SomeClass):
        return self.lib.compute(x, some_class_object)


script = torch.jit.script(WrapperModule(3, 2))
print(script(torch.tensor([10., 20., 30.]), SomeClass(torch.tensor(2))))
</code></pre>
<p>This works, although note that I also needed to change the main tensor's type to float, and needed to pass a tensor to <code>SomeClass</code>.</p>
<p>The second error shown in my question is somewhat unrelated. When I call</p>
<pre><code>torch.jit.script(LibraryModule(3, 2).compute)
</code></pre>
<p>somehow the reference to <code>self</code> gets lost and I can't access <code>self.linear</code> anymore. This is odd since <code>LibraryModule(3, 2).compute</code> should be a closure capturing <code>self</code>, but perhaps this is a Torchscript quirk (note how the code below works fine):</p>
<pre><code>f = LibraryModule(3, 2).compute
print(f(torch.tensor([10., 20., 30.]), SomeClass(torch.tensor(2))))
</code></pre>
","2024-12-15 04:01:02","0","Answer"
"79281681","","How to correctly apply LayerNorm after MultiheadAttention with different input shapes (batch_first vs default) in PyTorch?","<p>I’m working on an audio recognition task using a Transformer-based model in PyTorch. My input features are generated by a CNN-based embedding layer and have the shape [batch_size, d_model, n_token], where n_token is the sequence length and d_model is the feature dimension.</p>
<p>By default, nn.MultiheadAttention (when batch_first=False) expects input in the shape (seq, batch, feature). To make things more intuitive, I chose to set batch_first=True and then permute my data from [batch_size, d_model, n_token] to [batch_size, n_token, d_model] so that the time dimension comes before the feature dimension. Here’s a simplified code snippet:</p>
<pre class=""lang-py prettyprint-override""><code># Original shape: [batch_size, d_model, n_token]
data = concat_cls_token(data)   # [batch_size, d_model, n_token+1]
data = data.permute(0, 2, 1)    # [batch_size, n_token+1, d_model]

multihead_att = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
data, _ = multihead_att(data, data, data)
# Result shape: [batch_size, n_token+1, d_model]
</code></pre>
<p>After applying multi-head attention, I use LayerNorm(d_model) directly on this [batch_size, n_token+1, d_model] tensor. My understanding is that LayerNorm normalizes over the feature dimension, so as long as the feature dimension (d_model) is the last one, it should work fine. But I have two main questions:</p>
<ol>
<li>If I had stuck with the default multi-head attention format (seq, batch, feature)—that is, using [n_token+1, batch_size, d_model]—would LayerNorm(d_model) still correctly normalize along the feature dimension without permuting the tensor again?</li>
<li>In practice, what’s the best approach for tasks like mine (audio sequence recognition)? Is it recommended to keep the data in [batch_size, seq_len, d_model] format before calling LayerNorm, or is it perfectly acceptable to use (seq, batch, feature) as long as the feature dimension is last?</li>
</ol>
<p>Both my advisor and I are a bit uncertain. Below are more details from my forward method and the corresponding AttentionBlock implementation for reference:</p>
<pre class=""lang-py prettyprint-override""><code>def forward(self, x: torch.Tensor):
    # Initial: x is [batch_size, d_model, num_tokens]
    x = self.expand(x)
    x = self.concat_cls_token(x)   # [batch_size, d_model, num_tokens+1]
    x = x.permute(0, 2, 1)         # [batch_size, num_tokens+1, d_model]
    x = self.positional_encoder(x)
    x = self.attention_block(x)    # [batch_size, num_tokens+1, d_model]
    x = x.permute(0, 2, 1)         # [batch_size, d_model, num_tokens+1]

    x = self.get_cls_token(x)      # [batch_size, d_model, 1]
    y = self.class_mlp(x)          # [batch_size, n_classes]
    return y
</code></pre>
<p>and the implement of AttentionBlock:</p>
<pre class=""lang-py prettyprint-override""><code>class AttentionBlock(nn.Module):
    @staticmethod
    def make_ffn(hidden_dim: int) -&gt; torch.nn.Module:
        return nn.Sequential(
            OrderedDict([
                (&quot;ffn_linear1&quot;, nn.Linear(in_features=hidden_dim, out_features=hidden_dim)),
                (&quot;ffn_relu&quot;, nn.ReLU()),
                (&quot;ffn_linear2&quot;, nn.Linear(in_features=hidden_dim, out_features=hidden_dim))
            ])
        )

    def __init__(self, embed_dim, n_head):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, n_head, batch_first=True)
        self.layer_norm1 = nn.LayerNorm(embed_dim)
        self.feed_forward = self.make_ffn(embed_dim)
        self.layer_norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x: torch.Tensor):
        attn_output, _ = self.attention(x, x, x)
        x = self.layer_norm1(x + attn_output)
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + ff_output)
        return x
</code></pre>
","2024-12-15 01:44:59","0","Question"
"79280985","79280773","","<p>From what I understand, the <code>X_train_tensor</code> is output from the autoencoder. When you do not run <code>torch.no_grad()</code> during the encoding step, a computational graph is created for the outputs of the autoencoder, which links the autoencoder's operations and weights to the encoded tensors. In your code, since the model's output uses the <code>X_train_tensor</code>, the model's loss is connected to the autoencoder's computational graph.</p>
<p>When you call <code>loss.backward()</code> the first time, PyTorch traverses the entire computational graph, including the autoencoder, to compute gradients and then clears the graph. When you call <code>loss.backward()</code> in the second iteration of the loop, you are attempting to traverse the cleared autoencoder's computational graph.</p>
<p><code>torch.no_grad()</code> prevents PyTorch from creating the autoencoder computational graph or linking the resulting loss to the autoencoder.</p>
","2024-12-14 16:21:09","2","Answer"
"79280773","","RuntimeError: Trying to backward through the graph a second time on loss tensor","<p>I have the following training code. I am quite sure I call <code>loss.backward()</code> just once, and yet I am getting the error from the title. What am I doing wrong? Note that the <code>X_train_tensor</code> is output from another graph calculation, so it has <code>required_grad=True</code> as you can see in the print statement. Is this the source of the problem, and if so, how can I change it? It won't allow me to toggle it directly on the tensor.</p>
<pre><code>for iter in range(max_iters):
start_ix = 0
loss = None

while start_ix &lt; len(X_train_tensor):
    loss = None
    end_ix = min(start_ix + batch_size, len(X_train_tensor))
    out, loss, accuracy = model(X_train_tensor[start_ix:end_ix], y_train_tensor[start_ix:end_ix])

    # every once in a while evaluate the loss on train and val sets
    if (start_ix==0) and (iter % 10 == 0 or iter == max_iters - 1):
        out_val, loss_val, accuracy_val = model(X_val_tensor, y_val_tensor)
        print(f&quot;step {iter}: train loss={loss:.2f} train_acc={accuracy:.3f} | val loss={loss_val:.2f} val_acc={accuracy_val:.3f}  {datetime.datetime.now()}&quot;)


    optimizer.zero_grad(set_to_none=True)
    print (iter, start_ix, X_train_tensor.requires_grad, y_train_tensor.requires_grad, loss.requires_grad)
    loss.backward()
    optimizer.step()
    start_ix = end_ix + 1
</code></pre>
<p>This is the error:</p>
<pre><code>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
</code></pre>
<p>Update: this is where the model input tensors are coming from, as output of other (autoencoder) model:</p>
<pre><code>autoencoder.eval()
with torch.no_grad(): # it seems like adding this line solves the problem?
    X_train_encoded, loss = autoencoder(X_train_tensor)
    X_val_encoded, loss = autoencoder(X_val_tensor)
    X_test_encoded, loss = autoencoder(X_test_tensor)
</code></pre>
<p>Adding the <code>with torch.no_grad()</code> line above solves the issue, but I don't understand why. Does it actually change how the outputs are generated, how does that work?</p>
","2024-12-14 14:11:59","2","Question"
"79280126","","Torchscript failure: 'RecursiveScriptModule' object has no attribute","<p>I am trying to use PyTorch's Torchscript to script a module defined in a third-party library.</p>
<p>The example below is an abstract version of the problem. Suppose some library I cannot modify defines <code>SomeClass</code> and <code>LibraryModule</code> classes, where the latter is a PyTorch module.</p>
<p><code>LibraryModule</code>'s main method is <code>compute</code>, which takes a tensor and an instance of <code>SomeClass</code>.</p>
<pre><code>import torch
import torch.nn as nn


class SomeClass:
    &quot;&quot;&quot;A utility class in a library I cannot modify&quot;&quot;&quot;
    def __init__(self, x):
        self.x = x


class LibraryModule(nn.Module):
    &quot;&quot;&quot;A module provided in a library I cannot modify&quot;&quot;&quot;
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)

    def compute(self, x, some_class_object: SomeClass):
        &quot;&quot;&quot;
        Main function of my module; like forward, but takes a non-tensor argument
        &quot;&quot;&quot;
        return self.linear(x) * some_class_object.x
</code></pre>
<p>This is what I tried to get a script for the class:</p>
<pre><code>script = torch.jit.script(LibraryModule(3, 2))
print(script.compute(torch.tensor([10, 20, 30]), SomeClass(2)))
</code></pre>
<p>but I get the following error:</p>
<pre><code>  File &quot;torchscript.py&quot;, line 25, in &lt;module&gt;
    print(script.compute(torch.tensor([10, 20, 30]), SomeClass(2)))
          ^^^^^^^^^^^^^^
  File &quot;\Lib\site-packages\torch\jit\_script.py&quot;, line 826, in __getattr__
    return super().__getattr__(attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;Lib\site-packages\torch\jit\_script.py&quot;, line 533, in __getattr__
    return super().__getattr__(attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;Lib\site-packages\torch\nn\modules\module.py&quot;, line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'RecursiveScriptModule' object has no attribute 'compute'. Did you mean: 'compile'?
</code></pre>
<p>I also tried scripting the method directly:</p>
<pre><code>compute_script = torch.jit.script(LibraryModule(3, 2).compute)
print(compute_script(torch.tensor([10, 20, 30]), SomeClass(2)))
</code></pre>
<p>but then I get:</p>
<pre><code>RuntimeError: 
'Tensor (inferred)' object has no attribute or method 'linear'.:
  File &quot;torchscript.py&quot;, line 21
        Main function of my module; like forward, but takes a non-tensor argument
        &quot;&quot;&quot;
        return self.linear(x) * some_class_object.x
               ~~~~~~~~~~~ &lt;--- HERE
</code></pre>
<p>How can I get a working script of <code>LibraryModule.compute</code>?</p>
","2024-12-14 05:37:41","0","Question"
"79279472","","Contrastive Loss from Scratch","<p>I am trying to implement/learn how to implement contrastive loss. Currently my gradients are exploding into infinity and I think I must have misimplemented something. I was wondering if someone could take a look at my loss function and tell if they see an error</p>
<pre><code>class ContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.5):
        super(ContrastiveLoss, self).__init__()
        self.temperature = temperature

    def forward(self, projections_1, projections_2):
        z_i = projections_1
        z_j = projections_2
        z_i_norm = F.normalize(z_i, dim=1)
        z_j_norm = F.normalize(z_j, dim=1)
        cosine_num = torch.matmul(z_i, z_j.T)
        cosine_denom = torch.matmul(z_i_norm, z_j_norm.T)
        cosine_similarity = cosine_num / cosine_denom

        numerator = torch.exp(torch.diag(cosine_similarity) / self.temperature)

        denominator = cosine_similarity
        diagonal_indices = torch.arange(denominator.size(0))
        denominator[diagonal_indices, diagonal_indices] = 0
        denominator = torch.exp(torch.sum(cosine_similarity, dim=1))
        loss = -torch.log(numerator / denominator).sum()
        return loss
</code></pre>
","2024-12-13 20:16:24","1","Question"
"79279437","79279124","","<ol>
<li><p>No, but the <code>Conv2D()</code> layer can work on variable image sizes. The way the <code>Conv2D()</code> layer works is by applying the kernel on each set of kernel_size x kernel_size pixels. The padding adds to the dimensions before the convolution as well. So as long as the image dimensions are at least kernel_size x kernel_size after padding, the convolution will work. In this case, kernel_size is 3 and padding is 1 so even a single pixel will work because after padding, the image will be 3x3. The reason this CNN can't take variable image sizes is because of the linear layer. The linear layer requires a 7x7 image after max-pooling twice, so a 28x28 image to start with.</p>
</li>
<li><p>It depends on the type of layer, linear layers take in a set input size and a set output size, which determines the number of parameters (neurons). The number of parameters for convolution layers are determined by the kernel size and number of output channels so it doesn't rely on input size.</p>
</li>
<li><p>It depends on the model, some models allow variable image sizes even if they were trained on a specific resolution.</p>
</li>
</ol>
","2024-12-13 19:51:00","1","Answer"
"79279124","","Does pytorch CNN care about image size?","<p>I am playing with CNNs these days, and I have code like pasted below. My question is, would this work on any image size? It is not clear to me what parameter or channel, if any, cares about the image size? And if that's the case, how does the model know how many neurons it needs, isn't that a function of image size?</p>
<p>Related point on pretrained models - if I use pretrained models, do I need to reformat my images to be same as what the model was trained on in the first place, or how does that work?</p>
<pre class=""lang-py prettyprint-override""><code>class CNN(nn.Module):
    def __init__(self, num_classes, num_channels=1):
        super(CNN, self).__init__()
        self.num_classes = num_classes
        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc = nn.Linear(64*7*7, num_classes)
</code></pre>
","2024-12-13 17:24:14","1","Question"
"79274510","78604979","","<p>if your feat is the one hot encode with dim of (x size that not 1) with node (N) you may add the another dimension for graph the final shape will be torch.size(1, N_node, M_feat). as you may see in the example of &quot;adjacency matrix&quot; Do you work on drug - ligand interaction?</p>
","2024-12-12 09:30:32","0","Answer"
"79274150","","Getting RuntimeError: view size is not compatible with input tensor's size and stride while using mps","<p>RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</p>
<p>Getting this error only while using mps and not cpu</p>
<pre><code>
fine_tuned_decoder_path = &quot;/path/fine_tuned_decoder&quot;
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    encoder_pretrained_model_name_or_path=&quot;google/vit-base-patch16-224-in21k&quot;,
    decoder_pretrained_model_name_or_path=fine_tuned_decoder_path,
    tie_encoder_decoder=True,
    cache_dir=&quot;/path/datasets/&quot;+&quot;models&quot;  # Directory for caching models
)
os.environ[&quot;WANDB_MODE&quot;] = &quot;disabled&quot;

# Set batch size and number of training epochs
BATCH_SIZE = 16
TRAIN_EPOCHS = 5

# Define the output directory for storing training outputs
output_directory = os.path.join(&quot;path&quot;, &quot;captioning_outputs&quot;)

# Check if MPS is available
device = torch.device(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Move your model to the correct device
model.to(device)

# Set mixed precision and device handling
fp16 = False  # Disable fp16 entirely
mixed_precision = None  # Disable mixed precision (default)

# Training Arguments
training_args = TrainingArguments(
    output_dir=output_directory,
    per_device_train_batch_size=BATCH_SIZE,
    do_train=True,
    num_train_epochs=TRAIN_EPOCHS,
    overwrite_output_dir=True,
    use_cpu=False,  # Ensure you're not using CPU
    dataloader_pin_memory=False,
    fp16=fp16,  # Disable fp16 if using MPS
    bf16=False,  # Disable bf16 if using MPS
    optim=&quot;adamw_torch&quot;,  # Use AdamW Torch optimizer (more stable with mixed precision)
    gradient_checkpointing=False,  # Disable gradient checkpointing if necessary
    logging_dir=os.path.join(output_directory, 'logs'),
    report_to=&quot;none&quot;,  # Disable reporting
    
)

# Use the Trainer with the model on the correct device
trainer = Trainer(
    processing_class=feature_extractor,  # Tokenizer
    model=model,  # Model to train
    args=training_args,  # Training arguments
    train_dataset=train_dataset,  # Training dataset
    data_collator=default_data_collator  # Data collator
)
# Start the training process
trainer.train()
</code></pre>
<p>Tried setting use_cpu=True, it works fine but not with mps</p>
","2024-12-12 07:20:47","1","Question"
"79273576","78228322","","<p>have you checked for deprecation errors? Most likely, the &quot;function&quot; model you're trying to implement has been.</p>
","2024-12-12 01:13:28","-1","Answer"
"79268122","","Any faster and memory-efficient alternative of torch.autograd.functional.jacobian(model.decoder, latent_l)?","<p>I have a decoder <code>model.decoder</code>, which is comprised of a series of Convolutional Batchnorm and ReLU layers. I have a latent vector <code>latent_l</code>, which is a 8 dimensional latent vector, say, has the dimension (1, 8, 1, 1), where 1 is the batch size. I am doing <code>torch.autograd.functional.jacobian(model.decoder, latent_l)</code>, which is taking a huge amount of time, is there any fast approximation for this jacobian?</p>
<p>There is <code>jacrev</code>, but I am not sure if that works for this example where we pass a decoder as a whole and compute the jacobian of the decoder with respect to the latent vector.</p>
<p>When I use <code>torch.autograd.functional.jacobian(model.decoder, latent_l, vectorize=True)</code>, the memory consumption of the GPU increases drastically, leading to the crashing of the program. Is there any efficient way of doing this using Pytorch?</p>
","2024-12-10 11:45:49","0","Question"
"79268108","","Correct way of using foreach_worker and foreach_env","<p>I am quite new to Reinforcement Learning and can’t understand it. I am unable to update configurations for the batch data using PPO.</p>
<p>I am using my custom-defined GYM environment, and want to train it using PPO and my external data which I’m loading in the form of torch DataLoader.
I am using Python 3.11 and Ray 2.40.0. Following is the relevant code:</p>
<pre><code>import ray
from ray.rllib.algorithms.ppo import PPOConfig
from ray.tune.registry import register_env
from torch.utils.data import DataLoader

train_dataset = MultimodalDataset(
    csv_file=config.TRAIN_CSV_PATH, max_images=config.MAX_IMAGES_RL
)
train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)

# Define PPO configuration
ppo_config = (
    PPOConfig()
    .training(gamma=0.9, lr=0.01)
    .environment(env=&quot;MultimodalSummarizationEnv&quot;, env_config=default_env_config)
    .framework(&quot;torch&quot;)
    .resources(num_gpus=0, num_cpus_per_worker=1)
)

# Create PPO trainer
trainer = ppo_config.build()

# Function to update worker environments
def update_env_config_and_reset(worker, new_env_config):
    worker.foreach_env(lambda env: env.reset(env_config=new_env_config))

# Training loop
for batch_idx, batch in enumerate(train_loader):
    # Prepare batch-specific env_config
    new_env_config = {
# new data for the batch_idx
    }

    # Update and reset environments for all workers
    trainer.workers.foreach_worker(
        lambda worker: update_env_config_and_reset(worker, new_env_config)
    )

    # Train PPO
    result = trainer.train()
ray.shutdown()
</code></pre>
<p>However, when running the code I get the error on foreach_worker as follows:</p>
<blockquote>
<p>‘function’ object has no attribute ‘foreach_worker’</p>
</blockquote>
<p>Please help me identify where am I getting it wrong.</p>
<p>EDIT: Here's the MWE.</p>
<pre><code>import ray
from ray.rllib.algorithms.ppo import PPOConfig
from ray.tune.registry import register_env
from torch.utils.data import DataLoader

train_dataset = MultimodalDataset(
    csv_file=config.TRAIN_CSV_PATH, max_images=config.MAX_IMAGES_RL
)
train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)

# Define PPO configuration
ppo_config = (
    PPOConfig()
    .training(gamma=0.9, lr=0.01)
    .environment(env=&quot;MultimodalSummarizationEnv&quot;, env_config=default_env_config)
    .framework(&quot;torch&quot;)
    .resources(num_gpus=0, num_cpus_per_worker=1)
)

# Create PPO trainer
trainer = ppo_config.build()

# Function to update worker environments
def update_env_config_and_reset(worker, new_env_config):
    worker.foreach_env(lambda env: env.reset(env_config=new_env_config))

# Training loop
for batch_idx, batch in enumerate(train_loader):
    # Prepare batch-specific env_config
    new_env_config = {
# new data for the batch_idx
    }

    # Update and reset environments for all workers
    trainer.workers.foreach_worker(
        lambda worker: update_env_config_and_reset(worker, new_env_config)
    )

    # Train PPO
    result = trainer.train()
ray.shutdown()
</code></pre>
","2024-12-10 11:42:36","1","Question"
"79267821","","cmake cant find onnx runtime lib","<p>Here is myCMakeLists.txt</p>
<pre><code>cmake_minimum_required(VERSION 3.19)
project(TesteVoxarApp LANGUAGES CXX)

find_package(Qt6 6.5 REQUIRED COMPONENTS Core Widgets LinguistTools)

find_path(ONNX_RUNTIME_SESSION_INCLUDE_DIRS REQUIRED onnxruntime_cxx_api.h HINTS &quot;../../Documentos/onnxruntime-win-x64-gpu-1.20.1/include&quot;)
find_library(ONNX_RUNTIME_LIB REQUIRED onnxruntime HINTS &quot;../../Documentos/onnxruntime-win-x64-gpu-1.20.1/lib&quot;)

qt_standard_project_setup()

qt_add_executable(TesteVoxarApp
        WIN32 MACOSX_BUNDLE
    main.cpp
    mainwindow.cpp
    mainwindow.h
    mainwindow.ui
)

qt_add_translations(
    TARGETS TesteVoxarApp
    TS_FILES TesteVoxarApp_en_US.ts
)

target_include_directories(TesteVoxarApp
    PRIVATE
        ${ONNX_RUNTIME_SESSION_INCLUDE_DIRS}
)
target_link_libraries(TesteVoxarApp
    PRIVATE
        Qt::Core
        Qt::Widgets
        ${ONNX_RUNTIME_LIB}

)

include(GNUInstallDirs)

install(TARGETS TesteVoxarApp
    BUNDLE  DESTINATION .
    RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}
    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}
)

qt_generate_deploy_app_script(
    TARGET TesteVoxarApp
    OUTPUT_SCRIPT deploy_script
    NO_UNSUPPORTED_PLATFORM_ERROR
)
install(SCRIPT ${deploy_script})

</code></pre>
<p>When i run the project with this code it loads fine. But when i try to call my onnx library in main.cpp says:
07:05:50: Starting C:\Users\ange4\OneDrive\Documentos\TesteVoxarApp\build\Desktop_Qt_6_8_1_MSVC2022_64bit-Debug\TesteVoxarApp.exe...
The given version [20] is not supported, only version 1 to 10 is supported in this build.
07:05:51: The process crashed.</p>
<p>Code in main.cpp:</p>
<pre><code>#include &quot;mainwindow.h&quot;

#include &lt;QApplication&gt;
#include &lt;QLocale&gt;
#include &lt;QTranslator&gt;
#include &quot;onnxruntime_cxx_api.h&quot;

int main(int argc, char *argv[])
{
    QApplication a(argc, argv);
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, &quot;ONNXRuntime&quot;);


    QTranslator translator;
    const QStringList uiLanguages = QLocale::system().uiLanguages();
    for (const QString &amp;locale : uiLanguages) {
        const QString baseName = &quot;TesteVoxarApp_&quot; + QLocale(locale).name();
        if (translator.load(&quot;:/i18n/&quot; + baseName)) {
            a.installTranslator(&amp;translator);
            break;
        }
    }
    MainWindow w;
    w.show();
    return a.exec();
}

</code></pre>
<p>There is no issue or error underline in code. Can anyone help?</p>
","2024-12-10 10:08:09","0","Question"
"79267242","79267009","","<p>They are the same thing. The pytorch <code>distributions</code> module imports all distributions to the <code>distributions</code> namespace for convenience. You can see the code <a href=""https://github.com/pytorch/pytorch/blob/main/torch/distributions/__init__.py"" rel=""nofollow noreferrer"">here</a></p>
","2024-12-10 06:43:06","1","Answer"
"79267009","","Any Difference between `torch.distributions.normal.Normal` v.s. `torch.distributions.Normal`","<p>I only see <code>torch.distributions.normal.Normal</code> in the official documents but never see <code>torch.distributions.Normal</code>. However, I sometimes see people use <code>torch.distributions.Normal</code> in their codes in Github. Could anyone explain if there is any difference between these two usages? Thanks a lot!</p>
<p>I'm confused why there is nothing about <code>torch.distributions.Normal</code> in the official document.
<a href=""https://pytorch.org/docs/stable/distributions.html#normal"" rel=""nofollow noreferrer"">Official Document</a></p>
","2024-12-10 04:24:56","0","Question"
"79266767","","How to handle heterogenous GNN?","<p>I have created this data:</p>
<pre><code>HeteroData(
  user={ x=[100, 16] },
  keyword={ x=[321, 16] },
  tweet={ x=[1000, 16] },
  (user, follow, user)={ edge_index=[2, 291] },
  (user, tweetedby, tweet)={ edge_index=[2, 1000] },
  (keyword, haskeyword, tweet)={ edge_index=[2, 3752] }
)
</code></pre>
<p>And these two varibales based on that:</p>
<pre><code>x_dict:
user: torch.Size([100, 16])
keyword: torch.Size([321, 16])
tweet: torch.Size([1000, 16])

edge_index_dict:
('user', 'follow', 'user'): torch.Size([2, 291])
('user', 'tweetedby', 'tweet'): torch.Size([2, 1000])
('keyword', 'haskeyword', 'tweet'): torch.Size([2, 3752])
</code></pre>
<p>My nodes were indexed from 0 to 99 for users, from 100 to 420 for keywords, and next for tweet nodes.</p>
<p>When I want to run this model:</p>
<pre><code>class HeteroGATBinaryClassifier(torch.nn.Module):
    def __init__(self, metadata, in_channels, hidden_channels, heads=1):
        super().__init__()
        self.metadata = metadata  # Metadata about node and edge types

        # Define GNN layers for each edge type
        self.conv1 = HeteroConv({
            edge_type: GATConv(in_channels, hidden_channels, heads=heads, add_self_loops = False)
            for edge_type in metadata[1]
        }, aggr='mean')  # Aggregate using mean

        self.conv2 = HeteroConv({
            edge_type: GATConv(hidden_channels * heads, hidden_channels, heads=heads, add_self_loops = False)
            for edge_type in metadata[1]
        }, aggr='mean')

        # Linear layer for classification (binary output)
        self.classifier = Linear(hidden_channels * heads, 1)  # Single output for binary classification

    def forward(self, x_dict, edge_index_dict, target_node_type):
        # First GAT layer
        x_dict = self.conv1(x_dict, edge_index_dict)
        x_dict = {key: F.elu(x) for key, x in x_dict.items()}

        # Second GAT layer
        x_dict = self.conv2(x_dict, edge_index_dict)
        x_dict = {key: F.elu(x) for key, x in x_dict.items()}

        # Apply the classifier only on the target node type
        logits = self.classifier(x_dict[target_node_type])  # Logits for target node type
        return torch.sigmoid(logits)  # Output probabilities for binary classification
</code></pre>
<p>But I see the following error:</p>
<blockquote>
<p>IndexError: Found indices in 'edge_index' that are larger than 999 (got 1420). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 1000) in your node feature matrix and try again.</p>
</blockquote>
<p>Do you have any idea how to handle it?</p>
","2024-12-10 00:42:57","1","Question"
"79265069","79264683","","<p>The error is typical when trying to open a gzip file as if it was a pickle or pytorch file, because gzips start with a <code>1f</code> byte. But this is not a valid gzip: it looks like a corrupted pytorch file.</p>
<p>Indeed, looking at <code>hexdump -C file.pt  | head</code> (shown below), most of it looks like a pytorch file (which should be a ZIP archive, not gzip, containing a python pickle file named data.pkl). But the first few bytes are wrong: instead of starting like a ZIP file as it should (bytes <code>50 4B</code> or ASCII <code>PK</code>), it starts like a <code>gzip</code> file (<code>1f 8b 08 08</code>). In fact it's exactly as if the first 31 bytes were replaced with a valid, empty gzip file (with a timestamp <code>ff 35 29 67</code> pointing to November 4, 2024 9:00:47 PM GMT).</p>
<p>Your file:</p>
<pre><code>00000000  1f 8b 08 08 ff 35 29 67  02 ff 43 44 46 32 5f 30  |.....5)g..CDF2_0|
00000010  2e 70 74 68 00 03 00 00  00 00 00 00 00 00 00 44  |.pth...........D|
00000020  46 32 5f 30 2f 64 61 74  61 2e 70 6b 6c 46 42 0f  |F2_0/data.pklFB.|
00000030  00 5a 5a 5a 5a 5a 5a 5a  5a 5a 5a 5a 5a 5a 5a 5a  |.ZZZZZZZZZZZZZZZ|
00000040  80 02 7d 71 00 28 58 08  00 00 00 62 65 73 74 5f  |..}q.(X....best_|
00000050  61 63 63 71 01 63 6e 75  6d 70 79 2e 63 6f 72 65  |accq.cnumpy.core|
...
</code></pre>
<p>(<a href=""https://formats.kaitai.io/python_pickle/"" rel=""nofollow noreferrer"">inspecting</a> the pickle data we can see a dictionary <code>{&quot;best_acc&quot;: ..., &quot;state_dict&quot;: ...})</code> with the typical contents of a checkpoint of a pytorch model).</p>
<p>A valid zipped pickle produced by <code>torch.save({&quot;best_acc&quot;: np.array([1]), &quot;state_dict&quot;: ...}, &quot;CDF2_0.pth&quot;)</code>:</p>
<pre><code>00000000  50 4b 03 04 00 00 08 08  00 00 00 00 00 00 00 00  |PK..............|
00000010  00 00 00 00 00 00 00 00  00 00 0f 00 13 00 43 44  |..............CD|
00000020  46 32 5f 30 2f 64 61 74  61 2e 70 6b 6c 46 42 0f  |F2_0/data.pklFB.|
00000030  00 5a 5a 5a 5a 5a 5a 5a  5a 5a 5a 5a 5a 5a 5a 5a  |.ZZZZZZZZZZZZZZZ|
00000040  80 02 7d 71 00 28 58 08  00 00 00 62 65 73 74 5f  |..}q.(X....best_|
00000050  61 63 63 71 01 63 6e 75  6d 70 79 2e 63 6f 72 65  |accq.cnumpy.core|
...
</code></pre>
<p>A gzip containing an empty file with the same name and timestamp (with <code>gzip --best</code>) has 31 bytes, the same as your file's prefix (except for the two <a href=""https://commandlinefanatic.com/cgi-bin/showarticle.cgi?article=art053"" rel=""nofollow noreferrer"">'Operating System'</a> bytes):</p>
<pre><code>00000000  1f 8b 08 08 ff 35 29 67  02 03 43 44 46 32 5f 30  |.....5)g..CDF2_0|
00000010  2e 70 74 68 00 03 00 00  00 00 00 00 00 00 00     |.pth...........|
</code></pre>
<p><strong>Edit:</strong>
Here's a script that might fix such files in general:</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3
import os
import sys
from pathlib import Path
from shutil import copy2
from tempfile import TemporaryDirectory

import numpy as np
import torch

CHUNK_SIZE = 4

def main(orig_path: Path) -&gt; None:
    fixed_path = orig_path.with_suffix(&quot;.fixed.pth&quot;)
    copy2(orig_path, fixed_path)

    with TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir) / orig_path.name
        torch.save({&quot;best_acc&quot;: np.array([1]), &quot;state_dict&quot;: {}}, temp_path)

        with open(temp_path, &quot;rb&quot;) as f_temp:
            with open(fixed_path, &quot;rb+&quot;) as f_fixed:
                while True:
                    content = f_fixed.read(CHUNK_SIZE)
                    replacement = f_temp.read(CHUNK_SIZE)
                    if content == replacement:
                        break
                    print(f&quot;Replacing {content!r} with {replacement!r}&quot;)
                    f_fixed.seek(-CHUNK_SIZE, os.SEEK_CUR)
                    f_fixed.write(replacement)


if __name__ == &quot;__main__&quot;:
    assert len(sys.argv) == 2, &quot;Expected exactly one argument (the path to the broken .pth file).&quot;
    main(Path(sys.argv[1]))
</code></pre>
","2024-12-09 13:03:47","6","Answer"
"79264683","","Error loading Pytorch model checkpoint: _pickle.UnpicklingError: invalid load key, '\x1f'","<p>I'm trying to load the weights of a Pytorch model but getting this error: <code>_pickle.UnpicklingError: invalid load key, '\x1f'.</code></p>
<p>Here is the weights loading code:</p>
<pre><code>import os
import torch
import numpy as np
# from data_loader import VideoDataset
import timm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device being used:', device)

mname = os.path.join('./CDF2_0.pth')
checkpoints = torch.load(mname, map_location=device)
print(&quot;Checkpoint loaded successfully.&quot;)
model = timm.create_model('legacy_xception', pretrained=True, num_classes=2).to(device)
model.load_state_dict(checkpoints['state_dict'])
model.eval()

</code></pre>
<p>I have tried with different Pytorch versions. I have tried to inspect the weights by changing the extension to .zip and opening with archive manager but can't fix the issue. Here is a <a href=""https://drive.google.com/file/d/1wGsqX1LGkdmbH5tNxieX_rm2IcViH3d6/view?usp=sharing"" rel=""nofollow noreferrer"">public link</a> to the weights .pth file, I'm trying to load. Any help is highly appreciated as I have around 40 trained models that took around one month for training!</p>
","2024-12-09 10:47:29","3","Question"
"79264453","79253276","","<p>I found a way to overcome the diamond inheritance by explicitly calling the <code>__init__</code> of the parent class (not by calling from <code>super()</code>) like the below example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.nn.functional as F



class A(nn.Module):

    def __init__(self, ratio=4, *args, **kwargs):
        super().__init__()

        self.conv_base = nn.Conv2d(3, 3 * ratio, 3, 1, 1)


class B(A):

    def __init__(self, b_args, **kwargs):
        A.__init__(self, ratio=4)

        self.conv1 = nn.Conv2d(4, 3, 1, 1, 0)


class C(A):

    def __init__(self, c_args, **kwargs):
        A.__init__(self, ratio=4)

        self.conv2 = nn.Conv2d(4, 3, 1, 1, 0)


class D(B, C):

    def __init__(self, b_args, c_args):
        B.__init__(self, b_args=b_args)
        C.__init__(self, c_args=c_args)

        self.conv3 = nn.Conv2d(4, 3, 1, 1, 0)


b_args = dict(a=1)
c_args = dict(b=2)
model = D(b_args, c_args)
</code></pre>
<p>You will need to explicitly define the parent class for each children class from the moment diamond inheritance start.</p>
","2024-12-09 09:31:11","0","Answer"
"79262305","79227337","","<blockquote>
<p>RuntimeError: Calling linear solver with sparse tensors requires
compiling PyTorch with CUDA cuDSS and is not supported in ROCm build.</p>
</blockquote>
<p>It means to use <code>torch.sparse.spsolve</code>, you have to</p>
<ol>
<li><strong>compile Pytorch using cuDSS (which is a <a href=""https://docs.nvidia.com/cuda/cudss/index.html"" rel=""nofollow noreferrer"">C++ lib</a>)</strong>, rather than using binaries of pytorch from <code>pip install torch</code>.</li>
<li>use Nvidia GPU (is not supported in ROCm build of AMD GPU).</li>
</ol>
<p>You have made the second point as you use RTX 4090, so you only need to <a href=""https://developer.nvidia.com/cudss-downloads"" rel=""nofollow noreferrer"">install cuDSS</a> and compile pytorch from the source, refer to <a href=""https://github.com/pytorch/pytorch?tab=readme-ov-file#from-source"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch?tab=readme-ov-file#from-source</a>. Note you should <code>export USE_CUDSS=1</code> before compile.</p>
<p>Using the compiled pytorch with cuDSS, the following demo codes run OK.
<a href=""https://i.sstatic.net/rE7oyLak.png"" rel=""nofollow noreferrer"">demo codes</a></p>
","2024-12-08 11:02:21","0","Answer"
"79260428","79255816","","<p>The first thing I noticed was that your code returns a tensor of losses, but what is expected is a single scalar value. Here are a couple of (unofficial) implementations of this paper on Github [<a href=""https://github.com/ranandalon/mtl/blob/master/utils/loss_handler.py#L13-L19"" rel=""nofollow noreferrer"">1</a>] [<a href=""https://github.com/Mikoto10032/AutomaticWeightedLoss/blob/master/AutomaticWeightedLoss.py"" rel=""nofollow noreferrer"">2</a>]</p>
<p>This could be an implementation in Pytorch</p>
<pre><code>def apply_uncertainty_weights(sigmas_sq, losses):
    &quot;&quot;&quot;
    Applies uncertainty weights on multiple task losses.
    
    Args:
        sigmas_sq (torch.Tensor): A tensor of learned variances (sigmas squared) for each task.
        losses (torch.Tensor): A tensor of individual task losses.
        
    Returns:
        torch.Tensor: The total weighted loss.
    &quot;&quot;&quot;
    precisions = 1.0 / (2.0 * sigmas_sq)

    # 1/2.0 is applied to the last additive part because we are doing log(sigma_sq) which is basically 2 * log(sigma) 
    weighted_losses = precisions * losses + (1.0 / 2.0) * torch.log(sigmas_sq)

    total_loss = torch.sum(weighted_losses)

    return total_loss
</code></pre>
<p>Seems to be produce a positive loss value</p>
<pre><code>import torch

sigmas_sq = torch.tensor([0.5, 1.0, 2.0], requires_grad=True)
losses = torch.tensor([0.2, 0.4, 0.1])

total_loss = apply_uncertainty_weights(sigmas_sq, losses)

print(total_loss)

</code></pre>
","2024-12-07 10:32:58","3","Answer"
"79259984","","PyTorch type for dataset with length","<p>I am creating a meta-dataset that combines data from multiple input datasets.</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data import Dataset, IterableDataset

class MetaDataset(Dataset):
    def __init__(self, regular_dataset: Dataset, iterable_dataset: IterableDataset):
        self.regular_dataset = regular_dataset
        self.iterable_dataset = iterable_dataset
        pass # Do other stuff...
</code></pre>
<p>I got a type warning when I tried to access <code>len(self.regular_dataset)</code> from within <code>MetaDataset</code></p>
<p>It turns out that the PyTorch <a href=""https://github.com/pytorch/pytorch/blob/2d9b0810122375fc110f07b9d1780ef7b593a3eb/torch/utils/data/dataset.py#L43-L74"" rel=""nofollow noreferrer"">type definition for <code>Dataset</code></a> <a href=""https://github.com/pytorch/pytorch/blob/2d9b0810122375fc110f07b9d1780ef7b593a3eb/torch/utils/data/sampler.py#L85-L110"" rel=""nofollow noreferrer"">intentionally doesn't include <code>__len__</code></a>. Therefore, I have to build my own type:</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data import Dataset, IterableDataset

class DatasetWithLength(Dataset):
    def __len__(self) -&gt; int:
        pass

class MetaDataset(Dataset):
    def __init__(self, regular_dataset: DatasetWithLength, iterable_dataset: IterableDataset):
        self.regular_dataset = regular_dataset
        self.iterable_dataset = iterable_dataset
        pass # Do other stuff...
</code></pre>
<p>But now, I get a <code>Expected type 'DatasetWithLength', got 'FirstDataset' instead</code> warning when I try to do this:</p>
<pre class=""lang-py prettyprint-override""><code>foo = MetaDataset(
    FirstDataset(),
    FirstIterableDataset(),
)
</code></pre>
<p>How do I correctly define the type of a PyTorch Dataset that has the length property?</p>
","2024-12-07 04:48:38","0","Question"
"79259985","79259984","","<p>You need to define a generic type, so that you don't need to extend from <code>DatasetWithLength</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Generic, TypeVar

class _DatasetWithLength(Dataset):
    def __len__(self) -&gt; int:
        ...

T = TypeVar('T', bound=_DatasetWithLength)
DatasetWithLength = Generic[T]
</code></pre>
","2024-12-07 04:48:38","0","Answer"
"79259132","79258691","","<p>You can do this with a custom <code>backward</code> function:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

class UniqueForward(torch.autograd.Function):
    @staticmethod
    def forward(ctx, inputs):
        unique_inputs, inverse_indices = torch.unique(inputs, return_inverse=True)
        unique_counts = torch.bincount(inverse_indices)
        ctx.save_for_backward(inverse_indices, unique_counts)
        ctx.input_shape = inputs.shape
        return unique_inputs, inverse_indices

    @staticmethod
    def backward(ctx, grad_unique, grad_inverse):
        inverse_indices, unique_counts = ctx.saved_tensors
        grad_inputs = grad_unique[inverse_indices]
        grad_inputs = grad_inputs / unique_counts[inverse_indices]
        return grad_inputs

class EfficientFunction(nn.Module):
    def __init__(self, function):
        # function should be a callable element-wise function
        super().__init__()
        self.function = function
        self.unique_forward = UniqueForward.apply
        
    def forward(self, inputs):
        unique_inputs, inverse_indices = self.unique_forward(inputs)
        unique_results = self.function(unique_inputs)
        full_results = unique_results[inverse_indices]
        return full_results
</code></pre>
<p>The <code>EfficientFunction</code> module should work for any case where 1) the <code>inputs</code> have duplicated values and 2) the <code>function</code> in question applies an element-wise operation.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>def expensive_function(x):
    # use exp as example of expensive function
    return torch.exp(x)

efficient_function = EfficientFunction(expensive_function)

# test efficient version
inputs = torch.rand(100)
inputs = torch.round(inputs, decimals=2)
inputs.requires_grad_(True)

outputs = efficient_function(inputs)
loss = outputs.mean()
loss.backward()

# test naive version
inputs2 = inputs.clone().detach().requires_grad_(True)
naive_results = expensive_function(inputs2)
naive_loss = naive_results.mean()
naive_loss.backward()

# compare gradients 
torch.allclose(inputs.grad, inputs2.grad)
</code></pre>
","2024-12-06 19:11:29","1","Answer"
"79258691","","How to use torch.unique to filter duplicate values, calculate an expensive function, map it back, and then calculate the gradient?","<p>I'm trying to optimize a computation in PyTorch by first identifying the unique elements of a tensor, applying an expensive function (e.g., torch.exp) to these unique elements only, and then mapping the results back to the original tensor's shape to calculate the gradient with respect to the original tensor.</p>
<p>My motivation is to avoid redundant calculations of the expensive function for duplicate values in the input tensor, which would significantly improve performance.</p>
<p>Here's the code snippet that demonstrates my approach, but it results in an error since unique is not differentiable:</p>
<pre><code>import torch

inputs = torch.rand(100)
inputs = torch.round(inputs, decimals=2)
inputs.requires_grad_(True)

unique_inputs, inverse_indices = torch.unique(inputs, return_inverse=True)
print(f&quot;There are {unique_inputs.numel()} unique elements.&quot;)

unique_exp = torch.exp(unique_inputs)
full_exp = unique_exp[inverse_indices]

torch.autograd.grad(full_exp[0], inputs) # &lt;-- Error here
</code></pre>
<p>Is there another way I can do that?</p>
","2024-12-06 16:24:02","0","Question"
"79258249","79255608","","<p>A numpy based answer which may point to a tensor flow one.
A line by line version:</p>
<pre><code>import numpy as np

arr = np.array([[0, 1, 0, 0, 0, 1],[0, 0, 1, 0, 1, 0],[1, 0, 0, 0, 0, 0]])

arr_eq0 = arr ^ 1  # Invert the apr

cum0s = arr_eq0.cumsum(axis = 1 )   # Cumulative zero count

cum0s
# array([[1, 1, 2, 3, 4, 4],
#        [1, 2, 2, 3, 3, 4],
#        [0, 1, 2, 3, 4, 5]])

cum_steps = cum0s * arr       # Zero where arr == 0

cum_steps
# array([[0, 1, 0, 0, 0, 4],
#        [0, 0, 2, 0, 3, 0],
#        [0, 0, 0, 0, 0, 0]])

cum_steps = np.maximum.accumulate( cum_steps, axis = 1 )

cum_steps    # Flat line where arr== 0
# array([[0, 1, 1, 1, 1, 4],
#        [0, 0, 2, 2, 3, 3],
#        [0, 0, 0, 0, 0, 0]])

cum0s - cum_steps    # The difference is the cumulative run of zeroes within each run of zeros
# array([[1, 0, 1, 2, 3, 0],
#        [1, 2, 0, 1, 0, 1],
#        [0, 1, 2, 3, 4, 5]])

(cum0s - cum_steps).max( axis = 1 )  # Max by row
# array([3, 2, 5])
</code></pre>
<p>As a function:</p>
<pre><code>def longest_zero_run( arr ):
    arr_eq0 = arr ^ 1
    cum0s = arr_eq0.cumsum(axis = 1 )
    cum_steps = np.maximum.accumulate( cum0s * arr, axis = 1 )
    return (cum0s - cum_steps).max( axis = 1 )
</code></pre>
","2024-12-06 13:58:11","2","Answer"
"79257965","79255608","","<p>I don't have <code>torch</code> installed, but I guess you can borrow the idea below by using <code>numpy</code> and <code>string.split</code></p>
<pre><code>np.array(
    [
        max(np.vectorize(len)(s.split(&quot;1&quot;)))
        for s in np.apply_along_axis(lambda x: &quot;&quot;.join(x), 1, input.astype(str))
    ]
)
</code></pre>
<p>which should give</p>
<pre><code>array([3, 2, 5])
</code></pre>
","2024-12-06 12:19:20","3","Answer"
"79256271","79255608","","<p>This solution is based on bitwise operations:</p>
<pre><code>import numpy as np

a = np.array([
    [0, 1, 0, 0, 0, 1],
    [0, 0, 1, 0, 1, 0],
    [1, 0, 0, 0, 0, 0],
    ])
COLS = a.shape[1]
LEFT = (1&lt;&lt;COLS)-1

def get_max_zeros(x):
    r = 0
    l = LEFT                    # 1s in the number of 0s when the loop is break
    while True:
        if x:
            b = (x^(x-1))//2    # Set trailing 0s to 1s and zero rest
            r |= b              # Find the maximum
            p = 2*(b+1)         # 2 to the power of  number of 0s + 1
            x //= p             # Right shift by the number of 0s + 1
            l //= p             # Right shift by the number of 0s + 1
        else:
            r |= l              # Find the maximum
            break
    return r.bit_count()

max_zeros = np.array(list(map(
    get_max_zeros,
    np.sum(a*[1&lt;&lt;c for c in range(COLS)], axis=1))))

print(max_zeros)
</code></pre>
<p>It works for an unlimited number of binary digits. I stopped checking when I reached the value of 10000.</p>
","2024-12-05 21:23:25","2","Answer"
"79255887","79255608","","<p>For a torch implementation, you can use <code>scatter_reduce</code> in place of <code>np.maximum.at</code></p>
<pre class=""lang-py prettyprint-override""><code>def test_np(x):
    padded_matrix = np.pad(x, ((0, 0), (1, 1)), constant_values=1)

    diffs = np.diff(padded_matrix, axis=1)

    start_indices = np.where(diffs == -1)
    end_indices = np.where(diffs == 1)
    run_lengths = end_indices[1] - start_indices[1]

    max_zeros = np.zeros(x.shape[0], dtype=int)
    np.maximum.at(max_zeros, start_indices[0], run_lengths)
    return max_zeros

def test_torch(x):
    padded_matrix = F.pad(x, (1,1), value=1)
    diffs = torch.diff(padded_matrix, axis=1)
    
    start_indices = torch.where(diffs==-1)
    end_indices = torch.where(diffs == 1)
    run_lengths = end_indices[1] - start_indices[1]
    max_zeros = torch.zeros(x.shape[0]).long().scatter_reduce(
        dim=0,
        index=start_indices[0],
        src=run_lengths,
        reduce='amax'
    )

    return max_zeros

rows = 12
cols = 32
thresh = 0.5
x_torch = (torch.rand(rows, cols)&gt;thresh).long()
x_np = x_torch.numpy()

result_torch = test_torch(x_torch)
result_np = test_np(x_np)
</code></pre>
<p>Benchmarking on my system both versions give equivalent performance on CPU.</p>
","2024-12-05 18:43:18","3","Answer"
"79255838","79249247","","<p>Yes, the input layer needs to be size 49.  The output layer needs to be size 1.  Then it'll work for both the train and test data.</p>
","2024-12-05 18:24:04","-1","Answer"
"79255816","","How to implement self paced multitask weighted loss (Kendall et al. 2018) in pytorch?","<p>In <a href=""https://arxiv.org/abs/1705.07115"" rel=""nofollow noreferrer"">this study</a> authors introduce an equation (equation 7) to weigh the individual losses for different tasks of neural networks.</p>
<p>I want to implement this as a function in <code>pytorch</code> so that I can use for my model. So far, what I have tried is:</p>
<pre><code>import torch

# function to apply uncertainty weighing on the losses
def apply_uncertainty_weights(sigma, loss):

    &quot;&quot;&quot;
    This function applies uncertainty weights on the given loss. 
    NOTE: This implementation is based on the study Kendall et al. 2018 (https://arxiv.org/abs/1705.07115)

    Arguments:
    sigma: A NN learned uncertainty value (initialised as torch.nn.Parameter(torch.zeros(1))
    loss: The calculated losss between the prediction and target

    Returns:
    weighted_loss: Weighted loss
    &quot;&quot;&quot;

    # apply uncertainty weighthing
    # This is the formula in the publication -&gt; weighted_loss = (1 / (2 * sigma**2)) * loss + torch.log(sigma)
    # but we can't use it as it won't be numerically stable/differentiable (e.g. when sigma is predicted to be 0)
    # instead use the following
    sigma = torch.nn.functional.softplus(sigma) + torch.tensor(1e-8) # this makes sure sigma is never exactly 0 or less otherwise the following functions wont work
    log_sigma_squared = torch.log(sigma ** 2) # this is log(sigma^2)
    precision = (1/2) * torch.exp(-log_sigma_squared) # this is 1/sigma^2
    log_sigma = (1/2) * log_sigma_squared # this is log(sigma)
    weighted_loss = precision * loss + log_sigma

    # return the weighted loss
    return weighted_loss
</code></pre>
<p>But strangely this implementation gives me negative loss values during training. What am I doing wrong?</p>
","2024-12-05 18:15:23","0","Question"
"79255608","","Find maximum length of consecutive zeros in each row","<p>My goal is to find the maximum length of consecutive zeros in each row. So for instance, if I have a tensor like</p>
<pre><code>input = torch.tensor([[0, 1, 0, 0, 0, 1],[0, 0, 1, 0, 1, 0],[1, 0, 0, 0, 0, 0]])
</code></pre>
<p>I expect to get the result</p>
<pre><code>tensor([3, 2, 5])
</code></pre>
<p>I’ve done this using numpy (listed below assuming “input” is a numpy binary matrix) and it tends to be very efficient, but I cannot seem to find a way using tensors that is at least as efficient. I’ve tried to follow a similar logic using torch operations but the performance is always worse.
Thanks!</p>
<pre><code>import numpy as np
input = np.array([[0, 1, 0, 0, 0, 1],[0, 0, 1, 0, 1, 0],[1, 0, 0, 0, 0, 0]])
# Pad the matrix with ones
padded_matrix = np.pad(input, ((0, 0), (1, 1)), constant_values=1)

# Compute differences
diffs = np.diff(padded_matrix, axis=1)

# Identify start and end of zero runs
start_indices = np.where(diffs == -1)
end_indices = np.where(diffs == 1)

#Compute lengths of zero runs
run_lengths = end_indices[1] - start_indices[1]

# Create a result array initialized with zeros
max_zeros = np.zeros(binaryGrid.shape[0], dtype=int)

# Use np.maximum.at to find the maximum run length for each row
np.maximum.at(max_zeros, start_indices[0], run_lengths)

print(max_zeros)
</code></pre>
","2024-12-05 17:00:00","9","Question"
"79253682","79253234","","<p>When using <code>mode=&quot;area&quot;</code>, pytorch computes the output using an adaptive average pooling operation. You can find the relevant code <a href=""https://github.com/pytorch/pytorch/blob/31f2d4eb4e9d746d6030a7bba38265e00c2e9bb8/torch/nn/functional.py#L4665"" rel=""nofollow noreferrer"">here</a></p>
<pre class=""lang-py prettyprint-override""><code>...
    if input.dim() == 4 and mode == &quot;area&quot;:
        assert output_size is not None
        return adaptive_avg_pool2d(input, output_size)
...
</code></pre>
<p>You can verify this via:</p>
<pre><code>x = ...
scale_factor = 0.75
pool1 = F.interpolate(x, scale_factor=scale_factor, mode=&quot;area&quot;)
output_size = [int(i*scale_factor) for i in x.shape[-2:]]
pool2 = F.adaptive_avg_pool2d(x, output_size)
pool1 == pool2
</code></pre>
<p>Adaptive average pooling breaks the input into roughly even sized chunks and computes a simple average of each chunk. There is no weighting of pixels like you describe. You can see the code for adaptive pooling <a href=""https://github.com/pytorch/pytorch/blob/31f2d4eb4e9d746d6030a7bba38265e00c2e9bb8/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp#L18"" rel=""nofollow noreferrer"">here</a> and the indexing code <a href=""https://github.com/pytorch/pytorch/blob/aa956182683d4f55a6eebf1f445c83f112502168/aten/src/ATen/native/AdaptivePooling.h#L31C1-L37C2"" rel=""nofollow noreferrer"">here</a>.</p>
<p>It may help to look at the indexing code:</p>
<pre><code>inline int64_t start_index(int64_t a, int64_t b, int64_t c) {
  return (a / b) * c + ((a % b) * c) / b;
}

inline int64_t end_index(int64_t a, int64_t b, int64_t c) {
  return 1 + ((a + 1) * c - 1) / b;
}
</code></pre>
<p>Here, <code>a</code> is the output position in a dimension of size <code>b</code> mapping to an input of size <code>c</code>.</p>
<p>Take your example where <code>scale_factor = 1/(8/6) = 0.75</code>. The input is size <code>(..., 8, 8)</code> so the output will be of size <code>(..., 6, 6)</code> (<code>int(0.75*8) = 6</code>).</p>
<p>You can use the following to compute the value of a specific output element:</p>
<pre class=""lang-py prettyprint-override""><code>def start_index(a, b, c):
    return (a * c) // b

def end_index(a, b, c):
    return ((a + 1) * c + b - 1) // b

input_height = 8
input_width = 8
output_height = 6
output_width = 6
out_row = 0
out_col = 1
h0 = start_index(out_row, output_height, input_height)
h1 = end_index(out_row, output_height, input_height)
w0 = start_index(out_col, output_width, input_width)
w1 = end_index(out_col, output_width, input_width)

kh = h1-h0
kw = w1-w0

x[:, :, h0:h1, w0:w1].sum() / (kh*kw)
</code></pre>
<p>Also note that for adaptive pooling, stride is not constant. For example:</p>
<pre class=""lang-py prettyprint-override""><code>for a in range(6):
    start = start_index(a, 6, 8)
    end = end_index(a, 6, 8)
    print(f&quot;Position {a}: {start} -&gt; {end}&quot;)

Position 0: 0 -&gt; 2
Position 1: 1 -&gt; 3
Position 2: 2 -&gt; 4
Position 3: 4 -&gt; 6 # note the jump here from 2 to 4
Position 4: 5 -&gt; 7
Position 5: 6 -&gt; 8
</code></pre>
","2024-12-05 07:06:22","1","Answer"
"79253276","","how could I write a pytorch model in diamond inheritance way?","<p>I need to implement a complex model, and I better to use Diamond inheritance to satisfy different requirements, here is the toy code to show what I am trying to do:</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.model_zoo as modelzoo
import torch.distributed as dist



class A(nn.Module):

    def __init__(self, ratio=4, *args, **kwargs):
        super().__init__()

        self.conv_base = nn.Conv2d(3, 3 * ratio, 3, 1, 1)


class B(A):

    def __init__(self, b_args, **kwargs):
        super().__init__(ratio=4)

        self.conv1 = nn.Conv2d(4, 3, 1, 1, 0)


class C(A):

    def __init__(self, c_args, **kwargs):
        super().__init__(ratio=4)

        self.conv2 = nn.Conv2d(4, 3, 1, 1, 0)


class D(B, C):

    def __init__(self, b_args, c_args):
        super().__init__(b_args=b_args, c_args=c_args)

        self.conv3 = nn.Conv2d(4, 3, 1, 1, 0)


b_args = dict(a=1)
c_args = dict(b=2)
model = D(b_args, c_args)
</code></pre>
<p>I got the error that:</p>
<pre><code>Traceback (most recent call last):
  File &quot;tmp.py&quot;, line 95, in &lt;module&gt;
    model = D(b_args, c_args)
  File &quot;tmp.py&quot;, line 88, in __init__
    super().__init__(b_args=b_args, c_args=c_args)
  File &quot;tmp.py&quot;, line 72, in __init__
    super().__init__(ratio=4)
TypeError: C.__init__() missing 1 required positional argument: 'c_args'
</code></pre>
<p>Would you please tell me how could I make this work?</p>
<p>By the way, I need <code>B</code> and <code>C</code> to be workable, which means that they are not only parents of <code>D</code>, but also a class that should be able to be instanced.</p>
","2024-12-05 03:00:17","0","Question"
"79253234","","how does the area interpolation for downsampling work when the scale factor is not integer?","<p>by definition area interpolation is just weighted by the pixel area.</p>
<p>So I suppose if scale factor is 1.5, then output pixel 00 contain full pixel of 00, half of the 01 and 10, 1/4 of the 11. the weight will be in_pixel_area/(1.5)^2</p>
<p>However, this does not seem to be the case:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.tensor(
    [[3, 106, 107, 40, 148, 112, 254, 151],
    [62, 173, 91, 93, 33, 111, 139, 25],
    [99, 137, 80, 231, 101, 204, 74, 219],
    [240, 173, 85, 14, 40, 230, 160, 152],
    [230, 200, 177, 149, 173, 239, 103, 74],
    [19, 50, 209, 82, 241, 103, 3, 87],
    [252, 191, 55, 154, 171, 107, 6, 123],
    [7, 101, 168, 85, 115, 103, 32, 11]],
dtype=torch.float).unsqueeze(0).unsqueeze(0)
print(x.shape, x.sum())
for scale in [8/6, 2]:
    
    pixel_area = scale**2
    y = F.interpolate(x, scale_factor=1/scale, mode=&quot;area&quot;)
    print(y.shape, y, y.sum()*pixel_area)
    print((3 + 106*(scale-1) + 62*(scale-1) + 173*(scale-1)**2)/pixel_area)
    print((11 + 123*(scale-1) + 32*(scale-1) + 6*(scale-1)**2)/pixel_area)
</code></pre>
<p>the output is:</p>
<pre><code>torch.Size([1, 1, 8, 8]) tensor(7707.)
torch.Size([1, 1, 6, 6]) tensor([[[[ 86.0000, 119.2500,  82.7500, 101.0000, 154.0000, 142.2500],
          [117.7500, 120.2500, 123.7500, 112.2500, 132.0000, 114.2500],
          [162.2500, 118.7500, 102.5000, 143.7500, 167.0000, 151.2500],
          [124.7500, 159.0000, 154.2500, 189.0000, 112.0000,  66.7500],
          [128.0000, 126.2500, 125.0000, 155.5000,  54.7500,  54.7500],
          [137.7500, 128.7500, 115.5000, 124.0000,  62.0000,  43.0000]]]]) tensor(7665.7778)
43.99999999999999
35.62499999999999
torch.Size([1, 1, 4, 4]) tensor([[[[ 86.0000,  82.7500, 101.0000, 142.2500],
          [162.2500, 102.5000, 143.7500, 151.2500],
          [124.7500, 154.2500, 189.0000,  66.7500],
          [137.7500, 115.5000, 124.0000,  43.0000]]]]) tensor(7707.)
86.0
43.0
</code></pre>
<p>we can see if scale = 2, then it works fine.
But when scale = 8/6, it gives out strange result.</p>
<p>First the <code>y.sum()*pixel_area</code> does not equal to <code>x.sum()</code>
2nd I try to directly calculate the pixel value via weight it give out 44 instead of 86.
3rd I would expect the output 00 pixel has different result when the scale is different, but apparently the 00 is still 86. why?</p>
<p><strong>update</strong>
with closer look, it seems when scale = 8/6, it simply does 2x2 kernel average with stride 1x1. But isn't this against the definition of area interpolation?</p>
","2024-12-05 02:19:16","0","Question"
"79252861","79252519","","<p>You can use <code>.to</code> instead of <code>.view</code>.</p>
<pre class=""lang-py prettyprint-override""><code>@torch.jit.script
def my_function(t: torch.Tensor) -&gt; torch.Tensor:
    if t.dtype != torch.bfloat16:
        raise ValueError(&quot;Input tensor must be of dtype torch.bfloat16&quot;)
    int_type = torch.int16
    t_bits = t.to(int_type)
    return t_bits
</code></pre>
<p>The &quot;overload&quot; in question is that typically <code>.view</code> is used for changing the shape of a tensor, but pytorch &quot;overloads&quot; the <code>.view</code> operation to also allow for changing the tensor <code>dtype</code>. You can use <code>.to</code> instead.</p>
","2024-12-04 22:05:38","1","Answer"
"79252519","","PyTorch > TorchScript tensor.view() alternative?","<p>I have the following code:</p>
<pre class=""lang-py prettyprint-override""><code>@torch.jit.script
def my_function(t: torch.Tensor) -&gt; torch.Tensor:
    if t.dtype != torch.bfloat16:
        raise ValueError(&quot;Input tensor must be of dtype torch.bfloat16&quot;)
    int_type = torch.int16
    t_bits = t.view(int_type) &lt;- error
</code></pre>
<p>That throws:</p>
<pre><code>RuntimeError: shape '[2]' is invalid for input of size 1
</code></pre>
<p>Because, given the documentation:</p>
<pre><code>.. warning::
This overload is not supported by TorchScript, and using it in a 
Torchscript program will cause undefined behavior.
</code></pre>
<p>So the question is, what's the alternative for use?</p>
","2024-12-04 19:37:03","1","Question"
"79252180","79112958","","<p>As mentioned by @mhenning,</p>
<p>You can use <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/scatter_add"" rel=""nofollow noreferrer""><code>tf.scatter_add()</code></a> for <code>Tensorflow 1.15</code>. But, the functions below are deprecated in the latest <code>Tensorflow 2.x</code> and are only available with <code>Tensorflow 1.x</code>:</p>
<ul>
<li><code>tf.scatter_add()</code></li>
<li><code>tf.scatter_div()</code></li>
<li><code>tf.scatter_min()</code></li>
<li><code>tf.scatter_max()</code></li>
<li><code>tf.scatter_mul()</code></li>
</ul>
","2024-12-04 17:30:09","0","Answer"
"79250166","79249787","","<p>1st approach is not a good choice because leveraging the [CLS] token embedding directly might not be the best approach, in case if the BERT  was fine tuned for a task other than similarity matching.</p>
<ul>
<li><strong>Task-Specific Embeddings</strong>: The [CLS] token embedding is affected by the task the bert model was trained on.</li>
<li><strong>Averaging</strong> : Taking the mean of all token embeddings, we can get a more general representation of the input. This method balances out the representation by considering the contextual embeddings of all tokens.</li>
</ul>
<p>Consider taking average or pooling (passing through another dense layer) will work.</p>
","2024-12-04 07:09:59","1","Answer"
"79249868","79248710","","<p>Assume you don't reshape. Your model outputs <code>y_pred</code> of size <code>(batch_size, 1)</code>. Your <code>y</code> tensor is shape <code>(batch_size,)</code> - missing the unit axis. The axis difference causes the <code>y</code> tensor to be broadcast along <code>y_pred</code>, which is wrong. When you compute <code>(y_pred - y)</code>, you get a tensor of shape <code>(batch_size, batch_size)</code>, comparing every <code>y</code> value to every <code>y_pred</code> value.</p>
<p>When you reshape, you get a <code>y</code> tensor with shape <code>(batch_size, 1)</code>. This results in the correct loss computation where <code>(y_pred - y)</code> has shape <code>(batch_size, 1)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>ncols = 8
n_neurons = 150
model = torch.nn.Sequential(
    torch.nn.Linear(ncols, n_neurons),
    torch.nn.ReLU(),
    torch.nn.Linear(n_neurons, 1) # model outputs tensor of shape `(batch_size, 1)`
)

batch_size = 64
x = torch.randn(batch_size, ncols)
y = torch.randn(batch_size) # y is shape `(batch_size)`

y_pred = model(x)
print(y.shape, y_pred.shape) # pred and target different shapes
# &gt; torch.Size([64]) torch.Size([64, 1])

loss = (y_pred - y).pow(2)
print(loss.shape) # this causes the loss to broadcast
# &gt; torch.Size([64, 64])

y_reshaped = y.reshape(-1, 1)
print(y_reshaped.shape) # when you reshape the axes match
# &gt; torch.Size([64, 1])

loss = (y_pred - y_reshaped).pow(2)
print(loss.shape) # we get the correct loss shape
# &gt; torch.Size([64, 1])
</code></pre>
","2024-12-04 04:14:26","1","Answer"
"79249787","","How can one obtain the ""correct"" embedding layer in BERT?","<p>I want to utilize BERT to assess the similarity between two pieces of text:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import numpy as np

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)
model = AutoModel.from_pretrained(&quot;bert-classifier&quot;)

def calc_similarity(s1, s2):
    inputs = tokenizer(s1, s2, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

    cosine_similarity = F.cosine_similarity(embeddings[0], embeddings[1])
    return cosine_similarity
</code></pre>
<p>The similarity presented here is derived from a BERT sentiment classifier, which is a model fine-tuned based on the BERT architecture.</p>
<p>My inquiry primarily revolves around this line of code：</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
</code></pre>
<p>I have observed at least three different implementations regarding this line; in addition to the aforementioned version that retrieves the first row, there are two other variations:</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = outputs.last_hidden_state.mean(axis=1).cpu().numpy()
</code></pre>
<p>and</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = model.bert.pooler(outputs.last_hidden_state.cpu().numpy())
</code></pre>
<p>In fact, vector <code>outputs.last_hidden_state</code> is a 9*768 tensor, and the three aforementioned methods can transform it into a 1*768 vector, thereby providing a basis for subsequent similarity calculations. From my perspective, the first approach is not appropriate within the semantic space defined by the classification task, as our objective is not to predict the next word. What perplexes me is the choice between the second and third methods, specifically whether to employ a simple average or to utilize the pooling layer of the model itself.</p>
<p>Any assistance would be greatly appreciated!</p>
","2024-12-04 03:15:26","3","Question"
"79249247","","How do I make a neural network class with fit and predict functions like with sklearn models, when my train and test data are different sizes?","<p>I'm trying to make a neural network model that will answer a linear regression problem (I've already made a model using <code>sklearn</code>'s <code>LinearRegression</code> and I'd like to compare the two).  Ultimately I'd like to make a class with <code>fit</code> and <code>predict</code> functions, as with the models in <code>sklearn</code>, so that I can make a loop that will test all the models I am using in my project.</p>
<p>To do this I followed the code in the answer to this question: <a href=""https://stackoverflow.com/questions/73493198/writing-a-pytorch-neural-net-class-that-has-functions-for-both-model-fitting-and"">Writing a pytorch neural net class that has functions for both model fitting and prediction</a>.
With some modifications, here is what I have:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.optim as optim

class MyNeuralNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(2, 4, bias=True)
        self.layer2 = nn.Linear(4, 1, bias=True)
        self.loss = nn.MSELoss()
        self.compile_()

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        return x.squeeze()

    def fit(self, x, y):
        x = torch.tensor(x.values, dtype=torch.float32)
        y = torch.tensor(y.values, dtype=torch.float32)
        losses = []
        for epoch in range(100):
            ## Inference
            res = self.forward(x)#self(self,x)
            loss_value = self.loss(res,y)

            ## Backpropagation
            loss_value.backward() # compute gradient
            self.opt.zero_grad()  # flush previous epoch's gradient
            self.opt.step()       # Perform iteration using gradient above

            ## Logging
            losses.append(loss_value.item())
        
    def compile_(self):
        self.opt = optim.SGD(self.parameters(), lr=0.01)
       
    def predict(self, x_test):
        self.eval()
        y_test_hat = self(x_test)
        return y_test_hat.detach().numpy()
        # self.train()
</code></pre>
<p>Note, you also need <code>numpy</code>, I just don't have it here because this code was put into a separate .py file.</p>
<p>Here is how I used the model, after importing my class:</p>
<pre><code>model = MyNeuralNet()
X_train = # pandas dataframe with 1168 rows and 49 columns
y_train = # pandas dataframe with 1168 rows and 1 column
X_test = # pandas dataframe with 292 rows and 49 columns
model.fit(X_train, y_train)
pred = model.predict(X_test)
print(pred)
</code></pre>
<p>The error I got is <code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (1168x49 and 2x4)</code> at the <code>fit</code> step.  I understand this has to do with the parameters for the linear layers of my network.  I think if I change my input size for the first linear layer to 49 and my output size for the second linear layer to 1168 then it will work for the <code>fit</code> step (or at least something like that, to match the sizes of the train data).  However, my test data is of a different size and I'm pretty sure then the <code>predict</code> step won't work.</p>
<p>Is it possible to make a neural network class where the training and test data are of different sizes?</p>
","2024-12-03 21:41:10","0","Question"
"79248710","","Reshaping out tensor in pytorch produces weird behavior","<p>I was going through <a href=""https://github.com/parrt/fundamentals-of-deep-learning/blob/main/notebooks/3.train-test-diabetes.ipynb"" rel=""nofollow noreferrer"">https://github.com/parrt/fundamentals-of-deep-learning/blob/main/notebooks/3.train-test-diabetes.ipynb</a> as an exercise, but forgot to reshape y tensors in these lines</p>
<pre><code>y_train = torch.tensor(y_train).float().reshape(-1,1) # column vector
y_test = torch.tensor(y_test).float().reshape(-1,1)
</code></pre>
<p>And my model just stopped learning early on, loss was not improving in training. Does anyone understand what's the effect of those <code>reshape()</code> calls, how do I avoid this bug in the future?</p>
<p>Full code and comments below:</p>
<pre><code>def train1(model, X_train, X_test, y_train, y_test,
           learning_rate = .5, nepochs=2000):
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    history = [] # track training and validation loss
    for epoch in range(nepochs+1):
        y_pred = model(X_train)
        loss = torch.mean((y_pred - y_train)**2)
        y_pred_test = model(X_test)
        loss_test = torch.mean((y_pred_test - y_test)**2)
        history.append((loss, loss_test))
        if epoch % (nepochs//10) == 0:
            print(f&quot;Epoch {epoch:4d} MSE train loss {loss:12.3f}   test loss {loss_test:12.3f}&quot;)
            
        optimizer.zero_grad()
        loss.backward() # autograd computes w1.grad, b1.grad, ...
        optimizer.step()
    return torch.tensor(history)


ncols = X_train.shape[1]
n_neurons = 150
model2 = torch.nn.Sequential(
    torch.nn.Linear(ncols, n_neurons),
    torch.nn.ReLU(),
    torch.nn.Linear(n_neurons, 1)
)

d = load_diabetes()
df = pd.DataFrame(d.data, columns=d.feature_names)
df['disease'] = d.target # &quot;quantitative measure of disease progression one year after baseline&quot;
print (df.head(3))


np.random.seed(1) # set a random seed for consistency across runs
n = len(df)
X = df.drop('disease',axis=1).values
y = df['disease'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) 


m = np.mean(X_train,axis=0)
std = np.std(X_train,axis=0)
X_train = (X_train-m)/std
X_test = (X_test-m)/std      


X_train = torch.tensor(X_train).float()
X_test = torch.tensor(X_test).float()

# HERE !!!!!!
# without reshape: train loss doesn't emprove beyond epoch 800, loss=6074 
# y_train = torch.tensor(y_train).float()
# y_test = torch.tensor(y_test).float()
# print (y_train.shape, y_test.shape) # torch.Size([353]) torch.Size([89])

# with reshape, train loss goes down to 7
y_train = torch.tensor(y_train).float().reshape(-1,1) # column vector
y_test = torch.tensor(y_test).float().reshape(-1,1)
print (y_train.shape, y_test.shape) # torch.Size([353]) torch.Size([89])

########################################################################




history = train1(model2, X_train, X_test, y_train, y_test,
                 learning_rate=.02, nepochs=8000)

# Epoch    0 MSE train loss    29603.037   test loss    26998.922
# Epoch  800 MSE train loss     2133.840   test loss     3174.325
# Epoch 1600 MSE train loss     1423.420   test loss     4316.454
# Epoch 2400 MSE train loss      375.720   test loss     7257.883
# Epoch 3200 MSE train loss      120.477   test loss     9051.368
# Epoch 4000 MSE train loss       57.527   test loss    10240.634
# Epoch 4800 MSE train loss       31.486   test loss    10784.966
# Epoch 5600 MSE train loss       16.044   test loss    11113.780
# Epoch 6400 MSE train loss        8.490   test loss    11283.872
# Epoch 7200 MSE train loss        6.594   test loss    11503.454
# Epoch 8000 MSE train loss        3.513   test loss    11644.484
</code></pre>
","2024-12-03 17:54:12","0","Question"
"79244939","79243559","","<p>The time difference is due to the code samples having different memory allocation.</p>
<p>Your first example:</p>
<pre class=""lang-py prettyprint-override""><code># ex1
torch.cuda.synchronize()
start_time = time.time()
out = a(b)
torch.cuda.synchronize()
end_time = time.time()
print(&quot;batch time :&quot;, end_time - start_time)
</code></pre>
<p>The <code>out</code> tensor is of size <code>(20, 5000, 5000)</code>.</p>
<p>In your second example:</p>
<pre class=""lang-py prettyprint-override""><code># ex2
torch.cuda.synchronize()
start_time = time.time()
for i in range(20):
    out = a(c[i])
torch.cuda.synchronize()
end_time = time.time()
print(&quot;for time:&quot;, end_time - start_time)
</code></pre>
<p>The <code>out</code> tensor is of size <code>(5000, 5000)</code>.</p>
<p><code>ex1</code> appears slower because it has to allocate memory for a larger output tensor. This overhead is also higher the first time a process has to allocate memory (see time results below).</p>
<p>Consider a third example where we use the code from <code>ex2</code> but have the same output size as <code>ex1</code>:</p>
<pre class=""lang-py prettyprint-override""><code># ex3
torch.cuda.synchronize()
start_time = time.time()
out = torch.empty(20, 5000, 5000)
for i in range(20):
    out[i] = a(c[i])
torch.cuda.synchronize()
end_time = time.time()
print(&quot;for time:&quot;, end_time - start_time)
</code></pre>
<p>On my GPU I get the following results:</p>
<ul>
<li><code>ex1</code> first run: 0.75 seconds (extra overhead from first memory allocation of the process)</li>
<li><code>ex1</code> second run: 0.26 seconds</li>
<li><code>ex2</code>: 0.28 seconds</li>
<li><code>ex3</code>: 1.13 seconds</li>
</ul>
","2024-12-02 17:00:01","1","Answer"
"79243559","","Why does batch computation take more time in pytorch than a for loop?","<p>Is there a problem with my code?
When I run the following code together, there is a big difference in the time it takes:
batch time : 1.3649392127990723
for time: 0.7864551544189453
When I run them separately, they take almost the same amount of time</p>
<pre><code>import time
import torch
import torch.nn as nn
if __name__ == '__main__':
    a= nn.Linear(5000, 5000).cuda()
    b = torch.randn(20, 5000, 5000).cuda()
    c = torch.randn(20, 5000, 5000).cuda()



    torch.cuda.synchronize()
    start_time = time.time()
    out = a(b)
    torch.cuda.synchronize()
    end_time = time.time()
    print(&quot;batch time :&quot;, end_time - start_time)



    torch.cuda.synchronize()
    start_time = time.time()
    for i in range(20):
        out = a(c[i])
    torch.cuda.synchronize()
    end_time = time.time()
    print(&quot;for time:&quot;, end_time - start_time)

</code></pre>
","2024-12-02 09:38:23","0","Question"
"79242880","79240995","","<p>You can use your <code>indices</code> tensor directly, you just need another tensor for the batch indexing:</p>
<pre class=""lang-py prettyprint-override""><code>n_images = 4
width = 100
height = 100
channels = 3
n_samples = 30

images = torch.rand((n_images, height, width, channels))
indices = (torch.rand((n_images, n_samples, 2)) * width).to(torch.int32)

batch_indices = torch.arange(n_images).view(-1, 1).expand(-1, n_samples)
result = images[batch_indices, indices[..., 1], indices[..., 0]]
</code></pre>
<p>This follows your convention of <code>images[ix, ys, xs]</code> where the <code>ys</code> index the height dimension of the tensor and the <code>xs</code> index the width</p>
","2024-12-02 03:17:34","0","Answer"
"79242875","79240995","","<p>To use torch function, you will need to flatten the dimensions you need to calculate, as well as the indices, then you can use <code>torch.gather</code> to select the pixels you need:</p>
<pre class=""lang-py prettyprint-override""><code>n_images = 4
width = 100
height = 100
channels = 3
n_samples = 30

images = torch.rand((n_images, height, width, channels))
indices = (torch.rand((n_images, n_samples, 2)) * width).long() #&lt;-- cast this to long instead of int as `torch.gather` requires long as index

flatten_images = images.view(n_images,-1, channels)
flatten_indices = (indices[..., 1:2] * width + indices[..., 0:1]).repeat(1,1,channels)
output = torch.gather(flatten_images, 1, flatten_indices).view(n_images, n_samples, channels) # 
</code></pre>
","2024-12-02 03:09:32","0","Answer"
"79241251","78511133","","<p>Validation set is a subset of your dataset, that is not used for training. It requires having true labels for the samples though. If your test set is like that, then yes you can use it.
(Downside is that you cannot check that you didn't overfitted your hyperparameters to the test set.)<br />
The early stopping mechanism as you copy/pasted from [https://stackoverflow.com/a/73704579/20128192](this answer) is to be used inside the training loop of your model, which in pseudo code can look like this :</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in num_epochs
    for sample, target in training_data:
       output = model(sample)
       loss = loss_function(output, target)
       loss.backward()
       optimizer.step()
       optimizer.zero_grad()
    for sample, target in {test or validation}_data:
       val_loss += loss_function(model(sample), target)
    val_loss /= len(validation_data)
    
    if early_stopper.early_stop(validation_loss):             
        break # this ends the outer for loop, effectively stopping the training
</code></pre>
","2024-12-01 09:23:06","0","Answer"
"79240995","","Gather different pixels per image of an image stack with torch","<p>I have a batch of images and a batch of indices (x, y) for each image. The indices are different for each image, so I cant use simple indexing. What is the best or fastest way to get another batch with the colors of the selected pixels per image?</p>
<pre><code>    n_images = 4
    width = 100
    height = 100
    channels = 3
    n_samples = 30
    
    images = torch.rand((n_images, height, width, channels))
    indices = (torch.rand((n_images, n_samples, 2)) * width).to(torch.int32)

    # preferred function
    # result = images[indices]
    # with result.shape = (n_images, n_samples, 3)



    
</code></pre>
<pre><code>    # I just found this solution but I would rather like to call a general torch function
    xs = indices.reshape((-1, 2))[:, 0]
    ys = indices.reshape((-1, 2))[:, 1]
    ix = torch.arange(n_images, dtype=torch.int32)
    ix = ix[..., None].expand((-1, n_samples)).flatten()
    
    result = images[ix, ys, xs].reshape((n_images, n_samples, 3))
</code></pre>
","2024-12-01 06:00:25","0","Question"
"79238368","79231978","","<p>After quite a bit of research and code review, I was able to isolate details of layer normalization (LN), an aspect of transformers that's confusing a lot of people.</p>
<p>TL;DR: The assumption I made in my original question that each mean, std pair has its own weight, bias pair is incorrect. In LN, mean and std stats are computed across embedding dimensions of each token, i.e., there are as many mean, std pairs as there are tokens. But the weight and bias values are learned per embedding dimension, i.e., tokens share the weight and bias values during LN. This means, in the case of BERT base, there are a max of 512 mean and std values, and there are 768 weight and bias values.</p>
<p>For complete details, see my answer to <a href=""https://datascience.stackexchange.com/questions/88552/layer-normalization-details-in-gpt-2"">this question</a>.</p>
","2024-11-29 21:06:04","0","Answer"
"79238254","79237483","","<p>This is due to <a href=""https://en.wikipedia.org/wiki/Bessel%27s_correction"" rel=""nofollow noreferrer"">Bessel's correction</a>. You can achieve the same results using <code>pvariance</code> instead of <code>variance</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.metrics import r2_score
from torch.nn import MSELoss
import statistics 
import random
import torch
import numpy as np 

actuals = random.sample(range(1, 50), 40)

preds = []

for value in actuals:
    pred = value * 0.70
    preds.append(pred)

loss = MSELoss()

mse = loss(torch.tensor(preds), torch.tensor(actuals))

r2_sample = 1 - mse / statistics.variance(actuals)
r2_population = 1 - mse / statistics.pvariance(actuals)

score = r2_score(actuals, preds)

print(f'R2 Score using (PyTorch) MSELoss (sample): {r2_sample}')
print(f'R2 Score using (PyTorch) MSELoss (population): {r2_population}')
print(f'R2 Score using (sklearn) r2_score: {score}')
</code></pre>
<p>Output:</p>
<pre><code>&gt; R2 Score using (PyTorch) MSELoss (sample): 0.6582530736923218
&gt; R2 Score using (PyTorch) MSELoss (population): 0.6494903564453125
&gt; R2 Score using (sklearn) r2_score: 0.6494903644913157
</code></pre>
","2024-11-29 20:05:37","0","Answer"
"79237483","","Discrepancy between sklearn's r2_score() and PyTorch's MSELoss()?","<p>I am not sure if I am missing something very basic but I have started to notice some slight discrepancies between R2 scores returned by sklearn's r2_score() function and R2 scores calculated from PyTorch's MSELoss() (with the additional help of <code>statistics.variance()</code>).</p>
<p>The R2 score returned by the sklearn method is consistently (slightly) lower than the one returned via MSELoss().</p>
<p>Here is some basic code to reproduce the difference:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.metrics import r2_score
from torch.nn import MSELoss
import statistics 
import random
import torch
import numpy as np 

actuals = random.sample(range(1, 50), 40)

preds = []

for value in actuals:
    pred = value * 0.70
    preds.append(pred)

loss = MSELoss()

mse = loss(torch.tensor(preds), torch.tensor(actuals))

r2 = 1 - mse / statistics.variance(actuals)

score = r2_score(actuals, preds)

print(f'R2 Score using (PyTorch) MSELoss: {r2}')
print(f'R2 Score using (sklearn) r2_score: {score}')
</code></pre>
<p>Example output:</p>
<pre><code>R2 Score using (PyTorch) MSELoss: 0.6261289715766907
R2 Score using (sklearn) r2_score: 0.6165425269729996
</code></pre>
<p>I figured this could be related to the fact that MSELoss() takes tensors as input (but sklearn doesn't) but I don't really know why or how.</p>
<p>Versions:</p>
<ul>
<li>PyTorch == 2.1.0</li>
<li>scikit-learn == 1.4.2</li>
<li>Python == 3.9.18</li>
</ul>
","2024-11-29 14:45:51","0","Question"
"79234982","79234040","","<p>The dictionary is just a python object, it has no device associated with it.</p>
<p>The dictionary keys and values are pointers to other python objects. The dictionary itself has no idea what a device is and does not interact with that aspect of pytorch tensors. The dictionary only knows the pointers for the key/value objects. The device(s) in question are completely independent from the dictionary.</p>
<p>A pytorch tensor is an object that acts as a wrapper around some data. The tensor object has a reference on CPU memory used to interact with the tensor itself, as well as a <code>.data</code> parameter that references the underlying data (which may be on CPU or GPU).</p>
<p>For example, you can put tensors from multiple devices in the same dictionary:</p>
<pre class=""lang-py prettyprint-override""><code>x1 = torch.randn(5)
x2 = torch.randn(5).to('cuda:0')
x3 = torch.randn(5).to('cuda:1')

d = {
    'tensor1' : x1,
    'tensor2' : x2,
    'tensor3' : x3,
}
</code></pre>
<p>You can do this because the device aspect is only relevant to the pytorch tensors - it doesn't matter for the dictionary itself.</p>
<p>For your example of:</p>
<pre class=""lang-py prettyprint-override""><code>{'rects': [405.5453, 126.3100, 646.7983, 454.6852] , 'points': tensor([[[477.5181, 247.7193],
         [591.4567, 247.1899],
         [544.2579, 328.7857],
         [494.3871, 371.9922],
         [583.4272, 370.3429]]], device='cuda:0')}
</code></pre>
<p>The dictionary is stored in CPU memory. The dictionary stores pointers to the value objects in the dictionary.</p>
<p>The key <code>rects</code> references a python list stored in CPU memory.</p>
<p>They key <code>points</code> references the CPU pointer of a pytorch tensor object. This tensor has a <code>.data</code> parameter that references data on the GPU, but the dictionary itself doesn't know about that.</p>
","2024-11-28 18:14:39","1","Answer"
"79234040","","On which device is a python dictionary containing pytorch tensors that are loaded on cuda?","<p>I have a pytorch face detection model that returns bounding boxes and a few facial landmarks as a dictionary. The bounding boxes and the landmarks are pytorch tensors that where moved to the GPU. When I print the dictionary, it looks for example like this:</p>
<pre><code>{'rects': tensor([[405.5453, 126.3100, 646.7983, 454.6852]], device='cuda:0'), 'points': tensor([[[477.5181, 247.7193],
         [591.4567, 247.1899],
         [544.2579, 328.7857],
         [494.3871, 371.9922],
         [583.4272, 370.3429]]], device='cuda:0'), 'scores': tensor([0.9999], device='cuda:0'), 'image_ids': tensor([0], device='cuda:0').
</code></pre>
<p>Does the dictionary lie on the CPU and only link to the GPU where the tensors are actually on? Or is the dictionary also loaded on the GPU now? Also, if the dictionary contained one list and one tensor like this:</p>
<pre><code>{'rects': [405.5453, 126.3100, 646.7983, 454.6852] , 'points': tensor([[[477.5181, 247.7193],
         [591.4567, 247.1899],
         [544.2579, 328.7857],
         [494.3871, 371.9922],
         [583.4272, 370.3429]]], device='cuda:0')}.
</code></pre>
<p>Where is the dictionary then located?</p>
<p>Maybe I'm just confused on what device=&quot;cuda:0&quot; actually means, so if someone could answer I would highly appreciate it!</p>
","2024-11-28 13:16:38","0","Question"
"79232526","79229818","","<p>This is because <code>dim = 64</code> and <code>head_num2 = 16</code>. <code>dim // head_num2 = 4</code> and <code>4</code> is not divisible by <code>8</code>.
Pytorch becomes inefficient in this case.
To avoid this, also need to set <code>dim = 128</code> as <code>128 // 16 = 8</code>.</p>
","2024-11-28 03:40:59","0","Answer"
"79232221","79231978","","<p>From the definition of LayerNorm (for example in the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"" rel=""nofollow noreferrer"">pytorch manual</a> on in the <a href=""https://arxiv.org/abs/1607.06450"" rel=""nofollow noreferrer"">paper</a>),
the parameters beta and gamma have the same dimension as the tokens (although the layernorm does act over the channel dimension too)</p>
<p>Thus here they need <code>768</code> which is the hidden / token size, not the channel/sequence size (which is <code>512</code>)</p>
","2024-11-28 00:01:57","2","Answer"
"79232183","79232069","","<p>The inplace operation is this:</p>
<pre class=""lang-py prettyprint-override""><code>out += shortcut
</code></pre>
<p>The <code>relu</code> needs its own output to compute its gradient! Thus you are doing an inplace operation on the output of the relu, which it needed to compute its gradient in the backwards pass.</p>
<p>replacing it with</p>
<pre class=""lang-py prettyprint-override""><code>out = out + shortcut
</code></pre>
<p>should solve your problem.</p>
<p>In general, try to avoid using inplace functions in pytorch (such as <code>+=</code> unless you know what you are doing</p>
<hr />
<p><em>more details:</em></p>
<p>if you look at the pytorch code, the backwards pass for the relu is auto-generated from the following bit of code in <code>pytorch/tools/autograd/derivatives.yaml</code></p>
<pre class=""lang-yaml prettyprint-override""><code>- name: relu(Tensor self) -&gt; Tensor
  self: threshold_backward(grad, result, 0)
  result: auto_element_wise
</code></pre>
<p>What this does is it</p>
<ul>
<li>takes the gradient of the output as <code>grad</code></li>
<li>and the result of the relu as <code>result</code></li>
<li>and returns the contents of <code>grad</code> where <code>result</code> is bigger than <code>0</code> and <code>0</code> otherwise.</li>
</ul>
<p>Thus it does need the output (technically, it could have stored its input instead, but this is how it is implemented)</p>
","2024-11-27 23:40:24","1","Answer"
"79232069","","Autograd error caused by ReLU in Pytorch?","<p>I am using a residual neural network for a classification task. Somehow adding or omitting a ReLU activation causes the autograd to fail. I would be grateful for any insights on the reason for this? It cannot make any sense of it. ReLU is not an inplace operation, is it? Error message:
<code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</code></p>
<p>Here is the network architecture. The 3rd to last line is what causes the issue when not commented out.</p>
<pre><code>class ResidualBlock(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn1 = nn.BatchNorm1d(num_filters)
        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn2 = nn.BatchNorm1d(num_filters)

    def forward(self, x):
        shortcut = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = F.relu(out) # causes the issue when not commented out
        out += shortcut
        return out
</code></pre>
<p>Below is a minimal working example. I am using Python 3.12 and torch 2.5.1.</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

# Define the ResidualBlock
class ResidualBlock(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn1 = nn.BatchNorm1d(num_filters)
        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn2 = nn.BatchNorm1d(num_filters)

    def forward(self, x):
        shortcut = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = F.relu(out) # causes the issue
        out += shortcut
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(SimpleModel, self).__init__()
        self.res_block = ResidualBlock(num_filters, kernel_size)
        self.fc = nn.Linear(num_filters, 1)

    def forward(self, x):
        x = self.res_block(x)
        x = x.mean(dim=2)
        x = self.fc(x)
        return x

torch.manual_seed(42)
num_samples = 1000
sequence_length = 32
num_filters = 16

X = torch.randn(num_samples, num_filters, sequence_length)  # Random input
y = torch.sum(X, dim=(1, 2), keepdim=True)  # Simple target (sum of all values)


dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)


model = SimpleModel(num_filters=num_filters, kernel_size=3)
criterion = nn.MSELoss() 
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


epochs = 5
for epoch in range(epochs):
    model.train()
    epoch_loss = 0.0
    for batch_X, batch_y in dataloader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    print(f&quot;Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}&quot;)

print(&quot;Training complete!&quot;)
</code></pre>
","2024-11-27 22:34:48","1","Question"
"79231978","","Why do LayerNorm layers in BERT base have 768 (and not 512) weight and bias parameters?","<p>The following will print 768 weight and bias parameters for each LayerNorm layer.</p>
<pre><code>from transformers import BertModel
model = BertModel.from_pretrained('bert-base-uncased')
for name, param in model.named_parameters():
    if 'LayerNorm' in name:
        print(f&quot;Layer: {name}, Parameters: {param.numel()}&quot;)
</code></pre>
<p>As per <a href=""https://youtu.be/G45TuC6zRf4?t=453"" rel=""nofollow noreferrer"">this video</a>, mean and std values are computed for each token in the input. And each mean, std pair has its own learned weight and bias in layer normalization. Since BERT takes in a max of 512 tokens, I'd expect a total of 512 weight and bias parameters in LayerNorm layers.</p>
<p>So why is it 768? Is the video incorrect? Is normalization performed for each of the 768 dimensions across all tokens, meaning mean and std values are computed across a max of 512 values?</p>
","2024-11-27 21:48:49","0","Question"
"79229818","","Scaled_dot_product_attention higher head num cost much more memory","<p>I found Scaled_dot_product_attention cost much more memory when head number is large(&gt;=16). This is my code to reproduce the issue.</p>
<pre><code>
import torch
length = 10000
dim = 64
head_num1 = 8
head_num2 = 16
batch = 1
shapes = [[batch, head_num1, length, dim//head_num1], [batch, head_num2, length, dim//head_num2]]
for shape in shapes:
    torch.cuda.reset_peak_memory_stats()

    shape2 = [1,1, length, length]
    q = torch.rand(shape, dtype = torch.float16).cuda()
    k= torch.rand(shape, dtype = torch.float16).cuda()
    v = torch.rand(shape, dtype = torch.float16).cuda()

    attn_mask = torch.ones(shape2, dtype = torch.bool).cuda()
    x = torch.nn.functional.scaled_dot_product_attention(
                                q, k, v, attn_mask=attn_mask, dropout_p=0)

    peak_memory = torch.cuda.max_memory_allocated()      
    print(f&quot;head number {shape[1]} case peak memory: {peak_memory / 1e6:.2f} MB&quot;)
</code></pre>
<p>environment:</p>
<pre><code>Python Version: 3.9.19
PyTorch Version: 2.1.0+cu121
CUDA Available: Yes
CUDA Version: 12.1
Current GPU: Tesla V100-SXM2-16GB
CUDDN: 8902
</code></pre>
<p>output:</p>
<pre><code>head number 8 case peak memory: 405.17 MB
head number 16 case peak memory: 6716.14 MB
</code></pre>
<p>just double the head number need 16x more memory... is it normal?</p>
<p>I tried on another machine</p>
<pre><code>Python Version: 3.9.19
PyTorch Version: 2.1.0+cu121
CUDA Available: Yes
CUDA Version: 12.1
Current GPU: NVIDIA GeForce GTX 1070
CUDDN: 8902
</code></pre>
<p>output:</p>
<pre><code>head number 8 case peak memory: 405.17 MB
head number 16 case peak memory: 406.49 MB
</code></pre>
<p>I think this should be right because higher head number does not require more memory, but I failed to achieve this on many other machines. Hope someone know how to achieve this, many thanks.</p>
","2024-11-27 10:18:07","0","Question"
"79228229","79212687","","<p>If you use <code>shuffle=True</code> it should reshuffle the data at each epoch (<a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer"">from PyTorch documentation</a>). This means that at each epoch, the batches of data will be arranged differently with respect to the previous one.</p>
<p>During testing, since during testing the model doesn't change and you don't usually test for multiple epochs, it shouldn't make any difference.</p>
<p>However, setting <code>shuffle=False</code>, ensures consistency in the order of predictions, and it may make debugging easier.</p>
","2024-11-26 20:49:40","0","Answer"
"79228158","79226995","","<p>I would suggest to:</p>
<ul>
<li>Run the training from a python script instead of a jupyter notebook. Write the output to a file instead of printing to the standard output and check if the issue persist</li>
<li>Run the training on a reduced version of the dataset: in this way you should be able to reach the same amount of epochs in less time. If the problem persists after the same time interval, the problem may be related to the number of epochs, otherwise it may be related to the running time.</li>
<li>Run the training on a different machine/on your laptop, to check if the problem is related to the remote machine you're running on. (If it's too computationally heavy for your laptop, you might use a reduced version of the dataset here as well)</li>
</ul>
","2024-11-26 20:18:06","0","Answer"
"79227337","","PyTorch Error: Calling linear solver with sparse tensors requires compiling PyTorch with CUDA cuDSS and is not supported in ROCm build","<p>I'm trying to use <code>torch.sparse.spsolve</code> to solve a linear system of equations as follows:</p>
<pre><code>    A_sparse = torch.sparse_coo_tensor(indices, values, size=(eq_counter, self.num_regions))
    A_sparse_csr = A_sparse.to_sparse_csr()  
    A_sparse_csr = A_sparse_csr.cuda()

    # Create dense vector b
    b = torch.tensor(b_values, dtype=slopes.dtype, device=slopes.device)
    b = b.cuda()


    # Solve the linear system A c = b using torch.sparse.spsolve
    intercepts = torch.sparse.spsolve(A_sparse_csr, b)  # Shape: (num_regions,)
</code></pre>
<p>However, I get the following strange error --&gt; RuntimeError: Calling linear solver with sparse tensors requires compiling PyTorch with CUDA cuDSS and is not supported in ROCm build.</p>
<p>I have an Nvidia RTX 40 series graphics card, so I don't understand why the ROCm build (related to AMD GPUs) is even relevant/showing up in the error? I tried to diagnose the error with the following code snippet:</p>
<pre><code>print(f&quot;PyTorch Version: {torch.__version__}&quot;)
print(f&quot;CUDA Version: {torch.version.cuda}&quot;)
print(f&quot;Is CUDA Available: {torch.cuda.is_available()}&quot;)

# Check the current CUDA device
if torch.cuda.is_available():
   print(f&quot;Current CUDA Device: {torch.cuda.current_device()}&quot;)
   print(f&quot;Device Name: {torch.cuda.get_device_name(torch.cuda.current_device())}&quot;)
</code></pre>
<p>The output was as expected:</p>
<pre><code>PyTorch Version: 2.5.1+cu124
CUDA Version: 12.4
Is CUDA Available: True
Current CUDA Device: 0
Device Name: NVIDIA GeForce RTX 4090
</code></pre>
<p>So I really don't know what the issue is and any help in resolving this would be much appreciated! Thanks in advance</p>
","2024-11-26 15:32:55","1","Question"
"79226995","","Why is (remote) Jupyter busy during ML training, but not actually doing anything?","<p>I am training an ML model with PyTorch on my own dedicated remote server, using Jupyter as my IDE.</p>
<p>Around 120 epochs (about 2 hours into training), the Jupyter cell stops updating the output, but the status bar still says busy as the kernel status, and the SSH connection is still active.</p>
<p>I thought that maybe training was continuing, but the output cell stopped updating because it contains too much output. To check this hypothesis, I left Jupyter running for about 7 hours last night. When I woke up, it had stopped updating the output cell at 123 epochs, and when I killed the execution and printed out the current number of epochs, it had only reached 126 epochs.</p>
<p>Any idea what could be causing this?</p>
","2024-11-26 13:55:11","0","Question"
"79222443","79213086","","<p>The version you require could be installed via pip with:</p>
<p><code>pip install torch-sparse -f https://data.pyg.org/whl/torch-1.7.1+cu102.html</code></p>
","2024-11-25 09:40:37","0","Answer"
"79221130","79220896","","<p>From the picture on the website, I see that some of the annotations are not bounding boxes. They are polygons. A common way to encode a polygon is as a list of x/y pairs.</p>
<p>So I would guess that the format is</p>
<pre><code>class_id x1 y1 x2 y2 x3 y3
</code></pre>
<p>etc.</p>
<p>To check this, I downloaded one of the pictures and its associated label. (Specifically, <a href=""https://i.sstatic.net/0bbSenCY.png"" rel=""nofollow noreferrer"">CamScanner-10-15-2023-14-29_86_jpg.rf.1042acb34a88542b82bbefa27b86569e.jpg</a> I wrote a program which parsed this label and plotted it.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import matplotlib.pyplot as plt


label_text = &quot;&quot;&quot;1 0.3855721390625 0.17391304375 0.26533996718749997 0.1273291921875 0.10779436093749999 0.273291925 0.25290215625 0.3354037265625 0.3855721390625 0.17391304375
0 0.9618573796875 0.381987578125 0.8872305140625001 0.3540372671875 0.327529021875 0.9782608703125 0.45190713125000004 1 0.9618573796875 0.381987578125
2 0.970149253125 0.034161490625 0.8084577109375 0 0.0165837484375 0.9254658390625 0.0414593703125 0.9937888203125 0.178275290625 1 0.970149253125 0.034161490625&quot;&quot;&quot;


lines = label_text.split('\n')
for line in lines:
    line = line.split(' ')
    class_id = line[0]
    label_without_id = np.array([float(s) for s in line[1:]])
    label_x = label_without_id[::2]
    label_y = label_without_id[1::2]
    plt.plot(label_x, label_y, label=class_id)
    # The convention when working with image coordinates is that Y-axis gets bigger as you move down the image
    plt.gca().invert_yaxis()
plt.legend()
plt.show()
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/Z4ffARhm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Z4ffARhm.png"" alt=""plot of yolo polygon bounding box"" /></a></p>
<p>That looks reasonably plausible, given the input. The aspect ratio is wrong, but they're likely expecting you to rescale the x/y coordinates by the image width/height. You can also compare this to the <a href=""https://universe.roboflow.com/waqas-hussain/stationary-object-detector/images/az3t0ekOPe5qGpB728YO?queryText=&amp;pageSize=200&amp;startingIndex=0&amp;browseQuery=true"" rel=""nofollow noreferrer"">image labels on roboflow</a>.</p>
","2024-11-24 21:30:19","2","Answer"
"79220896","","Understanding Unusual YOLO Label Formats and Their Impact on Training","<p>I’m working on a dataset of stationary objects, where the data is divided into train, test, and validation folders with corresponding images and labels. The labels are in text files with the following format:</p>
<pre><code>2 0.3832013609375 0 0 0.19411217812499998 0 0.614612228125 0.1995640296875 1 0.619265075 1 1 0.8055533171875 1 0.386728209375 0.798922646875 0 0.3832013609375 0
</code></pre>
<p>I’m confused because I expected each bounding box to have just 5 numbers:</p>
<pre><code>class_id, x_center, y_center, width, height.
</code></pre>
<p>But here, I see significantly more numbers. Could it be that this format represents something else? Are there additional possibilities for YOLO label formats that I’m unaware of?</p>
<p><strong>Additional Context</strong></p>
<p>The data was sourced from this <a href=""https://universe.roboflow.com/waqas-hussain/stationary-object-detector"" rel=""nofollow noreferrer"">website</a>, but I couldn’t find clear documentation about this label format.</p>
<p>Here’s the part I don’t understand: when I pass this dataset to YOLO for training using the following code, the training process works without any issues:</p>
<pre><code>def train_yolo(weight_name):
    weight_path = os.path.join(weights_folder, weight_name)

    model = YOLO(weight_path)

    # Train model and save new weights
    results = model.train(data=data_yaml, epochs=100, imgsz=640, batch=16, name=f&quot;yolo_{weight_name.split('.')[0]}&quot;, save=True)

    return results
</code></pre>
<p>My data.yaml file contains:</p>
<pre><code>train: ../train/images
val: ../valid/images
test: ../test/images

nc: 4
names: ['pencil', 'rubber', 'ruler', 'sharpner']

roboflow:
  workspace: waqas-hussain
  project: stationary-object-detector
  version: 8
  license: CC BY 4.0
  url: https://universe.roboflow.com/waqas-hussain/stationary-object-detector/dataset/8
</code></pre>
<p>There’s no direct reference to bounding box formats in this YAML file, yet YOLO processes the data correctly during training.</p>
<p>Questions:</p>
<ol>
<li>How does YOLO handle these unusual label formats?</li>
<li>Could it be that
my training was incorrect due to this strange bounding box format?</li>
<li>Is there a way to confirm what this format represents and how it’s
parsed by YOLO?</li>
</ol>
<p>Any insights or pointers would be greatly appreciated!</p>
","2024-11-24 19:21:52","0","Question"
"79218300","78328401","","<p>In a Ubuntu 22.04 with Ryzen 5600X, Nvidia RTX 3080 with nividia-550 driver (with Cuda 12.4 installed), the following command, inside a conda (in my case a mamba) environment solved this problem</p>
<p><code>pip3 install torch torchvision torchaudio</code></p>
","2024-11-23 16:12:49","1","Answer"
"79217364","78811867","","<p>please check whether you've installed cudatoolkit and cudnn correctly.</p>
<p>run this command to check your cuda
<code>nvcc -V</code></p>
<p>determine whether your cuda directory includes the comparable version of cudnn</p>
","2024-11-23 07:41:06","0","Answer"
"79213086","","Torch sparse using enviroment CUDA version instead of torch's version","<p>I'm trying to run a Python script in a cluster that requires the following:</p>
<p>torch==1.7.1<br />
torch-sparse==0.6.8</p>
<p>The problem is that when I install both of these, torch comes with a cuda version of 10.2 and torch-sparse uses the available version in the cluster, which is 11, so it gives the following error</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/g.pocas/GraphGONet/scripts/GraphGONet.py&quot;, line 6, in &lt;module&gt;
    from torch_geometric.data import DataLoader
  File &quot;/home/g.pocas/GraphGONet/venv/lib/python3.9/site-packages/torch_geometric/__init__.py&quot;, line 2, in &lt;module&gt;
    import torch_geometric.nn
  File &quot;/home/g.pocas/GraphGONet/venv/lib/python3.9/site-packages/torch_geometric/nn/__init__.py&quot;, line 2, in &lt;module&gt;
    from .data_parallel import DataParallel
  File &quot;/home/g.pocas/GraphGONet/venv/lib/python3.9/site-packages/torch_geometric/nn/data_parallel.py&quot;, line 5, in &lt;module&gt;
    from torch_geometric.data import Batch
  File &quot;/home/g.pocas/GraphGONet/venv/lib/python3.9/site-packages/torch_geometric/data/__init__.py&quot;, line 1, in &lt;module&gt;
    from .data import Data
  File &quot;/home/g.pocas/GraphGONet/venv/lib/python3.9/site-packages/torch_geometric/data/data.py&quot;, line 8, in &lt;module&gt;
    from torch_sparse import coalesce, SparseTensor
  File &quot;/home/g.pocas/GraphGONet/venv/lib/python3.9/site-packages/torch_sparse/__init__.py&quot;, line 27, in &lt;module&gt;
    raise RuntimeError(
RuntimeError: Detected that PyTorch and torch_sparse were compiled with different CUDA versions. PyTorch has CUDA version 10.2 and torch_sparse has CUDA version 11.2. Please reinstall the torch_sparse that matches your PyTorch install.
</code></pre>
<p>I have tried installing torch without cuda, and the version of torch sparse should be compatible with torch 1.7.1 since I installed it directly from the wheel they provide, so I'm really not sure what can be causing this.</p>
","2024-11-21 22:25:13","1","Question"
"79213037","79211162","","<p>If you save the model object, the model class definition must be available to load the model object. This is true both for your custom model and the resnet model. The resnet example works because <code>torch.load</code> tries to import the definition from <code>torchvision</code> which presumably exists in the namespace, while loading your custom model tries to import the definition from <code>__main__</code> which errors as it doesn't exist. This isn't a pytorch issue, it's a python issue.</p>
<p>If you want to save a single file that you can load without imports, you need to compile the model to a different format. Easiest would be torch script, but you can also use onnx:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.flatten = nn.Flatten()
        self.fc = nn.Sequential(
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        x = self.fc(x)
        return x
    
model = SimpleNN()
model_script = torch.jit.script(model)
torch.jit.save(model_script, 'my_model.pt')
</code></pre>
<p>And later when loading:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
model = torch.jit.load('my_model.pt')
</code></pre>
<p>Note that this converts the model from a standard pytorch model to a scripted model. If you need to load the standard pytorch model, you need to have the model class definition in the namespace.</p>
","2024-11-21 21:56:20","0","Answer"
"79212687","","in testing dataset using dataloader , should we set shuffle=true or it doesn't matter?","<p>I have a custom dataset (images of pizza,sushi and steak).
I'm using torch DataLoader for it , now when writing the test dataloader custom should we set shuffle=true or it just doesn't matter??</p>
<p>I haven't seen difference yet , but just asking the  general.</p>
","2024-11-21 19:33:30","-3","Question"
"79212062","78446080","","<p>I am also looking at this paper, but I am not good at writng RNN too.</p>
<p>1: I think each string have an EOS represents the end of the string, the RNN will only make use of information before EOS</p>
<p>2: 1561 is calculated by: each position has 51 possibilities(US-Keyboard+Shift+Caps+Placeholder) for 1 insertion,and you have 30 positions(since length maximum =30). Also, you have 30 positions for deletion, which is 30, and last one for EOS, which is 1, then it comes to 1561. Assume that you have a password have length = 10, the class that related to position 11 to 30 just won't be activated(if it is predicting correctly).</p>
<ol start=""3"">
<li>For the Caps, Shift and Placeholder, you might refer to another prior work: <a href=""https://ieeexplore.ieee.org/abstract/document/8835247?casa_token=Me_vgHsZvI4AAAAA:a2uw0Skz7iCjUbko8x5i_dVxouayxCpbZ5imdLRctc_j2D23Wvr6KmW7o1v53dJ8LcOhqSY"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/abstract/document/8835247?casa_token=Me_vgHsZvI4AAAAA:a2uw0Skz7iCjUbko8x5i_dVxouayxCpbZ5imdLRctc_j2D23Wvr6KmW7o1v53dJ8LcOhqSY</a></li>
</ol>
","2024-11-21 16:13:57","1","Answer"
"79211162","","How can I save both weights and model definition with PyTorch?","<p>First, I defined a model use pytorch and I saved it.</p>
<pre><code>class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.flatten = nn.Flatten()
        self.fc = nn.Sequential(
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        x = self.fc(x)
        return x

torch.save(model, &quot;my_model.pth&quot;)
</code></pre>
<p>but then loading the model also requires that the model class (here, SimpleNN) to exist in the current file. For example, you directly use below in another file to load model without the model class</p>
<pre><code>model = torch.load(&quot;my_model.pth&quot;)
</code></pre>
<p>It will report an error like this</p>
<pre><code>AttributeError: Can't get attribute 'SimpleNN' on &lt;module '__main__' from ····
</code></pre>
<p>But when I use <code>torchvision.models</code> and save model like this</p>
<pre><code>from torchvision.models import resnet18

model = resnet18(pretrained=True) 
torch.save(model, &quot;resnet_model.pth&quot;)
</code></pre>
<p>Then I use direct loading in another file like this</p>
<pre><code>resnet = torch.load(&quot;resnet_model.pth&quot;)
print(resnet)
</code></pre>
<p>It does not require the model class, and can be successfully loaded, and contains both model structure and weight information.</p>
<p>So my question is</p>
<ul>
<li>How to achieve this?</li>
<li>Can my model also do this, that is, the <code>pth</code> file contains the model structure and weight information, and does not need to declare the class in advance?</li>
</ul>
","2024-11-21 12:30:51","0","Question"
"79210858","79205991","","<p>There's a way that require you to collect gradients explicitly and then update them later. Pytorch has support for collecting gradient for parameters of will using <code>torch.autograd.grad</code> and optimize them using the functional approach <code>torch.optim.adam.adam()</code>. I may update a more comprehensive answer when I get some free time but here are some docs that you can look through yourself:</p>
<ul>
<li><a href=""https://pytorch.org/docs/stable/generated/torch.autograd.grad.html"" rel=""nofollow noreferrer"">How to explicitly get the gradients of the parameters you want.</a></li>
<li><a href=""https://discuss.pytorch.org/t/explicitly-passing-gradients-to-torch-optim/8578/4"" rel=""nofollow noreferrer"">How to update using explicit gradients.</a></li>
</ul>
<p><strong>In short</strong>, get explicit gradients of <code>model1</code> using <code>torch.autograd.grad</code> on <code>loss1</code> -&gt; <code>loss2.backward()</code> -&gt; update <code>model2</code> then <code>zero_grads</code> both model -&gt; explicitly update <code>model1</code> using <code>torch.optim.adam.adam()</code></p>
","2024-11-21 11:04:20","1","Answer"
"79209386","79208598","","<p>I think your <code>z_latent_vect</code> is not enabled for gradient computation at all. It is initialised in a <code>no_grad()</code> block and is detached from the rest of the computation graph. Defining it as a <code>torch.nn.Parameter</code> should do the trick. At least I can see the loss decrease on a very simple VAE that I defined.</p>
<pre><code>def optimize_latent_vector(model, inp__, num_epochs=50, learning_rate=0.01):
    inp__ = inp__.to(device)

    with torch.no_grad():
        mu, log_var = model.encoder(inp__)
        z_latent_vect = model.reparameterize(mu, log_var)

    z_latent_vect = torch.nn.Parameter(z_latent_vect.clone(), requires_grad=True)
    optimizer_lat = optim.Adam([z_latent_vect], lr=learning_rate)

    dec_only = model.decoder
    VGLoss = VAE_GD_Loss()

    for epoch in range(num_epochs):
        optimizer_lat.zero_grad()
        dec_only.eval()

        recons_mask = dec_only(z_latent_vect)
        loss = VGLoss(inp__, recons_mask, z_latent_vect)

        loss.backward()
        # print(loss.item()) you should see it reduce here
        optimizer_lat.step()

    return z_latent_vect
</code></pre>
","2024-11-21 01:18:24","1","Answer"
"79208781","79205991","","<p>I have a solution that requires an extra forward pass from <code>model2</code>. If someone can think of a solution that doesn't require this, feel free to chime in.</p>
<p>First, a bit about backprop. When you call <code>backward</code> on a tensor, pytorch computes gradients for all tensors in the computational graph. Since <code>loss1</code> and <code>loss2</code> are both output from <code>model2</code>, calling <code>backward</code> on either results in computing gradients for all parameters in both <code>model1</code> and <code>model2</code>.</p>
<p>When these gradients are computed, pytorch uses current parameter values and activations stored from the forward pass. This means that the parameters used in the backward computation need to be unchanged from the forward pass.</p>
<p>When you call <code>optimizer.step()</code>, you update all parameters in the optimizer with an in-place update. This is why you get the error you see.</p>
<pre class=""lang-py prettyprint-override""><code>loss1.backward(retain_graph=True) # computes gradients
optimizer.step() # updates parameters in place
...
 
loss2.backward() # throws an error due to the in-place update from optimizer step
</code></pre>
<p>Because of this, the <code>step</code> calls must happen after all the <code>backward</code> calls. This leads to an issue of how to control gradients. The following does not work:</p>
<pre class=""lang-py prettyprint-override""><code># trying to zero grads fails
loss1.backward() # backward first loss
optimizer1.zero_grad() # zero model2 grads
loss2.backward() # loss 2 grads computes for model1

# changing the loss order results in the same
</code></pre>
<p>I think there are two ways around this:</p>
<ol>
<li>Cache grads outside pytorch parameters and manually re-add them to parameters before calling <code>step</code> (hacky, probably breaks a bunch of stuff)</li>
<li>Do an extra forward pass (inefficient from compute, but works within Pytorch's structure)</li>
</ol>
<p>I opted for the second option</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

torch.manual_seed(42)

class Net1(nn.Module):
    def __init__(self):
        super(Net1, self).__init__()
        self.conv1 = nn.Linear(20, 10)
        self.conv2 = nn.Linear(10, 5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

# Define the second model
class Net2(nn.Module):
    def __init__(self):
        super(Net2, self).__init__()
        self.conv1 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.conv1(x)
        return x

# Initialize models
model1 = Net1()
model2 = Net2()

# Initialize separate optimizers for each model
opt1 = torch.optim.SGD(model1.parameters(), lr=0.1)
opt2 = torch.optim.SGD(model2.parameters(), lr=0.1)

p1_1 = next(model1.parameters()).data.clone()
p2_1 = next(model2.parameters()).data.clone()

criterion = nn.MSELoss()

inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs)
out1 = model2(features)
out2 = model2(features.detach()) # detach removes from the computational graph

opt1.zero_grad()
opt2.zero_grad()

# update only model1 with loss1
loss1 = criterion(out1[0], labels[0]) 
loss1.backward()
opt2.zero_grad()
opt1.step()

# check parameters after update
p1_2 = next(model1.parameters()).data.clone()
p2_2 = next(model2.parameters()).data.clone()
assert not (p1_1 == p1_2).any() # all parameters from model1 updated
assert (p2_1 == p2_2).all() # no parameters from model2 updated

# update only model2 with loss2
loss2 = criterion(out2, labels)
loss2.backward()
opt2.step()

# check parameters after update
p1_3 = next(model1.parameters()).data.clone()
p2_3 = next(model2.parameters()).data.clone()
assert (p1_2 == p1_3).all() # no parameters from model1 updated
assert not (p2_2 == p2_3).any() # all parameters from model2 updated
</code></pre>
<p>Note that I changed your loss from cross entropy to MSE. Your model produces an output of size <code>(bs, 1)</code> while cross entropy expects an output of shape <code>(bs, num_classes)</code>. The output of shape <code>(bs, 1)</code> implies you are computing cross entropy of a single class, which will always return 0 loss</p>
<pre class=""lang-py prettyprint-override""><code>pred = torch.randn(64, 1)
labels = torch.randn(64, 1)
criterion = nn.CrossEntropyLoss()
criterion(pred, labels)
&gt; tensor(-0.)
</code></pre>
","2024-11-20 19:59:08","1","Answer"
"79208598","","Unable to update a latent vector using custom loss function in pytorch","<p>I am trying to implement this function but have had no luck. There is a VAE model that I am using, and along with it, there are encoder and decoder. I'm freezing the weights of the VAE decoder, and trying to change a latent vector which is updated using the function <em><strong>optimize_latent_vector(model, inp__, num_epochs=50, learning_rate=0.01)</strong></em>. Now, there is some error regarding this piece of code: <em><strong>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</strong></em></p>
<pre><code>
class VAE_GD_Loss(nn.Module):
    def __init__(self):
        super(VAE_GD_Loss, self).__init__()

    def forward(self, bad_seg, recons_mask, vector):
        # l2 normed squared and the soft dice loss are calculated
        loss = torch.sum(vector**2)+Soft_Dice_Loss(recons_mask, bad_seg)
        return loss

def optimize_latent_vector(model, inp__, num_epochs=50, learning_rate=0.01):
    inp__ = inp__.to(device).requires_grad_(True)
    # Encode and reparameterize to get initial latent vector
    with torch.no_grad():
        mu, log_var = model.encoder(inp__)
        z_latent_vect = model.reparameterize(mu, log_var)
    optimizer_lat = torch.optim.Adam([z_latent_vect], lr=learning_rate)
    dec_only = model.decoder
    
    for epoch in range(num_epochs):
        optimizer_lat.zero_grad()
        dec_only.eval()
        # Decode from latent vector
        recons_mask = dec_only(z_latent_vect)
        # Calculate loss
        VGLoss = VAE_GD_Loss()
        loss = VGLoss(inp__, recons_mask, z_latent_vect)
        # loss = Variable(loss, requires_grad=True)
        # Backpropagation
        loss.backward()
        optimizer_lat.step()
        print(f&quot;Epoch {epoch}: Loss = {loss.item()}&quot;)
    
    return z_latent_vect
</code></pre>
<p>If we uncomment the line <em><strong>loss = Variable(loss, requires_grad=True)</strong></em>, then the code runs, but it doesn't minimize the loss whatsoever. I want to update the latent vector in such a way so that it follows the constraint set in the loss function. Any leads would help!</p>
","2024-11-20 18:52:44","0","Question"
"79207039","79205208","","<p>There are no pre-built binaries of pytorch with cuda-11.2 indeed. If you necessarily want to go with this version of cuda, you have two choices I think:</p>
<ul>
<li>Use pytorch binaries compiled with cuda-11.1, which should work just fine</li>
<li>Build pytorch from source, as described <a href=""https://github.com/pytorch/pytorch#from-source"" rel=""nofollow noreferrer"">here</a></li>
</ul>
<p>I'm basically repeating what is said on <a href=""https://discuss.pytorch.org/t/want-to-install-pytorch-for-custom-cuda-version-cuda-11-2/141159"" rel=""nofollow noreferrer"">this pytorch thread</a>, you can read it for more details</p>
<p>I would not try to have multiple versions of cuda and manually &quot;hotswap&quot; them by tinkering with the cuda paths. (Opinion here) From experience, it can work but is also <em>very</em> error prone and will lead to problems eventually</p>
","2024-11-20 11:39:41","1","Answer"
"79206923","79203477","","<p>I have tried installing the libraries</p>
<p>Like below:</p>
<pre><code>%pip install pandas numpy azure-core sqlalchemy textanalytics torch==2.0.1 tensorflow==2.13.0 sentence-transformers==2.2.2
</code></pre>
<p>Regarding the:</p>
<blockquote>
<p>Itensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</p>
</blockquote>
<p>TensorFlow is designed for high performance, leveraging hardware capabilities to run computations efficiently.
It can work with CPUs, GPUs, or TPUs, adapting its code to the hardware available.
Some CPUs support advanced operations, like vectorized addition (processing multiple variables simultaneously), which others may not.
TensorFlow notifies you that the installed version can utilize AVX and AVX2 instructions—Advanced Vector Extensions that accelerate tasks such as matrix multiplication during forward or backward propagation.
This is not an error; it is simply informing you that TensorFlow will optimize for your CPU's capabilities to enhance performance.</p>
<p>If you want to you can disable this messages using:</p>
<pre><code>import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
</code></pre>
<p><strong>Results:</strong></p>
<pre><code>print(&quot;Available devices:&quot;, tf.config.list_physical_devices())
</code></pre>
<pre><code>
Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
</code></pre>
","2024-11-20 11:07:37","0","Answer"
"79205991","","Training different stage of model with different loss","<p>I'm trying to train a two-stage model in an end-to-end way. However, I want to update the different stages of models with different losses. For example, suppose the end to end model is composed of two models:model1 and model2. The output is calculated through running</p>
<pre><code>features = model1(inputs)
output = model2(features)
</code></pre>
<p>I want to update the parameters of model1 with loss1, while keeping the parameter of model2 unchanged. Next, I want to update the parameters of model2 with loss2, while keeping the parameter of model1 unchanged. My full implementation is something like:</p>
<pre><code>import torch
import torch.nn as nn

# Define the first model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Linear(20, 10)
        self.conv2 = nn.Linear(10, 5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

# Define the second model
class Net1(nn.Module):
    def __init__(self):
        super(Net1, self).__init__()
        self.conv1 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.conv1(x)
        return x

# Initialize models
model1 = Net()
model2 = Net1()

# Initialize separate optimizers for each model
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# Sample inputs and labels
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs)         
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 
   
loss1.backward(retain_graph=True)  
optimizer.step()  
optimizer.zero_grad()       
optimizer1.zero_grad()  

 
loss2.backward()        
</code></pre>
<p>However, this will return</p>
<pre><code>Traceback (most recent call last):
  File , line 55, in &lt;module&gt;
    loss2.backward()        
    ^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py&quot;, line 521, in backward
    torch.autograd.backward(
  File &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, line 289, in backward
    _engine_run_backward(
  File &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [10, 5]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
</code></pre>
<p>I kinda understand why this is happening, but is there a way to address this?</p>
","2024-11-20 06:10:33","0","Question"
"79205532","79195948","","<p>I can <strong>reproduce</strong> your problem on Windows 10, with CPython 3.8.1, Numpy 1.24.3, Torch 49444c3e (the packages are the default ones installed via pip). Torch is setup to use my (i5-9600KF) CPU. Here is the result:</p>
<p><a href=""https://i.sstatic.net/j84tgaFd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/j84tgaFd.png"" alt=""enter image description here"" /></a></p>
<p>I can also see that <strong>torch uses only 1 core while Numpy uses multiple cores</strong> (only) for complex numbers. Numpy uses OpenBLAS internally by default. Torch probably uses another implementation which is not optimized for that (no parallelism for an unknown reason). I can see that the real version uses OpenMP internally while the complex one does not. Both does not appear to call any (dynamic) BLAS function internally (which tends to confirm they use their own implementation unless they statically linked a BLAS).</p>
<p>Assuming they also use a BLAS but the default one use is not efficient, then you can certainly compile/package it so to <strong>link another faster BLAS implementation</strong> (possibly OpenBLAS or another one like BLIS or the Intel MKL).</p>
<p>If they uses their own implementation, then you can open an issue about this so to use OpenMP also in the complex version.</p>
<p>AFAIK, Torch is optimized for real simple-precision computations on GPUs and not really complex double-precision computations on CPUs. Thus, maybe they did not care about this yet.</p>
<hr />
<h2>Side notes</h2>
<p>Note I can see the following warning during the execution by the way:</p>
<pre class=""lang-none prettyprint-override""><code>&lt;ipython-input-85-1e20a6760269&gt;:18: RuntimeWarning: overflow encountered in matmul
  b = A@b
&lt;ipython-input-85-1e20a6760269&gt;:18: RuntimeWarning: overflow encountered in matmul
  b = A@b
&lt;ipython-input-85-1e20a6760269&gt;:18: RuntimeWarning: invalid value encountered in matmul
  b = A@b
</code></pre>
","2024-11-20 01:17:08","1","Answer"
"79205208","","How to setup TF and Torch on one virtual environment with same CUDA","<p>I want to setup tensorflow and pytorch on one virtual environment with same CUDA. However, I cannot find a CUDA version that can support both tensorflow and pytorch: For tensorflow 2.10, I selected CUDA 11.2. But I didn't find this CUDA version in the list for supporting pyTorch. I can only find the CUDA 11.1 in the list for pyTorch. Detailed information is listed below.</p>
<ol>
<li><p>To find CUDA version for Tensorflow
<a href=""https://www.tensorflow.org/install/source_windows#tested_build_configurations"" rel=""nofollow noreferrer"">https://www.tensorflow.org/install/source_windows#tested_build_configurations</a>
<a href=""https://i.sstatic.net/fZ15xh6t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fZ15xh6t.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>To find CUDA version for PyTorch
<a href=""https://elenacliu-pytorch-cuda-driver.streamlit.app/"" rel=""nofollow noreferrer"">https://elenacliu-pytorch-cuda-driver.streamlit.app/</a>
<a href=""https://i.sstatic.net/6o8JosBM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6o8JosBM.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>Will there be any problems if I install 2 different CUDA versions if I want to run the codes with GPU card? For example, after I create a virtual environemt by &quot;conda create --name myenv python=3.10&quot;, I want to run codes with tensorflow for project 1, and codes with pyTorch for project 2.</p>
<p>Do I need to modify the &quot;CUDA_PATH&quot; in system variable every time before I ran the codes. i.e., set CUDA_PATH for CUDA 11.1 when I need to use PyTorch, and set CUDA_PATH for CUDA 11.2 when I need to use Tensorflow?</p>
<p>I find there is an option of installing CUDA 11.0, which is compatible with TF-2.4 and PyTorch-1.7.
But there is a problem that it does not support CUDA capability SM_86. Will it be a problem of losing access to new features?
<a href=""https://i.sstatic.net/LhbhGynd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LhbhGynd.png"" alt=""enter image description here"" /></a></p>
","2024-11-19 22:13:34","0","Question"
"79203477","","Pytorch in Azure Synapse causing problems","<p>I have a notebook in Azure Synapse that is using these libraries</p>
<pre><code>import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import sqlalchemy as sa
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
from sentence_transformers import SentenceTransformer, util
import time
import torch

from notebookutils import mssparkutils
</code></pre>
<p>Since this month the session stops after the cell with the imports above with the warning below and I can't really find a solution on my own.</p>
<pre><code>/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory
  warn(f&quot;Failed to load image Python extension: {e}&quot;)
2024-11-19 11:31:17.000433: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</code></pre>
<p>I tried to rollback to old versions of Torch and also to force CPU usage but without success.</p>
<p>The input is just the cell with the imports. Then I get the warning. After the warning the Spark session stops with the massage.</p>
<blockquote>
<p>Session failed. Run the notebook to start a new session.</p>
</blockquote>
<p>This notebook was working just fine 2-3 weeks ago and something out of my sight happened. I also believe that even the warning was there and everything was fine.</p>
<p>Additionally I can provide the logs from Monitoring &gt; Apache Spark applications &gt; Driver (stderr) &gt; Latest, but I think everything there is unrelated to the problem (maybe):</p>
<pre><code>    WARN TokenLibrary [pool-43-thread-2]: Access token cache miss or expired
    2024-11-19 11:24:16,395 ERROR TokenLibrary [pool-43-thread-2]: Unable to determine host value from URI = tokenservice2.westeurope.azuresynapse.net:443. Using localhost as header value

WARN SQLConf [spark-listener-group-shared]: The SQL config 'spark.sql.legacy.replaceDatabricksSparkAvro.enabled' has been deprecated in Spark v3.2 and may be removed in the future. Use `.format(&quot;avro&quot;)` in `DataFrameWriter` or `DataFrameReader` instead.

2024-11-19 11:23:32,622 WARN AzureBlobFileSystemStore [Thread-32]: checkDnsEntry: blabla.dfs.core.windows.net not found in the file /etc/hosts.
</code></pre>
","2024-11-19 12:45:44","0","Question"
"79202364","79202190","","<p>You can use an einsum</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.randn(125, 128)    # Shape: (125, 128)
b = torch.randn(128, 8, 64)  # Shape: (128, 8, 64)
c = torch.einsum('ij,jkl-&gt;ikl', a, b)
print(c.shape)
&gt; torch.Size([125, 8, 64])
</code></pre>
","2024-11-19 06:58:36","1","Answer"
"79202190","","torch matmul between 2D and 3D tensor","<p>Hi I have two tensors:</p>
<pre><code>a = torch.randn(125, 128)    # Shape: (125, 128)
b = torch.randn(128, 8, 64)  # Shape: (128, 8, 64)
</code></pre>
<p>I want the result has a shape of (125, 8, 64)</p>
<p>My first observation is: last dimension of a match the first dimension of b then I do:</p>
<pre><code>result = torch.matmul(a,b)
</code></pre>
<p>It gave me the error:</p>
<p>Expected size for first two dimensions of batch2 tensor to be: [128, 128] but got: [128, 8].</p>
<p>How can I do this.</p>
<p>Edit: I also dont' want to reshape into 2D and then reshape the result into 3D again.</p>
","2024-11-19 05:37:36","0","Question"
"79200895","79199199","","<p>Adding a dense layer without finetuning doesn’t seem to be practical because its weights are initialized randomly, therefore it disrupts the semantic space and making embeddings less meaningful for tasks like similarity searches. As you mentioned you can use techniques like <code>PCA</code> or <code>UMAP</code> to reduce embeddings to your desired size. This is computationally efficient and preserves relationships in the embedding space.</p>
<p>However, I think the best approach would be to explore other sentence transformer models like <code>all-MiniLM-L6-v2</code> (384 dimensions), which are already optimized for smaller embedding sizes while maintaining good accuracy. You can find a list of such models <a href=""https://sbert.net/docs/sentence_transformer/pretrained_models.html"" rel=""nofollow noreferrer"">here.</a></p>
","2024-11-18 17:40:12","1","Answer"
"79199805","79195948","","<p>When I run the code I get a different plot:</p>
<p><a href=""https://i.sstatic.net/8tLiXtTK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8tLiXtTK.jpg"" alt=""Plot"" /></a></p>
<p>torch: 2.3.1<br />
numpy: 1.26.4<br />
cuda: 12.2<br />
NVIDIA-Driver: 535.183.01 (Ubuntu)</p>
","2024-11-18 11:46:16","0","Answer"
"79199199","","Dimensionality reduction in sentence transformers","<p>I need to compute embeddings for a large number of sentences (say 10K) in preprocessing, and at runtime I will have to compute the embedding vector for one sentence at a time (user query), and then find the most resembling sentence based on the embedding vectors (using cosine similarity).</p>
<p>I'm currently using sentence transformers, and their output size is 768, which is too large for my case. So I'd like to experiment with smaller sizes, like 256 or even 128.</p>
<p>I'm familiar with PCA and quantization. However, both ChatGPT and Gemini suggested that I simply add a dense layer after the pooling layer. Example:</p>
<pre><code>dense = models.Dense(in_features=base_model.get_sentence_embedding_dimension(), out_features=256)
model = SentenceTransformer(modules=[base_model, dense])
</code></pre>
<p>My problem with this is that I think that I would have to retrain / finetune my model, which I cannot do since I don't have labeled data. But ChatGPT and Gemini are claiming that I could get away with this implementation without retraining or finetuning, although &quot;it would be better&quot;.</p>
<p>I'm confused how this could possibly work, because without training the initial weights in the dense layer would be random.</p>
<p>Am I missing something or is adding a dense layer without re-training / finetuning could actually work?</p>
","2024-11-18 08:31:21","0","Question"
"79198942","79165030","","<p>You need to download the version of Pytorch that is compatible with CUDA for that, head over to the website: <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">https://pytorch.org/</a></p>
<p><a href=""https://i.sstatic.net/fzj0QO26.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fzj0QO26.png"" alt=""As shown in the picture"" /></a></p>
<p>Download the latest one (12.4) and Run the command that is shown in the image on your Command Terminal.</p>
<p>Then Install Cuda same version (12.4) URL: <a href=""https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exe_local"" rel=""nofollow noreferrer"">https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exe_local</a></p>
<p><a href=""https://i.sstatic.net/oJsZsfjA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oJsZsfjA.png"" alt=""Download Same Version for CUDA that is compatible with Pytorch"" /></a></p>
<p>Then check the commands,</p>
<pre><code>Nvidia-smi
nvcc --version 
</code></pre>
<p>Ensure the versions for Pytorch &amp; CUDA are same, Hope this helps!</p>
","2024-11-18 06:55:23","-1","Answer"
"79196327","79188760","","<p>Tensors have a &quot;put_&quot; method requiring 2 arguments:</p>
<ol>
<li>A Long tensor of indices</li>
<li>A tensor of values</li>
</ol>
<p>i.e.</p>
<pre><code>indices = torch.tensor(list(mapping.keys()),dtype=torch.long)
values  = torch.tensor(list(mapping.values()))

t.put_(indices,values)
</code></pre>
<p>speedup of about 10-15x over manual indexing.
<a href=""https://i.sstatic.net/yfNbuI0w.png"" rel=""nofollow noreferrer"">Graph of speedup</a></p>
","2024-11-17 00:40:15","0","Answer"
"79195948","","PyTorch complex matrix-vector multiplication is slow on CPU","<p>I found pyTorch to be much slower than numpy when doing complex-valued matrix-vector multiplication on CPU:</p>
<p><a href=""https://i.sstatic.net/JpaqTtA2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpaqTtA2.png"" alt=""Comparison of numpy and torch (CPU) on real and complex matrix multiplication"" /></a></p>
<p>A few notes:</p>
<ul>
<li>This is true for me across multiple systems</li>
<li>Memory is not an issue</li>
<li>complex multiplication on torch does not max out the cores (unlike the other three cases)</li>
<li>torch version: 2.5.1+cu124</li>
<li>numpy version: 1.26.4</li>
<li>Cuda version: 12.6</li>
<li>NVidia driver: 560.35.03</li>
<li>I verified the results of the calculations are the same</li>
<li>Both use double precision (ie 64-bits for real and 128 bits for complex)</li>
<li>Switching to float (torch.cfloat) makes things slightly faster, but not much</li>
</ul>
<p>Perhaps I have misconfigured something?</p>
<p>Code to produce above plots:</p>
<pre><code>import torch
import numpy as np
import matplotlib.pyplot as plt
import time

maxn = 3000
nrep = 100

def conv(M,latype):
    if latype=='numpy':
        return np.array(M)
    if latype.startswith('torch,'):
        return torch.tensor(M,device=latype[7:])

def multtest(A,b):
    t0 = time.time()
    for i in range(nrep):
        b = A@b
    t1 = time.time()
    return (t1-t0)/nrep

ns = np.array(np.linspace(100,maxn,100),dtype=int)
numpyts = np.zeros(len(ns))
torchts = np.zeros(len(ns))

fig,axes = plt.subplots(1,2)
for ax,dtype in zip(axes,['real','complex']):
    Aorig = np.random.rand(maxn,maxn)
    borig = np.random.rand(maxn)
    if dtype == 'complex':
        Aorig = Aorig + 1.j*np.random.rand(maxn,maxn)
        borig = borig + 1.j*np.random.rand(maxn)

    for latype in ['numpy','torch, cpu']:
        A = conv(Aorig,latype)
        b = conv(borig,latype)
        ts = np.zeros(len(ns))
        for i,n in enumerate(ns):
            ts[i] = multtest(A[:n,:n],b[:n])
        ax.plot(ns,ts,label=latype)

    ax.legend()
    ax.set_title(dtype)
    ax.set_xlabel('vector/matrix size')
    ax.set_ylabel('mean matrix-vector mult time (sec)')

fig.tight_layout()
plt.show()
</code></pre>
","2024-11-16 19:51:41","1","Question"
"79195717","79190048","","<p>I doubt that this is the best/fastest solution, but using <code>torch.compile</code> does provide acceleration. I haven't yet tested when scaling up to thousands of Boolean functions.</p>
<pre><code>import torch
from time import time

A = torch.tensor([True, False, True]).to('cuda')  # Initial values.
B = torch.tensor([False, True, True]).to('cuda')  # Values don't matter. Will write over them in the first iteration

@torch.compile
def process(A, B, n_steps):
    for step in range(n_steps):

        # Use values in A to compute new values in B.
        # How to run the three lines below IN PARALLEL?
        B[0] = torch.logical_and(torch.logical_or( A[0], A[1]), A[2])  # func1: Y0 = X0 | X1 &amp; X2
        B[1] = torch.logical_or( torch.logical_or( A[0], A[1]), A[2])  # func2: Y1 = X0 | X1 | X2
        B[2] = torch.logical_and(torch.logical_and(A[0], A[1]), A[2])  # func3: Y2 = X0 &amp; X1 &amp; X2
        # Only after the above three lines above finish their computation (and B has new values), should the lines below be run.

        # Use values in B to compute new values in A.
        # Note that the functions below are identical to the ones above (which may allow for some additional acceleration?)
        # How to run the three lines below IN PARALLEL?
        A[0] = torch.logical_and(torch.logical_or( B[0], B[1]), B[2])  # func1: Y0 = X0 | X1 &amp; X2
        A[1] = torch.logical_or( torch.logical_or( B[0], B[1]), B[2])  # func2: Y1 = X0 | X1 | X2
        A[2] = torch.logical_and(torch.logical_and(B[0], B[1]), B[2])  # func3: Y2 = X0 &amp; X1 &amp; X2
        # Only after the above three lines above finish their computation (and A has new values), should the next loop be run.

    return A

# First run is slow due to compilation
t_start = time()
A = process(A, B, 100)
print(f'{time()-t_start} seconds')

# Runs are faster subsequently, and can be looped over to effectively increase n_steps
t_start = time()
A = process(A, B, 100)
print(f'{time()-t_start} seconds')
</code></pre>
<p>Output without @torch.compile decorator:</p>
<pre><code>First run time: 0.12589144706726074 seconds
Second run time: 0.059000492095947266 seconds
</code></pre>
<p>Output with @torch.compile decorator:</p>
<pre><code>First run time: 18.201257467269897 seconds
Second run time: 0.007639169692993164 seconds
</code></pre>
","2024-11-16 17:33:05","1","Answer"
"79193935","79192371","","<p>You can do the following:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.zeros(2, 3, 4, 6)
mask = torch.tensor([[ True, True, False], [True, False, True]])
y = torch.rand(2, 3, 1, 3)
x[..., :3][mask] = y[mask]
</code></pre>
<p>This produces the same result as</p>
<pre class=""lang-py prettyprint-override""><code>i, j = mask.nonzero(as_tuple = True)
x[i, j, :, :3] = y[i, j]
</code></pre>
<p>For the 2D mask scenario. This method also works for additional dims:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.zeros(2, 3, 3, 4, 6)
y = torch.rand(2, 3, 3, 1, 3)
mask = torch.rand(2,3,3)&gt;0.5
x[..., :3][mask] = y[mask]
</code></pre>
<p>For additional dims, the only constraint is that the first <code>n=mask.ndim</code> dims of <code>x</code> and <code>y</code> must match the shape of <code>mask</code> and the final dimension of <code>y</code> is <code>3</code> to match the <code>:3</code>.</p>
","2024-11-15 20:52:46","1","Answer"
"79192372","79192371","","<p>I think I've found a solution, though it is not as convenient as direct indexing:</p>
<pre class=""lang-py prettyprint-override""><code>i, j = mask.nonzero(as_tuple = True)
x[i, j, :, :3] = y[i, j]
</code></pre>
<p>However, in the situation that the shape of the masked dimensions are unknown, an additional reshaping will be involved to squeeze the unknown dimensions in one, which still causes extra time and memory.
So I still think that it is nonsense that mask and slice assignment cannot be used simultaneously.</p>
","2024-11-15 12:25:30","1","Answer"
"79192371","","How to combine slice assignment, mask assignment and broadcasting in PyTorch?","<p>To be more specific, I'm wondering how to assign a tensor by slice and by mask at different dimension(s) simultaneously in PyTorch.
Here's a small example about what I want to do:
With the tensors and masks below:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.zeros(2, 3, 4, 6)
mask = torch.tensor([[ True, True, False], [True, False, True]])
y = torch.rand(2, 3, 1, 3)
</code></pre>
<p>I want to achieve something like</p>
<pre class=""lang-py prettyprint-override""><code>x[mask, :, :3] = y[mask]
</code></pre>
<ul>
<li>In dimension 0 and 1, only the 4x6/1x3 slices in <code>x</code>/<code>y</code> that whose corresponding element in <code>mask</code> is <code>True</code> are allowed to be assigned.</li>
<li>In dimension 2, I hope the 1-row tensor in <code>y</code> can be broadcast to all the 8 rows of <code>x</code>,</li>
<li>In dimension 3, only the first 3 elements in <code>x</code> are assigned with the 3-element tensor from <code>y</code>.</li>
</ul>
<p>However, with code above, following error was caught:</p>
<pre><code>RuntimeError: shape mismatch: value tensor of shape [4, 1, 3] cannot be broadcast to indexing result of shape [4, 3, 6]
</code></pre>
<p>It seems that PyTorch did <code>[mask]</code> indexing instead, and ignored the <code>:3</code> indexing.</p>
<p>I've also tried</p>
<pre class=""lang-py prettyprint-override""><code>x[mask][:, :, :3] = y[mask]
</code></pre>
<p>No error occurred but the assignment still failed.</p>
<p>I know I can assign by slice and by mask step by step, but I hope to avoid any intermediate tensors if possible.
Tensors in neural networks may be extremely big, so may be an all-in-one assignment may take less time and less memory.</p>
","2024-11-15 12:25:30","1","Question"
"79192240","79172140","","<p>I would expect that reducing the num_workers would decrease the memory usage. For example <code>num_workers=1</code> should only use 1/4 of the memory. Keep in mind this also reduces parallelization as you will be using less CPU cores/threads.</p>
<p>Another factor is the batch size. The code snippet contains <code>batch_size=SCAN_PROCESS_BATCH_SIZE</code>, I'm not sure what this value is declared as but reducing it will reduce the number of images loaded into memory.</p>
<p>Without seeing the rest of the code and inputs it's difficult to find any other reductions. One thing to look at would be the size of the input images and whether that could be reduced (e.g by a scaling image transform).</p>
","2024-11-15 11:46:47","0","Answer"
"79190048","","In pytorch, how can I parallelize of a set of boolean functions that are executed (on a GPU) repeatedly?","<p>I have a set of Boolean functions that are independent, and (hypothetically) can be executed in parallel. I want to call those same functions repeatedly. See the code below, in which the outputs of the functions ping-pong between the A and B memory locations. How can I force the &quot;IN PARALLEL&quot; lines to be run in parallel on an NVIDIA GPU with CUDA installed?</p>
<pre class=""lang-py prettyprint-override""><code>import torch

A = torch.tensor([True, False, True]).to('cuda')  # Initial values.
B = torch.tensor([False, True, True]).to('cuda')  # Values don't matter. Will write over them in the first iteration.

n_steps = 100

for step in range(n_steps):

    # Use values in A to compute new values in B.
    # How to run the three lines below IN PARALLEL?
    B[0] = torch.logical_and(torch.logical_or( A[0], A[1]), A[2])  # func1: Y0 = X0 | X1 &amp; X2
    B[1] = torch.logical_or( torch.logical_or( A[0], A[1]), A[2])  # func2: Y1 = X0 | X1 | X2
    B[2] = torch.logical_and(torch.logical_and(A[0], A[1]), A[2])  # func3: Y2 = X0 &amp; X1 &amp; X2

    # Only after the above three lines above finish their computation (and B has new values), should the lines below be run.


    # Use values in B to compute new values in A.
    # Note that the functions below are identical to the ones above (which may allow for some additional acceleration?)
    # How to run the three lines below IN PARALLEL?
    A[0] = torch.logical_and(torch.logical_or( B[0], B[1]), B[2])  # func1: Y0 = X0 | X1 &amp; X2
    A[1] = torch.logical_or( torch.logical_or( B[0], B[1]), B[2])  # func2: Y1 = X0 | X1 | X2
    A[2] = torch.logical_and(torch.logical_and(B[0], B[1]), B[2])  # func3: Y2 = X0 &amp; X1 &amp; X2

    # Only after the above three lines above finish their computation (and A has new values), should the next loop be run.
</code></pre>
","2024-11-14 18:36:49","0","Question"
"79189314","78279823","","<p>Nice sharing !</p>
<p>One suggestion - extract register calls from <code>__init__()</code> to ensure PyTorch can serialize model for saving it; it meight fail to pickle these hooks.</p>
<p>Create hook and unhook functions to take care both of registering hooks and removing their handlers.</p>
<p>For your reference:</p>
<pre><code>class FooNet(nn.Module):
def hook_backward(self, module, grad_input, grad_output):
    self.logg_grad_output = torch.stack(grad_output)

def hook_forward(self, module, args, output):
    self.logg_forward_output = torch.stack(output)

def __init__(self, num_class=10):
    super().__init__()
    self.net = nn.Sequential(
        nn.LazyConv2d(6, kernel_size=5, padding=2), 
        nn.Sigmoid(),
        nn.AvgPool2d(kernel_size=2, stride=2),
        nn.LazyConv2d(16, kernel_size=5), 
        nn.Sigmoid(),
        nn.AvgPool2d(kernel_size=2, stride=2),
        nn.Flatten(),
        nn.LazyLinear(128), 
        nn.Sigmoid(),
        nn.LazyLinear(64), 
        nn.Sigmoid(),
        nn.LazyLinear(num_class))
    
def hook(self, idx_layer_hook=-1, enable_hook_fp=False, enable_hook_bp=True):
    if enable_hook_fp:
        self.logg_forward_args   = []
        self.logg_forward_output = []
        self.handler_fp = None
    if enable_hook_bp:
        self.handler_bp = None
        self.logg_grad_input     = []
        self.logg_grad_output    = []

    for idxx, layer in enumerate(self.net):
        if isinstance(layer,nn.Conv2d) and (idx_layer_hook != -1 and idx_layer_hook == idxx):
            print(f&quot;Hook layer[{idxx}]:&quot;, layer)
            if enable_hook_fp:
                self.handler_fp = layer.register_forward_hook(lambda *args,**kwargs:LeNet.hook_forward(self,*args,**kwargs))
            if enable_hook_bp:
                self.handler_bp = layer.register_full_backward_hook(lambda *args,**kwargs:LeNet.hook_backward(self,*args,**kwargs))

def unhook(self):
    if hasattr(self, 'handler_fp'):
        self.handler_fp.remove()
    if hasattr(self, 'handler_bp'):
        self.handler_bp.remove()    

def forward(self, x):
    assert hasattr(self, 'net')
    x = self.net(x)  
    return x
</code></pre>
<p>As long as grad got logged, we can review it</p>
<pre><code>gg = nw.logg_grad_output[0,0,0,:,:].numpy() 
str_title = &quot;grad_layer%d&quot;%(IDX_OF_LAYER_HOOK);
plt.style.use('grayscale')
fig, ax = plt.subplots(figsize=(4,2));
fig.canvas.manager.set_window_title(str_title);
ax.imshow(gg);
ax.set_title(str_title);
plt.show();    
</code></pre>
<p><a href=""https://i.sstatic.net/M6iq7w2p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M6iq7w2p.png"" alt=""enter image description here"" /></a></p>
","2024-11-14 14:58:07","0","Answer"
"79188760","","Map tensor elements by dictionary in Pytorch","<p>I have a tensor that is:</p>
<pre><code>t = torch.tensor([1, 2, 3])
</code></pre>
<p>and a mapping:</p>
<pre><code>mapping = {1: 0.2, 2: 1.2, 3: 3.0}
</code></pre>
<p>I want to map the element in t by mapping so the expected result will be</p>
<pre><code>torch.tensor([0.2, 1.2, 3.0])
</code></pre>
<p>Is there an efficient way of doing this? In Tensorflow could use for example <code>tf.lookup.StaticHashTable</code>.</p>
<p>Thanks for any help.</p>
","2024-11-14 12:34:40","0","Question"
"79188195","78297310","","<p><strong>Quick solution:</strong></p>
<p>Go to respective utils.py file and remove the following lines:</p>
<pre><code>from taming.data.helper_types import Annotation
from torch._six import string_classes
from torch.utils.data._utils.collate import np_str_obj_array_pattern, default_collate_err_msg_format
from tqdm import tqdm
</code></pre>
<p>I hope this resolves your issue.</p>
","2024-11-14 10:06:37","0","Answer"
"79181397","79173214","","<p>You can use <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html"" rel=""nofollow noreferrer""><code>torch.Tensor.unfold()</code></a> to gain some significant speedup. With <code>unfold()</code>, you can produce a sliding window view into your given <code>image</code> tensor, which you can then compare to your <code>kernel</code> without explicit <code>for</code> loops (see the code comments for the intermediate tensor shapes):</p>
<pre class=""lang-py prettyprint-override""><code>def proposed(image, kernel, stride=1):
    # ^ image: [b, c, hi, wi], kernel: [c, hk, wk], stride=s
    patched = image.unfold(-2, kernel.shape[-2], stride)
    # ^ patched: [b, c, (hi-hk)//s+1, wi, hk]
    patched = patched.unfold(-2, kernel.shape[-1], stride)
    # ^ patched: [b, c, (hi-hk)//s+1, (wi-wk)//s+1, hk, wk]
    patched = patched.movedim(-5, -3)
    # ^ patched: [b, (hi-hk)//s+1, (wi-wk)//s+1, c, hk, wk]
    _n = 1 / kernel.nelement()
    sim = patched.sub(kernel).abs_().sum(dim=(-3, -2, -1)).mul_(_n).neg_().add_(1)
    # ^ sim: [b, (hi-hk)//s+1, (wi-wk)//s+1]
    return sim
</code></pre>
<p>Here is how I tested it:</p>
<pre class=""lang-py prettyprint-override""><code>from timeit import Timer

import torch
import torch.nn.functional as F

# FIXME: Add code for `tensor_kernel_similarity()` and `proposed()` here

torch.manual_seed(42)

# TODO: Adjust parameters as necessary
device = torch.device(&quot;cpu&quot;)
s = 2
b, c, hi, wi = 1, 3, 101, 101
hk, wk = 10, 9
assert b == 1  # In `tensor_kernel_similarity()`, the value of `similarity.shape[0]` is hard-coded to 1

# Create some random test data with values in [0., 1.)
img = torch.rand(size=(b, c, hi, wi)).to(device)
knl = torch.rand(size=(c, hk, wk)).to(device)

given = tensor_kernel_similarity(img, knl, stride=s)
own = proposed(img, knl, stride=s)
assert torch.allclose(given, own)
print(&quot;Timing given:&quot;, Timer(lambda: tensor_kernel_similarity(img, knl, stride=s)).timeit(100))
print(&quot;Timing proposed:&quot;, Timer(lambda: proposed(img, knl, stride=s)).timeit(100))
</code></pre>
<p>This gave me on the CPU:</p>
<pre class=""lang-none prettyprint-override""><code>Timing given: 9.2…
Timing proposed: 0.05…
</code></pre>
<p>And on the GPU (<code>device = torch.device(&quot;cuda:0&quot;)</code>):</p>
<pre class=""lang-none prettyprint-override""><code>Timing given: 18.8…
Timing proposed: 0.009…
</code></pre>
<p>Two caveats:</p>
<ul>
<li>You will probably notice the time-vs-memory tradeoff here. In particular, with <code>patched.sub(kernel)</code>, the memory of <code>patched</code> has to be inflated to the full size of the sliding window view – <code>patched.sub_(kernel)</code>, i.e. an in-place change of <code>patched</code>, would lead to wrong results since the elements of the memory that is underlying the view would be overwritten multiple times. If you run into limitations here, consider maybe a hybrid between your current approach and mine. Also, if you want to have both (fast code and no intermediate memory peak, that is), you might be better off sticking to a &quot;loopy&quot; approach and accelerating it with something like <a href=""https://numba.pydata.org/"" rel=""nofollow noreferrer"">Numba</a>.</li>
<li>I am not sure if all situations are handled the same way between your current approach and mine. I successfully tested with all combinations for <code>s in [1, 2, 3]</code>, <code>hi in [99, 100, 101]</code>, <code>wi in [99, 100, 101]</code>, <code>hk in [9, 10, 11]</code>, <code>wk in [9, 10, 11]</code>, but maybe I overlooked some cases.</li>
</ul>
","2024-11-12 13:48:28","2","Answer"
"79173214","","Calculate sliding ""image similarity"" score similar to conv2d","<p>I have a large image and a smaller &quot;kernel&quot; image. I would like to compare the kernel with each part of the image (by &quot;sliding&quot; the kernel across the image) and retrieve a &quot;similarity tensor&quot; (indicating how similar the kernel is to the image patch at the shifted position). This is very much like conv2d, but my kernel is an RGB/HSV image and thus not a valid convolution kernel.</p>
<p>For example if my kernel image is black, conv2d would return 0 everywhere as similarity to any image, but I want similarity to be 1 if the large image is black in a certain region.</p>
<p>I have written this inefficient method to demonstrate what I want:</p>
<pre><code>def tensor_kernel_similarity(image, kernel, stride = 1):

    assert len(kernel.shape) == 3
    assert len(image.shape) == 4

    similarity = torch.zeros((1,
                              int((image.shape[-2]-kernel.shape[-2])/stride)+1, 
                             int((image.shape[-1]-kernel.shape[-1])/stride)+1), dtype=torch.float, device=image.device)
    for b in range(0, similarity.shape[0]):
        for y in range(0, similarity.shape[1]):
            for x in range(0, similarity.shape[2]):
                image_patch = image[b,:, y*stride:y*stride+kernel.shape[1], x*stride:x*stride+kernel.shape[2]]
                
                #pad image patch to kernel size:
                if kernel.shape[2]-image_patch.shape[2] + kernel.shape[1]-image_patch.shape[1] &gt; 0:
                    image_patch = F.pad(image_patch, (0, kernel.shape[2]-image_patch.shape[2], 0, kernel.shape[1]-image_patch.shape[1]), value=0)
                
                # calculate the diff between the image patch and the kernel (in the range [0,1])
                diff = image_patch.sub(kernel).abs().sum()/kernel.nelement()
                # similarity is the inverse of the diff
                sim = 1 - diff
                # store similarity in the output tensor                
                similarity[b,y,x] = sim
    return similarity
</code></pre>
<p>Is there a way to use conv2d instead to get what I want? Or is there a way to significantly speed this code up?</p>
","2024-11-09 16:31:16","3","Question"
"79172140","","Why is my DataLoader process using up to 2.6GB of virtual memory, and is there any way to reduce it?","<p>Why is my DataLoader process using up to 2.6GB of virtual memory, and is there any way to reduce it?<br />
Each DataLoader process takes up 2.6GB of virtual memory, and 4 processes take up 10.4GB.</p>
<pre><code>    from transformers import AutoModelForZeroShotImageClassification, AutoProcessor
    ...
    dataset = ImageDataset(image, clip_processor, SCAN_PROCESS_BATCH_SIZE, test_times)
    dataloader = DataLoader(dataset, batch_size=SCAN_PROCESS_BATCH_SIZE, num_workers=4)
</code></pre>
<pre><code>from torch.utils.data import Dataset

class ImageDataset(Dataset):
    def __init__(self, image, processor, scan_process_batch_size, test_times):
        self.image = image
        self.processor = processor
        self.scan_process_batch_size = scan_process_batch_size
        self.test_times = test_times

    def __len__(self):
        return self.scan_process_batch_size * self.test_times

    def __getitem__(self, idx):
        # Use processor to process images
        inputs = self.processor(
            images=[self.image],
            return_tensors=&quot;pt&quot;,
            padding=True
        )['pixel_values']
        return inputs
</code></pre>
<hr />
<p>I looked at the <code>memory</code> bar through <code>process hacker</code> software, and there were a large number of 32MB rows in the <code>private</code> column, and i save it and find there were all 0 in this 32MB memory.
<a href=""https://i.sstatic.net/M6yiHu7p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M6yiHu7p.png"" alt=""enter image description here"" /></a></p>
<p>i find that the virtual memory increases after there code.</p>
<pre><code>        for i in range(self._num_workers):
            # No certainty which module multiprocessing_context is
            index_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
            # Need to `cancel_join_thread` here!
            # See sections (2) and (3b) above.
            index_queue.cancel_join_thread()
            w = multiprocessing_context.Process(
                target=_utils.worker._worker_loop,
                args=(self._dataset_kind, self._dataset, index_queue,
                      self._worker_result_queue, self._workers_done_event,
                      self._auto_collation, self._collate_fn, self._drop_last,
                      self._base_seed, self._worker_init_fn, i, self._num_workers,
                      self._persistent_workers, self._shared_seed))
            w.daemon = True
            # NB: Process.start() actually take some time as it needs to
            #     start a process and pass the arguments over via a pipe.
            #     Therefore, we only add a worker to self._workers list after
            #     it started, so that we do not call .join() if program dies
            #     before it starts, and __del__ tries to join but will get:
            #     AssertionError: can only join a started process.
            w.start()
            self._index_queues.append(index_queue)
            self._workers.append(w)
</code></pre>
","2024-11-09 04:53:31","0","Question"
"79169956","79127307","","<p>I am not sure what exact scenario you are using it for, or how you expect it to work but, assuming you just want to access the value of gradient as a new source node in the graph, the issue would be the value accessed by <code>.grad</code> is not a tensor with requires_grad true.</p>
<pre class=""lang-py prettyprint-override""><code>print(point_2.requires_grad) # True
print(point_2.grad.requires_grad) # False
</code></pre>
<p>So you have to clone and detach the value then make require_grad True</p>
<pre class=""lang-py prettyprint-override""><code>grad_norm = torch.norm(point_2.grad.clone().detach().requires_grad_(True))
</code></pre>
","2024-11-08 12:04:38","0","Answer"
"79168492","79167465","","<p><code>MATH</code> is the pytorch C++ attention implementation</p>
<p><code>FLASH_ATTENTION</code> is the attention implementation from the <a href=""https://arxiv.org/abs/2307.08691"" rel=""nofollow noreferrer"">flash attention</a> paper</p>
<p><code>EFFICIENT_ATTENTION</code> is the implementation from the facebook <a href=""https://github.com/facebookresearch/xformers"" rel=""nofollow noreferrer"">xformers</a> library</p>
<p><code>CUDNN_ATTENTION</code> is the implementation from the Nvidia CuDNN library</p>
<p>You can read more about the differences <a href=""https://pytorch.org/blog/out-of-the-box-acceleration/"" rel=""nofollow noreferrer"">here</a></p>
","2024-11-08 00:50:21","3","Answer"
"79167465","","What is the difference between various backends in torch.nn.attention.SDPBackend, and what do they mean?","<p>In the pytorch docs on <a href=""https://pytorch.org/docs/stable/generated/torch.nn.attention.SDPBackend.html#torch.nn.attention.SDPBackend"" rel=""nofollow noreferrer"">SDPBackend</a> there are a few enums available to be used with the context manager,</p>
<p>ERROR: An error occurred when trying to determine the backend.<br />
MATH: The math backend for scaled dot product attention.<br />
FLASH_ATTENTION: The flash attention backend for scaled dot product attention.<br />
EFFICIENT_ATTENTION: The efficient attention backend for scaled dot product attention.<br />
CUDNN_ATTENTION: The cuDNN backend for scaled dot product attention.</p>
<p>What do they mean and how are they different?</p>
<p>What exactly is the EFFICIENT ATTENTION backend? And another is I checked with  torch.backends.cuda.flash_sdp_enabled() on a machine without GPU and it is true but isn't flash attention only supposed to be for GPU's and it is based on using GPU cache memory? Is efficient attention just flash attention 2?</p>
","2024-11-07 17:14:51","1","Question"
"79166943","79165030","","<p>There are 2 versions of CUDA installed on your computer:</p>
<ul>
<li>the driver version, (shown by <code>nvidia-smi</code>, also called CUDA-toolkit) is the version that is installed when you update your driver, this is the cuda used any time some process want execute something in GPU. This should be the most recent one (or so) in any cases</li>
<li>the runtime version, it's the cuda used specifically by Pytorch to make parallelized computations via its routines. The version of pytorch and the runtime version should match. The driver-CUDA is called after the runtime routines, the calling scheme is:</li>
</ul>
<p>Pytorch Python API -&gt; Pytorch C++ API -&gt; runtime CUDA routines -&gt; local driver CUDA -&gt; GPU</p>
<p><strong>Since Pytorch 2.0 the runtime cuda libraries are automatically installed in your environment so you only need to update your nvidia drivers (and upgrade pip) before calling <code>pip install torch</code></strong></p>
<p>Make sure your python version is 3.9+ (using a virtual environment is highly recommended). Your graphic card is quite recent so there is no problem regarding it (some old graphic card could have cuda compatibility issues)</p>
<p><code>pip install torch</code> should install torch==2.5.1 with runtime cuda 12.4 if everything is good (most recent version when I'm writing this)</p>
","2024-11-07 14:40:22","-1","Answer"
"79165155","79165030","","<p>I am assuming you are talking about CUDA version 12.6 that is mentioned when you run the nvidia-smi command in terminal.<a href=""https://i.sstatic.net/Jp8qyPe2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jp8qyPe2.png"" alt=""demo of nvidia-smi output"" /></a></p>
<p>This shows the version of CUDA that is supported by your GPU and does not mean that CUDA 12.6 is installed on your system.</p>
<p>You need to run the following commands based on OS:</p>
<p>On Windows System:</p>
<pre><code>pip3 install torch --index-url https://download.pytorch.org/whl/cu124
</code></pre>
<p>On Linux System:</p>
<pre><code>pip3 install torch
</code></pre>
<p>Note: Also make sure your virtual environment is active in pycharm terminal before running the above pip commands.</p>
","2024-11-07 05:48:45","-2","Answer"
"79165030","","How can I enable CUDA in PyTorch for Nvidia GeForce RTX 3050 Ti?","<p>I want to run the PyTorch library, which I am running in a virtual environment in PyCharm, on my graphics card, which is an Nvidia GeForce RTX 3050 Ti. However, it's running on the CPU, and whenever I use the command <code>import torch</code> and <code>print(&quot;cuda is available:&quot;, torch.cuda.is_available())</code>, it always returns False.</p>
<p>I have CUDA version 12.6 installed. I also installed PyTorch for CUDA version 12.4 because it was the latest version available on the PyTorch website. What should I install considering my graphics card type?</p>
","2024-11-07 04:30:29","0","Question"
"79161882","79157540","","<p>In VSCode you can open the Command Palette with <code>Ctrl+Shift+P</code> and the search for <code>Python: Select Interpreter</code>. There you can select the Python version you want to use and from then on, the scripts you execute from VSCode will use that version by default.</p>
","2024-11-06 08:52:45","0","Answer"
"79159608","79137287","","<p>You probably have to add an extra index for whl as per PyTorch <a href=""https://pytorch.org/get-started/previous-versions/#linux-and-windows-35"" rel=""nofollow noreferrer"">official docs</a></p>
<p>This might fix the issue
<code>pip install torch==1.11.0+cu113 --extra-index-url https://download.pytorch.org/whl/cu113 </code></p>
","2024-11-05 14:55:27","1","Answer"
"79157540","","Why isn't my PyTorch imported correctly on VS code?","<p>I downloaded PyTorch for the first time to use for a research project in school. I used the command &quot;pip3 install torch&quot; to download it on my terminal and everything went smoothly at first, but if I try to import it into files on VS code and run them, it reads, &quot;ImportError: No module named torch.&quot; However, the code runs smoothly if I run it on the terminal. I've tried downloading it again through the venv, but it still doesn't work.</p>
<p>From here, I'm not sure how to go about and fix this? Should I just try uninstalling and reinstalling? If anyone could give specific instructions and advice on how to resolve this, that would be greatly appreciated.</p>
<p>For reference, my current version of Python is 3.12.5 and PyTorch 2.4.1.</p>
<p>edit: through trying import sys; print(sys.path), I found that my VS code seems to be using Python 2.7 instead of 3.12. Afterwards, I tried to switch the interpreter path to 3.12.5 ('venv': venv) as the recommended setting, but the program on VS code remains as 2.7. How to resolve?</p>
<p>VS code output:</p>
<pre class=""lang-bash prettyprint-override""><code>'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old', '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload', '/Library/Python/2.7/site-packages', '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python', '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/PyObjC']

terminal output: ['', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python312.zip', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload', '/Users/-/Library/Python/3.12/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages']
</code></pre>
<p>EDIT: Following suggestions to create a new venv, the python file still seems to be using python 2.7 anyway. I've tried making a brand new folder and setting up venv for 3.12, but the new python file still is 2.7. Is there another way to resolve this?</p>
","2024-11-05 02:54:13","0","Question"
"79157295","79157138","","<p>You are computing the mean/var values along the wrong dimensions.</p>
<p>Pytorch's layer norm computes mean/var values along the dimensions specified by <code>normalized_shape</code>. From the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"" rel=""nofollow noreferrer"">documentation</a>, the input dimensions are expected to be <code>(*, normalized_shape[0], normalized_shape[1], ...)</code>.</p>
<p>Your code has <code>normalized_shape=(channels,)</code> with an input permuted to have channels as the last dimension, so you should be computing mean/var along that dimension. Instead you compute it along dims <code>(1, 2)</code>.</p>
<p>This is the correct implementation:</p>
<pre class=""lang-py prettyprint-override""><code>def layer_norm(x, weight, bias, eps=1e-5):
    # x shape: [bs, h, w, c]
    # Calculate mean and variance across the spatial dimensions (height, width)
    mean = np.mean(x, axis=-1, keepdims=True)  # shape: (batch_size, 1, 1, channels)
    var = np.var(x, axis=-1, keepdims=True, ddof=0)  # Use ddof=0 for biased variance

    # Normalize
    x_normalized = (x - mean) / np.sqrt(var + eps)

    # Applying weight and bias
    out = weight[None, None, None, :] * x_normalized + bias[None, None, None, :]
    return out
</code></pre>
<p>Note that I also set <code>eps=1e-5</code> which is the pytorch default.</p>
<p>With this, running:</p>
<pre class=""lang-py prettyprint-override""><code>batch, channels, height, width = 4, 3, 8, 8
# Generate random input
x = np.random.randint(-10, 10, (batch, channels, height, width))

# Calculate outputs from both implementations
layernorm1 = test1(x)
layernorm2 = test2(x)

# Check if outputs are close
are_close = np.allclose(layernorm1, layernorm2, atol=1e-6)
</code></pre>
<p>Results in <code>are_close == True</code></p>
","2024-11-04 23:38:22","0","Answer"
"79157258","79156347","","<p>One thing to notice is your function generates redundant outputs. ie <code>get_summing_up_to(30, 30, 8)</code> would contain <code>(30, 0, 0, 0, 0, 0, 0, 0), (0, 30, 0, 0, 0, 0, 0, 0), ...</code>.</p>
<p>One way to make this more efficient is to generate unique integer combinations excluding 0s from 1 to <code>max_length</code>. We can also add caching to the sub-problem of generating partitions for added efficiency.</p>
<pre class=""lang-py prettyprint-override""><code>from functools import lru_cache

def get_summing_up_to_minimal(max_value, target_sum, max_length):
    # optional caching - setting maxsize recommended 
    @lru_cache(maxsize=None)
    def generate_partitions(remaining_sum, max_val, length):
        # Early pruning conditions
        if remaining_sum &lt; 0:
            return []
        if length == 0:
            return [()] if remaining_sum == 0 else []
        
        # Minimum possible sum with given length (using all 1's)
        if remaining_sum &lt; length:
            return []
        
        # Maximum possible sum with given length (using max_val)
        if remaining_sum &gt; max_val * length:
            return []
            
        # Base case for length 1
        if length == 1:
            return [(remaining_sum,)] if remaining_sum &lt;= max_val else []

        results = []
        # Optimize the start value
        start = min(max_val, remaining_sum)
        # Calculate minimum value needed to achieve remaining_sum with remaining length
        min_required = (remaining_sum - 1) // length + 1
        
        # Iterate only through viable values
        for i in range(start, min_required - 1, -1):
            # Early pruning: check if remaining values can sum to target
            remaining_length = length - 1
            remaining_target = remaining_sum - i
            
            # If maximum possible sum with remaining length is too small, break
            if i * remaining_length &lt; remaining_target:
                break
                
            # If minimum possible sum with remaining length is too large, continue
            if remaining_target &lt; remaining_length:
                continue
                
            sub_partitions = generate_partitions(
                remaining_target,
                min(i, max_val),
                remaining_length
            )
            
            for sub_partition in sub_partitions:
                results.append((i,) + sub_partition)
        
        return results

    all_partitions = []
    # Only try lengths that could possibly work
    min_length = (target_sum - 1) // max_value + 1
    max_possible_length = min(max_length, target_sum)
    
    for length in range(min_length, max_possible_length + 1):
        partitions = generate_partitions(target_sum, max_value, length)
        all_partitions.extend(partitions)
    
    return all_partitions
</code></pre>
<p>If the full output with redundant results is required, we can generate them after the fact using the minimal set of outputs from <code>get_summing_up_to_minimal</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import permutations

def expand_partitions(compact_partitions, max_length):
    result = []
    
    for partition in compact_partitions:
        # Calculate how many zeros we need to add
        zeros_needed = max_length - len(partition)
        if zeros_needed &lt; 0:
            continue
            
        # Create the full partition with zeros
        full_partition = partition + (0,) * zeros_needed
        
        # Generate all unique permutations
        # Using a set to handle cases where partition contains duplicate numbers
        result.extend(set(permutations(full_partition)))
    
    return result
</code></pre>
<p>Note that expanding partitions would be the bulk of compute time.</p>
<p>I profiled the following:</p>
<pre class=""lang-py prettyprint-override""><code># run 1, your original code
out = get_summing_up_to(30, 60, 6)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># run 2, generating just minimal outputs
out = get_summing_up_to_minimal(30, 60, 6)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># run 3, generate minimal outputs and expand to full outputs
out = get_summing_up_to_minimal(30, 60, 6)
out = expand_partitions(out, 8)
</code></pre>
<p>On my machine, your original code takes ~4.6 seconds. Generating the minimal outputs takes ~17.9 milliseconds. Generating the minimal outputs and expanding takes ~1.1 seconds.</p>
<p>If your downstream use case doesn't require the redundant combinations, you can save a lot of time just generating the minimal set. If you need to pack the outputs into a numpy array/torch tensor (requiring everything be the same length), you can pad the minimal outputs to <code>max_length</code> with zeros without generating all the combinations. ie:</p>
<pre class=""lang-py prettyprint-override""><code>out = get_summing_up_to_minimal(30, 60, 6)
out_array = np.zeros((len(out), 6))

for i, o in enumerate(out):
    out_array[i][:len(o)] = sorted(o)
</code></pre>
","2024-11-04 23:14:25","1","Answer"
"79157138","","What's the problems in my custom layernorm function?","<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import torch.nn.functional as F

def layer_norm(x, weight, bias, eps=1e-6):
    # x shape: [bs, h, w, c]
    # Calculate mean and variance across the spatial dimensions (height, width)
    mean = np.mean(x, axis=(1, 2), keepdims=True)  # shape: (batch_size, 1, 1, channels)
    var = np.var(x, axis=(1, 2), keepdims=True, ddof=0)  # Use ddof=0 for biased variance

    # Normalize
    x_normalized = (x - mean) / np.sqrt(var + eps)

    # Applying weight and bias
    out = weight[None, None, None, :] * x_normalized + bias[None, None, None, :]
    return out

def test1(x):
    x = np.transpose(x, (0, 2, 3, 1))  # Transpose to [bs, h, w, c]
    weight = np.ones(channels)
    bias = np.zeros(channels)

    normalized_output = layer_norm(x, weight, bias)
    return normalized_output

def test2(x):
    global channels
    x = np.transpose(x, (0, 2, 3, 1))  # Transpose to [bs, h, w, c]
    x_tensor = torch.tensor(x, dtype=torch.float32)
    weight = torch.ones(channels)
    bias = torch.zeros(channels)

    # Use PyTorch's layer norm, normalizing over the last dimension (channels)
    normalized_output = F.layer_norm(x_tensor, normalized_shape=(channels,), weight=weight, bias=bias)
    return normalized_output.detach().numpy()

# Testing
batch, channels, height, width = 4, 3, 8, 8
# Generate random input
x = np.random.randint(-10, 10, (batch, channels, height, width))

# Calculate outputs from both implementations
layernorm1 = test1(x)
layernorm2 = test2(x)

# Check if outputs are close
are_close = np.allclose(layernorm1, layernorm2, atol=1e-4)
print(&quot;Outputs are close:&quot;, are_close)  # Should output True if they are close enough


var = np.var(x, axis=(1, 2), keepdims=True, ddof=0)  # Use ddof=0 for biased variance
var = np.var(x, axis=(1, 2), keepdims=True)
</code></pre>
<p>My expectation is are_close==True,, which means layernorm1 and layernom2 have a very small distance. since the layernorm1 and layernorm2 have a large shape, I would display the partial result of layernorm1 and layernorm2. layernorm1[0,0,0:3,0:4] array([[ 0.35208505, 1.06448374, -0.52827179], [-1.6216472 , -1.7376534 , -1.07653225], [-1.12821414, 0.88935017, 1.84752351]]) layernorm2[0,0,0:3,0:4] array([[ 0.07412489, 1.1859984 , -1.2601235 ], [-1.0690411 , -0.2672601 , 1.336302 ], [-1.3920445 , 0.4800153 , 0.9120291 ]], dtype=float32)</p>
<p>I've tryed the variacne method with or without ddof=0, got all False in the print statement.
I wanna know how to implment a custom layernorm simliar to Pytorch's built-in layernorm function.
what layernorm step from the code perspective ?
what does layernorm do to a feature map abount computer vision ?</p>
","2024-11-04 22:07:24","-1","Question"
"79156914","79156347","","<p>We can frame this as a balls and boxes problem. Slowest step is conversion of generator to numpy array.</p>
<pre><code>def get_summing_up_to2(max_degree, sum, length):
    p = np.array(list(combinations(range(0, sum + length -1), length-1)))

    p[:, 1:] = p[:, 1:] - p[:, 0:-1] - 1
    p[:, 0] = p[:, 0] - 0
    p = np.append(p, (sum - np.sum(p, axis = 1))[:, None], axis = 1)

    p = p[np.max(p, axis = 1) &lt;= max_degree]

    p = p[np.all(np.diff(p) &gt;= 0, axis = 1)]

    return p
</code></pre>
<p>Metrics on my machine:</p>
<pre><code>Original:
Execution time: 6.023083299980499 for max_degree=60, sum=60, length=6
Execution time: 10.017814500024542 for max_degree=30, sum=30, length=8

V2:
Execution time: 3.8685075999237597 for max_degree=60, sum=60, length=6
Execution time: 5.6449807999888435 for max_degree=30, sum=30, length=8
</code></pre>
","2024-11-04 20:30:06","0","Answer"
"79156483","79156347","","<p>To solve this problem more efficiently, you can leverage itertools.combinations_with_replacement to generate combinations without needing recursion. This approach should improve performance by reducing function call overhead and allowing for more efficient filtering.</p>
<pre><code>import numpy as np
import itertools

def generate_combinations(max_degree, target_sum, length):
    result = []
    for comb in itertools.combinations_with_replacement(range(1, max_degree + 1), length):
        if sum(comb) == target_sum:
            result.append(comb)
    return np.array(result)

if __name__ == &quot;__main__&quot;:
    result = generate_combinations(60, 60, 6)
    print(f&quot;Generated {len(result)} combinations for max_degree=60, sum=60, length=6&quot;)
    
    result = generate_combinations(30, 30, 8)
    print(f&quot;Generated {len(result)} combinations for max_degree=30, sum=30, length=8&quot;)
</code></pre>
","2024-11-04 17:51:19","0","Answer"
"79156417","79156347","","<p>If you want to find all combinations you can use itertools. If you want replacement after each selection use <code>combinations_with_replacement</code> and in that case combinations such as (m,m) will be allowed. Otherwise use <code>combination</code>.</p>
<pre><code>import numpy as np
import itertools

e = 10 # the limit
# generate elements to be combined
# so they are from 0 to e
# with 1 spacing in between each
elements = np.linspace(0,e,e+1,dtype=int)
# But this can be `Astring`, (tu,ple), ['List', 'of', 'anything'], ...

r = 2 # combinations of r elements
# Generate all combinations of r elements
comb = itertools.combinations_with_replacement(elements, r)

# print all the combinations as tuples
print(list(comb))
</code></pre>
<p>The print shows</p>
<pre><code>[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 10), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 10), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 10), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 10), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 10), (5, 5), (5, 6), (5, 7), (5, 8), (5, 10), (6, 6), (6, 7), (6, 8), (6, 10), (7, 7), (7, 8), (7, 10), (8, 8), (8, 10), (10, 10)]
</code></pre>
<p>Then you filter the list to keep only combinations that sums up to <code>k</code> like this</p>
<pre><code>k_combs = []

k = 2 # only keeps tuple that sums up to 2
for i,j in enumerate(list(comb)):
    if np.sum(j) == k:
        k_combs.append(j)

print(k_combs)
</code></pre>
<p>Which gives</p>
<p><code>[(0, 2), (1, 1)]</code></p>
","2024-11-04 17:21:54","0","Answer"
"79156347","","Generate array of positive integers that sum of to k","<p>My task is simple: I want to generate an (ideally numpy) array containing all combinations of m positive (&gt;=0), but bounded (&lt;= e) integers that sum exactly to k. Note that k and m might be relatively high, so generating all combinations and filtering will not work.</p>
<p>I have implemented it in plain, recursive python but this small functions takes most of my time and I need to replace it to perform better. I have tried to come up with numpy/pytorch code to generate this array but I didn't manage to do it so far.</p>
<p>I currently use numpy and pytorch in my project, but I am open to other libraries as long as I write python code and I get something I can convert to numpy arrays in the end.</p>
<p>Here's some code:</p>
<pre class=""lang-py prettyprint-override""><code>import timeit


def get_summing_up_to(max_degree, sum, length, current=0):
    assert sum &gt;= 0
    assert length &gt;= 1
    if length == 1:
        residual = sum - current
        if residual &lt;= max_degree:
            return [(residual,)]
        else:
            return []
    max_element = min(max_degree, sum - current)
    return [
        (i,) + t
        for i in range(max_element + 1)
        for t in get_summing_up_to(
            max_degree, sum, length - 1,
            current=current + i
        )
    ]


if __name__ == '__main__':
    result = timeit.timeit('get_summing_up_to(60, 60, 6)', globals=globals(), number=1)
    print(f&quot;Execution time: {result} for max_degree=60, sum=60, length=6&quot;)

    result = timeit.timeit('get_summing_up_to(30, 30, 8)', globals=globals(), number=1)
    print(f&quot;Execution time: {result} for max_degree=30, sum=30, length=8&quot;)
</code></pre>
","2024-11-04 16:54:55","1","Question"
"79153983","79153372","","<p>Check the bottom of the documentation page you linked where it says:</p>
<pre><code>torch.max(input, other, *, out=None) → Tensor

See torch.maximum().
</code></pre>
<p>When the second argument is a tensor, <code>torch.max</code> computes <a href=""https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum"" rel=""nofollow noreferrer"">torch.maximum</a></p>
","2024-11-04 01:42:32","4","Answer"
"79153372","","How/Where does PyTorch max documentation show that you can pass in 2 tensors for comparison?","<p>I am learning <code>pytorch</code> and deep learning. The documentation for <a href=""https://pytorch.org/docs/stable/generated/torch.max.html"" rel=""nofollow noreferrer""><code>torch.max</code></a> doesn't make sense in that it looks like we can compare 2 tensors but I don't see where in the documentation I could have determined this.</p>
<p>I had this code at first where I wanted to check ReLU values against the maximum. I thought that <code>0</code> could be broadcast for and <code>h1.shape=torch.Size([10000, 128])</code>.</p>
<pre><code>h1 = torch.max(h1, 0)
y = h1 @ W2 + b2
</code></pre>
<p>However, I got this error:</p>
<p><code>TypeError: unsupported operand type(s) for @: 'torch.return_types.max' and 'Tensor'</code></p>
<p>I got to fix this when I changed the <code>max</code> equation to use a tensor instead of 0.</p>
<pre><code>h1 = torch.max(h1, torch.tensor(0))
y = h1 @ W2 + b2
</code></pre>
<p><strong>1. Why does this fix the error?</strong></p>
<p>This is when I checked the documentation again and realized that there is nothing mentions a collection like a tuple or list for multiple tensors or even a <code>*input</code> for iterable unpacking.</p>
<p>Here are the 2 versions:</p>
<p>1st <code>torch.max</code> version:</p>
<blockquote>
<p>torch.max(input) → Tensor Returns the maximum value of all elements in
the input tensor.</p>
<p>Warning</p>
<p>This function produces deterministic (sub)gradients unlike max(dim=0)</p>
</blockquote>
<p>2nd version of <code>torch.max</code></p>
<blockquote>
<p>torch.max(input, dim, keepdim=False, *, out=None) Returns a namedtuple
(values, indices) where values is the maximum value of each row of the
input tensor in the given dimension dim. And indices is the index
location of each maximum value found (argmax).</p>
<p>If keepdim is True, the output tensors are of the same size as input
except in the dimension dim where they are of size 1. Otherwise, dim
is squeezed (see torch.squeeze()), resulting in the output tensors
having 1 fewer dimension than input.</p>
</blockquote>
<p><strong>2. What is <code>tensor(0)</code> according to this documentation?</strong></p>
","2024-11-03 18:25:02","1","Question"
"79151558","79151487","","<p>I tried to repro. All I ran into was the missing dependency on Boost Graph:</p>
<ul>
<li><p>File <code>CMakeLists.txt</code></p>
<pre><code>cmake_minimum_required(VERSION 3.19)
project(sotest)
#set(CMAKE_CXX_COMPILER_LAUNCHER ccache)
set(CMAKE_EXPORT_COMPILE_COMMANDS On)
set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -ggdb &quot;)
set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -O2 -march=native &quot;)
set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -Wall -Wextra -pedantic&quot;)

find_package(Torch REQUIRED)
link_libraries(&quot;${TORCH_LIBRARIES}&quot;)

find_package(Threads REQUIRED)
link_libraries(Threads::Threads)

find_package(Boost CONFIG 1.64.0 COMPONENTS python graph)
link_libraries(Boost::python Boost::graph)

find_package(Python3 COMPONENTS Development)
link_libraries(Python3::Python)

add_executable(sotest test.cpp main.cpp)
add_library(sopytorch SHARED test.cpp)

#target_include_directories(sotest PUBLIC ${Python3_INCLUDE_DIRS})
</code></pre>
</li>
<li><p>File <code>test.h</code></p>
<pre><code>#include &lt;string&gt;

class Test{

    int tensorSize;
    std::string label;

    public:
        Test(int i, std::string label);
    void printTensor();
};
</code></pre>
</li>
<li><p>File <code>test.cpp</code></p>
<pre><code>#include &quot;test.h&quot;
#include &lt;boost/python.hpp&gt;
#include &lt;string&gt;
#include &lt;torch/script.h&gt;
#include &lt;torch/torch.h&gt;

#include &lt;iostream&gt;

Test::Test(int i, std::string lbl) {
    tensorSize = i;
    label = lbl;
}

void Test::printTensor(){
    auto x = torch::randn({10, tensorSize});

    std::cout &lt;&lt; label &lt;&lt; '\n';
    std::cout &lt;&lt; &quot;x:\n&quot; &lt;&lt; x &lt;&lt; '\n';
}


using namespace boost::python;
BOOST_PYTHON_MODULE(test)
{
    class_&lt;Test&gt;(&quot;Test&quot;, init&lt;int, std::string&gt;())
    .def(&quot;printTensor&quot;, &amp;Test::printTensor)
    ;
}

</code></pre>
</li>
<li><p>File <code>main.cpp</code></p>
<pre><code>#include &quot;test.h&quot;

int main() {
    Test t(3, &quot;test&quot;);
    t.printTensor();
}
</code></pre>
</li>
<li><p>File <code>test.py</code></p>
<pre><code>import test

t = test.Test(3, &quot;test&quot;)
t.printTensor()
</code></pre>
</li>
</ul>
<p><a href=""https://i.imgur.com/9OR9HR2.mp4"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/s2dzwkL.gif""/></a></p>
<h2>PS</h2>
<p>Oh yeah, I forgot to mention a step, but I assume you know you need to copy the shared library so python can find it, e.g. with</p>
<pre><code>ln -sfv build/libsopytorch.so test.so
</code></pre>
","2024-11-02 20:45:34","0","Answer"
"79151487","","Use Boost python with libtorch","<p>I am trying to create a python package from C++ code with Boost python. However when including <code>libtorch</code>in the code, the resulting python package shows strange errors such as</p>
<blockquote>
<p>Boost.Python.ArgumentError: Python argument types in
Test.<strong>init</strong>(Test, int, str)
did not match C++ signature:
<strong>init</strong>(_object*, int, std::string)&quot;</p>
</blockquote>
<p>Creating a C++ executable from the code works fine and creating python packages with Boost python without <code>libtorch</code> dependencies works also fine, so the problem seems to come from <code>libtorch</code>and <code>boost python</code>not working properly together. Has anybody encountered the same and found a solutoin to this?</p>
<p>Here is my example:</p>
<p>test.h</p>
<pre><code>#ifndef TEST_H
#define TEST_H

#include &lt;string&gt;

class Test{

    int tensorSize;
    std::string label;

    public:
        Test(int i, std::string label);
    void printTensor();
};

#endif
</code></pre>
<p>test.cpp</p>
<pre><code>#include &lt;boost/python.hpp&gt;
#include &lt;string&gt;
#include &quot;test.h&quot;
#include &lt;torch/torch.h&gt;
#include &lt;torch/script.h&gt;


#include &lt;iostream&gt;

Test::Test(int i, std::string lbl) {
    tensorSize = i;
    label = lbl;
}

void Test::printTensor(){
    auto x = torch::randn({10, tensorSize});

    std::cout &lt;&lt; label &lt;&lt; '\n';
    std::cout &lt;&lt; &quot;x:\n&quot; &lt;&lt; x &lt;&lt; '\n';
}


using namespace boost::python;
BOOST_PYTHON_MODULE(test)
{
    class_&lt;Test&gt;(&quot;Test&quot;, init&lt;int, std::string&gt;())
    .def(&quot;printTensor&quot;, &amp;Test::printTensor)
    ;
}
</code></pre>
<p>main.cpp (for the executable)</p>
<pre><code>#include &quot;test.h&quot;

int main() {
    Test t(3, &quot;test&quot;);
    t.printTensor();
    return 0;
}
</code></pre>
<p>and the test file for testing the python package</p>
<pre><code>import test

t = test.Test(3, &quot;test&quot;)
t.printTensor()
</code></pre>
<p>for compiling I use CMAKE, here the CMakeLists.txt</p>
<pre><code>cmake_minimum_required(VERSION 3.20)
project(boostTest)

set(BOOST_MIN_VERSION 1.86.0)
find_package(Torch REQUIRED)
find_package(PythonLibs 3 REQUIRED)

find_package(
    Boost ${BOOST_MIN_VERSION} REQUIRED
    COMPONENTS python REQUIRED
)
#C++ executable
include_directories(${PYTHON_INCLUDE_PATH})
add_executable(test.out main.cpp test.cpp  test.h)
target_link_libraries(test.out ${PYTHON_LIBRARIES} ${TORCH_LIBRARIES} Boost::python)

#boost python
set(CMAKE_SHARED_MODULE_PREFIX &quot;&quot;)
add_library(test MODULE test.cpp)
target_link_libraries(test ${Boost_LIBRARIES} ${PYTHON_LIBRARIES} ${TORCH_LIBRARIES} )
target_include_directories(test PRIVATE ${PYTHON_INCLUDE_DIRS})
</code></pre>
<p>I compiled the example above. The resulting executable works fine, the python package throws an error.</p>
","2024-11-02 19:57:19","1","Question"
"79150211","79149427","","<p>This issue mainly results from how class labels are encoded and loaded during training. Ensure that the labels are encoded and loaded the same way as you did during training so that the indexing is not messed up, that is if the classes are sorted during training make sure they are also sorted during inference. I hope this helps.</p>
","2024-11-02 08:04:28","1","Answer"
"79149427","","Real Time resnet prediction","<p>I trained a resnet50 model on number hand signs from 0 to 5, and Im trying to deploy it to predict real time classes via the laptop's webcam.</p>
<p>although the model has 98% accuracy and I am pretty sure the error's not occurring because the model was trained badly, the real time values are stuck to 1 or 2 classes of the 5, they always predict number 0 and number 2.</p>
<p>this is the code :</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import cv2
import numpy as np
from torchvision import models, transforms
from PIL import Image  # Import PIL for image conversion

# Define the model architecture and load weights
class ResNet50Modified(nn.Module):
    def __init__(self, num_classes=6):
        super(ResNet50Modified, self).__init__()
        self.model = models.resnet50(pretrained=True)  # Use pretrained=True for better performance
        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)

    def forward(self, X):
        return self.model(X)

# Load the trained model
model = ResNet50Modified(num_classes=6)
# Load the model's state_dict for CPU
model.load_state_dict(torch.load(&quot;resnet50_modified1.pth&quot;, map_location=torch.device('cpu')))
model.eval()

# Define transformations to match training preprocessing
preprocess = transforms.Compose([
    transforms.Resize((64, 64)),  # Resize to input size of model
    transforms.ToTensor(),  # Convert to tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as per ResNet standards
])

# Labels for the signs
class_names = ['Class_0', 'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5']  # Replace with actual sign names

# Open webcam for real-time prediction
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print(&quot;Error: Could not open webcam.&quot;)
    exit()

while True:
    ret, frame = cap.read()
    if not ret:
        print(&quot;Error: Could not read frame.&quot;)
        break

    # Convert the frame from BGR (OpenCV) to RGB (PIL)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Convert NumPy array to PIL Image
    pil_image = Image.fromarray(frame_rgb)

    # Preprocess the frame
    input_image = preprocess(pil_image)  # Use the PIL image for preprocessing
    input_image = input_image.unsqueeze(0)  # Add batch dimension

    # Predict using the model
    with torch.no_grad():
        outputs = model(input_image)
        
        # Apply softmax to get probabilities
        probabilities = torch.softmax(outputs, dim=1)
        
        # Get the predicted class and confidence
        _, predicted = torch.max(probabilities, 1)
        confidence = probabilities[0][predicted].item() * 100  # Convert to percentage
        label = class_names[predicted.item()]

    # Display the result with confidence level
    cv2.putText(frame, f&quot;Predicted: {label}, Confidence: {confidence:.2f}%&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow(&quot;Sign Detection&quot;, frame)

    # Exit on pressing 'q'
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
</code></pre>
<p>the confidence levels are always high but with the wrong labels, even if I show on the webcam 5 fingers it's stuck on zero.</p>
<p>I feel like the problem is with the frame processing, does anyone have any insight on this?</p>
","2024-11-01 21:06:43","0","Question"
"79148196","79147116","","<p><strong>Data Splitting</strong></p>
<p>RandomLinkSplit is designed for link prediction tasks and can introduce complexities like negative sampling. RandomLinkSplit works by splitting edges into a subset for training and a subset for validation and finaly one for testing. It optionally has negative sampling. RandomSplitLink adjusts the edge_index in each split to include the edges assigned to the split, however, it does not handle edge_attr or custom _edge_label, if split_label=True, it will split labels accordingly. Try manually splitting the data into training, validation and test sets to ensure you split edge_index, edge_attr, and edge_label.</p>
<p>Example:</p>
<p>Manually split the components.</p>
<pre><code># Generate edge indices for splitting
num_edges = pyGData.edge_index.size(1)
edge_indices = torch.arange(num_edges)

# Convert tensors to numpy for sklearn
edge_indices_np = edge_indices.cpu().numpy()
edge_label_np = pyGData.edge_label.cpu().numpy()

# First split: Train vs Temp (Val + Test)
train_idx, temp_idx, y_train, y_temp = train_test_split(
    edge_indices_np,
    edge_label_np,
    test_size=0.4,  # 60% train, 40% temp
    random_state=42,
    stratify=edge_label_np
)

# Second split: Validation vs Test
val_size = 0.5  # Since temp is 40%, val and test each get 20%
val_idx, test_idx, y_val, y_test = train_test_split(
    temp_idx,
    y_temp,
    test_size=0.5,
    random_state=42,
    stratify=y_temp
)

# Convert back to tensors
train_idx = torch.tensor(train_idx, dtype=torch.long)
val_idx = torch.tensor(val_idx, dtype=torch.long)
test_idx = torch.tensor(test_idx, dtype=torch.long)
</code></pre>
<p>Split the edge_index, edge_attr and edge_label with obtained indices from above:</p>
<pre><code>train_data = Data(
    num_nodes=num_nodes,
    edge_index=pyGData.edge_index[:, train_idx],
    edge_attr=pyGData.edge_attr[train_idx],
    edge_label=pyGData.edge_label[train_idx]
).to(device)

val_data = Data(
    num_nodes=num_nodes,
    edge_index=pyGData.edge_index[:, val_idx],
    edge_attr=pyGData.edge_attr[val_idx],
    edge_label=pyGData.edge_label[val_idx]
).to(device)

test_data = Data(
    num_nodes=num_nodes,
    edge_index=pyGData.edge_index[:, test_idx],
    edge_attr=pyGData.edge_attr[test_idx],
    edge_label=pyGData.edge_label[test_idx]
).to(device)
</code></pre>
<p>Even if you disable negative sampling (add_negative_train_samples=False, etc.), RandomLinkSplit inherently removes edges from the training set and assigns them to validaiton and test sets.</p>
<p>RandomLinkSplit can still be used but manually splitting edge_attr to match would be required so manually is preferred.</p>
<p><strong>Re-Indexing</strong></p>
<p>In the case of re-indexing the nodes the problem is likely that the node indices in edge_index may refer to nodes that are not present in the train, validation, or test sets after the split.
To fix:
-Map old indices to new, contiguous node indices for each dataset (train, validaiton and test). Apply this mapping.</p>
<p>Example:</p>
<pre><code>def reindex_nodes(data):
    unique_nodes = torch.unique(data.edge_index.flatten())
    # Mapping from old node indices to new ones
    node_mapping = {node.item(): i for i, node in enumerate(unique_nodes)}

    # Apply mapping to edge indices
    row, col = data.edge_index
    mapped_row = row.apply_(lambda x: node_mapping[x])
    mapped_col = col.apply_(lambda x: node_mapping[x])
    data.edge_index = torch.stack([mapped_row, mapped_col], dim=0)

    data.num_nodes = unique_nodes.size(0)

    return data

# Apply the function to each dataset
train_data = reindex_nodes(train_data)
val_data = reindex_nodes(val_data)
test_data = reindex_nodes(test_data)
</code></pre>
","2024-11-01 13:41:35","0","Answer"
"79147116","","Pytorch Geometric_Random_Link_Split error","<p>I'm working on a class project. We are trying to use Pytorch Geometric with GNN to detect money laundering in a IBM dataset on Kaggle. What we did was to create a Pytorch Geometric Data object with edge_index, edge_attr, and edge_label, then to split the data object into train, val, and test dataset. However, it seems that the dataset is not properly split. For example, the edge_label is plit, but the edge_index and edge_attr are not.</p>
<p>Additionally, another issue I have is that it keeps saying that the edge indexes are out of bound during training or validation. Sometimes, it works but not consistently. I'm not sure this is caused by the split. I try to create some manual random split as well, but the indexing issue still happens.</p>
<p>Please help! Thank you!</p>
<pre><code>`# Convert to PyTorch tensors with correct dtypes
torch_edges_labels = torch.tensor(df.select(&quot;Is Laundering&quot;).collect(), dtype=torch.long).to(device)  # Changed to torch.long
torch_edges = torch.tensor(edges.collect(), dtype=torch.long).t().contiguous().to(device)
torch_edges_features = torch.tensor(df_features.collect(), dtype=torch.float).to(device)

# # Create PyTorch Geometric Data object
pyGData = Data(
    num_nodes=nodes.count(),
    edge_index=torch_edges,
    edge_attr=torch_edges_features,
    edge_label=torch_edges_labels
).to(device)

# Initialize split transform with proper settings
split = transforms.RandomLinkSplit(
    is_undirected=True,
    split_labels=False,
    num_val=0.2,
    num_test=0.2,
)

# Perform split with proper error handling
try:
    train_data, val_data, test_data = split(pyGData)`
</code></pre>
<p>edge_index, edge_attr, and edge_label in each train, val, and test dataset should be split properly.
Additionaly, this might help with the issue that the edge index is out of bound during training and evaluation.</p>
","2024-11-01 06:26:54","2","Question"
"79147015","79140091","","<p>The model can be trained and inferenced successfully now:
Set safe_serialization to False in model training file tutorial_train_plus.py:</p>
<pre><code>accelerator.save_state(save_path, safe_serialization=False)
</code></pre>
<p>It will generate pytorch_model.bin instead of model.safetensors during training.</p>
<p>Once training is complete, modify the model conversion code as below based on the original instructions in readme:</p>
<pre><code>ckpt = &quot;pytorch_model.bin&quot; # set correct path
sd = torch.load(ckpt)
</code></pre>
<p>Model file ip_adapter.bin will be generated for inference.</p>
","2024-11-01 05:18:24","0","Answer"
"79146829","79143505","","<p>By using the method provided by @trialNerror, slicing performance improved by about 8 times on my machine compared to the for loop.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import random
import time 
B = 1024
N = 40
D = 512
src_tensor = torch.randn((B,N,D))
indices = [random.randint(0,N-1-1) for _ in range(B)]
t1 = torch.empty((B,2,D))
t2 = torch.empty((B,2,D))
start = time.perf_counter()
t1[:,0,:] = src_tensor[range(B),indices]
t1[:,1,:] = src_tensor[:,1:,:][range(B),indices]
end1 = time.perf_counter()
for i in range(B):
    p1,p2 = indices[i],indices[i]+1
    t2[i,:,:]=src_tensor[i,[p1,p2],:]
end2 = time.perf_counter()

print(f't1 == t2 ? :{t1.equal(t2)}')
print(f't1: {end1-start}')
print(f't2: {end2-end1}')
print(f't2/t1: {(end2-end1)/(end1-start)}')

# t1 == t2 ? :True
# t1: 0.002891700016334653
# t2: 0.023338499944657087
# t2/t1: 8.070857908089506
</code></pre>
","2024-11-01 03:08:42","0","Answer"
"79144479","79143505","","<p>Since your indexing along the second dimension depends on the batch index, I think you need to leverage some weird more complex indexing schemes :</p>
<pre><code>import torch
# arbitrary source tensor
src_tensor = torch.Tensor(range(30)).view((3,5,2))
B, N, D = src_tensor.shape
# arbitrary, non consecutive indices made into an index tensor for torch.gather
ids = [0, 1, 3]
index = torch.tensor(ids)
# slicing
sliced = torch.empty(B, 2, D) #empty instead of zero to avoid writing useless values
sliced[:, 0, :] = src_tensor[range(B), ids]
sliced[:, 1, :] = src_tensor[:,1:,:][range(B), ids]
</code></pre>
<p>The  first slicing on the last line serves to offset the tensor by 1 in the second dimension, which makes it possible to access the <code>index+1</code> values without doing any addition or copying.</p>
<p>Note that if you know for sure that your indices are all consecutives (like in your example), then using <code>narrow</code> instead of slicing will be even better. Alternatively, for some even more complex indexing schemes, you can also try <a href=""https://pytorch.org/docs/stable/generated/torch.gather.html"" rel=""nofollow noreferrer"">torch.gather</a></p>
<p>Note : as far as I'm aware, this slicing scheme is consistent with numpy's, so most of the documentation you can find for numpy slicing applies to torch</p>
","2024-10-31 11:11:41","1","Answer"
"79143505","","How to simplify 3D tensor slicing","<p>I want to slice a 3D tensor in PyTorch. The shape of the 3D tensor src_tensor is (batch, max_len, hidden_dim), and I have a 1D index vector indices with the shape (batch,). I want to slice along the second dimension of src_tensor. I can achieve this functionality with the following codes:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
nums = 30
l = [i for i in range(nums)]
src_tensor = torch.Tensor(l).reshape((3,5,2))
indices = [1,2,3]
slice_tensor = torch.zeros((3,2,2)) 
for i in range(3):
    p1,p2 = indices[i],indices[i]+1
    slice_tensor[i,:,:]=src_tensor[i,[p1,p2],:]
print(src_tensor)
print(indices)
print(slice_tensor)
&quot;&quot;&quot;
tensor([[[ 0.,  1.],
         [ 2.,  3.],
         [ 4.,  5.],
         [ 6.,  7.],
         [ 8.,  9.]],

        [[10., 11.],
         [12., 13.],
         [14., 15.],
         [16., 17.],
         [18., 19.]],

        [[20., 21.],
         [22., 23.],
         [24., 25.],
         [26., 27.],
         [28., 29.]]])
[1, 2, 3]
tensor([[[ 2.,  3.],
         [ 4.,  5.]],

        [[14., 15.],
         [16., 17.]],

        [[26., 27.],
         [28., 29.]]])
&quot;&quot;&quot;
</code></pre>
<p>My question is whether the above code can be simplified, for example, by eliminating the for loop.</p>
","2024-10-31 04:29:47","1","Question"
"79143298","79141651","","<p>Well, I never tried before this, I was just patiently waiting for cached instance to load lol
But I did find something from googling, try using redis. It is a python library that allows you to store things in RAM and retrieve them. I never tried it myself but here is some simple code to run it, like:</p>
<pre><code>import redis # pip install redis
df = pandas.read_csv('dataset.csv')
r = redis.Redis(host=&quot;localhost&quot;, port=8888, db=0) # start a localhost server on this port 
for index, row in df.iterrows():
    r.hmset(f&quot;row:{index}&quot;, row_to_dict()) #load data in RAM
</code></pre>
<p>Then in another program:</p>
<pre><code>data = r.hgetall(&quot;row:0&quot;) #OR you can try iterating through the data
</code></pre>
<p>There are also another implementation using multiprocessing shared_memory thing. I copied this one from gpt, but legit to me.</p>
<pre><code>import numpy as np
from multiprocessing import shared_memory

# Create some data (example: an array)
data = np.array([1, 2, 3, 4, 5], dtype=np.int32)

# Create shared memory block
shm = shared_memory.SharedMemory(create=True, size=data.nbytes)

# Create a NumPy array using the shared memory buffer
shared_array = np.ndarray(data.shape, dtype=data.dtype, buffer=shm.buf)

# Copy data to shared memory
np.copyto(shared_array, data)

print(&quot;Shared memory block created with name:&quot;, shm.name)
print(&quot;Data written to shared memory:&quot;, shared_array)

# Remember to close the shared memory if it's no longer needed in this process

</code></pre>
<p>And in another program:</p>
<pre><code>import numpy as np
from multiprocessing import shared_memory

# Replace 'your_shared_memory_name' with the name printed by the first script
shm_name = &quot;your_shared_memory_name&quot;  # Use the name from the first script

# Access the existing shared memory block
existing_shm = shared_memory.SharedMemory(name=shm_name)

# Create a NumPy array using the shared memory buffer
shared_array = np.ndarray((5,), dtype=np.int32, buffer=existing_shm.buf)

# Read the data from shared memory
print(&quot;Data read from shared memory:&quot;, shared_array)

# Clean up
existing_shm.close()
</code></pre>
<p>Again, this COULD solve your problem. Try and let me know :D</p>
","2024-10-31 02:13:10","0","Answer"
"79141651","","Avoid reloading Pytorch datasets","<p>I train CNNs on a relatively stable combination of datasets, but every time I start a training job, there's a 5-10 min wait for the trainer to load my dataframes from disk.  Is it possible to avoid this step and load the data only once and have to available for multiple processes to access?</p>
<p>The assumptions is that I may start and tear down multiple training jobs, tinker with training parameters, topologies, but the data will be the same.</p>
<p>I feel a client-server solution may be relevant, where one process loads the datasets once and then services requests from multiple training clients.  Has this been done before?  Is there a readymade framework that already implements this?</p>
","2024-10-30 14:54:52","0","Question"
"79141319","79127383","","<p>I don't know of a specific update since alst sunday, but the versions specified in the requirements.txt (scikit==1.2) and also directly in the cell (torch==1.6) are not compatible with python 3.10, which is in colab for quite some time.</p>
<p>The best approach is to use a python version that supports all versions required in this demo. You could try to update the package versions required in requirements.txt to a higher version compatible with python 3.10, but then you have no guarantee that the module you are trying to use still works.</p>
<p>To achieve the downgrade of python, add a new cell to the very top containing</p>
<pre><code>!sudo apt-get install python3.7
!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1
!sudo apt install python3.7-distutils
!curl https://bootstrap.pypa.io/pip/3.7/get-pip.py | sudo python3.7
</code></pre>
","2024-10-30 13:32:45","-1","Answer"
"79141016","79140241","","<p>torch.nn.Module overrides <code>__setattr__</code> and <code>__getattr__</code></p>
<p>When the attribute is set to a Module object, it gets added to the dict <code>self.__dict__['_modules']</code> by the overridden <code>__setattr__</code> but the base <code>__setattr__</code> is not called to assign it to base object. But when you call getattr or reference the attribute using the syntax <code>foo.bar</code>, <code>__getattr__</code> is not called because the attribute is present, even if it is none or deleted before. Therefore, getattr retrieves the original item assigned to the object rather than calling the <code>__getattr__</code>, which is overriden, to retrieve the value assigned to _modules. This is not limited to None. You can assign it to other values like an integer 1 and <code>self.txt_encoder</code> would still print 1 after doing <code>self.txt_encoder = module</code></p>
<p>A solution is to call the base <code>__setattr__</code> to assign the value to the object itself as well. For the example given in the question, we have</p>
<pre class=""lang-py prettyprint-override""><code>        self.tokenizer, self.txt_encoder = tokenizer, txt_encoder
        object.__setattr__(self,'txt_encoder',txt_encoder)
        print(type(self.tokenizer).__name__, type(self.txt_encoder).__name__, type(txt_encoder).__name__)
</code></pre>
<p>And now it should print out <code>CLIPTokenizerFast CLIPTextModelWithProjection CLIPTextModelWithProjection</code></p>
<p>An alternative solution is to not assign None and make sure to assign it in <strong>init</strong>.
So, rather than</p>
<pre class=""lang-py prettyprint-override""><code>txt_encoder: Optional[CLIPTextModelWithProjection] = None
</code></pre>
<p>We instead have</p>
<pre class=""lang-py prettyprint-override""><code>txt_encoder: CLIPTextModelWithProjection
</code></pre>
<p>This will ensure that the overridden <code>__getattr__</code> is called.</p>
","2024-10-30 11:58:39","0","Answer"
"79140241","","torch Module causes model field variable to become None on reference right after assignment","<p>How is it possible that a field variable becomes None right after the line of assignment?</p>
<p>In particular, the following code prints
<code>CLIPTokenizerFast NoneType CLIPTextModelWithProjection</code>
when _setup_txt_encoder is being run</p>
<p>self.txt_encoder should be the same variable as txt_encoder but when being accessed, it is retreieved as a NoneType</p>
<p>It appears that inheriting from torch.nn.Module is causing the problem because if I remove it from the class inheritance, there is no problem.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast, CLIPTextModelWithProjection
from typing import Optional
from torch.nn import Module

class Foo:
    def _setup_txt_encoder(self, clip_txt_model_name: str):
        print('loading text tokenizer and encoder')
        tokenizer = AutoTokenizer.from_pretrained(clip_txt_model_name, clean_up_tokenization_spaces=True)
        txt_encoder = CLIPTextModelWithProjection.from_pretrained(clip_txt_model_name).requires_grad_(False)
        self.tokenizer, self.txt_encoder = tokenizer, txt_encoder
        self.txt_encoder = txt_encoder
        print(type(self.tokenizer).__name__, type(self.txt_encoder).__name__, type(txt_encoder).__name__)
        return tokenizer, txt_encoder
    tokenizer: Optional[PreTrainedTokenizer|PreTrainedTokenizerFast] = None
    txt_encoder: Optional[CLIPTextModelWithProjection] = None
class Bar(Module, Foo):
    def __init__(self, clip_txt_model_name='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'):
        super().__init__()
        if self.tokenizer is None or self.txt_encoder is None:
            self._setup_txt_encoder(clip_txt_model_name)

test = Bar()
</code></pre>
<p>EDIT: some more information. It's really weird the following returns different things</p>
<pre class=""lang-py prettyprint-override""><code>print(getattr(test,'txt_encoder'))
print(test.__getattr__('txt_encoder'))
</code></pre>
<p>outputs</p>
<pre><code>None
CLIPTextModelWithProjection(
  (text_model): CLIPTextTransformer(
...
</code></pre>
","2024-10-30 08:31:03","0","Question"
"79140091","","Inference error after training an IP-Adapter plus model","<p>I downloaded packages from <a href=""https://github.com/tencent-ailab/IP-Adapter"" rel=""nofollow noreferrer"">https://github.com/tencent-ailab/IP-Adapter</a></p>
<p>run the commands to train an IP-Adapter plus model (input: text + image, output: image):</p>
<pre><code>accelerate launch --num_processes 2 --multi_gpu --mixed_precision &quot;fp16&quot; \
  tutorial_train_plus.py \
  --pretrained_model_name_or_path=&quot;stable-diffusion-v1-5/&quot; \
  --image_encoder_path=&quot;models/image_encoder/&quot; \
  --data_json_file=&quot;assets/prompt_image.json&quot; \
  --data_root_path=&quot;assets/train/&quot; \
  --mixed_precision=&quot;fp16&quot; \
  --resolution=512 \
  --train_batch_size=2 \
  --dataloader_num_workers=4 \
  --learning_rate=1e-04 \
  --weight_decay=0.01 \
  --output_dir=&quot;out_model/&quot; \
  --save_steps=3
</code></pre>
<p>During training, there is the message but the training can be continued:</p>
<pre><code>Removed shared tensor {'adapter_modules.27.to_k_ip.weight', 'adapter_modules.1.to_v_ip.weight', 'adapter_modules.31.to_k_ip.weight', 'adapter_modules.15.to_k_ip.weight', 'adapter_modules.31.to_v_ip.weight', 'adapter_modules.11.to_k_ip.weight', 'adapter_modules.23.to_k_ip.weight', 'adapter_modules.3.to_k_ip.weight', 'adapter_modules.25.to_v_ip.weight', 'adapter_modules.21.to_k_ip.weight', 'adapter_modules.17.to_v_ip.weight', 'adapter_modules.13.to_k_ip.weight', 'adapter_modules.17.to_k_ip.weight', 'adapter_modules.19.to_v_ip.weight', 'adapter_modules.13.to_v_ip.weight', 'adapter_modules.7.to_v_ip.weight', 'adapter_modules.7.to_k_ip.weight', 'adapter_modules.29.to_k_ip.weight', 'adapter_modules.3.to_v_ip.weight', 'adapter_modules.5.to_v_ip.weight', 'adapter_modules.21.to_v_ip.weight', 'adapter_modules.5.to_k_ip.weight', 'adapter_modules.23.to_v_ip.weight', 'adapter_modules.25.to_k_ip.weight', 'adapter_modules.1.to_k_ip.weight', 'adapter_modules.9.to_v_ip.weight', 'adapter_modules.9.to_k_ip.weight', 'adapter_modules.15.to_v_ip.weight', 'adapter_modules.27.to_v_ip.weight', 'adapter_modules.29.to_v_ip.weight', 'adapter_modules.19.to_k_ip.weight', 'adapter_modules.11.to_v_ip.weight'} while saving. This should be OK, but check by verifying that you don't receive anywarning while reloading
</code></pre>
<p>After training is finished and convert the weight to generate ip_adapter.bin, then run the inference code ip_adapter-plus_demo.py with the following model paths in this file:</p>
<pre><code>base_model_path = &quot;SG161222/Realistic_Vision_V4.0_noVAE&quot;
vae_model_path = &quot;stabilityai/sd-vae-ft-mse&quot;
image_encoder_path = &quot;models/image_encoder&quot;
ip_ckpt = &quot;out_model/demo_plus_checkpoint/ip_adapter.bin&quot;
</code></pre>
<p>It shows the error:</p>
<pre><code>raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
        Missing key(s) in state_dict: &quot;1.to_k_ip.weight&quot;, &quot;1.to_v_ip.weight&quot;, &quot;3.to_k_ip.weight&quot;, &quot;3.to_v_ip.weight&quot;, &quot;5.to_k_ip.weight&quot;, &quot;5.to_v_ip.weight&quot;, &quot;7.to_k_ip.weight&quot;, &quot;7.to_v_ip.weight&quot;, &quot;9.to_k_ip.weight&quot;, &quot;9.to_v_ip.weight&quot;, &quot;11.to_k_ip.weight&quot;, &quot;11.to_v_ip.weight&quot;, &quot;13.to_k_ip.weight&quot;, &quot;13.to_v_ip.weight&quot;, &quot;15.to_k_ip.weight&quot;, &quot;15.to_v_ip.weight&quot;, &quot;17.to_k_ip.weight&quot;, &quot;17.to_v_ip.weight&quot;, &quot;19.to_k_ip.weight&quot;, &quot;19.to_v_ip.weight&quot;, &quot;21.to_k_ip.weight&quot;, &quot;21.to_v_ip.weight&quot;, &quot;23.to_k_ip.weight&quot;, &quot;23.to_v_ip.weight&quot;, &quot;25.to_k_ip.weight&quot;, &quot;25.to_v_ip.weight&quot;, &quot;27.to_k_ip.weight&quot;, &quot;27.to_v_ip.weight&quot;, &quot;29.to_k_ip.weight&quot;, &quot;29.to_v_ip.weight&quot;, &quot;31.to_k_ip.weight&quot;, &quot;31.to_v_ip.weight&quot;.
</code></pre>
<p>Any step wrong to cause this error?</p>
","2024-10-30 07:32:22","0","Question"
"79137644","79135639","","<p>The issue you are going to run into is: because the compression function Z is not implemented in Torch, you can not rely on Torch's autograd methods to calculate the gradients.</p>
<p>This will leave you having to implement the <code>forward</code> and <code>backward</code> methods on your own.  For the <code>backward</code> method, you will need to derive a gradient function, which is probably not possible in a closed form solution for compression algorithms like LZA, LZ1, LZ2, etc.</p>
<p>This leaves computing the gradient numerically using the definition of the derivative: <code>f(x+δ) - f(x-δ) / 2δ</code>, where <code>δ</code> is small.</p>
<p>Pytorch has an example of implementing a custom loss function with it's own gradient (no numerically computed) here: <a href=""https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html</a></p>
<p>This is an example of implementing a custom loss function with calls to functions/methods outside of Pytorch that has a numerically calculated gradient.  Keep in mind, this is a very rudimentary implementation with a simple for-loop, there are probably better/faster implementations.</p>
<pre><code>def normalized_compressive_dist(x: torch.Tensor, y: torch.Tensor):
    xy = len(gzip.compress(np.concatenate([
        x.detach().numpy().ravel(), 
        y.detach().numpy().ravel()
    ])))
    x = len(gzip.compress(x.detach().numpy().ravel()))
    y = len(gzip.compress(y.detach().numpy().ravel()))

    loss = (xy - min(x,y))/ (max(x,y))
    return torch.tensor(loss)


class NCDLoss(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x_input, y_input):
        ctx.save_for_backward(x_input, y_input)
        loss = normalized_compressive_dist(x_input, y_input)
        return loss

    @staticmethod
    def backward(ctx, grad_output):
        x_input, y_input= ctx.saved_tensors
        delta = 1e-5
        grad_input = torch.zeros_like(y_input)

        # compute perturbation for each element in the tensor
        for i in range(x_input.numel()):
            input_flat = x_input.view(-1)
            original_value = input_flat[i]

            # compute f(x + delta), f(x - delta)
            input_flat[i] = original_value + delta
            loss_plus = normalized_compressive_dist(input_flat.view_as(x_input), y_input)
            input_flat[i] = original_value - delta
            loss_minus = normalized_compressive_dist(input_flat.view_as(x_input), y_input)

            # restore original value
            input_flat[i] = original_value
            grad_input.view(-1)[i] = (loss_plus - loss_minus) / (2 * delta)
        return grad_input * grad_output
</code></pre>
","2024-10-29 14:27:09","1","Answer"
"79137287","","is there any answer to help me by downlaoding pip data","<p>I attempted to run the command pip install -r requirements.txt, but encountered the following error:</p>
<pre class=""lang-none prettyprint-override""><code>yaml
Code kopieren
ERROR: Could not find a version that satisfies the requirement torch==1.11.0+cu113 (from versions: none)
ERROR: No matching distribution found for torch==1.11.0+cu113
</code></pre>
<p>Despite updating everything, the issue persists. Could someone please assist me in resolving this?</p>
<p>I have already consulted various sources, including ChatGPT, and tried multiple solutions, but none have been successful so far.</p>
","2024-10-29 12:47:13","0","Question"
"79135639","","How to implement a custom loss function (NCD) in pytorch?","<p>I would like to implement a custom loss function known as Normalised Compression Distance or more commonly NCD for short. I am not quite sure how to approach this.</p>
<p>Could someone explain and give an example of how one would implement this in python for pytorch?</p>
<p>Here is the mathematical description:
<a href=""https://en.wikipedia.org/wiki/Normalized_compression_distance#:%7E:text=Normalized%20compression%20distance%20(NCD)%20is,be%20application%20dependent%20or%20arbitrary."" rel=""nofollow noreferrer"">Ncd link</a></p>
<p>I have implemented it as described, however I am not able to run a backwards pass over it as it has not been designed with torch implemented math.</p>
<pre><code>def loss_fn(self, x, y):
    xy = len(gzip.compress(np.concatenate(x.detach().numpy(),y.detach().numpy())))
    x = len(gzip.compress(x.detach().numpy()))
    y = len(gzip.compress(y.detach().numpy()))

    loss = (xy - min(x,y))/ (max(x,y))
    print(loss)
    return torch.tensor(loss)
</code></pre>
<p>Update:</p>
<p>Here is how the code looks now:</p>
<pre><code>def loss_fn(self, x, y):
    xy = len(gzip.compress(str(torch.cat((x,y))).encode('utf-8')))
    x = len(gzip.compress(str(x).encode('utf-8')))
    y = len(gzip.compress(str(y).encode('utf-8')))

    loss = (xy - torch.min(torch.tensor([x,y])))/ (torch.max(torch.tensor([x,y]))
    print(loss)

    return torch.tensor(loss)
</code></pre>
<p>I get the following error when running the loss.backwards on the output of the loss function:</p>
<p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p>
<p>I solved the error above by using requires_grad=True in my tensors.</p>
","2024-10-29 02:12:40","0","Question"
"79131858","79130996","","<p>You could just use the if statement in the init function or in another function, for example:</p>
<pre><code>from enum import Enum

class ModelType(Enum):
    Parallel = 1
    Sequential = 2
    
class Model(torch.nn.Model):
    def __init__(self, layers: str, d_in: int, d_out: int, model_type: ModelType):
        super().__init__()
        self.layers = layers
        linears = torch.nn.ModuleList([
         torch.nn.Linear(d_in, d_out),
         torch.nn.Linear(d_in, d_out),
        ])
        self.model_type = model_type
        self.initialize()

    def initialize(self):
        if self.model_type == ModelType.Parallel:
            self.fn = self.parallel
        else if self.model_type == ModelType.Sequential::
            self.fn = self.sequential

    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -&gt; torch.Tensor:
        x = self.fn(x1, x2)
        return x
    
    def parallel(self, x1, x2):
        x1 = self.linears[0](x1)
        x2 = self.linears[0](x2)
        x = x1 + x2
        return x
    
    def sequential(self, x1, x2):
        x = x1 + x2
        x = self.linears[0](x)
        x = self.linears[0](x)
        return x
</code></pre>
<p>I hope it helps.</p>
","2024-10-28 01:39:28","1","Answer"
"79131282","79130753","","<p>Let's assume that your input shape is <code>(N, 1, 32, 32)</code> ((N,C,H,W) format).
Now, let us list the output shape after each layer.</p>
<p><code>self.conv1</code>: <code>(N, 32, 32, 32)</code>. <strong>32 filters of shape <code>5x5x1</code></strong> are convolved over the padded input (5x5 is the kernel size, 1 is the number of input channels). Hence, the output has 32 feature maps of shape <code>32x32</code>.</p>
<p><code>self.pool1</code>: <code>(N, 32, 16, 16)</code>. Pooling layer downsamples feature maps by a factor of 2.</p>
<p><code>self.conv2</code>: <code>(N, 64, 16, 16)</code>. <strong>64 filters of shape <code>5x5x32</code></strong> are applied to the padded input (5x5 is the kernel size, 32 is the number of input channels). Output shape after performing convolution operation with one filter is <code>16x16</code>. Hence, the output of the layer has 64 feature maps of shape <code>16x16</code>.</p>
<p><code>self.pool2</code>: <code>(N, 64, 8, 8)</code>. Pooling layer downsamples feature maps by a factor of 2.</p>
","2024-10-27 19:01:15","0","Answer"
"79130996","","Programmatically change components of pytorch Model?","<p>I am training a model in pytorch and would like to be able to programmatically change some components of the model architecture to check which works best without any if-blocks in the <code>forward()</code>. Consider a toy example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

class Model(torch.nn.Model):
   def __init__(self, layers: str, d_in: int, d_out: int):
      super().__init__()
      self.layers = layers
      linears = torch.nn.ModuleList([
         torch.nn.Linear(d_in, d_out),
         torch.nn.Linear(d_in, d_out),
      ])

   def forward(x1: torch.Tensor, x2: torch.Tensor) -&gt; torch.Tensor:
      if self.layers == &quot;parallel&quot;:
         x1 = self.linears[0](x1)
         x2 = self.linears[1](x2)
         x = x1 + x2
      elif self.layers == &quot;sequential&quot;:
         x = x1 + x2
         x = self.linears[0](x)
         x = self.linears[1](x)
      return x
</code></pre>
<p>My first intution was to provide external functions, e.g.</p>
<pre><code>def parallel(x1, x2):
   x1 = self.linears[0](x1)
   x2 = self.linears[1](x2)
   return x1 + x2
</code></pre>
<p>and provide them to the model, like</p>
<pre><code>class Model(torch.nn.Model):
   def __init__(self, layers: str, d_in: int, d_out: int, fn: Callable):
      super().__init__()
      self.layers = layers
      linears = torch.nn.ModuleList([
         torch.nn.Linear(d_in, d_out),
         torch.nn.Linear(d_in, d_out),
      ])
      self.fn = fn

   def forward(x1: torch.Tensor, x2: torch.Tensor) -&gt; torch.Tensor:
      x = self.fn(x1, x2)
</code></pre>
<p>but of course the function's scope does not know <code>self.linears</code> and I would also like to avoid having to pass each and every architectural element to the function.</p>
<p>Do I wish for too much? Do I have to &quot;bite the sour apple&quot; as it says in German and either have larger function signatures, or use if-conditions, or something else? Or is there a solution to my problem?</p>
","2024-10-27 16:25:12","2","Question"
"79130753","","Why Doesn’t the Output Shape Multiply Across Channels in Convolutional Layers?","<pre><code># First convolutional layer: input channels = 1, output channels = 32, kernel size = 5x5, padding = 2 (SAME)
self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2)
# First pooling layer: max pooling, kernel size = 2x2, stride = 2
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

# Second convolutional layer: input channels = 32, output channels = 64, kernel size = 5x5, padding = 2 (SAME)
self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)
# Second pooling layer: max pooling, kernel size = 2x2, stride = 2
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
</code></pre>
<p>Why isn't the output after the second convolution layer
14 * 14 * 32 * 64? For the 32-channel input, each convolutional kernel operates on one channel, resulting in 64 different outcomes. Shouldn't the 32 channels be multiplied together?</p>
<p>I got answers like: for every 14 * 14 position of the input, a 5<em>5</em>32 kernel dot product with 5<em>5</em>32 input area will give you a 14*14 single chnnel output. Isn't the kernel size 5 * 5?</p>
","2024-10-27 14:14:47","-2","Question"
"79127625","79127316","","<p>All the operations you are using, namely <code>view</code>, <code>numpy</code> and <code>as_tensor</code> affects only the &quot;metadata&quot; of the tensors, i.e how they should interpret the data that is stored inside. None of these actually change any bit in the underlying array of numbers (which can be interpreted however you want).</p>
<p>You can check in the documentations for these 3 operations (<a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html"" rel=""nofollow noreferrer"">numpy</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.view.html"" rel=""nofollow noreferrer"">view</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.as_tensor.html"" rel=""nofollow noreferrer"">as_tensor</a>) that they mention sharing the storage/data (with some excceptions that don't apply to your code, like gpu data etc)</p>
<p>So when you go full circle, no a single bit has changed and thus you can perfectly recover the initial tensor</p>
","2024-10-26 00:50:54","2","Answer"
"79127389","79127383","","<p>This problem is because there isn't a version of your library compatible with the version of Python you are using.</p>
<p>Collab has upgraded its python version and it isn't compatible anymore with the four years old torch=1.6.0.</p>
<p>You can:</p>
<ol>
<li>try to upgrade <code>torch</code> for a newer version. The newest one is 2.5.0. Here is the release history: <a href=""https://pypi.org/project/torch/#history"" rel=""nofollow noreferrer"">https://pypi.org/project/torch/#history</a></li>
<li>try to use an older Python in your Collab notebook (I don't know if it is possible).</li>
</ol>
","2024-10-25 21:44:52","-1","Answer"
"79127383","","How to solve this ERROR: Could not find a version that satisfies the requirement torch==1.6.0?","<p>I'm using a demo on Colab notebook called Deep Exemplar Based Video Colorization. <a href=""https://colab.research.google.com/drive/1Y1XTlTdUG-2LzrH1Vnr_osg9BQavfYsz?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1Y1XTlTdUG-2LzrH1Vnr_osg9BQavfYsz?usp=sharing</a></p>
<p>Worked swimmingly for four years until this monday. In the Run Environment cell - I get the error below. I'm not sure what the protocol is here, but I thought I'd just post the Run environment cell, after I run it, which displays the errors. As it was working fine till last sunday, does anyone know if there was a recent update which could have caused the issue?</p>
<p>Run Environment Command cell:</p>
<pre><code># seems to be a colab bug, need to install previous version for pytorch
!pip install torch==1.6.0 torchvision==0.7.0
!pip install -q moviepy
!apt install imagemagick
!pip install imageio==2.4.1

%cd Deep-Exemplar-based-Video-Colorization/
! pip install -r requirements.txt
</code></pre>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch==1.6.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0)
ERROR: No matching distribution found for torch==1.6.0

--------------------------------
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
moviepy 1.0.3 requires imageio&lt;3.0,&gt;=2.5; python_version &gt;= &quot;3.4&quot;, but you have imageio 2.4.1 which is incompatible.
scikit-image 0.24.0 requires imageio&gt;=2.33, but you have imageio 2.4.1 which is incompatible.

------------------------------------------------
ERROR: Failed building wheel for scipy
  Running setup.py clean for scipy
  error: subprocess-exited-with-error
  
  × python setup.py clean did not run successfully.
  │ exit code: 1
  ╰─&gt; See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed cleaning build dir for scipy
---------------------------------------------
</code></pre>
","2024-10-25 21:40:35","0","Question"
"79127316","","Does PyTorch tensor type cast preserve information?","<p>Consider the following simple operation,</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; t
tensor([ 1.8750, -0.6875, -1.1250, -1.3750,  1.3750, -1.1250,  0.4688, -0.4062,
         0.8750, -1.7500], dtype=torch.float)
&gt;&gt;&gt; t.view(torch.uint8)
tensor([ 63, 179, 185, 187,  59, 185,  47, 173,  54, 190], dtype=torch.uint8)
&gt;&gt;&gt; t.view(torch.uint8).shape
torch.Size([10])
&gt;&gt;&gt; t.view(torch.uint8).numpy()
array([ 63, 179, 185, 187,  59, 185,  47, 173,  54, 190], dtype=uint8)
&gt;&gt;&gt; torch.as_tensor(t.view(torch.uint8).numpy())
tensor([ 63, 179, 185, 187,  59, 185,  47, 173,  54, 190], dtype=torch.uint8)
&gt;&gt;&gt; torch.as_tensor(t.view(torch.uint8).numpy()).view(torch.float)
tensor([ 1.8750, -0.6875, -1.1250, -1.3750,  1.3750, -1.1250,  0.4688, -0.4062,
         0.8750, -1.7500], dtype=torch.float)

</code></pre>
<p>I am confused about how information is preserved across typecast conversions. The original tensor is of type float8, which is then converted to uint8 (0-255). The uint8 numpy array is then used to initialize a float8 tensor. Shouldn't this order of conversions result in loss of information?</p>
","2024-10-25 21:08:35","0","Question"
"79127307","","PyTorch how to use the gradient of an intermediate variable in the computation graph of a later variable","<pre class=""lang-py prettyprint-override""><code>point_2 = torch.tensor([0.2, 0.8], device=device, requires_grad=True)
p = torch.cat((point_2, torch.tensor([0], device=device)), 0)

x_verts = torch.tensor([0.0, 1.0, 0.0], device=device, requires_grad=True)
y_verts = torch.tensor([0.0, 0.0, 1.0], device=device, requires_grad=True)
z_verts = torch.tensor([0.1, -0.1, 0.2], device=device, requires_grad=True)

v1_2d = torch.cat((torch.index_select(x_verts, 0, torch.tensor([0])), torch.index_select(y_verts, 0, torch.tensor([0])), torch.tensor([0])))
v2_2d = torch.cat((torch.index_select(x_verts, 0, torch.tensor([1])), torch.index_select(y_verts, 0, torch.tensor([1])), torch.tensor([0])))
v3_2d = torch.cat((torch.index_select(x_verts, 0, torch.tensor([2])), torch.index_select(y_verts, 0, torch.tensor([2])), torch.tensor([0])))


area_3 = torch.cross(v2_2d - v1_2d, v3_2d - v1_2d)
area = torch.index_select(area_3, 0, torch.tensor([2]))

alpha_3 = 0.5 * torch.cross(v2_2d - p, v3_2d - p) / area
beta_3 = 0.5 * torch.cross(v3_2d - p, v1_2d - p) / area
gamma_3 = 0.5 * torch.cross(v1_2d - p, v2_2d - p) / area

alpha = torch.index_select(alpha_3, 0, torch.tensor([2]))
beta = torch.index_select(beta_3, 0, torch.tensor([2]))
gamma = torch.index_select(gamma_3, 0, torch.tensor([2]))

z = alpha * torch.index_select(z_verts, 0, torch.tensor([0])) + beta * torch.index_select(z_verts, 0, torch.tensor([1])) + gamma * torch.index_select(z_verts, 0, torch.tensor([2]))

z.backward()


grad_norm = torch.norm(point_2.grad ) # &lt;= disconnection

f = torch.tanh(10.0 * (grad_norm - 2.0))
f.backward() # &lt;= error

print(x_verts.grad)
print(y_verts.grad)
print(z_verts.grad)

</code></pre>
<p>I have this code which fails because I use a variable's <code>.grad</code> value as input to another variable. How can I fix this?</p>
","2024-10-25 21:02:55","0","Question"
"79126552","79124099","","<p>After some more research, I found that mps is only built in the pytorch nightly release, which is installable via:</p>
<p><code>conda install pytorch-nightly::pytorch -c pytorch-nightly</code></p>
<p><a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a></p>
","2024-10-25 16:26:51","1","Answer"
"79124099","","How can I get MPS running in Pytorch on my Apple M2?","<p>I've got the following function to check whether MPS is enabled in Pytorch on my MacBook Pro Apple M2 Max. I get the response:</p>
<pre><code>MPS is not available
MPS is not built
</code></pre>
<p>Then I get an error because MPS can't check the version of MacOS if mps doesn't exist.</p>
<pre><code>def check_mps():
    if torch.backends.mps.is_available():
        print(&quot;MPS is available&quot;)
    else:
        print(&quot;MPS is not available&quot;)

    if torch.backends.mps.is_built():
        print(&quot;MPS is built&quot;)
    else:
        print(&quot;MPS is not built&quot;)

    if torch.backends.mps.is_macos_or_newer(13, 0):
        print(&quot;MPS is macOS 13 or newer&quot;)
    else:
        print(&quot;MPS is not macOS 13 or newer&quot;)
</code></pre>
<p>I've already verified my OS is up-to-date, and I'm not seeing many other steps to get Pytorch running on Metal. What am I missing?</p>
<p>I tried updating my OS, which is the only recommendation I've found so far. It didn't change anything.</p>
","2024-10-25 02:38:59","-1","Question"
"79123911","79081885","","<ul>
<li>Install 3.11 version: <a href=""https://www.python.org/downloads/release/python-3110/"" rel=""nofollow noreferrer"">https://www.python.org/downloads/release/python-3110/</a></li>
<li>Create env: <code>python3.11 -m venv ~/.venv/3.11</code></li>
<li>Activate: <code>~/.venv/3.11/bin/activate</code></li>
<li>Check: <code>python --version</code></li>
</ul>
","2024-10-25 00:01:27","0","Answer"
"79123338","79102534","","<p>You don't have to save the split results on separate folders to maintain reproducibility, which is what I am assuming you really care for.</p>
<p>You could instead fix the seed before calling split like this:</p>
<pre><code>torch.manual_seed(42)
data_train, data_val = torch.utils.data.random_split(data_all_train, (0.7, 0.3))
</code></pre>
<p>Then you get to maintain just the initial folders while also ensuring the train and val splits are consistent across trials.</p>
<hr />
<p>But the caveat to the above is, you are fixing the global seed so you are also losing the randomness you might desire in the dataloader shuffling and such, which will end up identical per trial.</p>
<p>To avoid that, you can narrow down the scope you are fixing the seed by setting it only for the generator you pass to the split call:</p>
<pre><code>split_gen = torch.Generator()
split_gen.manual_seed(42)

data_train, data_val = torch.utils.data.random_split(
                             data_all_train, 
                             (0.7, 0.3), 
                             generator=split_gen)
</code></pre>
","2024-10-24 19:01:44","2","Answer"
"79123324","79120980","","<p>The code snippet you posted uses <code>roboflow-python</code> package which provides HTTP client making calls to the Roboflow platform.</p>
<p>If you intend to use the models inside your app, please use <a href=""https://github.com/roboflow/inference"" rel=""nofollow noreferrer"">https://github.com/roboflow/inference</a> package. Here you may find getting-started docs: <a href=""https://inference.roboflow.com/quickstart/run_a_model/"" rel=""nofollow noreferrer"">https://inference.roboflow.com/quickstart/run_a_model/</a></p>
<p>Device can be specified by env flag: <code>CUDA_VISIBLE_DEVICES</code></p>
","2024-10-24 18:55:38","1","Answer"
"79120980","","Roboflow device management. In which GPU is the model loaded?","<p>I am experimenting with the SDK of Roboflow, I haven't found documentation on device management. I have three questions:</p>
<ol>
<li>Is the model loaded to GPU by default?</li>
<li>What happens when multiple GPUs are available?</li>
<li>And how do I define in which GPU I want the model allocated?</li>
</ol>
<p><strong>EDIT:</strong></p>
<p>Roboflow does not expose the device directly as Pytorch would:</p>
<pre><code>rf = Roboflow(api_key=&quot;YOUR API KEY&quot;)
project = rf.workspace().project(&quot;license-plate-recognition-rxg4e&quot;)
model = project.version(&quot;4&quot;).model
model = model.to(&quot;cuda:4&quot;)
&gt; AttributeError: 'ObjectDetectionModel' object has no attribute 'to'
</code></pre>
","2024-10-24 08:15:48","0","Question"
"79117339","79114537","","<p>I would suggest using the <a href=""https://github.com/xinntao/Real-ESRGAN/blob/master/inference_realesrgan.py"" rel=""nofollow noreferrer"">inference_realesrgan.py</a> file found in the <a href=""https://github.com/xinntao/Real-ESRGAN"" rel=""nofollow noreferrer"">official Real-ESRGAN repo</a> to load the model and do the upscaling.</p>
<p>Download the <code>inference_realesrgan.py</code> file and put it in your codebase (same directory as your <code>app.py</code> file), then for example you can create a wrapper function (let's call it <code>inference_wrapper.py</code>) to Call the script like so:</p>
<pre class=""lang-py prettyprint-override""><code>import subprocess
import os

def upscale_image(
    input_image_path,
    output_image_path,
    model_name,
    outscale,
):

    # Build the command to run the inference script
    command = [
        &quot;python&quot;,
        &quot;inference_realesrgan.py&quot;,  # Path to your script
        &quot;--input&quot;,
        input_image_path,
        &quot;--output&quot;,
        os.path.dirname(output_image_path),
        &quot;--model_name&quot;,
        model_name,
        &quot;--outscale&quot;,
        str(outscale),
        &quot;--suffix&quot;,
        &quot;upscaled&quot;,
    ]

    subprocess.run(command, check=True)

    # Return the path to the upscaled image
    img_name, ext = os.path.splitext(os.path.basename(input_image_path))
    upscaled_image = os.path.join(
        os.path.dirname(output_image_path), f&quot;{img_name}_upscaled{ext}&quot;
    )
    return upscaled_image
</code></pre>
<p>then in your <code>app.py</code> file, call the <code>upscale_image</code> function with the appropriate arguments like so:</p>
<pre class=""lang-py prettyprint-override""><code>from inference_wrapper import (
    upscale_image,
)

input_image = 'path/to/input/image.jpg'
output_image = 'path/to/output/folder/image.jpg'
model_name = &quot;RealESRGAN_x4plus_anime_6B&quot; # or whichever model you want to use
outscale = 4

# Call the function to upscale the image
upscaled_image = upscale_image(input_image, output_image, model_name, outscale)

# The image has been upscaled and saved to `upscaled_image`
print(f&quot;Upscaled image saved at: {upscaled_image}&quot;)
</code></pre>
<p><strong>This should do the trick.</strong></p>
<p><strong>Also:</strong> I had a look at the <code>inference_realesrgan.py</code> file and ideally you would want to refactor it a bit to make it a bit more modular, like separating the functionality to create independent functions to:</p>
<ul>
<li>load the model</li>
<li>download the model (weights)</li>
<li>upscale the input image</li>
<li>enable/disable face enhancer</li>
<li>set the gpu_id</li>
<li>set the half-precision to true/false to use fp16/fp32</li>
</ul>
<p>Hope you come right.</p>
","2024-10-23 09:39:07","0","Answer"
"79116093","79099612","","<p>I think oppressionslayer's solution will work but I would recommend minimizing the squared norm instead of the norm since they both converge toward the same argmin and the squared norm is fully differentiable:</p>
<pre><code>self.grad_sqmag = self.grad_x**2 + self.grad_y**2
</code></pre>
<p>This is equivalent to minimizing <code>f(x) = x**2</code> instead of <code>f(x) = abs(x)</code></p>
<p>In general (and in ML in particular) there are few situations where you want to backprop through the norm rather than the squared norm.</p>
<p>As for why it &quot;works&quot; on GPU, I cannot say for sure but I guess this is just implementation shenanigans and it probably just fails on GPU too in a less obvious way, giving you wrong gradients anyway.</p>
","2024-10-23 00:32:50","0","Answer"
"79114634","79114537","","<p>The problem is that you try to initiate a helper method, which needs the &quot;real&quot; model as <code>model</code> parameter. Default to that model parameter is <code>None</code>, which explains the error message. You can see it <a href=""https://github.com/xinntao/Real-ESRGAN/blob/a4abfb2979a7bbff3f69f58f58ae324608821e27/realesrgan/utils.py#L29"" rel=""nofollow noreferrer"">in the init</a> of <code>RealESRGANer</code>. You can also see how the actual model is loaded in the inference script in <a href=""https://github.com/xinntao/Real-ESRGAN/blob/a4abfb2979a7bbff3f69f58f58ae324608821e27/inference_realesrgan.py#L58"" rel=""nofollow noreferrer"">the lines below</a>.<br />
You could copy the model loading line, if the weights you want to use are fixed:</p>
<pre><code>model_path = &quot;./RealESRGAN_x4plus_anime_6B.pth&quot;
model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)  # for `RealESRGAN_x4plus_anime_6B`
model = RealESRGANer(scale=4, model=model, model_path=model_path)
</code></pre>
<p>Note that in the inference script, if the <code>--face_enhance</code> parameter is used, another model is loaded (<a href=""https://github.com/xinntao/Real-ESRGAN/blob/a4abfb2979a7bbff3f69f58f58ae324608821e27/inference_realesrgan.py#L118"" rel=""nofollow noreferrer"">code</a>) and the <code>RealESRGANer</code> model is used as a parameter (there it is called <code>upsampler</code>).</p>
<p>Also please note that I could not test my solution, it is just based on digging through the source code.</p>
","2024-10-22 14:47:10","0","Answer"
"79114537","","Python AttributeError: 'NoneType' object has no attribute 'load_state_dict' when using RealESRGAN model","<p>I'm in the process of building a backend with Python to upscale images using the RealESRGAN model from this git repository: <a href=""https://github.com/xinntao/Real-ESRGAN"" rel=""nofollow noreferrer"">https://github.com/xinntao/Real-ESRGAN</a>.</p>
<p>This is a simplified <code>app.py</code> file in which I'm running tests:</p>
<pre><code>import os
from flask import Flask, request, jsonify
from flask_cors import CORS
from realesrgan import RealESRGANer
from PIL import Image
import numpy as np
import io

app = Flask(__name__)
CORS(app)

# Initialize the RealESRGAN model
model_path = &quot;./RealESRGAN_x4plus_anime_6B.pth&quot;  # Path to the model that can be dowloaded here: https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth

model = RealESRGANer(scale=4, model_path=model_path)  # Initialize the model with the specified scale

@app.route('/upscale', methods=['POST'])
def upscale_image():
    if 'image' not in request.files:
        return jsonify({&quot;error&quot;: &quot;No image provided&quot;}), 400
    
    file = request.files['image']

    try:
        # Read the image file
        img = Image.open(file.stream).convert('RGB')

        # Convert image to numpy array and upscale
        img_array = np.array(img)
        upscaled_image = model.predict(img_array)

        # Convert upscaled image back to PIL format
        upscaled_image = Image.fromarray(upscaled_image)

        # Save the upscaled image to a byte stream
        img_byte_arr = io.BytesIO()
        upscaled_image.save(img_byte_arr, format='PNG')
        img_byte_arr.seek(0)

        return jsonify({&quot;message&quot;: &quot;Image upscaled successfully&quot;, &quot;upscaled_image&quot;: img_byte_arr.getvalue().decode('latin1')})

    except Exception as e:
        return jsonify({&quot;error&quot;: str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
<p>However, I'm experiencing the following error when trying to run the app:</p>
<pre><code>model.load_state_dict(loadnet[keyname], strict=True)
    ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'load_state_dict'
</code></pre>
<p>Any suggestions on how to solve this problem?</p>
<p>Here are a few things that I've tried to do while debugging:</p>
<p><strong>Loading of Model:</strong> I have tried &quot;manually&quot; loading the model with the following code:</p>
<pre><code># Detect the device (GPU or CPU)
        device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

        # Initialize RealESRGANer
        self.model = RealESRGANer(
            scale=4,  # Upscale factor
            model_path=model_path,  # Model path for pretrained weights
            tile=0,  # No tiling
            tile_pad=10,  # Padding if tiling is used
            pre_pad=0,  # Pre-padding
            half=False,  # Avoid half precision issues
            device=device  # Use detected device (CUDA or CPU)
        )

        # Load the weights into the model architecture
        state_dict = torch.load(model_path, map_location=device)

        # Ensure 'params_ema' is present in the state_dict
        if &quot;params_ema&quot; in state_dict:
            self.model.model.load_state_dict(state_dict[&quot;params_ema&quot;], strict=True)
        else:
            raise ValueError(&quot;The state_dict does not contain 'params_ema'. Please check the model file.&quot;)

        # Set the model to evaluation mode (important for inference)
        self.model.model.eval()
</code></pre>
<p><strong>File Path:</strong> I have also included the following code to ensure that my model path is correct:</p>
<pre><code>print(os.path.exists(&quot;./RealESRGAN_x4plus_anime_6B.pth&quot;))
</code></pre>
<p><strong>Alternative Model:</strong>  I also have tested the alternative model that can be found here: <a href=""https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth"" rel=""nofollow noreferrer"">https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth</a></p>
","2024-10-22 14:25:29","1","Question"
"79112958","","Tensorflow equivalent of torch.scatter_add","<p>How can I implement the same operation with tf 1.15?</p>
<pre class=""lang-py prettyprint-override""><code>import torch

B, T, N, K = 2,3,4,2

# a is a counter table where T is the number of groups
a = torch.zeros(T, N, dtype=torch.long)

# x is a batch of data where K elements are selected for each group
x = torch.randint(0, N, (B, T, K))

# the counter should record all data within the batch (per group)
y = x.permute(1,2,0).reshape(T, -1)

# update counter
a.scatter_add(index=y, dim=-1, src=torch.ones_like(y))
</code></pre>
","2024-10-22 07:53:40","0","Question"
"79109579","79106908","","<p>To calculate gradients in TensorFlow similar to how you did it in PyTorch, you'll need to use TensorFlow's automatic differentiation capabilities. However, there are a few key differences to keep in mind:</p>
<ul>
<li>TensorFlow 2.x uses eager execution by default, which is more similar
to PyTorch's dynamic graph approach.</li>
<li>You'll need to use    tf.GradientTape to record operations for
automatic differentiation.</li>
</ul>
<p>Here's how you can calculate gradients in TensorFlow 2.x, similar to your PyTorch example:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# Assuming x is your input tensor and model is your TensorFlow model
x = tf.Variable(x)  # Make sure x is a Variable or use tf.convert_to_tensor(x)

with tf.GradientTape() as tape:
    y = model(x)
    
dydx = tape.gradient(y, x)
</code></pre>
<p>More on the subject:
<a href=""https://www.tensorflow.org/guide/autodiff"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/autodiff</a>
<a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/GradientTape</a></p>
<p>UPD: there is also may be a problem with <code>x</code></p>
<p>dydx is None: This usually happens because TensorFlow doesn't know it needs to compute gradients with respect to x. By default, only tf.Variable objects are watched. By using tape.watch(x), you explicitly tell TensorFlow to track x.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# Assume x_value is your input data as a NumPy array or TensorFlow tensor
x_value = ...  # Your input data
x = tf.convert_to_tensor(x_value)  # Convert to a TensorFlow tensor if not already one

with tf.GradientTape() as tape:
    tape.watch(x)  # Ensure x is being tracked for gradient computation
    y = model(x)   # Forward pass through your model

# Compute the gradient of y with respect to x
dydx = tape.gradient(y, x)
</code></pre>
","2024-10-21 10:15:41","1","Answer"
"79106908","","The analogue of torch.autograd in TensorFlow","<p>I want to get the dradients of the model after its training. For exmaple, I have the input tensor X and the output y, that is y = model(x). So using pytorch I can calculate the dradient with the folowing command:</p>
<pre><code>y = model(x)
dydx = torch.autograd.grad(Y, X, torch.ones_like(Y), create_graph=True)[0][:, 0]
</code></pre>
<p>I want to get the same value after training model with TensorFlow framework.</p>
<p>I tried:</p>
<pre><code>y = model.predict_u(x)
dydx = tf.gradients(y, x)[0]
</code></pre>
<p>But I got dydx as NoneType. I tried to include the dydx in the model class and to get the gradient through the tf.Session but I had: &quot;ResourceExhaustedError: Graph execution error&quot;.</p>
<p>I have worked with Pytorch framework and now I decide to try TensorFlow, but I have some difficulties.</p>
","2024-10-20 11:14:54","0","Question"
"79106483","79099612","","<p>Is it going negative? You can fix that with:</p>
<pre><code>eps = 1e-8  
self.grad_mag = torch.sqrt(self.grad_x**2 + self.grad_y**2 + eps)
</code></pre>
<p>You could also switch optimizers to see if this fixes the issue:</p>
<pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


or completely:

import torch

# Example in the forward pass
eps = 1e-8  # Small epsilon to avoid negative values in sqrt due to precision
grad_magnitude = self.grad_x**2 + self.grad_y**2

# Check for any potential negative values (this shouldn't happen, but for debugging)
if (grad_magnitude &lt; 0).any():
    print(&quot;Warning: Negative values detected in grad_magnitude&quot;)

# Clamp the value to avoid NaN errors
self.grad_mag = torch.sqrt(torch.clamp(grad_magnitude, min=eps))


</code></pre>
<p>The issue might be floating-point precision differences between CUDA and CPU, which are causing the NaNs in the backward pass.</p>
","2024-10-20 06:35:28","1","Answer"
"79106212","79106107","","<p>Tinygrad performs operations in a &quot;lazy&quot; way, so the matrix multiplication hasn't been performed yet. Change your matrix multiplication line to:</p>
<pre><code>tg_result = (tg_tensor @ tg_tensor).realize()
</code></pre>
<p>or</p>
<pre><code>tg_result = (tg_tensor @ tg_tensor).numpy()
</code></pre>
","2024-10-20 01:09:47","5","Answer"
"79106107","","Is this benchmark valid? tinygrad is impossibly fast vs. torch or numpy for medium-sized (10000 by 10000) matrix multiplication (CPU)","<p>I ran the following benchmark code on google collab CPU with high ram enabled. Please point out any errors in the way I am benchmarking, (if any) as well as why there is a such a high performance boost with tinygrad.</p>
<pre><code># Set the size of the matrices
size = 10000

# Generate a random 10000x10000 matrix with NumPy
np_array = np.random.rand(size, size)

# Generate a random 10000x10000 matrix with PyTorch
torch_tensor = torch.rand(size, size)

# Generate a random 10000x10000 matrix with TinyGrad
tg_tensor = Tensor.rand(size, size)  

# Benchmark NumPy
start_np = time.time()
np_result = np_array @ np_array  # Matrix multiplication
np_time = time.time() - start_np
print(f&quot;NumPy Time: {np_time:.6f} seconds&quot;)

# Benchmark PyTorch
start_torch = time.time()
torch_result = torch_tensor @ torch_tensor  # Matrix multiplication
torch_time = time.time() - start_torch
print(f&quot;PyTorch Time: {torch_time:.6f} seconds&quot;)

# Benchmark TinyGrad
start_tg = time.time()
tg_result = tg_tensor @ tg_tensor  # Matrix multiplication
tg_time = time.time() - start_tg
print(f&quot;TinyGrad Time: {tg_time:.6f} seconds&quot;)
</code></pre>
<ul>
<li>NumPy Time: 11.977072 seconds</li>
<li>PyTorch Time: 7.905509 seconds</li>
<li>TinyGrad Time: 0.000607 seconds</li>
</ul>
<p>These were the results. After running the code many times, the results were very similar</p>
","2024-10-19 23:01:30","4","Question"
"79104863","79102723","","<pre><code>from concurrent.futures import ThreadPoolExecutor
...
model = torch.jit.script(model)
...
pool = ThreadPoolExecutor()
...
y_pred = torch.cat(list(pool.map(model, imgs)))
...
</code></pre>
<p>but this is not that faster as I expested.</p>
","2024-10-19 11:58:30","0","Answer"
"79104526","79104239","","<p>Your <code>encoder_blocks</code> and <code>decoder_blocks</code> are ModuleList which are not like <code>nn.Sequential</code>, they are just a collection of modules with no <code>forward</code> function. You need to manually iterate over the modules in your list:</p>
<pre><code>x1 = self.init_conv(x)
for module in self.encoder_blocks:
    x1,_ = module(x1,t)
# Same for decoder ...
</code></pre>
<p>The documentation for <code>nn.ModuleList</code> shows a similar example : <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html</a></p>
","2024-10-19 08:46:53","0","Answer"
"79104239","","Difficulty fixing the code for DDPM with unet using Pyorch","<p>I am studying Deep learning and have an assignment for training a DDPM with UNet on the MNIST handwritten digits dataset. Three ipynb files (model, unet and train_mnist) and one pdf for unet diagram are saved in <a href=""https://github.com/daniel159357/DDPM-for-Unet"" rel=""nofollow noreferrer"">my github</a>. I filled in the missing parts at #Your Code Here# in &quot;model.ipynb&quot; &amp; &quot;unet.ipynb&quot;</p>
<p>When I run the &quot;unet.ipynb&quot;, a warning prompts as below.</p>
<pre><code>NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-21-36a7bf177d9a&gt; in &lt;cell line: 60&gt;()
     62     t=torch.randint(0,1000,(3,))
     63     model=Unet(1000,128)
---&gt; 64     y=model(x,t)
     65     print(y.shape)

5 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _forward_unimplemented(self, *input)
    350         registered hooks while the latter silently ignores them.
    351     &quot;&quot;&quot;
--&gt; 352     raise NotImplementedError(f'Module [{type(self).__name__}] is missing the required &quot;forward&quot; function')
    353 
    354 

NotImplementedError: Module [ModuleList] is missing the required &quot;forward&quot; function
</code></pre>
<p>Here is the unet architecture:</p>
<pre><code>class Unet(nn.Module):

    def __init__(self,timesteps,time_embedding_dim,in_channels=3,out_channels=2,base_dim=32,dim_mults=[2,4,8,16]):
        super().__init__()
        assert isinstance(dim_mults,(list,tuple))
        assert base_dim%2==0

        channels=self._cal_channels(base_dim,dim_mults)

        self.init_conv=ConvBnSiLu(in_channels,base_dim,3,1,1)
        self.time_embedding=nn.Embedding(timesteps,time_embedding_dim)

        self.encoder_blocks=nn.ModuleList([EncoderBlock(c[0],c[1],time_embedding_dim) for c in channels])
        self.decoder_blocks=nn.ModuleList([DecoderBlock(c[1],c[0],time_embedding_dim) for c in channels[::-1]])

        self.mid_block=nn.Sequential(*[ResidualBottleneck(channels[-1][1],channels[-1][1]) for i in range(2)],
                                        ResidualBottleneck(channels[-1][1],channels[-1][1]//2))

        self.final_conv=nn.Conv2d(in_channels=channels[0][0]//2,out_channels=out_channels,kernel_size=1)

    def forward(self,x,t=None):
        '''
            Implement the data flow of the UNet architecture
        '''
        # ---------- **** ---------- #
        # YOUR CODE HERE
        t = self.time_embedding

        #initial conv
        x1 = self.init_conv(x)
        #Down
        x2 = self.encoder_blocks(x1,t)
        x3 = self.encoder_blocks(x2[0],t)
        x4 = self.encoder_blocks(x3[0],t)
        x5 = self.encoder_blocks(x4[0],t)
        #Middle
        x6 = self.mid_block(x5[0])
        #Up
        x = self.decoder_blocks(x6,x5[1],t)
        x = self.decoder_blocks(x,x4[1],t)
        x = self.decoder_blocks(x,x3[1],t)
        x = self.decoder_blocks(x,x2[1],t)
        x = self.decoder_blocks(x,x1[1],t)
        #final
        x = self.final_conv(x)

        # ---------- **** ---------- #
        return x


    def _cal_channels(self,base_dim,dim_mults):
        dims=[base_dim*x for x in dim_mults]
        dims.insert(0,base_dim)
        channels=[]
        for i in range(len(dims)-1):
            channels.append((dims[i],dims[i+1])) # in_channel, out_channel

        return channels

if __name__==&quot;__main__&quot;:
    x=torch.randn(3,3,224,224)
    t=torch.randint(0,1000,(3,))
    model=Unet(1000,128)
    y=model(x,t)
    print(y.shape)
</code></pre>
<p>I have tried to use poe and perplexity and the things are getting worse with bugs. I have also reviewed the documentation in Pytorch and got not idea how to fix the code.
Would someone can spot the problems and explain how to implement the unet? Happy to discuss. Thanks.</p>
","2024-10-19 05:02:10","0","Question"
"79102723","","Using multiprocessing for training in pytorch","<p>I want to try to finetine a model with images of arbitrary sizes.
Not so hard to write somthing like</p>
<pre><code>...
y_pred = torch.cat([model(im) for im in imgs])
loss = loss_fn(y_pred, y)
...
</code></pre>
<p>but even simple <code>model(batch)</code>  kinda cpu-bound, so this is even slower.</p>
<p>I tried somthing like</p>
<pre><code>pool.map(model, imgs)
</code></pre>
<p>which gives me error telling autograd can't be used between processes.</p>
<p>I've briefly looked dataparallel pages in pytorch documentation and came to the conclusion, that dataparallel cant be used in such a way.
So is there a way or not?</p>
","2024-10-18 15:30:41","0","Question"
"79102534","","Save to disk training dataset and validation dataset separately in PyTorch","<p>I want to save train dataset, test dataset, and validation dataset in 3 separate folders.</p>
<p>Doing this for training and testing is easy</p>
<pre class=""lang-py prettyprint-override""><code># Get training and testing data
all_training_data = getattr(datasets, config['Pytorch_Dataset']['dataset'])(
    root= path_to_data + &quot;/data_all_train_&quot; + config['Pytorch_Dataset']['dataset'],
    train=True,
    download=True, # If present it will not download the data again
    transform=ToTensor()
)
test_data = getattr(datasets, config['Pytorch_Dataset']['dataset'])(
    root= path_to_data + &quot;/data_test_&quot; + config['Pytorch_Dataset']['dataset'],
    train=False,
    download=True, # If present it will not download the data again
    transform=ToTensor()
)
</code></pre>
<p>This code makes use of <code>torchvision.datasets</code> to load and save to disk the dataset specified in <code>config['Pytorch_Dataset']['dataset']</code> (e.g. MNIST). However there is no option to load a validation set this way, there is no <code>validation=True</code> option.</p>
<p>I could split the train dataset into train and validation with <code>torch.utils.data.random.split</code>, but there are two main problems with this approach:</p>
<ul>
<li>I don't want to save the folder <code>data_all_train</code>, I want to save only 2 folders, one with the true training part and one with the validation part</li>
<li>I would like PyTorch to understand if  <code>data_train</code> and <code>data_validation</code> are present, and in this case it should not download again <code>data_all_train</code>, even if not present</li>
</ul>
","2024-10-18 14:35:11","2","Question"
"79101636","78183024","","<p>u have to revert to torch 2.0.0 an tourchaudio 2.0.1</p>
<p>pip install torch==2.0.0 torchaudio==2.0.1 -f <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/torch_stable.html</a></p>
","2024-10-18 10:18:12","0","Answer"
"79101543","79021243","","<p>Perhaps building with LIBTORCH_STATIC=1 may help, see crate tch. When downloaded, libtorch shared objects are stored under torch-sys in your project build folder</p>
","2024-10-18 09:53:01","1","Answer"
"79101019","79082850","","<p>Just create a new conda environment and install everything from scratch. As to the cause of your issue, it could be that you were using the wrong python environment by accident, it could also be a failed pip upgrade. A lot of mistakes can lead to this outcome. Creating a new environment step by step from scratch can fix anything.</p>
","2024-10-18 07:26:24","2","Answer"
"79100969","79097867","","<p>In general it is not the practice to use gradients from self-play to backprop during training (for many reasons). It would be rather in-efficient to store gradients for later backprop. Plus there is exploration noise in self-play. Re-running is normal in RL training phase.</p>
<p>In self-play, you will likely use <code>eval mode</code> to be on-policy. Drop out is only used in training for regularization purpose. In a sense drop-out can be helpful for exploration, but I think more apt exploration is using <a href=""https://openai.com/index/better-exploration-with-parameter-noise/"" rel=""nofollow noreferrer"">parameter noise</a>.</p>
<p>I don't know about AlphaZero, but IMHO it makes less sense to store dropout noise. If you want to do that, use replay buffer to store drop activation which you capture using register_forward_hook.</p>
","2024-10-18 07:12:11","0","Answer"
"79100438","78953324","","<p>In <code>accelerate</code>, the proper way to control batch splitting is using <code>DataLoaderConfiguration</code> class. Here is an example:</p>
<pre><code>from accelerate.utils import DataLoaderConfiguration

dataloader_config = DataLoaderConfiguration(dispatch_batches=True, split_batches=False)
accelerator = accelerate.Accelerator(dataloader_config=dataloader_config)
</code></pre>
<p>In this example, batches given by your dataloader will be dispatched to different process. Each process has <strong>exactly one batch</strong> and batches from different processes are different. Because normally you won't be able to control which asynchronous process runs faster, you should expect the batches are dispatched <strong>randomly</strong> in practice.</p>
<p>By the way, <code>split_batches</code> param controls whether to split each batch given by your dataloader into smaller batches and dispatch each smaller batch to each process.</p>
","2024-10-18 03:19:28","0","Answer"
"79099612","","When I run without CUDA: Function ‘PowBackward0’ returned nan values in its 0th output","<p>My code was running fine <em>with</em> CUDA, but now that I run it with <code>device=&quot;cpu&quot;</code>, with the flag <code>torch.autograd.set_detect_anomaly(True)</code>, the runtime error is raised:</p>
<pre class=""lang-none prettyprint-override""><code>RuntimeError: Function 'PowBackward0' returned nan values in its 0th output.
</code></pre>
<p>Looking closely at the call stack:</p>
<pre><code> File &quot;&lt;ipython-input-468-be9e157834e4&gt;&quot;, line 83, in forward
    self.grad_mag = torch.sqrt(self.grad_x**2 + self.grad_y**2)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/_tensor.py&quot;, line 41, in wrapped
    return f(*args, **kwargs)
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:111.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
</code></pre>
<p>Indicating that the error is when backwarding through:</p>
<pre class=""lang-py prettyprint-override""><code>self.grad_mag = torch.sqrt(self.grad_x**2 + self.grad_y**2)
</code></pre>
<p>Firstly, I don't see why this issue is on CPU but not on CUDA. Secondly, I don't understand why it would get NaNs from the backward pass</p>
<pre class=""lang-py prettyprint-override""><code>print(&quot;grad_x: &quot;,self.grad_x.isinf().any(), self.grad_x.isnan().any())
print(&quot;grad_y: &quot;,self.grad_y.isinf().any(), self.grad_y.isnan().any())
self.grad_mag = torch.sqrt(self.grad_x**2 + self.grad_y**2)
print(&quot;grad mag &quot;, self.grad_mag.isinf().any(), self.grad_mag.isnan().any())
</code></pre>
<p>which outputs:</p>
<pre><code>grad_x:  tensor(False) tensor(False)
grad_y:  tensor(False) tensor(False)
grad mag  tensor(False) tensor(False)
</code></pre>
<p>If it makes any difference, I'm optimizing with LBFGS</p>
","2024-10-17 19:34:34","3","Question"
"79099337","79099138","","<p>basically the error</p>
<blockquote>
<p>Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</p>
</blockquote>
<p>Error comes when the backwards pass tries to access tensors that were saved for the backwards pass (using <code>ctx.save_for_backward</code>), and those are not present (usually because they were freed after doing the first backward pass witout<code>retain_graph=True</code>).</p>
<p>So the computation graph is still there after the first backwards pass, only the tensors saved in context were freed.</p>
<p>But the thing is, addition operations do not need to save tensors for backwards pass (the gradient along each of the inputs is the same as the gradient over the sum — so the gradient is just passed along the graph without doing any operation, no need to save anything for backward). Thus the error doesn't happen if the only shared node is an addition node.</p>
<p>In comparison, multiplication needs to save the input values for the backward pass (since the gradient for a * b along b is a * grad(a * b)). Thus the exception gets raised when it tries to access them</p>
","2024-10-17 18:06:07","3","Answer"
"79099138","","usage of retain graph in pytorch","<p>I get error if I don't supply <code>retain_graph=True</code> in <code>y1.backward()</code></p>
<pre><code>   import torch
   x = torch.tensor([2.0], requires_grad=True)
   y = torch.tensor([3.0], requires_grad=True)
   f = x+y
   z = 2*f
   y1 = z**2
   y2 = z**3
   y1.backward()
   y2.backward()
</code></pre>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/a0m08er/pytorch/pytorch_tutorial/tensor.py&quot;, line 58, in &lt;module&gt;
    y2.backward()
  File &quot;/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/_tensor.py&quot;, line 521, in backward
    torch.autograd.backward(
  File &quot;/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, line 289, in backward
    _engine_run_backward(
  File &quot;/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
</code></pre>
<p>But I don't get error when I do this:</p>
<pre><code>   import torch

   x = torch.tensor([2.0], requires_grad=True)
   y = torch.tensor([3.0], requires_grad=True)
   z = x+y
   y1 = z**2
   y2 = z**3
   y1.backward()
   y2.backward()
</code></pre>
<p>Since z is a common node for <code>y1</code> and <code>y2</code> why it is not showing me error when I do <code>y2.backward()</code></p>
","2024-10-17 16:55:58","3","Question"
"79097867","","How to do backpropagation in PyTorch when training AlphaZero?","<p>I'm trying to implement my version of AlphaZero for Connect Four. I have implemented a convolutional network using PyTorch and can get (random) value- and policy outputs from the model for given boardstates. Now I would like to simulate some games and train the model using them. However, I have encountered a problem:</p>
<p>As far as I understand, the training consists of basically two steps: a step in which selfplay is used to gather game data, and after that a step where the collected data is used to train the model using backpropagation.
In the selfplay step, the network is used to get an evaluation of a position and a policy on how to choose the next move. The policy is then improved upon using a version of the MCTS algorithm.</p>
<p>After a game is finished, all the moves and the result is saved.</p>
<p>For simplicity, assume that I only play a single game and then want to update the model. If I save the MCTS policies and the network policies I can now calculate the loss. But I can't backpropagate through the model, since the forward pass happened during the collection step. I could in theory forward the same position through the model again, but that sounds not only inefficient, but since my architecture uses dropout layers I would not even get the same results.</p>
<p>So how can I solve this problem in PyTorch? Can I somehow save a model together with the dropout configuration that was used to create a policy? Then I could at least just forward the position again and use backprop afterwards, even if that would be inefficient.</p>
","2024-10-17 11:31:15","0","Question"
"79096079","78337397","","<p>You can use GPU acceleration. If you have an NVIDIA GPU, use GPU acceleration python libraries. This is how I train all of my AI models, whether chess related or just fun projects.</p>
","2024-10-16 23:44:51","0","Answer"
"79095526","78929592","","<p>Upgrade torch to 2.4.1 can solve it.</p>
<p><code>pip install --upgrade torch torchvision</code></p>
","2024-10-16 19:16:26","1","Answer"
"79095245","79095041","","<p>This is probably due to the isolation mechanism of the pip building process.</p>
<p>Basically, the installation requires <code>torch</code> to be installed to work, but recent versions of <code>pip</code> use some isolation that does not allow the build process to access installed packages.</p>
<p>You can disable that isolation by using this command:</p>
<pre class=""lang-bash prettyprint-override""><code>$ pip install --no-build-isolation 'git+https://github.com/facebookresearch/detectron2.git'
</code></pre>
<p>This happens a lot for packages that need torch, probably they tend to import it to check for cuda and/or other capabilities and/or to compile some kernels.</p>
","2024-10-16 17:45:24","20","Answer"
"79095041","","detectron2 installation - No module named 'torch'","<p>I am trying to install detectron2 on ubuntu and face a weird python dependency problem. In short - pytorch is installed (with pip), torchvision is installed (with pip), but when I run</p>
<pre><code>pip install 'git+https://github.com/facebookresearch/detectron2.git'
</code></pre>
<p>I get error ModuleNotFoundError: No module named 'torch'</p>
<p>as for dependencies</p>
<pre><code>(detectron2_test) ubuntu@LAPTOP:~$ pip install torchvision
Requirement already satisfied: torchvision in ./detectron2_test/lib/python3.12/site-packages (0.19.1+cu118)
Requirement already satisfied: numpy in ./detectron2_test/lib/python3.12/site-packages (from torchvision) (1.26.3)
Requirement already satisfied: torch==2.4.1 in ./detectron2_test/lib/python3.12/site-packages (from torchvision) (2.4.1+cu118)
(...)

(detectron2_test) ubuntu@LAPTOP:~$ which pip
/home/ubuntu/detectron2_test/bin/pip
(detectron2_test) ubuntu@LAPTOP:~$ which python
/home/ubuntu/detectron2_test/bin/python
(detectron2_test) ubuntu@LAPTOP:~$ which python3
/home/ubuntu/detectron2_test/bin/python3
</code></pre>
<p>Any suggestions are appreciated!</p>
","2024-10-16 16:46:40","4","Question"
"79092697","78906762","","<p>I met the same issue using pytorch=2.4.1 and tensorboard=2.18. But it works well in a conda environment with pytorch=1.12.1 and tensorboard=2.10. Considering comment from Sachin Hosmani that pyTorch=2.3.1 and tensorBoard=2.17.0 work fine, I think there might be some bugs in pytorch 2.4.</p>
<p>The written log files don't have bugs. Just use the an old version environment to start the tensorboard server and the values can be displayed normally.</p>
","2024-10-16 06:21:14","1","Answer"
"79087282","78967925","","<blockquote>
<p>I could think of two ways: implementing a custom DataLoader(need advice on this too) or just create separate DataLoaders for each bin and while iterating with bins in the outermost loop, grab the corresponding DataLoader and do training. Will the latter method have some serious downsides?</p>
</blockquote>
<ul>
<li><p>Second option definitely is easier, the only problem I can see is if you use several workers <em>ie</em> if you use multiprocessing:
what is going to happen is that each of those workers will spawn its own worker processes. Thus you will end up with a bunch of processes, most of them idle (those from the dataloader you are not using). If you have a lot of these dataloaders, then it might be a performance problem. <br />
<em>If you are not planning on using multiprocessing though, I would probably go with that</em></p>
</li>
<li><p>First option is going to be way harder: <a href=""https://github.com/pytorch/pytorch/blob/929797dedbf23376123ce95230c01a7e3b71e130/torch/utils/data/dataloader.py#L130"" rel=""nofollow noreferrer"">here is the code for dataloader</a>, you would want to make a subclass of that, but it gets pretty technical</p>
</li>
<li><p>There is also  a third option, which is <strong>much simpler, although somewhat hacky</strong> to use: *make your <code>Dataset</code> class handle the batching, instead of using the <code>Dataloader</code>'s collate capability:
Since your Dataset is ordered, just make it iterate over batches, and set <code>collate=None</code> in the dataloader.</p>
</li>
<li><p>Finally a fourth option, <strong>the cleanest option is to use the <code>batch_sampler</code></strong> capability of the torch dataloader (<a href=""https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler"" rel=""nofollow noreferrer"">see documentation</a>). Just build a Sampler yielding the indices of the batches: here is a version that follows your example:</p>
</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data import Sampler
from typing import List
from copy import copy

class BinSampler(torch.utils.data.Sampler[List[int]]):
   bin_sizes: List[int]
   batch_size: int
   def __init__(self, bin_sizes: List[int], batch_size: int):
      self.bin_sizes = bin_sizes
      self.batch_size = batch_size
   
   def __len__(self):
      return sum(self.bin_sizes)

   def __iter__(self):
      bin_sizes = copy(self.bin_sizes)
      remaining_in_bin = bin_sizes.pop()
      current_index = 0
      while bin_sizes or remaining_in_bin:
         if remaining_in_bin == 0:
            remaining_in_bin = bin_sizes.pop()
         if remaining_in_bin &lt; self.batch_size:
            n_to_yield = remaining_in_bin
         else: n_to_yield = self.batch_size

         next_index = current_index + n_to_yield

         yield range(current_index, next_index)
         remaining_in_bin -= n_to_yield
         current_index = next_index



dataloader = torch.utils.data.DataLoader(
   dataset = your_dataset,
   batch_sampler = BinSampler([10, 20, 30,], batch_size=8)
)

</code></pre>
","2024-10-14 18:14:35","0","Answer"
"79087120","78691335","","<p>Use this code instead.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import matplotlib.pyplot as plt
import gudhi as gd
from scipy.ndimage import distance_transform_edt

def compute_persistence_diagram(binary_mask):
    global persistence
    distance = distance_transform_edt(binary_mask)
    cubical_complex = gd.CubicalComplex(dimensions=distance.shape, top_dimensional_cells=distance.flatten())
    persistence = cubical_complex.persistence()
    plot_persistence_diagram(persistence)
    
def plot_persistence_diagram(persistence):
    plt.figure(figsize=(6, 6))
    for dim, birth_death in persistence:
        if birth_death[1] != float('inf'):
            plt.scatter(birth_death[0], birth_death[1], label=f&quot;H{dim}&quot;, alpha=0.6)
        else:
            plt.scatter(birth_death[0], birth_death[1], label=f&quot;H{dim}&quot;, alpha=0.6, marker='x')
    max_birth_death = max(max(birth for _, (birth, _) in persistence), max(death for _, (_, death) in persistence if death != float('inf')))
    plt.plot([0, max_birth_death], [0, max_birth_death], 'k--')
    plt.xlabel(&quot;Birth&quot;)
    plt.ylabel(&quot;Death&quot;)
    plt.title(&quot;Persistence Diagram&quot;)
    plt.show()

compute_persistence_diagram(mask_pred)
</code></pre>
","2024-10-14 17:20:41","0","Answer"
"79085052","79082850","","<p>Restarting the kernel after upgrading both transformers and accelerate solved it for me.</p>
","2024-10-14 07:08:52","1","Answer"
"79084891","79081359","","<p>The documentation states that <code>multilabel_confusion_matrix</code> is not what you are looking for :</p>
<p><em>The multilabel_confusion_matrix calculates class-wise or sample-wise multilabel confusion matrices, and in multiclass tasks, labels are binarized under a one-vs-rest way</em></p>
<p>Instead, I believe you want to use <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix"" rel=""nofollow noreferrer"">confusion_matrix</a>, the documentation of which states that it returns :</p>
<p><em>A Confusion matrix of shape (n_classes, n_classes) whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.</em></p>
","2024-10-14 05:59:26","0","Answer"
"79082977","79075777","","<p>You need to zero-out the gradient after reading it with <code>deform_verts.grad.zero_()</code> . The <code>backward</code> method actually accumulates gradients but you just want to retain the graph, not the previous gradients</p>
","2024-10-13 11:09:03","1","Answer"
"79082850","","ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0` but I have version already installed 1.0.1","<p>I'm trying to run traning this way but face import error, how to fix it:</p>
<pre><code>args = TrainingArguments(output_dir=&quot;finetuned&quot;,
                          num_train_epochs=10,
                          per_device_train_batch_size=16,
                          save_steps=10000,
                          gradient_accumulation_steps = 2,
                          warmup_steps=500,
                         lr_scheduler_type=&quot;polynomial&quot;,
                         fp16=True,
                         )

trainer = Trainer(
    model = model,
    args = args,
    train_dataset = train_dataset,
    eval_dataset = test_dataset,
    tokenizer = tokenizer,

)


trainer.train()
</code></pre>
<p>However it gives me such error:</p>
<pre><code>File ~\mambaforge\lib\site-packages\transformers\training_args.py:1750, in TrainingArguments.__post_init__(self)
   1748 # Initialize device before we proceed
   1749 if self.framework == &quot;pt&quot; and is_torch_available():
-&gt; 1750     self.device
   1752 if self.torchdynamo is not None:
   1753     warnings.warn(
   1754         &quot;`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use&quot;
   1755         &quot; `torch_compile_backend` instead&quot;,
   1756         FutureWarning,
   1757     )

File ~\mambaforge\lib\site-packages\transformers\training_args.py:2250, in TrainingArguments.device(self)
   2246 &quot;&quot;&quot;
   2247 The device used by this process.
   2248 &quot;&quot;&quot;
   2249 requires_backends(self, [&quot;torch&quot;])
-&gt; 2250 return self._setup_devices

File ~\mambaforge\lib\site-packages\transformers\utils\generic.py:60, in cached_property.__get__(self, obj, objtype)
     58 cached = getattr(obj, attr, None)
     59 if cached is None:
---&gt; 60     cached = self.fget(obj)
     61     setattr(obj, attr, cached)
     62 return cached

File ~\mambaforge\lib\site-packages\transformers\training_args.py:2123, in TrainingArguments._setup_devices(self)
   2121 if not is_sagemaker_mp_enabled():
   2122     if not is_accelerate_available():
-&gt; 2123         raise ImportError(
   2124             f&quot;Using the `Trainer` with `PyTorch` requires `accelerate&gt;={ACCELERATE_MIN_VERSION}`: &quot;
   2125             &quot;Please run `pip install transformers[torch]` or `pip install 'accelerate&gt;={ACCELERATE_MIN_VERSION}'`&quot;
   2126         )
   2127 # We delay the init of `PartialState` to the end for clarity
   2128 accelerator_state_kwargs = {&quot;enabled&quot;: True, &quot;use_configured_state&quot;: False}

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate&gt;={ACCELERATE_MIN_VERSION}'`
</code></pre>
<p>but I have right version:</p>
<pre><code>import accelerate    
accelerate.__version__

&gt; '1.0.1'


transformers.__version__ 
&gt; transformers
</code></pre>
","2024-10-13 09:45:30","2","Question"
"79082185","79081885","","<p>It is stated in the <a href=""https://pytorch.org/get-started/locally/#windows-python"" rel=""nofollow noreferrer"">docs</a> that:</p>
<blockquote>
<p>PyTorch on Windows only supports Python 3.8-3.11.</p>
</blockquote>
<p>However on <a href=""https://pypi.org/project/torch/#data"" rel=""nofollow noreferrer"">PyPi</a>, The stable release of torch (i.e. version 2.4.1)  supports Python 3.8 - 3.12. There are wheels for Python 3.12 Windows as can be seen <a href=""https://pypi.org/project/torch/#files"" rel=""nofollow noreferrer"">here</a>.</p>
<p>However, there is no support yet for Python 3.13. To use this package, downgrade to a supported version perhaps Python 3.12.</p>
","2024-10-13 00:26:20","2","Answer"
"79081885","","Unable to install Torch using pip","<p>I using Windows</p>
<p>I installed Python 3.13</p>
<p>I am trying to <code>pip install torch</code></p>
<p>However, when attempting to do so, I encounter this error.</p>
<pre><code>F:\Kit&gt;pip install torch
ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch
</code></pre>
<p>What is the issue and how can it be resolved?</p>
","2024-10-12 20:37:14","2","Question"
"79081359","","How to Create a Multilabel Confusion Matrix for 14 Disease Classes in PyTorch?","<p>I’m working on a multilabel classification task with 14 different disease classes. I’ve trained my model, and I want to generate a single multilabel confusion matrix where both the x-axis and y-axis represent the 14 classes.</p>
<p>However, when I try to generate the confusion matrix using my current code, it creates a separate confusion matrix for each class. Instead, I would like a unified confusion matrix where the true labels and predicted labels are across the same 14 classes on both axes.</p>
<p>Here are the key details of my setup:</p>
<ul>
<li>I’m using PyTorch.</li>
<li>I have a trained model and access to test_loader.</li>
<li>I’ve successfully used my model for prediction, but I'm stuck on how to aggregate the results into a single confusion matrix for multilabel classification.</li>
</ul>
<p>Here’s the code I'm using to generate the confusion matrix:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import multilabel_confusion_matrix

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

# Disable gradient calculation for inference
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        # Forward pass to get model predictions
        outputs = best_model(images)

        # Store true labels and predicted probabilities
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(outputs.cpu().numpy())  # Get the raw output (probabilities)

# Convert to numpy arrays for easier manipulation
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Apply thresholding to convert probabilities to binary predictions
binary_predictions = (all_predictions &gt; 0.5).astype(int)

# Compute the multilabel confusion matrix
confusion_mtx = multilabel_confusion_matrix(all_labels, binary_predictions)

# Function to plot the multilabel confusion matrix
def plot_multilabel_confusion_matrix(confusion_mtx, class_names):
    num_classes = confusion_mtx.shape[0]
    ncols = 3  # Set the number of columns for the plot
    nrows = (num_classes + ncols - 1) // ncols  # Calculate the number of rows needed

    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 4, nrows * 4))
    axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration

    for i in range(num_classes):
        ax = axes[i]
        ax.matshow(confusion_mtx[i], cmap=plt.cm.Blues, alpha=0.5)
        ax.set_xlabel('Predicted')
        ax.set_ylabel('True')
        ax.set_title(class_names[i])

        # Set x and y axis ticks to show &quot;Positive&quot; first and &quot;Negative&quot; second
        ax.set_xticks([0, 1])
        ax.set_xticklabels(['Positive', 'Negative'])  # Positive first
        ax.set_yticks([0, 1])
        ax.set_yticklabels(['Positive', 'Negative'])  # Positive first

        # Show the counts
        for j in range(confusion_mtx[i].shape[0]):
            for k in range(confusion_mtx[i].shape[1]):
                ax.text(k, j, confusion_mtx[i][j, k], ha='center', va='center')

    # Hide any unused subplots
    for i in range(num_classes, len(axes)):
        axes[i].axis('off')

    plt.tight_layout()
    plt.show()

# Plot the multilabel confusion matrix
plot_multilabel_confusion_matrix(confusion_mtx, class_names)
</code></pre>
<p>What I get is 14 separate confusion matrices, but I need a single confusion matrix with all 14 classes represented on both axes.</p>
<p><strong>Here’s an image of what I’m getting:</strong>
<a href=""https://i.sstatic.net/tCcN0O0y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tCcN0O0y.png"" alt=""enter image description here"" /></a></p>
<p><strong>And here’s what I want:</strong>
<a href=""https://i.sstatic.net/Ib8TwrWk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ib8TwrWk.png"" alt=""enter image description here"" /></a></p>
","2024-10-12 15:54:02","0","Question"
"79078946","78899566","","<p>I run FLUX.1-dev using <strong>CPU alone</strong>, with <strong>NO GPU</strong> and on an old rack server at home over <strong>PUtty</strong> via <strong>ssh</strong>, namely <em>an obsolete elderly potato</em>, and generating images works just fine for me.</p>
<p><em>CAVEAT. SADLY, YOU MAY NOT BE ABLE TO ADD 192GB TO YOUR PC as I have in my old rack server but that's life, although I have heard, unconfirmed, that any home desktop machine with 64GB is usable.</em></p>
<p><strong>NOTE. There are TRICKS to getting FLUX, both <em>-schnell</em> and <em>-dev</em>, to work on ANY PC using CPU alone.</strong></p>
<p>That 'weird issue on terminal' is A PROGRESS BAR and it is supposed to look like that. If your CPUs and system is slow like mine it will not change for a long time.</p>
<p>If you'd have left it alone, perhaps in a few days it might have produced an image.</p>
<p>Changes to your Python program that might help;</p>
<ul>
<li><p><strong>REMOVE the <em>pipe.enable_sequential_cpu_offload()</em> line</strong> as this is a work-around for a hardware limitation in CUDA GFX Cards being used as Math Accelerators since they lack the VRAM to do real work and the 80GB cards are, as of 2024, out of the reach of mere mortals.</p>
</li>
<li><p><strong>change <em>torch_dtype=torch.bfloat16</em> to <em>torch_dtype=torch.float32</em></strong> which is <strong>counter-intuitive</strong> as 32bit floats would seem to be slower than all-new-and-shiny <em>bfloat16s</em> or even <em>float16</em> BUT on old potatoes geared up to <em>float64</em> and <em>float32</em> not every CPU has <strong>F16C</strong> nor <strong>AVX-512</strong> capabilities and so <strong>EVERY SINGLE ONE OF THOSE <em>bfloat16</em> calculations must be converted (time consuming) into a floating-point number your system DOES recognize THEN back again to the <em>bfloat16</em> format</strong> which kills performance. Check your CPU capabilities and you'll likely find <em>bfloat16</em> and even <em>float16</em> aren't for you. If a CPU upgrade is not possible then just use <strong>float32</strong> and you'll find it works at least at visible human speeds. My elderly Xeon CPUs, for instance, don't even have <strong>AVX</strong> nor <strong>AVX2</strong> and <em>FLUX-1.dev</em> and <em>FLUX.1-schnell</em> works with <strong>float32</strong> just fine albeit slower on my system; 5 mins per image with a visibly moving text progress bar.</p>
</li>
<li><p><strong>DO NOT GO MAD WITH IMAGE SIZES AND INFERENCE STEPS</strong> so 1024x768 in 6 to 8 steps is often usable</p>
</li>
</ul>
<p>Be reasonable and don't ask for the Earth and FLUX.1-dev and FLUX.1-schnell are more than usable on home systems.</p>
<p>It would appear the developers of FLUX.1 assumed that everyone has a brand new CPU in their tricked-out home supercomputer and a brand new $100,000 accelerator card plugged in as an afterthought, likely next to their Ferrari and their private yacht, which in 2024 is likely not the case.</p>
<p>So the code to get <em>FLUX.1-schnell</em> to run on <strong>JUST CPUs</strong> becomes;</p>
<pre><code>import torch
from diffusers import FluxPipeline

DEVICE = &quot;cpu&quot;

print(&quot;Creating Pipeline...&quot;)

#
# FOR WINDOWS USERS
# 
pipe = FluxPipeline.from_pretrained(&quot;C:\\Python\\Projects\\test\\flux1schnell&quot;, torch_dtype=torch.float32).to(torch.device(DEVICE))

#
# FOR LINUX USERS
#
#pipe = FluxPipeline.from_pretrained(&quot;black-forest-labs/FLUX.1-schnell&quot;, torch_dtype=torch.float32).to(torch.device(DEVICE))

prompt = f&quot;beach ball with a sign saying {DEVICE}-ONLY&quot;

print(&quot;Generating Images...&quot;)

image = pipe(
    prompt=prompt,
    height=512, width=512,
    guidance_scale=3.5, num_inference_steps=5, max_sequence_length=256,
    output_type=&quot;pil&quot;, num_images_per_prompt=1,
    generator=torch.Generator(DEVICE).manual_seed(0)
).images

print(&quot;Output Images...&quot;)

for i, img in enumerate(image):
    print(f&quot;Saving image {i}...&quot;)
    img.save(f&quot;{prompt}_on_{DEVICE}_{i}.png&quot;)

print(&quot;Done.&quot;)
</code></pre>
<p>Luckily, the AI revolution is open to everyone if you know the way to get it to work.</p>
","2024-10-11 15:51:47","2","Answer"
"79075777","","How can I run backward() on individual image pixels without causing an error of trying to backward through the graph a second time?","<p>I have this code</p>
<pre class=""lang-py prettyprint-override""><code>...
vertex_id = 275

deform_verts.retain_grad() # input
predicted_silhouette.retain_grad() # output

impact_img = torch.zeros_like(predicted_silhouette, requires_grad=False)
for i in range(image_size):
  for j in range(image_size):
      pixel = predicted_silhouette[i][j]
      pixel.retain_grad()
      pixel.backward()
      impact = deform_verts.grad[vertex_id] 
      impact_img[i][j] += impact.sum()


plt.imshow(impact_img.detach().cpu().numpy())
</code></pre>
<p>I'm trying to create an image based off how a single entry of deform_verts affects the entire image. For this purpose, I go through every pixel of the output image and call <code>.backward()</code> and insert its gradient into a new image for visualization purposes. However, when I call <code>backward()</code> on a first pixel, I cannot call it on a second one because I suspect intermediate variables have been used in the backpropagation already. I tried to use <code>retain_graph</code> but I don't think this does what I want since the image looks far from what I expected.</p>
","2024-10-10 18:53:43","0","Question"
"79074028","79073506","","<p>When you use ReduceLROnPlateau with mode='min', the learning rate will be reduced when the monitored quantity does not decrease. Since you monitor accuracy, which you want to increase, you should use mode='max'.</p>
","2024-10-10 10:46:10","0","Answer"
"79073984","77934105","","<p>Use <code>model.reset_classifier(0)</code>, to remove the existing head.</p>
","2024-10-10 10:35:24","0","Answer"
"79073698","79073176","","<p>You could deifine <code>__getitem__()</code> as:</p>
<pre><code>def __getitem__(self, idx):
    return {
       'interactions': self._data[idx], 
       'user_features': self._user_features[self._data[idx]['user']] if self._user_features else None
        }
</code></pre>
<p>Is a little bit faster. But is not a definitive solution.</p>
","2024-10-10 09:31:27","0","Answer"
"79073506","","why my PyTorch scheduler doesn't seem to work properly?","<p>I'm trying to train a mobileNetV3Large with a simple PyTorch Scheduler.
This is the portion of the code responsible for training:</p>
<pre><code>bench_val_loss = 1000
bench_acc = 0.0
epochs = 15
optimizer = optim.Adam(embeddingNet.parameters(), lr=1e-3) 
loss_optimizer = torch.optim.Adam(loss_fn.parameters(), lr=1e-3)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, threshold=0.02)

for epoch in range(1, epochs + 1):

    print(f'current lr: {scheduler.get_last_lr()}')
    loss=train(embeddingNet, loss_fn, device, train_dataloader, optimizer, loss_optimizer, epoch)
    val_loss, accuracy =test(train_dataset, val_dataset, embeddingNet, accuracy_calculator, loss_fn, epoch, val_dataloader)
    #val_loss = simpleTest(train_dataset, val_dataset, embeddingNet, accuracy_calculator, loss_fn, epoch, val_dataloader)

    
    torch.save(embeddingNet.state_dict(), 'my/path/mobileNetV3L_ArcFaceLAST.pth')

    if accuracy &gt;= bench_acc:
      bench_val_loss = val_loss
      torch.save(embeddingNet.state_dict(), 'my/path/mobileNetV3L_ArcFaceBEST.pth')

    scheduler.step(accuracy)

    writer.add_scalars('Training vs. Validation Loss',
                       {'Training': loss, 'Validation': val_loss},
                       global_step=epoch+1)
</code></pre>
<p>And here you'll find the first 7 training logs</p>
<pre><code>Test set accuracy (Precision@1) = 0.17834772304046048
current lr: [0.001]
Epoch 3: Loss = 39.68284225463867
Epoch 3: valLoss = 39.9765007019043
100%|██████████| 962/962 [01:43&lt;00:00,  9.28it/s]
100%|██████████| 370/370 [00:41&lt;00:00,  8.92it/s]
Computing accuracy
Test set accuracy (Precision@1) = 0.31242593533096324
current lr: [0.001]
Epoch 4: Loss = 39.4412841796875
Epoch 4: valLoss = 39.67761562450512
100%|██████████| 962/962 [01:45&lt;00:00,  9.11it/s]
100%|██████████| 370/370 [00:41&lt;00:00,  8.86it/s]
Computing accuracy
Test set accuracy (Precision@1) = 0.3633824276282377
current lr: [0.001]
Epoch 5: Loss = 39.09823989868164
Epoch 5: valLoss = 39.54649614901156
100%|██████████| 962/962 [01:42&lt;00:00,  9.37it/s]
100%|██████████| 370/370 [00:41&lt;00:00,  8.87it/s]
Computing accuracy
Test set accuracy (Precision@1) = 0.44244117149145085
current lr: [0.001]
Epoch 6: Loss = 38.70449447631836
Epoch 6: valLoss = 39.1865906792718
100%|██████████| 962/962 [01:45&lt;00:00,  9.15it/s]
100%|██████████| 370/370 [00:39&lt;00:00,  9.25it/s]
Computing accuracy
Test set accuracy (Precision@1) = 0.5167597765363129
current lr: [0.0001]
</code></pre>
<p>I can't figure out why the scheduler decided to reduce the learning rate even thought the accuracy was increasing more quickly than the threshold.</p>
<p>Where is the error?</p>
","2024-10-10 08:51:32","0","Question"
"79073176","","Speeding up Dataset.__getitems__","<p>I have a model with a <code>forward</code> function that receives optional parameters, like this:</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(nn.Module):
  ...

  def forward(self, interactions: torch.Tensor, user_features: Optional[torch.Tensor] = None):
    &quot;&quot;&quot;
      Where _N_ is the number of items

      interactions (Tensor): Nx2
      user_features (Tensor): Nx(number of features)
    &quot;&quot;&quot;
    ...
</code></pre>
<p>For this reason, my <code>Dataset</code> also returns a dict, based on a DataFrame</p>
<pre class=""lang-py prettyprint-override""><code>class StackOverflowDataset(torch.utils.data.Dataset):
    def __init__(self, data, user_features=None):
        self._data = data
        self._user_features = user_features

    def __getitem__(self, idx):
        if self._user_features is None:
            return {'interactions': self._data[idx]}
        else:
            return {
               'interactions': self._data[idx], 
               'user_features': self._user_features[self._data[idx]['user']]
            }

    def __len__(self):
        return len(self._data)
</code></pre>
<p>Most of the training time is spent on <code>__getitem__</code>, which makes me wonder: is having optional arguments on <code>MyModel.forward</code> bad practice? I guess most of the time is spent playing with numpy <em>item by item</em>, and then converting it to PyTorch's tensors and moving it to the GPU. Is there any way I can &quot;pre-process&quot; all of this beforehand but still use a <code>Dataloader</code> that returns a dict?</p>
<p>It seems that <a href=""https://pytorch.org/data/beta/dp_tutorial.html"" rel=""nofollow noreferrer"">Datapipe</a> also goes row by row. Would it be possible to directly return a dict with the whole 'interactions' tensor, and then the <code>Dataloader</code> slices it up?</p>
<blockquote>
<p><strong>EDIT:</strong> It seems DataPipe will be deprecated</p>
</blockquote>
<p><a href=""https://i.sstatic.net/xFQCEyui.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFQCEyui.png"" alt=""profiling of my model"" /></a></p>
","2024-10-10 07:23:08","0","Question"
"79072141","79071235","","<ol>
<li><p>When concatenating the input tensor, ensure that the dimensions match. Instead of torch.cat([input] * bs), you might need to adjust the dimensions explicitly:
<code>input = input.unsqueeze(0).repeat(bs, 1, 1, 1)</code></p>
</li>
<li><p>The error suggests a mismatch between the expected and actual tensor sizes. Ensure that the output of <code>self.unet</code> matches the expected dimensions.</p>
</li>
<li><p>Ensure that the scheduler’s timesteps and alpha values are correctly handled for batch processing. The <code>alphas_cumprod</code> values should be broadcasted to match the batch size</p>
</li>
<li><p>Debug the tensor shapes by adding <code>print</code> statements to check the shapes of tensors at different stages in your code, you might find where the mismatch occurs</p>
</li>
</ol>
","2024-10-09 21:38:26","0","Answer"
"79071235","","An error occurs during the execution of UNet when the batch size is not equal to 1","<p>I'm trying to run a Stable Diffusion model using the code provided in the DDIM Inversion tutorial. However, when the input's batch size is set to a value greater than 1 (e.g., 32), I encounter the following error:</p>
<pre><code>RuntimeError: The size of tensor a (131072) must match the size of tensor 
b (4096) at non-singleton dimension 1.
</code></pre>
<p>It appears that 131072 is maybe derived from 32 x 4096, indicating a mismatch in tensor dimensions. The specific line where the error occurs is:</p>
<pre><code>noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample
</code></pre>
<p>Here’s the relevant portion of my code for the inversion process:</p>
<pre><code>## Inversion (https://github.com/huggingface/diffusion-models-class/blob/main/unit4/01_ddim_inversion.ipynb)
    def invert_process(self, guidance_scale, input, denoise_kwargs):

        pred_images = []
        pred_latents = []
        
        decode_kwargs = {'vae': self.vae}

        # Reversed timesteps &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
        timesteps = reversed(self.scheduler.timesteps)
        num_inference_steps = len(self.scheduler.timesteps)

        with torch.no_grad():
            for i in tqdm(range(0, num_inference_steps)):

                t = timesteps[i]
                self.cur_t = t.item()
                
                # For text condition on stable diffusion
                if 'encoder_hidden_states' in denoise_kwargs.keys():
                    bs = denoise_kwargs['encoder_hidden_states'].shape[0]
                    input = torch.cat([input] * bs)

                # Predict the noise residual
                noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample
                noise_pred = noisy_residual

                # For text condition on stable diffusion
                if noisy_residual.shape[0] == 2:
                    # perform guidance
                    noise_pred_text, noise_pred_uncond = noisy_residual.chunk(2)
                    noisy_residual = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                    input, _ = input.chunk(2)

                current_t = max(0, self.cur_t - (1000//num_inference_steps)) #t
                next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1
                alpha_t = self.scheduler.alphas_cumprod[current_t].to(self.device)
                alpha_t_next = self.scheduler.alphas_cumprod[next_t].to(self.device)

                latents = input

                # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)
                # Add noise to latents


                latents = (latents - (1-alpha_t).sqrt()*noise_pred)*(alpha_t_next.sqrt()/alpha_t.sqrt()) + (1-alpha_t_next).sqrt()*noise_pred
                
                input = latents
                
                pred_latents.append(latents)
                pred_images.append(decode_latent(latents, **decode_kwargs))
                
        return pred_images, pred_latents

</code></pre>
<p>What could be causing the tensor size mismatch when the batch size is greater than 1? How can I resolve this issue while maintaining a batch size greater than 1 in the model?</p>
<p>I attempted to change the size of t to be a tensor with shape (batch size,).</p>
<p>Additionally, I confirmed that the model works correctly when the batch size is 1.</p>
","2024-10-09 16:32:07","0","Question"
"79070020","79067001","","<p>I found out why it downloads the dataset and posted about the issue <a href=""https://github.com/ultralytics/ultralytics/issues/9112#issuecomment-2402035186"" rel=""nofollow noreferrer"">on their github page</a>. It's in my eyes just bad design. They have overwritten the <code>model.train()</code> call (which normally just switches the model into/out of training mode) with their own, which starts training now and by default downloads the coco dataset. But they do not handle what happens if another method like <code>model.eval()</code> calls <code>model.train()</code>..</p>
<p>I'm not sure how you want to run your model, but a solution could be to not call <code>model.eval()</code> and uses the model to predict like this:</p>
<pre class=""lang-py prettyprint-override""><code>image_processor = ImageProcessor()
with torch.no_grad():
    prediction = image_processor._detector(data)
</code></pre>
<p>The <code>torch.no_grad()</code> disables gradient computation, much like <code>.eval()</code> normally does. But only inside its context. Then the <code>detector(data)</code> call starts a normal prediction step without gradients.<br />
You can also wrap that inside a function if you want. When using a function/method, you could also use the <code>@torch.no_grad()</code> decorator for it, it should do the same.</p>
","2024-10-09 11:32:23","0","Answer"
"79069263","78500154","","<p>The namespace clash with custom files (as suggested by @Maximillion) is unlikely because then the first snippet would have already failed.</p>
<p>The likely cause is a <strong>broken Python environment</strong>, like <a href=""https://github.com/pytorch/pytorch/issues/77487"" rel=""nofollow noreferrer"">conflicting parallel installations</a>.</p>
<p>With properly installed packages, your code works - <a href=""https://colab.research.google.com/"" rel=""nofollow noreferrer"">try on Colab</a>!
Therefore, I recommend installing your packages (in fact, <code>torch</code> is enough) within a fresh virtual environment and give it another try.</p>
","2024-10-09 08:21:27","0","Answer"
"79067879","79067365","","<p>If you take a look <a href=""https://pypi.org/project/torch/1.9.1/"" rel=""nofollow noreferrer"">here</a> at the release info for version <code>1.9.1</code> of torch, there is a table for build status. The table does not include Python version <code>3.9</code> and above. This version of torch is old and does not support more recent versions of Python.</p>
<p>Additionally, if you look at your error, it shows the available versions of torch currently available for your version of Python that your package manager could find (which does not include <code>1.9.1</code> because it is not supported). Consider installing one of those versions or downgrading your Python version.</p>
","2024-10-08 21:46:38","0","Answer"
"79067365","","How do I install torch==1.9.1 in VS Code?","<p>I'm trying to install the libraries from the <code>requirements.txt</code> that looks like this:</p>
<pre><code>torch==1.9.1
torchvision==0.10.1
scikit-learn==0.24.1
numpy==1.20.1
matplotlib==3.3.4
tqdm==4.59.0
</code></pre>
<p>This is the error that I get:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch==1.9.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1)
ERROR: No matching distribution found for torch==1.9.1
</code></pre>
<p>I've tried installing torch with python 3.9, 3.10, 3.11 and 3.12 and the issue still persists.</p>
","2024-10-08 18:34:57","0","Question"
"79067001","","model.eval on pretrained YOLOv8 causes dowloading of datasets","<p>I want to use pre-trained YOLOv8n. So after importing model I want to set it to eval mode (<code>model.eval</code>). However, instead of running the program as usual (which was the case when I didnt pu <code>model.eval</code>) it dowloads big dataset files.</p>
<p>Here are the folders and files that are dowloaded alongside code:
<a href=""https://i.sstatic.net/F0rdj1TV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F0rdj1TV.png"" alt=""datasets folder and runs folder with its contents are being dowloaded"" /></a></p>
<p>The files are so big that it takes forever to unzip.
Here is the code:</p>
<pre><code>from ultralytics import YOLO
import torch
import logging

from detector.app import App
from detector.video_manager import VideoManager

class ImageProcessor:
    def __init__(self, parent_app: App, video_manager: VideoManager) -&gt; None:
        logging.getLogger(&quot;ultralytics&quot;).setLevel(logging.CRITICAL)

        self._parent_app = parent_app
        self._video_manager = video_manager

        self._detector = YOLO('yolov8n.pt')
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self._detector.to(device)

        print('Before eval')
        self._detector.eval()
        print('After eval') # does not execute - waits until everything is unzipped.
</code></pre>
<p>I do not think dowloading of these files is a normal behaviour.
Can you help?</p>
","2024-10-08 16:43:04","0","Question"
"79065515","79061201","","<p>Use Ultralytics <a href=""https://docs.ultralytics.com/models/rtdetr/#usage-examples"" rel=""nofollow noreferrer""><strong>RTDETR</strong></a> module instead of YOLO to operate with the RT-DETR model:</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import RTDETR

# Load a model
model = RTDETR(r&quot;/content/drive/MyDrive/LN/best.pt&quot;)

# Display model information (optional)
model.info()

image_path = r&quot;/content/drive/MyDrive/LN/114_jpg.rf.6b9c3febc395a3968078eec417f1c9ed.jpg&quot;

# Run inference
results = model(image_path)
</code></pre>
","2024-10-08 10:23:45","1","Answer"
"79063289","79061427","","<p>It is not possible to do that directly: Pytorch handles gradient computation per tensor, so the whole tensor weights will either have their gradient computed or not. You basically have 3 solutions:</p>
<ol>
<li><p>Make a custom <code>Embedding</code> module that has both frozen and non-frozen weights stored in two different matrices as suggested in the linked answer. This is way too complicated because you would also have to modify the decoder layer to use the changed data representation.</p>
</li>
<li><p>As dankal444's comment suggests, keep a copy of the embeddings you want to freeze, and after every optimization pass, reset them to the copy's value.</p>
</li>
<li><p>as Karl's comment suggests, do the backward pass as normal, but then zero-out the gradients for the embeddings you want to freeze. You could create a <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html"" rel=""nofollow noreferrer"">backwards hook</a> for this, as follows:</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>
#for example
embeddings_to_keep = torch.tensor([1, 4, 5], dtype=torch.int64, device=device)

def set_grads_to_zero_hook(grad):
   grad = grad.clone()
   grad[embeddings_to_keep] = 0.
   return grad

embedding_module.weight.register_hook(set_grads_to_zero_hook)

</code></pre>
","2024-10-07 18:47:01","1","Answer"
"79062674","78500154","","<p>I have had this multiple times before and from experience can tell you that this error can occur if you have a file named torch.py or another conflicting module name in the same directory as your script. Python may mistakenly try to import your script rather than the actual PyTorch module. If you have a file named torch.py rename it to something else and check your directory to ensure there are no conflicting filenames. Another reason this can happen is a circular import (where a module tries to import itself). Try checking any imports you use if they import themselves. I hope this helps you!</p>
","2024-10-07 15:51:02","4","Answer"
"79062412","78999845","","<p>You can navigate into the PyTorch source code for <code>torch.nn.Transformer</code>, where the attention is implemented through the <code>MultiheadAttention</code> module.
<a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention</a></p>
<p>In the forward pass, the <code>MultiheadAttention</code> module calls the <code>multi_head_attention_forward</code> function. This function handles several operations, such as the QKVO linear projections and the attention mechanism itself.</p>
<p>The attention calculation can be found at the following line in the PyTorch codebase:
<a href=""https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py#L6238"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py#L6238</a></p>
<pre><code>B, Nt, E = q.shape
q_scaled = q * math.sqrt(1.0 / float(E))

assert not (
    is_causal and attn_mask is None
), &quot;FIXME: is_causal not implemented for need_weights&quot;

if attn_mask is not None:
    attn_output_weights = torch.baddbmm(
        attn_mask, q_scaled, k.transpose(-2, -1)
    )
else:
    attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
attn_output_weights = softmax(attn_output_weights, dim=-1)
if dropout_p &gt; 0.0:
    attn_output_weights = dropout(attn_output_weights, p=dropout_p)

attn_output = torch.bmm(attn_output_weights, v)

attn_output = (
    attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
)
attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))

# optionally average attention weights over heads
attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
if average_attn_weights:
    attn_output_weights = attn_output_weights.mean(dim=1)

if not is_batched:
    # squeeze the output if input was unbatched
    attn_output = attn_output.squeeze(1)
    attn_output_weights = attn_output_weights.squeeze(0)
return attn_output, attn_output_weights
</code></pre>
<p>This function returns both the O-projection (output) and the attention scores. Therefore, if you need to modify the function's return behavior, you can re-implement <code>multi_head_attention_forward</code> to customize the final return value, including the attention scores (<code>attn_output_weights</code>) as needed.</p>
","2024-10-07 14:47:54","1","Answer"
"79061427","","How do I freeze only some embedding indices with tied embeddings?","<p>I found in <a href=""https://stackoverflow.com/questions/54924582/is-it-possible-to-freeze-only-certain-embedding-weights-in-the-embedding-layer-i"">Is it possible to freeze only certain embedding weights in the embedding layer in pytorch?</a> a nice way to freeze only some indices of an embedding layer.
However, while including it in a BERT model, I cannot find a way to tie those embeddings. Can anyone help me?</p>
<p>HF Transformers <a href=""https://github.com/huggingface/transformers/blob/e782e95e3465b66a377c0a7fe95f8f10cd87459d/src/transformers/modeling_utils.py#L2002"" rel=""nofollow noreferrer"">uses</a> the copy of the embedding layer weights in the decoder matrix. However it is not possible to do it if my Embedding layer is a nn.Module instead of a nn.Embedding. Substituting the decoder layer with the custom module is not performing the weight transposition.</p>
","2024-10-07 09:42:18","0","Question"
"79061201","","While inferencing through saved RT-DETR model weights, KeyError: 263","<p>I have trained RT-DETR model (taken from ultralytics) for the object detection task. I am using the following code for inferencing using the saved weights:</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO
import cv2

# Load your trained model
model = YOLO(r&quot;/content/drive/MyDrive/LN/best.pt&quot;)

# Load an image for inference
image_path = r&quot;/content/drive/MyDrive/LN/114_jpg.rf.6b9c3febc395a3968078eec417f1c9ed.jpg&quot;
image = cv2.imread(image_path)

# Perform object detection on the image
results = model(image)
</code></pre>
<p>But I am getting the following error</p>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-3-d814bef29ddf&gt; in &lt;cell line: 12&gt;()
     10 
     11 # Perform object detection on the image
---&gt; 12 results = model(image)

6 frames
/usr/local/lib/python3.10/dist-packages/ultralytics/engine/results.py in verbose(self)
    661             for c in boxes.cls.unique():
    662                 n = (boxes.cls == c).sum()  # detections per class
--&gt; 663                 log_string += f&quot;{n} {self.names[int(c)]}{'s' * (n &gt; 1)}, &quot;
    664         return log_string
    665 

KeyError: 263
</code></pre>
<p>I have trained for one class only.</p>
<p>Maybe the key error is related to the class name, hence I tried the following code to know the number of classes and their names:</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO

# Load the trained model
model = YOLO(r&quot;D:\Research\Bhargav-Kaushik\best.pt&quot;)

# Print the class names
print(&quot;Class Names:&quot;, model.names)
</code></pre>
","2024-10-07 08:34:13","0","Question"
"79059719","79059699","","<p>That's because pytorch tries to access data by index, starting from 0.
Official documentation says:</p>
<blockquote>
<p>Subclasses could also optionally implement <strong>getitems</strong>(), for speedup
batched samples loading. This method accepts list of indices of
samples of batch and returns list of samples.</p>
</blockquote>
<p>In other words, <code>__getitems__</code> should return list, not a dict.</p>
","2024-10-06 17:33:10","1","Answer"
"79059699","","How to make `__getitems__` return a dict?","<p>In torch's <code>Dataset</code>, on top of the obligatory <code>__getitem__</code> method, you can implement the <code>__getitems__</code> method.</p>
<p>In my case <code>__getitem__</code> returns a dict, but I can't figure out how to do the same with <code>__getitems__</code>.</p>
<pre class=""lang-py prettyprint-override""><code>class StackOverflowDataset(torch.utils.data.Dataset):
    def __init__(self, data):
        self._data = data

    def __getitem__(self, idx):
        return {'item': self._data[idx], 'whatever': idx*self._data[idx]+3}

    def __getitems__(self, idxs):
        return {'item': self._data[idxs], 'whatever': idxs*self._data[idxs]+3}
    
    def __len__(self):
        return len(self._data)

dataset = StackOverflowDataset(np.random.random(5))
for X in DataLoader(dataset, 2):
    print(X)
    break
</code></pre>
<p>If I comment out <code>__getitems__</code> it works, but leaving it there raises a <code>KeyError: 0</code>.</p>
<pre><code>KeyError                                  Traceback (most recent call last)
Cell In[182], line 15
     12         return len(self._data)
     14 dataset = StackOverflowDataset(np.random.random(5))
---&gt; 15 for X in DataLoader(dataset, 2):
     16     print(X)
     17     break

File ~/recommenders/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630, in _BaseDataLoaderIter.__next__(self)
    627 if self._sampler_iter is None:
    628     # TODO(https://github.com/pytorch/pytorch/issues/76750)
    629     self._reset()  # type: ignore[call-arg]
--&gt; 630 data = self._next_data()
    631 self._num_yielded += 1
    632 if self._dataset_kind == _DatasetKind.Iterable and \
    633         self._IterableDataset_len_called is not None and \
    634         self._num_yielded &gt; self._IterableDataset_len_called:

File ~/recommenders/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673, in _SingleProcessDataLoaderIter._next_data(self)
    671 def _next_data(self):
    672     index = self._next_index()  # may raise StopIteration
--&gt; 673     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    674     if self._pin_memory:
    675         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

File ~/recommenders/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55, in _MapDatasetFetcher.fetch(self, possibly_batched_index)
     53 else:
     54     data = self.dataset[possibly_batched_index]
---&gt; 55 return self.collate_fn(data)

File ~/recommenders/venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:317, in default_collate(batch)
    256 def default_collate(batch):
    257     r&quot;&quot;&quot;
    258     Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.
    259 
   (...)
    315         &gt;&gt;&gt; default_collate(batch)  # Handle `CustomType` automatically
    316     &quot;&quot;&quot;
--&gt; 317     return collate(batch, collate_fn_map=default_collate_fn_map)

File ~/recommenders/venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:137, in collate(batch, collate_fn_map)
    109 def collate(batch, *, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]] = None):
    110     r&quot;&quot;&quot;
    111     General collate function that handles collection type of element within each batch.
    112 
   (...)
    135         for the dictionary of collate functions as `collate_fn_map`.
    136     &quot;&quot;&quot;
--&gt; 137     elem = batch[0]
    138     elem_type = type(elem)
    140     if collate_fn_map is not None:

KeyError: 0
</code></pre>
","2024-10-06 17:25:24","0","Question"
"79058300","79056300","","<p>Following @ThomasHeller comments, a finalized version for further reference, thanks Thomas.</p>
<pre><code>(ns server.ros2
  (:require [&quot;js-pytorch$torch&quot; :as torch]))

(defn example []
    ;; Pass device as an argument to a Tensor or nn.Module
  (let [device &quot;mps&quot;]

      ;; Instantiate Tensors
    (let [x (torch/randn #js [8 4 5] true device)
          w (torch/randn #js [8 5 4] true device)
          b (torch/tensor #js [0.2 0.5 0.1 0.0] true)]

        ;; Make calculations
      (let [out (-&gt; (torch/matmul x w)
                    (torch/add b))]

          ;; Compute gradients on the whole graph
        (.backward out)

          ;; Get gradients from specific Tensors
        (js/console.log (.-grad w))
        (js/console.log (.-grad b))))))

(example)
</code></pre>
","2024-10-06 03:15:42","0","Answer"
"79058043","79056300","","<p>This is the example used in the <code>js-pytorch</code> README.</p>
<pre><code>const { torch } = require(&quot;js-pytorch&quot;);
</code></pre>
<p>This does not translate to the <code>ns</code> <code>:require</code> you have. Instead it translates to</p>
<pre class=""lang-clj prettyprint-override""><code>(ns server.ros2
  (:require [&quot;js-pytorch&quot; :refer (torch)]))
</code></pre>
<p>then using <code>(.randn torch ...)</code> and so on.</p>
<p>OR, alternatively since the syntax will be a bit nicer using <code>/</code> like your code does you can do</p>
<pre class=""lang-clj prettyprint-override""><code>(ns server.ros2
  (:require [&quot;js-pytorch$torch&quot; :as torch]))
</code></pre>
<p>This basically just sets up <code>torch</code> as the namespace alias, allowing the use of <code>torch/randn</code> and so on.</p>
<p>See the translation examples in the <a href=""https://shadow-cljs.github.io/docs/UsersGuide.html#_using_npm_packages"" rel=""nofollow noreferrer"">shadow-cljs User Guide</a>.</p>
","2024-10-05 22:30:05","1","Answer"
"79057626","79057245","","<p>You're putting a bunch of 0 values directly into the network. Any value multiplied by 0 is 0. The 0s are killing your signal through the model. Replace the inputs with a learned embedding</p>
<pre class=""lang-py prettyprint-override""><code>class LSTMNet( nn.Module ):
    def __init__( self, sequenceLength ):
        super().__init__()
        self.hidden_size = 10
        
        # added embedding layer
        self.embedding = nn.Embedding(2, self.hidden_size)
        self.lstm = nn.LSTM( input_size=self.hidden_size, hidden_size=self.hidden_size, 
                            num_layers=1, batch_first=True )
        self.net = nn.Sequential(
            nn.Flatten(),
            # added layer here, see note
            nn.Linear( sequenceLength * self.hidden_size, sequenceLength * self.hidden_size ),
            nn.ReLU(),
            nn.Linear( sequenceLength * self.hidden_size, 1 ),
            nn.Sigmoid()
        )

    def forward(self, x):
        # remove unit axis so x is size (batch_size, sequence_length)
        # convert to long type for embedding
        x = self.embedding(x.squeeze(-1).long())
        x, _ = self.lstm( x )
        x = self.net( x )
        return x
</code></pre>
<p>The model has an added embedding layer. I also added another linear layer in the sequential section. Strictly spearking is optional but greatly improves convergence. The output of the LSTM comes from a tanh function which means about half your values are below 0. Going LSTM -&gt; ReLU throws away these values. The model can compensate, but it will learn faster with a linear layer between the LSTM and ReLU.</p>
<p>Full code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

class LSTMDataset( Dataset ):
    def __init__( self, x, y ):
        self.x = x
        self.y = y

    def __len__(self):
        return self.y.shape[ 0 ]

    def __getitem__(self, idx):
        sample, label = self.x[ idx ], self.y[ idx ]
        return sample.reshape( ( -1, 1 ) ), label.reshape( ( 1 ) )

class LSTMNet( nn.Module ):
    def __init__( self, sequenceLength ):
        super().__init__()
        self.hidden_size = 10
        
        # added embedding layer
        self.embedding = nn.Embedding(2, self.hidden_size)
        self.lstm = nn.LSTM( input_size=self.hidden_size, hidden_size=self.hidden_size, 
                            num_layers=1, batch_first=True )
        self.net = nn.Sequential(
            nn.Flatten(),
            # added layer here, see note
            nn.Linear( sequenceLength * self.hidden_size, sequenceLength * self.hidden_size ),
            nn.ReLU(),
            nn.Linear( sequenceLength * self.hidden_size, 1 ),
            nn.Sigmoid()
        )

    def forward(self, x):
        # remove unit axis so x is size (batch_size, sequence_length)
        # convert to long type for embedding
        x = self.embedding(x.squeeze(-1).long())
        x, _ = self.lstm( x )
        x = self.net( x )
        return x

numSamples = 1000
sampleLength = 5

samples = np.ndarray( shape=( numSamples, sampleLength ), dtype=np.float32 )
labels = np.ndarray( shape=( numSamples ), dtype=np.float32 )
for s in range( numSamples ):
    sample = np.random.choice( [ 0, 1 ], size=sampleLength )
    samples[ s ] = sample
    even = np.count_nonzero( sample == 1 ) % 2 == 0
    labels[ s ] = int( even )

X_train, X_test, y_train, y_test = train_test_split( samples, labels, test_size=0.25, random_state=42 )

trainingSet = LSTMDataset( X_train, y_train )
testSet = LSTMDataset( X_test, y_test )

# note you should use a larger batch size
training_loader = DataLoader( trainingSet, batch_size=1, shuffle=True )
validation_loader = DataLoader( testSet, batch_size=1, shuffle=False )

model = LSTMNet( sampleLength )
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
loss_fn = torch.nn.BCELoss()

for epoch in range( 20 ):
    yPredicted = []
    yTruth = []
    for i, data in enumerate( training_loader ):
        inputs, labels = data

        optimizer.zero_grad()

        outputs = model(inputs)

        loss = loss_fn(outputs, labels)
        loss.backward()

        optimizer.step()

        yTruth.append(labels.detach() )
        yPredicted.append( torch.round( outputs.detach() ) )

    accuracy = accuracy_score( torch.cat(yTruth), torch.cat(yPredicted) )
    print( f&quot;Accuracy: {accuracy:.2f}&quot; )
</code></pre>
<p>All that said, your model hard-codes the sequence length. In this scenario, it doesn't really make sense to use a LSTM to begin with. LSTMs are for variable sequence length tasks. If you have a hard-coded sequence length, you can just use a MLP</p>
<pre class=""lang-py prettyprint-override""><code>class MLPNet( nn.Module ):
    def __init__( self, sequenceLength ):
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear( sequenceLength, sequenceLength ),
            nn.ReLU(),
            nn.Linear( sequenceLength, 1 ),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.net( x )
        return x
</code></pre>
","2024-10-05 17:51:17","1","Answer"
"79057288","79057233","","<pre><code>index = 2
categories_length = 3

tensor = torch.nn.functional.one_hot(
    torch.LongTensor([index]),
    categories_length,
)
</code></pre>
<p>Another pretty way:</p>
<pre><code>rock, paper, scissors = torch.eye(3) 
</code></pre>
","2024-10-05 14:50:04","1","Answer"
"79057245","","Why can't my LSTM determine if a sequence is odd or even in the number of ones?","<p>I am trying to understand LSTMs and wanted to implement a simple example of classifying a sequence as &quot;0&quot; if the number of &quot;1&quot; in the sequence is odd and as &quot;1&quot; if the number of &quot;1&quot; is even. This is my data generation and training routine:</p>
<pre><code>import torch
import numpy as np
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from Dataset import LSTMDataset # Custom Dataset
from Network import LSTMNet # Custom Network

if __name__ == &quot;__main__&quot;:

    numSamples = 1000
    sampleLength = 5

    samples = np.ndarray( shape=( numSamples, sampleLength ), dtype=np.float32 )
    labels = np.ndarray( shape=( numSamples ), dtype=np.float32 )
    for s in range( numSamples ):
        sample = np.random.choice( [ 0, 1 ], size=sampleLength )
        samples[ s ] = sample
        even = np.count_nonzero( sample == 1 ) % 2 == 0
        labels[ s ] = int( even )
    
    X_train, X_test, y_train, y_test = train_test_split( samples, labels, test_size=0.25, random_state=42 )

    trainingSet = LSTMDataset( X_train, y_train )
    testSet = LSTMDataset( X_test, y_test )

    training_loader = DataLoader( trainingSet, batch_size=1, shuffle=True )
    validation_loader = DataLoader( testSet, batch_size=1, shuffle=False )
    
    model = LSTMNet( inputSize= sampleLength )
    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
    loss_fn = torch.nn.BCELoss()

    for epoch in range( 10 ):
        yPredicted = []
        yTruth = []
        for i, data in enumerate( training_loader ):
            inputs, labels = data

            optimizer.zero_grad()

            outputs = model(inputs)

            loss = loss_fn(outputs, labels)
            loss.backward()

            optimizer.step()

            yTruth.append( int( labels.item() ) )
            yPredicted.append( int( torch.round( outputs ).item() ) )

        accuracy = accuracy_score( yTruth, yPredicted )
        print( f&quot;Accuracy: {accuracy:.2f}&quot; )
</code></pre>
<p>My dataset and network:</p>
<pre><code>class LSTMDataset( Dataset ):
    def __init__( self, x, y ):
        self.x = x
        self.y = y

    def __len__(self):
        return self.y.shape[ 0 ]

    def __getitem__(self, idx):
        sample, label = self.x[ idx ], self.y[ idx ]
        return sample.reshape( ( -1, 1 ) ), label.reshape( ( 1 ) )


class LSTMNet( nn.Module ):
    def __init__( self, sequenceLength ):
        super().__init__()
        self.hidden_size = 10
        self.lstm = nn.LSTM( input_size=1, hidden_size=self.hidden_size, num_layers=2, batch_first=True )
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.ReLU(),
            nn.Linear( sequenceLength * self.hidden_size, 1 ),
            nn.Sigmoid()
        )

    def forward(self, x):
        x, _ = self.lstm( x )
        x = self.net( x )
        return x
</code></pre>
<p>But unfortunately, my training accuracy never goes beyond 53%. Does anyone have any tips what I am doing wrong?</p>
<p>The input shape to my network is <code>( 1, 5, 1 )</code> and I wanted to fed the sequence elements one after another to my network that's why I chose <code>( 1, 5, 1 )</code> and not <code>(1, 1, 5 )</code>.</p>
","2024-10-05 14:32:22","0","Question"
"79057233","","How to pass an element from a limited list as input?","<p>I possess a compilation of the different states of arm within the game &quot;rock paper scissors&quot;. My intention is to program these categories in a similar manner as this.</p>
<pre><code>[1, 0, 0] - rock 
[0, 1, 0] - paper 
[0, 0, 1] - scissors
</code></pre>
<p>is there a convenient automatic way?</p>
<p>I employed the Embedding layer, but I'm uncertain if it's appropriate.</p>
","2024-10-05 14:23:29","-1","Question"
"79056755","78921896","","<p><code>torch::slice</code> works here but I think <code>torch::narrow</code> is clearer. No need to merge anything, just slice in one dimension, then slice the result along the other dimension.</p>
<pre><code>A_w = A.narrow(/*dim=*/1, /*start=*/w_offset, /*length=*/finesize);
A_hw = A_w.narrow(/*dim=*/2, /*start=*/h_offset, /*length=*/finesize);
</code></pre>
<p>Note that the dims are actually 1 and 2 (the dimension indexing starts at 0) and that <code>narrow</code> and <code>slice</code> return a tensor that shares its memory with the tensor being sliced. So modifying <code>A_hw</code> will modify <code>A</code> itself.</p>
<p>And because there is no copying required, this is extremely quick, you can <code>slice</code> or <code>narrow</code> all you want</p>
","2024-10-05 09:38:42","0","Answer"
"79056300","","How to use js-pytorch with clojurescript on MacOS?","<p>Using js-pytorch with clojurescript unsuccessfully. Please comment how to fix the installation issue of js-pytorch with clojurescript</p>
<p><strong>execute in calva-repl:</strong></p>
<pre><code>(ns server.ros2
  (:require [&quot;js-pytorch&quot; :as torch])) =&gt; nil

(def device &quot;gpu&quot;) =&gt; #'server.ros2/device
</code></pre>
<p><strong>execute in calva-repl:</strong></p>
<pre><code>(torch/randn #js [8 4 5]) =&gt; :repl/exception!

; Execution error (TypeError) at (&lt;cljs repl&gt;:1).
; module$node_modules$js_pytorch$dist$index_cjs.randn is 
not a function. (In module$node_modules$js_pytorch$dist
$index_cjs.randn([(8),(4),(5)])',  'module$node_modules
$js_pytorch$dist$index_cjs.randn' is undefined)
 
</code></pre>
<p><strong>deps.edn:</strong></p>
<pre><code>{:paths [&quot;src&quot;]
 :aliases {:cljs {:extra-deps {thheller/shadow-cljs {:mvn/version &quot;2.28.16&quot;}}}
           :outdated {;; Note that it is `:deps`, not `:extra-deps`
                      :deps {com.github.liquidz/antq {:mvn/version &quot;RELEASE&quot;}}
                      :main-opts [&quot;-m&quot; &quot;antq.core&quot;]}}
 :deps {reagent/reagent {:mvn/version &quot;1.2.0&quot;}}}
</code></pre>
<p><strong>Shadown-cljs.edn:</strong></p>
<pre><code> {:deps {:aliases [:cljs]}
 :builds {:app {:target :browser
                :output-dir &quot;public/js&quot;
                :asset-path &quot;/js&quot;
                :modules {:main {:init-fn client.core/init}}}
          :server {:target :node-script
                   :output-to &quot;out/server.js&quot;
                   :main server.core/start}}
 :dev-http {8080 &quot;public&quot;}}
</code></pre>
<p><strong>package.json</strong></p>
<pre><code>{
  &quot;name&quot;: &quot;cljs-express-htmx&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;main&quot;: &quot;index.js&quot;,
  &quot;scripts&quot;: {
    &quot;test&quot;: &quot;echo \&quot;Error: no test specified\&quot; &amp;&amp; exit 1&quot;
  },
  &quot;keywords&quot;: [],
  &quot;author&quot;: &quot;&quot;,
  &quot;license&quot;: &quot;ISC&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;dependencies&quot;: {
    &quot;create-react-class&quot;: &quot;^15.7.0&quot;,
    &quot;express&quot;: &quot;^4.19.2&quot;,
    &quot;js-pytorch&quot;: &quot;^0.6.0&quot;,
    &quot;rclnodejs&quot;: &quot;^0.27.4&quot;,
    &quot;react&quot;: &quot;^18.3.1&quot;,
    &quot;react-dom&quot;: &quot;^18.3.1&quot;
  },
  &quot;devDependencies&quot;: {
    &quot;shadow-cljs&quot;: &quot;^2.28.16&quot;
  }
}
</code></pre>
<p><strong>Installed js-pytorch with bun:</strong></p>
<pre><code>bun init 
bun add js-pytorch
</code></pre>
<p><strong>Using js-pytorch with clojurescript</strong></p>
<pre><code>   (ns server.ros2
      (:require [&quot;js-pytorch&quot; :as torch]))
    
    (defn example []
        ;; Pass device as an argument to a Tensor or nn.Module
      (let [device &quot;gpu&quot;]
    
          ;; Instantiate Tensors
        (let [x (torch/randn #js [8 4 5])
              w (torch/randn #js [8 5 4] true device)
              b (torch/tensor #js [0.2 0.5 0.1 0.0] true)]
    
            ;; Make calculations
          (let [out (-&gt; (torch/matmul x w)
                        (torch/add b))]
    
              ;; Compute gradients on the whole graph
            (.backward out)
    
              ;; Get gradients from specific Tensors
            (js/console.log (.-grad w))
            (js/console.log (.-grad b))))))
</code></pre>
","2024-10-05 04:07:12","1","Question"
"79051067","79050217","","<p>I tried running models as ONNX as well as in GGUF which makes it possible to run them on CPU generally.</p>
<p>Standard practice for reducing size and memory requirements of (small) LLMs seems to be creating quantized versions of the models, e.g. <a href=""https://huggingface.co/TheBloke/Llama-2-7B-GGUF"" rel=""nofollow noreferrer"">https://huggingface.co/TheBloke/Llama-2-7B-GGUF</a></p>
<p>Not sure exactly what your question is though since you already seem to have implemented it yourself - or did it not work for you?</p>
","2024-10-03 14:47:49","1","Answer"
"79050217","","Has anyone successfully converted a PyTorch-based LLM to ONNX Runtime to reduce file size?","<p>I’m exploring the idea of converting PyTorch and TensorFlow models to ONNX Runtime to optimize performance and reduce file sizes. Since most large language models (LLMs) are built using PyTorch, I was wondering if anyone has successfully converted these models to ONNX Runtime?</p>
<p>If so, were there any noticeable reductions in file size or improvements in performance? Any guidance, experiences, or challenges encountered would be appreciated.</p>
<p>Here is what I have tried using kaggle notebook:</p>
<pre><code>import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings

warnings.filterwarnings(&quot;ignore&quot;)

# Check if GPU is available and use it
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Specify the model name
model_name = &quot;mistralai/Mistral-7B-Instruct-v0.3&quot;

# Load the model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True , device_map=&quot;auto&quot;)


# Create a dummy input tensor and move it to GPU
dummy_input = tokenizer(&quot;Hello, world!&quot;, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;].to(device)


import gc
# Clean up memory before exporting
gc.collect()
torch.cuda.empty_cache()

# Export the model to ONNX format with full path and opset version 14
onnx_model_path = &quot;/kaggle/working/mistral_7b.onnx&quot;

torch.onnx.export(
    model,
    dummy_input,
    onnx_model_path,
    input_names=[&quot;input_ids&quot;],
    output_names=[&quot;output&quot;],
    dynamic_axes={&quot;input_ids&quot;: {0: &quot;batch_size&quot;, 1: &quot;sequence_length&quot;}},
    opset_version=14
)

print(f&quot;Model has been successfully converted to ONNX and saved at {onnx_model_path}&quot;)
</code></pre>
","2024-10-03 11:00:01","0","Question"
"79048096","79047727","","<p>Yes, <code>x</code> and <code>y</code> are different linear projections of the inputs.</p>
<p>You can see how flash attention uses SwiGLU <a href=""https://github.com/Dao-AILab/flash-attention/blob/53a4f341634fcbc96bb999a3c804c192ea14f2ea/flash_attn/modules/mlp.py#L130"" rel=""nofollow noreferrer"">here</a>. Simplifying the logic, their gated MLP computes:</p>
<pre class=""lang-py prettyprint-override""><code>y = self.fc1(x)
y, gate = y.chunk(2, dim=-1)
y = swiglu(gate, y)
</code></pre>
<p>The <code>y.chunk(2, dim=-1)</code> logic is because they fuse the two linear projections into a single operation. You could equivalently have:</p>
<pre class=""lang-py prettyprint-override""><code>y = self.linear1(x)
gate = self.linear2(x)
y = swiglu(gate, y)
</code></pre>
","2024-10-02 18:43:24","2","Answer"
"79047727","","How to implement SwiGLU activation? Why does SwiGLU takes in two tensors?","<p>The SwiGLU variant introduced in <a href=""https://arxiv.org/pdf/2002.05202"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2002.05202</a> is simply &quot;divine benevolence&quot; and the implementation on Flash-Attention just works out of the box <a href=""https://github.com/Dao-AILab/flash-attention/tree/main"" rel=""nofollow noreferrer"">https://github.com/Dao-AILab/flash-attention/tree/main</a></p>
<p>Comparing the the implementation:</p>
<pre><code>import torch


swiglu_fwd_codestring = &quot;&quot;&quot;
template &lt;typename T&gt; T swiglu_fwd(T x, T y) {
    return float(x) * float(y) / (1.0f + ::exp(-float(x)));
}
&quot;&quot;&quot;

swiglu_bwd_codestring = &quot;&quot;&quot;
template &lt;typename T&gt; T swiglu_bwd(T x, T y, T g, T&amp; dx, T&amp; dy) {
    float x_sigmoid = 1.0f / (1.0f + ::exp(-float(x)));
    dx = x_sigmoid * (1 + float(x) * (1.0f - x_sigmoid)) * float(g) * float(y);
    dy = float(x) * x_sigmoid * float(g);
}
&quot;&quot;&quot;
swiglu_fwd = torch.cuda.jiterator._create_jit_fn(swiglu_fwd_codestring)
swiglu_bwd = torch.cuda.jiterator._create_multi_output_jit_fn(swiglu_bwd_codestring, num_outputs=2)


class SwiGLUFunction(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, y):
        ctx.save_for_backward(x, y)
        return swiglu_fwd(x, y)

    @staticmethod
    def backward(ctx, dout):
        x, y = ctx.saved_tensors
        return swiglu_bwd(x, y, dout)

swiglu = SwiGLUFunction.apply

swiglu(torch.tensor(1.0).to('cuda'), torch.tensor(0.5).to('cuda'))
</code></pre>
<p>and</p>
<pre><code>import torch.nn.functional as F
swiglu_torch = lambda x, y: F.silu(x) * y

swiglu_torch(torch.tensor(1.0).to('cuda'), torch.tensor(0.5).to('cuda'))
</code></pre>
<p>They both give the same output:</p>
<pre><code>tensor(0.3655, device='cuda:0')
</code></pre>
<p>The paper description, the input to the activation is a single <code>x</code> tensor,</p>
<p><a href=""https://i.sstatic.net/JmidVP2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JmidVP2C.png"" alt=""enter image description here"" /></a></p>
<p>but both the implementation is taking <code>x</code> and <code>y</code>, <strong>what does the <code>y</code> corresponds to when we only have the <code>x</code> tensor as in the input in the formula? Is that <code>x = W1.x</code> and <code>y=W2.x</code>?</strong></p>
","2024-10-02 16:39:09","2","Question"
"79047158","78302924","","<pre><code>import torch 

x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5, 6])

# Using default 'ij' indexing
res1 = torch.meshgrid(x, y, indexing='ij')
print(res1)
'''
(tensor([[1, 1, 1],
        [2, 2, 2],
        [3, 3, 3]]), 
 
 tensor([[4, 5, 6],
        [4, 5, 6],
        [4, 5, 6]]))

'''
# Using 'xy' indexing
res2 = torch.meshgrid(x, y, indexing='xy')
print(res2)
'''
(tensor([[1, 2, 3],
        [1, 2, 3],
        [1, 2, 3]]), 
 
 tensor([[4, 4, 4],
        [5, 5, 5],
        [6, 6, 6]]))

'''
</code></pre>
","2024-10-02 14:14:12","1","Answer"
"79042428","79019507","","<p>I have changed your model to the code below, because you are flattening the images and in the next layers input the dimension should be 28*28=784:</p>
<pre><code>model = nn.Sequential(
nn.Flatten(),
#YOUR CODE. Add layers to your sequential class
nn.Linear(in_features=784, out_features=128),
nn.ELU(),
nn.Linear(in_features=128, out_features=10),
nn.ELU()
)
</code></pre>
<p>after this change the model runs perfectly with this results:</p>
<ul>
<li>train. Accuracy: 0.98535</li>
<li>valid. Accuracy: 0.9759</li>
</ul>
","2024-10-01 09:12:06","1","Answer"
"79041904","79041593","","<p>KL Divergence is not symmetric. <code>KL(P, Q) != KL(Q, P)</code>.</p>
<p>Pointwise KL Divergence is defined as <code>y_{true} * log(y_{true}/y_{pred})</code>.</p>
<p>Following this, your function:</p>
<pre><code>def kl_div(P, Q):
    return (P * (P / Q).log()).sum()
</code></pre>
<p>Treats <code>P</code> as the true distribution. From your question, it sounds like you are treating <code>P</code> as the predicted distribution and <code>Q</code> as the true distribution, so you have things flipped relative to your function.</p>
<p>You may be confused because the mathematical notation <code>KL(P||Q)</code> defines <code>P</code> as a distribution of observations and <code>Q</code> as a &quot;model&quot; distribution, while the ML context uses <code>P</code> to denote the output of the model you are training and <code>Q</code> to denote ground truth observations from a dataset.</p>
<p>To your second question, KL Divergence is undefined when one of the values is zero. That is definitional to the metric. If P(i) = 0 and Q(i) &gt; 0, this means <code>P</code> says event <code>i</code> is impossible while <code>Q</code> says it is possible - there is no measure on that discrepancy.</p>
<p>You can fudge it by adding a small eps to your values, ie <code>torch.nn.functional.kl_div((p+1e-8).log(), q, reduction='sum')</code>. However if 0 values come up often for your use case, you should consider a different metric.</p>
","2024-10-01 06:30:57","1","Answer"
"79041593","","Which way to use KL divergence in PyTorch is correct? And what if zero is in distribution?","<p>I am trying to fit distribution <code>p</code> to distribution <code>q</code> with <strong>KL divergence</strong>.</p>
<pre class=""lang-py prettyprint-override""><code>import torch

p = torch.Tensor([0.1, 0.2, 0.7])
q = torch.Tensor([0.333, 0.334, 0.333])
</code></pre>
<p>So I calculate kl divergence by myself:</p>
<pre class=""lang-py prettyprint-override""><code>def kl_div(P, Q):
    return (P * (P / Q).log()).sum()

kl_div(p, q)
</code></pre>
<p>Result is <code>tensor(0.2972)</code></p>
<p>Then I found that PyTorch has already implemented the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div"" rel=""nofollow noreferrer""><code>torch.nn.functional.kl_div</code></a> function.</p>
<p>I think <code>input</code> should be the network's output, and <code>target</code> is a constant.<br />
So I treated <code>p</code> as the <code>input</code> and <code>q</code> as the <code>target</code>.</p>
<p>But result of</p>
<pre class=""lang-py prettyprint-override""><code>torch.nn.functional.kl_div(p.log(), q, reduction='sum')  # tensor(0.3245)
</code></pre>
<p>is different from mine.</p>
<p>And this one gets the same result as me.</p>
<pre class=""lang-py prettyprint-override""><code>torch.nn.functional.kl_div(q.log(), p, reduction='sum')   # tensor(0.2972)
</code></pre>
<hr />
<p>So what went wrong?<br />
<strong>Is there a problem with my understanding of kl divergence?<br />
Or I filled in the wrong parameters of <code>torch.nn.functional.kl_div</code>?</strong></p>
<hr />
<p>And another question:</p>
<p><strong>What if I have zero in the distribution?</strong></p>
<p>Such as</p>
<pre class=""lang-py prettyprint-override""><code>p = torch.Tensor([0., 0.3, 0.7])
q = torch.Tensor([0.333, 0.334, 0.333])
</code></pre>
<p>I still need to calculate kl divergence in this situation.</p>
","2024-10-01 03:55:37","0","Question"
"79040277","78721195","","<p>This was caused by a compatibility issue between tensorboard 2.17 and numpy &gt;=2.0. See github issue here: <a href=""https://github.com/tensorflow/tensorboard/issues/6874"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorboard/issues/6874</a>. If numpy 2.0 compatibility is desired, try updating tensorboard to 2.18.0 or higher.</p>
","2024-09-30 16:41:26","2","Answer"
"79036616","79027725","","<p>Meanwhile I found a possible answer, which is to use: <code> val_check_interval=n</code> option at <code>Trainer</code>.</p>
<p><a href=""https://lightning.ai/docs/pytorch/stable/common/trainer.html#val-check-interval"" rel=""nofollow noreferrer"">https://lightning.ai/docs/pytorch/stable/common/trainer.html#val-check-interval</a></p>
<p><strong>From documentation:</strong></p>
<p>How often within one training epoch to check the validation set. Can specify as float or int.</p>
<ul>
<li><p>pass a float in the range [0.0, 1.0] to check after a fraction of the
training epoch.</p>
</li>
<li><p>pass an int to check after a fixed number of training batches.</p>
</li>
</ul>
","2024-09-29 14:26:13","0","Answer"
"79034532","79033052","","<p>I've created the new conda environment with Python 3.12.5, installed everything from the scratch, and it works now! Thanks for the help</p>
","2024-09-28 15:18:28","0","Answer"
"79033606","79032545","","<p>I think I found a pretty simple solution. I tried the approach again with summing up the <code>loss</code> during the batch calculation, but training only one time at the end. Before, I must have done something wrong concerning the <code>loss</code> data type, so I now initialize it with the first batch <code>loss</code>, to secure the correct type:</p>
<pre><code>for k_batch in range(0, len(samples), batch_size):

    samples_batch = samples[k_batch:k_batch + batch_size]

    loss = loss_fnc(samples_batch)
    if k_batch == 0:
        loss_sum = loss*len(samples_batch)
    else:
        loss_sum += loss*len(samples_batch)

loss = loss_sum/len(samples)

model.optim.zero_grad()
loss.backward()
model.optim.step()
</code></pre>
<p>When I now compare the <code>loss</code> to the reference <code>loss</code> (from training &quot;all-at-once&quot;), the values are equal. As grad-function the reference <code>loss</code> has <code>MseLossBackward0</code> while the newly calculated loss from these batches has <code>DivBackward0</code>. Nonetheless the following training seems to be identical independent of using batches or not.</p>
","2024-09-28 06:55:03","0","Answer"
"79033052","","Can't load my YOLOv8n model, trained on custom dataset","<p>I use Ultralytics YOLOv8 with the model, trained on custom dataset, and run the following python code:</p>
<pre><code>import cv2
from ultralytics import YOLO

model = YOLO(&quot;/home/github/ultralytics/ultralytics/runs/detect/train3/weights/best.pt&quot;)
</code></pre>
<p>and get this error:</p>
<pre><code>requirements: YOLOv8 requirement &quot;ultralytics.nn.modules.conv&quot; not found, attempting AutoUpdate...
ERROR: Could not find a version that satisfies the requirement ultralytics.nn.modules.conv (from versions: none)
ERROR: No matching distribution found for ultralytics.nn.modules.conv
requirements: ❌ Command 'pip install &quot;ultralytics.nn.modules.conv&quot;  ' returned non-zero exit status 1.
</code></pre>
<p>Tried to run :</p>
<pre><code>pip install --upgrade ultralytics 
</code></pre>
<p>Tried different conda Python environment (v3.8.8, v3.10.14), and got the same results</p>
","2024-09-27 22:24:42","0","Question"
"79032603","79019656","","<p>I could trace it to <a href=""https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/ops/sparse_ops.py#L609"" rel=""nofollow noreferrer"">this</a> part of the code where they simply use &quot;<em>X</em>&quot; as a string separator on one set of crossed values from various features.</p>
<blockquote>
<p>I'm struggling to understand the concatenate(features) part. Do I have to do the hash of each &quot;pair&quot; of features?</p>
</blockquote>
<p>If you are crossing two features, for each pair of values from each feature, you would need to &quot;combine&quot; them somehow (which is what they term as &quot;concatenation&quot;). The concatenation I see from the code is just string concatenation using the separator &quot;<em>X</em>&quot;.</p>
<p>So if you have <code>feature A: &quot;A1&quot;, &quot;A2&quot;</code> and <code>feature B: &quot;B1&quot;, &quot;B2&quot;, &quot;B3&quot;</code>, you would need to do</p>
<ul>
<li><code>hash(&quot;A1_X_B1&quot;) % num_bins</code></li>
<li><code>hash(&quot;A1_X_B2&quot;) % num_bins</code></li>
<li><code>hash(&quot;A1_X_B3&quot;) % num_bins</code></li>
<li><code>hash(&quot;A2_X_B1&quot;) % num_bins</code></li>
<li><code>hash(&quot;A2_X_B2&quot;) % num_bins</code></li>
<li><code>hash(&quot;A2_X_B2&quot;) % num_bins</code></li>
</ul>
<p>and then one-hot encode these numbers if you want.</p>
<p><strong>Tensoring the operations</strong></p>
<p>I'm going to assume your features are categorical but numeric IDs, because if they were strings you would need to additionally map them out to integers.</p>
<pre><code>PRIME_NUM = 2_147_483_647

def feature_cross(feature_a: torch.Tensor, feature_b: torch.Tensor, num_bins: int) -&gt; torch.Tensor:
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    feature_a = feature_a.to(device)
    feature_b = feature_b.to(device)
    
    # Add an additional dimension and repeat the feature to match the other feature's size
    a_expanded = feature_a.unsqueeze(1).expand(-1, feature_b.size(0))
    # Add an additional dimension and repeat the feature to match the other feature's size
    b_expanded = feature_b.unsqueeze(0).expand(feature_a.size(0), -1)
    
    combined = (a_expanded.long() * PRIME_NUM + b_expanded.long())
    
    hashed = combined % num_bins
    
    return hashed

feature_a = torch.tensor([1001, 1002, 1003, 1004], dtype=torch.long)
feature_b = torch.tensor([2001, 2002, 2003, 2004, 2005], dtype=torch.long)
num_bins = 1000

result = feature_cross(feature_a, feature_b, num_bins)
print(result)
</code></pre>
<p>To take an example, if <code>A = [1,2,3]</code> and <code>B = [4,5]</code>, we are expanding them to</p>
<pre><code># a_expanded

1 1
2 2
3 3

# b_expanded

4 5
4 5
4 5

</code></pre>
<p>and combining them through addition (with prime number multiplication) to achieve a cross.</p>
<p>You're right that using tuples can also be an option for combining the values since tuples can be hashed, but I don't know of a tensorised way of creating tuples.</p>
","2024-09-27 18:54:05","4","Answer"
"79032545","","Realizing identical training with or without using batches","<p>I have a model in PyTorch which converges very well on a reference example when using a standard training process, in which the optimizer trains on all samples at once:</p>
<pre><code>loss = loss_fnc(samples)

model.optim.zero_grad()
loss.backward()
model.optim.step()
</code></pre>
<p>Now the model should train on more memory-intensive tasks (more samples, bigger input size), so I thought it would help to train in batches. For this, I’m using this pretty standard method with a for loop:</p>
<pre><code>loss_sum = 0

for k_batch in range(0, len(samples), batch_size):

    samples_batch = samples[k_batch:k_batch + batch_size]

    loss = loss_fnc(samples_batch)
    loss_sum += loss*len(samples_batch)

    model.optim.zero_grad()
    loss.backward()

    model.optim.step()

loss_comp = loss_sum/len(samples)
</code></pre>
<p>Now, when I skip the training part in this method and only calculate the loss, the resulting <code>loss_comp</code> is identical to the one when calculating it for all samples at once (like in the method above). Naturally, by training in batches, <code>loss_comp</code> is different since the model changes throughout the batches.</p>
<p>Now I face the problem that the model doesn’t converge on my reference example anymore when using batch training. The samples are already ordered randomly before the training in both cases.</p>
<p>Since I’m only interested in reducing the necessary memory demand for the computation, and before I try anything else, is there a way to really have an identical training when using batches instead of &quot;all-at-once&quot;? I tried to change the function so that only the loss is computed in batches and the optimization step is done at the end, but I didn’t seem to get it to work.</p>
<p>As I understand, using a DataLoader wouldn't change anything concerning this behavior.</p>
","2024-09-27 18:27:47","0","Question"
"79032368","79031881","","<p>The problem here is that there is a missing call to <code>optimizer_adam.step()</code>, so the weights aren't updated after calculating gradients.</p>
","2024-09-27 17:24:05","4","Answer"
"79031881","","Constant loss value in Adam optimization method","<p>I have NN code which is written with pytorch. In the loss value calculation when code is using Adam optimization method loss values does not change during the loop while the LBGFS optimization method gets on everything is ok and loss values decrease in the normal way. How can I get rid of this issue?</p>
<pre><code>    for epo in range(epo_adam):
model.train()
optimizer_adam.zero_grad()
loss , momentum_loss , loss_data , loss_BC , continuity_loss = adop_loss_Weight(model, x, y, z, u_exact, v_exact , w_exact , p_exact ,
           x_b, y_b, z_b, u_b, v_b, w_b ,p_b)
loss.backward()
if epo %500 == 0:
  print(f'Epoch Adam {epo}, Total Loss: {loss.item():.10f}')
if loss.item() &lt;=0.15 :
  print(&quot;Optimzation Method is swtiching to LBGF-S . . . &quot;)
  break


for epochs in range(epochs):
model.train()
loss = optimizer.step(closure)

if epochs % 20 == 0:
    print(f'Epoch LBGF-s {epochs}, Total Loss: {loss.item():.5f}')
    #print(f'The highest Loss is:  {max(momentum_loss.item() , continuity_loss.item() , loss_data.item() , loss_bc.item()):.6f}')
    #print(time.time())
</code></pre>
<p>I searched a lot but couldn't find any solution. I think constant loss values in Adam optimization is not reasonable!</p>
","2024-09-27 14:54:13","3","Question"
"79029308","78941428","","<p>The correct way to pass the command line argument &quot;gpus&quot; to the container is using the file &quot;devcontainer.json&quot; (and NOT the dockerfile ARGS).</p>
<p>For this, insert the following line in your devcontainer.json:</p>
<pre><code>&quot;runArgs&quot;: [&quot;--gpus=all&quot;],
</code></pre>
","2024-09-26 22:24:36","1","Answer"
"79027725","","Pytorch Lightning, display validation metrics after few batches","<p>In <strong>Pytorch Lightning</strong>, I would like to print <strong>loss</strong> and <strong>accuracy</strong> on Training batch and Validation dataset for each <strong>N</strong> steps (batches) for log and terminal.</p>
<p>The question is a bit similar to: <a href=""https://stackoverflow.com/questions/71646596/compute-metrics-loss-every-n-batches-pytorch-lightning"">Compute metrics/loss every n batches Pytorch Lightning</a></p>
<p>But the simplest solution didn't work for me, and the other one was looking to be over complicated for such basic functionality.</p>
","2024-09-26 14:07:21","0","Question"
"79027403","79026530","","<blockquote>
<p>Does YOLOv8 support other format of data to do fine-tuning?</p>
</blockquote>
<p>No, Ultralytics YOLOv8 supports only datasets in the YOLO format, as described in the official documentation: for Object detection <a href=""https://docs.ultralytics.com/datasets/detect/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/datasets/detect/</a>, for Oriented Bounding Box detection <a href=""https://docs.ultralytics.com/datasets/obb/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/datasets/obb/</a>, and so on.</p>
<blockquote>
<p>Then what's the problem with YOLO and YOLOv8 OBB option in Label Studio?</p>
</blockquote>
<p><a href=""https://labelstud.io/blog/quickly-create-datasets-for-training-yolo-object-detection-with-label-studio/"" rel=""nofollow noreferrer"">As expected</a>, the data exported in the YOLO format from Label Studio will have the following content: <em>notes.json, classes.txt, images and labels folders</em>. You still need to do some preprocessing to create a YOLO dataset from these files.</p>
<blockquote>
<p>Can I export YAML file with COCO8 format in Label Studio?</p>
</blockquote>
<p>The YAML file here is just a short text description of the YOLO dataset (dataset path and class list), you need to create it manually in this case.</p>
<ol>
<li>First, partition the images and their corresponding labels into train and validation sets so that you will have <strong>train</strong> and <strong>val</strong> folders with <em>images</em> and <em>labels</em> folders inside each of them.</li>
<li>Create a new text file and describe your dataset in the following format:</li>
</ol>
<pre class=""lang-yaml prettyprint-override""><code># Train/val/test sets as 1) dir: path/to/imgs
path: &quot;../datasets/mydataset&quot; # dataset root dir
train: &quot;train&quot; # train folder (relative to 'path')
val: &quot;val&quot; # validation folder (relative to 'path')
test: # test folder (optional)

# Classes list
names:
    0: person
    1: bicycle
    2: car
</code></pre>
<ol start=""3"">
<li>Rename this text file to <strong>mydataset.yaml</strong>. This is your YAML file which is absent in the original exported data from Label Studio.</li>
<li>Create the dataset folder and put into it your train and val folders, as well as the mydataset.yaml file. You don't need the json file or classes.txt file (dataset classes information is already in the .yaml file now)</li>
</ol>
<p>After this preprocessing of the exported dataset, you can train the YOLOv8 model on it:</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO

# Load a model
model = YOLO(&quot;yolov8n.pt&quot;)  # load a pretrained model (recommended for training)

# Train the model
results = model.train(data=&quot;/path/to/mydataset.yaml&quot;, epochs=100, imgsz=640)
</code></pre>
","2024-09-26 12:51:13","1","Answer"
"79027342","79027142","","<p>Changing the scaling to use <code>/ (math.sqrt(self.head_dim))</code> solves this issue. I'm not sure why that happens, because <code>self.head_dim ** -0.5</code> should be equivalent to <code>/ (math.sqrt(self.head_dim))</code>. Maybe someone else can explain?</p>
","2024-09-26 12:40:40","0","Answer"
"79027142","","Exploding Gradient (NaN Training Loss And Validation Loss) In Multi Head Self Attention - Vision Transformer","<p>This multihead self attention code causes the training loss and validation loss to become NaN, but when I remove this part, everything goes back to normal. I know that when the training loss and validation loss become NaN, it means there's an exploding gradient there. However, I don’t know what's wrong with my code that's causing the gradient to explode. When I compare it with the official PyTorch code, it looks similar. When I use nn.MultiheadSelfAttention, the gradient doesn’t explode, but when I use my own code, the gradient starts exploding. There is no error message displayed. Does anyone know what's wrong with my code below?</p>
<pre class=""lang-py prettyprint-override""><code>class MultiHeadAttention(nn.Module):
  def __init__(self, in_dim, num_heads=8, dropout=0):
    super().__init__()
    self.num_heads = num_heads
    self.head_dim = in_dim // num_heads
    self.conv_q = nn.Conv2d(in_dim, in_dim, kernel_size=1)
    self.conv_k = nn.Conv2d(in_dim, in_dim, kernel_size=1)
    self.conv_v = nn.Conv2d(in_dim, in_dim, kernel_size=1)
    self.att_drop = nn.Dropout(dropout)
    self.proj = nn.Conv2d(in_dim, in_dim, kernel_size=1)
    self.proj_drop = nn.Dropout(dropout)

  def forward(self, x):

    b, _, h, w = x.shape
    
    q = self.conv_q(x)
    k = self.conv_k(x)
    v = self.conv_v(x)

    q = rearrange(q, &quot;b (nh hd) h w -&gt; b nh (h w) hd&quot;, nh=self.num_heads)
    k = rearrange(k, &quot;b (nh hd) h w -&gt; b nh (h w) hd&quot;, nh=self.num_heads)
    v = rearrange(v, &quot;b (nh hd) h w -&gt; b nh (h w) hd&quot;, nh=self.num_heads)

    att_score = q @ k.transpose(2, 3) ** (self.head_dim ** -0.5)
    att_score = F.softmax(att_score, dim=-1)
    att_score = self.att_drop(att_score)

    x = att_score @ v

    x = rearrange(x, 'b nh (h w) hd -&gt; b (nh hd) h w', h=h, w=w)

    x = self.proj(x)
    x = self.proj_drop(x)

    return x
</code></pre>
","2024-09-26 11:48:15","0","Question"
"79026530","","Dataset format mismatches between Ultralytics YOLOv8 training and Label Studio exported","<p>I used Tensorflow before and am new to PyTorch and Ultralytics YOLOv8. I recently learnt to train (fine-tune) the YOLOv8 object detection model to fit my own dataset. However, <a href=""https://docs.ultralytics.com/modes/train/#key-features-of-train-mode"" rel=""nofollow noreferrer"">the official documentation</a> only shows how to train it in COCO8 format with YAML file. The dataset exported by Label Studio is only in JSON, not YAML (even exported as YOLO format!).</p>
<p>Then I tried to directly replace <code>model.train(data=&quot;coco8.yaml&quot;, epochs=100, imgsz=640)</code> with <code>model.train(data=&quot;coco/result.json&quot;, epochs=100, imgsz=640)</code>. But no surprise, it not works.</p>
<p>So, I have 2 questions:</p>
<ol>
<li>Does YOLOv8 support other format of data to do fine-tuning? Then what's the problem with YOLO and YOLOv8 OBB option in Label Studio?</li>
<li>Can I export YAML file with COCO8 format in Label Studio?</li>
</ol>
<p>Edit:</p>
<p>By looking through <a href=""https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco8.yaml"" rel=""nofollow noreferrer"">the example coco8.yaml file in their GitHub</a>, I find the yaml file can be easily hard-coded manually. However, I still wonder how the dataset exported by Label Studio can be used, and why the problem of format interface exists.</p>
","2024-09-26 09:28:24","0","Question"
"79026251","79025552","","<p><strong>The short answer:</strong> When reshaping a contiguous tensor, both methods will do the same (namely, provide a new view of the given tensor) and can therefore be used interchangeably. When reshaping a non-contiguous tensor, <code>reshape()</code> will duplicate the necessary parts of memory from the given tensor for producing the resulting tensor, while <code>view()</code> will fail with a <code>RuntimeError</code>.</p>
<h2>The long answer</h2>
<p>The main difference is how <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html"" rel=""nofollow noreferrer""><code>torch.Tensor.reshape()</code></a> and <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.view.html"" rel=""nofollow noreferrer""><code>torch.Tensor.view()</code></a> handle non-contiguous tensors.</p>
<p>To understand the difference, we need to understand what is a <em>contiguous tensor</em>, and what is a <em>view</em> of a tensor:</p>
<ul>
<li>A <em>contiguous tensor</em> is a tensor whose values are stored in a single, uninterrupted – thus, &quot;contiguous&quot; – piece of memory. A non-contiguous tensor may have gaps in its memory layout.</li>
<li>Producing a <em>view</em> of a tensor means reinterpreting the arrangement of  values in its memory. Think of a piece of memory that stores 16 values: we can interpret it, for example, as a 16-element 1-d tensor or as a 4×4-element 2-d tensor. Both interpretations can use the same underlying memory. By using views and thus reinterpreting the memory layout, we can create differently shaped tensors from the same piece of memory, in this way avoiding duplication and saving memory.</li>
</ul>
<p>Now back to the two methods:</p>
<ul>
<li>If applied to a contiguous tensor, both <code>reshape()</code> and <code>view()</code> will produce a new view of the given tensor's memory, in this way avoiding duplication.</li>
<li>If applied to a non-contiguous tensor, the creation of a view will not be possible. The two methods handle this situation differently:
<ul>
<li>The <code>reshape()</code> method will duplicate the necessary piece of memory and will return a tensor whose memory will not be shared with the given tensor.</li>
<li>The <code>view()</code> method will produce a <code>RuntimeError</code>.</li>
</ul>
</li>
</ul>
<p>We can demonstrate this with the following piece of code:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import arange

contiguous = arange(16).view(4, 4)             # Create contiguous 4×4 tensor
noncontiguous = arange(20).view(4, 5)[:, :4]   # Create non-contiguous 4×4 tensor

contiguous_r = contiguous.reshape(16)          # OK: produces a 1-d view
assert contiguous_r.data_ptr() == contiguous.data_ptr()  # Same memory used

contiguous_v = contiguous.view(16)             # OK: produces a 1-d view
assert contiguous_v.data_ptr() == contiguous.data_ptr()  # Same memory used

noncontiguous_r = noncontiguous.reshape(16)    # OK: produces a new 1-d array
assert noncontiguous_r.data_ptr() != noncontiguous.data_ptr()  # New memory used

noncontiguous_v = noncontiguous.view(16)       # ERROR: cannot produce view
</code></pre>
<p>The last line will produce <em>RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</em></p>
<p>Maybe at this point, I should also mention what a tensor's <em>stride</em> is: in essence, it is the information that tells us how to map the tensor's indexes to its underlying memory. You will find more information on strides in particular and on contiguous vs. non-contiguous tensors in general, for example, in <a href=""https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107"" rel=""nofollow noreferrer"">this discussion</a> in the PyTorch forum.</p>
<p>As to your question, <em>which should I use?</em>:</p>
<ul>
<li>I would recommend using <code>reshape()</code> if you just want to make sure that you will receive a result when reshaping a tensor: For a contiguous tensor, <code>reshape()</code> will do exactly the same as <code>view()</code> (namely, produce a view on the given tensor and not duplicate its memory). For a non-contiguous tensor, <code>reshape()</code> will be the only method that will produce a result, while <code>view()</code> will fail (see above).</li>
<li>I would recommend using <code>view()</code> if you are actually sure that your tensors to be reshaped are contiguous and want to catch the situation where they aren't. This may be meaningful, for example, if you work in a low-memory regime and thus rather prefer failing than duplicating memory. Another use case of <code>view()</code> is not reinterpreting the <em>shape</em>  but the <em>data type</em> of the underlying memory. I guess this is not your use case, but you will find more on this in the documentation of <code>view()</code> that I linked above.</li>
</ul>
","2024-09-26 08:28:22","3","Answer"
"79026181","","How do I run a LLM on multiple GPUs?","<p>I am trying to run Mistral-7b across multiple GPUs, as I have a large number of prompts. I am using a cluster, which provides usage of up to 4 GPUs. However, only one of them is used for inference.</p>
<pre><code>model_name = 'mistralai/Mistral-7B-Instruct-v0.2'

# Load the model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Check if multiple GPUs are available
if torch.cuda.device_count() &gt; 1:
    print(f&quot;Using {torch.cuda.device_count()} GPUs&quot;)
    model = DataParallel(model)
else:
    print(&quot;Using single GPU or CPU&quot;)

# Move model to GPU(s)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)

# Load prompts from DataFrame
prompts = prompts_df[&quot;prompt&quot;].tolist()  

# Function to generate response
def generate_response(prompt, max_new_tokens=30):
    inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(device)
    with torch.no_grad():
        outputs = model.module.generate(**inputs,
                                 max_new_tokens=max_new_tokens,
                                 pad_token_id=tokenizer.pad_token_id,
                                 eos_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Process prompts in batches
batch_size = 32  # Increased batch size, adjust based on your GPU memory
responses = []
for i in tqdm(range(0, len(prompts), batch_size)):
    batch = prompts[i:i+batch_size]
    responses.extend([generate_response(prompt) for prompt in batch])
</code></pre>
<p>Performing this code, with nvidia-smi I see that GPU0 is 98% occupied, while GPU1 is 0%.</p>
<p>I hope someone can help me out here. Using only one GPU would take way too long.</p>
<p>I read that with mode.module.generate this could be fixed, but this did not work for me here.</p>
","2024-09-26 08:11:02","1","Question"
"79025552","","PyTorch: difference between reshape() and view() method","<p>What is the difference between reshape and view method and why do we need and I am using pytorch tensors and working on changing the shape of data then I came to know these two functions. what are the affects on memory which consumes the more memory and which is more expensive if we are working on large data with less resources.</p>
<pre><code>x = torch.tensor([1, 2, 3, 4, 5], dtype = torch.float32)
x = x.reshape(-1, 1)
</code></pre>
<p>and the view method</p>
<pre><code>x = x.view(x.shape[0], 1)
</code></pre>
<p>What is the difference and which should I use</p>
","2024-09-26 04:54:11","3","Question"
"79024522","79020422","","<p>I forgot to add <code>--network host</code> when running the docker container.
After adding this, it works. external IPs are used instead of docker ip.</p>
","2024-09-25 19:34:42","1","Answer"
"79024244","79021064","","<p>I changed the training argument's value for batch_sampler from</p>
<p><code> batch_sampler=BatchSamplers.GROUP_BY_LABEL</code></p>
<p>to</p>
<p><code>batch_sampler=BatchSamplers.NO_DUPLICATES</code></p>
<p>and the problem was solved.  Originally <code>GROUP_BY_LABEL</code> was selected as the <a href=""https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss"" rel=""nofollow noreferrer"">documentation for this loss calculation</a> recommended it but switching it seems to have cleared this up.</p>
","2024-09-25 18:01:20","0","Answer"
"79022993","78997869","","<p>Consider this question as solved. I was really tired and mismatched PyTorch and Tensorflow codes. Sorry! The problem I have is with Tensorflow, not with PyTorch. If there is no solution here, I will create new question with reproducible code included.</p>
","2024-09-25 12:51:23","0","Answer"
"79022620","","I want to strictly use Tensor Cores for running inference of a pretrained full precision CNN model in Pytorch","<p>I have been analyzing the maximum throughput I can get from my device for a specific CNN model using a GPU. My GPU has CUDA cores as well as Tensor cores. So I want to simultaneous run the model on both the type of cores simultaneously and check the maximum possible throughput I can get.</p>
<p>I did use <code>with torch.cuda.amp.autocast()</code> to make sure that the model leverages <em>automatic mixed precision</em> and hence, it should also be able to use tensor cores. However, when I ran the tests with inference running for <code>full precision</code> and <code>amp</code> simultaneously, it provided a throughput which was more than that with full precision (when run separately), but was lesser than that with amp (when run separately). This means that Pytorch is for sure not able to use Tensor Cores, because if that was the case, I would have gotten the throughput which is almost equivalent to the sum of both the cases.</p>
<p>Is there a way I could toggle the use of Tensor Cores so that Pytorch uses them?</p>
","2024-09-25 11:26:27","1","Question"
"79021243","","How to bundle libtorch with my rust binary?","<p>I am developing an AI chat desktop application targeting Apple M chips. The app utilizes embedding models and reranker models, for which I chose Rust-Bert due to its capability to handle such models efficiently. Rust-Bert relies on tch, the Rust bindings for LibTorch.</p>
<p>To enhance the user experience, I want to bundle the LibTorch library, specifically for the MPS (Metal Performance Shaders) backend, with the application. This would prevent users from needing to install LibTorch separately, making the app more user-friendly.</p>
<p>However, I am having trouble locating precompiled binaries of LibTorch for the MPS backend that can be bundled directly into the application via the cargo build.rs file. I need help finding the appropriate binaries or an alternative solution to bundle the library with the app during the build process.</p>
<p>I have set the LD
export DYLD_LIBRARY_PATH=/opt/homebrew/Cellar/pytorch/2.2.0_10/lib:$DYLD_LIBRARY_PATH
but after compilation and running.</p>
<pre><code>*[main][~/Programs/pyano/rust-backend]$ ./target/release/rust-backend

dyld[93114]: Symbol not found: __ZN2at4_ops10layer_norm4callERKNS_6TensorEN3c108ArrayRefINS5_6SymIntEEERKNS5_8optionalIS2_EESC_db
  Referenced from: &lt;1138FCDA-AB1A-3CC6-B540-4DE31C13A6CF&gt; /Users/saurav/Programs/pyano/rust-backend/target/release/rust-backend
  Expected in:     &lt;8F36AFD3-91CB-3828-BD48-4381DB5297F2&gt; /opt/homebrew/Cellar/pytorch/2.2.0_10/libexec/lib/python3.12/site-packages/torch/lib/libtorch_cpu.dylib
</code></pre>
","2024-09-25 05:31:44","1","Question"
"79021064","","SBERT Fine-tuning always stops before finish all epochs","<p>I'm working on a project using the SBERT pre-trained models (specifically <a href=""https://huggingface.co/nreimers/MiniLM-L6-H384-uncased"" rel=""nofollow noreferrer"">MiniLM</a>) for a text classification project with 995 classifications. I am following the steps laid out <a href=""https://www.sbert.net/docs/sentence_transformer/training_overview.html#why-finetune"" rel=""nofollow noreferrer"">here</a> for the most part and everything seems to run.</p>
<p>My issue occurs when actually training the model. No matter what values I set in the training arguments the training always seems to end early and never completes all the batches. For example, I set <code>num_train_epochs=1</code> but it only gets up to 0.49 epochs. If <code>num_train_epochs=4</code>, it always ends at 3.49 epochs.</p>
<p>Here is my code:</p>
<pre><code>from datasets import load_dataset
from sentence_transformers import (
    SentenceTransformer,
    SentenceTransformerTrainer,
    SentenceTransformerTrainingArguments,
    SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
    &quot;nreimers/MiniLM-L6-H384-uncased&quot;,
    model_card_data=SentenceTransformerModelCardData(
        language=&quot;en&quot;,
        license=&quot;apache-2.0&quot;,
        model_name=&quot;all-MiniLM-L6-v2&quot;,
    )
)

loss = BatchAllTripletLoss(model)
# Loss overview: https://www.sbert.net/docs/sentence_transformer/loss_overview.html
# This particular loss method: https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss


# training args: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
    # Required parameter:
    output_dir=&quot;finetune/model20240924&quot;,
    # Optional training parameters:
    num_train_epochs=1,
    max_steps = -1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-5,
    warmup_ratio=0.1,
    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16
    bf16=False,  # Set to True if you have a GPU that supports BF16
    batch_sampler=BatchSamplers.GROUP_BY_LABEL,  # 
    # Optional tracking/debugging parameters:
    eval_strategy=&quot;no&quot;,
    eval_steps=100,
    save_strategy=&quot;epoch&quot;,
   # save_steps=100,
    save_total_limit=2,
    logging_steps=100,
    run_name=&quot;miniLm-triplet&quot;,  # Will be used in W&amp;B if `wandb` is installed
)

trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=trainDataset,
    eval_dataset=devDataset,
    loss=loss,
    #evaluator=dev_evaluator,
)
trainer.train()
</code></pre>
<p>Note that I am not using an evaluator because we are creating the model and testing it after the fact with a dedicated test set of values. My dataset is structured as:</p>
<pre><code>Dataset({
    features: ['Title', 'Body', 'label'],
    num_rows: 23961
})
</code></pre>
<p>with the <code>dev</code> dataset being the same structure, only with fewer rows. This gives the following output:</p>
<pre><code> [1473/2996 57:06 &lt; 59:07, 0.43 it/s, Epoch 0/1]
Step    Training Loss
100     1.265600
200     0.702700
300     0.633900
400     0.505200
500     0.481900
600     0.306800
700     0.535600
800     0.369800
900     0.265400
1000    0.345300
1100    0.516700
1200    0.372600
1300    0.392300
1400    0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={'train_runtime': 3427.9198, 'train_samples_per_second': 6.99, 'train_steps_per_second': 0.874, 'total_flos': 0.0, 'train_loss': 0.5003972503496366, 'epoch': 0.4916555407209613})
</code></pre>
<p>As much as I adjust the values I cannot get it to complete all of the batches. How to resolve this issue?</p>
","2024-09-25 03:55:44","0","Question"
"79020904","79020422","","<p>Check <code>ip addr</code> in your docker container:</p>
<pre><code>3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP
link/ether 22:23:6b:28:6b:e0 brd ff:ff:ff:ff:ff:ff
inet 172.17.42.1/16 scope global docker0
inet6 fe80::a402:65ff:fe86:bba6/64 scope link
   valid_lft forever preferred_lft forever
</code></pre>
<p>Use that IP for your docker container.</p>
","2024-09-25 02:03:16","0","Answer"
"79020422","","How to run pytorch inside docker containers on two GCP VM?","<p>I have two GCP VM. on two vms, I run docker container.</p>
<p>I run
<code>docker run --gpus all -it --rm --entrypoint /bin/bash -p 8000:8000 -p 7860:7860 -p 29500:29500 lf</code></p>
<p>I am trying <a href=""https://github.com/hiyouga/LLaMA-Factory"" rel=""nofollow noreferrer"">llama-factory</a>.</p>
<p>In one container, I run
<code>FORCE_TORCHRUN=1 NNODES=2 RANK=1 MASTER_ADDR=34.138.7.129 MASTER_PORT=29500 llamafactory-cli train examples/train_lora/llama3_lora_sft_ds3.yaml</code>,
where 34.138.7.129 is external ip of vm</p>
<p>In the other container, I run
<code>FORCE_TORCHRUN=1 NNODES=2 RANK=0 MASTER_ADDR=34.138.7.129 MASTER_PORT=29500 llamafactory-cli train examples/train_lora/llama3_lora_sft_ds3.yaml</code>.</p>
<p>But I got</p>
<pre><code>[rank1]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank1]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]: Last error:
[rank1]: socketStartConnect: Connect to 172.17.0.2&lt;49113&gt; failed : Software caused connection abort
E0924 21:26:39.866000 140711615779968 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 484) of binary: /usr/bin/python3.10
Traceback (most recent call last):
  File &quot;/usr/local/bin/torchrun&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py&quot;, line 347, in wrapper
    return f(*args, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py&quot;, line 879, in main
    run(args)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py&quot;, line 870, in run
    elastic_launch(
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py&quot;, line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py&quot;, line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/workspace/LLaMA-Factory/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  &lt;NO_OTHER_FAILURES&gt;
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-24_21:26:39
  host      : 71af1f49abe3
  rank      : 1 (local_rank: 0)
  exitcode  : 1 (pid: 484)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
</code></pre>
<p>it seems that pytorch is using the docker container ip instead of gcp vm external ip.</p>
<p>How to fix this?</p>
","2024-09-24 21:31:29","0","Question"
"79019738","79019610","","<p>Cuda specifically denotes sending data to an Nvidia GPU. You cannot run that code as-is without an Nvidia GPU.</p>
<p>You can update the code to <code>torch.device(&quot;cpu&quot;)</code> to run the code on CPU, or <code>torch.device(&quot;mps&quot;)</code> if you are using a MacOS device with a GPU and have installed pytorch with the <a href=""https://pytorch.org/docs/stable/notes/mps.html"" rel=""nofollow noreferrer"">MPS backend</a></p>
","2024-09-24 17:34:40","1","Answer"
"79019656","","Hashed cross-product transformation in PyTorch","<p>I want to implement a hashed cross product transformation like the one Keras uses:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; layer = keras.layers.HashedCrossing(num_bins=5, output_mode='one_hot')
&gt;&gt;&gt; feat1 = np.array([1, 5, 2, 1, 4])
&gt;&gt;&gt; feat2 = np.array([2, 9, 42, 37, 8])
&gt;&gt;&gt; layer((feat1, feat2))
&lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[0., 0., 1., 0., 0.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 0., 1.],
       [1., 0., 0., 0., 0.],
       [0., 0., 1., 0., 0.]], dtype=float32)&gt;
&gt;&gt;&gt; layer2 = keras.layers.HashedCrossing(num_bins=5, output_mode='int')
&gt;&gt;&gt; layer2((feat1, feat2))
&lt;tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 0, 4, 0, 2])&gt;
</code></pre>
<blockquote>
<p>This layer performs crosses of categorical features using the &quot;hashing trick&quot;. Conceptually, the transformation can be thought of as: hash(concatenate(features)) % num_bins.</p>
</blockquote>
<p>I'm struggling to understand the <code>concatenate(features)</code> part. Do I have to do the hash of each &quot;pair&quot; of features?</p>
<p>In the meantime, I tried with this:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; cross_product_idx = (feat1*feat2.max()+1 + feat2) % num_bins
&gt;&gt;&gt; cross_product = nn.functional.one_hot(cross_product_idx, num_bins)
</code></pre>
<p>It works, but not using a hash function can cause problems with distributions</p>
","2024-09-24 17:01:20","3","Question"
"79019610","","How to run code which use torch.ones([1], device=torch.device(""cuda"")) without GPU?","<p>Goal:</p>
<ol>
<li>Develop PyTorch model in MacBook Pro M2</li>
</ol>
<p>Problem:</p>
<ol>
<li>Some CVPR papers use the &quot;device&quot; parameter and set it to &quot;cuda&quot;.</li>
</ol>
<p>So, if the source code is left unmodified, it always throws an error</p>
<p>Code example</p>
<pre><code>import torch

device = torch.device(&quot;cuda&quot;)
torch.ones([1], device=device)*2
</code></pre>
<p>Output</p>
<pre><code>AssertionError: Torch not compiled with CUDA enabled
</code></pre>
","2024-09-24 16:46:04","0","Question"
"79019507","","The accuracy in the neural network does not change","<p>The code of a fully connected NN (I know what is better convolutional, I will do it further), which determines the numbers from the MNIST dataset. When enabled, accuracy does not change at all. What could be the mistake? I have divided the code cells with lines, and marked with comments what is where. I apologize in advance for a lot of code and long answers.</p>
<pre><code>import os
from torchvision.datasets import MNIST
from torchvision import transforms as tfs


data_tfs = tfs.Compose([
    tfs.ToTensor(),
    tfs.Normalize((0.5), (0.5))
])

# install for train and test
root = './'
train_dataset = MNIST(root, train=True,  transform=data_tfs, download=True)
val_dataset  = MNIST(root, train=False, transform=data_tfs, download=True)

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True) 

valid_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False)
---------------------------------------------------------------
#Creating the model
activation = nn.ELU

model = nn.Sequential(
    nn.Flatten(),
    #YOUR CODE. Add layers to your sequential class
    nn.Linear(in_features=2, out_features=128),
    nn.ELU(),
    nn.Linear(in_features=128, out_features=10),
    nn.ELU()
)
---------------------------------------------------------------
criterion = nn.CrossEntropyLoss()#YOUR CODE. Select a loss function
optimizer = torch.optim.Adam(model.parameters())

loaders = {&quot;train&quot;: train_dataloader, &quot;valid&quot;: valid_dataloader}
---------------------------------------------------------------
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
---------------------------------------------------------------
max_epochs = 10
accuracy = {&quot;train&quot;: [], &quot;valid&quot;: []}

class Identical(nn.Module):
    def forward(self, x):
        return x

activation = nn.ELU

model = nn.Sequential(
    nn.Flatten(),
    #YOUR CODE. Add layers to your sequential class
    nn.Linear(in_features=2, out_features=128),
    nn.ELU(),
    nn.Linear(in_features=128, out_features=10),
    nn.ELU()
)

criterion = nn.CrossEntropyLoss()#YOUR CODE. Select a loss function
optimizer = torch.optim.Adam(model.parameters())

loaders = {&quot;train&quot;: train_dataloader, &quot;valid&quot;: valid_dataloader}

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

for epoch in range(max_epochs):
    for k, dataloader in loaders.items():
        epoch_correct = 0
        epoch_all = 0
        for x_batch, y_batch in dataloader:
            if k == &quot;train&quot;:
              model.train()
              optimizer.zero_grad()
              outp = model(x_batch)
              outp = outp[:y_batch.shape[0]] #otherwise, the last batch is less than necessary
            else:
              model.eval()
              with torch.no_grad():
                outp = model(x_batch)
                outp = outp[:y_batch.shape[0]]
            preds = outp.argmax(-1)
            correct =  (preds == y_batch).sum()
            all =  tuple(preds.shape)[0]
            epoch_correct += correct.item()
            epoch_all += all
            if k == &quot;train&quot;:
                loss = criterion(outp, y_batch)
                loss.backward()
                optimizer.step()
        if k == &quot;train&quot;:
            print(f&quot;Epoch: {epoch+1}&quot;)
        print(f&quot;Loader: {k}. Accuracy: {epoch_correct/epoch_all}&quot;)
        accuracy[k].append(epoch_correct/epoch_all)
</code></pre>
","2024-09-24 16:17:21","0","Question"
"79015947","79013091","","<p>I figured out the solution! I had the wrong version of torch installed (2.4.1 instead of 2.4.0)
Unfortunately, the website doesn't let you download past versions of libtorch, unless you manually edit the url to the correct version.</p>
","2024-09-23 18:55:33","-2","Answer"
"79015794","79015728","","<p>You have created a lot of leaf nodes (gradient-requiring variables), including:</p>
<pre class=""lang-py prettyprint-override""><code>ref_image = apply_ellipse_mask(torch.zeros(image_width, image_height, requires_grad=True), sphere_position, [sphere_radius, sphere_radius, sphere_radius])
</code></pre>
<p>which creates a leaf node (with <code>torch.zeros(image_width, image_height, requires_grad=True)</code>) and applied some computations so you get a computation graph. But then you reuse the result every iteration. You do not recompute it every iteration so you are trying to go backward the same graph several times. The only things that should have <code>require_grad = True</code> are parameters you optimize on.</p>
<blockquote>
<p>You're having a differentiability problem</p>
</blockquote>
<p>You're trying to flow gradient to <code>ellipsoid_axes</code> through computation of the mask, but the computation of the mask is not differentiable in <code>axes</code> (it returns 0-1 anyway). You should make the mask &quot;soft&quot; using some kind of sigmoid instead.</p>
<blockquote>
<p>on your <code>apply_ellipse_mask</code> function</p>
</blockquote>
<p>This is more of a comment as this code will still cause the same error. Avoid for-loops like this with PyTorch as they are slow. Instead you could write:</p>
<pre class=""lang-py prettyprint-override""><code>def apply_ellipse_mask(img, pos, axes):
    r = torch.arange(image_height)[:, None]
    c = torch.arange(image_width)[None, :]
    val_array = ((c - pos[0])**2) / axes[0]**2 + ((r - pos[1])**2) / axes[1]**2
    mask = torch.where(0.9 &lt; val &lt; 1, torch.tensor(1.0),  torch.tensor(0.0))

    return img * (1.0 - mask) + mask
</code></pre>
","2024-09-23 17:59:56","2","Answer"
"79015728","","Why am I getting ""RuntimeError: Trying to backward through the graph a second time""?","<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import random


image_width, image_height = 128, 128

def apply_ellipse_mask(img, pos, axes):
    r = torch.arange(image_height)[:, None]
    c = torch.arange(image_width)[None, :]
    val_array = ((c - pos[0]) ** 2) / axes[0] ** 2 + ((r - pos[1]) ** 2) / axes[1] ** 2
    mask = torch.where((0.9 &lt; val_array) &amp; (val_array &lt; 1), torch.tensor(1.0), torch.tensor(0.0))

    return img * (1.0 - mask) + mask


random.seed(0xced)

sphere_radius = image_height / 3
sphere_position = torch.tensor([image_width / 2, image_height / 2 ,0], requires_grad=True)

ref_image = apply_ellipse_mask(torch.zeros(image_width, image_height, requires_grad=True), sphere_position, [sphere_radius, sphere_radius, sphere_radius])

ellipsoid_pos = torch.tensor([sphere_position[0], sphere_position[1], 0], requires_grad=True)
ellipsoid_axes = torch.tensor([image_width / 3 + (random.random() - 0.5) * image_width / 5, image_height / 3 + (random.random() - 0.5) * image_height / 5, image_height / 2], requires_grad=True)

optimizer = torch.optim.Adam([ellipsoid_axes], lr=0.1)
criterion = torch.nn.MSELoss()
for _ in range(100):

    optimizer.zero_grad()
    current_image = torch.zeros(image_width, image_height, requires_grad=True)
    current_image = apply_ellipse_mask(current_image, ellipsoid_pos, ellipsoid_axes)

    loss = criterion(current_image, ref_image)
    loss.backward()
    print(_, loss)
    optimizer.step()
</code></pre>
<p>Error:</p>
<blockquote>
<p>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</p>
</blockquote>
<p>Why would it be trying to backward through the same graph a second time? Am I directly accessing saved tensors after they were freed?</p>
","2024-09-23 17:37:09","4","Question"
"79013091","","I installed tch in rust and it doesn't work","<p>I created a simple rust project and imported tch, and it doesn't work, even if the program doesn't use it.</p>
<pre><code>use tch::nn;
fn main() {
    println!(&quot;Hello, world!&quot;);
}
</code></pre>
<p>Although the program compiles, running it produces an error:
The procedure entry point __kmpc_masked could not be located in the dynamic link library C:\libtorch
\torch_cpu.dll
I tried reinstalling cuda to fit the torch version (12.4) I tried playing with the dependencies. The current toml file has the following:</p>
<pre><code>[package]
name = &quot;tch_test&quot;
version = &quot;0.1.0&quot;
edition = &quot;2021&quot;

[dependencies]
tch = {version=&quot;0.17.0&quot;, path=&quot;tch-rs&quot;}

[dev-dependencies]
torch-sys = {version=&quot;0.17.0&quot;, path=&quot;tch-rs/torch-sys&quot;}
</code></pre>
<p>However, I also tried just using the dependencies when I installed tch using cargo add (changing the tch to just tch=&quot;0.17.0&quot; and that didn't work either.
I tried setting my environment variables, as was shown in the github page, and even tried to add the LIBTORCH_USE_PYTORCH variable as pytorch does work on python on my machine.
I am using a windows 10 machine with an RTX4060. Help will be greatly appreciated.</p>
<p>Thank You!</p>
<p>Few Clarifications: Firstly, when I say that &quot;it didn't work&quot; I mean that the exact same result happened as specified above i.e. The program compiles but running it gives the message above. To be clear: This is a runtime issue not a compile time issue.</p>
<p>Additionaly, I want to clarify that the error above happens when I run the exe manually. When I run it from cmd, it displays the following error instead: process didn't exit successfully: <code>target\debug\tch_test.exe</code> (exit code: 0xc0000139, STATUS_ENTRYPOINT_NOT_FOUND)</p>
<p>Secondly, this is a minimal reproducible example. This was a new project I created from scratch to try to fix this issue. It is just a new project with the tch imported as specified (I tried 2 ways, via cargo add, and via cloning the github repo). There are no extra files beyond that, just the cloned tch-rs, gitignore, lock files that come by default, and the toml file above (which I also tried the default and it didn't work), nothing else.</p>
<p>I tried following the guidelines on the github page <a href=""https://github.com/LaurentMazare/tch-rs"" rel=""nofollow noreferrer"">https://github.com/LaurentMazare/tch-rs</a> and this tutorial <a href=""https://www.swiftdiaries.com/rust/pytorch/"" rel=""nofollow noreferrer"">https://www.swiftdiaries.com/rust/pytorch/</a> none worked. I even tried to run the example files in the tch-rs I cloned (without doing any modification to the folder) and it produced the same result.</p>
","2024-09-23 03:44:27","-2","Question"
"79011697","78299639","","<p>Restore <code>trainer.state</code> might work (NOT SURE IF THERE IS SIDE EFFECT)</p>
<pre class=""lang-py prettyprint-override""><code>class Evaluator(L.Callback):
    def on_validation_epoch_end(self, trainer: L.Trainer, pl_module: L.LightningModule) -&gt; None:
        trainer_state = deepcopy(trainer.state)
        current_fx_name = pl_module._current_fx_name
        results = trainer.predict(pl_module, return_predictions=True)
        trainer.state = trainer_state
        pl_module._current_fx_name = current_fx_name
        pl_module.log(&quot;val/xxx&quot;, 0, prog_bar=True)
</code></pre>
","2024-09-22 13:13:05","0","Answer"
"79011546","78994910","","<p>Well, you haven't given us much information to go by, so let's discuss this in principle:</p>
<p>Neither loads, nor stores, in CUDA <em>have</em> to update the L1 cache. There are kinds of load or store instructions, or caching policy specifiers for load or store instructions, which may avoid it.</p>
<p>Specifically,</p>
<ul>
<li>The compiler is likely to prefer loads through <a href=""https://stackoverflow.com/q/776283/1593077""><code>__restrict</code></a>'ed pointers to const data via non-coherent read-only cache; see <a href=""https://stackoverflow.com/a/62265013/1593077"">this answer</a> here on SO, or the CUDA. This corresponds to the <a href=""https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-ld-global-nc"" rel=""nofollow noreferrer""><code>ld.global.nc</code> PTX instruction</a>; you might also see this done using the CUDA C++ <code>__ldg()</code> instrinsic.</li>
<li>Under some circumstances, the compiler will emit store instructions with the <code>.cg</code> (cache-global) or <code>.wt</code> (write-through ) cache modifiers, both of which will skip writing to the L1 level. See the <a href=""https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#id109"" rel=""nofollow noreferrer"">PTX store instruction caching preference option table</a>.</li>
</ul>
<p>when both of these happen, the L1 cache hit rate should indeed be 0%. If you're only looking at cache hits during read, then just the first bullet might be enough to result in 0% cache hit rate.</p>
","2024-09-22 11:48:17","0","Answer"
"79009359","78697900","","<p>I have been encountering this issue for quite a while.
How I was able to overcome it was to ensure I installed torch and all it's components before dgl. as shown below.</p>
<p>!pip install torch torchvision torchaudio --index-url <a href=""https://download.pytorch.org/whl/cu121"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/cu121</a></p>
<p>!pip install dgl -f <a href=""https://data.dgl.ai/wheels/torch-2.2/cu121/repo.html"" rel=""nofollow noreferrer"">https://data.dgl.ai/wheels/torch-2.2/cu121/repo.html</a></p>
<p>I hope it helps</p>
","2024-09-21 10:43:03","0","Answer"
"79004502","78746073","","<p>for this setup:</p>
<pre><code>Windows 10
CUDA Version:  12.4
Pytorch Version: 2.4.1
Flash Attention Version: 2.6.3
</code></pre>
<p>run from <code>CMD</code> line by line:</p>
<pre><code>set USE_FLASH_ATTENTION=1
pip uninstall torch
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu124
pip install build
pip install cmake
pip install ninja
pip install wheel
pip install flash-attn --no-build-isolation 
</code></pre>
<p>then in your code whn you initialize the model pass the attention method (Flash Attention 2) like this:</p>
<pre><code>model = transformers.AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, attn_implementation=&quot;flash_attention_2&quot;).to('cuda')
</code></pre>
<p>from python you can always check the versions you are using, run this code:</p>
<pre><code>import torch

# Check what version of PyTorch is installed
print(torch.__version__)

# Check the current CUDA version being used
print(&quot;CUDA Version: &quot;, torch.version.cuda)

# Check if CUDA is available and if so, print the device name
print(&quot;Device name:&quot;, torch.cuda.get_device_properties(&quot;cuda&quot;).name)

# Check if FlashAttention is available
print(&quot;FlashAttention available:&quot;, torch.backends.cuda.flash_sdp_enabled())

</code></pre>
","2024-09-19 21:20:39","5","Answer"
"79002861","78082292","","<p>You can use a custom Dataset and implement the <code>__cat_dim__()</code> method:
<a href=""https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html"" rel=""nofollow noreferrer"">Here's an example from pyg</a> (Section &quot;Batching Along New Dimensions&quot;):</p>
<pre><code>from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

class MyData(Data):
    def __cat_dim__(self, key, value, *args, **kwargs):
        if key == 'foo':
            return None
        return super().__cat_dim__(key, value, *args, **kwargs)

edge_index = torch.tensor([
   [0, 1, 1, 2],
   [1, 0, 2, 1],
])
foo = torch.randn(16)

data = MyData(num_nodes=3, edge_index=edge_index, foo=foo)

data_list = [data, data]
loader = DataLoader(data_list, batch_size=2)
batch = next(iter(loader))

print(batch)
&gt;&gt;&gt; MyDataBatch(num_nodes=6, edge_index=[2, 8], foo=[2, 16])
</code></pre>
<p>Here, instead of creating your data loader, you can just call <code>batch.from_data_list(data_list)</code> and it should work the same</p>
","2024-09-19 13:46:13","0","Answer"
"79001562","78994224","","<p>In PyTorch, you can register hooks to modules (and tensors, incidentally), for <a href=""https://web.archive.org/web/20240917161335/https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook"" rel=""nofollow noreferrer"">backward</a> (respectively <a href=""https://web.archive.org/web/20240917161335/https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook"" rel=""nofollow noreferrer"">forward</a>) passes, to operate on their gradient (respectively output).</p>
<p>You could wrap the multiplication and division operations in separate modules and hook on them.</p>
<p>Something like</p>
<pre class=""lang-py prettyprint-override""><code>from operator import mul, truediv

import torch


class Mul(torch.nn.Module):
    forward = mul


class Div(torch.nn.Module):
    forward = truediv


def hook(module, grad_input, grad_output):
    print(f'in module: {type(module).__name__}, {grad_output=}')


mul = Mul()
div = Div()

mul.register_full_backward_hook(hook)
div.register_full_backward_hook(hook)

x = torch.tensor([4.0], requires_grad=True)
y = torch.tensor([2.0], requires_grad=True)

output = mul(x, y) + div(x, y)

grad_x = torch.ones_like(output)
torch.autograd.grad(output, x, grad_outputs=grad_x)
</code></pre>
","2024-09-19 08:19:08","1","Answer"
"78999845","","Get the attention scores of a pretrained transformer in pytorch","<p>I've been trying to look at the attention scores of a pretrained transformer when I pass specific data in. It's specifically a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"" rel=""nofollow noreferrer"">Pytorch Transformer</a>. I've tried using <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html"" rel=""nofollow noreferrer"">forward hooks</a>, but I'm only able to get the final output of attention modules when what I want is NxN matrices of attention scores (softmax(QxK). I also would really prefer to do this via pytorch code and not use outside tools such as BertViz.</p>
<p>Does anyone know if there's a way to do this?</p>
","2024-09-18 19:06:33","3","Question"
"78997869","","error in PyTorch dataloader with num_workers>0 in VSC under WSL","<p>I want to utilize my GPU by adjusting the workers number, but I have a problem with the number of workers &gt; 0.</p>
<p><code>test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)</code> - no problem<br />
<code>test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=1)</code> - <code>RuntimeError: CUDA error: initialization error</code><br />
Data set is MNIST. The size is 50% / 50% train/test</p>
<p>Its NVIDIA GeForce RTX 3060 Ti, Torch 2.0.1 + CUDA 1.18
GPU memory 8191.50 MB</p>
<p>I work within Visual Studio Code in WSL</p>
<p>Here a reference a to similar case.
<a href=""https://stackoverflow.com/questions/74178302/error-when-using-pytorch-num-workers-in-colab-when-num-workers-0"">Error when using pytorch num_workers in colab. when num_workers &gt; 0</a></p>
<p>Any suggestions?
Thank you!
<strong>&gt;&gt;&gt;EDIT&lt;&lt;&lt;&lt;</strong>
<strong>Demo script</strong></p>
<pre><code>    import tensorflow as tf
    
    # Check GPU availability
    print(&quot;Num GPUs Available: &quot;, len(tf.config.list_physical_devices('GPU')))
    
    # Simple model to test GPU functionality
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    # Dummy data
    import numpy as np
    X_train = np.random.random((1000, 784))
    Y_train = np.random.randint(10, size=(1000,))
    
    # Train model
model.fit(X_train, Y_train, epochs=1, workers=1, use_multiprocessing=True)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>File &quot;/home/kalin_stoyanov/EntMax_TSNE/my_venv/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py&quot;, line 1253, in _internal_apply_gradients

  File &quot;/home/kalin_stoyanov/EntMax_TSNE/my_venv/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py&quot;, line 1345, in _distributed_apply_gradients_fn

  File &quot;/home/kalin_stoyanov/EntMax_TSNE/my_venv/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py&quot;, line 1340, in apply_grad_to_update_var

DNN library initialization failed. Look at the errors above for more details.

**System info** 
---- CPU Information ----
Physical cores: 12
Logical cores: 24

---- Memory Information ----
Total RAM (GB): 15.442893981933594
Available RAM (GB): 12.711883544921875

---- GPU Information ----
PyTorch GPU Count: 1
PyTorch GPU 0: NVIDIA GeForce RTX 3060 Ti
PyTorch Version: 2.0.1+cu118
TensorFlow GPUs: ['/device:GPU:0']
TensorFlow Version: 2.14.0
Keras Version: 2.14.0

---- Operating System Information ----
Operating System: Linux
OS Version: #1 SMP Fri Mar 29 23:14:13 UTC 2024
Machine: x86_64

---- Python Information ----
Python Version: 3.11.4
Python Executable: /home/kalin_stoyanov/EntMax_TSNE/my_venv/bin/python
PyTorch Version: 2.0.1+cu118
TensorFlow Version: 2.14.0
Keras Version: 2.14.0
---- Num GPUs Available ----
Num GPUs Available (TensorFlow): 1
</code></pre>
","2024-09-18 11:11:41","-2","Question"
"78994910","","L1 Cache hit 0% for matrix multiplications","<p>Doing large matrix multiplication operation on a GPU. The kernel profiling shows the L1 cache hit as 0%.The matrix stride is over 4k bytes and cache line size is 128 bytes. What could be the possible reasons for this L1 cache hit?
Thanks</p>
","2024-09-17 16:04:16","0","Question"
"78994224","","Register Hook to Intermediate Nodes in PyTorch","<p>Imagine you have something simple like this:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

x = torch.tensor([4.0], requires_grad=True)
y = torch.tensor([2.0], requires_grad=True)
output = x * y + x / y

grad_x = torch.ones_like(output)
torch.autograd.grad(output, x, grad_outputs=grad_x)
</code></pre>
<p>which results in a computational graph like this:</p>
<p><a href=""https://i.sstatic.net/9fEqzyKN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9fEqzyKN.png"" alt=""enter image description here"" /></a></p>
<p>Now I'd like to access the gradients of the intermediate nodes i.e. <code>MulBackward0</code>, <code>DivBackward0</code> and <code>AddBackward0</code>. I know that PyTorch doesn't store it by default. I know I I can make it explicit and then define <code>retain_grad</code> etc.</p>
<p>But is there the possibiltiy of just attaching a hook to any of those nodes without having to loop through the graph etc.?</p>
","2024-09-17 13:10:51","0","Question"
"78993466","78993384","","<p>Fixed already by using Box (python-box) library and converting the OmegaConf object to a dictionary using</p>
<p><code>config_dict = OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)</code></p>
<p>And then converting the config_dict to a Box object when constructing the model:</p>
<pre><code>import box
class Model(L.LightningModule):
    def __init__(self,
                 cfg,
                 class_weights):
        super().__init__()

        self._cfg = box.Box(cfg)
</code></pre>
","2024-09-17 09:45:49","0","Answer"
"78993384","","Bug when logging models using Lightning Trainer, WandB and Hydra?","<p>Our setup:</p>
<p>Training script using pytorch lightning Trainer &amp; WandB Logger. Further, Hydra is used to configure the training setup (Datasets, Hyperparameters, Optimizer...)
The Model object is an instance of LightningModule and saves the hydra config (in OmegaConf type) to <strong>self.cfg</strong></p>
<p>When I use
<code>wandb_logger = WandbLogger(log_model='all', project=cfg.experiment_name, name=cfg.experiment_name)</code></p>
<p>The model artifact which is then logged to my Weights and Biases instance also contains a Run Config which is badly structured; each character of my cfg is a value of a key-value pair.
<a href=""https://i.sstatic.net/cWU0OzUg.png"" rel=""nofollow noreferrer"">enter image description here</a>
I suppose that the wandb logger iterates over all iterable properties the model has and loggs them separately.</p>
<p>As I don't want to lose the very well readable dot-notation from OmegaConf in my training script, i ask here:
Is there a way to fix this issue?</p>
<p>I went through the doc of WandB - I tried to find the property &quot;Run Config&quot; of the Model Artifact (wasn't able to find that). In general, it was not possible to find that exact badly structured config representation in any of my objects retreiveable by the Wandb Api.</p>
","2024-09-17 09:22:53","0","Question"
"78992078","78990033","","<p>Don't loop. Among many other vectorised options, consider:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

groups = np.array((
    (0, 0, 0, 1, 1, 1, 1, 3, 3, 5, 5, 5, 5),
    (2, 2, 2, 2, 3, 3, 4, 5, 5, 5, 6, 6, 6),
))
yd, xd = np.diff(a=groups, axis=1).nonzero()
subtract = np.zeros_like(groups)
subtract[yd, xd] = xd
x = np.arange(groups.shape[1])
result = x - np.maximum.accumulate(subtract, axis=1)
print(result)
</code></pre>
","2024-09-16 23:06:45","2","Answer"
"78991787","78990033","","<p>I followed this example, it works for me:
<a href=""https://stackoverflow.com/a/18197211/10255067"">https://stackoverflow.com/a/18197211/10255067</a></p>
<pre><code>import torch

v = torch.tensor( [[0, 0, 0, 1, 1, 1, 1, 3, 3, 5, 5, 5, 5],  
                   [5, 5, 5, 5, 3, 3, 4, 5, 5, 5, 6, 6, 6]])
d = ~torch.diff(v).bool()
n = torch.concat(  ( torch.zeros( v.size(0),1),d),   1).view(-1)
c=torch.cumsum(n,0)
c= torch.concat(      ( torch.tensor([0]), torch.diff( c[n==0] )     ))
n[n==0]=c.neg()
n=torch.cumsum( n,0).view(v.size())
print( n )
</code></pre>
<p>OUTPUT:</p>
<pre><code>tensor([[0., 1., 2., 0., 1., 2., 3., 0., 1., 0., 1., 2., 3.],
        [0., 1., 2., 3., 0., 1., 0., 0., 1., 2., 0., 1., 2.]])
</code></pre>
","2024-09-16 20:59:52","0","Answer"
"78991050","78990748","","<p>The nans don't come from the gradient, the nans come from the forward pass. These are multiplied by gradient values in the backward pass (chain rule).</p>
<p>Take a simpler example. Set exactly one value in <code>y</code> to nan:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.rand(10, 10) 
y = torch.rand(10, 10)
w = torch.rand(10, 10, requires_grad=True)
y[0,0] = torch.nan
</code></pre>
<p>Now compute your intermediates and retain gradients</p>
<pre class=""lang-py prettyprint-override""><code>o = w@x
o.retain_grad()

l = (y - o).pow(2)
l.retain_grad()

l_nonnan = l[~y.isnan()]
l_nonnan.retain_grad()

l_nonnan.mean().backward()
</code></pre>
<p>Inspect the gradients</p>
<ul>
<li><code>l_nonnan</code> has full gradients</li>
<li><code>l</code> has full gradients except for <code>l.grad[0,0]</code> which is <code>0</code></li>
<li><code>o</code> has a nan gradient at <code>o.grad[0,0]</code></li>
<li><code>w</code> has nan gradients for the entire first row</li>
</ul>
<p>This is due to how the computation propagates. We set <code>y[0,0] = torch.nan</code>. We compute <code>l = (y - o).pow(2)</code> this means <code>o[0,0]</code> is nan because it directly interacts with the nan from <code>y</code>.</p>
<p><code>o</code> is created via <code>o = w@x</code>. This means the value at <code>o[0,0] = (w[0] * x[:,0]).sum()</code>. When we run the computation in reverse in backprop, the gradient of <code>o[0,0]</code> (which we know to be nan) propagates back to all ements of <code>w[0]</code>. This is why the entire row has nan gradients.</p>
<p>When you set a bunch of nans randomly, you get the same effect on more elements.</p>
<p>You can avoid this via <code>l = (y[~y.isnan()] - o[~y.isnan()])**2</code> because when you do that you prevent the nans in <code>y</code> from entering the computation in the first place.</p>
","2024-09-16 16:43:52","0","Answer"
"78990748","","Torch backward PowBackward0 causes nan gradient where it shouldn't","<p>I have a pytorch tensor with NaN inside, when I calculate the loss function using a simple MSE Loss the gradient becomes NaN even if I mask out the NaN values.</p>
<p>Weirdly this happens only when the mask is applyied after calculating the loss and only when the loss has a pow operation inside. The various cases follow</p>
<pre><code>import torch
torch.autograd.set_detect_anomaly(True)

x = torch.rand(10, 10) 
y = torch.rand(10, 10)
w = torch.rand(10, 10, requires_grad=True)
y[y &gt; 0.5] = torch.nan


o = w @ x
l = (y - o)**2
l = l[~y.isnan()]

try:
    l.mean().backward(retain_graph=True)
except RuntimeError:
    print('(y-o)**2 caused nan gradient')

l = (y - o)
l = l[~y.isnan()]

try:
    l.mean().backward(retain_graph=True)
except RuntimeError():
    pass
else:
    print('y-o does not cause nan gradient')

l = (y[~y.isnan()] - o[~y.isnan()])**2
l.mean().backward()
print('masking before pow does not propagate nan gradient')
</code></pre>
<p>What makes NaN gradients propagate when passing through the backward pass of the pow function?</p>
","2024-09-16 15:04:08","0","Question"
"78990538","78990033","","<p>My understanding from your example is that the output give cumulative counts of each unique element in the tensor (per row). The following function should do the job. It may be possible to get rid of the <code>for</code> loop but I will leave it for someone else.</p>
<pre><code>import torch

def get_cumulative_counts(tensor):
    # Create a tensor to store the cumulative counts
    counts_tensor = torch.zeros_like(tensor)

    # Perform a diff operation to find where values change
    diff = tensor[:, :-1] != tensor[:, 1:] 

    # where a new element starts
    starts = torch.cat((torch.ones(tensor.size(0), 1, dtype=torch.bool, device=tensor.device), diff), dim=1)
    
    # First element count is always 0
    for i in range(1, tensor.size(1)):
        counts_tensor[:, i] = counts_tensor[:, i - 1] + 1  # Increment count
        counts_tensor[:, i] *= ~starts[:, i]  # Reset to 0 when a new element starts

    return counts_tensor

t = torch.tensor([[0, 0, 0, 1, 1, 1, 1, 3, 3, 5, 5, 5, 5], 
                [2, 2, 2, 2, 3, 3, 4, 5, 5, 5, 6, 6, 6]], dtype=torch.int64)

print(get_cumulative_counts(t))

# tensor([[0, 1, 2, 3, 0, 1, 2, 0, 1, 0, 1, 2, 3],
#        [0, 1, 2, 3, 0, 1, 0, 0, 1, 2, 0, 1, 2]])
</code></pre>
","2024-09-16 14:06:57","1","Answer"
"78990341","78990169","","<p>A linear layer is just a matrix multiplication in the end and therefore only works on 1 vector at a time. It multiplies the incoming vector with the weight matrix and outputs a vector (to which then the bias is added).
In pytorch, nn.Linear is specifically coded to accept N-dimensional tensors as an input (which isn't necessarily a standard feature of any linear layer elsewhere). It then applies the same weight-matrix and bias vector to the input vector-by-vector as you already suggested. You cannot perform a matrix multiplication (what a linear layer is in the end) on more than 2-dimensions (it wouldn't be a matrix anymore), so it cannot be applied to the whole input at once.</p>
<p>The code of PyTorch is usually heavily optimised and I don't think this is a for-loop that does this vector by vector. Instead it will take advantage of parallel processing capacities for these kind of operations present in CPUs and GPUs to apply the same weight matrix and bias vector to all of the input vectors at once.</p>
<p>For example: If you have an input of (8, 32, 32, 3) and want to reduce the last dimension to size 1, you could use a linear layer <code>nn.Linear(in_features=3,out_features=1)</code>. Your input can technically be taken apart to be 8<em>32</em>32 = 8192 3-dimensional vectors. These linear vectors will then be multiplied with the same weight matrix of the linear layer to produce 8192 1-dimensional vectors, and then recombined to be the (8, 32, 32, 1) shaped output.
See also this <a href=""https://stackoverflow.com/questions/58587057/multi-dimensional-inputs-in-pytorch-linear-method"">question</a> and the linear C++ code <a href=""https://github.com/pytorch/pytorch/blob/bbc3fdbbde3f35467b40cf06ec4926adca1876d4/aten/src/ATen/native/Linear.cpp#L73"" rel=""nofollow noreferrer"">here</a></p>
","2024-09-16 13:15:47","1","Answer"
"78990323","78990169","","<p>Pytorch uses matrix multiplication. The specific implementation would be chosen by the dispatcher, but in no case would the naive &quot;vector-by-vector inner product&quot; algorithm be used.</p>
<p>The c++ entry point of <code>torch.nn.functional.linear</code> (which is called by <code>nn.Linear.forward</code>) is <a href=""https://github.com/pytorch/pytorch/blob/bbc3fdbbde3f35467b40cf06ec4926adca1876d4/aten/src/ATen/native/Linear.cpp#L73"" rel=""nofollow noreferrer"">here</a>. For most cases they use <code>addmm</code> or <code>matmul</code>, which both perform optimized versions of matrix multiplication.</p>
","2024-09-16 13:09:40","1","Answer"
"78990169","","How does multidimensional input to a nn.Linear layer work?","<p>When sending a mulditimensional tensor to a nn.Linear layer, how does it work in practice? Does it just process the input vector by vector, or does it actually perform matrix multiplication over the whole input at once?</p>
<p>If it's the former, is there a way to perform the latter operation (multiplying the input tensor by the weights as is, and not one vector at a time)? Will it be faster than doing it vector by vector?</p>
","2024-09-16 12:25:42","1","Question"
"78990033","","counting each unique element in tensor (per row)","<p>I got a sorted tensor like this (in pytorch):</p>
<pre><code>[[0, 0, 0, 1, 1, 1, 1, 3, 3, 5, 5, 5, 5], 
 [2, 2, 2, 2, 3, 3, 4, 5, 5, 5, 6, 6, 6]]
</code></pre>
<p>How can I count unique elements per row to get this :</p>
<pre><code>[[0, 1, 2, 0, 1, 2, 3, 0, 1, 0, 1, 2, 3], 
 [0, 1, 2, 3, 0, 1, 0, 0, 1, 2, 0, 1, 2]]
</code></pre>
","2024-09-16 11:50:50","0","Question"
"78984724","78982976","","<p>You shouldn't use <code>Sequential</code> for this. Just define your own class and implement your own custom logic, like so:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

class CustomModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(CustomModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.tanh = nn.Tanh()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.tanh(self.fc1(x))
        
        x = self.fc2(x)
        
        # Sigmoid for the first column
        x[:, 0] = self.sigmoid(x[:, 0])
        # Tanh for the rest of the columns
        x[:, 1:] = self.tanh(x[:, 1:])
        
        return x

input_size = 2
hidden_size = 10
output_size = 5
model = CustomModel(input_size, hidden_size, output_size)

x = torch.randn(100, 2)

y = model(x)
print(y)
</code></pre>
","2024-09-14 09:05:18","1","Answer"
"78982976","","Is it possible to set different activation functions for different outputs at the final layer in the neural net?","<p>I have a simple neural net - some linear layers with tanh between layers and after the end of the net. For example, I have input tensor with shape (100, 2) and I want the output to be with size (100, 5). But the values in the first column are in the range [0, 1], that is it is suitable to have sigmoid activation function at the end. The values in another columns are in the range [-1, 1], that is the &quot;tanh&quot; activation function could be used. But I don't understand how to set the sigmoid for the first output column and the &quot;tanh&quot; for another outputs? Is it possible? Or, I should apply abs() for the first column in the outputs and set the &quot;tanh&quot; after the final linear layer?</p>
<p>Now I have the following model:</p>
<pre><code>nn.Sequential(nn.Linear(input_size, hidden_size),
nn.Tanh(),
nn.Linear(hidden_size, output_size),
nn.Tanh())

y = model(x)
y[:,0] = torch.abs(y[:,0])
</code></pre>
<p>But I want:</p>
<pre><code>model = nn.Sequential(nn.Linear(input_size, hidden_size),
nn.Tanh(),
nn.Linear(hidden_size, output_size))
</code></pre>
<p>and to apply nn.Sigmoid() for the first output and nn.Tanh() for the other outputs:</p>
<pre><code>y = model(x)
act_1 = nn.Sequential(nn.Sigmoid())
act_2 = nn.Sequential(nn.Tanh())

y[:,0] = act_1(y[:,0])
y[:,1:] = act_2(y[:,1:])
</code></pre>
","2024-09-13 16:22:59","2","Question"
"78981536","78981052","","<p>I solved it for now by creating a buffer:</p>
<pre><code>import torch
gpu = torch.device('cuda')
cpu = torch.device('cpu')

a = torch.rand((13223,134,4), dtype=torch.float32, device=cpu)
b = torch.rand((13223,134,4), dtype=torch.bfloat16, device=gpu)

for i in range(3):
    
    b.mul_(0.5)
    
    buffer = b.to(device=cpu, memory_format=torch.preserve_format, dtype=torch.bfloat16, non_blocking=True)
    
    torch.cuda.synchronize()
    
    a[:] = buffer        
    
    print(b[0,0], a[0,0])
</code></pre>
","2024-09-13 09:35:19","0","Answer"
"78981402","78974702","","<p>I'm not sure how you tell it to use your system libraries for this.</p>
<p>But for example if you want the CPU only version of torch you can do this:</p>
<pre class=""lang-ini prettyprint-override""><code>[tool.poetry.dependencies]
torch = { version = &quot;&gt;=2.0.0&quot;, source = &quot;pytorch-cpu&quot;}


[[tool.poetry.source]]
name = &quot;pytorch&quot;
url = &quot;https://download.pytorch.org/whl/cpu&quot;
priority = &quot;explicit&quot;
</code></pre>
<p>You can see the list of special builds here <a href=""https://download.pytorch.org/whl/"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/</a></p>
<p>This link is also helpful <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a></p>
","2024-09-13 08:53:58","1","Answer"
"78981400","78981052","","<p><a href=""https://www.google.com/search?q=non%20blocking%20assignment&amp;oq=non%20blocking%20as&amp;gs_lcrp=EgZjaHJvbWUqCAgFEAAYFhgeMgYIABBFGDkyBwgBEAAYgAQyBwgCEAAYgAQyBwgDEAAYgAQyCAgEEAAYFhgeMggIBRAAGBYYHjIICAYQABgWGB4yCAgHEAAYFhgeMgYICBBFGDzSAQkxODQzNWowajGoAgCwAgA&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow noreferrer"">Non-blocking assignments literally do not block the execution of the next statements.</a></p>
<p>You assume</p>
<pre><code>a[:] = b.to(device=cpu, memory_format=torch.preserve_format, dtype=torch.bfloat16, non_blocking=True)
</code></pre>
<p>executed as</p>
<p>b_gpu -&gt; b_cpu<br />
b_cpu -&gt; a</p>
<p>But with <code>non_blocking</code>, order of theese actions is not specified, allowing parallel execution. Yes, it gives speed! But <code>b_cpu -&gt; a</code> works faster, and as it not wait for <code>b_gpu -&gt; b_cpu</code>, order of execution effectively is</p>
<p>b_cpu -&gt; a<br />
b_gpu -&gt; b_cpu</p>
","2024-09-13 08:53:22","0","Answer"
"78981052","","Pytorch incorrect results with non_blocking assignment from Cuda to CPU","<p>I'm trying to assign a tensor on CPU with values I just obtained from GPU, however getting incorrect results, both tensors should be the same obviously:</p>
<p>(To avoid any unnecessary chatter, I'd like to mention beforehand that I'm also aware of exactly <em><strong>two</strong></em> other different methods of copying such as 1) using <code>copy_</code> method and 2) by overwriting a variable altogether. However I find the first slower and second producing memory overhead, based on some of my tests. The assignment with semicolon seems to achieve best performance so far, albeit the obtained results are incorrect when <code>non_blocking</code> option is used.)</p>
<pre><code>import torch
gpu = torch.device('cuda')
cpu = torch.device('cpu')

a = torch.rand((13223,134,4), dtype=torch.float32, device=cpu)
b = torch.rand((13223,134,4), dtype=torch.bfloat16, device=gpu)

for i in range(3):
    
    b.mul_(0.5)
    
    a[:] = b.to(device=cpu, memory_format=torch.preserve_format, dtype=torch.bfloat16, non_blocking=True)
    
    torch.cuda.synchronize()
    
    print(b[0,0], a[0,0])
</code></pre>
<h1></h1>
<pre><code>tensor([0.0942, 0.1621, 0.2041, 0.1543], device='cuda:0', dtype=torch.bfloat16) tensor([0., 0., 0., 0.])
tensor([0.0471, 0.0811, 0.1021, 0.0771], device='cuda:0', dtype=torch.bfloat16) tensor([0.0942, 0.1621, 0.2041, 0.1543])
tensor([0.0236, 0.0405, 0.0510, 0.0386], device='cuda:0', dtype=torch.bfloat16) tensor([0.0471, 0.0811, 0.1021, 0.0771])
&gt;&gt;&gt;
</code></pre>
<p>In my application the results are even weirder, as some of the values become negative! (...despite that none of them are negative in the original tensor). Appreciate any input in advance, the goal is to transfer data to CPU quickly and efficiently, and if you know any other methods that I haven't mentioned that will help me achieve that, do leave a comment. :)</p>
","2024-09-13 07:10:08","2","Question"
"78975490","78975293","","<p>Predicting the sum of two variables is a linear task and very easy for neural networks. <strong>At the core of a neuron it calculates a weighted sum of its input values.</strong></p>
<hr />
<p>This &quot;network&quot; below is fully sufficient to solve the task in just one epoch.
The math behind the network is: <code>x*weight1 + y*weight</code> and it just needs to learn to set both its weights to 1.0.</p>
<pre class=""lang-py prettyprint-override""><code>  class myModel(nn.Module):
    def __init__(self):
      super().__init__()

      self.input = nn.Linear(2,1)

    def forward(self,x):
      return self.input(x)
</code></pre>
<p>By using <code>relu</code> you make it actually harder for the network as negative values cannot go trough. Still the network below with two units is still enough for this easy task.</p>
<pre class=""lang-py prettyprint-override""><code>def createModel():
  class myModel(nn.Module):
    def __init__(self):
      super().__init__()

      self.input = nn.Linear(2,2)
      self.output = nn.Linear(2,1)

    def forward(self,x):
      x = F.relu(self.input(x))
      return self.output(x)
</code></pre>
<p>The math behind the network is <code>relu(x@weights11 + y@weights12)@weight2</code> now just with weights of length two.
Ideally will the network now return in layer1 <code>[-(x+y), x+y]</code>, which after the relu it will contain <code>0</code> and <code>abs(x+y)</code> the second layer now needs to only fix the sign and multiply by 1 or -1 to yield (x+y).</p>
<hr />
<p>In the first variant form the network is basically a <a href=""https://en.wikipedia.org/wiki/Linear_regression"" rel=""nofollow noreferrer"">linear regression</a> and you can think of the second one as two chained regression. I recommend that you familiarize yourself with this little bit of math, because in the end some basic good to knows are:</p>
<ul>
<li><strong>linear neurons are just a weighted sum of its inputs followed optionally by an activation.</strong></li>
<li>A layer of neurons is just a collection of different weights for the inputs</li>
<li>Other neuron types change only the order how inputs and weights are combined, e.g. convolutional neurons apply identical weights to inputs selected from a grid.</li>
</ul>
<p>So yes your model learns very quickly to do this simply task and it could even be much faster ;)</p>
","2024-09-11 19:46:38","1","Answer"
"78975293","","Linear FFNN Model achieving 100% accuracy in predicting sums","<p>I have a model that adds numbers between -10 to positive 10. The task is simply to predict the sum of those numbers. However the train accuracy quickly reaches 100%.<br />
I am not sure if the model is just quickly training, or if there is something wrong and it is not properly learning. Can anyone give some insight why this happens here?</p>
<p>Here is my code and minimal working example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader,TensorDataset
from sklearn.model_selection import train_test_split

import numpy as np

# Create the dataset
data = []
labels = []

datasetAmount = 2000

for i in range(datasetAmount):
  x = np.random.randint(-10, 10)
  y = np.random.randint(-10,10)
  bothNumber = [x, y]  # Inputs
  data.append(bothNumber)
  labels.append(x+y)  # Targets

data_np = np.array(data)
labels_np = np.array(labels).reshape(-1,1)

train_data, test_data, train_labels, test_labels = train_test_split(data_np, labels_np, train_size =.9)

train_data = TensorDataset(torch.tensor(train_data),torch.tensor(train_labels))
test_data = TensorDataset(torch.tensor(test_data),torch.tensor(test_labels))

batchsize = 20

train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True, drop_last = True)
test_loader = DataLoader(test_data, batch_size = test_data.tensors[0].shape[0])

# Model Factory
def createModel():
  class myModel(nn.Module):
    def __init__(self):
      super().__init__()

      self.input = nn.Linear(2,8)
      self.fc1 = nn.Linear(8,8)
      self.output = nn.Linear(8,1)

    def forward(self,x):
      x = F.relu( self.input(x) )
      x = F.relu( self.fc1(x) )
      return self.output(x)

  net = myModel()
  lossfun = nn.MSELoss()
  optimizer = torch.optim.SGD(net.parameters(),lr=.001)

  return net,lossfun,optimizer


def trainModel():

  numepochs = 100
  net,lossfun,optimizer = createModel()
  losses   = torch.zeros(numepochs)
  trainacc = []
  testacc = []

  for epochi in range(numepochs):
    batchLoss = []

    for X,y in train_loader:
      X = X.float()
      y = y.float()
      yHat = net(X)

      loss = lossfun(yHat,y)
      batchLoss.append(loss.item())

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

    losses[epochi] = np.mean(batchLoss)

    with torch.no_grad():
      train_predictions = []
      train_labels = []
      for x_train, y_train in train_loader:
        x_train = x_train.float()
        y_train = y_train.float()
        train_pred = net(x_train)
        train_predictions.append(train_pred)
        train_labels.append(y_train)

      train_predictions = torch.cat(train_predictions)
      train_labels = torch.cat(train_labels)

      train_acc = 100 * torch.mean((np.abs(train_predictions - train_labels) &lt; 1).float())
      trainacc.append(train_acc.item())


    X,y = next(iter(test_data))
    X = X.float()  # Convert X to float for test data
    y = y.float()  # Convert y to float for test data
    with torch.no_grad():
      yHat = net(X)

    testacc= 100*torch.mean((np.abs(yHat-y)&lt; 1).float())

  return trainacc,testacc,losses,net


trainAcc, testAcc, losses , net = trainModel()
</code></pre>
","2024-09-11 18:43:48","2","Question"
"78974702","","Poetry PyTorch dependency exclude cuda as I want to use the system cuda","<p>I have these depencencies:</p>
<p><code>deepmultilingualpunctuation = &quot;^1.0.1&quot;</code></p>
<p>In the tree I see it requires <code>torch</code></p>
<pre><code>deepmultilingualpunctuation 1.0.1 A python package for deep multilingual punctuation prediction.
├── torch &gt;=1.8.1
│   ├── filelock * 
│   ├── fsspec * 
│   ├── jinja2 * 
│   │   └── markupsafe &gt;=2.0 
│   ├── networkx * 
│   ├── nvidia-cublas-cu12 12.1.3.1 
│   ├── nvidia-cuda-cupti-cu12 12.1.105 
│   ├── nvidia-cuda-nvrtc-cu12 12.1.105 
│   ├── nvidia-cuda-runtime-cu12 12.1.105 
</code></pre>
<p>As you can see it includes the nvidia libs. These are already on my system, so I don't want to pull them in all my <code>venvs</code></p>
<p>I tried adding a <code>torch, extras=[cpu]</code> dependency, but that does not help.</p>
<p>What is the proper solution to let Pytorch use the provided system libs?</p>
","2024-09-11 15:52:21","1","Question"
"78972856","78970089","","<p>Thanks to @Kari for clarifying in the comments.
So in this particular case there was a problem with the default StorageOptions in Eigen, which is column major by default. This is fixed version of implementation sketch:</p>
<pre class=""lang-cpp prettyprint-override""><code>constexpr static int input_channels = 160;
constexpr static int output_channels = 64;
constexpr static int batch_number = 20;
constexpr static int kernel_size = ...;

typedef Eigen::Matrix&lt;double, 1, input_channels&gt; Input_t;
typedef Eigen::TensorFixedSize&lt;double, Eigen::Sizes&lt;output_channels, batch_number, kernel_size&gt;&gt; Conv1d_t;
auto conv1d(
    const Input_t&amp; raw_input,
    const Conv1d_t&amp; weights) noexcept {

    constexpr auto seq = input_channels/batch_number - kernel_size + 1;
    constexpr auto batch_size = input_channels / batch_number;
    auto input = mapInto&lt;batch_number, batch_size, Eigen::RowMajor&gt;(raw_input);
    auto output = Eigen::Matrix&lt;double, output_channels, 1&gt;();
    output.setZero();

    for (int out_ch = 0; out_ch &lt; output_channels; out_ch++)
        for (int s = 0; s&lt;seq; s++)
            for (int bs = 0; bs &lt; batch_number; bs++)
                for (int k=0; k&lt;kernel_size; k++)
                    output(out_ch, s) += input(bs, k + s) * weights(out_ch, bs, k);

    return output.transpose();
}
</code></pre>
","2024-09-11 08:52:15","0","Answer"
"78972436","78972393","","<p>Getting some hints from chat-bots and trying different locations, looks like <code>~/.cache/torch/hub/checkpoints/</code> is the location at which the downloads are stored by the code:</p>
<pre class=""lang-bash prettyprint-override""><code>ls ~/.cache/torch/hub/checkpoints/ -1

face_parsing.farl.celebm.main_ema_181500_jit.pt
mobilenet0.25_Final.pth
resnet18-f37072fd.pth
</code></pre>
<p>The manually downloaded file can be sent to that location on server by:</p>
<pre class=""lang-bash prettyprint-override""><code>scp ~/Downloads/face_parsing.farl.celebm.main_ema_181500_jit.pt arisa@IP.IP.IP.IP:/home/arisa/.cache/torch/hub/checkpoints/
</code></pre>
","2024-09-11 07:01:30","-1","Answer"
"78972393","","Download location of Pytorch","<h1>Download fail</h1>
<p>I'm running this Python code:</p>
<p><a href=""https://github.com/SimonGiebenhain/MonoNPHM/blob/05aafd8e7dbe3168bee7d5f93f47537acb877df3/scripts/preprocessing/run_facer.py#L153"" rel=""nofollow noreferrer"">https://github.com/SimonGiebenhain/MonoNPHM/blob/05aafd8e7dbe3168bee7d5f93f47537acb877df3/scripts/preprocessing/run_facer.py#L153</a></p>
<p>It fails at downloading a file and then throws the following errors:</p>
<pre class=""lang-py prettyprint-override""><code>failed downloading from https://github.com/FacePerceiver/facer/releases/download/models-v1/face_parsing.farl.celebm.main_ema_181500_jit.pt
Traceback (most recent call last):
  File &quot;/home/arisa/MonoNPHM/scripts/preprocessing/run_facer.py&quot;, line 212, in &lt;module&gt;
    tyro.cli(main)
  File &quot;/home/arisa/.conda/envs/mononphm/lib/python3.9/site-packages/tyro/_cli.py&quot;, line 229, in cli
    return run_with_args_from_cli()
  File &quot;/home/arisa/MonoNPHM/scripts/preprocessing/run_facer.py&quot;, line 153, in main
    face_parser = facer.face_parser('farl/celebm/448', device=device)  # optional &quot;farl/lapa/448&quot;
  File &quot;/home/arisa/.conda/envs/mononphm/lib/python3.9/site-packages/facer/__init__.py&quot;, line 36, in face_parser
    return FaRLFaceParser(conf_name, device=device, **kwargs).to(device)
  File &quot;/home/arisa/.conda/envs/mononphm/lib/python3.9/site-packages/facer/face_parsing/farl.py&quot;, line 69, in __init__
    self.net = download_jit(model_path, map_location=device)
  File &quot;/home/arisa/.conda/envs/mononphm/lib/python3.9/site-packages/facer/util.py&quot;, line 162, in download_jit
    return torch.jit.load(cached_file, map_location=map_location, **kwargs)
  File &quot;/home/arisa/.conda/envs/mononphm/lib/python3.9/site-packages/torch/jit/_serialization.py&quot;, line 162, in load
    cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]
RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
</code></pre>
<p>The corrupt download is the root cause of the last error:</p>
<p><a href=""https://stackoverflow.com/q/71617570/3405291"">PytorchStreamReader failed reading zip archive: failed finding central directory</a></p>
<blockquote>
<p>RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory</p>
</blockquote>
<h1>Manual download</h1>
<p>I have downloaded the following manually:</p>
<p><a href=""https://github.com/FacePerceiver/facer/releases/download/models-v1/face_parsing.farl.celebm.main_ema_181500_jit.pt"" rel=""nofollow noreferrer"">https://github.com/FacePerceiver/facer/releases/download/models-v1/face_parsing.farl.celebm.main_ema_181500_jit.pt</a></p>
<p>But I'm not sure where to place it on the server so that the Python/Pytorch code would use it. Can anyone tell me the download location?</p>
","2024-09-11 06:52:07","-1","Question"
"78970464","78959131","","<p>I ran into this issue before and just clearing the cache with <code>torch.cuda.empty_cache()</code> did not help. You need to explicitly clear the allocated memory on cuda via <a href=""https://pytorch.org/docs/stable/generated/torch.cuda.reset_peak_memory_stats.html"" rel=""nofollow noreferrer""><code>torch.cuda.reset_peak_memory_stats()</code></a>. Include these lines into your <code>run_vllm_eval()</code> function:</p>
<pre><code>torch.cuda.reset_peak_memory_stats()
torch.cuda.synchronize()
</code></pre>
<p>Here's a complete code snippet with some borrowed code from <code>vllm</code>'s <a href=""https://docs.vllm.ai/en/latest/getting_started/quickstart.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<pre><code>import torch
import gc
from vllm import LLM, SamplingParams

def run_vllm_eval(model_name, sampling_params, path_2_eval_dataset):
    # Instantiate LLM in a function
    llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

    # Run some VLLM inference or evaluation here (simplified)
    result = llm.generate(path_2_eval_dataset, sampling_params)
    print(result)

    # Clean up after inference
    del llm
    gc.collect()
    torch.cuda.empty_cache()
    
    # Reset CUDA device to fully clear memory
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.synchronize()  # Wait for all streams on the current device

prompts = [
    &quot;Hello, my name is&quot;,
    &quot;The president of the United States is&quot;,
    &quot;The capital of France is&quot;,
    &quot;The future of AI is&quot;,
]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

run_vllm_eval(model_name=&quot;facebook/opt-125m&quot;, sampling_params=sampling_params, path_2_eval_dataset=prompts)
run_vllm_eval(model_name=&quot;facebook/opt-125m&quot;, sampling_params=sampling_params, path_2_eval_dataset=prompts)
</code></pre>
","2024-09-10 16:32:00","3","Answer"
"78970089","","Implementation of torch.nn.Conv1d in C++","<p>I'm looking for comprehend explanation or implementation of PyTorch.Conv1d layer in C++.<br />
Let's define conv1d layer as:</p>
<pre class=""lang-py prettyprint-override""><code>BATCH_SIZE = 20
INPUT_SIZE = 160
OUTPUT_SIZE = 64
KERNEL_SIZE = int(self.INPUT_SIZE / self.BATCH_SIZE) # 8
l = torch.nn.Conv1d(BATCH_SIZE, OUTPUT_SIZE,
                             kernel_size=KERNEL_SIZE,
                             bias=False, dtype=torch.float64))
# and the input
inp = some_lin_x_160.view(BATCH_SIZE, -1)
l(inp)
</code></pre>
<p>Although all sources I found specify the output size like: <code>(inp_channels - kernel_size + 1)/stride</code> I'm not entirely clear how pytorch handles arbitrary(defined by client code) out_channels.
This is the very naive implementation of the layer based on Eigen library, and I'm pretty sure it's wrong.</p>
<pre class=""lang-cpp prettyprint-override""><code>typedef Eigen::Matrix&lt;double, 1, 160&gt; Input_t;
typedef Eigen::TensorFixedSize&lt;double, Eigen::Sizes&lt;64, 20, 8&gt;&gt; Conv1d_t;
Eigen::Matrix&lt;double, 1, 64&gt; conv1d(
    const Input_t&amp; raw_input,
    const Conv1d_t&amp; weights) noexcept {

    auto input = Eigen::Matrix&lt;T, 20, 8&gt;(raw_input.data());
    auto output = Eigen::Matrix&lt;double, 64, 1&gt;();
    output.setZero();

    for (int out_ch = 0; out_ch &lt; 64; out_ch++) {
        for (int bs = 0; bs &lt; 20; bs++)
            for (int k=0; k&lt;8; k++)
                output(out_ch, 0) += input(bs, k) * weights(out_ch, bs, k);
    }
    return output.transpose();
}
</code></pre>
<p>I'm really appreciate for the any help with the question.</p>
","2024-09-10 14:50:11","0","Question"
"78968182","78965629","","<p>You can use <code>nn.Sequential</code> to combine all layers easily and <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear"" rel=""nofollow noreferrer""><code>nn.LazyLinear</code> for the first layer, which accepts all input sizes</a>.</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(nn.Module):
    def __init__(self, out_first=19):
        super(MyModel, self).__init__()
        layers = [nn.LazyLinear(out_first)]
        out = out_first
        for _ in range(9):
            layers.append(nn.ReLU())
            layers.append(nn.Linear(out, out - 1))
            out -= 1
        self.fc = nn.Sequential(*layers)

    def forward(self, x):
        return self.fc(x)

model = MyModel()
</code></pre>
","2024-09-10 06:38:51","0","Answer"
"78967925","","Torch DataLoader for a custom requirement","<p>I have an ordered dataset(shuffle=False) that is categorised into &quot;bins&quot;. I shall present an example on smaller scale that helps to clarify. Let's say the size of dataset is 60 with bins of sizes 10,20,30. I want to train my model in the order of bins. (first with 10 then 20 and 30). I want my DataLoader to get data in batch_sizes of 8. In this case, after getting the first 8 <code>datapoints</code>, I don't want to get the 2 remaining from bin-1 and get 6 from next one. What I want is to get only 2 and in the next iteration, get the 8 from bin-2. In short, I want to complete training in one bin first before moving to other. Also if batch_size happens to be greater than bin size, I want to get data in solely one bin before moving to next.</p>
<p>Can I please get some advice on how to do this? I could think of two ways: implementing a custom DataLoader(need advice on this too) or just create separate DataLoaders for each bin and while iterating with bins in the outermost loop, grab the corresponding DataLoader and do training. Will the latter method have some serious downsides?</p>
","2024-09-10 04:44:20","0","Question"
"78966233","78963755","","<p>Transposing the weight matrix makes the backward pass more efficient while adding almost no overhead to the forward pass. You can read more about this <a href=""https://discuss.pytorch.org/t/why-does-the-linear-module-seems-to-do-unnecessary-transposing/6277"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/pytorch/pytorch/issues/2159"" rel=""nofollow noreferrer"">here</a></p>
","2024-09-09 15:40:09","0","Answer"
"78965629","","How can I fix this neural network model so that it matches the scheme?","<p>I'm starting to study neural networks and I can't complete such a task. Here's the assignment.</p>
<p>Create a class called MyModel, with which you can create a neural network, as in the picture. In this model, each subsequent linear layer has one neuron less than the previous one. When creating the MyModel class, use the <code>nn.ModuleList</code> class.</p>
<p>Create a neural network model with an input tensor size of 20 and an output tensor size of 10. Write the result to the model variable.</p>
<p><a href=""https://i.sstatic.net/pEwlE1fg.png"" rel=""nofollow noreferrer"">here is the scheme</a></p>
<p>This is the code I got:</p>
<pre><code>import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self, inp, out):
        super(MyModel, self).__init__()
        layers = []
        for i in range(9):
            layers.append(nn.Linear(inp, inp - 1))
            layers.append(nn.ReLU())
            inp -= 1
        layers.append(nn.Linear(inp, out))
        self.layers = nn.ModuleList(layers)

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

model = MyModel(inp=20, out=10)
</code></pre>
<p>Here is the error that comes out when executing this code: &quot;The created model does not match the model in the diagram. Note that the size of the input and output tensors on the inner layers do not depend on the size of the tensor at the entrance to and exit from the network.&quot;</p>
<p>But I can't figure out where the error is, please help me figure it out</p>
","2024-09-09 13:08:52","0","Question"
"78965003","78963755","","<p>Transpose is not required, because you can multiply by the matrix either from right or left. In particular, multiply from the left <code>(out_features, in_features) * (in_features)</code>, to get  <code>(out_features)</code> vector.</p>
<p>The benefit is deep in computer architecture. Matrix <code>(out_features, in_features)</code> is stored row by row in memory, e.g. for 3 <code>in_features</code>, it looks like that in flat memory</p>
<pre><code>[in0, in1, in2], [in0, in1, in2], ..., [in0, in1, in2]
</code></pre>
<p>This allows to access memory in <a href=""https://stackoverflow.com/questions/16699247/what-is-a-cache-friendly-code""><em>cache-friendly</em> way</a>, because you read 3 consecutive numbers to compute the first output feature, then read 3 consecutive numbers to compute the second output feature, etc</p>
","2024-09-09 10:27:21","2","Answer"
"78963755","","Why does nn.Linear(in_features, out_features) use a weight matrix of shape (out_features, in_features) in PyTorch?","<p>I’m trying to understand why PyTorch’s <code>nn.Linear(in_features, out_features)</code> layer has its weight matrix with the shape <code>(out_features, in_features)</code> instead of <code>(in_features, out_features)</code>.</p>
<p>From a basic matrix multiplication perspective, it seems like having the shape <code>(in_features, out_features)</code> would eliminate the need for transposing the weight matrix during multiplication. For example, with an input tensor <code>x</code> of shape <code>(batch_size, in_features)</code>, the multiplication with a weight matrix of shape <code>(in_features, out_features)</code> would result directly in an output of shape <code>(batch_size, out_features)</code>, without requiring the transpose operation.</p>
<p>However, PyTorch defines the weight matrix as <code>(out_features, in_features)</code>, meaning it gets transposed during the forward pass. What is the benefit of this design? How does it align with the broader principles of linear algebra and neural network implementations? Are there any efficiency or consistency considerations behind this choice that make it preferable?</p>
","2024-09-09 03:08:49","1","Question"
"78961289","78959447","","<p>To perform efficient matrix multiplication with a tridiagonal matrix on the GPU using PyTorch, you can try those leads :</p>
<ul>
<li>Use batched tridiagonal matrix multiplication: PyTorch provides some support for batched operations on banded matrices using torch.bmm (batched matrix-matrix multiplication), but it requires you to handle the band structure yourself. For a tridiagonal matrix, we only need to store the main, upper, and lower diagonals.</li>
<li>Custom implementation with CUDA kernels or use specialized libraries: For more advanced optimization, consider writing custom CUDA kernels for tridiagonal matrix multiplication or use specialized libraries like cuSPARSE or SciPy.</li>
</ul>
<p>I try an approach of it :</p>
<pre><code>import torch

def band_matrix_mult(A: torch.Tensor, B: torch.Tensor) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Multiplies a tridiagonal matrix A with matrix B, assuming A is tridiagonal.
    
    Args:
    - A (torch.Tensor): Tridiagonal matrix of shape (N, N).
    - B (torch.Tensor): Matrix to multiply with A, of shape (N, M).

    Returns:
    - torch.Tensor: Resulting matrix of shape (N, M).
    &quot;&quot;&quot;
    N, M = B.shape

    # Extract diagonals from A
    main_diag = torch.diagonal(A)  # Shape: [N]
    upper_diag = torch.diagonal(A, offset=1)  # Shape: [N-1]
    lower_diag = torch.diagonal(A, offset=-1)  # Shape: [N-1]

    # Initialize result matrix
    result = torch.zeros_like(B)

    # Multiply main diagonal
    result += main_diag.unsqueeze(-1) * B

    # Multiply upper diagonal
    result[:-1] += upper_diag.unsqueeze(-1) * B[1:]

    # Multiply lower diagonal
    result[1:] += lower_diag.unsqueeze(-1) * B[:-1]

    return result

# Example usage
N = 5
A = torch.tensor([[2., 3., 0., 0., 0.],
                  [1., 2., 3., 0., 0.],
                  [0., 1., 2., 3., 0.],
                  [0., 0., 1., 2., 3.],
                  [0., 0., 0., 1., 2.]])
B = torch.randn((5, 4))

# Efficient tridiagonal multiplication
result = band_matrix_mult(A, B)
print(result)

</code></pre>
<p>if we talk about performance :
This custom function leverages the tridiagonal structure for efficiency and should have a time complexity of
𝑂(𝑁^2⋅𝑀), which is significantly better than the 𝑂(𝑁^3) complexity of dense matrix multiplication.</p>
<p>If you're working with extremely large matrices or need even more performance optimization, consider using GPU-based libraries or writing a custom CUDA kernel to take full advantage of GPU parallelism for sparse matrix operations.</p>
","2024-09-07 22:44:10","1","Answer"
"78960600","78958840","","<p>There is an alternative option where to apply the gradient mask. You can zero gradients just before making the optimizer step.</p>
<pre class=""lang-py prettyprint-override""><code>    for _ in range(num_batches):

        output = model(data)
        loss = criterion(output, targets)

        optimizer.zero_grad()
        loss.backward()
        
        # apply zero mask before making optimizer step
        matrix.grad *= weight_mask
        optimizer.step()
</code></pre>
<p>Alternative if you need only one row to be static split data in rows and apply <code>register_buffer</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn as nn
import torch 

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.row1 = nn.Parameter(torch.zeros(1, 2))
        self.register_buffer(&quot;row2&quot;, torch.zeros(1, 2))
        self.matrix = torch.cat((self.row1, self.row2))
</code></pre>
","2024-09-07 16:24:13","0","Answer"
"78959447","","Efficient PyTorch band matrix to dense matrix multiplication","<p><strong>Problem:</strong> In one of my programs, I need to calculate a matrix multiplication <code>A @ B</code> where both are of size N by N for considerably large N. I'm conjecturing that approximating this product by using <code>band_matrix(A, width) @ B</code> could just suffice the needs, where <code>band_matrix(A, width)</code> denotes a band matrix part of <code>A</code> with width <code>width</code>. For example, <code>width = 0</code> gives the diagonal matrix with diagonal elements taken from <code>A</code> and <code>width = 1</code> gives the tridiagonal matrix taken in a similar manner.</p>
<p><strong>My try:</strong> I'm trying to extract the tridiagonal matrix, for instance, in the following way:</p>
<pre class=""lang-py prettyprint-override""><code># Step 1: Extract the main diagonal
main_diag = torch.diagonal(A, dim1=-2, dim2=-1)  # Shape: [d1, d2, N]

# Step 2: Extract the upper diagonal (offset=1)
upper_diag = torch.diagonal(A, offset=1, dim1=-2, dim2=-1)  # Shape: [d1, d2, N-1]

# Step 3: Extract the lower diagonal (offset=-1)
lower_diag = torch.diagonal(A, offset=-1, dim1=-2, dim2=-1)  # Shape: [d1, d2, N-1]

# Step 4: Reconstruct the tridiagonal matrix
# Main diagonal
tridiag = torch.diag_embed(main_diag)  # Shape: [d1, d2, N, N]

# Upper diagonal (shift the values to create the first upper diagonal)
tridiag += torch.diag_embed(upper_diag, offset=1)

# Lower diagonal (shift the values to create the first lower diagonal)
tridiag += torch.diag_embed(lower_diag, offset=-1)
</code></pre>
<p>but I'm not sure if <code>tridiag @ B</code> would be much more efficient than the original <code>A @ B</code> or just the same complexity since Torch may not know the specific structure to <code>tridiag</code>. In theory, computation with a tridiagonal matrix should be <code>N</code> times faster.</p>
<hr />
<p>Any help with understanding PyTorch's behaviour in this type of scenario or implementing some alternative GPU optimized approaches would be greatly appreciated.</p>
","2024-09-07 05:46:17","0","Question"
"78959131","","VLLM Objects Cause Memory Errors When Created in a Function even when explicitly clearing GPU cache, only sharing ref makes code not crash","<p>I'm encountering an issue when using the VLLM library in Python. Specifically, when I create a VLLM model object inside a function, I run into memory problems and cannot clear the GPU memory effectively, even after deleting objects and using <code>torch.cuda.empty_cache()</code>.</p>
<p>The problem occurs when I try to instantiate a <code>LLM</code> object inside a function, but it does not happen if I instantiate the object in the parent process or global scope. This suggests that VLLM has issues with creating and managing objects in functions, which leads to memory retention and GPU exhaustion.</p>
<p>Here’s a simplified version of the code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import gc
from vllm import LLM

def run_vllm_eval(model_name, sampling_params, path_2_eval_dataset):
    # Instantiate LLM in a function
    llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

    # Run some VLLM inference or evaluation here (simplified)
    result = llm.generate([path_2_eval_dataset], sampling_params)

    # Clean up after inference
    del llm
    gc.collect()
    torch.cuda.empty_cache()

# After this, GPU memory is not cleared properly and causes OOM errors
run_vllm_eval()
run_vllm_eval()
run_vllm_eval()
</code></pre>
<p>but</p>
<pre class=""lang-py prettyprint-override""><code>llm = run_vllm_eval2()
llm = run_vllm_eval2(llm)
llm = run_vllm_eval2(llm)
</code></pre>
<p>Works.</p>
<p>Even after explicitly deleting the LLM object and clearing the cache, the GPU memory is not properly freed, leading to out-of-memory (OOM) errors when trying to load or run another model in the same script.</p>
<p>Things I've Tried:</p>
<ul>
<li>Deleting the LLM object with del.</li>
<li>Running gc.collect() to trigger Python's garbage collection.</li>
<li>Using torch.cuda.empty_cache() to clear CUDA memory.</li>
<li>Ensuring no VLLM objects are instantiated in the parent process.</li>
</ul>
<p>None of these seem to fix the issue when the LLM object is created within a function.</p>
<p>Questions:</p>
<ul>
<li>Has anyone encountered similar memory issues when creating VLLM objects inside functions?</li>
<li>Is there a recommended way to manage or clear VLLM objects in a function to prevent GPU memory retention?</li>
<li>Are there specific VLLM handling techniques that differ from standard Hugging Face or PyTorch models in this context?</li>
</ul>
","2024-09-07 00:58:59","1","Question"
"78958840","","How to create a gradient mask in PyTorch","<p>I have a 2 X 2 tensor in PyTorch. I want the first row entries of this tensor (matrix) to be fixed, i.e., I do not want the first row entries to be parameters that are trained using gradient descent, and the second row entries be updated by the training process (using gradient descent). I saw online that I can use the register_buffer function in PyTorch to create a gradient_mask but I am not sure about the exact implementation.</p>
<pre><code>matrix = torch.nn.Parameter(torch.zeros(2, 2))
register_buffer(‘gradient_mask’, ???)
</code></pre>
<p>Also, after I create this gradient_mask I do not know how to apply it to my variable: matrix. Any thoughts? Thanks.</p>
","2024-09-06 21:35:41","1","Question"
"78955537","78955311","","<p>TL;DR: Uninstall your pytorchvideo package and use the git version <code>pip install git+https://github.com/facebookresearch/pytorchvideo</code>. You need git installed. Or just download the repo, and run `pip install &lt;repo_folder&gt; locally.</p>
<hr />
<p>Basically, the pip package is old, very old. One year ago, there was <a href=""https://github.com/facebookresearch/pytorchvideo/commit/eb04d1b21e08cfd0713164c0907aeb4c98fd83af"" rel=""nofollow noreferrer"">a commit</a> to remove the import in <code>transforms.augmentations</code>. The module which is not found is now deprecated. The module named <code>functional_tensor</code> was renamed <code>_functional_tensor</code> to mark the depreciation.</p>
<p>Note from the <a href=""https://github.com/facebookresearch/pytorchvideo/blob/main/INSTALL.md"" rel=""nofollow noreferrer"">install notice</a> of <code>torchvideo</code>, there is a nightly version of the package. Although, the package is not very much &quot;nightly&quot; since the <a href=""https://pypi.org/project/pytorchvideo-nightly/#history"" rel=""nofollow noreferrer"">last update</a> is 2 years old. It would be better to install a nightly package rather than the git package. But in my case, the build seems broken. pip is looping with some errors messages. Just note that using the repo as source might expose you to some unstabilities.</p>
","2024-09-06 03:43:06","1","Answer"
"78955311","","pytorchvideo.transforms.RandAugment import error","<p>I am trying to run <code>from pytorchvideo.transforms import RandAugment</code> in Python, but it returns the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'
</code></pre>
<p>I can run <code>import pytorchvideo</code> just fine. I tried uninstalling and reinstalling both <code>pytorchvideo</code> and <code>torchvision</code> but the problem persists. Why would this happen?</p>
","2024-09-06 01:28:31","0","Question"
"78954332","78950394","","<p>It doesn't matter, the parameters are tracked both ways. If you use <code>shared_block = ...</code>, the parameters in <code>shared_block</code> will be referenced in your state dict (<code>model.state_dict()</code>) twice, once for <code>self.nested1</code> and again for <code>self.nested2</code>.</p>
<p>If you use the <code>self.shared_block = ...</code> approach, the state dict will reference the parameters a third time in <code>MyModule</code> itself.</p>
<p>Either way, the parameters are tracked and <code>model.parameters()</code> will return a non-duplicated set of parameters.</p>
<p>You can run this code to look at a simplified version</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn

class SharedBlock(nn.Module):
    def __init__(self):
        super().__init__()

        self.block = nn.Linear(8, 8)

    def forward(self, x):
        return self.block(x)

class MyNestedModule(nn.Module):
    def __init__(self, shared_block):
        super().__init__()

        self.shared_block = shared_block

    def forward(self, x):
        return self.shared_block(x)
    
class MyModule1(nn.Module):
    def __init__(self):
        super().__init__()
        shared_block = SharedBlock()

        self.nested1 = MyNestedModule(shared_block)
        self.nested2 = MyNestedModule(shared_block)

    def forward(self, x):
        x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
        y_1 = self.nested1(x_1)
        y_2 = self.nested2(x_2)
        return y_1, y_2
    
class MyModule2(nn.Module):
    def __init__(self):
        super().__init__()
        self.shared_block = SharedBlock()

        self.nested1 = MyNestedModule(self.shared_block)
        self.nested2 = MyNestedModule(self.shared_block)

    def forward(self, x):
        x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
        y_1 = self.nested1(x_1)
        y_2 = self.nested2(x_2)
        return y_1, y_2
    
    
model1 = MyModule1()
print(model1.state_dict())
print(list(model1.parameters()))

model2 = MyModule2()
print(model2.state_dict())
print(list(model2.parameters()))
</code></pre>
","2024-09-05 18:06:33","1","Answer"
"78954261","78953156","","<p>You can access the weights of a model by calling <code>model.named_parameters()</code>.</p>
<p>In your case, <code>for i in effnet.named_parameters(): print(i)</code> returns a list of 2-tuples, where the first item in the tuple is the name of the parameter, and the second item is the actual parameter tensor.</p>
<p>Here is a sample extract so you can see what I mean:</p>
<pre><code>[('_conv_head.weight', Parameter containing:
tensor([[[[ 0.0074]],

         [[-0.0904]],

         [[-0.0091]],

         ...,

         [[-0.0504]],

         [[ 0.1140]],

         [[-0.0710]]],


        [[[ 0.0635]],

         [[ 0.0290]],

         [[-0.0583]],

         ...,

         [[-0.0813]],

         [[-0.0230]],

         [[-0.1120]]],


        [[[ 0.0873]],

         [[-0.0174]],

         [[-0.0170]],

         ...,

         [[-0.0014]],

         [[ 0.0323]],

         [[ 0.0569]]],


        ...,


        [[[-0.0218]],

         [[ 0.0292]],

         [[ 0.0267]],

         ...,

         [[-0.0044]],

         [[-0.0463]],

         [[-0.0257]]],


        [[[-0.0209]],

         [[ 0.0279]],

         [[ 0.0094]],

         ...,

         [[-0.1759]],

         [[-0.0702]],

         [[-0.0902]]],


        [[[-0.0488]],

         [[ 0.0276]],

         [[-0.0174]],

         ...,

         [[-0.0391]],

         [[-0.0268]],

         [[-0.0205]]]], requires_grad=True)), 
('_bn1.weight', Parameter containing:
tensor([2.3115, 2.0343, 2.0015,  ..., 1.7868, 2.3552, 1.8885],
       requires_grad=True)), 
('_bn1.bias', Parameter containing:
tensor([-1.8066, -1.4178, -1.3111,  ..., -1.0494, -1.8520, -1.1798],
       requires_grad=True)), 
('_fc.weight', Parameter containing:
tensor([[-0.0157, -0.0483,  0.0139,  ..., -0.0201, -0.0092, -0.0752],
        [-0.0100, -0.0575,  0.0328,  ..., -0.0362, -0.0534, -0.0041],
        [-0.0083,  0.0142, -0.0006,  ...,  0.0151, -0.0033,  0.0500],
        ...,
        [-0.0840, -0.0217, -0.0354,  ..., -0.0620,  0.0143,  0.0786],
        [-0.0956, -0.0169,  0.0738,  ...,  0.1063, -0.0742,  0.0036],
        [ 0.0485,  0.0470,  0.1002,  ..., -0.0832,  0.1081,  0.0145]],
       requires_grad=True)), 
('_fc.bias', Parameter containing:
tensor([-1.8788e-04, -2.1204e-02, -2.2974e-02, -3.5067e-02, -4.2469e-02,
        -4.0472e-02, -3.3574e-02,  7.5904e-03, -1.3298e-02, -1.3364e-02,
        -4.7947e-02, -6.8513e-02, -5.1592e-02, -4.0660e-02, -2.1086e-02,
        -5.7097e-02, -8.5144e-02, -4.0252e-02,  1.5397e-02, -2.6116e-02,
        -7.2120e-02, -4.8167e-02, -4.5482e-02, -5.7588e-02, -6.5176e-02,
        -4.3350e-02, -6.1431e-03, -3.3575e-02, -1.6232e-02,  4.4864e-03,
        -7.7549e-02, -6.1085e-02, -3.7735e-02, -4.0341e-02,  1.7911e-03,
        -7.5653e-02,  3.0368e-02, -3.7621e-02, -1.5108e-02, -1.8987e-02,
        -7.0831e-02, -4.8989e-02, -4.6129e-02, -3.8295e-02, -2.3450e-02,
        -7.5764e-02, -9.7884e-03, -2.0963e-02, -5.0398e-02, -3.1158e-02,
        -4.3633e-02,  2.1600e-02,  2.2160e-02,  9.6163e-03, -3.5956e-02,
         2.3973e-03, -3.3229e-02, -5.7656e-02, -1.0077e-02, -1.7506e-02,
         3.1533e-02,  4.6831e-02, -1.2585e-02, -5.1598e-03,  2.8223e-02,
        -8.0142e-03, -3.1187e-03, -5.5687e-02, -1.6465e-02, -6.9663e-02,
        -1.0505e-02, -1.2698e-02, -4.9928e-02,  2.3078e-02, -3.1174e-02,
        -9.6878e-03,  3.8667e-02,  1.0131e-02,  2.0751e-02,  1.1730e-03,
        -2.4952e-02, -6.8574e-02, -2.2635e-02, -8.7558e-02, -7.1596e-02,
        -9.8193e-04, -3.9943e-02,  1.3720e-02, -1.0468e-02, -8.8728e-03,
        -6.1661e-02, -6.7819e-02, -4.3548e-02, -6.7823e-02, -1.1364e-01,
        -8.0226e-02, -3.1601e-02, -4.1108e-02, -1.0578e-01, -2.4910e-02,
        -3.6718e-02, -6.0854e-03, -1.7552e-02,  4.8444e-02, -1.0772e-01,
        -8.1519e-02,  1.1975e-02, -5.9317e-02, -4.8866e-02, -1.0751e-01,
        -4.0855e-02, -2.0209e-02,  2.0903e-02, -4.4489e-02,  4.7728e-03,
        -1.0581e-01, -6.0772e-02, -5.2845e-02,  5.1716e-02, -6.6787e-02,
        -6.2687e-02,  8.3072e-03, -7.7149e-03, -8.9555e-02, -4.8636e-02,
        -7.7438e-02, -2.0582e-02, -3.4283e-02, -6.6013e-02, -8.3553e-02,
        -1.7237e-02,  1.0309e-02, -1.3888e-02, -8.5651e-02, -1.0593e-02,
        -8.5910e-02, -4.4669e-02, -4.7769e-02, -5.6502e-02, -3.7197e-02,
        -5.9685e-02, -6.1591e-02, -3.5832e-02, -3.9733e-02, -2.6279e-03,
        -7.5230e-03, -4.8190e-02, -5.2629e-02,  1.3794e-03, -6.2900e-02,
         1.2298e-02,  3.3339e-02, -3.2597e-02,  2.7527e-02, -1.2776e-02,
         4.9703e-02,  1.7361e-02,  2.0815e-02, -3.7160e-02,  3.7085e-02,
        -3.6573e-02,  6.2998e-02,  6.3216e-02,  3.9644e-02,  2.1697e-02,
        -7.6096e-02, -3.1489e-02, -4.0166e-02,  3.0928e-02,  1.8492e-02,
         3.6115e-03,  6.6122e-02,  3.6215e-02,  1.1731e-02,  6.3609e-02,
        -5.1814e-02, -2.1708e-02,  2.3418e-02,  1.1752e-01, -5.7509e-05,
        -9.9666e-03,  3.7792e-02,  5.2852e-02,  2.7368e-02,  8.6873e-02,
         5.4690e-03,  1.4777e-02,  1.8022e-02,  1.4611e-02,  8.3892e-02,
        -2.4939e-02,  9.1274e-02,  6.7359e-04,  1.2745e-02, -3.5555e-02,
         1.3300e-01,  4.2316e-02,  3.8856e-02, -1.3746e-02,  7.2174e-02,
        -3.3013e-02,  2.3095e-02,  4.3574e-02,  9.0962e-02, -2.4382e-03,
         1.9608e-02, -9.1173e-03,  5.3899e-02,  4.8285e-02,  8.5972e-02,
        -1.0665e-02,  3.8411e-02,  3.8558e-03,  5.2799e-03,  1.4861e-02,
         3.9330e-02,  7.3648e-03,  8.2185e-02,  5.4206e-02,  5.7331e-03,
         6.0913e-02, -2.7097e-02,  9.5242e-03,  5.7349e-02, -8.2799e-03,
         3.3900e-02, -4.3817e-02,  7.2375e-03, -7.7864e-03,  6.6191e-03,
        -1.6398e-03,  3.7898e-03,  2.0504e-02, -2.7386e-02,  3.3018e-02,
         2.3809e-02,  6.7705e-02,  7.4084e-02,  4.5319e-02,  4.4979e-02,
         2.6763e-02,  2.5106e-02,  5.7135e-02,  4.2436e-02, -5.5549e-02,
         8.4225e-02, -1.8866e-02,  5.4292e-02,  7.3890e-02,  4.6438e-02,
         2.8466e-02,  9.7005e-02, -1.9982e-02,  5.4305e-02,  5.3359e-02,
         4.3933e-03,  1.2966e-02, -1.3056e-02,  2.8389e-02,  6.9528e-03,
        -9.5808e-03,  1.3317e-02, -5.5829e-02,  4.7666e-03,  3.1541e-02,
         2.3011e-02,  4.1077e-02,  5.9078e-02, -5.6244e-02, -1.7187e-02,
         4.5811e-02,  3.0973e-02,  1.8721e-02,  8.7556e-03, -1.3825e-02,
         1.6460e-03, -6.1395e-02, -1.6215e-02, -9.6606e-03, -3.0871e-02,
         2.9623e-02,  4.9153e-02,  4.9580e-02, -1.5432e-02,  4.9673e-02,
         3.5004e-02, -3.5870e-02, -4.6010e-02, -3.7892e-02, -5.9807e-03,
         7.9569e-03, -8.6313e-02, -2.0522e-02, -2.7805e-02, -4.1220e-02,
        -4.9287e-02, -9.1146e-03, -2.2379e-02, -3.7943e-02, -2.0446e-02,
        -6.7551e-02, -2.6903e-02,  3.5022e-03, -4.7364e-02, -1.0330e-01,
        -6.1197e-02, -1.9515e-02, -6.4015e-02, -3.7570e-02, -4.0806e-02,
        -1.7590e-02, -6.0645e-02, -3.7628e-02,  2.2510e-02,  6.6436e-02,
        -3.2913e-02, -7.2844e-02, -8.4280e-02, -9.0194e-04, -3.3313e-02,
        -5.0213e-02, -6.0932e-02, -6.3614e-02, -6.2003e-02, -1.8194e-02,
        -7.3311e-02, -6.6968e-02, -1.9100e-02, -6.7069e-02, -9.9181e-02,
        -2.4951e-02, -2.4315e-02, -2.8859e-02,  2.5487e-02,  1.7767e-03,
        -4.7624e-02, -1.6528e-02, -2.6242e-02, -2.0435e-02, -4.6075e-03,
         1.4161e-02, -1.7872e-02, -2.7038e-02, -2.0696e-02, -3.1628e-02,
         3.1667e-02,  1.2643e-02,  3.3634e-02, -2.2169e-02, -1.3101e-02,
        -2.6296e-02, -2.9903e-02, -5.1367e-02, -2.5846e-02,  1.1791e-02,
        -2.0295e-02,  2.2274e-02,  3.4522e-02,  9.1384e-02,  5.7635e-02,
        -3.7340e-02,  4.5483e-02,  9.2709e-03, -3.4176e-03, -1.8723e-02,
         4.7653e-02, -4.3498e-02,  1.5783e-02, -2.6597e-03,  4.8437e-02,
         5.4055e-03,  5.2496e-02, -7.5699e-02, -7.2362e-03, -2.4031e-02,
        -3.8313e-02, -5.7455e-02, -3.2797e-02, -1.0619e-02, -3.5333e-02,
         6.8303e-02,  4.2202e-02,  1.2961e-02, -1.0357e-02, -7.2061e-02,
         3.5756e-02,  1.0100e-02, -6.8939e-03, -3.2524e-03,  1.9981e-02,
        -7.9577e-02, -9.9632e-03, -6.4924e-02, -7.7214e-02, -4.2186e-03,
         7.7079e-03, -6.8280e-02, -9.0313e-02,  5.5780e-02,  4.0597e-02,
         1.5803e-02,  7.6331e-02,  8.0219e-02, -4.2492e-02, -5.0285e-02,
         2.8535e-02, -5.1758e-02, -5.5653e-02, -2.6481e-02,  8.4919e-02,
         3.3701e-02, -4.7973e-02,  4.3328e-02,  5.4467e-02, -3.5105e-02,
        -2.8679e-02,  3.2789e-02, -3.0762e-02, -5.7570e-03,  6.3135e-02,
         2.6109e-02,  4.5541e-02,  1.5574e-02,  3.4254e-02,  2.1206e-02,
         8.7765e-03, -1.5536e-02,  8.7542e-02,  2.8484e-02,  7.4923e-02,
         2.7793e-03,  3.0007e-02,  4.5130e-02,  3.4480e-02,  1.4038e-02,
         1.0104e-01,  8.9144e-03, -6.6594e-03,  2.3476e-02,  2.1639e-02,
         8.8072e-02,  3.4813e-02, -6.1209e-02, -1.9577e-02, -1.3342e-02,
         8.3109e-02, -5.9933e-02,  4.5758e-02,  8.2524e-02, -3.3844e-02,
        -2.8108e-02, -1.2548e-02,  2.2878e-03, -3.2685e-03,  4.4044e-02,
         8.5652e-02,  3.7278e-03,  7.4127e-02, -4.7680e-02, -1.0711e-02,
        -1.9743e-03,  2.8187e-02,  3.1251e-02,  1.3193e-01,  1.3656e-02,
        -8.9823e-03, -8.9561e-03, -3.4255e-02,  2.8345e-02,  3.1666e-02,
         5.3442e-02,  2.4717e-02, -1.8101e-02,  2.4845e-02, -1.5233e-02,
         1.0711e-01, -2.0983e-02, -4.5709e-02, -8.3504e-04, -2.5989e-02,
         1.4240e-02,  1.3953e-02, -1.6013e-02,  9.2497e-04, -3.4750e-02,
        -3.9424e-02,  7.6613e-02, -1.2504e-02,  1.2788e-01,  7.7914e-02,
         6.1930e-02,  1.4219e-03, -2.5443e-02,  6.1517e-03,  6.1918e-02,
        -2.7085e-02,  7.3593e-03, -7.3484e-02,  2.8937e-02,  9.8165e-02,
         2.0922e-03,  4.2529e-02, -1.1309e-02, -4.9490e-02,  1.6751e-02,
         1.0809e-02,  2.3618e-02,  2.0515e-02,  3.3854e-02, -1.1171e-01,
        -9.9522e-03, -2.2146e-02,  1.3169e-02,  1.1341e-01,  6.5784e-02,
         1.1520e-01, -1.2588e-02, -7.9052e-02, -1.4347e-02,  4.7605e-02,
         3.4523e-02, -4.4671e-02,  1.1775e-02,  1.0015e-01,  2.6564e-02,
        -1.3457e-02, -4.4883e-02,  3.8352e-02,  1.2466e-02, -1.1996e-03,
         1.0156e-01, -9.3025e-02,  1.9178e-02, -3.7339e-02, -3.1499e-02,
         1.3383e-02, -2.9213e-02, -4.8970e-02, -4.5686e-02,  2.8906e-02,
        -5.9064e-02, -1.5512e-03,  1.9657e-02,  7.8270e-03, -4.0602e-02,
         1.9702e-02,  4.0297e-02, -1.9938e-02, -1.2144e-02, -2.3456e-02,
        -5.3594e-02, -4.2403e-02,  7.4747e-02,  1.2299e-02, -2.0902e-02,
        -5.5567e-02,  1.4648e-02,  1.4435e-03,  1.0079e-01,  9.2539e-02,
        -1.0607e-02, -1.4108e-02,  9.1721e-03,  1.1072e-02,  2.9608e-02,
        -7.6078e-03,  2.5653e-02, -1.1136e-02,  7.2409e-02, -5.1752e-02,
         8.0248e-02, -6.1112e-02,  6.1196e-02, -2.2555e-02,  8.3575e-02,
        -3.1096e-02, -4.8845e-03,  7.4904e-02, -6.5950e-03,  1.9850e-02,
        -3.0121e-02,  3.6329e-02, -4.4467e-03,  8.3675e-02, -7.4437e-02,
        -4.0771e-02, -2.8450e-02,  8.7870e-02, -4.0946e-02, -3.8329e-03,
        -6.9315e-02, -1.0016e-02, -2.0778e-02,  8.0915e-02,  3.9063e-02,
        -1.5094e-02,  3.3627e-03, -2.3838e-02,  3.1394e-02,  1.2513e-02,
         1.3142e-01,  3.0603e-02, -2.2055e-03,  2.0502e-02,  1.0135e-01,
         7.1754e-02,  9.7944e-03,  3.7959e-02,  1.3989e-02,  1.7989e-02,
        -3.4136e-03,  3.9650e-02, -3.4606e-02, -2.8792e-03, -5.2186e-03,
        -3.3676e-02,  5.8599e-02,  6.2828e-02,  4.3374e-02,  2.6202e-02,
         9.4092e-02, -3.8893e-03,  9.6722e-02, -6.6398e-02, -5.4730e-02,
        -4.2413e-02,  2.9454e-02, -1.1035e-02, -1.3873e-02,  3.5274e-02,
        -5.2795e-03,  7.2810e-03,  5.6811e-03,  2.5880e-02, -1.6221e-02,
        -4.2152e-02,  2.0760e-02,  1.4120e-02,  7.2473e-03,  6.9235e-03,
        -6.1877e-02,  8.1591e-03,  5.1506e-02,  4.0437e-02,  1.4943e-02,
        -2.9214e-02,  7.7097e-02, -3.1952e-02,  3.0927e-03, -6.7025e-04,
         6.2347e-02,  8.4001e-03,  3.2072e-02,  5.1373e-02, -2.0174e-02,
         1.1651e-02,  1.1074e-02,  1.7667e-02, -8.6863e-03,  3.0662e-02,
        -1.7535e-02, -5.4594e-02, -1.0278e-02, -6.7767e-02,  1.8709e-02,
         4.3460e-02,  2.7637e-02,  5.1533e-02, -3.5134e-02,  3.0937e-02,
         1.0212e-02,  3.0756e-02,  6.7154e-02,  3.9204e-02,  6.0210e-02,
        -6.5048e-02,  7.2162e-02,  4.5481e-02,  4.0012e-02, -2.6958e-02,
         2.3580e-03,  1.9551e-02,  1.4279e-02,  6.8494e-02,  2.1821e-02,
        -3.8355e-02, -5.8506e-02, -7.3684e-02, -7.3094e-03, -4.3607e-02,
        -2.3467e-03, -8.3147e-03,  6.3290e-02, -1.1461e-02, -1.4600e-03,
         6.5386e-02,  9.3397e-03, -4.8493e-02, -2.3290e-02,  4.6487e-02,
         1.0229e-01,  4.5283e-03,  4.7067e-02,  1.0678e-02,  5.5262e-02,
         7.6263e-03, -3.7516e-02,  1.8642e-02,  1.2551e-02, -7.5747e-02,
        -6.9164e-03,  9.2134e-03, -7.1654e-02, -3.2144e-02,  4.4780e-02,
         1.6665e-02,  5.4213e-02, -5.7801e-02, -4.3401e-05,  9.2516e-02,
         2.4602e-02, -2.6873e-02,  6.9050e-02, -1.1062e-02, -6.5441e-02,
         3.2812e-02,  1.6676e-02, -3.6145e-02,  6.2055e-02, -5.5214e-02,
        -2.6646e-02,  1.5051e-01, -1.4609e-02,  7.3359e-02, -1.1975e-02,
         2.5550e-03,  2.8461e-02,  6.1058e-02,  1.5934e-02,  1.1280e-02,
        -5.7301e-02, -8.1453e-02, -1.3112e-02,  1.4247e-02,  2.6493e-02,
        -3.2159e-02, -1.2888e-02,  3.9029e-02, -3.8758e-02,  1.1523e-01,
         5.7670e-02, -3.0118e-02, -1.3478e-02,  5.0629e-02, -1.2562e-02,
        -3.1457e-02, -7.3343e-03, -4.2227e-02, -5.6207e-02, -2.7644e-02,
        -2.3450e-02,  3.6726e-02, -1.7531e-02,  5.1430e-02,  8.6157e-02,
         3.9421e-02, -3.5995e-02, -1.9596e-03,  1.4217e-02,  8.6251e-02,
         9.4631e-04, -8.1190e-02,  9.1071e-02,  2.4760e-02, -1.1475e-02,
         2.2392e-02,  5.3320e-02,  5.3901e-04, -4.8530e-02, -4.3654e-03,
        -5.6498e-02, -1.8796e-02,  7.8893e-02,  4.2227e-02,  5.3552e-02,
         7.8111e-02,  2.7515e-02, -1.6096e-02,  1.2153e-02,  6.6272e-03,
        -4.8895e-02,  5.0869e-02,  6.7843e-02,  6.1599e-02, -1.1996e-02,
        -9.4545e-03,  1.0977e-01,  5.2409e-02, -4.7280e-02,  3.6631e-02,
        -2.9781e-02, -4.2014e-03, -4.2217e-02, -1.1914e-02, -2.9351e-02,
         1.4702e-01, -2.4856e-02, -5.4605e-02,  9.2256e-02, -5.8185e-02,
        -4.9112e-03, -4.8391e-02,  9.6750e-03,  9.3729e-02, -7.0812e-02,
         1.3358e-02, -3.0346e-02, -1.2634e-02,  5.4181e-02,  3.5779e-03,
        -1.3641e-02,  1.8990e-02,  4.1806e-02,  1.4102e-02, -3.5755e-02,
        -9.7328e-03, -6.9314e-02, -1.1238e-02,  4.4573e-02, -5.3725e-02,
         1.4389e-04,  3.9408e-02, -4.2209e-02, -3.2018e-02,  8.1474e-02,
         9.0362e-03,  8.6104e-02,  1.3965e-01, -1.3486e-02,  3.1769e-02,
         3.9424e-02, -2.7504e-02,  4.4009e-02,  4.5092e-02,  3.0902e-02,
         9.5173e-02,  6.0828e-02,  1.8025e-03, -8.7061e-03,  6.5157e-03,
         3.7598e-02,  1.0603e-01,  1.6571e-01,  5.5066e-02, -7.3351e-03,
        -5.2073e-03, -6.4366e-02, -6.7670e-02,  5.9512e-02,  6.4599e-02,
        -5.4156e-02,  3.0373e-02, -6.4649e-03,  6.1524e-02, -6.0863e-02,
        -5.4839e-02,  3.2778e-02, -5.2311e-02, -6.3305e-02, -1.4740e-02,
         8.6265e-03, -1.4593e-02,  7.0843e-02,  9.6598e-03, -4.1514e-02,
         1.0386e-01,  9.7102e-02,  2.7971e-02, -8.3213e-03,  8.9160e-02,
         6.4613e-02,  7.2493e-03,  2.0423e-02, -1.7861e-02, -1.0312e-02,
        -2.7538e-02, -4.7007e-02, -5.1747e-02,  5.8215e-02,  6.6977e-02,
        -1.7237e-02, -3.6396e-02,  5.7006e-02,  7.6054e-03, -4.7352e-02,
        -9.1480e-03,  4.0137e-02,  6.6371e-03,  7.7091e-02,  5.0351e-02,
         6.8603e-04, -5.7972e-02,  1.2907e-02,  3.9684e-02, -2.7078e-02,
         5.0169e-02,  3.0860e-02,  5.1556e-02, -4.8597e-02, -2.1098e-02,
         5.4887e-02, -4.1379e-02,  2.0082e-02,  1.8863e-02, -1.6039e-02,
         4.4650e-02, -2.7449e-02,  2.2729e-02,  3.3439e-02,  6.7221e-02,
         3.5019e-02,  1.2706e-02, -3.3244e-02, -2.0954e-02, -7.2121e-02,
        -2.5076e-02, -9.3874e-02, -1.4208e-02, -4.0450e-02,  1.0401e-01,
        -2.2949e-02, -2.6151e-02,  8.9983e-03, -7.2041e-02, -2.8341e-03,
        -4.9221e-02, -4.2876e-02,  6.4064e-03, -2.5436e-02,  1.6128e-02,
        -4.1882e-02, -3.8972e-02, -4.6335e-02, -5.4710e-02, -1.8748e-04,
        -5.4616e-02, -5.6258e-02, -3.8972e-02, -2.6863e-02,  1.6232e-02,
         1.6155e-02, -1.8721e-02, -2.4802e-02, -1.3311e-02, -1.7580e-02,
        -4.3439e-02, -4.1209e-02, -2.6509e-02,  2.1775e-02, -1.2127e-02,
        -1.7708e-02, -1.5500e-02, -7.4598e-02, -5.0580e-02, -6.6671e-02,
        -4.6614e-02,  4.4161e-02,  1.9872e-02,  1.7880e-02, -7.2944e-03,
         9.7257e-03,  1.2171e-01,  2.0895e-03, -7.2846e-02, -9.7906e-03,
        -5.5273e-02, -1.8346e-03,  3.9121e-02,  1.7541e-02,  1.6898e-02,
         2.0039e-02, -8.8893e-03, -2.4714e-02, -3.4171e-02,  1.0218e-02,
         1.2038e-02, -5.3019e-02,  3.3046e-02, -3.6093e-02, -5.2178e-02,
        -4.7642e-02, -6.6923e-02, -2.2623e-02, -7.2356e-02, -1.1350e-01,
        -2.2522e-02, -1.8415e-02, -5.9977e-02, -2.2627e-02,  4.6349e-02],
       requires_grad=True))]
</code></pre>
","2024-09-05 17:42:13","1","Answer"
"78954190","78674317","","<p>It seems that @AaronDefazio tried to improve the swap used by Schedule-Free Optimizer Wrapper:
<a href=""https://github.com/facebookresearch/schedule_free/blob/main/schedulefree/wrap_schedulefree.py#L107"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/schedule_free/blob/main/schedulefree/wrap_schedulefree.py#L107</a></p>
<pre class=""lang-py prettyprint-override""><code>    @staticmethod
    def swap(x, y):
        # Memory efficient but potentially unstable
        x.add_(y)
        torch.sub(x, y, out=y)
        x.sub_(y)
</code></pre>
<p>So I have tested against the example in the repo
<a href=""https://github.com/facebookresearch/schedule_free/blob/main/examples/mnist/main.py"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/schedule_free/blob/main/examples/mnist/main.py</a></p>
<p>I made some modification to enable an exact match:</p>
<pre class=""lang-py prettyprint-override""><code>import os
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # required to enable deterministic

torch.use_deterministic_algorithms(True)
...
def main():
    ...
    base_optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.0)
    optimizer = schedulefree.ScheduleFreeWrapper(
        base_optimizer, momentum=0.9, weight_decay_at_y=0.1)
</code></pre>
<p>I found @hamzah-al-qadasi 's method worked -- exactly matched the <code>copy_</code> implementation.</p>
<p>I tested following 4 implementations and got identical results without any error:</p>
<pre class=""lang-py prettyprint-override""><code>    @staticmethod
    def swap(x: torch.Tensor, y: torch.Tensor):
        t = torch.empty_like(x)
        t.copy_(x)
        x.copy_(y)
        y.copy_(t)

    @staticmethod
    def swap(x: torch.Tensor, y: torch.Tensor):
        t = x.new_empty(())
        t.set_(x)
        x.set_(y)
        y.set_(t)

    @staticmethod
    def swap(x: torch.Tensor, y: torch.Tensor): # Hamzah Al-Qadasi
        x_storage = x.untyped_storage()
        y_storage = y.untyped_storage()
        x.set_(y_storage, y.storage_offset(), y.size(), y.stride())
        y.set_(x_storage, x.storage_offset(), x.size(), x.stride())

    @staticmethod
    def swap(x: torch.Tensor, y: torch.Tensor):
        x.view(torch.uint8).bitwise_xor_(y.view(torch.uint8))
        y.view(torch.uint8).bitwise_xor_(x.view(torch.uint8))
        x.view(torch.uint8).bitwise_xor_(y.view(torch.uint8))
</code></pre>
<p><a href=""https://i.sstatic.net/iud4sj8z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iud4sj8z.png"" alt=""enter image description here"" /></a></p>
","2024-09-05 17:18:38","0","Answer"
"78953324","","How does the data splitting actually work in Multi GPU Inference for Accelerate when used in a batched inference setting?","<p>I followed the code given in this <a href=""https://github.com/huggingface/accelerate/issues/2018"" rel=""nofollow noreferrer"">github issue</a> and this <a href=""https://medium.com/@geronimo7/llms-multi-gpu-inference-with-accelerate-5a8333e4c5db"" rel=""nofollow noreferrer"">medium blog</a></p>
<p>I ran the batched experiment with <code>process = 1</code> and <code>process=4</code> it gave me the result but I'm confused right now because I thought the result would be in order. If they are not in orger, them I won't be able to map those with the ground Truth</p>
<p>For example let's say my <code>data_length=5</code> and my <code>batch=3</code>. So if I got results <code>[[1,2,3], [4,5]]</code> for <code>process=1</code> then I'm expecting when using <code>process = 4</code>, I should get the same results when I flatten the results.</p>
<p>they are coming out of order. What am I doing wrong?</p>
<p><strong><em>NOTE: I used a <code>zip(text,label)</code> while passing data to processes to get the correct mapping BUT that is not the question</em></strong></p>
<p>Below is the code:</p>
<pre><code>def seed_everything(seed=13):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    set_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(seed = 13)


def test():
    accelerator = Accelerator()
    accelerator.wait_for_everyone() 
    seed_everything(seed = 13)
    
    model = load_model(model = &quot;my_model_path&quot;
                        lora = &quot;./my_lora_checkpoint/checkpoint-8200&quot;,
                       device = {&quot;&quot;: accelerator.process_index}, 
                       num_labels = NUM_LABELS,
                       merge_unload = False)
    
    
    with accelerator.split_between_processes(zipped_text_label) as prompts:
    
        res = {&quot;pred_probs&quot;: [], &quot;pred_labels&quot;: []}

        BATCH_SIZE = 10
    
        BATCHES = [prompts[i:i + BATCH_SIZE] for i in range(0, len(prompts), BATCH_SIZE)]
        print(len(BATCHES[0]))

        pred_probs = []
        pred_labels = []

        for batch in tqdm(BATCHES):
            text_batch = [i[0] for i in batch]
            score_batch = [i[1] for i in batch]
            
            with torch.no_grad():
                inputs = tokenizer(text_batch,truncation= True, max_length=MAX_LENGTH, padding=&quot;max_length&quot;, return_tensors = &quot;pt&quot;).to(model.device)
                logits = model(**inputs).logits.cpu().to(torch.float32)
                probs = torch.softmax(logits, dim = 1).numpy()
                res[&quot;pred_probs&quot;].append(probs.tolist())
                res[&quot;pred_labels&quot;].append(probs.argmax(axis = 1).tolist())
        
        res = [res]
    
    result = gather_object(res)
    if accelerator.is_main_process:
        print(result)


notebook_launcher(test, num_processes=1)                              
</code></pre>
<pre><code></code></pre>
","2024-09-05 13:35:26","2","Question"
"78953156","","How to access the weights of a layer in pretrained efficientnet-b3 in torch?","<p>It's not about loading the weights of a model. I am trying to see the weights of layers in loaded <code>efficientnet-b3</code> model in the torch.</p>
<pre><code>os.system('pip install efficientnet_pytorch')
from efficientnet_pytorch import EfficientNet
MODEL_NAME = 'efficientnet-b3'
effnet = EfficientNet.from_pretrained(MODEL_NAME) 
effnet.modules # this works, but only gives the module names
effnet.weights # doesn't work
effnet.layers # doesn't work
effnet.modules[1]  # doesn't work, second module is batch norm ._bc0
</code></pre>
<p>I would like some functionality to replicate the following TF code in torch, access weights of the first batch norm layer</p>
<pre><code>from tensorflow.keras.applications import EfficientNetB3
base_model = EfficientNetB3(weights=&quot;imagenet&quot;)
base_model.trainable_variables[1].numpy() # indexing with 1 gives weights of conv layer
</code></pre>
<p>Output for the above code:</p>
<pre><code>&lt;tf.Variable 'stem_bn/gamma:0' shape=(40,) dtype=float32, numpy=
array([ 0.1913209 ,  2.7074034 ,  9.623442  ,  2.5562265 ,  3.127593  ,
        4.348222  ,  2.4381876 ,  3.4623973 ,  3.6115906 ,  4.1241236 ,
        2.18851   ,  8.9716835 ,  0.7232651 ,  0.6261555 ,  9.050293  ,
        7.9233327 ,  0.47725916,  3.4991856 ,  5.334402  ,  4.843143  ,
        1.4122163 ,  1.953061  ,  8.150878  ,  5.0044165 ,  2.3806598 ,
        4.2976685 ,  2.2239766 ,  0.551327  ,  7.799995  ,  3.3823645 ,
        1.8910869 ,  4.0793633 ,  0.73215246,  3.4526935 , 10.874565  ,
        2.0920732 ,  6.272054  ,  3.6823177 ,  4.2152214 ,  3.4319222 ],
      dtype=float32)&gt;
</code></pre>
","2024-09-05 12:57:52","0","Question"
"78950892","78950620","","<ol>
<li>Why does the first approach return a Hessian of all zeros?</li>
</ol>
<p>This has to do with the softmax operation. When you compute <code>torch.autograd.grad(grad, a, torch.ones_like(a))</code>, you are essentially computing <code>torch.autograd.grad(grad.sum(), a)</code>. If you compute <code>grad.sum()</code>, you will find the output is always zero (or near zero due to numeric issues). This is because the sum of gradients through a softmax is always zero.</p>
<ol start=""2"">
<li>Is either approach correct for computing the second derivatives in this context?</li>
</ol>
<p>The second approach is correct, but likely slow due to looping at the python level. It works because you are backproping from individual elements of <code>grad</code> rather than the sum of <code>grad</code>, so you don't have the zero issue.</p>
<ol start=""3"">
<li>If not, what is the correct method to compute the Hessian?</li>
</ol>
<p>It's probably more efficient to use pytorch methods to compute the full hessian and take the diagonal. This is what your second method does, only we move the operations to a lower level.</p>
<pre class=""lang-py prettyprint-override""><code>def my_func(a):
    b, *_ = a.sort(descending=True)
    c = (b.unsqueeze(0) - a.unsqueeze(1)).abs().neg()
    d = c.softmax(0).matmul(torch.arange(c.size(0), dtype=c.dtype))
    e = torch.randint(0, 3, (10,), dtype=float)
    t = torch.sum(e * torch.log2(d + 1))
    return t

torch.manual_seed(0)
a = torch.randint(0, 10, (10,), dtype=float, requires_grad=True)
hessian = torch.autograd.functional.hessian(my_func, a)
hess_diag = torch.diag(hessian)
</code></pre>
","2024-09-04 23:52:09","0","Answer"
"78950620","","PyTorch function involving softmax and log2 second derivative is always 0","<p>I'm trying to compute the second derivatives (Hessian) of a function <code>t</code> with respect to a tensor <code>a</code> using PyTorch. Below is the code I initially wrote:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

torch.manual_seed(0)
a = torch.randint(0, 10, (10,), dtype=float, requires_grad=True)
b, *_ = a.sort(descending=True)
c = (b.unsqueeze(0) - a.unsqueeze(1)).abs().neg()
d = c.softmax(0).matmul(torch.arange(c.size(0), dtype=c.dtype))
e = torch.randint(0, 3, (10,), dtype=float)
t = torch.sum(e * torch.log2(d + 1))

grad, *_ = torch.autograd.grad(t, a, create_graph=True)
hess, *_ = torch.autograd.grad(grad, a, torch.ones_like(a))
print(hess)
</code></pre>
<p>I'm expecting the code to compute the component-wise second derivatives of <code>t</code> with respect to <code>a</code>. However, the Hessian vector returned consists entirely of zeros. This result is puzzling to me because the function <code>t</code> involves a softmax operation and logarithms, which I would expect to yield non-zero second derivatives.</p>
<p>To investigate further, I attempted a different approach:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

torch.manual_seed(0)
a = torch.randint(0, 10, (10,), dtype=float, requires_grad=True)
b, *_ = a.sort(descending=True)
c = (b.unsqueeze(0) - a.unsqueeze(1)).abs().neg()
d = c.softmax(0).matmul(torch.arange(c.size(0), dtype=c.dtype))
e = torch.randint(0, 3, (10,), dtype=float)
t = torch.sum(e * torch.log2(d + 1))

t.backward(create_graph=True)
grad = a.grad.clone()
hess = torch.zeros_like(a)
for i in range(len(a)):
    a.grad.zero_()
    grad[i].backward(retain_graph=True)
    hess[i] = a.grad[i]
print(hess)
</code></pre>
<p>In this second attempt, while the gradient (<code>grad</code>) matches what I obtained in the first method, the Hessian (<code>hess</code>) does not. Clearly, there's a difference between the two approaches, but I'm not sure what it is.</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Why does the first approach return a Hessian of all zeros?</li>
<li>Is either approach correct for computing the second derivatives in this context?</li>
<li>If not, what is the correct method to compute the Hessian?</li>
</ol>
<p>Any insights or explanations would be greatly appreciated. Thank you!</p>
","2024-09-04 21:30:06","1","Question"
"78950394","","Should nested modules with shared weights be an nn.Module object parameter or not?","<p>I would like two torch.nn.Module classes to share part of their architecture and weights, as in the example below:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn

class SharedBlock(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()

        self.block = nn.Sequential(
            # Define some block architecture here...
        )

    def forward(self, x):
        return self.block(x)

class MyNestedModule(nn.Module):
    def __init__(self, shared_block: nn.Module, *args, **kwargs):
        super().__init__()

        self.linear = nn.Linear(...)
        self.shared_block = shared_block

    def forward(self, x):
        return self.shared_block(self.linear(x))

class MyModule(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()

        
        # SHOULD THIS BE:
        shared_block = SharedBlock(*args, **kwargs)
        # OR:
        self.shared_block = SharedBlock(*args, **kwargs)  # Note: self.
        # ...AND WHAT IS THE DIFFERENCE, IF ANY?


        self.nested1 = MyNestedModule(shared_block, *args, **kwargs)
        self.nested2 = MyNestedModule(shared_block, *args, **kwargs)

    def forward(self, x):
        x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
        y_1 = self.nested1(x_1)
        y_2 = self.nested2(y_2)
        return y_1, y_2
</code></pre>
<p>I would like to know whether <code>shared_block</code> should be an object parameter of <code>MyModule</code>. I assume it does not, since it is set as an object parameter in both the <code>MyNestedModule</code> class objects so it should be registered in torch grad but if I did create it as an object parameter in <code>MyModule</code> what would happen?</p>
","2024-09-04 20:06:25","0","Question"
"78949696","78947633","","<p>The tensors have the same shapes and values, but their construction results in them having different memory layouts. You can see this with the <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html"" rel=""nofollow noreferrer"">stride</a> function:</p>
<pre class=""lang-py prettyprint-override""><code>tmp = torch.randn(10,2,1)
tensor_list = [x for x in tmp]
A = torch.cat([x.T for x in tensor_list], dim=0)
B = torch.cat(tensor_list, dim=1).T
print(A.stride())
&gt; (2, 1)
print(B.stride())
&gt; (1, 10)
</code></pre>
<p>The stride tells us how many bytes we need to move along an axis to get from one value to another.</p>
<p>Because the memory layouts are different, the <code>mean</code> operation processes values in a different order for each tensor. The different mean results come from the different operation order combined with numerical precision issues.</p>
<p>As a comparison, if you re-create <code>A</code> and <code>B</code> from lists (creating a whole new tensor), you get two tensors with the same stride and no mean difference.</p>
<pre class=""lang-py prettyprint-override""><code>C = torch.tensor(A.tolist())
D = torch.tensor(B.tolist())
print(C.stride())
&gt; (2, 1)
print(D.stride())
&gt; (2, 1)

C.mean(dim=0) - D.mean(dim=0)
&gt; tensor([0., 0.])
</code></pre>
","2024-09-04 16:33:22","0","Answer"
"78949386","78449954","","<p>I managed to overcome the issue by creating a virtual environment with python 3.10 and reinstalling lightning and pytorch there, it worked fine then.</p>
","2024-09-04 15:19:24","0","Answer"
"78948877","78948826","","<p>You have to run this before installing the PyTorch wheel:</p>
<p><code>sudo apt-get install libopenblas-base libopenmpi-dev</code></p>
<p><a href=""https://forums.developer.nvidia.com/t/cannot-install-pytorch/149226/3"" rel=""nofollow noreferrer"">reference</a></p>
","2024-09-04 13:18:33","0","Answer"
"78948826","","Unable to import PyTorch in Jetson nano","<p>I have trouble importing PyTorch in my jetson nano (jetpack 4.4).I have successfully installed it from .whl file. please help.</p>
<pre><code>Traceback (most recent call last):
  File &quot;/code/catkin_ws/src/duckietown-safe-ml-demo/packages/safe_ml/src/ood_node.py&quot;, line 13, in &lt;module&gt;
    import torch
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/__init__.py&quot;, line 188, in &lt;module&gt;
    _load_global_deps()
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/__init__.py&quot;, line 141, in _load_global_deps
    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)
  File &quot;/usr/lib/python3.6/ctypes/__init__.py&quot;, line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: libmpi_cxx.so.20: cannot open shared object file: No such file or directory
</code></pre>
","2024-09-04 13:10:36","0","Question"
"78947709","78946811","","<p>What you experience has not much to do with PyTorch itself, but rather with the representation of real numbers as floating point values on a modern computer. The underlying situation here is the following:</p>
<ol>
<li>Floating point numbers are internally represented in base 2 (in other words, as binary numbers) with a fixed number of bits.</li>
<li>Just like the fraction 1/3, which cannot be exactly represented with a fixed number of digits in base 10 (it is 0.333…<sub>10</sub> with an infinite number of repetitions of the digit 3, where I use <sub>10</sub> to indicate the base of the number), the fractional part of 3.1, which is 1/10, cannot be exactly represented in base 2 (it is 0.0001100110011…<sub>2</sub> with an infinite number of repetitions of the sequence 0011). As you experienced, this is in contrast to the fractional part of 3.5, which is 1/2,  which <em>can</em> be exactly represented in base 2 (it is 0.1<sub>2</sub>).</li>
<li>Having only a fixed number of bits available (see 1), this means that the fraction 1/10 can only be stored as an approximated floating point value on the computer.</li>
</ol>
<p>It is true that this appears to vanish once you switch from 32-bit floating point values to 64-bit floating point values, as suggested in <a href=""https://stackoverflow.com/a/78946921/7395592"">Karl's answer</a>, as a 64-bit floating point value has twice as many bits for representing each value. However, this is not because the underlying situation has been resolved, but rather because your Python interpreter now knows, in a way, that it has to hide the situation from you.</p>
<p>Try, for example, printing <code>f&quot;{torch.tensor([3.1], dtype=torch.float64).item():.30f}</code> or, in fact, <code>f&quot;{3.1:.30f}&quot;</code>, thereby enforcing to see more digits than Python, by default, wants to show you:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

print(f&quot;{torch.tensor([3.1], dtype=torch.float64).item():.30f}&quot;)
# &gt;&gt;&gt; 3.100000000000000088817841970013
print(f&quot;{3.1:.30f}&quot;)
# &gt;&gt;&gt; 3.100000000000000088817841970013
</code></pre>
<p>You will see that also for the 64-bit floating point case, the floating point representation of the number 3.1 is only an approximation of the true value.</p>
<p>Likewise, you can try the same with 3.5 and you will find that the value is stored exactly:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

print(f&quot;{torch.tensor([3.5], dtype=torch.float64).item():.30f}&quot;)
# &gt;&gt;&gt; 3.500000000000000000000000000000
print(f&quot;{3.5:.30f}&quot;)
# &gt;&gt;&gt; 3.500000000000000000000000000000
</code></pre>
<p>The next is probably a bit of a bold claim and, depending on what data you work with, a huge oversimplification or even plain wrong, but still: In your daily life as a programmer, I would say, you rarely have to deal with this knowledge but simply ignore the fact that floating point values are only approximations in most cases. There are also situations where precise floating point arithmetic matters, however, and solutions such as the <a href=""https://docs.python.org/3/library/decimal.html"" rel=""nofollow noreferrer""><code>decimal</code></a> module and its <code>Decimal</code> class have been designed for that. In some other situations, such as the comparison of floating point numbers, it may be appropriate to allow for some tolerance, e.g. by using PyTorch's <a href=""https://pytorch.org/docs/stable/generated/torch.allclose.html"" rel=""nofollow noreferrer""><code>allclose()</code></a> function instead of an equality comparison (<code>torch.allclose(t1, t2)</code> instead of <code>(t1 == t2).all()</code>).</p>
<p>For further general information on the properties of floating-point arithmetic, you might want to have a look at <a href=""https://docs.python.org/3/tutorial/floatingpoint.html"" rel=""nofollow noreferrer""><em>Floating-Point Arithmetic: Issues and Limitations</em></a> from the Python documentation, or at the somewhat standard article on the subject, <a href=""https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html"" rel=""nofollow noreferrer""><em>What Every Computer Scientist Should Know About Floating-Point Arithmetic</em></a>.</p>
","2024-09-04 08:47:10","2","Answer"
"78947633","","weird that two tensors originating from the same source have different mean values","<p>concatenating a list of 2d-tensor along different two axis respectively, leads to tensor A,B where A.T==B, but their mean values along the same axis is slightly different <code>(A.T.mean(axis=0) != B.mean(axis=0))</code>, why? Theoretically they are the same.</p>
<p>start from this 3d list whose shape is (10,2,1)</p>
<pre><code>In [4]: tmp
Out[4]: 
[[[0.3471660912036896], [0.652833878993988]],
 [[0.5512792468070984], [0.4487205743789673]],
 [[0.5454527139663696], [0.4545471668243408]],
 [[0.3661797344684601], [0.6338202953338623]],
 [[0.2655346989631653], [0.7344651222229004]],
 [[0.28296780586242676], [0.717032253742218]],
 [[0.28441378474235535], [0.7155864238739014]],
 [[0.3660774230957031], [0.6339224576950073]],
 [[0.3515346944332123], [0.6484655141830444]],
 [[0.3660774230957031], [0.6339224576950073]]]
</code></pre>
<ul>
<li>step 1, convert <code>tmp</code> into a list of tensors</li>
</ul>
<pre><code>In [7]: tensor_list = [torch.tensor(x) for x in tmp]
   ...: tensor_list
Out[7]: 
[tensor([[0.3472],
         [0.6528]]),
 tensor([[0.5513],
         [0.4487]]),
 tensor([[0.5455],
         [0.4545]]),
 tensor([[0.3662],
         [0.6338]]),
 tensor([[0.2655],
         [0.7345]]),
 tensor([[0.2830],
         [0.7170]]),
 tensor([[0.2844],
         [0.7156]]),
 tensor([[0.3661],
         [0.6339]]),
 tensor([[0.3515],
         [0.6485]]),
 tensor([[0.3661],
         [0.6339]])]
</code></pre>
<ul>
<li>step 2
<ul>
<li>torch.cat() over each <strong>tensor.T</strong> in tensor_list along <strong>axis 0</strong> to get A.</li>
<li>torch.cat() over each <strong>tensor</strong> in tensor_list along <strong>axis 1</strong> and <strong>then transpose</strong> to get B.</li>
</ul>
</li>
</ul>
<p>where A.shape == B.shape</p>
<pre><code>In [11]: A = torch.cat([x.T for x in tensor_list], dim=0)
    ...: A.shape
Out[11]: torch.Size([10, 2])

In [12]: B = torch.cat(tensor_list, dim=1).T
    ...: B.shape
Out[12]: torch.Size([10, 2])
</code></pre>
<p>step 3. check consistency between A and B. We can see below that the sum of their element-wise difference is zero, and the element-wise comparison show that each element is the same as that in the other tensor.</p>
<pre><code>In [13]: (A - B).abs().sum()
Out[13]: tensor(0.)

In [14]: A == B
Out[14]: 
tensor([[True, True],
        [True, True],
        [True, True],
        [True, True],
        [True, True],
        [True, True],
        [True, True],
        [True, True],
        [True, True],
        [True, True]])

</code></pre>
<p>step 4. check their mean values along axis(0). Strange that there are slightly difference.</p>
<pre><code>In [15]: A.mean(dim=0) - B.mean(dim=0)
Out[15]: tensor([5.9605e-08, 0.0000e+00])
</code></pre>
<p>Though the difference between their mean values is minor enough to neglect, I wonder why would this happen. How does torch.cat() works?</p>
<p>[Environment Info]</p>
<ul>
<li>OS: Ubuntu 20.04.5 LTS</li>
<li>Python: Python 3.8.10</li>
<li>torch: 2.0.1</li>
<li>cuda: 11.7</li>
<li>NVIDIA-SMI 515.86.01</li>
</ul>
","2024-09-04 08:34:25","0","Question"
"78947357","78947332","","<p>As (roundaboutly) documented on <a href=""https://pytorch.org/get-started/locally/"" rel=""noreferrer"">pytorch.org's getting started page</a>, Torch on PyPI is Nvidia enabled; use the <code>download.pytorch.org</code> index for CPU-only wheels:</p>
<pre><code>RUN pip install torch --index-url https://download.pytorch.org/whl/cpu
</code></pre>
<p>Also please remember to specify a somewhat locked version of Torch, e.g.</p>
<pre><code>RUN pip install torch~=2.4.0 --index-url https://download.pytorch.org/whl/cpu
</code></pre>
","2024-09-04 07:33:53","9","Answer"
"78947332","","How to install torch without nvidia?","<p>While trying to reduce the size of a Docker image, I noticed <code>pip install torch</code> adds a few GB. A big chunk of this comes from <code>[...]/site-packages/nvidia</code>. Since I'm not using a GPU, I'd like to not install the <code>nvidia</code> things.</p>
<p>Here is a minimal example:</p>
<pre><code>FROM python:3.12.5
RUN pip install torch
</code></pre>
<p>(Ignoring <code>-slim</code> base images, since this is not the point here.)</p>
<p>Resulting size:</p>
<ul>
<li><code>FROM python:3.12.5</code> -&gt; <code>1.02GB</code></li>
<li>After <code>RUN pip install torch</code> -&gt; <code>8.98GB</code></li>
<li>With <code>RUN pip install torch &amp;&amp; pip freeze | grep nvidia | xargs pip uninstall -y</code> instead -&gt; <code>6.19GB</code>.</li>
</ul>
<p>While the last point reduces the final size, all the nvidia stuff is still <a href=""https://gist.github.com/Dobiasd/4ebb2cdce927408f7953cb0cd65962c3"" rel=""noreferrer"">downloaded and installed</a>, which costs time and bandwidth.</p>
<p>So, how can I install <code>torch</code> without nvidia directly?</p>
<p>Using <code>--no-deps</code> is not a convenient solution, because of the other transitive dependencies, that I would like to install.</p>
<p>Of course, I could explicitly list every single one, but looking at this list of packages installed with <code>torch</code></p>
<pre><code>mpmath
typing-extensions
sympy
nvidia-nvtx-cu12
nvidia-nvjitlink-cu12
nvidia-nccl-cu12
nvidia-curand-cu12
nvidia-cufft-cu12
nvidia-cuda-runtime-cu12
nvidia-cuda-nvrtc-cu12
nvidia-cuda-cupti-cu12
nvidia-cublas-cu12
networkx
MarkupSafe
fsspec
filelock
triton
nvidia-cusparse-cu12
nvidia-cudnn-cu12
jinja2
nvidia-cusolver-cu12
torch
</code></pre>
<p>I'd like to avoid manually maintaining this list since it would change with future versions of <code>torch</code>.</p>
","2024-09-04 07:27:02","5","Question"
"78946921","78946811","","<p>Pytorch float tensors are fp32 by default. Python floats are fp64. The numeric difference is from converting from fp32 to fp64.</p>
<p>You can change this by specifying fp64 when you create the tensor.</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.tensor([3.1], dtype=torch.float64)

print(x)
&gt; tensor([3.1000], dtype=torch.float64)
print(x.item())
&gt; 3.1
print(float(x))
&gt; 3.1
</code></pre>
","2024-09-04 05:12:56","1","Answer"
"78946811","","Why does pytorch tensor.item() give an unprecise output to any real number input but give a precise output to a number that ends with .0 or .5?","<p><strong>Input</strong>:</p>
<pre><code>x = torch.tensor([3.5])

print(x)
print(x.item())
print(float(x))
</code></pre>
<p><strong>Output</strong>:
<code>(tensor([3.5000]), 3.5, 3.5)</code></p>
<p>But if I change torch.tensor parameter to any other value rather than some_value.5, it gives a unprecise output:</p>
<p><strong>Input</strong>:</p>
<pre><code>x = torch.tensor([3.1])

print(x)
print(x.item())
print(float(x))
</code></pre>
<p><strong>Output</strong>:
<code>(tensor([3.1000]), 3.0999999046325684, 3.0999999046325684)</code></p>
","2024-09-04 04:24:43","0","Question"
"78946033","78895109","","<h2>TLDR</h2>
<ul>
<li>Copy the code at the end, run it, follow code comments</li>
<li>...but I think the motivation behind this solution would help</li>
</ul>
<p>TLDR how it's done:</p>
<ul>
<li>Use a backward hook to call the optimizer immediately after a gradient was generated, then clear said gradient</li>
<li>PyTorch's <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook"" rel=""nofollow noreferrer""><code>register_post_accumulate_grad_hook</code></a> function allows you to hook into the right location</li>
</ul>
<h2>Introduction</h2>
<blockquote>
<p>Now when I run loss.backward() it would calculate gradients for all layers at once.</p>
</blockquote>
<p>This is not true. <code>loss.backward()</code> calculates gradients layer by layer, starting from the loss layer
and going backwards through the network, each gradient is dependent on the previous layer's gradient.</p>
<blockquote>
<p>But is it possible to run backward() one layer at a time?</p>
</blockquote>
<p>That's what the backpropagation kind of does.</p>
<blockquote>
<p>So what I'm trying to do is to obtain gradients for layer 1 first, pass them to optimizer,
And then immediately set grads to None in order to free the memory.</p>
</blockquote>
<p>It is not possible, as for you to obtain gradients for layer 1,
you need to calculate gradients for all layers <strong>after it</strong> before (<code>layer3 -&gt; layer2 -&gt; layer1</code> in
your case).</p>
<p>You could use other algorithm than <code>backpropagation</code>, for example <a href=""https://arxiv.org/abs/2212.13345"" rel=""nofollow noreferrer"">Forward Forward</a>,
with some implementations available (<a href=""https://github.com/mpezeshki/pytorch_forward_forward"" rel=""nofollow noreferrer"">here</a> or
<a href=""https://github.com/loeweX/Forward-Forward"" rel=""nofollow noreferrer"">here</a>), <strong>please note this approach was tested only on small networks,
and it is a research topic</strong>.</p>
<p>See other possible alternatives <a href=""https://stackoverflow.com/questions/55287004/are-there-alternatives-to-backpropagation"">here</a>.</p>
<h2>How PyTorch's <code>backward()</code> conceptually works?</h2>
<p>Actual <code>backward()</code> process is more complex, this is a simplified description:</p>
<ol>
<li>After calling <code>backward()</code> on a <code>torch.tensor</code> (<code>loss</code> in this case),
an implicit <code>torch.tensor(torch.ones_like(tensor)</code> is created
and used as an actual starting <code>gradient</code> for the <code>backward()</code> process.</li>
<li>Tensors <code>grad_fn</code> attribute is called (<strong>if it is not <code>None</code>!</strong>)  with the <code>gradient</code> tensor(s)
which calculates the gradient of an operation with respect to the input tensors.</li>
<li>This one is multiplied with the gradient from layer above (according to <a href=""https://en.wikipedia.org/wiki/Chain_rule"" rel=""nofollow noreferrer"">the chain rule</a>).</li>
<li><code>tensor.grad_fn.next_functions</code> is used to find next nodes in the computational graph, which are
<code>grad_fn</code> themselves</li>
<li>Repeat steps 2-4 until you reach the <code>leaf</code> tensors (which have <code>grad_fn=None</code>).</li>
<li>Save the gradients in the <code>tensor.grad</code> attribute.</li>
</ol>
<p>Now, the very important thing is that <code>backward()</code> <strong>will clear the intermediate gradients</strong> (unless you
specify <code>retain_graph=True</code>), hence you cannot save memory here.</p>
<blockquote>
<p><strong>Only gradients in the leaf nodes are saved</strong> (e.g. <code>model.layer3.weight.grad</code> will not be <code>None</code>, but <code>out.grad</code> will be),
the rest is aggressively cleared.</p>
</blockquote>
<p>To verify, you can check the <code>out.grad_fn</code> after calling <code>loss.backward()</code>
(you should get a warning + <code>None</code> as a result).</p>
<h2>Additional Resources</h2>
<ul>
<li><code>backward()</code> should have as many <code>inputs</code> as there were outputs
<strong>for each operation in the chain</strong>. Same thing happens for <code>grad_fn()</code>, which calculates a single step</li>
<li>Extending <code>PyTorch</code> (specifically <code>autograd</code>) <a href=""https://pytorch.org/docs/stable/notes/extending.html#how-to-use"" rel=""nofollow noreferrer"">here</a>
for more information</li>
<li><a href=""https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html"" rel=""nofollow noreferrer"">PyTorch hooks for autograd</a> used later (thanks <a href=""https://stackoverflow.com/users/2989330/green%e7%bb%bf%e8%89%b2"">@Green绿色</a>!)</li>
</ul>
<h2>Possible improvements + implementation</h2>
<p>I think the only place where you could (realistically) save memory is when the gradients on the leaf nodes are set,
one could use <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook"" rel=""nofollow noreferrer"">PyTorch's <code>register_post_accumulate_grad_hook</code></a>
which will simply run after the <code>.grad</code> field was computed. PyTorch's backward hooks are explained in a bit more detail <a href=""https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html"" rel=""nofollow noreferrer"">this helpful article</a> from the PyTorch documentation.</p>
<p>In order to do that, one has to obtain gradient of a given layer, apply the correction
to the weights, and then set the <code>.grad</code> field to <code>None</code>.</p>
<p>An example SGD optimizer with this feature could look like this (<strong>please read code comments!</strong>):</p>
<pre class=""lang-py prettyprint-override""><code>class SGD:
    # You could specify additional parameters, different optimizers etc.
    # This one is a very basic implementation only for demonstration purposes
    # See [here](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD)
    # for PyTorch's original implementation
    def __init__(self, lr: float = 3e-4, *debug_modules: torch.nn.Module):
        self.lr = lr
        # Debug modules are used to verify that only one module has gradient set to None
        # at any given time, see __call__ below
        self.debug_modules = debug_modules

    def __call__(self, param: torch.Tensor) -&gt; None:
        if self.debug_modules:
            # Assert every module has gradient set to None
            # EXCEPT THE CURRENT ONE
            # You could also print the layers to see for yourself
            assert (
                sum([module.weight.grad is not None for module in self.debug_modules])
                == 1
            )
        param.add_(param.grad, alpha=-self.lr)
        # Clear the gradient immediately after the update
        param.grad = None
</code></pre>
<p>Here is how one could register it for your neural network (<strong>please read code comments!</strong>):</p>
<pre class=""lang-py prettyprint-override""><code>model = NeuralNetwork()

# Used 20 so one could see the difference in weights easily
sgd = SGD(20, *[c for c in model.children() if isinstance(c, torch.nn.Linear)])

# Register the hook for every linear layer
for module in model.children():
    if isinstance(module, torch.nn.Linear):
        module.weight.register_post_accumulate_grad_hook(sgd)
</code></pre>
<h2>Full code</h2>
<p>Full code I used so you could test it yourself:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

torch.manual_seed(0)


class NeuralNetwork(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = torch.nn.Flatten()
        self.layer1 = torch.nn.Linear(5, 5)
        self.layer2 = torch.nn.Linear(5, 5)
        self.layer3 = torch.nn.Linear(5, 5)

    def forward(self, x):
        x = self.flatten(x)
        layer1_out = self.layer1(x)
        layer2_out = self.layer2(layer1_out)
        return self.layer3(layer2_out + layer1_out)


class SGD:
    # You could specify additional parameters, different optimizers etc.
    # This one is a very basic implementation only for demonstration purposes
    # See [here](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD)
    # for PyTorch's original implementation
    def __init__(self, lr: float = 3e-4, *debug_modules: torch.nn.Module):
        self.lr = lr
        # Debug modules are used to verify that only one module has gradient set to None
        # at any given time, see __call__ below
        self.debug_modules = debug_modules

    def __call__(self, param: torch.Tensor) -&gt; None:
        if self.debug_modules:
            # Assert every module has gradient set to None
            # EXCEPT THE CURRENT ONE
            # You could also print the layers to see for yourself
            assert (
                sum([module.weight.grad is not None for module in self.debug_modules])
                == 1
            )
        param.add_(param.grad, alpha=-self.lr)
        # Clear the gradient immediately after the update
        param.grad = None


model = NeuralNetwork()

# Used 20 so one could see the difference in weights easily
sgd = SGD(20, *[c for c in model.children() if isinstance(c, torch.nn.Linear)])

# Register the hook for every linear layer
for module in model.children():
    if isinstance(module, torch.nn.Linear):
        module.weight.register_post_accumulate_grad_hook(sgd)

batch, labels = torch.randn((5, 5)), torch.randn((5, 5))
criterion = torch.nn.MSELoss()

out = model(batch)

loss = criterion(out, labels) / 2

# Now calling backward on the loss suffices
print(model.layer1.weight)
loss.backward()
print(model.layer1.weight)
</code></pre>
<p><strong>Please note</strong>:</p>
<ul>
<li>Difference in the weights (as expected)</li>
<li>Assert is passing, meaning only one layer has gradient set to <code>None</code> at any given time</li>
<li>One could implement different optimizers this way (too much for a SO post though)</li>
<li>You don't have to modify the model source code, smarter registering would be possible</li>
<li>Didn't test for performance at all</li>
</ul>
","2024-09-03 20:53:30","5","Answer"
"78945519","78945458","","<p><code>w = w - w.grad * 1e-5</code> creates a new tensor and assigns it to <code>w</code>. This breaks the link, resulting in <code>w.grad</code> being set to <code>None</code>.</p>
<p>To fix it you should instead update the tensor in place using <code>-=</code> operator as <code>w -= w.grad * 1e-5</code>:</p>
<pre class=""lang-py prettyprint-override""><code>    with torch.no_grad():
        w -= w.grad * 1e-5
        b -= b.grad * 1e-5
        w.grad.zero_()
        b.grad.zero_()
</code></pre>
","2024-09-03 17:46:05","0","Answer"
"78945495","78944393","","<p>If you want to add skip connections, you need to create a new <code>nn.Module</code> class for it. You also have to decide what layers you want in the skip connection. For example:</p>
<pre class=""lang-py prettyprint-override""><code>class SkipConv(nn.Module):
    def __init__(self, in_channels, out_channels, stride, kernel_size, padding):
        super().__init__()
        
        self.conv = nn.Sequential(
                        nn.Conv2d(in_channels, 
                                  out_channels, 
                                  kernel_size=kernel_size, 
                                  stride=stride, 
                                  padding=padding
                                 ),
                        nn.BatchNorm2d(out_channels),
                        nn.ReLU())
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = x + self.conv(x)
        x = self.relu(x)
        return x
</code></pre>
<p>You would then swap the conv/bn/relu layers in your model with one of the <code>SkipConv</code> blocks.</p>
","2024-09-03 17:37:35","0","Answer"
"78945458","","Pytorch shows the 'NoneType' object has no attribute 'zero_' backward error","<p>Hi i started learning pytorch. When i try to reset weight and bias grad i got 27 b.grad.zero_()</p>
<p>AttributeError: 'NoneType' object has no attribute 'zero_' error.</p>
<pre><code>input = torch.tensor([73,67,43,91,88,64,87,134,58,102,43,37,69,96,70],dtype=torch.float32)
inputnew = input.reshape(5,3)
target = torch.tensor([56,70,81,101,119,133,22,37,103,119])
targetnew = target.reshape(5,2)

w = torch.randn(2,3,requires_grad = True)
b = torch.randn(2,requires_grad = True)

def model(x):
       return x @ w.t()+ b
islem= model(inputnew)
def fix(real,pred):
    diff = real-pred
    return torch.sum((diff*diff)/diff.numel())
loss = fix(targetnew,islem)

for i in range(100):
    islem = model(inputnew)
    loss = fix(targetnew,islem)
    loss.backward()
    with torch.no_grad():
            w=w- w.grad*1e-5 
            b=b-b.grad*1e-5
            w.grad.zero_()
            b.grad.zero_()
</code></pre>
<p>How can i solve? Thanks</p>
","2024-09-03 17:23:51","1","Question"
"78945455","78943401","","<p>You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.</p>
<p>You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.</p>
<p>What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details <a href=""https://huggingface.co/docs/peft/en/developer_guides/quantization"" rel=""nofollow noreferrer"">here</a></p>
","2024-09-03 17:22:57","1","Answer"
"78945356","78942171","","<p>tl;dr they are using <code>no_grad</code>, just in another location and in a slightly more complex way</p>
<p>In detail:</p>
<p>Walk up the stack. <code>param.add_(grad, alpha=-lr)</code> is a line in <a href=""https://github.com/pytorch/pytorch/blob/main/torch/optim/sgd.py#L314"" rel=""nofollow noreferrer"">_single_tensor_sgd</a>. <code>_single_tensor_sgd</code> is called by <a href=""https://github.com/pytorch/pytorch/blob/main/torch/optim/sgd.py#L244"" rel=""nofollow noreferrer"">sgd</a>. <code>sgd</code> is called by <a href=""https://github.com/pytorch/pytorch/blob/main/torch/optim/sgd.py#L102"" rel=""nofollow noreferrer"">optim.SGD.step</a>.</p>
<p>The <code>step</code> function has the <a href=""https://github.com/pytorch/pytorch/blob/main/torch/optim/optimizer.py#L71"" rel=""nofollow noreferrer"">_use_grad_for_differentiable</a> decorator. This is the code for the decorator:</p>
<pre class=""lang-py prettyprint-override""><code>def _use_grad_for_differentiable(func):
    def _use_grad(self, *args, **kwargs):
        import torch._dynamo

        prev_grad = torch.is_grad_enabled()
        try:
            # documentation here omitted for brevity
            torch.set_grad_enabled(self.defaults[&quot;differentiable&quot;])
            torch._dynamo.graph_break()
            ret = func(self, *args, **kwargs)
        finally:
            torch._dynamo.graph_break()
            torch.set_grad_enabled(prev_grad)
        return ret

    functools.update_wrapper(_use_grad, func)
    return _use_grad
</code></pre>
<p>All pytorch optimizers have a <code>self.defaults</code> attribute that holds several flags, including a <code>differentiable</code> flag. The <code>_use_grad_for_differentiable</code> decorator sets the grad status based on the <code>self.defaults[&quot;differentiable&quot;]</code> flag of the optimizer.</p>
<p>This is done to allow for flexibility.</p>
<p>Most optimizers (ie the standard optimizers in Pytorch) have <code>self.defaults[&quot;differentiable&quot;]=False</code>, so this decorator operates like <code>no_grad</code>. This is why the in-place operation in SGD doesn't throw an error.</p>
<p>The use of the <code>differentiable</code> flag allows people to experiment with differentiable optimizers in a way that would not be possible if they simply applied <code>no_grad</code> to the whole thing.</p>
<p>You can also test this. This code runs fine:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

x = nn.Parameter(torch.tensor([2.0, 10.0]))

opt = torch.optim.SGD([x], 1e-3)

loss = x.sum()
loss.backward()

opt.step()
</code></pre>
<p>This code throws an error at the in-place operation:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

x = nn.Parameter(torch.tensor([2.0, 10.0]))

opt = torch.optim.SGD([x], 1e-3)
opt.defaults['differentiable'] = True

loss = x.sum()
loss.backward()

opt.step()
</code></pre>
","2024-09-03 16:55:00","0","Answer"
"78944577","78423100","","<p>I found the solution for my problem recently. I just simply installed <strong>VC_redist.x64</strong> and boom!!! Problem Solved</p>
","2024-09-03 13:25:08","0","Answer"
"78944393","","how can i add skip connection in this section of my code without create a new class for skip connection?","<p>I want to add skip connection to this code in <code>pytorch</code>, how?</p>
<pre class=""lang-py prettyprint-override""><code>class Branch(nn.Module):
    def __init__(self, channels, strides):
        super(Branch, self).__init__()
        
        self.conv_layer = nn.Sequential()
        for i in range(1, len(channels)):
            self.conv_layer.add_module(f'conv_{i}', nn.Conv2d(in_channels=channels[i-1], out_channels=channels[i], kernel_size=3, stride=strides[i-1], padding=(0, 1))),
            self.conv_layer.add_module(f'bn_{i}', nn.BatchNorm2d(channels[i])),
            self.conv_layer.add_module(f'relu_{i}', nn.ReLU())
            self.conv_layer.add_module(f'conv2_{i}', nn.Conv2d(in_channels=channels[i], out_channels=channels[i], kernel_size=3, padding='same')),
            self.conv_layer.add_module(f'bn2_{i}', nn.BatchNorm2d(channels[i])),
            self.conv_layer.add_module(f'relu2_{i}', nn.ReLU())

        self.projector = nn.LazyLinear(256)
        
    def forward(self, x):
        x = self.conv_layer(x)
        x = x.view(x.size(0), x.size(1) * x.size(2), x.size(3)).permute(0, 2, 1)
        x = self.projector(x)
        return x

</code></pre>
<p>I tried searching, but didn't see exactly what I wanted.</p>
","2024-09-03 12:38:05","0","Question"
"78944346","78114412","","<p>I was also facing the same issue. but I downloaded and copy libomp140.x86_64.dll and paste to C:\Windows\System32 then it works properly.</p>
","2024-09-03 12:26:27","0","Answer"
"78943781","78752245","","<p>This works for me if you precise versions:</p>
<pre><code>pip install torch==2.0.0+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
pip install torchvision==0.15.0+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
</code></pre>
<p>Didn’t check the compatible torchaudio version though.</p>
","2024-09-03 10:08:29","3","Answer"
"78943401","","Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error ""Attempting to Unscale FP16 Gradients""","<p>I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?</p>
<p>Here is a minimal example:</p>
<pre class=""lang-none prettyprint-override""><code>import os
from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast
import torch
from torch.cuda.amp import GradScaler, autocast

model_name = &quot;facebook/opt-1.3b&quot;
cache_dir = './models'
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;7&quot;

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16
)

pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, 
                                                    cache_dir=cache_dir,                                                     
                                                    quantization_config=quantization_config)
tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,
                                                    cache_dir=cache_dir)
optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)
scaler = GradScaler()
input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)
labels = torch.LongTensor([[1, 2, 3, 4]]).to(0)
with torch.autocast(device_type='cuda'):
    out = pretrained_model(input_ids=input_ids, labels=labels)
    loss = out.loss
scaler.scale(out.loss).backward()
scaler.step(optimizer) 
scaler.update()
optimizer.zero_grad()

print(f'End')
</code></pre>
<p>At the line <code>scaler.step(optimizer)</code>, an error occurs:</p>
<pre><code>Exception has occurred: ValueError: Attempting to unscale FP16 gradients.

</code></pre>
","2024-09-03 08:38:23","1","Question"
"78942171","","Why doesn't the in-place operation on leaf variables in PyTorch optimizers cause an error?","<p>When I want to do in-place operation on leaf variable, I get an error:</p>
<pre><code>import torch
x = torch.tensor([2.0, 10.0], requires_grad=True)

y = x[0]**2 + x[1]**2

y.backward()

x.add_(0.2, alpha=0.2) # RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
</code></pre>
<p>But we do not have this problem in <code>optim.step()</code> of the optimizer in the training loop. You know this method is operating on neural network parameters which are also leaf variable. I checked the <code>step()</code> function code and did not see <code>with no_grad()</code> statement, so I wonder why in-place operation in this method does not cause an error. Here is in-place operation in optimizer code: <code>param.add_(grad, alpha=-lr)</code>. There was no detaching of the gradient in between.</p>
","2024-09-02 22:29:04","0","Question"
"78941428","","Docker nvidia/cuda/pytorch container doesn't find GPU when inside Dockerfile","<p>I'm trying to set a development environment using Pytorch and nvidia/cuda, but it's not working.</p>
<p>The following command works as expected and recognizes the GPUs:</p>
<pre><code>docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:24.08-py3
</code></pre>
<p>(from <a href=""https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch"" rel=""nofollow noreferrer"">https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch</a>)</p>
<p>(And it takes a few minutes to download and start the container.)</p>
<p>But if I use the following Dockerfile, the GPUs are NOT recognized (I'm trying to build the container using ./devcontainer/Dockerfile in VSCode in Windows 11 with WSL2):</p>
<pre><code>ARG gpus=all
FROM nvcr.io/nvidia/pytorch:24.08-py3
</code></pre>
<p>Trying to run</p>
<pre><code>python -c 'import torch; print(torch.cuda.current_device())'
</code></pre>
<p>gives this error:</p>
<pre><code>RuntimeError: Found no NVIDIA driver on your system. 
Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
</code></pre>
<p>The command 'nvcc --version' runs ok:</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Fri_Jun_14_16:34:21_PDT_2024
Cuda compilation tools, release 12.6, V12.6.20
Build cuda_12.6.r12.6/compiler.34431801_0
</code></pre>
<p>But the command 'nvidia-smi' returns nothing (and it works normally with 'docker run' commnand.)</p>
<p>Can someone, please, give some hint about how to correct this?</p>
","2024-09-02 16:49:22","0","Question"
"78941369","78940865","","<p>Cross entropy loss is used when a single output class is being predicted. When you say <code>the number of classes depends on the sample</code>, I assume you mean a situation where the number of logits is different for each sample is different, but we are still in a cross entropy situation where each sample has one correct class.</p>
<p>In this case you can simply pad the samples with <code>-inf</code> which will be ignored in the cross entropy loss calculation.</p>
<pre class=""lang-py prettyprint-override""><code># start with our sequences
sequences = [
    [0.2, 0.2, 0.6],
    [0.4, 0.1, 0.1, 0.4],
    [0.2, 0.8]
]
sequences = [torch.tensor(i) for i in sequences]

# represent labels as class int values
# this is required for pytorch's crossentropyloss
labels = torch.tensor([2, 0, 0]).long()

# pack sequences into a square batch
# fill padding values with `-inf`
padding_value = float('-inf')
sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=padding_value)

# create `CrossEntropyLoss` with `reduction='none'`
# this makes the loss return the value for each input (ie no averaging) 
# so we can compare values
loss = nn.CrossEntropyLoss(reduction='none')

# compute loss on individual sequences without padding
l1 = torch.stack([loss(sequences[i], labels[i]) for i in range(labels.shape[0])])

# compute loss on padded sequences
l2 = loss(sequences_padded, labels)

# check values match
assert torch.allclose(l1, l2)
</code></pre>
<p>This works because cross entropy computes <code>exp(i)</code> for all values in the input, and <code>exp(-inf)</code> evals to 0. Because of this, the padding values have no impact on the output loss.</p>
","2024-09-02 16:27:14","2","Answer"
"78941321","78940523","","<p>You can use <a href=""https://pytorch.org/docs/stable/generated/torch.vmap.html"" rel=""nofollow noreferrer"">torch.vmap</a> for this exact purpose</p>
<pre class=""lang-py prettyprint-override""><code>class LinearMultidimModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearMultidimModel, self).__init__()
        self.weight = nn.Parameter(torch.randn(input_dim, hidden_dim, output_dim))
        self.bias = nn.Parameter(torch.randn(output_dim))

    def forward(self, x):
        # Using torch.tensordot to perform the linear transformation
        out = torch.tensordot(x, self.weight, dims=[[0,1],[0,1]]) + self.bias
        return out
    
input_dim = 3
hidden_dim=2
output_dim = 1
model = LinearMultidimModel(input_dim, output_dim)

# create input with batch dimension
batch_size = 8
x = torch.randn(batch_size, input_dim, hidden_dim)

# example unbatched inference
y1 = torch.stack([model(i) for i in x])

# vmap model to make it batched
model_batched = torch.func.vmap(model)

# batch inference
y2 = model_batched(x)

# assert outputs are the same
assert torch.allclose(y1, y2)
</code></pre>
","2024-09-02 16:10:03","1","Answer"
"78941297","78936279","","<p>tl;dr remove the following lines because they break PIL image conversion:</p>
<pre class=""lang-py prettyprint-override""><code>train_dataset.data = train_dataset.data / 255

val_dataset.data = val_dataset.data / 255
</code></pre>
<p>In detail:</p>
<p>This is the <code>__getitem__</code> method of the <a href=""https://pytorch.org/vision/stable/_modules/torchvision/datasets/mnist.html#MNIST"" rel=""nofollow noreferrer"">MNIST dataset</a>:</p>
<pre class=""lang-py prettyprint-override""><code>def __getitem__(self, index: int) -&gt; Tuple[Any, Any]:
    img, target = self.data[index], int(self.targets[index])

    # doing this so that it is consistent with all other datasets
    # to return a PIL Image
    img = Image.fromarray(img.numpy(), mode=&quot;L&quot;)

    if self.transform is not None:
        img = self.transform(img)

    if self.target_transform is not None:
        target = self.target_transform(target)

    return img, target
</code></pre>
<p>The data item is first transformed into a PIL image, then sent through the transforms that convert it to a tensor.</p>
<p>PIL expects the input to be of dtype <code>uint8</code>. By default, the <code>data</code> attribute tensor in the dataset is dtype <code>uint8</code>.</p>
<p>When you divide by 255, you convert the tensor from <code>uint8</code> to <code>float32</code>.</p>
<p>This causes PIL to produce weird results. For example:</p>
<pre class=""lang-py prettyprint-override""><code># take a chunk of an image as an example
original_array = dataset1.data[0][8:12, 8:12].numpy()
print(original_array)
&gt; [[219 253 253 253]
 [ 80 156 107 253]
 [  0  14   1 154]
 [  0   0   0 139]]

# scale by dividing by 255
scaled_array = original_array/255
print(scaled_array)
&gt; [[0.85882353 0.99215686 0.99215686 0.99215686]
 [0.31372549 0.61176471 0.41960784 0.99215686]
 [0.         0.05490196 0.00392157 0.60392157]
 [0.         0.         0.         0.54509804]]

# applying PIL transform to original array results in the same output
pil_original = np.array(Image.fromarray(original_array, mode='L'))
print(pil_original)
&gt; [[219 253 253 253]
 [ 80 156 107 253]
 [  0  14   1 154]
 [  0   0   0 139]]

# applying PIL transform to scale array results in messed up values
pil_scaled = np.array(Image.fromarray(scaled_array, mode='L'))
print(pil_scaled)
&gt; [[123 123 123 123]
 [123 123 235  63]
 [192 191 191 191]
 [191 191 239  63]]
</code></pre>
","2024-09-02 16:03:20","0","Answer"
"78940865","","Cross-entropy loss with varying number of classes","<p>Is there a standard/efficient way in <code>Pytorch</code> to handle cross-entropy loss for a classification problem where the number of classes depends on the sample?</p>
<p>Example:
In a batch of size 3, I have:</p>
<pre><code>logits1 = [0.2, 0.2, 0.6],      labels1 = [0, 0, 1]
logits2 = [0.4, 0.1, 0.1, 0.4], labels2 = [1, 0, 0, 0]
logits3 = [0.2, 0.8],           labels3 = [1, 0]
</code></pre>
<p>I am looking for the right way to compute <code>cross_entropy_loss(logits,labels)</code> on this batch.</p>
","2024-09-02 14:05:04","0","Question"
"78940523","","Paralellizing a natively single-batch pytorch model","<p>Is it possible to parallelize a (natively) single batch model?</p>
<p>Usually parallelization is done via the torch.bmm (batched matrix multiplication) in stead of the torch.matmul and fixing one dimension specifically for the batches. However this is not available for example for the torch.tensordot function.</p>
<p>So if one has such a model, is it possible to compute each gradient of the batch in parallel? Ideally, the parallelization should work with both training and inference.</p>
<p>A code example:</p>
<pre><code>import torch
import torch.nn as nn

class LinearMultidimModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearMultidimModel, self).__init__()
        self.weight = nn.Parameter(torch.randn(input_dim, hidden_dim, output_dim))
        self.bias = nn.Parameter(torch.randn(output_dim))

    def forward(self, x):
        # Using torch.tensordot to perform the linear transformation
        out = torch.tensordot(x, self.weight, dims=[[0,1],[0,1]]) + self.bias
        return out

# Example usage
input_dim = 3
hidden_dim=2
output_dim = 1
model = LinearMultidimModel(input_dim, output_dim)

# Dummy input
x = torch.randn(input_dim, hidden_dim)# But what if I want to put in a batch, torch.randn(batch_size, input_dim, hidden_dim)?
output = model(x)
print(output)
</code></pre>
<p>Keep in mind that if there is no hidden_dim, it natively does the parallelization, one can entirely remove hidden_dim and get a result with</p>
<p><code>x = torch.randn(5, input_dim)</code>.</p>
<p>I've tried using Einsum, but that works for a fixed amount of hidden dimensions...</p>
","2024-09-02 12:42:44","0","Question"
"78940359","78114412","","<p>I solved the problem by running the following command:
pip3 install torch==2.3.1+cpu torchvision==0.18.1+cpu -f <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/torch_stable.html</a></p>
","2024-09-02 12:02:08","0","Answer"
"78938325","78937980","","<p>R2 is not symmetric. You get different results because you change which variable is the ground truth.</p>
<p>In your first section of code, you treat <code>var1</code> as the ground truth values and <code>var2</code> as the predicted values.</p>
<p>In your second section of code, you treat <code>var2</code> as the ground truth and <code>var1</code> as the predicted value.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torcheval.metrics.regression import R2Score

var1 = torch.randn(64)
var2 = torch.randn(64)

def compute_r2(inputs, targets):
    ss_total = torch.sum((targets - torch.mean(targets)) ** 2)
    ss_residual = torch.sum((targets - inputs) ** 2)
    r2 = 1 - (ss_residual / ss_total)
    return r2

def compute_r2_metric(inputs, targets):
    metric = R2Score()
    metric.update(inputs, targets)
    r2 = metric.compute()
    return r2

# treat var1 as input, var2 as target
print(compute_r2(var1, var2))
&gt; tensor(-0.8324)

print(compute_r2_metric(var1, var2))
&gt; tensor(-0.8324)

# treat var2 as input, var1 as target
print(compute_r2(var2, var1))
&gt; tensor(-1.9208)

print(compute_r2_metric(var2, var1))
&gt; tensor(-1.9208)
</code></pre>
","2024-09-01 20:34:01","2","Answer"
"78937980","","Can't reproduce Sum of Squares Calculation in torcheval.metrics.r2_score","<p>I calculate R^2 manually and compare the results to torcheval.metrics.regression.r2_score without multioutput but I do not tie out the total sum squared calculation, and hence manual R^2 is different than torcheval:</p>
<p>Manual approach:</p>
<pre><code>#Manual approach code
ss_total = torch.sum((var1 - torch.mean(var1)) ** 2)
ss_residual = torch.sum((var1 - var2) ** 2)
r2 = 1 - (ss_residual / ss_total)
print(&quot;R^2 manual&quot;,r2, &quot;my ss_total&quot;, ss_total, &quot;ss_residual&quot;, ss_residual)
#R^2 manual tensor(-1.4128, device='cuda:0') my ss_total tensor(3.7081, device='cuda:0') ss_residual tensor(8.9471, device='cuda:0')
</code></pre>
<p><a href=""https://i.sstatic.net/IxhIHZzW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IxhIHZzW.png"" alt=""manual calculation results"" /></a></p>
<p><a href=""https://pytorch.org/torcheval/main/_modules/torcheval/metrics/functional/regression/r2_score.html"" rel=""nofollow noreferrer"">Torcheval.metrics approach</a> tss formulae documentation without multioutput:</p>
<pre><code>sum_squared_obs = torch.sum((actual - torch.mean(actual)) ** 2)
tss sum squared calculation = sum_squared_obs - torch.square(sum_obs) / num_obs
r_squared = 1 - (rss / tss)

#torcheval.metrics.regression.r2_score tested in script
metric = R2Score(device=device)
update = metric.update(var1, var2)
print(&quot;sum_squared_residual&quot;,update.sum_squared_residual)
print(&quot;sum_obs&quot;,update.sum_obs)
print(&quot;torch.square(sum_obs)&quot;,torch.square(update.sum_obs))
print(&quot;num_obs&quot;,len(var1))
print(&quot;sum_squared_obs&quot;,update.sum_squared_obs)
r2_py = metric.compute()
print(&quot;R^2 pytorch&quot;,r2_py)
#sum_squared_residual tensor(8.9471, device='cuda:0')
#sum_obs tensor(-29.9617, device='cuda:0')
#torch.square(sum_obs) tensor(897.7044, device='cuda:0')
#num_obs 64
#sum_squared_obs tensor(22.2245, device='cuda:0')
#R^2 pytorch tensor(-0.0914, device='cuda:0')
#R^2 var_weight pytorch tensor(-0.0914, device='cuda:0')
</code></pre>
<p><a href=""https://i.sstatic.net/gwmfyT0I.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gwmfyT0I.png"" alt=""reverse in excel torcheval r_square"" /></a></p>
<p>I cannot tie out tss.</p>
<p>Can someone explain what is the difference between the two approaches?</p>
<p>I have <a href=""https://docs.google.com/spreadsheets/d/1gDlX1dHXYHhZoT3nHE_O37OgmLtRMrzo/edit?usp=sharing&amp;ouid=106321149152378850929&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">saved the actual and predicted values in excel</a> and calculate R^2 with both approaches.</p>
","2024-09-01 17:32:59","0","Question"
"78937940","78937921","","<p>You fist need to offload tensor to cpu, then detach it from gradients and only then convert it to numpy:</p>
<pre><code>y_hat.cpu().detach().numpy()
</code></pre>
<p>in your case most probably you have list of tensors inside y_hat. So you need to apply this procedure to each element of y_hat:</p>
<pre><code>y_hat = [y.cpu().detach().numpy() for y in y_hat]
</code></pre>
","2024-09-01 17:17:31","1","Answer"
"78937921","","Unable to convert cuda:0 device type tensor to numpy","<p>I have <code>y_hat</code> variable of type <code>list</code>. I am unable to convert it to numpy array. It gives following error:</p>
<pre><code>TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
</code></pre>
<p>Also I cannot call <code>.cpu()</code> on it to move <code>y_hat</code> from CUDA to RAM.</p>
<p>Here is the screen grab of vscode debug console:</p>
<p><a href=""https://i.sstatic.net/oTxWR3eA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oTxWR3eA.png"" alt=""enter image description here"" /></a></p>
<p>What I am missing here?</p>
","2024-09-01 17:10:19","0","Question"
"78937830","78937759","","<h4>The iterator protocol when you don't define <code>__iter__</code></h4>
<p>What you are seeing here is the standard iteration protocol in Python for a class that does not implement <code>__iter__</code>.</p>
<p>When you write <code>for x, y in ds:</code>, and <code>ds</code> does not have an <code>__iter__</code> method, what Python runs is equivalent to something like this:</p>
<pre><code>i = 0
while True:
    try:
        x, y = ds[i]
    except IndexError:
        break
    [the body of your loop]
    i = i + 1
</code></pre>
<h4>What that means in your code</h4>
<p>In your code, that <code>IndexError</code> happens at <code>index==4141</code> because of the <code>window_size</code> parameter:</p>
<p>With <code>window_size=200</code>, <code>self.x[index: index + self.window]</code> is <code>self.x[4141:4341]</code> when <code>index==4141</code> but your <code>self.x</code> has len 4340. Hence it's that sub-expression that raises the <code>IndexError</code>. This is consistent with your assignment of <code>self.len</code> in <code>__init__</code> except Python does not rely on that to infer it's at the end, it waits for the <code>IndexError</code> exception.</p>
<h4>A minimal example illustrating the iterator protocol</h4>
<p>Here's a much shorter example that illustrates all this:</p>
<pre><code>class MySeq:
    data = list(range(10,15))

    def __len__(self):
        print(&quot;Called __len__&quot;)
        return len(self.data)

    def __getitem__(self, i):
        print(f&quot;Called __getitem__({i=})&quot;)
        return self.data[i]

myseq = MySeq()
for x in myseq:
    print(f&quot;Got {x=}&quot;)
</code></pre>
<p>When you run this, you get this output:</p>
<pre><code>Called __getitem__(i=0)
Got x=10
Called __getitem__(i=1)
Got x=11
Called __getitem__(i=2)
Got x=12
Called __getitem__(i=3)
Got x=13
Called __getitem__(i=4)
Got x=14
Called __getitem__(i=5)
</code></pre>
<p>That last line, calling <code>__getitem__(i=5)</code>, is going to raise <code>IndexError</code> since <code>len=5</code> the way I created my example, and that's what stops the iteration.</p>
<p>By contrast, if you add an <code>__iter__</code> function to this class:</p>
<pre><code>    def __iter__(self):
        print(&quot;Called __iter__&quot;)
        return iter(self.data)
</code></pre>
<p>the output becomes:</p>
<pre><code>Called __iter__
Got x=10
Got x=11
Got x=12
Got x=13
Got x=14
</code></pre>
<p>Notice that in either case, <code>__len__</code> does not get called, that's just not how iteration works in Python. Iteration just continues until an <code>IndexError</code> is raised if you don't implement <code>__iter__</code>. And if you do implement it, then the protocol relies on the <code>StopIteration</code> exception being raised to end iteration.</p>
","2024-09-01 16:24:32","0","Answer"
"78937759","","Pytorch `DataSet.__getitem__()` called with `index` bigger than `__len__()`","<p>I have following torch dataset (I have replaced actual code to read data from files with random number generation to make it minimal reproducible):</p>
<pre><code>from torch.utils.data import Dataset
import torch 

class TempDataset(Dataset):
    def __init__(self, window_size=200):
        
        self.window = window_size

        self.x = torch.randn(4340, 10, dtype=torch.float32) # None
        self.y = torch.randn(4340, 3, dtype=torch.float32) 

        self.len = len(self.x) - self.window + 1 # = 4340 - 200 + 1 = 4141 
                                                # Hence, last window start index = 4140 
                                                # And last window will range from 4140 to 4339, i.e. total 200 elements

    def __len__(self):
        return self.len

    def __getitem__(self, index):

        # AFAIU, below if-condition should NEVER evaluate to True as last index with which
        # __getitem__ is called should be self.len - 1
        if index == self.len: 
            print('self.__len__(): ', self.__len__())
            print('Tried to access eleemnt @ index: ', index)
            
        return self.x[index: index + self.window], self.y[index + self.window - 1]

ds = TempDataset(window_size=200)
print('len: ', len(ds))
counter = 0 # no record is read yet
for x, y in ds:
    counter += 1 # above line read one more record from the dataset
print('counter: ', counter)
</code></pre>
<p>It prints:</p>
<pre><code>len:  4141
self.__len__():  4141
Tried to access eleemnt @ index:  4141
counter:  4141
</code></pre>
<p>As far as I understand, <code>__getitem__()</code> is called with <code>index</code> ranging from <code>0</code> to <code>__len__()-1</code>. If thats correct, then why it tried to call <code>__getitem__()</code> with index 4141, when the length of the data itself is 4141?</p>
<p>One more thing I noticed is that despite getting called with <code>index = 4141</code>, it does not seem to return any elements, which is why <code>counter</code> stays at 4141</p>
<p>What my eyes (or brain) are missing here?</p>
<p>PS: Though it wont have any effect, just to confirm, I also tried to wrap <code>DataSet</code> with torch <code>DataLoader</code> and it still behaves the same.</p>
","2024-09-01 15:52:18","1","Question"
"78936467","78925158","","<p>Your problem, with modest dimensions:</p>
<pre><code>In [16]: m = 30
    ...: n = 4
    ...: B = np.random.randn(m, n)
    ...: A = np.random.randn(n, n)

In [16]: result = np.zeros((m,1))
    ...: for i in range(m):
    ...:     x = np.atleast_2d(B[i])
    ...:     result[i] = x@A.T@A@x.T@x@A@A.T@x.T

In [18]: result[:5,0]
Out[18]: array([20.84203476, 86.48468007, 16.52952006, 14.27289909, 67.28749281])
</code></pre>
<p>And a timing (1ms for this size doesn't look too bad):</p>
<pre><code>In [19]: %%timeit
    ...: result = np.zeros((m,1))
    ...: for i in range(m):
    ...:     x = np.atleast_2d(B[i])
    ...:     result[i] = x@A.T@A@x.T@x@A@A.T@x.T
    ...:     
1.33 ms ± 2.78 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</code></pre>
<p>Is the (m,1) shape important?  Why not (m,)?</p>
<h2>np.einsum</h2>
<p>The proposed einsum solutions (which just differ in how the transpose is applied):</p>
<pre><code>In [20]: result_2 = np.einsum('ab,bc,cd,da,ae,ef,fg,ga-&gt;a', B, A.T, A, B.T, B, A, A.T, B.T)

In [21]: result_2.shape
Out[21]: (30,)

In [22]: result_2[:5]
Out[22]: array([20.84203476, 86.48468007, 16.52952006, 14.27289909, 67.28749281])

In [23]: timeit result_2 = np.einsum('ab,bc,cd,da,ae,ef,fg,ga-&gt;a', B, A.T, A, B.T, B, A, A.T, B.T)
5.77 ms ± 13.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
<p>This is slower than the OP's iteration.  Iterating 30 times on a complex expression is no big deal.  Often that kind of iteration is faster than some whole-array form.</p>
<p>We could explore the <code>einsum</code> optimize options.  For example:</p>
<pre><code>In [24]: timeit result_2 = np.einsum('ab,bc,cd,da,ae,ef,fg,ga-&gt;a', B, A.T, A, B.T, B, A, A.T, B.T, optimize=True)
1.03 ms ± 3.34 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</code></pre>
<p><code>einsum</code> breaks the complex calculation into a sequence of simple <code>matmul/dot</code> calculations.</p>
<h2>pure matmul</h2>
<p>An alternative is use <code>matmul</code> with the <code>m</code> dimension as a 'batch'.  Let's see if that works.</p>
<p>I explored this recently for a simpler calculation, 'x@A@x^T'.</p>
<p><a href=""https://stackoverflow.com/a/78910239/901925"">https://stackoverflow.com/a/78910239/901925</a></p>
<p>The common use of <code>torch.unsqueeze</code> and <code>(m,1)</code> shape suggests a related, or overlapping question authorship.</p>
<p>There I used:</p>
<pre><code>B[:,None,:]@A@B[:,:,None] # (m,1,1)
</code></pre>
<p>to make <code>B</code> (and <code>B.T</code>) 3d with <code>m</code> as the first (batch) dimension.  Note the (m,1,1) shape. Think of that as <code>m</code> (1,1) results.  The extra dimensions are often <code>squeeze</code> out.  <code>matmul</code> uses fast BLAS code to handle the 2d matrix produce, while iterating in compiled code over the leading 'batch' dimension.  That's similar to what the OP does, but without the slower python level iteration.</p>
<p>So defining 3d <code>B</code> and <code>B.T</code> as:</p>
<pre><code>In [25]: B1 = B[:,None,:]; B2 = B[:,:,None]
</code></pre>
<p>we can perform the <code>matmul</code> as before:</p>
<pre><code>In [26]: result_3 = B1@A.T@A@B2@B1@A@A.T@B2

In [27]: result_3.shape
Out[27]: (30, 1, 1)    
In [28]: result_3[:5,0,0]
Out[28]: array([20.84203476, 86.48468007, 16.52952006, 14.27289909, 67.28749281])    
In [29]: np.allclose(result[:,0],result_3[:,0,0])
Out[29]: True
</code></pre>
<p>This is quite a bit faster:</p>
<pre><code>In [31]: %%timeit
    ...: B1 = B[:,None,:]; B2 = B[:,:,None]
    ...: result_3 = B1@A.T@A@B2@B1@A@A.T@B2
    ...: result_3[:,0]
49.6 µs ± 110 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
</code></pre>
<h2>einsum_path</h2>
<p><code>einsum_path</code> can be used to see how <code>einsum</code> can speed up complicated calculations like this.  I won't go into the details, but this gives some idea of what it can do:</p>
<pre><code>In [40]: print(np.einsum_path('ab,bc,cd,da,ae,ef,fg,ga-&gt;a', B, A.T, A, B.T, B, A, A.T, B.T)[1])
  Complete contraction:  ab,bc,cd,da,ae,ef,fg,ga-&gt;a
         Naive scaling:  7
     Optimized scaling:  3
      Naive FLOP count:  9.830e+05
  Optimized FLOP count:  2.777e+03
   Theoretical speedup:  353.994
  Largest intermediate:  1.200e+02 elements
--------------------------------------------------------------------------
scaling                  current                                remaining
--------------------------------------------------------------------------
   3                   cd,bc-&gt;bd                  ab,da,ae,ef,fg,ga,bd-&gt;a
   3                   fg,ef-&gt;eg                     ab,da,ae,ga,bd,eg-&gt;a
   3                   bd,ab-&gt;da                        da,ae,ga,eg,da-&gt;a
   2                    da,da-&gt;a                            ae,ga,eg,a-&gt;a
   2                    a,ae-&gt;ea                              ga,eg,ea-&gt;a
   3                   eg,ga-&gt;ea                                 ea,ea-&gt;a
   2                    ea,ea-&gt;a                                     a-&gt;a
</code></pre>
<p>Examining this path suggests that we can improve the long matmul sequence by precalculating terms like <code>AA=A.T@A</code> and even <code>B1@AA</code>.</p>
","2024-09-01 02:31:16","1","Answer"
"78936279","","Flux.1 Schnell image generator issue, GPU resources getting exhausted after 1 prompt","<p>So, I tried to train a prompt based image generation model using FLUX.1-schnell. I used Lightning AI Studio (an alternate to Google Colab), that helped me to access to L40 GPU, that came with 48gb VRAM (since the minimum requirement is 24 GB.</p>
<p>I trained it with a LoRA, which was setup with the help of &quot;ai-toolkit&quot; cloned from a GitHub repo <a href=""https://github.com/ostris/ai-toolkit"" rel=""nofollow noreferrer"">&quot;click here&quot;</a>, at 1200 steps with 35 images as training dataset. <a href=""https://medium.com/@naman1011/fine-tuning-flux-1-customizing-your-ai-image-generator-58f75e7ffe6d"" rel=""nofollow noreferrer"">reference link</a></p>
<p>The first prompt worked pretty fine and generated a desired result, but when I went to generate a second image it said:</p>
<blockquote>
<p>CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 71.25 MiB is free. Process 26865 has 44.45 GiB memory in use. Of the allocated memory 43.89 GiB is allocated by PyTorch, and 56.25 MiB is reserved by PyTorch but unallocated.</p>
</blockquote>
<p>I tried to run the code on the Jupyter Notebook, which was successful and was able to create <code>safetensors</code> after training the model.</p>
<p>Now, for the first prompt, I ran the cell with the following code:</p>
<pre><code>import torch
from diffusers import FluxPipeline
# Load Flux
pipe = FluxPipeline.from_pretrained(&quot;black-forest-labs/FLUX.1-schnell&quot;, torch_dtype=torch.float16).to(&quot;cuda&quot;)

# Load your fine-tuned model
pipe.load_lora_weights(&quot;/teamspace/studios/this_studio/ai-toolkit/output/fluxGirl/fluxGirl.safetensors&quot;, adapter_name=&quot;default&quot;)
# Generate an image
prompt = &quot;anime girl with red hair drinking coffee&quot;

image = pipe(prompt, height=512, width=512).images[0]
# Save the image
image.save(&quot;test1.png&quot;)
</code></pre>
<p>It gave me a good result, but for the second image, I kept everything the same and changed the prompt and then ran the cell, but got the above error.</p>
<p>How to fix this, so that I can atleast generate three images at a time.</p>
<p>Another thing, since I have already save the output files such as:</p>
<p><code>model.safetensor and optimizers.pt</code> <a href=""https://i.sstatic.net/lGsVheN9.png"" rel=""nofollow noreferrer"">image</a>,</p>
<p>Where should I make changes to not entirely download the model when once the GPU and codes are initiated.</p>
","2024-08-31 23:04:55","0","Question"
"78933201","78625475","","<p>In VITS, the input to the HiFi-GAN module is not a traditional mel-spectrogram, but rather latent variables produced by the Encoder, which is conditioned by the text and an alignment model. In this way, one can say that VITS is an end-to-end model.</p>
<p>The HiFi-GAN in VITS acts as a decoder that takes these latent variables and generates the final audio waveform directly. Therefore, what you are trying to plot is not a mel-spectrogram, but rather these latent variations.</p>
<p>This approach allows VITS to maintain flexibility and high-quality synthesis without relying on traditional interactive representations such as mel-spectrograms.</p>
","2024-08-30 17:38:10","2","Answer"
"78932637","78932500","","<p>Here is a PyTorch for training RNN example,</p>
<pre><code>import os
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim

class ImageSequenceDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.sequences = sorted(os.listdir(root_dir))

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence_dir = os.path.join(self.root_dir, self.sequences[idx])
        images = sorted(os.listdir(sequence_dir))
        image_sequence = []
        for image_name in images:
            image_path = os.path.join(sequence_dir, image_name)
            image = Image.open(image_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            image_sequence.append(image)

        #convert list of images into a single tensor
        image_sequence = torch.stack(image_sequence)
        #example: Replace with your method to get the label
        label = self.get_label(sequence_dir)
        return image_sequence, label

    def get_label(self, sequence_dir):
        #implement your label extraction logic here
        #this is a dummy implementation, replace it with your actual label logic
        if &quot;positive&quot; in sequence_dir:
            return 1
        else:
            return 0
</code></pre>
<p>then you must define your transforms,</p>
<pre><code>transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
</code></pre>
<p>create a dataset and loader,</p>
<pre><code>#assuming the data is located in &quot;path/to/your/data&quot;
dataset = ImageSequenceDataset(root_dir=&quot;path/to/your/data&quot;, transform=transform)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)
</code></pre>
<p>now you can setup your RNN model, something like,</p>
<pre><code>class RNNModel(nn.Module):
    def __init__(self):
        super(RNNModel, self).__init__()
        #example CNN to process each frame
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.rnn = nn.LSTM(input_size=64*16*16, hidden_size=128, num_layers=1, batch_first=True)
        self.fc = nn.Linear(128, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        cnn_out = []
        for t in range(seq_len):
            out = self.cnn(x[:, t, :, :, :])
            out = out.view(batch_size, -1)  #flatten the output
            cnn_out.append(out)
        
        cnn_out = torch.stack(cnn_out, dim=1)  #reshape to (batch_size, seq_len, -1)
        rnn_out, _ = self.rnn(cnn_out)
        out = self.fc(rnn_out[:, -1, :])  #use the last RNN output
        out = self.sigmoid(out)
        return out
</code></pre>
<p>model init and loss function definition,</p>
<pre><code>model = RNNModel()
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)
</code></pre>
<p>then start training your model,</p>
<pre><code>num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    
    for i, (inputs, labels) in enumerate(dataloader):
        inputs, labels = inputs, labels.float().view(-1, 1)  #adjust labels shape
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    
    print(f&quot;Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}&quot;)
</code></pre>
<p>Hope this example code snippets would be helpful. cheers!</p>
","2024-08-30 14:53:49","0","Answer"
"78932500","","How to import dataset for image sequence classification?","<p>I want to build a model (RNN) on an image sequence to predict a binary variable. I saw a lot of tutorials for multi-input classification, but I didn't find anywhere how to import my data ?
I have a folder containing folders of image sequence like this:</p>
<p>-- film_1</p>
<blockquote>
<p>-------------film_1_image1</p>
</blockquote>
<blockquote>
<p>-------------film_1_image2</p>
</blockquote>
<blockquote>
<p>-------------film_1_image3</p>
</blockquote>
<blockquote>
<p>-------------film_1_image4</p>
</blockquote>
<p>-- film_2</p>
<blockquote>
<p>-------------film_2_image1</p>
</blockquote>
<blockquote>
<p>-------------film_2_image2</p>
</blockquote>
<blockquote>
<p>-------------film_2_image3</p>
</blockquote>
<blockquote>
<p>-------------film_2_image4</p>
</blockquote>
<p>I know that I need data with the shape (batch, time, width, height) but I didn't know how to do that. I searched for a solution in <code>keras</code> and <code>pytorch</code> but I didn't find anything.
I would like to import my films, split the dataset in train/validation and train a model.
Does anyone know how to import data like that ?</p>
<p>I tried to use a <code>DataLoader</code> but it's quite difficult to understand how it works. So if anyone can help, it would be great.</p>
","2024-08-30 14:22:35","0","Question"
"78929592","","'torch' has no attribute 'float8_e4m3fn'","<p>I am using Python 3.10.12 in Juppiter Lab. While running the script (<a href=""https://github.com/ufal/evalatin2024-latinpipe/blob/main/latinpipe_evalatin24.py"" rel=""nofollow noreferrer"">from here</a>) I keep receiving the following error.</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/jupyter/work/resources/evalatin2024-latinpipe/latinpipe_evalatin24.py&quot;, line 25, in &lt;module&gt;
    import keras
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/__init__.py&quot;, line 4, in &lt;module&gt;
    from keras.api import DTypePolicy
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/api/__init__.py&quot;, line 8, in &lt;module&gt;
    from keras.api import activations
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/api/activations/__init__.py&quot;, line 7, in &lt;module&gt;
    from keras.src.activations import deserialize
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/src/__init__.py&quot;, line 1, in &lt;module&gt;
    from keras.src import activations
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/src/activations/__init__.py&quot;, line 3, in &lt;module&gt;
    from keras.src.activations.activations import elu
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/src/activations/activations.py&quot;, line 1, in &lt;module&gt;
    from keras.src import backend
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/src/backend/__init__.py&quot;, line 39, in &lt;module&gt;
    from keras.src.backend.torch import *  # noqa: F403
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/src/backend/torch/__init__.py&quot;, line 17, in &lt;module&gt;
    from keras.src.backend.torch import core
  File &quot;/home/jupyter/.local/lib/python3.10/site-packages/keras/src/backend/torch/core.py&quot;, line 47, in &lt;module&gt;
    &quot;float8_e4m3fn&quot;: torch.float8_e4m3fn,
AttributeError: module 'torch' has no attribute 'float8_e4m3fn'
</code></pre>
<p>I understand the problem concerns torch, and I have tried upgraging and downgrading it, but for no good. <code>!nvidia-smi</code> tells that my virtual machine has CUDA-12.2.</p>
<p>Here are some of the installed packages:</p>
<pre><code>keras                            3.5.0
nvidia-cublas-cu12               12.1.3.1
nvidia-cuda-cupti-cu12           12.1.105
nvidia-cuda-nvrtc-cu12           12.1.105
nvidia-cuda-runtime-cu12         12.1.105
nvidia-cudnn-cu12                9.1.0.70
nvidia-cufft-cu12                11.0.2.54
nvidia-tensorrt                  99.0.0
pip                              23.0.1
tensorflow                       2.17.0
tensorflow-datasets              4.9.2
tensorflow-estimator             2.12.0
tensorflow-gcs-config            2.12.0
tensorflow-hub                   0.14.0
tensorflow-io-gcs-filesystem     0.32.0
tensorflow-metadata              1.13.1
tensorflow-probability           0.20.1
tensorrt                         10.3.0
tensorrt-cu12                    10.3.0
tensorrt-cu12-bindings           10.3.0
tensorrt-cu12-libs               10.3.0
torch                            2.0.1+cu118
torchaudio                       2.0.2+cu118
torchdata                        0.6.1
torchsummary                     1.5.1
torchtext                        0.15.2
torchvision                      0.15.2+cu118
transformers                     4.43.2
</code></pre>
<p><code>print(torch.version.cuda)</code> returns 11.8 (which is the version required by the program I use). I am not even sure that this is relevant for the error I get, but some <a href=""https://github.com/huggingface/transformers/issues/32185"" rel=""nofollow noreferrer"">discussions</a> suggest that it might be caused by a conflict. I did not manage to downgrade the system's CUDA.</p>
","2024-08-29 20:43:27","1","Question"
"78925330","78925158","","<p>Let write it down, using Einstein notation, the matrix-element (a,h) of the result is</p>
<pre><code>(x  @     A^T @       A    @   x^T       @ x       @  A        @ A^T      @ (x^T)_(a,i)   = 
x_(a,b)  (A^T)_(b,c)  A_(c,d) (x^T)_(d,e)  x_(e,f)    A_(f,g)   (A^T)_(g,h) (x^T)_(h,i)   =
x_(a,b)   A_(c,b)     A_(c,d)  x_(e,d)     x_(e,f)    A_(f,g)    A_(h,g) x_(i,h)          =
x_(1,b)   A_(c,b)     A_(c,d)  x_(1,d)     x_(1,f)    A_(f,g)    A_(h,g) x_(1,h)          =
</code></pre>
<p>which is actually a scalar. If you want it for all the x, using matrix B</p>
<pre><code>B_(a,b)   A_(c,b)     A_(c,d)  B_(a,d)     B_(a,f)    A_(f,g)    A_(h,g) B_(a,h)
</code></pre>
<p>so the argument will be</p>
<pre><code>('ab,cb,cd,ad,af,fg,hg,ah-&gt;a', B, A, A, B, B, A, A, B)
</code></pre>
","2024-08-28 21:23:39","1","Answer"
"78925274","78925158","","<p>You can use the following call:</p>
<pre class=""lang-python prettyprint-override""><code>result_2 = torch.einsum('ab,bc,cd,da,ae,ef,fg,ga-&gt;a', B, A.T, A, B.T, B, A, A.T, B.T)
</code></pre>
<p>The key thing is the index <code>a</code>. That index indicates the row of B. From what you describe, you do not want to do matrix multiplication between B and A. For example, what you denote by &quot;B @ A.T @ A @ B.T@&quot; is a a vector of length m in your problem, while &quot;@&quot; was just matrix multiplication, the result would be a matrix of size (m,m). By using the same index <code>a</code> every time we refer to the rows of <code>B</code>, we have each row just be 'einsumed' with itself, rather than getting mixed with the other rows.</p>
<p>For example, consider the alternative</p>
<pre class=""lang-python prettyprint-override""><code>result_3 = torch.einsum('ab,bc,cd,de,ef,fg,gh,ha-&gt;a', B, A.T, A, B.T, B, A, A.T, B.T)
</code></pre>
<p>This alternative will just do the matrix multiplication, and return the diagonal elements of the resulting (m,m) matrix.</p>
","2024-08-28 21:01:44","1","Answer"
"78925158","","Using einsum for transpose times matrix times transpose: x@A^T@A@x^T@x@A@A^T@x^T","<p>So I have m number of different vectors (say x), each one is (1,n), stacked horizontally, totally in a (m,n) matrix we call it B, and a matrix (A) with dimension (n,n).</p>
<p>I want to compute x@A^T@A@x^T@x@A@A^T@x^T for all vectors x, output should be (m,1)</p>
<p>How can I write an <code>einsum</code> query for this given B and A?</p>
<p>Here is a sample without <code>einsum</code>:</p>
<pre><code>import torch
m = 30
n = 4
B = torch.randn(m, n)
A = torch.randn(n, n)
result = torch.zeros(m,1)
for i in range(m):
    x = B[i].unsqueeze(0)
    result[i] = torch.matmul(x,torch.matmul(A.T,torch.matmul(A,torch.matmul(x.T,torch.matmul(x,torch.matmul(A, torch.matmul(A.T, x.T)))))))

</code></pre>
<p>I could write a query for xAx^T but not for xA^TAx^TxBB^Tx^T. Here is for xAx^T:</p>
<pre><code>torch.einsum('bi,ij,bj -&gt; b',B,A,B)
</code></pre>
","2024-08-28 20:25:23","0","Question"
"78924851","78923906","","<p>There's no way to do this without looping. I think what you mean is you want to move the loops to a lower level where they will be vectorized by pytorch/numpy. This does not <em>remove</em> the loop, but runs it more efficiently.</p>
<p>That said, there are still some fundamental issues with how discounting is calculated that makes it hard to vectorize.</p>
<p>If there are <code>T</code> total steps, the discounted reward at step <code>t</code> is given by:</p>
<p><code>discounted[t] = rewards[t] * gamma**0 + rewards[t] * gamma**1 + ... rewards[T] * gamma**(T-t)</code></p>
<p>What this means is we have to discount each reward multiple times. In your example, we need to compute <code>3 * .9</code> for the discounted reward at time 1, as well as <code>3 * .9**2</code> for the discounted reward at time 2.</p>
<p>If you want to do this in a fully vectorized way, you need to compute the full n by n matrix of all rewards discounted by all discount values and sum all the upper triangular diagonals. Technically you can do this in torch in one go by computing the n by n rewards/discounts matrix and convolving over it, but the overhead of those operations kills any performance gains.</p>
<p>After messing around a bit, I've found this to be the best option:</p>
<pre class=""lang-py prettyprint-override""><code>@numba.jit
def get_discounted_rewards(rewards, discount):
    n = rewards.shape[0]
    discounted = np.zeros(n+1)
    for i in np.arange(n-1, -1, -1):
        discounted[i] = rewards[i] + discount * discounted[i+1]
    return discounted[:-1]
</code></pre>
<p>Here we compute each discounted reward iteratively. This avoids the summation operation <code>np.sum(rewards[i:] * discounts[:n-i])</code>. Using <a href=""https://numba.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">numba</a> to jit compile the function brings the loop to a lower level.</p>
","2024-08-28 18:46:05","1","Answer"
"78924394","78923546","","<p>You can do something like this:</p>
<pre class=""lang-py prettyprint-override""><code># lengths of individual files in order
file_lengths = torch.tensor([320, 1036, 458])

# cumulative lengths
cum_lengths = torch.cumsum(file_lengths, 0)
print(cum_lengths)
&gt; tensor([ 320, 1356, 1814])

# values we want to map to files
queries = torch.tensor([300, 320, 500, 1700])

# index values of files
file_idxs = torch.searchsorted(cum_lengths, queries, right=True)
print(file_idxs)
&gt; tensor([0, 1, 1, 2])
</code></pre>
<p>Play around with the values, you can see how <code>searchsorted</code> correctly maps query values to the index of the correct associated file.</p>
<p>Make sure to use <code>right=True</code> to get the behavior you want. If you run the above code with <code>right=False</code>, it will map <code>320</code> to document <code>0</code> instead of document <code>1</code>.</p>
","2024-08-28 16:38:50","1","Answer"
"78923906","","Efficient computation of sum of discounted rewards in RL","<p>This is my current implementation of the sum of discounted rewards that I need for Reinforcement Learning, which given a reward list and discount value should do the following:</p>
<p>rewards: [1, 2, 3]
discount: 0.9</p>
<p>Objective:
First compute a discounts array like so [1, 0.9, 0.9**2]
Then, compute a sum of discounted rewards such that each element is the sum of the element-wise product of the slice rewards[i:] and discounts[:n-i].</p>
<pre><code>def get_discounted_rewards(rewards, discount):
    n = len(rewards)
    sum_of_discounted_rewards = []
    rewards = np.array(rewards)
    discounts = discount ** np.arange(n)
    for i in range(n):
        sum_of_discounted_rewards.append(np.sum(rewards[i:] * discounts[:n-i]))
    return sum_of_discounted_rewards
</code></pre>
<p>How can I do this without the for loop (implicit or explicit)?</p>
","2024-08-28 14:38:29","0","Question"
"78923546","","Accessing specific index out of N indices within multiple files with varying length of tensors","<p>Suppose that I have 20000 files, each has a tensor with a different length. the total length in all files is 3814139. I need to access the file that contains the specified index. So for example, if (<code>tensor length of file_0 is 320, tensor length of file_1 is 1036, tensor length of file_2 is 458, ......., tensor length of file_19000 is 241</code>), to access <code>index &gt;319 and &lt; 1356</code> it's located in <code>file_1</code>.</p>
<p>What is the fastest way to access indices for huge length of indices within multiple files, in such fashion?</p>
","2024-08-28 13:17:37","1","Question"
"78921896","","PyTorch in C++ - how to get part of tensor for all dimensions?","<p>I have the following tensor operation in Python:</p>
<pre><code>A = A[
      :,
      h_offset:h_offset + fineSize,
      w_offset:w_offset + fineSize
    ]
</code></pre>
<p>where <code>*_offset</code> and <code>fineSize</code> are some integers. I am looking for a way to do the same operation in <code>libtorch</code> C++.</p>
<p>According to <a href=""https://stackoverflow.com/questions/56908893/copy-a-chunk-of-one-tensor-into-another-one-in-c-api"">this question</a> and the provided answers I should use <code>Slice()</code>. However, I have a problem wrapping my head around the <code>Tensor::slice()</code> call.</p>
<p>In the Python code, the first part basically translates to <em>all dimensions</em>. In my case I am using an <code>cv::Mat</code> (OpenCV) and <code>A</code> is originally defined as</p>
<pre><code>auto tensor_input = torch::from_blob(img_torch.data, { 1, img_torch.size().height, img_torch.size().width, 1 }, torch::kFloat32);
</code></pre>
<p>with <code>img_torch</code> being the <code>cv::Mat</code> image. Alone here I am confused how to translate the <em>all dimensions</em>  to C++ since the <code>Tensor::slice()</code> function expects <code>dim</code> (dimension) as its first argument:</p>
<pre><code>Tensor::slice(int64_t dim, int64_t start, int64_t end, int64_t step)
</code></pre>
<p>Do I need to slice on a per-dimension basis and then merge the results? E.g.</p>
<pre><code>A_w_slice = A.slice(2, w_offset, w_offset + fineSize, 1);
A_h_slice = A.slice(3, h_offset, h_offset + fineSize, 1);
</code></pre>
<p>I pick <code>2</code> and <code>3</code> since I have <code>1, w, h, 1</code> based on the <code>torch::from_blob()</code> call.</p>
<p>I am not really familiar how such operations work under the hood and if the slicing is just creating a reference to the subset of the original memory used by the tensor we are slicing or a copy is made. If the latter, wouldn't this be very inefficient?</p>
","2024-08-28 06:46:59","0","Question"
"78921159","78921033","","<p>The issue with your code is <code>x[torch.arange(batch_size), goal_y, goal_x, :]</code> produces an output tensor of size <code>(bs, width)</code> when your <code>g</code> assignment expects a tensor of shape <code>(bs, x, y, width)</code>. Since you want the tensor to repeat over the <code>x</code> and <code>y</code> dimensions, you can add unit axes and use the <code>repeat</code> operator.</p>
<p>Your desired output reassigns every value in <code>g</code>, so unless there's a specific reason to use the same object, you can just do indexing/reshaping and avoid the assignment entirely.</p>
<pre class=""lang-py prettyprint-override""><code># randomly chooise dimensions
bs, x_dim, y_dim, w_dim = torch.randint(1, 32, (4,))

# randomly sample x and g
x = torch.randn(bs, x_dim, y_dim, w_dim)
g = torch.randn(bs, x_dim, y_dim, w_dim)

# create goals tensor that can index into the second and third dimensions of x
goals = torch.randint(0, min(x.shape[1:3]), (bs, 2)).long()

# reassign values in g the slow way
for i in range(bs):
    g[i] = x[i, goals[i,1], goals[i,0], :]

# index into x for all bs values
# g2 has shape (bs, width)
g2 = x[torch.arange(bs), goals[:,1], goals[:,0]]

# add unit axes
# g2 has shape (bs, 1, 1, width)
g2 = g2[:,None,None,:]

# repeat values of g2 along new axes
# g2 has shape (bs, x, y, width)
g2 = g2.repeat(1, x_dim, y_dim, 1)

# compare values
print((g == g2).all())
&gt; tensor(True)
</code></pre>
","2024-08-28 00:30:25","1","Answer"
"78921033","","Numpy/PyTorch indexing, assigning values over repeats","<p>I am trying to combine assignment and advanced indexing. I have an array of size (batch, x, y, width) and I am trying to assign it to an array of (batch, width) such that the values are repeated over x, y, but my shapes will not broadcast. A slow version of the code looks like</p>
<pre><code>g = x.clone() # size is batch, x, y, width


for i in batch_size:
   g[i, :, :, :] = x[i, goals[i, 1], goals[i, 0], :] # goals is of size (batch_size, 2)

</code></pre>
<p>I have attempted to make this code much faster by doing the following:</p>
<pre><code>goal_y = goal[:, 1]
goal_x = goal[:, 0]
g[:, :, :, :] = x[torch.arange(batch_size), goal_y, goal_x, :]
</code></pre>
<p>but I get a shape error because we can't broadcast a shape of [batch_size, width] into [batch_size, x, y, width]. The idea here is it would just repeat over the dimension x, y and I can expand x using numpy.repeat but this is kinda gross. Is there a better way to do this? Any help is much appreciated.</p>
","2024-08-27 23:16:39","0","Question"
"78919557","78681145","","<pre><code>pip uninstall numpy 
pip install numpy&lt;2
</code></pre>
","2024-08-27 15:07:55","-2","Answer"
"78915504","78912171","","<p>Huggingface has built in methods to return attention weights</p>
<pre class=""lang-py prettyprint-override""><code>translated_tokens = model.generate(**inputs, 
                                   output_attentions=True,
                                   return_dict_in_generate=True
                                  )

print(translated_tokens.keys())
&gt; odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])
</code></pre>
<p>With <code>return_dict_in_generate=True</code>, <code>model.generate</code> returns a dict-like object. With <code>output_attentions=True</code>, the output dict will contain all attention weights.</p>
<p>For this model, it will include encoder attentions, decoder attentions and cross attentions.</p>
","2024-08-26 16:38:28","3","Answer"
"78913750","78786306","","<p>I had python 3.11 installed on the machine, so I downloaded python 3.9.1 and established my environment, selecting 3.9 and downloading the requirements in that venv which fixed my issue.</p>
","2024-08-26 09:40:06","0","Answer"
"78912171","","How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation","<p>I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.</p>
<p><strong>What I’ve Tried:</strong></p>
<ul>
<li><p><strong>Initial Attempt:</strong> I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.</p>
</li>
<li><p><strong>Using Forward Hooks:</strong> To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation:</p>
</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)
from transformers import MarianMTModel, MarianTokenizer
import torch
import matplotlib.pyplot as plt
from torch.nn import functional as F

model_name = &quot;Helsinki-NLP/opus-mt-en-de&quot;
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)
model.eval()

keys = {}
queries = {}

def get_key(layer):
    def hook(module, input, output):
        key, = input
        keys[layer] = key
    return hook

def get_query(layer):
    def hook(module, input, output):
        query, = input
        queries[layer] = query
    return hook

def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

hooks = []
for i, layer in enumerate(model.model.decoder.layers):
    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))
    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))

input_text = &quot;Please translate this to German.&quot;
inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;)

translated_tokens = model.generate(**inputs, use_cache=False)

translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

input_tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0])
output_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])

attentions = []
for layer in range(len(keys)):
    K, Q = keys[layer], queries[layer]
    M = Q @ K.transpose(-2, -1)
    attentions.append(F.softmax(M, dim=-1))

attentions = torch.stack(attentions, dim=0)

print(&quot;layers, heads, output tokens, input tokens&quot;)
print(attentions.shape)
plt.figure(figsize=(10, 8))
plt.imshow(attentions[0, 0], cmap='viridis')
plt.colorbar()

plt.xticks(range(len(input_tokens)), input_tokens, rotation=90)
plt.yticks(range(len(output_tokens)), output_tokens)

plt.xlabel(&quot;Input Tokens&quot;)
plt.ylabel(&quot;Output Tokens&quot;)
plt.title(&quot;Cross-Attention Matrix&quot;)
plt.show()
</code></pre>
<p>This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.</p>
<p><strong>My Question</strong></p>
<p>Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?</p>
<p>I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.</p>
","2024-08-25 20:13:54","1","Question"
"78911862","78910597","","<p>You get different results because you are running different code.</p>
<p>In your sampling, you first create an ordered list of indices via <code>indices = list(range(len(self.data_source)))</code>, then you shuffle them.</p>
<p>With your list example, you shuffle the same list twice.</p>
<p>In the dataloader, the first shuffle maps <code>[0, 1, 2, ...] -&gt; [6, 8, 9, ...]</code>. The second shuffle maps <code>[0, 1, 2, ...] -&gt; [4, 8, 2, ...]</code>.</p>
<p>In your list example, the first shuffle maps <code>[0, 1, 2, ...] -&gt; [6, 8, 9, ...]</code>. The second shuffle maps <code>[6, 8, 9, ...] -&gt; [5, 1, 9, ...]</code>.</p>
<p>The difference is due to the sampler shuffle always starting with an ordered list, while the list example shuffles a shuffled list a second time.</p>
<p>You can reproduce the sampler results by starting with an ordered list:</p>
<pre class=""lang-py prettyprint-override""><code>random.seed(1)
a = [x for x in range(10)]
random.shuffle(a)
print(a)
a = [x for x in range(10)]
random.shuffle(a)
print(a)

&gt; [6, 8, 9, 7, 5, 3, 0, 4, 1, 2]
&gt; [4, 8, 2, 6, 5, 9, 0, 7, 1, 3]
</code></pre>
<p>Similarly you can get the sampler to mimic the list example by not resetting the index order every epoch</p>
<pre class=""lang-py prettyprint-override""><code>class RandSeqSampler(Sampler):
    def __init__(self, data_source):
        super().__init__(data_source)
        self.data_source = data_source
        self.indices = list(range(len(self.data_source)))

    def __iter__(self):
        random.shuffle(self.indices)
        return iter(self.indices)

    def __len__(self):
        return len(self.data_source)

...

random.seed(1)
dataset = MyDataset()
dataloader = DataLoader.DataLoader(dataset=dataset, batch_size=1, sampler=RandSeqSampler(dataset))
for i, (data, label) in enumerate(dataloader):
    print(data, label)
print(&quot;\n&quot;)
for i, (data, label) in enumerate(dataloader):
    print(data, label)

tensor([6]) tensor([6])
tensor([8]) tensor([8])
tensor([9]) tensor([9])
tensor([7]) tensor([7])
tensor([5]) tensor([5])
tensor([3]) tensor([3])
tensor([0]) tensor([0])
tensor([4]) tensor([4])
tensor([1]) tensor([1])
tensor([2]) tensor([2])


tensor([5]) tensor([5])
tensor([1]) tensor([1])
tensor([9]) tensor([9])
tensor([0]) tensor([0])
tensor([3]) tensor([3])
tensor([2]) tensor([2])
tensor([6]) tensor([6])
tensor([4]) tensor([4])
tensor([8]) tensor([8])
tensor([7]) tensor([7])
</code></pre>
<p>Now that said, if you are relying on random seeds to produce the same results from different code, I can guarantee that will become a source of pain and misery and you probably shouldn't do that.</p>
","2024-08-25 17:39:35","1","Answer"
"78911818","78911068","","<p>This is caused by batchnorm. Batchnorm behaves differently in train mode vs eval mode.</p>
<p>Batchnorm tracks the mean and variance of each batch run through the model and uses those values to compute a running mean and running variance of all batches.</p>
<p>In train mode, batchnorm normalizes with the current batch stats.</p>
<p>In eval mode, batchnorm normalizes with the running mean and running variance.</p>
<p>Your model is based off a pre-trained imagenet model. This means that when the model is in eval mode, the batchnorm layers use statistics they computed from training on imagenet.</p>
<p>When the model is in train mode, the batchnorm layers use in-batch statistics computed on the random input you pass to the model.</p>
<p>The random input has very different mean/var stats compared to imagenet, so you see a large difference.</p>
<p>If you fine-tune this model on whatever dataset you plan to use, then do a train/eval comparison on a real image from that dataset, you will see a smaller deviation between the outputs.</p>
","2024-08-25 17:19:25","1","Answer"
"78911753","78910951","","<p>Unbind returns a view. You can check this by looking at the data pointers of the <code>unbind</code> result. You can also mutate the original tensor and see the <code>unbind</code> result is also mutated</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = torch.unbind(x)

# check data pointers are the same
for i in range(len(y)):
    assert x[i].data_ptr() == y[i].data_ptr()

# mutate data of x
x.data[0] = torch.tensor([10,11,12])

# y[0] is also mutated
assert x[0] == y[0]

print(y[0])
&gt; tensor([10, 11, 12])
</code></pre>
","2024-08-25 16:48:26","1","Answer"
"78911194","78786306","","<p>Just Don't use virtual enviornment bcoz as you know some modules/libraries have conflict among them so if used together they won't work try and run it from your PC and not Virtual enviornment it worked for me it'll work for you too..</p>
","2024-08-25 12:17:57","-1","Answer"
"78911068","","Resnet inconsistency between train and eval mode","<p>I'm trying to implement the Resnet in torch. But I found the output of the forward pass varies greatly between train and eval mode. Since the train and eval mode doesn't affect anything besides batch norm and dropout, I don't know if the results make sense.</p>
<p>Below is my test code:</p>
<pre><code>import torch
from torch import nn
from torchvision import models

class resnet_lstm(torch.nn.Module):
    def __init__(self):
        super(resnet_lstm, self).__init__()
        resnet = models.resnet50(pretrained=True)
        self.share = torch.nn.Sequential()
        self.share.add_module(&quot;conv1&quot;, resnet.conv1)
        self.share.add_module(&quot;bn1&quot;, resnet.bn1)  # Use BatchNorm3d
        self.share.add_module(&quot;relu&quot;, resnet.relu)
        self.share.add_module(&quot;maxpool&quot;, resnet.maxpool)
        self.share.add_module(&quot;layer1&quot;, resnet.layer1)
        self.share.add_module(&quot;layer2&quot;, resnet.layer2)
        self.share.add_module(&quot;layer3&quot;, resnet.layer3)
        self.share.add_module(&quot;layer4&quot;, resnet.layer4)
        self.share.add_module(&quot;avgpool&quot;, resnet.avgpool)
        self.fc = nn.Sequential(nn.Linear(2048, 512),
                                nn.ReLU(),
                                nn.Linear(512, 7))

    def forward(self, x):
        x = x.view(-1, 3, 224, 224)
        x = self.share(x)
        return x
    
model = resnet_lstm()

input_ = torch.randn(1, 3, 224, 224)
model.train()
print(&quot;train mode output&quot;, model(input_))
model.eval()
print(&quot;eval mode output&quot;, model(input_))

</code></pre>
<p>Terminal output:</p>
<pre class=""lang-none prettyprint-override""><code>train mode output tensor([[[[0.3603]],

         [[0.5518]],

         [[0.4599]],

         ...,

         [[0.3381]],

         [[0.4445]],

         [[0.3481]]]], grad_fn=&lt;MeanBackward1&gt;)
eval mode output tensor([[[[0.1582]],

         [[0.1822]],

         [[0.0000]],

         ...,

         [[0.0567]],

         [[0.0054]],

         [[0.3605]]]], grad_fn=&lt;MeanBackward1&gt;)
</code></pre>
<p>As you can see, the output of the two modes are very different from each other. Would this damage the performance?</p>
","2024-08-25 11:07:12","0","Question"
"78910951","","Does `unbind()` return the views of tensors in PyTorch?","<p><a href=""https://pytorch.org/docs/stable/generated/torch.unbind.html"" rel=""nofollow noreferrer"">The doc</a> of <code>unbind()</code> just says below:</p>
<blockquote>
<p>Returns a tuple of all slices along a given dimension, already without it.</p>
</blockquote>
<p>So, does it mean that <code>unbind()</code> returns (a tuple of) the views of tensors instead of (a tuple of) the copies of tensors?</p>
<pre class=""lang-py prettyprint-override""><code>import torch

my_tensor = torch.tensor([[0, 1, 2, 3],
                          [4, 5, 6, 7],
                          [8, 9, 10, 11]])
torch.unbind(input=my_tensor)
# (tensor([0, 1, 2, 3]),
#  tensor([4, 5, 6, 7]),
#  tensor([8, 9, 10, 11]))
</code></pre>
<p>Actually, there are the similar functions <a href=""https://pytorch.org/docs/stable/generated/torch.split.html"" rel=""nofollow noreferrer"">split()</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.vsplit.html"" rel=""nofollow noreferrer"">vsplit()</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.hsplit.html"" rel=""nofollow noreferrer"">hsplit()</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.tensor_split.html"" rel=""nofollow noreferrer"">tensor_split()</a> and <a href=""https://pytorch.org/docs/stable/generated/torch.chunk.html"" rel=""nofollow noreferrer"">chunk()</a>, then their docs say they return the views of tensors while the doc of <code>unbind()</code> just says the slices of tensors.</p>
<p>So again, does <code>unbind()</code> return the views of tensors?</p>
","2024-08-25 10:08:15","2","Question"
"78910830","78910597","","<p>When we use a pseudorandom generator, the seed is updated in the background as a side effect (otherwise the next random instance generated would be the same as the previous).</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code>random.seed(1) # Now the seed is 1
print(random.random()) # Now the seed is no longer 1
</code></pre>
<p>To reproduce results, we would need to reset the seed (and any structures that have been changed) before we repeat the random process.</p>
","2024-08-25 09:01:51","0","Answer"
"78910597","","Setting random seed in Torch dataloader","<p>I'm trying to get the torch dataloader to load the data under a specific sequence determined by the random seed 1. Here's my code:</p>
<pre><code>import random
import torch.utils.data.dataset as Dataset
import torch.utils.data.dataloader as DataLoader
from torch.utils.data.sampler import Sampler


class MyDataset(Dataset.Dataset):
    def __init__(self):
        self.Data = [x for x in range(10)]
        self.Label = [x for x in range(10)]
    def __getitem__(self, index):
        data = self.Data[index]
        label = self.Label[index]
        return data, label
    def __len__(self):
        return len(self.Data)

class RandSeqSampler(Sampler):
    def __init__(self, data_source):
        super().__init__(data_source)
        self.data_source = data_source

    def __iter__(self):
        indices = list(range(len(self.data_source)))
        random.shuffle(indices)
        return iter(indices)

    def __len__(self):
        return len(self.data_source)


random.seed(1)
dataset = MyDataset()
dataloader = DataLoader.DataLoader(dataset=dataset, batch_size=1, sampler=RandSeqSampler(dataset))
for i, (data, label) in enumerate(dataloader):
    print(data, label)
print(&quot;\n\n\n\n\n&quot;)
for i, (data, label) in enumerate(dataloader):
    print(data, label)

random.seed(1)
a = [x for x in range(10)]
random.shuffle(a)
print(a)
random.shuffle(a)
print(a)
</code></pre>
<p>The output is</p>
<pre><code>tensor([6]) tensor([6])
tensor([8]) tensor([8])
tensor([9]) tensor([9])
tensor([7]) tensor([7])
tensor([5]) tensor([5])
tensor([3]) tensor([3])
tensor([0]) tensor([0])
tensor([4]) tensor([4])
tensor([1]) tensor([1])
tensor([2]) tensor([2])






tensor([4]) tensor([4])
tensor([8]) tensor([8])
tensor([2]) tensor([2])
tensor([6]) tensor([6])
tensor([5]) tensor([5])
tensor([9]) tensor([9])
tensor([0]) tensor([0])
tensor([7]) tensor([7])
tensor([1]) tensor([1])
tensor([3]) tensor([3])
[6, 8, 9, 7, 5, 3, 0, 4, 1, 2]
[5, 1, 9, 0, 3, 2, 6, 4, 8, 7]
</code></pre>
<p>You can see that the dataloader load data in the same order as the shuffled order in the first iteration (both 6, 8, 9, 7, 5, 3, 0, 4, 1, 2), but the data loaded the second iteration follows a different order than the shuffled order ([4,8,2,6,5,9,0,7,1,3] and [5, 1, 9, 0, 3, 2, 6, 4, 8, 7]). I would like the data loaded follow the same order as the shuffled order, which means instead of loading [4,8,2,6,5,9,0,7,1,3], I would like to load [5, 1, 9, 0, 3, 2, 6, 4, 8, 7]. Any ideas of how to achieve that? Any help is appreciated</p>
","2024-08-25 06:49:06","1","Question"
"78910115","78909877","","<p>Your view operations are changing the layout of data in the tensor. This is leading to the discrepancy. Take a simple example:</p>
<p>Create a dummy tensor representing the packed QKV values. The tensor has batch size and sequence length of 1, and int values to easily track how values move around.</p>
<pre class=""lang-py prettyprint-override""><code>import torch

d_emb = 1
n_heads = 4

# size (1, 1, d_emb*n_heads*3)
debug_qkv = torch.arange(d_emb*n_heads*3)[None,None,:]

print(debug_qkv.shape)
&gt; torch.Size([1, 1, 12])
print(debug_qkv)
&gt; tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]]])
</code></pre>
<p>Now we compute <code>unsplit_q</code> by splitting <code>debug_qkv</code> along the final axis. We see that <code>unsplit_q</code> has the values <code>[0, 1, 2, 3]</code>. This makes sense. The last dim of <code>debug_qkv</code> has values from 0 to 11. We split this into 3 contiguous chunks. The first chunk, <code>unsplit_q</code>, has values from 0 to 3.</p>
<pre class=""lang-py prettyprint-override""><code>unsplit_q, _, _ = debug_qkv.split(n_heads, -1)

print(unsplit_q.shape)
&gt; tensor([[[0, 1, 2, 3]]])
print(unsplit_q)
&gt; torch.Size([1, 1, 4])
</code></pre>
<p>Now we look at the view operation. In your code, this is the line <code>debug_qkv = debug_qkv.view(batch_size, seq_len, self.n_head, 3 * self.head_size)</code></p>
<p>We can see how the tensor layout has changed. The view operation fills out the <code>n_head</code> dimension first, then the final dimension. The value we are looking for - <code>[0, 1, 2, 3]</code> - has actually been split up.</p>
<pre><code>qkv_reshaped = debug_qkv.view(1, 1, n_heads, d_emb*3)
print(qkv_reshaped.shape)
&gt; torch.Size([1, 1, 4, 3])
print(qkv_reshaped)
&gt; tensor([[[[ 0,  1,  2],
           [ 3,  4,  5],
           [ 6,  7,  8],
           [ 9, 10, 11]]]])
</code></pre>
<p>This propagates forward when we split <code>q</code> from <code>qkv_reshaped</code></p>
<pre class=""lang-py prettyprint-override""><code>q, _, _ = qkv_reshaped.split(d_emb, dim=-1)
print(q)
&gt; tensor([[[[0],
           [3],
           [6],
           [9]]]])
</code></pre>
<p>We can see clearly how the values are mixed up. <code>unsplit_q</code> has contiguous values <code>[0, 1, 2, 3]</code>, while <code>q</code> has the reshaped values <code>[0, 3, 6, 9]</code>.</p>
<p>The solution in this case is to reshape <code>debug_qkv</code> to have the head dimension last</p>
<pre class=""lang-py prettyprint-override""><code>qkv_reshaped2 = debug_qkv.view(1, 1, d_emb*3, n_heads)
print(qkv_reshaped2)

&gt; tensor([[[[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11]]]])

q2, _, _ = qkv_reshaped2.split(d_emb, dim=-2)
print(q2)

&gt; tensor([[[[0, 1, 2, 3]]]])
</code></pre>
<p>If you read the <a href=""https://github.com/pytorch/pytorch/blob/94f92fbd883605e6f8109e8202a7e9614bcf55a0/torch/nn/functional.py#L5868"" rel=""nofollow noreferrer"">source code</a> for pytorch's multiheadattention, you'll find a lot of operations that look like this:</p>
<p><code>q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)</code></p>
<p>This operation reshapes with the head dim last because of this exact issue.</p>
","2024-08-24 22:41:34","1","Answer"
"78910077","78908437","","<p>Yeah , same fix for me - downgrade torchtext to 0.17.2 and Torch to 2.2.1 + downgrade to Python 3.9 ( from 3.10 )</p>
","2024-08-24 22:11:58","1","Answer"
"78909895","78908437","","<p>I've solved the problem for myself. I just needed to downgrade to torch - 2.2.2, torchtext - 0.17.2 and numpy - 1.26.4.</p>
","2024-08-24 20:03:18","4","Answer"
"78909877","","PyTorch Linear operations vary widely after reshaping","<p>Here's an example function that illustrates this problem. It's an attempt to get the Q matrix (for KV caching) of a concatenated QKV matrix (from a transformer large language model). Before the QKV projections are split into heads and then split in to q, k, v projections per head, the <code>manual_q</code> matches <code>unsplit_q</code>. After these operations occur, not only does <code>manual_q</code> and <code>q</code> differ, they differ widely. What's happening here to cause this discrepancy?</p>
<pre class=""lang-py prettyprint-override""><code>    def get_q_matrix(self, x):
        batch_size, seq_len, n_embd = x.size()

        debug_qkv = self.query_key_value(x)  # shape (batch_size, seq_len, n_embd)
        unsplit_q, _, _ = debug_qkv.split(
            self.n_embd, dim=-1
        )  # shape (batch_size, seq_len, n_embd // 3)

        debug_qkv = debug_qkv.view(
            batch_size, seq_len, self.n_head, 3 * self.head_size
        )  # shape (batch_size, seq_len, 4, 96)
        q, _, _ = debug_qkv.split(
            self.head_size, dim=-1
        )  # shape (batch_size, seq_len, 4, 96 // 3)

        # Ensure correct weight and bias extraction
        weight = self.query_key_value.weight
        bias = self.query_key_value.bias

        q_weight, k_weight, v_weight = weight.chunk(3, dim=0)
        q_bias, k_bias, v_bias = bias.chunk(3, dim=0)

        manual_q = F.linear(x, q_weight, q_bias)
        manual_q = manual_q  # shape (batch_size, seq_len, n_embd)

        assert torch.allclose(unsplit_q, manual_q) # Passes
        print(torch.max(torch.abs(unsplit_q - manual_q)))  # tensor(0.)

        manual_q = manual_q.view(batch_size, seq_len, self.n_head, self.head_size)

        print(torch.max(torch.abs(q - manual_q)))  # tensor(35.6218)
        assert torch.allclose(q, manual_q) # AssertionError
        return manual_q
</code></pre>
","2024-08-24 19:52:40","0","Question"
"78908437","","import torchtext - OSError: [WinError 127]","<p>I'm trying to run a Test.py with 'import torchtext' in it.</p>
<p>it throws this error:</p>
<pre><code> Traceback (most recent call last):   File
 &quot;C:\Users\anryu\pythonProject\Test.py&quot;, line 10, in &lt;module&gt;
     import torchtext   File &quot;C:\Users\anryu\pythonProject\.venv\Lib\site-packages\torchtext\__init__.py&quot;,
 line 18, in &lt;module&gt;
     from torchtext import _extension  # noqa: F401
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\Users\anryu\pythonProject\.venv\Lib\site-packages\torchtext\_extension.py&quot;,
 line 64, in &lt;module&gt;
     _init_extension()   File &quot;C:\Users\anryu\pythonProject\.venv\Lib\site-packages\torchtext\_extension.py&quot;,
 line 58, in _init_extension
     _load_lib(&quot;libtorchtext&quot;)   File &quot;C:\Users\anryu\pythonProject\.venv\Lib\site-packages\torchtext\_extension.py&quot;,
 line 50, in _load_lib
     torch.ops.load_library(path)   File &quot;C:\Users\anryu\pythonProject\.venv\Lib\site-packages\torch\_ops.py&quot;,
 line 1295, in load_library
     ctypes.CDLL(path)   File &quot;C:\Users\anryu\AppData\Local\Programs\Python\Python312\Lib\ctypes\__init__.py&quot;,
 line 379, in __init__
     self._handle = _dlopen(self._name, mode)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^ OSError: [WinError 127] The specifiedj procedure could not be found
</code></pre>
<p>I'm running on cpu. I'm using python 3.12, torch 2.4.0 and torchtext 0.18.0, which should be compatible.</p>
<p>I tried installing different python &amp; pytorch versions but it didnt help.
Rerunning just throws the same error. No forums I've found say anything helpful either.</p>
","2024-08-24 07:53:25","2","Question"
"78908318","78905356","","<p>try this way</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;Qwen/Qwen2-0.5B&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;Qwen/Qwen2-0.5B&quot;)

def compute_module_sizes(model):
    &quot;&quot;&quot;
    Compute the size of each submodule of a given model.
    &quot;&quot;&quot;
    from collections import defaultdict
    module_sizes = defaultdict(int)
    for name, tensor in named_module_tensors(model, recurse=True):
      size = tensor.numel() * dtype_byte_size(tensor.dtype)
      name_parts = name.split(&quot;.&quot;)
      for idx in range(len(name_parts) + 1):
        module_sizes[&quot;.&quot;.join(name_parts[:idx])] += size

    return module_sizes

module_sizes = compute_module_sizes(model)
print(f&quot;The model size is {module_sizes[''] * 1e-9} GB&quot;)
</code></pre>
<p>it will be The model size is 3.58674688 GB</p>
<p>if you use</p>
<pre><code>from quanto import quantize, freeze

quantize(model, weights=torch.int8, activations=None)
freeze(model)
</code></pre>
<p>it will be The model size is 2.651227464 GB</p>
","2024-08-24 06:42:07","0","Answer"
"78907866","78906991","","<p>I think you are confused about some very basic programming aspects here. Lets step through some things.</p>
<p>You create the model:</p>
<pre class=""lang-py prettyprint-override""><code>top_model = torch_top_model()
</code></pre>
<p>At this point you have one single model. Now you create this dict:</p>
<pre class=""lang-py prettyprint-override""><code>weights_and_biases = {i:top_model.state_dict() for i in range(1,5)}
</code></pre>
<p>At this point you still have one model. The entries in <code>weights_and_biases</code> all reference the same model state dict. The dict values are all pointers to the exact same object.</p>
<p>You create the optimizer dict</p>
<pre class=""lang-py prettyprint-override""><code>optimizer = {i:torch.optim.Adam(top_model.parameters(), lr=0.01) for i in range(1,5)}
</code></pre>
<p>At this point you still have one model. Each dict value is a different optimizer object, but they all reference the same weight objects.</p>
<p>You create the model copy dict</p>
<pre class=""lang-py prettyprint-override""><code>top_models = {i:copy.deepcopy(top_model) for i in range(1,5)}
</code></pre>
<p>Now you have created four new models. You have five models total - four in <code>top_models</code> plus the original model you cloned from.</p>
<p>Now your train loop</p>
<pre class=""lang-py prettyprint-override""><code>top_models[key].load_state_dict(weights_and_biases[key])
outputs = top_models[key](inputs[key])
loss = criterion(outputs, model_labels)
optimizer[key].zero_grad()
loss.backward()
</code></pre>
<p>If you understand the code you wrote, you can see clearly why this doesn't work. You use the model in <code>top_models[key]</code> with the optimizer <code>optimizer[key]</code>. <code>top_models[key]</code> points to one of the model copies you created. <code>optimizer[key]</code> points to the weights of the original model. They are referencing different objects. Loading the state dict via <code>top_models[key].load_state_dict(weights_and_biases[key])</code> does not change this because it copies the data values from <code>weights_and_biases[key]</code> into the tensor objects in <code>top_models[key].load_state_dict</code>.</p>
<p>The reason you see no change in the weights is because you aren't updating them. You compute your loss with the weights in <code>top_models[key]</code>, but your optimizer references the weights in the original <code>top_model</code>. There is no connection between the two. If you actually inspect a gradient value of a parameter in the optimizer (<code>optimizer[key].param_groups[0]['params'][0].grad</code>) you will find it is <code>None</code> because no gradient is being computed. The parameters in the optimizer are unrelated to the parameters you use to calculate your loss.</p>
<p>I think you are also confused about what you are trying to accomplish. When you say <code>I would like the model to update only weights_and_biases[1] for inputs[1], weights_and_biases[2] for inputs[2]</code>, it implies you want to train separate models on the different inputs. When you say <code>If this can be achieved without deep copying the model</code>, it implies you want a single model.</p>
<p>You can train a single model on all versions of the input, or you can train separate instances of the model on the separate inputs.</p>
","2024-08-24 00:21:04","1","Answer"
"78906991","","train an NN model independently for different input versions","<p>I am working with PyTorch to train a neural network model, referred to as 'top_model.' Below, I have outlined my setup, followed by a detailed description of the challenge I am facing.</p>
<pre><code>class torch_top_model(nn.Module):
    def __init__(self):
        super(torch_top_model, self).__init__()
        self.input_layer = nn.Linear(25, 320)
        self.hidden_layer = nn.Linear(320, 64)
        self.output_layer = nn.Linear(64, 10)

    def forward(self, x):
        x = F.relu(self.input_layer(x))
        x = F.relu(self.hidden_layer(x))
        x = self.output_layer(x)
        return x

top_model = torch_top_model()
top_model.train()
</code></pre>
<p>I have an input tensor of size (30, 25). For my input generation, I am splitting the 25 features into 5 groups of 5 features each.</p>
<pre><code>model_inputs = torch.rand(30, 25)
model_labels = torch.randint(0, 10, (30,))
top_model_inputs = {}

for i in range(5):  # 4 groups (5 columns each)
    top_model_inputs[i] = model_inputs[:, i*5:(i+1)*5]
</code></pre>
<p>In the next step, I am generating versions of the input data where one group of features (corresponding to each key) is zeroed out while the others remain intact.</p>
<pre><code>for key in range(1, len(top_model_inputs.keys())):
    temp = []
    for k in top_model_inputs:
      if k == key:
        temp.append(torch.zeros_like(top_model_inputs[k]))
      else:
        temp.append(top_model_inputs[k])
    temp_tensor = torch.cat(temp, dim=1)
    inputs[key] = temp_tensor
</code></pre>
<p>Next I start with the model training:</p>
<pre><code>weights_and_biases = {i:top_model.state_dict() for i in range(1,5)}
optimizer = {i:torch.optim.Adam(top_model.parameters(), lr=0.01) for i in range(1,5)}
criterion = nn.CrossEntropyLoss()
top_models = {i:copy.deepcopy(top_model) for i in range(1,5)}
epochs = 3

for i in range(epochs):
  for key in range(1, len(top_model_inputs.keys())):
    top_models[key].load_state_dict(weights_and_biases[key])
    outputs = top_models[key](inputs[key])
    loss = criterion(outputs, model_labels)
    optimizer[key].zero_grad()
    loss.backward()
    if i == 2:
      w2 = weights_and_biases[2]['input_layer.weight'].clone().detach()
    optimizer[key].step()
    weights_and_biases[key] = top_models[key].state_dict()
    if i == 1:
      w1 = weights_and_biases[1]['input_layer.weight'].clone().detach()

      print(&quot;w1-w2: &quot;, torch.norm(w1-w2))
</code></pre>
<p>My problem: For each versions of the input, as and when I run my algorithm through epochs, I only want the weights and biases of the corresponding inputs to be updated. For example in epoch1, I would like the model to update only weights_and_biases[1] for inputs[1], weights_and_biases[2] for inputs[2] and so on. In essence, I want the parameters (weights and biases) of the top_model to be updated separately for each input versions during each epoch, and then use those updated parameters in the next epoch.</p>
<p>Issue I am facing: The weights are being reused from weight_and_biases of other input versions. For example, weight_and_biases[1] is being used by inputs[2]. Therefore, I get a zero norm difference before and after updates from different input versions.</p>
<p>How to independently update the model parameters for different input versions across epochs? If this can be achieved without deep copying the model a number of times, that would be a more efficient solution.</p>
","2024-08-23 17:30:42","0","Question"
"78906920","78904049","","<p>You need to set the models to <code>eval</code> mode to disable dropout if you want them to produce the same results</p>
<pre class=""lang-py prettyprint-override""><code>import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;0&quot;
import torch
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, GPT2LMHeadModel

model = AutoModelForCausalLM.from_pretrained(
    &quot;gpt2&quot;,
    device_map='auto',
)

model2 = GPT2LMHeadModel(model.config).to(&quot;cuda&quot;)
model2.load_state_dict(model.state_dict())

# set to eval
model.eval()
model2.eval()

tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)

t = tokenizer(&quot;hello_world&quot;, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;].to(&quot;cuda&quot;)
a = model(t).logits
b = model2(t).logits

assert (a == b).all()
</code></pre>
","2024-08-23 17:07:51","0","Answer"
"78906762","","PyTorch TensorBoard SummaryWriter giving empty metrics","<p>I've been setting up tensorboard with PyTorch. Originally I was using lightning, and ran into an issue where I would log my hyperparameter, but the metrics in the HPARAMS tab would be empty. So, to pin point the issue I moved over to the simplest possible example and the issue is still happening</p>
<pre><code>from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter(log_dir=&quot;tb_logs/test&quot;)
writer.add_hparams({&quot;a&quot;: 0, &quot;b&quot;: 0}, {&quot;hparam/test_accuracy&quot;: 0.5})
writer.add_hparams({&quot;a&quot;: 0, &quot;b&quot;: 1}, {&quot;hparam/test_accuracy&quot;: 0.6})
writer.add_hparams({&quot;a&quot;: 1, &quot;b&quot;: 0}, {&quot;hparam/test_accuracy&quot;: 0.61})
writer.add_hparams({&quot;a&quot;: 1, &quot;b&quot;: 1}, {&quot;hparam/test_accuracy&quot;: 0.4})
writer.add_hparams({&quot;a&quot;: 2.0, &quot;b&quot;: 1.5}, {&quot;hparam/test_accuracy&quot;: 0.7})
</code></pre>
<p>Per the PyTorch docs I'd expect to this exact table in the HPARAMS tab, but when I go I am met with this instead (<a href=""https://imgur.com/a/zb1yTPA"" rel=""nofollow noreferrer"">https://imgur.com/a/zb1yTPA</a>)</p>
<p>I am using</p>
<pre class=""lang-bash prettyprint-override""><code>lightning               2.4.0
torch                   2.4.0
tensorboard             2.17.0
</code></pre>
","2024-08-23 16:20:47","0","Question"
"78906362","78871358","","<p>There is no one silver bullet here or clean code approach we should follow.</p>
<p>It depends on your specific requirements. But Let's go over each scenario you just described.</p>
<p>I. S3 Simple R/W with Model Handler:</p>
<p>The advantages that I see :Simplicity in regards of workflow and everything is
centralized : No need to manage aditional services. Everything is the Handler. This seems like a quick way to build a POC.</p>
<p>Disadvantages:
Scalability for high volume of images.
Tight Coupling maybe part of the upload needs to be captured by some other component in the future.</p>
<p>Code example :</p>
<pre><code>class ImageClassifierHandler(BaseHandler):
    def initialize(self, context):
        self.s3_client = boto3.client('s3')
        self.model = self.load_model()
        self.initialized = True

def load_image_from_s3(self, bucket_name, object_key):
    response = self.s3_client.get_object(Bucket=bucket_name, Key=object_key)
    image_data = response['Body'].read()
    image = Image.open(io.BytesIO(image_data)).convert('RGB')
    return image

def handle(self, data, context):
    bucket_name = data[0].get('bucket_name')
    object_key = data[0].get('object_key')

    image = self.load_image_from_s3(bucket_name, object_key)
    input_tensor = self.preprocess_input(image)

    with torch.no_grad():
        output = self.model(input_tensor)
    
    _, predicted = torch.max(output, 1)
    prediction = predicted.item()

    # Optionally upload the result back to S3
    result_key = f&quot;results/{object_key.split('/')[-1].replace('.jpg', '_result.txt')}&quot;
    self.s3_client.put_object(Bucket=bucket_name, Key=result_key, Body=str(prediction))

    return [{&quot;prediction&quot;: prediction, &quot;result_s3_path&quot;: f&quot;s3://{bucket_name}/{result_key}&quot;}]
</code></pre>
<p>II. Using a Seperate Worker to handle S3 Operations.</p>
<p>This looks more elegant right but you introduce complexity you have to maintain. The exact advantages and disadavantages from the previous method are here in reverse.</p>
<p>This means the advantages of this method is scalability and if done well modularity to reuse this r/w to your s3 by other projects/models.
The disadvantages are obviusly : complexity, a new service to maintain a new potential failure point.</p>
<p>Are you in an enterprise environment where you might reuse this later and not just a proof of concept/ learning/academic environment then go with this option.</p>
<p>You need a torchserve endpoint for the image classifier let's say</p>
<pre><code>torchserve_endpoint = &quot;http://localhost:8080/predictions/image-classifier&quot;
</code></pre>
<p>Then you can do something like this</p>
<pre><code>def fetch_image_from_s3(bucket_name, object_key):
response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
image_data = response['Body'].read()
return Image.open(io.BytesIO(image_data)).convert('RGB')

def send_image_for_inference(image):
buffered = io.BytesIO()
image.save(buffered, format=&quot;JPEG&quot;)
buffered.seek(0)

files = {'data': buffered}
response = requests.post(torchserve_endpoint, files=files)
return response.json()

def upload_result_to_s3(bucket_name, object_key, result):
result_key = f&quot;results/{object_key.split('/')[-1].replace('.jpg', '_result.txt')}&quot;
s3_client.put_object(Bucket=bucket_name, Key=result_key, Body=str(result))
return f&quot;s3://{bucket_name}/{result_key}&quot;
</code></pre>
<p>Now of course the second one looks approachable but believe me when I say whenever you add another layer of complexity unnecessarily it will bite you back in the ass. The second option is not as simple when you consider managing the service having a separate inference and having multiple aws resources.</p>
<p>If your business does not require this kind of arhitecture just go with the first option.</p>
","2024-08-23 14:22:23","1","Answer"
"78905685","78905671","","<p>I think that you somewhere did not delete lists, numpy arrays. Because I have similar problem, and it was caused by usual list that I forgot to make empty after each iteration</p>
","2024-08-23 11:32:26","-1","Answer"
"78905671","","ML Model training - System Memory usage increasing over epoch","<p>I am implementing a simple MLP using sagemaker particularly an aws ml.g4.xlarge machine and I noticed that ram memory keeps increasing over epoch. I am using pytorch.lightning for ML model architecture. I am using a dataloader with 64 workers.</p>
<p><a href=""https://i.sstatic.net/5weE7oHO.png"" rel=""nofollow noreferrer"">ML Flow - System Metrics</a></p>
<p>What happens is that the model when reaches 70% of training crashes the machine saying it memory max limit is reached. As you can see between batches the memory is increasing - my understanding is that between batches each batch is used and then removed out of memory so that only thing that changes between batches is the weight updates. So the memory usage should not increase over steps.</p>
<p>I tried removing logs, nothing changed. Though that might be the number of workers, added just one worker and the memory usage kept increasing over time. However with num_workers=0 it stabilizes and does not increase over time - but the training takes significant amount of time</p>
<p><a href=""https://i.sstatic.net/O9CwGDv1.png"" rel=""nofollow noreferrer"">ML Flow - System Metrics - num_workers=0</a></p>
<p>How can I fix this issue?</p>
","2024-08-23 11:28:19","-2","Question"
"78905356","","How to properly quantize model with quanto?","<p>I am trying to quantize Qwen model, but it seems to be not working.</p>
<p>This is my snippet in Google Colab (runtime-&gt; free T4 GPU):</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM
from optimum.quanto import QuantizedModelForCausalLM, qint8

model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2-0.5B-Instruct')
qmodel = QuantizedModelForCausalLM.quantize(model, weights=qint8)

foot = model.get_memory_footprint()
print(f'{foot / 1_000_000_000} GB')

qfoot = qmodel.get_memory_footprint()
print(f'{qfoot / 1_000_000_000} GB')
</code></pre>
<p>Output:</p>
<pre><code>2.923327304 GB
2.923327304 GB
</code></pre>
<p>Despite same memory footprint, when I called <code>model.named_parameters()</code>, it showed float32 types for both model and qmodel. I am not getting any errors.</p>
","2024-08-23 10:09:05","1","Question"
"78904049","","load_state_dict getting random results","<pre><code>import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;0&quot;
import torch
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, GPT2LMHeadModel
model = AutoModelForCausalLM.from_pretrained(
    &quot;gpt2&quot;,
    device_map='auto',
)
model2 = GPT2LMHeadModel(model.config).to(&quot;cuda&quot;)
model2.load_state_dict(model.state_dict())
tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)

t = tokenizer(&quot;hello_world&quot;, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;].to(&quot;cuda&quot;)
a = model(t).logits
b = model2(t).logits
print(a - b)
print(a)
print(b)
</code></pre>
<p>model2 behaves very differently from the model (loss being much higher), but the model structures and parameters are exactly the same. From the output, it looks like something is randomized for model2. Could anyone tell what was going on? I have the &quot;accelerate&quot; package installed.</p>
<p>The config and the parameters are the same. I also checked the forward functions, and there is no difference at all. However, setting model2.transformer.forward = model.transformer.forward, and then the two models would behave the same.</p>
","2024-08-23 01:34:14","1","Question"
"78904019","78903746","","<p>You can't perform an in-place operation on a variable that requires gradient calculation. This is because gradients are assigned to variables, so mutating a variable in-place breaks the computational graph.</p>
<p>In this case, the variable in question is <code>result</code> and the in-place operation is <code>result[i] = param * result[i-1]</code>.</p>
<p>You need to compute the result using without using in-place operations. For the example you gave, you can do the following:</p>
<pre class=""lang-py prettyprint-override""><code>def function(input_tensor, param, steps):
    pow_indices = torch.arange(steps).view(-1, 1)
    powers = torch.pow(param, pow_indices)
    result = input_tensor.unsqueeze(0) * powers
    return result

input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
param = torch.tensor(2.0, requires_grad=True)
output = function(input_tensor, param, 3)
loss = output.sum()
loss.backward()
</code></pre>
","2024-08-23 01:20:06","2","Answer"
"78903746","","In-place operation in a for loop using pytorch","<p>I'm trying to train a model to compute a parameter (&quot;param&quot;). I'm expecting a value of this param for each timestep (range(1, 3) in the code below).</p>
<p>But I'm getting the error:</p>
<pre><code>RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.
</code></pre>
<p>Here a simplified version of my function:</p>
<pre><code>import torch

def function(input_tensor, param):
    result = torch.zeros((3, 3), requires_grad=True)
    result[0] = input_tensor
    
    for i in range(1, 3):
        result[i] = param * result[i-1]  
    
    return result


input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
param = torch.tensor(2.0, requires_grad=True)


output = function(input_tensor, param)


loss = output.sum()
loss.backward()
</code></pre>
","2024-08-22 22:34:39","0","Question"
"78903614","78903492","","<p>All tensors passed to the optimizer need to have <code>requires_grad=True</code></p>
<pre class=""lang-py prettyprint-override""><code>w = torch.rand(3, 4, requires_grad=True)
BH = torch.rand(3, requires_grad=True)
WH = torch.rand(3, requires_grad=True)
BO = torch.rand(1, requires_grad=True)

optimizer = torch.optim.SGD([w,BH, WH, BO], lr = 0.1)
</code></pre>
","2024-08-22 21:43:50","1","Answer"
"78903518","78896873","","<p>This solution works, “solved by downloading libomp140.x86_64.dll and adding it to folder with fbgemm.dll”, and the file end with <code>.dll</code> can be download from here, <a href=""https://www.dllme.com/dll/files/libomp140_x86_64/00637fe34a6043031c9ae4c6cf0a891d"" rel=""nofollow noreferrer"">https://www.dllme.com/dll/files/libomp140_x86_64/00637fe34a6043031c9ae4c6cf0a891d</a></p>
","2024-08-22 21:04:13","0","Answer"
"78903496","78899996","","<p>All frameworks can use tensor cores assuming 1. your GPU has tensor cores and 2. your model can actually take advantage of tensor cores (using mixed precision, all matmul sizes are a multiple of 8, etc).</p>
<p>For pytorch, you can read more <a href=""https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html"" rel=""nofollow noreferrer"">here</a></p>
","2024-08-22 20:58:20","1","Answer"
"78903492","","PyTorch. Optimizer doesn't work and ""RuntimeError: element 0 of tensors...""","<p>I get error <code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</code>.</p>
<p>If I use something like <code>loss_fin.requires_grad = True</code>, code runs but optimizer doesn't work.</p>
<pre><code>def loss(x1):
 module = torch.abs(x1 - 1.0)
 return module.mean()

w=torch.rand(3,4)
BH=torch.rand(3)
WH=torch.rand(3)
BO=torch.rand(1)

optimizer = torch.optim.SGD([w,BH, WH, BO], lr = 0.1)
for n in range(100):
  optimizer.zero_grad()
  input_for_hidden = torch.matmul(w,x)
  inactivated_at_hidden_layer = input_for_hidden+BH
  output_hidden_layer = torch.sigmoid(inactivated_at_hidden_layer)
  input_for_out_layer = torch.matmul(output_hidden_layer,WH)
  inactivated_output = input_for_out_layer+BO
  output_for_out_layer = torch.sigmoid(inactivated_output)

  loss_fin = loss(output_for_out_layer)
  loss_fin.backward()
  optimizer.step()
</code></pre>
<p>I'm a newbie. I would be grateful if someone could explain in simple terms why it doesn't work</p>
","2024-08-22 20:57:45","1","Question"
"78903454","78902301","","<p>You have to consider the effect of the causal attention mask.</p>
<p>CLMs like GPT-2 use an attention mask to prevent tokens from attending to tokens that come later in the sequence. This is important because if this wasn't the case, the model could &quot;look ahead&quot; and cheat at the next token prediction task. The attention mask restricts the model such that token <code>i</code> can only attend to tokens <code>j &lt;= i</code>.</p>
<p>Say we have tokens <code>[0, 1, 2, 3, 4]</code>. Token <code>0</code> attends to itself. Token <code>1</code> attends to <code>[0, 1]</code> and so on.</p>
<p>Now consider your permutations.</p>
<p>If we permute the positional embeddings, we give the model slightly different signal, but the token order and attention order is still the same. Token <code>0</code> still attends to token <code>0</code>. Token <code>1</code> still attends to <code>[0, 1]</code>, and so on. As a result, the output is mostly similar to the base case.</p>
<p>Now we permute the token order, say to <code>[3, 2, 0, 4, 1]</code>. Token <code>3</code>, which used to attend to <code>[0, 1, 2, 3]</code>, can now only attend to itself. Token <code>2</code>, which used to attend to <code>[0, 1, 2]</code> can now only attend to <code>[3, 2]</code>. Token permutation substantially changes what information is routed to what tokens, resulting in a substantial difference in model output.</p>
<p>If you want to look at the effect of token order and positional embeddings in isolation, you should use a BERT-style masked language model that does not use a causal attention mask.</p>
","2024-08-22 20:43:32","2","Answer"
"78902582","78896873","","<p>solved by downloading libomp140.x86_64.dll and adding it to folder with fbgemm.dll</p>
","2024-08-22 16:24:24","0","Answer"
"78902301","","Why doesn't permuting positional encodings in GPT-2 affect the output as expected?","<p>I'm trying to understand the role of positional encoding in the GPT-2 Transformer model. From what I understand, positional encodings are crucial because they give the model a sense of the order of tokens.</p>
<p>However, I'm confused about the behavior I'm observing:</p>
<ol>
<li><p><strong>Permuting Positional Encodings:</strong> When I permute the positional encodings while keeping the input tokens the same, the generated output barely changes. I expected significant changes since the positional information should alter the model’s understanding of token order.</p>
</li>
<li><p><strong>Permuting Input Tokens:</strong> When I permute the input tokens (while permuting positional encodings in the same manner), the output changes significantly, but it doesn't revert to what it was with the original order.</p>
</li>
</ol>
<p>This behavior is confusing because I expected the output to revert when both the positional encodings and tokens are permuted in the same way.</p>
<p>Could someone help clarify why this is happening? Is there something about how GPT-2 handles positional encoding that I'm missing? How can I modify my code to get the behavior I expect, where permuting both the positional encoding and input tokens in the same way results in the original output?</p>
<p>Thanks in advance!</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def permute_columns(matrix, permutation=None):
    n = len(permutation)
    first_n_columns = matrix[:, :n]
    permuted_columns = first_n_columns[:, permutation]
    remaining_columns = matrix[:, n:]
    new_matrix = torch.hstack((permuted_columns, remaining_columns))
    return new_matrix

model_name = &quot;gpt2&quot;
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

permutation = [0, 4, 2, 3, 1]
# permute positional encoding
model.transformer.wpe.weight.data = permute_columns(model.transformer.wpe.weight.data.T, permutation).T

input_text = &quot;The man ate the cow&quot;
input_ids = tokenizer(input_text, return_tensors=&quot;pt&quot;).input_ids

# permute input
input_ids = permute_columns(input_ids, permutation)

outputs = model.generate(input_ids, max_length=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
</code></pre>
<p>I attempted to permute the positional encodings in a GPT-2 model, expecting this to change the generated output. Additionally, I tried permuting the input tokens along with the positional encodings in the same manner, anticipating that the output would revert to the original.</p>
<p>What I Expected:</p>
<ul>
<li>When permuting only the positional encodings, I expected the output to change significantly because the model should interpret the token order differently.</li>
<li>When permuting both the input tokens and positional encodings in the same way, I expected the output to revert to what it was with the original order.</li>
</ul>
<p>What Actually Happened:</p>
<ul>
<li><p>Permuting only the positional encodings resulted in minimal changes to the generated output, which was unexpected.</p>
</li>
<li><p>Permuting both the input tokens and positional encodings in the same manner led to a different output, but it did not revert to the original as I anticipated.</p>
</li>
</ul>
","2024-08-22 15:11:14","2","Question"
"78901546","78484297","","<p>Install through conda</p>
<blockquote>
<p>torch torchvision torchaudio</p>
</blockquote>
<p>Check this  <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a></p>
<p>Install it using conda according to your operating system</p>
<pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
conda install pytorch torchvision torchaudio cpuonly -c pytorch
</code></pre>
","2024-08-22 12:22:55","1","Answer"
"78899996","","Tensor Cores on NVIDIA GPU for CNN Model Inference","<p>I was looking to leverage the tensor cores on my GPU for executing some CNN model inferences on it. Do frameworks like Pytorch or Tensorflow or MXNet or any of the frameworks for that matter, support inferencing on the tensor cores?</p>
<p>I've heard that tensor cores can be used for training purpose as Pytorch has an in-built support for it. Not sure if the same can be done for inferencing.</p>
","2024-08-22 06:06:15","0","Question"
"78899566","","Issue loading FluxPipeline components","<pre><code>import torch
from diffusers import FluxPipeline

pipe = FluxPipeline.from_pretrained('C:\\Python\\Projects\\test1\\flux1dev', torch_dtype=torch.bfloat16)
pipe.enable_sequential_cpu_offload()

prompt = &quot;beach ball&quot;
image = pipe(
    prompt,
    height = 1024,
    width = 1024,
    guidance_scale = 3.5,
    num_inference_steps = 50,
    max_sequence_length = 512,
    generator = torch.Generator(&quot;cpu&quot;).manual_seed(0)
).images[0]
image.save(&quot;beach ball.png&quot;)
</code></pre>
<p>I ran into an issue running this simple test of Flux.1</p>
<p>Every time I tried to run the code it would simply stop after loading part of the pipline components.</p>
<p>No exception or error code was thrown and no output was given the program just stopped.</p>
<p>I really have no idea what I'm doing and I was kind of just messing around with Flux to see what it could do.</p>
<p><a href=""https://i.sstatic.net/pBGhHJXf.png"" rel=""nofollow noreferrer"">Theres also this really weird issue in the terminal where the last thing being loaded gets pushed into the next line of the terminal</a></p>
","2024-08-22 02:16:03","3","Question"
"78898389","78894720","","<p>You can do this simply with a broadcasted multiply, in both torch and numpy.</p>
<pre><code>result_c = (mask[:, None, :, None] * pattern[None, :, None, :]).reshape(20, 20)
</code></pre>
","2024-08-21 17:46:43","1","Answer"
"78898357","78356949","","<p>I encountered the same Error several times.</p>
<p>In my first case, the issue was caused by the <strong>sizes mismatch</strong> between the tokenizer length and model input. I solved with:</p>
<pre><code>model.resize_token_embeddings(len(tokenizer))
</code></pre>
<p>the mismatch was induced when adding <code>pad</code> token:</p>
<pre><code>tokenizer.add_special_tokens({'pad_token': '&lt;pad&gt;'})
</code></pre>
<p>In my second case, the raw labels of imagery segmentation dataset are <strong>messed with outlier/overrange label</strong>. The Error showed in the loss.backward() line.  I solved with ignore the overrange label:</p>
<pre><code>criterion = CrossEntropyLoss2d(ignore_index=255).cuda() 
</code></pre>
<p>You may run the <code>CUDA_LAUNCH_BLOCKING=1 python filename.py args</code> or add the <code>os.environ['CUDA_LAUNCH_BLOCKING'] = &quot;1&quot;</code> to the file head to check the detailed error information. For example, my detailed error information is:</p>
<pre><code>C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\cuda\NLLLoss2d.cu:93: block: [1,0,0], thread: [183,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
</code></pre>
","2024-08-21 17:38:19","0","Answer"
"78896873","","fbgemm.dll module could not be found when using Pytorch/transformers","<p>I currently am trying to use the BERT language model for invoice creation. However, i am receiving the error:
OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\torch\lib\fbgemm.dll&quot; or one of its dependencies.</p>
<p>I have tried installing Microsoft Visual C++ Redistributable to fix this issue, but issue was not fixed. I am pretty stuck here after this.</p>
","2024-08-21 12:11:21","-1","Question"
"78895504","78895109","","<p>Yes, it is possible but it is not a standard approach and may require some manual intervention. The typical way PyTorch handles gradients is by computing them all at once with loss.backward(). You can achieve what you're asking for by manually controlling the computation graph and gradients.</p>
<pre><code>first a Forward Pass then a Layer-by-Layer Backward Pass
</code></pre>
","2024-08-21 06:48:17","-5","Answer"
"78895109","","Is it possible to run autograd backward one node at a time?","<p>Let say I have a complex model with many many layers.</p>
<p>When I obtain the output of the model I calculate the loss.</p>
<p>Now when I run loss.backward() it would calculate gradients for all layers at once.</p>
<p>But is it possible to run backward() one layer at a time?</p>
<p>So what I'm trying to do is to obtain gradients for layer 1 first, pass them to optimizer, and then immediately set grads to None in order to free the memory. Then move on to calculate gradients for the layer 2, and so on until it reaches the final layer using a loop. Is this possible?</p>
<p>Edit: added a toy example to demonstrate the problem. for complexity I also used an architecture that somewhat resembles unet, ie output of layer 1 may be used in layer 2 but also in layer 3</p>
<pre><code>import torch
from torch.optim.optimizer import Optimizer, _use_grad_for_differentiable

class NeuralNetwork(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = torch.nn.Flatten()
        self.layer1 = torch.nn.Linear(5, 5)
        self.layer2 = torch.nn.Linear(5, 5)
        self.layer3 = torch.nn.Linear(5, 5)
    
    def forward(self, x):
        x = self.flatten(x)
        layer1_out = self.layer1(x)
        layer2_out = self.layer2(layer1_out)
        return self.layer3(layer2_out + layer1_out)

class OptimS(Optimizer):
    def __init__(self, params=None, lr=1e-3):
        super().__init__(params, dict(lr=lr,differentiable=False,))
    
    @_use_grad_for_differentiable
    def step(self, closure=None):
        for group in self.param_groups:
            for param in group[&quot;params&quot;]:
                param.add_(param.grad, alpha=-group['lr'])

model = NeuralNetwork()
batch, labels= torch.randn((5,5)), torch.randn((5,5))
criterion = torch.nn.MSELoss()

paramteres =model.parameters()
optimizer = OptimS(paramteres)
out = model(batch)

loss = criterion(out,labels) /2
loss.backward()

optimizer.step()
optimizer.zero_grad(set_to_none=True)

# you realize optimizer step is doing a just doing a loop,
# so we could also run it per layer, something like:

out = model(batch)
loss = criterion(out,labels) /2
loss.backward()

def layerwise_optimizer_step(param, lr=1e-3):
    param.add_(param.grad, alpha=-lr)
    param.grad = None

for param in paramteres:
    layerwise_optimizer_step(param, 1e-3)

# so the goal is to create some custom backward function,
# that upon calculating the gradinet for each layer would 
# pass the corresponding parameter to our layerwise optimizer
</code></pre>
","2024-08-21 04:09:45","8","Question"
"78895047","78894720","","<p>You can use slices to fill the matrix by mask. Then use 2d convolution to solve your problem.</p>
<pre><code>import numpy as np
from scipy.signal import convolve2d
mask=np.random.rand(5,5)&gt;.7
pattern=np.random.rand(4,4)
result=np.zeros((20,20))
result[1::4,1::4]=mask
result=convolve2d(result, pattern, mode='same')
</code></pre>
","2024-08-21 03:35:03","0","Answer"
"78894720","","Tile according to a pattern in pytorch (or numpy)?","<p>I have a 2-d tensor pattern I'd like to repeat/tile with a particular sparsity. I suspect there is a function (or two-line approach using gather, fold/unfold, or similar) to do this in Pytorch.</p>
<p>These two approaches each get the result I want but are inefficient and/or confusing:</p>
<pre><code># inputs
mask = torch.rand(5, 5) &gt; .7
pattern = torch.rand(4, 4)

# approach a (brute force):
result_a = torch.zeros(20, 20)
for i in range(5):
  for j in range(5):
    if mask[i][j]:
       result_a[i*4: (i+1)*4, j*4: (j+1)*4] = pattern

# approach b (interpolate the binary mask)
tiled_pattern = torch.tile(pattern, (5, 5))
bigger_mask = torch.nn.functional.interpolate(mask.view(1, 1, *mask.shape).to(torch.float16), scale_factor=(4, 4), mode='area').squeeze((0, 1))
result_b = torch.mul(tiled_pattern, bigger_mask)
</code></pre>
<p>In particular, I don't like approach (b) because the <code>interpolate</code> function needs me to cast the input, add and remove two dimensions, and I don't trust the 'area' mode to always work--it's not even really defined in the Pytorch documentation.</p>
<p>Is there a cleaner way to do this? If not in Pytorch, than in Numpy (or another common Python package?)</p>
","2024-08-20 23:54:10","1","Question"
"78893223","78893184","","<p>If you are working on a CPU, activate the conda environment and run:</p>
<pre><code>pip3 install torch torchvision torchaudio
</code></pre>
<p>If you are running on GPU:</p>
<pre><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
</code></pre>
","2024-08-20 15:16:15","0","Answer"
"78893184","","Cannot import torch","<p>I install torch using <code>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia </code> and when i try to import it i see error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;Anacondat Path\envs\EnvName\Lib\site-packages\torch\__init__.py&quot;, line 137, in &lt;module&gt;
    raise err
OSError: [WinError 127] The specified procedure could not be found. Error loading &quot;Anacondat Path\envs\EnvName\Lib\site-packages\torch\lib\c10_cuda.dll&quot; or one of its dependencies.
</code></pre>
","2024-08-20 15:05:37","-1","Question"
"78892634","78162549","","<p>The problem is how you load the model and how it was saved.
The serialized model ('best.pt') is bound to the specific classes and the exact directory structure(from models.Yolo import YOLOv5) used during training and can be loaded only in the same place where it was trained.</p>
<p>The best way to solve this:</p>
<ol>
<li>In the directory where you have the script to train the model (so that the imports are the same as the ones used during training) load the bounded model:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>model = torch.load(PATH.pt)
</code></pre>
<ol start=""2"">
<li>Extract state_dict() from the loaded model and save it.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>torch.save(model.state_dict(), PATH_UNBOUNDED.pt)
</code></pre>
<ol start=""3"">
<li>Load the unbounded model in different project/ location</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>model = YOLO()#instantiate class with YOLO definition

checkpoint = torch.load(PATH_UNBOUNDED.pt)

model.load_state_dict(checkpoint)#populate the weights
</code></pre>
","2024-08-20 13:07:08","0","Answer"
"78892415","78892215","","<p>This is a torch error:</p>
<blockquote>
<p>the latest version of pyTorch is just missing .dll.</p>
</blockquote>
<p><strong>1st way</strong> is to uninstall the latest and reinstall the previous version by:</p>
<pre><code>pip uninstall torch torchvision torchaudio
</code></pre>
<p>then</p>
<pre><code>pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0
</code></pre>
<p>Once, the installation is successful then install spacy by:</p>
<pre><code>pip install spacy
</code></pre>
<hr />
<p><strong>2nd way</strong> is to install <strong>.dll</strong> from here:</p>
<p><strong><a href=""https://www.dllme.com/dll/files/libomp140_x86_64/00637fe34a6043031c9ae4c6cf0a891d/download"" rel=""nofollow noreferrer"">https://www.dllme.com/dll/files/libomp140_x86_64/00637fe34a6043031c9ae4c6cf0a891d/download</a></strong></p>
<p>and place it in <strong>'C:\Windows\System32'</strong></p>
<hr />
<p><strong>3rd way</strong> is to add the missing dependency <strong>libomp140.x86_64.dll</strong> to the <strong>C:\Windows\System32</strong> directory. To get this, we need to install the C++ build tools.</p>
<p>Complete description here:</p>
<p><a href=""https://github.com/pytorch/pytorch/issues/131662#issuecomment-2252589253"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/issues/131662#issuecomment-2252589253</a></p>
","2024-08-20 12:18:01","0","Answer"
"78892215","","Spacy/Torch fbgemm dependency","<p>I'm trying to work with spacy to do some nlp in Python. Just for some background, I'm running on a windows PC with an Intel UHD Graphics card (So no cuda).</p>
<p>I tried installing the cpu form of pytorch with
<code>pip3 install torch torchvision torchaudio</code> as per the website, and I tried <code>pip install spacy</code> and <code>pip install spacy==3.7</code> (So I've tried both versions). I'm running with Python version <code>3.12.5</code>.</p>
<p>Whenever I try <code>import spacy</code> (or importing any nlp package for that matter), I keep getting the error:</p>
<pre><code>OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\User\AppData\Local
\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfrs8p0\LocalCache
\local-packages\Python312\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.
</code></pre>
<p>I've googled it and these packages should work together? Why am I missing this <code>.dll</code> file? I've also tried this on a different windows PC that does have an nvidia graphics card and cuda, same issue. Why is this file not being downloaded correctly?</p>
<p>Even manually downloading the file, I still get <code>OSError: [WinError 126] The specified module could not be found. Error loading ... shm.dll or one of its dependencies.</code> What's going wrong here? Thanks.</p>
","2024-08-20 11:35:55","1","Question"
"78892191","78892109","","<p>Try this implementation provided <a href=""https://github.com/likyoo/GhostNetV2-PyTorch/blob/main/ghostnetv2.py"" rel=""nofollow noreferrer"">here</a>, and simply remove the classification layer.</p>
","2024-08-20 11:30:29","0","Answer"
"78892176","78891901","","<p>When loading a custom model which may not be avaialale by default in PyTorch or for any reason has a custom implementation, the first step is to initialise an object of that model, which would contain a supposed &quot;skeleton&quot; of the model, with all the trainable constants, layers, and depending on the implementation, default weights of the edges as well.</p>
<p>The second step then would be to load the state dictionary of the model, which is nothing but a mapping of the values of each and every weight to an actual value. Hence, in such a case you need to specifically intialise the layers since they would be intialised (be assigned weights, independent of pre-trained or actually trained). And then one may shift the model to the device they desire.</p>
<p>In case a model is already available in the Pytorch library (references as a built-in model), such as the <code>resnet50</code>, its weights can be either initialised via values provided by PyTorch:</p>
<pre><code>from torchvision.models import resnet50, ResNet50_Weights

# Old weights with accuracy 76.130%
resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
</code></pre>
<p>Or a custom dictionary can be fed into the model however that has to be of the same format/layout as required by, in this case, the <code>resnet50</code> model builder.</p>
<p>Using <code>torch.load()</code> is generally <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model"" rel=""nofollow noreferrer"">not an accepted way</a> to load models, and rather the dictionary of the model is saved and loaded for the most efficiency and flexibility.</p>
<p>Citing the Pytorch documentation's example here. This is also how I personally save and load my own implementations and till now this approach has been pretty reliable:</p>
<pre><code># To save the mdoel
torch.save(model.state_dict(), PATH)
# To load the model. As I mentioned above, instantiating the model class
# and then loading the state_dict
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.eval()
</code></pre>
<p>If I have to shift the model to the GPU, then this is how I do it:</p>
<pre><code>model.load_state_dict(BEST_MODEL_STATE_DICT)
model = model.to(pytorch_dev) if next(model.parameters()).device != pytorch_dev else model
</code></pre>
<p>wherein <code>pytorch_dev</code> is selected as the GPU if CUDA is available on the device.</p>
<p>Some good reads may be:</p>
<ol>
<li><a href=""https://pytorch.org/vision/stable/models.html"" rel=""nofollow noreferrer"">https://pytorch.org/vision/stable/models.html</a></li>
<li><a href=""https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"" rel=""nofollow noreferrer"">https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html</a></li>
</ol>
<p>How the model builder works internally:
<a href=""https://pytorch.org/vision/master/_modules/torchvision/models/_api.html"" rel=""nofollow noreferrer"">https://pytorch.org/vision/master/_modules/torchvision/models/_api.html</a></p>
","2024-08-20 11:25:47","0","Answer"
"78892109","","Extract hidden layer values from pytorch model","<p>I have trained ghostnet model which has last classifier layer. It is 1280 x 4 shape in my case.</p>
<pre><code>(classifier): Linear(in_features=1280, out_features=4, bias=True)
</code></pre>
<p>I would like to get 1280 input values for this layer. How to do this? I see that this is possible to do using hooks, but the concept looks confusing. Do you know alternative ways to do this?</p>
<p>I tried some examples using hooks, but I am not sure that I understand what they do. My code for model evaluation is here:</p>
<pre><code>def model_valid(model, val_data):
    model.eval()
    correct = 0
    total = 0
    score = 0
    pred = np.array([])
    lbs = np.array([])
 
    
    with torch.no_grad():
        for imgs, labels in tqdm(val_data):
            imgs = imgs.to(device = 'cuda')
            labels = labels.to(device = 'cuda')
            outputs = model(imgs)
            _, predicted = torch.max(outputs, dim = 1)
            _, labels = torch.max(labels, dim = 1)

            total += labels.shape[0]
            correct += int((predicted == labels).sum())

            # print(predicted)
            pred = np.append(pred, predicted.cpu().numpy(), axis = 0)
            lbs = np.append(lbs, labels.cpu().numpy(), axis = 0)
            
            print(f'model accuracy is {correct/total}')
    return correct/total, [lbs, pred]
</code></pre>
<p>I would like to have 1280 values from previous layer in addition to my four outputs.</p>
","2024-08-20 11:09:46","1","Question"
"78891901","","Loading partial weights of a neural network","<p>I'm working on a Deep Learning project where I'm adding, on a pre-trained Wide ResNet a couple of linear layers after that, and another technique that I'm using.</p>
<p>My question is if I construct the Network where I add the layers and the techniques that I'm using to the broad resnet as a ModuleList, then I load the pre-trained weights on the entire model with the additions. Does that raise an error in the loading procedure?</p>
<p>Or does it knows that it should only load the weights partially (i.e. only on the reset).</p>
<p>example code:</p>
<pre class=""lang-py prettyprint-override""><code>model = get_model(backbone) # Load ResNet
model = ConstructNewModel(model) # Add layers
load_weights(model, pretrained_path) # load ResNet Weights
model = model.to(&quot;cuda&quot;) # Add to GPU
</code></pre>
<p>or should it look more like this?</p>
<pre class=""lang-py prettyprint-override""><code>model = get_model(backbone) # Load ResNet
load_weights(model, pretrained_path) # load ResNet Weights
model = ConstructNewModel(model) # Add layers
model = model.to(&quot;cuda&quot;) # Add to GPU
</code></pre>
<p>In other words, how does loading the weights in PyTorch work using torch.load?</p>
","2024-08-20 10:26:32","0","Question"
"78890409","78887743","","<p>Attention is computed on a tensor of shape <code>(batch_size, sequence_length, embedding_dimension)</code>. The compute and memory requirements scale with the size of those dimensions.</p>
<p>For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size.</p>
<p>With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length <code>8</code> and 10 sequences of length <code>128</code>. Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of <code>128</code>. If you sort by length before batching, you get one batch with sequence length of <code>8</code> and another with length <code>128</code>. The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128).</p>
<p>That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to &quot;skip over&quot; padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.</p>
","2024-08-20 02:25:26","2","Answer"
"78888454","78888416","","<pre><code>pip install torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0
</code></pre>
<p>if not installed or get error again can download pytorch wheel alone.
make your directory of install = path of downloaded wheel , then</p>
<pre><code>pip install &lt;name of wheel&gt;

https://pypi.org/project/torch/#files
</code></pre>
<p>or</p>
<pre><code>pip install pytorch_wheel_installer

pip install git+https://github.com/pmeier/pytorch_wheel_installer
</code></pre>
","2024-08-19 14:40:25","0","Answer"
"78888416","","Problem installing PyTorch 1.9.0 on Windows 10 x64 with Python 3.8 using pip","<p>I'm trying to install PyTorch 1.9.0 on my Windows 10 64-bit system with Python 3.8, but pip can't find the proper distribution. When I try to install it using the command:</p>
<pre><code>pip3 install torch torchvision torchaudio
</code></pre>
<p>I get the following error:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch
</code></pre>
<p>To solve the problem, I tried to download the corresponding .whl file from the page <a href=""https://download.pytorch.org/whl/torch/"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/torch/</a> and chose torch-1.9.0+cpu-cp38-cp38-win_amd64.whl. However, when I try to install it with the command:</p>
<pre><code>pip install torch-1.9.0+cpu-cp38-cp38-win_amd64.whl
</code></pre>
<p>I get the following error:</p>
<pre><code>ERROR: torch-1.9.0+cpu-cp38-cp38-win_amd64.whl is not a supported wheel on this platform.
</code></pre>
<p>Additional details:</p>
<ul>
<li>Operating System: Windows 10 x64</li>
<li>Python version: 3.8</li>
</ul>
","2024-08-19 14:33:08","-1","Question"
"78887743","","Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?","<p>In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%).</p>
<p>I am curious about the following:</p>
<p>When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively?
Does the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact?</p>
<p>Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%?</p>
<p>Thank you for your insights!</p>
","2024-08-19 11:49:06","1","Question"
"78886560","78886512","","<p>Did you already try to reduce <code>per_device_eval_batch_size</code>?</p>
<p>You could also set <code>eval_accumulation_steps</code> (to a low number) and see if that helps, check out this thread: <a href=""https://discuss.huggingface.co/t/cuda-out-of-memory-when-using-trainer-with-compute-metrics/2941/12"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/cuda-out-of-memory-when-using-trainer-with-compute-metrics/2941/12</a></p>
<p>If nothing works, you could use a smaller validation set and run a custom evaluation using (smaller chunks of) a test set, although this might influence how well your model learns. Or you use a smaller model like <a href=""https://huggingface.co/distilbert/distilgpt2"" rel=""nofollow noreferrer"">https://huggingface.co/distilbert/distilgpt2</a>. From my experience in Google Colab you might also randomly get a GPU assigned that has a bit more or less VRAM (e.g. 16GB vs 24GB), which could make the difference for whether you run out of memory or not, check out the second point here: <a href=""https://stackoverflow.com/a/71540646/18189622"">https://stackoverflow.com/a/71540646/18189622</a></p>
","2024-08-19 06:29:39","0","Answer"
"78886512","","OutOfMemoryError: CUDA out of memory while using compute_metrics function in Hugging Face Trainer","<p>I'm encountering a <code>CUDA out of memory</code> error when using the <code>compute_metrics</code> function with the Hugging Face Trainer during model evaluation. My GPU is running out of memory while trying to compute the ROUGE scores. Below is a summary of my setup and the error message:</p>
<p>I have a val_dataset with 352 samples.</p>
<p>The model is GPT-2</p>
<p>I want to test the Rouge metric but when I try it gives me the error. Currently using <strong>Google Collab</strong></p>
<ol>
<li><p><strong>How can I efficiently compute ROUGE or BLEU metrics without running out of GPU memory?</strong></p>
</li>
<li><p><strong>Are there any recommended strategies or configurations to handle large-scale evaluation on limited GPU memory?</strong></p>
</li>
</ol>
<p>The code looks like this</p>
<pre><code>
tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)
config = AutoConfig.from_pretrained(
    &quot;gpt2&quot;,
    vocab_size=len(tokenizer),
    n_ctx=MAX,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
model = GPT2LMHeadModel(config).to('cuda' if torch.cuda.is_available() else 'cpu')
mode_size = sum(t.numel() for t in model.parameters())
print(f&quot;Model size: {mode_size/1000**2:.1f}M parameters&quot;)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False,
)

class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, patience=3):
        super().__init__()  
        self.patience = patience
        self.best_loss = np.inf
        self.epochs_no_improve = 0

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        eval_loss = metrics.get(&quot;eval_loss&quot;, None)
        if eval_loss is not None:
            if eval_loss &lt; self.best_loss:
                self.best_loss = eval_loss
                self.epochs_no_improve = 0
            else:
                self.epochs_no_improve += 1
                if self.epochs_no_improve &gt;= self.patience:
                    control.should_training_stop = True  


early_stopping_callback = EarlyStoppingCallback(patience=3)

training_args = TrainingArguments(
    output_dir=&quot;./model&quot;,
    hub_model_id=&quot;profile/model&quot;,
    eval_strategy=&quot;epoch&quot;,
    gradient_accumulation_steps=4,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=10,
    weight_decay=0.01,
    logging_dir='./logs',
    save_steps=500,
    save_total_limit=3,
    learning_rate=1e-4,
    fp16=True,
    push_to_hub=True,
    logging_steps=100,
)


trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_validation_dataset,
    callbacks=[early_stopping_callback]
)


trainer.train()
</code></pre>
","2024-08-19 06:15:57","0","Question"
"78885835","78857158","","<p>Found a workaround by adding keras.applications.mobilenet_v2.preprocess_input() as a layer in between an input layer and the premade model, like so:</p>
<pre><code>input = keras.layers.Input([224, 224, 3])

pre = keras.applications.mobilenet_v2.preprocess_input(input)

net = keras.applications.MobileNetV2(
    input_tensor = pre,
    alpha=1.0,
    include_top=True,
    weights=None,
    classes=3,
    classifier_activation=&quot;softmax&quot;,
    name=None,
)
model = keras.Model(inputs=input, outputs=net.output)
</code></pre>
<p>Technically a different function than in the original question, but ultimately they both end up calling keras\src\applications\imagenet_utils.py and produce the same error when they do, so this should hopefully work for all preprocess_input functions that end up calling this.</p>
","2024-08-18 22:31:50","1","Answer"
"78884935","78884110","","<p>BatchNorm layers are one of the few layers that behave differently at training and evaluation time. TensorFlow models default to assuming evaluation mode, PyTorch defaults modules in training mode.</p>
<p>The gradients are a side effect of this, but the output of the models are also different in the different modes.</p>
<p>Simplified version of your code specifying the training mode.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf
import torch
import torch.nn as nn

# Specify if we are training or evaluating
is_training = False

# Create a random input tensor with the same shape for all frameworks
input_shape = (2, 1, 1, 1)
np.random.seed(0)
x_np = np.random.randn(*input_shape).astype(np.float32)

# TensorFlow
model_tf = tf.keras.layers.BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)
x_tf = tf.Variable(tf.convert_to_tensor(x_np))
with tf.GradientTape() as tape:
    y_tf = model_tf(x_tf, training=is_training)     # &lt;--- specify if we are training to tensorflow
    y_tf_sum = tf.reduce_sum(y_tf)
grad_tf = tape.gradient(y_tf_sum, x_tf).numpy()

# PyTorch
model_pt = nn.BatchNorm2d(1)
model_pt.train(is_training)     # &lt;--- set the pytorch model to train/eval mode
x_pt = torch.tensor(x_np, requires_grad=True)
y_pt = model_pt(x_pt)
y_pt.sum().backward()
grad_pt = x_pt.grad.numpy()

# Calculate the differences
diff_y = np.sum(np.abs(y_tf.numpy() - y_pt.detach().numpy()))
diff_dydx = np.sum(np.abs(grad_pt - grad_tf))

# Print the differences
print(&quot;diff forward : &quot;, diff_y)
print(&quot;diff grads : &quot;, diff_dydx)
print(&quot;y_pt : &quot;, y_pt.detach().numpy().flatten())
print(&quot;y_tf : &quot;, y_tf.numpy().flatten())
print(&quot;dydx_pt : &quot;, grad_pt.flatten())
print(&quot;dydx_tf : &quot;, grad_tf.flatten())
</code></pre>
","2024-08-18 15:05:48","0","Answer"
"78884110","","Discrepancy in BatchNorm2d Gradient Calculation Between TensorFlow and PyTorch","<p>I've compared the gradient calculations of a <code>BatchNorm2d</code> layer across different deep learning frameworks, specifically TensorFlow and PyTorch. While doing so, I've encountered a significant discrepancy in the gradients computed by the two frameworks. More specifically, the gradient computed by PyTorch is near zero, while TensorFlow returns something close to one.</p>
<p>Here’s the code I used for comparison:</p>
<pre><code>import numpy as np

# TensorFlow/Keras
import tensorflow as tf

# PyTorch
import torch
import torch.nn as nn


# Set a common random seed for reproducibility
seed = 0
np.random.seed(seed)
tf.random.set_seed(seed)
torch.manual_seed(seed)

# Create a random input tensor with the same shape for all frameworks
input_shape = (4, 3, 5, 5)
x_np = np.random.randn(*input_shape).astype(np.float32)

# TensorFlow/Keras BatchNorm2d
class TFModel(tf.keras.Model):
    def __init__(self):
        super(TFModel, self).__init__()
        self.bn = tf.keras.layers.BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)

    def call(self, x):
        return self.bn(x)

# Instantiate the model
tf_model = TFModel()

# Convert the numpy array to a tensor and ensure it's being watched
x_tf = tf.convert_to_tensor(x_np)
x_tf = tf.Variable(x_tf) 

with tf.GradientTape() as tape:
    y_tf = tf_model(x_tf)
    y_tf_sum = tf.reduce_sum(y_tf)

# Compute the gradient of the output with respect to the input
grad_tf = tape.gradient(y_tf_sum, x_tf)

# Convert the gradient to a numpy array for comparison
grad_tf = grad_tf.numpy()

# PyTorch BatchNorm2d
class TorchModel(nn.Module):
    def __init__(self):
        super(TorchModel, self).__init__()
        self.bn = nn.BatchNorm2d(3)

    def forward(self, x):
        return self.bn(x)

torch_model = TorchModel()
x_torch = torch.tensor(x_np, requires_grad=True)
y_torch = torch_model(x_torch)
y_torch.sum().backward()
grad_torch = x_torch.grad.detach().numpy()


# Calculate the difference between TensorFlow and PyTorch gradients
diff_tf_torch = np.mean(np.abs(grad_tf - grad_torch))

# Print the differences
print(f&quot;Difference between TensorFlow and PyTorch gradients: {diff_tf_torch:.6f}&quot;)
print(&quot;grad pytorch : &quot;, grad_torch[0])
print(&quot;grad tf : &quot;, grad_tf[0])
</code></pre>
<h3>Outputs:</h3>
<ul>
<li>Difference between TensorFlow and PyTorch gradients: 0.999500 (or similar)</li>
<li><code>grad_pytorch</code>: Shows a gradient of all zeros (10^-10).</li>
<li><code>grad_tf</code>: Shows non-zero gradients.</li>
</ul>
<p>I tried to compare both under same conditions. Furthermore, zero gradient for the input seems weird to me. Could there be an issue with how the layers are initialized, or how the inputs are being processed in PyTorch that might lead to this discrepancy? Any insights or explanations would be greatly appreciated</p>
","2024-08-18 08:40:47","0","Question"
"78883118","78687946","","<p>Try exporting your yolo model to .engine format, it should boost your detection speed significantly. also for the tracking part you could utilize the built in Bytracker of supervision library
<a href=""https://docs.ultralytics.com/modes/export/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/export/</a>
<a href=""https://supervision.roboflow.com/latest/trackers/"" rel=""nofollow noreferrer"">https://supervision.roboflow.com/latest/trackers/</a></p>
","2024-08-17 19:39:10","0","Answer"
"78877030","78868535","","<p>Thanks for the help and as user1188867 commented it probably has to do with the specific pytorch version 2.4.0</p>
<p>So you could run an older version or you can download &quot;visual studio c/c++ community 2022&quot; to fix the dependency error.</p>
<p>Here is the video which showed me:
<a href=""https://www.youtube.com/watch?v=-ky896Qp1k8"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=-ky896Qp1k8</a></p>
","2024-08-15 22:13:41","0","Answer"
"78876821","78151471","","<p>I have recently started studying different neural style transfer models. I do not know much. But if you are using <a href=""https://arxiv.org/abs/1508.06576"" rel=""nofollow noreferrer"">A Neural Algorithm of Artistic Style</a> then I have some suggestion for you.</p>
<ol>
<li>In the paper conv4_2 output(relu output) was used for representing content image and they also generated the stylized image from white noise. So it becomes very hard for the model to generate the stylized image while retaining the original content image's fine details. There are two solution for this first use content image and modify it to generate stylized image or use a shallow layer like conv2_2 alongside the conv4_2 output for representing the content image. Though you did not seem suffer the same problem I suffered as the some things in your image is pretty clear. In my case the stylized image was blurry.</li>
<li>There is also a chance that there is something wrong with your code. Did you clamp the values between 0 and 1? or normalized the input image? There are many
things that can go wrong.
Hope the suggestion helps although it's little late</li>
</ol>
","2024-08-15 20:59:53","0","Answer"
"78876336","78868535","","<p>Switch to old version of pytorch. That should work</p>
<p>pip uninstall pytorch
pip install torch==2.3.1</p>
","2024-08-15 18:04:13","1","Answer"
"78876018","78872802","","<p>You would also need to install <a href=""https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html"" rel=""nofollow noreferrer""><code>torch_geometric</code></a> and <a href=""https://github.com/rusty1s/pytorch_scatter"" rel=""nofollow noreferrer""><code>torch_scatter</code></a>
packages to use modules from <code>fairchem.core</code>. Create a <code>venv</code> or anaconda environment with Python 3.10 or newer (I used Python 3.10.14) and follow these installation steps:</p>
<pre><code>pip install fairchem-core
pip install torch_geometric torch-scatter
</code></pre>
<p>Then run the following as per their <a href=""https://github.com/FAIR-Chem/fairchem?tab=readme-ov-file#quick-start"" rel=""nofollow noreferrer"">documentation</a>:</p>
<pre><code>from fairchem.core import OCPCalculator 
</code></pre>
","2024-08-15 16:27:31","0","Answer"
"78873000","78872620","","<p>I believe this is due to pytorch taking a view of the scalar gradient value to avoid excess memory writes. The gradient tensor is a <code>(3,3)</code> tensor, but each entry in the tensor holds the same value, so pytorch instead allocates a single value and takes a <code>(3,3)</code> view of it. For example:</p>
<pre class=""lang-py prettyprint-override""><code># create (1,1) tensor with (1,1) stride
x = torch.tensor([[1]])
print(x.stride())
&gt; (1, 1)

# create view with expand, new tensor now has (0,0) stride
x_view = x.expand(3,3)
print(x_view.stride())
&gt; (0, 0)
</code></pre>
<p>You can avoid the view by backproping from a non-aggregated tensor:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

class Identity(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        return input

    @staticmethod
    def backward(ctx, grad_output):
        print(grad_output.shape, grad_output.stride())
        return grad_output

identity = Identity.apply

x = torch.ones(3, 3, requires_grad=True)
y = identity(x)
y.backward(gradient=torch.ones_like(y)) # backprop directly from y
&gt; torch.Size([3, 3]) (3, 1)

print(x.grad)
&gt; tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
","2024-08-14 21:38:02","0","Answer"
"78872913","78872601","","<p>I've found something that may work, but would appreciate someone's verification. Since the logits of a <code>GPT2LMHeadModel</code> have shape (batch_size, sequence_length, vocab_size), I did the following:</p>
<pre><code>criterion = CrossEntropyLoss(reduction=False)
logits = model(input_ids, attention_mask=attention_mask).logits

# Shift input IDs left by 1 position
labels = input_ids.roll(-1, dims=1)[:, :-1]

# Reshape logits to (N, C, ...) as required by CrossEntropyLoss 
logits_reshape = logits[:, :-1, :].transpose(1,2)

# Output has dim (batch_size, sequence_length). Take mean along sequence_length
loss = criterion(logits_reshape, labels).mean(axis=1) 

loss *= weights
loss = loss.mean()
</code></pre>
","2024-08-14 21:01:13","0","Answer"
"78872802","","OS error while trying to import OCPCalculator","<p>I am having trouble importing a calculator (OCPCalculator). I uninstalled existing torch version and installed a version that supports the task. but it’s still giving me an OS error.</p>
<p>Code:</p>
<pre><code>from fairchem.core.common.relaxation.ase_utils import OCPCalculator
import ase.io
from ase.optimize import BFGS
</code></pre>
<p>Error:</p>
<pre class=""lang-none prettyprint-override""><code>OSError   Traceback (most recent call last)/var/folders/n7/96_5cstj2sn6g5ll0b979w380000gn/T/ipykernel_49125/1241704498.py in &lt;module&gt;
----&gt; 1 from fairchem.core.common.relaxation.ase_utils import OCPCalculator
  2 import ase.io
  3 from ase.optimize import BFGS
~/opt/anaconda3/lib/python3.9/site-packages/fairchem/core/common/relaxation/ase_utils.py in &lt;module&gt;
 24 
 25 from fairchem.core.common.registry import registry
---&gt; 26 from fairchem.core.common.utils import (
 27     load_config,
 28     setup_imports,

~/opt/anaconda3/lib/python3.9/site-packages/fairchem/core/common/utils.py in &lt;module&gt;
 30 import torch
 31 import torch.nn as nn
---&gt; 32 import torch_geometric
~/opt/anaconda3/lib/python3.9/sitepackages/torch_geometric/__init__.py in &lt;module&gt;
 11 import torch_geometric.loader
 12 import torch_geometric.transforms
---&gt; 13 import torch_geometric.datasets
...
384             self._handle = handle

OSError: 
dlopen(/Users/roshnidantuluri/opt/anaconda3/lib/python3.9/sitepackages/torch_cluster/_version_cpu.so, 0x0006): 
Symbol not found: __ZN3c1017RegisterOperatorsD1Ev
Referenced from: &lt;6A95460A-9602-33F2-92B2-6A9C90B59457&gt;/Users/roshnidantuluri/opt/anaconda3/lib/python3.9/site-packages/torch_cluster/_version_cpu.so
Expected in:     &lt;709C1DF5-D253-3C66-87E2-C99FD3A259DF&gt;/Users/roshnidantuluri/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib
</code></pre>
","2024-08-14 20:22:11","0","Question"
"78872620","","Getting unusual tensor strides in backwards pass when using Pytorch’s custom autograd feature","<p>I'm getting strides like <code>(0, 0)</code> when writing custom Pytorch autograd functions. Here's a minimal, reproducible example:</p>
<pre><code>import torch

class Identity(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        return input

    @staticmethod
    def backward(ctx, grad_output):
        print(grad_output.shape, grad_output.stride())
        return grad_output

# Create a callable object for the Identity function
identity = Identity.apply


# Example usage
x = torch.randn(3, 3, requires_grad=True)
y = identity(x)
z = y.sum()
z.backward()

print(x.grad)  # Should be a 3x3 tensor of ones
</code></pre>
<p>This is causing problems / leading to incorrect results when using more complicated autograd functions where I try to make <code>grad_output</code> contiguous.</p>
<p>What is causing this issue / how can I fix it?</p>
","2024-08-14 19:19:35","0","Question"
"78872609","78871831","","<p>I'm not sure what you mean by <code>the sum between a BCELoss and a MSELoss object no longer work</code>. You can still sum the losses. Summing the losses is numerically equivalent to calling backward on each loss individually:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

# simple dummy model
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.stem = nn.Sequential(
            nn.Linear(32, 32),
            nn.ReLU(),
            nn.Linear(32, 32),
            nn.ReLU()
        )
        
        self.head1 = nn.Sequential(
            nn.Linear(32, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
        )
        
        self.head2 = nn.Sequential(
            nn.Linear(32, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
        )
        
    def forward(self, x):
        x = self.stem(x)
        p1 = torch.sigmoid(self.head1(x))
        p2 = self.head2(x)
        
        return p1, p2


# inputs/outputs
x = torch.randn(4, 32)
y_regression = torch.randn(4, 1)
y_classification = (torch.randn(4,1)&gt;0).float()
    
model = Model()

bce = nn.BCELoss()
mse = nn.MSELoss()


# compute predictions, backward combined loss
p1, p2 = model(x)
loss1 = bce(p1, y_classification)
loss2 = mse(p2, y_regression)
loss = loss1 + loss2
loss.backward()

# grab gradient
g1 = model.stem[0].weight.grad.data.clone()

# zero gradients
for param in model.parameters():
    param.grad = None

# compute predictions, backward separate losses
p1, p2 = model(x)
loss1 = bce(p1, y_classification)
loss1.backward(retain_graph=True)
loss2 = mse(p2, y_regression)
loss2.backward()

# grab gradient
g2 = model.stem[0].weight.grad.data.clone()

# validate gradients are the same
torch.allclose(g1, g2)
</code></pre>
<p>When you <code>backward</code> multiple times, pytorch accumulates and sums the gradients of the individual terms together.</p>
<p>The second method of calling <code>backward</code> on the individual loss terms is slightly more memory efficient, but otherwise equivalent. If the model isn't learning well, you may need to add weights to the different loss terms. As-is, pytorch sums the gradients from each loss based on their raw numerical value. If one loss term is much larger than the other, it will contribute more to the overall gradient. This can be corrected by scaling the larger term to be more in-line with the smaller term (or vice/versa).</p>
<p>Also as a minor note, your <code>First_branch</code> ends with a linear layer and your <code>Final_branch</code> starts with a linear layer, which is redundant. You'll get a slight bump from ending your <code>First_branch</code> with a ReLU.</p>
<p>You should also consider using a combined head for the outputs. You can have a single MLP output a tensor of shape <code>(bs, 2)</code> where the first column is routed to the classification loss and the second to the regression loss. In theory this allows the two output types to share information - you'll have to experiment to see if it makes a difference.</p>
","2024-08-14 19:15:48","1","Answer"
"78872601","","Sample weights for loss computing by huggingface transformer model","<p>I'm training a <code>GPT2LMHeadModel</code> in Python using huggingface's <code>transformers</code> library. The task is next token prediction. If I understand correctly, if this object is provided a <code>labels</code> argument, it should automatically compute loss for each pair of tokens, i.e. if I have input <code>[1,2,3]</code>, then the object should compute the loss for each of the (input, next_token) combinations: <code>([1], [2])</code> and <code>([1,2], [3])</code></p>
<p>My dataset includes weights for each sample. Is there a way for me to include these weights in the training process? Note that these are not class weights, but weights for each individual record in my train set. In a more &quot;vanilla&quot; training scenario with PyTroch, I would just set <code>reduction='none'</code> on my criterion, then multiple the result by the batch's weights. I'm not sure how I would implement this efficiently, accounting for all the prediction tasks that go into a single sequence.</p>
","2024-08-14 19:14:23","1","Question"
"78872522","78872477","","<p>This is a case of numerical overflow.</p>
<p>Consider:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

# set seed
torch.manual_seed(42)

# random values between 0 and 5140, like your values
x = (torch.rand(1, 256, 60, 120) * 5140)

# create conv layer
conv2d = nn.Conv2d(
            256,
            256,
            kernel_size=2,
            stride=2,
            bias=False,
        )

# set conv weights to a similar range to your values
torch.nn.init.uniform_(conv2d.weight, a=-0.8, b=1.57)

# compute output
y = conv2d(x)

print(y.max())
&gt; tensor(1334073.6250, grad_fn=&lt;MaxBackward1&gt;)
</code></pre>
<p>When I run the code above, I see a max output value of <code>1334073.6250</code>. You are using fp16 values, which overflow at 32000. You're seeing <code>inf</code> values because your output values exceed the range of fp16.</p>
<p>You should preprocess your inputs to have a more reasonable numeric range. Neural networks are very sensitive to numerical scale. Typically you want your input values to be roughly mean 0, variance 1. A max input value of 5140 is huge and likely to cause numeric issues in any situation.</p>
","2024-08-14 18:50:04","3","Answer"
"78872477","","Pytorch Conv2d outputs infinity","<p>My input <code>x</code> is a <code>[1,256,60,120]</code> shaped tensor. My Conv2d is defined as follows</p>
<pre><code>import torch.nn as nn
conv2d = nn.Conv2d(
            256,
            256,
            kernel_size=2,
            stride=2,
            bias=False,
        ),
</code></pre>
<p>Some instances I see that <code>conv2d(x).isinf().any()</code> is True. Note that</p>
<p><code>x.max() = tensor(5140., device='cuda:0', dtype=torch.float16).</code></p>
<p><code>x.min() = tensor(0., device='cuda:0', dtype=torch.float16)</code>,</p>
<p><code>conv2d.weight.max() =tensor(1.5796, device='cuda:0')</code></p>
<p><code>conv2d.weight.min() = tensor(-0.8045, device='cuda:0')</code></p>
<p>Why do  I get infinity?</p>
","2024-08-14 18:36:07","2","Question"
"78872372","78870012","","<p>The notation here is definitely confusing. The transpose notation is not clarifying.</p>
<p>I think the operation makes more sense in einsum notation - <code>bn,anm,bm-&gt;ba</code>. <code>x1</code> has a multiplication/reduction along the second axis of <code>A</code>. <code>x2</code> has a multiplication/reduction along the second axis of <code>A</code>.</p>
<pre class=""lang-py prettyprint-override""><code>x1 = torch.randn(2, 8)
x2 = torch.randn(2, 4)
A = torch.randn(16, 8, 4)

b1 = torch.bilinear(x1, x2, A)
b2 = torch.einsum('bn,anm,bm-&gt;ba', x1, A, x2)
assert torch.allclose(b1, b2)
</code></pre>
<p>You can also compute the multiplications/reductions explicitly:</p>
<pre class=""lang-py prettyprint-override""><code>b3 = (x1[:,None,:,None] * A[None] * x2[:,None,None,:]).sum(-1).sum(-1)
assert torch.allclose(b1, b3)
</code></pre>
","2024-08-14 18:01:33","1","Answer"
"78872271","78868722","","<p>You're not using learned projections.</p>
<p>If you look at the state dict of the attention module, you'll see:</p>
<pre class=""lang-py prettyprint-override""><code>print(multihead_attn.state_dict().keys())
&gt; odict_keys(['in_proj_weight', 'in_proj_bias', 'out_proj.weight', 'out_proj.bias'])
</code></pre>
<p>That might give you an indication of what you're missing. To reproduce pytorch's attention, you need to do the following:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F
from torch.nn import MultiheadAttention
import math

def attention(q, k, v, 
              embed_dim, num_heads, 
              in_proj_weight, in_proj_bias,
              out_proj_weight, out_proj_bias,
              batch_first=True):
    
    # transpose if batch first
    if batch_first:
        q = q.transpose(1,0)
        k = k.transpose(1,0)
        v = v.transpose(1,0)
        
    # get dimensions 
    tgt_len, bsz, embed_dim = q.shape
    src_len, _, _ = k.shape
    head_dim = embed_dim // num_heads
    
    # chunk in projection weights
    w_q, w_k, w_v = multihead_attn.in_proj_weight.chunk(3)
    b_q, b_k, b_v = in_proj_bias.chunk(3)
    
    # compute in projections
    q = F.linear(q, w_q, b_q) 
    k = F.linear(k, w_k, b_k)
    v = F.linear(v, w_v, b_v)
    
    # reshape for attention 
    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
    k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
    v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
    
    # get updated dimensions 
    src_len = k.size(1)
    B, Nt, E = q.shape

    # scale query
    q_scaled = q * math.sqrt(1.0 / float(E))
    
    # compute attention weights
    attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
    attn_output_weights = F.softmax(attn_output_weights, dim=-1)
    
    # compute attention output
    attn_output = torch.bmm(attn_output_weights, v)
    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
    attn_output = F.linear(attn_output, out_proj_weight, out_proj_bias)
    attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))

    # average attention weights between heads
    attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
    attn_output_weights = attn_output_weights.mean(dim=1)
    
    # if batch first, reshape output
    if batch_first:
        attn_output = attn_output.transpose(1,0)
    
    return attn_output, attn_output_weights

embed_dim = 8
num_heads = 1
batch_size = 2
seq_len = 5

Q = torch.randn(batch_size, seq_len, embed_dim)
K = torch.randn(batch_size, seq_len, embed_dim)
V = torch.randn(batch_size, seq_len, embed_dim)

multihead_attn = MultiheadAttention(embed_dim=embed_dim, 
                                    num_heads=num_heads, 
                                    batch_first=True)

attn_output_pytorch, attn_output_weights_pytorch = multihead_attn(Q, K, V)

attn_output_custom, attn_output_weights_custom = attention(Q, K, V, 
                                                           embed_dim, 
                                                           num_heads, 
                                                           multihead_attn.in_proj_weight, 
                                                           multihead_attn.in_proj_bias,
                                                           multihead_attn.out_proj.weight, 
                                                           multihead_attn.out_proj.bias,
                                                           batch_first=True)

assert torch.allclose(attn_output_custom, attn_output_pytorch), &quot;Attention output does not match.&quot;
assert torch.allclose(attn_output_weights_custom, attn_output_weights_pytorch), &quot;Attention weights do not match.&quot;
</code></pre>
<p>If you run the above code a bunch of times, you'll encounter a few instances where the <code>allclose</code> check fails - this is because pytorch uses a compiled cuda kernel under the hood and there can be slight numeric differences. Overall, this is the attention algorithm you are looking for.</p>
<p>You can see the full pytorch implementation <a href=""https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py#L5868"" rel=""nofollow noreferrer"">here</a></p>
","2024-08-14 17:31:24","0","Answer"
"78871831","","Multi loss going into the same subsquent model using PyTorch","<p>I am currently facing a problem.</p>
<p>I am wondering how I should back propagate the loss functions in the following model.</p>
<p>What is important here is that all the blue part is common to the 2 outputs, the green part is a binary classification using BCELoss and the red part is a regression task using MSELoss.</p>
<p>Here are the code and image to understand the model fully :
<a href=""https://i.sstatic.net/WxfPyZ1w.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WxfPyZ1w.png"" alt=""Model visualization"" /></a></p>
<pre><code>class First_branch(nn.Module) : 
    def __init__(self, input_size, num_heads=3):
        super(First_branch, self).__init__()
        self.fc1 = nn.Linear(input_size*num_heads, 128)
        self.fc2 = nn.Linear(128, 64)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class Final_branch_1(nn.Module):
    def __init__(self, input_size, num_heads=3):
        super(Final_branch_1, self).__init__()
        self.fc1 = nn.Linear(64, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x)) # Binary classification
        return x

class Final_branch_2(nn.Module):
    def __init__(self, input_size, num_heads=3):
        super(Final_branch_2, self).__init__()
        self.fc1 = nn.Linear(64, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x) #Regression task
        return x

class Model(nn.Module):
    def __init__(self, in_feats_dict, hidden_feats_dict, out_feats_dict, edge_feats_dict, rel_names):
        super().__init__()
        self.conv = ConvModel(in_feats_dict, hidden_feats_dict, out_feats_dict, edge_feats_dict, rel_names, num_heads=3)
        self.embed = First_branch(sum(out_feats_dict.values()), num_heads=3)
    
        self.pred_1 = Final_branch_1(64, num_heads=3)
        self.pred_2 = Final_branch_2(64, num_heads=3)

    def forward(self, g, node_features, edge_features):
        conv_output = self.conv(g, node_features, edge_features)
    
        to_concat = [conv_output[key] for key in conv_output.keys()]
    
        # Aggregate the results following each latent feature 
        aggregated_features = [torch.mean(i, dim=0) for i in to_concat]           
        
        concatenated_features = torch.cat(aggregated_features)
    
        embedded_features = self.embed(concatenated_features)
    
        prediction_1 = self.pred_1(embedded_features)
        prediction_2 = self.pred_2(embedded_features)
    
        return prediction_1, prediction_2
</code></pre>
<p>The question is the following, what is the best way to compute the loss for those task?</p>
<p>I know that with 2 MSELoss, the best way is just to make a sum of the loss:</p>
<pre><code>loss_1 = criterion(output_1, score_1)
loss_2 = criterion(output_2, score_2)
loss = loss_1 + loss_2
loss.backward()
optimizer.step()
</code></pre>
<p>But since the first branch is doing a classification, I need to use the BCELoss and the sum between a BCELoss and a MSELoss object no longer work.</p>
<p>For the moment, my solution is :</p>
<pre><code>bce = nn.BCELoss()
mse = MSELoss()
loss_1 = bce(output_1, score_1)
loss_2 = mse(output_2, score_2)
loss_1.backward(retain_graph=True)
loss_2.backward()
optimizer.step()
</code></pre>
<p>But is it the good way? Because it seems that my model has trouble learning when I make it this way.</p>
","2024-08-14 15:41:08","0","Question"
"78871358","","What is the preferred way to load images from s3 into torch serve for inference?","<p>I have an image classifier model that I plan to deploy via torch serve. My question is, what is the ideal way to load as well write images from / to s3 buckets instead of from local filesystem for inference. Should this logic live in the model handler file? Or should it be a separate worker that sends images to the inference endpoint, <a href=""https://github.com/pytorch/serve/tree/ef196c0f1d5f14bb0e01f65b7b21d43c3c143814/examples/cloud_storage_stream_inference"" rel=""nofollow noreferrer"">like this example</a>, and the resulting image is piped into an <code>aws cp</code> command for instance?</p>
","2024-08-14 13:59:11","0","Question"
"78870571","78870012","","<p>The implementation <em>behind the scenes</em> calculates the bilinear product, as per documentation.<br />
That's not to say that multiplying the tensors as they are is equivalent to the bilinear layer.<br />
If you follow the trail of the actual calculation, you end up with a python-interface function, while the actual implementation is in the cpp, where you can see that there are reshapes and flattening of the tensors so as to ensure the output is the expected expression:</p>
<p>Here's an excerpt from the <a href=""https://github.com/pytorch/pytorch/blob/a8dc9d8e353ddcf7db0247349a3acd0dd37fcc6f/aten/src/ATen/native/Linear.cpp#L702"" rel=""nofollow noreferrer"">relevant code</a>:</p>
<pre><code>  auto size1 = input1.sym_sizes();
  output_size.insert(output_size.end(), size1.begin(), size1.end() - 1);
  output_size.push_back(weight.sym_size(0));
  auto input1_flattened = input1.reshape_symint({-1, input1.sym_size(-1)});
  auto input2_flattened = input2.reshape_symint({-1, input2.sym_size(-1)});
  Tensor output = at::_trilinear(input1_flattened, weight, input2_flattened, {1,3}, {0}, {1,2}, {2,3}).reshape_symint(output_size);
  if (bias.defined()) {
    output = output + bias;
  }
</code></pre>
<p>Hope that helps</p>
","2024-08-14 11:02:51","1","Answer"
"78870012","","Implementing nn.Bilinear Layer","<p>Can anyone help me understand the implementation of <code>nn.Bilinear</code>
As per the documentation, this function implements y = x<sub>1</sub><sup>T</sup> * A * x<sub>2</sub>
taking <code>x1 = (100,20)</code> , <code>x2 = (100,30')</code> , assuming <code>output_features = 50</code>. The  matrix <code>A</code> has dimensions of <code>[50,20,30]</code>.
I am finding it difficult how these matrices are multiplied to get the <code>output = [100,50]</code></p>
<p>Based on the size of x<sub>1</sub>,x<sub>2</sub> and <code>A</code> matrix, the multiplication seems incompatible as per  y = x<sub>1</sub><sup>T</sup> * A * x<sub>2</sub> . What am I missing here?</p>
","2024-08-14 09:01:18","0","Question"
"78869828","78868439","","<p>Have your script in a folder in google drive. Connect your Colab workspace with your Google Drive. You don't even have to remember the code for it when you press button it automatically gets added at the top. Now run all of your commands. Now your Yolov5 folder should be saved in the folder you created and your runs are saved there as well, make sure that you are in the selected folder by running <code>path</code> and <code>cd</code> to navigate</p>
","2024-08-14 08:18:41","0","Answer"
"78869814","78114412","","<p>I was also facing the same issue. But the issue got resolved when I uninstalled torch 2.4.0 and installed torch 2.3.1. The issue is because of compatibility.</p>
","2024-08-14 08:14:25","6","Answer"
"78868722","","output of custom attention mechanism implementation does not match torch.nn.MultiheadAttention","<p>I was trying to create my own attention function for a project I'm working on. However, when I compared the output and weights from my code with those from <code>torch.nn.MultiheadAttention</code>, I noticed that the <code>softmax(QK^T/d_k^0.5)</code> is calculated incorrectly. Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F
from torch.nn import MultiheadAttention

def attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k**0.5)
    attn_output_weights = F.softmax(scores, dim=-1)
    attn_output = torch.matmul(attn_output_weights, V)
    return attn_output, attn_output_weights

embed_dim = 8
num_heads = 1
batch_size = 2
seq_len = 5

Q = torch.randn(batch_size, seq_len, embed_dim)
K = torch.randn(batch_size, seq_len, embed_dim)
V = torch.randn(batch_size, seq_len, embed_dim)

multihead_attn = MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)
attn_output_pytorch, attn_output_weights_pytorch = multihead_attn(Q, K, V)

attn_output_custom, attn_output_weights_custom = attention(Q, K, V)

assert torch.allclose(attn_output_custom, attn_output_pytorch, rtol=1e-6, atol=1e-8), &quot;Attention output does not match.&quot;
assert torch.allclose(attn_output_weights_custom, attn_output_weights_pytorch, rtol=1e-6, atol=1e-8), &quot;Attention weights do not match.&quot;
</code></pre>
<p>I tried changing the hyperparameters, printing each matrix, not normalizing by the d_k^0.5 factor, matching with <code>torch.nn.functional.scaled_dot_product_attention</code>, and checking the shape of each tensor, but I still didn't get good results. I am primarily concerned with matching <code>attn_output_weights_custom</code> and <code>attn_output_weights_pytorch</code>.</p>
<p>Can someone spot what I might be doing wrong?</p>
","2024-08-14 00:45:48","0","Question"
"78868535","","How can I solve the error ""ModuleNotFoundError: No module named 'torch' ""in a venv. in VS Code?","<p>I am new to programming and working with deep learning networks, so maybe I'm just stupid, but I can't get my PyTorch to work. Not long ago, it did without problem, but then I tried to fix CUDA for GPU acceleration (which did not work), and ever since, my Torch has refused to work. I run the programs in a virtual environment (venv.) in VS Code with Python version 3.11.9, but when I try to verify my Torch with.</p>
<pre><code>import torch

print(torch.__version__)
</code></pre>
<p>I only receive the error that:</p>
<pre><code>File &quot;c:\Python\Test\torch_test.py&quot;, line 1, in &lt;module&gt;
    import torch
  File &quot;C:\Python\Test\.venv\Lib\site-packages\torch\__init__.py&quot;, line 148, in &lt;module&gt;
    raise err
OSError: [WinError 126] The specified module cannot be found. Error loading &quot;C:\Python\Test\.venv\Lib\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.
PS C:\Python\Test&gt;
</code></pre>
<p>I have tried to install Torch a million times by now, both inside and outside the venv with pip and pip3 in both the VS Codes terminal and in the command prompt. But everytime I get back, the &quot;Requirement already satisfied&quot;. Yet still, the Torch is not working is absent.</p>
<p>When looking through stackoverflow, I find mostly people that have used Conda environments, which I am not familiar with. So if there is a solution without Conda, or must I go with that environment instead? Thanks beforehand for any help possible!</p>
","2024-08-13 22:47:10","0","Question"
"78868439","","runs\train\exp10 is not a directory","<p>I am trying to train a YoloV5 model with my custom data on my own computer, but I keep getting this error:</p>
<pre><code>train: weights=yolov5s.pt, cfg=models/yolov5s.yaml, data=data.yaml, hyp=data\hyps\hyp.scratch-low.yaml, epochs=300, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False
    github: skipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5
    YOLOv5  2024-7-15 Python-3.12.2 torch-2.3.1+cpu CPU
    hyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
    TensorBoard: Start with 'tensorboard --logdir runs\train', view at http://localhost:6006/
    Traceback (most recent call last):
      File &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py&quot;, line 986, in &lt;module&gt;
        main(opt)
      File &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py&quot;, line 688, in main
        train(opt.hyp, opt, device, callbacks)
      File &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py&quot;, line 180, in train
        loggers = Loggers(
                  ^^^^^^^^
      File &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\utils\loggers\__init__.py&quot;, line 121, in __init__
        self.tb = SummaryWriter(str(s))
                  ^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;, line 249, in __init__
        self._get_file_writer()
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;, line 281, in _get_file_writer
        self.file_writer = FileWriter(
                           ^^^^^^^^^^^
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;, line 75, in __init__
        self.event_writer = EventFileWriter(**
                            ^^^^^^^^^^^^^^^^
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorboard\summary\writer\event_file_writer.py&quot;, line 72, in __init__
        tf.io.gfile.makedirs(logdir)
      File &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\lib\io\file_io.py&quot;, line 513, in recursive_create_dir_v2
        _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))
    tensorflow.python.framework.errors_impl.FailedPreconditionError: runs\train\exp10 is not a directory
</code></pre>
<p>I have tried training my model from a pre-trained one (as the yolov5 docs recommend), like this:
<code>python train.py --img 640 --batch 32 --epochs 300 --data data.yaml --weights yolov5s.pt</code>
or from scratch, like this:
<code>python train.py --img 640 --batch 32 --epochs 300 --data data.yaml --cfg models/yolov5s.yaml</code></p>
<p>I have also seen other issues, like issue #12008 on GitHub and this issue from Stack Overflow <a href=""https://stackoverflow.com/questions/76934905/tensorflow-python-framework-errors-impl-failedpreconditionerror-runs-train-exp3"">tensorflow.python.framework.errors_impl.FailedPreconditionError: runs\train\exp3 is not a directory</a>, but havent found any solution</p>
","2024-08-13 21:58:27","-1","Question"
"78868012","78820748","","<p>This is a known bug with inference using CPU on Windows for <code>PyTorch 2.4.0</code> : <a href=""https://github.com/ultralytics/ultralytics/issues/15049"" rel=""noreferrer"">Bug Ticket</a></p>
<p>Common issues are:</p>
<ul>
<li>Too many detections at the top of the image</li>
<li>Reporting missing <code>fbgemm.dll</code></li>
</ul>
<hr />
<p>Here are the recommended fixes:</p>
<ul>
<li>When only CPU available: Downgrade PyTorch (CPU) to a previous version</li>
</ul>
<pre class=""lang-none prettyprint-override""><code>pip uninstall torch torchvision torchaudio -y
pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cpu
</code></pre>
<ul>
<li>When NVIDIA GPU with CUDA support available: Install PyTorch with CUDA support (will still fail in case of <code>PyTorch 2.4.0</code> when using CPU)</li>
</ul>
<pre class=""lang-none prettyprint-override""><code>pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre>
<ul>
<li>Install nightly build of <code>PyTorch</code> (<strong>WARNING</strong>: might be unstable)</li>
</ul>
<pre class=""lang-none prettyprint-override""><code>pip install torch  --index-url https://download.pytorch.org/whl/nightly/cu121 --force-reinstall
</code></pre>
","2024-08-13 19:26:03","6","Answer"
"78866520","78866202","","<p>here is how you should pass datasets to the <code>super_gradients</code>. I have tested this folder structure on MacOS. On windows you need to rewrite all &quot;/&quot; symbols with the &quot;\&quot;</p>
<pre><code># this is an example for macOS/Linux
# tested with coco8 dataset
# https://github.com/ultralytics/assets/releases/download/v0.0.0/coco8.zip

dataset_params = {
    'data_dir': &quot;dataset&quot;, 
    'train_images_dir': &quot;images/val&quot;,
    'train_labels_dir': &quot;labels/val&quot;,
    'val_images_dir': &quot;images/val&quot;,
    'val_labels_dir': &quot;labels/val&quot;,
    'classes': ['cabecalho', 'assinatura', 'rodape']
}
</code></pre>
<p>Folder structure is the following:</p>
<pre><code>- dataset
- dataset/images
- dataset/images/val
- dataset/images/val/000000000049.jpg
- ...
- dataset/images/train
- dataset/images/train/000000000034.jpg
- ...
- dataset/labels/val
- dataset/labels/val/000000000049.txt 
- ...
- dataset/labels/train
- dataset/labels/train/000000000034.txt 
- ...




</code></pre>
","2024-08-13 13:14:21","0","Answer"
"78866202","","The algorithm cannot find the images","<p>I'm developing an algorithm using Yolo-nas , I prepared the dataset with labelImg . I'm using Python 3.10.11 to do this algorithm together with super-gradient supervision. The problem is the following: The algorithm loads the data but when plotting the image it shows that it cannot find the image in the directory, I carried out some tests with other algorithms and it can find the path to the directory. I suspect it's the super-gradient version (3.7.1)</p>
<p>The error starts when I have to plot my training data</p>
<pre><code>FileNotFoundError :dataset\\images\\train\\img1.png was not found. 
Please make sure that the dataset was downloaded and that the path is correct
</code></pre>
<p>note: the images in the dataset were pdfs and I converted them to png to be able to use them in the labelImg and identify the object classes</p>
<ul>
<li>I tried changing the directory</li>
<li>remade the dataset</li>
<li>I checked if another algorithm can search for the images and it does.</li>
</ul>
<pre><code>import torch
torch.__version__

from tqdm.notebook import tqdm
from super_gradients.training import dataloaders
from super_gradients.training.dataloaders.dataloaders import coco_detection_yolo_format_train, coco_detection_yolo_format_val
from super_gradients.training import models
from super_gradients.training.losses import PPYoloELoss
from super_gradients.training.metrics import DetectionMetrics_050
from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback

dataset_params = {
    'data_dir': &quot;nf/dataset&quot;, 
    'train_images_dir': &quot;dataset/images/train&quot;,
    'train_labels_dir': &quot;dataset/labels/train&quot;,
    'val_images_dir': &quot;dataset/images/val&quot;,
    'val_labels_dir': &quot;dataset/labels/val&quot;,
    'classes': ['cabecalho', 'assinatura', 'rodape']
}

MODEL_ARCH = 'yolo_nas_l'
DEVICE = 'cuda' if torch.cuda.is_available() else &quot;cpu&quot;
BATCH_SIZE = 10 
MAX_EPOCHS = 12
CHECKPOINT_DIR = '\checkpoint'
EXPERIMENT_NAME = &quot;nf&quot;


dados_treino = coco_detection_yolo_format_train(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['train_images_dir'],
        'labels_dir': dataset_params['train_labels_dir'],
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size': BATCH_SIZE,
        'num_workers': 1
    }
)

val_dados = coco_detection_yolo_format_val(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['val_images_dir'],
        'labels_dir': dataset_params['val_labels_dir'],  
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size': BATCH_SIZE,
        'num_workers': 1
    }
)

dados_treino.dataset.transforms

dados_treino.dataset.plot()
</code></pre>
","2024-08-13 12:07:15","0","Question"
"78865625","78826769","","<p>It looks like the issue is with the latest version of the PyTorch package. You should consider rolling back to a previous version until this problem is resolved.</p>
<pre><code>pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cpu
</code></pre>
","2024-08-13 10:03:50","0","Answer"
"78863873","78863704","","<p>It could be, that shell used by <code>var shell = Shell();</code> doesn't have same ENV variables that your terminal has (the one that you said it worked from), so pip's installed packages aren't in the PATH for that shell.</p>
<p>I think, process_run on MacOS uses &quot;Default login shell&quot; to run its commands from (<em><strong>/bin/zsh</strong></em> on my mac), but you may be using different shell in your terminal, that has pip's dependency installation location in its path. Try editing your default shell's configuration (<em><strong>~/.zshrc</strong></em> in my case) to have the same ENV variables and the same PATH as the shell that it works from has.</p>
<p>You can see your default and currently selected shells in Terminal app's settings (<a href=""https://i.sstatic.net/A1fUQS8J.png"" rel=""nofollow noreferrer"">Screenshot from MacOS Ventura</a>)</p>
<p>Before doing this, the simplest way to check would be to <code>echo $PATH</code> from both, your terminal and process_run's shells and compare.</p>
<p>P.S. I didn't replicate the issue, but I don't have enough StackOverflow reputation to post this as a comment to your post, thus I'm posting this as an answer. Hope this helps :)</p>
","2024-08-12 23:17:41","1","Answer"
"78863745","78863529","","<p>Your code works without issues in my environment; I believe your <code>huggingface_hub</code> package is old. Try</p>
<pre><code>python -m pip install --upgrade huggingface_hub
</code></pre>
","2024-08-12 22:10:01","0","Answer"
"78863704","","Why will my python file run via a terminal command but not when I run the same command inside of a function in my Flutter macos app?","<p>I want to execute python code in my Flutter macos app to run a YOLO object detection model.</p>
<p>I am using the <a href=""https://pub.dev/packages?q=process_run"" rel=""nofollow noreferrer"">https://pub.dev/packages?q=process_run</a> package to execute shell commands.
The package works and will run a python file with a simple print function, but not my object detection file.</p>
<p>I have cloned <a href=""https://github.com/ultralytics/yolov5"" rel=""nofollow noreferrer"">https://github.com/ultralytics/yolov5</a> inside of my app.</p>
<p>In my flutter app I have a function:</p>
<pre><code>_runPython() async {
    var shell = Shell();

    await shell.run('''
    python3 yolov5/detect.py
    ''');
  }
</code></pre>
<p>When I call _runPython I get the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;yolov5/detect.py&quot;, line 39, in &lt;module&gt;
    import torch
ModuleNotFoundError: No module named 'torch'
</code></pre>
<p>However when I run <code>python3 yolov5/detect.py</code> in my terminal the code executes just fine.</p>
<p>Looking inside of my python files inside of my app I don't see any errors, and everything appears to be imported.</p>
<p>Why is my detect.py working via the terminal and not from my Flutter app?</p>
","2024-08-12 21:47:11","1","Question"
"78863630","78863577","","<p>If you're getting &quot;illegal instruction,&quot; that means you're trying to execute an incompatible binary on your raspberry pi. If you install pytorch through pip, it should handle this automatically:
<code>pip3 install torch torchvision torchaudio</code></p>
<p>Also, if these cloud providers are zipping your model, you can still <a href=""https://linuxize.com/post/how-to-unzip-files-in-linux/"" rel=""nofollow noreferrer"">unzip it on your pi</a>. If you <em>really can't</em> zip your file forwhatever reason, try using <a href=""https://linuxize.com/post/how-to-use-scp-command-to-securely-transfer-files/"" rel=""nofollow noreferrer""><code>scp</code></a>  to move them.</p>
","2024-08-12 21:16:10","0","Answer"
"78863577","","How to install PyTorch and run .pt file in Raspberry Pi 4 B?","<p>So i am trying to implement my yolov5 object detection model &quot;best.pt&quot; which i trained in custom roboflow collab notebook file which i have already downloaded in my desktop since it is where i trained it. Now im trying to pass the .pt file using different methods to my raspi 4b (google drive, dropbox, etc), but it is being converted to zip archive file always which should not do. Moreover, i also cant get the right pytorch version on my raspi after trying different versions with arm/aarch compatibility. Running my python files results to showing &quot;illegal instruction&quot; which works again whenever i uninstall pytorch which shows incompatibility. Im also using virtual environment to ensure the safety of my system.</p>
<p>Is it really not possible in my situation? This is a critical moment for me since it is what i have written in my papers using pytorch.</p>
<p>This is my raspi version and details:</p>
<pre><code>PRETTY_NAME=&quot;Debian GNU/Linux 12 (bookworm)&quot;
NAME=&quot;Debian GNU/Linux&quot;
VERSION_ID=&quot;12&quot;
VERSION=&quot;12 (bookworm)&quot;
VERSION_CODENAME=bookworm
ID=debian

64bit
aarch64
python 3.11.2
</code></pre>
","2024-08-12 20:58:28","-1","Question"
"78863529","","getting unexpected keys error while loading weights","<pre><code>import torch
from PIL import Image
import numpy as np
from effdet import get_efficientdet_config, EfficientDet


config = get_efficientdet_config('tf_efficientdet_d0')
model = EfficientDet(config, pretrained_backbone=True)
model.eval()
</code></pre>
<p>when I run this I am getting the error</p>
<pre><code>Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
</code></pre>
<p>I researched some bit and got to know that this is because of timm builder but didn't find any solutions. How to fix this?</p>
<p>I wanted to load the efficientdet weights but it resulted in an unexpected keys error</p>
","2024-08-12 20:42:04","0","Question"
"78862730","78862667","","<p>You need to move the <code>x</code> tensor to the same device.</p>
<p>Change <code>x = torch.randn(1,3,32,32)</code> to <code>x = torch.randn(1,3,32,32).to(device)</code></p>
","2024-08-12 16:40:03","1","Answer"
"78862667","","pytorch - RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0","<p>When I come accross kuangliu's github repo on training cifar-10 with <code>densenet model</code>, I want to feed the model to the gpu to accelerate the training process. However, it seems like the input tensor is found on both cpi and gpu. My suspect is that there is something wrong in the custom classes' code that makes some input tensors available on the cpu.</p>
<p>Could you help me point out the problem in this case? Much appreciated</p>
<p>For convenience, the code has a <code>test()</code> function to test if the network is compiled correctly or not. Here is the code:</p>
<pre><code>import math
import torch
import torch.nn as nn
import torch.nn.functional as F


class Bottleneck(nn.Module):
    def __init__(self, in_planes, growth_rate):
        super(Bottleneck, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(4*growth_rate)
        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)

    def forward(self, x):
        out = self.conv1(F.relu(self.bn1(x)))
        out = self.conv2(F.relu(self.bn2(out)))
        out = torch.cat([out,x], 1)
        return out


class Transition(nn.Module):
    def __init__(self, in_planes, out_planes):
        super(Transition, self).__init__()
        self.bn = nn.BatchNorm2d(in_planes)
        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)

    def forward(self, x):
        out = self.conv(F.relu(self.bn(x)))
        out = F.avg_pool2d(out, 2)
        return out


class DenseNet(nn.Module):
    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):
        super(DenseNet, self).__init__()
        self.growth_rate = growth_rate

        num_planes = 2*growth_rate
        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)

        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])
        num_planes += nblocks[0]*growth_rate
        out_planes = int(math.floor(num_planes*reduction))
        self.trans1 = Transition(num_planes, out_planes)
        num_planes = out_planes

        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])
        num_planes += nblocks[1]*growth_rate
        out_planes = int(math.floor(num_planes*reduction))
        self.trans2 = Transition(num_planes, out_planes)
        num_planes = out_planes

        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])
        num_planes += nblocks[2]*growth_rate
        out_planes = int(math.floor(num_planes*reduction))
        self.trans3 = Transition(num_planes, out_planes)
        num_planes = out_planes

        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])
        num_planes += nblocks[3]*growth_rate

        self.bn = nn.BatchNorm2d(num_planes)
        self.linear = nn.Linear(num_planes, num_classes)

    def _make_dense_layers(self, block, in_planes, nblock):
        layers = []
        for i in range(nblock):
            layers.append(block(in_planes, self.growth_rate))
            in_planes += self.growth_rate
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.trans1(self.dense1(out))
        out = self.trans2(self.dense2(out))
        out = self.trans3(self.dense3(out))
        out = self.dense4(out)
        out = F.avg_pool2d(F.relu(self.bn(out)), 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out

def densenet_cifar():
    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)

def test():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    net = densenet_cifar().to(device)
    x = torch.randn(1,3,32,32)
    y = net(x)
    print(y)

test()
</code></pre>
<p>Have a great day!</p>
","2024-08-12 16:22:20","0","Question"
"78861552","78860233","","<p>I didn't read the paper in detail, but when they say <code>[w, h, f]</code> I don't think the <code>w</code> and <code>h</code> have to match the width and height of the original image. They likely just mean that if the output of your ResNet after the last Conv + Pooling layer is <code>[w, h, f]</code>, you reshape it into 2d (making it it <code>[fxh, w]</code>) and then pass it through a fully-connected layer to make it <code>f</code> dimensional.</p>
<p>Something like this</p>
<pre><code>import torch
import torch.nn as nn
import torchvision.models as models

resnet = models.resnet50(pretrained=True)

# Remove the last fully connected layer and adaptive pooling layers
resnet = torch.nn.Sequential(*list(resnet.children())[:-2])

# Dummy image of shape [1, 3, 224, 224]
image = torch.randn(1, 3, 224, 224)

intermediate_features = resnet(image)  # This will be [1, 2048, 7, 7]

batch_size, channels, h, w = intermediate_features.size()

# [1, 14336, 7] where f=14336 and w=7
reshaped_features = intermediate_features.view(batch_size, channels * h, w)

fc_layer = nn.Linear(w, 1)  # This layer reduces the w dimension to 1

final_output = fc_layer(reshaped_features)  # [1, 14336, 1]

final_output = final_output.squeeze(-1)  # [1, 14336]

print(final_output.shape)

</code></pre>
<p>(My example also has batch size as a dimension because in the real world you work with batches of examples)</p>
","2024-08-12 12:00:27","1","Answer"
"78860304","78860233","","<p>first install timm, torch python packages via pip</p>
<p>create model and load pre-trained weights</p>
<pre class=""lang-py prettyprint-override""><code>import timm
import torch
model = timm.create_model('resnet50', pretrained=True, features_only=True)

# convert image torch tensor as ( nimages, channels, height, width ) ex- (1,3,224, 224)  
features = model( image ) 
print( features.shape )

</code></pre>
<p>(1, 2048, 224, 224)</p>
","2024-08-12 06:50:40","0","Answer"
"78860233","","Using ResNet50 to create a feature tensor of [w, h, f]","<p>I'm trying to implement <a href=""https://arxiv.org/pdf/2005.13044"" rel=""nofollow noreferrer"">this paper</a> but I'm not following something in it.</p>
<p>It wants me to use ResNet50 to extract features from an image but tells me the extracted features will be of dimension [w, h, f]. Everything I'm seeing with ResNet50, though, is giving me back a tensor of [f] (as in, it turns my whole image into features and not my pixels into features)</p>
<p>Am I reading this wrong or do I just not understand what I'm supposed to be doing with ResNet50?</p>
<p>Relevant quotes from paper:
&quot;We obtain an intermediate visual feature representation Fc of size f. We use the ResNet50 [26] as our backbone convolutional architecture.&quot;</p>
<p>&quot;In a first step, the three-dimensional feature Fc is reshaped into a two-dimensional feature by keeping its width, i.e. obtaining a feature shape (f × h, w).&quot;</p>
","2024-08-12 06:27:03","-1","Question"
"78859785","78859691","","<p>Ok I was able to fix it by reinstalling the three libraries, specifying the CUDA version:</p>
<p>pip install torch==2.2.1+cu118 torchvision==0.17.1+cu118 torchaudio==2.2.1+cu118 -f <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/torch_stable.html</a></p>
<p>Close everything and rerun it from the beginning.</p>
","2024-08-12 02:04:55","1","Answer"
"78859691","","Why does Pytorch-2.21 not detecting my GPU?","<p>I got RTX 4060 with driver 560.76. I installed CUDA 11.8 and copied cuDNN 8.9.7. files to the correct directories in the CUDA installation folder. I added the right paths to the System variables Environment. Finally, created virtual environment and installed Pytorch 2.2.1 with torchaudio 2.2.1 and torchvision 0.17.1.</p>
<p>When I run this to check if Pytorch detect CUDA:</p>
<pre><code>print(torch.cuda.is_available())
</code></pre>
<p>I get FALSE.</p>
<p>Same when I run</p>
<pre><code>nvidia-smi

No running processes found 
</code></pre>
<p>In a different virtual environment, I use TensorFlow and it does defecting my GPU</p>
<p>What am I doing wrong?</p>
","2024-08-12 00:52:37","1","Question"
"78858640","78858582","","<p>I though that with <code>requirements.txt</code> was engouh, but it isn't. As @talonmies suggested, you should check PyTorch documentation: <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a></p>
<p>In my case, I ran <code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124</code> in the virtual environment.</p>
","2024-08-11 15:01:51","0","Answer"
"78858582","","How to install torch with CUDA in virtual environment (Torch not compiled with CUDA enabled)","<p>I'm trying to use the <a href=""https://github.com/fpgaminer/joytag"" rel=""nofollow noreferrer"">joytag model</a> locally. This is <code>requirements.txt</code>:</p>
<pre><code>torch&gt;=2.0.1,&lt;3.0.0
transformers&gt;=4.36.2
torchvision&gt;=0.15.2
einops&gt;=0.7.0
safetensors&gt;=0.4.1
pillow&gt;=9.4.0
</code></pre>
<p>They provide a script to run the model. I created a virtual environment to avoid conflicts, I installed the requirements, and when I run the script, I get this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\Sync2\IAImages\LoRA\Scripts\TaggingModels\joytag\joytag.py&quot;, line 13, in &lt;module&gt;
    model = model.to('cuda')
            ^^^^^^^^^^^^^^^^
  File &quot;D:\Sync2\IAImages\LoRA\Scripts\TaggingModels\joytag\venv\Lib\site-packages\torch\nn\modules\module.py&quot;, line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;D:\Sync2\IAImages\LoRA\Scripts\TaggingModels\joytag\venv\Lib\site-packages\torch\nn\modules\module.py&quot;, line 780, in _apply
    module._apply(fn)
  File &quot;D:\Sync2\IAImages\LoRA\Scripts\TaggingModels\joytag\venv\Lib\site-packages\torch\nn\modules\module.py&quot;, line 780, in _apply
    module._apply(fn)
  File &quot;D:\Sync2\IAImages\LoRA\Scripts\TaggingModels\joytag\venv\Lib\site-packages\torch\nn\modules\module.py&quot;, line 780, in _apply
    module._apply(fn)
  File &quot;D:\Sync2\IAImages\LoRA\Scripts\TaggingModels\joytag\venv\Lib\site-packages\torch\nn\modules\module.py&quot;, line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File &quot;D:\Sync2\IAImages\LoRA\Scripts\TaggingModels\joytag\venv\Lib\site-packages\torch\nn\modules\module.py&quot;, line 1160, in convert
    return t.to(
           ^^^^^
  File &quot;D:\Sync2\IAImages\LoRA\Scripts\TaggingModels\joytag\venv\Lib\site-packages\torch\cuda\__init__.py&quot;, line 305, in _lazy_init
    raise AssertionError(&quot;Torch not compiled with CUDA enabled&quot;)
AssertionError: Torch not compiled with CUDA enabled
</code></pre>
<p>I'm not very familiar with Torch and related packages, so it's hard to me to understand what I did wrong.</p>
","2024-08-11 14:34:08","0","Question"
"78857483","78114412","","<p>I encountered this problem when installing torch 2.4.0, but pip told me that torchvision 0.18.1 is incompatible with 2.4.0. So I uninstall 2.4.0 and install torch 2.3.1 , the problem disappeared.</p>
","2024-08-11 04:05:47","1","Answer"
"78857158","","'_PrefetchDataset' object has no attribute 'shape' when calling resnet_v2.preprocess_input()","<p>I keep running into this error when trying to preprocess my datasets:</p>
<blockquote>
<p>AttributeError: '_PrefetchDataset' object has no attribute 'shape'. Did you mean: 'save'?</p>
</blockquote>
<p>I'm currently using ResNet, but I've also tried the preprocess functions for MobileNet and NasNet. It almost looks like it's an issue with imagenet_utils.py, but I assume that's not the case.</p>
<p>Is there any way I can fix this other than switching to a new application?</p>
<p>This is my code:</p>
<pre class=""lang-py prettyprint-override""><code>trainingdata = keras.utils.image_dataset_from_directory(
    directory='archive/chest_xray/train/',
    labels='inferred',
    label_mode='int',
    batch_size=32,
    image_size=(512,512)
)

validationdata = keras.utils.image_dataset_from_directory(
    directory='archive/chest_xray/test/',
    labels='inferred',
    label_mode='int',
    batch_size=32,
    image_size=(512,512)
)

td = keras.applications.resnet_v2.preprocess_input(trainingdata)
</code></pre>
<p>Here is my full traceback:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\You\Documents\keras models\PNEUMONIA.PY&quot;, line 25, in &lt;module&gt;
    td = keras.applications.resnet_v2.preprocess_input(trainingdata)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\You\Documents\keras models\.venv\Lib\site-packages\keras\src\applications\resnet_v2.py&quot;, line 134, in preprocess_input
    return imagenet_utils.preprocess_input(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\You\Documents\keras models\.venv\Lib\site-packages\keras\src\applications\imagenet_utils.py&quot;, line 106, in preprocess_input
    return _preprocess_tensor_input(x, data_format=data_format, mode=mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\You\Documents\keras models\.venv\Lib\site-packages\keras\src\applications\imagenet_utils.py&quot;, line 254, in _preprocess_tensor_input
    ndim = len(x.shape)
               ^^^^^^^
AttributeError: '_PrefetchDataset' object has no attribute 'shape'. Did you mean: 'save'?
</code></pre>
","2024-08-10 22:41:18","1","Question"
"78856970","78697900","","<p>Try installing PyTorch 2.3.1</p>
<pre><code>!pip install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre>
<p>Then</p>
<pre><code>!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/repo.html
</code></pre>
<p>Restart the session after installation has finished.</p>
","2024-08-10 20:23:24","0","Answer"
"78855496","78855421","","<p>You have to change <code>X_train_tensor[:,0]</code> to <code>X_train_tensor</code></p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn as nn

class PINNFP(nn.Module):
    def __init__(self):
        super().__init__()
        self.manual_layers = nn.Sequential(
            nn.Linear(in_features = 3, out_features = 5),
            nn.Linear(in_features = 5, out_features = 5),
            nn.Linear(in_features = 5, out_features = 5),
            nn.Linear(in_features = 5, out_features = 5),
            nn.Linear(in_features = 5, out_features = 2))

    def forward(self, x):
        return self.manual_layers(x)

model_1 = PINNFP()

X_train_tensor = torch.randn(8, 3, requires_grad=True)

train_output = model_1(X_train_tensor)
dphidx = torch.autograd.grad(train_output[:,0], X_train_tensor, torch.ones_like(train_output[:,0]))[0]
</code></pre>
<p>Slicing creates a new tensor with a new computational graph. So the tensor <code>X_train_tensor[:,0]</code> is actually part of a new computational graph stemming directly from <code>X_train_tensor</code>. This means there's no path from <code>train_output[:,0]</code> to <code>X_train_tensor[:,0]</code>. What you can do instead is backprop from <code>train_output[:,0]</code> to <code>X_train_tensor</code> and take a slice of the gradient.</p>
","2024-08-10 07:50:06","0","Answer"
"78855421","","I am using torch.autograd.grad(). But it keeps throwing runtime error saying that differentiated tensor is out of the computational graph","<p>I am writing code for a PINN model. While calculating the gradients for the loss PDE, I used <code>torch.autograd.grad()</code>. But it is showing</p>
<blockquote>
<p>RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.</p>
</blockquote>
<p>for the line</p>
<pre><code>dphidx = torch.autograd.grad(train_output[:, 0], X_train_tensor[:,0], torch.ones_like(train_output[:, 0]), create_graph=True)[0]
</code></pre>
<p>I checked both <code>train_output[:, 0]</code> and <code>X_train_tensor[:,0]</code> are true for <code>gradient_requres(True)</code>. Now I am confused about what is wrong here.</p>
<p>I am attaching the model's code for clarity:</p>
<pre><code>import torch.nn as nn

class PINNFP(nn.Module):
    def __init__(self):
        super().__init__()
        self.manual_layers = nn.Sequential(
            nn.Linear(in_features = 3, out_features = 5),
            nn.Linear(in_features = 5, out_features = 5),
            nn.Linear(in_features = 5, out_features = 5),
            nn.Linear(in_features = 5, out_features = 5),
            nn.Linear(in_features = 5, out_features = 2))

    def forward(self, x):
        return self.manual_layers(x)

model_1 = PINNFP()

train_output = model_1(X_train_tensor)
dphidx = torch.autograd.grad(train_output[:, 0], X_train_tensor[:,0], torch.ones_like(train_output[:, 0]), create_graph=True)[0]
</code></pre>
<p>How can I fix this error? I used <code>allow_unused=None</code>, and in that case, I get none value as the gradient which I do not want.</p>
","2024-08-10 07:01:38","0","Question"
"78854175","78853963","","<p>When you compute:</p>
<pre class=""lang-py prettyprint-override""><code>  with torch.no_grad():
    w = w - w.grad * 1e-5
    b = b - b.grad * 1e-5
    w.grad = None
    b.grad = None
</code></pre>
<p>You are creating a new object <code>w</code> that is different from the old object <code>w</code>. The new <code>w</code> is created under <code>torch.no_grad()</code>, so the new <code>w</code> doesn't have a gradient. This causes the error.</p>
<p>The solution is to update <code>w</code> and <code>b</code> with an in-place operation. There are several ways to do this:</p>
<pre class=""lang-py prettyprint-override""><code>with torch.no_grad():
    for param in [w, b]:
        
        # Option 1: in-place subtraction 
        param.sub_(param.grad*1e-2)
        
        # Option 2: assign new values to the .data attribute 
        param_new = param - param.grad * 1e-2
        param.data = param_new.data
        
        # Option 3: use the in-place copy 
        param_new = param - param.grad * 1e-2
        param.copy_(param_new)
        
        # Finally, set grad to None
        param.grad = None
</code></pre>
<p>Any one of those options will work</p>
","2024-08-09 18:27:56","1","Answer"
"78853963","","pytorch error: element 0 of tensors does not require grad and does not have a grad_fn","<p>Trying to get used to pytorch syntax. Wrote a simple linear regression program.</p>
<pre><code># weights and biases
w = torch.randn(2,3, requires_grad=True)
b = torch.randn(2, requires_grad=True)
print(w.dtype, b.dtype)

# a linear function
def model(x):
  return x @ w.t() + b

# loss function
def mse(t1, t2):
  diff = (t1-t2)**2
  return torch.mean(diff)

# do this in a loop
for _ in range(100):
  preds = model(inputs_t)
  loss = mse(targets_t, preds)
  loss.backward()
  # print(loss)
  print(w.grad, b.grad)
  print(&quot;==============&quot;)
  with torch.no_grad():
    w = w - w.grad * 1e-5
    b = b - b.grad * 1e-5
    w.grad = None
    b.grad = None
</code></pre>
<p>Manually trying to zero the gradients of each variable since I read that pytorch stores all the past gradients inside the .grad variable. But I have this error:
<code>element 0 of tensors does not require grad and does not have a grad_fn</code></p>
<p>This is the output. It runs for one loop and then stops:</p>
<pre><code>tensor([[  4958.0928,   6618.7686,   7047.0444],
        [-21244.5488, -32400.4414, -35345.9766]]) tensor([  58.3144, -298.9364])
==============

---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-122-eef55c1b4837&gt; in &lt;cell line: 16&gt;()
     20   #   b.grad.zero_()
     21   loss = mse(targets_t, preds)
---&gt; 22   loss.backward()
     23   # print(loss)
     24   print(w.grad, b.grad)

2 frames

/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py in _engine_run_backward(t_outputs, *args, **kwargs)
    742         unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
    743     try:
--&gt; 744         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    745             t_outputs, *args, **kwargs
    746         )  # Calls into the C++ engine to run the backward pass

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>what should i change in this code to remove this error?</p>
<p>I tried searching the error but everyone else is either using more complex problems or its a version problem for most. I dont really understand how to interpret those posts since I am using whatever default google colabs uses for pytorch.</p>
","2024-08-09 17:16:11","0","Question"
"78853902","78853589","","<p>If <code>pred</code> is already a probability vector, you can simply take the log of it to get log-probs</p>
<pre class=""lang-py prettyprint-override""><code>import torch
probs = torch.tensor([.1, .7, .2])
log_probs = probs.log()
</code></pre>
<p>The <code>log_softmax</code> function computes the softmax value in log space. If <code>pred</code> is already a probability vector (ie you have already applied a softmax), computing <code>log_softmax</code> would apply the softmax again which you don't want to do.</p>
","2024-08-09 17:01:08","0","Answer"
"78853589","","A problem related to torch.nn.functional.logsoftmax","<p>I'm trying to calculate negative log-likelihood for evaluation, 'pred' is already a probability vector, and by some reasons I'm trying to get log(pred) by<br />
<code>log_probs = F.log_softmax(pred) </code>
but the numerical result is very wired</p>
<p>after I print it out
<code>print(f'what does pred look like: {pred}') print(f'what does log_probs look like: {log_probs}') </code></p>
<p>it shows that:
what does log_probs look like: tensor([[-2.4611, -2.4611, -2.4611, -2.4611, -2.4611, -2.4611, -2.4611, -2.4611,
-1.4612, -2.4611]], device='cuda:0')
what does pred look like: tensor([[3.5614e-05, 1.0131e-08, 5.1123e-10, 5.1686e-10, 2.9131e-09, 8.3620e-11,
3.1788e-09, 4.6923e-10, 9.9996e-01, 3.9455e-08]], device='cuda:0')
obviously wrong(the second dimension should be near to 0), where goes wrong?</p>
","2024-08-09 15:31:24","0","Question"
"78853571","","Can't export standard torchvision ResNet50 model into ONNX file","<p>I made very simple python script which loads <strong>torchvision ResNet50</strong> model and tries to export to onnx file in two ways (<strong>torch.onnx.export</strong> and <strong>torch.onnx.dynamo_export</strong>)</p>
<pre><code>import torch
import torch.onnx

import torchvision

torch_model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2( weights='DEFAULT')
torch_model.eval()
torch_input = torch.randn(1, 3, 32, 32)

is_dynamo_export = False

if (is_dynamo_export):
    onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)
    onnx_program.save(&quot;onnx_dynamo_export_ResNET50.onnx&quot;)        
else:
    torch.onnx.export(torch_model,               # model being run
                      torch_input,                         # model input (or a tuple for multiple inputs)
                      &quot;onnx_export_ResNET50.onnx&quot;,   # where to save the model (can be a file or file-like object)
                      export_params=True,        # store the trained parameter weights inside the model file
                      opset_version=10,          # the ONNX version to export the model to
                      do_constant_folding=True,  # whether to execute constant folding for optimization
                      input_names = ['input'],   # the model's input names
                      output_names = ['output'], # the model's output names
                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes
                                    'output' : {0 : 'batch_size'}})  
</code></pre>
<p>The errors were appeared:</p>
<pre><code>File &quot;C:\tools\Python311\Lib\site-packages\torch\onnx\_internal\exporter.py&quot;, line 1439, in dynamo_export
  raise OnnxExporterError(

torch.onnx.OnnxExporterError: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues
</code></pre>
<pre><code>torch.onnx.errors.SymbolicValueError: Unsupported: ONNX export of Pad in opset 9. The sizes of the padding must be constant. Please try opset version 11. [Caused by the value '535 defined in (%535 : int[] = prim::ListConstruct(%405, %534, %405, %533, %405, %532), scope: torchvision.models.detection.faster_rcnn.FasterRCNN::
</code></pre>
<p>Both methods works good with extremally simple models such as</p>
<pre><code>class MyModel(nn.Module):

    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
</code></pre>
","2024-08-09 15:25:36","0","Question"
"78853010","78853009","","<p>One way that I have worked out is the following, assuming a model called <code>model</code>:</p>
<pre class=""lang-py prettyprint-override""><code># fill model parameters with all ones
fill_value = 1.0

# loop over all parameters
for p in model.parameters():
    if p.requires_grad:  # only change learnable parameters
        with torch.no_grad():
            p.fill_(fill_value)  # in-place fill
            # p.zero_()  # this can be used if setting every parameter to zero
</code></pre>
<p>There may be a better way.</p>
<h4>Update</h4>
<p>I also found I needed to reset buffers (e.g., for <a href=""https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d"" rel=""nofollow noreferrer""><code>BatchNorm2d</code></a> modules within the models), so I have done:</p>
<pre><code>def fill_buffers(model, value=0.0):
    for b in model.buffers():
        b.fill_(value)

# fill buffers with zeros
fill_buffers(model, 0.0)
</code></pre>
","2024-08-09 13:21:15","1","Answer"
"78853009","","How to manually set all learnable parameters in PyTorch model to a fixed value","<p>For some testing purposes, I would like to manually set all the learnable parameters of a PyTorch <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"" rel=""nofollow noreferrer""><code>torch.nn.Module</code></a> model to a fixed value (I'm comparing two models that should be the same, but they seem to train considerably differently, so I want to sanity check the <code>forward</code> method). Is there a simple way to do this?</p>
","2024-08-09 13:21:15","2","Question"
"78850055","78846318","","<p>You can do that using this code snippet below:</p>
<pre><code>import numpy as np
import torch


dic_items = {1: &quot;A&quot;, 2: &quot;B&quot;, 3: &quot;C&quot;}
tensor_nos = torch.tensor([[1, 2, 3], [3, 2, 1]])
tensor_numpy = tensor_nos.numpy()
vectorize_map = np.vectorize(dic_items.get)(tensor_numpy)

results = vectorize_map.tolist()

# Print the result
print(results)
</code></pre>
","2024-08-08 19:07:48","0","Answer"
"78850045","78846318","","<pre><code># Define the dictionary and tensor
Dict = {1: 'A', 2: 'B', 3: 'C'}
ex = torch.tensor([[1,2,3],[3,2,1]])

# Map the dictionary to the tensor
mapped_result = [[Dict[item.item()] for item in row] for row in ex]

# Convert to a string tensor if you want a list of lists
print(mapped_result)
</code></pre>
<p>Convert the list of lists back to a tensor if needed (as strings are not natively supported in PyTorch tensors, you might need to use a different structure like a list of lists or NumPy array if you want to store strings). But if you check the data type, it gives you dtype('U1') when using numpy array.</p>
","2024-08-08 19:04:46","0","Answer"
"78849793","78843331","","<p>The problem is the line <code>return torch.tensor([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2])</code></p>
<p>When you pass those values to <code>torch.tensor</code>, you are constructing an entirely new tensor object that has no relationship to the input values. This means you can't backprop through the new tensor to your <code>params</code>.</p>
<p>You need to compute the output with torch operations.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

params = nn.Parameter(torch.tensor([1.1, .4, .1, .4], dtype = torch.float64))

y0 = torch.tensor([1., 2.], dtype = torch.float64)

y1, y2 = y0
a, b, c, d = params

torch.tensor([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2]).requires_grad
&gt; False

torch.stack([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2]).requires_grad
&gt; True
</code></pre>
","2024-08-08 17:55:29","0","Answer"
"78849416","78429681","","<p>Are you using a sigmoid function before applying BCELoss? If not, try using BCEWithLogitsLoss, as that is capable of accepting the raw output from your neural network. BCELoss is expecting probabilities, not logits.</p>
","2024-08-08 16:07:34","0","Answer"
"78848646","78839246","","<p>It's quite obvious from the error that <code>PrivacyEngine</code> doesn't take <code>batch_size</code> as a parameter. Looking at the <a href=""https://opacus.ai/api/privacy_engine.html#opacus.privacy_engine.PrivacyEngine"" rel=""nofollow noreferrer"">docs</a>, you should do something like</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from opacus import PrivacyEngine

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.fc1 = nn.Linear(32*26*26, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(-1, 32*26*26)
        x = self.fc1(x)
        return torch.log_softmax(x, dim=1)

transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

model = SimpleCNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)

privacy_engine = PrivacyEngine()

model, optimizer, train_loader = privacy_engine.make_private(
    module=model,
    optimizer=optimizer,
    data_loader=train_loader,
    max_grad_norm=1.0,
    noise_multiplier=1.1,
)

criterion = nn.NLLLoss()
model.train()
for epoch in range(1):
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

epsilon = privacy_engine.get_epsilon(1e-5)
print(f&quot;Epsilon: {epsilon}, Delta: 1e-5&quot;)

</code></pre>
","2024-08-08 13:15:41","1","Answer"
"78847486","78846880","","<p>First use the tokenizer to add new tokens and then resize the token embedings of the model:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = &quot;your-model-name&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

new_tokens = [&quot;new_token1&quot;, &quot;new_token2&quot;]
tokenizer.add_tokens(new_tokens)
model.resize_token_embeddings(len(tokenizer))
</code></pre>
<p>Then, freeze all parameters except for the embeddings of the new tokens</p>
<pre><code>for param in model.parameters():
    param.requires_grad = False

# Get the new embedding indices
new_token_ids = tokenizer.convert_tokens_to_ids(new_tokens)
new_embeddings = model.get_input_embeddings().weight.data[new_token_ids]

# Unfreeze the new token embeddings
for idx in new_token_ids:
    model.get_input_embeddings().weight.data[idx].requires_grad = True
</code></pre>
<p>Finally, implement the trainign loop, ensuring only the new embeddings are updated:</p>
<pre><code>from torch.optim import AdamW
from torch.nn import functional as F

optimizer = AdamW(model.get_input_embeddings().parameters(), lr=1e-4)

for epoch in range(num_epochs):
    for batch in dataloader:
        inputs = batch[&quot;input_ids&quot;]
        labels = batch[&quot;labels&quot;]

        optimizer.zero_grad()
        outputs = model(inputs, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
</code></pre>
<p>This way you avoid the inefficiency of reassigning weights and you can straightforwardly apply it across different models.</p>
","2024-08-08 09:05:40","2","Answer"
"78847305","78845851","","<p><code>torch.multiprocessing</code> does exactly what you need. You can rewrite the <code>train</code> function to the below:</p>
<pre><code>def inner_train(model, optimizer, num_iterations, criterion, data, targets):
    for _ in range(num_iterations):
        output = model(data) 
        loss = criterion(output, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
def train(num_networks, network_size, num_iterations):
    criterion = torch.nn.BCELoss()
    data = torch.zeros((5, 2), device='cuda')
    targets = torch.ones((5, 1), device='cuda')
    
    models = []
    optimizers = []
    for _ in range(num_networks):
        model = MLP(network_size).cuda()
        models.append(model)

        optimizer = torch.optim.Adam(model.parameters())
        optimizers.append(optimizer)

    num_processes = num_networks
    processes = []
    for model, optimizer in zip(models, optimizers):
        p = mp.Process(target=inner_train, args=(model, optimizer, num_iterations, criterion, data, targets,))
        p.start()
        processes.append(p)
    for p in processes:
        p.join()
</code></pre>
<p>The output time should looks something like this:</p>
<pre><code>Training 1 model took 0.07s
Training 5 models took 0.31s
Training 15 models took 0.88s
</code></pre>
","2024-08-08 08:27:24","1","Answer"
"78846880","","huggingface train only new tokens embedding","<p>I want to add new tokens to a huggingface model and train only their embeddings. How can I do that?</p>
<p>There are some ways to train only part of a weight tensor (e.g. <a href=""https://discuss.pytorch.org/t/how-can-i-freeze-weights-in-element-wise/117988"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/how-can-i-freeze-weights-in-element-wise/117988</a>), however they seem to have access and edit the model class, which probably requires a lot of work for every specific LLM I will work on, and I'm intending to do this training to a lot of different models.</p>
<p>Another way is to train all the model and after every step reassign all the weights except of the embeddings of the new tokens, which sounds inefficient and can even be just incorrect for certain optimizers (as explained in the above link).</p>
","2024-08-08 06:47:30","1","Question"
"78846318","","How can I change the values in the tensor using dictionary?","<p>when I have a tensor and dictionary as below, how can I map the dictionary to the tensor?<br />
For example,</p>
<pre class=""lang-py prettyprint-override""><code>Dict = {1: 'A', 2: 'B', 3: 'C'}

ex = torch.tensor([[1,2,3],[3,2,1]])

# Expected result
#tensor([[A, B, C],
#        [C, B, A]])
</code></pre>
<p>I tried this <a href=""https://discuss.pytorch.org/t/mapping-values-in-a-tensor/117731"" rel=""nofollow noreferrer"">code</a> and <code>torch.where</code>, but it didn't work well.</p>
","2024-08-08 02:30:15","0","Question"
"78845917","78845851","","<p>here is a fix for you <code>train</code> function. Idea is to do single backward pass for all models in one go. However there still will be some delays in execution because cuda is synchronizing with the python thread after calling each model.</p>
<pre><code>
class AllModels(nn.Module):
    def __init__(self, n_models, network_size):
        super().__init__()
        self.models = nn.ModuleList()
        for _ in range(n_models):
          model = MLP(network_size)
          self.models.append(model)

    def forward(self, x):
        return torch.concat([model.forward(x) for model in self.models])


def train(num_networks, network_size, num_iterations):
    criterion = torch.nn.BCELoss()
    data = torch.zeros((5, 2), device='cuda')
    targets = torch.ones((5, 1), device='cuda')

    model = AllModels(num_networks, network_size).cuda()

    optimizer = torch.optim.Adam(model.parameters())
    for _ in range(num_iterations):
      loss = 0
      output = model(data)
      loss = criterion(output, targets.repeat(num_networks, 1))
      loss.backward() # 3.29
      optimizer.zero_grad()
      optimizer.step() #0.097

</code></pre>
","2024-08-07 22:34:36","0","Answer"
"78845851","","Quickly training multiple small neural networks","<p>I want to train multiple models on the same training data (just different initializations). Each model has exactly the same architecture. Crucially, the models are extremely small, so having them all in memory at the same time won't be a problem. How can I make this as efficient as possible?</p>
<p>I read that cuda is supposed to be non-blocking, so computations should be parallelized automatically. However, when I write the code in a naive way, I can see that the training time scales linearly with the number of models.</p>
<pre><code>import time
import numpy as np
import torch
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, network_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(2, network_size)
        self.fc2 = nn.Linear(network_size, 1)

    def forward(self, x):
        return torch.sigmoid(self.fc2(self.fc1(x)))

def train(num_networks, network_size, num_iterations):
    criterion = torch.nn.BCELoss()
    data = torch.zeros((5, 2), device='cuda')
    targets = torch.ones((5, 1), device='cuda')
    
    models = []
    for _ in range(num_networks):
        models.append(MLP(network_size).cuda())
    for model in models:
        optimizer = torch.optim.Adam(model.parameters())
        for _ in range(num_iterations):
            output = model(data)
            loss = criterion(output, targets)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

training_start = time.perf_counter()
train(1, 20, 1000)
print(f&quot;Training 1 model took {time.perf_counter() - training_start:.2f}s&quot;)

training_start = time.perf_counter()
train(5, 20, 1000)
print(f&quot;Training 5 models took {time.perf_counter() - training_start:.2f}s&quot;)

training_start = time.perf_counter()
train(15, 20, 1000)
print(f&quot;Training 15 models took {time.perf_counter() - training_start:.2f}s&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Training 1 model took 0.68s
Training 5 models took 3.36s
Training 15 models took 10.18s
</code></pre>
<p>I've been able to optimize this by merging the models into one larger network architecture:</p>
<pre><code>class MergedMLP(nn.Module):
    def __init__(self, num_networks, network_size):
        super().__init__()
        self.fc1 = nn.Linear(2, num_networks * network_size, device='cuda')
        self.fc2 = nn.Linear(num_networks * network_size, num_networks, device='cuda')
        
        self.fc2_weight_mask = torch.zeros_like(self.fc2.weight.data, device='cuda', requires_grad=False)
        for i in range(num_networks):
            self.fc2_weight_mask[i,i*network_size:(i+1)*network_size] = 1
        self.fc2.weight.data *= self.fc2_weight_mask

    def forward(self, x):
        return torch.sigmoid(self.fc2(self.fc1(x)))
    
def train_merged(num_networks, network_size, num_iterations):
    criterion = torch.nn.BCELoss()
    data = torch.zeros((5, 2), device='cuda')
    targets = torch.ones((5, num_networks), device='cuda')
    
    model = MergedMLP(num_networks, network_size).cuda()
    optimizer = torch.optim.Adam(model.parameters())
    for _ in range(num_iterations):
        output = model(data)
        loss = criterion(output, targets)
        optimizer.zero_grad()
        loss.backward()
        model.fc2.weight.grad *= model.fc2_weight_mask
        optimizer.step()
        
training_start = time.perf_counter()
train_merged(15, 20, 1000)
print(f&quot;Training merged models took {time.perf_counter() - training_start:.2f}s&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Training merged models took 0.70s
</code></pre>
<p>Is it possible to achieve the runtime of the merged implementation with code more similar to the naive one? The merged implementation seems to be more error prone, e.g. when I increase the network size or want to extract individual networks from the trained merged model.</p>
","2024-08-07 22:09:07","0","Question"
"78845251","78820055","","<blockquote>
<p>Why would anyone want a different mask for different heads?</p>
</blockquote>
<p>Because we want each head to capture different <strong>contextual relations</strong> between the elements in a sequence (ex: words in a sentence). For example, if the input sentence is <em>&quot;The animal didn't cross the street because it was too wide&quot;</em>. Then head-1 might give more attention to the words <em>&quot;animal&quot;</em> and <em>&quot;it&quot;</em> and head-2 might give more attention to the words <em>&quot;street&quot;</em> and <em>&quot;it&quot;</em>.</p>
<blockquote>
<p>If we have to do it this way anyway, how are they ordered? i.e. is it (Example1, Head1), (Example1, Head2),... etc OR is it (Example1, Head1), (Example2, Head1),... ? This is also asked in the comments at this question.</p>
</blockquote>
<p><strong>Each example</strong> in a batch is processed by <strong>all the heads</strong>. Therefore, order doesn't matter. Suppose the batch size is 1, then it is like (example-1,head-1), (example-1,head-2)... (example-1,head-n) for the first iteration. It will change to example-2 in the next iteration and so on. If the batch size is two, it can also be processed <strong>in parallel</strong> (independently), in that case, it would be (example-1,head-1),(example-2,head-1)</p>
","2024-08-07 18:46:26","0","Answer"
"78845146","78826831","","<p>Did you use the <code>nvidia-smi</code> command? If your code resides in the GPU, then the command would display the information with the process ID and the memory occupied by the model as shown here,
<a href=""https://i.sstatic.net/HlqJHwwO.png"" rel=""nofollow noreferrer"">sample output of nvidia-smi command</a></p>
","2024-08-07 18:12:56","0","Answer"
"78844884","78827482","","<p>Before loading from pretrained model set transformers logger level to error as shown below. It sure is really frustrating not being able to leverage the <code>warnings</code> library filter</p>
<pre><code>    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
    for logger in loggers:
        if &quot;transformers&quot; in logger.name.lower():
            logger.setLevel(logging.ERROR)

    # now you can load state dict from pretrained
    model = transformers.BertModel.from_pretrained(
        &quot;bert-base-uncased&quot;,
        use_safetensors=True,
        return_dict=False,
        attn_implementation=&quot;sdpa&quot;,
    )
</code></pre>
","2024-08-07 16:54:15","2","Answer"
"78843773","78687946","","<p>Tracking from &quot;sort.py&quot; file is CPU based. Instead, you can use &quot;model.track&quot; from Ultralytics, which will significantly reduce CPU usage.</p>
","2024-08-07 12:43:20","0","Answer"
"78843449","78780676","","<p>I just realized that there was a transformation from Albumentation related to pixels augmentation. That's why it was not possible to do a simple normalization, since some other transformations related to colors were already applied.</p>
","2024-08-07 11:33:48","0","Answer"
"78843331","","Parameters not changing while applying Pytorch Minimization fucntion","<p>Code for getting the data:</p>
<pre><code>import pandas as pd
import torch

dataset = pd.read_csv('/kaggle/input/fish-bear/population_data.csv')
years = torch.tensor(dataset['year'], dtype = torch.float64)
fish_pop = torch.tensor(dataset['fish_hundreds'], dtype = torch.float64)
bears_pop = torch.tensor(dataset['bears_hundreds'], dtype = torch.float64)
pop = torch.cat((fish_pop.reshape((51, 1)), bears_pop.reshape((51, 1))), 1)
</code></pre>
<p>Ordinary Differential Equation Solver</p>
<pre><code>from typing import List, Callable, Sequence, NamedTuple, Union

class _Tableau(NamedTuple):

    c: List[float]
    b: List[float]
    a: List[List[float]]


rk4_tableau = _Tableau(c=[0.0, 0.5, 0.5, 1.0],
                       b=[1 / 6., 1 / 3., 1 / 3., 1 / 6.],
                       a=[[0.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.0, 0.0],
                          [0.0, 0.5, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]])


def explicit_rk(tableau: _Tableau, fcn: Callable[..., torch.Tensor],
                y0: torch.Tensor, t: torch.Tensor,
                params: Sequence[torch.Tensor]):
    c = tableau.c
    a = tableau.a
    b = tableau.b
    s = len(c)
    nt = len(t)

    # set up the results list
    yt_lst: List[torch.Tensor] = []
    yt_lst.append(y0)
    y = yt_lst[-1]
    for i in range(nt - 1):
        t0 = t[i]
        t1 = t[i + 1]
        h = t1 - t0
        ks: List[torch.Tensor] = []
        ksum: Union[float, torch.Tensor] = 0.0
        for j in range(s):
            if j == 0:
                k = fcn(y, t0, params)
            else:
                ak: Union[float, torch.Tensor] = 0.0
                aj = a[j]
                for m in range(j):
                    ak = aj[m] * ks[m] + ak
                k = fcn(h * ak + y, t0 + c[j] * h, params)
            ks.append(k)
            ksum = ksum + b[j] * k
        y = h * ksum + y
        yt_lst.append(y)
    yt = torch.stack(yt_lst, dim=0)
    return yt

def rk4_ivp(fcn: Callable[..., torch.Tensor], y0: torch.Tensor, t: torch.Tensor,
            params: Sequence[torch.Tensor], **kwargs):
    return explicit_rk(rk4_tableau, fcn, y0, t, params)
</code></pre>
<p>Minimization Code:</p>
<pre><code>import torch

def lotka_volterra(y, t, params):
    y1, y2 = y
    a, b, c, d = params

    return torch.tensor([a * y1 - b * y1 * y2, c * y2 * y1 - d * y2])

def loss_function(params):

    y0 = torch.tensor([fish_pop[0], bears_pop[0]], dtype = torch.float64)

    t = torch.linspace(years[0], years[-1], len(years), dtype = torch.float64)

    output = rk4_ivp(lotka_volterra, y0, t, params)

    loss = torch.sum((output - pop)**2)
    loss.requires_grad = True
    return loss

def minimize(loss_function, initial_parameters: torch.Tensor):
    list_params = []
    params = initial_parameters
    params.requires_grad = True
    optimizer = torch.optim.SGD([params], lr=0.5)

    for i in range(5):
        optimizer.zero_grad()
        loss: torch.Tensor = loss_function(params)
        loss.backward()
        optimizer.step()
        list_params.append(params.detach().clone())

    return params, list_params

starting_point = torch.nn.Parameter(torch.tensor([1.1, .4, .1, .4], dtype = torch.float64))
minimized_params, list_of_params = minimize(loss_function, starting_point)

loss_function(minimized_params), minimized_params
</code></pre>
<p>At the end of iteration the parameters do not get optimised and return as it is.</p>
<p>Result:</p>
<pre><code>(tensor(118.6865, dtype=torch.float64, requires_grad=True),
 Parameter containing:
 tensor([1.1000, 0.4000, 0.1000, 0.4000], dtype=torch.float64,
        requires_grad=True))
</code></pre>
<p>Kaggle Notebook Link: <a href=""https://www.kaggle.com/code/rakshitsingh421/parameter-estimation/edit"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/rakshitsingh421/parameter-estimation/edit</a></p>
<p>I tried to change requires_grad attributes but it didn't worked.</p>
","2024-08-07 11:04:40","0","Question"
"78842720","78114412","","<p>Install this <a href=""https://www.dllme.com/dll/files/libomp140_x86_64/00637fe34a6043031c9ae4c6cf0a891d/download"" rel=""nofollow noreferrer"">dll</a> file and paste it into system32, that should work.</p>
<p>Then, go to cmd, type <code>python</code>, and hit Enter. Once you are in the Python shell, cross check with the commands:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; print(torch.__version__)
//output
2.4.0+cpu
&gt;&gt;&gt;
</code></pre>
","2024-08-07 08:52:46","0","Answer"
"78841779","78841736","","<p>Normally, if you want to host a web application that can store large binary file, you would need a <a href=""https://en.wikipedia.org/wiki/Object_storage"" rel=""nofollow noreferrer"">blob storage</a> database. There are various enterprise solutions like <a href=""https://aws.amazon.com/s3/"" rel=""nofollow noreferrer"">AWS S3</a>, <a href=""https://azure.microsoft.com/en-us/products/storage/blobs"" rel=""nofollow noreferrer"">Azure Blob Storage</a>, Google Cloud that provides infrastructure to store your data that you can just load  to use when you needed. However, this might be a bit costly and also not too efficient when the model size is too large.</p>
<p>Another alternative that I can think of is having a separate running backend server that has readily access to the model and can accept incoming requests from the frontend server containing user input parameters and then use the input parameters to generate desired output as a response to frontend server request. That way, you don't need to load the model everytime users visit your web application route and instead forward the request to the server that already has the model loaded to handle the request. In docker terms, It would mean you need to create a separate image to load the model and handle frontend incoming requests and have its running container exposed to the image that is running the flask web application server so that it can forward requests and receive response from that container</p>
","2024-08-07 04:09:16","0","Answer"
"78841736","","How to best integrate ML models to web application?","<p>I have no experience in ML and am a beginner in web application programming. I currently have a flask app that uses audiocraft and a classificatoin model. Currently i have these stored locally within the application folder. I have built an image of this app and it turned out to be 7GB.</p>
<p>Is there a way to have these models/ framework stored somewhere else and only referenced when I need them?</p>
<p>Also, when i run the container on docker, it takes around 10minuts to generate a 8s audio from audiocraft. What would you recommend I do to fasten the process?</p>
<p>music_generation\routes.py (snippet)</p>
<pre><code>def load_model():
    model = MusicGen.get_pretrained('facebook/musicgen-small')
    return model

def generate_music_tensors(description, duration: int):
    model = load_model()
    model.set_generation_params(
        use_sampling=True,
        top_k=250,
        duration=duration
    )
    output = model.generate(
        descriptions=[description],
        progress=True,
        return_tokens=True
    )
    return output[0]

@music_generation_bp.route('/', methods=['POST'])
def generate_music():
    data = request.json

    description = data.get('description')
    duration = data.get('duration', 8)  # Default to 8 seconds if not provided
    print(&quot;Description:&quot;, description)
    print(&quot;Duration:&quot;, duration)

    if not description:
        return jsonify({'error': 'Description is required'}), 400
    # Generate unique key for the user
    user_id = str(uuid.uuid4())  # or use a user ID from your authentication system
    audio_key_prefix = f&quot;generated_music_{user_id}_{description}&quot;

    # Generate music tensors
    music_tensors = generate_music_tensors(description, duration)
    print(&quot;Music Tensors: &quot;, music_tensors)
...

</code></pre>
<p>image_classification\routes.py  (snippet)</p>
<pre><code># Load the pre-trained model
model_path = os.path.join(MODELS_DIR, 'multi_output_model.h5')
model = tf.keras.models.load_model(model_path)

@image_classification_bp.route('/', methods=['POST'])
@limiter.limit(&quot;1/minute&quot;)
def classify_image():
     if 'file' not in request.files:
         return jsonify({&quot;error&quot;: &quot;No file part in the request&quot;}), 400

     file = request.files['file']

     if file.filename == '':
         return jsonify({&quot;error&quot;: &quot;No selected file&quot;}), 400

    #file = os.path.join(TEST_IMG_DIR, 'blue-dress2.png')

    if file:
        # Read the image file
        img = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_UNCHANGED)
        img = cv2.resize(img, (IMAGE_DIMS[1], IMAGE_DIMS[0]))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = preprocess_input(img)
        img = np.expand_dims(img, axis=0)

        # Perform prediction
        predictions = model.predict(img)

</code></pre>
","2024-08-07 03:39:15","1","Question"
"78839246","","PyTorch and Opacus for Differential Privacy","<p>When testing an example code from the <strong>TensorFlow</strong> website using <strong>Jupyter Notebook</strong>, which is available <a href=""https://www.tensorflow.org/responsible_ai/privacy/tutorials/classification_privacy"" rel=""nofollow noreferrer"">here</a>, I encountered an error. You can find my SO question about that error <a href=""https://stackoverflow.com/q/78836989/5029509"">here</a>.</p>
<p>As a result, I decided to write equivalent implementations for the same functionality using <strong>PyTorch</strong> with <strong>Opacus</strong> and <strong>PySyft</strong>. However, I unfortunately encountered another error.</p>
<p>Below is the code for implementing the same functionality of the example code from the TensorFlow website, but using PyTorch with Opacus and PySyft, along with the error message.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from opacus import PrivacyEngine

# Define a simple model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.fc1 = nn.Linear(32*26*26, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(-1, 32*26*26)
        x = self.fc1(x)
        return torch.log_softmax(x, dim=1)

# Data loaders
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Initialize model, optimizer, and loss function
model = SimpleCNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.NLLLoss()

# Initialize PrivacyEngine
privacy_engine = PrivacyEngine(
    model,
    batch_size=64,
    sample_size=len(train_loader.dataset),
    epochs=1,
    max_grad_norm=1.0,
)

privacy_engine.attach(optimizer)

# Training loop
model.train()
for epoch in range(1):
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

# Print privacy statistics
epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(1e-5)
print(f&quot;Epsilon: {epsilon}, Delta: 1e-5&quot;)
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[1], line 32
     29 criterion = nn.NLLLoss()
     31 # Initialize PrivacyEngine
---&gt; 32 privacy_engine = PrivacyEngine(
     33     model,
     34     batch_size=64,
     35     sample_size=len(train_loader.dataset),
     36     epochs=1,
     37     max_grad_norm=1.0,
     38 )
     40 privacy_engine.attach(optimizer)
     42 # Training loop

TypeError: PrivacyEngine.__init__() got an unexpected keyword argument 'batch_size'
</code></pre>
","2024-08-06 13:17:08","1","Question"
"78838829","78114412","","<p>I've faced this problem and reinstalled pytorch and installed <a href=""https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170"" rel=""noreferrer"">VC_Redist</a> after trying many methods but all failed to run Pytorch properly.</p>
<p><strong>But the issue solved by simply downloading <a href=""https://www.dllme.com/dll/files/libomp140_x86_64/00637fe34a6043031c9ae4c6cf0a891d/download"" rel=""noreferrer"">libomp140.x86_64.dll</a> and place it in 'C:\Windows\System32'</strong> and it finally works after long pain of trying this and that!</p>
","2024-08-06 11:43:38","34","Answer"
"78834773","78460184","","<p>Could it be related to <a href=""https://docs.kidger.site/jaxtyping/faq/"" rel=""nofollow noreferrer"">https://docs.kidger.site/jaxtyping/faq/</a> &quot;flake8 or ruff are throwing an error&quot;?</p>
<p>For single dimension shape checks, you might have to prepend a space to the shape string, which would be &quot; seq_len&quot;.</p>
","2024-08-05 13:51:11","0","Answer"
"78834005","78687394","","<p>The only layers that change the output shape in your <code>Generator</code> network are the <code>nn.ConvTranspose2d</code> layers. The other layers, including <code>BatchNorm2d</code>, <code>nn.ReLU</code>, <code>nn.Tanh</code> have no impact on the output shapes. Thus, your question essentially boils down to how to calculate the output shape of a <code>nn.ConvTranspose2d</code> layer?</p>
<p>Here is how: given input tensors with shape (N, C_in, H_in, W_in), where N, C_in, H_in and W_in are the input batch size, channel, height, and width respectively, you can calculate the output tensor shape (N, C_out, H_out, W_out) using the following formula:</p>
<pre><code>H_out = (H_in − 1) × stride[0] − 2 × padding[0] + dilation[0] × (kernel_size[0] − 1) + output_padding[0] + 1
W_out = (W_out − 1) × stride[1] − 2 × padding[1] + dilation[1] × (kernel_size[1] − 1) + output_padding[1] + 1
</code></pre>
<p>Where the parameters including C_in, C_out, kernel_size, stride, dilation, padding,... are set when defining the <code>nn.ConvTranspose2d</code> layer. You can read more about each of them on <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html"" rel=""nofollow noreferrer"">PyTorch documentations</a>.</p>
<p>For example your test input is (num_test, 10, 1, 1), and your <code>test_hidden_block</code> layer has <code>C_in=10</code>, <code>C_out=20</code>, <code>kernel_size=(4, 4)</code>, <code>stride=(1, 1)</code>, <code>padding=(0, 0)</code>, so:</p>
<pre><code>H_out = (1 - 1) × 1 - 2 × 0 + 1 × (4 - 1) + 0 + 1 = 4
W_out = (1 - 1) × 1 - 2 × 0 + 1 × (4 - 1) + 0 + 1 = 4
</code></pre>
<p>Hence the output shape of this layer is (num_test, 20, 4, 4).</p>
<p>I find the visual representations of this <a href=""https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11"" rel=""nofollow noreferrer"">blog</a> on transposed convolutions very helpful.</p>
","2024-08-05 10:35:27","0","Answer"
"78832239","78828715","","<p>The issue isn't on your end. The confusion arises from Meta not clearly distinguishing between the distributions via Hugging Face and download.sh.</p>
<p>To resolve this, you can download the model files using the Hugging Face CLI:</p>
<pre><code>!huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --local-dir meta-llama/Meta-Llama-3-8B-Instruct
</code></pre>
<p>This method will provide you with the config.json and tokenizer.json files.</p>
<p>Additionally, you can try downloading other versions manually. For instance, someone shared a link to the configuration file on Hugging Face:</p>
<p><a href=""https://huggingface.co/unsloth/llama-3-8b/blob/main/config.json"" rel=""nofollow noreferrer"">llama-3-8b/config.json</a></p>
","2024-08-04 22:13:00","5","Answer"
"78831083","78791245","","<blockquote>
<p>No forums I've found say anything helpful either.</p>
</blockquote>
<p>I have the exact same feeling. No forums are helping with this error.</p>
<p>Do you have an error window pop up (if you are using Windows) that says this?</p>
<p><a href=""https://i.sstatic.net/O5EJbd18.png"" rel=""nofollow noreferrer"">Error window</a></p>
<p><strong>EDIT</strong>: I've solved the problem for myself. I just needed to downgrade to <code>torch-2.2.2</code> and <code>torchtext-0.17.2</code>. To do this:</p>
<pre><code>pip install torch==2.2.2 torchtext==0.17.2
</code></pre>
","2024-08-04 12:31:00","1","Answer"
"78829601","78823685","","<p>One possible solution is to call CrossEntropyLoss once for each label/target and add up the losses. This also works if the labels have different numbers of classes.</p>
<p>Here is an example using your data:</p>
<pre><code>inputs = torch.tensor([
    [5.65, 3.56, 0.94, 9.23, 6.43],
    [7.43, 3.95, 1.24, 7.22, 2.66],
    [9.31, 2.42, 2.91, 2.64, 6.28],
    [8.19, 5.12, 1.32, 3.12, 8.41],
    [9.35, 1.92, 3.12, 4.13, 3.14],
    [8.43, 9.72, 7.23, 8.29, 9.18],
    [4.32, 2.12, 3.84, 9.42, 8.19],
    [3.92, 3.91, 2.90, 8.19, 8.41],
    [7.89, 1.92, 4.12, 8.19, 7.28],
    [5.21, 2.42, 3.10, 0.31, 1.31]]) #samples,features
targets = torch.tensor([
    [0, 2, 1],
    [0, 0, 0],
    [2, 0, 2],
    [0, 2, 0],
    [0, 1, 1],
    [1, 0, 2],
    [0, 1, 0],
    [2, 0, 2],
    [0, 1, 2],
    [2, 0, 0]]) #samples,labels
manual_class_weight = torch.tensor([0.425,0.15,0.425]) #classes
class_counts = torch.stack([label.unique(return_counts=True)[1] \
                           for label in targets.T]) #labels,classes
print(class_counts)
class_weights = manual_class_weight.unsqueeze(0)/class_counts #labels,classes

torch.manual_seed(0)
model = nn.Linear(5,3*3) #features -&gt; labels*classes
optimizer = torch.optim.Adam(model.parameters(),lr=0.1)
for epoch in range(100):
    predicted_logits = model(inputs).view(-1,3,3) #samples,labels,classes
    loss = sum(F.cross_entropy(predicted_logits[:,i_label],
                            targets[:,i_label],
                            class_weights[i_label]) \
            for i_label in range(3))
    if epoch%10==0:
        accuracy = (predicted_logits.argmax(dim=2)==targets).float().mean()
        print(f&quot;epoch: {epoch}  loss: {loss:.2f}  accuracy: {accuracy:.2f}&quot;)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>
<p>This code results in:</p>
<pre><code>tensor([[6, 1, 3],
        [5, 3, 2],
        [4, 2, 4]])
epoch: 0  loss: 13.68  accuracy: 0.20
epoch: 10  loss: 3.59  accuracy: 0.57
epoch: 20  loss: 2.00  accuracy: 0.63
epoch: 30  loss: 1.51  accuracy: 0.73
epoch: 40  loss: 1.22  accuracy: 0.83
epoch: 50  loss: 1.10  accuracy: 0.83
epoch: 60  loss: 1.02  accuracy: 0.87
epoch: 70  loss: 0.97  accuracy: 0.83
epoch: 80  loss: 0.93  accuracy: 0.90
epoch: 90  loss: 0.90  accuracy: 0.87
</code></pre>
<p>If all labels have the same number of classes and no (or equal) class weights, then this can be done in parallel with only one call (<a href=""https://discuss.pytorch.org/t/crossentropyloss-for-multiple-output-classification/111660"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/crossentropyloss-for-multiple-output-classification/111660</a>). I wrote some code that achieves parallelism in your case and gives the exact same output as before, but I am not sure whether it is actually faster:</p>
<pre><code>unreduced_loss = F.cross_entropy(
    predicted_logits.permute(0,2,1), #bc. class-dim expected as second!
    targets,reduction=&quot;none&quot;) #samples,labels
weight = class_weights[torch.arange(3),targets] #samples,labels
loss = (unreduced_loss*weight/weight.sum(0)).sum()
</code></pre>
<p>Often it is also suggested to use Binary Cross Entropy for multi-label classification, but I think they aren't mathematically equivalent.</p>
","2024-08-03 19:23:51","0","Answer"
"78829389","78501569","","<p>I was able to solve this by installing:</p>
<pre><code>pip install torch==2.2
</code></pre>
","2024-08-03 17:50:37","2","Answer"
"78828715","","Finding config.json for Llama 3.1 8B","<p>I installed the Llama 3.1 8B model through Meta's <a href=""https://github.com/meta-llama/llama-models"" rel=""nofollow noreferrer"">Github page</a>, but I can't get their example code to work. I'm running the following code in the same directory as the Meta-Llama-3.1-8B folder:</p>
<pre><code>import transformers
import torch

pipeline = transformers.pipeline(
  &quot;text-generation&quot;,
  model=&quot;Meta-Llama-3.1-8B&quot;,
  model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16},
  device=&quot;cuda&quot;
)
</code></pre>
<p>The error is</p>
<pre><code>OSError: Meta-Llama-3.1-8B does not appear to have a file named config.json
</code></pre>
<p>Where can I get <code>config.json</code>?</p>
<p>I've installed the latest <code>transformers</code> module, and I understand that I can access the remote model on HuggingFace. But I'd rather use my local model. Is this possible?</p>
","2024-08-03 12:54:18","2","Question"
"78828313","78805977","","<p>I see this issue was recently reported at <a href=""https://github.com/pytorch/pytorch/issues/129446"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/issues/129446</a></p>
<p>Note also that the example code has been modified to say <code>output.append(h_t[-1].clone())</code></p>
<p>I'm also implementing the same basic code and seeing the same RuntimeError.</p>
<p>EDIT: So I fixed my RuntimeError and you've probably got the same issue.   If you follow the code for <code>h_0</code> you'd think that it was a single tensor, like <code>h_0 = torch.zeros(num_layers, batch_size, hidden_size)</code>.  However, if you do this then when you write to <code>h_t[layer]</code> you overwrite it so destroying the needed gradient information.  If h_0 is a list of tensors, then the old object persists and the gradient works.</p>
","2024-08-03 09:14:04","0","Answer"
"78827482","","Can't suppress warning from transformers/src/transformers/modeling_utils.py","<p>My implementation for the AutoModel AutoTokenizer classes are fairly simple:</p>
<pre><code>from transformers import AutoModel, AutoTokenizer
import numpy as np
from rank_bm25 import BM25Okapi
from sklearn.neighbors import NearestNeighbors

class EmbeddingModels:

    def bert(self, model_name, text):
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        inputs = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()
        return embeddings
    
    def create_chunks(self, text, chunk_size):
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
</code></pre>
<p>But I can't get this warning to go away:</p>
<pre><code>A parameter name that contains 'beta' will be renamed internally to 'bias'. 
Please use a different name to suppress this warning.
A parameter name that contains 'gamma' will be renamed internally to 'weight'. 
Please use a different name to suppress this warning.
</code></pre>
<p>There is no reference to the word beta or gamma anywhere in my repo.</p>
<p>Updating the package, suppressing the warnings with <code>import warnings</code></p>
","2024-08-02 23:04:25","1","Question"
"78826831","","Why torch cuda counting on cpu?","<p>I am training a model and my <code>torch.device('cuda' if torch.code.is_available() else 'cpu') </code>outputs a code. But the task manager shows a gpu load of 0%, and a CPU load of 35 - 40%.To install torch, I used <code>pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code><a href=""https://i.sstatic.net/c8zpXsgY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/c8zpXsgY.jpg"" alt=""enter image description here"" /></a></p>
","2024-08-02 18:41:10","0","Question"
"78826769","","""OSError: [WinError 126] Issue with PyTorch Import - Error Loading 'fbgemm.dll' on Windows 10""","<p>I'm trying to run a code that throws an error when importing PyTorch. The pip install command works, but the import fails. The error is as follows:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\vinig\OneDrive\Desktop\porraa\porraa.py&quot;, line 1, in &lt;module&gt;
    import torch
  File &quot;C:\Users\vinig\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\__init__.py&quot;, line 148, in &lt;module&gt;
    raise err
OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\vinig\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.
</code></pre>
<p>I've tried reinstalling Python, PyTorch, modifying the PATH, creating a virtual environment, and nothing works.</p>
","2024-08-02 18:18:59","2","Question"
"78823685","","How to apply class weights to using Pytorch's CrossEntropyLoss to solve an imbalanced data classification problem for Multi-class Multi-output problem","<p>I'm trying to use a weighted loss function to handle class imbalance in my data. My problem is a multi-class and multi-output problem. For example (my data has five output/target columns (output_1, output_2, output_3) and I have three classes (class_0, class_1, and class_2) in each target column. I am currently using pytorch's cross entropy loss function <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html</a> and I see that it has a weight parameter but my understanding is that this the same weight would be applied uniformly to each output/target, but I want to apply separate weights for each class in each output/target.</p>
<p>For concreteness, I could have data that looks like this</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
<th>OUTPUT_1</th>
<th>OUTPUT_2</th>
<th>OUTPUT_3</th>
</tr>
</thead>
<tbody>
<tr>
<td>5.65</td>
<td>3.56</td>
<td>0.94</td>
<td>9.23</td>
<td>6.43</td>
<td>0</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>7.43</td>
<td>3.95</td>
<td>1.24</td>
<td>7.22</td>
<td>2.66</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>9.31</td>
<td>2.42</td>
<td>2.91</td>
<td>2.64</td>
<td>6.28</td>
<td>2</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>8.19</td>
<td>5.12</td>
<td>1.32</td>
<td>3.12</td>
<td>8.41</td>
<td>0</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>9.35</td>
<td>1.92</td>
<td>3.12</td>
<td>4.13</td>
<td>3.14</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>8.43</td>
<td>9.72</td>
<td>7.23</td>
<td>8.29</td>
<td>9.18</td>
<td>1</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>4.32</td>
<td>2.12</td>
<td>3.84</td>
<td>9.42</td>
<td>8.19</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>3.92</td>
<td>3.91</td>
<td>2.90</td>
<td>8.19</td>
<td>8.41</td>
<td>2</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>7.89</td>
<td>1.92</td>
<td>4.12</td>
<td>8.19</td>
<td>7.28</td>
<td>0</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>5.21</td>
<td>2.42</td>
<td>3.10</td>
<td>0.31</td>
<td>1.31</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>whereby,</p>
<pre><code>     the proportion in output 1 is : 0 = 0.6, 1 = 0.1, 2 = 0.3
     the proportion in output 2 is : 0 = 0.4, 1 = 0.3, 2 = 0.3
     the proportion in output 3 is : 0 = 0.4, 1 = 0.2, 2 = 0.4
</code></pre>
<p>I want to apply the class weight based on the distribution of classes in each output column such that it renormalizes (or rebalances? not sure what the terminology to use here is) class 1 to 0.15 and class 0 and class 2 to 0.425 each (so for output_1 the weights would be [0.425/0.6, 0.15/0.1, 0.425/0.3], for output 2 it'll be [0.425/0.4, 0.15/0.3, 0.425/0.3] etc). Rather, what I understand the weight parameter in pytorch's crossentropy loss function is currently doing, is it'll apply a single class weight to each output column. I'm wondering if i'm missing something and there's a way to do this using pytorch's crossentropyloss function?</p>
","2024-08-02 03:34:55","0","Question"
"78821012","78820748","","<p>I was facing same problem with yolov8n.pt model. I tried with downgrading ultralytics version from 8.2.70 to 8.2.60. The issue is resolved now.</p>
","2024-08-01 12:38:58","7","Answer"
"78820748","","A lot of incorrect detection using YOLOv8","<p>I was trying to run YOLOv8 using visual code studio. Installed ultralytics and ran <code>yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'</code> on the vs code terminal.<br />
However the output i received was</p>
<pre><code>2 persons, 1 bicycle, 5 cars, 10 motorcycles, 73 boats, 3 stop signs, 1 dog, 10 horses, 10 cows, 32 bears, 1 giraffe, 63 umbrellas, 6 handbags, 9 frisbees, 15 snowboards, 5 surfboards, 12 knifes, 5 beds, 37 dining tables
</code></pre>
<p>which are clearly not part of this picture.
<img src=""https://i.sstatic.net/f3ZtJT6t.jpg"" alt="""" /></p>
<p>When i first installed ultralytics and tried running torch there was a missing dependency error. <code>fbgemm.ddl</code> was missing. Later when i installed vs_BuildTools this issue was resolved. Then i proceeded to run the code in a virtual environment where a program using torch ran without any errors. Then i proceeded to type this code snippet and encountered this problem. I also tried running using command prompt and jupyter notebook but the same issue persists.</p>
<p>I also did check if the versions are compatible, which they are. I haven't installed cuda yet is it because of that or are there any other issues which i am not aware of?</p>
","2024-08-01 11:33:58","5","Question"
"78820056","78819850","","<p>With <code>pip</code> you can install anything that was packed as a Python wheel. For example you can install Cmake with <code>pip install cmake</code>; it will be installed from <a href=""https://pypi.org/project/cmake/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/cmake/#files</a> where it has been packaged.</p>
<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L151-L165"" rel=""nofollow noreferrer""><code>pip install tensorflow[and-cuda]</code></a> installs CUDA drivers packaged as wheels: <a href=""https://pypi.org/project/nvidia-cublas-cu12/"" rel=""nofollow noreferrer"">https://pypi.org/project/nvidia-cublas-cu12/</a> , <a href=""https://pypi.org/project/nvidia-cuda-runtime-cu12/"" rel=""nofollow noreferrer"">https://pypi.org/project/nvidia-cuda-runtime-cu12/</a> and so on…</p>
","2024-08-01 09:07:27","2","Answer"
"78820055","","Why is attn_mask in PyTorch' MultiheadAttention specified for each head separately?","<p>PyTorch <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention"" rel=""nofollow noreferrer"">MultiheadAttention</a> allows to specify the attention mask, either as 2D or as 3D. The former will be broadcasted over all N batches the latter allows one to specify specific masks for each example in the batch. All seems to make sense. However, from the documentation, the 3D mask is defined as follows:</p>
<blockquote>
<p>(N⋅num_heads,L,S), where N is the batch size, L is the target sequence
length, and S is the source sequence length.</p>
</blockquote>
<p>Two questions araise:</p>
<ol>
<li>Why would anyone want a different mask for different heads?</li>
<li>If we have to do it this way anyway, how are they ordered? i.e. is it (Example1, Head1), (Example1, Head2),... etc OR is it (Example1, Head1), (Example2, Head1),... ? This is also asked in the comments at this <a href=""https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion"">question</a>.</li>
</ol>
","2024-08-01 09:07:23","0","Question"
"78819850","","How/Why can pip install CUDA drivers?","<p>Nowadays, installing pytorch or tensorflow with pip also installs CUDA drivers, see e.g. <a href=""https://www.tensorflow.org/install/pip"" rel=""nofollow noreferrer"">https://www.tensorflow.org/install/pip</a></p>
<p>Since when / how is that possible? My understanding for a long time was that pip could only install python packages and that one of the most important benefits of conda was that it could also install other stuff.</p>
","2024-08-01 08:21:56","1","Question"
"78817856","78816599","","<p>Simply save the wav file in the memory as a bytes stream</p>
<pre><code>from io import BytesIO

audio = AudioSegment.from_file(&quot;audio.m4a&quot;)

# DO SOMETHING

f = BytesIO()
audio.export(f, format=&quot;wav&quot;) # I want to skip this line
f.name = &quot;test.wav&quot;

diarization = pipeline(f)
</code></pre>
","2024-07-31 18:21:05","1","Answer"
"78816764","78816668","","<p>I think you could solve this using the <a href=""https://scikit-image.org/docs/stable/api/skimage.draw.html#skimage.draw.line"" rel=""nofollow noreferrer"">skimage.draw.line</a> method where you input the starting (i,j) and ending (0,0) coordinates and it calculates the indices belonging to the line which you can use for indexing.</p>
<p>Worked example:</p>
<pre><code>from skimage.draw import line
import numpy as np
import matplotlib.pyplot as plt
arr = np.zeros((100, 100))
i, j = 10, 80
origin = 50, 50
rr, cc = line(*origin, i, j)
arr[rr, cc] = 1
plt.imshow(arr, cmap='gray', interpolation='nearest')
</code></pre>
<p><a href=""https://i.sstatic.net/bZlfG53U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZlfG53U.png"" alt=""Gray plot showing the line"" /></a></p>
<p>Or if you want the line to continue:</p>
<pre><code>arr = np.zeros((100, 100))
rr, cc = line(i, j, 2*origin[0]-i, 2*origin[1]-j)
arr[rr, cc] = 2
plt.imshow(arr, cmap='gray', interpolation='nearest')
</code></pre>
<p><a href=""https://i.sstatic.net/bZqttOKU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZqttOKU.png"" alt=""Longer line"" /></a></p>
","2024-07-31 13:55:44","1","Answer"
"78816668","","Get directional elements in matrix","<p>Lets suppose that I have a point of interest in my matrix that is NxN. The point is located in the position ij. So, given the index ij, is there a simple way to get the line elements passing trough ij and to the origin (that is located in the middle of the matrix) ?</p>
<p>I am using torch and I though that using torch.diag would be a first start but acttualy this function does not pass in the middle of the matrix.</p>
<pre><code>def directionalK(kx,ky, indices):
    '''Function that provides the K values at a given direction dictated by the indices'''
    kx_grid,ky_grid = torch.meshgrid(kx,kx, indexing='ij')
    k_grid = torch.sqrt(kx_grid**2 + ky_grid**2) 
    k_grid[...,:len(k_grid)//2] *=-1 
    y,x = indices
    diag = x - len(k_grid)//2
</code></pre>
","2024-07-31 13:35:49","1","Question"
"78816599","","How do I passa PyDub AudioSegment object into a pyannote.audio Pipeline?","<p>Here is my so far code</p>
<pre><code># SETUP
from pydub import AudioSegment
import torch
pipeline.to(torch.device(&quot;cuda&quot;))

from pyannote.audio import Pipeline
pipeline = Pipeline.from_pretrained(
    &quot;pyannote/speaker-diarization-3.1&quot;,
    use_auth_token=&quot;hf_#####&quot;)



audio = AudioSegment.from_file(file_path)

# DO SOMETHING

audio.export(&quot;temp_file.wav&quot;, format=&quot;wav&quot;) # I want to skip this line

diarization = pipeline(&quot;temp_file.wav&quot;)

</code></pre>
<p>I want to skip the part exporrt it to a temp file then have the Pipeline process it, so I dont need to borther removing the temp file</p>
","2024-07-31 13:22:15","1","Question"
"78815502","78792688","","<p>My guess is: your <code>.pt</code> file is most likely broken/corrupted. Most of the references that you posted in your question hint at the same cause.</p>
<p>One can reproduce the problem as follows (that isn't to say that your <code>.pt</code> file was produced this way, but rather is intended to show that a corrupted file can trigger exactly the message that you saw):</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from pathlib import Path

# TODO: Adjust location as suitable
pt_file = Path(&quot;~/test.pt&quot;).expanduser()

pt_file.write_text(&quot;v&quot;)  # Write nothing but 'v' to .pt file
torch.load(pt_file)  # Raises &quot;UnpicklingError: invalid load key, 'v'.&quot;
</code></pre>
<p>My suggestions for further diagnoses and steps would be:</p>
<ol>
<li><p>Check the size of your <code>.pt</code> file: Is it reasonably large? For example, if it holds the weights of some relatively modern neural network, its size should probably be on the order of megabytes or gigabytes, rather than bytes or kilobytes.</p>
</li>
<li><p>Can you open the <code>.pt</code> file with an archive manager? Using <a href=""https://pytorch.org/docs/stable/generated/torch.save.html"" rel=""nofollow noreferrer""><code>torch.save()</code></a> with recent PyTorch versions (≥1.6) saves <code>.pt</code> files as zip archives by default. This means if the file is not corrupted and if it was created this way, you should be able to open it with an archive manager (maybe you need to change the suffix to <code>.zip</code> first).</p>
</li>
<li><p>Look at the contents of your <code>.pt</code> file with a text editor or hex editor. What do you see?</p>
</li>
<li><p>Finally, try to get the <code>.pt</code> file again from its source (re-download, re-copy, etc.) or from a backup if this is possible; or else try to recreate it yourself (by re-training your model).</p>
</li>
</ol>
","2024-07-31 09:27:33","1","Answer"
"78811867","","Update on ONNXRuntimeError: LoadLibrary failed with error 126 onnxruntime\capi\onnxruntime_providers_cuda.dll","<p>After countless tries, I got confused in <a href=""https://stackoverflow.com/q/78810947/12196632"">this issue</a> but finally solved it. Now, back to the initial problem that made me lose my hair: CUDA does not seem to be used when I run my model with <code>pytorch 2.4.0+cu124</code> and <code>onnxruntime-gpu</code>.</p>
<pre class=""lang-py prettyprint-override""><code>2024-07-30 14:32:46.5323116 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 onnxruntime::TryGetProviderInfo_CUDA] D:\a\_work\1\s\onnxruntime\core\session\provider_bridge_ort.cc:1426 onnxruntime::ProviderLibrary::Get [ONNXRuntimeError] : 1 : FAIL : LoadLibrary failed with error 126 &quot;&quot; when trying to load &quot;C:\Users\david\miniconda3\envs\Pose2Sim\Lib\site-packages\onnxruntime\capi\onnxruntime_providers_cuda.dll&quot;

2024-07-30 14:32:46.5491752 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 onnxruntime::python::CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.
load C:\Users\david\.cache\rtmlib\hub\checkpoints\rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.onnx with onnxruntime backend
</code></pre>
<br>
<p>I run Windows 11 with python 3.11. I installed the following packages:</p>
<pre><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
pip install onnxruntime-gpu
</code></pre>
<p>What follows seems to be in line with <a href=""https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements"" rel=""nofollow noreferrer"">ONNXruntime-gpu requirements</a>
Running <code>pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/</code> like suggested <a href=""https://onnxruntime.ai/getting-started"" rel=""nofollow noreferrer"">there</a> does not make any difference.</p>
<pre class=""lang-py prettyprint-override""><code>import torch; import onnxruntime as ort; print(torch.cuda.is_available(),ort.get_available_providers())
print(f'torch version: {torch.__version__}, cuda version: {torch.version.cuda}, cudnn version: {torch.backends.cudnn.version()}, onnxruntime version: {ort.__version__}')

True ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
torch version: 2.4.0+cu124, cuda version: 12.4, cudnn version: 90100, onnxruntime version: 1.18.1
</code></pre>
<p><a href=""https://i.sstatic.net/3KSZp9Gl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3KSZp9Gl.png"" alt=""enter image description here"" /></a></p>
<br>
<p>Dependency walker on the missing library gives this. <strong>It seems like onnxruntime-gpu is expecting cuDNN 8.x. Why? This is not what the doc says, unless I am missing something?</strong></p>
<p><a href=""https://i.sstatic.net/7HOPNmeK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7HOPNmeK.png"" alt=""enter image description here"" /></a></p>
","2024-07-30 12:55:45","1","Question"
"78811654","78810947","","<p>I got it in the end: I installed pytorch 2.4.0+cu118, which uses cuDNN 9. However, cuDNN 9 is not supported by ONNXruntime-gpu 1.18.1+cu11.8, so I needed to install pytorch 2.3.1 instead, which ships with cuDNN 8:</p>
<p><code>pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p>
<p>Useful links:</p>
<ul>
<li><a href=""https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements"" rel=""nofollow noreferrer"">https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements</a></li>
<li><a href=""https://pytorch.org/get-started/previous-versions/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/previous-versions/</a></li>
</ul>
","2024-07-30 12:15:42","3","Answer"
"78810947","","ONNXRuntimeError: LoadLibrary failed with error 126 onnxruntime\capi\onnxruntime_providers_cuda.dll","<p>CUDA does not seem to be used when I run my model.<br />
Any help would be much appreciated!</p>
<pre class=""lang-py prettyprint-override""><code>2024-07-30 11:14:52.5255821 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 onnxruntime::TryGetProviderInfo_CUDA] D:\a\_work\1\s\onnxruntime\core\session\provider_bridge_ort.cc:1426 ```
onnxruntime::ProviderLibrary::Get [ONNXRuntimeError] : 1 : FAIL : LoadLibrary failed with error 126 &quot;&quot; when trying to load &quot;C:\Users\david\miniconda3\envs\Pose2Sim\lib\site-packages\onnxruntime\capi\onnxruntime_providers_cuda.dll&quot;

2024-07-30 11:14:52.5390456 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 onnxruntime::python::CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.
load C:\Users\david\.cache\rtmlib\hub\checkpoints\yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend
</code></pre>
<br>
<p>I run Windows 11. I installed like so but got the same error when trying with pytorch with Cuda 12.4.</p>
<pre><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install onnxruntime-gpu
</code></pre>
<p>The following looks sensible and in accordance with the requirements here: <a href=""https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements"" rel=""nofollow noreferrer"">https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements</a>.</p>
<pre class=""lang-py prettyprint-override""><code>import torch; import onnxruntime as ort; print(torch.cuda.is_available(),ort.get_available_providers())
print(f'torch version: {torch.__version__}, cuda version: {torch.version.cuda}, cudnn version: {torch.backends.cudnn.version()}, onnxruntime version: {ort.__version__}')

True ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
torch version: 2.4.0+cu118, cuda version: 11.8, cudnn version: 90100, onnxruntime version: 1.18.1
</code></pre>
<p>Nvidia-smi gives this. I doubt the missing power information has anything to do with the issue but it's probably worth noting it, I could be wrong.</p>
<p><a href=""https://i.sstatic.net/2fbWVO0M.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2fbWVO0M.png"" alt=""enter image description here"" /></a></p>
<p>Dependency walker on the missing library gives this. I suppose this is the root of my problem, but I'm not sure what to do with it.
<a href=""https://i.sstatic.net/Qs9zqPPn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qs9zqPPn.png"" alt=""enter image description here"" /></a></p>
","2024-07-30 09:40:18","0","Question"
"78810790","78786306","","<p>Here is what worked for me:</p>
<pre><code>pip uninstall torch torchvision torchaudio

pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121
</code></pre>
<p>I am on Windows, using free pycharm, no conda. My GPU: NVIDIA GeForce RTX 4060 Ti</p>
","2024-07-30 09:10:28","2","Answer"
"78809749","78805337","","<p>I haven't had the time to check your problems carefully, but this seem like it can be solved with <a href=""https://pytorch.org/docs/stable/generated/torch.vmap.html"" rel=""nofollow noreferrer"">Pytorch's vmap</a>.</p>
","2024-07-30 03:48:43","0","Answer"
"78809675","78780584","","<p><code>loss = criterion(output, y.long())</code> by taking <code>y.long()</code>, you effectively disconnect <code>y</code> from computational graph again.</p>
","2024-07-30 03:15:30","0","Answer"
"78809520","78809420","","<p>This is a numerical precision error. Using 64 bit floats matches python.</p>
<pre class=""lang-py prettyprint-override""><code>a = 1.0 / 10.0
b = torch.tensor(1.0/10.0, dtype=torch.float32)
c = torch.tensor(1.0/10.0, dtype=torch.float64)

print(f&quot;{a:.10f}, {b.item():.10f}, {c.item():.10f}&quot;)
&gt; 0.1000000000, 0.1000000015, 0.1000000000

print(a == b.item(), a == c.item())
False True
</code></pre>
","2024-07-30 01:44:04","2","Answer"
"78809516","78809420","","<pre><code>import torch    
a = torch.tensor(1.0/10.0, dtype=torch.float64)
print(&quot;{:.10f}, {:.10f}, {:.10f}&quot;.format(1.0/10.0, torch.tensor(1.0/10.0), a))
</code></pre>
<p>It's normal that you can do that</p>
","2024-07-30 01:42:25","1","Answer"
"78809420","","Pytorch looses precision when converting numbers into tensors","<p>How come PyTorch makes such a weird mistake in higher digits in division?</p>
<pre><code>a = torch.tensor(1.0/10.0)`

print(&quot;{:.10f}, {:.10f}, {:.10f}&quot;.format(1.0/10.0, torch.tensor(1.0/10.0), a))
</code></pre>
<p>Output:</p>
<pre><code>0.1000000000, **0.1000000015**, **0.1000000015**
</code></pre>
<p>My Python version is 3.12.4 and Torch is 2.3.0.post100.</p>
","2024-07-30 00:40:22","0","Question"
"78808850","78805977","","<p>The code snippet is incorrect. For all layers after the first, the input to the nth layer is the output of the n-1th layer.</p>
<p>This is clarified in the <code>num_layers</code> section of the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<pre><code>num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1
</code></pre>
<p>This is also consistent with the shape of the weights used:</p>
<pre><code>weight_ih_l[k] – the learnable input-hidden weights of the k-th layer, of shape (hidden_size, input_size) for k = 0. Otherwise, the shape is (hidden_size, num_directions * hidden_size)
</code></pre>
<p>Layers after the first have a <code>weight_ih_l[k]</code> shape of <code>(hidden_size, num_directions * hidden_size)</code>, which is incompatible with using the raw input.</p>
<p>I think this is just an oversight on the documentation. Under the hood, pytorch uses CUDA routines for RNN - the code in the documentation is not indicative of the actual implementation</p>
","2024-07-29 19:54:16","0","Answer"
"78808380","78807230","","<p>The difference is <code>array[:] += 1</code> is an in-place operation, while <code>array = array +1</code> is not.</p>
<p>To start, lets create the arrays and look at their data ids</p>
<pre class=""lang-py prettyprint-override""><code>array = np.arange(1, 10)
tensor = torch.from_numpy(array)
print(id(array), id(tensor))
&gt; (140232345845456, 140232355106384)
</code></pre>
<p>In the above, <code>array</code> and <code>tensor</code> are objects with a specific ID.</p>
<p><code>array</code> and <code>tensor</code> have different IDs, as they are different objects. Under the hood, <code>tensor</code> points to the same piece of memory as <code>array</code>. This is the point of using <code>torch.from_numpy</code> - it creates a tensor referencing the same memory which avoids copying the data from the numpy array.</p>
<p>Now we update with <code>array[:] += 1</code>. This is an <em>in-place</em> operation, meaning we mutate the underlying data of <code>array</code>. When we print the IDs of <code>array</code> and <code>tensor</code>, note that they are the same as above. We are looking at the same objects. Then we print <code>array</code> and <code>tensor</code> themselves. We added 1 to <code>array</code>, the values of <code>array</code> are updated. We see the values of <code>tensor</code> are also updated. This is because <code>tensor</code> references the same memory as <code>array</code> and we updated <code>array</code> with an <em>in-place</em> operation, changing that piece of memory.</p>
<pre class=""lang-py prettyprint-override""><code>array[:] += 1
print(id(array), id(tensor))
&gt; (140232345845456, 140232355106384)

print(array, tensor)
&gt; [ 2  3  4  5  6  7  8  9 10] tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<p>Now we update with <code>array = array +1</code>. This is not an in-place operation, so we are creating a new <code>array</code> reference. When we look at the ID values, we see that <code>array</code> has a different ID, while <code>tensor</code> has the same ID.</p>
<p>The variable <code>array</code> now references a new object, while <code>tensor</code> references the old object. This is why <code>array = array + 1</code> updates <code>array</code> but not <code>tensor</code>.</p>
<pre class=""lang-py prettyprint-override""><code>array = array + 1
print(id(array), id(tensor))
&gt; (139781912744080, 140232355106384)

print(array, tensor)
[ 3  4  5  6  7  8  9 10 11] tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
","2024-07-29 17:30:43","0","Answer"
"78808109","78715358","","<p>This is actually possible with <code>copy_</code>:</p>
<pre><code>z = torch.tensor(1, dtype=torch.int32)
z.copy_(3.)
&gt;&gt;&gt; tensor(3, dtype=torch.int32)
</code></pre>
<p>This will also cast it to the dtype of my variable.</p>
","2024-07-29 16:16:15","1","Answer"
"78807335","78807069","","<p>You can achieve this in PyTorch with the following approach:</p>
<pre class=""lang-py prettyprint-override""><code>def get_unique_elements_first_idx(tensor):
    # sort tensor
    sorted_tensor, indices = torch.sort(tensor)
    # find position of jumps
    unique_mask = torch.cat((torch.tensor([True]), sorted_tensor[1:] != sorted_tensor[:-1]))
    return indices[unique_mask]
</code></pre>
<p>Example usage:</p>
<pre class=""lang-py prettyprint-override""><code>v1 = torch.tensor([2, 3, 3])
v2 = torch.tensor([1, 2, 6, 2, 3, 10, 4, 6, 4])

# Mask to find elements in v2 that are not in v1
mask = ~torch.isin(v2, v1)
v2_without_v1 = v2[mask]

# Get unique elements and their first indices
unique_indices = get_unique_elements_first_idx(v2_without_v1)

print(unique_indices)           #[0, 3, 1, 2]
print(v2[mask][unique_indices]) #[1, 4, 6, 10]
</code></pre>
<p>P.S. On my computer, the function processes a vector of size 10 million in about (1.1 ± 0.1)s.</p>
","2024-07-29 13:24:10","4","Answer"
"78807230","","I'm learning Pytorch and don't understand from_numpy() behaviors","<p>I'm currently learning Pytorch, and encountered some unexpected behaviour when using the torch.from_numpy() command.</p>
<pre><code>import torch as t
import numpy as np

array = np.arange(1, 10)
tensor = t.from_numpy(array)
print(array, tensor)
array[:] += 1 
print(array, tensor)
</code></pre>
<p>outputs to this:</p>
<pre><code>[1 2 3 4 5 6 7 8 9] tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
[ 2  3  4  5  6  7  8  9 10] tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<p>When I run the above code, the pytorch tensor change when the numpy array is changed and vice versa. This is expected behaviour according to pytorch documentation for from_numpy.</p>
<p>However, when I change the code a bit to:</p>
<pre><code>import torch as t
import numpy as np

array = np.arange(1, 10)
tensor = t.from_numpy(array)
print(array, tensor)
array = array +1
print(array, tensor)
</code></pre>
<p>the output becomes:</p>
<pre><code>[1 2 3 4 5 6 7 8 9] tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
[ 2  3  4  5  6  7  8  9 10] tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<p>Weirdly, if i change the array line to array += 1, the tensor and numpy array behaves as expected.
Can anyone explain why?
I'm using google collab to run this, and using cpu.</p>
","2024-07-29 13:03:02","0","Question"
"78807069","","How to implement in pytorch - numpy's .unique WITH(!) return_index = True?","<p>In  numpy.unique there is an option <strong>return_index=True</strong> - which returns positions of unique elements (first occurrence - if several).</p>
<p>Unfortunately, there is no such option in torch.unique !</p>
<p><strong>Question:</strong> What are the fast and torch-style ways to get indexes of the unique elements ?</p>
<p>=====================</p>
<p>More generally my issue is the following: I have two vectors v1, v2, and I want to get positions of those elements in v2, which are not in v1 and also for repeated elements I need only one position.   Numpy's unique with return_index = True immediately gives the solution.  How to do it in torch ? If we know that vector v1 is sorted, can it be used to speed up the process ?</p>
","2024-07-29 12:26:16","3","Question"
"78805977","","Multilayer RNN from scratch having input dilemma and inplace replace error during backprop in PyTorch","<p>As per the definition of multilayer RNN, each time step's output from the first layer is used as input for the same time step of the second layer and so on <a href=""https://stackoverflow.com/questions/69294045/how-is-stacked-rnn-num-layers-1-implemented-on-pytorch"">Refer here.</a>
However, in the implementation of RNN stated in PyTorch's official page <a href=""https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"" rel=""nofollow noreferrer"">RNN</a> they are continuing to use the original input <code>x[t]</code> in all the layers for a given timestep. The code given below shows that in the layer loop.</p>
<h2>RNN implementation as given in PyTorch's docs</h2>
<pre><code>def forward(x, h_0=None):
    if batch_first:
        x = x.transpose(0, 1)
    seq_len, batch_size, _ = x.size()
    if h_0 is None:
        h_0 = torch.zeros(num_layers, batch_size, hidden_size)
    h_t_minus_1 = h_0
    h_t = h_0
    output = []
    for t in range(seq_len):
        for layer in range(num_layers):
            h_t[layer] = torch.tanh(
                x[t] @ weight_ih[layer].T
                + bias_ih[layer]
                + h_t_minus_1[layer] @ weight_hh[layer].T
                + bias_hh[layer]
            )
        output.append(h_t[-1])
        h_t_minus_1 = h_t
    output = torch.stack(output)
    if batch_first:
        output = output.transpose(0, 1)
    return output, h_t
</code></pre>
<p>Would not it be something like this which I have written? The input in the 1st hidden layer should be the original input <code>x[t]</code> and then the input to 2nd hidden layer should be output of first hidden layer's output or <code>h[layer-1]</code> (with an affine transformation to make it size compatible with weights and biases). Refer to the code below.</p>
<h2>My Implementation</h2>
<pre><code>def forward(self, x, h_0=None):
        batch_size, seq_len, _ = x.size()
        x = x.transpose(0, 1) #make sequence first for ease of computation
        if h_0 is None:
            h_0 = torch.zeros(num_layers, batch_size, hidden_dim)
        h_t_minus_1 = h_0
        h_t = h_0
        output_list = []
        for t in range(seq_len):
            #sequential latent space
            for layer in range(num_layers):
                if layer == 0:
                    current_input = x[t] if layer == 0 
                else 
                    current_input = F.linear(h_t[layer-1], self.w_hh2.T, bias = None)
                h_t[layer] = torch.tanh(
                                 current_input @ self.w_ih[layer].T  + 
                                 h_t_minus_1[layer] @ self.w_hh[layer].T + 
                                 self.b_hh[layer])

            output = F.linear(h_t[-1], self.w_oh, self.b_oh)
            output_list.append(output)
            h_t_minus_1 = h_t
        output_list = torch.stack(output_list).transpose(0, 1)
        return output_list

</code></pre>
<p>But unfortunately, my implementation gives inplace modification error while backprop.</p>
<pre><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 128]], which is output 0 of AsStridedBackward0, is at version 68; expected version 67 instead.
</code></pre>
<p>at line containing</p>
<pre><code>h_t[layer] = torch.tanh(
                                 current_input @ self.w_ih[layer].T  + 
                                 h_t_minus_1[layer] @ self.w_hh[layer].T + 
                                 self.b_hh[layer])
</code></pre>
","2024-07-29 08:04:00","0","Question"
"78805337","","Efficient processing of many small torch.nn module","<p>I am looking for a more efficient implementation of the following method:</p>
<pre><code>function_list = [torch.sin, torch.exp, torch.tanh, etc.] #length: batchsize
function_choice = [0,1,2,3,2, etc.] #length: batchsize

def weird_function(x):
    # x have shape [1, dimension]
    y = torch.zeros(batchsize,dimension)
    for i in range(batchsize):
        y[1,:] = function_list[function_choice[i]](x)
    return y
</code></pre>
<p>In English, the first row of the returned value y is some function applied to the input x, and another row of y is another function applied to the input x.</p>
<p>My problem is that if I just write the code like this, the program is slow when batchsize is large (say, several hundred). Is there a better way to achieve the same purpose? Thank you for your help in advance.</p>
","2024-07-29 03:47:51","0","Question"
"78803982","78803752","","<p>One issue I see is that you aren't using multiple processes to load training examples.  If your program is spending way more time loading training examples (done by the CPU) than actually training on them (done by the GPU), that could explain the low GPU utilization.</p>
<p>To be more specific, each time your dataset loads an example, it has to parse a file and apply a bunch of data augmentations.  It's hard to be sure by just reading the code, but both of those steps could be pretty expensive.</p>
<p>If this is actually the problem, here are two possible ways to fix it. First is to use multiple dataloader processes. This is really easy to do; just pass the <code>num_workers</code> argument to the <code>DataLoader</code> constructor.  The only downside with this approach is that you need a lot of CPUs to make the most of it, and cloud providers might not give you very many.  Second is to preload the entire dataset.  This is only an option for relatively small datasets, but if that applies to you, and if you can cache the results so you don't need to do the full preloading every time, this would probably be the fastest approach.</p>
","2024-07-28 14:12:19","2","Answer"
"78803752","","My PyTorch Model in Kaggle uses 100% CPU and 0% GPU During Training","<p>I'm trying to fine-tune a PyTorch classification model to classify plant disease images. I have properly initialized the CUDA device and sent the model, train, validation, and test data to the device. However, when training the model, it uses 100% CPU and 0% GPU. Why is this happening?</p>
<pre class=""lang-py prettyprint-override""><code>device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience):
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []
    best_val_loss = np.inf
    patience_counter = 0
    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

    for epoch in range(num_epochs):
        model.train()
        running_loss, running_corrects, total = 0.0, 0, 0

        for inputs, labels in tqdm(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            running_corrects += torch.sum(preds == labels.data)
            total += labels.size(0)

        epoch_loss = running_loss / total
        epoch_acc = running_corrects.double() / total
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc.item())

        val_loss, val_acc = evaluate_model(model, val_loader, criterion)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
        
        print(f&quot;Epoch {epoch}/{num_epochs-1}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}&quot;)

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pth')
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter &gt;= patience:
                print(&quot;Early stopping&quot;)
                break

        lr_scheduler.step()

    return train_losses, train_accuracies, val_losses, val_accuracies
</code></pre>
<p>Here is my notebook: <a href=""https://www.kaggle.com/code/nirmalsankalana/efficientnet-with-augmentation"" rel=""nofollow noreferrer"">EfficientNet with Augmentation</a></p>
<p>Edit:
I resize the dataset into 256X256 and run the code. When I'm checking my GPU usage surprisingly usage is periodically going up and down. As @kale-kundert said there is a performance bottleneck in data loading and applying augmenting pipeline.</p>
<p><a href=""https://i.sstatic.net/3GsNZVVl.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3GsNZVVl.gif"" alt=""enter image description here"" /></a></p>
","2024-07-28 12:24:12","1","Question"
"78802328","78799800","","<p>The issue is caused by the <code>Tanh</code> activation.</p>
<p>If you look at the outputs of the <code>Tanh</code> activation after training, you can see the values saturate at <code>1</code> or <code>-1</code> for all activations. This results in gradients vanishing and rounding to zero.</p>
<p>If you change <code>nn.Tanh</code> to <code>nn.ReLU</code> you will see nonzero gradient values.</p>
","2024-07-27 19:48:06","0","Answer"
"78799800","","Why is the gradient wrt input zero?","<p>This snippet tries to take gradient wrt the input to a trained model</p>
<pre><code>import torch
import torch.nn as nn

DATASET_SIZE = 10
TUNING_EPOCH = 10
RANDOMIZATION = &quot;rand&quot;
shape = (10000,)
bsize = 1
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

input = eval(f&quot;torch.{RANDOMIZATION}&quot;)(bsize, *shape).to(device).requires_grad_()
numel = input.numel()
model = nn.Sequential(
    nn.Flatten(),
    nn.Linear(numel, numel//2),
    nn.Tanh(),
    nn.Linear(numel//2, 1)
).to(device)
model_optimizer = torch.optim.Adam(model.parameters())
optimizer = torch.optim.Adam([input])
dataset = [(eval(f&quot;torch.{RANDOMIZATION}_like&quot;)(input), (torch.rand(bsize,1)+8).to(device)) for i in range(DATASET_SIZE)]

model.train()
for e in range(TUNING_EPOCH):
    for x, y in dataset:
        model_optimizer.zero_grad()
        output = model(x)
        loss = torch.nn.MSELoss()(output, y)
        loss.backward(inputs=list(model.parameters()))
        model_optimizer.step()

model.eval()
optimizer.zero_grad()
output = model(input)
output.backward(inputs=input)
print(&quot;\ninput.grad&quot;, input.grad)
</code></pre>
<p>The gradient <code>input.grad</code>, however, is all 0. There are 3 individual fixes</p>
<ol>
<li>set <code>TUNING_EPOCH = 0</code>, i.e. not training the model</li>
<li>set <code>RANDOMIZATION = &quot;randn&quot;</code>, i.e. a different input distribution</li>
<li>set <code>shape = (1000,)</code>, making the input smaller</li>
</ol>
<p>Why would these three changes individually fix the zero gradient issue?</p>
","2024-07-26 20:28:49","0","Question"
"78799486","78798069","","<p>From the <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">pytorch website</a> - you can install the CPU only version with:</p>
<p><code>pip3 install torch torchvision torchaudio</code></p>
<p>or</p>
<p><code>conda install pytorch torchvision torchaudio cpuonly -c pytorch</code></p>
","2024-07-26 18:25:34","1","Answer"
"78799152","78786306","","<p><strong>Uninstalled all</strong>:</p>
<pre><code>pip uninstall torch torchvision torchaudio
</code></pre>
<p>Then <strong>installed the one-step older version</strong>:</p>
<pre><code>pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0
</code></pre>
<p>Now, it's working fine.</p>
<p>Initially, I got this problem after the installation of torchvision, torch was working fine before that.</p>
","2024-07-26 16:40:05","18","Answer"
"78798424","78786306","","<p>as you have found out yourself, let's summarize it briefly because it's difficult to read the solution between comments.</p>
<p>To fix this problem we need to add the missing dependency <strong>libomp140.x86_64.dll</strong> to the <strong>C:\Windows\System32</strong> directory. To achieve this, we need to install the <strong>C++ build tools</strong>, as pointed out in: <a href=""https://github.com/pytorch/pytorch/issues/131662#issuecomment-2252589253"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/131662#issuecomment-2252589253</a>.</p>
<p>Follow the issue in the official PyTorch repository on GitHub to stay up to date on this issue: <a href=""https://github.com/pytorch/pytorch/issues/131662"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/131662</a></p>
<p>EDIT: will be fixed in version 2.4.1</p>
","2024-07-26 13:48:50","13","Answer"
"78798069","","How do I Install pytorch without GPU dependencies?","<p>I have a project that depends on <a href=""https://pypi.org/project/torch/2.0.1/"" rel=""nofollow noreferrer"">torch==2.0.1</a>, but it will run on CPU not GPU. torch has some large cuda/cublas/cudnn dependencies that I believe are only needed when running on GPU. These packages, which I don't believe I need, account for ~4.5 GB of by ~6.5 GB image.</p>
<p>How can I install torch without installing the GPU specific dependencies? I'm using poetry 1.8.2 to install packages in my <code>Linux 6.6.22-linuxkit x86_64</code>.</p>
<p>I tried <code>ARG CUDA_VISIBLE_DEVICES=&quot;&quot;</code> in my Dockerfile before running <code>poetry install</code> hoping that these dependencies would not be installed, but that didn't work. I know that env variable is more of a runtime thing that tells torch to use CPU, but the packages have already been install by runtime. I though I'd just try setting it in my Dockerfile just for the heck of it.</p>
<p><strong>NEW TEXT BELOW AS OF 2024.07.29 ~12:12 PM GMT</strong></p>
<p>I know about cpuonly and other ways if torch is a direct dependency. Problem is that it is a dependency of a direct dependency, so I can't do it directly.</p>
","2024-07-26 12:32:41","-1","Question"
"78797883","78796348","","<p>Assigning a new value to <code>w</code> changes that reference to the underlying tensor. The tensor itself does not disappear (as a reference to that tensor is still kept by autograd). You can learn more about variable assignment here: <a href=""https://realpython.com/python-variables/#variable-assignment"" rel=""nofollow noreferrer"">https://realpython.com/python-variables/#variable-assignment</a></p>
<blockquote>
<p>An object’s life begins when it is created, at which time at least one reference to it is created. During an object’s lifetime, additional references to it may be created, as you saw above, and references to it may be deleted as well. An object stays alive, as it were, so long as there is at least one reference to it.
When the number of references to an object drops to zero, it is no longer accessible. At that point, its lifetime is over. Python will eventually notice that it is inaccessible and reclaim the allocated memory so it can be used for something else. In computer lingo, this process is referred to as garbage collection.</p>
</blockquote>
<p>In contrast, if you perform an in-place operation to the tensor directly, an error will be thrown (if <code>w</code> itself does not require gradient, an error will be thrown during backward):</p>
<pre><code>import torch

x = torch.tensor([1.0], requires_grad=True)
w = torch.tensor([2.0], requires_grad=True)
y = w * x

w.fill_(4.0)
</code></pre>
<p><code>RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.</code></p>
","2024-07-26 11:50:28","0","Answer"
"78797725","78114412","","<p>I had the fbgemm.dll or one of its dependencies is missing error, and I just fixed it. I tried the vc_redist reinstallation, downloading vscommunity, and nothing worked.</p>
<p>What worked for me is this:</p>
<ol>
<li>Install miniconda</li>
<li>Add the folder miniconda and miniconda/Scripts to the user PATH</li>
<li>Check your python version // In my case 3.12.2</li>
<li>conda create -n my_environment python=3.12.2</li>
<li>conda activate my_environment</li>
<li>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia //Because previusly I had installed <a href=""https://developer.nvidia.com/cuda-11-8-0-download-archive"" rel=""nofollow noreferrer"">https://developer.nvidia.com/cuda-11-8-0-download-archive</a></li>
<li>conda install -c conda-forge librosa pandas numpy</li>
<li>Then you make a testTorch.py to check if you can import torch</li>
</ol>
<p>And it worked. I hope this helps you.</p>
","2024-07-26 11:09:18","0","Answer"
"78797713","78484297","","<p>I installed pythorch as follows and everything worked well. Installing with pip did not work.</p>
<pre class=""lang-none prettyprint-override""><code>conda install -c pytorch pytorch
</code></pre>
","2024-07-26 11:05:32","4","Answer"
"78796348","","How does PyTorch remember overwritten tensors?","<p>I am trying to understand how PyTorch keeps track of overwritten tensor values for autodifferentiation.</p>
<p>In the code below, <code>w</code> is overwritten before autodifferentiation, yet PyTorch remembers the original value of <code>w</code> when executing <code>z.backward()</code>.</p>
<pre><code>import torch

x = torch.tensor([1.0], requires_grad=True)
w = torch.tensor([2.0], requires_grad=True)
y = w * x

w = torch.tensor([4.0], requires_grad=True)
z = y ** 2

z.backward()
print(x.grad)
</code></pre>
<p>I expected that x.grad would evaluate to 2*w**2*x=2*16*1=32, given that w=4 at the time of calling <code>z.backward()</code>. However, The result is x.grad=2*4*1=8, where PyTorch &quot;remembers&quot; that <code>w</code> was equal to 2 at the time when <code>y</code> was defined.</p>
<p>How does PyTorch do this?</p>
","2024-07-26 05:32:25","-1","Question"
"78795967","78791245","","<p>Turns out I was using PyTorch for CUDA 12.4 but torchtext only supports CUDA 12.1, so I had to change the compute platform.</p>
<p>TLDR; torchtext 0.18.0 and pytorch 2.4.0 work together on CUDA 12.1 but not 12.4</p>
","2024-07-26 01:50:24","0","Answer"
"78795465","78795407","","<p>Have you tried the <code>safe_serialization</code> flag? Setting to false will prevent the model from being saved as a .safetensors file.</p>
<p>Here's a base example:-</p>
<pre><code>from transformers import AutoModel

model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
model.save_pretrained('./model', safe_serialization=False)
torch.load('./model/pytorch_model.bin')
</code></pre>
<p>In your case this would be</p>
<pre><code>model = PeftModel.from_pretrained(model, peft_model_id)
model = model.merge_and_unload()
model.save_pretrained('./model', safe_serialization=False)
torch.load('./model/pytorch_model.bin')
</code></pre>
<p>Edit:- Who the HF model class inherits from determines the serializer used when its set to off. <code>torch.load</code> uses pickle to deserialize. The <code>PeftModel</code> uses <code>PreTrainedModel</code> so you should be able to load it in with the above method.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>HF Model Parent/Donor Class</th>
<th>.save_pretrained(safe_serialization=?)</th>
<th>serializer</th>
<th>Native Pytorch Deserialisation Support</th>
</tr>
</thead>
<tbody>
<tr>
<td>PretrainedModel</td>
<td>off</td>
<td>pickle</td>
<td>Y</td>
</tr>
<tr>
<td>TFPretrainedModel</td>
<td>off</td>
<td>h5</td>
<td>N</td>
</tr>
<tr>
<td>FlaxPretrainedModel</td>
<td>off</td>
<td>msgpack</td>
<td>N</td>
</tr>
</tbody>
</table></div>
<p><a href=""https://huggingface.co/docs/transformers/v4.43.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"" rel=""nofollow noreferrer"">save_pretrained()</a></p>
<p><a href=""https://github.com/huggingface/peft/blob/v0.12.0/src/peft/peft_model.py#L134"" rel=""nofollow noreferrer"">Peft model donor</a></p>
","2024-07-25 21:16:33","1","Answer"
"78795407","","load a model from checkpoint folder in pyTorch","<p>I am trying to load a model from a certain checkpoint and use it for inference. The checkpoint folder looks like this. How do I load the model in torch from this folder. The <a href=""https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html"" rel=""nofollow noreferrer"">resources</a> I could find are for loading from a checkpoint file, not a folder.</p>
<p><a href=""https://i.sstatic.net/wi97AlyY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wi97AlyY.png"" alt=""enter image description here"" /></a></p>
<pre><code>import whisper_timestamped as whisper
from transformers import AutoProcessor, WhisperForConditionalGeneration
from peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model
from peft import PeftModel, PeftConfig
import torch
from datasets import Dataset, Audio
from transformers import AutoFeatureExtractor, WhisperModel

peft_model_id = &quot;aben118/finetuned_model/checkpoint-3900&quot; 

language = &quot;en&quot;
task = &quot;transcribe&quot;
peft_config = PeftConfig.from_pretrained(peft_model_id)
model = WhisperForConditionalGeneration.from_pretrained(
    peft_config.base_model_name_or_path, load_in_8bit=False, device_map=&quot;auto&quot;
)
model = PeftModel.from_pretrained(model, peft_model_id)
print(model)
model = model.merge_and_unload()
model.save_pretrained(&lt;model_path&gt;)
</code></pre>
<p>But it saves it in <code>.safetensors</code> format. I want it to be a model that i can load using <code>torch.load</code>.</p>
","2024-07-25 20:57:29","0","Question"
"78794794","78423100","","<p>I found a simple fix for this issue. Get a recent copy of libomp140.x86_64.dll, and put it in C:\Windows\System32. The latest versions of C++ redistributables or Visual Studio 2022 may not put that file on your machine like they used to. I got mine from another machine where I installed VS etc. over a year ago.
<a href=""https://stackoverflow.com/a/78794748/10549172"">https://stackoverflow.com/a/78794748/10549172</a></p>
","2024-07-25 17:49:11","0","Answer"
"78794748","78484297","","<p>I was able to resolve this issue and it was related to a missing dependency for:</p>
<p>libomp140.x86_64.dll</p>
<p>I was setting up a clean Windows 11 Pro system and wanted to get the latest and greatest Torch with Cuda 12.4 on there for development. Until I got exactly the error you are mentioning. I tried installing and reinstalling different versions of the C++ redistributables to no avail. I also installed Visual Studio Pro 2022. Nothing would get me that file.</p>
<p>Remedy: <strong>Get a copy of libomp140.x86_64.dll and put it in C:\Windows\Sytem32</strong></p>
<p>After I did that, everything started working. For some reason, installing the newest VS and/or C++ redistributables will no longer put that file in there. I got mine from a (slightly) older install that had Visual Studio Pro on it. So, you may need to get it yourself until there is some kind of official fix. <strong>Disclaimer</strong>: I have no idea if this fix is safe or causes conflicts. Please note those here if you have issues.</p>
","2024-07-25 17:36:14","8","Answer"
"78794175","78781313","","<p>I don't follow what you are trying to do on a conceptual level, but I can help you diagnose why torch is complaining.</p>
<h3><strong>Issue #1</strong> - Incorrect Matrix Shapes</h3>
<p>For any given observation and prediction (n=1), cross entropy loss will need the real class value y <code>(1,1)</code> and the prediction probabilities for all classes (in this case:- <code>(1,2)</code>.</p>
<p>In each epoch you are feeding the model a tensor of shape <code>(100, 10, 15)</code> and outputting logits of shape <code>(100, 1)</code>. If 100 refers to the number of individual examples - then I found this output shape odd. If for each observation you would like to a binary prediction, an output shape of <code>(100, 2)</code> would make more sense.</p>
<p>That is because internally <code>nn.CrossEntropyLoss</code> will generate the probabilities for each class. For example, the following would work</p>
<pre><code># note the dtypes
outputs = torch.randn(100, 2, requires_grad=True)  # logits
y_train_tensor = torch.randint(low=0, high=1, size=(100,))  # labels

output = loss(outputs, y_train_tensor)
</code></pre>
<h4>Issue #2 - labels dtype</h4>
<p>You've also set the dtype of <code>y_train_tensor</code> to <code>bool</code>. I don't think that torch converts this internally, so best to use a numeric dtype such as <code>torch.int</code>.</p>
<p>Now, to actually solve the problem (which of the tensors to reshape) will depend upon your task and what the model output shape represents.</p>
<p>Hope this helps.</p>
","2024-07-25 15:18:37","0","Answer"
"78792688","","Unpickling Error: magic_number = pickle_module.load(f, **pickle_load_args) _pickle.UnpicklingError: invalid load key, 'v'","<p>When I am trying to load a <code>.pt</code> file i am seeing the following error,</p>
<pre><code>str1='Dataset/ALL_feats_cgqa.pt'
m = torch.load(str1)
</code></pre>
<p>the error is as follows,</p>
<pre><code>  File &quot;/home/Storage1/pythonCodeArea/train.py&quot;, line 21, in load_embeddings
    m = torch.load(str1)
  File &quot;/home/.local/lib/python3.10/site-packages/torch/serialization.py&quot;, line 1040, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File &quot;/home/.local/lib/python3.10/site-packages/torch/serialization.py&quot;, line 1262, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
_pickle.UnpicklingError: invalid load key, 'v'.
</code></pre>
<p>I have no idea about this error. Any help will be highly appreciated.</p>
<p>I have gone through these references without any solution, <a href=""https://github.com/danielgatis/rembg/issues/262"" rel=""nofollow noreferrer"">[1]</a>, <a href=""https://github.com/WongKinYiu/yolor/issues/269"" rel=""nofollow noreferrer"">[2]</a>, <a href=""https://stackoverflow.com/questions/63063723/how-to-fix-pickle-module-loadf-pickle-load-args-pickle-unpicklingerror-i"">[3]</a>, <a href=""https://discuss.pytorch.org/t/trying-to-load-a-model-in-pytorch-1-6-results-in-pickle-unpicklingerror-invalid-load-key-xad/95615"" rel=""nofollow noreferrer"">[4]</a>.</p>
","2024-07-25 10:03:38","2","Question"
"78791245","","""OSError: [WinError 127] The specified procedure could not be found"" when importing torchtext","<p>When importing <code>get_tokenizer</code> from <code>torchtext</code>:</p>
<pre><code>from torchtext.data.utils import get_tokenizer
</code></pre>
<p>it throws this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\PythonProjects\FakeNews\train.py&quot;, line 4, in &lt;module&gt;
    from torchtext.data.utils import get_tokenizer
  File &quot;D:\PythonProjects\FakeNews\venv\Lib\site-packages\torchtext\__init__.py&quot;, line 18, in &lt;module&gt;
    from torchtext import _extension  # noqa: F401
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;D:\PythonProjects\FakeNews\venv\Lib\site-packages\torchtext\_extension.py&quot;, line 64, in &lt;module&gt;
    _init_extension()
  File &quot;D:\PythonProjects\FakeNews\venv\Lib\site-packages\torchtext\_extension.py&quot;, line 58, in _init_extension
    _load_lib(&quot;libtorchtext&quot;)
  File &quot;D:\PythonProjects\FakeNews\venv\Lib\site-packages\torchtext\_extension.py&quot;, line 50, in _load_lib
    torch.ops.load_library(path)
  File &quot;D:\PythonProjects\FakeNews\venv\Lib\site-packages\torch\_ops.py&quot;, line 1295, in load_library
    ctypes.CDLL(path)
  File &quot;C:\Users\Owner\AppData\Local\Programs\Python\Python312\Lib\ctypes\__init__.py&quot;, line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [WinError 127] The specified procedure could not be found
</code></pre>
<p>I'm using torch 2.4.0 and torchtext 0.18.0, which <em>should</em> be compatible.</p>
<p>Reinstalling and <code>--upgrade</code> did not work. Rerunning just throws the same error. No forums I've found say anything helpful either.</p>
","2024-07-25 02:28:53","1","Question"
"78791097","78164797","","<p>You need to use <code>%pip install llama-index-finetuning</code></p>
","2024-07-25 01:07:37","0","Answer"
"78790970","78786306","","<p>I had this same problem a few hours ago when trying to use PyTorch. This problem is related to some incompatibility between the CUDA version and PyTorch version on Windows. The way I solved it was by uninstalling everything related to Nvidia (even the graphics drivers) and installing <a href=""https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=11&amp;target_type=exe_local"" rel=""nofollow noreferrer"">CUDA Toolkit Version 12.4</a> and <a href=""https://developer.nvidia.com/cudnn-downloads?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exe_local"" rel=""nofollow noreferrer"">cuDNN Version 9.2.1</a>. This setup works well with <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">PyTorch 2.4.0</a>. Another thing that was missing on my system was <a href=""https://visualstudio.microsoft.com/pt-br/thank-you-downloading-visual-studio/?sku=Community&amp;channel=Release&amp;version=VS2022&amp;source=VSLandingPage&amp;cid=2030&amp;passive=false"" rel=""nofollow noreferrer"">Microsoft Visual Studio</a>, which CUDA uses for certain tasks. I found a <a href=""https://www.youtube.com/watch?v=krAUwYslS8E"" rel=""nofollow noreferrer"">tutorial</a> on YouTube that taught me how to do it from beginning to end. It even shows how to download PyTorch correctly. It helped me a lot.</p>
<p>Just for the record, before doing this, my drivers were exactly like yours. My GPU is a 1660 Ti.</p>
","2024-07-24 23:39:06","-2","Answer"
"78790095","78484297","","<p>I've tried several things, including getting the latest MS Visual C++ 2015-2022 Redistributable (x64) - it was the 14.40.33810 at this moment. I'm using CPU - not GPU, so I found an string to use to force the CPU installation of torch (and torchvision):</p>
<pre><code>pip3 install torch==2.3.1+cpu torchvision==0.18.1+cpu -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>No special reason for that torch version, just wanted one compatible with CPU. The one for torchvision is compatible with torch.</p>
<p>Importing torch later worked.</p>
","2024-07-24 18:51:58","2","Answer"
"78787810","78775750","","<p>can you elaborate further on the problem you have received? To obtain the class, you need to apply torch.argmax. The outputs are probabilities.</p>
<p>For binary classification, should be using nn.BCELoss.</p>
<pre><code>import torch
from torch import nn , optim
from torch.utils.data import DataLoader, Dataset
device = 'cuda' if torch.cuda.is_available() else 'cpu'

x = df_train.drop(['Response'], axis =1) #Response - Target
y = df_train['Response']
x_train, x_test, y_train, y_test = train_test_split(x,y, stratify=y)
x_train, x_test, y_train, y_test = x_train.to_numpy(), x_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()

class model_nn(nn.Module):
    
    def __init__(self,input_size,output_size):
        super(model_nn,self).__init__()
        
        self.input_size = input_size
        self.output_size= output_size
        
        self.linear_1 = nn.Linear(self.input_size, 10)
        self.act_1    = nn.Tanh()
        self.linear_2 = nn.Linear(10, self.output_size)
        self.softmax =  nn.Softmax()
        self.act_2 = nn.Sigmoid()
        self.double() 
        
    def forward(self,x):
        x = self.act_1(self.linear_1(x))
        x = self.act_2(self.linear_2(x))
        return x
    
model =  model_nn(10,2).to(device)


class Dataset(Dataset):
    
    def __init__(self,x,y):
        
        self.x = torch.from_numpy(x)
        self.y = torch.from_numpy(y)
        
    def __getitem__(self, index):
        
        return self.x[index], self.y[index]
    
    def __len__(self):
        return len(self.x)
    
train_dataset = Dataset(x_train, y_train)
test_dataset  = Dataset(x_test, y_test)
train_loader = DataLoader(dataset = train_dataset, batch_size=256, shuffle=True)
test_loader  = DataLoader(dataset = test_dataset,  batch_size=256, shuffle=True)
optimazer = optim.Adam(model.parameters(), lr = 0.01)
loss_fm   = nn.BCELoss()


def train(model,train_loader,test_loader):
    
    
    for epoch in range(40):
        run_loss_train = 0
        run_loss_valid = 0
        for num_batch, (inputs , outputs) in enumerate(train_loader):
            model.train()
            inputs = inputs.to(device)
            outputs = outputs.to(device)
            optimazer.zero_grad()
            predict_outputs = model(inputs)
            loss = loss_fm(predict_outputs, outputs.long() )
    
            loss.backward()
            optimazer.step()
            run_loss_train += loss.item()
        run_loss_train = run_loss_train/num_batch
            
        with torch.no_grad():
            model.eval()
            for num_batch_val, (inpiut , output) in enumerate(test_loader):
                
                predict_out = model(inpiut)
                loss = loss_fm(predict_out,output.long())
                run_loss_valid += loss.item()
        run_loss_valid = run_loss_valid/num_batch_val
        if epoch%10 == 0:
            print(epoch,'run_loss_train',np.round(run_loss_train,3), 'run_loss_valid',np.round(run_loss_valid,3))
train(model,train_loader,test_loader)

</code></pre>
","2024-07-24 10:35:24","0","Answer"
"78786306","","FBGEMM load error trying to use PyTorch on Windows","<p>I'm working on a code that uses Whisper, and I need PyTorch with CUDA to improve the speed of the model execution, I have CUDA installed (verified using <code>nvidia-smi</code> command where it shows that I have CUDA 12.6) and I installed PyTorch using the command <code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121</code>, but when I try to import torch in Python (<code>import torch</code>) I get the error:</p>
<blockquote>
<p>OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\Windows10\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.</p>
</blockquote>
<p>I tried uninstalling PyTorch completely and reinstalling several times but it didn't work, apparently the installation is successful but for some reason Python cannot access it, i have Python 3.11.3. Any help is appreciated</p>
","2024-07-24 03:44:58","15","Question"
"78785958","78785761","","<p>Use <a href=""https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.str.contains_any.html"" rel=""nofollow noreferrer""><code>polars.Expr.str.contains_any</code></a>.</p>
<pre class=""lang-py prettyprint-override""><code>CATEGORIES = [&quot;people&quot;, &quot;nature&quot;, &quot;urban&quot;, ...]
filtered_df = (
            pl.scan_parquet(parquet_file)
                .filter(pl.col(&quot;CATEGORIES&quot;).str.contains_any(CATEGORIES))
                .filter(pl.col(&quot;MAX_WIDTH&quot;) &gt;= HORIZONTAL_RES)
                .filter(pl.col(&quot;MAX_HEIGHT&quot;) &gt;= VERTICAL_RES)
                .collect()
            )
</code></pre>
","2024-07-23 23:43:07","0","Answer"
"78785761","","How to scan over a list of keywords in parquet using polars scan_parquet","<p>I have a parquet file with metadata of categories. I want to scan this parquet file using scan_parquet from polars like below:</p>
<pre><code>filtered_df = (
            pl.scan_parquet(parquet_file)
                .filter(pl.col(&quot;CATEGORIES&quot;).str.contains(&quot;people&quot;, literal=True) | pl.col(&quot;CATEGORIES&quot;).str.contains(&quot;nature&quot;, literal=True) | pl.col(&quot;CATEGORIES&quot;).str.contains(&quot;urban&quot;, literal=True)) | ....
                .filter(pl.col(&quot;MAX_WIDTH&quot;) &gt;= HORIZONTAL_RES)
                .filter(pl.col(&quot;MAX_HEIGHT&quot;) &gt;= VERTICAL_RES)
                .collect()
            )
</code></pre>
<p>I have a list of categories<br />
['nature, 'people,'urban', ..... 'n_categories]</p>
<p>How do I write the condition of filtering in that scan parquet to go over each category like in the example above. This list is not always fixed since it depends on user input.</p>
<p>I did not try anything since I don't know how to replace this line with a for loop iterating over the categories list.</p>
<pre><code>.filter(pl.col(&quot;CATEGORIES&quot;).str.contains(&quot;people&quot;, literal=True) | pl.col(&quot;CATEGORIES&quot;).str.contains(&quot;nature&quot;, literal=True) | pl.col(&quot;CATEGORIES&quot;).str.contains(&quot;urban&quot;, literal=True))
</code></pre>
","2024-07-23 22:12:34","0","Question"
"78785554","78784723","","<p>I solved it just by taking the loss from only the last output rather than taking all the losses and sum them up. It fixed my issue but I still don't understand why my first approach doesn't work!</p>
<pre><code>for epoch in range(EPOCHS):
    dataset = CustomDataset(10000)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)
    model.train()
    total_loss = 0

    for length, sequence, next_number in dataloader:
        optimizer.zero_grad()
        h = torch.zeros(BATCH_SIZE)

        for i in range(length):
            x = torch.cat([h, sequence[0, i].unsqueeze(0)])
            h = model(x)[0].unsqueeze(0)
            
            if i == length - 1: loss = criterion(model(x)[1], next_number[0])
            
        loss.backward()
        optimizer.step()
        total_loss += loss.item() 
        
    print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')
</code></pre>
","2024-07-23 20:57:53","0","Answer"
"78784723","","Why my RNN does not converge to a simple task?","<p>I want to create a recursice model to solve the most simple sequence that I know, Arithmetic progression. With having <code>a</code> as the base and <code>d</code> as the step size, the sequence would be as follows:</p>
<p><code>a, a+d, a+2d, a+3d, a+4d, ...</code></p>
<p>To solve this, denoting hidden state as <code>h</code>, the model has to learn a simple 2*2 matrix. This is actually setting <code>h1 = t0</code>.</p>
<p><a href=""https://i.sstatic.net/Qsy1qmgn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qsy1qmgn.png"" alt=""enter image description here"" /></a></p>
<p>To put it in other words, you can see it like this too:</p>
<p><a href=""https://i.sstatic.net/U7fmayED.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/U7fmayED.png"" alt=""enter image description here"" /></a></p>
<p>So this model with a 2*2 fully connected layer should be able to learn this matrix:</p>
<pre><code>class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(2, 2, bias=False)

    def forward(self, x):
        x = self.fc1(x)
        return x
</code></pre>
<p>But to my surprise is does not converge! There should be something wrong with my setup. If you help me find it I will appreciate it. I suspect the problem should be in my training loop.</p>
<p>P.S. I intentionally set batch size to 1 right now. I want to work with padding the input data later. The model should learn without batches anyway.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np

class CustomDataset(Dataset):
    def __init__(self, size):
        self.size = size

    def __len__(self):
        return self.size

    def __getitem__(self, index):
        a0 = (np.random.rand() - 0.5) * 200
        d = (np.random.rand() - 0.5) * 40
        length = np.random.randint(2, MAX_Length_sequence + 1)

        sequence = np.arange(length) * d + a0
        next_number = sequence[-1] + d

        return length, torch.tensor(sequence, dtype=torch.float32), torch.tensor(next_number, dtype=torch.float32)

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(2, 2, bias=False)

    def forward(self, x):
        x = self.fc1(x)
        return x

# Hyperparameters
EPOCHS = 10
BATCH_SIZE = 1
LEARNING_RATE = 0.001
DATASET_SIZE = 10000
criterion = nn.MSELoss()

# Model
model = Model()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

</code></pre>
<p>My traning loop:</p>
<pre><code>for epoch in range(EPOCHS):
    dataset = CustomDataset(DATASET_SIZE)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)
    model.train()
    total_loss = 0

    for length, sequence, next_number in dataloader:
        optimizer.zero_grad()
        loss = 0
        h = torch.zeros(BATCH_SIZE)

        for i in range(length):
            x = torch.cat([h, sequence[0, i].unsqueeze(0)])
            y = sequence[0, i + 1] if i != length - 1 else next_number[0]

            output = model(x)
            h, y_hat = output[0].unsqueeze(0), output[1]

            loss += criterion(y_hat, y)

        loss.backward()
        optimizer.step()
        total_loss += loss.item() 
        
    print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')
</code></pre>
","2024-07-23 16:59:29","0","Question"
"78781391","78781390","","<p>I can't just copy the <code>preprocess_function</code> from <a href=""https://huggingface.co/docs/transformers/tasks/sequence_classification"" rel=""nofollow noreferrer"">the guide</a> and have to expand on the function like this:</p>
<pre><code>def preprocess_function(examples):
   label_mapping = {'i': 0, 'g': 1}
   inputs = tokenizer(
    examples[&quot;text&quot;], truncation=True, padding=True, return_tensors=&quot;pt&quot;
)
   labels = [label_mapping[label] for label in examples[&quot;labels&quot;]]
   labels = torch.tensor(labels)
   return {
    &quot;input_ids&quot;: inputs[&quot;input_ids&quot;],
    &quot;attention_mask&quot;: inputs[&quot;attention_mask&quot;],
    &quot;labels&quot;: labels,
   }
</code></pre>
","2024-07-23 02:57:48","0","Answer"
"78781390","","HF transformers: ValueError: Unable to create tensor","<p>I was following <a href=""https://huggingface.co/docs/transformers/tasks/sequence_classification"" rel=""nofollow noreferrer"">this guide for text classification</a>
and i gotten and error:</p>
<pre><code>ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.
</code></pre>
<p>I tried adding <code>padding=True</code> and <code>truncation=True</code> to <code>preprocessing_tokenizer()</code>  but the same error arises.</p>
<p><a href=""https://stackoverflow.com/questions/75623118/valueerror-unable-to-create-tensor-issue-for-a-transformer-model"">These answers did not help me that much either.</a></p>
","2024-07-23 02:57:48","0","Question"
"78781371","78778376","","<p>The issue is due to the magnitude of the values you are trying to predict. The model on initialization will predict values around mean 0, var 1. Since this is very far away from the values you want to predict, the loss is high.</p>
<p>You should preprocess your values to have a more reasonable scale. Since your output values cover multiple orders of magnitude in range, you should try a log transform.</p>
","2024-07-23 02:48:49","1","Answer"
"78781313","","CrossEntropyLoss on PyTorch LSTM model with one classification per timestep","<p>I am trying to make an LSTM model that will detect anomalies in timeseries data. It takes 5 inputs and produces 1 boolean output (True/False if anomaly is detected). The anomaly pattern will usually be between 3 - 4 timesteps in a row. Unlike most LSTM examples where they are forecasting to predict future data, or classifying a whole sequence of data, I am trying to have a True/False detection flag output at every timestep (True at the last timestep point in the patter if it is detected).</p>
<p>Unfortunately it seems like CrossEntropyLoss doesn't allow for anything more than 1D output tensors, and in this case it will be 2D [num sequences, length of sequence with boolean data]</p>
<p>Here is some example code of what I am trying to produce:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define LSTM classifier model
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Input - 100 examples containing 5 data points per timestep (where there are 10 timesteps)
X_train = np.random.rand(100, 10, 5)
# Output - 100 examples containing 1 True/False output per timestep to match the input
y_train = np.random.choice(a=[True, False], size=(100, 10))  # Binary labels (True or False)

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.bool)

# Define model parameters
input_size = X_train.shape[2] # 5 inputs per timestep
hidden_size = 4 # Pattern we are trying to detect is usually 4 timesteps long
num_layers = 1
output_size = 1 # True/False

# Instantiate the model
model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
num_epochs = 10
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')

# Test the model
X_test = np.random.rand(10, 10, 5) # Generate some test data - same dimensions as input
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
with torch.no_grad():
    predictions = model(X_test_tensor)
    predicted_outputs = torch.argmax(predictions, dim=1)
    print(&quot;Predicted Outputs:&quot;, predicted_outputs)
</code></pre>
<p>Do I need to re-shape the output (or make the number of outputs from the LSTM equal to the sequence length), or perhaps use a different loss function, or a model other than LSTM?</p>
","2024-07-23 02:11:04","0","Question"
"78780676","","Denormalized image with wrong colors in DeepLabV3 shown in Tensorboard using pytorch","<p>I am trying to print some images using the tensorboard. After reading this <a href=""https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/"" rel=""nofollow noreferrer"">link</a> I saw that input images are normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225] but once I try to denormalize them with the following</p>
<pre><code>mean = (0.485, 0.456, 0.406)
std = (0.229, 0.224, 0.225)
mean = torch.Tensor(mean)[None, ..., None, None]
std = torch.Tensor(std)[None, ..., None, None]
return torch.clamp((img * std + mean)*255, 0, 255)
</code></pre>
<p>I receive a very strange set of image colors:</p>
<p><a href=""https://i.sstatic.net/V0zl8udt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/V0zl8udt.png"" alt=""enter image description here"" /></a></p>
<p>And after changing the channels I've got this (the number of rows doesn't matter, that was a change)</p>
<p><a href=""https://i.sstatic.net/IYGiHeoW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IYGiHeoW.png"" alt=""enter image description here"" /></a></p>
<p>When I don't do any normalization, the images are like this</p>
<p><a href=""https://i.sstatic.net/Lh8h1Mxd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Lh8h1Mxd.png"" alt=""enter image description here"" /></a></p>
<p>Has anyone had something like this before? I believe I did as I had to do with the normalization but maybe I'm wrong on it</p>
","2024-07-22 20:33:24","0","Question"
"78780662","78780576","","<p>Allocations generally do not scale well. The thing is you perform a <code>std::make_shared&lt;BestMatches&gt;</code> for each iteration and it needs to allocate not only a <code>BestMatches</code> object but also a <em>control block</em> (the reference-counting data structure). This means at least 2 allocations per iteration.</p>
<p>I am not sure you actually need a shared pointer. A simple <code>std::unique_ptr</code> is certainly enough here since the pointer is not actually shared. In fact, I do not think you actually need dynamic allocation at all. Indeed, <strong><code>BestMatches</code> can certainly be allocated on the stack once in a parallel section and reused between multiple iteration</strong>. If you need to call the constructor and there is no way to easily recycle the object, then you can recycle this memory area using a <strong>placement new</strong> (don't forget to call the destructor though).</p>
<p>I wonder if lines like <code>at::Tensor matches = pBestMatches-&gt;getMatches();</code> also perform allocations. If <code>at::Tensor</code> is not a view of the tensor, then memory should be allocated and a copy is certainly done. Both operations do not scale. Can't you <strong>use views or avoid creating copies</strong>?</p>
<p>I assume other functions of the loops a <strong>thread safe</strong> and you did test your code carefully. <strong>Race conditions</strong> not only break the code but also tends to make the code slower.</p>
<p>I also assume <code>result.index_put_</code> just perform a basic 1D tensor copy. If so, then accessing <code>results</code> is not really a problem since the default OpenMP scheduling policy tends not to cause much false sharing issue.</p>
","2024-07-22 20:27:58","0","Answer"
"78780584","","Calculate gradient with respect to data label in Pytorch","<p>I'm working on implementing a technique from a <a href=""https://arxiv.org/pdf/2303.10837"" rel=""nofollow noreferrer"">research paper</a> where I need to calculate gradients with respect to data labels. Here's the approach I'm following:</p>
<p><a href=""https://i.sstatic.net/C5lXC0rk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C5lXC0rk.png"" alt=""enter image description here"" /></a></p>
<p>Calculate the gradient of the loss with respect to the model's parameters (grad1).
Calculate the gradient of grad1 with respect to the data labels.
However, I'm encountering an issue where the gradient with respect to the data labels is always None. It seems that the data labels (y) are not part of the computational graph (y.grad_fn returns None).</p>
<pre><code>import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.nn.utils import parameters_to_vector

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Load CIFAR10 dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True)

model = LeNet()
criterion = nn.CrossEntropyLoss()

# Get a batch of data
x, y = next(iter(trainloader))

y = y.float().requires_grad_(True)

output = model(x)
loss = criterion(output, y.long())

first_order_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)

Jm_list = []

for grad in first_order_grads:
    if grad is not None:
        for grad_element in parameters_to_vector(grad): 
            Jm = torch.autograd.grad(grad_element, y, retain_graph=True, allow_unused=True)[0]
            Jm_list.append(Jm)
</code></pre>
<p>I'm looking for advice on two points:</p>
<p>How can I include data labels in the computational graph?
Are there alternative methods to calculate gradients with respect to data labels effectively?
Any insights or references to similar implementations would be greatly appreciated!</p>
","2024-07-22 20:02:26","0","Question"
"78780576","","PyTorch C++ extension: efficiently accumulate results from threads in tensor","<p>I am implementing a PyTorch C++ extension that employs parallellism. The desired result is a N x D matrix, and each thread computes one row of this result. What is the recommended way to accumulate these rows?</p>
<br>
<hr>
<p>To put this into context, my code looks as follows (simplified):</p>
<pre class=""lang-cpp prettyprint-override""><code>at::Tensor nearestKKeys(at::Tensor queries, at::Tensor keys, int k, int maxLeafSize) {
    /*
    queries: (Nq, D)
    keys: (Nk, D)
    k: int
    return: (Nq, k)
    */
    int Nq = queries.size(0);

    at::Tensor result = at::empty({Nq, k}, at::ScalarType::Int);
    at::Tensor indices = at::arange({Nk}, keys.device())
    BallTreePtr pBallTree = buildBallTree(
        keys,
        indices,
        maxLeafSize,
        0
    );
    
    #pragma omp parallel for
    for (int n=0; n&lt;Nq; n++) {
        at::Tensor query = queries.index({n});
        
        BestMatchesPtr pBestMatches = std::make_shared&lt;BestMatches&gt;(k);
        // places the desired result for this query in *pBestMatches
        pBallTree-&gt;query(query, query_norm, pBestMatches, k);
        
        // place the desired row in results
        at::Tensor matches = pBestMatches-&gt;getMatches();
        result.index_put_({n}, matches);
   }
}
</code></pre>
<p>This is currently not very efficient, and my profiler tells me that the threads spend a lot of time waiting on each other. I believe this is because of the <code>result</code> variable being shared between threads. I don't think that <code>pBallTree</code> being shared really matters (because it's a pointer), but do tell me if I'm mistaken. So, hence my question: What is the recommended way to accumulate these rows?</p>
","2024-07-22 20:00:06","0","Question"
"78778376","","Pytorch massive loss when learning from basic data","<p>I have a very basic machine learning application I'm trying to make, basically I want the model to try and predict each month of the next year based on the dummy data I've provided but I'm getting huge loss <code>Loss: 5206342.5000</code> and the predicated values are so off from what I would expect <code>Predicted values for each month of next year: [-0.043424129486083984, 0.041442010551691055, -0.16847632825374603, -0.18227830529212952, 0.4619046151638031, 1.128817081451416, 0.05658513307571411, 0.1113058477640152, 0.2900705337524414, 0.21394489705562592, 0.2618725001811981, 0.3829265236854553]</code> I'm very new to machine learning, why am I seeing such massive loss and such wrong values predicted?</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np

# Example historical data (replace with your actual data loading)
data = {
    'ProjectId': ['666d60f2604698f1383d94e4', '666d60e2604698f1383d9495', '666d5fb1604698f1383d8e8f'],
    'Year': ['2023/2024', '2023/2024', '2023/2024'],
    'July': [0, 0, 0],
    'August': [0, 0, 0],
    'September': [0, 0, 0],
    'October': [0, 0, 0],
    'November': [0, 0, 0],
    'December': [10000, 5000, 100000],
    'January': [0, 0, 120000],
    'February': [0, 0, 0],
    'March': [0, 0, 0],
    'April': [0, 0, 0],
    'May': [0, 0, 0],
    'June': [0, 0, 0]
}

df = pd.DataFrame(data)

# Example: Extract X (input features) and y (target values)
X = df.drop(['ProjectId', 'Year'], axis=1).values.astype(np.float32)  # Convert to float32
y = df.drop(['ProjectId', 'Year'], axis=1).values.astype(np.float32)  # Assuming y includes next year's forecast

# Example: Splitting into training and validation sets
# Assume last row for validation, rest for training
X_train = torch.tensor(X[:-1], dtype=torch.float32).unsqueeze(0)  # Add batch dimension (1 sample)
y_train = torch.tensor(y[:-1], dtype=torch.float32).unsqueeze(0)  # Add batch dimension (1 sample)
X_val = torch.tensor(X[-1:], dtype=torch.float32).unsqueeze(0)  # Example single project for validation

# Define LSTM model for forecasting
class LSTMForecast(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMForecast, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])  # Predict based on the last LSTM output
        return out

# Initialize the model, loss function, and optimizer
input_size = X_train.shape[-1]
hidden_size = 64
output_size = y_train.shape[-1]

model = LSTMForecast(input_size, hidden_size, output_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Example: Making predictions for next year's forecast
model.eval()
with torch.no_grad():
    forecast = model(X_val)
    forecast_values = forecast.numpy().tolist()[0]  # Convert to numpy and list

print(f'Predicted values for each month of next year: {forecast_values}')
</code></pre>
","2024-07-22 11:15:09","1","Question"
"78777844","78748344","","<p>The reason is that you got the ouput of shape [batch, hidden_size], which is 1,768 I guess. You cannot fit it into a argmax and do tokenization as <strong>768 is the dimension of vector space instead of vocab.</strong>
Try using GPT2LMHeadModel:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2LMHeadModel
import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot;
import torch

# Model and tokenizer paths
model_path = &quot;/mnt/sda/agent_mxz/models/gpt2&quot;
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# Input texts
texts = [&quot;Replace me by any text you'd like.&quot;, &quot;Hello, this is&quot;, &quot;Write a story for me.&quot;]

# Ensure padding is done on the left
tokenizer.padding_side = &quot;left&quot;

# Define PAD Token = EOS Token
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id
model.config.do_sample = False

# Tokenize the inputs with padding
encoded_inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
print(encoded_inputs)
# Get model output
outputs = model(**encoded_inputs)
# outputs1 = model.generate()
# Print the outputs
# print(outputs[0][1])
print(tokenizer.batch_decode(torch.argmax(outputs[0], dim=-1)))
</code></pre>
<p>It will give you ['. the with a means, want like.\n', ',HelloHelloHelloHelloHello Hello Hello hello hello', ',Write&quot;&quot;Write write write the I.']</p>
","2024-07-22 09:13:17","0","Answer"
"78775750","","Binary classification in pytorch","<p>I want to make a binary classification. The classes in the original data set are unbalanced, 90% and 10%. When using CrossEntropyLoss() at the exit of my function, I get that all objects belong to the null class</p>
<p>At the same time, I tried to iterate over the hyperparameters of the model, but it did not help</p>
<p>I have great doubts that I am using the CrossEntropyLoss loss function correctly, but searching the Internet did not help me find the answer to this question</p>
<pre><code>import torch
from torch import nn , optim
from torch.utils.data import DataLoader, Dataset
device = 'cuda' if torch.cuda.is_available() else 'cpu'

x = df_train.drop(['Response'], axis =1) #Response - Target
y = df_train['Response']
x_train, x_test, y_train, y_test = train_test_split(x,y, stratify=y)
x_train, x_test, y_train, y_test = x_train.to_numpy(), x_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()

class model_nn(nn.Module):
    
    def __init__(self,input_size,output_size):
        super(model_nn,self).__init__()
        
        self.input_size = input_size
        self.output_size= output_size
        
        self.linear_1 = nn.Linear(self.input_size, 10)
        self.act_1    = nn.Tanh()
        self.linear_2 = nn.Linear(10, self.output_size)
        self.softmax =  nn.Softmax()
        self.act_2 = nn.Sigmoid()
        self.double() 
        
    def forward(self,x):
        x = self.act_1(self.linear_1(x))
        x = self.act_2(self.linear_2(x))
        return x
    
model =  model_nn(10,2).to(device)


class Dataset(Dataset):
    
    def __init__(self,x,y):
        
        self.x = torch.from_numpy(x)
        self.y = torch.from_numpy(y)
        
    def __getitem__(self, index):
        
        return self.x[index], self.y[index]
    
    def __len__(self):
        return len(self.x)
    
train_dataset = Dataset(x_train, y_train)
test_dataset  = Dataset(x_test, y_test)
train_loader = DataLoader(dataset = train_dataset, batch_size=256, shuffle=True)
test_loader  = DataLoader(dataset = test_dataset,  batch_size=256, shuffle=True)
optimazer = optim.Adam(model.parameters(), lr = 0.01)
loss_fm   = nn.CrossEntropyLoss()


def train(model,train_loader,test_loader):
    
    
    for epoch in range(40):
        run_loss_train = 0
        run_loss_valid = 0
        for num_batch, (inputs , outputs) in enumerate(train_loader):
            model.train()
            inputs = inputs.to(device)
            outputs = outputs.to(device)
            optimazer.zero_grad()
            predict_outputs = model(inputs)
            loss = loss_fm(predict_outputs, outputs.long() )
    
            loss.backward()
            optimazer.step()
            run_loss_train += loss.item()
        run_loss_train = run_loss_train/num_batch
            
        with torch.no_grad():
            model.eval()
            for num_batch_val, (inpiut , output) in enumerate(test_loader):
                
                predict_out = model(inpiut)
                loss = loss_fm(predict_out,output.long())
                run_loss_valid += loss.item()
        run_loss_valid = run_loss_valid/num_batch_val
        if epoch%10 == 0:
            print(epoch,'run_loss_train',np.round(run_loss_train,3), 'run_loss_valid',np.round(run_loss_valid,3))
train(model,train_loader,test_loader)
</code></pre>
<p>please tell me, what am I doing wrong?</p>
","2024-07-21 16:30:08","0","Question"
"78773038","78772023","","<p>DiffSharp is based on TorchSharp, which I think is more mature (but still quite flaky in places, especially when called from F#). Here is your example in TorchSharp instead:</p>
<pre class=""lang-ml prettyprint-override""><code>open TorchSharp

let ( ** ) (a : torch.Tensor) (b : float) =
    a.pow(b.ToScalar())

// Define tensors with requires_grad=True to track computation history
let x = torch.tensor(2.0, requires_grad=true)
let y = torch.tensor(3.0, requires_grad=true)
 
// Perform a computation
let z = x ** 2 + y ** 3
printfn $&quot;Output tensor z: {z.item&lt;float&gt;()}&quot;
 
// Compute gradients
z.backward()
printfn $&quot;Gradient of x: {x.item&lt;float&gt;()}&quot;
printfn $&quot;Gradient of y: {y.item&lt;float&gt;()}&quot;
</code></pre>
<p>Output:</p>
<pre><code>Output tensor z: 31
Gradient of x: 2
Gradient of y: 3
</code></pre>
","2024-07-20 14:32:50","0","Answer"
"78772143","78727829","","<p>Flask by default runs in a single-threaded mode, which can limit the GPU utilization when handling multiple requests concurrently. To address this, consider using asynchronous processing with tools like asyncio or ThreadPoolExecutor to handle multiple requests concurrently.</p>
<p>While executing your code, you can monitor the GPU memory utilization using the command: <code>nvidia-smi</code>, which will help you check if the GPU is being used.</p>
<p>The following code uses the <code>asyncio</code> library which will speed up the code.</p>
<pre class=""lang-py prettyprint-override""><code>for k, j in enumerate(images):
    user[f&quot;{count + k}.{request.args['type']}&quot;] = 'a'
    tasks.append(asyncio.create_task(j.save(f&quot;s/{count + k}.{request.args['type']}&quot;)))

asyncio.run(asyncio.wait(tasks))
</code></pre>
<p><em>It is a more useful tool for building models in web servers <a href=""https://www.gradio.app/guides/quickstart"" rel=""nofollow noreferrer""><code>gradio</code></a>, which is a Python library for building model web servers.</em></p>
","2024-07-20 07:33:06","0","Answer"
"78772023","","Simple DiffSharp example","<p>How to convert this simple PyTorch snippet to DiffSharp? It's supposed to be similar but many functions are not to be found.</p>
<pre><code># Define tensors with requires_grad=True to track computation history
x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)
 
# Perform a computation
z = x**2 + y**3
print(&quot;Output tensor z:&quot;, z)
 
# Compute gradients
z.backward()
print(&quot;Gradient of x:&quot;, x.grad)
print(&quot;Gradient of y:&quot;, y.grad)
</code></pre>
","2024-07-20 06:34:21","0","Question"
"78771915","78770642","","<p>Try instead using <a href=""https://www.nextflow.io/docs/latest/config.html#config-docker"" rel=""nofollow noreferrer""><code>docker.runOptions</code></a>. For example with the following in your <code>nextflow.config</code>:</p>
<pre><code>docker {

    enabled = true
    runOptions = '--shm-size=256m'
}
</code></pre>
<p>And the following in your <code>main.nf</code>:</p>
<pre><code>process test_proc {

    debug true

    container 'ubuntu:24.04'

    &quot;&quot;&quot;
    df -h /dev/shm
    &quot;&quot;&quot;
}

workflow {

    test_proc()
}
</code></pre>
<p>Results:</p>
<pre><code>$ nextflow run main.nf 

 N E X T F L O W   ~  version 24.04.3

Launching `main.nf` [stupefied_shannon] DSL2 - revision: 306a1b97c1

executor &gt;  local (1)
[ef/0639d1] process &gt; test_proc [100%] 1 of 1 ✔
Filesystem      Size  Used Avail Use% Mounted on
shm             256M     0  256M   0% /dev/shm

</code></pre>
","2024-07-20 05:22:42","1","Answer"
"78770642","","--shm_size change for Docker in Nextflow pipeline: containerOptions not seeming to work","<p>I am trying to increase the shared memory size for the Docker image being used to run a Nextflow pipeline. Per some indications online, I have added the following to my config file:</p>
<pre><code>docker {
    enabled = true
    userEmulation = true 
    containerOptions = '--shm-size=256m'
}
</code></pre>
<p>No errors are thrown, and in Nextflow towers it looks like the config was successfully ingested. However, I am still running into issues where I run out of shared memory when training a pytorch model. To check whether my shm-size change was actually doing anything, I added a command to check what the size is within the nextflow process script section: df -h /dev/shm, and it looks like the shm was in fact not updated -- it still is the default 64M which apparently is normal for Docker.</p>
<pre><code>Filesystem      Size  Used Avail Use% Mounted on
shm              64M     0   64M   0% /dev/shm
</code></pre>
<p>I've been trying different approaches to this for hours and it's pretty frustrating -- is this the right way to do this? I also tried setting the directive directly within the process instead of at the top level config, and this also did not work. Is there a workaround or another way to increase the shm size? It's preventing me from running pytorch training using multiple workers, which means my trainings are super slow.</p>
<p>Thanks.</p>
","2024-07-19 17:48:26","1","Question"
"78769222","78727829","","<p>There are couple of things that might lead to such different outputs. [EDITED]</p>
<ol>
<li><p>Check if you have installed GPU version of torch on server and your model is actually able to use it. Add the line <code>print(torch.cuda.is_available())</code> in 4th line of code. It should print <code>True</code>. While the code is being executed, monitor GPU usage by repetitively giving cmd <code>nvidia-smi</code> on another terminal. GPU usage should increase when executing script on GPU.</p>
</li>
<li><p>Compare environments locally and on server. Give commands - <code>pip freeze &gt; env_libs.txt</code> on server and locally. It will save file env_libs.txt on server and on local. Compare both files. The packages listed in both files should be of same version ideally.</p>
</li>
<li><p>Try to run the flask app using native <code>app.run()</code> function without using waitress server for debugging. If it works using native flask only, you can try Gunicorn to host your app instead of waitress. WSGI servers interfere with your resources and can prevent the flask app to run on GPU. You need to configure them in order to let flask use the GPU.</p>
</li>
<li><p>Does the local script gives similar output when deployed on server without flask or different output? It should give similar output. if it doesn't give, then we need to solve it first before integrating it with flask.</p>
</li>
<li><p>This step should be tried if your local script gives intended output on server as in step 4. Check if intended inputs are going to the flask model in a proper way. You can save the args to a JSON/csv/txt file before feeding it to pipeline. The output difference might be due to different color spaces. You might be missing some post processing step required before you save the model output image. Check if there is some change in output when you feed data to model in batches. Print shapes of both outputs on server and local to debug. Inspecting both output tensors can give some clues on post-processing required.</p>
</li>
</ol>
<p>Another thing is you can try the server script locally by running it on localhost. Check its output on localhost. This will check if code is correct or there is some env related issue.</p>
","2024-07-19 11:51:31","0","Answer"
"78764632","78764497","","<h2>There is clipping because you have an 8bit PIL image</h2>
<p>Looking at the documentation of <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.ToPILImage.html#topilimage"" rel=""nofollow noreferrer"">ToPILImage</a>, we can see that different PIL image modes are selected depending on the tensor, in your case:</p>
<blockquote>
<p>If the input has 3 channels, the mode is assumed to be RGB.</p>
</blockquote>
<p>And <a href=""https://pillow.readthedocs.io/en/stable/handbook/concepts.html#modes"" rel=""nofollow noreferrer"">from the PIL documentation</a>:</p>
<blockquote>
<p>RGB (3x8-bit pixels, true color)</p>
</blockquote>
<p>So your tensor is automatically converted to an image with pixel values ranging from 0..255 (8 bit). So values are clipped when they are converted to a PIL image</p>
<h2>You have an error in your own clipping function</h2>
<pre><code>def between_0_and_255(value: int):
    return value % 255
</code></pre>
<p>should be</p>
<pre><code>def between_0_and_255(value: int):
    return value % 256
</code></pre>
<p>With that, the output becomes</p>
<pre><code>[2.2489082969432315, 2.428571428571429, 2.6399999999999997]
[573, 619, 673]
[61, 107, 161]
[ 61 107 161]
</code></pre>
","2024-07-18 13:10:29","1","Answer"
"78764627","78764038","","<p>You can directly treat your mask as the weights for a multinomial distribution and sample from it. Here is a minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import matplotlib.pyplot as plt
import torch

generator = torch.Generator()
generator.manual_seed(98237)

# Create example mask
mask = torch.zeros((256, 256))
mask[32:187, 53:123] = 1.
mask[150:220, 89:198] = 1.

n_samples = 10_000

# define the probability density function and sample
pdf = mask / mask.sum()

coords = torch.multinomial(pdf.flatten(), n_samples, replacement=True, generator=generator)

x, y = torch.unravel_index(coords, pdf.shape)
x_dithered = x + torch.rand(x.shape, generator=generator) - 0.5
y_dithered= y + torch.rand(y.shape, generator=generator) - 0.5

# Create a 2D histogram of the samples
hist = np.histogram2d(x_dithered, y_dithered, bins=(np.arange(0, 256), np.arange(0, 256)))[0]

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].imshow(mask, origin=&quot;lower&quot;, cmap=&quot;gray&quot;)
axes[0].set_title(&quot;Mask&quot;)

axes[1].imshow(hist, origin=&quot;lower&quot;, cmap=&quot;viridis&quot;)
axes[1].set_title(&quot;Samples&quot;)
</code></pre>
<p>Which gives:</p>
<p><a href=""https://i.sstatic.net/rYzo2GkZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rYzo2GkZ.png"" alt=""enter image description here"" /></a></p>
<p>The trick here is to convert the mask into a one dimensional probability mass function (PMF) and then sample the integer indices. Next convert those back to a two dimensional array indices using unravelling and add uniformly distributed noise to &quot;smear&quot; the position within the support of a single pixel.</p>
<p>This approach generalizes to arbitrary PMFs, not only masks.</p>
<p>I hope this helps!</p>
","2024-07-18 13:10:08","3","Answer"
"78764497","","When calling torchvision.transforms.Normalize and converting to PIL.Image, how are values above 1 handled?","<p>Applying <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html"" rel=""nofollow noreferrer"">torchvision.Normalize</a> should lead to values below 0 and above 1, and thus below 0 and 255 when switching to integer values.</p>
<p>However, when watching the values, this seems NOT to be the case. How is this handled?</p>
<p>Please find below a code sample to reproduce the issue.</p>
<p>For a bit of context, I am trying to integrate a neural network using onnx into a C++ code, but I fail to reproduce my python results as values below 0 and above 1 are clipped.</p>
<pre><code>from PIL import Image
from torchvision import transforms




def make_transforms(normalize: bool = True) -&gt; transforms.Compose:    
    results = [
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ]

    if normalize:
        results.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))

    return transforms.Compose(results)


def main() -&gt; float:

    resize_a_white_image_without_normalization() # works as expected

    resize_a_white_image_WITH_normalization() # how are value &gt; 255 handled???


def resize_a_white_image_WITH_normalization():
    
    # given a given image 
    white = (255, 255, 255)
    white_img = Image.new(&quot;RGB&quot;, (300, 300), white)

    # when resizing the image, and normalizing it
    resized_img = make_transforms(normalize=True)(white_img)
    resized_img_pil = transforms.ToPILImage()(resized_img)

    # expected normalized value 
    normalized_val = [(1.0 - 0.485) / 0.229, (1.0 - 0.456) / 0.224, (1 - 0.406) / 0.225]
    normalized_val_int = [int(i * 255) for i in normalized_val]

    print(normalized_val) # [2.2489082969432315, 2.428571428571429, 2.6399999999999997] &gt; 1 ??
    print(normalized_val_int)                                   # [573, 619, 673] &gt; 255??
    print([between_0_and_255(i) for i in normalized_val_int])   # [63, 109, 163]
    print(np.array(resized_img_pil)[0,0])   # [ 61 107 161] ???? still different from above!


def resize_a_white_image_without_normalization() -&gt; float:

    # given a given image 
    white = (255, 255, 255)
    white_img = Image.new(&quot;RGB&quot;, (300, 300), white)


    # when resizing the image, but not normalizing it, 
    resized_img = make_transforms(normalize=False)(white_img)
    # then all pixels should remain white
    assert (np.array(resized_img) == np.ones_like(np.array(resized_img))).all()



def between_0_and_255(value: int):
    return value % 255


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
","2024-07-18 12:43:46","0","Question"
"78764038","","Generate n random 2D points within a valid region","<p>I want to generate a fixed number of random-uniformly distributed xy points within a valid mask.
The generated points should be continuous, i.e. can be &quot;between pixels&quot;.
I'm looking for an efficient solution since this will be part of a pytorch training loop.</p>
<p>Say I need 100 valid points, my current solution looks like this:</p>
<ol>
<li>Generate 500 random points</li>
<li>Sample each point and check if the value is &gt;0.5 (within the valid region)</li>
<li>Throw away all invalid points and additionally all surplus points so I end up with 100</li>
</ol>
<p>There must be a more efficient solution for this, right?</p>
<p>This is some example code to demo the problem:</p>
<pre><code>import numpy as np
import cv2
import torch
import torch.nn.functional as F

# Create example mask
valid_mask = np.ones((300, 400), dtype=np.uint8) * 255
center = (valid_mask.shape[1] // 2, valid_mask.shape[0] // 2)
angle = 30
rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
rotated_image = cv2.warpAffine(valid_mask, rotation_matrix, (valid_mask.shape[1], valid_mask.shape[0]), flags=cv2.INTER_NEAREST)

# display image
cv2.imshow('mask', rotated_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

# generate 500 random points
num_points = 500
points = np.random.rand(num_points, 2) * 2 - 1  # Scale to [-1, 1]

# Convert image and points to tensors
rotated_image_tensor = torch.tensor(rotated_image, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255  # Shape (1, 1, H, W)
points_tensor = torch.tensor(points, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape (1, 1, N, 2)

# Use grid_sample to sample from the image at each point
sampled_values = F.grid_sample(rotated_image_tensor, points_tensor, mode='bilinear', padding_mode='zeros', align_corners=False)

# Squeeze to remove unnecessary dimensions and get actual values
sampled_values = sampled_values.squeeze().numpy()

# Filter points where the sampled value is greater than 0.01
valid_points = points[sampled_values &gt; 0.5]
print(len(valid_points))

# Display valid points
rotated_image = cv2.cvtColor(rotated_image, cv2.COLOR_GRAY2BGR)
for point in valid_points:
    x, y = point
    x = int((x + 1) * rotated_image.shape[1] / 2)
    y = int((y + 1) * rotated_image.shape[0] / 2)
    cv2.circle(rotated_image, (x, y), 1, (0, 255, 0), -1)

cv2.imshow('mask', rotated_image)
cv2.waitKey(0)
</code></pre>
<p>Any help would be appreciated.</p>
","2024-07-18 11:12:29","1","Question"
"78761440","78757193","","<p>To apply convolutions to sequence data, you want to use a 1D convolution (ie <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html"" rel=""nofollow noreferrer"">Conv1D</a> instead of <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"" rel=""nofollow noreferrer"">Conv2D</a>).</p>
<p>The 1D conv operation expects an input of shape <code>(batch_size, channels, d)</code> which matches your tensor of shape <code>(batch_size, sequence_length, d)</code>.</p>
<p>HOWEVER convolutions (both 1D and 2D) expect a fixed size input. This is problematic because sequence data is typically variable length in the <code>sequence_length</code> dimension.</p>
<p>You can get around this by defining a maximum sequence length and padding all inputs to that sequence length. You could do something like this:</p>
<pre class=""lang-py prettyprint-override""><code>def pad_tensor(x, max_size):
    batch_size, sequence_length, d = x.size()
    
    pad_size = max_size - sequence_length
    
    if pad_size &lt;= 0:
        return x
    
    padding = (0, 0,
               0, pad_size,
               0, 0)
    
    padded_x = torch.nn.functional.pad(x, padding, mode='constant', value=0)
    
    return padded_x

class PaddedConv1D(nn.Module):
    def __init__(self, max_size, **conv_kwargs):
        super().__init__()
        self.max_size = max_size
        self.conv = nn.Conv1d(in_channels=max_size, out_channels=max_size, **conv_kwargs)
        
    def forward(self, x):
        sequence_length = x.shape[1]
        x = pad_tensor(x, self.max_size)
        x = self.conv(x)
        return x[:, :sequence_length]
</code></pre>
<p>However but this is computationally wasteful. You're better off sticking with an operation that works with variable sequence lengths.</p>
<p>That said, it seems like your intent is to replace the feed forward layer of the transformer. In this case using a convolution doesn't really make sense. The convolution operates across the sequence length, while the FF layer operates on each input time step individually. You could (technically speaking) do a 1D conv along the embedding dimension but this doesn't really make sense. Convolving along the embedding dimension actually uses more flops to produce the same output size compared to a FF layer. You can benchmark the following code to verify.</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(8, 17, 64)

ff = nn.Linear(64, 64)

conv = nn.Conv1d(64, 64, 3, padding=1)

y1 = ff(x)
y2 = conv(x.permute(0, 2, 1)).permute(0,2,1)
</code></pre>
","2024-07-17 20:06:59","0","Answer"
"78759829","78752899","","<p>As already hinted at in <a href=""https://stackoverflow.com/questions/78752899#comment138849375_78752899"">@xdurch0's comment</a>, I think you have fallen trap to a misconception here. Weight decay and decaying the learning rate are two different things – at least conceptually¹:</p>
<ul>
<li><em>Weight decay</em> is used for regularizing the weights in your network by penalizing large weight values. It is used as a means to prevent overfitting and thus to help generalization.</li>
<li>By <em>decaying the learning rate</em>, one hopes to converge to a good solution: it follows the idea that a larger initial learning rate helps &quot;exploring the loss landscape&quot;, while decreasing it in later stages of training helps &quot;homing in&quot; on the final solution.</li>
</ul>
<p>Crucially, altering the weight decay value does <em>not</em> alter the learning rate value!</p>
<h2>Use a learning rate scheduler</h2>
<p>To achieve a  decaying learning rate in PyTorch, one usually makes use of a <em>learning rate scheduler</em>, typically in the following way:</p>
<ol>
<li>One creates a scheduler instance that receives, as one of its inputs, the optimizer instance.</li>
<li>At the end of each training epoch, one calls <code>step()</code> on the scheduler (just like calling <code>step()</code> on the optimizer after processing a training batch). This will trigger the scheduler to calculate the new learning rate value for the next epoch, which it will then use for adjusting the optimizer accordingly.</li>
</ol>
<p>The following is an excerpt of the <a href=""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"" rel=""nofollow noreferrer"">PyTorch documentation</a>, demonstrating this approach (comments added by me):</p>
<pre class=""lang-py prettyprint-override""><code>optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# 1. Create scheduler, provide it with optimizer
scheduler = ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    for input_, target in dataset:
        optimizer.zero_grad()
        output = model(input_)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    # 2. Trigger calculation and setting of next learning rate value
    scheduler.step()
</code></pre>
<p>Here, an <a href=""https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html"" rel=""nofollow noreferrer""><code>ExponentialLR</code></a> scheduler is used, which decays the learning rate exponentially, according to a fixed schedule (i.e. only depending on its initial parameters; in particular, the decay factor, but independent of the current network performance). There are also adaptive schedulers, which adjust the learning rate depending on the actual network performance; for example, <a href=""https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html"" rel=""nofollow noreferrer""><code>ReduceLROnPlateau</code></a> reduces the learning rate by a given factor as soon as a loss plateau of a given length has been detected.</p>
<p><sub>¹) Mathematically, for some optimizers, learning rate and weight decay are implicitly coupled, which is one of the reasons why <em>AdamW</em> was derived from the <em>Adam</em> optimizer in the first place. For more information, one might want to read the <a href=""https://arxiv.org/abs/1711.05101"" rel=""nofollow noreferrer"">AdamW paper</a>. But this is not the problem described in the question.</sub></p>
","2024-07-17 13:32:32","1","Answer"
"78758399","78756612","","<p>I solved this problem via DELETING ALL TORCH AND CUDA modules on my PC. Then I used Pytorch with Cuda Website and downloaded 11.8 version.</p>
","2024-07-17 08:22:36","1","Answer"
"78757193","","How to change shape of multi head self-attention output to a shape that can be fed to convolution layer?","<p>I encountered an error like this:</p>
<p>The output of MHSA (multi-head self-attention) is as follows:</p>
<pre><code>torch.Size([20, 197, 768])
</code></pre>
<ul>
<li>20 for batch size</li>
<li>197 for sequence length (previously 196, after adding the class token it became 197)</li>
<li>768 for embedding dimension</li>
</ul>
<p>I want to reshape it to fit the format below in order to feed it to a convolutional layer:</p>
<pre><code>torch.Size([batch_size, channel, width, height])
</code></pre>
<p>I've attempted to achieve this by adding a new dimension using the following approach:</p>
<pre><code>torch.unsqueeze(1)
torch.transpose(1, 3)
</code></pre>
<p>This successfully allows feeding to the convolutional layer. However, I'm unsure if this approach is correct, so please correct me if it's not.</p>
<p>Currently, I'm trying a different approach:</p>
<pre><code>new_size = int(math.sqrt(sequence_length))
torch.transpose(1, 2).view(batch_size, embed_dim, new_size, new_size)
</code></pre>
<p>This resulted in an error stating that the shape is invalid for an input of size (some_number). This is because the sequence length (197) doesn't square perfectly, yielding a decimal number, and the view function expects an integer input, the square operation yields 16 after converting to an integer, but batch_size * 768 * 16 * 16 does not equal batch_size * 197 * 768, leading to the error</p>
<p>Is my analysis correct? And how can I resolve this issue? and is there any better approach?</p>
","2024-07-17 00:24:06","-1","Question"
"78757132","78750026","","<p>PyTorch only provides pip packages for Arm 64bit (aarch64) so you’ll need to install a 64 bit version of the OS on your Raspberry Pi</p>
<p>Ref - <a href=""https://pytorch.org/tutorials/intermediate/realtime_rpi.html#installing-pytorch-and-opencv"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/realtime_rpi.html#installing-pytorch-and-opencv</a></p>
","2024-07-16 23:56:06","0","Answer"
"78757109","78712685","","<p>I tried to use efficient-kan and it did not work either. You can try other libraries:</p>
<ol>
<li>C# implementation:  <a href=""http://openkan.org/DLpiecewise.html"" rel=""nofollow noreferrer"">http://openkan.org/DLpiecewise.html</a></li>
<li>C++ implementation: <a href=""http://openkan.org/DLpiecewiseCPP.html"" rel=""nofollow noreferrer"">http://openkan.org/DLpiecewiseCPP.html</a></li>
<li>C# spline <a href=""http://openkan.org/DLsplines.html"" rel=""nofollow noreferrer"">http://openkan.org/DLsplines.html</a></li>
</ol>
<p>The training is done by the different method, not Broyden, details are here:
<a href=""http://openkan.org/KANscore.html"" rel=""nofollow noreferrer"">http://openkan.org/KANscore.html</a></p>
","2024-07-16 23:46:33","0","Answer"
"78756612","","torch.cuda.is_available() returns False even after installing PyTorch with CUDA","<p>I have recently installed PyTorch with CUDA support on my machine, but when I run <code>torch.cuda.is_available()</code>, it returns <code>False</code>. I verified my GPU setup using <code>nvidia-smi</code>, and it seems that my system recognizes the GPU correctly.</p>
<p>Here are the steps I've followed so far:</p>
<ol>
<li><p>Installed PyTorch with CUDA support using the command <code>pip install torch torchvision torchaudio</code>.</p>
</li>
<li><p>Confirmed that my GPU is recognized by my system using <code>nvidia-smi</code>.</p>
</li>
<li><p>Verified that the correct CUDA version is installed.</p>
</li>
</ol>
<p>Despite this, running <code>torch.cuda.is_available()</code> still returns <code>False</code>. What could be the reason for this issue, and how can I resolve it?</p>
<h3>Additional Information:</h3>
<ul>
<li><p>Operating System: Windows 11</p>
</li>
<li><p>Python Version: 3.10</p>
</li>
<li><p>PyTorch Version: 2.3.1</p>
</li>
<li><p>CUDA Version: 12.5.82</p>
</li>
<li><p>GPU Model: NVIDIA GeForce RTX 4060</p>
</li>
</ul>
<h3>What I've Tried:</h3>
<ul>
<li><p>Reinstalling PyTorch with CUDA support.</p>
</li>
<li><p>Updating GPU drivers.</p>
</li>
<li><p>Rebooting the system.</p>
</li>
</ul>
<pre><code>import torch

print(torch.cuda.is_available())  # Returns False
print(torch.cuda.current_device())  # Raises RuntimeError: No CUDA GPUs are available
</code></pre>
<p>I have recently installed PyTorch with CUDA support on my machine, but when I run <code>torch.cuda.is_available()</code>, it returns <code>False</code>.</p>
","2024-07-16 20:25:57","0","Question"
"78756402","78753120","","<p>If I understand your question correctly, you want to check if some element is in a set. To do this you can use the <code>iter()</code> function together with <code>not in</code>. For example:</p>
<pre><code>for t in tensors:
    if t not in iter(unique_values):
        print(t)
</code></pre>
<p>Will print any value (e.g. tensor) in the list <code>tensors</code> which are not in the set <code>unique_values</code>.</p>
","2024-07-16 19:23:25","0","Answer"
"78755336","78755215","","<p>As you notice, the <code>(bs, num_channels, length)</code> is natural for the convolution operation. The convolution is easily generalized to any dimension; it's just another summation over this dimension. So this is the perspective of the convolution. We can compare the documentations of <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html"" rel=""nofollow noreferrer"">Conv1d</a> and <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"" rel=""nofollow noreferrer"">Conv2d</a>, the formula is the same, only the <code>weight</code> and the convolution operator change, silently.</p>
<p>LSTM and other sequential networks do not share this point of view. Their goal is to process a sequence of <em>features</em>. One illustration for this is the Vision Transformer[1], a image (2D) sequential model, flatten the inputs during its preprocessing. A sequential network only knows one dimension and treats the input as a sequence of elements.</p>
<p>So, in PyTorch, they kept the specificity of each process: sequential network and CNN. I do think this is a solid and clear choice. Note that if you work on more specific domain Deep Learning packages, their dimension orders are arbitrary.</p>
<p>[1]: ViT Paper <a href=""https://arxiv.org/abs/2010.11929"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2010.11929</a>. Obviously, Vision transformers literature have many way to add &quot;2D&quot; positional information. The easiest way is the positional embedding.</p>
","2024-07-16 15:03:02","0","Answer"
"78755215","","Difference between the expected input tensor order for LSTM and Conv1d?","<p>I am working with time series data and have noticed a discrepancy in the input tensor order required for LSTM and Conv1d/BatchNorm1d/Dropout1d layers in PyTorch. For example, say I have an input tensor that has the shape <code>(Batch Size, Sequence Length, Features)</code></p>
<p>When using an LSTM layer, I can simply pass this tensor through</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

batch_size, seq_length, features = 32, 10, 8
input_tensor = torch.randn(batch_size, seq_length, features)

lstm = nn.LSTM(input_size=features, hidden_size=16, batch_first=True)
output, _ = lstm(input_tensor)

</code></pre>
<p>However, for a Conv1d layer, I need to permute the tensor dimensions before passing it through the layer:</p>
<pre class=""lang-py prettyprint-override""><code>input_tensor_permuted = input_tensor.permute(0, 2, 1)

conv1d = nn.Conv1d(in_channels=features, out_channels=16, kernel_size=3)
output = conv1d(input_tensor_permuted)

</code></pre>
<p>Similarly, if the data input were organized as (Batch Size, Features, Sequence Length), I would need to permute it before passing it to the LSTM:</p>
<pre class=""lang-py prettyprint-override""><code>input_tensor_alt = torch.randn(batch_size, features, seq_length)

input_tensor_alt_permuted = input_tensor_alt.permute(0, 2, 1)

output, _ = lstm(input_tensor_alt_permuted)

</code></pre>
<p>I understand that Conv1d and friends operates across the sequence dimension due to the nature of convolutional operations. But why do RNN layers like LSTM break the mold and expect a different input shape?</p>
<p>Could someone explain the reason behind this design choice?</p>
","2024-07-16 14:35:18","0","Question"
"78754718","78718554","","<p>UPDATED answer</p>
<p>Because your CustomFE imports already freezer Encoder (with <code>requires_grad = False</code>) you have that kind of situation where all weights of CustomFE are frozen. Thus by default CustomFE is not trainable. You will need to unfreeze it manually:</p>
<pre><code>
model = PPO(&quot;MlpPolicy&quot;, env='FrozenLake8x8', policy_kwargs=policy_kwargs)

# get model feature extractor
feature_extr: CustomFeatureExtractor = model.policy.features_extractor

# convert all parameters to trainable
for name, param in feature_extr.named_parameters():
    param.requires_grad = True

# check parameters before training
encoder = feature_extr.model.encoder
for name, param in encoder[0].named_parameters():
    print(name, param.mean())


# train the model
model.learn(total_timesteps = 5)


# check parameters after training (if mean changed parameters are training)
feature_extr: CustomFeatureExtractor = model.policy.features_extractor
encoder = feature_extr.model.encoder
for name, param in encoder[0].named_parameters():
    print(name, param.mean())
</code></pre>
","2024-07-16 12:55:52","1","Answer"
"78753120","","How to check if some tensor values are in a set of tensor values","<p>I have a list of tensors and I want to check if any of the values within that list is not in a set containing tensor values, here is what I have tried but I see this code finds all the values are not in unique_values which is wrong!</p>
<pre><code>`data_with_id = Data(x=x_without_id, edge_index=edge_index.t().contiguous(),edge_weight=edge_feats,y=ground_truth_labels) data_with_id.id_column = torch.tensor(id_column, dtype=torch.float)
</code></pre>
<p>#I make a sequence which is Data objects within specific time steps.`</p>
<pre><code>for i in range(start_timestep-1, start_timestep+ timestep-1):
                np.set_printoptions(suppress=True)
                id_column_tensor = torch.tensor(sequence[i].id_column)
                list_tensor = torch.tensor([float(f&quot;{value:.4f}&quot;) if isinstance(value, float) else value for value in id_column_tensor])
                for j in unique_values:
                    if not torch.isin(torch.tensor(j, dtype=list_tensor.dtype), list_tensor):    
                        missing_info.append(j)
</code></pre>
<p>Here is the values in unique_values:</p>
<pre><code>unique_values: {tensor(5008.), tensor(7.), tensor(5004.), tensor(0.), tensor(3.1000), tensor(7.), tensor(11.2000), tensor(12.1000), tensor(17.), tensor(5008.), tensor(18.), tensor(1.), tensor(9.2000)}
</code></pre>
","2024-07-16 07:08:42","0","Question"
"78752899","","LR not decaying for pytorch AdamW even after hundreds of epochs","<p>I have the following code using <code>AdamW</code> optimizer from Pytorch:</p>
<pre><code>optimizer = AdamW(params=self.model.parameters(), lr=0.00005)
</code></pre>
<p>I tried to log in using wandb as follows:</p>
<pre><code>lrs = {f'lr_group_{i}': param_group['lr']
       for i, param_group in enumerate(self.optimizer.param_groups)}
wandb.log({&quot;train_loss&quot;: avg_train_loss, &quot;val_loss&quot;: val_loss, **lrs})
</code></pre>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html"" rel=""nofollow noreferrer"">Note that</a> default value for <code>weight_decay</code> parameter is <code>0.01</code> for <code>AdamW</code>.</p>
<p>When I checked wandb dashboard, it showed the same LR for AdamW even after 200 epochs and it was not decaying at all. I tried this in several runs.</p>
<p><a href=""https://i.sstatic.net/IdppF0Wk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IdppF0Wk.png"" alt=""enter image description here"" /></a></p>
<p>Why the LR decay is not happening?</p>
<p>Also, it was showing LR for only one param group. Why is it so? It seems that I miss something basic here. Can someone point out?</p>
","2024-07-16 06:09:44","0","Question"
"78752373","78751815","","<p>This is due to the <code>ignore_index</code> parameter. You can pass a specific label index value to the loss function that will be ignored (ie for padding tokens and whatnot). The default value for this is <code>-100</code>.</p>
<p>In your code, you set some label values to <code>-100</code>:</p>
<p><code>labels[torch.rand(labels.shape) &lt; 0.2] = -100</code></p>
<p>The associated loss values are not included in the mean reduction, but are included when you manually calculate <code>losses.mean()</code>.</p>
<p>If we change the loss calculation to ignore the <code>-100</code> values like the default mean reduction does, we get matching answers:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

# Generate random predictions and labels
preds = torch.randn(8, 10, 100)  
labels = torch.randint(high=100, size=(8, 10)) 
# replace some values with -100
labels[torch.rand(labels.shape) &lt; 0.2] = -100

preds, labels = preds.view(-1, 100), labels.view(-1)

def compare_losses(preds, labels):
    # Define loss functions
    loss_fn = nn.CrossEntropyLoss(reduction='none')
    loss_fn_mn = nn.CrossEntropyLoss(reduction='mean')

    # Compute losses
    losses = loss_fn(preds, labels)
    weighted_loss = losses[labels != -100].mean() # ignore -100 labels

    # Compute mean loss using built-in mean reduction
    loss = loss_fn_mn(preds, labels)

    # Print and check if the results are identical
    return torch.isclose(loss, weighted_loss), loss.item(), weighted_loss.item()

compare_losses(preds, labels)

&gt; (tensor(True), 5.122199058532715, 5.122198581695557)
</code></pre>
","2024-07-16 01:13:51","1","Answer"
"78752245","","How to install pytorch compatible with CUDA 11.7?","<p>I have &quot;NVIDIA GeForce RTX 2070&quot; GPU on my machine. I found CUDA 11.7 is the latest version of CUDA thats compatible with this GPU and works with pytorch. I have installed CUDA 11.7. Now I am trying to install pytorch that works with CUDA 11.7.
I tried this:
pip install torch torchvision torchaudio --extra-index-url <a href=""https://download.pytorch.org/whl/cu117"" rel=""noreferrer"">https://download.pytorch.org/whl/cu117</a></p>
<p>But I am only getting cpu version: 2.3.1+cpu</p>
<p>I also tried through conda and thats didnt help either:
conda install pytorch torchvision torchaudio cudatoolkit=11.7 -c pytorch</p>
<p>How do I get the GPU compatible version?</p>
","2024-07-15 23:39:11","5","Question"
"78751815","","Inconsistent results between PyTorch loss function for `reduction=mean`","<p>In particular, the following code block compares using</p>
<p><code>nn.CrossEntropyLoss(reduction='mean')</code> with <code>loss_fn = nn.CrossEntropyLoss(reduction='none')</code></p>
<p>followed by <code>loss.mean()</code>.</p>
<p>The results are surprisingly not the same.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

# Generate random predictions and labels
preds = torch.randn(8, 10, 100)  
labels = torch.randint(high=100, size=(8, 10)) 
# replace some values with -100
labels[torch.rand(labels.shape) &lt; 0.2] = -100

preds, labels = preds.view(-1, 100), labels.view(-1)

def compare_losses(preds, labels):
    # Define loss functions
    loss_fn = nn.CrossEntropyLoss(reduction='none')
    loss_fn_mn = nn.CrossEntropyLoss(reduction='mean')

    # Compute losses
    losses = loss_fn(preds, labels)
    weighted_loss = losses.mean()

    # Compute mean loss using built-in mean reduction
    loss = loss_fn_mn(preds, labels)

    # Print and check if the results are identical
    return torch.isclose(loss, weighted_loss), loss.item(), weighted_loss.item()

compare_losses(preds, labels)
</code></pre>
<p>Returns</p>
<pre><code>(tensor(False), 4.997840404510498, 3.748380184173584)
</code></pre>
","2024-07-15 20:45:26","0","Question"
"78750026","","Installing Pytorch on Raspberry Pi 3 Model B Rev 1.2","<p>I am installing Pytorch on Raspberry Pi 3 Model B Rev 1.2.</p>
<p>Here is what I have tried:
(from pytorch website <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a> )</p>
<pre><code>(venv) pi@raspberrypi:~/pytorch $ pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
Looking in indexes: https://download.pytorch.org/whl/cpu, https://www.piwheels.org/simple
ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch
</code></pre>
<p>Tried other methods but getting same error.</p>
","2024-07-15 13:11:31","0","Question"
"78748914","78746073","","<p>There are several steps I took to successfully install <a href=""https://pypi.org/project/flash-attn/"" rel=""nofollow noreferrer"">flash attention</a> after encountering a similar problem and spending almost half a day on it.</p>
<p>First, you have to make sure the PyTorch version installed on your device is compatible with the CUDA version, although I believe this is a small problem.</p>
<p>Secondly, you would also have to download the Visual Studio Tools related to C++. After reading up online, it has to do with the fact that certain python package wheels do not automatically come with C++ libraries so you would have to <a href=""https://visualstudio.microsoft.com/vs/community/"" rel=""nofollow noreferrer"">manually download it</a>.</p>
<p>Additionally, when downloading them, you would also most likely have to keep in mind to download <a href=""https://stackoverflow.com/questions/78515942/cuda-compatibility-with-visual-studio-2022-version-17-10"">CUDA 12.4 or later ones</a> to ensure compatibility.</p>
<p>Finally, according to their website, you would have to ensure the ninja package is installed for faster installation, if not you could take 6 hours like my installation. And make sure to use <code>pip install flash-attn --no-build-isolation</code>.</p>
<p>However, a word of caution is to check the hardware support for flash attention. There are only few advanced hardware GPUs they support currently, and I did not read this so I went through all of this for nothing as my GPU is not supported by flash attention.</p>
<p>Hope this works for you!</p>
","2024-07-15 08:51:16","3","Answer"
"78748344","","GPT-2 model from hugging face always generate same result","<h3>Why were all the results I got from the GPT-2 model the same no matter what I fed into it?</h3>
<p>The following are my operating details.</p>
<p>First I download the needed files from the official website. These files included config.json, merges.txt, pytorch_model.bin, tokenizer.json, tokenizer_config.json and vocab.json. Then I stored them in the root path of the project ./gpt2.</p>
<p>Second, I loaded the model and predicted the next word based on the input context. The code is displayed as follows.</p>
<pre class=""lang-py prettyprint-override""><code>
model = GPT2Model.from_pretained('./gpt2') 
gpt_tokenizer=GPT2Tokenizer.from_pretrained('./gpt2')
start_context=&quot;The white man worked as a &quot; 
ids_text=gpt_tokenizer(start_ontext,return_tensor='pt') 
output=model(**ids_text) 
output=output.last_hidden_state[:,-1,:] 
idx_next=torch.argmax(output,dim=-1,keepdim=True) 
ids=idx_next.squeeze(0) 
text=gpt_tokenizer.decode(ids.tolist()) 
print(text) 
</code></pre>
<p>Here, the text always indicates age, even though I changed the start_context to other, like &quot;I see a cat under&quot;.</p>
<p>I hope someone can tell me the reason and help me work it out, thanks.</p>
","2024-07-15 06:31:59","0","Question"
"78747678","78745180","","<p><a href=""https://stackoverflow.com/users/10044560/karl"">Karl</a>'s comment lead me to the answer. The two matrices had a different <code>stride()</code>, which must have lead Pytorch to choose a different order of operations (I'm satisfied with this level of understanding).</p>
<p>We can reproduce this with <code>.T.contiguous().T</code>:</p>
<pre><code>w = hf_model.layers[0].block_sparse_moe.experts[0].w3.weight
w2 = w.T.contiguous().T
i = torch.randn(3, 4096)
</code></pre>
<pre><code>&gt;&gt;&gt; torch.all(i @ w.T == i @ w2.T)
tensor(False)
&gt;&gt;&gt; w.stride(), w2.stride()
((4096, 1), (1, 14336))
</code></pre>
<p><a href=""https://gist.github.com/joelburget/fd18a958add96a108a0455633da2e1aa"" rel=""nofollow noreferrer"">Full ipynb gist</a></p>
","2024-07-15 00:18:02","2","Answer"
"78747019","78746708","","<p>Here's a way to do this using only torch tensor operations based on the constraints in the question:</p>
<ul>
<li>Find the indexes of all 1s</li>
<li>Create an array of false values of the same length</li>
<li>Set the value of indexes ones + 1, ones + 2, and ones + 3 to True. The indexes need to be clamped to <code>len(x) - 1</code> prevent out of bounds indexing.</li>
</ul>
<pre><code>def f(x):
    ones = (x == 1).nonzero(as_tuple=True)[0]
    flag = torch.zeros_like(x, dtype=torch.bool)
    flag[torch.clamp(ones + 1, max=len(x) - 1)] = True
    flag[torch.clamp(ones + 2, max=len(x) - 1)] = True
    flag[torch.clamp(ones + 3, max=len(x) - 1)] = True
    return flag


print(f(torch.tensor([9, 0, 6, 5, 2, 1, 10, 18, 26, 0, 11])))
print(f(torch.tensor([9, 0, 6, 5, 2, 1, 10, 18])))
print(f(torch.tensor([1, 10, 18])))
</code></pre>
<p>Output:</p>
<pre><code>tensor([False, False, False, False, False, False,  True,  True,  True, False,
        False])
tensor([False, False, False, False, False, False,  True,  True])
tensor([False,  True,  True])
</code></pre>
","2024-07-14 17:31:59","0","Answer"
"78746786","78746708","","<p>My attempt (if I understood well the question)</p>
<pre><code>r=(idx==1).cumsum()-(idx==1)
r-=((idx==0) &amp; (r!=0)).cumsum()
flags = r.astype(bool) # If needed to have bools instead of 0/1
</code></pre>
<p>There are probably other tricks like that (honestly, that is the kind of cumulative problems for which it is often easier to just use numba or C, or etc. I take that if you use torch, it is because you plan to run this on GPU/TPU or some specific hardware, so it is not necessarily easi).</p>
<p>But if the goal is to hijack numpy's internal <code>for</code> loop to avoid writing some ourselves (which is the goal of most of the <code>numpy</code> tagged questions I answer), then, this one is probably as good as any.</p>
","2024-07-14 15:47:33","2","Answer"
"78746708","","Create a boolean array with True for any value between two entries in a certain array","<p>I have a tensor called <code>idx</code> which has integers from <code>0</code> to <code>27</code>, for instance:</p>
<pre><code>idx = torch.tensor([9, 0, 6, 5, 2, 1, 10, 18, 26, 0, 11])
</code></pre>
<p>I want to generate another array/tensor which has boolean values. I want the value to be True when it is strictly between <code>1</code> (on the left) and <code>0</code> (on the right) or the end of the tensor:</p>
<pre><code>flag = torch.tensor([False, False, False, False, False, False, True, True, True, False, False])
</code></pre>
<p>Notice that if the <code>0</code> does not appear after the <code>1</code>, I still want <code>True</code> until the end of the tensor. That is,</p>
<pre><code>idx = torch.tensor([9, 0, 6, 5, 2, 1, 10, 18])
flag = torch.tensor([False, False, False, False, False, False, True, True])
</code></pre>
<p>Is there a fast way to do this without an expensive for loop? Or if the for-loop is unavoidable, what's the most efficient way to do this?</p>
<p>Importantly, my <code>idx</code> will always have <code>0</code> and <code>1</code> at regular intervals and they will always alternate. In particular, there are always exactly <code>3</code> (in this case) elements between <code>1</code> and <code>0</code> or between <code>0</code> and <code>1</code>.</p>
","2024-07-14 15:06:50","1","Question"
"78746073","","How to solve ""Torch was not compiled with flash attention"" warning?","<p>I am using the Vision Transformer as part of the <a href=""https://github.com/openai/CLIP"" rel=""noreferrer"">CLIP</a> model and I keep getting the following warning:</p>
<pre><code>..\site-packages\torch\nn\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
</code></pre>
<p>The code works, but I'm guessing that it's not as fast as possible since there's no FA.</p>
<p>I have tried running the ViT while trying to force FA using: <code>with torch.nn.attention.sdpa_kernel(torch.nn.attention.SDPBackend.FLASH_ATTENTION):</code> and still got the same warning.</p>
<p>For reference, I'm using Windows 11 with Python 3.11.9 and torch 2.3.1+cu121.</p>
","2024-07-14 09:55:11","8","Question"
"78745180","","Pytorch: matrices are equal but have different result","<p>How is this possible?</p>
<pre><code>&gt;&gt;&gt; torch.all(w1 == w2)
tensor(True)
&gt;&gt;&gt; torch.all(i1 @ w1.T == i1 @ w2.T)
tensor(False)
</code></pre>
<p>The matrices are all the same dtype, on the same device, etc:</p>
<pre><code>&gt;&gt;&gt; i.dtype, w1.dtype, w2.dtype
(torch.float32, torch.float32, torch.float32)
&gt;&gt;&gt; i.device, w1.device, w2.device
(device(type='cpu'), device(type='cpu'), device(type='cpu'))
&gt;&gt;&gt; i.shape, w1.shape, w2.shape
(torch.Size([3, 4096]), torch.Size([14336, 4096]), torch.Size([14336, 4096]))
&gt;&gt;&gt; diff = (i @ w1.T) - (i @ w2.T)
&gt;&gt;&gt; diff
tensor([[ 1.6391e-07, -5.3644e-07,  3.8743e-07,  ...,  4.7684e-07,
         -5.9605e-08, -3.8743e-07],
        [ 3.5763e-07,  2.9802e-08, -4.7684e-07,  ...,  8.6427e-07,
          8.9407e-08, -1.1921e-07],
        [ 3.5763e-07,  3.4273e-07, -4.4703e-08,  ...,  2.1607e-07,
          2.3656e-07,  7.4506e-08]], grad_fn=&lt;SubBackward0&gt;)
&gt;&gt;&gt; torch.max(torch.abs(diff))
tensor(7.1526e-06, grad_fn=&lt;MaxBackward1&gt;)
&gt;&gt;&gt; torch.all(w1 - w2 == 0)
tensor(True)
</code></pre>
<p>Seeing <a href=""https://stackoverflow.com/questions/63824484/pytorch-same-input-different-output-not-random"">PyTorch Same Input Different Output (Not Random)</a>, I also set:</p>
<pre><code>&gt;&gt;&gt; torch.manual_seed(0)
&gt;&gt;&gt; torch.backends.cudnn.deterministic = True
&gt;&gt;&gt; torch.backends.cudnn.benchmark = False
</code></pre>
<p>This didn't change anything.</p>
<p>For context, I discovered this <a href=""https://github.com/TransformerLensOrg/TransformerLens/compare/main...joelburget:TransformerLens:mixtral-1l"" rel=""nofollow noreferrer"">while debugging</a> a difference between two different implementations of <a href=""https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"" rel=""nofollow noreferrer"">Mixtral</a>. So <code>i</code> corresponds to what's called <code>current_state</code> and <code>w1</code> is <code>expert_mlp.W_gate.T</code>, while <code>w2</code> is the corresponding matrix from the Huggingface implementation of Mixtral (where it's called <code>w1</code>, confusingly).</p>
","2024-07-13 23:18:44","2","Question"
"78745071","78555597","","<p>The problem is the weights you're using are for a model pre-trained with a higher resolution, namely 518 as Simon mentioned. To use the model at your preferred image_size 224, you have to interpolate the positional embeddings of the model. PyTorch provides a function for this themselves, it's hidden in the <a href=""https://github.com/pytorch/vision/blob/ab0b9a436bd64c4d0309f1b700868c2fe73c0f3e/torchvision/models/vision_transformer.py#L789"" rel=""nofollow noreferrer"">source code</a>. It takes in the <code>state_dict</code> of the pre-trained weights and returns them with the correct <code>pos_embedding</code> size! Then you should be able to load them in your model.</p>
","2024-07-13 21:56:42","0","Answer"
"78743675","78734964","","<p>I ran into the same issue, found the answer in <a href=""https://www.reddit.com/r/ollama/comments/1c8ddv8/comment/l3mj1ix/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button"" rel=""nofollow noreferrer"">this Reddit post</a>.</p>
<p>Set the <code>CUDA_VISIBLE_DEVICES</code> environment variable to <code>0,1</code> before running <code>ollama serve</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>:/# export CUDA_VISIBLE_DEVICES=0,1
:/# echo $CUDA_VISIBLE_DEVICES
0,1
:/# ollama serve
</code></pre>
<p>Tested on an A4000 pod.</p>
","2024-07-13 11:16:56","1","Answer"
"78739851","78736538","","<p>Try this?</p>
<pre><code>python -c &quot;import cupy; import cupyx&quot;
</code></pre>
","2024-07-12 10:25:47","0","Answer"
"78737555","78703313","","<p>For someone else that runs into the same issue, the cause seems to be an issue with JetBrains PyCharm's Jupyter Notebook support. I am filing a bug report with them as well. Running jupyter-notebook externally showed the correct version of numpy being used and the code works as expected.</p>
","2024-07-11 20:26:05","0","Answer"
"78737177","78737007","","<p>Pytorch do support using an <code>Optimizer</code> to maximize a loss via setting <code>maximize=True</code>. You can see in <a href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"" rel=""nofollow noreferrer"">here</a>.</p>
","2024-07-11 18:24:11","1","Answer"
"78737007","","Doing Gradient Descent Ascent using PyTorch","<p>I want to perform GDA for minimax problems of form $\min_x\max_y f(x,y)$. This is the same objective as that in GANs but GDA takes gradient steps simultaneously.
In particular, we need</p>
<p>$$ x_{k+1} = x_k - \nabla_x f(x_k,y_k); y_{k+1} = y_k + \nabla_y f(x_k,y_k) $$</p>
<p>How can I achieve this using pytorch? It does not seem to be possible right now the way the optimizers are implemented to be always doing gradient descent. I thought of the following method, which seems like a cheap hack to be honest.</p>
<pre><code>descent_optim = torch.optim.SGD([X], lr=lr)
ascent_optim = torch.optim.SGD([y], lr=lr)
while True:
  x_copy = x.detach().clone()
  loss = f(x,y)
  descent_optim.zero_grad()
  loss.backward()
  descent_optim.step()
  loss = - f(x_copy,y)
  ascent_optim.zero_grad()
  loss.backward()
  ascent_optim.step()
</code></pre>
","2024-07-11 17:44:23","0","Question"
"78736538","","Cannot use GPU, CuPy is not installed","<p>I have a GPU enabled machine.</p>
<p>O.S: Ubuntu 20.04.6 LTS</p>
<p>nvcc version: 12.2</p>
<p>Nvidia Driver Version: 535.183.01</p>
<p>Pytorch version 2.3.1+cu121</p>
<p>spaCy version 3.7.5</p>
<p>Python version 3.8.10</p>
<p>Pipelines : en_core_web_sm (3.7.1)</p>
<p>I am using a virtual environment.</p>
<p>I received the output <strong>True</strong> for the following command</p>
<pre><code>&gt;&gt;&gt;import torch
&gt;&gt;&gt; torch.cuda.is_available()
</code></pre>
<p>However, when I ran</p>
<pre><code>&gt;&gt;&gt;import spacy
&gt;&gt;&gt; spacy.require_gpu()
</code></pre>
<p>I got the error:</p>
<pre><code>File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/abc/spacy_test_env/lib/python3.8/site-packages/thinc/util.py&quot;, line 230, in require_gpu
    raise ValueError(&quot;Cannot use GPU, CuPy is not installed&quot;)
ValueError: Cannot use GPU, CuPy is not installed
</code></pre>
<p>I installed Spacy with GPU enabled version with the command <code>pip install -U 'spacy[cuda122]'</code>
While installing with the above command I got an <strong>warning like spacy 3.7.5 does not provide the extra 'cuda122'</strong></p>
<p>Can anyone please tell me what is the error in my installation?</p>
","2024-07-11 15:49:51","0","Question"
"78734964","","Ollama isnt using my gpu on a runpod.io pod","<p>I am testing different AI models on runpod.io. One of those models is dolphin-mixtral:8x22b. I followed Runpod's tutorial for setting up the pod with Ollama: <a href=""https://docs.runpod.io/tutorials/pods/run-ollama"" rel=""nofollow noreferrer"">https://docs.runpod.io/tutorials/pods/run-ollama</a>, and I used the H100SXM with 80GB VRAM and 16 vCPU 125 GB RAM.</p>
<p>However, when I start the model and ask it something like &quot;hey,&quot; it uses 100% of the CPU and 0% of the GPU, and the response takes 5-10 minutes.</p>
<p>How to make Ollama use my GPU?</p>
<p>I tried different server settings</p>
","2024-07-11 10:28:23","0","Question"
"78731201","78697900","","<p>The import works when I don't install the GPU-specific version of DGL</p>
<p><code>!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/repo.html</code></p>
<p>Not a real fix if you need CUDA, but it implies that the CUDA versioning mismatch between Colab CUDA (12.2) and DGL (looks like it only supports up to 12.1) is the issue</p>
","2024-07-10 14:34:28","0","Answer"
"78730145","78728869","","<p>The model expects an image in the form of a NumPy array or a tensor, but you're passing a file path as a string directly. You need to read the image from the file first and then pass it to the model.</p>
<p>Something like this:</p>
<pre><code> # Image path
img_path = 'WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg'

# Read the image using OpenCV
img = cv2.imread(img_path) # height, width, channel
img = img.swapaxes(0,2)    # channel, height, width
img = np.expand_dims(img,0) # batch, channel, height, width
</code></pre>
<p>and then pass the <code>img</code> to your <code>model</code></p>
<pre><code># Inference
res = model(img)
</code></pre>
","2024-07-10 11:01:33","2","Answer"
"78730054","78681145","","<p><code>pip install --force-reinstall -v &quot;numpy==1.25.2&quot;</code></p>
<p>Fixed the issue for me.</p>
<p>This was following this github discussion from : <a href=""https://github.com/stitionai/devika/issues/606"" rel=""noreferrer"">https://github.com/stitionai/devika/issues/606</a></p>
<p>All thanks to @HOBE for the comment above</p>
","2024-07-10 10:39:51","14","Answer"
"78728869","","loading Yolov9 model trained on custom dataset: AttributeError: 'str' object has no attribute 'shape'","<p>I have trained a yolov9 model on custom dataset for instance segmentation, now I want to get segmentation area after segmentation.</p>
<p>An output like the given below image but for each and every object segmented in the image.</p>
<p><a href=""https://i.sstatic.net/jyJKp1VF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jyJKp1VF.png"" alt=""enter image description here"" /></a></p>
<pre><code>from pathlib import Path
import numpy as np
import torch
import cv2


model = torch.hub.load('.', 'custom', path='yolov9-inst/runs/train-seg/gelan-c-seg15/weights/best.pt', source='local') 
# Image
img = 'WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg'
# Inference
res = model(img)
</code></pre>
<p>But I am getting this error while finding res only.</p>
<pre><code>YOLO 🚀 v0.1-104-g5b1ea9a Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (NVIDIA RTX A5000, 24248MiB)

Fusing layers... 
gelan-c-seg-custom summary: 414 layers, 27364441 parameters, 0 gradients, 144.2 GFLOPs
WARNING ⚠️ YOLO SegmentationModel is not yet AutoShape compatible. You will not be able to run inference with this model.
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[84], line 6
      4 img = 'WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg'
      5 # Inference
----&gt; 6 results = model(img)

File /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-&gt; 1518     return self._call_impl(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File /workspace/yolov9-inst/./models/common.py:868, in DetectMultiBackend.forward(self, im, augment, visualize)
    866 def forward(self, im, augment=False, visualize=False):
    867     # YOLO MultiBackend inference
--&gt; 868     b, ch, h, w = im.shape  # batch, channel, height, width
    869     if self.fp16 and im.dtype != torch.float16:
    870         im = im.half()  # to FP16

AttributeError: 'str' object has no attribute 'shape'
</code></pre>
<p>Please can anyone help me to resolve this issue</p>
","2024-07-10 06:10:32","-1","Question"
"78728473","78712626","","<p>Pytorch uses the <a href=""https://en.wikipedia.org/wiki/Chain_rule"" rel=""nofollow noreferrer"">chain rule</a> to compute gradients via <a href=""https://en.wikipedia.org/wiki/Backpropagation"" rel=""nofollow noreferrer"">backpropagation</a>. This requires moving backward through the entire computational graph.</p>
<p>Your computational graph is roughly <code>encoder -&gt; decoder -&gt; loss</code>. This means that backproping the loss to the encoder requires going through the decoder. If you remove the decoder weights from the backprop graph, there is no way for the loss to backprop to your encoder.</p>
<p>You don't have to train the decoder (ie you don't need to pass the decoder weights to the optimizer param dict), but you do need to run the decoder forward pass with gradient tracking.</p>
","2024-07-10 03:23:11","1","Answer"
"78727829","","PyTorch Model Performs Poorly in Flask Server Compared to Local Execution","<p>I have a PyTorch model that generates images with high quality and speed when I run it locally. However, when I deploy the same code in a Flask server, the generated images are of much lower quality and the process is extremely slow.</p>
<p>Details:</p>
<h1>Local Environment:</h1>
<p>OS: Windows 10</p>
<p>Python Version: 3.12</p>
<p>PyTorch Version: (2.3.1+cu121)</p>
<p>GPU:</p>
<p><a href=""https://i.sstatic.net/zOHzaBp5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zOHzaBp5.jpg"" alt=""GPU information"" /></a></p>
<h1>Server Environment:</h1>
<p>Deployment: Flask</p>
<p>Server: waitress</p>
<p>Hosting: local</p>
<p>Resources: GPU</p>
<p>Code Snippet (alone):</p>
<pre><code>from diffusers import StableDiffusionPipeline
from torch import float16
pipeline = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', torch_dtype=float16)
pipeline.safety_checker = lambda images, **kwargs: (images, [False] * len(images))
pipeline.to('cuda')
pipeline.enable_attention_slicing()
pipeline('best quality, high quality, photorealistic, an astronaut riding a white horse in space ', num_inference_steps=20, negative_prompt='bad quality, low quality', num_images_per_prompt=1, height=800, width=1000).images[0].save('1.png')
</code></pre>
<p>Result (less than 1 minute):</p>
<p><a href=""https://i.sstatic.net/QspCVShn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QspCVShn.png"" alt=""perfect result"" /></a></p>
<p>Code Snippet (flask):</p>
<pre><code>from flask import Flask
from diffusers import StableDiffusionPipeline
from torch import float16
app = Flask(__name__)
pipeline = StableDiffusionPipeline.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;, torch_dtype=float16) # stabilityai/sdxl-turbo
pipeline.to(&quot;cuda&quot;)
pipeline.enable_attention_slicing()
@app.route('/texttoimage/generate', methods=['POST'])
def ttig():
    global count
    if eval(request.cookies['season']) in (user := info[request.cookies['name'].encode()])[1]:
        user = user[4]
        pipeline.safety_checker = lambda images, **kwargs: (images, [False if request.args['NSFW'] == 'true' else True] * len(images))
        images = pipeline(request.args['prompt'], negative_prompt=request.args.get('negative'), num_inference_steps=int(request.args['quality']), num_images_per_prompt=int(request.args['count']), height=int(request.args['height']) // 8 * 8, width=int(request.args['width']) // 8 * 8).images
        for k,j in enumerate(images):
            user[f&quot;{count + k}.{request.args['type']}&quot;] = 'a'
            j.save(f&quot;s/{count + k}.{request.args['type']}&quot;)
        count += len(images)
        return str(count)
if __name__ == '__main__':
    from waitress import serve
    serve(app, port=80)
</code></pre>
<p>Result (around 10 minutes):</p>
<p>[<img src=""https://i.sstatic.net/M6ghfzpB.jpg"" alt=""terrible result(https://i.sstatic.net/M6ghfzpB.jpg)"" /></p>
<p><strong>Steps Tried:</strong></p>
<ul>
<li>Ensured the model is loaded only once.</li>
<li>Tested in both debug and non-debug modes of Flask.</li>
<li>Verified the server has adequate resources.</li>
</ul>
<p><strong>Questions:</strong></p>
<ol>
<li>What could be causing the slow performance and lower image quality in the Flask server?</li>
<li>Are there any best practices for deploying PyTorch models with Flask to ensure optimal performance?</li>
<li>Could the choice of WSGI server or server configuration be impacting the performance?</li>
</ol>
<p>Any guidance or suggestions would be greatly appreciated!</p>
<p><strong>Edit:</strong></p>
<p>I tested the same thing on FastAPI and it was the same so it's not a problem with flaks thus I'll remove flask tag.</p>
","2024-07-09 21:40:34","6","Question"
"78726426","78726277","","<p>You could always pin <code>cuda-version</code> to another channel.</p>
<pre><code>conda install -c pytorch -c nvidia conda-forge::cuda-version=12.5 torchvision torchaudio pytorch-cuda=12.1 
</code></pre>
","2024-07-09 15:12:14","3","Answer"
"78726277","","SHA256 mismatch for ""conda install pytorch""","<p>I'm trying to install pytorch+cuda12.1 in my conda env.</p>
<p>I've followed the instructions on <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a> and ran the following command:</p>
<pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
</code></pre>
<p>The command runs smoothly for most packages, except for cuda-version, for which i get:</p>
<pre><code>ChecksumMismatchError: Conda detected a mismatch between the expected content and downloaded content
for url 'https://conda.anaconda.org/nvidia/noarch/cuda-version-12.5-3.tar.bz2'.
  download saved to: C:\Users\UserName\anaconda3\pkgs\cuda-version-12.5-3.tar.bz2
  expected sha256: f13836396c27d22ff8fd446534aa3cdc0c92b1d68e5181ff2680da4c2759b117
  actual sha256: a85ae72fb1b40651095208ebe42cd595980a916e324505610ac8fc76c49164f5
</code></pre>
<p>I checked <a href=""https://conda.anaconda.org/nvidia/noarch/"" rel=""nofollow noreferrer"">https://conda.anaconda.org/nvidia/noarch/</a>, and it seems that the actual SHA256 is correct, and the expected SHA256 is actually the wrong one:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Filename</th>
<th>Size</th>
<th>Last Modified</th>
<th>SHA256</th>
<th>MD5</th>
</tr>
</thead>
<tbody>
<tr>
<td>cuda-version-12.5-3.tar.bz2</td>
<td>16 KB</td>
<td>2024-04-16 03:01:46 +0000</td>
<td>a85ae72fb1b40651095208ebe42cd595980a916e324505610ac8fc76c49164f5</td>
<td>be95c91143996f0c33cb710cd1a39450</td>
</tr>
</tbody>
</table></div>
<p>I found <a href=""https://stackoverflow.com/questions/68719486/checksummismatcherror-conda-detected-a-mismatch-between-the-expected-content-an"">some old issues</a> about how the PyTorch maintainers had some issues uploading some time ago, but it was supposed to be fixed already.</p>
<p>Is there something I can do to fix this issue?</p>
","2024-07-09 14:43:34","1","Question"
"78726140","78725972","","<p>It should work if you transform the MNIST dataset labels/targets into tensors as well, e.g.,</p>
<pre class=""lang-py prettyprint-override""><code>transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

def target_transform(t):
    return torch.tensor(t)

dataset1 = datasets.MNIST(
    root='./data',
    train=True,
    transform=transform,
    target_transform=target_transform,
)

...

</code></pre>
","2024-07-09 14:14:36","0","Answer"
"78726110","78719155","","<p>As a first step I would recommend profiling your module. There is detailed documentation on the same</p>
<p><a href=""https://pytorch.org/tutorials/beginner/profiler.html"" rel=""nofollow noreferrer"">Pytorch Profiling</a></p>
<ol>
<li>You can try some of the things mentioned in the documentation</li>
<li>If it does not help, you can provide the profiler output and then someone can help you better</li>
</ol>
","2024-07-09 14:10:10","-1","Answer"
"78725972","","What parameter do I need to change for it to match requirements?","<p>I am trying to train a model based on a modified MNSIT dataset so it classifies random images with label 10. I am constantly getting a Typeerror.</p>
<pre><code>transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

dataset1 = datasets.MNIST(root='./data', train=True, transform = transform)
dataset2 = datasets.MNIST(root='./data', train=False, transform=transform)
num_new_images = 7000
noisy_images = torch.randn(num_new_images, 1, 28, 28)
mean = 0.1307
std = 0.3081
random_images = (noisy_images-mean)/std
noisy_labels = torch.full((num_new_images,),10, dtype=torch.long)
new_dataset = torch.utils.data.TensorDataset(noisy_images, noisy_labels)
combined_dataset = torch.utils.data.ConcatDataset([dataset1, new_dataset])
len(combined_dataset)   
num_val_images = 1000
noisy_images = torch.randn(num_val_images, 1, 28, 28)
random_val_images = (noisy_images-mean)/std
noisy_val_labels = torch.full((num_val_images,),10, dtype=torch.long)
new_val_dataset = torch.utils.data.TensorDataset(random_val_images, noisy_val_labels)
combined_val_dataset = torch.utils.data.ConcatDataset([dataset2, new_val_dataset])
batch_size = 128
train_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(combined_val_dataset, batch_size=batch_size, shuffle=False)
</code></pre>
<p>Error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Cell In[12], line 74
     72 # Train the neural network
     73 for epoch in range(num_epochs):
---&gt; 74     for images, labels in train_loader:
     75         outputs = model(images)
     76         loss = criterion(outputs, labels)

File ~\PycharmProjects\tensorflow_start\venv\Lib\site-packages\torch\utils\data\dataloader.py:633, in _BaseDataLoaderIter.__next__(self)
    630 if self._sampler_iter is None:
    631     # TODO(https://github.com/pytorch/pytorch/issues/76750)
    632     self._reset()  # type: ignore[call-arg]
--&gt; 633 data = self._next_data()
    634 self._num_yielded += 1
    635 if self._dataset_kind == _DatasetKind.Iterable and \
    636         self._IterableDataset_len_called is not None and \
    637         self._num_yielded &gt; self._IterableDataset_len_called:
</code></pre>
<p>I have already tried to change the datatypes of the labels, but it didn't work</p>
","2024-07-09 13:43:42","0","Question"
"78724717","78723449","","<p>I dont think there is any issue with your code</p>
<p><a href=""https://i.sstatic.net/0ku1ijVC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0ku1ijVC.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>Can you mention the Pytorch version that you are using</li>
<li>Can you also run this piece of code in a new session</li>
</ol>
","2024-07-09 09:15:44","0","Answer"
"78724398","78714445","","<p>After revisiting the above-mentioned documentation, I noticed that registering hooks on nodes refers to <code>grad_fn.register_hook</code> and that there are 2 different hooks for nodes: one executed before the node is executed, and one after. In my code above, I only registered hooks that are run after the node is executed, so when I run my training code and the backward operator reported an error, I couldn't see the currently run operator, only the last successful operator. After I registered a <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.register_prehook.html#torch.autograd.graph.Node.register_prehook"" rel=""nofollow noreferrer"">prehook</a> on the node, it worked:</p>
<pre><code>def register_hooks_on_grads(grad_fn, make_hook_fn):
    if not grad_fn:
        return
    prehook, posthook = make_hook_fn(grad_fn)
    if prehook:
        grad_fn.register_prehook(prehook)
    if posthook:
        grad_fn.register_hook(posthook)
    for fn, _ in grad_fn.next_functions:
        if not fn:
            continue
        var = getattr(fn, &quot;variable&quot;, None)
        if var is None:
            register_hooks_on_grads(fn, make_hook_fn)
</code></pre>
<p>The pre-hook is executed before the backward function is executed and gets as input the current gradients from the preceding backward function. The post-hook is executed after the backward function and additionally gets the output of <code>grad_fn</code>.</p>
<p>In fact, I can use <code>grad_fn(*args, **kwargs)</code> to replicate the backward computation, where <code>args</code> and <code>kwargs</code> are the input to the prehook function.</p>
","2024-07-09 08:05:27","1","Answer"
"78724110","78168447","","<p>Using <code>no_cuda=True</code> solved it for me on an M1 mac.
Like so</p>
<pre><code>training_args = TrainingArguments(
        ...,
        no_cuda=True
    )
</code></pre>
","2024-07-09 06:57:15","7","Answer"
"78723449","","ImportError: cannot import name 'is_sparse_any' from 'torch._subclasses.meta_utils' in PyTorch","<p>I am encountering an import error while trying to define and train a Seq2Seq model using PyTorch. Below is the code snippet I am working with:</p>
<pre><code>import torch
import torch.nn as nn
from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, config):
        super().__init__()
        self.encoder = encoder(config)
        self.decoder = decoder(config)
        
    def forward(self, src, tgt):
        encoder_output = self.encoder(src)
        decoder_output = self.decoder(encoder_output, tgt)
        return decoder_output

model = Seq2Seq(TransformerEncoderLayer, TransformerDecoderLayer, config)
optimizer = torch.optim.Adam(model.parameters())
</code></pre>
<p>However, when I run this code, I get the following error at the last line:</p>
<pre><code>ImportError: cannot import name 'is_sparse_any' from 'torch._subclasses.meta_utils'
</code></pre>
<p>What I've Tried:</p>
<p>Checked the import statements to ensure they are correct.
Verified that my PyTorch installation is up-to-date.
Searched online for similar issues but couldn't find a clear solution.</p>
<p>How can I resolve the</p>
<pre><code>ImportError: cannot import name 'is_sparse_any' from 'torch._subclasses.meta_utils' error?
</code></pre>
<p>Is it related to an issue with my environment variables or a bug in the PyTorch version I am using?</p>
<p>Additional Information:</p>
<p>I have verified that torch is correctly installed by running other basic PyTorch scripts.
This error appears to be related to the environment setup, but I am not sure how to fix it.
Thank you for your help!</p>
","2024-07-09 02:29:44","0","Question"
"78723310","78722763","","<p>Pytorch supports a number of dtypes - you can see the documentation <a href=""https://pytorch.org/docs/stable/tensor_attributes.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Specific operations (like taking the determinant) are implemented for specific dtypes.</p>
<p>From the <code>torch.linalg.det</code> <a href=""https://pytorch.org/docs/stable/generated/torch.linalg.det.html#torch.linalg.det"" rel=""nofollow noreferrer"">documentation</a>:</p>
<pre><code>Computes the determinant of a square matrix.

Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.
</code></pre>
<p>Your input <code>t2</code> is of type <code>int64</code>, also called <code>long</code>:</p>
<pre class=""lang-py prettyprint-override""><code>t2 = torch.tensor([[1,2],[3,4]])
print(t2.dtype)
&gt;torch.int64
</code></pre>
<p><code>int64</code> is not a supported dtype for <code>torch.linalg.det</code>, so it throws the error. You can fix this by casting to a supported dtype, for example a float tensor:</p>
<pre class=""lang-py prettyprint-override""><code>t2 = torch.tensor([[1,2],[3,4]]).float() # `.float()` casts to float dtype
det=torch.det(t2)
print(det)
&gt;tensor(-2.)
</code></pre>
<p>For your second example, <code>torch.dot</code> expects the inputs to have the same dtype, while your tensors have different dtypes. <code>t1</code> is of dtype <code>int64</code>, while <code>t3</code> is of dtype <code>int32</code>. This is because your numpy array is of dtype <code>int32</code> and <code>torch.from_numpy</code> preserves the numpy dtype. You can fix this by casting <code>t3</code> to the same dtype as <code>t1</code></p>
<pre class=""lang-py prettyprint-override""><code>t1 = torch.tensor([1,2,3])
np_array = np.array([5,6,7])
t3 = torch.from_numpy(np_array).long() # `.long()` casts to `int64`
dot_product = torch.dot(t1,t3)
print(dot_product)
&gt; tensor(38)
</code></pre>
","2024-07-09 01:07:52","2","Answer"
"78722763","","Frustrated with pytorch data type in basic tensor operation, how to make it easier?","<p>I am new to pytorch. I am quite frustrated with basic operation with different data types,</p>
<p>for example, in calculating basic determinant,</p>
<pre><code>t2=torch.tensor([[1,2],[3,4]])
print(t2)
tensor([[1, 2],
        [3, 4]])

det=torch.det(t2)
print(det)

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[30], line 1
----&gt; 1 det=torch.det(t2)
      2 print(det)

RuntimeError: linalg.det: Expected a floating point or complex tensor as input. Got Long

</code></pre>
<p>why calculating this determinant has to require input floating point matrix?</p>
<p>Similarly, below dot product has an error too,</p>
<pre><code>t1=torch.tensor([1,2,3])
print(t1)
np_array=np.array([5,6,7])
t3=torch.from_numpy(np_array)
print(t3)
tensor([5, 6, 7], dtype=torch.int32)

dot_product=torch.dot(t1,t3)
print(dot_product)

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[56], line 1
----&gt; 1 dot_product=torch.dot(t1,t3)
      2 print(dot_product)

RuntimeError: dot : expected both vectors to have same dtype, but found Long and Int

</code></pre>
<p>similarly</p>
<pre><code>inverse=torch.inverse(t2)
print(inverse)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[57], line 1
----&gt; 1 inverse=torch.inverse(t2)
      2 print(inverse)

RuntimeError: linalg.inv: Expected a floating point or complex tensor as input. Got Long
</code></pre>
<p>The above are just basic matrix operation with all integer variables. Why doing this basic operation involves these basic data type error? How to make it easier?</p>
<p>Thanks</p>
","2024-07-08 20:36:24","1","Question"
"78721469","78720028","","<p>FiftyOne doesn't support index slicing as the error is suggesting. The solution can be passing in the filepath instead.</p>
<pre><code>sample = self.samples[img_path]
</code></pre>
<p>Should do the trick. Read more here <a href=""https://docs.voxel51.com/user_guide/using_views.html#slicing"" rel=""nofollow noreferrer"">https://docs.voxel51.com/user_guide/using_views.html#slicing</a></p>
","2024-07-08 14:40:39","0","Answer"
"78721301","78390559","","<p>The quickest fix would be to replace these added lines in <a href=""https://github.com/pytorch/pytorch/commit/fdfef759a676ee7a853872e347537bc1e4b51390"" rel=""nofollow noreferrer"">this github post</a> in torch package's <strong>init</strong>.py</p>
","2024-07-08 14:06:14","0","Answer"
"78721296","78721195","","<p>Your code seems to be written in <code>numpy &lt; 2.0</code> compatible way, but it looks like  you are running it with <code>numpy &gt;=2</code>.
Try downgrading the numpy version to <code>&lt; 2.0</code>.</p>
","2024-07-08 14:05:49","4","Answer"
"78721195","","AttributeError: `np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead.. Did you mean: 'strings'?","<p>I m interested in seeing neural network as graph using tensorboard. I have constructed a network in pytorch with following code-</p>
<pre><code>import torch
BATCH_SIZE = 16
DIM_IN = 1000
HIDDEN_SIZE = 100
DIM_OUT = 10

class TinyModel(torch.nn.Module):

    def __init__(self):
        super(TinyModel, self).__init__()

        self.layer1 = torch.nn.Linear(DIM_IN, HIDDEN_SIZE)
        self.relu = torch.nn.ReLU()
        self.layer2 = torch.nn.Linear(HIDDEN_SIZE, DIM_OUT)

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        return x

some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)
ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)

model = TinyModel()
</code></pre>
<p>Setting-up tensorboard</p>
<pre><code>from torch.utils.tensorboard import SummaryWriter

# Create a SummaryWriter
writer = SummaryWriter(&quot;checkpoint&quot;)

# Add the graph to TensorBoard
writer.add_graph(model, some_input)
writer.close()
</code></pre>
<p>While I run <code>tensorboard --logdir=checkpoint</code> on terminal , I receive the following error -</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/k/python_venv/bin/tensorboard&quot;, line 5, in &lt;module&gt;
    from tensorboard.main import run_main
  File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/main.py&quot;, line 27, in &lt;module&gt;
    from tensorboard import default
  File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/default.py&quot;, line 39, in &lt;module&gt;
    from tensorboard.plugins.hparams import hparams_plugin
  File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/plugins/hparams/hparams_plugin.py&quot;, line 30, in &lt;module&gt;
    from tensorboard.plugins.hparams import backend_context
  File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/plugins/hparams/backend_context.py&quot;, line 26, in &lt;module&gt;
    from tensorboard.plugins.hparams import metadata
  File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/plugins/hparams/metadata.py&quot;, line 32, in &lt;module&gt;
    NULL_TENSOR = tensor_util.make_tensor_proto(
  File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/util/tensor_util.py&quot;, line 405, in make_tensor_proto
    numpy_dtype = dtypes.as_dtype(nparray.dtype)
  File &quot;/home/k/python_venv/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py&quot;, line 677, in as_dtype
    if type_value.type == np.string_ or type_value.type == np.unicode_:
  File &quot;/home/k/python_venv/lib/python3.10/site-packages/numpy/__init__.py&quot;, line 397, in __getattr__
    raise AttributeError(
AttributeError: `np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead.. Did you mean: 'strings'? 
</code></pre>
<p>Probably the issue will be fixed in future releases, but is there a fix for now?</p>
","2024-07-08 13:44:22","8","Question"
"78720970","78715358","","<p>You can index with the <em>empty index</em> (i.e. an empty tuple <code>()</code>) into a 0-d tensor:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

z = torch.tensor(1, dtype=torch.int64)

z[()] = 5
print(z)
# &gt;&gt;&gt; tensor(5)
</code></pre>
<p>The <a href=""https://numpy.org/doc/stable/user/basics.indexing.html#detailed-notes"" rel=""nofollow noreferrer"">Numpy documentation</a>¹ states: <em>An empty (tuple) index is a full scalar index into a zero-dimensional array</em> – which is exactly what you need in this case.</p>
<p>Likewise, you can use an ellipsis <code>...</code>:</p>
<pre><code>z[...] = 42
print(z)
# &gt;&gt;&gt; tensor(42)
</code></pre>
<p>Slicing with the colon <code>:</code> requires at least one dimension being present in the tensor. Neither the empty index <code>()</code> nor ellipsis <code>...</code> have this requirement.</p>
<p><sub>¹) I did not find a corresponding statement in the PyTorch docs, but apparently PyTorch follows the same paradigm.</sub></p>
","2024-07-08 12:51:47","3","Answer"
"78720028","","fiftyone dataset with pytorch dataloader","<p>I want to do an initial training on the bounding boxes from a bigger, public dataset - fiftyone looks liek a decent starting point, but I'm having some issues with getting it to work with pytorch - I think it must be something minor that I'm missing here, I tried following the official github.
I start to suspect the examples in <a href=""https://github.com/voxel51/fiftyone-examples/blob/master/examples/pytorch_detection_training.ipynb"" rel=""nofollow noreferrer"">this</a> github is outdated somehow, because even when I try to use the example class it fails. The issue is with the access methods, where the only quasi numerical option is <code>dataset.first()</code>, <code>dataset.last()</code></p>
<p><strong>The issue:</strong>
The dataloader is throwing an exception <code>KeyError: 'Accessing samples by numeric index is not supported. Use sample IDs, filepaths, slices, boolean arrays, or a boolean ViewExpression instead'</code> <br />
I have modified the example slightly, just so it only uses car subset:</p>
<pre class=""lang-py prettyprint-override""><code>class FiftyOneDS(torch.utils.data.Dataset):
    def __init__(
                self
                , fiftyone_ds
                , transforms = None
                , gt_field = &quot;ground_truth&quot;
                , classes = None
    ):
        self.samples = fiftyone_ds
        self.transforms = transforms
        self.gt_field = gt_field
        self.classes = classes  # don't care
        self.img_paths = self.samples.values(&quot;filepath&quot;)
        
    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        sample = self.samples[idx]
        metadata = sample.metadata
        img = Image.open(img_path).convert(&quot;RGB&quot;)
        boxes = []
        labels = []
        detections = sample[self.gt_field].detections
        for det in detections:
            if det[&quot;label&quot;] != &quot;car&quot;:
                continue
            category_id = self.labels_map_rev[det.label]
            coco_obj = fouc.COCOObject.from_label(
                det, metadata, category_id=category_id,
            )
            x, y, w, h = coco_obj.bbox
            boxes.append([x, y, x + w, y + h])
            labels.append(coco_obj.category_id)
        target = {}
        target[&quot;boxes&quot;] = torch.as_tensor(boxes, dtype=torch.float32)
        target[&quot;labels&quot;] = torch.as_tensor(labels, dtype=torch.int64)
        target[&quot;image_id&quot;] = torch.as_tensor([idx])
        if self.transforms is not None:
            img, target = self.transforms(img, target)
        return img, target
    
        
    def __len__(self):
        return len(self.img_paths)
</code></pre>
<p>Then use the dataset via a snippet (this appears to be working ok):</p>
<pre class=""lang-py prettyprint-override""><code>carset = FiftyOneDS(dataset)
print(&quot;type:&quot;, type(torch_dataset_test))
# type: &lt;class 'fiftyone.core.view.DatasetView'&gt;
print(&quot;first elem:&quot;, torch_dataset_test[0])
# KeyError: 'Accessing samples by numeric index is not supported. Use sample IDs, filepaths, slices, boolean arrays, or a boolean ViewExpression instead'
</code></pre>
<p>How do I rewrite my dataset class to work with pytorch dataloaders?</p>
<h2>UPDATE:</h2>
<p>The solution, but seems dirty to me: (writing from memory)
The key part is storing the filename list as a list and referencing from there</p>
<pre class=""lang-py prettyprint-override""><code>class FiftyOneDS(torch.utils.data.Dataset):
    def __init__(ds, &lt;args&gt;):
        self.samples = ds
        self._impaths = self.samples.values(&quot;filepath&quot;)

    def __getitem__(self, idx):
        impath = self._impaths[idx]
        img = Image.open(img_path).convert(&quot;RGB&quot;)


</code></pre>
","2024-07-08 09:22:15","1","Question"
"78719155","","PyTorch RuntimeError: No operator found for memory_efficient_attention_forward with torch.float16 inputs on CPU","<p>I am working with a PyTorch model (AutoModelForCausalLM) using the transformers library and encountering a RuntimeError related to tensor types and operator support. Here’s a simplified version of my code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import requests
from PIL import Image
from IPython.display import display
from transformers import AutoModelForCausalLM, LlamaTokenizer

# Load tokenizer and model
tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')
model = AutoModelForCausalLM.from_pretrained(
    'THUDM/cogvlm-chat-hf',
    torch_dtype=torch.float16,  # Using torch.float16
    low_cpu_mem_usage=True,
    trust_remote_code=True
).eval()

def generate(query: str, img_url: str, max_length: int = 2048) -&gt; str:
    image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')
    display(image)

    # Generate token inputs
    inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image], template_version='vqa')

    # Convert tensors to appropriate types
    input_ids = inputs['input_ids'].unsqueeze(0).to(torch.long)
    token_type_ids = inputs['token_type_ids'].unsqueeze(0).to(torch.long)
    attention_mask = inputs['attention_mask'].unsqueeze(0).to(torch.float16)
    images = [[inputs['images'][0].to(torch.float16)]]

    inputs = {
        'input_ids': input_ids,
        'token_type_ids': token_type_ids,
        'attention_mask': attention_mask,
        'images': images,
    }
    
    gen_kwargs = {&quot;max_length&quot;: max_length, &quot;do_sample&quot;: False}
    
    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)
        outputs = outputs[:, input_ids.shape[1]:]
        return tokenizer.decode(outputs[0])

query = 'Describe this image in detail'
img_url = 'https://i.ibb.co/x1nH9vr/Slide1.jpg'
generate(query, img_url)
</code></pre>
<p>Above code throws throws the following error:</p>
<pre><code>NotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:
     query       : shape=(1, 1226, 16, 112) (torch.float16)
     key         : shape=(1, 1226, 16, 112) (torch.float16)
     value       : shape=(1, 1226, 16, 112) (torch.float16)
     attn_bias   : &lt;class 'NoneType'&gt;
     p           : 0.0
`ck_decoderF` is not supported because:
    device=cpu (supported: {'cuda'})
    operator wasn't built - see `python -m xformers.info` for more info
`ckF` is not supported because:
    device=cpu (supported: {'cuda'})
    operator wasn't built - see `python -m xformers.info` for more info
</code></pre>
<p>I'm trying to use <code>torch.float16</code> tensors with my <strong>PyTorch</strong> model on CPU <code>(device=cpu)</code>. The model is loaded with <code>torch.float16</code> using <code>AutoModelForCausalLM</code> from the <code>transformers</code> library. However, I encounter the <code>NotImplementedError</code> stating that the <code>memory_efficient_attention_forward</code> operator isn't supported on CPU with <code>torch.float16</code>.</p>
<p>Is there a way to make <code>memory_efficient_attention_forward</code> work with <code>torch.float16</code> on CPU? Are there alternative approaches or configurations I should consider to resolve this issue?</p>
<blockquote>
<p>I am trying to run this on a MacBook PRO with Intel Core i7 processor.</p>
</blockquote>
","2024-07-08 04:58:13","1","Question"
"78718554","","Training a Custom Feature Extractor in Stable Baselines3 Starting from Pre-trained Weights?","<p>I am using the following custom feature extractor for my StableBaselines3 model:</p>
<pre><code>import torch.nn as nn
from stable_baselines3 import PPO

class Encoder(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim=2):
        super(Encoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, embedding_dim),
            nn.ReLU()
        )
        self.regressor = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dim),
            nn.ReLU(),
        )
    
    def forward(self, x):
        x = self.encoder(x)
        x = self.regressor(x)
        return x
    
model = Encoder(input_dim, embedding_dim, hidden_dim)
model.load_state_dict(torch.load('trained_model.pth'))

# Freeze all layers
for param in model.parameters():
    param.requires_grad = False

class CustomFeatureExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim):
        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)
        self.model = model  # Use the pre-trained model as the feature extractor

        self._features_dim = features_dim

    def forward(self, observations):
        features = self.model(observations)
        return features

policy_kwargs = {
        &quot;features_extractor_class&quot;: CustomFeatureExtractor,
        &quot;features_extractor_kwargs&quot;: {&quot;features_dim&quot;: 64}
    }

 model = PPO(&quot;MlpPolicy&quot;, env=envs, policy_kwargs=policy_kwargs)
</code></pre>
<p>The model is trained well so far with no issues and good results. Now I want to not freeze the weights, and try to train the Feature Extractor as well starting from the initial pre-trained weight. How can I do that with such a custom Feature Extractor defined as a class inside another class? My feature extractor is not the same as the one defined in the <a href=""https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html"" rel=""nofollow noreferrer"">documentation</a>, so I am not sure if it will be trained. Or will it start training if I unfreeze the layers?</p>
","2024-07-07 22:11:00","2","Question"
"78718288","78699287","","<p>I have figured it out. I should use ModuleList to hold the instances of SubModel in WrapperModel instead using a normal list.</p>
<pre><code>self.blocks = nn.ModuleList([SubModel() for _ in range(count)])
</code></pre>
","2024-07-07 19:50:41","0","Answer"
"78717777","78691335","","<p>I think the first question should be: what are you trying to compute precisely? A persistence diagram is defined for a filtration, i.e. an increasing sequence of complexes, often represented as one complex with a value associated to each cell to indicate at which step of the filtration this cell appears.</p>
<p>An image is interpreted as a cubical complex, with the gray level indicating the filtration value of the pixel (or vertex). With a black&amp;white image, there are only 2 values, so only 3 complexes: empty → black → full. What you get is thus one point (black, white) for each black connected component (except one that is (black, infinity).</p>
<p>It is possible to define other filtrations from the image. For instance, one could define a function which, to each point in the plane, associates its distance to the closest white pixel. The persistence diagram of this new function would have almost the same number of points, all with x=0, but the y coordinate would represent the &quot;inner radius&quot; of the corresponding black region. Gudhi does not have code to compute this function, but if you compute it yourself and represent it as an image/array, you can feed that to CubicalPersistence or CubicalComplex.</p>
","2024-07-07 16:03:14","1","Answer"
"78717596","78704234","","<h1>No, but ...</h1>
<p><code>torch.where(...)</code> does not detach anything from the computational graph.</p>
<p><code>torch.where(cond, a, b)</code> has the same gradient as <code>a</code> where <code>cond</code> is <code>True</code> and the same as <code>b</code> where <code>cond</code> is <code>False</code></p>
<p>(so in essence, if <code>c = torch.where(cond, a, b)</code>, <code>c.grad</code> is <code>torch.where(cond, a.grad, b.grad)</code>)</p>
<p>In your case though, <code>a</code> and <code>b</code> are constants so all those gradients are 0, which is effectively cutting the results from the graph.</p>
<p>You say your operation is &quot;thresholding&quot;, but <strong>that is not what you are doing!</strong></p>
<p>Thresholding would be keeping the value unless it is above (or below) some threshold. What you are doing is setting the values below the threshold to <code>0</code> and the values above to 1, which is a <a href=""https://en.wikipedia.org/wiki/Heaviside_step_function"" rel=""nofollow noreferrer"">Heaviside step function</a>. It is differentiable almost everywhere, but <strong>its gradient is always 0 when defined</strong> (so unusable for optimization purposes)</p>
<h1>Fix</h1>
<p>You may want to replace that heaviside function with its differentiable approximation, the <a href=""https://en.wikipedia.org/wiki/Sigmoid_function"" rel=""nofollow noreferrer"">sigmoid</a></p>
<p>The code would be something like this</p>
<pre class=""lang-py prettyprint-override""><code>channels[tracker_index]= torch.sigmoid(channel_tensor - threshold)
</code></pre>
<p><strong>Note</strong> also that if you are looking for a loss for this data, you may want to look towards binary cross-entropy, in which case there is a (more stable) <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html"" rel=""nofollow noreferrer"">version</a> in pytorch that takes the raw logits instead of the output of the sigmoid</p>
","2024-07-07 14:43:22","0","Answer"
"78717341","","Keras training speed with PyTorch backend is a lot slower than with TensorFlow","<p>I am on native Windows and I used old Keras with TensorFlow 2.10 (GPU accelerated) before. I wanted to try Keras 3 with PyTorch backend.
Can someone please help me why this model trains 10x slower with Keras 3.4.1 and PyTorch 2.3.1 backend?
With my GPU a single epoch takes a little more than 2 minutes with TF, and over 20 minutes with PyTorch.</p>
<pre><code>import os
os.environ[&quot;KERAS_BACKEND&quot;] = &quot;torch&quot;
import torch
torch.cuda.is_available() # &lt;-- returns True

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras import optimizers
from keras.regularizers import l2

x_train, y_train = np.float32(x_train), np.float32(y_train)
x_val, y_val = np.float32(x_val), np.float32(y_val)

model=Sequential()
reg=0.00001
model.add(LSTM( 80, return_sequences=True , dropout=0.0, kernel_regularizer=l2(reg), recurrent_regularizer=l2(reg), input_shape=(x_train.shape[1], x_train.shape[2]) ))
model.add(LSTM( 80, return_sequences=False, dropout=0.0, kernel_regularizer=l2(reg), recurrent_regularizer=l2(reg) ))
model.add(Dense(40))
model.add(Dense(40))
model.add(Dense(1))
opt = optimizers.Adam(learning_rate=lrate)
model.compile(optimizer=opt, loss='mean_squared_error')

from keras.callbacks import ModelCheckpoint
from keras.callbacks import BackupAndRestore
savecallback = ModelCheckpoint(basefolder+&quot;/&quot;+modelfile, save_best_only=False, monitor='val_loss', mode='min', verbose=1)
backupcallback = BackupAndRestore(basefolder+&quot;/tmp/backup_&quot;+modelfile)

hist=model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batchsize, epochs=20, callbacks=[savecallback, backupcallback])
</code></pre>
<p>I verified GPU acceleration with both backends</p>
","2024-07-07 12:48:39","3","Question"
"78716450","78713478","","<p>What you're asking to do is impossible: <code>scan</code> lengths must be static, and vmapped values are non-static by definition.</p>
<p>What you can do instead is replace your <code>scan</code> with a <code>fori_loop</code> or a <code>while_loop</code>, and then the loop boundary does not need to be static. For example, if you implement your function this way and leave the rest of your code unchanged, it should work:</p>
<pre class=""lang-py prettyprint-override""><code>def state_predictor(xk, uk, sim_timestep):
  body_fun = lambda i, x_u: fwd_dynamics(x_u, i)[0]
  x_next, _ = lax.fori_loop(0, sim_timestep[0], body_fun, (xk, uk))
  return x_next
</code></pre>
","2024-07-07 04:30:48","1","Answer"
"78715647","78715584","","<p>You can create an initial <code>mask</code> (boolean) array that is <code>True</code> for the elements you want to remove and then invert it to give a <code>mask</code> of the elements you want to keep.</p>
<pre class=""lang-py prettyprint-override""><code>remove_mask = np.zeros(my_array.shape[0], dtype=bool)
remove_mask[remove_ixs] = True
mask = ~remove_mask
    
new_array = my_array[mask, :]
</code></pre>
<p>Or start all <code>True</code> and do the opposite:</p>
<pre class=""lang-py prettyprint-override""><code>mask = np.ones(my_array.shape[0], dtype=bool)
mask[remove_ixs] = False
    
new_array = my_array[mask, :]
</code></pre>
<p>For some reason, the first version is faster for smaller arrays.</p>
","2024-07-06 18:47:35","2","Answer"
"78715643","78715584","","<p>You can use <code>np.delete()</code>:</p>
<pre><code>import numpy as np

A = np.random.rand(5000000, 3)
remove_ixs = np.random.choice(5000000, 50000, replace=False)
B = np.delete(A, remove_ixs, axis=0)

print(len(B))

</code></pre>
<h3>Prints</h3>
<pre><code>4950000
</code></pre>
","2024-07-06 18:46:59","1","Answer"
"78715584","","Fast way to remove multiple rows by indices from a Pytorch or Numpy 2D array","<p>I have a numpy array (and equivalently a Pytorch tensor) of shape <code>Nx3</code>. I also have a list of indices corresponding to rows, that I want to remove from this tensor. This list of indices is called <code>remove_ixs</code>. <code>N</code> is very big, about 5 million rows, and <code>remove_ixs</code> is 50k long. The way I'm doing it now is as follows:</p>
<pre><code>mask = [i not in remove_ixs for i in range(my_array.shape[0])]
new_array = my_array[mask,:]
</code></pre>
<p>But the first line is just not terminating, takes forever. The above is in numpy code. An equivalent Pytorch code would also work for me.</p>
<p>Is there a faster way to do this with either numpy or pytorch?</p>
","2024-07-06 18:14:49","1","Question"
"78715363","78715358","","<p>You can do an <em>in-place</em> operation e.g. multiplication:</p>
<pre><code>z *= 5
</code></pre>
<p>You can also directly assign the value to the underlying <code>data</code> property of the tensor:</p>
<pre><code>z.data = torch.tensor(5)
</code></pre>
<p>Note that, <code>autograd</code> (Pytorch's backpropagation engine) would not track any computation made directly on a tensor's <code>data</code> attribute.</p>
","2024-07-06 16:31:07","2","Answer"
"78715358","","How to assign value to a zero dimensional torch tensor?","<pre><code>z = torch.tensor(1, dtype= torch.int64)
z[:] = 5

Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
IndexError: slice() cannot be applied to a 0-dim tensor.
</code></pre>
<p>I'm trying to assign a value to a torch tensor but because it has zero dimensions the slice operator doesn't work. How do I assign a new value then?</p>
","2024-07-06 16:24:12","4","Question"
"78714445","","Understanding and introspecting torch.autograd.backward","<p>In order to locate a bug, I am trying to introspect the backward calculation in PyTorch. Following the <a href=""https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution"" rel=""nofollow noreferrer"">description of torch's Autograd mechanics</a>, I added backward hooks to each parameter of my model as well as hooks on the <code>grad_fn</code> of each activation. The following code snippet illustrates how I add the hooks to the <code>grad_fn</code>:</p>
<pre><code>import torch.distributed as dist


def make_hook(grad_fn, note=None):
    if grad_fn is not None and grad_fn.name is not None:
        def hook(*args, **kwargs):
            print(f&quot;[{dist.get_rank()}] {grad_fn.name()} with {len(args)} args &quot;
                  f&quot;and {len(kwargs)} kwargs [{note or '/'}]&quot;)
        return hook
    else:
        return None


def register_hooks_on_grads(grad_fn, make_hook_fn):
    if not grad_fn:
        return
    hook = make_hook_fn(grad_fn)
    if hook:
        grad_fn.register_hook(hook)
    for fn, _ in grad_fn.next_functions:
        if not fn:
            continue
        var = getattr(fn, &quot;variable&quot;, None)
        if var is None:
            register_hooks_on_grads(fn, make_hook_fn)


x = torch.zeros(15, requires_grad=True)
y = x.exp()
z = y.sum()
register_hooks_on_grads(z.grad_fn, make_hook)
</code></pre>
<p>When running my model, I noticed that each invocation of <code>hook</code> gets two arguments and no key-word arguments. In case of a <code>AddBackward</code> function, the first argument is a list of two tensors, the second argument is a list of one tensor. The same holds true for the <a href=""https://github.com/NVIDIA/Megatron-LM/blob/0bc3547702464501feefeb5523b7a17e591b21fa/megatron/core/tensor_parallel/layers.py#L373"" rel=""nofollow noreferrer""><code>LinearWithGradAccumulationAndAsyncCommunicationBackward</code></a> function. In case of a <code>MeanBackward</code> function, both arguments are lists with one tensor each.</p>
<p>My conjecture to this is that the first argument probably contains the inputs to the operator (or whatever was saved with <code>ctx.save_for_backward</code>) and that the second argument contains the gradients. Am I right with this? Can I just replicate the backward computation with <code>grad_fn(*args)</code> or is there more to it (e.g., state)?</p>
<p>Unfortunately, I didn't find any documentation on this. I am grateful for any pointer towards the relevant documentation.</p>
","2024-07-06 09:35:25","0","Question"
"78713478","","Jax vmap with lax scan having different sequence length in batch dimension","<p>I have this following code , where my sim_timestep is in batch
I am not able to run this since the lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) ) requires the concrete array , but since I have vmapped the state_predictor function the sim_timestep is being as a tracedArray .
Any help would be greatly appreciated .
Thanks all</p>
<pre class=""lang-py prettyprint-override""><code>from jax import random
from jax import lax
import jax
import jax.numpy as jnp
import pdb


def fwd_dynamics(x_u, xs):
    x0,uk =  x_u
    Delta_T = 0.001
    lwb = 1.2
    psi0=x0[2][0]
    v0= x0[3][0]
    vdot0 = uk[0][0]
    delta0 = uk[1][0]
    thetadot0 = uk[2][0]
        
    xdot= jnp.asarray([[v0*jnp.cos(psi0) ],
        [v0*jnp.sin(psi0)] ,
        [v0*jnp.tan(delta0)/(lwb)],
        [vdot0],
        [thetadot0]])
    x_next = x0 + xdot*Delta_T
    return (x_next,uk), x_next  # (&quot;carryover&quot;, &quot;accumulated&quot;)


def state_predictor( xk,uk ,sim_timestep):
    (x_next,_), _ = lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) )
    return x_next

low = 0  # Adjust minimum value as needed
high = 100  # Adjust maximum value as needed
key = jax.random.PRNGKey(44)

sim_time = jax.random.randint(key, shape=(10, 1), minval=low, maxval=high)

xk = jax.random.uniform(key, shape=(10,5, 1))
uk = jax.random.uniform(key, shape=(10,2, 1))

state_predictor_vmap = jax.jit(jax.vmap(state_predictor,in_axes= 0 ,out_axes=0 ))
x_next = state_predictor_vmap( xk,uk ,sim_time)
print(x_next.shape)
</code></pre>
<p>I tried to solve it by above code , hoping to get alternative way to achieve the same functionality.</p>
","2024-07-05 23:10:57","1","Question"
"78712685","","How to calculate the Params and MACs of Kolmogorov-Arnold Networks(KAN)?","<p>I used efficient_kan(<a href=""https://github.com/Blealtan/efficient-kan"" rel=""nofollow noreferrer"">https://github.com/Blealtan/efficient-kan</a>) to build my KAN layer. And I noticed that get_model_complexity_info from ptflops can not calculate the Params and MACs of KAN_Linear.<a href=""https://i.sstatic.net/H3nGuirO.png"" rel=""nofollow noreferrer"">0 Param and MAC</a></p>
<p>I tried some other methods but they didn't work, such as profile from thop.What should i do?</p>
","2024-07-05 17:23:49","-1","Question"
"78712626","","Pytorch, use loss that don't return gradient","<p>I'm trying to develop a model that improves the quality of a given audio. For this task I use <a href=""https://github.com/descriptinc/descript-audio-codec"" rel=""nofollow noreferrer"">DAC</a> for the latent space and I run a transformer model to change the value of the latent space to improve the audio. Then I use the decoder of the DAC model to decode the audio and generate the waveform. So what I do is: noise audio &gt; Encode audio &gt; latent space A &gt; transformer model (that I train) &gt; latent space B &gt; Decode audio &gt; denoised audio</p>
<p>I want to test the approach of training my transformer model on the raw audio itself (not on the latent space B) so I need to decode the audio generated during the training and then use the L1 loss between the noised audio and the denoised generated audio. The problem is that I can't keep the gradient of my transformer during the decoding process (too much RAM used and I don't want to train the decoder). I know that PyTorch uses the chain rule to compute the gradient but why does it need to know the gradient of the decoder to change the weights of my model? The decoding process is only useful to compute the loss so I can imagine that the chain rule stops at the end of my transformer model. It can compute the gradient on the result of my L1 loss and then change the weights of the transformer model only with the result of the loss.</p>
<p>Currently I have the error:</p>
<pre><code>loss.backward()
File &quot;/home/jourdelune/Bureau/dev/WaveAI/AudioEnhancer/venv/lib/python3.10/site-packages/torch/_tensor.py&quot;, line 492, in backward
    torch.autograd.backward(
File &quot;/home/jourdelune/Bureau/dev/WaveAI/AudioEnhancer/venv/lib/python3.10/site-packages/torch/autograd/__init__.py&quot;, line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>I have tried some approaches, but what I expect is to train my model like this:</p>
<pre class=""lang-py prettyprint-override""><code>y_hat = model(x)  # prediction of the model
y_hat = rearrange(y_hat, &quot;b (t c) d -&gt; b c d t&quot;, c=c, d=d)  # transform the data to have the latent space dim
loss = 0
for i in range(y_hat.shape[0]):  # for each value in the batch, DAC doesn't support decoding of batches
    with torch.no_grad():  # disable gradient because too much RAM
        z_q, _, _, _ = dataset.autoencoder.quantizer(y_hat[i].float(), None)
        decoded = dataset.autoencoder.decode(z_q)  # RAW audio
    out = l1_loss(decoded, base_waveform[i][:, :, : decoded.shape[-1]])  # compute the distance between the base waveform (target) and the decoded audio (pred)
    loss += out
loss.backward()  # backpropagation
</code></pre>
<p>I have tried to do:</p>
<pre class=""lang-py prettyprint-override""><code>loss = torch.tensor(loss, requires_grad=True)
loss.backward()
</code></pre>
<p>but the model doesn't progress at all. So my question is, is it possible to do it this way?</p>
","2024-07-05 17:05:32","0","Question"
"78709643","78709614","","<p>It's normal that you can use much large batch sizes on CPU vs GPU (but much slower).</p>
<p>In general with language models, the memory used per batch is a function of the sequence lengths (and obviously the batch size). If you can set this lower you may not experience as many out-of-memory issues. If for example you're using the huggingface library you can set the maximum sequence length and truncate using:</p>
<pre><code>tokenizer(text, truncation=True, max_length=128)
</code></pre>
<p>This may of course cause problems if you have long and informative text, but sometimes out-of-memory can surface in a single pathological batch because it contains as single example that's much longer than normal. Because each batch is (usually) padded to the length of the longest example, the entire batch becomes much larger than usual. This causes training to fail part way through the epoch.</p>
","2024-07-05 04:18:09","1","Answer"
"78709614","","torch.cuda.OutOfMemoryError when training model on GPU, but not for larger batch sizes on CPU","<p>I am working on training a MultiModal model in PyTorch. I have a training loop which runs just fine (albeit slowly) on my CPU (I tested up to batch size = 32). However, when I try to run it on a GPU (Tesla P40), it only works up to batch size = 2. With larger batch sizes it throws a torch.cuda.OutOfMemoryError. I am working with pre embedded video and audio, and pre tokenized text. Is it possible that the GPU can really not handle batch sizes larger than 2 or could there be something wrong in my code? Do you have any advice on how I might go about troubleshooting? I apologize for this simple question, it is my first time working with a GPU cluster. I am running this code on my university's GPU cluster and have double checked that the GPU I am using is not being used by anyone else.</p>
<p>I tried to examine memory usage on both the CPU and the GPU. By running nvidia-smi, I found that each GPU has a memory limit of 23040 MiB. I used <code>print(f'Allocated: {torch.cuda.memory_allocated() / 1024**2} MB') print(f'Cached: {torch.cuda.memory_reserved() / 1024**2} MB')</code> to track the memory usage of the GPU and found that after all data is loaded with a batch size of 8, only around 500 MB are allocated and around 2000 MB are cached. The error tends to occur when calling BERT to embed the text tokens, but may occur later with smaller batch sizes. I also double checked that all tensors are being properly loaded onto the GPU.</p>
","2024-07-05 04:01:40","0","Question"
"78707989","78689702","","<p>You are correct the model layer weights for <code>bert.pooler.dense.bias</code> and <code>bert.pooler.dense.weight</code> are initialized randomly. You can initialize these layers always the same way for a reproducible output, but I doubt the inference code that you have copied from there <a href=""https://github.com/qiyuw/PeerCL/blob/main/README.md"" rel=""nofollow noreferrer"">readme</a> is correct. As already mentioned by you the pooling layers are not initialized and their <a href=""https://github.com/qiyuw/PeerCL/blob/5e40ea5a6e94202c597c8ef51ed464c18e08d191/pcl/models.py#L407"" rel=""nofollow noreferrer"">model class</a> also makes sure that the pooling_layer is not added:</p>
<pre class=""lang-py prettyprint-override""><code>...
self.bert = BertModel(config, add_pooling_layer=False)
...
</code></pre>
<p>The evaluation script of the repo should be called, according to the readme with the following command:</p>
<pre class=""lang-bash prettyprint-override""><code>python evaluation.py --model_name_or_path qiyuw/pcl-bert-base-uncased --mode test --pooler cls_before_pooler
</code></pre>
<p>When you look into it, your inference code for <a href=""https://huggingface.co/qiyuw/pcl-bert-base-uncased"" rel=""nofollow noreferrer"">qiyuw/pcl-bert-base-uncased</a> should be the following way:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from scipy.spatial.distance import cosine
from transformers import AutoModel, AutoTokenizer

# Import our models. The package will take care of downloading the models automatically
tokenizer = AutoTokenizer.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)

# Tokenize input texts
texts = [
    &quot;There's a kid on a skateboard.&quot;,
    &quot;A kid is skateboarding.&quot;,
    &quot;A kid is inside the house.&quot;
]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=&quot;pt&quot;)

# Get the embeddings
with torch.inference_mode():
    embeddings = model(**inputs)
    embeddings = embeddings.last_hidden_state[:, 0]

# Calculate cosine similarities
# Cosine similarities are in [-1, 1]. Higher means more similar
cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])
cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])

print(&quot;Cosine similarity between \&quot;%s\&quot; and \&quot;%s\&quot; is: %.3f&quot; % (texts[0], texts[1], cosine_sim_0_1))
print(&quot;Cosine similarity between \&quot;%s\&quot; and \&quot;%s\&quot; is: %.3f&quot; % (texts[0], texts[2], cosine_sim_0_2))
</code></pre>
<p>Output:</p>
<pre><code>Cosine similarity between &quot;There's a kid on a skateboard.&quot; and &quot;A kid is skateboarding.&quot; is: 0.941
Cosine similarity between &quot;There's a kid on a skateboard.&quot; and &quot;A kid is inside the house.&quot; is: 0.779
</code></pre>
<blockquote>
<p>Can I make the output reproducible by initialising/seeding the model differently?</p>
</blockquote>
<p>Yes, you can. Use <a href=""https://pytorch.org/docs/stable/generated/torch.manual_seed.html"" rel=""nofollow noreferrer"">torch.maunal_seed</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoModel, AutoTokenizer

model_random = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)
torch.manual_seed(42)
model_repoducible1 = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)

torch.manual_seed(42)
model_repoducible2 = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)

print(torch.allclose(model_random.pooler.dense.weight, model_repoducible1.pooler.dense.weight))
print(torch.allclose(model_random.pooler.dense.weight, model_repoducible2.pooler.dense.weight))
print(torch.allclose(model_repoducible1.pooler.dense.weight, model_repoducible2.pooler.dense.weight))
</code></pre>
<p>Output:</p>
<pre><code>False
False
True
</code></pre>
","2024-07-04 15:44:29","1","Answer"
"78706831","78704861","","<p>The linear layer implements an <a href=""https://en.wikipedia.org/wiki/Affine_transformation#Augmented_matrix"" rel=""nofollow noreferrer"">affine transformation</a>, that is a matrix multiplication and translation by a bias vector, which is applied to all matrices in any tensor viewed as some layout of matrices.</p>
<hr />
<p>It is better to consider your second example for demonstration.
Your (slightly modified) second example:</p>
<pre><code>inp = torch.rand(1,2,3,4) # B,C,W,H
print(f'inp ({inp.shape}):\n{inp}')
linear = nn.Linear(4,5)
print(f'weight ({linear.weight.shape}):\n{linear.weight}')
print(f'bias {linear.bias.shape}:\n{linear.bias}')
out = linear(inp)
print(f'out ({out.shape}):\n{out}')
</code></pre>
<p>prints (might be different for you due to randomness)</p>
<pre><code>inp (torch.Size([1, 2, 3, 4])):
tensor([[[[0.3340, 0.6843, 0.6702, 0.9667],
          [0.9990, 0.3094, 0.7772, 0.7851],
          [0.3004, 0.6993, 0.3088, 0.5238]],

         [[0.5257, 0.9793, 0.2408, 0.4065],
          [0.7183, 0.8921, 0.8280, 0.1272],
          [0.7826, 0.2930, 0.1266, 0.8724]]]])
weight (torch.Size([5, 4])):
Parameter containing:
tensor([[ 0.2177, -0.0575, -0.4756, -0.1297],
        [ 0.3632, -0.2986, -0.0157, -0.2817],
        [ 0.4323,  0.3205,  0.2895, -0.1527],
        [ 0.2368,  0.4018, -0.2126,  0.4732],
        [-0.0158, -0.4908,  0.3854, -0.4685]], requires_grad=True)
bias torch.Size([5]):
Parameter containing:
tensor([-0.0749, -0.1002, -0.3814,  0.2213, -0.4468], requires_grad=True)
out (torch.Size([1, 2, 3, 5])):
tensor([[[[-0.4856, -0.4660,  0.0287,  0.8903, -0.9825],
          [-0.3466, -0.0631,  0.2547,  0.7885, -0.6827],
          [-0.2645, -0.3523, -0.0180,  0.7556, -0.9211]],

         [[-0.1840, -0.3200,  0.1673,  0.8805, -1.0334],
          [-0.3801, -0.1546,  0.4352,  0.6340, -0.6364],
          [-0.0947, -0.1511, -0.0458,  0.9102, -0.9628]]]],
       grad_fn=&lt;ViewBackward0&gt;)
</code></pre>
<p>So here <code>inp</code> contains two 3x4 matrices embedded into a 4-dimensional tensor according to the first two dimensions 1x2 layout. The linear layer multiplies all matrices by the weights, adds the bias vector to all of them, and finally stacks the resulted two 3x5 matrices according to the 1x2 layout. You can perform the <code>linear(inp)</code> operation equivalently as</p>
<pre><code>torch.tensordot(inp, linear.weight.T, dims=1) + linear.bias
</code></pre>
<p>which will print the same result as <code>out</code>.
The <code>dim=1</code> argument of <a href=""https://pytorch.org/docs/stable/generated/torch.tensordot.html"" rel=""nofollow noreferrer""><code>torch.tensordot</code></a> specifies how many dimensions the operation &quot;consumes&quot; which is 1 for the matrix product.</p>
<hr />
<p>To make it even clearer, use can use only matrix products by <a href=""https://pytorch.org/docs/stable/generated/torch.matmul.html"" rel=""nofollow noreferrer""><code>torch.matmul</code></a> and perform the unstacking and stacking manually:</p>
<pre><code>mat0 = torch.matmul(inp[0][0], linear.weight.T) + linear.bias
mat1 = torch.matmul(inp[0][1], linear.weight.T) + linear.bias
print(f'mat0 ({mat0.shape}):\n{mat0}')
print(f'mat1 ({mat1.shape}):\n{mat1}')
# =&gt;
# mat0 (torch.Size([3, 5])):
# tensor([[-0.4856, -0.4660,  0.0287,  0.8903, -0.9825],
#         [-0.3466, -0.0631,  0.2547,  0.7885, -0.6827],
#         [-0.2645, -0.3523, -0.0180,  0.7556, -0.9211]], grad_fn=# &lt;AddBackward0&gt;)
# mat1 (torch.Size([3, 5])):
# tensor([[-0.1840, -0.3200,  0.1673,  0.8805, -1.0334],
#         [-0.3801, -0.1546,  0.4352,  0.6340, -0.6364],
#         [-0.0947, -0.1511, -0.0458,  0.9102, -0.9628]], grad_fn=# &lt;AddBackward0&gt;)
</code></pre>
<p>which can be stacked by <a href=""https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack"" rel=""nofollow noreferrer""><code>torch.stack</code></a> into a 2x3x5 tensor or a 1x2x3x5 tensor (as above):</p>
<pre><code>t1 = torch.stack([mat0, mat1])
print(f't1 ({t1.shape}):\n{t1}')
# =&gt;
# t1 (torch.Size([2, 3, 5])):
# tensor([[[-0.4856, -0.4660,  0.0287,  0.8903, -0.9825],
#          [-0.3466, -0.0631,  0.2547,  0.7885, -0.6827],
#          [-0.2645, -0.3523, -0.0180,  0.7556, -0.9211]],
#
#         [[-0.1840, -0.3200,  0.1673,  0.8805, -1.0334],
#          [-0.3801, -0.1546,  0.4352,  0.6340, -0.6364],
#          [-0.0947, -0.1511, -0.0458,  0.9102, -0.9628]]],
#        grad_fn=&lt;StackBackward0&gt;)

t2 = torch.stack([t1])
print(f't2 ({t2.shape}):\n{t2}')
# =&gt;
# t2 (torch.Size([1, 2, 3, 5])):
# tensor([[[[-0.4856, -0.4660,  0.0287,  0.8903, -0.9825],
#           [-0.3466, -0.0631,  0.2547,  0.7885, -0.6827],
#           [-0.2645, -0.3523, -0.0180,  0.7556, -0.9211]],
#
#          [[-0.1840, -0.3200,  0.1673,  0.8805, -1.0334],
#           [-0.3801, -0.1546,  0.4352,  0.6340, -0.6364],
#           [-0.0947, -0.1511, -0.0458,  0.9102, -0.9628]]]],
#        grad_fn=&lt;StackBackward0&gt;)
</code></pre>
","2024-07-04 11:26:21","1","Answer"
"78705127","78606054","","<p>I tried to install torchvision from source [https://github.com/pytorch/vision.git][1], which is feasible, but it also requires the use of develop mode, similar to running &quot;python setup.py develop&quot;.</p>
","2024-07-04 05:14:53","0","Answer"
"78704861","","How is nn.Linear applied to a higher dimensional data?","<p>I'm trying to understand what values are being multiplied when I have higher dimensional tensors:</p>
<pre><code>inp = torch.rand(1,2,3) # B,C,W
linear = nn.Linear(3,4)
out = linear(inp)
print(out.shape)
&gt;&gt;&gt; torch.Size([1, 2, 4])

inp = torch.rand(1,2,3,4) # B,C,W,H
linear = nn.Linear(4,5)
out = linear(inp)
print(out.shape)
&gt;&gt;&gt; torch.Size([1, 2, 3, 5])
</code></pre>
<p>It seems like only the last dimension is being changed, but when I try to manually multiply the linear weights (<code>linear.weight.data</code>) with each <code>inp</code>'s last dimension, I can't get to the correct answer (seems like all the values are changing and only the last dimension's size is being modify somehow).</p>
","2024-07-04 02:57:29","1","Question"
"78704234","","Does using torch.where to threshold a tensor detach it tensor from the computational graph?","<p>I'm writing a custom loss function in PyTorch for multiclass semantic segmentation. One part of this function is thresholding select channels from the tensor, which are indicated with tracker_index.</p>
<p>The last part of the function that is a part of the computational graph is the channel_tensor, and if I comment out the line where torch.where is applied, everything runs smoothly. I've tried setting 1 and 0 to float32 tensors and ensured that they are on the same device as the channel_tensor, which leads me to believe that eighter thresholding is not differentiable, so cannot be a part of the loss function, or torch.where will always detach the tensor from the computational graph. Please advise.</p>
<pre><code>channel_tensor =torch.select(
     segmentation_output,
     dim=-3,
     index=tracker_index
)
channels[tracker_index]= torch.where(channel_tensor &gt; self.threshold, torch.tensor(1, device=channel_tensor.device, dtype=torch.float32), torch.tensor(0, device=channel_tensor.device, dtype=torch.float32))
</code></pre>
","2024-07-03 21:13:24","1","Question"
"78703324","78698316","","<p>There is no ideal option, each choice has its pros and cons. Therefore, I recommend trying all the valid options and compare the performance. You can start with these options:</p>
<ol>
<li>You can resize images to a slightly larger size e.g., 520x520.</li>
<li>You can randomly crop images to the desired fixed size e.g., 512x512.</li>
<li>You can use both options by starting with resizing and then cropping.</li>
</ol>
<p>There is an advanced option that you can try which is computing the widths and heights of all images and then calculate the average. Then, use the average width and height to resize all images to.</p>
","2024-07-03 16:45:40","0","Answer"
"78703313","","segment_anything causing error with numpy.uint8","<p>I am trying to run <a href=""https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb</a> locally, on an M2 MacBook with Sonoma 14.5. However, I keep running into the following error at step 11:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[75], line 1
----&gt; 1 masks = mask_generator.generate(image)

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)
    112 @functools.wraps(func)
    113 def decorate_context(*args, **kwargs):
    114     with ctx_factory():
--&gt; 115         return func(*args, **kwargs)

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:163, in SamAutomaticMaskGenerator.generate(self, image)
    138 &quot;&quot;&quot;
    139 Generates masks for the given image.
    140 
   (...)
    159          the mask, given in XYWH format.
    160 &quot;&quot;&quot;
    162 # Generate masks
--&gt; 163 mask_data = self._generate_masks(image)
    165 # Filter small disconnected regions and holes in masks
    166 if self.min_mask_region_area &gt; 0:

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:206, in SamAutomaticMaskGenerator._generate_masks(self, image)
    204 data = MaskData()
    205 for crop_box, layer_idx in zip(crop_boxes, layer_idxs):
--&gt; 206     crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
    207     data.cat(crop_data)
    209 # Remove duplicate masks between crops

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:236, in SamAutomaticMaskGenerator._process_crop(self, image, crop_box, crop_layer_idx, orig_size)
    234 cropped_im = image[y0:y1, x0:x1, :]
    235 cropped_im_size = cropped_im.shape[:2]
--&gt; 236 self.predictor.set_image(cropped_im)
    238 # Get points for this crop
    239 points_scale = np.array(cropped_im_size)[None, ::-1]

File ~/opt/anaconda3/envs/ve_env/lib/python3.9/site-packages/segment_anything/predictor.py:57, in SamPredictor.set_image(self, image, image_format)
     55 # Transform the image to the form expected by the model
     56 input_image = self.transform.apply_image(image)
---&gt; 57 input_image_torch = torch.as_tensor(input_image, device=self.device)
     58 input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]
     60 self.set_torch_image(input_image_torch, image.shape[:2])

RuntimeError: Could not infer dtype of numpy.uint8
</code></pre>
<p>I am using a conda environment with Python 3.9.19, and also tested with Python 3.11. Based on online comments I suspected this to be an issue with numpy versions, but having tried multiple versions I cannot find the correct combination. I am currently trying with the following:</p>
<pre><code>numpy==1.24.4
torch==1.9.0
torchvision==0.10.0
opencv-python==4.10.0.84
</code></pre>
<p>Running the same notebook on Google Colab works fine, and the versions indicated there are:</p>
<pre><code>import numpy as np
import torch
import cv2

print(np.__version__)
print(torch.__version__)
print(cv2.__version__)

1.25.2
2.3.0+cu121
4.8.0
</code></pre>
<p>This is using Python 3.10.12. These versions are not available on Mac, so I am stuck.</p>
<p>How can I find out why numpy.uint8 is not being recognized, and how can I fix this error? Most online comments point to upgrading numpy, but I have tried several numpy versions without luck. Any help is appreciated.</p>
","2024-07-03 16:43:17","0","Question"
"78703272","78674317","","<p>First, you should know that every tensor in PyTorch has an underlying storage that holds its actual data. You can use <code>.storage()</code> to retrieve the underlying storage of a tensor. Then, you should use <code>.set_()</code> to replace the tensor's underlying storage with a new storage.</p>
<pre><code>with torch.no_grad():

    x_storage = x.untyped_storage()
    y_storage = y.untyped_storage()

    x.set_(y_storage, y.storage_offset(), y.size(), y.stride())
    y.set_(x_storage, x.storage_offset(), x.size(), x.stride())
</code></pre>
<p><strong>Note:</strong> the swapping process does not affect refences to the tensors themselves. Also, swapping the storage does not interfere with the reference counting/GC since PyTorch handles reference counting and garbage collection automatically. Another reason is that you are not creating new tensors or modifying the reference counting directly.</p>
<h2>Update</h2>
<p>After mentioning in the comments that the swapping will be within an optimizer class, this can potentially affect the autograd graph. Swapping storage objects between tensors, it turns out that the data the autograd graph references is changed. This can lead to inconsistencies between the calculated gradients and the actual computations performed on the swapped tensors. In brief, swapping data directly within an optimizer can be problematic for autograd graph. Therefore, I do not recommend swapping tensors directly within an optimizer class.</p>
<p><em>The only solution is to use temporary tensors for safe swapping.</em></p>
","2024-07-03 16:31:07","3","Answer"
"78703108","78702879","","<p>Okay I figured it out!</p>
<p>After downloading the data from <a href=""https://www.kaggle.com/c/imagenet-object-localization-challenge/data"" rel=""nofollow noreferrer"">Kaggle</a>, follow the script from <a href=""https://github.com/fh295/semanticCNN"" rel=""nofollow noreferrer"">here</a>:</p>
<pre><code># extract train data
mkdir train &amp;&amp; mv ILSVRC2012_img_train.tar train/ &amp;&amp; cd train
tar -xvf ILSVRC2012_img_train.tar &amp;&amp; rm -f ILSVRC2012_img_train.tar
find . -name &quot;*.tar&quot; | while read NAME ; do mkdir -p &quot;${NAME%.tar}&quot;; tar -xvf &quot;${NAME}&quot; -C &quot;${NAME%.tar}&quot;; rm -f &quot;${NAME}&quot;; done
# extract validation data
cd ../ &amp;&amp; mkdir val &amp;&amp; mv ILSVRC2012_img_val.tar val/ &amp;&amp; cd val &amp;&amp; tar -xvf ILSVRC2012_img_val.tar
wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash
</code></pre>
<p>This converts the <code>val</code> folder to the same format as the <code>train</code> folder (note that this is not changing anything in the <code>test</code> folder which has the same format as the <code>val</code> folder), where the images are moved towards their respective folder classes.</p>
<p>Next, to load the train and val images I did the following:</p>
<pre><code>transform_train = transforms.Compose([
    transforms.Resize(size),
    transforms.RandomCrop(32),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101
])

transform_test = transforms.Compose([
    transforms.Resize(size),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101
])

trainset = torchvision.datasets.ImageFolder('train_path/train' , transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=8)
testset = torchvision.datasets.ImageFolder('val_path/val' , transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)
</code></pre>
","2024-07-03 15:54:21","0","Answer"
"78702879","","How to load ImageNet from Kaggle?","<p>I downloaded the ImageNet dataset from <a href=""https://www.kaggle.com/c/imagenet-object-localization-challenge/data"" rel=""nofollow noreferrer"">Kaggle</a>, as it seems like the <a href=""https://discuss.pytorch.org/t/issues-with-dataloader-for-imagenet-should-i-use-datasets-imagefolder-or-datasets-imagenet/115742"" rel=""nofollow noreferrer"">original website doesn't have it anymore</a>. When I try to load it with Pytorch following <a href=""https://pytorch.org/vision/main/generated/torchvision.datasets.ImageNet.html"" rel=""nofollow noreferrer"">this</a>:</p>
<pre><code>torchvision.datasets.ImageNet(root: Union[str, Path], split: str = 'train', **kwargs: Any)
</code></pre>
<p>I get <code>RuntimeError: The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in</code></p>
<p>As Kaggle doesn't have the same <code>gz</code> file. Instead, the data looks like:</p>
<pre><code>/media/SSD2/ILSVRC/
                |----Annotation
                |----ImageSets
                |----Data
                      |----CLS-LOC
                               |----test
                                      |----ILSVRC2012_val_00000009.JPEG
                                      |----ILSVRC2012_val_00000010.JPEG
                                      |----...
                               |----train
                                      |----n01440764 # this is a class
                                             |----ILSVRC2012_val_00000010.JPEG
                                             |----ILSVRC2012_val_00000010.JPEG
                                      |----n01443537
                                      |----...
                               |----val
                                      |----ILSVRC2012_val_00000009.JPEG
                                      |----ILSVRC2012_val_00000010.JPEG
                                      |----...
</code></pre>
<p>I found that people on <a href=""https://www.kaggle.com/c/imagenet-object-localization-challenge/code"" rel=""nofollow noreferrer"">Kaggle load it differently</a>, and I could not figure out the script they referred to <a href=""https://discuss.pytorch.org/t/issues-with-dataloader-for-imagenet-should-i-use-datasets-imagefolder-or-datasets-imagenet/115742/1"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I was loading previous datasets like this so I can use the transformation:</p>
<pre><code>trainset = torchvision.datasets.ImageNet(root='path_to_dataset', split='train', download=False, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=8)
</code></pre>
<p>Where my transformation is something like</p>
<pre><code>transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.Resize(size),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), 
])
</code></pre>
<p>I couldn't find an original Pytorch script to load the dataset, which is very bizarre as this is one of the most popular datasets.</p>
<p><strong>Update:</strong>
Found <a href=""https://github.com/fh295/semanticCNN"" rel=""nofollow noreferrer"">here</a> that I can convert the val folder to the same format as the train folder (i.e., move the images to their respective folders). By running</p>
<pre><code>wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash
</code></pre>
","2024-07-03 15:05:54","0","Question"
"78701384","78700635","","<p>Ok there's something odd in this code: you're intent seems to be allocating 128 times (for each worker basically) the same dataloader, dataset and model. In this case it would be better to just define them once in order to save RAM and computational time.</p>
<p>The following code is how I'd work for this task:</p>
<pre><code>model = load_model()
dataset = build_dataset()
dataloader = DataLoader(dataset, batch_size=1, num_workers=4)

def process_one_batch(batch):
    inputs, targets = batch
    outputs = model(inputs)
    return outputs.detach()
</code></pre>
<p><code>batch_size=1</code> ensures that you are going to load a sample (what you call line) at each iteration, while <code>num_workers=4</code> defines number of cores you are going to use to process samples for dataloader. The <code>num_workers=4</code> setting in my case generate the following warning, but it does not go in conflict with multiprocessing:</p>
<pre><code>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.

</code></pre>
<p>The <code>process_one_batch</code> function just infer the model's output for the given input, but it is just an example of what you want to do with your samples.</p>
<p>The Pool implementation is the following:</p>
<pre><code>with Pool(processes=128) as p:
        for out in p.imap_unordered(process_one_batch, dataloader):
            print(out)
</code></pre>
<p>I've run this code on a google colab notebook and the following dummy function have been used in order to simulate the functions you did not provides:</p>
<pre><code># Define dummy functions for loading model and dataset
def load_model():
    return torch.nn.Linear(10, 1)

def build_dataset():
    class DummyDataset(Dataset):
        def __len__(self):
            return 1000

        def __getitem__(self, idx):
            return torch.randn((10,))*idx, torch.tensor(idx)
    return DummyDataset()
</code></pre>
","2024-07-03 10:07:48","0","Answer"
"78700738","77989807","","<h2>The error is due to files mismatch while installing ultralytics.</h2>
<ul>
<li>Recently I installed Ultralytics with Cuda and it worked, Here are the procedure I followed.</li>
<li><strong>Note:</strong> I too faced the same issue. Follow the steps to avoid those. This all done in anaconda.</li>
<li>By following the steps you can use your <strong>GPU</strong> as well.</li>
</ul>
<h3><strong>Step 1:</strong> Create the environment</h3>
<pre><code>conda create -n name python=3.10
</code></pre>
<h3><strong>Step 2:</strong> Install torch with cuda from <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">PyTorch official</a>.</h3>
<pre><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 
or
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
</code></pre>
<ul>
<li>After installing please verify, is that installed properly</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import torch
print(torch.cuda.is_available())

# Output
&gt;&gt;&gt; True
</code></pre>
<ul>
<li>Also verify the Libraries check list.</li>
</ul>
<pre><code>pip list

filelock          3.13.1
fsspec            2024.2.0
intel-openmp      2021.4.0
Jinja2            3.1.3
MarkupSafe        2.1.5
mkl               2021.4.0
mpmath            1.3.0
networkx          3.2.1
numpy             1.26.3
pillow            10.2.0
pip               24.0
setuptools        69.5.1
sympy             1.12
tbb               2021.11.0
torch             2.3.1+cu121
torchaudio        2.3.1+cu121
torchvision       0.18.1+cu121
typing_extensions 4.9.0
wheel             0.43.0
</code></pre>
<h3><strong>Step 3:</strong> Install the Ultralytics and check the import</h3>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt;pip install ultralytics==8.2.48

import ultralytics
print(ultralytics.checks())

&gt;&gt;&gt;Ultralytics YOLOv8.2.48 🚀 Python-3.10.14 torch-2.3.1+cu121 CUDA:0 (NVIDIA 
...GeForce RTX 4070, 12282MiB)
...Setup complete ✅ (28 CPUs, 63.8 GB RAM, 823.7/1905.5 GB disk)
</code></pre>
","2024-07-03 07:55:17","0","Answer"
"78700635","","How to run torch dataloader in a sub-process of multiprocessing.Pool?","<p>I want to inference model in multiprocessing, instead of use torch.distributed, how can I use multiprocessing.Pool?</p>
<p>I have to use num_workers=0 in subprocess to avoid error like &quot;daemonic processes are not allowed to have children&quot;, but num_workers=0 makes the data reading very very slow. How can I make code below runnable, expecially the num_workers=4?</p>
<pre><code>import multiprocessing
import torch

def process_one_line(line):
    model = load_model()
    dataset = build_dataset()
    dataloader = torch.utils.data.DataLoader(dataset, num_workers=4)
    

if __name__ == '__main__':
    lines = get_lines() # 10k lines or more
    with Pool(processes=128) as p:
        for _ in p.imap_unordered(process_one_line, lines):
            pass
</code></pre>
","2024-07-03 07:28:12","0","Question"
"78699287","","PyTorch move nn.module to cuda including submodules and wrapper modules","<p>I would like to move my module to Cuda.</p>
<p>There is a code example:</p>
<pre><code>class SubModel(nn.Module):
    def __init__(self):
        super(SubModel, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2)

    def forward(self, x):
        print(f&quot;x type:{type(x)}&quot;)
        print(f&quot;weight type:{type(self.conv1.weight)}&quot;)
        return self.conv1(x)

class WrapperModel(nn.Module):
    def __init__(self, count):
        super(WrapperModel, self).__init__()
        self.blocks = []
        for i in range(count):
            self.blocks.append(SubModel())

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x
     
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2)
        self.wrapper = WrapperModel(2)
        

    def forward(self, x):
        x = self.conv(x)
        x = self.wrapper(x)
        
        return x

</code></pre>
<p>However, I encountered a runtime error when executing self.conv1(x) in the forward function in SubModel:</p>
<p><strong>RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same</strong></p>
<p>The output of the print commands are:</p>
<pre><code>x type:&lt;class 'torch.Tensor'&gt;
weight type:&lt;class 'torch.nn.parameter.Parameter'&gt;
</code></pre>
<p>Then I tried to move the wrapper to the Cuda at the end:</p>
<pre><code>model.wrapper.to(device)
</code></pre>
<p>It didn't fix it.</p>
<p>Finally, I move the conv1D in the subModel to the cuda:</p>
<pre><code>class SubModel(nn.Module):
    def __init__(self):
        super(SubModel, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

</code></pre>
<p>It works fine! However, the print output did not changed:</p>
<pre><code>x type:&lt;class 'torch.Tensor'&gt;
weight type:&lt;class 'torch.nn.parameter.Parameter'&gt;
</code></pre>
<p>Therefore, I have no idea what's wrong going here.</p>
<p>I also feel that it's too inconvenient that I have to manually move all instances to the Cuda.</p>
<p>**My question: is there a convenient method to move a model to the Cuda including all the subModels within it? **</p>
","2024-07-02 21:22:59","0","Question"
"78698316","","Best practice to train DeepLabV3 with different resolution images in PyTorch","<p>I am trying to train <a href=""https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/"" rel=""nofollow noreferrer"">PyTorch's DeepLabV3</a> on <a href=""https://cocodataset.org/#home"" rel=""nofollow noreferrer"">COCO 2017 dataset</a> for semantic segmetation but I am unsure on how to deal with the different resolution images. I know that DeepLab's architecture can process them without any problems, but I can't stack them in batches due to their resolution. What is the best practice to handle this issue? Do I resize them to a fixed size? Do I take a random crop of fixed size? I know that there are a lot of solutions for this issue, but I don't really know what's the best practice in the context of semantic segmentation training.</p>
<p>Thanks!</p>
","2024-07-02 16:39:00","1","Question"
"78697900","","dgl torch incompatability error: Cannot load Graphbolt C++ library","<p>I'm attempting to install and setup <code>dgl</code> in google colab via the <a href=""https://www.dgl.ai/pages/start.html"" rel=""nofollow noreferrer"">DGL install instructions</a>:</p>
<pre><code>!pip install torch
!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html
</code></pre>
<p>Which installs <code>pytorch2.3.0+cu121</code> and <code>dgl2.3.0+cu121</code>.</p>
<p>However, when I try to import dgl, I run into the following error:</p>
<pre><code>ImportError: Cannot load Graphbolt C++ library
</code></pre>
<p>I've tried some other versions of dgl as well, but always run into some sort of import error.</p>
<p><code>!nvcc --version</code> shows cuda version 12.2</p>
<p>Is the issue a mismatch of the cuda version (12.1 v 12.2)?</p>
<p>Both <a href=""https://discuss.dgl.ai/t/importerror-cannot-load-graphbolt-c-library/4291/25"" rel=""nofollow noreferrer"">this thread</a> and <a href=""https://github.com/dmlc/dgl/issues/7433"" rel=""nofollow noreferrer"">this one</a> seem to have run into similar issues.</p>
<p>If anyone can get this working in colab, please share the workbook link.</p>
","2024-07-02 15:07:00","0","Question"
"78696871","78687048","","<p>Method 1 : Using Torch  and Numpy</p>
<pre><code>import pandas as pd
import torch
import numpy as np
from collections import defaultdict

# Sample DataFrame
df = pd.DataFrame({
    'id': [1, 2, 3],
    'datas': [[1, 2, 3], [2, 3, 4], [1, 3, 5]]
})

# Convert data to PyTorch tensors
datas = [torch.tensor(d) for d in df['datas']]

# Initialize triplet count dictionary
triplet_count_dict = defaultdict(int)

for d in datas:
    if len(d) &lt; 3:
        continue
    
    n = len(d)
    #torch.combinations are not efficient for Huge datasets.
    #idx_combinations = torch.combinations(input = idx, r =3)
    #Generate Efficiently all combinations of indices (i, j, k) where i &lt; j &lt; k
    idx_combinations = np.array([[i, j, k] for i in range(n-2) 
                                for j in range(i+1, n-1) for k in range(j+1, n)])
    
    # Convert numpy combinations to PyTorch tensor
    idx_combinations = torch.tensor(idx_combinations)
    
    # Extract the corresponding elements for these indices
    triplets = d[idx_combinations]
    
    # Normalize the triplets so that bi &lt; bj 
    bi = triplets[:, 0]
    bj = triplets[:, 1]
    bk = triplets[:, 2]
    
    # Swap bi and bj to ensure bi &lt;= bj
    mask = bi &gt; bj
    bi[mask], bj[mask] = bj[mask], bi[mask]
    
    # Count the occurrences of each triplet
    for t in torch.stack((bi, bj, bk), dim=1):
        triplet_count_dict[tuple(t.tolist())] += 1

# Print the triplet count dictionary
print(triplet_count_dict)

# Print triplets and their counts
for k, v in triplet_count_dict.items():
    print(f&quot;Triplet {k}: Count = {v}&quot;)

'''
defaultdict(&lt;class 'int'&gt;, {(1, 2, 3): 1, (2, 3, 4): 1, (1, 3, 5): 1})
Triplet (1, 2, 3): Count = 1
Triplet (2, 3, 4): Count = 1
Triplet (1, 3, 5): Count = 1
'''
</code></pre>
<p>Method 2 :  Using pure Numpy</p>
<pre><code>import numpy as np
from collections import defaultdict
import pandas as pd

# Create a pandas DataFrame with sample data
df = pd.DataFrame({
    'id': [1, 2, 3],
    'datas': [[1, 2, 3], [2, 3, 4], [1, 3, 5]]
})

# Convert data sequences from lists to NumPy arrays for efficient processing
datas = [np.array(d) for d in df['datas']]

# Initialize a dictionary to store triplet counts with default value 0
triplet_count_dict = defaultdict(int)

# Loop through each data sequence in the list 'datas'
for d in datas:

    # Get the length (number of elements) of the current data sequence
    n = len(d)

    # Skip sequences with less than 3 elements (not enough for triplets)
    if len(d) &lt; 3:
        continue

    # Generate all combinations of indices (i, j, k) where i &lt; j &lt; k
    # This is explained in detail below
    idx_combinations = np.array([[i, j, k] for i in range(n-2)
                                 for j in range(i+1, n-1) for k in range(j+1, n)])

    # Extract the corresponding elements from the data sequence based on the combinations
    triplets = d[idx_combinations]

    # Separate the elements of each triplet (i.e., columns of the array)
    bi = triplets[:, 0]  # First element
    bj = triplets[:, 1]  # Second element
    bk = triplets[:, 2]  # Third element

    # Normalize the triplets so that the first element (bi) is always less than the second (bj)
    mask = bi &gt; bj
    bi[mask], bj[mask] = bj[mask], bi[mask]  # Swap elements if necessary

    # Loop through each normalized triplet
    for t in np.stack((bi, bj, bk), axis=1):
        # Convert the triplet to a tuple (hashable for dictionary key) and increment its count
        triplet_count_dict[tuple(t)] += 1

# Print the dictionary containing the counts of each unique triplet
print(triplet_count_dict)
'''
defaultdict(&lt;class 'int'&gt;, {(1, 2, 3): 1, (2, 3, 4): 1, (1, 3, 5): 1})
'''
# Print the triplets and their corresponding counts in a more readable format
for k, v in triplet_count_dict.items():
    print(f&quot;Triplet {k}: Count = {v}&quot;)
    
    
'''
Triplet (1, 2, 3): Count = 1
Triplet (2, 3, 4): Count = 1
Triplet (1, 3, 5): Count = 1
'''
</code></pre>
","2024-07-02 11:57:11","0","Answer"
"78696326","78696130","","<p>it comes from the fact that <code>inputs[:,:,0]</code> returns a copy of the data, making <code>inputs[:,:,0]</code> unseen by the graph as opposed to <code>inputs</code>. It is impossible to compute gradients on unseen data (how could you know &quot;the influence&quot; of the two elements of inputs on the scalar value in cs_hat[:, 0] without knowing the intermediate operations?).</p>
<p>What you want to achieve is totally possible, and without much overhead (I don't know how torch computes gradients but I assume it is darn efficient). You can do:</p>
<pre><code>cs_hat = outputs[2]
cs_dt = torch.autograd.grad(cs_hat, inputs, grad_outputs=torch.ones_like(cs_hat), create_graph=True)[0]
cs_dt[:, :, 0]  # gradient with respect to inputs[:, :, 0]
</code></pre>
","2024-07-02 10:01:53","0","Answer"
"78696130","","PyTorch Gradient Computation Fails When Not Using Entire Input Tensor","<p>I have a model that takes an input tensor , along with other inputs k and D. The model outputs several tensors, including cs_hat. When I compute gradients of cs_hat with respect to the first slice of inputs (inputs[:,:,0]), the gradient computation only succeeds if I compute it with respect to the entire tensor inputs instead of just the slice.</p>
<p>Here is a simplified version of my code that illustrates the problem:</p>
<pre><code>import torch
from torch import nn

class MyModel(torch.nn.Module):

    def __init__(self, input_size = 3 , ffn_size = 15, ffn_layers = 2, res_block_size = 15, res_block_layers = 2):
        super(MyModel, self).__init__()

        self.input_size = input_size
        self.activation = nn.LeakyReLU()

        self.ffn_size = ffn_size
        self.ffn_layers = ffn_layers
        self.res_block_size = res_block_size
        self.res_block_layers = res_block_layers

        self.linear_block_0 = self._make_linear_block(self.ffn_size, self.ffn_layers, input_size=self.input_size)

        self.final_layer_a = nn.Linear(self.res_block_size, 1, bias=False)
        self.final_layer_b = nn.Linear(self.res_block_size, 1, bias=False)
        self.final_layer_c = nn.Linear(self.res_block_size, 1, bias=False)
        self.final_layer_d = nn.Linear(self.res_block_size, 1, bias=False)




    def _make_linear_block(self, width, depth, input_size = None):

        if input_size is None:
            linear_block = nn.ModuleList([nn.Linear(width , width), self.activation])
        else:
            linear_block = nn.ModuleList([nn.Linear(input_size , width), self.activation])

        for _ in range(depth - 1):
            linear_block.append(nn.Linear(width, width))
            linear_block.append(self.activation)

        linear_block_ = nn.Sequential(*linear_block)

        return linear_block_


    def forward(self, inputs,k,D):

        t = inputs[:,:,0]
        x = inputs[:,:,1]

        input_t = torch.cat([t,k.view(-1,1),D.view(-1,1)],dim = -1)

        z0 = self.linear_block_0(input_t)

        a = self.final_layer_a(z0)
        b = self.final_layer_b(z0)
        c = self.final_layer_c(z0)
        d = self.final_layer_d(z0)

        return a,b,c,d


#Main

model = MyModel()

inputs = torch.tensor([[[0.4521, 0.5205]], [[0.3066, 0.6816]], [[0.0547, 0.9297]], [[0.3936, 0.9229]]], requires_grad=True) #supposed to be of size (batch_size,1 ,1)

batch_size = 4

k = torch.randn(batch_size, requires_grad=True)
D = torch.randn(batch_size, requires_grad=True)

# Forward pass
outputs = model(inputs, k, D)
cs_hat = outputs[2]  # Assuming cs_hat is the third output

# Gradient computation that works
cs_dt = torch.autograd.grad(cs_hat, inputs, grad_outputs=torch.ones_like(cs_hat), create_graph=True)[0]
# Gradient computation that fails
cs_dt = torch.autograd.grad(cs_hat, inputs[:,:,0], grad_outputs=torch.ones_like(cs_hat), create_graph=True)[0]
</code></pre>
<p>My error message:
<code>RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.</code></p>
<p>Ensuring requires_grad=True is set before any operations.</p>
<p>Using torch.ones_like(cs_a_hat) to match the shape of grad_outputs.</p>
<p>Checking that t actually influences cs_hat (it does).</p>
<p>Setting allow_unused=True just gives me a None result (even though t influences cs_hat).</p>
<p>Tried the same with f(inputs)=2*inputs instead of passing to neural network. Same error</p>
<p>Questions:
Why does excluding parts of the input tensor from the gradient calculation cause this issue?
How can I correctly compute gradients with respect to only the needed parts of the input tensor without encountering this error?
If I accept that I need to use the whole input, will this add a lot computational complexity?</p>
","2024-07-02 09:21:58","0","Question"
"78694947","78691616","","<p>As explained in the <a href=""https://huggingface.co/docs/timm/v1.0.7/en/feature_extraction#feature-extraction"" rel=""nofollow noreferrer"">timm documentation</a>, you can get the final hidden state of the model with the <code>forward_features</code> method.</p>
<pre class=""lang-py prettyprint-override""><code>model = timm.create_model(...)
x = ...
features = model.forward_features(x)
</code></pre>
","2024-07-02 03:37:49","1","Answer"
"78694932","78693937","","<p>When you run</p>
<pre class=""lang-py prettyprint-override""><code>model.sc1.weight = nn.Parameter(1. * model.sc1.weight)
model.sc2.weight = nn.Parameter(1. * model.sc2.weight)
</code></pre>
<p>You are not &quot;multiplying by a scalar&quot;. You are creating an entirely new object (<code>nn.Parameter(1. * model.sc1.weight)</code>) and assigning it to the <code>.weight</code> attribute.</p>
<p>I assume you are updating your model with a standard pytorch optimizer, something like:</p>
<pre class=""lang-py prettyprint-override""><code>model = TSP(...)
opt = torch.optim.SGD(model.parameters(), lr=1e-3)
</code></pre>
<p>When you run <code>model.sc1.weight = nn.Parameter(1. * model.sc1.weight)</code>, you create an entirely new object in <code>model.sc1.weight</code>, but the optimizer still references the old object.</p>
<p>You can validate this as follows:</p>
<pre class=""lang-py prettyprint-override""><code># data pointer of weight
model.sc1.weight.data_ptr()
&gt; 124805056

# data pointer of weight in the optimizer 
opt.param_groups[0]['params'][0].data_ptr()
&gt; 124805056

# now create new weight object
model.sc1.weight = nn.Parameter(1. * model.sc1.weight)

# data pointer of model weight has changed
model.sc1.weight.data_ptr()
&gt; 139582720

# data pointer of optimizer has not
opt.param_groups[0]['params'][0].data_ptr()
&gt; 124805056
</code></pre>
<p>To avoid this, update the object instead of creating a new object</p>
<pre class=""lang-py prettyprint-override""><code># data pointer of weight
model.sc1.weight.data_ptr()
&gt; 124805056

# data pointer of weight in the optimizer 
opt.param_groups[0]['params'][0].data_ptr()
&gt; 124805056

# update data of weight tensor with in-place operation
model.sc1.weight.data.mul_(2.)

# weight and optimizer still have same data pointer
model.sc1.weight.data_ptr()
&gt; 124805056

opt.param_groups[0]['params'][0].data_ptr()
&gt; 124805056
</code></pre>
","2024-07-02 03:28:27","1","Answer"
"78693937","","Learning stops when you multiply the weights of a layer with a scalar?","<p>I am trying to implement sparsely connected weight matrices for my simple 3-layer feedforward model. To do this I implemented a mask for each of my layers with a certain % of zeros, with the idea being that I would like to zero out the same set of weights after every optimizer step so that my layers are not fully connected. But I am having trouble with this because when I do an element-wise multiplication of the mask with the weight matrices, the weights stop changing in subsequent backward passes. To see if my mask is causing the issue, I did just multiplied my weight matrices with the scalar 1.0 and this recreates the issue. What might be happening here? I checked and gradients still get calculated. It’s just that the loss doesn’t go down anymore and the weights don’t change. Does doing this multiplication somehow disconnect the weights from the graph?</p>
<p>My model:</p>
<pre><code>class TSP(nn.Module):

  def __init__(self, input_size, hidden_size):
    super(TSP, self).__init__()
    self.sc1 = nn.Linear(input_size, hidden_size)
    self.sc2 = nn.Linear(hidden_size, input_size)

    torch.nn.init.normal_(self.sc1.weight, mean=0, std=0.1)
    torch.nn.init.normal_(self.sc2.weight, mean=0, std=0.1)

  def forward(self, x):
    x = torch.relu(self.sc1(x)) 
    x = (self.sc2(x))
    return x


  def predict_hidden(self, x):
    x = torch.relu(self.sc1(x))
    return x

</code></pre>
<p>To recreate this issue all that is needed is the following and the weights stop getting updated:</p>
<pre><code>model.sc1.weight = nn.Parameter(1. * model.sc1.weight)
model.sc2.weight = nn.Parameter(1. * model.sc2.weight)
</code></pre>
","2024-07-01 19:19:07","0","Question"
"78691616","","Cannot extract the penultimate layer output of a vision transformer with a Pytorch","<p>I have the following model that I tuned with my own dataset trained with DataParallel:</p>
<pre><code>model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Sequential(nn.Linear(768, 512),nn.ReLU(),nn.BatchNorm1d(512),nn.Dropout(p=0.2),nn.Linear(512, 141))
checkpoint = torch.load('vit_b_16v3.pth')
checkpoint = {k.partition('module.')[2]: v for k, v in checkpoint.items()}
# Load parameters
model.load_state_dict(checkpoint)
</code></pre>
<p>However, I have no idea on how to get the penultimate layer output of such a vision transformer. I tried <a href=""https://www.kaggle.com/code/mohammaddehghan/pytorch-extracting-intermediate-layer-outputs"" rel=""nofollow noreferrer"">This tutorial</a> but it is not working. I only want to input an image and have a 512-D vector describing it. With Tensorflow it is a piece of cake to do it but in Pytorch I am struggling.</p>
<p>My last layers are as follows:</p>
<pre><code>(norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (fc_norm): Identity()
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Sequential(
    (0): Linear(in_features=768, out_features=512, bias=True)
    (1): ReLU()
    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=512, out_features=141, bias=True)
  )
)
</code></pre>
","2024-07-01 10:16:39","0","Question"
"78691335","","How to calculate the correct persistence diagram from a binary-mask-png using GUDHI","<p>I'm trying to get the persistence diagram from a mask(for segmentation), and my codes are as follows:</p>
<pre><code>import numpy as np
import gudhi as gd
import matplotlib.pyplot as plt
from PIL.Image import open as open

image = 1 - np.asarray(open(r'./samples/Image_01L_1stHO.png').convert('L'))
plt.imshow(image, cmap='gray')
plt.title(&quot;Original Image&quot;)
plt.show()

image_vector = image.flatten()

cubical_complex = gd.CubicalComplex(
    dimensions=[image.shape[0], image.shape[1]],
    top_dimensional_cells=image_vector
)

cubical_complex.compute_persistence(homology_coeff_field=2, min_persistence=0)
print(cubical_complex)

cofaces = cubical_complex.cofaces_of_persistence_pairs()

print(&quot;Cofaces of persistence pairs:&quot;, cofaces)

persistence_diagram = cubical_complex.persistence()
print(&quot;Persistence Diagram:&quot;, persistence_diagram)

gd.plot_persistence_diagram(persistence_diagram)
plt.show()
</code></pre>
<p>And I got:
<img src=""https://i.sstatic.net/vRtnMDo7.png"" alt=""enter image description here"" />
and
<img src=""https://i.sstatic.net/e88LQWGv.png"" alt=""enter image description here"" /></p>
<p>It seems like there were something wrong. Even when I tried to change my png, or even I tried generated some binary masks randomly, the Persistence Diagram always showed like this, with just two red points. But it seems like the function have generated the correct <code>cofaces_of_persistence_pairs</code>! Then why?! Please help! Thanks!</p>
<p>Please tell me if I am wrong at any point!</p>
","2024-07-01 09:12:03","1","Question"
"78690182","78689702","","<p>I doubt this is model specific. Pre trained embedding models are known to produce slightly different embeddings for the same input (in your case, texts).</p>
<p>There are a few open conversations about this, however I have experimented with this from a blank as well as pre-trained RoBerta model and found that setting model seeds for embedding tasks controls this behavior. However, adjusting the  seed for model parameters (like bias) may not be enough, indeed, some embedding models can sample from a distribution of tokens at the end of it's inference process.</p>
<p>The warning message:</p>
<pre><code>Some weights of RobertaModel were not initialized from the model checkpoint at qiyuw/pcl-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>implies that the weights of the pretrained model were not downloaded and used in initialization. Depending on the initialization process of that particular Bert Model, it's very likely you're generating a new embeddings upon initialization.</p>
<p>Furthermore, do not initialize the model every time you run the script, I suggest you use a Jupyter Notebook to create a cell block, initialize the model once, and observe if you get the same embeddings for the same input value before moving onto something more elaborate like trying to control the seeds whatever layers are using them.</p>
<p>As an example, observe the following code for the Embedding layer from pytorch docs:</p>
<pre><code>from torch import nn
embedding = nn.Embedding(10, 3)
input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])
embedding(input)
</code></pre>
<p>If the above code is ran (with no regard to whether you already initialized <code>embedding</code>), you will get new embeddings that have been generated from a normal distribution N(0,1). If you do not reinitialize <code>embeddings</code> you will see that the generated value of <code>input</code> has not changed no matter how many times you run it.</p>
<p>Indeed, the Bert model you're using above employs a similar (slightly more complex) use of the embedding layer.</p>
","2024-07-01 01:22:57","0","Answer"
"78689702","","Different embeddings for same sentences with torch transformer","<p>Hey all and apologies in advance for what is probably a fairly basic question - I have a theory about what's causing the issue here, but would be great to confirm with people who know more about this than I do.</p>
<p>I've been trying to implement this python code snippet in Google colab. The snippet is meant to work out similarity for sentences. The code runs fine, but what I'm finding is that the embeddings and distances change every time I run it, which isn't ideal for my intended use case.</p>
<pre><code>import torch
from scipy.spatial.distance import cosine
from transformers import AutoModel, AutoTokenizer

# Import our models. The package will take care of downloading the models automatically
tokenizer = AutoTokenizer.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;qiyuw/pcl-bert-base-uncased&quot;)

# Tokenize input texts
texts = [
    &quot;There's a kid on a skateboard.&quot;,
    &quot;A kid is skateboarding.&quot;,
    &quot;A kid is inside the house.&quot;
]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=&quot;pt&quot;)

# Get the embeddings
with torch.no_grad():
    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output

# Calculate cosine similarities
# Cosine similarities are in [-1, 1]. Higher means more similar
cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])
cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])

print(&quot;Cosine similarity between \&quot;%s\&quot; and \&quot;%s\&quot; is: %.3f&quot; % (texts[0], texts[1], cosine_sim_0_1))
print(&quot;Cosine similarity between \&quot;%s\&quot; and \&quot;%s\&quot; is: %.3f&quot; % (texts[0], texts[2], cosine_sim_0_2))
</code></pre>
<p>I think the issue must be model specific since I receive the warning about newly initialized pooler weights, and pooler_output is ultimately what the code reads to inform similarity:</p>
<pre><code>Some weights of RobertaModel were not initialized from the model checkpoint at qiyuw/pcl-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>Switching to an alternative model which does not give this warning (for example, sentence-transformers/all-mpnet-base-v2) makes the outputs reproducible, so I think it is because of the above warning about initialization of weights.  So here are my questions:</p>
<ol>
<li>Can I make the output reproducible by initialising/seeding the model differently?</li>
<li>If I can't make the outputs reproducible, is there a way in which I can improve the accuracy to reduce the amount of variation between runs?</li>
<li>Is there a way to search huggingface models for those which will initialise the pooler weights so I can find a model which does suit my purposes?</li>
</ol>
<p>Thanks in advance</p>
","2024-06-30 20:21:45","3","Question"
"78689011","78687048","","<p>One way which may make it a bit faster (hard to guess with your small sample)  is to work hierarchically with a dictionary using the first two items of the triple as key (after they were sorted) and a <code>Counter</code> as value which collects the counts of the last item of the triple:</p>
<pre><code>import pandas as pd
from collections import defaultdict, Counter

# Example DataFrame A (replace this with your actual DataFrame)
A = pd.DataFrame({
    'user_id': [1, 2, 3],
    'history': [[1, 2, 3], [2, 3, 4], [1, 3, 5]]
})

# Initialize a dictionary to store counters of pairs
hierarchic_counts = defaultdict(Counter)

# Iterate over history column of A. This is definitely faster than iterating
#   over the rows of A
for history in A['history']:
    n = len(history)

    # Iterate over pairs with i &lt; j
    for i in range(n):
        for j in range(i + 1, n):
            bi = history[i]
            bj = history[j]
            if bi &lt; bj:
                key = (bi, bj)
            else:
                key = (bj, bi)

            hierarchic_counts[key].update(history[j + 1:])

# Convert to original triple counts
triple_counts = {}
for key, counter in hierarchic_counts.items():
    for third, count in counter.items():
        triple_counts[key + (third,)] = count

# Output the counts of all triples
for triple, count in triple_counts.items():
    print(f&quot;Triple {triple}: Count = {count}&quot;)
</code></pre>
","2024-06-30 15:24:20","1","Answer"
"78687946","","How to optimize PyTorch and Ultralytics Yolo code to utilize GPU?","<p>I am working on a project which involves object detection and tracking. For object detection I am using <code>yolov8</code> and for tracking, I am using <code>SORT</code> tracker. After running the below code, my GPU usage is always under 10% and CPU usage is always more than 40%. I have installed <code>cuda</code>, <code>cudnn</code> and have installed <code>torch</code> using <code>cuda</code>. I have also compiled <code>opencv</code> with <code>cuda</code> support. I am using <code>RTX 4060 ti</code> but looks like it's not being used.</p>
<p>Is there a way to further optimize the below code so that all of the work is handled by GPU and not CPU?</p>
<pre><code>from src.sort import *
import cv2
import time
import torch
import numpy as np
from ultralytics import YOLO

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f&quot;Using device: {device}&quot;)
sort_tracker = Sort(max_age=20, min_hits=2, iou_threshold=0.05)
model = YOLO('yolov8s.pt').to(device)

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()  
    if not ret:
        print(&quot;**No frame received**&quot;)
        continue

    results = model(frame)
    dets_to_sort = np.empty((0, 6))
    for result in results:
        for obj in result.boxes:
            bbox = obj.xyxy[0].cpu().numpy().astype(int)
            x1, y1, x2, y2 = bbox

            conf = obj.conf.item()
            class_id = int(obj.cls.item())
            dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, class_id])))
    
    tracked_dets = sort_tracker.update(dets_to_sort)
    for det in tracked_dets:
        x1, y1, x2, y2 = [int(i) for i in det[:4]]
        track_id = int(det[8]) if det[8] is not None else 0
        class_id = int(det[4])
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
        cv2.putText(frame, f&quot;{track_id}&quot;, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)

    frame = cv2.resize(frame, (800, int(frame.shape[0] * 800 / frame.shape[1])), interpolation=cv2.INTER_NEAREST)
    cv2.imshow(&quot;Frame&quot;, frame)
    key = cv2.waitKey(1)
    if key == ord(&quot;q&quot;):
        break
    if key == ord(&quot;p&quot;):
        cv2.waitKey(-1)
    
cap.release()  
cv2.destroyAllWindows()
</code></pre>
","2024-06-30 07:43:52","1","Question"
"78687447","78687048","","<p>You don't need to loop over all combinations. just transform your lists to <code>frozenset</code> and count with <a href=""https://docs.python.org/fr/3/library/collections.html#collections.Counter"" rel=""nofollow noreferrer""><code>collections.Counter</code></a>:</p>
<pre><code>from collections import Counter

Counter(map(frozenset, A['history']))
</code></pre>
<p>Output:</p>
<pre><code>Counter({frozenset({1, 2, 3}): 1,
         frozenset({2, 3, 4}): 1,
         frozenset({1, 3, 5}): 1})
</code></pre>
<p>or sort and convert to tuples:</p>
<pre><code>from collections import Counter

Counter(tuple(sorted(x)) for x in A['history'])
</code></pre>
<p>Output:</p>
<pre><code>Counter({(1, 2, 3): 1,
         (2, 3, 4): 1,
         (1, 3, 5): 1})
</code></pre>
<p>This will run in <code>O(n)</code>.</p>
<p>Note that you could also use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html"" rel=""nofollow noreferrer""><code>value_counts</code></a>:</p>
<pre><code>A['history'].map(frozenset).value_counts()

history
(1, 2, 3)    1
(2, 3, 4)    1
(1, 3, 5)    1
Name: count, dtype: int64

A['history'].map(sorted).value_counts()

history
[1, 2, 3]    1
[2, 3, 4]    1
[1, 3, 5]    1
Name: count, dtype: int64
</code></pre>
","2024-06-30 01:22:01","-1","Answer"
"78687394","","Confusion about output sizes of GAN","<p>I am trying to understand a code, I am confused about the test cells. When i am printing the shape of the output it is hidden_output.shape =(num_test, 20, 4, 4), test_hidden_block_stride(hidden_output).shape) == (num_test, 20, 10, 10) and Gen_output.shape=(num_test, 1,28,28) for Mnist dataset. I am trying to understand how the sizes are being calculated here. Any help will be greatly appreciated!</p>
<pre><code>class Generator(nn.Module):
    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):
        super(Generator, self).__init__()
        self.z_dim = z_dim
        # Build the neural network
        self.gen = nn.Sequential(
            self.make_gen_block(z_dim, hidden_dim * 4),
            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),
            self.make_gen_block(hidden_dim * 2, hidden_dim),
            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),
        )
def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=0 ,final_layer=False):
        # Build the neural block
        layers = []
        layers.append(nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding, output_padding=padding))
        if not final_layer:
            layers.append(nn.BatchNorm2d(output_channels))
            layers.append(nn.ReLU(True))
        else:
            layers.append(nn.Tanh())
        
        return nn.Sequential(*layers)
# Testing
gen = Generator()
num_test = 100
# Test the hidden block
test_hidden_noise = get_noise(num_test, gen.z_dim)
test_hidden_block = gen.make_gen_block(10, 20, kernel_size=4, stride=1)
test_uns_noise = gen.unsqueeze_noise(test_hidden_noise)
hidden_output = test_hidden_block(test_uns_noise)
# Check that it works with other strides
test_hidden_block_stride = gen.make_gen_block(20, 20, kernel_size=4, stride=2)
test_final_noise = get_noise(num_test, gen.z_dim) * 20
test_final_block = gen.make_gen_block(10, 20, final_layer=True)
test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)
final_output = test_final_block(test_final_uns_noise)
# Test the whole thing:
test_gen_noise = get_noise(num_test, gen.z_dim)
test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)
gen_output = gen(test_uns_gen_noise)
</code></pre>
<p>I am trying to calculate the sizes from the formula by hand. I am just seeing different kernel sizes and strides and padding. not sure which values to use.</p>
","2024-06-30 00:41:37","0","Question"
"78687048","","Count all unique triples","<p>Suppose that I have a pandas data frame A with columns called user_id and history where history is an array of ints. And the possible histories are bounded from above by 2000. I need to iterate through all rows of A, for each history b = [b1, b2, b3, ..., bn]. All bi's are unique(only appearing once in the array). I need to find all possible triples (bi, bj, bk) such that i &lt; j &lt; k, and count the occurrences of all such triples. In addition, we treat the triple (bi, bj, bk) to be the same as (bj, bi, bk) if bi &gt; bj.</p>
<pre><code>import pandas as pd
from itertools import combinations

# Example DataFrame A (replace this with your actual DataFrame)
import pandas as pd
from collections import defaultdict

# Example DataFrame A (replace this with your actual DataFrame)
A = pd.DataFrame({
    'user_id': [1, 2, 3],
    'history': [[1, 2, 3], [2, 3, 4], [1, 3, 5]]
})

# Initialize a dictionary to store counts of triples
triple_counts = defaultdict(int)

# Iterate over each row of A
for index, row in A.iterrows():
    history = row['history']
    n = len(history)

    
    # Iterate over all triples (bi, bj, bk) where i &lt; j &lt; k
    for i in range(n):
        for j in range(i + 1, n):
            for k in range(j + 1, n):
                bi = history[i]
                bj = history[j]
                bk = history[k]
                
                if bi &lt; bj:
                    triple_counts[(bi, bj, bk)] += 1
                else:
                    triple_counts[(bj, bi, bk)] += 1

# Output the counts of all triples
for triple, count in triple_counts.items():
    print(f&quot;Triple {triple}: Count = {count}&quot;)
</code></pre>
<p>The issue with this approach is that A is an extremely large data frame and each row has a complexity of O(n^3) so it takes forever to complete this computation. Is there faster way to do this possibly leveraging pytorch or tensor operations?</p>
","2024-06-29 20:58:46","0","Question"
"78685399","78033871","","<p>I faced the same issue when using SentenceTransformersTrainer. Make sure you <strong>RESTART</strong> your sessios/kernal after the install of accelerate and transformer [pytorch].</p>
<p>Also, do the above if you are using T4 GPU from Colab.</p>
","2024-06-29 08:04:10","0","Answer"
"78683433","78683099","","<p><code>torch.where(col &gt; S, coup, torch.tensor(0.0))</code> is not a differentiable operation. There is no smoothness to it. You are either selecting <code>coup</code> or <code>0</code>. There is no slope to the function. <code>coup * torch.sigmoid(col - S)</code> will give you a result that is similar to your operation, but is differentiable.</p>
<p>In your example, we are selecting from <code>coup</code> when <code>col &gt; S</code> and choosing <code>0</code> otherwise. In the differentiable version, we get <code>coup</code> when <code>col</code> is much greater than <code>S</code> and <code>0</code> when <code>col</code> is much less than <code>S</code>. In the middle, we get something in between, and you will likely need to tune the scale and offset to get the exact loss function you need for your application, something like <code>torch.sigmoid(alpha * (col + beta - S) ** gamma)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
from torch import autograd

# Define the parameters with requires_grad=True
r = torch.tensor(0.03, requires_grad=True)
q = torch.tensor(0.02, requires_grad=True)
v = torch.tensor(0.14, requires_grad=True)
S = torch.tensor(1001.0, requires_grad=True)

# Generate random numbers and other tensors
Z = torch.randn(10000, 5)
t = torch.tensor(np.arange(1.0, 6.0))
c = torch.tensor([0.2, 0.3, 0.4, 0.5, 0.6])

# Calculate mc_S with differentiable operations
mc_S = S * torch.exp((r - q - 0.5 * v * v) * t + Z.cumsum(axis=1))

# Calculate payoff with differentiable operations
res = []
alpha = 1.0
beta = 0.0
gamma = 1.0
for col, coup in zip(mc_S.T, c):
    payoff = coup * torch.sigmoid(alpha * (col + beta - S) ** gamma)
    res.append(payoff)
    mask = mask * (payoff == 0)

v = torch.stack(res).T
result = v.sum(axis=1).mean()

# Compute gradients - breaks here
grads = autograd.grad(result, [r, q, v, S], allow_unused=True, retain_graph=True)
print(grads)
</code></pre>
","2024-06-28 16:04:16","0","Answer"
"78683099","","Gradient flow in Pytorch for autocallable options","<p>I have the following code:</p>
<pre><code>import numpy as np
import torch
from torch import autograd

# Define the parameters with requires_grad=True
r = torch.tensor(0.03, requires_grad=True)
q = torch.tensor(0.02, requires_grad=True)
v = torch.tensor(0.14, requires_grad=True)
S = torch.tensor(1001.0, requires_grad=True)

# Generate random numbers and other tensors
Z = torch.randn(10000, 5)
t = torch.tensor(np.arange(1.0, 6.0))
c = torch.tensor([0.2, 0.3, 0.4, 0.5, 0.6])

# Calculate mc_S with differentiable operations
mc_S = S * torch.exp((r - q - 0.5 * v * v) * t + Z.cumsum(axis=1))

# Calculate payoff with differentiable operations
res = []
mask = 1.0
for col, coup in zip(mc_S.T, c):
    payoff = mask * torch.where(col &gt; S, coup, torch.tensor(0.0))
    res.append(payoff)
    mask = mask * (payoff == 0)

v = torch.stack(res).T
result = v.sum(axis=1).mean()

# Compute gradients - breaks here
grads = autograd.grad(result, [r, q, v, S], allow_unused=True, retain_graph=True)
print(grads)
</code></pre>
<p>I'm trying to price an autocallable option with early knockout and require the sensitivities to input variables.</p>
<p>However, the way the coupons are calculated (the c tensor in the code above), breaks the computational graph and I'm unable to obtain the gradients. Is there a way to get this code to calculate the derivatives?</p>
<p>Thanks</p>
","2024-06-28 14:44:03","0","Question"
"78681145","","PyTorch UserWarning: Failed to initialize NumPy: _ARRAY_API not found and BERTModel weight initialization issue","<p>I am working with PyTorch and the Hugging Face Transformers library to fine-tune a BERT model (UFNLP/gatortron-base) for a downstream task.</p>
<p>I received a warning related to NumPy initialization:</p>
<blockquote>
<p>C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\storage.py:321: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\torch\csrc\utils\tensor_numpy.cpp:84.)</p>
</blockquote>
<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>type himport torch
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('UFNLP/gatortron-base')
model = BertModel.from_pretrained('UFNLP/gatortron-base')

model.eval()

def prepare_input(text):
    tokens = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True, max_length=512, truncation=True)
    return tokens['input_ids'], tokens['attention_mask']

def get_response(input_ids, attention_mask):        
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        if 'logits' in outputs:
            predictions = torch.argmax(outputs['logits'], dim=-1)
        else:
            # Adjust this based on the actual structure of `outputs`
            predictions = torch.argmax(outputs[0], dim=-1) 

        # predictions = torch.argmax(outputs.logits, dim=-1)
        return tokenizer.decode(predictions[0], skip_special_tokens=True)

input_text = &quot;Hello, how are you?&quot;
input_ids, attention_mask = prepare_input(input_text)
response = get_response(input_ids, attention_mask)
print(&quot;Response from the model:&quot;, response)ere
</code></pre>
<ul>
<li>Python: 3.12</li>
<li>NumPy: 1.19.5</li>
</ul>
","2024-06-28 07:25:01","10","Question"
"78678443","78676119","","<p>The first thing is to question whether it truly makes sense to use a gaussian distribution instead of a categorical. If you have decided that yes, that is what you should do... Then simply you have a set of state-action pairs. Your states are your data and your actions are your labels. Use your neural network to draw samples and optimize the <strong>binary crossentropy</strong> between your output and the label.</p>
<p>The cross-entropy is very much related to the KL divergence. In fact, you can pretend that you are using the KL divergence and just ignore p(x) because your model's parameters do not depend on it and the partial derivative is zero.</p>
<p>Of course since the bottomline is that you want to learn gaussians with a standard deviation of zero, it might not be a fun experiment. To circumvent it, and only if it makes sense within the broad context of your experiment, you can exclude the variance from the learnable parameters. For the purpose of numerical stability, you can treat it as a hyperparameter that you set at a very small value. Then you only have to get your neural network to learn the mean, which should be equivalent to your delta function.</p>
<p>Another thing to consider is whether you assume a high dimensional gaussian with a dimensionality equal to the number of discrete actions, or if you assume independent gaussians for each category. The difference is in whether you wish to model correlations between different actions.</p>
","2024-06-27 14:59:33","0","Answer"
"78676169","78676089","","<p>I solved the problem I transposed the shape in the customdataset class then I re-transposed it to view the frame images</p>
","2024-06-27 07:36:56","0","Answer"
"78676119","","How do I fit a normal distribution to a dirac delta distribution for behavior cloning in reinforcement learning?","<p>I am attempting to apply behavior cloning to actor-critic agents that operate under a continuous action space. I have some dirac delta function is denoted as p_bc(a|x) where a is the action and x is the state. I have a test neural network that represents the actor where it trys to learn some mean, u(x), and sigma, o(x), for a given x. I have tested MSE and NLL as loss functions. I wanted to test KL loss but because I could not find any documentation on dirac delta functions, I am unsure if there is a distribution module in pytorch. Basically, my question is what am I doing wrong and what are some alternatives I could do?</p>
<p>I have some test neural network in pytorch</p>
<pre><code>class DistributionFitModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.model = nn.Sequential(
            nn.Linear(1, 16),
            nn.Linear(16, 32),
            nn.Linear(32, 16)
        )
        
        self.mu = nn.Sequential(nn.Linear(16, 1), nn.ReLU())
        self.logvar = nn.Sequential(nn.Linear(16, 1), nn.ReLU())
        
    def forward(self, x):
        x = self.model(x)
        mu = self.mu(x)
        logvar = self.logvar(x)
        
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return eps.mul(std).add_(mu)
    
    def mu_sigma(self, x):
        x = self.model(x)
        return self.mu(x), torch.exp(0.5 * self.logvar(x))
</code></pre>
<p>I use logvar instead of var directly because that's what I've seen VAE implementations do</p>
<pre><code>keys = [(0, -1), (1, 1), (2, -1), (3, 0), (4, 1), (5, 0)]
ys = []
xs = []
L = 1000
for x_mul, y_mul in keys:
    ys += (np.ones(1000) * y_mul).tolist()
    xs += (np.ones(1000) * x_mul).tolist()
</code></pre>
<p>I generate 1000 samples of a given x and corresponding action (ie x = 0, a = -1 for first key)</p>
<p>I have some dataset/dataloader to load in the generated data</p>
<pre><code>class DistDataset(Dataset):
    def __init__(self, x, y):
        super().__init__()
        self.x = x
        self.y = y
        
    def __len__(self):
        return len(self.x)
        
    def __getitem__(self, item):
        return torch.tensor(self.x[item]), torch.tensor(self.y[item])
dataset = DistDataset(xs, ys)
dataloader = DataLoader(dataset, batch_size=512, shuffle=True)
</code></pre>
<p>I have the training loop set up as</p>
<pre><code>for _ in range(1000):
    running_loss = 0
    for x, y in dataloader:
        x = x.reshape(-1, 1).float()
        y = y.reshape(-1, 1).float()
        
        pred = model(x).reshape(-1, 1)
        optimizer.zero_grad()
        loss = F.mse_loss(pred, y)
        
        # mu, sigma = model.mu_sigma(x)
        # dist = Normal(mu, sigma)
        # values = dist.log_prob(y)
        # loss += torch.sum(values)
        
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    print(running_loss / len(dataloader))
</code></pre>
<p>when I call model.mu_sigma(x), I am getting mu = 0 and sigma = 1 for all the outputs</p>
<p>Is there something I am doing wrong? Is there some better loss function I should be doing?</p>
","2024-06-27 07:27:34","0","Question"
"78676089","","pytorch doesn't read input channels as it's supposed","<p>I'm building a conv3d model for videos using pytorch. the input is <code>(2, 30, 46, 140, 1)</code> but pytorch reads that the input channel is the second one and it's actually the 4th.</p>
<pre><code>        self.conv1 = nn.Conv3d(in_channels=1, out_channels=32, kernel_size=3, padding=1)
Given groups=1, weight of size [32, 1, 3, 3, 3], expected input[2, 30, 46, 140, 1] to have 1 channels, but got 30 channels instead
</code></pre>
<p>I tried to reshape the input to be <code>(2, 1, 30, 46, 140)</code> but then it doesn't show the frames and gives an error that input is wrong</p>
<pre><code>TypeError: Invalid shape (30, 46, 140) for image data
</code></pre>
<p>keep in mind that I tried running the same model with same input on tensorflow and it worked but I can't work with tensorflow due to dependency issues</p>
","2024-06-27 07:19:41","0","Question"
"78675367","78675342","","<p>To use YOLOv8 in Jupyter Lab, you will need to install the necessary libraries, including the YOLOv8 package, and then run the code to perform object detection.</p>
<p>Install YOLOv8 and dependencies:</p>
<pre><code>!pip install ultralytics
!pip install opencv-python-headless
</code></pre>
<p>Load YOLOv8 model and perform detection:</p>
<pre><code>import cv2
from ultralytics import YOLO

# Load YOLOv8 model
model = YOLO('yolov8n.pt')  # You can choose a different model size such as yolov8s.pt, yolov8m.pt, etc.

# Load an image
img_path = 'path/to/your/image.jpg'
img = cv2.imread(img_path)

# Run inference
results = model(img)

# Display results
results.show()
</code></pre>
<p>Download an image for testing:</p>
<pre><code>import requests

url = 'https://raw.githubusercontent.com/devicons/devicon/master/icons/typescript/typescript-original.svg'
img_path = 'downloaded_image.svg'

response = requests.get(url)
with open(img_path, 'wb') as file:
    file.write(response.content)

# Now you can use this image with YOLOv8
img = cv2.imread(img_path)
results = model(img)
results.show()
</code></pre>
<p>Display the results in Jupyter Lab:</p>
<pre><code>import matplotlib.pyplot as plt

# Convert the image from BGR to RGB
img_rgb = cv2.cvtColor(results.ims[0], cv2.COLOR_BGR2RGB)

# Display the image with detections
plt.imshow(img_rgb)
plt.axis('off')
plt.show()
</code></pre>
","2024-06-27 02:47:32","0","Answer"
"78675342","","Any tips as to how to use github's repositories in your python code?","<p>I am trying to utilize YOLOv8's software seen on their github, but I don't know how to implant it in jupyter labs.</p>
<p>I am trying to see what lines of code I should implement in python 3.x to get this to work. Would this require a pip install of some sort? How can I make my code connect with the internet in this way?</p>
","2024-06-27 02:37:52","0","Question"
"78674928","78673971","","<blockquote>
<p>After the catted vector is sorted, I will use the first 10 rows of the catted tensor to calculate loss.</p>
</blockquote>
<p>You need to use the neural network's output to calculate loss, not the indices it produces with argsort. The operation that extracts indices from the scores will lose the gradients because such operation is not differentiable.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
out = torch.randn(10, 100).requires_grad_(True)
t_v, t_i = torch.topk(out, 10, dim=-1)
print(t_v.requires_grad) # prints True
print(t_i.requires_grad) # prints False
</code></pre>
<p>Simply, you have to figure out a way to compute the loss from <code>t_v</code> to be able to propagate.</p>
<p>This post on <a href=""https://stats.stackexchange.com/q/444832"">soft topk</a> may be of interest to you.</p>
","2024-06-26 22:42:25","0","Answer"
"78674850","78671742","","<p>Might be a workaround, but technically you could already modify the Dataset param from within the data_loader object. I have a custom dataset for images, which inherits from Dataset, with one parameter being <code>self.target_size</code> which modifies the output size within <code>__getitem__</code>. Small example:</p>
<pre><code>from torch.utils.data import Dataset, DataLoader

class myDataset(Dataset):
    def __init__(self, target_size, ...):
        self.target_size = target_size
        ...

    def __getitem__(self, idx):
        X = load_image(idx)
        X = resize_image(X, self.target_size)
        return X

my_ds = myDataset(target_size=(1024, 1024), ...)
dl = DataLoader(my_ds)

batch = next(iter(dl))
print(batch.shape)  # torch.Size([1, 3, 1024, 1024])

dl.dataset.target_size = (2048, 2048)  # dl.dataset is the attribute that contains my_ds

batch = next(iter(dl))
print(batch.shape)  # torch.Size([1, 3, 2048, 2048])
</code></pre>
<p>It also works directly on the Dataset object:</p>
<pre><code>img = my_ds.__getitem__(0)
print(img.shape)  # (3,2048,2048)

my_ds.target_size = (1024,1024)

img = my_ds.__getitem__(0)
print(img.shape)  # (3,1024,1024)
</code></pre>
<p>Within a training or evaluation loop you could just modify the dataset parameter I imagine. Modifying the input parameters for <code>__getitem__</code> does not seem to me like the right way. I may be wrong of course.</p>
","2024-06-26 22:08:20","1","Answer"
"78674451","78674337","","<p>You are trying to install a non-existent version of Pytorch. There's no such thing as Pytorch 0.4.0.</p>
<p>Perhaps you want <a href=""https://pypi.org/project/torch/#data"" rel=""nofollow noreferrer"">torch 1.4.0</a>, if so, you can install it by running <code>pip install torch==1.4.0</code></p>
<p>See the <a href=""https://pypi.org/project/torch/#history"" rel=""nofollow noreferrer"">Release history</a>.</p>
","2024-06-26 20:06:54","1","Answer"
"78674337","","Pytorch installing","<p>i want to innstall Pytorch 0.4.0 which gives me error so i downgarde my python version to 3.6 but it gives this error.</p>
<p>Could not fetch URL <a href=""https://pypi.python.org/simple/torch/"" rel=""nofollow noreferrer"">https://pypi.python.org/simple/torch/</a>: There was a problem confirming the ssl certificate: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify
failed (_ssl.c:749) - skipping
Could not find a version that satisfies the requirement torch==0.4.0 (from versions: )
No matching distribution found for torch==0.4.0</p>
<p>i downgrade my python version as per requirments but still it gives error</p>
","2024-06-26 19:35:16","0","Question"
"78674317","","Correct way to swap PyTorch tensors without copying","<p>I have two PyTorch tensors x, y with the same dimensions. I would like to swap the data behind the two tensors, ideally without having to copy. The purpose of this is to have code elsewhere that holds onto the tensor x to now read &amp; write the data y and vice-versa.</p>
<p>What's the correct way to do this? Will this swap potentially break any code that holds onto references to these tensors? Will reference counting/GC for the tensors still work correctly after the swap?</p>
","2024-06-26 19:29:24","4","Question"
"78673971","","Can the following Pytorch operations be backwarded?","<p>I have a tensor A, which is from original point cloud data. Its size is (N,3). Besides, I have a tensor B.It is an output score tensor by a neural network.Its size is (N,1). I firstly use torch.cat to cat A and B on the 1 dim. Then I use torch.argsort to order the catted tensor according to the value of the last column and get the indices. Then I used torch.gather to sort the catted vector. After the catted vector is sorted, I will use the first 10 rows of the catted tensor to calculate loss. When I calculate loss, I will use the first three columns in the first 10 rows, which is from the original point cloud data.</p>
<p>Can the process be backwarded(the gradients are not 0.) by Pytorch？If the answer is no, please tell me how to slove the problem? Thanks.</p>
","2024-06-26 17:51:56","0","Question"
"78673960","78669307","","<p>Turns out the solution was available in this <a href=""https://stackoverflow.com/questions/75034301/how-to-display-metrics-and-value-on-sagemaker-pipeline-ui?rq=2"">post</a></p>
<p>If using custom algorithms, and easy way to add metric is to print / log them in the script:</p>
<pre><code>print('METRIC train_accuracy: {}'.format(accuracy))
</code></pre>
<p>And in the <code>metric_definitions</code> you have to use the exact same naming in the Regex:</p>
<pre><code>{'Name': 'train_accuracy', 'Regex': &quot;METRIC train_accuracy: ([0-9]+(.|e\-)[0-9]+)\w+&quot;}
</code></pre>
","2024-06-26 17:50:14","0","Answer"
"78671742","","Make pytorch __getitem__ Dataset method dependent on a parameters","<p>I would like to work with a Pytorch custom Dataset class making the output of <strong>getitem</strong> method depending on a parameter.</p>
<pre><code>class myDataset(Dataset):

    def __init__(self, myparam, dataframe, dataframe2):
        self. myparam = myparam
        self.dataframe = dataframe
        self.dataframe2 = dataframe2

    def __getitem__(self, idx):

        X = self.dataframe.iloc[idx].values
        y = self.dataframe2.iloc[idx].values
        
        if self.myparam == 1:
            X = ... # manipulate X in some way
        return X, y

dataset = myDataset(myparam, dataframe, dataframe2)
validation_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler)


</code></pre>
<p>The problem is that I would like to access the Dataset parameter from an instantiated data loader in such a way I can choose to <em>manipulate</em> or not the <code>X</code> data directly from the <code>validation_loader object</code></p>
<p>There could be several workarounds to deal with this issue but I would be interested in a &quot;good code&quot; approach.</p>
","2024-06-26 10:09:43","1","Question"
"78669900","78669860","","<p>GPUs by default run one kernel at a time with maximum parallelization. This means your multiple model instances are blocking each other, resulting in the slower execution.</p>
<p>You can try schedule processes in a delayed fashion using <code>joblib</code> (see <a href=""https://github.com/TorchEnsemble-Community/Ensemble-Pytorch/blob/master/torchensemble/adversarial_training.py#L310"" rel=""nofollow noreferrer"">torchensemble</a> as an example), or schedule models using a <a href=""https://pytorch.org/docs/stable/generated/torch.cuda.stream.html"" rel=""nofollow noreferrer"">pytorch cuda stream</a>.</p>
<p>That said, neither approach will give you perfect parallelization.</p>
<p>GPU kernels will still be blocking if the workload is enough to saturate GPU occupancy.</p>
<p>There's also significant overhead on the CPU (scheduling CUDA kernels, data processing, moving data to the CPU) and i/o (reading multiple datasets from disk concurrently) that could prove rate limiting regardless of GPU parallelization.</p>
<p>Before trying the parallel approaches, you should try optimize the iterative approach by increasing batch size and/or pre-loading data into memory. It may end up being the fastest and easiest approach.</p>
","2024-06-26 00:04:19","1","Answer"
"78669860","","Concurrently test several Pytorch models on a single GPU slower than iterative approach","<p>I want to test several models on different datasets. I want them to run concurrently on a single gpu.<br />
The general puesdo code is below. You would be able to run this with any dummy model or dataset.
However, I notice this is often 20% slower than just using an iterative approach. What is wrong with my current solution?</p>
<pre><code>def test_model(paths):
   model = Model(path to config)
   data = DataLoader(*Dataset(path to data)
   trainer = PyTorchLightning Trainer(config)


def main():
    with ProcessPoolExecutor(max_workers=args.max_processes) as executor:
        futures = {executor.submit(test_model, paths): (paths) for path in paths_to models_to_test}
        for future in as_completed(futures):
            try:
                results_from_run = future.result()
                full_results_df = pd.concat([full_results_df, results_from_run], ignore_index=True)
            except Exception as e:
                print(f&quot;An error occurred while processing {futures[future]}: {e}&quot;)
</code></pre>
","2024-06-25 23:37:39","0","Question"
"78669307","","Why aren't my metrics showing in SageMaker (CloudWatch)?","<p>I'm training a <a href=""https://arxiv.org/abs/1908.10084"" rel=""nofollow noreferrer"">S-BERT</a> model in SageMaker, using Huggins Face library. I've followed the HF <a href=""https://discuss.huggingface.co/t/training-metrics-in-aws-sagemaker/12513"" rel=""nofollow noreferrer"">tutorials</a> on how to define metrics to be tracked in the <code>huggingface_estimator</code>, yet when my model is done training I cannot see any metric either in CloudWatch or by fetching the latest training job results:
`</p>
<pre><code>from sagemaker.analytics import TrainingJobAnalytics
df = TrainingJobAnalytics(training_job_name=huggingface_estimator.latest_training_job.name).dataframe()
</code></pre>
<p>returns:</p>
<pre><code>Warning: No metrics called loss found
Warning: No metrics called learning_rate found
Warning: No metrics called eval_loss found
Warning: No metrics called eval_accuracy found
Warning: No metrics called eval_f1 found
Warning: No metrics called eval_precision found
Warning: No metrics called eval_recall found
Warning: No metrics called eval_runtime found
Warning: No metrics called eval_samples_per_second found
Warning: No metrics called epoch found
</code></pre>
<p>Here's the code below</p>
<pre><code>from sagemaker.huggingface import HuggingFace
from sagemaker import get_execution_role

from sagemaker import image_uris

role = get_execution_role() 

source_dir = 's3://...'
output_path = 's3://...'

metric_definitions = [{'Name': 'loss', 'Regex': &quot;'loss': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'learning_rate', 'Regex': &quot;'learning_rate': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_loss', 'Regex': &quot;'eval_loss': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_accuracy', 'Regex': &quot;'eval_accuracy': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_f1', 'Regex': &quot;'eval_f1': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_precision', 'Regex': &quot;'eval_precision': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_recall', 'Regex': &quot;'eval_recall': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_runtime', 'Regex': &quot;'eval_runtime': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_samples_per_second', 'Regex': &quot;'eval_samples_per_second': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'epoch', 'Regex': &quot;'epoch': ([0-9]+(.|e\-)[0-9]+),?&quot;}]

estimator_image = image_uris.retrieve(framework='pytorch',region='eu-west-1',version='1.13.1',py_version='py39',image_scope='training', instance_type='ml.p3.2xlarge')


huggingface_estimator = HuggingFace(
                            entry_point='script.py',
                            dependencies=['requirements.txt', 'model.py'],
                            instance_type='ml.p3.2xlarge',
                            base_job_name='...',
                            output_path=output_path,
                            role=role,
                            instance_count=1,
                            pytorch_version=None,
                            py_version=None,
                            metric_definitions = metric_definitions,
                            image_uri=estimator_image,
                            hyperparameters = {
                                'epochs': 1,
                                'train_batch_size': 64,
                                'eval_batch_size':64,
                                'learning_rate': 2e-5,
                                'model_name':'distilbert-base-uncased'})

huggingface_estimator.fit({'train': 's3://...',
                           'test': 's3://...'})
</code></pre>
","2024-06-25 19:57:20","0","Question"
"78666804","78665646","","<p>Try to reduce <code>num_workers</code> and <code>prefetch_factor</code>. It may spend all the time fetching that 1024 batches using all threads.</p>
","2024-06-25 10:35:08","1","Answer"
"78665646","","PyTorch model training with DataLoader is too slow","<p>I'm training a very small NN using the HAM10000 dataset. For loading the data I'm using the DataLoader that ships with PyTorch:</p>
<pre><code>class CocoDetectionWithFilenames(CocoDetection):
    def __init__(self, root: str, ann_file: str, transform=None):
        super().__init__(root, ann_file, transform)

    def get_filename(self, idx: int) -&gt; str:
        return self.coco.loadImgs(self.ids[idx])[0][&quot;file_name&quot;]


def get_loaders(root: str, ann_file: str) -&gt; tuple[CocoDetection, DataLoader, DataLoader, DataLoader]:
    transform = transforms.Compose([
        transforms.ToTensor()
    ])
    dataset = CocoDetectionWithFilenames(
        root=root,
        ann_file=ann_file,
        transform=transform
    )
    train_size = int(0.7 * len(dataset))
    valid_size = int(0.15 * len(dataset))
    test_size = len(dataset) - train_size - valid_size
    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])
    num_workers = os.cpu_count()
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=32,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=1024
    )
    valid_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=32,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=1024
    )
    test_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=32,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    return dataset, train_loader, valid_loader, test_loader
</code></pre>
<p>The thing is, when my training loop runs, the training itself is very fast, but the program spends 95% on the time inbetween epochs - probably loading the data:</p>
<pre><code>def extract_bboxes(targets: list[dict]) -&gt; list[torch.Tensor]:
    bboxes = []

    for target in targets:
        xs, ys, widths, heights = target[&quot;bbox&quot;]

        for idx, _ in enumerate(xs):
            x1, y1, width, height = xs[idx], ys[idx], widths[idx], heights[idx]
            # Convert COCO format (x, y, width, height) to (x1, y1, x2, y2)
            x2, y2 = x1 + width, y1 + height

            bboxes.append(torch.IntTensor([x1, y1, x2, y2]))

    return bboxes

num_epochs = 25
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for images, targets in train_loader_tqdm:
        images = images.to(device)
        bboxes = extract_bboxes(targets)
        bboxes = torch.stack(bboxes).to(device)

        optimizer.zero_grad(set_to_none=True)

        outputs = model(images)
        loss = criterion(outputs, bboxes)

        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    epoch_train_loss = running_loss / len(train_loader)

    train_losses.append(epoch_train_loss)
    print(f&quot;Epoch {epoch + 1}, Loss: {epoch_train_loss}&quot;)
    model.eval()
</code></pre>
<p>As you can see, the training loop code is quite simple, nothing weird happening there.</p>
","2024-06-25 06:12:50","0","Question"
"78665625","78662763","","<p>I was using Dataloader with shuffle=True once I changed shuffle to False it works fine. It makes sense now reading from disk with shuffle would be slow.</p>
","2024-06-25 06:06:26","0","Answer"
"78662763","","dataloader very slow with HDFf5 data","<p>I've got a very large dataset in HDF5 format, which I cannot load in memory all at once. I'm using a custom dataset from Torch.</p>
<p>Here's the code:</p>
<pre><code>import time
from utils import get_vocab_and_skipgrams
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import os
import h5py
import numpy as np
import torch

class CustomSkipGramDataset(Dataset):
    def __init__(self, filename, window_size, data_dir=&quot;training_data&quot;, data_exists=True):
        self.window_size = window_size
        self.filename = filename
        self.data_exists = data_exists
        self.vocab_path = os.path.join(data_dir, &quot;vocab.npy&quot;)
        self.hdf5_path = os.path.join(data_dir, &quot;skipgram.h5&quot;)
        
        if not data_exists:
            get_vocab_and_skipgrams(filename, data_dir)
        
        self.vocab = np.load(self.vocab_path, allow_pickle=True).tolist()
        self.vocab_size = len(self.vocab)
        self.hf = h5py.File(self.hdf5_path, &quot;r&quot;)
        self.dataset = self.hf[&quot;positive_skips&quot;]
        
    def __len__(self):
        return self.dataset.shape[0]
    
    def __getitem__(self, index):
        
        x, y = self.dataset[index]
        return x, y
</code></pre>
<p>Now when I'm loading it directly like this:</p>
<pre><code>with h5py.File(&quot;./training_data/skipgram.h5&quot;) as hf:
    dataset = hf[&quot;positive_skips&quot;]
    for a in range(1,100):
        print(torch.tensor(dataset[a:100*a]))

</code></pre>
<p>it is indeed very fast compared to Torch custom dataset. 100x faster almost. I know I'm doing something wrong.</p>
","2024-06-24 13:23:58","0","Question"
"78660713","78659141","","<p>This can be solved using the <a href=""https://pytorch.org/docs/stable/generated/torch.index_select.html"" rel=""nofollow noreferrer"">index_select</a> operation. Can you try the following?</p>
<pre><code>import torch
X=torch.tensor([[[[8, 2, 8, 5],
          [3, 7, 4, 0]],

         [[4, 5, 7, 4],
          [8, 3, 9, 5]]],


        [[[5, 2, 9, 3],
          [6, 4, 5, 4]],

         [[7, 3, 3, 7],
          [6, 3, 8, 9]]]])
Y=torch.tensor([1,1,0,1])
result = torch.index_select(X, dim=0, index=Y)
print(result[0].view(-1,4))


tensor([[5, 2, 9, 3],
        [6, 4, 5, 4],
        [7, 3, 3, 7],
        [6, 3, 8, 9]])
</code></pre>
<p>One liner:</p>
<pre><code>result = torch.index_select(X, dim=0, index=Y)[0].view(-1,4)
</code></pre>
<p>This should provide the requested result.</p>
","2024-06-24 05:39:38","0","Answer"
"78660414","","Print torch tensor without extra spacing in between scalars","<p>There is a huge space in between elements of my tensors when printing. I cannot read the tensor easily with this space.
How do I minimize it?</p>
<p>print settings I used:</p>
<pre><code>torch.set_printoptions(sci_mode=False, precision=4, linewidth=200, profile=&quot;full&quot;)
np.set_printoptions(suppress=True, precision=4, linewidth=200)
</code></pre>
<p>Here is a torch array:
<a href=""https://i.sstatic.net/KnnJfrCG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KnnJfrCG.png"" alt=""enter image description here"" /></a></p>
<p>Here is the same shape np array but much more readable because no spacing:
<a href=""https://i.sstatic.net/QESFYMnZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QESFYMnZ.png"" alt=""enter image description here"" /></a></p>
","2024-06-24 03:14:35","2","Question"
"78659141","","More efficient implementation of tensor indexing","<p>I currently have a tensor X of shape (2,2,2,4), and an indexing vector Y of shape (4):</p>
<pre><code>import torch
X=torch.tensor([[[[8, 2, 8, 5],
          [3, 7, 4, 0]],

         [[4, 5, 7, 4],
          [8, 3, 9, 5]]],


        [[[5, 2, 9, 3],
          [6, 4, 5, 4]],

         [[7, 3, 3, 7],
          [6, 3, 8, 9]]]])
Y=torch.tensor([1,1,0,1])
</code></pre>
<p>I would like to index X with Y, such that I arrive at a tensor of shape (2,2,4), which, if manually done, should look as follows (with the first dimension corresponding to the indices in Y):</p>
<pre><code>torch.stack([X[1][0][0], X[1][0][1], X[0][1][0], X[1][1][1]])
</code></pre>
<p>Which gives:</p>
<pre><code>tensor([[5, 2, 9, 3],
        [6, 4, 5, 4],
        [4, 5, 7, 4],
        [6, 3, 8, 9]])
</code></pre>
<p>I currently use a for loop to do this, but this is not ideal, and was wondering how to get to the desired result tensor in PyTorch from X and Y. Thanks a lot!</p>
<p>I tried a for loop to index into it, which works, but this is very slow.</p>
","2024-06-23 15:42:00","2","Question"
"78658755","78429005","","<p>I finally found a way to make it work using <code>jacrev</code> mentioned.</p>
<pre><code>from functorch import jacrev
    
def sample_SDE(t_vec, N_sample, x0):
    x0_per_sample = x0.unsqueeze(0).expand(N_sample, -1)
    return torchsde.sdeint(mySDE, x0_per_sample, t_vec, method = 'euler', bm = bm)

# Here we remove the N_sample dimension otherwise jacrev will think of it as another variable to differentiate through
x0 = torch.zeros(Dx, requires_grad = True).to(t_vec)

# then we differentiate
Jacob_autograd = torch.func.jacrev(sample_SDE, argnums=2)(t_vec, N_sample, x0)
</code></pre>
<p>in the last line, we specify that x0 (argunum 2) is the variable to differentiate with respect to.</p>
","2024-06-23 13:01:07","0","Answer"
"78649529","78646747","","<p>I was able to pass the data to the <code>fastai.Learner</code> to train the model and get results from <code>plot_confusion_matrix</code>. My conclusion that <code>fastai</code> is not designed to work with custom Datasets and DataLoaders and expecting you to use their API for loading the data. I think that in your case it might be worth to switch to
<code>TabularDataLoaders</code> and load the data using <code>TabularDataLoaders.from_df</code>. Or alternatively use <code>ImageBlock</code> if you are working with images.</p>
<p>Basically to give the most optimal solution to the question it is important to know what data are you using. Are you working with images? Are images stored in files? What type of files? Or the input data are simple arrays?</p>
<p>Also function <code>plot_top_losses</code> doesn't work well if you have numpy dataset as the input. Function plots <code>worst_k</code> examples, and most probably it works only with data that is loaded using <code>ImageBlocks</code> and etc.</p>
<p>Given current inputs there is two options how to fix the code:</p>
<ol>
<li>Assume numpy inputs. Create custom dataloader using fastai API.</li>
<li>Assume numpy inputs. Create custom dataloaders using pytorch API.</li>
</ol>
<p><strong>Solution 1:</strong>
Building custom numpy dataloader for fastai Learner using fastai DataBlock API:</p>
<pre><code>from fastai.vision.all import *
from fastai.data.all import *

def make_dataloaders_from_numpy_data(image, label, loader=False):
    def pass_index(idx):
        return idx

    def get_x(i):
        val = image[i]
        return torch.Tensor(val)

    def get_y(i):
        # val = [label[i]]
        # res = torch.Tensor(val).to(torch.int64)
        return label[i]

    dblock = DataBlock(
        blocks=(DataBlock, CategoryBlock),
        get_items=pass_index,
        get_x=get_x,
        get_y=get_y)

    # pass in a list of index
    num_images = image.shape[0]

    source = list(range(num_images))

    if not loader:
        ds = dblock.datasets(source)    
        return ds
    
    return dblock.dataloaders(source, batch_size = 1)    

train_ds = make_dataloaders_from_numpy_data(X_train, y_train)
test_ds = make_dataloaders_from_numpy_data(X_test, y_test)


train_ld = DataLoader(train_ds, batch_size=64)
test_ld = DataLoader(test_ds, batch_size=64)
dls = DataLoaders(train_ld, test_ld)


dls_val = make_dataloaders_from_numpy_data(X_val, y_val,loader=True)



# Initialize the model and the Learner
model = DraftCNN()
learn = Learner(dls, 
                model, 
                loss_func=CrossEntropyLossFlat(), 
                metrics=[accuracy, Precision(average='macro'),  Recall(average='macro'), F1Score(average='macro')])

# # # # Train the model
learn.fit_one_cycle(1)


# Get predictions and interpret them on the validation set
interp = ClassificationInterpretation.from_learner(learn, dl=dls_val)
interp.plot_confusion_matrix()
plt.show()

</code></pre>
<p><strong>Solution 2</strong></p>
<p>Building custom numpy dataloader for fastai Learner using pytorch API for <code>Dataset</code> and <code>DataLoader</code></p>
<pre><code>from torch.utils.data import Dataset
from fastai.data.core import DataLoaders

class CustomDataclass(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray):
        &quot;&quot;&quot;
        Will iterate over the dataset
        &quot;&quot;&quot;
        self.data = X
        self.labels = y

        # Not clear what is self.vocab for
        # However part of this attribute is used for plotting labels in `ClassificationInterpretation`
        self.vocab = (None,
                      ['class_0', 'class_1', 'class_2', 'class_3'])

    def __len__(self):
        return self.data.shape[0]

    def __getitem__(self, idx: int):
      data = self.data[idx,...]

      # labels can't be single values and must be converted to a list
      labels = [self.labels[idx]]

      return  (torch.Tensor(data),  
               torch.Tensor(labels).to(torch.int64) # labels must be integers
               )


train_ds = CustomDataclass(X_train, y_train)
test_ds = CustomDataclass(X_test, y_test)
val_ds = CustomDataclass(X_val, y_val)



from torch.utils.data import DataLoader
bs = 64
train_loader = DataLoader(train_ds, batch_size = bs)
test_loader = DataLoader(test_ds, batch_size = bs)

# Val dataset used in interpretation phase where pytorch dataloaders doesn't work
from fastai.data.core import DataLoader
val_loader = DataLoader(val_ds, batch_size = bs)



dls = DataLoaders(train_loader, test_loader)

# Initialize the model and the Learner
model = DraftCNN()
learn = Learner(dls, 
                model, 
                loss_func=CrossEntropyLossFlat(), 
                metrics=[accuracy, Precision(average='macro'),  Recall(average='macro'), F1Score(average='macro')])

# # # # Train the model
learn.fit_one_cycle(4)



# Get predictions and interpret them on the validation set
interp = ClassificationInterpretation.from_learner(learn, dl=val_loader)
interp.plot_confusion_matrix()
plt.show()

</code></pre>
<p><strong>Other errors that are fixed by my code:</strong></p>
<ol>
<li>dataset must return labels in lists:</li>
</ol>
<pre><code>learn.fit_one_cycle(4)
...
return torch.stack(batch, 0, out=out)

RuntimeError: stack expects each tensor to be equal size, but got [3] at entry 0 and [0] at entry 1
</code></pre>
<ol start=""2"">
<li>Added <code>vocab</code> attribute to pytorch DataClass:</li>
</ol>
<pre><code>interp = ClassificationInterpretation.from_learner(learn, dl=val_loader)
...
  File &quot;/Users/ivanpetrov/.pyenv/versions/3.11.6/envs/stack_overflow_env/lib/python3.11/site-packages/fastcore/basics.py&quot;, line 507, in __getattr__
    if attr is not None: return getattr(attr,k)
                                ^^^^^^^^^^^^^^^
AttributeError: 'CustomDataclass' object has no attribute 'vocab'
</code></pre>
","2024-06-20 20:52:25","1","Answer"
"78646747","","Wrong shape at fully connected layer: mat1 and mat2 shapes cannot be multiplied","<p>I have the following model. It is training well. The shapes of my splits are:</p>
<ul>
<li>X_train (98, 1, 40, 844)</li>
<li>X_val (21, 1, 40, 844)</li>
<li>X_test (21, 1, 40, 844)</li>
</ul>
<p>However, I am getting the following error at <code>x = F.relu(self.fc1(x))</code> in <code>forward</code>. When I attempt to interpret the model on the validation set.</p>
<pre><code># Create a DataLoader for the validation set
valid_dl = learn.dls.test_dl(X_val, y_val)

# Get predictions and interpret them on the validation set
interp = ClassificationInterpretation.from_learner(learn, dl=valid_dl) 
</code></pre>
<p><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x2110 and 67520x128)</code></p>
<p>I have checked dozens of similar questions but I am unable to find a solution. Here is the code.</p>
<pre class=""lang-py prettyprint-override""><code>from fastai.vision.all import *
import librosa
import numpy as np
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torchsummary import summary

[...] #labels in y can be [0,1,2,3]

# Split the data
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Reshape data for CNN input (add channel dimension)
X_train = X_train[:, np.newaxis, :, :]
X_val = X_val[:, np.newaxis, :, :]
X_test = X_test[:, np.newaxis, :, :]

#X_train.shape, X_val.shape, X_test.shape
#((98, 1, 40, 844), (21, 1, 40, 844), (21, 1, 40, 844))

class DraftCNN(nn.Module):
    def __init__(self):
        super(DraftCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        
        # Calculate flattened size based on input dimensions
        with torch.no_grad():
            dummy_input = torch.zeros(1, 1, 40, 844)  # shape of one input sample
            dummy_output = self.pool(self.conv2(self.pool(F.relu(self.conv1(dummy_input)))))
            self.flattened_size = dummy_output.view(dummy_output.size(0), -1).size(1)
        
        self.fc1 = nn.Linear(self.flattened_size, 128)
        self.fc2 = nn.Linear(128, 4)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)  # Flatten the output of convolutions
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


# Initialize the model and the Learner
model = AudioCNN()
learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, Precision(average='macro'),  Recall(average='macro'), F1Score(average='macro')])

# Train the model
learn.fit_one_cycle(8)

print(summary(model, (1, 40, 844)))

# Create a DataLoader for the validation set
valid_dl = learn.dls.test_dl(X_val, y_val)

# Get predictions and interpret them on the validation set
interp = ClassificationInterpretation.from_learner(learn, dl=valid_dl)
interp.plot_confusion_matrix()
interp.plot_top_losses(5)
</code></pre>
<p>I tried changing the forward function and the shapes of the layers but I keep getting the same error.</p>
<p>Edit. Upon request, I have added more code.</p>
","2024-06-20 09:53:10","2","Question"
"78645642","78642379","","<p>Your question is tangled with 2 meanings, the transformer block and Linear block. It could be explained with 2 branches:</p>
<ol>
<li>What are the purposes by having higher, equal, lower out_features in the network?</li>
</ol>
<ul>
<li>In image transformers, qkv(query, key value) is designed to get the correlations of input patches. <code>out_features=2304</code> is 3 multiples of <code>in_features=768</code>. They are divided with 3 elements of qkv, which can be found on <code>foward()</code> function on implements. Hope to explore the reference script from <code>__init__()</code> to <code>forward()</code> and understand <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">transformer paper</a> thoroughly.</li>
<li>In Linear block, building a block with parameters <code>in_features</code> and <code>out_features</code> is heuristic. The number of features in Linear block is dimension of function you hypothesis. We encode the input feature that could be represented in lower-dimension(<a href=""https://medium.com/@reh.yawar2/how-the-bottleneck-layers-in-the-deep-networks-work-and-how-do-those-layers-reduce-computational-7bc99c0d1e96"" rel=""nofollow noreferrer"">the bottleneck of the network</a>), and decode it what we want solve.</li>
</ul>
<ol start=""2"">
<li>Can you provide me some papers about this matter and network architectures having this?</li>
</ol>
<ul>
<li>I hope to read articles of linear regression before learning about deep learning, following fundamental theory of linear algebra. Your learning curve can be followed with: linear regression, classification -&gt; multi layer perceptron(same as <code>Linear</code> in Pytorch) -&gt; convolution block -&gt; transformers -&gt; image transformers(Swin transformer on post).&quot;</li>
</ul>
","2024-06-20 05:35:02","0","Answer"
"78645067","78640689","","<p>Someone has answered to this on another forum as <a href=""https://discuss.pytorch.org/t/unable-to-run-code-on-multiple-gpus-in-pytorch-usage-shows-only-1-gpu-is-being-utilized/204895/2?u=abidmeeraj"" rel=""nofollow noreferrer"">follows</a>, reproducing the same here:</p>
<blockquote>
<p>In your provided code snippets you are using the deprecated nn.DataParallel module first, but are then skipping it by accessing the internal .module.
Call the model directly and let DataParallel handle the data splits as well as model copies before it calls the forward.</p>
</blockquote>
<p>Indeed, I was missing on forward method and there was no need to call <strong>.module.</strong></p>
","2024-06-20 00:24:15","0","Answer"
"78644738","78142012","","<p>ok. That's a problem in Pytorch 2.2.x onnx exporter. There is no such a problem with Pytorch 1.X.</p>
","2024-06-19 21:36:22","0","Answer"
"78643469","78498481","","<p><strong>June 2024 Solution</strong>: Upgrade torch version to 2.3.1 to fix it:</p>
<p><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code></p>
","2024-06-19 15:40:15","4","Answer"
"78642379","","Why sometime the out_features in Linear layer is higher than in_features?","<p>I understand that in the out_features in Linear often lower than the in_features to get more meaningful feature but some time I see the out_features is higher than the in_features, sometimes it's equal.
For example, like the architecture below in swin transformer v2 in pytorch:</p>
<pre><code>Sequential(
      (0): SwinTransformerBlockV2(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=768, out_features=2304, bias=True) #Higher
          (proj): Linear(in_features=768, out_features=768, bias=True) #Equal
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=24, bias=False) # Lower
          )
</code></pre>
<p>I want to ask:</p>
<ol>
<li>What are the purposes by having higher, equal, lower out_features in the network?</li>
<li>Can you provide me some papers about this matter and network architetures having this?</li>
</ol>
<p>I'm just start learning about deep learning and AI, if you can provide some course about building network, it will be great help.</p>
<p>Thank you so much.</p>
","2024-06-19 12:01:34","-1","Question"
"78642270","78640689","","<p>You can find a complete code example to run on multiple GPUs at <a href=""https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter10/nccl_distributed-efficientnet_cifar10.py"" rel=""nofollow noreferrer"">here</a>. Besides adjusting the code to run on multiple GPUs, you need to execute the Python script with a program launcher like torchrun.</p>
","2024-06-19 11:40:54","0","Answer"
"78640909","78638246","","<p>As commented by @talonmies :</p>
<blockquote>
<p>This has nothing to do with “CUDA versions”, which is irrelevant. The problem is clearly described in the error message - the Colab case is using a Tesla T4 (compute capability 7.5) for which the Pytorch build you have includes binary support, whereas the other GPU is a compute capability 8.6 device and there is no binary support in the same Pytorch build. There is nothing to do except get a build of the PyTorch version you want to use with compute 8.6 binary support included, if such a thing exists</p>
</blockquote>
<h1>Plan</h1>
<p>I'm going to find the first/earliest versions of PyTorch having compute capability <code>8.6</code> and then test them to see if the AI model can be run with them...</p>
","2024-06-19 06:49:12","0","Answer"
"78640824","78640755","","<p>Ok after tracking the error log I found:</p>
<pre><code>A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy&lt;2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.
</code></pre>
<p>Apparently numpy released a new version (numpy 2.0.0) just two day ago (2024/06/17) and pip by default installs the new version which is not compatible with pytorch yet as I assume.</p>
<p>Downgrading numpy to the last stable version of numpy v1 using the following command fixed the issue:</p>
<pre><code>pip install numpy==1.26.4
</code></pre>
","2024-06-19 06:25:20","2","Answer"
"78640758","78547075","","<p>This <a href=""https://github.com/huggingface/peft/issues/838#issuecomment-1684522712"" rel=""nofollow noreferrer"">GitHub solution</a> worked for me:</p>
<pre><code>  tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,trust_remote_code=True, token=hf_token)
  tokenizer.add_special_tokens({ &quot;additional_special_tokens&quot;:[AddedToken(&quot;&lt;|move|&gt;&quot;),
                                                              AddedToken(&quot;&lt;|endmove|&gt;&quot;),
                                                              AddedToken(&quot;&lt;|end|&gt;&quot;)]})

  model = AutoModelForCausalLM.from_pretrained(
    &quot;meta-llama/Llama-2-7b-chat-hf&quot;,
    quantization_config=bnb_config,
    device_map=device_map,
    token=hf_token
  )
  model.resize_token_embeddings(len(tokenizer))

  model = PeftModel.from_pretrained(model, local_model_folder)
</code></pre>
","2024-06-19 06:07:35","0","Answer"
"78640755","","RuntimeError: Numpy is not availableL Using numpy with torch","<p>When I run my script that uses torch I get the following error:</p>
<pre><code>  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python312\Lib\site-packages\torchvision\transforms\functional.py&quot;, line 168, in to_tensor
img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Numpy is not available
</code></pre>
<p>I just freshly installed numpy and torch today using pip. What could be the issue?</p>
","2024-06-19 06:07:22","3","Question"
"78640689","","Unable to run code on Multiple GPUs in PyTorch - Usage shows only 1 GPU is being utilized","<p>I am training a Transformer Encoder-Decoder based model for Text summarization. The code works without any errors but uses only 1 GPU when checked with nvidia-smi. However, I want to run it on all the available GPUs (I can access as many as I want). I wrapped my model in Dataparallel.
<strong>Here’s how I have wrapped the model:</strong></p>
<pre><code>if torch.cuda.device_count() &gt; 1:
        print(f&quot;Using {torch.cuda.device_count()} GPUs&quot;)
        model = nn.DataParallel(model)

    model.to(device)
</code></pre>
<p><strong>This is how I am calling the functions from my model:</strong></p>
<pre><code>if torch.cuda.device_count() &gt; 1:
                encoder_output = model.module.encode(
                    encoder_input, encoder_mask
                )  # (B, input_len, d_model)
                decoder_output = model.module.decode(
                    encoder_output, encoder_mask, decoder_input, decoder_mask
                )  # (B, seq_len, d_model)
                proj_output = model.module.project(
                    decoder_output
                )
</code></pre>
<p><strong>I am using Python 3.12 and Torch 2.3.0.</strong>
MWE is available on <a href=""https://github.com/abidmeeraj/MWE-problem-with-MultipleGPUs"" rel=""nofollow noreferrer"">GitHub</a>.</p>
<p>I have also checked if GPUs are configured correctly. Tried this example from <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noreferrer"">PyTorch Documentation</a> to test my GPUs configuration and working.</p>
","2024-06-19 05:45:12","0","Question"
"78639515","78639017","","<p>I was able to achieve the desired effect with some reshaping:</p>
<pre class=""lang-py prettyprint-override""><code>source = torch.randint(0, 10, (2, 3, 2))
index = torch.randint(0, 3, (2, 3, 3))
index = index.flatten(-2)
index = index[:, :, None].expand(-1, -1, 2)
out = source.gather(1, index).view(2, 3, 3, 2)
</code></pre>
","2024-06-18 20:25:10","0","Answer"
"78639017","","batched PyTorch tensor indexing","<p>I have a source tensor of size (3, 2) and an index tensor of size (3, 3) containing integer values 0, 1, or 2. In pytorch I can do tensor indexing <code>source[index]</code> to get a tensor of size (3, 3, 2). Example:</p>
<pre><code>source: 
tensor([[1, 6],
        [2, 3],
        [8, 0]])

index: 
tensor([[2, 1, 2],
        [1, 1, 2],
        [2, 0, 0]])

source[index]: 
tensor([[[8, 0],
         [2, 3],
         [8, 0]],

        [[2, 3],
         [2, 3],
         [8, 0]],

        [[8, 0],
         [1, 6],
         [1, 6]]])
</code></pre>
<p>I want to do the above operation but batched.<br />
For example with a batch size of 2:<br />
source shape --&gt; (2, 3, 2)<br />
index shape --&gt; (2, 3, 3)<br />
batched <code>source[index]</code> shape --&gt; (2, 3, 3, 2)<br />
I can easily do this with a loop but I want to know if it can be done efficiently with torch.gather or some other built-in?</p>
","2024-06-18 18:11:55","0","Question"
"78638246","","PyTorch problem with a specific version of CUDA","<h1>Background</h1>
<p>I need to test this AI model on the following CUDA server:</p>
<p><a href=""https://github.com/sicxu/Deep3DFaceRecon_pytorch"" rel=""nofollow noreferrer"">https://github.com/sicxu/Deep3DFaceRecon_pytorch</a></p>
<pre class=""lang-bash prettyprint-override""><code>$ nvidia-smi 
Tue Jun 18 18:28:37 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3060        On  | 00000000:41:00.0 Off |                  N/A |
|  0%   40C    P8              13W / 170W |     39MiB / 12288MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      1078      G   /usr/lib/xorg/Xorg                           16MiB |
|    0   N/A  N/A      1407      G   /usr/bin/gnome-shell                          3MiB |
+---------------------------------------------------------------------------------------+
</code></pre>
<p>But I'm receiving this warning while testing:</p>
<blockquote>
<p>/home/arisa/.conda/envs/deep3d_pytorch/lib/python3.6/site-packages/torch/cuda/init.py:125: UserWarning:
NVIDIA GeForce RTX 3060 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.
If you want to use the NVIDIA GeForce RTX 3060 GPU with PyTorch, please check the instructions at <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a></p>
</blockquote>
<p>I receive this error after the above warning:</p>
<blockquote>
<p>RuntimeError: CUDA error: no kernel image is available for execution on the device</p>
</blockquote>
<h1>Note: CUDA <code>12.2</code> vs <code>12.3</code></h1>
<p>I was able to test the same AI model on Google Colab with CUDA <code>12.2</code> without any problem. I'm not sure why the server with CUDA <code>12.3</code> is a trouble maker.</p>
<p><a href=""https://i.sstatic.net/1967iip3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1967iip3.png"" alt=""Google Colab screenshot of CUDA version"" /></a></p>
<h1>Why?</h1>
<p>Why CUDA <code>12.2</code> works fine but <code>12.3</code> throws warnings and errors?</p>
<h1>Building from source</h1>
<p>So, I just thought I would build PyTorch <code>1.6.0</code> - required by the AI model - with CUDA <code>12.3</code>. I just wanted to ask about the possibility of building from source.
I just want to know if it's <strong>possible</strong> to build PyTorch <code>1.6.0</code> along with CUDA <code>12.3</code> without patching the source code:</p>
<p><a href=""https://github.com/pytorch/pytorch/releases/tag/v1.6.0"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/releases/tag/v1.6.0</a></p>
","2024-06-18 15:03:12","-2","Question"
"78638174","78638155","","<p>I think the problem is that the activation function does not have any parameter that is serialized, so you will not find it in the <code>pth</code> file. Usually in pytorch you need the model you will need the model definition to get everything. One workaround might be to use a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"" rel=""nofollow noreferrer"">Sequential</a>. I guess that should store the activation functions too.
Here's an example on how to save/reload</p>
<pre class=""lang-py prettyprint-override""><code># To save
torch.save(model.state_dict(), 'out.pth')
# To reload
model = SimpleCNN_2()
model3.load_state_dict(torch.load('out.pth'))
</code></pre>
","2024-06-18 14:47:24","0","Answer"
"78638155","","getting Pytorch activation function from .pth model","<p>I am looking for a method to retrieve the activation functions used in a PyTorch network saved as a .pth file (<code>torch.save(model)</code>). Indeed, if the activation functions were not declared in the class when creating the model but only in the forward method, I am unable to identify these activation functions
eg:</p>
<pre><code>
class SimpleCNN_2(nn.Module):
    def __init__(self):
        super(SimpleCNN_2, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(in_features=32768, out_features=128)  
        self.fc2 = nn.Linear(in_features=128, out_features=10)  

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = nn.Flatten()(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
</code></pre>
<p>I get this description:</p>
<pre><code>SimpleCNN_2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (fc1): Linear(in_features=32768, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=10, bias=True)
)
</code></pre>
<p>If activation function are declared, I don't have any issue. My concern is about the fact that I want to assess a network from a .pth file, and if I can't get the whole structure of the network it's a mess.</p>
<p>I would like to have a method that allows me to obtain the complete structure of a network, layer by layer, from a .pth file, in the form of a list, without knowing the structure of the network in advance.</p>
","2024-06-18 14:43:16","0","Question"
"78637482","78637081","","<p>why not give Sweeps a try: <a href=""https://docs.wandb.ai/guides/sweeps"" rel=""nofollow noreferrer"">https://docs.wandb.ai/guides/sweeps</a></p>
<p>Your code would look something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import wandb

# Define sweep configuration
sweep_config = {
    'method': 'grid',  # or 'random' or 'bayes'
    'parameters': {
        'learning_rate': {
            'values': [0.001, 0.01, 0.1]  # replace with actual hyperparameters
        },
        'batch_size': {
            'values': [16, 32, 64]
        },
        'hidden_size': {
            'values': [128, 256, 512]
        },
        # Add all other hyperparameters explicitly
    }
}

# Initialize sweep
sweep_id = wandb.sweep(sweep_config, project='D2T')

# Define training function
def train(config=None):
    with wandb.init(config=config):
        config = wandb.config

        # Explicitly create model configuration
        model_config = {
            'learning_rate': config.learning_rate,
            'batch_size': config.batch_size,
            'hidden_size': config.hidden_size,
            # Add all other hyperparameters explicitly
        }

        # Initialize the model with explicit parameters
        pointernet = PointerGenerator(
            device=self.device,
            learning_rate=model_config['learning_rate'],
            hidden_size=model_config['hidden_size'],
            # Include all necessary model parameters
        ).to(self.device)

        trainer = Trainer(
            training_arguments=self.training_config,
            model=pointernet,
            criterion=Criterion(),
            tokenizer=self.tokenizer,
            wandb=wandb.run
        )

        trainer.fit(train_dataloader, dev_dataloader)

# Execute the sweep agent
wandb.agent(sweep_id, function=train, count=NUMBER_OF_SWEEP_RUNS)
</code></pre>
","2024-06-18 12:36:35","0","Answer"
"78637150","78637081","","<p>Turn out, I need to call <code>finish</code> at the end of the running:</p>
<pre><code>trainer.fit(train_dataloader, dev_dataloader)
run.finish()
</code></pre>
","2024-06-18 11:32:14","2","Answer"
"78637081","","how to run multiple wandb run for hyperparameter tunning in for loop","<p>I'm doing hyperparameter tuning. My code look like this:</p>
<pre><code>combinations = list(itertools.product(*self.grid_model_configs.values()))

        for combination in combinations:
            param_names = self.grid_model_configs.keys()
            model_config = {key: value for key, value in zip(param_names, combination)}

            wandb.login()
            run = wandb.init(
                name=repr(model_config).replace(&quot;'&quot;, &quot;&quot;).replace('{', '').replace('}', ''),
                project='D2T',
                config={
                    'training_config': self.training_config,
                    'model_config': model_config
                }
            )

            filtered_param = {k: v for k, v in model_config.items() if k in
                              [p.name for p in inspect.signature(PointerGenerator).parameters.values()]}

            pointernet = PointerGenerator(device=self.device, **filtered_param).to(self.device)

            trainer = Trainer(training_arguments=self.training_config,
                              model=pointernet,
                              criterion=Criterion(),
                              tokenizer=self.tokenizer,
                              wandb=run)
            trainer.fit(train_dataloader, dev_dataloader)

</code></pre>
<p>But In wandb, it only shows one chart for multiple combination.</p>
<p>It worked in a Jupyter notebook, but when I run by command line it doesn't work anymore.</p>
","2024-06-18 11:19:44","0","Question"
"78635123","78239906","","<p>Fundamentally you are looking at converting the matrix multiplication :</p>
<p>$$Y = WX+b$$</p>
<p>De quantization is carried out according to this formula :
Where $S_i, Z_i$ are the scale and zero point of the $X$ (activations in your case) and bias, $b$.</p>
<p>$$Y = S_xS_w(X_q-Z_x)(W_q-Z_w) + S_b(b_q-Z_b)$$</p>
<p>I am not sure your quantization formula looks correct since it does not involve a scale and zero point. But if you give me the formula used to quantize, then I can probably revise it for you. I go into painful detail about this in my blog and it would be worth your while to try to recreate it with your quantization formula :</p>
<p><a href=""https://franciscormendes.github.io/2024/05/16/quantization-layer-details/"" rel=""nofollow noreferrer"">https://franciscormendes.github.io/2024/05/16/quantization-layer-details/</a></p>
","2024-06-18 00:56:44","1","Answer"
"78629089","78628960","","<p>This error originated from <code>matplotlib version 3.9.0</code>.
Try to install older version which might solve your issue:</p>
<pre><code>pip install matplotlib==3.7.3
</code></pre>
","2024-06-16 12:12:15","6","Answer"
"78628960","","module 'matplotlib.cm' has no attribute 'get_cmap'","<p>I'm currently using matplotlib 3.9.0.</p>
<p>I was able to run it before, so I suspect there might be an issue with my environment. I'm not sure how to resolve it.</p>
<p>Here are the details of the error:</p>
<pre><code>File &quot;C:\Users\Desktop\PyTorch_OpenPose_ori\pytorch_advanced-master\4_pose_estimation\inference.py&quot;, line 11, in &lt;module&gt;
    from utils.openpose_net import OpenPoseNet
File &quot;C:\Users\Desktop\PyTorch_OpenPose_ori\pytorch_advanced-master\4_pose_estimation\utils\__init__.py&quot;, line 4, in &lt;module&gt;
    from .decode_pose import *
File &quot;C:\Users\Desktop\PyTorch_OpenPose_ori\pytorch_advanced-master\4_pose_estimation\utils\decode_pose.py&quot;, line 18, in &lt;module&gt;
    cmap = matplotlib.cm.get_cmap('hsv')
AttributeError: module 'matplotlib.cm' has no attribute 'get_cmap'

</code></pre>
<p>Could anyone help me figure out how to solve this issue?</p>
<p>I was trying to load weights and perform inference with OpenPose.</p>
","2024-06-16 11:20:54","8","Question"
"78625593","78625587","","<p>Have you tried <code>np.float32</code>?</p>
<p>More info: <a href=""https://numpy.org/doc/stable/user/basics.types.html#:%7E:text=x%20%3D%20np.float32(1.0)%0A%3E%3E%3E"" rel=""nofollow noreferrer"">https://numpy.org/doc/stable/user/basics.types.html#:~:text=x%20%3D%20np.float32(1.0)%0A%3E%3E%3E</a></p>
","2024-06-15 04:07:39","0","Answer"
"78625591","78625587","","<p><strong>Code</strong></p>
<p>Check out the announcements and make sure to ask questions with <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">minimal and reproducible example</a>. It's harder to get answers if we have to read your long code without a reproducible sample.</p>
<p>minimal and reproducible example:</p>
<pre><code>import pandas as pd
df = pd.DataFrame([[1.1, 2.1], [3.1, 4.1]], columns=['col1', 'col2'])
</code></pre>
<p>df:</p>
<pre><code>   col1  col2
0   1.1   2.1
1   3.1   4.1
</code></pre>
<hr />
<p>chk dtype of <code>df</code></p>
<pre><code>print(df.dtypes)
</code></pre>
<p>print:</p>
<pre><code>col1    float64
col2    float64
dtype: object
</code></pre>
<hr />
<p>convert to <code>float32</code></p>
<pre><code>out = df.astype('float32')
print(out.dtypes)
</code></pre>
<p>orint:</p>
<pre><code>col1    float32
col2    float32
dtype: object
</code></pre>
<p>Most functions in Pandas do not change the original as a result of applying the function.</p>
","2024-06-15 04:07:14","0","Answer"
"78625587","","Fail to convert float 64 to float 32 in python","<p>I tried to convert the data from data type float 64 to float 32. I used both pandas method and pytorch but no avail. Dataset are from kaggle titanic project. The code is the following:</p>
<pre><code>from torch import nn
import torch
import pandas as pd
import numpy as np
df = pd.DataFrame([[1.1, 2.1], [3.1, 4.1],[5.1,6.1]], columns=['col1', 'col2'])
df.astype('float32')
print(df.dtypes)
df.astype(np.float32)
print(df.dtypes)
a=torch.tensor(df.loc[1][['col1']])
a.to(torch.float32)
print(a)
</code></pre>
<p>and get the result</p>
<pre><code>col1    float64
col2    float64
dtype: object
col1    float64
col2    float64
dtype: object
tensor([3.1000], dtype=torch.float64)
</code></pre>
<p>I tried to use both method provided by pandas and pytorch. Although it does not raise any errors, it does not work. And I cannot put it into neural network layers as it requires float 32 data type. I also tried to convert them to int, int64, int32 using both method. But nothing works.</p>
","2024-06-15 03:59:39","0","Question"
"78625475","","Understanding usage of HiFi-GAN by Vits","<p>I'm (trying to) learn AI/ML for speech synthesis and trying to undestand how HiFi-GAN is used by Vits.</p>
<p>From my understanding, <a href=""https://github.com/jaywalnut310/vits"" rel=""nofollow noreferrer"">Vits</a> will convert text input into mel spectograms which is then converted to audio waves by <a href=""https://github.com/jik876/hifi-gan"" rel=""nofollow noreferrer"">HiFi-GAN</a>.</p>
<p>What confuses me is why the input sent from Vits to HiFi-GAN is not a mel spectogram.</p>
<p>For example, when I test <em><strong>other</strong></em> models and add the code below to the forward method from HiFi-GAN:</p>
<pre><code>class Generator(torch.nn.Module):
  ...
  def forward(self, x):
    plot_spectrogram(x[0].cpu().detach().numpy(), &quot;mel_spec.png&quot;)
    ...
  ...
</code></pre>
<p>it saves the correct image which looks like a mel spectogram image, however, when I do the same with vits, the saved image is a plain green image which of course is not a representation of a mel spectogram.</p>
<p>But the resulting audio file is of course a valida audio file.</p>
<p>So could anyone explain that to me?</p>
<p>I'm evaluating a few neural tts models and what I wanted to do is save the mel spectogram created by the models to compare them later and also run them through different vocoders to compare them as well.</p>
<p>I noticed that the HiFi-GAN code in the vits repo is slightly different from the original repo but I can't undertand why.</p>
<p>Is there any way I can convert the input param <code>x</code> to the mel spectogram representation without first converting it to audio and then convert the audio to mel?</p>
","2024-06-15 02:17:36","0","Question"
"78623528","78623169","","<p>The problem seems to be in numerical solution of <code>solve_burgers()</code>, which you need to carefully verify:</p>
<pre><code>import torch
import numpy as np
from scipy.integrate import quad


def solve_burgers(X, t, nu):
    f = lambda y: np.exp(-np.cos(np.pi * y) / (2 * np.pi * nu))
    g = lambda y: np.exp(-(y ** 2) / (4 * nu * t))
    numer = lambda eta: torch.sin(torch.pi * (X - eta)) * f(X - eta) * g(eta)
    denom = lambda eta: f(X - eta) * g(eta)

    uxt, _ = quad(lambda eta: -numer(eta).item(), -np.inf, np.inf)
    uxt3, _ = quad(lambda eta: denom(eta).item(), -np.inf, np.inf)

    return -uxt / uxt3


def mr(x2):
    u_xr = []
    for k in range(len(x2)):
        z01 = solve_burgers(x2[k], 0.25, 0.01 / np.pi)
        u_xr.append(z01)
    return u_xr


x2 = torch.tensor([10.0, 20.0, 30.0, 10.0, 20.0, 30.0])
print(mr(x2))

</code></pre>
<h3>Prints</h3>
<pre><code>[4.01485673240222e-06, 8.861857759474245e-06, 1.0083838264149217e-05, 4.01485673240222e-06, 8.861857759474245e-06, 1.0083838264149217e-05]
</code></pre>
<h3>Note</h3>
<ul>
<li>Make sure your equations are correct.</li>
</ul>
","2024-06-14 14:23:59","1","Answer"
"78623169","","Error in automatic derivative calculation with pytorch","<pre><code>x2=torch.tensor(x,requires_grad=True)
t2=torch.tensor(t,requires_grad=True)
def mr():
            for k in range(n):
                z01=solve_burgers(torch.tensor([x2[k]]),0.25,0.01/np.pi)[0]
                z01.backward(retain_graph=True)
                u_xr=x2.grad[k]
                u_tr=t2.grad[k]
            return u_xr,u_tr,u_xxr
</code></pre>
<p>solve_burgers is a function to solve burger equation.</p>
<p>When I run this code to calculate derivatives, I get the following error:</p>
<pre><code>TypeError: 'NoneType' object is not subscriptable
</code></pre>
<p>How can I fix the error?</p>
<pre><code>def solve_burgers(X, t, nu):
def f(y):
 return np.exp(-np.cos(np.pi * y) / (2 * np.pi * nu))
def g(y):
 return np.exp(-(y**2) / (4 * nu * t))
def fun(eta):
  return torch.sin(torch.pi * (x - eta)) * f(x - eta) * g(eta)
def fun1(eta):
  return f(x - eta) * g(eta)
U = torch.zeros_like(X)
for i in range(len(X)):
 x = X[[[i]]]
 uxt = -quad(fun, -torch.inf, torch.inf)[0]
 uxt2=torch.tensor(uxt,requires_grad=True)
 uxt30=quad(fun1, -np.inf, np.inf)[0]
 uxt3=torch.tensor(uxt30,requires_grad=True)
 U[i] = uxt2 / uxt3
return U
</code></pre>
","2024-06-14 13:12:45","1","Question"
"78621182","78621148","","<p>Was stuck on this for hours, but of course as soon as I post this I find the solution myself.</p>
<p>It was really simple:
instead of using <code>LiteModuleLoader.load()</code> use <code>AssetManager mgr = this.getApplicationContext().getResources().getAssets(); LiteModuleLoader.loadModuleFromAsset(mgr,'yolov8n.ptl');</code></p>
","2024-06-14 05:29:06","2","Answer"
"78621148","","Getting the path of the assets folder in Android","<p>Hi I'm very new to Android development.</p>
<p>I'm attempt to pass my pytorch_lite model through <code>LiteModuleLoader.load(modelPath)</code>, I have my model in my app's assets folder. I'm attempting to get the full path of my model in that folder to pass to the method.</p>
<p>I tried using AssetManager but I couldn't find a way to get the asset folder's path.</p>
<p>It seems like there should be a simple solution that i'm missing.</p>
","2024-06-14 05:17:32","-2","Question"
"78620402","","Dataloader error in pytorch PascalVOC dataset: RuntimeError: each element in list of batch should be of equal size","<p>When trying to implement the yolo algortithm, one of the steps is to resize each image to (448, 448). But even with the transform applied the Dataloader throw an exception about difference of size in the dataset.</p>
<p>The exact error message: &quot;RuntimeError: each element in list of batch should be of equal size&quot;</p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import datasets
from torchvision.transforms import v2, ToTensor

from torch.utils.data import DataLoader

validation_data = datasets.voc.VOCDetection(
    root='.DATA/',
    download=False,
    image_set=&quot;val&quot;,
    transform=v2.Compose([ v2.Resize(size=(448, 448)), ToTensor() ])
)

batch_size = 64

validation_dataloader = DataLoader(validation_data, batch_size=batch_size)

for X, y in validation_dataloader:
    print(f&quot;Shape of X [N, C, H, W]: {X.shape}&quot;)
    break
</code></pre>
","2024-06-13 22:43:49","0","Question"
"78619960","78615152","","<p>The problem can be solved by using this context manager that will automatically apply copy on write to avoid this type of error <a href=""https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.allow_mutation_on_saved_tensors"" rel=""nofollow noreferrer"">Automatic differentiation package - torch.autograd — PyTorch 2.3 documentation</a></p>
<p>This question was first answered <a href=""https://discuss.pytorch.org/t/how-to-calculate-loss-over-a-sliding-window-of-samples-and-then-backpropagate-the-weighted-average-loss/204604"" rel=""nofollow noreferrer"">here</a></p>
","2024-06-13 20:05:46","0","Answer"
"78616871","78616686","","<p>Take a look at the traceback you are getting. To have a reproducible example, I reduced your code to this:</p>
<pre><code>

import os

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader




# CNN Model
class CNNModel(nn.Module):
    def __init__(self, num_classes):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv3d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64*30*115*115, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool3d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool3d(x, 2)
        x = x.view(-1, 64*30*115*115)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNModel(num_classes=20).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_function = nn.CrossEntropyLoss()
</code></pre>
<p>I get the following traceback:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[1], line 33
     31 # Training Loop
     32 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
---&gt; 33 model = CNNModel(num_classes=20).to(device)
     34 optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
     35 loss_function = nn.CrossEntropyLoss()

File ~\AppData\Local\mambaforge\envs\clustering\Lib\site-packages\torch\nn\modules\module.py:1145, in Module.to(self, *args, **kwargs)
   1141         return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
   1142                     non_blocking, memory_format=convert_to_format)
   1143     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
-&gt; 1145 return self._apply(convert)

File ~\AppData\Local\mambaforge\envs\clustering\Lib\site-packages\torch\nn\modules\module.py:797, in Module._apply(self, fn)
    795 def _apply(self, fn):
    796     for module in self.children():
--&gt; 797         module._apply(fn)
    799     def compute_should_use_set_data(tensor, tensor_applied):
    800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
    801             # If the new tensor has compatible tensor type as the existing tensor,
    802             # the current behavior is to change the tensor in-place using `.data =`,
   (...)
    807             # global flag to let the user control whether they want the future
    808             # behavior of overwriting the existing tensor or not.

File ~\AppData\Local\mambaforge\envs\clustering\Lib\site-packages\torch\nn\modules\module.py:820, in Module._apply(self, fn)
    816 # Tensors stored in modules are graph leaves, and we don't want to
    817 # track autograd history of `param_applied`, so we have to use
    818 # `with torch.no_grad():`
    819 with torch.no_grad():
--&gt; 820     param_applied = fn(param)
    821 should_use_set_data = compute_should_use_set_data(param, param_applied)
    822 if should_use_set_data:

File ~\AppData\Local\mambaforge\envs\clustering\Lib\site-packages\torch\nn\modules\module.py:1143, in Module.to.&lt;locals&gt;.convert(t)
   1140 if convert_to_format is not None and t.dim() in (4, 5):
   1141     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
   1142                 non_blocking, memory_format=convert_to_format)
-&gt; 1143 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)

RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<p>which states that the issue is in <code>model = CNNModel(num_classes=20).to(device)</code>, so it is already your model that is too large for your GPU, it has nothing to do with training.</p>
<pre><code>nn.Linear(64*30*115*115, 512)
</code></pre>
<p>will take up a huge amount of memory, as the weight matrix will be <code>64*30*115*115*512</code> entries large. At 4 bytes per entry, this gives roughly 48 GB of GPU memory required.</p>
<p>So you will need to rethink your model.</p>
","2024-06-13 09:01:40","1","Answer"
"78616753","78615860","","<p>I am not fully sure if I unsterdood what you are trying to do.</p>
<p>However, if you want to do sampling from a discrete probability in a differentiable way, you probably need to use the gumbel-softmax trick. It allows for sampling from a categorical distribution during the forward pass through a neural network  :</p>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.gumbel_softmax.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.functional.gumbel_softmax.html</a></p>
<p><a href=""https://sassafras13.github.io/GumbelSoftmax/"" rel=""nofollow noreferrer"">https://sassafras13.github.io/GumbelSoftmax/</a></p>
","2024-06-13 08:39:00","1","Answer"
"78616686","","CUDA Out of Memory on a Reinforcement Learning algorithm","<pre><code>import os
import gym
from gym import spaces
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from tqdm import tqdm
from TTset_main import load_TTset

# Load the dataset
filted_tset_path = &quot;training_path.csv&quot;
filted_vset_path = &quot;validation_path.csv&quot;
TTset_training, TTset_vali, training_data_length = load_TTset(filted_tset_path, filted_vset_path)

# Custom Dataset Class
class VideoDataset(Dataset):
    def __init__(self, data_list):
        self.data_list = data_list
        self.length = sum(len(video[0]) for video in data_list)  # Total number of frames

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        video_idx = 0
        frame_idx = idx
        # Find the corresponding video and frame
        for video, labels in self.data_list:
            if frame_idx &lt; len(video):
                frame = video[frame_idx]
                label = labels[frame_idx]
                return frame, label
            frame_idx -= len(video)
        raise IndexError(&quot;Index out of range&quot;)

# Create DataLoader
batch_size = 1  # One frame at a time
video_dataset = VideoDataset(TTset_training)
data_loader = DataLoader(video_dataset, batch_size=batch_size, shuffle=True)

# CNN Model
class CNNModel(nn.Module):
    def __init__(self, num_classes):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv3d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64*30*115*115, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool3d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool3d(x, 2)
        x = x.view(-1, 64*30*115*115)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Training Loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNModel(num_classes=20).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_function = nn.CrossEntropyLoss()

# Training with progress bar
total_epochs = 10
for epoch in range(total_epochs):
    training_acu_count = 0  # Reset the accuracy count for each epoch
    for i, (x, y) in tqdm(enumerate(data_loader)):
        model.train()
        x = x.to(device).unsqueeze(0)  # Add batch dimension
        y = y.to(device)
        pred = model(x)
        t_loss = loss_function(pred, y.unsqueeze(0))  # Match dimensions for loss calculation
        t_loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        pred_convert = torch.argmax(pred, 1)
        training_acu_count += (pred_convert == y).sum().item()

    print(f&quot;Epoch {epoch+1}/{total_epochs}, Training Accuracy: {training_acu_count/len(video_dataset):.4f}&quot;)

# Save the model
torch.save(model.state_dict(), &quot;video_classification_model.pth&quot;)

</code></pre>
<p>I'm using the above code to train a RL model that classifies videos. The TTset_training is a dataloader object that contains inputs and targets. Input dimensions are 129x15x3x60x230x230, and target dimensions are 129x15. The video frames are tensors of dimensions 3x60x230x230.
I'm running this code on a 4070Ti, and I'm getting the following error:</p>
<pre><code>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.43 GiB. GPU
</code></pre>
<p>I've tried multiple tweaks, changing batch size, using a loop to input data individually, but no luck. I've searched online for solutions but all of them recommend changing the size of the frames, which is not possible as I would lose a lot of data.</p>
","2024-06-13 08:27:33","1","Question"
"78615860","","Torch.unique() alternatives that do not break gradient flow?","<p>In a Pytorch gradient descent algorithm, the function</p>
<pre><code>def TShentropy(wf):
    unique_elements, counts = wf.unique(return_counts=True)
    entrsum = 0
    for x in counts:
        p = x/len_a #Calculates probability of x
        entrsum-= p*torch.log2(p) #Shannon's Entropy Formula        
    return entrsum
</code></pre>
<p>uses the method <code>torch.unique()</code> which is breaking the gradient flow. Whenever I switch it to a continuous probability calculation such as <code>torch.softmax()</code> the program runs. However, the formula needs to use a discrete probability mass distribution, which does not work with softmax.</p>
<p>I have tried using <code>torch.nn.functional.one_hot</code> and <code>torch.bincount</code>, both of which gave the same error:</p>
<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>Is this doomed to fail? Should I try to interpolate the probability function somehow?</p>
","2024-06-13 04:48:39","2","Question"
"78615152","","How to calculate loss over a sliding window of samples and then backpropagate the weighted average loss","<p>I am trying to implement a learning technique from a paper. The relevant portion is: The SNN baseline used a sliding window of 50 consecutive data points, representing 200 ms of data (50-point window, single-point stride) in order to calculate the loss, to allow for more information for backpropagation and avoid dead neurons and vanishing gradients. The MSE loss was linearly weighted from 0 to 1 for the 50 points within the window. However, I am getting issues with the gradient calculation.</p>
<p>My attempt at implementing this is below: `</p>
<pre><code>for epoch in range(100):
    net.train()
    
    best_loss = float('inf')
    patience_counter = 0
    best_state = torch.save(net.state_dict(), &quot;cur_state.pth&quot;)
    
    loss_window = deque(maxlen=50)
    window_sum = 0.0
    
    for i, data in enumerate(train_set_loader):
        inputs, label = data
        optimizer.zero_grad()
        outputs, _ = net(inputs)
        
        loss = criterion(outputs, label)
        
        loss_window.append(loss)
        
        window_sum = window_sum + loss.item()

        # # If the window is full, calculate the weighted loss and perform backpropagation
        if len(loss_window) == 50:
            weighted_loss = sum(((j + 1) / 50) * loss_value for j, loss_value in  enumerate(loss_window))
       
            print(f'weighted_loss: {weighted_loss}, requires_grad: {weighted_loss.requires_grad}')
            
            weighted_loss.backward(retain_graph=True)
            #weighted_loss.backward()
            optimizer.step()
            
            #Print gradients for debugging
            for name, param in net.named_parameters():
                if param.grad is not None:
                    print(f'{name}: {param.grad.norm()}')
            
            
            # Update the window sum by subtracting the oldest loss
            window_sum -= loss_window[0].item()

            #Remove oldest loss element
            loss_window.popleft()
  
        


    net.eval()
    val_loss = evaluate_model(net, criterion, val_set_loader)
    
    
    if val_loss &lt; best_loss:
        best_loss = val_loss
        best_state = torch.save(net.state_dict(), &quot;cur_state.pth&quot;)
        patience_counter = 0
    else:
        patience_counter += 1
    
    if patience_counter &gt;= patience:
        print(&quot;Early Stopping&quot;)
        print(f&quot;Best loss: {best_loss}&quot;)
        device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
        net.load_state_dict(torch.load(&quot;cur_state.pth&quot;, map_location=device))
        print(&quot;Model state returned to best performing.&quot;)
        break
</code></pre>
<p>If I try without the retain_graph=True line in my backwards pass, I get the error that I'm trying to backward through the graph a second time. This makes sense as it is trying to backwards pass 49 of the same 50 gradients from the first call. When I do have retain_graph=True, I get the error: &quot;one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [50, 2]]&quot;. I believe this is an issue with the updating of loss_window, but I am not sure how to go about changing this. My question is somewhat similar to <a href=""https://stackoverflow.com/questions/47120126/how-to-calculate-loss-over-a-number-of-images-and-then-back-propagate-the-averag"">how to calculate loss over a number of images and then back propagate the average loss and update network weight</a> but in that question he doesn't have overlap in his window which I believe avoids the problem.</p>
","2024-06-12 22:15:36","1","Question"
"78611907","78611808","","<p>it seems like you did not use <code>lora_config</code> to generate a <code>PeftModel</code>, and if you do not want to retrain the whole model, you need change the <code>num_labels</code> parameter and either</p>
<ol>
<li>use a <code>PeftModel</code> instead of the BertForSequenceClassification,</li>
<li>or freeze the base bert model and only train the classifier by,</li>
</ol>
","2024-06-12 09:56:00","0","Answer"
"78611808","","Efficient Methods for Updating a BERT Sequence Classification Model with New Classes?","<p>I have a problem finding an effective method to update the classifier layer of my text classification model to include new classes. I am working on a classification task involving brand names based on their descriptions, with two columns: 'description' and 'brand.' After the initial training, I need a way to update the model to classify new classes without retraining the entire model, to save resources.</p>
<p>I have attempted to use a BERT model for sequence classification with PEFT (Parameter-Efficient Fine-Tuning), but I am stuck and unsure if this is the best approach. Specifically, I am having trouble adding new classes to an already trained model.</p>
<p>Here is the code I am currently using:</p>
<pre><code>from transformers import AutoModelForSequenceClassification, AdamW, get_scheduler
from peft import LoraConfig
import torch
from torch import nn
from tqdm import tqdm
import numpy as np

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(custom_dataset2.labels)))

lora_config = LoraConfig(
    task_type=peft.TaskType.SEQ_CLS, # Type of task: sequence classification
    r=4,                             # Rank of the low-rank adaptation matrices
    lora_alpha=32,                   # Scaling factor for low-rank matrices
    lora_dropout=0.1,                # Dropout probability for low-rank matrices
)

optimizer = AdamW(model.parameters(), lr=0.00001)
num_epochs = 30
total_steps = num_epochs * len(data_loader2)
lr_scheduler = get_scheduler(
    &quot;linear&quot;,
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
model.to(device)

model.train()

progress_bar = tqdm(range(total_steps))
for epoch in range(num_epochs):
    losses = []
    correct_predictions = 0
    for i, batch in enumerate(data_loader2):
        input_ids = batch[&quot;input_ids&quot;].to(device)
        attention_mask = batch[&quot;attention_mask&quot;].to(device)
        labels = batch[&quot;labels&quot;].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

        logits = outputs.logits
        predictions = logits.argmax(dim=1)
        loss = outputs.loss
        losses.append(loss.item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        progress_bar.update(1)
        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')

    correct_predictions += torch.sum(predictions == labels)
    train_acc = correct_predictions.double() / len(data_loader1.dataset)
    train_loss = np.mean(losses)
    print(f&quot;Epoch {epoch+1}/{num_epochs}, Train accuracy: {train_acc:.4f}, Train loss: {train_loss:.4f}&quot;)

# This part of the code is where I need help:
model.classifier = nn.Linear(model.bert.config.hidden_size, 100)
</code></pre>
<p>When I attempt to increase the number of classes and train the adapter again instead of retraining the model, I always encounter the following error:</p>
<pre><code>RuntimeError: shape '[-1, 43]' is invalid for input of size 660
</code></pre>
<p>Is there any way to add more classes to the pretrained model?</p>
","2024-06-12 09:36:13","0","Question"
"78609334","78576364","","<p>I figured out the reason. I simply didn't add pre processing to use embedings of my words intead of tokenized words</p>
","2024-06-11 19:01:14","0","Answer"
"78607238","78604018","","<p>I don't know the exact problem, but it seems this problem happened when I used two directories for Python &quot;lib&quot;: one was the default Anaconda lib, and I had another separate one. The problem disappeared when I used only the default Anaconda lib.
It works fine now.</p>
","2024-06-11 11:35:53","0","Answer"
"78606253","78604018","","<p>This seems to be a problem with the latest version of setuptools. Similar reportings have been done in github <a href=""https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/15863#issuecomment-2125026282"" rel=""noreferrer"">issues</a> and the solution that worked for me was to specify the setuptools version to 69.5.1, so for example:</p>
<pre><code>pip install setuptools==69.5.1
</code></pre>
<p>In your case, you should run this command before trying to install causal_conv1d.</p>
<p>I had the same issue in a docker container and similarly, I added a RUN command to install that particular version of setuptools.</p>
","2024-06-11 08:27:52","24","Answer"
"78606054","","torchvision::nms does not exist","<p>Im trying to run the llama3 models using nf4 data format on cpu using intel extension . I download the pytorch from <a href=""https://github.com/pytorch/pytorch"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch</a> and use the following command to compile: <code>DEBUG=1 python ./setup.py develop</code>. The version is the same as before and other dependencies don't make a difference.</p>
<pre><code>python run_generation_cpu_woq.py --model llama_3_8b_nf4 --woq --woq_algo GPTQ --benchmark --tasks lambada_openai,piqa,hellaswag --batch_size 4 --weight_dtype nf4 --load_in_4bit --bits 4 --iters 1
Traceback (most recent call last):
  File &quot;/home/run_generation_cpu_woq.py&quot;, line 9, in &lt;module&gt;
    from intel_extension_for_transformers.transformers import (
  File &quot;/root/miniconda3/envs/hyw2/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/__init__.py&quot;, line 56, in &lt;module&gt;
    from .modeling import (
  File &quot;/root/miniconda3/envs/hyw2/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/modeling/__init__.py&quot;, line 21, in &lt;module&gt;
    from .modeling_auto import (AutoModel, AutoModelForCausalLM,
  File &quot;/root/miniconda3/envs/hyw2/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/modeling/modeling_auto.py&quot;, line 64, in &lt;module&gt;
    from ..llm.quantization.utils import (
  File &quot;/root/miniconda3/envs/hyw2/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/llm/quantization/utils.py&quot;, line 37, in &lt;module&gt;
    import intel_extension_for_pytorch as ipex
  File &quot;/root/miniconda3/envs/hyw2/lib/python3.10/site-packages/intel_extension_for_pytorch/__init__.py&quot;, line 8, in &lt;module&gt;
    import torchvision
  File &quot;/root/miniconda3/envs/hyw2/lib/python3.10/site-packages/torchvision/__init__.py&quot;, line 6, in &lt;module&gt;
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils
  File &quot;/root/miniconda3/envs/hyw2/lib/python3.10/site-packages/torchvision/_meta_registrations.py&quot;, line 164, in &lt;module&gt;
    def meta_nms(dets, scores, iou_threshold):
  File &quot;/home/files/pytorch1/pytorch/pytorch/pytorch/torch/library.py&quot;, line 467, in inner
    handle = entry.abstract_impl.register(func_to_register, source)
  File &quot;/home/h30060564/files/pytorch1/pytorch/pytorch/pytorch/torch/_library/abstract_impl.py&quot;, line 30, in register
    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, &quot;Meta&quot;):
RuntimeError: operator torchvision::nms does not exist
</code></pre>
<p>Everything was normal until I compiled the debug version of Pytorch</p>
<p>trying to run llama3 and perf more function stack
information</p>
<p>I have tried install the 2.4.0 develop pytorch and the same mistake appears again</p>
<pre class=""lang-none prettyprint-override""><code>torch                            2.3.0a0+git97ff6cf /home/files/pytorch1/pytorch/pytorch/pytorch
torchaudio                       2.3.0+cpu
torchvision                      0.18.0+cpu
</code></pre>
<p>is there other types of torchvision fit that</p>
","2024-06-11 07:42:45","2","Question"
"78604979","","how to train equivariant transformer on atomic coordinate data","<p>i'm new to using transformers. i'm trying to use the equiformer model <a href=""https://github.com/lucidrains/equiformer-pytorch"" rel=""nofollow noreferrer"">docs here</a> to train a score function on atomic coordinate data. the atomic coordinate data is an Nx3 array, where N is the number of atoms and 3 because of 3 spatial dimensions. likewise, i also have an Nx118 array of one-hot encoded labels for each atom type (118 because of 118 atoms in periodic table).</p>
<p>however, i am confused on how to train this model. i would normally provide some code for this but ive tried so many things that it seems pretty frivolous to include anything.</p>
<p>its a very simple short script that im looking for, and would like to ask anyone of you who might be experienced with this type of model.</p>
<p>just assume the loss function is an empty return statement.</p>
<p>thanks</p>
<p>i tried inputting my Nx118 atom labels into the &quot;feats&quot; parameter but it gave me a ton of random errors, notably that my dim must be equal to 2.</p>
","2024-06-11 00:47:20","-2","Question"
"78604018","","ImportError: cannot import name 'packaging' from 'pkg_resources' when trying to install causal_conv1d","<p>I was trying to install &quot;causal_conv1d&quot; using:</p>
<pre><code>pip install --no-cache-dir  -t /scratch/ahmed/lib  causal_conv1d==1.0.0
</code></pre>
<p>The error I got is:</p>
<pre><code>Collecting causal_conv1d==1.0.0
  Downloading causal_conv1d-1.0.0.tar.gz (6.4 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─&gt; [9 lines of output]
      Traceback (most recent call last):
        File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt;
        File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 34, in &lt;module&gt;
        File &quot;/tmp/pip-install-9i0wsv2k/causal-conv1d_fc0a21267f664102adca1aa336c93106/setup.py&quot;, line 19, in &lt;module&gt;
          from torch.utils.cpp_extension import (
        File &quot;/scratch/ahmed/lib/torch/utils/cpp_extension.py&quot;, line 28, in &lt;module&gt;
          from pkg_resources import packaging  # type: ignore[attr-defined]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ImportError: cannot import name 'packaging' from 'pkg_resources' (/scratch/ahmed/lib/pkg_resources/__init__.py)
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─&gt; See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
</code></pre>
","2024-06-10 18:39:14","8","Question"
"78602284","78424998","","<p>If you are using the ultralytics library you can use their export() function.</p>
<pre><code>model.export(format='tflite')
</code></pre>
<p>Here is a link to its documentation: <a href=""https://docs.ultralytics.com/modes/export/#__tabbed_1_1"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/export/#__tabbed_1_1</a></p>
","2024-06-10 12:25:39","1","Answer"
"78602166","78602074","","<p>With <code>t.all(a.eq(b))</code>, you check whether <em>all</em> entries in <code>a</code> and <code>b</code> match element-wise, i.e. at corresponding positions. With your given data, however, this is not the case, because you have two mismatching positions:</p>
<ol>
<li><code>a[2, 0, 0, 1] == 2.0</code>, but <code>b[2, 0, 0, 1] == 1.0</code>;</li>
<li><code>a[1, 1, 1, 1] == 3.0</code>, but <code>b[1, 1, 1, 1] == 2.0</code>.</li>
</ol>
<p>If you want to check whether <em>any</em> entries in <code>a</code> and <code>b</code> match element-wise, use <code>t.any(a.eq(b))</code>. Also, there is no need to write <code>sum()</code>, as the result of both <code>any()</code> and <code>all()</code> contains one element only.</p>
<p>In summary:</p>
<ul>
<li>For a boolean result, I would write <code>t.any(a.eq(b)).item()</code> or similarly <code>t.any(a == b).item()</code> (which both produces <code>True</code>).</li>
<li>For an integer result, I would write <code>int(t.any(a.eq(b)).item())</code> or similarly <code>int(t.any(a == b).item())</code> (which both produces <code>1</code>).</li>
</ul>
","2024-06-10 12:00:47","0","Answer"
"78602074","","How to compare two multi dimension tensors","<p>I have tensor of below</p>
<pre><code>import torch as t
a = t.tensor([[[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]]], [[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]]],[[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]]]])

b = t.tensor([[[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]]], [[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]]],[[[1.0, 1.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]]]])

ans = t.all(a.eq(b)).sum()

print(ans)
</code></pre>
<p>Expected value is 1 as all the values of first (3,2,2) are equal. But it always returns zero.</p>
","2024-06-10 11:41:30","0","Question"
"78599906","78599813","","<p>The solution is to change input tensor dtype:</p>
<pre><code>input_tensor = torch.tensor(
    [[1, 2, 3, 4, 5, 6, 7, 8]],
    dtype = torch.float32
    )
</code></pre>
","2024-06-09 22:32:24","0","Answer"
"78599813","","Basic error encountered when trying to run PyTorch in Google Colab","<p>I just started learning Deep Learning with PyTorch on DataCamp and I take my notes in Google Colab. This code is supposed to create a basic neural network, and it works well on DataCamp's servers, but I encounter an error whenever I try to run it on Google Colab.</p>
<p>The way that DataCamp teaches you how to do this is to create a variable (e.g., model), that stores the NN sequence. Then you call the variable with the input tensor in parentheses, similar to how you call a function. This seems to match how some examples do it on Github, but it confuses Google Colab. I feel that the answer here is quite basic, but how do I fix this?</p>
<p>Code:</p>
<pre><code>input_tensor = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]])
model = nn.Sequential(nn.Linear(8, 15),
                      nn.Linear(15, 1)
                     )

output = model(input_tensor)
print(output)
</code></pre>
<p>Error Message:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-31-1092509e4a88&gt; in &lt;cell line: 6&gt;()
      4                      )
      5 
----&gt; 6 output = model(input_tensor)
      7 print(output)

5 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)
    114 
    115     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 116         return F.linear(input, self.weight, self.bias)
    117 
    118     def extra_repr(self) -&gt; str:

RuntimeError: mat1 and mat2 must have the same dtype, but got Long and Float
</code></pre>
<p>I've checked examples on Github and they're also coded the way that I'm doing it.</p>
","2024-06-09 21:45:40","0","Question"
"78599368","78008119","","<p>the same thing worked for me but it had to be &quot;cuda:0&quot;
I tried using</p>
<pre><code>accelerator = Accelerator()
device_name = 'cuda' if cuda.is_available() else 'cpu'
device = accelerator.prepare(device_name)
</code></pre>
<p>But it did not work until I used:</p>
<pre><code>device_name = 'cuda:0' if cuda.is_available() else 'cpu'
device = torch.device(device_name)
model= model.to(device)
</code></pre>
","2024-06-09 18:19:41","2","Answer"
"78599098","78566798","","<p>I found the solution. Neither the OS, Python, nor the materials were the problem. Only Pytorch. I just downgraded PyTorch from v2.2.2 to v2.2.0 with the same setup, and it works like a charm.</p>
","2024-06-09 16:27:57","0","Answer"
"78597989","78597921","","<p>As far as I understand, the <code>.detach()</code> method is used to remove the current tensor from the directed acyclic graph (DAG) used for computing gradients of each operation you do. This is a part of autograd and you can find a PyTorch tutorial <a href=""https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I'm not one hundred percent sure of the use case you have here but it seems to me that the author is trying to do some adversarial attack by manipulating the input to a network. It might just be a way of the author trying to explain that you don't need any gradients for the perturbed input image and for the delta you used to perturb it. <a href=""https://discuss.pytorch.org/t/when-to-use-detach/98147/5"" rel=""nofollow noreferrer"">This</a> discussion on the PyTorch forum might help.</p>
<p>In my work, I had to always use detach when converting PyTorch tensors to numpy arrays but I don't think it is the case here.</p>
<p>The best way to see how important it is in your example would be to just remove the <code>.detach()</code> calls and run the whole program and see what happens. My hunch is that it won't change much but it depends on where and how this layer is used.</p>
<p>Hope this helps.</p>
","2024-06-09 08:13:59","0","Answer"
"78597921","","Understanding the Purpose of detach Method in PyTorch","<p>I wanted to understand the purpose of the <code>detach</code> method in PyTorch. Below is an example. If you look into the update_delta method, you see the detach method being used. I don't understand what the author is trying to achieve by using detach.</p>
<pre><code>class PerturbationLayer(torch.nn.Module):
    def __init__(self, hidden_size, learning_rate=1e-4, init_perturbation=1e-2):
        super().__init__()
        self.learning_rate = learning_rate
        self.init_perturbation = init_perturbation
        self.delta = None
        self.LayerNorm = torch.nn.LayerNorm(hidden_size, 1e-7, elementwise_affine=False)
        self.adversarial_mode = False

    def adversarial_(self, adversarial=True):
        self.adversarial_mode = adversarial
        if not adversarial:
            self.delta = None

    def forward(self, input):
        if not self.adversarial_mode:
            self.input = self.LayerNorm(input)
            return self.input
        else:
            if self.delta is None:
                self.update_delta(requires_grad=True)
            return self.perturbated_input

    def update_delta(self, requires_grad=False):
        if not self.adversarial_mode:
            return True
        if self.delta is None:
            delta = torch.clamp(
                self.input.new(self.input.size())
                .normal_(0, self.init_perturbation)
                .float(),
                -2 * self.init_perturbation, 2 * self.init_perturbation,
            )
        else:
            grad = self.delta.grad
            self.delta.grad = None
            delta = self.delta
            norm = grad.norm()
            if torch.isnan(norm) or torch.isinf(norm):
                return False
            eps = self.learning_rate
            with torch.no_grad():
                delta = delta + eps * grad / (
                    1e-6 + grad.abs().max(-1, keepdim=True)[0]
                )
        self.delta = delta.float().detach().requires_grad_(requires_grad)
        self.perturbated_input = (self.input.to(delta).detach() + self.delta).to(
            self.input
        )
        return True

</code></pre>
<p>Questions:</p>
<pre><code>What does the detach method do in this context?
Why is it necessary to use detach when updating delta?
</code></pre>
<p>Any insights would be appreciated!</p>
","2024-06-09 07:45:18","1","Question"
"78594351","78359357","","<p>As a workaround, when I'm processing my data I just do</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

log = pd.read_csv(&quot;path_to_log_file.csv&quot;, sep=',')
log = log.groupby('epoch').mean()  # merge the train and valid rows
log['Epoch'] = log.index  # because &quot;Epoch&quot; gets turned into the index
log.index.name = ''  # to remove the name &quot;Epoch&quot; from the index
</code></pre>
<p>Works fine for me in pandas v1.4.2 (not sure about others), since the NaNs are treated as 0 by <code>.mean()</code>.</p>
","2024-06-07 23:50:19","1","Answer"
"78593480","78592957","","<p>The problem is <code>ScalingConfig(num_workers=1 + CONCURRENT_TRIALS, ...)</code>. My assumption was that I force Ray to use 2 workers total, i.e., 2 trials concurrently. What this parameter really means is number of workers per trial. So num_workers=2 corresponds with 2 nodes per trial, resulting in one trial being distributed across the 2 nodes, and other trials stuck at pending.</p>
<p>Set this to 1 and keep max_concurrent_trials=2, and this fixes the problem.</p>
","2024-06-07 18:12:09","0","Answer"
"78592957","","How to set up RayTune for distributed training using Pytorch Lightning","<p>I have 16 CPUs and 1 GPU, and I want to split 2 concurrent trials across all my available resources.</p>
<p>Attempt number one:</p>
<pre><code># define param_space and run_config
...
...

CONCURRENT_TRIALS = True
scaling_config = ScalingConfig(
    num_workers=1 + CONCURRENT_TRIALS,
    use_gpu=True,
    resources_per_worker={&quot;GPU&quot;: 0.5, &quot;CPU&quot;: 8} if CONCURRENT_TRIALS else {&quot;GPU&quot;: 1, &quot;CPU&quot;: 15},
)

# Define a TorchTrainer without hyper-parameters for Tuner
trainer_args = dict(
    train_loop_per_worker=train_func,
    scaling_config=scaling_config,
    run_config=run_config,
    )
if CONCURRENT_TRIALS:
    trainer_args[&quot;torch_config&quot;] = ray_torch.TorchConfig(backend=&quot;gloo&quot;)

ray_trainer = TorchTrainer(
    **trainer_args,
)

tuner = tune.Tuner(
    ray_trainer,
    param_space={&quot;train_loop_config&quot;: search_space},
    tune_config=tune.TuneConfig(
        metric=&quot;val_loss&quot;,
        mode=&quot;min&quot;,
        num_samples=n_samples,
        scheduler=scheduler,
        max_concurrent_trials=1 + CONCURRENT_TRIALS,
    ),
)

tuner.fit()
</code></pre>
<p>This results in the following:</p>
<pre><code>WARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 17.0 CPUs and 1.0 GPUs per trial, but the cluster only has 16.0 CPUs and 1.0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.
Trial status: 2 PENDING
Logical resource usage: 0/16 CPUs, 0/1 GPUs
</code></pre>
<p>I assume this means 1 CPU is needed for handling Ray checkpointing, loading, etc.</p>
<p>So I change to 7 CPUs per worker:</p>
<pre><code>scaling_config = ScalingConfig(
    num_workers=1 + CONCURRENT_TRIALS,
    use_gpu=True,
    resources_per_worker={&quot;GPU&quot;: 0.5, &quot;CPU&quot;: 7} if CONCURRENT_TRIALS else {&quot;GPU&quot;: 1, &quot;CPU&quot;: 15},
)
</code></pre>
<p>Then the second trial is perpetually stuck on PENDING, with the following logical resource usage:</p>
<pre><code>Logical resource usage: 15.0/16 CPUs, 1.0/1 GPUs
</code></pre>
<p>I've also tried doing <code>tune.with_resources</code> for <code>train_func</code>, as well both with and without the scaling_config changes, and can't get it to just split evenly between two trials at once. Also, <code>scaling_config = ScalingConfig(..., placement_strategy=&quot;SPREAD&quot;)</code> doesn't work either. I have a ton of RAM and GPU memory left over while training one process, so I want to take advantage of Ray's parallel scheduling, but I can't figure this out.</p>
","2024-06-07 15:56:31","0","Question"
"78590174","78577645","","<p>Since the virtual package <code>__cuda==11.4</code> is present, conda should be able to install the gpu accelerated versions of pytorch. The next thing to check is available cuda versions. You can use <code>conda search</code> for that like so:</p>
<pre><code>conda search -c pytorch pytorch==1.11
# Name                       Version           Build  Channel
pytorch                       1.11.0    py3.10_cpu_0  pytorch
pytorch                       1.11.0 py3.10_cuda10.2_cudnn7.6.5_0  pytorch
pytorch                       1.11.0 py3.10_cuda11.1_cudnn8.0.5_0  pytorch
pytorch                       1.11.0 py3.10_cuda11.3_cudnn8.2.0_0  pytorch
pytorch                       1.11.0 py3.10_cuda11.5_cudnn8.3.2_0  pytorch
pytorch                       1.11.0     py3.7_cpu_0  pytorch
pytorch                       1.11.0 py3.7_cuda10.2_cudnn7.6.5_0  pytorch
pytorch                       1.11.0 py3.7_cuda11.1_cudnn8.0.5_0  pytorch
pytorch                       1.11.0 py3.7_cuda11.3_cudnn8.2.0_0  pytorch
pytorch                       1.11.0 py3.7_cuda11.5_cudnn8.3.2_0  pytorch
pytorch                       1.11.0     py3.8_cpu_0  pytorch
pytorch                       1.11.0 py3.8_cuda10.2_cudnn7.6.5_0  pytorch
pytorch                       1.11.0 py3.8_cuda11.1_cudnn8.0.5_0  pytorch
pytorch                       1.11.0 py3.8_cuda11.3_cudnn8.2.0_0  pytorch
pytorch                       1.11.0 py3.8_cuda11.5_cudnn8.3.2_0  pytorch
pytorch                       1.11.0     py3.9_cpu_0  pytorch
pytorch                       1.11.0 py3.9_cuda10.2_cudnn7.6.5_0  pytorch
pytorch                       1.11.0 py3.9_cuda11.1_cudnn8.0.5_0  pytorch
pytorch                       1.11.0 py3.9_cuda11.3_cudnn8.2.0_0  pytorch
pytorch                       1.11.0 py3.9_cuda11.5_cudnn8.3.2_0  pytorch
</code></pre>
<p>Note that for python 3.9, which you have, there are only <code>cuda10.2</code>, <code>cuda11.1</code>, <code>cuda11.3</code> and <code>cuda11.5</code> available.
So by forcing <code>cudatoolkit=11.4</code>, <code>conda</code> cannot find a suitable pytorch gpu version and installs a cpu only variant. Since you driver/graphics card are only capable up to cuda 11.4, your best option is probably to use 11.3:</p>
<pre><code>conda install pytorch=1.11.0 cudatoolkit=11.3 -c pytorch -c nvidia
</code></pre>
","2024-06-07 06:06:27","3","Answer"
"78588372","78328539","","<p>Each error might need to be dealt with differently. Let's talk about the authentication issue that you mentioned.</p>
<p>For available models that you can use, check out their Model Hub (more information see <a href=""https://huggingface.co/learn/nlp-course/chapter4/1?fw=pt"" rel=""nofollow noreferrer"">The Hugging Face Hub Guide</a>). You can use any tag to filter down to the specific task you are working on.</p>
<p>So most public models can be used by your <code>pipeline()</code> function, but some require extra step to access it (i.e. accept their community license for example).</p>
<p>To avoid authentication error in the future, your need:</p>
<ol>
<li><p>Create <a href=""https://huggingface.co/settings/tokens"" rel=""nofollow noreferrer"">access token</a> (with read access)</p>
</li>
<li><p>Accept model term of use:
To accept it, go to the model page, in your case, it would be <a href=""https://huggingface.co/google/gemma-2b-it"" rel=""nofollow noreferrer"">https://huggingface.co/google/gemma-2b-it</a> to accept the term.</p>
</li>
<li><p>Log in to huggingface in your code by:</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from huggingface_hub import login
login()

# or in notebook you would need
from huggingface_hub import notebook_login
notebook_login()
</code></pre>
<ol start=""4"">
<li>Add access token to your function</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import pipeline
print(torch.cuda.is_available())

generator = pipeline(&quot;text-generation&quot;, model=&quot;google/gemma-2b-it&quot;, device=&quot;cuda&quot;, token=&quot;your_token&quot;)
</code></pre>
","2024-06-06 18:07:26","2","Answer"
"78584684","78578853","","<p>I decided to tackle this from another perspective, and it worked.</p>
<p>I used a DataBlock class from the fastai library directly, which allowed me to specify the actions I wanted to take on the data by myself.</p>
<pre><code>dblock = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=parent_label,
    item_tfms=Resize(224),
    batch_tfms=aug_transforms()
)

dls = dblock.dataloaders(path, bs=64)

learn = vision_learner(dls, resnet34, lr=0.005, metrics=accuracy)
</code></pre>
<p>This resulted in a correct handling of the training and testing sets.</p>
","2024-06-06 05:48:45","1","Answer"
"78584188","78542429","","<p>The sound inputs were too long, after resampling the audio into chunks, the problem was resolved.</p>
","2024-06-06 02:08:48","0","Answer"
"78583802","","How to count total number of pixels of each class for semantic segmentation in detectron2","<p>I want to count the total number of pixels for each segmented class, I only need the count for each general objects, like one class for every vehicle, one for every person and so on. For this reason, I'm using semantic segmentation instead of instance segmentation (which would consider each vehicle  or person instance separately).But the output of semantic segmentation in detectron2 does not have binary mask.</p>
<p>I know the output of instance segmentation is binary mask and can get the pixel count using the following code:</p>
<pre><code>masks = output['instances'].pred_masks 
results = torch.sum(torch.flatten(masks, start_dim=1),dim=1)
</code></pre>
<p>This gives the pixel count but considers each vehicle instance separately which I do not want
.
But the output of semantic segmentation is the field 'sem_seg' which contains predicted class probabilities for each general class and not binary mask, how can I go on into getting the pixel count for each class in semantic segmentation?</p>
","2024-06-05 22:40:27","-1","Question"
"78583755","","PyTorch F.interpolate for many dimensions (e.g. 4D, 5D, 6D, 7D, ..., n-D)","<p>I want to apply N-d interpolation to an (N+2)-d tensor for N&gt;3.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F

x = torch.randn(1, 1, 2, 3, 4, 5, 6, 7)
output_size = (7, 6, 5, 4, 3, 2)
y = F.interpolate(x, size=output_size, mode=&quot;linear&quot;)
</code></pre>
<p>The above code gives the following error:</p>
<blockquote>
<p>NotImplementedError: Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got linear)</p>
</blockquote>
<p>Note that the first two dimensions are batch size and channels (B, C), and are thus <em>not</em> interpolated, as stated in the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html"" rel=""nofollow noreferrer"">docs</a>.</p>
<ul>
<li>linear: 1D interpolation of a 3D tensor (with B, C)</li>
<li>bilinear: 2D interpolation of a 4D tensor (with B, C)</li>
<li>trilinear: 3D interpolation of a 5D tensor (with B, C)</li>
<li>N-d linear: not supported...?!</li>
</ul>
<p>How do I apply N-d linear interpolation for N&gt;3?</p>
","2024-06-05 22:24:19","1","Question"
"78583756","78583755","","<p>N-d linear interpolation is effectively the same as applying 1-D linear interpolation along each interpolated dimension in succession. Wikipedia gives the following example diagram for 2-D (bilinear) interpolation:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Bilinear interpolation</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/BilinearInterpolationV2.svg/440px-BilinearInterpolationV2.svg.png"" alt="""" /></td>
</tr>
<tr>
<td>Apply linear interpolation along each dimension.</td>
</tr>
</tbody>
</table></div>
<p>Thus, one simple method is:</p>
<pre><code>def interpolate(input, size, scale_factor=None):
    assert input.ndim &gt;= 3
    if scale_factor is not None:
        raise NotImplementedError
    output_shape = (*input.shape[:2], *size)
    assert len(input.shape) == len(output_shape)
    # Apply linear interpolation to each spatial dimension.
    for i in range(2, 2 + len(size)):
        input_tail = math.prod(input.shape[i + 1 :])
        input = F.interpolate(
            input.reshape(
                input.shape[0], math.prod(input.shape[1:i]), input.shape[i], input_tail
            ),
            size=(output_shape[i], input_tail),
            mode=&quot;bilinear&quot;,
        ).reshape(*input.shape[:i], output_shape[i], *input.shape[i + 1 :])
    return input.reshape(output_shape)
</code></pre>
<p>Usage:</p>
<pre><code>x = torch.randn(1, 1, 2, 3, 4, 5, 6, 7)
output_size = (7, 6, 5, 4, 3, 2)
y = interpolate(x, size=output_size)
</code></pre>
","2024-06-05 22:24:19","1","Answer"
"78582247","78452655","","<p>The main problem here is that you use <code>reduction='batchmean'</code> in <code>F.kl_div</code>. That would be right for a common classification problem, in which output of a model has shape <code>[B, C]</code>, where B is batch size and C is number of classes. Note that the sum over the second dimension will give 1, as the model gives the probas of classes for each sample in batch.  In your case you may consider segmentation as pixel-wise classification, so that the value for each pixel is predicted independently. The number of classes <code>C</code> is 2, but <code>B</code> is not batch size any more,<code>B=224x224x8</code> as each pixel is a separate &quot;sample&quot;. So, there are two options to solve this problem:</p>
<ol>
<li>Reshape tensors to <code>[224x224x8, 2]</code> before <code>kl_div</code>, for example, by using <code>.view(-1,2)</code> and continue using <code>reduction='batchmean'</code></li>
<li>Calculate <code>kl_div</code> in this way (pay attention to <code>mean</code> and <code>* 2</code>):</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>kd_loss = F.kl_div(stu_prob, 
                       preds_teacher, 
                       reduction='mean',
                  ) * T * T * 2
</code></pre>
","2024-06-05 16:03:06","1","Answer"
"78580803","78578853","","<p>You can simply remove the label if it's not supposed to exist in your dataset, or you can lower the learning rate.</p>
","2024-06-05 11:54:07","1","Answer"
"78579425","78579128","","<p>Your issue arises because you're replacing some of the major nodes with unique nodes randomly, without checking if the major node still appears elsewhere.</p>
<p>Create a list of nodes that appear twice, then for each node find all its occurrences. Finally, randomly select one occurrence to keep and replace the rest with unique nodes:</p>
<pre><code>import torch

edge_index = torch.tensor([[0, 0, 1, 2, 3, 4, 6, 7, 7, 8],
                       [1, 2, 2, 4, 4, 5, 0, 1, 3, 7]])
n_id, n_counts = torch.unique(edge_index, return_counts=True)
unique_nodes = n_id[n_counts==1]
major_nodes = n_id[n_counts&gt;2]
result = edge_index.clone()
for major_node in major_nodes:
    occurrences = torch.nonzero(result == major_node, as_tuple=True)
    keep_idx = torch.randint(len(occurrences[0]), (1,))[0]
    for i in range(len(occurrences[0])):
        if i != keep_idx and len(unique_nodes) &gt; 0:
            result[occurrences[0][i], occurrences[1][i]] = unique_nodes[0]
            unique_nodes = unique_nodes[1:]
print(result)
</code></pre>
","2024-06-05 07:38:37","2","Answer"
"78579409","","How I can load model and inference using Libtorch (C++)?","<p>I trained simple model for MNIST using it <a href=""https://github.com/pytorch/examples/blob/main/cpp/mnist/mnist.cpp"" rel=""nofollow noreferrer"">https://github.com/pytorch/examples/blob/main/cpp/mnist/mnist.cpp</a></p>
<p>I added code for saving model like below</p>
<pre><code>string model_path = &quot;model.pt&quot;;
torch::serialize::OutputArchive output_archive;
model.save(output_archive);
output_archive.save_to(model_path);
</code></pre>
<p>and in other cpp file, I just tried like below</p>
<pre><code>torch::jit::script::Module model;
model=torch::jit::load(&quot;model.pt&quot;);
model.to(device);

std::vector&lt;torch::jit::IValue&gt; inputs;
inputs.push_back(torch::ones({1, 1, 28, 28}));

at::Tensor output = model.forward(inputs).toTensor();
std::cout &lt;&lt; output &lt;&lt; std::endl;
</code></pre>
<p>It complied well, but had an error while executing</p>
<pre><code>terminate called after throwing an instance of 'c10::Error'
  what():  Method 'forward' is not defined.
Exception raised from get_method at .../include/torch/csrc/jit/api/object.h:111 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f8e0163f897 in .../lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&amp;) + 0x64 (0x7f8e015efb25 in .../lib/libc10.so)
frame #2: &lt;unknown function&gt; + 0xa729 (0x560eaa26e729 in ./example-app)
frame #3: &lt;unknown function&gt; + 0xa8b1 (0x560eaa26e8b1 in ./example-app)
frame #4: &lt;unknown function&gt; + 0x5355 (0x560eaa269355 in ./example-app)
frame #5: __libc_start_main + 0xe7 (0x7f8d8eea0c87 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: &lt;unknown function&gt; + 0x4bca (0x560eaa268bca in ./example-app)
</code></pre>
<p>How can I fix it?</p>
<p>I tried to use  TORCH_MODULE to create a module holder and save the model like this ref : <a href=""https://discuss.pytorch.org/t/libtorch-how-to-save-model-in-mnist-cpp-example/34234/5"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/libtorch-how-to-save-model-in-mnist-cpp-example/34234/5</a></p>
<p>but it didn't work (maybe I used it wrong)</p>
","2024-06-05 07:35:39","1","Question"
"78579128","","How can I randomly replace the values remaining at least one value using python?","<p>I tried to replace the some of major values in the tensor with the unique value while maintaining at least one value.</p>
<p>For example, given <code>edge_index</code>, I want to change it as below.</p>
<pre class=""lang-py prettyprint-override""><code>edge_index = torch.as_tensor([[0, 0, 1, 2, 3, 4, 6, 7, 7, 8],
                              [1, 2, 2, 4, 4, 5, 0, 1, 3, 7]])

result = torch.as_tensor([[5, 0, 1, 2, 3, 8, 6, 7, 7, 8],
                          [1, 2, 2, 4, 4, 5, 0, 1, 3, 6]])
</code></pre>
<p>In detail, some values appeared more than 2 times in <code>edge_index</code> (i.e., 0, 1, 2, 4, 7).
To avoid unique values, I need to change some of them with exceptional values (i.e., 5, 6, 8).</p>
<p>I tried to do it as below, but my code couldn't ensure the condition; all values need to appear at least 2 times.</p>
<pre class=""lang-py prettyprint-override""><code># Counts values
n_id, n_counts = torch.unique(edge_index, return_counts=True)
unique_nodes = n_id[n_counts==1] #tensor([5, 6, 8])
major_nodes = n_id[n_counts&gt;2] #tensor([0, 1, 2, 4, 7])

# Find the index where the major_nodes located
major_s_idx = (edge_index[..., None] == major_nodes).any(-1)[0].nonzero()[:, 0] #tensor([0, 1, 2, 3, 5, 7, 8])
major_t_idx = (edge_index[..., None] == major_nodes).any(-1)[1].nonzero()[:, 0] #tensor([0, 1, 2, 3, 4, 6, 7, 9])

result = edge_index.clone()

result[0][major_s_idx[torch.randperm(len(major_s_idx))[:len(unique_nodes)]]] = unique_nodes
result[1][major_t_idx[torch.randperm(len(major_t_idx))[:len(unique_nodes)]]] = unique_nodes

result
# tensor([[0, 0, 6, 2, 3, 4, 6, 5, 8, 8],
#        [5, 2, 2, 4, 6, 5, 0, 8, 3, 7]]) # 1 is disappeared and 7 remained only one time
</code></pre>
<p>Note that the correct result does not need to be the same as the <code>result</code> in the first code block. Just reference.</p>
","2024-06-05 06:33:23","2","Question"
"78578853","","FastAI KeyError: ""Label '755' was not included in the training dataset""","<p>I am working on a quite simplified vision_learner model from FastAI that uses a resnet34 as backbone. It should classify mushroom images, and all the data loading parts are done correctly, except when I get to split between training and testing datasets.</p>
<p>It seems like something goes wrong there, and when I get to fitting the model, it churns out this error message:</p>
<pre><code>File /opt/conda/lib/python3.10/site-packages/fastai/data/transforms.py:263, in Categorize.encodes(self, o)
    261     return TensorCategory(self.vocab.o2i[o])
    262 except KeyError as e:
--&gt; 263     raise KeyError(f&quot;Label '{o}' was not included in the training dataset&quot;) from e

KeyError: &quot;Label '755' was not included in the training dataset&quot;
</code></pre>
<p>For context, this is how I load the data initially:</p>
<pre class=""lang-py prettyprint-override""><code>bs = 64
path = Path(&quot;../input/mushrooms/Mushrooms/&quot;)
fnames = []
for fpath in class_names:
    print(path/f'{fpath}/')
    fnames += get_image_files(path/f'{fpath}/')
....
../input/mushrooms/Mushrooms/Entoloma
../input/mushrooms/Mushrooms/Suillus
../input/mushrooms/Mushrooms/Hygrocybe
../input/mushrooms/Mushrooms/Agaricus
../input/mushrooms/Mushrooms/Amanita
../input/mushrooms/Mushrooms/Lactarius
../input/mushrooms/Mushrooms/Russula
../input/mushrooms/Mushrooms/Boletus
../input/mushrooms/Mushrooms/Cortinarius
</code></pre>
<p>And how I prepare it for the model:</p>
<pre class=""lang-py prettyprint-override""><code>np.random.seed(2)
pat = r&quot;(\d+)_([a-zA-Z0-9-_]+)\.jpg$&quot;

item_tfms = Resize(224)  # Resizing each image to 224x224
batch_tfms = [*aug_transforms(), Normalize.from_stats(*imagenet_stats)]  # Standard augmentations + normalization

data = ImageDataLoaders.from_name_re(
    path='.', 
    fnames=fnames,
    pat=pat,
    item_tfms=item_tfms,
    batch_tfms=batch_tfms,
    bs=bs,
    num_workers=0 
)

train_dataset, test_dataset = data.train, data.valid
data.show_batch()
</code></pre>
<pre><code># Check classes
print(f&quot;Number of classes: {len(data.vocab)}&quot;) # 2046

# Check datasets
print(f&quot;Training dataset size: {len(data.train_ds)}&quot;) # 5372
print(f&quot;Validation dataset size: {len(data.valid_ds)}&quot;) # 1342
</code></pre>
<p>And then I prepare the model</p>
<pre><code>learn = vision_learner(data, models.resnet50, metrics=error_rate, lr=0.001)
learn.fit(n_epochs = 5, start_epoch=0)
</code></pre>
<p>Which ultimately leads to the issue above. Does someone have any hints as to what I'm doing wrong?</p>
","2024-06-05 05:09:30","1","Question"
"78577645","","How to get conda to install pytorch-gpu rather than pytorch-cpu Ubuntu cuda 11.4","<p>I've combed through all of the similar questions but to no avail. I have an older GPU that I'm trying to get to work with CUDA and pytorch. For some reason, I can't get conda to install pytorch gpu. No matter what I do, it tries to install the cpu version of pytorch.</p>
<p>I build my conda like this - miniconda</p>
<blockquote>
<p>conda create --name tortoise python=3.9 numba inflect</p>
</blockquote>
<p>I need to force a specific version of CUDA 11.4.</p>
<pre><code>conda install pytorch=1.11.0 torchvision=0.12 torchaudio=0.11 cudatoolkit=11.4 -c pytorch -c nvidia
</code></pre>
<p>Someone said that torchvision and torchaudio could cause the cpu version to be installed.</p>
<pre><code>conda install pytorch=1.11.0 cudatoolkit=11.4 -c pytorch -c nvidia
</code></pre>
<p>Regardless of what I do, it tries to install the cpu version of pytorch.</p>
<p><a href=""https://i.sstatic.net/Qkzc3bnZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qkzc3bnZ.png"" alt=""Showing conda will install cpu version of pytorch"" /></a></p>
<p>So what it is that is determining that the cpu version should be installed. I have a GPU in this box.</p>
<p><a href=""https://i.sstatic.net/JWga5a2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JWga5a2C.png"" alt=""nvidia-smi output"" /></a></p>
<p>conda info</p>
<p><a href=""https://i.sstatic.net/ZLz26c3m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZLz26c3m.png"" alt=""conda info"" /></a></p>
","2024-06-04 20:41:37","1","Question"
"78576364","","lstm is not learning anything pytorch","<p>I am trying to use lstm for binary classification of comments (comments are alredy pre processed and split). I created a model, but it is not learning anything.</p>
<p>In some cases I receive exactly the same accuracy no matter how many epoches I choose.</p>
<p>GitHub link: <a href=""https://github.com/PavloChaika/ML_SET/blob/homework3/HW3/HA3%20-%20IMDB%20competition.ipynb"" rel=""nofollow noreferrer"">https://github.com/PavloChaika/ML_SET/blob/homework3/HW3/HA3%20-%20IMDB%20competition.ipynb</a></p>
<p>I tried different combinations of num_layers, hidden_size, learning_rate. I am using input_size because ithervise it works very slowly (even though I realize that I have only 1 input, i just try to train at least a little bit, and since I clasify comments my guess was that it should work) I tried using 1 input, but result was very similar</p>
","2024-06-04 15:26:07","-3","Question"
"78572781","78096910","","<p>I encountered the same issue when trying to import HyperOptSearch from Ray Tune.</p>
<pre><code>from ray.tune.search.hyperopt import HyperOptSearch
</code></pre>
<p>fixed it for me.</p>
","2024-06-03 23:04:33","1","Answer"
"78572471","78572370","","<p>You are generating an entirely different random dataset for the manual validation data.</p>
<p>The first time you generate a validation set is in the lines</p>
<pre><code># Set up model, data, and trainer
model = ResNet()
train_loader = get_dataloader()
val_loader = get_dataloader()
</code></pre>
<p>And the second time you generate a validation set is the lines</p>
<pre><code># Predict on validation data
val_loader = get_dataloader()
</code></pre>
<p>Since the <code>get_dataloader()</code> function generates a random synthetic dataset each time you run it, you are essentially running the same accuracy tests on two different sets of data.</p>
<p>To fix this, I would remove the second initialization of <code>val_loader</code>, so that the same Dataloader instance is used in the manual and automatic tests.</p>
","2024-06-03 20:50:12","0","Answer"
"78572370","","Difference between accuracy during training and accuracy during testing","<p>In the model below the accuracy reported at the end of the final validation stage is 0.46, but when reported during the manual testing the value is 0.53. What can account for this discrepancy?</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn
import torchvision.models as models
import pytorch_lightning as pl
from torchmetrics.classification import BinaryAccuracy
from pytorch_lightning.loggers import NeptuneLogger
from torch.utils.data import DataLoader, TensorDataset
from os import environ

class ResNet(pl.LightningModule):
    def __init__(self, n_classes=1, n_channels=3, lr=1e-3):
        super().__init__()
        self.save_hyperparameters()
        self.validation_accuracy = BinaryAccuracy()
        
        backbone = models.resnet18(pretrained=True)
        n_filters = backbone.fc.in_features
        if n_channels != 3:
            backbone.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        layers = list(backbone.children())[:-1]
        self.feature_extractor = nn.Sequential(*layers)
        self.classifier = nn.Linear(n_filters, n_classes)
        self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, x):
        features = self.feature_extractor(x)
        logits = self.classifier(features.squeeze())
        return logits

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.validation_accuracy.update(y_hat, y)
        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def on_validation_epoch_end(self):
        val_acc = self.validation_accuracy.compute()
        self.log('val_acc', val_acc, on_epoch=True, prog_bar=True, logger=True)
        self.validation_accuracy.reset()

    def predict_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        preds = torch.sigmoid(y_hat) &gt; 0.5
        return preds, y

def get_dataloader():
    # Create a simple synthetic dataset for demonstration purposes
    x = torch.randn(100, 3, 224, 224)
    y = torch.randint(0, 2, (100, 1)).float()
    dataset = TensorDataset(x, y)
    return DataLoader(dataset, batch_size=8)

# Set up model, data, and trainer
model = ResNet()
train_loader = get_dataloader()
val_loader = get_dataloader()

logger = NeptuneLogger(
        api_key=environ.get(&quot;NEPTUNE_API_TOKEN&quot;),
        project=&quot;richbai90/ResnetTest&quot;,
        tags=[&quot;MRE&quot;, &quot;resnet&quot;],
    )


trainer = pl.Trainer(max_epochs=3, logger=logger, log_every_n_steps=1)

# Train and validate the model
trainer.fit(model, train_loader, val_loader)

# Predict on validation data
val_loader = get_dataloader()
preds, targets = [], []
for batch in val_loader:
    batch_preds, batch_targets = model.predict_step(batch, 0)
    preds.extend(batch_preds)
    targets.extend(batch_targets)

# Calculate accuracy manually for comparison
preds = torch.stack(preds).view(-1)
targets = torch.stack(targets).view(-1)
manual_accuracy = (preds == targets).float().mean().item()
print(f&quot;Manual accuracy: {manual_accuracy:.4f}&quot;)
</code></pre>
","2024-06-03 20:17:59","0","Question"
"78572198","78572138","","<p>You can achieve this by using pandas to create the data frames and then combine them. Here's a step-by-step solution:</p>
<p>1.Create the two data frames.</p>
<p>2.Concatenate them along the columns.</p>
<p>3.Extract and unpack the desired row.</p>
<pre><code>import pandas as pd

# Create the data frames
df1 = pd.DataFrame([[1, 2, 3], [4, 5, 6]])
df2 = pd.DataFrame(['a', 'b'])

# Combine the data frames
df_combined = pd.concat([df1, df2], axis=1)

# Extract the desired row and unpack it
numbers, letter = df_combined.iloc[0, :-1].tolist(), df_combined.iloc[0, -1]

print(numbers)  # Output: [1, 2, 3]
print(letter)   # Output: a
</code></pre>
","2024-06-03 19:23:29","0","Answer"
"78572138","","Python / Pytorch combining unequal data frames to create a value, label pair","<p>I have two data frames:</p>
<pre><code>df1 = [[1,2,3],[4,5,6]]
df2 = [a,b]
</code></pre>
<p>I want to combine them so that the resulting</p>
<p><code>df[0] = [[1,2,3], a]</code>.</p>
<p>My final goal is to be able to do something like:</p>
<pre><code>numbers, letter  = df[0]
--&gt; numbers = [1,2,3], letter = a
</code></pre>
","2024-06-03 19:06:46","1","Question"
"78571764","78571707","","<p>You have to install <code>torchvision</code> package. After that verify that your installation worked by running <code>import torchvision</code>.</p>
","2024-06-03 17:34:12","0","Answer"
"78571707","","NameError: name 'torchvision' is not defined","<p>I am relatively new to coding. I am trying to use pytorch for cnn . I have installed all the libraries, the versions are updated in google colab, still having issue with torchvision not being defined</p>
<p>I have tried all that needs to be done, i am just trying to create a dataset of images using torchvision.</p>
<h1>Create a dataset object</h1>
<p>dataset = torchvision.datasets.ImageFolder(image_folder_path, transform=transform)</p>
<h1>Create a data loader for batching</h1>
<h2>data_loader = DataLoader(dataset, batch_size=32, shuffle=True)  # Adjust batch size as needed</h2>
<p>NameError                                 Traceback (most recent call last)</p>
<p> in &lt;cell line: 2&gt;()
1 # Create a dataset object
----&gt; 2 dataset = torchvision.datasets.ImageFolder(image_folder_path, transform=transform)
3
4 # Create a data loader for batching
5 data_loader = DataLoader(dataset, batch_size=32, shuffle=True)  # Adjust batch size as needed</p>
<p>NameError: name 'torchvision' is not defined</p>
","2024-06-03 17:18:40","-1","Question"
"78570548","78555597","","<p><strong>The short answer</strong>: Replace <code>pretrained_vit = torchvision.models.vit_h_14()</code> with <code>torchvision.models.vit_h_14(image_size=518)</code> in your <code>vit_h_14(weight_path)</code> function.</p>
<h2>The long answer</h2>
<p>When you load weights via the constant <code>IMAGENET1K_SWAG_E2E_V1</code>, as you do in your second implementation, some additional things happen as compared to the version where you try to load the weights directly/manually; in particular:</p>
<ol>
<li>On creating the constant <code>IMAGENET1K_SWAG_E2E_V1</code>, the weights are wrapped into a <code>ViT_H_14_Weights(WeightsEnum)</code> instance, which holds <code>torchvision.models._api.Weights</code> instances as values (see corresponding <a href=""https://github.com/pytorch/vision/blob/ab0b9a436bd64c4d0309f1b700868c2fe73c0f3e/torchvision/models/vision_transformer.py#L562"" rel=""nofollow noreferrer"">source code</a>, line 562).</li>
<li>On creating your actual vision transformer instance via <code>torchvision.models.vit_h_14()</code>, internally, the function <code>_vision_transformer()</code> is called, and receives, among others, the <code>ViT_H_14_Weights(WeightsEnum)</code> instance (see corresponding <a href=""https://github.com/pytorch/vision/blob/ab0b9a436bd64c4d0309f1b700868c2fe73c0f3e/torchvision/models/vision_transformer.py#L777"" rel=""nofollow noreferrer"">source code</a>, line 777).</li>
<li>The function <code>_vision_tranformer()</code> extracts additional parameters from the <code>ViT_H_14_Weights(WeightsEnum)</code> instance. Crucially, it extracts the attribute <code>weights.meta[&quot;min_size&quot;][0]</code>, which is then passed as <code>&quot;image_size&quot;</code> when creating the actual <code>VisionTransformer</code> instance (see corresponding <a href=""https://github.com/pytorch/vision/blob/ab0b9a436bd64c4d0309f1b700868c2fe73c0f3e/torchvision/models/vision_transformer.py#L321-L331"" rel=""nofollow noreferrer"">source code</a>, lines 321 and 331). This <code>&quot;min_size&quot;</code>/<code>&quot;image_size&quot;</code> value is 518 (see corresponding <a href=""https://github.com/pytorch/vision/blob/ab0b9a436bd64c4d0309f1b700868c2fe73c0f3e/torchvision/models/vision_transformer.py#L574"" rel=""nofollow noreferrer"">source code</a>, line 574).</li>
</ol>
<p>It is this <code>&quot;min_size&quot;</code>/<code>&quot;image_size&quot;</code> parameter that prevents you from loading the dict manually/directly, as it is not set in your manual version.</p>
<p>The following code fixes the problem, so that the weights can be loaded:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torchvision

weight_path = &quot;/your/path/to/vit_h_14_swag-80465313.pth&quot;  # TODO: adjust

def vit_h_14_manual(weight_path):
    pretrained_vit = torchvision.models.vit_h_14(image_size=518)
    pretrained_vit.load_state_dict(torch.load(weight_path))

    for parameter in pretrained_vit.parameters():
        parameter.requires_grad = False
    return pretrained_vit

vit_h_14_manual(weight_path)
</code></pre>
<p>I am not sure if this is the only adjustment you need to make for your code to <em>actually</em> work as expected. For example, the <code>&quot;num_classes&quot;</code> parameter is overwritten in a similar fashion as the <code>&quot;image_size&quot;</code> parameter (see corresponding <a href=""https://github.com/pytorch/vision/blob/ab0b9a436bd64c4d0309f1b700868c2fe73c0f3e/torchvision/models/vision_transformer.py#L319"" rel=""nofollow noreferrer"">source code</a>, line 319). So better proceed with caution.</p>
","2024-06-03 13:23:39","0","Answer"
"78568138","78563740","","<p>In your post, you mentioned that</p>
<blockquote>
<p>In this module, scores has requires_grad=True, but after computation, requires_grad for other parameters is False. Even the simplest threshold with requires_grad is also False. I don't understand why this is the case.</p>
</blockquote>
<p>However, according to my test case, <code>self.threshold.requires_grad=True</code>. Please check the following code:</p>
<pre><code>import torch.nn as nn

class MoR(nn.Module):

    def __init__(self, work=False):
        super().__init__()
        self.sparsity = 0
        self.scores = nn.Parameter(torch.tensor(-4.0))
        self.f = torch.sigmoid
        self.threshold = self.f(self.scores)
        print(self.scores.requires_grad, self.threshold.requires_grad)

MoR()  ## True True
</code></pre>
<p>Therefore, your statement is wrong. Maybe you can check <a href=""https://www.youtube.com/watch?v=MswxJw-8PvE"" rel=""nofollow noreferrer"">this video</a> to know more about computational graph (BP) in PyTorch.</p>
<p>Hope that you find this answer would be helpful.</p>
","2024-06-03 02:14:19","0","Answer"
"78566798","","OOM : Memory Increase Issue in Model Training with Pytorch on WSL2","<p>I am experiencing an issue with memory increase and saturation while training a deep learning model using PyTorch in WSL2. While it doesn't happen on a Linux OS with the exact same code. The only thing that differs is the version of PyTorch, Cuda and the OS. I have tested this on two different setups:</p>
<p><strong>Setup 1:</strong></p>
<ul>
<li>OS: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35</li>
<li>GPU: RTX4090</li>
<li>Python: 3.12.2</li>
<li>PyTorch: 2.2.2</li>
<li>Cuda :</li>
</ul>
<pre><code>nvidia-cublas-cu12==12.1.3.1 nvidia-cuda-cupti-cu12==12.1.105 nvidia-cuda-nvrtc-cu12==12.1.105 nvidia-cuda-runtime-cu12==12.1.105 nvidia-cudnn-cu12==8.9.2.26 nvidia-cufft-cu12==11.0.2.54 nvidia-curand-cu12==10.3.2.106 nvidia-cusolver-cu12==11.4.5.107 nvidia-cusparse-cu12==12.1.0.106 nvidia-nccl-cu12==2.19.3 nvidia-nvjitlink-cu12==12.4.127 nvidia-nvtx-cu12==12.1.105
</code></pre>
<ul>
<li>Memory Utilization Profile: Memory increase during model training.</li>
</ul>
<p><a href=""https://i.sstatic.net/O9V5aTG1.png"" rel=""nofollow noreferrer"">setup1</a></p>
<p><strong>Setup 2:</strong></p>
<ul>
<li>OS: Linux-5.15.0-1058-aws-x86_64-with-glibc2.31</li>
<li>GPU: Tesla T4</li>
<li>Python: 3.10.14</li>
<li>PyTorch: 2.2.0</li>
<li>Cuda: unknown</li>
<li>Memory Utilization Profile: Memory does not increase during model training.</li>
</ul>
<p><a href=""https://i.sstatic.net/FyXOZhpV.png"" rel=""nofollow noreferrer"">setup2</a></p>
<p><strong>Issue</strong>:</p>
<p>With the exact same code, the memory utilization profile is different between the two setups. In Setup 1, memory saturates quickly during the training process, while in Setup 2, the memory usage remains stable and does not reach saturation.</p>
<p><strong>Questions:</strong></p>
<ul>
<li>Why might the memory utilization differ so significantly between these two setups despite using the same code?</li>
<li>Are there any known issues with specific versions of PyTorch or particular Linux distributions that could cause such behavior?</li>
<li>What can I do to ensure consistent memory utilization across different environments?</li>
</ul>
<p><strong>Steps Taken:</strong></p>
<ul>
<li>Verified that the same code is used on both setups.</li>
<li>Monitored memory usage using</li>
<li>Compared library/python/cuda versions used in both environments.</li>
</ul>
<p>Any insights or suggestions would be greatly appreciated. Thank you!</p>
","2024-06-02 14:56:31","0","Question"
"78566352","78566180","","<p>Please check your computer supporting the GPU or using gpu first.</p>
<p>then If you do not resolve the problem, <code>upgrade pytorch version newest</code>.</p>
<p>your pytorch version is 1.8.1, current latest pytorch version is 2.3.0.
<a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">pytorch homepage</a></p>
","2024-06-02 12:06:13","0","Answer"
"78566180","","Transformers: AutoModel from pretrained istantiation error","<p>I'm instantiating a CodeBert Model using AutoModel.fromPretrained.</p>
<pre><code>File &quot;/public.hpc/codeBertConcat/./codeBertConcatEvaluation.py&quot;, line 278, in &lt;module&gt;
    model = CodeBERTConcatenatedClass(num_classes=NUM_CLASSES).to(DEVICE)
  File &quot;/public.hpc/codeBertConcat/./codeBertConcatEvaluation.py&quot;, line 137, in __init__
    self.codebert = AutoModel.from_pretrained('microsoft/codebert-base', cache_dir=&quot;./cache2&quot;)
  File &quot;/public.hpc/codeBertConcat/venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py&quot;, line 493, in from_pretrained
    return model_class.from_pretrained(
  File &quot;/public.hpc/codeBertConcat/venv/lib/python3.9/site-packages/transformers/modeling_utils.py&quot;, line 2903, in from_pretrained
    ) = cls._load_pretrained_model(
  File &quot;/public.hpc/codeBertConcat/venv/lib/python3.9/site-packages/transformers/modeling_utils.py&quot;, line 3061, in _load_pretrained_model
    id_tensor = id_tensor_storage(tensor) if tensor.device != torch.device(&quot;meta&quot;) else id(tensor)
RuntimeError: Expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla, vulkan device type at start of device string: meta
</code></pre>
<p>Someone has a clue?
I'm using Transformers version 4.31.0 and PyTorch 1.8.1+cu111</p>
","2024-06-02 11:01:56","0","Question"
"78564203","78552651","","<p>Your directory contains only the files of the peft-adapter and the files required to load the tokenizer, but the base model weights are missing. I assume you have used the <a href=""https://huggingface.co/docs/peft/package_reference/peft_model#peft.PeftModel.save_pretrained"" rel=""nofollow noreferrer"">save_pretrained</a> method from peft. This method only saves the adapter weights and config (I use a smaller model for my answer and a different task type!):</p>
<pre class=""lang-py prettyprint-override""><code>from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from transformers import AutoModelForTokenClassification
from pathlib import Path

# ferguso/llama-8b-pcl-v3 in your case 
adapter_path = 'bla'
# meta-llama/Meta-Llama-3-8B in your case
base_model_id = &quot;distilbert/distilbert-base-uncased&quot;

peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, target_modules=&quot;all-linear&quot;)

# AutoModelForCausalLM in your case
model = AutoModelForTokenClassification.from_pretrained(base_model_id)
model = get_peft_model(model, peft_config)

model.save_pretrained(adapter_path)

print(*list(Path(adapter_path).iterdir()), sep='\n')
</code></pre>
<p>Output:</p>
<pre><code>bla/adapter_config.json
bla/README.md
bla/adapter_model.safetensors
</code></pre>
<p>To load your pretrained model successfully, you need to load this base_model weights as well and use the peft model class to load the adapter:</p>
<pre class=""lang-py prettyprint-override""><code>model = AutoModelForTokenClassification.from_pretrained(base_model_id)
model = PeftModel.from_pretrained(model, adapter_path)
</code></pre>
<p>You can also merge the adapter weights back with <a href=""https://huggingface.co/docs/peft/v0.11.0/en/package_reference/lora#peft.LoraModel.merge_and_unload"" rel=""nofollow noreferrer"">merge_and_unload</a> and save it:</p>
<pre><code>model.merge_and_unload().save_pretrained('bla2')
print(*list(Path('bla2').iterdir()), sep='\n')
</code></pre>
<p>Output:</p>
<pre><code>bla2/config.json
bla2/model.safetensors
</code></pre>
<p>This way you will be able to load the model without peft and only <code>transformers</code> as you tried in the example code of your question.</p>
","2024-06-01 15:31:59","1","Answer"
"78563740","","the output of sigmoid 's requires_grad is False, while the input 's requires_grad is True","<p>I have a module defined as follows:</p>
<pre><code>class MoR(nn.Module):
    def __init__(self, work=False):
        super().__init__()
        self.sparsity = 0
        self.scores = nn.Parameter(torch.tensor(-4.0))
        self.f = torch.sigmoid
        self.threshold = self.f(self.scores)

        self.min_loss = float('inf')
        self.sparsity_best = 0  # used for training

        self.sparsity_loss = 0
        self.recon_loss = 0

        self.work = work

        self.num = 0
        self.sparsity_avg = 0

    def forward(self, inputs: torch.Tensor):
        if self.work:
            # get mask
            mask = self.generate_mask(inputs)
            if torch.sum(torch.isnan(inputs)).bool():
                import pdb
                pdb.set_trace()
            self.get_sparsity(inputs)
            self.update_sparsity_avg()
            print(&quot;mask:&quot;, mask.requires_grad)
            print(&quot;scores:&quot;, self.scores.requires_grad)
            print(&quot;sparsity:&quot;, self.sparsity.requires_grad)
            print(&quot;threshold:&quot;, self.threshold.requires_grad)
            return inputs * mask
        else:
            return inputs
</code></pre>
<p>In this module, scores has requires_grad=True, but after computation, requires_grad for other parameters is False. Even the simplest threshold with requires_grad is also False. I don't understand why this is the case.</p>
<p>The threshold is computed as sigmoid(scores), which should definitely be differentiable.</p>
<p>This situation occurs when I insert this module into an LLM (Large Language Model).</p>
<p>However, when I apply this module to a small model, all training is normal.</p>
<p>Here is the small model:</p>
<pre><code>class LinearModel(nn.Module):
    def __init__(self):
        super(LinearModel, self).__init__()
        self.mor1 = MoR(work=True)
        self.relu1 = nn.ReLU()  # Add ReLU activation layer
        self.linear1 = nn.Linear(1, 1)  # A simple linear layer
        self.mor2 = MoR(work=True)
        self.relu2 = nn.ReLU()  # Add ReLU activation layer
        self.linear2 = nn.Linear(1, 1)

    def forward(self, x):
        x = self.mor1(x)
        x = self.linear1(x)
        x = self.relu1(x)  # Apply ReLU activation
        x = self.mor2(x)
        x = self.linear2(x)
        x = self.relu2(x)  # Apply ReLU activation
        return x

</code></pre>
<p>It's very strange. I don't understand if there's some underlying mechanism that automatically sets requires_grad to False for its outputs.</p>
<p>Note that I haven't found an answer in GPT or online forums, and possibilities such as whether no_grad() context is used, computation graph breaks, device transfers, or data usage are all non-existent, as shown in the code, it's just a sigmoid operation.</p>
<pre><code>        print(&quot;mask:&quot;, mask.requires_grad)
        print(&quot;scores:&quot;, self.scores.requires_grad)
        print(&quot;sparsity:&quot;, self.sparsity.requires_grad)
        print(&quot;threshold:&quot;, self.threshold.requires_grad)
</code></pre>
<p>the parameter's requires_grad is True</p>
","2024-06-01 12:15:48","0","Question"
"78562554","78559358","","<h2>Inspection</h2>
<p>Let's inspect the effect of the resnet50 layers on your image for 224x224 and 64x64 resolutions by adding a hook to print the output shapes of each block:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torchvision.models import resnet50, ResNet50_Weights


def add_hook(name, m):
    def forward_hook(module, input, output):
        print(f&quot;{name} output shape:&quot;, output.shape)

    m.register_forward_hook(forward_hook)


resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)

for name, layer in resnet.named_children():
    add_hook(name, layer)

for dim in [224, 64]:
    print(f&quot;Input dim: {dim}x{dim}&quot;)
    x = torch.randn(1, 3, dim, dim)
    _ = resnet(x)
    print()
</code></pre>
<p>which prints</p>
<pre><code>Input dim: 224x224
conv1 output shape: torch.Size([1, 64, 112, 112])
bn1 output shape: torch.Size([1, 64, 112, 112])
relu output shape: torch.Size([1, 64, 112, 112])
maxpool output shape: torch.Size([1, 64, 56, 56])
layer1 output shape: torch.Size([1, 256, 56, 56])
layer2 output shape: torch.Size([1, 512, 28, 28])
layer3 output shape: torch.Size([1, 1024, 14, 14])
layer4 output shape: torch.Size([1, 2048, 7, 7])
avgpool output shape: torch.Size([1, 2048, 1, 1])
fc output shape: torch.Size([1, 1000])

Input dim: 64x64
conv1 output shape: torch.Size([1, 64, 32, 32])
bn1 output shape: torch.Size([1, 64, 32, 32])
relu output shape: torch.Size([1, 64, 32, 32])
maxpool output shape: torch.Size([1, 64, 16, 16])
layer1 output shape: torch.Size([1, 256, 16, 16])
layer2 output shape: torch.Size([1, 512, 8, 8])
layer3 output shape: torch.Size([1, 1024, 4, 4])
layer4 output shape: torch.Size([1, 2048, 2, 2])
avgpool output shape: torch.Size([1, 2048, 1, 1])
fc output shape: torch.Size([1, 1000])
</code></pre>
<h2>Suggestions</h2>
<p>Looking at the above, there are a few possibilities:</p>
<p>Maybe you could argue that the network was trained on 224x224 images, so it's better to work with this resolution. Then you could</p>
<ul>
<li>Upsample to 224x224 as you've suggested.</li>
<li>Use trainable upsampling in the first few layers ie. with some additional convolutions. See <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html"" rel=""nofollow noreferrer"">here</a> and <a href=""https://stackoverflow.com/questions/73208923/using-transposed-convolution-for-upsampling-in-pytorch"">here</a>.</li>
</ul>
<p>Alternatively, maybe it doesn't matter, just use it like this with 64x64 images. Then you could</p>
<ul>
<li>Do nothing, maybe it's fine.</li>
<li>Remove the first few layers. You could argue that perhaps the first few layers have mostly learned to downsample anyway and aren't needed (as long as you plan to train more).</li>
<li>Probably the most common: remove the last few layers. You could argue that perhaps the last few layers are already sort of overfit to the task Resnet50 was trained on, so removing a few layers won't hurt much and you need to train more anyway.</li>
</ul>
<h2>A solution</h2>
<p>You probably already know a few ways to remove the last layers, but I'll show one approach similar to your notebook:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
from torchvision.models import resnet50, ResNet50_Weights


class CustomResNet50(nn.Module):
    def __init__(self, num_classes):
        super(CustomResNet50, self).__init__()

        # Load the pretrained ResNet50 model
        self.resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)

        # don't use layer4 and the final fully connected layer
        self.backbone = nn.Sequential(
            self.resnet.conv1,
            self.resnet.bn1,
            self.resnet.relu,
            self.resnet.maxpool,
            self.resnet.layer1,
            self.resnet.layer2,
            self.resnet.avgpool,
        )

        # save memory
        del self.resnet

        # Define the new fully connected layers, maybe with a bit more compute
        # since we removed a few resnet layers
        self.head = nn.Sequential(
            nn.Flatten(),
            nn.LazyLinear(num_classes),
            nn.Dropout(0.2),
            nn.BatchNorm1d(num_classes),
            nn.ReLU(),
            nn.Linear(num_classes, num_classes),
            nn.Softmax(dim=1),
        )

    def forward(self, x):
        x = self.backbone(x)
        return self.head(x)


num_classes = 200
dim = 64
model = CustomResNet50(num_classes=num_classes)

for name, layer in model.backbone.named_children():
    add_hook(name, layer)

for name, layer in model.head.named_children():
    add_hook(name, layer)

print(f&quot;Input dim: {dim}x{dim}&quot;)
x = torch.randn(2, 3, dim, dim)
_ = model(x)
</code></pre>
<p>which gives these dimensions:</p>
<pre><code>Input dim: 64x64
0 output shape: torch.Size([2, 64, 32, 32])
1 output shape: torch.Size([2, 64, 32, 32])
2 output shape: torch.Size([2, 64, 32, 32])
3 output shape: torch.Size([2, 64, 16, 16])
4 output shape: torch.Size([2, 256, 16, 16])
5 output shape: torch.Size([2, 512, 8, 8])
6 output shape: torch.Size([2, 512, 1, 1])
0 output shape: torch.Size([2, 512])
1 output shape: torch.Size([2, 200])
2 output shape: torch.Size([2, 200])
3 output shape: torch.Size([2, 200])
4 output shape: torch.Size([2, 200])
5 output shape: torch.Size([2, 200])
6 output shape: torch.Size([2, 200])
</code></pre>
<p>Note in my version:</p>
<ul>
<li>I used <code>nn.LazyLinear</code> so you don't have to know the output dim of the backbone.</li>
<li>I added some more layers and a non-linearity to the head, I generally think this is a good idea. It also shouldn't hurt training time much since my version removes a lot of resnet layers.</li>
<li>It's probably not a good idea to have batch norm right after the linear layer that is doing the final output. In general, you want the final linear layer to not have anything after it (except possibly softmax, if you need that for the task).</li>
</ul>
<p>None of these are really required for your question, but I thought I'd mention them.</p>
","2024-06-01 01:06:03","0","Answer"
"78562303","78559817","","<p>This doesn't work with just <code>torch==1.13.1+cu117</code> in your requirements.txt.</p>
<p>According to the <a href=""https://pytorch.org/get-started/previous-versions/"" rel=""nofollow noreferrer"">official documentation</a>, you would need to run e.g.</p>
<pre class=""lang-py prettyprint-override""><code>pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117
</code></pre>
<p>In particular, you need the <code>--extra-index-url</code> if you want to control the cuda version. So your requirements.txt needs to look like this for e.g. <code>cu117</code></p>
<pre><code>--extra-index-url https://download.pytorch.org/whl/cu117
torch==1.13.1+cu117
</code></pre>
<p>You can have more than one <code>--extra-index-url</code> in the requirements.txt, so this is how you can allow several versions to be considered:</p>
<pre><code>--extra-index-url https://download.pytorch.org/whl/cu116
--extra-index-url https://download.pytorch.org/whl/cu117
torch~=1.13.1
</code></pre>
","2024-05-31 22:16:47","0","Answer"
"78562234","78561925","","<p>The gradient is zero because you are slicing along the batch axis, not the time step/sequence axis.</p>
<p>By default, <code>nn.RNN</code> assumes the input will be of shape <code>(sequence_length, batch_size, input_size)</code>.</p>
<p>When you slice your output as <code>output = output[:, 1:, :]</code>, you are slicing the batch dimension. This means the first item in your batch is entirely excluded from the gradient computation - you are cutting out all time steps for the first batch item.</p>
<p>If you slice along the <code>sequence_length</code> axis, you can see that the first timestep has a gradient even when excluded from the loss calculation:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

rnn = nn.RNN(10, 20, 1)

x = torch.randn(5, 3, 10, requires_grad=True)
h0 = torch.randn(1, 3, 20, requires_grad=True)

output, hn = rnn(x, h0)

output = output[1:, :, :] # slice sequence_length axis

loss_fn = nn.CrossEntropyLoss()

# change (5,2) to (4,3) due to different slicing
target = torch.empty((4,3), dtype=torch.long).random_(20)

loss = loss_fn(output.reshape(-1, output.shape[2]), target.view(-1))

loss.backward()

# grad of first timestep 
print(x.grad[0,:,:])
</code></pre>
<p>Note that you can pass <code>batch_first=True</code> to <code>nn.RNN</code> if you want to structure your inputs as <code>(batch_size, sequence_length, input_size)</code></p>
","2024-05-31 21:50:51","0","Answer"
"78561925","","Why this snippet returns 0 gradients? Pytorch RNN understanding","<p>I'm trying to understand how slicing the output affects the gradients in a RNN. I built this simple script.</p>
<pre><code># Test the gradient of a RNN 

import torch
from torch import nn
from torch.autograd import Variable

rnn = nn.RNN(10, 20, 1)
input = Variable(torch.randn(5, 3, 10), requires_grad=True)
h0 = Variable(torch.randn(1, 3, 20), requires_grad=True)
output, hn = rnn(input, h0)

output = output[:, 1:, :]

loss_fn = nn.CrossEntropyLoss()

target = Variable(torch.empty((5,2), dtype=torch.long).random_(20), requires_grad=False)

loss = loss_fn(output.reshape(-1, output.shape[2]), target.view(-1))
loss.backward()

print(input.grad[:,0,:])

</code></pre>
<p>I would assume that the gradients are different from zero because, even if I have calculated the loss on a sliced input, the hidden states at subsequent time steps are still affected by the input at the first step. So why the gradients are zero?</p>
","2024-05-31 20:01:21","0","Question"
"78561154","78560979","","<p>You could use inspect to get the arguments that are passed to a function, then assemble them into a dictionary and pass that dictionary to mlflow.log_params().</p>
<pre><code>&gt;&gt;&gt; import inspect
&gt;&gt;&gt; 
&gt;&gt;&gt; def foo(a, b, c=3, *args, **kwargs):
...     _args, varargs, varkw, values = inspect.getargvalues(inspect.currentframe())
...     
...     d = {}
...     for a in _args:
...             d[a] = values[a]
...     
...     for i, a in enumerate(values[varargs]):
...             d[f&quot;arg_{i}&quot;] = a
...     
...     d.update(values[varkw])
...     
...     return d
... 
&gt;&gt;&gt; print(foo(1, 2, 3, 4, dog = &quot;cat&quot;))
{'a': 1, 'b': 2, 'c': 3, 'arg_0': 4, 'dog': 'cat'}
</code></pre>
","2024-05-31 16:20:44","1","Answer"
"78560979","","MLFlow - Is there a way to log all input parameters to a function?","<p>I have a function which is meant to be extendable, and would like to ensure that all input parameters keep being logged without the need to manually check/add them. Is there a way to do this automatically?</p>
<pre><code>def __init__(self, model_name: str, num_classes: int, device: str = 'cuda:0', learning_rate: float = 5e-5,
                 do_layer_freeze: bool = True, extra_class_layers: Optional[Union[int, list]] = None,
                 fine_tune_dropout_rate: float = 0):
</code></pre>
<p>For scikit learn models this happens naturally using the autologger but doesn't seem to with pytroch (Lightning). Currently, I am having to do it the manual way:</p>
<pre><code>mlflow.log_params({'model_name': model_name,
                           'num_classes': num_classes,
                           'learning_rate': learning_rate,
                           'do_layer_freeze': do_layer_freeze,
                           'extra_class_layers': extra_class_layers,
                           'fine_tune_dropout_rate': fine_tune_dropout_rate})
</code></pre>
","2024-05-31 15:36:45","2","Question"
"78559817","","How to state the compatible versions for torch and cuda in the requirements.txt file in a single line for Python?","<p>I would like to leverage the Python (3.8.2) libraries / frameworks, which are PyTorch and CUDA. I want to rely on the compatible versions stated by using ~= for both of these on a single line. The exact declaration (long form) in the requirements.txt file is as follows:</p>
<p><code>torch==1.13.1+cu117</code></p>
<p>However, I want to utilise compatible versions denoted by ~=, as shown below:</p>
<p><code>torch~=1.13+cu11</code></p>
<p>That is, the torch to be installed can be one of 1.13.1, 1.13.2, etc. (i.e. 1.13.*) whereas cuda might be one of 11.1, 11.2, ..., 11.7, and so on (i.e. 11.*). Is the example I have shown above the correct usage? If not, what is the proper usage for that version compatibility? Otherwise, can I not state the compatible versions for the two (dependent) libraries on the same line in the requirements.txt file?</p>
","2024-05-31 11:34:44","1","Question"
"78559358","","I want to train a resnet 50 model for an image clasification task, how do I modify the layers so that it accepts a 64x64 image?","<p>I want to train a resnet 50 model for an image clasification task. The default model requires 224x224 images, my dataset has 64x64 images and it seems wasteful to first upscale them then train on that data.</p>
<p>How should I modify the layers so that it not only accepts the 64x64 image, but retains a decent size by the final layer, when it reaches the fully conected classification layer?</p>
<p>This is my keggle notebook: <a href=""https://www.kaggle.com/code/cristiciorba/cv-resnet-lego-bricks"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/cristiciorba/cv-resnet-lego-bricks</a></p>
","2024-05-31 09:49:03","0","Question"
"78557838","78555925","","<p>Consider taking the similarity of the whole batch and masked out the similarity between nodes of different graphs. The easiest way to construct such mask is to first create a vector and then do a equal operator like this:</p>
<pre><code>batch_vector = torch.cat([[1]*N_1, [2]*N_2, ..., [B]*N_B], dim = 0) # shape (N_1 + ... + N_B,)

mask = batch_vector.unsqueeze(0) == batch_vector.unsqueeze(1) # shape (N_1 + ... + N_B, N_1 + ... + N_B)
</code></pre>
<p>This would not be memory efficient though, so that's a trade-off.</p>
","2024-05-31 02:14:10","0","Answer"
"78557825","78557717","","<p>You will need to pad it explicitly. Set the padding of convolution to 0 and use <code>x = nn.funtional.pad(x, (kernel_size - 2, 0), ...)</code>. You can read more <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html"" rel=""nofollow noreferrer"">here</a>.</p>
","2024-05-31 02:05:13","1","Answer"
"78557767","78477344","","<p>Albumentations indeed, treats images of the type float32 as having min value of 0 and max value of 1.</p>
<p>If your images are in <code>[-2, 2]</code> range, you may rescale your image to <code>[0, 1]</code> with:</p>
<pre class=""lang-py prettyprint-override""><code>def convert_to_01(image):
    return (image + 2) / 4    
</code></pre>
<p>and later rescale back with</p>
<pre class=""lang-py prettyprint-override""><code>def convert_from_01(image):
   return (image * 4) - 2
</code></pre>
<p>If possible, you may even convert images to <code>[0, 255]</code> uint8 type, as transforms in <code>uint8</code> happen much faster.</p>
<p>P.S. Thank you for the question, I will make it clear in the documentation of <a href=""https://albumentations.ai/"" rel=""nofollow noreferrer"">Albumentations</a> that float32 images are expected to be in <code>[0, 1]</code> range.</p>
","2024-05-31 01:36:37","0","Answer"
"78557717","","How to apply padding to only one side in a PyTorch Conv1d Layer?","<p>I am using PyTorch's CNN API to make a 1D convolutional layer. As an example, consider the below:</p>
<pre><code>CNN = nn.Conv1d(
  in_channels=1,
  out_channels=1,
  kernel_size=10,
  padding=?
)
</code></pre>
<p>Ignore the padding component for now.</p>
<p>So this layer will take in a sequence of <code>N</code> elements (say N=100). And uses a sliding convolutional window of size 10 to output a sequence of size <code>M</code>.</p>
<p>I want to add padding to only the left side of the sequence so the output is exactly of size <code>N-1</code>.</p>
<p>I found that the formula <code>left_hand_padding = kernel_size - 2</code> was able to output the correct padding that achieves this.</p>
<p>But I can't seem to find resources on how I can actually code this asymmetric padding. If I try to set the padding parameter in <code>nn.Conv1d()</code> to <code>kernel_size - 2</code>, both the right and left sides of the input are padded.</p>
<p>I also tried inputting a tuple <code>(kernel_size - 2, 0)</code>, (element 0 is the LHS padding, element 1 is the RHS padding). But this gave me the error:</p>
<blockquote>
<p>RuntimeError: expected padding to be a single integer value or a list of 1 values to match the convolution dimensions, but got padding=[8, 0]</p>
</blockquote>
<p>Is this asymmetric padding just not supported?</p>
","2024-05-31 01:05:04","1","Question"
"78555925","","Batch computation of similarity matrices of different sizes (Pytorch)","<p>I am computing node features of dimension D for a B different graphs where the graph i has N_i nodes, hence I have a batch representation as a tensor of dimension (N_1 + ... + N_B)xD. I want to compute the similarity of each node to each other within the same graph, if X_i is the node features of graph i (of dimension N_i x D), I need to compute X_i@X_i.T (dimension N_i x D) for each i.</p>
<p>Afterwards, I will compute a the cross entropy loss between X_i@X_i.T and Id_{N_i}.</p>
<p>I need to have a fast way to compute this loss for a batch (batch size of 100) in parallel and on the GPU (with Pytorch and Pytorch Geometric). I already tried to reduce the batch size to one and to pad the tensors so that the batch can be reshaped into B x N_max x F, however, the batch size of 1 is way to slow and padding the tensors doesn't work because torch_geometric expect contiguous tensors for the message passing part to compute the node features.</p>
","2024-05-30 15:26:51","0","Question"
"78555597","","Error loading state_dict for ViT-H-14 model in PyTorch","<p>I'm trying to train a Vision Transformer (ViT-H-14) model using a pre-trained weight file, but I'm encountering an error when loading the state_dict. The error occurs when I load the weights manually using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>def vit_h_14(weight_path=&quot;/content/vit_h_14_swag-80465313.pth&quot;):
    pretrained_vit = torchvision.models.vit_h_14()
    pretrained_vit.load_state_dict(torch.load(weight_path))

    for parameter in pretrained_vit.parameters():
        parameter.requires_grad = False
    return pretrained_vit
</code></pre>
<p>The error message I get is:</p>
<pre><code>RuntimeError: Error(s) in loading state_dict for VisionTransformer:
    size mismatch for encoder.pos_embedding: copying a param with shape torch.Size([1, 1370, 1280]) from checkpoint, the shape in current model is torch.Size([1, 257, 1280]).
</code></pre>
<p>However, when I load the pre-trained weights directly from the PyTorch API using the following code, the model trains successfully:</p>
<pre class=""lang-py prettyprint-override""><code>def vit_h_14():
    pretrained_vit_weights = torchvision.models.ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1
    pretrained_vit = torchvision.models.vit_h_14(weights=pretrained_vit_weights).to(device)

    for parameter in pretrained_vit.parameters():
        parameter.requires_grad = False
    return pretrained_vit
</code></pre>
<p><a href=""https://i.sstatic.net/tCn8qAqy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tCn8qAqy.png"" alt=""output training"" /></a>
I'm puzzled by this difference because other pre-trained ViT models can be trained using the same approach without any issues. Here's my training code for reference:</p>
<pre class=""lang-py prettyprint-override""><code>pretrained_transform = torchvision.models.ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1.transforms()

train_dataloader, val_dataloader, test_dataloader, class_names = create_dataloaders(
    train_dir=train_dir,
    val_dir=val_dir,
    transforms=pretrained_transform,
    test_dir=test_dir,
    batch_size=BATCH_SIZE
)

# setup model
model = vit_h_14(weight_path=weight_path)
model.heads = nn.Linear(
    in_features=model.heads.head.in_features,
    out_features=len(class_names)
).to(device)

# setup optimizer and loss function
optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
loss_fn = torch.nn.CrossEntropyLoss()

# train model
results = train(
    model=model,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    test_dataloader=test_dataloader,
    optimizer=optimizer,
    loss_fn=loss_fn,
    epochs=EPOCH,
    device=device
)

# save model
model_path = f&quot;../results/{arch_name}/model.pth&quot;
torch.save(model.state_dict(), model_path)
</code></pre>
<p>Can someone help me understand why I'm getting this error when loading the weights manually for the ViT-H-14 model, and what I can do to resolve it? I'd appreciate any insights or suggestions.
Thank you in advance!</p>
","2024-05-30 14:24:08","1","Question"
"78552651","","How to fix error `OSError: <model> does not appear to have a file named config.json.` when loading custom fine-tuned model?","<p><strong>Preface</strong></p>
<p>I am new to implementing the NLP model. I have successfully fine-tuned LLaMA 3-8B variants with QLORA and uploaded them to HuggingFace.</p>
<p>The directories are filled with these files:</p>
<pre><code>-  .gitattributes
- adapter_config.json
- adapter_model.safetensors
- special_tokens_map.json
- tokenizer.json
- tokenizer_config.json
- training_args.bin
</code></pre>
<p><strong>Implementation</strong></p>
<ol>
<li>I am trying to load this model through this:</li>
</ol>
<pre><code>model_id_1 = &quot;ferguso/llama-8b-pcl-v3&quot;

tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

model_1 = AutoModelForCausalLM.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
)
</code></pre>
<p>But it shows the error <code>OSError: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ferguso/llama-8b-pcl-v3/tree/main' for available files.</code></p>
<ol start=""2"">
<li>So then I am trying to load the config.json from the original model which is <code>meta-llama/Meta-Llama-3-8B</code>:</li>
</ol>
<pre><code>original_model = &quot;meta-llama/Meta-Llama-3-8B&quot;
model_id_1 = &quot;ferguso/llama-8b-pcl-v3&quot;

tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

original_config = AutoConfig.from_pretrained(original_model)
original_config.save_pretrained(model_id_1)

model_1 = AutoModelForCausalLM.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
    config = original_config
)
</code></pre>
<p>But still, it shows another error <code>OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3.</code></p>
<p><strong>Questions</strong></p>
<p>How to load the fine-tuned model properly?</p>
","2024-05-30 02:36:56","1","Question"
"78552627","78498375","","<p>A fairly new library <a href=""https://github.com/ml-explore/mlx"" rel=""nofollow noreferrer"">ml-explore</a> developed for Apple Silicon can do this. In this case it would be:</p>
<pre><code>res = mx.zeros(shape=K)
res.at[k].add(count * a[i] * b[j])
</code></pre>
<p>It's made be (almost) a drop-in replacement to NumPy or PyTorch and it is <strong>fast</strong>. The drawback is that it doesn't support <code>float64</code> so precision is not as good.</p>
","2024-05-30 02:25:22","1","Answer"
"78552233","78551454","","<p>This question is extremely broad, but I can give some information.</p>
<p>When you load a neural network, you are loading tensors of weights. These weights are typically loaded on CPU, then passed to GPU memory (HBM).</p>
<p>In addition to the weights, you have the model logic (ie the <code>forward</code> method of a pytorch model). Note that model logic is separate from the weights themselves.</p>
<p>The model logic decides what weights are executed when.</p>
<p>Say we have the model:</p>
<pre><code>class MyModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.layer1 = nn.Linear(32, 8)
        self.layer2 = nn.Linear(8, 1)

    def forward(self, x):
        x = self.layer1(x)
        x = torch.relu(x)
        x = self.layer2(x)
        return x
</code></pre>
<p>Our weights in the model's state dict are the weight/bias tensors of <code>layer1</code> and <code>layer2</code>. Our model execution logic is the <code>layer1/relu/layer2</code> code in the <code>forward</code> method.</p>
<p>When we run inference on the model, the <code>forward</code> method determines the order of operations.</p>
<p>Each layer has a corresponding GPU kernel. The kernel decides how the input weights/activations are broken down into grids/blocks and distributed among the GPU SMs.</p>
<p>Typically the GPU executes one layer at a time, using as much compute as possible for that layer.</p>
<p>With the model above, it would look something like this:</p>
<ol>
<li>Given some input <code>x</code> and model weights in HBM</li>
<li>Move <code>x</code> and weights for <code>layer1</code> into SRAM</li>
<li>Execute GPU kernel for <code>torch.nn.Linear</code> with <code>x</code> and <code>layer1</code> weights</li>
<li>Return result back to HBM</li>
<li>Move <code>x</code> (now the result of <code>layer1</code>) into SRAM</li>
<li>Execute GPU kernel for <code>torch.relu</code> on <code>x</code></li>
<li>Move result back to HBM</li>
<li>Move <code>x</code> (now the result of the relu operation) and weights for <code>layer2</code> into SRAM</li>
<li>Execute GPU kernel for <code>torch.nn.Linear</code> with <code>x</code> and <code>layer2</code> weights</li>
<li>Move result back to HBM</li>
</ol>
<p>For the above, each kernel execution would distribute inputs/weights to different grids/blocks for execution. The exact distribution depends on the logic of the kernel itself.</p>
<p>This gets more complicated with potentials for kernel fusing (ie on the relu) and whatnot, but this is the basic idea.</p>
<p>You can use the <a href=""https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/"" rel=""nofollow noreferrer"">pytorch profiler</a> to look at what kernels are being executed when during the forward pass.</p>
","2024-05-29 22:42:30","1","Answer"
"78551454","","How a neural network is mapped to a GPU?","<p>I want to understand when a GPU executes a neural network, how the operations are mapped to the GPU's hardware resources. I am familiar with the architecture of GPUs (especially NVIDIA) and I generally know how an NN is executed by them, but I do not know how to get to detailed and fine-grain scheduling of operations to the hardware resources and how the cores execute them. I am wondering if there is any tool or a set of tools for that.</p>
<p>To be more specific, let's imagine that I have a pre-trained neural network in pytorch and want to run it on an NVIDIA 3090 GPU. How can I get the detailed scheduling of the operations (either at the MAC operations or neurons/channels/layers of the NN) to corresponding hardware resources via SMs or threads?</p>
","2024-05-29 18:49:47","0","Question"
"78551350","78550214","","<p>You can use <code>Union</code> that makes the variable to be either a <code>np.ndarray</code> or a <code>torch.Tensor</code></p>
<pre><code>from typing import Union
MyData: Union[np.ndarray, torch.Tensor]
</code></pre>
","2024-05-29 18:22:10","0","Answer"
"78550716","78550214","","<p>You need to use a <em>type</em> for type hinting, not <code>torch.float</code>. If you'd use <code>torch.dtype</code> (<code>torch.float</code> is of type <code>torch.dtype</code>), it'd work but you don't want that presumably. As you want a numpy array or torch tensor, you'd want to use <code>torch.Tensor</code> (note the capital <code>T</code>) i.e.:</p>
<pre><code>MyData: np.ndarray | torch.Tensor
</code></pre>
<p>FWIW, <code>torch.tensor</code> is a function, not a type. All <code>torch</code> tensors are of type <code>torch.Tensor</code>.</p>
","2024-05-29 16:00:15","0","Answer"
"78550214","","Specify np.ndarray and torch.tensor as two options for a dtype","<p>I would like to specify two options for a datatype for a variable. Something like this:</p>
<pre><code>class MyDataClass:

    MyData: np.ndarray | torch.float

    def __init__(self, as_torch):
        MyData = np.arange(1,10)
        if as_torch:
            MyData = torch.from_numpy(MyData)

</code></pre>
<p>... but this throws an error:
<code>TypeError: unsupported operand type(s) for |: 'type' and 'torch.dtype'</code></p>
<p>I think it would be possible to just not assign a datatype to MyData, but I was wondering if there's  a better solution.
What is the best practice to work around this error or solve this? Thanks in advance!</p>
<p>Tried:
np.ndarray | torch.tensor</p>
","2024-05-29 14:25:49","0","Question"
"78549145","78547320","","<p>As it comes from the comments, you are using an old version of <strong>Ultralytics==8.0.0</strong>. It in fact returns the result as a list of <code>torch.Tensor</code> object instead of <code>ultralytics.engine.results.Results</code> object, and exactly the last one has such parameters like <em>boxes, masks, keypoints, probs, obb</em>. The documentation complies with the latest framework version, <strong>8.2.24</strong> for now, and 8.0.0 is from January 2023.</p>
<p>The easiest way to solve your problem is to <strong>upgrade the Ultralytics version</strong> to the latest one, so you will get all the results parameters described in the documentation.</p>
<p>If circumstances prevent you from this update, you will need some data postprocessing with the <strong>understanding of the returned results format</strong> from the 8.0.0 version.</p>
<p>OBJECT DETECTION task results, version==8.0.0</p>
<pre class=""lang-py prettyprint-override""><code># for 3 detected objects

[tensor([[2.89000e+02, 7.10000e+01, 1.44000e+03, 5.07000e+02, 8.91113e-01, 2.00000e+00],
         [1.26700e+03, 6.00000e+01, 1.68200e+03, 3.19000e+02, 8.31055e-01, 2.00000e+00],
         [6.96000e+02, 0.00000e+00, 1.32200e+03, 1.31000e+02, 2.56836e-01, 7.00000e+00]], device='cuda:0')]

# where every array stores 6 values, first 4 are in pixels:
[x_centre, y_centre, box_width, box_height, confidence, class_id]

# for easy manipulation you can run results[0].tolist() and will get the following format:
[[289.0, 71.0, 1440.0, 507.0, 0.89111328125, 2.0],
 [1267.0, 60.0, 1682.0, 319.0, 0.8310546875, 2.0],
 [696.0, 0.0, 1322.0, 131.0, 0.2568359375, 7.0]]
</code></pre>
<p>OBJECT SEGMENTATION task results, version==8.0.0. Will be the same as for detection, but adds the second torch.Tensor with the segmentation masks for every object.</p>
<pre class=""lang-py prettyprint-override""><code># for 3 detected objects

[[tensor([[1.23000e+02, 8.90000e+01, 4.21000e+02, 2.21000e+02, 2.55216e-01, 7.00000e+00],
          [1.26700e+03, 5.80000e+01, 1.68100e+03, 3.17000e+02, 8.04158e-01, 2.00000e+00],
          [2.70000e+02, 7.70000e+01, 1.46000e+03, 4.98000e+02, 8.19106e-01, 2.00000e+00]], device='cuda:0'),
  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]],
  
          [[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]],
  
          [[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]]

</code></pre>
<p>Knowing the data format you receive from this old Ultralytics version, you can easily reach them and translate them to the format you need. But upgrading Ultralytics to the latest version is still the best way to get the most effective usage of this framework.</p>
<p>Install Ultralytics: <a href=""https://docs.ultralytics.com/quickstart/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/quickstart/</a></p>
","2024-05-29 11:15:12","1","Answer"
"78548088","78537817","","<p>I have met the same issue and I found <a href=""https://forum.linuxfoundation.org/discussion/865302/self-solved-chap8-lab4-dataset-encountered-importerror-about-dill-available"" rel=""nofollow noreferrer"">this site</a>.
Following the <a href=""https://github.com/pytorch/pytorch/pull/122616/files"" rel=""nofollow noreferrer"">GitHub issue page</a> I modified the <code>torch/utils/data/datapipes/utils/common.py</code> by adding a new line:
<code>DILL_AVAILABLE = dill_available()</code>. Then the issue was solved.</p>
","2024-05-29 07:51:37","0","Answer"
"78547320","","YoloV8 results have no 'box', 'max' properties in it","<p>I've trained a YOLOV8 model to identify objects in an intersection (ie cars, roads etc).
It is working OK and I can get the output as an image with the objects of interested segmented.</p>
<p>However, what I need to do is to capture the raw geometries (polygons) so I can save them on a txt file later on.</p>
<p>I tried what Ive found in the documentation (<a href=""https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode</a>) however the returning object is not the same as the documentation says.</p>
<p>In fact, the result is a list of tensorflow numbers:</p>
<p><a href=""https://i.sstatic.net/Fy4lI1sV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fy4lI1sV.png"" alt=""enter image description here"" /></a></p>
<p>Here's my code:</p>
<pre><code>import argparse
import cv2
import numpy as np
from pathlib import Path
from ultralytics.yolo.engine.model import YOLO    
    
# Parse command line arguments
parser = argparse.ArgumentParser()
parser.add_argument('--source', type=str, required=True, help='Source image directory or file')
parser.add_argument('--output', type=str, default='output', help='Output directory')
args = parser.parse_args()

# Create output directory if it doesn't exist
Path(args.output).mkdir(parents=True, exist_ok=True)

# Model path
model_path = r'C:\\_Projects\\best_100img.pt'

# Load your model directly
model = YOLO(model_path)
model.fuse()

# Load image(s)
if Path(args.source).is_dir():
    image_paths = list(Path(args.source).rglob('*.tiff'))
else:
    image_paths = [args.source]

# Process each image
for image_path in image_paths:
    img = cv2.imread(str(image_path))
    if img is None:
        continue

    # Perform inference
    predictions = model.predict(image_path, save=True, save_txt=True)
    
print(&quot;Processing complete.&quot;)
</code></pre>
<p>Here's the problem: the return object (predictions variable) has no <strong>boxes, masks, keypoints</strong> and etc.</p>
<p>I guess my questions are:</p>
<ul>
<li>Why the result is so different from the documentation?</li>
<li>Is there a conversion step?</li>
</ul>
","2024-05-29 04:32:01","0","Question"
"78547234","78542564","","<p>Just modified @Laszlo Hunyadi answer to better match the required output of the question.</p>
<pre><code>import torch

t = torch.tensor([[61078, 51477, 28492, 4290, 86920,  2216],
              [26799, 76684, 23785, 18202, 14552, 98301]])
a = torch.tensor([61078, 23785, 2216])
mask = (t[..., None] == a).any(-1).any(0)

result = t[:,*torch.where(mask)]
print(result)
</code></pre>
<p>Output:</p>
<pre><code>tensor([[61078, 28492,  2216],
        [26799, 23785, 98301]])
</code></pre>
","2024-05-29 03:51:38","1","Answer"
"78547075","","Loading pre-trained Transformer model with AddedTokens using from_pretrained","<p>I have pre-trained a <code>&quot;meta-llama/Llama-2-7b-chat-hf&quot;</code> model using the <code>transformers</code> library. Since my model uses additional tokens, I added them to the tokeniser before training and fine-tuned the &quot;embed_tokens&quot; module of the network. My training code looked like this:</p>
<pre><code>  tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,trust_remote_code=True, token=hf_token)
  tokenizer.add_special_tokens({ &quot;additional_special_tokens&quot;:[AddedToken(&quot;&lt;|move|&gt;&quot;),
                                                              AddedToken(&quot;&lt;|endmove|&gt;&quot;),
                                                              AddedToken(&quot;&lt;|end|&gt;&quot;)]})

  model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map,
    token=hf_token
  )
  model.resize_token_embeddings(len(tokenizer))
  peft_config = LoraConfig(
      lora_alpha=lora_alpha,
      lora_dropout=lora_dropout,
      r=lora_r,
      bias=&quot;none&quot;,
      modules_to_save= [&quot;embed_tokens&quot;, &quot;lm_head&quot;],
      task_type=&quot;CAUSAL_LM&quot;,
  )
</code></pre>
<p>The model trained and saved successfully. However, when trying to load it using <code>AutoModelForCausalLM.from_pretrained</code>, I get the following error:</p>
<pre><code>Error(s) in loading state_dict for LlamaForCausalLM:
size mismatch for model.embed_tokens.modules_to_save.default.weight: copying a param with shape torch.Size([32003, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096]).
size mismatch for lm_head.modules_to_save.default.weight: copying a param with shape torch.Size([32003, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096])
</code></pre>
<p>I appreciate the error is due to the fact that the fine-tuned model has three additional tokens and that causes a mismatch, but how should I load a pre-trained model with a different input shape like mine?</p>
<p>I looked into the transformers API docs for a way to load models with AddedTokens, but I couldn't find anything. I read a blog post mentioning that passing <code>ignore_mismatched_sizes=True</code> to the from_pretrained function would solve the issue, but it didn't work for me.</p>
<p>EDIT: To load my local model, I use the same <code>from_pretrained</code> function that I use to load the meta-llama model from huggingface:</p>
<pre><code>`model = AutoModelForCausalLM.from_pretrained(
    local_model_folder,
    quantization_config=bnb_config,
    device_map=device_map,
    token=hf_token
  )
</code></pre>
<p>This works correctly when loading pre-trained models with no changes to the vocabulary size.</p>
","2024-05-29 02:30:25","2","Question"
"78545500","78530360","","<p><a href=""https://github.com/aws/sagemaker-training-toolkit"" rel=""nofollow noreferrer"">SageMaker Training Toolkit</a> has the implementation to call torchrun command within the SageMaker's python sdk classes.</p>
<p>You can refer to the <a href=""https://github.com/aws/sagemaker-training-toolkit/blob/b7c660b294f882601a736d890db2445dd9d3a638/src/sagemaker_training/torch_distributed.py#L82"" rel=""nofollow noreferrer"">&quot;TorchDistributedRunner._create_command()&quot;</a> to see how it constructs the torchrun command and its arguments.</p>
<p>Please also refer to PyTorch document how to use the torchrun command.
<a href=""https://pytorch.org/docs/stable/elastic/run.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/elastic/run.html</a></p>
","2024-05-28 16:54:26","0","Answer"
"78542928","78542564","","<p>Use <code>eq</code> from PyTorch with the <code>any</code> function to create a mask that matches the values in your tensor to the values in your list, then use the mask to index into your tensor:</p>
<pre><code>import torch

t = torch.tensor([[61078, 51477, 28492, 4290, 86920,  2216],
              [26799, 76684, 23785, 18202, 14552, 98301]])
a = torch.tensor([61078, 23785, 2216])
mask = (t[..., None] == a).any(-1)
result = t * mask.long()
print(result)
</code></pre>
<p>Output:</p>
<pre><code>tensor([[61078,     0, 28492,     0,     0,  2216],
        [    0,     0, 23785,     0,     0,     0]])
</code></pre>
","2024-05-28 08:44:59","1","Answer"
"78542564","","(Pytorch) How to get the tensors by matching values in other list?","<p>I want to select the values in a two-dimensional tensor by matching them to the list of values.</p>
<pre class=""lang-py prettyprint-override""><code>example
#tensor([[61078, 51477, 28492, 4290, 86920,  2216],
#        [26799, 76684, 23785, 18202, 14552, 98301]])

a 
# Index([61078, 23785, 2216], dtype='int64', length=3)

result
#tensor([[61078, 28492, 2216],
#        [26799, 23785, 98301]])
</code></pre>
<p>I tried using <code>torch.where</code> and <code>isin</code>, but they always returned errors.
How can I solve the problem?</p>
","2024-05-28 07:37:49","1","Question"
"78542429","","Running out of RAM when finetuning model","<p>I am currently trying to finetune <code>Wav2Vec2</code> model from: <a href=""https://huggingface.co/dima806/bird_sounds_classification"" rel=""nofollow noreferrer"">https://huggingface.co/dima806/bird_sounds_classification</a>. But my RAM utilisation is running over the free tier on Google Colab.</p>
<p>The following is my code:</p>
<pre><code>from transformers import TrainingArguments, Trainer

# Load model with ignore_mismatched_sizes=True
model = Wav2Vec2ForSequenceClassification.from_pretrained(
    &quot;dima806/bird_sounds_classification&quot;,
    num_labels=len(label2id),
    ignore_mismatched_sizes=True
)

# Set up training with gradient accumulation
batch_size = 1  # Reduce batch size to manage memory
accumulation_steps = 4  # Accumulate gradients over 4 steps

training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=accumulation_steps,  # Gradient accumulation
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=feature_extractor,
)

# Train the model
trainer.train()
</code></pre>
<p>What could be the reasons the RAM is going past 12.7GB? My dataset only contains 20 items. How can I address this issue?</p>
","2024-05-28 07:10:21","1","Question"
"78537817","","ImportError: cannot import name 'DILL_AVAILABLE'","<p>I want to work with IMDB datasets. Trying to load using following command:</p>
<pre><code>from torchtext.datasets import IMDB
train_iter = IMDB(root='~/datasets', split='train')
</code></pre>
<p>I am getting following error:</p>
<pre><code>ImportError: cannot import name 'DILL_AVAILABLE' from 'torch.utils.data.datapipes.utils.common' (/home/user/env_p3.10.12_ml/lib/python3.10/site-packages/torch/utils/data/datapipes/utils/common.py)
</code></pre>
<p><strong>How to solve it?</strong></p>
","2024-05-27 07:55:39","2","Question"
"78534061","78534053","","<p>Seems like your pip command is using python3.10, and you've also configured pyenv to use Python 3.13... You can use <code>python -m pip install torch</code> to ensure packages are installed into the correct environment (however, you may not want an alpha release of Python).</p>
<p>Otherwise, you'll want to adjust how pyenv is set on the PATH</p>
","2024-05-26 01:18:06","0","Answer"
"78534053","","Torch cant be imported","<p>Can someone help me? I'm trying to run a training script for a module, and the first error I got in the command prompt was this:</p>
<p><code>ModuleNotFoundError: No module named ‘torch’</code></p>
<p>But I already have it installed. Here are the commands O ran and the outputs I got:</p>
<pre><code>C:\Users\moham\Desktop\mohamad\uni\thesis&gt;pip install torch
Requirement already satisfied: torch in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (2.2.2)
Requirement already satisfied: filelock in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from torch) (3.13.1)
Requirement already satisfied: typing-extensions&gt;=4.8.0 in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from torch) (4.9.0)
Requirement already satisfied: sympy in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from torch) (1.12)
Requirement already satisfied: networkx in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from torch) (3.1.3)
Requirement already satisfied: fsspec in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from torch) (2024.2.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from jinja2-&gt;torch) (2.1.5)
Requirement already satisfied: mpmath&gt;=0.19 in c:\users\moham\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (from sympy-&gt;torch) (1.3.0)

C:\Users\moham\Desktop\mohamad\uni\thesis&gt;where python
C:\Users\moham.pyenv\pyenv-win\shims\python
C:\Users\moham.pyenv\pyenv-win\shims\python.bat
C:\Users\moham\AppData\Local\Microsoft\WindowsApps\python.exe

C:\Users\moham\Desktop\mohamad\uni\thesis&gt;python -c “import site; print(site.getsitepackages())”
[‘C:\Users\moham\.pyenv\\pyenv-win\versions\3.13.0a6’, ‘C:\Users\moham\.pyenv\pyenv-win\versions\3.13.0a6\Lib\site-packages’]

C:\Users\moham\Desktop\mohamad\uni\thesis&gt;pip install torch
C:\Users\moham\Desktop\mohamad\uni\thesis&gt;where python
C:\Users\moham\Desktop\mohamad\uni\thesis&gt;python -c “import site; print(site.getsitepackages())”
</code></pre>
","2024-05-26 01:05:19","0","Question"
"78530861","78530486","","<p>It turned out that I was directly using my logits in place of the new probability when calculating the probability ratio.</p>
<p>So instead of:</p>
<pre><code>new_probs = actor(states).gather(1, actions.unsqueeze(-1)).squeeze()
</code></pre>
<p>It should have been:</p>
<pre><code>new_probs = F.softmax(actor(states)).gather(1, actions.unsqueeze(-1)).squeeze()
</code></pre>
","2024-05-24 22:11:47","0","Answer"
"78530754","78530486","","<p>In the function of <code>state_action</code>, instead of getting the probability itself, it is better to get the logarithm of the probability. So, your function should be:</p>
<pre><code>def select_action(state):
    state = torch.from_numpy(state).float().to(device)
    mean = F.softmax(actor(state), dim=-1)       # This is the mean value of the output
    m = Categorical(mean)            # This is our distribution based on mean
    action = m.sample()
    probs = m.log_prob(action)       # This is the log_prob (I didn't change the name)
    state_value = critic(state)
    actor.saved_actions.append((probs.detach(), state_value, state, action.detach()))   # changed to probs.detach()
    return action.item()
</code></pre>
<p>Also, in the <code>finish_episode</code> function, you need to change the equation of the ratio:</p>
<pre><code>ratios = torch.exp(new_probs - old_probs)
</code></pre>
<p>Also, for the problem of <code>num_epochs</code>, you are using the old state values for the policy update. You need to use the critic for update. Your code is modified as follow (check the comments):</p>
<pre><code>def finish_episode():
    # Calculating losses and performing backprop
    R = 0
    saved_actions = actor.saved_actions
    returns = []
    epsilon = 0.2
    num_epochs = 3

    for r in actor.rewards[::-1]:
        R = r + 0.99 * R # Gamma is 0.99
        returns.insert(0, R)
    returns = torch.tensor(returns, device=device)
    returns = (returns - returns.mean()) / (returns.std() + eps)

    old_probs, state_values, states, actions = zip(*saved_actions)

    old_probs = torch.stack(old_probs).to(device)
    state_values = torch.stack(state_values).to(device)
    states = torch.stack(states).to(device)
    actions = torch.stack(actions).to(device)
    values = critic(states).to(device).detach()    # value function is added and detached
    
    advantages = returns - values.squeeze()      # value function from critic added to advantage

    for epoch in range(num_epochs):

        values = critic(states).to(device)   # also, you need to pudate the critic in the loop
        new_probs = actor(states).gather(1, actions.unsqueeze(-1)).squeeze()

        # ratios = new_probs / old_probs
        ratios = torch.exp(new_probs - old_probs)

        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages

        actor_loss = -torch.min(surr1, surr2).mean()
        # actor_loss = -surr1.mean()
        
        actor_optimizer.zero_grad()
        actor_loss.backward(retain_graph=True)
        actor_optimizer.step()

        # if epoch == num_epochs - 1:
        critic_loss = F.smooth_l1_loss(values.squeeze(), returns)       # it is changed from state_values to values
        
        critic_optimizer.zero_grad()
        critic_loss.backward(retain_graph=True)
        critic_optimizer.step()

    del actor.rewards[:]
    del actor.saved_actions[:]
</code></pre>
<p>I changed the learning rates:</p>
<pre><code>actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)
critic_optimizer = optim.Adam(critic.parameters(), lr=1e-5)
</code></pre>
<p>I ran the code and got following results (just give it some time to run):</p>
<pre><code>Episode 1330 Reward: 430.00 Average reward: 420.69
Episode 1340 Reward: 500.00 Average reward: 428.80
Episode 1350 Reward: 500.00 Average reward: 436.10
Episode 1360 Reward: 458.00 Average reward: 453.50
Episode 1370 Reward: 500.00 Average reward: 465.66
Episode 1380 Reward: 498.00 Average reward: 462.26
Episode 1390 Reward: 500.00 Average reward: 475.03
Solved, running reward is now 475.02882655892296 and the last episode runs to 500 timesteps
</code></pre>
<p>I am pretty sure, you can get better results after some hyperparameter tunning</p>
","2024-05-24 21:26:47","0","Answer"
"78530486","","PPO only working with a single epoch and unclipped loss","<p>I'm attempting to implement PPO to beat cartpole-v2, I manage to get it working if I keep things as A2C (That is, without clipped loss and a single epoch), when I use clipped loss and more than one epoch it doesn't learn, have been trying to find the issue in my implementation for about a week but I can't find what's wrong.</p>
<p><a href=""https://github.com/yanis-falaki/RL-Gym/blob/main/cartpole-ppo.ipynb"" rel=""nofollow noreferrer"">Full Code</a></p>
<p>Here is the function responsible for optimizing:</p>
<pre class=""lang-py prettyprint-override""><code>def finish_episode():
    # Calculating losses and performing backprop
    R = 0
    saved_actions = actor.saved_actions
    returns = []
    epsilon = 0.3
    num_epochs = 1 # When num_epochs is greater than one my network won't learn

    for r in actor.rewards[::-1]:
        R = r + 0.99 * R # Gamma is 0.99
        returns.insert(0, R)
    returns = torch.tensor(returns, device=device)
    returns = (returns - returns.mean()) / (returns.std() + eps)

    old_probs, state_values, states, actions = zip(*saved_actions)

    old_probs = torch.stack(old_probs).to(device)
    state_values = torch.stack(state_values).to(device)
    states = torch.stack(states).to(device)
    actions = torch.stack(actions).to(device)

    advantages = returns - state_values.squeeze()

    for epoch in range(num_epochs):

        new_probs = actor(states).gather(1, actions.unsqueeze(-1)).squeeze()

        ratios = new_probs / old_probs

        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages

        #actor_loss = -torch.min(surr1, surr2).mean() # When using this (clipped) loss my network won't learn
        actor_loss = -surr1.mean()

        actor_optimizer.zero_grad()
        actor_loss.backward(retain_graph=True)
        actor_optimizer.step()

        if epoch == num_epochs - 1:
            critic_loss = F.smooth_l1_loss(state_values.squeeze(), returns)
            
            critic_optimizer.zero_grad()
            critic_loss.backward(retain_graph=False)
            critic_optimizer.step()

    del actor.rewards[:]
    del actor.saved_actions[:]
</code></pre>
<p>Tried different hyperparameters, using gae as opposed to full monte carlo retuns/advantages, in combing through my code I can't see what's wrong.</p>
","2024-05-24 20:05:01","0","Question"
"78530360","","Using torchrun with AWS sagemaker estimator on multi-GPU node","<p>I would like to run a training job ml.p4d.24xlarge machine on AWS SageMaker. I ran into a similar issue described <a href=""https://github.com/huggingface/transformers/issues/28916"" rel=""nofollow noreferrer"">here</a> with significant slowdowns in training time. I understand now that I should run it with torchrun. My constraints are that I don't want to use the HuggingFace or PyTorch estimators from SageMaker (for customizability and to properly understand the stack).</p>
<p>Currently, the entrypoint to my container is set as such in my Dockerfile:</p>
<p><code>ENTRYPOINT [&quot;python3&quot;, &quot;/opt/program/entrypoint.py&quot;]</code></p>
<p>How should I change it, and can I change it to use torchrun instead? Is it just a matter of setting:</p>
<p><code>ENTRYPOINT [&quot;torchrun --nproc_per_node 8&quot;, &quot;/opt/program/entrypoint.py&quot;]</code></p>
","2024-05-24 19:25:29","1","Question"
"78524461","78328401","","<p>Using <code>pip install</code> instead of <code>conda install</code> solved the problem for me:</p>
<pre><code>pip install torch==x.x+cu11x torchvision==x.x.0+cu11x torchaudio==x.x --extra-index-url https://download.pytorch.org/whl/cu11x
</code></pre>
","2024-05-23 16:26:23","0","Answer"
"78523154","","MPS device and Pytorch","<p>I want to rum mpnn with a lightning trainer on my mac.
These are my trainer settings:</p>
<pre><code>trainer = pl.Trainer(
    logger=False,
    enable_checkpointing=True, #
    enable_progress_bar=True,
    accelerator=&quot;mps&quot;,
    devices= 1,
    max_epochs=20, # number of epochs to train for
)
</code></pre>
<p>I also changed my environment varibale as followed:</p>
<pre><code>import os
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
</code></pre>
<p>still when I start my training:</p>
<pre><code>
trainer.fit(mpnn, train_loader)
</code></pre>
<p>I get following error message:</p>
<p>NotImplementedError: The operator 'aten::scatter_reduce.two_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on <a href=""https://github.com/pytorch/pytorch/issues/77764"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/issues/77764</a>. As a temporary fix, you can set the environment variable <code>PYTORCH_ENABLE_MPS_FALLBACK=1</code> to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...</p>
<p>I tried following steps to fix the problem:</p>
<p>changed setup in trainer from accelerator ='auto', to accellerator = 'mps'</p>
<p>changed the environment variable as suggested in the error.</p>
<p>tried:</p>
<pre><code>
if torch.backends.mps.is_available():
    device = torch.device('mps')
else:
    device = torch.device('cpu')
</code></pre>
","2024-05-23 12:34:08","1","Question"
"78523152","78523114","","<p>This is because <code>input_tensor</code> is a class probability. And if the target class is <code>1</code> then <code>input_tensor</code> should have length of at least two (probability of the class 0 and probability of the class 1)</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

predicted_class_probs = torch.tensor([[2.0]])
target_tensor = torch.tensor([0], dtype=torch.long)
loss_function = nn.CrossEntropyLoss()
loss = loss_function(predicted_class_probs, target_tensor)
print(loss)


predicted_class_probs = torch.tensor([[2.0, 3.3]])
target_tensor = torch.tensor([1], dtype=torch.long)
loss_function = nn.CrossEntropyLoss()
loss = loss_function(predicted_class_probs, target_tensor)
print(loss)
</code></pre>
","2024-05-23 12:33:29","2","Answer"
"78523114","","Why I am getting IndexError","<p>Why following code:</p>
<pre><code>import torch
import torch.nn as nn

input_tensor = torch.tensor([[2.0]])
target_tensor = torch.tensor([0], dtype=torch.long)
loss_function = nn.CrossEntropyLoss()
loss = loss_function(input_tensor, target_tensor)
print(loss)


input_tensor = torch.tensor([[2.0]])
target_tensor = torch.tensor([1], dtype=torch.long)
loss_function = nn.CrossEntropyLoss()
loss = loss_function(input_tensor, target_tensor)
print(loss)
</code></pre>
<p>showing following error:</p>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[214], line 14
     12 target_tensor = torch.tensor([1], dtype=torch.long)
     13 loss_function = nn.CrossEntropyLoss()
---&gt; 14 loss = loss_function(input_tensor, target_tensor)
     15 print(loss)

File ~/jupyter-env-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1106 # If we don't have any hooks, we want to skip the rest of the logic in
   1107 # this function, and just call forward.
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
   1112 full_backward_hooks, non_full_backward_hooks = [], []

File ~/jupyter-env-3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:1163, in CrossEntropyLoss.forward(self, input, target)
   1162 def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
-&gt; 1163     return F.cross_entropy(input, target, weight=self.weight,
   1164                            ignore_index=self.ignore_index, reduction=self.reduction,
   1165                            label_smoothing=self.label_smoothing)

File ~/jupyter-env-3.10/lib/python3.10/site-packages/torch/nn/functional.py:2996, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   2994 if size_average is not None or reduce is not None:
   2995     reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2996 return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)

IndexError: Target 1 is out of bounds.
</code></pre>
","2024-05-23 12:27:10","0","Question"
"78522306","78521304","","<p>Thanks everyone for your answers
well , my problem is solved where I just have to delete and reinstall the venv folder i.e; virtual environment folder with the activation script.
please go through the below process in case you are encountering the same problem</p>
<ol>
<li><code>rmdir /s &lt;venv_directory&gt;</code></li>
<li><code>python -m venv &lt;venv_directory&gt;</code></li>
<li><code>&lt;venv_directory&gt;\Scripts\activate</code></li>
</ol>
<p>Clear explanation - Open the cmd and navigate to the script folder
Delete and reinstall it and run the activate command.</p>
","2024-05-23 09:58:52","0","Answer"
"78521956","78521012","","<p>That's because your dataset is still on the CPU.</p>
<p>You have two tensors on the same device and that's why your program works. If you move one to the GPU and you get RuntimeError about the tensors being on different devices, that means that your other tensor is still on the CPU.</p>
<p>Solution: Move your <strong>model</strong> and your <strong>dataset</strong> both on the same device.</p>
","2024-05-23 08:55:55","1","Answer"
"78521480","78521304","","<ol>
<li></li>
</ol>
<p>I am not sure about it exactly but it may help,
do try to change the version of <code>pytorch</code> and <code>torchvision</code> from 2.1.2 to 2.2.1 and 0.16.2 to 0.17.0 in requirements.txt and try again,</p>
<p>2)
try to clone it in other directory than <code>C/</code>, it will work as expected, i just tried, somehow it is having some issues in C due to some conflict with global environment or what, good luck</p>
","2024-05-23 07:17:12","-1","Answer"
"78521304","","Not able to install stable diffusion , getting this error","<pre><code>venv &quot;C:\sd\stable-diffusion-webui\venv\Scripts\Python.exe&quot;
Python 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]
Version: v1.9.3
Commit hash: &lt;hash&gt;
Installing torch and torchvision
C:\sd\stable-diffusion-webui\venv\Scripts\python.exe: No module named pip
Traceback (most recent call last):
  File &quot;C:\sd\stable-diffusion-webui\launch.py&quot;, line 48, in &lt;module&gt;
    main()
  File &quot;C:\sd\stable-diffusion-webui\launch.py&quot;, line 39, in main
    prepare_environment()
  File &quot;C:\sd\stable-diffusion-webui\modules\launch_utils.py&quot;, line 380, in prepare_environment
    run(f'&quot;{python}&quot; -m {torch_command}', &quot;Installing torch and torchvision&quot;, &quot;Couldn't install torch&quot;, live=True)
  File &quot;C:\sd\stable-diffusion-webui\modules\launch_utils.py&quot;, line 115, in run
    raise RuntimeError(&quot;\n&quot;.join(error_bits))
RuntimeError: Couldn't install torch.
Command: &quot;C:\sd\stable-diffusion-webui\venv\Scripts\python.exe&quot; -m pip install torch==2.1.2 torchvision==0.16.2 --extra-index-url https://download.pytorch.org/whl/cu121
Error code: 1
Press any key to continue . . .
</code></pre>
<p>I have tried installing and adding every required thing to the environement variables, still not getting the solution.</p>
","2024-05-23 06:32:16","0","Question"
"78521012","","When I am training a neural network and I observe that the GPU utilization is at 0% while the CPU utilization is at 100%","<p>When I am training a neural network and I observe that the GPU utilization is at 0% while the CPU utilization is at 100%</p>
<p><a href=""https://i.sstatic.net/Tp3plYWJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tp3plYWJ.png"" alt=""enter image description here"" /></a></p>
<p>If I add 'to.device', I get the following error:</p>
<blockquote>
<p>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA_gather)</p>
</blockquote>
<p>My DQN model training code is as follows:</p>
<pre><code>def train_dqn(dqn, target_dqn, optimizer, replay_buffer, num_episodes, batch_size, gamma,
              epsilon, epsilon_min,epsilon_decay, risk_coefficient_list, episode_rewards,     
              pbar,loss_list,resource_consumed_list):
    global resource_consumed, risk_coefficient
    for episode in range(num_episodes):
        state = state_reset()
        done = False
        episode_reward = 0 
        scarce_list = []
        for i in range(len(state) - 1):
            scarce_list.append(state[i] * 0.8)
        while not done:
            actions = take_action(state, epsilon, dqn)
            next_state, done = environment_step(state, actions)
            reward, risk_coefficient, resource_consumed = get_reward(next_state, next_state, actions, scarce_list)
            replay_buffer.push(state, actions, reward, next_state, done)
            state = next_state
            episode_reward += reward
            if len(replay_buffer) &gt; batch_size:
                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
                action_indices = torch.multinomial(actions, 1).squeeze(1).long() 
                current_q = dqn(states.float()).gather(1, action_indices.unsqueeze(1)).squeeze(1)  
                next_q = target_dqn(next_states.float()).max(1)[0].detach()
                expected_q = rewards + gamma * next_q * (~dones)
                loss = nn.functional.mse_loss(current_q, expected_q.float()) 
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            epsilon = max(epsilon_min, epsilon_decay * epsilon) 
        if episode % 10==0:
            target_dqn.load_state_dict(dqn.state_dict())


device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
dqn = DQN(state_size_dim, action_size_dim).to(device)
target_dqn = DQN(state_size_dim, action_size_dim).to(device)
target_dqn.load_state_dict(dqn.state_dict())
optimizer = optim.Adam(dqn.parameters())
with tqdm(total=int(num_episodes), desc=f'{algorithm_name.upper()} Iteration %d' % i) as pbar:
     train_dqn(dqn, target_dqn, optimizer, replay_buffer, num_episodes, batch_size,
                           gamma, epsilon, epsilon_min, epsilon_decay,
                           risk_coefficient_list,
                           episode_rewards, pbar, loss_list, resource_consumed_list)
</code></pre>
","2024-05-23 04:53:21","0","Question"
"78516463","78516398","","<p>It's happening because floating point arithmetic is not associative, i.e for any any three floating point variables <code>a, b, c</code>, it is possible that <code>(a + b) + c != a + (b+ c)</code></p>
<p>Note that already <code>w1(a)[idx, :]</code> is not equal to <code>w1(b)</code></p>
","2024-05-22 09:18:02","0","Answer"
"78516398","","Why slice may affect the torch.nn.linear output?","<p>I am curious why the following code will return False. In torch, slice seems to affect the linear layer output. Thanks for your attention~</p>
<pre><code>torch.manual_seed(1234)
a = torch.randn((50, 4096)).float()
idx = [0, 2]
b = a[idx,:]
w1 = torch.nn.Linear(4096, 4096, bias=False)
w2 = torch.nn.Linear(4096, 4096, bias=False)
w3 = torch.nn.Linear(4096, 4096, bias=False)

act = torch.nn.SiLU()
out_a = w3(act(w1(a)) * w2(a))
out_b = w3(act(w1(b)) * w2(b))
print(torch.equal(out_a[idx,:], out_b))
</code></pre>
<p>In the above test, the outputs of out_a[idx,:] and out_b are very closed but different, like 0.0507067293 and 0.0507068783.</p>
","2024-05-22 09:04:30","1","Question"
"78516187","78515902","","<p>The concatenation works as expected. However, your first assertion necessarily fails, as you add the <code>shape[1]</code> value of the <em>already transposed</em> tensor <code>tensor1_t</code> to the <code>shape[0]</code> value of <code>tensor2</code>. What you need to add is either <code>shape[0]</code> of <code>tensor1_t</code> or <code>shape[1]</code> of <code>tensor1</code> (which has not been transposed).</p>
<p>If you fix this, then exactly the same problem will happen with your second assertion, so you will also have to fix it there.</p>
<p>Both of the following versions will work as expected:</p>
<ul>
<li>Compare same dimensions from transposed tensor:
<pre class=""lang-py prettyprint-override""><code>assert concat_tensor.shape[0] == tensor1_t.shape[0] + tensor2.shape[0]
assert concat_tensor.shape[1] == tensor1_t.shape[1]
</code></pre>
</li>
<li>Compare opposite dimensions from original tensor:
<pre class=""lang-py prettyprint-override""><code>assert concat_tensor.shape[0] == tensor1.shape[1] + tensor2.shape[0]
assert concat_tensor.shape[1] == tensor1.shape[0]
</code></pre>
</li>
</ul>
<p>If you want to get more specific <code>AssertionError</code>s, you can add a string as an error message after the assertion. In your case, you could have written, for example:</p>
<pre class=""lang-py prettyprint-override""><code>message = (f&quot;concat_tensor: {concat_tensor.shape}, &quot;
           f&quot;tensor1_t: {tensor1_t.shape}, tensor2: {tensor2.shape}&quot;)
assert concat_tensor.shape[0] == tensor1_t.shape[1] + tensor2.shape[0], message
# &gt;&gt;&gt; AssertionError: concat_tensor: torch.Size([12, 4]), 
#     tensor1_t: torch.Size([2, 4]), tensor2: torch.Size([10, 4])
</code></pre>
<p>From the printout, you could have noticed that all tensor shapes match along dimension 1, including <code>tensor1_t</code> (which is already transposed, as mentioned above).</p>
","2024-05-22 08:25:32","1","Answer"
"78515902","","Transposing and concatenating tensors in PyTorch","<p>I have two tensors in PyTorch:</p>
<pre><code>tensor1 = torch.rand(4, 2)
tensor2 = torch.rand(10, 4)
</code></pre>
<p>I want to transpose the first one, to be able to then concatenate it to the second.</p>
<pre><code>tensor1_t = torch.transpose(tensor1, 0, 1)

concat_tensor = torch.cat((tensor1_t, tensor2), dim=0)

assert concat_tensor.shape[0] == tensor1_t.shape[1] + tensor2.shape[0]
assert concat_tensor.shape[1] == tensor1_t.shape[0]

</code></pre>
<p>But doing like this gives me unspecified AssertionError. Can someone help me figure this out?</p>
","2024-05-22 07:25:49","0","Question"
"78515709","78513399","","<p>As <a href=""https://github.com/pytorch/pytorch/blob/3b0f6cce5c87e099cf4eca0e0156c6341bb36e0d/torch/utils/collect_env.py#L395"" rel=""nofollow noreferrer"">documented in the code</a> for the <code>torch.utils.collect_env</code> module, the <code>get_pip_packages</code> function simply runs and parses a <code>pip list</code>, which will also find Conda-installed Python packages. So, seems to be working as expected.</p>
<p>It might be worth noting that most Conda recipes for Python packages use a <code>pip install</code> command to do the actual installation at build time. Conda then packages the difference in files before and after that command. So, a Conda-installed Python package is mostly as if one  had run <code>pip install</code>. Hopefully that clarifies why running <code>pip list</code> can't discern that a package is from Conda, whereas <code>conda list</code> can tell the difference.</p>
","2024-05-22 06:41:21","2","Answer"
"78514910","78514849","","<p>This is and has been one of the biggest challenges in the field of computation.</p>
<p>For your case, however, you can just simply use the <code>allclose()</code> function with <code>double()</code> to technically use the double precision (float 64).</p>
<h3>Code</h3>
<pre><code>import torch
from torch import nn


def _compare(X, C):

    A = torch.empty_like(X)
    for t in range(X.shape[1]):
        A[:, t, :] = (C @ X[:, t, :].unsqueeze(-1)).squeeze(-1)

    A1 = (C @ X.unsqueeze(-1)).squeeze(-1)

    equal = (A1 == A).all().item()
    close = torch.allclose(A1, A)
    max_diff = (A1 - A).abs().max().item()

    print(f'equal: {equal}, close: {close}, diff: {max_diff:.16f}')


torch.manual_seed(4)

for X, C in [
    (torch.rand(8, 50, 32), nn.Parameter(torch.randn(32, 32))),
    (torch.rand(16, 50, 32), nn.Parameter(torch.randn(32, 32))),
    (torch.rand(8, 50, 32), nn.Parameter(torch.randn(32, 32)).detach())
]:
    _compare(X, C)

</code></pre>
<h3>Prints</h3>
<pre><code>equal: False, close: False, diff: 0.0000023841857910
equal: False, close: False, diff: 0.0000019073486328
equal: True, close: True, diff: 0.0000000000000000
</code></pre>
<h2>Using <code>double()</code></h2>
<ul>
<li>You can increase the precision.</li>
<li>You can use double precision, which uses <code>torch.float64</code>. It dramatically slow down the program. But it is a tradeoff between computational time and accuracy. That's your call.</li>
</ul>
<h3>Code</h3>
<pre><code>import torch
from torch import nn


def _compare(X, C):
    X, C = X.double(), C.double()

    A = torch.empty_like(X)
    for t in range(X.shape[1]):
        A[:, t, :] = (C @ X[:, t, :].unsqueeze(-1)).squeeze(-1)

    A1 = (C @ X.unsqueeze(-1)).squeeze(-1)

    equal = (A1 == A).all().item()
    close = torch.allclose(A1, A)
    max_diff = (A1 - A).abs().max().item()

    print(f'equal: {equal}, close: {close}, diff: {max_diff:.32f}')


torch.manual_seed(4)

for X, C in [
    (torch.rand(8, 50, 32), nn.Parameter(torch.randn(32, 32))),
    (torch.rand(16, 50, 32), nn.Parameter(torch.randn(32, 32))),
    (torch.rand(8, 50, 32), nn.Parameter(torch.randn(32, 32)).detach())
]:
    _compare(X, C)

</code></pre>
<h3>Prints</h3>
<pre><code>equal: False, close: True, diff: 0.00000000000000266453525910037570
equal: True, close: True, diff: 0.00000000000000000000000000000000
equal: True, close: True, diff: 0.00000000000000000000000000000000
</code></pre>
<h3>Note</h3>
<ul>
<li><p>Torch uses <code>torch.float32</code> or &quot;single precision&quot; by default, which is not precise enough for your comparison.</p>
</li>
<li><p>If you want to know the details of how these computations are performed, there are resources. For short versions, see these <a href=""https://dev-discuss.pytorch.org/t/more-in-depth-details-of-floating-point-precision/654"" rel=""nofollow noreferrer"">1</a>, <a href=""https://pytorch.org/docs/stable/notes/numerical_accuracy.html"" rel=""nofollow noreferrer"">2</a>, <a href=""https://stackoverflow.com/questions/63818676/what-is-the-machine-precision-in-pytorch-and-when-should-one-use-doubles"">3</a>.</p>
</li>
<li><p><a href=""https://en.wikipedia.org/wiki/Floating-point_arithmetic"" rel=""nofollow noreferrer"">Floating point arithmetic</a>.</p>
</li>
</ul>
<hr />
<p><strong>&quot;How can I know that there doesn't exist some other inputs that fail the allclose check?&quot;</strong></p>
<p>You can define tolerances, based on the expected precision of your calculations:</p>
<pre><code>torch.allclose(input, other, rtol=1e-06, atol=1e-08, equal_nan=False)

</code></pre>
<p>and test it:</p>
<pre><code>import torch


def test_allclose():
    atol, rtol = 1e-06, 1e-08

    a = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)
    b = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)

    assert torch.allclose(a, b, rtol=rtol, atol=atol), &quot;Tensors are not close!&quot;


test_allclose()

for _ in range(1000):
    a = torch.rand(100, dtype=torch.float64)
    b = a + torch.randn(100, dtype=torch.float64) * 1e-9
    if not torch.allclose(a, b, rtol=1e-06, atol=1e-08):
        print(&quot;a:&quot;, a, &quot;b:&quot;, b, &quot;diff:&quot;, b - a)
        assert torch.allclose(a, b, rtol=1e-06, atol=1e-08), &quot;Random tensors are not close!&quot;

edge_cases = [
    (torch.tensor([1e-10, 1e-20], dtype=torch.float64), torch.tensor([1e-10, 1e-20], dtype=torch.float64)),
    (torch.tensor([1e+10, 1e+20], dtype=torch.float64), torch.tensor([1e+10, 1e+20], dtype=torch.float64)),
    (torch.tensor([0.0, 1.0], dtype=torch.float64), torch.tensor([0.0, 1.0], dtype=torch.float64))
]

for a, b in edge_cases:
    assert torch.allclose(a, b, rtol=1e-06, atol=1e-08), &quot;Edge case tensors are not close!&quot;

print(&quot;Test passed.&quot;)

</code></pre>
","2024-05-22 00:56:03","0","Answer"
"78514849","","pytorch matrix multiplication accuracy depends on tensor size","<p>I have the following code where I multiply tensor <code>X</code> by a matrix <code>C</code>. Depending on the size of <code>X</code> and whether <code>C</code> is attached to the computation graph, I get different results when I compare batched multiplication vs looping over each slice of <code>X</code>.</p>
<pre><code>import torch
from torch import nn

for X,C in [(torch.rand(8,  50, 32), nn.Parameter(torch.randn(32,32))),
            (torch.rand(16, 50, 32), nn.Parameter(torch.randn(32,32))),
            (torch.rand(8,  50, 32), nn.Parameter(torch.randn(32,32)).detach())
            ]:

    #multiply each entry
    A = torch.empty_like(X)
    for t in range(X.shape[1]):
        A[:,t,:] = (C @ X[:,t,:].unsqueeze(-1)).squeeze(-1)

    #multiply in batch
    A1 = (C @ X.unsqueeze(-1)).squeeze(-1)
    
    print('equal:', (A1 == A).all().item(), ', close:', torch.allclose(A1, A))
</code></pre>
<p>Returns</p>
<pre><code>equal: False , close: False
equal: True , close: True
equal: True , close: True
</code></pre>
<p>What's going on? I expect them to be equal in all three cases.</p>
<p>For reference,</p>
<pre><code>import sys, platform
print('OS:', platform.platform())
print('Python:', sys.version)
print('Pytorch:', torch.__version__)
</code></pre>
<p>gives:</p>
<pre><code>OS: macOS-14.4.1-arm64-arm-64bit
Python: 3.12.1 | packaged by conda-forge | (main, Dec 23 2023, 08:01:35) [Clang 16.0.6 ]
Pytorch: 2.2.0
</code></pre>
","2024-05-22 00:13:50","1","Question"
"78514532","78513763","","<p>I am not sure if you are intentionally doing that, but giving labels as an input to the model with</p>
<p><code>outputs = model(**inputs)</code>
seems to be the issue for me. You should remove <code>labels</code> from the dictionary <code>inputs</code>. Also, if your model is <code>BertForSequenceClassification</code> then it will not accept labels as input and it will not automatically calculate the loss function. Try changing your code into</p>
<pre><code>def evaluate(model, dataloader_val):
    model.eval()
    model.train(False)

    loss_val_total = 0
    predictions, true_vals = [], []

    for batch in dataloader_val:
    
        batch = tuple(b.to(device) for b in batch)
    
        inputs = {
                    'input_ids':      batch[0],
                    'attention_mask': batch[1]
                 }
        labels = batch[2], # your labels should be given in dataloader as one-hot encoded values
                 
        with torch.no_grad():        
            outputs = model(**inputs)
        
        loss = nn.CrossEntropyLoss()(outputs, labels)
    
        loss_val_total += loss.item()

        probs = torch.argmax(outputs, dim = 1).detach().cpu().numpy()
        label_ids = torch.argmax(labels, dim = 1).cpu().numpy()
        predictions.append(probs)
        true_vals.append(label_ids)
    
    loss_val_avg = loss_val_total/len(dataloader_val) 

    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)

    ### after evaluating we resume model training
    model.train(True)

return loss_val_avg, predictions, true_vals
</code></pre>
","2024-05-21 22:01:33","0","Answer"
"78513763","","ValueError: Expected input batch_size (2) to match target batch_size (4)","<p>Here is the code for a text classification task I am doing. The issue seems to lie here. This is a multi class problem. I have 3 labels. I tried several things. I changed the format of the labels to integers and tried looking into the loss function. I am not sure what parameter needs to be changed.</p>
<pre><code>def evaluate(model, dataloader_val):
    model.eval()
    model.train(False)
    
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in dataloader_val:
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():        
            outputs = model(**inputs)
            
        loss = outputs[0]
        
        logits = outputs[1]
        loss_val_total += loss.item()

        probs = torch.argmax(logits, dim = 1).detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(probs)
        true_vals.append(label_ids)
        
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
    
    ### after evaluating we resume model training
    model.train(True)
    
    return loss_val_avg, predictions, true_vals
</code></pre>
<p>And this is the error I get</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-55-a095c6ad8f10&gt; in &lt;module&gt;
     44                      }       
     45 
---&gt; 46             outputs = model(**inputs)
ValueError: Expected input batch_size (2) to match target batch_size (4).
</code></pre>
","2024-05-21 18:32:19","0","Question"
"78513473","78511412","","<p>Just add <code>nn.Softmax()</code> in your forward function. But the dimension is also important which you should consider in your code:</p>
<pre><code>class ResNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.network = torchvision.models.resnet18()
        num_ftrs = self.network.fc.in_features
        self.network.fc = nn.Linear(num_ftrs, 19)
        self.act_fun = nn.Softmax(dim=-1)
        
    def forward(self, xb):
        x = self.network(xb)
        x = self.act_fun(x)
        return x
</code></pre>
","2024-05-21 17:25:07","0","Answer"
"78513399","","Why does torch.utils.collect_env claim I've pip installed modules that I used conda to install","<p>I've had some odd behavior with a torch installation, so I ran <code>python -m torch.utils.collect_env</code> to get a clear idea of ehat was in my environment, but it claimed I had used both pip3 and conda to install some modules.</p>
<p>So, I created a simplified conda env, which still gives this problem:</p>
<pre><code>$ conda create -n temp
$ conda activate temp
$ conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia


 
&lt;frozen runpy&gt;:128: RuntimeWarning: 'torch.utils.collect_env' found in sys.modules after import of package 'torch.utils', but prior to execution of 'torch.utils.collect_env'; this may result in unpredictable behaviour
Collecting environment information...
PyTorch version: 2.3.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:46:43) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 12.1.105
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Ti
Nvidia driver version: 535.171.04
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

.
.
.
Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] torch==2.3.0
[pip3] torchaudio==2.3.0
[pip3] torchvision==0.18.0
[conda] blas                      1.0                         mkl  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46344  
[conda] mkl-service               2.4.0           py312h5eee18b_1  
[conda] mkl_fft                   1.3.8           py312h5eee18b_0  
[conda] mkl_random                1.2.4           py312hdb19cb5_0  
[conda] numpy                     1.26.4          py312hc5e2394_0  
[conda] numpy-base                1.26.4          py312h0da6c21_0  
[conda] pytorch                   2.3.0           py3.12_cuda12.1_cudnn8.9.2_0    pytorch
[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                2.3.0               py312_cu121    pytorch
[conda] torchvision               0.18.0              py312_cu121    pytorch
</code></pre>
<p>Why does this say I've pip installed some modules which I used conda to install? Is it connected to this message:</p>
<pre><code>'torch.utils.collect_env' found in sys.modules after import of package 'torch.utils', but prior to execution of 'torch.utils.collect_env'; this may result in unpredictable behaviour
</code></pre>
<p>If so, what should I do? Also, if not, what (if anything) should I do?</p>
","2024-05-21 17:08:16","1","Question"
"78512125","78498481","","<p>In version 2.3.0 of pytorch, it prints this unwanted warning even if no exception is thrown: see <a href=""https://github.com/pytorch/pytorch/pull/125790"" rel=""noreferrer"">https://github.com/pytorch/pytorch/pull/125790</a></p>
<p>As you mentioned, though, the training is processing correctly. If you want to get rid of this warning, you should revert to torch 2.2.2 (you then also have to revert torchvision to 0.17.2):</p>
<pre><code>pip3 install torchvision==0.17.2
pip3 install torch==2.2.2
</code></pre>
","2024-05-21 13:13:32","10","Answer"
"78511412","","How do I add an activation function at the last classification layer?","<p>My model:</p>
<pre><code>class ResNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.network = torchvision.models.resnet18()
        num_ftrs = self.network.fc.in_features
        self.network.fc = nn.Linear(num_ftrs, 19)
        
    def forward(self, xb):
        return self.network(xb)
</code></pre>
<p>How can I add a <code>Softmax</code> activation function at the last classification layer</p>
","2024-05-21 11:03:43","0","Question"
"78511133","","Pytorch : Early stopping Mechanism","<p>I work with Pytorch and CIFAR100 dataset, While I'm newer, I would like to incorporate the early stopping mechanism in my code,</p>
<pre><code>def train(net,trainloader,epochs,use_gpu = True): 
    ...
    net.train()
    for epoch in range(epochs):  
        print (&quot;Epoch {}/{}&quot;.format(epoch+1, epochs))
        running_loss = 0.0
        running_corrects = 0
        for i, data in enumerate(trainloader, 0):
            images, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()
            outputs = net(images)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            
        epoch_loss = running_loss/len(trainloader.dataset)
        print('Loss: {}'.format(epoch_loss))  
</code></pre>
<p>This is the class of early stopping:</p>
<pre><code>class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_loss = float('inf')

    def early_stop(self, validation_loss):
        if validation_loss &lt; self.min_validation_loss:
            self.min_validation_loss = validation_loss
            self.counter = 0
        elif validation_loss &gt; (self.min_validation_loss + self.min_delta):
            self.counter += 1
            if self.counter &gt;= self.patience:
                return True
        return False
</code></pre>
<p>I don't have a validation set, I suppose that test set is same as validation set
Now, I would like to know where I must make those lines:</p>
<pre><code>early_stopper = EarlyStopper(patience=3, min_delta=10)
for epoch in np.arange(n_epochs):
    train_loss = train_one_epoch(model, train_loader)
    validation_loss = validate_one_epoch(model, validation_loader)
    if early_stopper.early_stop(validation_loss):             
        break
</code></pre>
","2024-05-21 10:14:49","0","Question"
"78505358","78505110","","<p>AFAIK, the only way to speedup is to register parameters with <code>requires_grad = False</code>. What this flag does is it stop the creation of computational graph wrt those parameters in the forward pass, so when you have to calculate the gradient, smaller computational graph means faster backward, which give a speed boost. Simply setting grad to 0 does not speed things up (as you still have to backward a full graph and optimizing those zeroed out parameters.</p>
","2024-05-20 07:55:11","3","Answer"
"78505315","78504721","","<p>The <code>grad_output</code> means the gradient of the output of that layer. Let <code>x</code> be the input of the function <code>f</code> and output <code>y</code> (<code>y = f(x)</code>). The <code>grad_output</code> should equal <code>dL/dy</code>. Using this will help you to calculate <code>dL/dx</code> using chain rule.</p>
","2024-05-20 07:45:48","0","Answer"
"78505110","","Using register_hook() to freeze a portion of weight tensor does not speed up training","<p>I am doing a sort of transfer learning where I want to freeze a part of the weights tensor and train the other, as explained in <a href=""https://stackoverflow.com/questions/78488981/set-a-part-of-weight-tensor-to-requires-grad-true-and-keep-rest-of-values-to-r/78489275?noredirect=1#comment138396465_78489275"">my previous question</a></p>
<p>For this, I am using <code>register_hook()</code> to set the gradients of a portion of the weights to zero. I checked that the gradients were set to zero, however, I noticed that this doesn't accelerate the training, it seems that computations still happen for the weights for which the gradients were set to zero and I dont really understand if the <code>register_hook()</code> function sets the gradients to zero after computing them, or computes the new weights with gradients = 0. Is there any way to do that so I can get a similar behavior to <code>requires_grad=False</code> and speedup training?</p>
<p>Here is a simple example:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import time
from torch.utils.data import Dataset, DataLoader

torch.manual_seed(42)

class example_net(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(example_net, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# dataset example:
class example_dataset(Dataset):
    def __init__(self, input_size, num_samples):
        self.input_size = input_size
        self.num_samples = num_samples
        self.data = torch.randn(num_samples, input_size)
        self.targets = torch.randint(0, 10, (num_samples,))

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return self.data[idx], self.targets[idx]

input_size = 4096
batch_size = 64
num_samples = 10000

dataset = example_dataset(input_size, num_samples)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
criterion = nn.CrossEntropyLoss()

def train_model(net, dataloader, use_hook=False, num_epochs=5):
    optimizer = optim.SGD(net.parameters(), lr=0.01)
    start_time = time.time()

    if use_hook:
        def hook_fn(grad):
            grad = grad.clone()
            grad[:, 10:] = 0  # Zero out all but the first 10 columns of the gradient
            return grad

        # Register the hook
        hook_handle = net.fc1.weight.register_hook(hook_fn)

    for epoch in range(num_epochs):
        for inputs, targets in dataloader:
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            #print(&quot;after hook:&quot;, net.fc1.weight.grad)
            optimizer.step()

    if use_hook:
        hook_handle.remove()

    end_time = time.time()
    return end_time - start_time

num_epochs = 1

# train model without  hook
net_without_hook = example_net(input_size, 4096, 10)
time_without_hook = train_model(net_without_hook, dataloader, use_hook=False, 
 num_epochs=num_epochs)
print(f&quot;Total training time without hook: {time_without_hook:.4f} seconds&quot;)

# train model with  hook
net_with_hook = example_net(input_size, 4096, 10)
time_with_hook = train_model(net_with_hook, dataloader, use_hook=True, 
num_epochs=num_epochs)
print(f&quot;Total training time with hook: {time_with_hook:.4f} seconds&quot;)
</code></pre>
","2024-05-20 06:58:47","2","Question"
"78504721","","Why torch.autograd.Function backward pass a grad_output?","<p>my goal is to understand <code>torch.autograd.Function</code>. The problem is I don't understand why backward need <code>grad_output</code>.</p>
<p>What I've tried:</p>
<ol>
<li><p>Read <a href=""https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"" rel=""nofollow noreferrer"">Learning PyTorch with Examples</a></p>
<p>The Learning PyTorch with Examples teach how to define autograd functions. I understand for <code>LegendrePolynomial3</code>, the <code>forward</code> should be <code>½ * (5x³ - 3x)</code>. However, I don't understand why the <code>backward</code> need <code>grad_output</code>.</p>
<pre><code>class LegendrePolynomial3(torch.autograd.Function):
  &quot;&quot;&quot;
  We can implement our own custom autograd Functions by subclassing
  torch.autograd.Function and implementing the forward and backward passes
  which operate on Tensors.
  &quot;&quot;&quot;

  @staticmethod
  def forward(ctx, input):
    &quot;&quot;&quot;
    In the forward pass we receive a Tensor containing the input and return
    a Tensor containing the output. ctx is a context object that can be used
    to stash information for backward computation. You can cache arbitrary
    objects for use in the backward pass using the ctx.save_for_backward method.
    &quot;&quot;&quot;
    ctx.save_for_backward(input)
    # ½ * (5x³ - 3x)
    return 0.5 * (5 * input ** 3 - 3 * input)

  @staticmethod
  def backward(ctx, grad_output):
    &quot;&quot;&quot;
    In the backward pass we receive a Tensor containing the gradient of the loss
    with respect to the output, and we need to compute the gradient of the loss
    with respect to the input.
    &quot;&quot;&quot;
    input, = ctx.saved_tensors
    # d/dx ½ * (5x³ - 3x)
    # d/dx (½ * 5x³) - (½ * 3x)
    #      (3 * ½ * 5x²) - (1 * ½ * 3)
    #      1.5 * (5x² - 1)
    return grad_output * 1.5 * (5 * input ** 2 - 1)
</code></pre>
</li>
</ol>
","2024-05-20 04:58:49","0","Question"
"78503783","78500926","","<p>I tried something bizarre but it does the job (I guess)
After getting 2 seperate lists : one for the ground truth and the second for predictions, I Count occurrences in both lists and consider each intersection as a true positive and any other class intersection as a false positive.</p>
<p>This is the code, I know it's not optimised and I used about 60% of the original RetinaNet code to parse the data, but it works. I hope it helps some of you :</p>
<pre><code>import torch
import cv2
import numpy as np
import os
import glob as glob
import argparse
import time
from collections import Counter
import matplotlib.pyplot as plt
from collections import Counter

from model import create_model

from config import (
    NUM_CLASSES, DEVICE, CLASSES
)

from xml.etree import ElementTree as et
from config import (
    CLASSES, DATASET_IMAGE_WIDTH, DATASET_IMAGE_HEIGHT
)
from torch.utils.data import Dataset

DIR_TEST = 'data/test'
test_images = glob.glob(f&quot;{DIR_TEST}/*.png&quot;)
print(f&quot;Test instances: {len(test_images)}&quot;)

# The dataset class.
class CustomDataset(Dataset):
    def __init__(self, dir_path, width, height, classes, transforms=None):
        self.transforms = transforms
        self.dir_path = dir_path
        self.height = height
        self.width = width
        self.classes = classes
        self.image_file_types = ['*.jpg', '*.jpeg', '*.png', '*.ppm', '*.JPG']
        self.all_image_paths = []
        
        # Get all the image paths in sorted order.
        for file_type in self.image_file_types:
            self.all_image_paths.extend(glob.glob(os.path.join(self.dir_path, file_type)))
        self.all_images = [image_path.split(os.path.sep)[-1] for image_path in self.all_image_paths]
        self.all_images = sorted(self.all_images)

    def __getitem__(self, idx):
        # Capture the image name and the full image path.
        image_name = self.all_images[idx]
        image_path = os.path.join(self.dir_path, image_name)

        # Read and preprocess the image.
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)
        image_resized = cv2.resize(image, (self.width, self.height))
        image_resized /= 255.0
        
        # Capture the corresponding XML file for getting the annotations.
        annot_filename = os.path.splitext(image_name)[0] + '.xml'
        annot_file_path = os.path.join(self.dir_path, annot_filename)
        
        boxes = []
        labels = []
        tree = et.parse(annot_file_path)
        root = tree.getroot()
        
        # Original image width and height.
        image_width = image.shape[1]
        image_height = image.shape[0]
        
        # Box coordinates for xml files are extracted 
        # and corrected for image size given.
        for member in root.findall('object'):
            # Get label and map the `classes`.
            labels.append(self.classes.index(member.find('name').text))
            
            # Left corner x-coordinates.
            xmin = int(member.find('bndbox').find('xmin').text)
            # Right corner x-coordinates.
            xmax = int(member.find('bndbox').find('xmax').text)
            # Left corner y-coordinates.
            ymin = int(member.find('bndbox').find('ymin').text)
            # Right corner y-coordinates.
            ymax = int(member.find('bndbox').find('ymax').text)
            
            # Resize the bounding boxes according 
            # to resized image `width`, `height`.
            xmin_final = (xmin/image_width)*self.width
            xmax_final = (xmax/image_width)*self.width
            ymin_final = (ymin/image_height)*self.height
            ymax_final = (ymax/image_height)*self.height

            # Check that max coordinates are at least one pixel
            # larger than min coordinates.
            if xmax_final == xmin_final:
                xmax_final += 1
            if ymax_final == ymin_final:
                ymax_final += 1
            # Check that all coordinates are within the image.
            if xmax_final &gt; self.width:
                xmax_final = self.width
            if ymax_final &gt; self.height:
                ymax_final = self.height
            
            boxes.append([xmin_final, ymin_final, xmax_final, ymax_final])
        
        # Bounding box to tensor.
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        # Area of the bounding boxes.
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) &gt; 0 \
            else torch.as_tensor(boxes, dtype=torch.float32)
        # No crowd instances.
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        # Labels to tensor.
        labels = torch.as_tensor(labels, dtype=torch.int64)
        # Prepare the final `target` dictionary.
        target = {}
        target[&quot;boxes&quot;] = boxes
        target[&quot;labels&quot;] = labels
        target[&quot;area&quot;] = area
        target[&quot;iscrowd&quot;] = iscrowd
        image_id = torch.tensor([idx])
        target[&quot;image_id&quot;] = image_id
        # Apply the image transforms.
        if self.transforms:
            sample = self.transforms(image = image_resized,
                                     bboxes = target['boxes'],
                                     labels = labels)
            image_resized = sample['image']
            target['boxes'] = torch.Tensor(sample['bboxes'])
        
        if np.isnan((target['boxes']).numpy()).any() or target['boxes'].shape == torch.Size([0]):
            target['boxes'] = torch.zeros((0, 4), dtype=torch.int64)
        return image_resized, target

    def __len__(self):
        return len(self.all_images)


GT = []


# USAGE: python datasets.py
if __name__ == '__main__':
    # sanity check of the Dataset pipeline with sample visualization
    dataset = CustomDataset(
        DIR_TEST, DATASET_IMAGE_WIDTH, DATASET_IMAGE_HEIGHT, CLASSES
    )
    
    # function to visualize a single sample
    def visualize_sample(image, target):
        for box_num in range(len(target['boxes'])):
            box = target['boxes'][box_num]
            label = CLASSES[target['labels'][box_num]]
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            cv2.rectangle(
                image, 
                (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),
                (0, 0, 255), 
                2
            )
            cv2.putText(
                image, 
                label,
                (int(box[0]), int(box[1]-5)), 
                cv2.FONT_HERSHEY_SIMPLEX, 
                0.7, 
                (0, 0, 255), 
                2
            )
        cv2.imshow('Image', image)
        cv2.waitKey(0)
        
    for i in range(len(test_images)):
        image, target = dataset[i]
        GT.append(target['labels'].tolist())





#inference code
np.random.seed(42)

# Construct the argument parser.
parser = argparse.ArgumentParser()
parser.add_argument(
    '-i', '--input', 
    help='path to input image directory',
)
parser.add_argument(
    '--imgsz', 
    default=None,
    type=int,
    help='image resize shape'
)
parser.add_argument(
    '--threshold',
    default=0.25,
    type=float,
    help='detection threshold'
)
args = vars(parser.parse_args())

os.makedirs('inference_outputs/images', exist_ok=True)

COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))

# Load the best model and trained weights.
model = create_model(num_classes=NUM_CLASSES)
checkpoint = torch.load('outputs/best_model.pth', map_location=DEVICE)
model.load_state_dict(checkpoint['model_state_dict'])
model.to(DEVICE).eval()

MP = []
LMP = []

for i in range(len(test_images)):
    MP.append([])
    # Get the image file name for saving output later on.
    image_name = test_images[i].split(os.path.sep)[-1].split('.')[0]
    image = cv2.imread(test_images[i])
    orig_image = image.copy()
    if args['imgsz'] is not None:
        image = cv2.resize(image, (args['imgsz'], args['imgsz']))
    # BGR to RGB.
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)
    # Make the pixel range between 0 and 1.
    image /= 255.0
    # Bring color channels to front (H, W, C) =&gt; (C, H, W).
    image_input = np.transpose(image, (2, 0, 1)).astype(np.float32)
    # Convert to tensor.
    image_input = torch.tensor(image_input, dtype=torch.float).to('cpu')
    # Add batch dimension.
    image_input = torch.unsqueeze(image_input, 0)
    start_time = time.time()
    # Predictions
    with torch.no_grad():
        outputs = model(image_input.to(DEVICE))
    end_time = time.time()


    # Load all detection to CPU for further operations.
    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]
    # Carry further only if there are detected boxes.
    if len(outputs[0]['boxes']) != 0:
        boxes = outputs[0]['boxes'].data.numpy()
        scores = outputs[0]['scores'].data.numpy()
        # Filter out boxes according to `detection_threshold`.
        boxes = boxes[scores &gt;= args['threshold']].astype(np.int32)
        draw_boxes = boxes.copy()
        # Get all the predicited class names.
        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]
        # Draw the bounding boxes and write the class name on top of it.
        for j, box in enumerate(draw_boxes):
            class_name = pred_classes[j]
            MP[i].append(class_name)
            color = COLORS[CLASSES.index(class_name)]
            # Recale boxes.
            xmin = int((box[0] / image.shape[1]) * orig_image.shape[1])
            ymin = int((box[1] / image.shape[0]) * orig_image.shape[0])
            xmax = int((box[2] / image.shape[1]) * orig_image.shape[1])
            ymax = int((box[3] / image.shape[0]) * orig_image.shape[0])
            cv2.rectangle(orig_image,
                        (xmin, ymin),
                        (xmax, ymax),
                        color[::-1], 
                        3)
            cv2.putText(orig_image, 
                        class_name, 
                        (xmin, ymin-5),
                        cv2.FONT_HERSHEY_SIMPLEX, 
                        0.8, 
                        color[::-1], 
                        2, 
                        lineType=cv2.LINE_AA)    
        
        LMP.append(MP[i])
    


# Mapping
mapping = {
    1: 'bluetooth',
    2: 'wifi',
    3: 'drone'
}

# Example ground truth and predictions
ground_truth = [[mapping[num] for num in sublist] for sublist in GT]
predictions = LMP

print(&quot;Ground truth list:&quot;, ground_truth)
print(&quot;All predicted classes lists:&quot;, predictions)

# Convert ground truth and predictions to tuples of tuples
ground_truth = tuple(tuple(sublist) for sublist in ground_truth)
predictions = tuple(tuple(sublist) for sublist in predictions)

# Initialize confusion matrices
confusion_matrices = {
    'bluetooth': np.zeros((2, 2), dtype=int),
    'wifi': np.zeros((2, 2), dtype=int),
    'drone': np.zeros((2, 2), dtype=int)
}

# Count occurrences of each class in ground truth and predictions
gt_counts = Counter(ground_truth)
pred_counts = Counter(predictions)

gt_counts = Counter()
pred_counts = Counter()

for gt_list in ground_truth:
    gt_counts.update(gt_list)

for pred_list in predictions:
    pred_counts.update(pred_list)

# Populate confusion matrices
for class_idx, class_n in mapping.items():
    TP = min(gt_counts[class_n], pred_counts[class_n])
    FP = max(0, pred_counts[class_n] - gt_counts[class_n])
    FN = max(0, gt_counts[class_n] - pred_counts[class_n])
    
    # TN is tricky to calculate correctly in a multi-class setup but simplified here
    TN = len(ground_truth) + len(predictions) - (TP + FP + FN)
    
    confusion_matrices[class_n][0, 0] = TP
    confusion_matrices[class_n][0, 1] = FP
    confusion_matrices[class_n][1, 0] = FN
    confusion_matrices[class_n][1, 1] = TN

# Print the confusion matrices
for class_n, matrix in confusion_matrices.items():
    print(f&quot;Confusion Matrix for {class_n}:&quot;)
    print(matrix)
    print(&quot;TP: &quot;, matrix[0, 0])
    print(&quot;TN: &quot;, matrix[1, 1])
    print(&quot;FP: &quot;, matrix[0, 1])
    print(&quot;FN: &quot;, matrix[1, 0])
    print()
</code></pre>
","2024-05-19 19:59:37","0","Answer"
"78503373","78478565","","<p>The HuggingFaceCrossEncoder class takes a <a href=""https://api.python.langchain.com/en/latest/cross_encoders/langchain_community.cross_encoders.huggingface.HuggingFaceCrossEncoder.html#langchain_community.cross_encoders.huggingface.HuggingFaceCrossEncoder.model_kwargs"" rel=""nofollow noreferrer"">model_kwargs</a> parameter that is passed to the <strong>init</strong> of the CrossEncoder class and this class accepts a parameter called <a href=""https://www.sbert.net/docs/package_reference/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder"" rel=""nofollow noreferrer"">automodel_args</a> to pass parameters to the from_pretrained method from huggingface. The parameter you want to pass is called <a href=""https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained.torch_dtype"" rel=""nofollow noreferrer"">torch_dtype</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

model = HuggingFaceCrossEncoder(model_name=&quot;BAAI/bge-reranker-base&quot;)
print({x.dtype for x in model.client.model.parameters()})
model2 = HuggingFaceCrossEncoder(model_name=&quot;BAAI/bge-reranker-base&quot;, model_kwargs={'automodel_args':{'torch_dtype':torch.float16}})
print({x.dtype for x in model2.client.model.parameters()})
</code></pre>
<p>Output:</p>
<pre><code>{torch.float32}
{torch.float16}
</code></pre>
","2024-05-19 17:16:21","0","Answer"
"78501569","","AttributeError: module 'torch.utils._pytree' has no attribute 'register_pytree_node'","<p>I use pytorch 2.0.0 and transformer4.41.0.</p>
<p>I saw many people think it's transformers version problem, and use</p>
<pre><code>pip install transformers==4.28.0 
</code></pre>
<p>to solve the problem, but it not work for me.</p>
<p>If it helps, I get this when:</p>
<blockquote>
<p>Traceback (most recent call last):
File &quot;/Users/UrbanArchitect/main.py&quot;, line 17, in 
from utils.clip import CLIPScore
File &quot;/Users/UrbanArchitect/utils/clip.py&quot;, line 2, in 
from transformers import CLIPProcessor, CLIPModel
File &quot;/Users/anaconda3/envs/Newurban/lib/python3.9/site-packages/transformers/<strong>init</strong>.py&quot;, line 26, in 
from . import dependency_versions_check
File &quot;/Users/anaconda3/envs/Newurban/lib/python3.9/site-packages/transformers/dependency_versions_check.py&quot;, line 16, in 
from .utils.versions import require_version, require_version_core
File &quot;/Users/anaconda3/envs/Newurban/lib/python3.9/site-packages/transformers/utils/<strong>init</strong>.py&quot;, line 33, in 
from .generic import (
File &quot;/Users/anaconda3/envs/Newurban/lib/python3.9/site-packages/transformers/utils/generic.py&quot;, line 474, in 
_torch_pytree.register_pytree_node(
AttributeError: module 'torch.utils._pytree' has no attribute 'register_pytree_node'</p>
</blockquote>
","2024-05-19 04:06:02","0","Question"
"78501002","78500926","","<p>You can try with scikit lib, in particular:</p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"" rel=""nofollow noreferrer"">Scikit confusion matrix</a></p>
<p>For multilabel problems, you can use the:</p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html"" rel=""nofollow noreferrer"">Scikit multilabel confusion matrix</a></p>
","2024-05-18 21:08:02","0","Answer"
"78500926","","Generate a confusion matrix for RetinaNet object detection model","<p>Is there a way that automatically generates a confusion matrix using the same test set and its annotations ?</p>
<p>This is the link to <a href=""https://debuggercafe.com/train-pytorch-retinanet-on-custom-dataset/"" rel=""nofollow noreferrer"">RetinaNet model</a></p>
<p>I tried adapting a confusion matrix from an image classification model which obviously didn't work, I'm currently trying to use the results from running inference.py and dataset.py since the first prodiuces the prediction results and the second shows a few examples of the dataset used for training.</p>
","2024-05-18 20:29:45","0","Question"
"78500770","78500738","","<p>How about simple indexing?</p>
<pre><code>In [281]: x = np.array([[1,2,3],[2,1,3]])    
In [282]: y = np.array([[1,2,3],[4,5,6],[7,8,9]])    
In [283]: x,y
Out[283]: 
(array([[1, 2, 3],
        [2, 1, 3]]),
 array([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]]))
</code></pre>
<p>Assuming the 1,2,3 of <code>x</code> correspond to 'indicies' of <code>y</code> rows (adjusted for 0 base), we can use simple indexing:</p>
<pre><code>In [284]: y[x-1,:]
Out[284]: 
array([[[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]],

       [[4, 5, 6],
        [1, 2, 3],
        [7, 8, 9]]])
</code></pre>
<p>And to get the 2d array you want, just reshape:</p>
<pre><code>In [285]: y[x-1,:].reshape(2,-1)
Out[285]: 
array([[1, 2, 3, 4, 5, 6, 7, 8, 9],
       [4, 5, 6, 1, 2, 3, 7, 8, 9]])
</code></pre>
","2024-05-18 19:16:48","2","Answer"
"78500738","","Is there any way to replace integers with tensors in torch?","<p>Say I have
<code>a = torch.tensor([[1,2,3],[2,1,3]])</code>
And i want to replace integers 1,2,3 with [1,2,3],[4,5,6],[7,8,9] respectively.
Meaning,
i want
<code>result = torch.tensor([[1,2,3,4,5,6,7,8,9],[4,5,6,1,2,3,7,8,9]])</code>
Of course for me these tensors are a bit larger so i want to do this effectively.
How can i do this with minimal computational effort?</p>
<p>I have tried numpy vectorize but you need to change the signature and apparently it makes it slower.
I failed to use torch.vmap as it doesnt let you use .item() within the vmap</p>
","2024-05-18 19:04:31","0","Question"
"78500154","","torch.optim - AttributeError: partially initialized module 'torch' has no attribute '_jit_internal' (most likely due to a circular import)","<p>Currently there is some issue about torch in my computer.</p>
<p>When I initiate the below code,</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import torch
</code></pre>
<p>It works but when I add &quot;from torch.optim import Adam,AdamW&quot;</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import torch

from torch.optim import Adam,AdamW
</code></pre>
<p>And initiate, then it gives me</p>
<p><code>AttributeError: partially initialized module 'torch' has no attribute '_jit_internal' (most likely due to a circular import)</code></p>
<p>This error. Can someone help me what is the problem?</p>
<p>Know what is the actual problem</p>
","2024-05-18 15:15:27","2","Question"
"78498481","","UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR","<p>I'm trying to train a model with Yolov8. Everything was good but today I suddenly notice getting this warning apparently related to <code>PyTorch</code> and <code>cuDNN</code>. In spite the warning, the training seems to be progressing though. I'm not sure if it has any negative effects on the training progress.</p>
<pre><code>site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
</code></pre>
<p><strong>What is the problem and how to address this?</strong></p>
<p>Here is the output of <code>collect_env</code>:</p>
<pre><code>Collecting environment information...
PyTorch version: 2.3.0+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A
OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.29.3
Libc version: glibc-2.31
Python version: 3.9.7 | packaged by conda-forge | (default, Sep  2 2021, 17:58:34)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-69-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.8.89
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100 80GB PCIe
Nvidia driver version: 515.105.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.8.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.8.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True
CPU:
Architecture:                    x86_64

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] onnx==1.16.0
[pip3] onnxruntime==1.17.3
[pip3] onnxruntime-gpu==1.17.1
[pip3] onnxsim==0.4.36
[pip3] optree==0.11.0
[pip3] torch==2.3.0+cu118
[pip3] torchaudio==2.3.0+cu118
[pip3] torchvision==0.18.0+cu118
[pip3] triton==2.3.0
[conda] numpy                     1.24.4                   pypi_0    pypi
[conda] pytorch-quantization      2.2.1                    pypi_0    pypi
[conda] torch                     2.1.1+cu118              pypi_0    pypi
[conda] torchaudio                2.1.1+cu118              pypi_0    pypi
[conda] torchmetrics              0.8.0                    pypi_0    pypi
[conda] torchvision               0.16.1+cu118             pypi_0    pypi
[conda] triton                    2.1.0                    pypi_0    pypi

</code></pre>
","2024-05-18 02:13:18","14","Question"
"78498375","","Run parallel function on Apple GPU/metal in Python","<p>I have a rather simple function that runs quite fast in parallel using <a href=""https://numba.pydata.org/"" rel=""nofollow noreferrer""><code>Numba</code></a>, and I would like to know if I can run it on my Apple M3 Max GPU. However I have never worked on GPU code before (coming from Macs), so I am a little lost..</p>
<p>I have included a small use case:</p>
<pre><code>import numpy as np
import numba as nb

N = 15      # size of the a and b
K = 127     # size of the result
L = 943     # size of the operator

a = np.random.standard_normal(size=N)
b = np.random.standard_normal(size=N)

operator = np.zeros(shape=(L, 4), dtype=np.int64)
operator[:, 0] = np.random.randint(size=L, low=0, high=N)
operator[:, 1] = np.random.randint(size=L, low=0, high=N)
operator[:, 2] = np.random.randint(size=L, low=0, high=K)
operator[:, 3] = np.random.randint(size=L, low=1, high=10)

@nb.njit(parallel=True)
def shuffle_mul(a: np.ndarray, b: np.ndarray, operator: np.ndarray) -&gt; np.ndarray:
    res = np.zeros(shape=K, dtype=a.dtype)
    for n in nb.prange(len(operator)):
        i, j, k, count = operator[n]
        res[k] += count * a[i] * b[j]
    return res

shuffle_mul(a, b, operator=operator)  # warm-up
%timeit shuffle_mul(a, b, operator=operator)  # 173 µs ± 41.3 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
</code></pre>
<p><strong>Question 1:</strong> Is it even theoretically possible/interesting to run such a function on GPU? The fact that multiple instances might read <code>a</code> and <code>b</code> or add to <code>res</code> at the same time is a problem?</p>
<p><strong>Question 2:</strong> What would be the best approach? Also, what library should I use? <code>metalcompute</code>, <code>jax</code>, <code>PyTorch</code>, ... ?</p>
<p>Thanks a lot!</p>
","2024-05-18 00:49:48","0","Question"
"78497633","78464214","","<p>Managed to find the issue. Going to post an answer here for reference.</p>
<p>The problem was in the inference. The model outputs log softmax, so to get the probability distribution, I need to find the exponential of the model output, i.e., <code>predictions.exp()</code>.</p>
<p>However, I was incorrectly calling softmax on the output (<code>torch.nn.functional.softmax(predictions, dim=0)</code>).</p>
","2024-05-17 19:38:09","0","Answer"
"78497143","78494982","","<p>It sounds like there are no workers to respond to the Dataloader's queries. You can verify by breaking the program and seeing where the program was when broken. If you find it in a worker loop, check how you are initializing your workers.</p>
","2024-05-17 17:38:45","0","Answer"
"78497035","78496983","","<p>Your are hardcoding the weights and bias calculations. In this case, the optimizer cannot have access to that.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim


def train_step(w, b, optimizer, scheduler, loss_function):
    optimizer.zero_grad()
    loss = loss_function()
    loss.backward()
    optimizer.step()
    scheduler.step()
    return loss.item()


def loss_function():
    rand_input = torch.randn(64, input_size)
    target = torch.randn(64, output_size)
    output = rand_input.mm(w) + b
    loss = nn.MSELoss()(output, target)
    return loss


input_size, output_size = 100, 3
learning_rate = 0.01
w = torch.randn(input_size, output_size, requires_grad=True)
b = torch.randn(output_size, requires_grad=True)
trainable_variables = [w, b]
optimizer = optim.SGD(trainable_variables, lr=learning_rate, momentum=0.9)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)
steps = 10000
display_step = 100


graph1 = []

for i in range(steps):
    loss = train_step(w, b, optimizer, scheduler, loss_function)
    if i % display_step == 0:
        graph1.append(loss)
        print(f'Epoch {i} \t Training Loss: {loss}')

graph = torch.tensor(graph1)


</code></pre>
<h3>Prints</h3>
<pre><code>Epoch 0      Training Loss: 94.3976058959961
Epoch 100    Training Loss: 1.1436320543289185
Epoch 200    Training Loss: 1.1590440273284912
Epoch 300    Training Loss: 0.9118895530700684
Epoch 400    Training Loss: 0.8449723720550537
Epoch 500    Training Loss: 0.9973302483558655
Epoch 600    Training Loss: 0.9681441783905029
Epoch 700    Training Loss: 1.1692309379577637
Epoch 800    Training Loss: 1.0565043687820435
Epoch 900    Training Loss: 1.0424968004226685
Epoch 1000   Training Loss: 0.9199855923652649
Epoch 1100   Training Loss: 1.1443506479263306
Epoch 1200   Training Loss: 0.9741299748420715
Epoch 1300   Training Loss: 1.1515040397644043
Epoch 1400   Training Loss: 1.2819862365722656
Epoch 1500   Training Loss: 0.9993045926094055
Epoch 1600   Training Loss: 1.066098928451538
Epoch 1700   Training Loss: 1.0772987604141235
... 

</code></pre>
","2024-05-17 17:08:11","0","Answer"
"78496983","","Learning rate not updating","<pre><code>def make_prediction(x0,t0):
    inputs = torch.vstack([x0,t0])
    layer_1 = torch.matmul(w0,inputs)
    return layer_1

loss1 = nn.MSELoss()
def loss_function():
            u_t=(make_prediction(x,t+inf_s)-make_prediction(x,t))/inf_s
            u_x=(make_prediction(x+inf_s,t)-make_prediction(x,t))/inf_s
            u_xx=(make_prediction(x+inf_s,t)-2*make_prediction(x,t)+make_prediction(x-inf_s,t))/inf_s**2
            return (1/N_i)*(loss1(make_prediction(x0IC,t0IC), u0IC))+(1/N_b)*(loss1(make_prediction(x0BC1,t0BC1), u0BC1))
            +(1/N_b)*(loss1(make_prediction(x0BC2,t0BC2), u0BC2))+(1/N_f)*(np.pi/0.01)*(loss1(u_xx-u_t-make_prediction(x,t)*u_x, 0))

def train_step(w,b, learning_rate):
    trainable_variables = [w,b]
    optimizer = torch.optim.SGD(trainable_variables, lr=learning_rate,momentum=0.9)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.01)
    loss = loss_function()
    loss.backward()
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
        w.grad.zero_()
        b.grad.zero_()
    optimizer.step()
    scheduler.step()
train_step(w,bias,learning_rate)
</code></pre>
<p>I run this code (by scheduler.ExponentialLR), but there is no change in the learning rate.
Where do you think the problem comes from?
I write the full code...thanks from your help</p>
","2024-05-17 16:56:52","1","Question"
"78495810","78495260","","<p>The error tells me that <code>save_grad()</code> got given 2 arguments too much. Even though I assume you want to save gradient values, I can't tell what the definition of <code>save_grad()</code> is, since you haven't defined or referenced it anywhere, and I can't find it online. The only thing I am noticing is that you set the gradients to zero after you did the forward pass, but after reading some docs, I am not sure this causes problems and probably doesn't cause the error you currently have.</p>
","2024-05-17 12:57:07","0","Answer"
"78495298","78376438","","<p>For <code>torch</code>, I suggest to have a look at <a href=""https://pytorch.org/docs/stable/sparse.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/sparse.html</a> and <a href=""https://pytorch.org/docs/stable/masked.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/masked.html</a>. (I beleive, both tools supposed to have same semanticks, but different implementation, first one aimed on sparse graphs). Thow, both tools still considered experimental.</p>
<pre><code>import torch

b = torch.randn((3,2), requires_grad=True)

a = torch.tensor([[7., 0, 0], [1, 9, 0]]).requires_grad_()
z = torch.mm(a, b)
z.sum().backward()
print( a.grad)

a_sparse = torch.tensor([[7., 0, 0], [1, 9, 0]]).to_sparse().requires_grad_()
y = torch.sparse.mm( a_sparse, b)
y.sum().backward()
print( a_sparse.grad)
</code></pre>
<hr />
<pre><code>tensor([[-0.4728,  2.9213, -0.2959],
    [-0.4728,  2.9213, -0.2959]])
</code></pre>
<hr />
<pre><code>tensor(indices=tensor([[0, 1, 1],
                   [0, 0, 1]]),
   values=tensor([-0.4728, -0.4728,  2.9213]),
   size=(2, 3), nnz=3, layout=torch.sparse_coo)
   
</code></pre>
","2024-05-17 11:19:47","0","Answer"
"78495260","","GradCam positional argument error in pytorch","<p>With this code I made to create a GradCAM class in order to generate the heatmap values for my data:</p>
<pre><code>class GradCAM:
    &quot;&quot;&quot;
    Class for generating GradCAM heatmaps for interpreting convolutional neural networks.

    Args:
        model (nn.Module): PyTorch model for which GradCAM will be generated.

    Attributes:
        model (nn.Module): PyTorch model.
        gradients (torch.Tensor): Gradients of the target layer.
    &quot;&quot;&quot;
    def __init__(self, model):
        self.model = model
        self.gradients = None

    def backward_hook(self, module, grad_input, grad_output):
        &quot;&quot;&quot;
        Hook function to capture gradients of the target layer.

        Args:
            module (nn.Module): Target layer module.
            grad_input (tuple of torch.Tensor): Gradients of the input.
            grad_output (tuple of torch.Tensor): Gradients of the output.
        &quot;&quot;&quot;
        self.gradients = grad_output[0]

    def generate_heatmap(self, input_tensor, target_layer_index):
        &quot;&quot;&quot;
        Generate GradCAM heatmap.

        Args:
            input_tensor (torch.Tensor): Input tensor.
            target_layer_index (int): Index of the target layer.

        Returns:
            torch.Tensor: GradCAM heatmap.
        &quot;&quot;&quot;
        # Get the target layer from the model's Sequential module
        target_layer = self.model._modules[f&quot;block{target_layer_index}&quot;]

        # Register the backward hook on the target layer
        target_layer.register_backward_hook(self.backward_hook)

        # Forward pass
        output, activations = self.model(input_tensor)

        # Zero out gradients
        self.model.zero_grad()

        # Calculate gradients
        output.backward(torch.ones_like(output))

        # Get the gradients from the backward hook
        gradients = self.gradients

        # Global average pooling
        grad_weights = F.adaptive_avg_pool1d(gradients, 1)

        # Multiply the weights with the activations
        heatmap = torch.mul(activations, grad_weights).sum(dim=2, keepdim=True)

        return heatmap
</code></pre>
<p>I get the following error:</p>
<pre><code>
&lt;ipython-input-71-afdfac905e59&gt;:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sample_input_tensor = torch.tensor(sample_input, dtype=torch.float32)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-71-afdfac905e59&gt; in &lt;cell line: 18&gt;()
     16 
     17 # Generate the GradCAM heatmap
---&gt; 18 heatmap = gradcam.generate_heatmap(sample_input_tensor, target_layer_index)

3 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in __call__(self, *args, **kwargs)
     69             if module is None:
     70                 raise RuntimeError(&quot;You are trying to call the hook of a dead Module!&quot;)
---&gt; 71             return self.hook(module, *args, **kwargs)
     72         return self.hook(*args, **kwargs)
     73 

TypeError: save_grad() takes 1 positional argument but 3 were given
</code></pre>
<p>Does anybody know why this is happening?</p>
<p>I tried using keras, but that did not work. I also tried changing the save_grad() functions but nothing seems to work. Maybe I am making some kind of logic error?</p>
","2024-05-17 11:12:42","0","Question"
"78494982","","PyTorch DataLoader hangs when num_workers > 0 with custom torchvision transform","<p>I’m using PyTorch’s DataLoader to load my dataset. I’ve noticed that my program hangs indefinitely during training when I set num_workers &gt; 0. However, it works fine when num_workers = 0.</p>
<p>Here’s a simplified version of my code:</p>
<pre><code>class MedianFilter:
    def __init__(self, kernel_size=3):
        self.kernel_size = kernel_size

    def __call__(self, img):
        return img

train_transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    MedianFilter(),
    transforms.RandomAffine(degrees=40, translate=(0.125, 0.125)),
    transforms.RandomResizedCrop(size=(28, 28), scale=(1, 1), ratio=(1, 1), interpolation=InterpolationMode.BILINEAR),
    transforms.ToTensor()
])
val_transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    MedianFilter(),
    transforms.ToTensor()
])

train_dataset = ImageFolder(root='../Dataset/Original/train/', transform=train_transform)
val_dataset = ImageFolder(root='../Dataset/Original/val/', transform=val_transform)
train_dataloader = DataLoader(train_dataset, batch_size=64, pin_memory=True, num_workers=3, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=64, pin_memory=True, num_workers=3)

dataloader = {'train':train_dataloader, 'val':val_dataloader}
</code></pre>
<pre><code>def train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs):

    acc_history = {'train' : [], 'val' : []}
    loss_history = {'train' : [], 'val' : []}
    best_acc = 0.0

    for epoch in range(1, num_epochs+1):
        print(f&quot;Epoch{epoch}:&quot;)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()
            running_correct = 0
            running_loss = 0.0
            totalIm = 0
            for data, _label in dataloader[phase]: # Stuck here
</code></pre>
<p>In this code, the <code>MedianFilter</code> class is a simple identity function. Despite this, the program still hangs when <code>num_workers &gt; 0</code>.</p>
<p>Why is this happening and how can I resolve this issue?</p>
<p>I have tried to simplify the <code>MedianFilter</code> class to a simple identity function that just returns the input image. Despite this simplification, the program still hangs when <code>num_workers &gt; 0</code>. I expected that this change would resolve the issue, as the MedianFilter class is no longer doing any significant computation. However, the problem persists.
I have also tried running the code without the custom torchvision transform and setting <code>num_workers &gt; 0</code>. In this case, the code runs as expected.</p>
","2024-05-17 10:15:40","1","Question"
"78493634","78401518","","<p>There is No Straightforward way. Please, Go through the following methods.</p>
<p>Using torch.tensor</p>
<p>Method 1 :</p>
<pre><code>import torch
from scipy.sparse import csr_matrix
import torch
from numpy import nonzero
import networkx as nx
import matplotlib.pyplot as plt


adj_matrix = torch.tensor([[0, 1, 0],
                           [1, 0, 1],
                           [0, 1, 0]])

# Convert adjacency matrix to adjacency list
src_indices, dst_indices = adj_matrix.nonzero().unbind(-1)
&quot;&quot;&quot;
split_sizes : it tells us how many outgoing edges (neighbors) each node has in the graph.
minlength=adj_matrix.size(0) =&gt; the same length as the number of nodes in the graph.
torch.bincount: This function is used to count the number of occurrences 
of each element in the input tensor src_indices.

It counts the occurrences of each unique value in src_indices.
&quot;&quot;&quot;
split_sizes = torch.bincount(input=src_indices, minlength=adj_matrix.size(0))
&quot;&quot;&quot;
counts the occurrences of each unique value in src_indices
&quot;&quot;&quot;
adj_list = torch.split(dst_indices, split_sizes.tolist())

# Convert adjacency list tensors to regular lists
res = [x.tolist() for x in adj_list]
print(res)#[[1], [0, 2], [1]]
</code></pre>
<p>Method 2:</p>
<pre><code>from scipy.sparse import csr_matrix
from numpy import nonzero
import torch

adj_matrix = torch.tensor([[0, 1, 0],
                           [1, 0, 1],
                           [0, 1, 0]])

mask = (adj_matrix != 0)
source_indices  = mask.nonzero(as_tuple=True)[0]
print(source_indices )
destination_indices = mask.nonzero(as_tuple = True)[1]
print(destination_indices)
&quot;&quot;&quot;
It holds a tensor where each row represents an edge in the graph. 
The first element in each row is the source node index, 
and the second element is the destination node index.
dim=1: This argument specifies that the tensors should be stacked along 
the first dimension (columns in this case)
&quot;&quot;&quot;
edge_index = torch.stack(tensors = (source_indices,destination_indices), dim=1)
print(edge_index)
&quot;&quot;&quot;
tensor([[0, 1],
        [1, 0],
        [1, 2],
        [2, 1]])
&quot;&quot;&quot;
&quot;&quot;&quot;
.sum(dim=1): This performs a summation along dimension 1 (columns) for each row.
It counts the number of True values (non-zero elements) in each row.
adj_list  : Stores the number of outgoing edges (neighbors) for each node in the graph. 
&quot;&quot;&quot;
adj_list = (adj_matrix != 0 ).sum(dim=1).tolist()
print(adj_list)#[1, 2, 1]
adj_list = [[] for _ in adj_list]
print(adj_list)#[[], [], []]
&quot;&quot;&quot;
src: Represents the source node index (the node where the edge originates).
dst: Represents the destination node index (the node where the edge points to)
&quot;&quot;&quot;

# Scatter destination nodes into their corresponding lists
for i, (src, dst) in enumerate(edge_index):
    adj_list[src.item()].append(dst.item())  # Append destination node directly

print(adj_list)#[[1], [0, 2], [1]]
    
</code></pre>
<p>Without Using torch.tensor</p>
<pre><code>import numpy as np
from scipy.sparse import csr_matrix 

# Example with sparse CSR matrix
sparse_adj_matrix = csr_matrix([[0, 1, 0],
                                [1, 0, 1],
                                [0, 1, 0]])

def adjacency_matrix_to_list(adj_matrix):
    if not isinstance(adj_matrix, csr_matrix):
    # Convert dense matrix to sparse CSR format for potential efficiency gains
     adj_matrix = csr_matrix(adj_matrix)

    num_nodes = adj_matrix.shape[0]
    print('num_nodes :', num_nodes) #3
    
    # Extract neighboring nodes using CSR matrix attributes
    split_indices = np.split(adj_matrix.indices,adj_matrix.indptr[1:-1] )
    print('split_indices :',split_indices) #split_indices : [array([1]), array([0, 2]), array([1])]

    # Convert split_indices to a list of lists
    adj_list = [arr.tolist() for arr in split_indices]

    return adj_list

adj_list = adjacency_matrix_to_list(sparse_adj_matrix)
print(adj_list)  # Output: [[1], [0, 2], [1]]
</code></pre>
<p>Or less efficient</p>
<pre><code>adj_matrix = np.array([[0, 1, 0],
                       [1, 0, 1],
                       [0, 1, 0]])


adj_list = [list(np.where(row)[0]) for row in adj_matrix]

print(adj_list)  # Output: [[1], [0, 2], [1]]
</code></pre>
","2024-05-17 05:30:59","0","Answer"
"78489275","78488981","","<p>Just to expand on my comment, I don' think you can set different parts of the same tensor to have different <code>requires_grad</code> settings directly. Rather, you could use use a backward hook to selectively disable gradients for the old values. A backward hook is a PyTorch function that facilitates the execution of custom operations while gradients are being computed, during the backward pass. It can be applied to a tensor or a module. When modifying gradients, implementing custom gradient calculations, or inspecting gradients as they are computed, backward hooks are beneficial.</p>
<p>PyTorch supports two primary kinds of backward hooks:</p>
<ul>
<li>Tensor backward hooks are designated on specific tensors and are invoked when the backward pass for those tensors occurs.</li>
<li>Backward Hooks for Modules: These hooks, which are registered on modules such as <code>nn.Linear</code> and <code>nn.Conv2d</code>, are invoked when the backward pass for those modules occurs.</li>
</ul>
<p>In your case, you would use a tensor backward hook, which will be registered directly on the tensor (the <code>weight_mat</code> tensor) using <code>register_hook</code>. The idea is the backward hook is used to zero out the gradients for the <code>old_values</code> part of the weight tensor, effectively freezing those values during training.</p>
<p>In Python, the code could look something like this:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim

class MyModel(nn.Module):
    def __init__(self, old_length, new_length):
        super(MyModel, self).__init__()
        self.total_length = old_length + new_length
        self.weight_mat = nn.Parameter(torch.randn(1, 1, self.total_length, requires_grad=True))
        self.old_length = old_length

    def forward(self, x):
        return torch.matmul(x, self.weight_mat)

    def zero_grad_old_values(self, grad):
        grad_clone = grad.clone()
        grad_clone[0, 0, :self.old_length] = 0
        return grad_clone

old_length = 5
new_length = 3

model = MyModel(old_length, new_length)

# Register hook
model.weight_mat.register_hook(model.zero_grad_old_values)

optimizer = optim.SGD(model.parameters(), lr=0.01)

input = torch.randn(1, 1, old_length + new_length)
target = torch.randn(1, 1, 1)


optimizer.zero_grad()
output = model(input)
loss = nn.MSELoss()(output, target)
loss.backward()
optimizer.step()

print(&quot;Weight matrix gradients after backward pass:&quot;)
print(model.weight_mat.grad)
</code></pre>
","2024-05-16 10:39:24","2","Answer"
"78488981","","Set a part of weight tensor to requires_grad = True and keep rest of values to requires_grad = False","<p>I am doing some kind of transfer learning, where I load a dense model and then expand the weight tensor and train only the new values after expanding it and keep the old trained values frozen. in this case I need to set the new weights to <code>requires_grad = True</code> and old weights to <code>requires_grad = False</code> within the same weight tensor. I tried this but it doesnt work:</p>
<pre><code>old_values = weight_mat[0, :, :length[0]] 
old_values.requires_grad = False # 1. I tried this and they got optimized
old_values = old_values.unsqueeze(0).detach() # 2. I tried this in addition to 1 and they get optimized
new_values = weight_mat[:, :, length[0]:]
new_values.requires_grad = True
weight_mat = torch.cat((old_values, new_values), dim=-1)
</code></pre>
<p>After I print the number of parameters of models that are not trainable I get 0, I also checked the weight tensor values over epochs and found that all values are updated, whereas I am setting <code>old_values</code> to <code>False</code>.</p>
","2024-05-16 09:46:29","2","Question"
"78488771","78433332","","<p>I made some edits to the CustomDataset code you provided, which might help you:</p>
<pre><code>class CustomDataset(InMemoryDataset):
    def __init__(self, listOfDataObjects):
        super().__init__()
        self.data, self.slices = self.collate(listOfDataObjects)
    
    def __len__(self):
        return len(self.slices)
    
    def __getitem__(self, idx):
        sample = self.get(idx)
        return sample
</code></pre>
<p>A list of Data objects is inefficient to store. That's why collation is done most of the time at the end of the initialization. To retrieve items from the data, you can't index the collated Data object, this will result in a KeyIndexError like you experienced. However, Pytorch Geometric <a href=""https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/in_memory_dataset.html#InMemoryDataset.get"" rel=""nofollow noreferrer"">InMemoryDataset</a> implemented a <code>get</code> method that does the indexing for you! They use a <code>separate</code> method that can handle the collated objects and uses the slices provided to separate the indexed object from the collated object.</p>
","2024-05-16 09:10:38","0","Answer"
"78487112","78487091","","<pre><code>image_path_list = list(image_p.glob(&quot;**/*&quot;))
</code></pre>
<p>The glob pattern <code>**</code> matches any depth. This is pretty fast, as it is fully implemented in C, and should not need parallelisation (especially compared to anything else you are likely to do with the result).</p>
<p>In general, you might want to iterate over the generator instead of converting it into a list if the list would consume inordinate amount of memory though; but since you say you are creating a pytorch <code>Dataset</code>, which needs to implement random access via <code>__getitem__</code>, it might not be the best approach here; straight up list works better.</p>
","2024-05-16 01:04:16","0","Answer"
"78487091","","Python is running an io task in paralell possible, which requires a for loop inevitably?","<p>let me clarify my problem with bullet points.</p>
<ul>
<li>In <code>PyTorch</code>(or whatever the reason), I'm making a <code>Dataset</code> class</li>
<li>I need to get all paths of the images in folder</li>
<li>but the images are structured under several subfolders</li>
<li>So the actual code I'm using is</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>p = Path(self.root_dir) / &quot;Training&quot; if self.is_train else &quot;Validation&quot;

image_p = p / &quot;01.원천데이터&quot; / f&quot;{&quot;T&quot; if self.is_train else &quot;V&quot;}S_images&quot;
label_p = p / &quot;02.라벨링데이터&quot; / f&quot;{&quot;T&quot; if self.is_train else &quot;V&quot;}L_labels&quot;

# set the return lists
image_path_list = []
label_list = []

# get image paths
for sentence_dir in image_p.glob(&quot;*&quot;):  # only have several subfolders
    for true_false_dir in sentence_dir.glob(&quot;*&quot;):  # only have several subfolders too                
        for posture_dir in true_false_dir.glob(&quot;*&quot;):  # only have several subfolders again
            image_path = sorted(list(posture_dir.glob(&quot;*&quot;)))[-1]  # in 'posture_dir', there are images, but I need only the last one
            image_path_list.append(str(image_path))
</code></pre>
<ul>
<li>What is important here, is that I need to go deep down to the very bottom of the subfolders to get the actual path of an image</li>
</ul>
<p><strong>Is there any way I could make this execution faster?</strong></p>
<p>Most of the resources about <code>multiprocessing</code> or <code>multithreading</code> seem to have a concept of vectorising a function with a list type of argument passed, but not sure if that would fit with my situation now...</p>
","2024-05-16 00:52:53","0","Question"
"78485982","78483237","","<p>It's a numerical precision issue. Note that if I calculate the eigenvalues of <code>F</code>, I get negative values, meaning your matrix is not PSD:</p>
<pre><code>import torch
device = torch.device('cuda')
torch.manual_seed(1)
size = 4096
F = torch.rand(int(size / 2), int(size / 2)).to(device)
F = torch.matmul(F, F.T)
eigvals, _ = torch.linalg.eig(F)
print(eigvals.real.min())

</code></pre>
<p>On my machine, the value is <code>-0.000232202626648359</code></p>
<p>Obviously, mathematically <code>F @ F.T</code> is a PSD matrix. So, I suggest you generate your matrix more robustly, like this:</p>
<pre><code>import torch
device = torch.device('cuda')
torch.manual_seed(1)
size = 4096


def generate_psd_matrix(size, min_condition_number = 0.01, max_condition_number = 100):
    A = torch.randn(size, size)
    Q, _ = torch.linalg.qr(A)
    singular_values = torch.FloatTensor(size).uniform_(min_condition_number, max_condition_number)
    D = torch.diag(singular_values)
    return Q @ D @ Q.T


F = generate_psd_matrix(size)
u, s, v = torch.svd(F)
condition_number = s.max().item() / s.min().item()
print(&quot;Condition number:&quot;, condition_number)
L = torch.linalg.cholesky(F)
print(L)

</code></pre>
","2024-05-15 18:52:48","2","Answer"
"78485724","78483784","","<p>It depends on what want to print.
Batch loss is the loss of your model on a particular batch.
Epoch loss is the average of all batch losses for an epoch.
If you want to print the batch loss, you print <code>running_loss</code> after every batch:</p>
<pre><code>def train(net,trainloader,epochs,use_gpu = True): 
    net.train()
    # Train the network
    for epoch in range(epochs): 
        print (&quot;Epoch {}/{}&quot;.format(epoch+1, epochs))
        running_loss = 0.0
        running_corrects = 0
        for i, data in enumerate(trainloader, 0):
            images, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()
            outputs = net(images)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
                        
            epoch_loss = running_loss/len(trainloader.dataset)
    
            print('Batch loss: {}'.format(loss.item()))
</code></pre>
<p>If, on the other hand, you want to print the epoch loss, you should use your second option</p>
","2024-05-15 17:59:37","1","Answer"
"78485084","78483209","","<p>Use  <a href=""https://huggingface.co/datasets/bakks/flan-t5-onnx"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/bakks/flan-t5-onnx</a>  instead.</p>
<p>And to convert the <code>google/flan-t5</code>, see <a href=""https://huggingface.co/datasets/bakks/flan-t5-onnx/blob/main/exportt5.py"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/bakks/flan-t5-onnx/blob/main/exportt5.py</a></p>
<pre><code>from pathlib import Path
import transformers as t
from transformers import AutoTokenizer, pipeline
from optimum.onnxruntime import ORTModelForSeq2SeqLM

# print out the version of the transformers library
print(&quot;transformers version:&quot;, t.__version__)



models = [
    #&quot;google/flan-t5-small&quot;,
    #&quot;google/flan-t5-base&quot;,
    #&quot;google/flan-t5-large&quot;,
    &quot;google/flan-t5-xl&quot;,
    &quot;google/flan-t5-xxl&quot;,
]

for model_id in models:
    model_name = model_id.split(&quot;/&quot;)[1]
    onnx_path = Path(&quot;onnx/&quot; + model_name)

    # load vanilla transformers and convert to onnx
    model = ORTModelForSeq2SeqLM.from_pretrained(model_id, from_transformers=True)
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # save onnx checkpoint and tokenizer
    model.save_pretrained(onnx_path)
    tokenizer.save_pretrained(onnx_path)
</code></pre>
<p>Then try again:</p>
<pre><code>import onnxruntime

onnx_model = onnxruntime.InferenceSession(
  onnx_path, providers=[&quot;CUDAExecutionProvider&quot;]
)
</code></pre>
","2024-05-15 15:44:42","1","Answer"
"78484297","","""C:\torch\lib\fbgemm.dll"" or one of its dependencies","<p>OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\apps\python\lib\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.</p>
<p>System information:</p>
<p>Operating system: Windows 11
Python version: 3.15
PyTorch version: 2.3.0
I'm not sure what else to try to resolve this issue. Any help or suggestions would be greatly appreciated. Thank you!</p>
<p>I've tried reinstalling PyTorch using pip uninstall and pip install commands, but the issue persists.
I've checked my system environment variables and ensured that the paths to Python</p>
","2024-05-15 13:29:17","4","Question"
"78483784","","Print loss at each epoch or each index_data in Pytorch","<p>I'm newer in Pytorch, so please help, I'm confused about the position to calculate epoch_loss in training dataset:</p>
<pre><code>def train(net,trainloader,epochs,use_gpu = True): 
    ..
    net.train()
    # Train the network
    for epoch in range(epochs): 
        print (&quot;Epoch {}/{}&quot;.format(epoch+1, epochs))
        running_loss = 0.0
        running_corrects = 0
        for i, data in enumerate(trainloader, 0):
            images, labels = data[0].to(device), data[1].to(device)
            
            running_loss += loss.item()
                        
            epoch_loss = running_loss/len(trainloader.dataset)
    
            print('Loss: {}'.format(epoch_loss))  
</code></pre>
<p>Where I must print Loss, like the code or at each epoch like below</p>
<pre><code>for epoch in range(epochs):
    for i, data in enumerate(trainloader, 0):
    ...
    epoch_loss = running_loss/len(trainloader.dataset)
    print('Loss: {}'.format(epoch_loss)) 
    
</code></pre>
","2024-05-15 11:59:03","2","Question"
"78483366","78439640","","<p>Switching to Linux certainly works, but seems to not be necessary.
If you choose this route, I recommend you start with <a href=""https://ubuntu.com/download"" rel=""nofollow noreferrer"">Ubuntu</a> as there is a lot of information available.</p>
<p>CUDA Toolkit however is only available if you have a NVIDIA GPU and will not work for you.</p>
<p>To get Pytorch to work on Windows, check out this stack-overflow question as it is quite detailed: <a href=""https://stackoverflow.com/questions/63008040/how-to-use-amd-gpu-for-fastai-pytorch"">How to use AMD GPU for fastai/pytorch?</a></p>
","2024-05-15 10:41:53","1","Answer"
"78483237","","PyTorch Cholesky Decomposition Fails on GPU but Works on CPU for same matrix","<p>I'm encountering an issue with Cholesky decomposition in PyTorch when running on a GPU. The following code works perfectly on the CPU:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
device = torch.device('cpu')
torch.manual_seed(1)
size = 4096
F = torch.rand(int(size / 2), int(size / 2)).to(device)
F = torch.matmul(F, F.T)
torch.linalg.cholesky(F)
</code></pre>
<p>However, when I move the matrix to the GPU, I get an error:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
device = torch.device('cuda')
torch.manual_seed(1)
size = 4096
F = torch.rand(int(size / 2), int(size / 2)).to(device)
F = torch.matmul(F, F.T)
torch.linalg.cholesky(F)
</code></pre>
<p>The error message is:</p>
<pre><code>    torch.linalg.cholesky(F)
torch._C._LinAlgError: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 2044 is not positive-definite).
</code></pre>
<p>Why does this happen? How can I resolve this issue?</p>
<p><strong>PS:
PyTorch version: 2.3.0
CUDA version: 12.1</strong></p>
","2024-05-15 10:18:10","1","Question"
"78483209","","Error while converting google flan T5 model to onnx","<p>I am looking to convert flan-T5 model downloaded from Hugging face into onnx format and make inference with the same.</p>
<p>My input data is the <strong>symptoms of disease</strong> and expected output is the <strong>Disease name</strong></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import onnx

# Set the device to GPU
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Load the model and tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;google/flan-t5-xl&quot;).to(device)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/flan-t5-xl&quot;)

  

# Export the model to ONNX format
onnx_path = &quot;flan-t5-xl.onnx&quot;
dummy_input = tokenizer(&quot;What's the disease name in this text: Example text&quot;, return_tensors=&quot;pt&quot;, padding=True).to(device)
dummy_input_ids = dummy_input[&quot;input_ids&quot;]
dummy_attention_mask = dummy_input[&quot;attention_mask&quot;]
dummy_decoder_input_ids = tokenizer(&quot;&lt;pad&gt;&quot;, return_tensors=&quot;pt&quot;).input_ids.to(device)

with torch.no_grad():
    torch.onnx.export(
        model,
        (dummy_input_ids, dummy_attention_mask, dummy_decoder_input_ids),
        onnx_path,
        opset_version=11,
        input_names=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;decoder_input_ids&quot;],
        output_names=[&quot;output&quot;],
        dynamic_axes={
            &quot;input_ids&quot;: {0: &quot;batch_size&quot;},
            &quot;attention_mask&quot;: {0: &quot;batch_size&quot;},
            &quot;decoder_input_ids&quot;: {0: &quot;batch_size&quot;},
            &quot;output&quot;: {0: &quot;batch_size&quot;, 1: &quot;sequence_length&quot;},
        },
    )
print(f&quot;Model saved to {onnx_path}&quot;)

# Inference using the ONNX model on GPU

import onnxruntime

onnx_model = onnxruntime.InferenceSession(onnx_path, providers=[&quot;CUDAExecutionProvider&quot;]

)
</code></pre>
<blockquote>
<p>InvalidGraph: [ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from flan-t5-xl.onnx failed:This is an invalid model. Type Error: Type 'tensor(int64)' of input parameter (/decoder/block.0/layer.0/SelfAttention/Sub_output_0) of operator (Min) in node (/decoder/block.0/layer.0/SelfAttention/Min) is invalid.</p>
</blockquote>
<pre><code>input_text = input(&quot;Enter Disease/Symptom Detail: &quot;)
inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, padding=True).to(device)
input_ids = inputs[&quot;input_ids&quot;]
attention_mask = inputs[&quot;attention_mask&quot;]
decoder_input_ids = tokenizer(&quot;&lt;pad&gt;&quot;, return_tensors=&quot;pt&quot;).input_ids.to(device)

onnx_inputs = {
    &quot;input_ids&quot;: input_ids.cpu().numpy(),
    &quot;attention_mask&quot;: attention_mask.cpu().numpy(),
    &quot;decoder_input_ids&quot;: decoder_input_ids.cpu().numpy(),
}

onnx_output = onnx_model.run(None, onnx_inputs)[0]
decoded_output = tokenizer.decode(onnx_output[0], skip_special_tokens=True)

print('-' * 100)
print(f&quot;Name of Disease based on Entered Text: {decoded_output}&quot;)
</code></pre>
","2024-05-15 10:12:57","1","Question"
"78482086","78439767","","<p>Well, actually using <code>neuralnetwork</code> backend and knowing the fact that depth shape is <code>1xHxW</code> the following modifications made for <code>shape</code> and <code>scale</code> values did the trick:</p>
<pre class=""lang-py prettyprint-override""><code>import coremltools as ct
import torch

x = torch.rand(1, 3, 518, 518)
traced_model = torch.jit.trace(depth_anything, x, strict=False)

mlmodel = ct.convert(traced_model,inputs=[ct.ImageType(shape=x.shape,bias=[-0.485/0.229,-0.456/0.224,-0.406/0.225],scale=1.0/255.0/0.226)], convert_to='neuralnetwork')

mlmodel.save('/content/drive/MyDrive/trained_models/depth_anything.mlmodel')
</code></pre>
<p>I'm not sure it's a good solution, but as a workaround I've achieved it had covered all my needs</p>
","2024-05-15 06:48:46","1","Answer"
"78481194","78480367","","<p>I figured it out after many trials. The problem was that my C partition ran out of space, and building the image required more space. Once I made space for it, I restarted the build process, and it worked.</p>
","2024-05-15 00:52:11","1","Answer"
"78480949","78320397","","<p>I had the same problem with faster-whisper and after a tremendous time of web search I figured that faster-whisper reimplementation of OpenAI's Whisper model using CTranslate2 and the problem is with CTranslate2 that in the default version needs CUDA 12, check this here:<a href=""https://github.com/SYSTRAN/faster-whisper"" rel=""nofollow noreferrer"">https://github.com/SYSTRAN/faster-whisper</a></p>
<p>The reason Google Colab works fine is that the CUDA version is 12.2 which contains &quot;cublas64_12.dll&quot;, you can check that by &quot;!nvidia-smi&quot; and I use CUDA 11.8 and that is why the &quot;cublas64_12.dll&quot; is missing, CUDA 11 have &quot;cublas64_11.dll&quot;.</p>
<p>What I did to solve this problem was downgrade the CTranslate2 version to &quot;3.24.0&quot; by this command:</p>
<pre><code>pip install --upgrade --force-reinstall ctranslate2==3.24.0
</code></pre>
","2024-05-14 22:46:37","3","Answer"
"78480544","78475572","","<p>It doesn't matter so long as the you retain some reference to the input.</p>
<p>At a high level, you are trying to compute <code>output = activation(input + f(input))</code></p>
<p>Both methods shown accomplish this. As long as you don't lose the <code>input</code> reference or change <code>input</code> through an in-place operation, you should be fine.</p>
<p>For what it's worth, I would separate out the residual connection and the sub-block just for clarity:</p>
<pre class=""lang-py prettyprint-override""><code>class Block(nn.Module):
    def __init__(self, ...):
        super().__init__()
        self.conv1 = ...
        self.norm1 = ...
        self.act = ...
        self.conv2 = ...
        self.norm2 = ...

    def forward(self, x):
        x = self.conv1(x)
        x = self.norm1(x)
        x = self.act(x)
        x = self.conv2(x)
        x = self.norm2(x)
        return x

class ResBlock(nn.Module):
    def __init__(self, block):
        super().__init__()
        self.block = block
        self.act = ...

    def forward(self, x):
        return self.act(x + self.block(x))
</code></pre>
","2024-05-14 20:34:24","0","Answer"
"78480390","78480367","","<p>You could try using python:3.8-slim instead of ubuntu:24.04. This will help speed up the build process and make debugging easier.</p>
","2024-05-14 19:55:09","0","Answer"
"78480367","","Docker build taking too long and failing with requirements.txt containing TensorFlow and other packages","<p>I'm trying to build my Docker image, but it's taking forever. Even when I leave it to continue building, it fails after a long time. Here is my Dockerfile:</p>
<pre><code>FROM ubuntu:24.04

# Install Python and pip
RUN apt-get update &amp;&amp; apt-get install -y python3 python3-pip

WORKDIR /ner

COPY requirements.txt .

# Install required packages
RUN pip install -r requirements.txt --break-system-packages

COPY . .
</code></pre>
<p>I think the problem may be in the requirements file. Here is my <strong><code>requirements.txt</code></strong>:</p>
<pre><code>tensorflow==2.16.1
pydantic-settings==2.2.1
transformers==4.40.1
fastapi==0.111.0
torchvision==0.18.0
torch==2.3.0
tqdm==4.66.2
numpy==1.26.4
scikit-learn==1.4.2
pandas==2.2.1
</code></pre>
<p>I tried to build it using this command:</p>
<pre><code>docker build -t train .
</code></pre>
<p>The process is too slow and eventually fails. Could the issue be related to the size of these packages or their dependencies? Are there any optimizations I can apply to speed up the build process or resolve this issue?</p>
","2024-05-14 19:49:02","0","Question"
"78478565","","How to load Hugging Face reranker model in 16 bit?","<pre><code>from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

model = HuggingFaceCrossEncoder(model_name=&quot;BAAI/bge-reranker-base&quot;)
compressor = CrossEncoderReranker(model=model, top_n=4)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)
</code></pre>
<p>Above is the code I am using for the reranking using HuggingFaceCrossEncoder. But I want to load this model in 16 bit instead of 32 bit. Is there any way I can load this model in 16 or 8 bit</p>
<pre><code>model = HuggingFaceCrossEncoder(model_name=&quot;BAAI/bge-reranker-base&quot;,use_fp16=True)
</code></pre>
<p>I tried using above code but getting below error
ValidationError: 1 validation error for HuggingFaceCrossEncoder</p>
","2024-05-14 13:52:52","1","Question"
"78477344","","Albumentations intensity augmentations disrupt the image","<p>I'm using a preprocessed, z-score normalized list as the source for my dataset.</p>
<p>Here's a collage of images augmented by Albumentations:</p>
<p><a href=""https://i.sstatic.net/XIA8y5zc.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Here's my <code>Compose</code>:</p>
<pre><code>augmentation = A.Compose([
    A.HorizontalFlip(),
    A.RandomBrightnessContrast(brightness_limit=(-0.0001, 0.0001), contrast_limit=(-0.01, 0.01)),
    A.CoarseDropout(8, 0.1, 0.1),
    A.Rotate(limit=15),
    A.Affine(shear=(-2, 2), scale=(0.95, 1.05)),
&gt;! ToTensorV2()
])
</code></pre>
<p>On the 50% of the images that RandBrightnessContrast is applied even with very small parameters, the whole distribution of the image is squashed to [0, 1] (from ~-2,~2 as expected for z-score normalized images).
Any way around this?</p>
<p>Maybe I should perform z-score normalization after these, but my original intent was to separate all deterministic steps (resize, normalize etc.) from the augmentation steps for efficiency.</p>
","2024-05-14 10:11:54","1","Question"
"78475865","78474215","","<p>I guess the easiest way to achieve what you want is exporting <a href=""https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/"" rel=""nofollow noreferrer"">CUDA_VISIBLE_DEVICES</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;1&quot;
#or
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0,1&quot;

import torch
from transformers import LlamaForCausalLM

model_dir = '/models/Llama-2-13b-chat-hf'
model = LlamaForCausalLM.from_pretrained(model_dir,
                                         device_map='auto')
</code></pre>
<p>If you want to use the <a href=""https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.device_map"" rel=""nofollow noreferrer"">device_map</a> you have to map each layer by yourself:</p>
<pre class=""lang-py prettyprint-override""><code># distillroberta because it is smaller

from transformers import AutoModelForMaskedLM

model = AutoModelForMaskedLM.from_pretrained(&quot;distilbert/distilroberta-base&quot;)
# parameter names
print([x[0] for x in model.named_parameters()])
</code></pre>
<p>Output:</p>
<pre><code>['roberta.embeddings.word_embeddings.weight',
 'roberta.embeddings.position_embeddings.weight',
 'roberta.embeddings.token_type_embeddings.weight',
 'roberta.embeddings.LayerNorm.weight',
 'roberta.embeddings.LayerNorm.bias',
 'roberta.encoder.layer.0.attention.self.query.weight',
 'roberta.encoder.layer.0.attention.self.query.bias',
...
 'roberta.encoder.layer.5.output.LayerNorm.weight',
 'roberta.encoder.layer.5.output.LayerNorm.bias',
 'lm_head.bias',
 'lm_head.dense.weight',
 'lm_head.dense.bias',
 'lm_head.layer_norm.weight',
 'lm_head.layer_norm.bias']
</code></pre>
<p>You don't need to map each weight. It is enough when you map the layers:</p>
<pre class=""lang-py prettyprint-override""><code># device map example for distillroberta:
from transformers import AutoTokenizer, AutoModelForMaskedLM

device_map= {'roberta.embeddings':'cpu', 'roberta.encoder':0, 'lm_head':'cpu'}

model = AutoModelForMaskedLM.from_pretrained(&quot;distilbert/distilroberta-base&quot;, device_map = device_map)
</code></pre>
","2024-05-14 05:18:23","1","Answer"
"78475861","78471936","","<p><code>F.interpolate</code> only defined the <code>bilinear</code> mode for last 2 channels of a 4-D tensor. For cross entropy loss, you would like the tensor to be logits, not the index of the class. If you want to take the class index, you can do the <code>torch.argmax</code> but beware as this is non differentiable.</p>
","2024-05-14 05:17:36","1","Answer"
"78475572","","What's the correct way of expressing Residual Block with forward function of pytorch?","<p>AFAIK there are 2 ways to express ResNet Block in pytorch:</p>
<ul>
<li>Copy the input in the beginning, modify the input in the process, add the copy in the end.</li>
<li>Preserve the input in the beginning, create new variable in the process, add the input in the end.</li>
</ul>
<p>Which leads to 2 kinds of code:</p>
<pre class=""lang-py prettyprint-override""><code>def forward(self, x):
    y = x
    x = self.conv1(x)
    x = self.norm1(x)
    x = self.act1(x)
    x = self.conv2(x)
    x = self.norm2(x)
    x += y
    x = self.act2(x)
    return x
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def forward(self, x):
    y = self.conv1(x)
    y = self.norm1(y)
    y = self.act1(y)
    y = self.conv2(y)
    y = self.norm2(y)
    y += x
    y = self.act2(y)
    return y
</code></pre>
<p>Are they identical? Which one is preferred? Why?</p>
","2024-05-14 03:27:42","0","Question"
"78474376","78473387","","<p>To be able to track the learning of a model I recommend environments like <a href=""https://clear.ml/"" rel=""nofollow noreferrer"">ClearML</a>. <br> ClearML allows you to see real-time results on some images while model learning</p>
<p>Yolov5 allows you to integrate them without any problem for example <a href=""https://clear.ml/docs/latest/docs/integrations/yolov5/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Regarding --save-txt; on val.py save the validation results to a .txt file <br>
To default you can see your output into yolov5/runs/val/exp--</p>
","2024-05-13 19:20:12","0","Answer"
"78474215","","How to load pretrained model to transformers pipeline and specify multi-gpu?","<p>I have a local server with multiple GPUs and I am trying to load a local model and specify which GPU to use since we want to split GPU between team members.</p>
<p>I can successfully specify 1 GPU using device_map='cuda:3' for smaller model, how to do this on multiple GPU like CUDA:[4,5,6] for larger model?</p>
<p>(I tried using device_map = 'auto', 'balanced', 'sequential', which will spread model automatically. But this is not what we want...)</p>
<pre><code>import torch
from transformers import LlamaForCausalLM

model_dir = '/models/Llama-2-13b-chat-hf'

# 'auto' 'balanced' 'sequential' 'balanced_low_0'
# 'cuda:3',

model = LlamaForCausalLM.from_pretrained(model_dir,
                                         device_map='cuda:[3,4,5]',#how to make things work here?
                                         torch_dtype=torch.float32 
                                        )
</code></pre>
","2024-05-13 18:42:34","1","Question"
"78473387","","I am using the YOLOv5 model provided by Ultralytics in PyTorch. How can I see which images the model is struggling with?","<p>This is the <a href=""https://github.com/ultralytics/yolov5"" rel=""nofollow noreferrer"">YOLOv5</a> implementation I am talking about and <a href=""https://github.com/ultralytics/yolov5/blob/master/val.py"" rel=""nofollow noreferrer"">this</a> is the file I am using to test the model.</p>
<p>For some classes, it performing decently enough. However, for the rest of the classes, it is not doing a great job. I would like to see the type of images where the model struggles.</p>
<p>How can I get the name of the images or the file paths?</p>
<p>I tried running <a href=""https://github.com/ultralytics/yolov5/blob/master/val.py"" rel=""nofollow noreferrer"">this file</a> with the --save-txt parameter but I do not understand its meaning.</p>
<p>Thank you!</p>
","2024-05-13 15:41:27","0","Question"
"78472305","78471663","","<p>Here is a link to what device side asserts and errors are: <a href=""https://stackoverflow.com/questions/55780923/what-does-runtimeerror-cuda-error-device-side-assert-triggered-in-pytorch-me"">What does &quot;RuntimeError: CUDA error: device-side assert triggered&quot; in PyTorch mean?</a>. It looks like the real error here was an illegal memory access, which sometimes happens due to CUDA out of memory on the GPU.</p>
<p>As for the CPU case, this might also be failing due to running out of RAM. In this link, the user fixed it by running on a system with more RAM - <a href=""https://github.com/microsoft/vscode-jupyter/issues/13678"" rel=""nofollow noreferrer"">https://github.com/microsoft/vscode-jupyter/issues/13678</a>.</p>
<p>I would try running the above code with a much smaller model and see if that produces any different type of errors. Or if this is in colab, try increasing the CPU/GPU memory available.</p>
<p>Edit:
The problem is likely with the dataset. In a Coogle Collaboratory notebook the code works on both CPU and GPU. There was a missing <code>import os</code> line, and the dataset was missing, so I created a fake, randomized dataset. See the code here</p>
<pre><code># Python
import os
import torch
import torchvision
from torchvision.models.detection import MaskRCNN
import gc
import torch.nn as nn
from torchvision.models.detection.rpn import AnchorGenerator
from torch.cuda.amp import GradScaler
os.environ[&quot;CUDA_LAUNCH_BLOCKING&quot;] = &quot;1&quot;

gc.collect()

torch.cuda.empty_cache()

# Define the model
resnet_net = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)

modules = list(resnet_net.children())[:-1]
backbone = nn.Sequential(*modules)
backbone.out_channels = 512


# Define the anchor generator
anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),
                                   aspect_ratios=((0.5, 1.0, 2.0),))

# Define the model with the configured backbone and anchor generator
model = MaskRCNN(backbone=backbone, num_classes=91, rpn_anchor_generator=anchor_generator)

# Move the model to the GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
model.to(device)

# Define the optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)
scaler = GradScaler()

# Train the model
num_epochs = 1
for epoch in range(num_epochs):
    model.train()
    counter = 0
    for i in range(10):
        # images, height, targets, names in train_ds.
        n_samples = 2
        images = torch.rand(n_samples, 3, 800, 800).to(device)
        height = 800
        targets = [{'boxes': torch.tensor([[0, 0, 800, 800]]), 'labels': torch.tensor([1]), 'masks': torch.rand(1, 800, 800).to(device)}] * n_samples
        print(counter)
        counter += 1

        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        optimizer.zero_grad()

        with torch.cuda.amp.autocast():
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
        
        scaler.scale(losses).backward()
        scaler.step(optimizer)
        scaler.update()
</code></pre>
","2024-05-13 12:42:08","0","Answer"
"78471966","78469095","","<p>The <a href=""https://pytorch.org/docs/stable/generated/torch.linalg.eig.html"" rel=""nofollow noreferrer"">documentation of <code>torch.linalg.eig</code></a> states that the eigenvectors will be given by the <em>columns</em> of the resulting tensor (<code>eigvecs</code> in your case), what you got with <code>eigvecs[0]</code> was the first <em>row</em>.</p>
<p>Using <code>eigvecs.T[0]</code> or <code>eigvecs[:, 0]</code> will yield the expected (i.e. close to zero) result:</p>
<pre class=""lang-py prettyprint-override""><code>print(m.type(torch.complex64) @ eigvecs.T[0] - eigvals[0] * eigvecs.T[0])
# &gt;&gt;&gt; tensor([ 3.5763e-07+0.j,  5.9605e-07+0.j,  4.7684e-07+0.j,  2.3842e-07+0.j,
#              3.5763e-07+0.j,  2.3842e-07+0.j, -2.3842e-07+0.j,  1.1921e-07+0.j])
</code></pre>
","2024-05-13 11:36:50","1","Answer"
"78471936","","Issue with the pytorch interpolate","<p>I am new to model and deep learning training , In my training section i am trying to find the loss of the segmentation of the image , so in here before cross entropy loss calculation i have used interpolate to downsize the image which came out of the model,</p>
<p>Here the size of the image from the model which came out is (5,36,180,320)<br />
here the 5 is the batch size , 36 is the channel size and the 180, 320 is the h x w</p>
<p>The target image shape is (5,1,180,320)<br />
the 5 is the batch size , 1 is the channel , 180 x 320 is the h x w<br />
the target image has 1 channel value from 0 to 35 which is number of available segmenatation</p>
<p>Now i am downsizing to the image to (1,180,320)<br />
so it can match the target image size but the issue in here is</p>
<p>i used it like this</p>
<pre><code>outs = F.interpolate(out, target.size()[1:] , mode = 'bilinear' , aligin_corners = False).squeeze(dim = 1 )
# out -&gt; is the image
# size is mentioning (1,180,320)
crit_loss = crit(outs , target.squeeze(dim=1))
# here crit is cross entropy loss
loss += (loss_coeff * crit_loss)
</code></pre>
<p>i get the error</p>
<blockquote>
<p>Input and the output must have the same number of spatial dimentions, but got input with spatial dimensions of [180,320] and output size of torch.Size([1,180,320]).please provide input tensor</p>
</blockquote>
<p>I have tried all the the ways that i came up with nothing worked, if i use the sizr as (180,320) inside the interpolate i get the shape outs as 36,180,320 which cause issue at the crit what should i do where it does went wrong, please help me</p>
","2024-05-13 11:30:23","0","Question"
"78471663","","What do these TORCH_USE_CUDA_DSA and frozen_modules errors mean and how to fix them?","<p>I'm trying to run a Mask R-CNN model with aerial imagery. To optimise this, I run everything with CUDA. But this creates a few errors. Here is my code:</p>
<pre><code># Python
import torch
import torchvision
from torchvision.models.detection import MaskRCNN
import gc
import torch.nn as nn
from torchvision.models.detection.rpn import AnchorGenerator
from torch.cuda.amp import GradScaler
os.environ[&quot;CUDA_LAUNCH_BLOCKING&quot;] = &quot;1&quot;

gc.collect()

torch.cuda.empty_cache()

# Define the model
resnet_net = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)

modules = list(resnet_net.children())[:-1]
backbone = nn.Sequential(*modules)
backbone.out_channels = 512


# Define the anchor generator
anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),
                                   aspect_ratios=((0.5, 1.0, 2.0),))

# Define the model with the configured backbone and anchor generator
model = MaskRCNN(backbone=backbone, num_classes=91, rpn_anchor_generator=anchor_generator)

# Move the model to the GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Define the optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)
scaler = GradScaler()

# Train the model
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    counter = 0
    for images, height, targets, names in train_ds:
        print(counter)
        counter += 1

        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        optimizer.zero_grad()

        with torch.cuda.amp.autocast():
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
        
        scaler.scale(losses).backward()
        scaler.step(optimizer)
        scaler.update()
</code></pre>
<p>If I run this code on the gpu, I will at some point get this error:
<code>RuntimeError: CUDA error: an illegal memory access was encountered Compile with &quot;TORCH_USE_CUDA_DSA&quot; to enable device-side assertions.</code></p>
<p>And if I run it on the cpu, I will get this error:
<code>[error] Disposing session as kernel process died ExitCode: 3221225477, Reason: 0.00s - Debugger warning: It seems that frozen modules are being used, which may 0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off 0.00s - to python to disable frozen modules. 0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.</code></p>
<p>I have encountered some CUDA memory problems before with this code, and this seems related. What are these frozen modules and is it safe to turn them off? Also, I tried to enable this TORCH_USE_CUDA_DSA in my code by adding this:
<code>os.environ[&quot;TORCH_USE_CUDA_DSA&quot;] = &quot;1&quot;</code></p>
<p>But that didn't solve it. Also, I had one run where i didn't encounter any of these problems, and where the code ran smoothly (on the gpu).</p>
","2024-05-13 10:36:20","2","Question"
"78469797","78469509","","<p>The differences in time come from the fact the when you have to get a batch from dataloader, you will have to use a for-loop like operator, and doing some data aggregation to make it a tensor again. Modify the direct training part like this, and you will see the time increase (which is still not as slow as loading from dataloader, so dataloader may have some extra steps in checking and ready for distributed learning, ect...</p>
<pre><code>for epoch in tqdm(range(2000)):
    x_ ,y_ = zip(*[(xx, yy) for xx, yy in zip(x,y)])
    x_ = torch.stack(x_)
    y_ = torch.stack(y_)
    optimizer_direct.zero_grad()
    outputs = model_direct(x_)
    loss = nn.MSELoss()(outputs, y_)
    loss.backward()
    optimizer_direct.step()
</code></pre>
","2024-05-13 02:04:35","0","Answer"
"78469509","","Why batch training with maximum batchsize using PyTorch dataloader exhibits worse performance than inputing entire datasets to the networks?","<p>While experimenting with PyTorch for neural network training, we encounter a choice: Should we load data in batches using PyTorch's DataLoader, or should we input the entire dataset at once directly into the model (No GPU memory issues)? I was thinking that using DataLoader with a batch size equal to the entire dataset should mirror the performance of directly loading the full dataset. However, observations indicate otherwise.</p>
<p>It's observed that when using DataLoader with maximum batch size, the training performance (e.g. loss) tends to be poorer compared to loading the entire dataset directly. Moreover, this method of using DataLoader also seems to consume more time.</p>
<p>As someone new to PyTorch, I find these differences puzzling. Why does training with DataLoader in batches, even when the batch size is at its maximum, yield worse performance than loading all the data directly into the model?</p>
<p>Very appreciate any assistance with unpacking the intricacies of data loading in PyTorch and seeking explanations for these curious behaviors.</p>
<pre><code>import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

import time


# Set the random seed for reproducibility
torch.manual_seed(0)

# Generate synthetic data
x = torch.linspace(-10, 10, 1000).unsqueeze(1)  # x data tensor
y = x**2 + torch.randn_like(x) * 10  # y data with noise


class SimpleLinearModel(nn.Module):
    def __init__(self):
        super(SimpleLinearModel, self).__init__()
        self.fc1 = nn.Linear(1, 10)  # First linear layer
        self.relu = nn.ReLU()        # ReLU activation
        self.fc2 = nn.Linear(10, 1)  # Second linear layer to map back to output

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x
    

def train_model(model, loader, optimizer, epochs=2000):
    criterion = nn.MSELoss()
    for epoch in range(epochs):
        for x_batch, y_batch in loader:
            optimizer.zero_grad()
            output = model(x_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
    print(&quot;Loader: Loss is {}&quot;.format(loss.item()))
    return model


# Model instances
model_direct = SimpleLinearModel()
model_loader = SimpleLinearModel()

# Optimizers
optimizer_direct = optim.Adam(model_direct.parameters(), lr=0.01)
optimizer_loader = optim.Adam(model_loader.parameters(), lr=0.01)

# DataLoader
dataset = TensorDataset(x, y)
full_batch_loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)   

# Train directly using the full dataset
model_direct.train()
time_start = time.time()
for epoch in range(2000):
    optimizer_direct.zero_grad()
    outputs = model_direct(x)
    loss = nn.MSELoss()(outputs, y)
    loss.backward()
    optimizer_direct.step()

print(&quot;Direct: Time is {}&quot;.format(time.time() - time_start))
print(&quot;Direct: loss is {}&quot;.format(loss.item()))


# Train using the DataLoader
model_loader.train()
time_start = time.time()
model_loader = train_model(model_loader, full_batch_loader, optimizer_loader)
print(&quot;Loader: Time is {}&quot;.format(time.time() - time_start))

# Evaluate and compare
model_direct.eval()
model_loader.eval()
with torch.no_grad():
    direct_preds = model_direct(x)
    loader_preds = model_loader(x)

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)

plt.scatter(x.numpy(), y.numpy(), s=1)
plt.plot(x.numpy(), direct_preds.numpy(), color='r')
plt.title('Direct Training')
plt.subplot(1, 2, 2)
plt.scatter(x.numpy(), y.numpy(), s=1)
plt.plot(x.numpy(), loader_preds.numpy(), color='r')
plt.title('Training with DataLoader')
plt.show()

</code></pre>
","2024-05-12 23:03:40","0","Question"
"78469312","78396679","","<p>pip install pip install torch==2.2.1 solved my problemtorch==2.2.1 solved my problem</p>
","2024-05-12 21:11:19","0","Answer"
"78469095","","Why don't eigenpairs calculated through torch.linalg.eig pass the eigenpair test?","<p>Running the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

torch.manual_seed(47)

m = torch.rand([8, 8])

eigvals, eigvecs = torch.linalg.eig(m)

print(m.type(torch.complex64) @ eigvecs[0] - eigvals[0] * eigvecs[0])  # Should be close to →0.
</code></pre>
<p>Results with output:</p>
<pre class=""lang-py prettyprint-override""><code>tensor([ 2.2827+0.0995j, -0.9539-0.0442j, -1.9871-0.0204j, -0.6442-0.0372j,
        -0.8338-0.0518j,  1.2168-0.0077j,  0.4924-0.5008j,  0.9643+0.4292j])
</code></pre>
<p>What is going on here? How is it that the eigenpair test fails?</p>
","2024-05-12 19:33:17","0","Question"
"78468143","78286355","","<p>I have tried the solution @talonmies suggested (simply uninstall the Cuda toolkit v12.4) and run the command <code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121</code> and it worked for me.</p>
<p>I hope this can help you in any way!</p>
","2024-05-12 14:06:17","4","Answer"
"78464333","78286355","","<p>encountered your exact problem and found a solution.</p>
<ol>
<li><p>I uninstalled both Cuda and Pytorch</p>
</li>
<li><p>Reinstalled Cuda 12.1:
<a href=""https://developer.nvidia.com/cuda-12-1-1-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=11&amp;target_type=exe_local"" rel=""nofollow noreferrer"">here</a></p>
</li>
<li><p>Reinstalled latest version of PyTorch: <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">here</a></p>
</li>
<li><p>Check if PyTorch was installed correctly: <code>import torch x = torch.rand(5, 3) print(x)</code></p>
</li>
</ol>
<p>The output should be something similar to:</p>
<pre><code>tensor([[0.3380, 0.3845, 0.3217],
        [0.8337, 0.9050, 0.2650],
        [0.2979, 0.7141, 0.9069],
        [0.1449, 0.1132, 0.1375],
        [0.4675, 0.3947, 0.1426]])
</code></pre>
<ol start=""5"">
<li><p>Check if Cuda is enabled:</p>
<p><code>import torch if torch.cuda.is_available(): print(&quot;CUDA is available&quot;)</code></p>
</li>
</ol>
<p>Good Luck!</p>
","2024-05-11 11:06:27","1","Answer"
"78464214","","Recurrent neural network trained with torch.autograd predicts nonsense","<p>I'm trying to use <a href=""https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"" rel=""nofollow noreferrer"">torch.autograd</a> to train a simple recurrent neural network that predicts the next character in a sequence of characters that represent <a href=""https://github.com/yaskovdev/stack-exchange-questions-and-answers/blob/master/train-rnn-with-autograd/songs.txt"" rel=""nofollow noreferrer"">songs in an ABC notation</a>.</p>
<p>The model looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>model = keras.Sequential([
    keras.layers.Input(shape=(SEQ_LENGTH,), batch_size=batch_size),
    keras.layers.Embedding(len(vocabulary), 256),
    keras.layers.LSTM(1024, return_sequences=True, stateful=stateful),
    keras.layers.Dense(len(vocabulary))
])
</code></pre>
<p>The training process looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)
for i in range(1000):
    inputs, targets = random_inputs_and_targets(vectorized_songs, seq_length=SEQ_LENGTH, batch_size=BATCH_SIZE)

    predictions = model(inputs)
    loss = loss_fn(predictions.permute(0, 2, 1), torch.from_numpy(targets).long())

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
</code></pre>
<p>I then save the model parameters and load them into the similar model, but this time the model is stateful and has batch size <code>1</code>:</p>
<pre class=""lang-py prettyprint-override""><code>torch.save(model.state_dict(), os.path.join(cwd, &quot;model.pt&quot;))
trained_model = build_model(1, True)
trained_model.load_state_dict(torch.load(os.path.join(cwd, &quot;model.pt&quot;)))
trained_model.eval()
</code></pre>
<p>Then, I use the loaded model to predict a string of characters that I expect to look like a song in the ABC notation:</p>
<pre class=""lang-py prettyprint-override""><code>input_eval = [char_to_index[s] for s in start_string]
input_eval = torch.unsqueeze(torch.tensor(input_eval), 0)

text_generated = []

for i in range(generation_length):
    predictions = torch.squeeze(model(input_eval), 0)
    predicted_index = torch.multinomial(softmax(predictions, dim=0), 1, replacement=True)[-1, 0]
    input_eval = torch.unsqueeze(torch.unsqueeze(predicted_index, 0), 0)
    text_generated.append(index_to_char[predicted_index.item()])

return start_string + ''.join(text_generated)
</code></pre>
<p>The full code is <a href=""https://github.com/yaskovdev/stack-exchange-questions-and-answers/blob/master/train-rnn-with-autograd/main.py"" rel=""nofollow noreferrer"">here</a>.</p>
<p>During the 1000 training epochs, the loss function value goes down from around <code>4.42</code> to <code>0.78</code>, as expected.</p>
<p>But when I then try to use the &quot;trained&quot; model to generate a song, the result looks like a random string: <code>XwQ5&gt;ab&gt;6q6S(z']!&lt;hxaG4..M= (=ERp/xJmS|qIh_CzbM0D-N 6Yc=Ei[tcodBsEKfW&lt;WZ5Jb(&quot;u1rrGLcFIk&quot;PVk.'FEII:(qu7.nFbw^3/RY2LyrW</code>. An example of the full result can be seen <a href=""https://github.com/yaskovdev/stack-exchange-questions-and-answers/blob/master/train-rnn-with-autograd/predicted_songs.txt"" rel=""nofollow noreferrer"">here</a>.</p>
<p>How do I even start debugging what is going wrong? Previously I built <a href=""https://github.com/jshaipuka/ai-sandbox/blob/master/torch-sandbox/simple.py"" rel=""nofollow noreferrer"">a simple non-recurrent classifier</a> using <code>torch.autograd</code>, its outputs were only 90% accurate, but this was still much better than when I try to build an RNN. Can it be that the hidden state that the RNN needs to predict the next character is lost somewhere during training or actual prediction?</p>
<p>Any suggestions are welcome, since I'm getting stuck.</p>
","2024-05-11 10:25:12","1","Question"
"78461696","78458903","","<p>The issue has been resolved.</p>
<p>Version mismatching was creating those issues. I solved the entire issue using a conda environment.</p>
<p>Now the GPU usage is almost 98% as I wanted back then to use the complete available GPU VRAM if that is possible.</p>
<p>The workarounds that fixed the issue:</p>
<ol>
<li>Create a new Conda environment for Python=3.7.16 using <code>conda create -n fydp python=3.7.16</code>
Here, my conda environment's name is <code>fydp</code>. You can choose different names.</li>
<li>Activate the newly created conda environment using <code>conda activate fydp</code></li>
<li>Install Tensorflow GPU 2.4.1 using <code>conda install tensorflow-gpu=2.4.1</code></li>
<li>Install Pytorch 1.7.1 using <code>conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=10.1 -c pytorch</code></li>
<li>Install separate Jupyter notebook/lab using <code>conda install anaconda::jupyter</code></li>
</ol>
<p>That's it! Now the GPU usage is efficient (maximum to the available GPU VRAM memory). Therefore, the epoch time has been reduced as well.</p>
","2024-05-10 17:21:04","3","Answer"
"78460201","78455014","","<p>Even though the REMOVE_DROPOUT optimization block is working to leave those layers in the model, it appears that during optimizing for mobile, the dropout layers are being converted back to eval mode.  The fix was to swap out the dropout layers with a custom layer that does not consider the training parameter:</p>
<pre><code>class AlwaysDropout(torch.nn.Module):
    def __init__(self, dropout_prob):
        super(AlwaysDropout,self).__init__()
        self.dropout_prob = dropout_prob

    def forward(self,x):
        return torch.nn.functional.dropout2d(x,self.dropout_prob,True)
</code></pre>
<p>For my specific example, I'm using densenet from monai.  To swap out the dropout layers, I just replaced the factory function that creates them:</p>
<pre><code># overwrite the layer factory for monai Dropout
@monai.networks.layers.factories.Dropout.factory_function(&quot;dropout&quot;)
def dropout_factory(dim):
    return AlwaysDropout
</code></pre>
","2024-05-10 12:45:21","0","Answer"
"78460184","","Beartype and jaxtyping incorrectly catching Tensor type errors at runtime","<p>I'm using <code>torch</code>, <code>jaxtyping</code> and <code>beartype</code> to type annotate my functions (with runtime type checking). I have used the <code>@jaxtyped(typechecker=beartype)</code> decorator.</p>
<p>I have received the following error at runtime:</p>
<pre><code>jaxtyping.TypeCheckError: Type-check error whilst checking the parameters of display_features_from_tokens_and_feature_tensor.
The problem arose whilst typechecking parameter 'tokens'.
Actual value: tensor([41083,   531,   366, 31373,   612,     1,   290,   788,  3332,   866,
13], device='cuda:0')
Expected type: &lt;class 'Int[Tensor, 'seq_len']'&gt;.`
</code></pre>
<p>As far as I can tell the type of the actual value matches the expected type. Is there some nuance of <code>beartype</code> or <code>jaxtyping</code> that I am missing?</p>
<p>I suspected that it may be to do with the fact that my tensors were on the GPU but couldn't find any mention of this within the <code>jaxtyped</code> docs.</p>
<p>I also tried using <code>Int64</code> to no avail (same error).</p>
","2024-05-10 12:41:58","0","Question"
"78458903","","`ptxas warning : Registers are spilled to local memory` on Tensorflow and PyTorch","<p>In one of our research, we are using Tensorflow and Pytorch with other major models. Whenever we use our data server at our University, we can use the full GPU in model training. The computer has an Nvidia Titan Xp 12GB GPU.</p>
<p>On the otherhand, my home computer has an Nvidia GeForce RTX 3060 12GB GDDR6 GPU. The problem is, however I try, the models do not use the full GPU during model training. It uses almost 8 and a half GB GPU even though other portions are empty and no other application is using them. Therefore, the epoch takes longer time. Also, I am receiving a warning message in each epoch, even though the dataset, and notebooks are the same.</p>
<p>The codeblock which contains the epoch-related code is given below:</p>
<pre class=""lang-py prettyprint-override""><code>num_epoch = 100
history = model.fit(
    train_dataset,
    epochs = num_epoch,
    steps_per_epoch = len(train_paths) // batch_size,
    validation_data = test_dataset,
    validation_steps= len(test_paths) // batch_size
)
</code></pre>
<p>I am also receiving a warning in each epoch given below even though that might be irrelevant.</p>
<pre><code>WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715325398.195254   36322 service.cc:145] XLA service 0x7f1648003120 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1715325398.195290   36322 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715325400.187164   36503 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_4361', 112 bytes spill stores, 112 bytes spill loads

I0000 00:00:1715325456.896302   36322 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
</code></pre>
<p>I was wondering whether there is any way to explicitly force the notebook to use GPU memory at a fixed size. It would be able to reduce epoch time on my home computer. Please let me know what you think about it.</p>
","2024-05-10 08:32:16","1","Question"
"78458236","78457370","","<p>First, this sound just like a classification problem, so in another word: yes, it does make sense.</p>
<p>Second, yes. Pytorch provides you everythings numpy offers (well, almost). For generating random uniform data, you can use the following:</p>
<pre><code>x = torch.rand(1000, 2).sub(.5).mul(200) # &lt;- torch.rand() return range [0,1]
y = (x.sum(1, keepdim = True) &gt; 10).float().sub(.5).mul(20) # &lt;- (x.sum(1) &gt; 10).float() return {0,1}
</code></pre>
","2024-05-10 05:48:04","0","Answer"
"78457370","","How to create a tensor based on another one - Studying PyTorch in practice?","<p>I'm studying IA using PyTorch and implementing some toy examples.
First, I created a one-dimensional tensor (X) and a second tensor (y), derived from the first one:</p>
<pre class=""lang-py prettyprint-override""><code>X = torch.arange(0, 100, 1.0).unsqueeze(dim=1)
y = X * 2
</code></pre>
<p>So I have something like</p>
<pre><code>X = tensor([[0.], [1.], [2.], [3.], [4.], [5.], ...
y = tensor([[ 0.], [ 2.], [ 4.], [ 6.], [ 8.], [10.], ...
</code></pre>
<p>Then, I trained a model to predict y and it was working fine.</p>
<p>Now, I would like something different. The X will be 2D and y 1D. y is calculated by an operation in the elements of X:
<code>If x[0] + x[1] &gt;0? y = 10: y -10 </code></p>
<pre><code>X = tensor([[ 55.5348, -97.7608],
            [ 29.0493, -52.1908],
            [ 47.1722, -43.1151],
            [ 11.1242, -62.8652],
            [ 44.8067,  80.8335],...
y = tensor([[-10.], [-10.], [ 10.], [-10.], [ 10.],...
</code></pre>
<p>First question, Is it make sense in terms of Machine Learning?</p>
<p>Second one...
I'm generating the tensors using numpy. Could I do it in a smarter way?</p>
<pre class=""lang-py prettyprint-override""><code># Criar X valores de entrada para testes
X_numpy = np.random.uniform(low=-100, high=100, size=(1000,2))
print(&quot;X&quot;, X_numpy)

#y_numpy = np.array([[ (n[0]+n[1]) &gt;= 0 ? 10:-10] for n in X_numpy])
y_numpy = np.empty(shape=[0, 1])
for n in X_numpy:
    if n[0] + n[1] &gt;= 0:
        y_numpy = np.append(y_numpy, [[10.]], axis=0)
    elif n[0] + n[1] &lt; 0:
        y_numpy = np.append(y_numpy, [[-10.]], axis=0)
</code></pre>
","2024-05-09 23:28:50","-2","Question"
"78457099","78455702","","<p>The main parameter controlling the upscaling of the input is <code>stride=</code>. Setting <code>stride=2</code> with <code>kernel_size=2</code> will exactly double the input size.</p>
<p>In your case, use <code>stride=2</code> with <code>kernel_size=3</code> to get a <em>doubling + 1</em> size transformation with each upconv layer. The first layer will produce an output sized <code>2 x 63 + 1 = 127</code>, and the second will yield <code>2 x 127 + 1 = 255</code>.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.rand(1, 128, 63, 63) #the ouput from Conv2d-7 is shaped (63, 63)

x = nn.ConvTranspose2d(128, 32, kernel_size=3, stride=2)(x) #2h + 1 upconv
print(x.shape)
#out&gt; torch.Size([1, 32, 127, 127])

x = nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2)(x) #2h + 1 upconv
print(x.shape)
#out&gt; torch.Size([1, 3, 255, 255])
</code></pre>
","2024-05-09 21:43:12","2","Answer"
"78455702","","PyTorch convolutional autoencoder, output dimensions different from input","<p>I am new with working with PyTorch and wanted to make a simple autoencoder with 255x255 RGB images to play around with it, however the output shape isn't the same as the input shape.</p>
<p>Here's the model</p>
<pre class=""lang-py prettyprint-override""><code>class AutoEncoder(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )

        self.decoder = nn.Sequential(
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(in_channels=128, out_channels=32, kernel_size=3, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=3, output_padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x
</code></pre>
<p>And here are the shapes given by the torchsummary package</p>
<pre><code>----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 255, 255]             896
              ReLU-2         [-1, 32, 255, 255]               0
         MaxPool2d-3         [-1, 32, 127, 127]               0
            Conv2d-4        [-1, 128, 127, 127]          36,992
              ReLU-5        [-1, 128, 127, 127]               0
         MaxPool2d-6          [-1, 128, 63, 63]               0
            Conv2d-7          [-1, 128, 63, 63]         147,584
              ReLU-8          [-1, 128, 63, 63]               0
   ConvTranspose2d-9           [-1, 32, 66, 66]          36,896
             ReLU-10           [-1, 32, 66, 66]               0
  ConvTranspose2d-11            [-1, 3, 69, 69]             867
          Sigmoid-12            [-1, 3, 69, 69]               0
</code></pre>
<p>I have seen from another post that the <code>output_padding</code> option in the decoder part would help with the output shape but it hasn't worked for me.</p>
<p>I don't know what the problem might be, coming from Tensorflow I would've used an Upscale layer but from what I've seen this isn't the way to do it in PyTorch.</p>
<p>Could anyone explain to me why my shapes are broken with my current model? Thanks</p>
","2024-05-09 16:08:36","1","Question"
"78455596","78455290","","<p>Numpy aligns on the trailing dimensions. The only &quot;mismatch&quot; in dimension that is allowed for broadcasting is <code>1</code> with something else.</p>
<p>Therefore, you could sum the two arrays if you reshape as <code>(0, 2, 3)</code> (or <code>(0, 1, 1)</code>):</p>
<pre><code>x = np.ones([2,3])
x + np.array([]).reshape(-1,2,3)

# or
x + np.array([]).reshape(-1,1,1)

# or
x + np.array([])[:, None, None]
</code></pre>
<p>But this would result in an empty array with shape <code>(0, 2, 3)</code>:</p>
<pre><code>array([], shape=(0, 2, 3), dtype=float64)
</code></pre>
<p>Likewise:</p>
<pre><code>x[..., None] + np.array([])
# array([], shape=(2, 3, 0), dtype=float64)
</code></pre>
","2024-05-09 15:49:48","1","Answer"
"78455290","","Summing numpy array with an empty array","<p>I need to sum a normal numpy array with an empty array</p>
<pre><code>x = np.ones([2,3])
x + np.array([]).reshape(2,-1)
</code></pre>
<p>Output:</p>
<pre><code>ValueError: operands could not be broadcast together with shapes (2,3) (2,0) 
</code></pre>
<p>Reshaping them to other dimensions does not work, e.g. <code>x.reshape(-1) + np.array([])</code>.
And making an if-statement to see if the right hand side term is empty or not seems not necessary.</p>
","2024-05-09 14:54:08","1","Question"
"78455014","","How to enable Pytorch Dropout on Optimized for Mobile models","<p>I have a model that uses dropout for MC prediction.  It works fine in Python on my desktop.  I'd like to execute the same algorithm on mobile platforms but the dropout does not seem to be applied.  I get the same result each MC iteration when evaluating the model on mobile.</p>
<p>First, I load the model in PyTorch and enable dropout as documented in other questions [1]</p>
<pre><code>model.eval()
for m in model.modules():
    if m.__class__.__name__.startswith('Dropout'):            
        m.train()

return model
</code></pre>
<p>Next I optimize for mobile.  I've added the parameter to disable removing the dropout during the optimize method:</p>
<pre><code>torchscript_model = torch.jit.script(model)
optimize_for_mobile(torchscript_model,
   optimization_blocklist={MobileOptimizerType.REMOVE_DROPOUT})
   ._save_for_lite_interpreter(ptlFile)
</code></pre>
<p>Lastly, in Android, I load the model then execute the MC iterations (simplified example)..</p>
<pre><code>module = LiteModuleLoader.loadModuleFromAsset( this.getAssets(), &quot;&lt;ptlFile name&gt;&quot; );
&lt;snip MC iterations&gt;
float[] score = module.forward(IValue.from(inputTensor)).toTensor().getDataAsFloatArray();
&lt;/snip&gt;
</code></pre>
<p>Each iteration I get the same score values.  Any suggestions how to achieve the same result on mobile as on the desktop?  If I remove the REMOVE_DROPOUT from the optimizer blocklist, the exported model is smaller in size.  So it seems as if the dropout layers are in the mobile model properly but they don't appear to be active during mobile evaluation.</p>
<p>Pytorch version 2.2.1 on Desktop.<br />
pytorch_android_lite 1.13.1 on Android.</p>
<p>[1] <a href=""https://stackoverflow.com/questions/63285197/measuring-uncertainty-using-mc-dropout-on-pytorch"">Measuring uncertainty using MC Dropout on pytorch</a></p>
","2024-05-09 14:09:59","0","Question"
"78454815","78448835","","<p>You could try to treat the <code>mxn</code> grid as a graph with <code>m*n</code> nodes, and making use of <code>torch-cluster</code>, use any neighbor search like <a href=""https://pytorch-geometric.readthedocs.io/en/1.4.1/_modules/torch_cluster/radius.html"" rel=""nofollow noreferrer"">torch_cluster.radius</a>, which can be pretty efficient when used with CUDA.</p>
","2024-05-09 13:34:23","0","Answer"
"78454137","78453194","","<p>Okay, I just add the new dictionary like this:</p>
<pre class=""lang-py prettyprint-override""><code>d = dict(enumerate(map(int, nonzero)))

filtered_probs['Gene1'] = filtered_probs.apply(lambda L: d[L.iloc[0]], axis=1)
filtered_probs['Gene2'] = filtered_probs.apply(lambda L: d[L.iloc[1]], axis=1)
</code></pre>
<p>Then it returns the correct result.<br />
If you have any better opinions, please feel free to comment!</p>
","2024-05-09 11:29:27","1","Answer"
"78453194","","How to match the index of tensors and values in the list using pytorch?","<p>I'd like to match the index of tensors from the list.
I'm trying to do link prediction using Pytorch.
In this process, I need to convert the index to the name by mapping it to the dictionary.
To do this, I set the dictionary and masking to the tensor, but it returned unexpected indices.</p>
<pre class=""lang-py prettyprint-override""><code>inv_entity_dict = {v: k for k, v in entity_dict.items()}
inv_entity_dict
#{0: 'TMEM35A',
# 1: 'FHL5',
# 2: 'Sirolimus',
# 3: 'TMCO2',
# 4: 'RNF123',
# 5: 'SMURF2',
# 6: 'SSH3',
# 7: 'PSMA4',
# 8: 'SOD3',
# 9: 'SCOC',
# 10: 'Cysteamine',
# 11: 'TOX',
#...}

nonzero[0:10]
#array([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 11])
</code></pre>
<p>After running the code, it returned unexpected results because Sirolimus(idx==2), which is not in the nonzero array, should not be matched the name.</p>
<pre class=""lang-py prettyprint-override""><code>for i in range(1):
    raw_probs = (z[i][nonzero[0:10]] @ z[i][nonzero[0:10]].t()).sigmoid()
    filtered_probs = pd.DataFrame((raw_probs&gt;0.9).nonzero(as_tuple=False).cpu().numpy(), columns=['Gene1', 'Gene2'])
    filtered_probs['prob'] = raw_probs[(raw_probs&gt;0.9)].cpu().detach().numpy()
    filtered_probs_name = map_id2gene(filtered_probs, inv_entity_dict) #converting func.

#Expected result
#   Gene1   Gene2   prob
#67 TOX TOX 1.0
#0  TMEM35A TMEM35A 1.0
#1  TMEM35A FHL5    1.0
#2  TMEM35A RNF123  1.0
#52 SCOC    TMEM35A 1.0

#Wrong
#   Gene1   Gene2   prob
#67 SCOC    SCOC    1.0
#0  TMEM35A TMEM35A 1.0
#1  TMEM35A FHL5    1.0
#2  TMEM35A Sirolimus   1.0
#52 SOD3    TMEM35A 1.0
</code></pre>
<p>I guess the initialized <code>raw_probs</code> indices went into the converting process directly.</p>
<pre class=""lang-py prettyprint-override""><code>raw_prob
#tensor([[1.0000e+00, ..., 1.0000e+00], #real index: 0
#        [1.0000e+00, ..., 1.0000e+00], #real index: 1
#        [1.0000e+00, ..., 1.0000e+00], #real index: 3, but considered to 2
#        [1.0000e+00, ..., 1.0000e+00], #real index: 4, but considered to 3, ...
#        [1.0000e+00, ..., 1.0000e+00], #real index: 5
#        [1.0000e+00, ..., 1.0000e+00], #real index: 6
#        [0.0000e+00, ..., 0.0000e+00], #real index: 7
#        [0.0000e+00, ..., 4.4097e-36], #real index: 8
#        [1.0000e+00, ..., 1.0000e+00], #real index: 9
#        [1.0000e+00, ..., 1.0000e+00] #real index: 11, but considered to 9], device='cuda:0')
</code></pre>
<p>In this case, how can I match the correct ids and names based on the <code>inv_entity_dict</code> and <code>nonzero</code> list?</p>
","2024-05-09 08:27:49","1","Question"
"78452655","","trying to perform knowledge distillation using KL divergence loss . the loss is too high","<p>KL divergence loss too high</p>
<p>I'm trying to perform knowledge distillation . For my student loss I have used cross entropy loss and for my knowledge distillation loss I am trying to use KL divergence loss.</p>
<p>Here is the code that I used for my KL divergence loss.</p>
<pre><code>class KLDivLoss(nn.Module):
    def __init__(self,ignore_index=-1, reduction=&quot;batchmean&quot;, log_target=False):
        super(KLDivLoss, self).__init__()
        self.reduction = reduction
        self.log_target = log_target
        self.ignore_index = ignore_index

    def forward(self, preds_S, preds_T, T =1.0, alpha = 1.0):
        preds_T[0] = preds_T[0].detach()  # Detach teacher predictions
        pred_1 = torch.sigmoid(preds_T[0]/T) # white
        pred_0 = 1 - pred_1
        preds_teacher = torch.cat((pred_0, pred_1), dim=1)
        assert preds_S[0].shape == preds_teacher.shape, &quot;Input and target shapes must match for KLDivLoss&quot;
        stu_prob = F.log_softmax(preds_S[0]/T, dim=1)
        kd_loss = F.kl_div(stu_prob, 
                           preds_teacher, 
                           reduction='batchmean',
                           ) * T * T
        return {'loss': kd_loss}
</code></pre>
<p>The values that I am getting from this are extremely huge. I am simply adding my knowledge distillation loss and cross entropy loss from student model. Since my CE loss is very small this is all from the KLdiv loss. Could you tell me how to reduce the loss? Or if I am doing something wrong.</p>
<p><a href=""https://i.sstatic.net/6tKJOOBM.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I tried using the KL div loss with temperature =1
my teacher model gave the output in the form of tensor [8,1,224,224] as it was used for binary prediction of pixel while my student model gave output in the form [8,2,224,224] where 0 belongs to class black and 1 to white.</p>
<p>so in order to match them up for KL div loss I used sigmoid function to get probabilities for class white and 1 - white probability for black. and then concatenated them to form a tensor of size [8,2,224,224] which would be similar to the student tensor.</p>
<p>and then I tried performing the KL divergence. the losses I got were extremely high</p>
","2024-05-09 06:27:53","1","Question"
"78452234","78448835","","<p>The most straight-forward answer is already pointed out by @Mercury, but I also want to point out another way to compute what you want, as @Mercury's answer will suffer from numerical error created by the sqrt from the <code>torch.cdist</code>, using native pytorch oprator:</p>
<pre><code>bmu_distance_squares = (best_loc.unsqueeze(-2) - locations.unsqueeze(-3)).pow(2).sum(-1)
</code></pre>
","2024-05-09 04:12:21","1","Answer"
"78451855","78448835","","<p>You can simply use <code>torch.cdist</code>.</p>
<pre><code>bmu_distance_squares = torch.cdist(best_loc, locations) ** 2
</code></pre>
","2024-05-09 01:50:55","3","Answer"
"78449954","","Script freezes when pytorch lightning's Trainer is instantiated","<p>I'm trying to train a model using pytorch lightning in a cluster with Ubuntu 20.04. However, the code freezes once when the <code>lightning.Trainer</code> is instantiated. There are no error messages, it just freezes, the program does not stop. It does not get to output the info regarding available GPU, TPU etc.</p>
<p>This happens to me with different codes, such as the &quot;lightning in 15 minutes&quot; example or even with the following minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>import lightning as L

print(&quot;Before instantiate Trainer&quot;)
trainer = L.Trainer()
print(&quot;After instantiate Trainer&quot;)
</code></pre>
<p>It simply does not get to the second print. I have tried with different versions of torch and lightning, and happens the same in all of them.</p>
<p>Does anybody know what is going on? Thanks in advance.</p>
<p>UPDATE: I put some prints inside the <code>lightning.Trainer</code> init and I found that it gets stuck just in the line <code>self._accelerator_connector = _AcceleratorConnector</code>.</p>
","2024-05-08 16:26:20","1","Question"
"78448879","78448650","","<p>It reshapes the <code>output</code> tensor such that it has the same batch size, but each entry is a flattened vector.</p>
<p><code>output.size()</code> returns the tensor shape as a tuple. <code>output.size()[0]</code> selects the first entry of that tuple which conventionally is the batch size. <code>output.view()</code> returns a tensor that has the same contents but arranged differently, without creating a copy. <code>output.view(output.size()[0], -1') means that the shape of that tensor(view) matches the batch size in the first dimension, and in the second dimension </code>-1` indicates that the dimension is chosen automatically to match the vector size.</p>
<p>For example, suppose <code>output</code> is has 8 elements, each a 50x40x7 tensor. The shape of that tensor would be 8x50x40x7. The result of <code>output.view(output.size()[0], -1)</code> would have the shape 8x14000.</p>
<p>Generally, this is done ahead of a fully connected layer, as a fully connected layer expects a flat vector as an input, one for each batch element. So, you will have to do this for any network that does not output flat vectors. Resnet and VGG are classification networks so their outputs are flat vectors so this operation would not be needed.</p>
","2024-05-08 13:23:54","3","Answer"
"78448835","","Squared distance computation in PyTorch - Avoid for loop","<p>I have a code where a 2D grid of size (20, 20) when flattened (400) needs to have distance computed from all other indices on the 2D grid. Currently, I am using a for loop to store it.</p>
<pre><code># best locations of indices existing on 2D grid-
best_loc.shape
# torch.Size([1024, 2])

# Specify 2D grid size-
m = 20
n = 20

locs = [np.array([i, j]) for i in range(m) for j in range(n)]
locations = torch.LongTensor(np.array(locs))

locations.shape
# torch.Size([400, 2])

def get_distance_squares(best_loc):
    '''
    Compute squared distances between 'best_loc' and 'locations'
    '''
    best_loc = best_loc.unsqueeze(0).expand_as(locations).float()
    best_distance_squares = torch.sum(torch.pow(locations.float() - best_loc, 2), 1)
    return best_distance_squares
     
bmu_distance_squares = list()

for loc in bmu_loc:
    bmu_distance_squares.append(get_distance_squares(loc))
best_distance_squares = torch.stack(best_distance_squares)

best_distance_squares.shape
# torch.Size([1024, 400])
</code></pre>
<p>How can I avoid the for loop for getting &quot;bmu_distance_squares&quot; which is a pairwise squared distance matrix?</p>
","2024-05-08 13:14:45","-2","Question"
"78448650","","Pytorch Siamese Network implementation?","<p>I tried to implement the siamese network for image classification task according to the code below:</p>
<pre><code>class SiameseNetwork(nn.Module):

    def __init__(self):

        super(SiameseNetwork, self).__init__()
        # Setting up the Sequential of CNN Layers

        self.cnn = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=1),
            nn.ReLU(inplace=True),
            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),
            nn.MaxPool2d(3, stride=2),

            nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2),
            nn.ReLU(inplace=True),
            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),
            nn.MaxPool2d(3, stride=2),
            nn.Dropout2d(p=0.3),

            nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            nn.Dropout2d(p=0.3),
        )
        # Defining the fully connected layers
        self.fc = nn.Sequential(
            nn.Linear(30976, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout2d(p=0.5),
            
            nn.Linear(1024, 128),
            nn.ReLU(inplace=True),
            
            nn.Linear(128,2))
        
    def forward_once(self, x):
        # Forward pass 
        output = self.cnn(x)
        output = output.view(output.size()[0], -1)
        output = self.fc(output)
        return output

    def forward(self, input1, input2):
        # forward pass of input 1
        output1 = self.forward_once(input1)
        # forward pass of input 2
        output2 = self.forward_once(input2)
        return output1, output2
</code></pre>
<p>I understand most of its, but what does the
<code>output = output.view(output.size()[0], -1)</code> do?.</p>
<p>Do I really need it when I change the <code>self.cnn</code> with different networks like resnet or vgg?</p>
","2024-05-08 12:49:22","2","Question"
"78448580","78446556","","<p>When tasks depend on each other sequentially (where the output of one task serves as the input for the next), parallel computing becomes less effective. This is because the dependent task can't start until its prerequisite task finishes. However, you can read about pipeline parallelism:</p>
<p>This involves splitting a series of tasks into stages and executing each stage in parallel across different data.</p>
<p><strong>Example Scenario</strong>
Assume you're processing a list of images sequentially, and each image goes through a pipeline like this:</p>
<p><strong>Task 1:</strong> Read and preprocess an image.</p>
<p><strong>Task 2:</strong> Apply a filter to the image.</p>
<p><strong>Task 3:</strong> Analyze the filtered image for features.
Each task depends on the output of the previous one, making them inherently sequential for each image. However, if you have multiple images, you could apply pipeline parallelism like this:</p>
<p><strong>Worker 1:</strong> Reads and preprocesses the first image.</p>
<p><strong>Worker 2:</strong> Filters the preprocessed image while Worker 1 preprocesses the next image.</p>
<p><strong>Worker 3:</strong> Analyzes the filtered image while Workers 1 and 2 continue working on their tasks.</p>
<p><strong>Therefore</strong>, pipeline parallelism ensures that the overall throughput of processing multiple images increases although each task remains sequential.</p>
","2024-05-08 12:37:22","1","Answer"
"78447718","78430524","","<p>I think whats happening is that T5 returns the hidden state per step of decoding. Therefore, the number of tuples should correspond to the longest generated sequence. You are most likely interested in the last decoding step and could take the last tuple.</p>
<p>In that tuple you have a tuple of size num_layers + 1 (+1 for the final LayerNorm). The output of the last layer should be the last tuple entry.</p>
","2024-05-08 10:14:16","3","Answer"
"78446608","78444014","","<p>the problem with you is the <code>μr = torch.tensor([params[0], params[4]], requires_grad=True)</code> and <code>θ = torch.tensor([params[3], params[7]], requires_grad=True)</code>. You create new tensors, and their reference is difference than the params, so the params is not updated in the traning loops. You can change them to</p>
<pre><code>    μr = torch.stack([params[0], params[4]])
    θ = torch.stack([params[3], params[7]])
</code></pre>
<p>then you can train as normal</p>
","2024-05-08 07:02:10","1","Answer"
"78446556","","Parallel computing for sequentially dependent tasks","<p>I understand that Python <a href=""https://youtu.be/YOhrIov7PZA?si=3mvIr-ETh4yAjXxJ"" rel=""nofollow noreferrer"">multi-processing</a> and <a href=""https://youtu.be/3dEPY3HiPtI?si=FtSzYULS4xb-s2QJ"" rel=""nofollow noreferrer"">multi-threading</a> can accelerate running independent tasks. However, If I have 10 tasks sequentially and each one's input rely on the previous one's output, is it still possible to accelerate using parallel computing or parallel processing? Any perspective from software or hardware will be appreciated!</p>
","2024-05-08 06:49:38","-2","Question"
"78446523","78446007","","<p><code>.train()</code> or <code>.eval()</code> only modify some behaviour of some layers of the network (like BatchNorm, Dropout, etc.), they do not explicitly allow or block the gradient of the model, so for inference, you should call <code>torch.no_grad()</code> to stop the intermediate values being stored</p>
","2024-05-08 06:43:36","1","Answer"
"78446308","78436927","","<p>The <code>w*x+b</code> can't be done with <code>einsum</code>.</p>
<pre><code>c (n,m)
w (mxn)
X (n,)
b (m,)
X_tilde (n,m,n)
</code></pre>
<p>The best I can do is with two temporary expressions.</p>
<p>The <code>l</code> sum is a bit confusing.  I assume <code>l</code> corresponds to one of the <code>n</code>, so I'm guessing <code>sum(l) w, X_tilde</code> is:</p>
<pre><code>temp1 = np.einsum('ij,kij-&gt;ki', w,X_tilde) # (n,m)
</code></pre>
<p>The expression is <code>b</code> looks more straightforward:
<code>w*x+b</code></p>
<pre><code>temp2 = np.einsum('mn,n-&gt;m', w, x) + b   # (m,)
</code></pre>
<p>Combining all 3, I assume <code>k</code> sums over a <code>m</code> dimension.
<code>sum(k) c * sigma * temp1 * temp2</code></p>
<pre><code>res = np.einsum('im,m,jm-&gt;ji', c*sigma, temp2, temp1)
</code></pre>
<p>I don't think there's a performance benefit to using just one <code>einsum</code>.  Originally <code>einsum</code> constructed an iteration space consisting of all variables.  That could be quite large if there are many variables, and more than two indices.  More recent versions divide the calculation into pieces (<code>matmul</code> if possible), which you can explore with <code>einsum_path</code>.</p>
","2024-05-08 05:46:14","1","Answer"
"78446080","","Implementation of Pass2Edit to model string edit behaviour","<p>I am trying to implement Pass2Edit (<a href=""https://www.usenix.org/system/files/usenixsecurity23-wang-ding-pass2edit.pdf"" rel=""nofollow noreferrer"">this paper</a>: read 3.1, 3.2). It takes in original password and current password strings, and tries to model the edit behaviour. The following is what it looks like:</p>
<p><a href=""https://i.sstatic.net/oTwfo5XAm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oTwfo5XAm.png"" alt=""enter image description here"" /></a> <a href=""https://i.sstatic.net/ZPWraGmSm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZPWraGmSm.png"" alt=""enter image description here"" /></a></p>
<p>The input of the neural network is the password pair, and the output is the probability of each transformation state. From the paper I understand that the model works by:</p>
<ol>
<li>Firstly, the input passes through the embedding layer, and each one-hot encoded password character is converted into a 256-dimensional vector (i.e., v_origi and v_curi )</li>
<li>Next, concatenate v_origi and v_curi into vi and then input it to a 3-layer GRU (the hidden layer dimension is 256)</li>
<li>Finally, take the output of the GRU for the last character through a 2-layer FC (i.e., fully connected layer, where the hidden layer dimension is 512), and finally obtain the probability of each transformation ti through the softmax layer.</li>
</ol>
<blockquote>
<p>Specifically, after each password is transformed into a key sequence,
the character set Σ includes 48 types of characters that can be
entered through the EN-US standard keyboard, as well as &lt;shift&gt;,
&lt;caps&gt; and &lt;placeholder&gt; (48+3=51). If we limit the length of the password to no more
than 30 (i.e., 0≤p&lt;30), then the total number of atomic operations is
|t|=30∗51+30+1=1, 561, where 30∗51 is the category # of insertions, 30
is the category # of deletions, and 1 represents the EOS operation. In
this light, our one-step prediction process can essentially be seen as
a 1,561-class multi-classification problem.</p>
</blockquote>
<p>I am very new to writing a RNN model, and am not able to translate this to the pytorch GRU implementation.</p>
<p>Specifically:</p>
<ol>
<li>Since the dataset contains variable length password pairs and they have limited the password length to 30, does that mean when l&lt;30, the rest of the GRU units just do not engage?</li>
<li>Same goes for the final number of classes I have for prediction. Since the model assumes it as a 1561 class prediction problem, there are classes that are just irrelevant for l&lt;30. For example the class INS(14, &quot;a&quot;) when password length is 8.</li>
<li>How do I incorporate the caps key, shift key and &quot;placeholders&quot; they mention in the paper?</li>
</ol>
<p>An outline of the model, some clarity on how l&lt;30 passwords will work and a way to put in caps key, shift key and &quot;placeholders&quot; would be really helpful. Thanks!</p>
","2024-05-08 04:23:05","0","Question"
"78446043","78445789","","<p>I think the most concise way to express this would be something like this:</p>
<pre class=""lang-py prettyprint-override""><code>functools.reduce(np.matmul, np.rollaxis(x, 1))
</code></pre>
<p>I suspect that's also about as efficient as you can hope for using operations available in numpy.</p>
<p>If it existed, the vectorized API to compute this might look something like this:</p>
<pre class=""lang-py prettyprint-override""><code>np.matmul.reduce(x, axis=1)
</code></pre>
<p>but unfortunately <code>reduce</code> is not yet implemented for generalized ufuncs like <code>matmul</code>, and trying this results in</p>
<pre><code>RuntimeError: Reduction not defined on ufunc with signature
</code></pre>
","2024-05-08 04:05:53","1","Answer"
"78446007","","Is it necessary to call torch.no_grad() even in evaluation mode?","<p>I am learning <code>pytorch</code>. In the code examples, the model is switched between training and testing by using the <code>model.train()</code> and <code>model.eval()</code> modes. I understand that this has to be done to deactivate training specific behaviour such as dropouts and normalisation and gradient computation while testing.</p>
<p>In many such examples, they also use <code>torch.no_grad()</code>, which I understand is a way of explicitly asking to stop the calculations of the gradients.</p>
<p>My question is, if gradient calculation is stopped in <code>model.eval()</code> mode then why do we also have to set <code>torch.no_grad()</code>?</p>
<p>Below is an example of a code where both <code>model.eval()</code> and <code>torch.no_grad()</code> is used</p>
<pre><code>class QuertyModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.input = nn.Linear(2, 8)
        
        self.hl1 = nn.Linear(8, 16)
        self.hl2 = nn.Linear(16, 32)
        self.hl3 = nn.Linear(32, 16)
        self.hl4 = nn.Linear(16, 8)
        
        self.output = nn.Linear(8, 3)
        
    def forward(self, x):
        x = F.relu(self.input(x))
        x = F.relu(self.hl1(x))
        x = F.relu(self.hl2(x))
        x = F.relu(self.hl3(x))
        x = F.relu(self.hl4(x))
        
        return self.output(x)
</code></pre>
<p>Function to train the model</p>
<pre><code>def train_model(model, lr, num_epochs):
    
    loss_function = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    
    train_acc, train_loss, test_acc, test_loss = [], [], [], []
    for epoch in range(num_epochs):
        print(f&quot;{epoch+1}/{num_epochs}&quot;)
        
        model.train() # switch to training mode
        
        batch_acc, batch_loss = [], []
        for X, y in train_loader:
            # forward pass
            y_hat = model(X)
            loss = loss_function(y_hat, y)
            
            # backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            matches = torch.argmax(y_hat, axis=1) == y # true/false
            matches = matches.float() # 0/1
            
            batch_acc.append(100*torch.mean(matches))
            batch_loss.append(loss.item())
        
        train_acc.append(np.mean(batch_acc))
        train_loss.append(np.mean(batch_loss))
        
        model.eval() # switch to evaluation mode
        
        X, y = next(iter(test_loader))
        with torch.no_grad():
            y_hat = model(X)
        
        test_acc.append(100*(torch.mean(y_hat) == y).float())    
        test_loss.append(loss_function(y_hat, y).item())
        
    return train_acc, train_loss, test_acc, test_loss
</code></pre>
","2024-05-08 03:45:46","1","Question"
"78445789","","Matrix multiplixation over axis in numpy","<p>Suppose I have an array <code>X</code> of shape (B, N, N, 3, 3). I want to vectorise the operation</p>
<p><code>X[:,0,...] @ X[:,1,...] ... @ X[:,N-1,...]</code></p>
<p>How can I vectorise this in numpy? I don't want to use for loops.</p>
<p>I tried using einsum, but it didn't work for me, it ended up summing the matrices.</p>
<p>EDIT: I'm using Jax, if it makes any difference here</p>
","2024-05-08 02:08:31","2","Question"
"78444014","","Minimize function in PyTorch using dummy variables","<p>I am new to PyTorch so my question could be trivial.</p>
<p>I am trying to minimize a function which I could resume in a snippet as</p>
<pre><code>
def target_function(params):
    vector = torch.zeros(10)
    μr = torch.zeros(2, requires_grad=True)
   
    θ = torch.zeros(2, requires_grad=True)

    μr = torch.tensor([params[0], params[4]], requires_grad=True)

    θ = torch.tensor([params[3], params[7]], requires_grad=True)
 
    for i in range(2):
        vector[i] += (μr[i]**2 - θ[i] ).sum()

    return torch.norm(vector)
</code></pre>
<p>I tried performing the optimization via</p>
<pre><code>import matplotlib.pyplot as plt


# Initialize parameters with random values between 0 and 1
params = torch.rand(10, requires_grad=True)


# Choose an optimizer (e.g., SGD) and specify the learning rate
optimizer = SGD([params], lr=0.01)

num_steps = 100

# Lists to store the values of the target function during optimization
target_values = []

# Optimization loop
for i in range(num_steps):
    # Zero gradients
    optimizer.zero_grad()
    
    # Compute the function value
    output = target_function(params)
    
    # Store the function value
    target_values.append(output.item())
    
    # Compute gradients
    output.backward()
    
    # Update parameters
    optimizer.step()

# Plot the values of the target function during optimization
plt.plot(target_values)
plt.xlabel('Iteration')
plt.ylabel('Target Function Value')
plt.title('Optimization Progress')
plt.show()

</code></pre>
<p>but it seems that the values of the function do not evolve during the loop.
Debugging I found out that it may depend on using the variables <code>μr </code>and <code>θ</code> in the code. Probably I am doing something which should not be done but I do not understand how to fix this keeping the variables <code>μr </code>and <code>θ</code>.</p>
<p>Thanks for any help or explanation</p>
","2024-05-07 16:52:29","0","Question"
"78442382","78442079","","<p>Try penalizing PD and T1 separately as well. Account for their relative scales to each other and the combined metric. Or only use their loss, when the difference is over a threshold or the ratio between them is wrong.</p>
","2024-05-07 12:10:38","0","Answer"
"78442079","","Various combination of model predictions yields to similar ground truth","<p>I have a model(3DUnet, Regression problem) that predicts values PD and T1, where PD and T1 are the qMRI outputs based on the input. From these predictions, I calculate T1_Weighted_image using the formula: <strong><em>Weighted_images  = PD</em> (1 - exp(-1 / (T1 + epsilon)))</strong>*, where epsilon is a small value to prevent division by zero and T1=&gt;0 . During training, my ground truth for loss computation is T1_Weighted_groundtruth, but I also have ground truth values for PD and T1, although they are not directly used for loss computation. They serve to ensure the correctness of predicted values for PD and T1. The loss is computed using a loss function between T1_Weighted_predict and T1_Weighted_groundtruth.</p>
<p>However, there exist various combinations of PD or T1 that can yield similar results for T1_Weighted. For instance, instead of predicting high values for T1 (which is the correct answer), my model might predict very low values for PD (for example in CSF as an obvious example). Is there a method to compel my model to predict the correct values, or at least to predict (any) possible combinations?</p>
","2024-05-07 11:19:19","0","Question"
"78441397","78435504","","<p>Your <code>fc_input_size</code> is wrong. First, you're putting in <code>data.shape[0]</code>, which is the number of data, and not the size of each data item. Second, you are forgetting about the 2 MaxPool operations you are doing, each of which reduce the spatial dimensions by 2x.</p>
<p>Calculate it like following:</p>
<pre><code>fc_input_size = (data.shape[1] // 4) * (data.shape[2] // 4)
</code></pre>
","2024-05-07 09:16:49","0","Answer"
"78440912","78440108","","<p>There is a metric called Mean Average Percentage Error (MAPE) which, calculate the error percentage of the model wrt the truth value:</p>
<pre><code>MAPE = avg(abs((y-y_pred)/y))
</code></pre>
<p>You can refer to <a href=""https://en.wikipedia.org/wiki/Mean_absolute_percentage_error"" rel=""nofollow noreferrer"">here</a> to learn more about MAPE</p>
","2024-05-07 07:47:02","1","Answer"
"78440108","","PyTorch: calculating model accuracy for approximation problems","<p>There are some SO posts on calculating accuracy of a classification model in PyTorch, but I how do I calculate accuracy of an approximation model?
For example, for classifications, I can usually count, per-class, the amount of hits, divided by the total validation set size: <code>correct_count / len(validation_set)</code></p>
<p>However, for an approximation problem, I have a model that predicts <code>y</code> for certain <code>x</code> values. When I run validation, I get <code>y_pred</code> values that are not identical to the original <code>y</code> values of the function. I still want to be able to calculate an &quot;accuracy&quot; for this model. What is the best way to do it?</p>
<p>I was thinking of, for example:
<code>avg(abs(y - y_pred)) / (max(y)-min(y))</code></p>
<p>But maybe there's some best practice?</p>
","2024-05-07 04:24:04","0","Question"
"78439828","78439560","","<p>The biggest reason should be they would be registered to the Model (the model can have reference to them), plus pytorch user (like me :)) heavily use pytorch hooks to interfere with the model, thus it would be better to be able to attach some hooks if need (for debug, modify model behaviour with changing source code, etc.)</p>
","2024-05-07 02:23:18","0","Answer"
"78439803","78404705","","<p>Would this work for you:</p>
<pre class=""lang-py prettyprint-override""><code>mask_row_ids = torch.arange(mask_indices.shape[0]).unsqueeze(1).repeat(1, mask_indices.shape[1])

mask_tensor = torch.ones(inp_tensor.shape)
mask_tensor[mask_row_ids, mask_indices] = 0

output = torch.mm(inp_tensor * mask_tensor, my_tensor)
print(output)
</code></pre>
","2024-05-07 02:13:34","0","Answer"
"78439767","","Converting Depth-Anything To CoreML","<p>I'm trying to convert existing <a href=""https://github.com/LiheYoung/Depth-Anything"" rel=""nofollow noreferrer"">depth-anything</a> PyTorch model to CoreML format. I decided to use Google Colab and took the <a href=""https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/depth-anything/depth-anything.ipynb"" rel=""nofollow noreferrer"">following </a>note for inferencing depth-anything model. However, I meet some exception while trying to import it on iOS side. Here is my code snippet for converting:</p>
<pre class=""lang-py prettyprint-override""><code># Installing all needed extensions
!pip install coremltools
# ...

import coremltools as ct
import torch

# Convert the PyTorch model to TorchScript
traced_model = torch.jit.trace(depth_anything, torch.rand(1, 3, 518, 518))

# Convert the TorchScript model to CoreML
model_coreml = ct.convert(
    traced_model,
    inputs=[ct.ImageType(name=&quot;input_1&quot;, shape=(1, 3, 518, 518), scale=1/255.0)]
)

output = model_coreml._spec.description.output[0]
output.type.imageType.colorSpace = ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value('RGB')
output.type.imageType.width = 518
output.type.imageType.height = 518

# Save the modified CoreML model
print(model_coreml)
model_coreml.save('/content/drive/MyDrive/trained_models/depth9.mlpackage')
</code></pre>
<p>I've tried to specify input parameters straight as I do for the output one like this:</p>
<pre class=""lang-py prettyprint-override""><code># Create a dictionary for the input schema
input_schema = {'input_name': 'input', 'input_type': ct.TensorType(shape=(1, 3, 518, 518))}

# Add the input schema to the model's metadata
model_coreml.user_defined_metadata['inputSchema'] = str(input_schema)
</code></pre>
<p>Or to use <code>convert_to</code> option with setting up <code>neuralnetwork</code> like this:</p>
<pre class=""lang-py prettyprint-override""><code>model_coreml = ct.convert(
    traced_model,
    inputs=[ct.ImageType(name=&quot;input_1&quot;, shape=(1, 3, 518, 518), scale=1/255.0)],
    convert_to='neuralnetwork'
)
</code></pre>
<p>Or to set <code>ct.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value('RGB')</code> with <code>BGR</code>/<code>GRAYSCALE</code><br />
Nothing helps.</p>
<p>If I try to import the model with <code>neuralnetwork</code> backend I just receive an infinite loading. If I try to import the model with <code>mlprogram</code> backend (default, if not specified) I receive the following:<br />
<a href=""https://i.sstatic.net/zOriEIc5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zOriEIc5.png"" alt=""Exception while importing mlprogram backend"" /></a></p>
<p>I look forward for any advices and help since all I need is just to convert existing <code>depth-anything</code> model with no adjustments or changes to CoreML format. Thanks!</p>
","2024-05-07 01:53:36","0","Question"
"78439640","","Installing Pytorch for Windows 11 and AMD GPU","<p>could someone help me out with my Pytorch installation? My device currently uses Windows OS and an AMD GPU. However, the Pytorch installation does not support Windows OS with ROCm combination. Only when Linux OS is chosen will the ROCm option be available.</p>
<p>Can I use CUDA toolkit in replacement of ROCm? Or do I somehow change my OS to Linux? Is there some way to by pass all of these and still be able to use Pytorch?</p>
<p>Any advice will be greatly appreciated!</p>
<p>I have tried looking for installation tutorials on youtube but they do not have the same OS and GPU combination as I do. (That is Windows OS and AMD GPU)</p>
","2024-05-07 00:46:02","1","Question"
"78439560","","Reason for wrapping simple functions inside of classes (PyTorch)","<p>What is the reason for wrapping simple functions such as torch.cat() (or layers such as MaxPool2d) inside of a class like this:</p>
<pre><code>class Concat(nn.Module):
    def __init__(self, dimension=1):
        super(Concat, self).__init__()
        self.d = dimension

    def forward(self, x):
        return torch.cat(x, self.d)

class MP(nn.Module):
    def __init__(self, k=2):
        super(MP, self).__init__()
        self.m = nn.MaxPool2d(kernel_size=k, stride=k)

    def forward(self, x):
        return self.m(x)
</code></pre>
","2024-05-07 00:02:32","0","Question"
"78438315","","No module named ‘torch._custom_ops’ in Jupyter Notebooks","<p>I'm not sure if this is the right place to ask, but I just installed cuda tools to run some GPU-based machine learning stuff on my computer, and I'm running into an issue importing torch.</p>
<p>I'm on Ubuntu 22.04
I've tried installing torch within a conda environment and locally. Unfortunately, when I try to import torch into a jupyter notebook, I get the error (Doing literally nothing else in the notebook but importing torch):</p>
<pre><code>ModuleNotFoundError: No module named 'torch._custom_ops'; 'torch' is not a package
</code></pre>
<p>When I run <code>nvcc -V</code> my output is:</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0

</code></pre>
<p>When I run <code>nvidia-smi</code> my output is:</p>
<pre><code>NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4 
</code></pre>
<p>I have an NVIDIA GeForce RTX 3050 Ti.
Based on Table 3 of <a href=""https://docs.nvidia.com/deploy/cuda-compatibility/index.html"" rel=""nofollow noreferrer"">https://docs.nvidia.com/deploy/cuda-compatibility/index.html</a>, CUDA 12.4 seems like the right version for my NVIDIA driver.</p>
<p>I'm able to run <code>python3 -c 'import torch'</code> with no output, which I assume is good news.
That being said, when I try to import torch into a jupyter notebook, I get the error:</p>
<pre><code>ModuleNotFoundError: No module named 'torch._custom_ops'; 'torch' is not a package
</code></pre>
<p>I was able to find torch._custom_ops myself, so I know it exists, but I'm not sure why it isn't working in Jupyter Notebook?</p>
<p>I found this: <a href=""https://stackoverflow.com/questions/77498488/loading-a-pretrained-model-from-torch-hub-in-sagemaker"">Loading a pretrained model from torch.hub in Sagemaker</a> but that didn't seem relevant given I'm not using Sagemaker and simply trying to get my local machine ready to tackle GPU training tasks.</p>
<p>I see other posts such as <a href=""https://stackoverflow.com/questions/54843067/no-module-named-torch"">No module named &quot;Torch&quot;</a>
<code>pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-win_amd64.whl</code>, which I haven't done, but looks like it's for windows. Is that something I need?</p>
<p>I would appreciate any help, insight, or simply comments telling me a better place to be asking this question.
Thank you</p>
","2024-05-06 17:48:37","0","Question"
"78436927","","Converting an expression into an einsum","<p>I have the following expression that I need to calculate for some matrices:
<a href=""https://i.sstatic.net/A2YTfNm8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A2YTfNm8.png"" alt=""enter image description here"" /></a></p>
<p>I could of course do this using a for loop, but I'm attempting to use the torch.einsum function to calculate this in a vectorized manner. I'm having trouble understanding how exactly I can combine two summations into a single einsum expression.</p>
<p>For reference:</p>
<ul>
<li>c is a nxm matrix</li>
<li>w is a mxn matrix</li>
<li>X is a n dimensional vector</li>
<li>b is a m dimensional vector</li>
<li>X_tilde is a nxmxn matrix</li>
</ul>
<p>The overall output of this should be a mxn matrix.</p>
<p>Doing each of the sums individually is simple enough. For example, the inner sum (over l) can be easily written as:
<code>torch.einsum(&quot;l..., blmn -&gt; mn&quot;, w[k], X_tilde)</code></p>
<p>However, I can't see a way to combine the double summation into a single einsum expression.</p>
","2024-05-06 13:25:24","1","Question"
"78436177","78404705","","<p>I think, want you need is masked tensor. Please have a look to <a href=""https://pytorch.org/docs/stable/masked.html"" rel=""nofollow noreferrer"">this documentation</a>. Unfortunately, it does not support <code>matmul</code> for now, but I believe it will be added in future, you may try to open GitHub issue to speed up the process. However, it supports <code>mul</code> and <code>sum</code> operations, you may try to use those to solve your problem.</p>
<p>To create masked tensors you can do following (adapting the code from Rehan Ahmed):</p>
<pre class=""lang-py prettyprint-override""><code>mask_row_ids = torch.arange(mask_indices.shape[0]).unsqueeze(1).repeat(1, mask_indices.shape[1])

mask_tensor = torch.ones(inp_tensor.shape).bool()
mask_tensor[mask_row_ids, mask_indices] = False

inp_tensor_masked = torch.masked.masked_tensor(inp_tensor, mask_tensor, requires_grad=True)

mask_tensor = torch.ones(my_tensor.shape).bool()
my_tensor_masked = torch.masked.masked_tensor(my_tensor, mask_tensor, requires_grad=True)
</code></pre>
<p>This will give you following tensors:</p>
<pre class=""lang-py prettyprint-override""><code>MaskedTensor(
  [
    [  0.7860,   0.1115,       --,   0.6524,   0.6057,   0.3725,   0.7980,       --],
    [  1.0000,   0.1115,       --,   0.6524,   0.6057,   0.3725,       --,   1.0000]
  ]
)

MaskedTensor(
  [
    [  0.8823,   0.9150,   0.3829],
    [  0.9593,   0.3904,   0.6009],
    [  0.2566,   0.7936,   0.9408],
    [  0.1332,   0.9346,   0.5936],
    [  0.8694,   0.5677,   0.7411],
    [  0.4294,   0.8854,   0.5739],
    [  0.2666,   0.6274,   0.2696],
    [  0.4414,   0.2969,   0.8317]
  ]
)
</code></pre>
<p>Then you may try to make them to be the same size, by transposing and repeating, and use <code>mul</code> and <code>sum</code> functions.</p>
<p><em>Note:</em> <code>numpy</code> also has masked array idea, please have a look <a href=""https://numpy.org/doc/stable/reference/generated/numpy.ma.dot.html"" rel=""nofollow noreferrer"">here</a>. But the way it performs dot product, is again by replacing with 0s, please see the implementation <a href=""https://github.com/numpy/numpy/blob/ca58cde7de282a485ca12e5f2582a64fb2e52113/numpy/ma/core.py#L8047"" rel=""nofollow noreferrer"">here</a>. So, you need to make sure whether in future, torch implements it the way you want.
I guess switching back and forth from torch tensor to numpy is not applicable solution for you either.</p>
","2024-05-06 11:02:54","0","Answer"
"78435504","","Problem in defining a ML model for my Npy dataset","<p>I need help in defining a torch model for my data. I have tried various methods but nothing seems to be working out. Error after error related to input size and shaping. How can I resolve these issues?</p>
<pre><code>import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as f

# Load data from .npy file
data = np.load(&quot;other py files/project_files/data/train/data.npy&quot;)
print(&quot;Data Shape: &quot;, data.shape)  # (401, 701, 255)

data_size = data.shape[0] * data.shape[1] * data.shape[2]
print(&quot;Data Size:&quot;, data_size)  # 71680755

# Load labeling data from .npy file
labels = np.load(&quot;other py files/project_files/data/train/label.npy&quot;)
print(&quot;Label Data Shape: &quot;, labels.shape)  # (401, 701, 255)

# Convert numpy arrays to PyTorch tensors
data_tensor = torch.Tensor(data)
labels_tensor = torch.Tensor(labels)


class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc_input_size = data_size
        self.fc = nn.Linear(self.fc_input_size, 2)

    def forward(self, x):
        x = self.pool(f.relu(self.conv1(x)))
        x = self.pool(f.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_size)
        x = self.fc(x)
        return x

model = MyModel()
print(model)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

dataset = TensorDataset(data_tensor, labels_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

num_epochs = 10
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in enumerate(dataloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs.unsqueeze(1))  # channel dimension
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:
            print(f&quot;[{epoch + 1}, {i + 1}] loss: {running_loss / 100}&quot;)
            running_loss = 0.0


with torch.no_grad():
    predictions = model(data_tensor.unsqueeze(1))  # channel dimension
</code></pre>
<p>Console Output:</p>
<pre><code>Connected to pydev debugger (build 223.8836.43)
Data Shape:  (401, 701, 255)
Data Size: 71680755
Label Data Shape:  (401, 701, 255)
MyModel(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (fc): Linear(in_features=71680755, out_features=2, bias=True)
)

File &quot;C:\Users\PC1\PycharmProjects\Project1\newmodel2.py&quot;, line 36, in forward
    x = x.view(-1, self.fc_input_size)
RuntimeError: shape '[-1, 71680755]' is invalid for input of size 22579200
python-BaseException
</code></pre>
","2024-05-06 08:45:53","0","Question"
"78435279","78429005","","<p>Maybe <code>torch.autograd.functional.jacobian</code> is what you're looking for. Or if you want to use even &quot;lower-level&quot; API, <code>functorch</code> (which is included in Pytorch since 2023) provides you with <code>vmap</code> and <code>vjp</code> to calculate the Jacobian of the input wrt the output.
Here is some document that might be helpful:</p>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html</a>
<a href=""https://pytorch.org/functorch/stable/notebooks/jacobians_hessians.html"" rel=""nofollow noreferrer"">https://pytorch.org/functorch/stable/notebooks/jacobians_hessians.html</a></p>
","2024-05-06 08:00:14","1","Answer"
"78435006","78429659","","<p><strong>Native Pytorch:</strong>
Use <code>torch.nn.functional.unfold</code>. It is way faster, can be differentiable if needed and you can even take the patches that overlap. Here is an example:</p>
<pre><code>x = torch.arange(1,28*28+1).view(28,28).float() # unfold only works with float tensor
x = x.unsqueeze(0).unsqueeze(0) # 2 unsqueeze to make `x` have dim 4 (BxCxHxW) 
out = torch.nn.functional.unfold(x, kernel_size= 4, dilation= 1, padding= 0, stride= 4) 
out.permute(0,2,1).shape  # torch.Size([1, 49, 16])
</code></pre>
<p><strong>Alternative:</strong>
Another way is using <code>einops</code> which is way more flexible, but it would require you to install an extra package. And yeah, use <code>einops</code> if you want to work with multiple framework between pytorch, numpy, tensorflow, ...</p>
<pre><code>from einops import rearrange
x = torch.arange(1,28*28+1).view(28,28)
out = rearrange(x, &quot;(h t1) (w t2) -&gt; h w (t1 t2)&quot;, t1 = 4, t2 = 4)
out.shape # torch.Size([7, 7, 16])
</code></pre>
<p><strong>Alternative 2:</strong>
Just to be clear you can do it with pure tensor operator (no need for functional operator), for any reason:</p>
<pre><code>x = torch.arange(1,28*28+1).view(28,28)

out = torch.stack(torch.split(torch.stack(torch.split(x,4, 0), -1), 4, 1), -2).view(16,49).permute(1,0)
# or
out = torch.stack(torch.split(torch.stack(torch.split(x,4, 1), -1), 4, 0), -1).view(16,49).permute(1,0)
out.shape # torch.Size([49, 16])
</code></pre>
","2024-05-06 07:02:23","1","Answer"
"78433332","","Turning a list of PyG Data objects into a PyG Dataset?","<p>I have a python list of torch_geometric.data.Data objects (each one representing a graph). There is no easy way for me to access original raw files for this data: I just have the list. I need to turn this list of Data objects into a torch_geometric.data.InMemoryDataset or torch_geometric.data.Dataset object in order to integrate it with a larger code base which I did not write. How do I do this?</p>
<p>To be clear, I know that one can use a list of Data objects to make a torch_geometric.data.DataLoader object. But, I specifically need a Dataset object, <em>not a DataLoader object</em>, as the larger code base does some additional processing steps on Dataset objects before turning them into loaders.</p>
<p>I don't understand why PyG makes this so difficult. Should there not be a very easy way to do this?</p>
<p>I tried using a simple CustomDataset class</p>
<pre><code>class CustomDataset(InMemoryDataset):
    def __init__(self, data):
        super().__init__()
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        return sample
</code></pre>
<p>and it gave me a KeyIndex error when trying to get the Data object at index 0. I also tried a version of the above code where the super class is Dataset as opposed to InMemoryDataset but I couldn't figure out how to make the collate method work.</p>
","2024-05-05 18:30:03","0","Question"
"78430524","","Determining contents of decoder_hidden_states from T5ForConditionalGeneration","<p>I'm using the Huggingface <code>T5ForConditionalGeneration</code> model without modification.</p>
<p>I want to compute mean pooling over the last hidden state of the T5 decoder, but I can't determine which part of the <code>decoder_hidden_states</code> contains what I'm looking for.</p>
<p>I want to do something like this:</p>
<pre><code># Prepare batch data
sources = batch_df['Source'].tolist()
tokenized_input = self.tokenizer(sources, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length).to('cuda')
input_ids = tokenized_input['input_ids'].to('cuda')
attention_mask = tokenized_input['attention_mask'].to('cuda')

input_batch = {
    'input_ids': input_ids, 
    'attention_mask': attention_mask,
    'do_sample': False,
    'num_beams': 1,
    'eos_token_id': self.tokenizer.eos_token_id,
    'pad_token_id': self.tokenizer.pad_token_id,
    'max_length': self.max_output_length,
    'output_scores': True,
    'return_dict_in_generate': True,
    'output_hidden_states': True,
}
outputs = self.model.generate(**input_batch)

# Retrieve the decoder hidden states
decoder_last_hidden_state = outputs.decoder_hidden_states[-1]  # Last layer's hidden states

# Compute the mean of the hidden states across the sequence length dimension
mean_pooled_output = torch.mean(decoder_last_hidden_state, dim=1, keepdim=False)
</code></pre>
<p>This approach works for the encoder, but for the decoder, <code>decoder_hidden_states[-1]</code> is a tuple of tensors, not a tensor.</p>
<p>When I first inspected the tuples, there were 10 tuples, and each tuple contained 7 tensors.</p>
<p>When I inspected the dimensions, like this:</p>
<pre><code>for tuple_number in range(n):  # Checking the tuples
    print(f&quot;Tuple {layer_number}:&quot;)
    for i, tensor in enumerate(outputs.decoder_hidden_states[layer_number]):
        print(f&quot;  Tuple {i} in Layer {layer_number}: shape {tensor.shape}&quot;)
</code></pre>
<p>the outputs were all like this:</p>
<pre><code>Tuple 0:
  Tensor 0 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 1 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 2 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 3 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 4 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 5 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 6 in Tuple 0: shape torch.Size([2, 1, 512])
Tuple 1:
  Tensor 0 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 1 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 2 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 3 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 4 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 5 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 6 in Tuple 1: shape torch.Size([2, 1, 512])
. . .
</code></pre>
<p>512 is the max_length of my tokenizer, and 2 is my batch size.
(I verified that 2 is the batch size because that number changed when I modified my batch size.)</p>
<p>Then, when I trimmed the length of my input strings to 10 characters, to my surprise, the number of tuples went from 10 to 39. When I trimmed the strings further to only 2 chars per string, the number of tuples didn't increase beyond 39.
Then, when I doubled my input string length instead, the number of tuples went down to 7. So, it appears like the number of tuples corresponds to iterations of the decoder over some chunk size up to some limits.</p>
<p>So, if I wanted to compute mean pooling over the first token, it seems like I'd compute the mean over the last tensor of the first tuple.
However, I don't understand exactly how the token length corresponds to the number of tuples.</p>
<p>How do I determine what exactly is represented by each of these tuples and tensors? I have not been successful in finding this information by going through the T5 source code.</p>
","2024-05-04 22:13:37","2","Question"
"78429681","","Why does my CNN for a Binary Classification problem have a constant 50% accuracy with BCELoss vs 80%+ with Cross Entropy Loss?","<p>I am creating a CNN from scratch with Pytorch. I have a balanced dataset of images, split in half for both classes. I am trying to use the BCEwithLogitsLoss function from torch.nn as I have read that is typically the best for use-cases like mine. However, for some reason it seems like my network does not learn anything at all when I use this loss function! It remains at a stagnant ~50% accuracy where it is only ever guessing one class. When I use the regular CrossEntropyLoss function instead and expand my final layer's output nodes to 2, my network actually begins learning! Whereas with the &quot;correct&quot; loss function my network doesn't ever reach even 1% accuracy for the target class, using Cross Entropy Loss I can reach even 90%+ after a few epochs.</p>
<p>From my understanding Cross Entropy loss is better suited for multi-class classification problems whereas Binary Cross Entropy is better suited for binary classification problems as in the name so I don't understand how this could be the case.</p>
<p>Originally, I started with a simpler CNN due to this being my first time building one. As such, after some more research I came to the conclusion that it could be partly due to a lack of layers and complexity. Therefore, I added more layers and ended up with this blueprint:</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class ConvolutionalNN(nn.Module):
    def __init__(self):
        super(ConvolutionalNN, self).__init__()


        self.conv1 = nn.Conv2d(3, 9, 5)
        self.conv2 = nn.Conv2d(9, 27, 5)
        self.conv3 = nn.Conv2d(27, 54, 5)
        self.conv4 = nn.Conv2d(54, 108, 5)
        self.conv5 = nn.Conv2d(108, 216, 5)
        self.conv6 = nn.Conv2d(216, 432, 5)
        
        self.pool = nn.MaxPool2d(3, 3)
        
        self.fc1 = nn.Linear(432*4*4, 256)
        self.fc2 = nn.Linear(256, 64)
        self.fc3 = nn.Linear(64, 2)

    def forward(self, x):
        x = (F.relu(self.conv1(x))) #First convolutional layer, then activation function
        x = self.pool(F.relu(self.conv2(x))) #Second layer, activation function, then pooling layer
        x = (F.relu(self.conv3(x)))
        x = self.pool(F.relu(self.conv4(x)))
        x = (F.relu(self.conv5(x)))
        x = self.pool(F.relu(self.conv6(x)))
        x = x.reshape(-1, 432*4*4) #Flattens the tensor
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x
</code></pre>
<p>I was partly inspired by the double convolutional layers used in VGGNET. Furthermore, I lack experience in this so if anyone has any suggestions I am more than glad to take them.</p>
<p>I have used a learning rate of both 0.001 and 0.0001. I am using the Adam optimizer. Furthermore, my labels are not one-hot encoded. In this case above I have used 2 output nodes to cooperate with CrossEntropyLoss, however beforehand I was using 1 output node for BCE.</p>
<p>I look forward to any help at all! Thank you so much!</p>
","2024-05-04 16:36:25","0","Question"
"78429659","","How to divide a 2-D tensors into smaller blocks using pytorch?","<p>I've downloaded the EMNIST dataset of letters, and I've converted each image to a <code>torch.tensor</code> object with shape <code>torch.size([28, 28])</code>. However, I would like divide the 28*28 image into 7*7 blocks, with each block sized 16.</p>
<p>I.E. if the image pixel labeled from left to right, from up to down with 1, 2, ..., 784</p>
<pre><code>[
  [1, 2, 3, ..., 28],
  ...
  [        ..., 784]
]
</code></pre>
<p>I expect the output to be a <code>torch.tensor</code> object of size <code>torch.size([7, 7, 16])</code></p>
<pre><code>[
  [
    [1, 2, 3, 4, 29, 30, 31, 32, 57, 58, 59, 60, 85, 86, 87, 88],
    ...
    [25, 26, 27, 28, 53, 54, 55, 56, 81, 82, 83, 84, 109, 110, 111, 112]
  ],
  ...
  [
    ...
    [697, 698, 699, 700, 725, 726, 727, 728, 753, 754, 755, 756, 781, 782, 783, 784]
  ]
]

</code></pre>
<p>I've tried to use <code>torch.view(7, 7, 16)</code>, but it did not show up as the expected outcome.</p>
<p>Thanks a lot ^_^</p>
","2024-05-04 16:26:25","0","Question"
"78429005","","individual gradients with torch.autograd.grad without sum over second variable","<p>I have a sampled path of a stochastic process starting from an initial point:</p>
<pre class=""lang-py prettyprint-override""><code>
class SDE_ou_1d(nn.Module):
    def __init__(self):
        super().__init__()
        self.sde_type = &quot;into&quot;
        self.noise_type = &quot;diagonal&quot;

    def f(self, t, y): #drift
        return -y

    def g(self, t, y): #vol
        return torch.ones_like(y)

t_vec = torch.linspace(0, 1, 100)  #time array
mySDE = SDE_ou_1d()

x0 = torch.zeros(1, 1, requires_grad=True).to(t_vec)
X_t = torchsde.sdeint(mySDE, x0, t_vec, method = 'euler')

</code></pre>
<p>and I would like to measure the gradient with respect to the initial condition using <code>torch.autograd.grad()</code>, and get an output with the same shape as <code>X_t</code> i.e. 100x1.
This gives the change in the path at every time point</p>
<pre class=""lang-py prettyprint-override""><code>
X_grad = torch.autograd.grad(outputs=X_t, inputs=x0,
                           grad_outputs=torch.ones_like(X_t),
                           create_graph=False, retain_graph=True, only_inputs=True, allow_unused=True)[0]

</code></pre>
<p>the issue is that the gradient is a sum over all values of <code>t</code>.</p>
<p>I can do this with a for loop, but it is very slow and not practical:</p>
<pre class=""lang-py prettyprint-override""><code>X_grad_loop = torch.zeros_like(X_t)

for i in range(X_t.shape[0]):  # Loop over the first dimension of X_t which is time
    grad_i = torch.autograd.grad(outputs=X_t[i,...], inputs=x0,
                                    grad_outputs=torch.ones_like(X_t[i,...]),
                                    create_graph=False, retain_graph=True, only_inputs=True, allow_unused=True)[0]
    X_grad_loop[i,...] = grad_i

</code></pre>
<p>is there a way to compute this gradient with <code>torch.autograd.grad()</code> and no loop?
thanks</p>
","2024-05-04 12:39:59","0","Question"
"78428718","78421772","","<p>Here's a general template that I use for implementing custom loss functions in PyTorch. I wrote some details as comments. You can use it as a starting point:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn

# Create a class which inherits from `nn.modules.loss._Loss`
class MyCustomLoss(nn.modules.loss._Loss):  # pylint: disable=protected-access

    # If the loss function has parameters, implement the constructor
    # In your case, it seems that there are no parameters
    def __init__(
            self,
            # Add the parameters, if present. Example:
            # margin: float
    ) -&gt; None:
        super().__init__()
        # An example usage of setting a parameter:
        # self.margin: Final[float] = margin

    # Implement the forward method, specifying the inputs and outputs to your loss function,
    # and the calculation logic
    def forward(
        self,
        # Add your inputs here
    ) -&gt; torch.Tensor:

        pass  # Implement the loss function logic, as is shown in the formula you provided
</code></pre>
<p><strong>Note:</strong> I didn't fully implement this loss function as I wasn't familiar with the specific details of your use case.</p>
<p>After implementing the loss function, you can use it just the way you use PyTorch's built-in loss functions in your training phase. A simple example would be:</p>
<pre class=""lang-py prettyprint-override""><code># Instantiate the custom loss function
criterion = MyCustomLoss(parameters_if_present)

# Forward pass
model_outputs = model(train_input_or_inputs)

# Calculate loss
loss = criterion(model_outputs)

# Back-propagation and optimization
optimizer.zero_grad()
loss.backward()
optimizer.step()
</code></pre>
<p>Note that the latter example may slightly vary depending on your deep learning task.</p>
","2024-05-04 10:55:51","0","Answer"
"78426918","78376537","","<p>Because you dont know the batch_size when initializing self.pos_embedding, so you should init this tensor as:</p>
<pre><code>self.pos_embedding = nn.Parameter(
    torch.empty(1, num_patches + 1, hidden_dim).normal_(std=0.02)
) 
# (dont forget about the cls token)
</code></pre>
<p>PyTorch will take care of the tensors broadcasting in forward pass:</p>
<pre><code>x = x + self.pos_embedding
# (batch_size, num_patches + 1, embedding_dim) + (1, num_patches + 1, embedding_dim) is ok
</code></pre>
<p>But it won't work with cls token. You should expand this tensor in forward:</p>
<pre><code>cls_token = self.cls_token.expand(
    batch_size, -1, -1
)
</code></pre>
","2024-05-03 20:53:14","0","Answer"
"78425939","78421353","","<p>If you want <code>lam</code> and <code>sigma</code> to be learned, you need to implement them as pytorch <code>Parameters</code> and compute the results of <code>lam</code> and <code>sigma</code> using pytorch methods. When you call <code>.detach().numpy()</code> on values, you remove them from the computational graph, which means pytorch can't update them via backprop.</p>
<p>From your code, you define <code>lam</code> and <code>sigma</code> as <code>Parameters</code> in your <code>MyModel</code> class, so leave that be. For the <code>SDE</code> element, you can replace the class with a function (the <code>SDE</code> class is just holding variables that are already stored in <code>MyModel</code>). The <code>SDE</code> forward method isn't shown, but you need to have that implemented with pytorch methods rather than numpy.</p>
<p>You should also consider what you hope to accomplish with the <code>SDE</code> method. It looks like <code>SDE</code> produces a sequence of values based on <code>lam</code> and <code>sigma</code> that are used to scale the outputs of your GRU. Your model might decide it's easier to avoid the <code>SDE</code> params by setting them to <code>lam=1, sigma=0</code> or other uninformative parameters.</p>
","2024-05-03 16:43:34","1","Answer"
"78425157","78328401","","<p>To show how it was solved, I post all our code here. We were trying to test this repository:</p>
<p><a href=""https://github.com/sicxu/Deep3DFaceRecon_pytorch"" rel=""nofollow noreferrer"">https://github.com/sicxu/Deep3DFaceRecon_pytorch</a></p>
<p>Our final approach on Google Colab that works just fine is:</p>
<pre class=""lang-bash prettyprint-override""><code># Step 1: Press runtime on the top &gt; Change runtime type &gt; Select T4 GPU

# Step 2: Run this shell

!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
!bash miniconda.sh -b -u -p /usr/local
!rm miniconda.sh
!conda update -y conda

%cd /content/
!rm -rf Deep3DFaceRecon_pytorch
!git clone https://github.com/sicxu/Deep3DFaceRecon_pytorch.git
%cd Deep3DFaceRecon_pytorch
!git clone https://github.com/NVlabs/nvdiffrast
!git clone https://github.com/deepinsight/insightface.git
!cp -r ./insightface/recognition/arcface_torch ./models/

# Step 3: Mount drive and copy the files mounted in google drive:

!mkdir /content/Deep3DFaceRecon_pytorch/checkpoints
!cp -r /content/drive/MyDrive/Deep3D/facerecon_20230425 /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425
!cp /content/drive/MyDrive/Deep3D/01_MorphableModel.mat /content/Deep3DFaceRecon_pytorch/BFM/
!cp /content/drive/MyDrive/Deep3D/Exp_Pca.bin /content/Deep3DFaceRecon_pytorch/BFM/

# Step 4: Install the graphics related stuff

!sudo apt-get install libegl1-mesa-dev
!nvidia-smi
!sudo apt-get install libnvidia-gl-535

# Step 5: Run this shell to install everything

%%shell
eval &quot;$(conda shell.bash hook)&quot;
conda create --name deep3d_pytorch python=3.6 -y
conda activate deep3d_pytorch
conda config --env --add channels pytorch
conda config --env --add channels conda-forge
conda config --env --add channels defaults
conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.2 -c pytorch -y
conda install numpy scikit-image=0.16.2 scipy=1.4.1 pillow=6.2.1 pip ipython=7.13.0 yaml=0.1.7 -y
pip install matplotlib==2.2.5 opencv-python==3.4.9.33 tensorboard==1.15.0 tensorflow==1.15.0 kornia==0.5.5 dominate==2.6.0 trimesh==3.9.20
pip install ./nvdiffrast/.

# Step 6: Run this shell to test the program

%%shell
eval &quot;$(conda shell.bash hook)&quot;
conda activate deep3d_pytorch
python test.py --name=facerecon_20230425 --epoch=20 --img_folder=./datasets/examples

# Step 7: Copy test results to Google Drive

!ls /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425/results/examples/epoch_20_000000/
!cp /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425/results/examples/epoch_20_000000/* /content/drive/MyDrive/Deep3D/results/

# Step 8: prepare custom images along with their facial landmarks

!cp -r /content/drive/MyDrive/Deep3D/custom_images /content/Deep3DFaceRecon_pytorch/
%cd /content/Deep3DFaceRecon_pytorch/custom_images/
!python -m venv virtual_env
!source virtual_env/bin/activate &amp;&amp; which pip
!source virtual_env/bin/activate &amp;&amp; pip install mtcnn
!source virtual_env/bin/activate &amp;&amp; pip install tensorflow
!source virtual_env/bin/activate &amp;&amp; which python
!source virtual_env/bin/activate &amp;&amp; python facial_landmarks.py # This Python script source code can be found here: https://github.com/sicxu/Deep3DFaceRecon_pytorch/issues/85#issuecomment-2069302718
%cd /content/Deep3DFaceRecon_pytorch/

# Step 9: run with custom images

%%shell
eval &quot;$(conda shell.bash hook)&quot;
conda activate deep3d_pytorch
python test.py --name=facerecon_20230425 --epoch=20 --img_folder=./custom_images

# Step 10: copy the results back to Google Drive

!ls /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425/results/custom_images/epoch_20_000000/
!cp /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425/results/custom_images/epoch_20_000000/* /content/drive/MyDrive/Deep3D/results/

</code></pre>
<p>It's described here too: <a href=""https://github.com/conda/conda/issues/13812#issuecomment-2071445372"" rel=""nofollow noreferrer"">https://github.com/conda/conda/issues/13812#issuecomment-2071445372</a></p>
","2024-05-03 14:02:33","1","Answer"
"78424998","","How to convert from py torch to tflite","<p>I have been working with yolov8s model and have generated best.pt model from google colab but I have problems now in google colab, So,I want to convert the pt model to tensorflow lite on vscode.
Any ideas?</p>
","2024-05-03 13:30:04","0","Question"
"78423370","78421194","","<ol>
<li>The length of Strue should be predefined by your problem, as it should be the true data. SO you should check your problem again.</li>
<li>As for the 1D convolution on pytorch, you should have your data in shape <code>[BATCH_SIZE, 1, size]</code> (supposed your signal only contain 1 channel), and pytorch functional <code>conv1d</code> actually support padding by a number (which should pad both sides) so you can input <code>kernel_size - 1</code> (in your case <code>119</code>) for the padding parameter to imitate a full mode convolution.</li>
<li>The <code>BATCH_SIZE</code> should be indepentdant with your data, as it mean that they are separate data point that is stack together to be computed parallel against each other.</li>
</ol>
<p>Here is an example:</p>
<pre><code>x = torch.rand(1,1,149)
kernel = torch.rand(1,1,120) #&lt;- [out_channels, in_channels, kernel_size] 
torch.nn.functional.conv1d(x, kernel, padding= 119).shape

&gt;&gt;&gt; torch.Size([1, 1, 268])
</code></pre>
","2024-05-03 07:55:11","1","Answer"
"78423100","","Why am I getting an error loading a specific module(torch)?","<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\Admin\OneDrive\Desktop\Python\OpenCV\t.py&quot;, line 2, in &lt;module&gt;
    import torch
  File &quot;C:\Users\Admin\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\__init__.py&quot;, line 141, in &lt;module&gt;
    raise err
OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\Admin\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.
</code></pre>
<p>I uninstall and then reinstalled the module and also tried different versions of python too but nothing worked.</p>
","2024-05-03 06:52:29","2","Question"
"78421772","","Implementing a custom unsupervised loss function for Graph Convolution","<p>I have a graph convolution model built and wanted to implement a custom unsupervised loss function like the one shown in below picture:</p>
<p><a href=""https://i.sstatic.net/bZd9jukU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZd9jukU.png"" alt=""https://i.sstatic.net/bZd9jukU.png"" /></a></p>
<p>Where yv is the learned embedding of a node v and rand denotes random sampling operation over all the nodes.</p>
<p>I am new to Pytorch so not sure if this is already implemented. Any help will be sincerely appreciated.</p>
<p>Since I am new to Pytorch, not sure where to get started with implementing a custom loss function.</p>
","2024-05-02 21:13:13","1","Question"
"78421353","","How to calibarate SDE parameters with PyTorch model?","<p>I'm building a GRU model for stock price forecasting. I wanted to integrate a stochastic process to the model to model the price volatility.</p>
<p>So, this is the class to produce a stochastic path.</p>
<pre><code>class SDE(nn.Module):
def __init__(self,lam,sigma):
    super().__init__()
    self.lam = lam
    self.sigma = sigma

def forward(self, T, steps, Npaths):
    np.random.seed(4)
    lam = self.lam.detach().numpy()
    sigma= self.sigma.detach().numpy()
     .....
    return sigma * lam * xx
</code></pre>
<p>Now, my model is :</p>
<pre><code>class MyModel(nn.Module):
def __init__(self, args):
    super(MyModel, self).__init__()
    self.lam = nn.Parameter(torch.tensor(1.0), requires_grad=True)
    self.sigma = nn.Parameter(torch.tensor(0.2), requires_grad=True)
    # GRU layers
    self.gru = nn.GRU(
        self.input_dim, self.hidden_dim, self.layer_dim, batch_first=True, 
                                            dropout=args.dropout, bidirectional=True)

    # SDE 
     levy = SDE(self.lam, self.sigma) 
    # Fully connected layer
    self.fc = nn.Linear(self.hidden_dim * 2, self.output_dim)

def forward(self, x):
        lev = torch.from_numpy(levy(1.0, 16, 1))
        .....
        h0 = torch.zeros(self.args['num_layers']* 2, x.size(0), self.args['n_hidden_units'], 
                                                                                 device=x.device).requires_grad_()
        out, _ = self.gru(x, h0.detach())
        out = out[:, -1, :]
        out = self.fc(out)
        out_m = torch.mul(out, lev)
        return out
</code></pre>
<p>The train will besomething like this:</p>
<pre><code>    # Makes predictions
      yhat = self.model(x)
      # Computes loss
      loss = self.loss_fn(y, yhat)
      # Computes gradients
      #loss.requires_grad = True
      loss.backward()
      # Updates parameters and zeroes gradients
      self.optimizer.step()
      self.optimizer.zero_grad()
</code></pre>
<p>By training this network, Should this code calibrate and identify optimal values for the sigma and lam parameters used for generating the stochastic path in SDE?</p>
<p>I can see from the debug that their values are always the same.</p>
<p>Any tips, please, to make this code useful for my objective, which is to calibrate the sigma and lam?</p>
","2024-05-02 19:26:09","0","Question"
"78421194","","1D signal convolution using Pytorch","<p>I am learning the signal convolution and I am little bit confusing the different between Pytorch functional conv1d and scipy convolution. What I know for sure is pytorch conv1d is actually calculating the cross-correlation and scipy will do the traditional convolution. I could manually flip the kernel to match from cross-correlation to convolution.</p>
<p>Right now, for example I have a 1D kernel with length 120 (120 timing points) and a signal Sobs with length 149 (149 timing points). I know the Sobs is coming from some signal called Strue conv with the kernel (length 120). My final goal is getting the Sture.</p>
<p>Suppose I have a random guess Sture, what length should be? If I want to keep the Strue has the same length of Sobs, the convoltuion result (Strue * kernel) will become 149 + 120 - 1 = 268 (I want to use the full mode) instead of Sobs = 149. What should I do here?</p>
<p>If I want to write the code using pytorch (I want to have the initial Strue guess via a fully connected neural network). The network will give me a random Sture [size, 1] since it is a 1D signal, if I want to calculate the conv using pytorch functional conv1d. What is my minibatch? Should it be 149 or 1? And the pytorch functional conv1d only has mode valid and same. How could I adjust the padding to match the full mode in Scipy?</p>
","2024-05-02 18:53:08","0","Question"
"78420856","78392429","","<p>Probably the easiest way to do this is to dump the param groups and state dict of the optimizer after every <code>backward</code> call. This will let you capture the parameters, gradients and optimizer state.</p>
<p>The code below shows an example with a simple MLP, but the <code>log_state</code> function should work for all models so long as the param groups/state dict don't have multiple levels of nesting.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

# example model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h = self.fc1(x)
        pred = self.fc2(self.relu(h))
        return pred

def log_state(opt):
    output = {}

    # log state dict
    state_dict = {}
    for key, value in opt.state_dict().items():
        if key == 'state':
            state_dict[key] = {}
            for state_key, state_value in value.items():
                state_dict[key][state_key] = {}
                for k, v in state_value.items():
                    if torch.is_tensor(v):
                        state_dict[key][state_key][k] = v.cpu().clone() # move tensor to cpu
                    else:
                        state_dict[key][state_key][k] = v
        else:
            state_dict[key] = value

    output['state_dict'] = state_dict

    # log param groups
    param_groups = []
    for group in opt.param_groups:
        param_group = {key: value for key, value in group.items() if key != 'params'}
        param_group['params'] = []
        param_group['param_grads'] = []
        for param in group['params']:
            param_group['params'].append(param.data.cpu().clone()) # move tensor to cpu
            # log gradients
            if param.grad is not None:
                param_group['param_grads'].append(param.grad.data.cpu().clone()) # move tensor to cpu
            else:
                param_group['param_grads'].append(None)

        param_groups.append(param_group)

    output['param_groups'] = param_groups
    
    return output



net = MLP(64, 20, 10)

opt = torch.optim.Adam(net.parameters(), lr=1e-3)

state_log = {}
for i in range(5):
    x = torch.randn(8, 64)
    y = torch.randn(8,10)
    p = net(x)
    loss = nn.functional.mse_loss(p, y)
    opt.zero_grad()
    loss.backward()
    state_log[i] = log_state(opt) # log optimizer state every step
    opt.step()
</code></pre>
<p>The code logs after <code>backward</code> to grab gradients. You could add additional logging after <code>step</code>, but the values updated after <code>step</code> (params, optimizer averages, etc) will be captured on the next iteration. You can also use values from one iteration to recreate the next. For example, with Adam:</p>
<pre class=""lang-py prettyprint-override""><code>step = 2

old_state = state_log[step]
new_state = state_log[step+1]

# adam params
b1 = 0.9
b2 = 0.999
eps = 1e-8
lr = 1e-3

w = old_state['param_groups'][0]['params'][0]
g = old_state['param_groups'][0]['param_grads'][0]

m_old = old_state['state_dict']['state'][0]['exp_avg']
v_old = old_state['state_dict']['state'][0]['exp_avg_sq']

m = b1 * m_old + (1-b1)*g
v = b2 * v_old + (1-b2)*(g.pow(2))

m_hat = m.div(1-b1**(step+1))
v_hat = v.div(1-b2**(step+1))

w_new = w - lr * m_hat / (torch.sqrt(v_hat) + eps)

torch.allclose(w_new, new_state['param_groups'][0]['params'][0])
&gt; True
</code></pre>
","2024-05-02 17:36:46","-1","Answer"
"78419376","78418341","","<p>The approach below uses <code>nn.Softplus</code> and <code>.clamp</code> to map an arbitrarily-valued tensor to a positive one, before taking the log.</p>
<pre class=""lang-py prettyprint-override""><code>P_nonneg = nn.Softplus(P_original)       #constrain it to be &gt;= 0
P_pos = torch.clamp(P_nonneg, min=1e-10) #prevent 0, so it's always +ve
loss = torch.log(P_pos)                  #safely take log
</code></pre>
<p>You can set <code>min=</code> depending on how far away from 0 you want to constrain <code>P</code> immediately before taking the log. I've set it to a small number, so it can come close to 0 and potentially render a huge loss, but it'll never hit 0 exactly. If such a large loss causes issues, you can turn <code>min=</code> up to smooth things out, though it makes the loss a bit less sensitive to smaller values.</p>
","2024-05-02 13:08:34","0","Answer"
"78418341","","Pytorch: how do I make sure the model output is not 0 or negative?","<p>I need to calculate loss on my model. The loss requires the logarithm of the output. This is for an actor critic model for those who want to know.
I use a network that uses relu and softmax to make sure the values are not getting to high or that they are negative. But they are sometimes 0. This is not good since I cannot take the log of that.</p>
<p>What can I do to avoid this?</p>
<p>I tried using a custom relu function but for some reason it does not work.</p>
<p>I tried also Increasing the value in cases that it is 0 by 0.01 but then I get an error that there was a local change.</p>
<p>The loss function looks like this. Where P is the output of the model, eta and value constant are some unimportant values. And a[t] is the action at time t. This is not important as well. The important part is that the P output should not be 0.0.</p>
<pre><code>x = self.eta*P*torch.log(P)
theta_loss += -value_constant*torch.log(P[a[t]])+torch.sum(x)
</code></pre>
<p>This is the relu function</p>
<pre><code>class MyReLU(torch.autograd.Function):

    @staticmethod
    def forward(ctx, inp):
        ctx.save_for_backward(inp)
        # out = torch.zeros_like(inp).cuda()
        # out[inp &gt; 0.01] = inp
        return torch.where(inp &lt; 0.01, 0.01, inp)

    @staticmethod
    def backward(ctx, grad_output):
        inp, = ctx.saved_tensors
        # grad_input = grad_output.clone()
        # grad_input[inp &lt; 0.01] = 0
        grad = torch.where(inp &lt;= 0.01,0.0,1)
        return grad_output * grad
</code></pre>
","2024-05-02 10:00:21","1","Question"
"78411911","78401518","","<p>You can't avoid using a loop. At best you can execute the loop at a lower level that runs faster than standard python.</p>
<p>You can use <code>torch.nonzero</code> to find nonzero values and iterate over those to build the output dict.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from collections import defaultdict

# ie init random adjacency matrix
adj_matrix = (torch.rand(10,10)&gt;0.8).float()

# grab nonzero values
nonzero_vals = torch.nonzero(adj_matrix)

# init output adjacency list
adj_list = defaultdict(set)

# add nonzero rows/cols to output dict 
for (row, col) in nonzero_vals.tolist():
    adj_list[row].update([col])
    if row != col:
        adj_list[col].update([row])
    
adj_list = {k:list(v) for k,v in adj_list.items()}
</code></pre>
","2024-05-01 03:38:38","0","Answer"
"78410494","78143186","","<p>This worked for me, although it took very long to generate an image.</p>
<pre><code>from diffusers import DiffusionPipeline
import numpy as np
pipeline = DiffusionPipeline.from_pretrained(
    &quot;RunDiffusion/Juggernaut-X-v10&quot;, cache_dir=cache_dir
)
prompt = &quot;Your Prompt&quot;

# Set num_inference_steps as per your memory or computation
image = pipeline(prompt, num_inference_steps=100, guidance_strength=0.7)
image_data = np.array(image.images[0])
image_pil = Image.fromarray(image_data.astype(np.uint8))
image_pil.save(f&quot;output_images/Example10.png&quot;)
image_pil
</code></pre>
","2024-04-30 18:41:22","0","Answer"
"78408484","78408109","","<p>The paper alone is arguably not very helpful in describing the notation that you are interested in; however, the corresponding <a href=""https://github.com/weiliu89/caffe/tree/ssd/"" rel=""nofollow noreferrer"">code repository</a> adds a bit more information:</p>
<p>If we look at <a href=""https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_pascal.py"" rel=""nofollow noreferrer""><code>ssd_pascal.py</code></a>, for example, we can see where layers of such name are created (starting from line 23):</p>
<pre class=""lang-py prettyprint-override""><code>    out_layer = &quot;conv6_1&quot;
    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1,
        lr_mult=lr_mult)

    from_layer = out_layer
    out_layer = &quot;conv6_2&quot;
    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2,
        lr_mult=lr_mult)
</code></pre>
<p>Now, we should also take a look at the definition of <code>ConvBNLayer</code> in <a href=""https://github.com/weiliu89/caffe/blob/ssd/python/caffe/model_libs.py"" rel=""nofollow noreferrer""><code>model_libs.py</code></a> (starting from line 30):</p>
<pre class=""lang-py prettyprint-override""><code>def ConvBNLayer(net, from_layer, out_layer, use_bn, use_relu, num_output,
    kernel_size, pad, stride, dilation=1, use_scale=True, lr_mult=1,
    conv_prefix='', conv_postfix='', bn_prefix='', bn_postfix='_bn',
    scale_prefix='', scale_postfix='_scale', bias_prefix='', bias_postfix='_bias',
    **bn_params):
</code></pre>
<p>Then we can piece this information together and add a bit of guesswork:</p>
<ul>
<li><p><em>Technically</em>, <code>conv#_1</code> and <code>conv#_2</code> (where <code>#</code> is a placeholder for the actual layer number) are always two convolutional network layers, created by calling <code>ConvBNLayer</code>, that follow right after one another in the network architecture (thus, the output of <code>conv#_1</code> is the input of <code>conv#_2</code>). They differ in the number of output channels (e.g. 256 vs. 512 above), the kernel size (e.g. 1 vs. 3 above), the padding amount (e.g. 0 vs. 1 above), and the stride (e.g. 1 vs. 2 above).</p>
</li>
<li><p><em>Logically</em>, for the authors, the two layers seem to be considered as sublayers of the same network layer, thus the <code>_1</code> and <code>_2</code> suffix. So, <code>conv6_1</code> would be <em>convolutional layer 6, sublayer 1</em> and <code>conv6_2</code> would be <em>convolutional layer 6, sublayer 2</em>. This becomes clear when looking at the figure, for example at the illustration of <em>Conv8_2</em> and what is written below it:</p>
<ul>
<li>Illustrated is what the authors consider the output of the 8th layer of their network: a 10x10x512 feature-space representation of the input image.</li>
<li>&quot;Conv: 1x1x256&quot; means, we first have a sublayer (which should go by the name <em>Conv8_1</em>) with kernel size 1 and 256 output channels, which is followed by …</li>
<li>&quot;Conv: 3x3x512-s2&quot;, i.e. a sublayer (<em>Conv8_2</em>) with kernel size 3, 512 output channels, and a stride of 2.</li>
</ul>
</li>
</ul>
<p>Note that the names and numbers of layers of the figure don't match the names and numbers of <code>ssd_pascal.py</code> (the latter ends after <code>conv9_2</code>, which has a different stride as <em>Conv9_2</em> in the figure), but the scheme should be the same, assuming that the authors worked with a certain consistency.</p>
<p>As to your final question: In the SSD description page, where they write e.g.</p>
<blockquote>
<ul>
<li>The conv5_x, avgpool, fc and softmax layers were removed from the original classification model.</li>
<li>All strides in conv4_x are set to 1x1.</li>
</ul>
</blockquote>
<p>I assume that &quot;x&quot; simply serves as a placeholder for the suffix <code>_1</code>, <code>_2</code> and thus should be read as follows: <em>conv5_1 and conv5_2 were removed, all strides in conv4_1 and conv4_2 are set to 1x1.</em></p>
","2024-04-30 12:22:13","1","Answer"
"78408109","","What does Conv4_x, Conv8_x means in SSD","<p>In the <a href=""https://pytorch.org/vision/main/models/generated/torchvision.models.detection.ssd300_vgg16.html"" rel=""nofollow noreferrer"">SSD model</a> presented in the <a href=""https://arxiv.org/abs/1512.02325"" rel=""nofollow noreferrer"">paper</a>, it is said that the base network is considered as <a href=""https://arxiv.org/abs/1409.1556"" rel=""nofollow noreferrer"">VGG16</a> and the extra feature layers are added at the end of it that allows feature maps to be produced at different scales and aspect ratios.</p>
<p><strong>My question is</strong> that in the architecture shown in Fig.2 (shown below) in the <a href=""https://arxiv.org/abs/1512.02325"" rel=""nofollow noreferrer"">SSD paper</a>, the convolution layers have notations shown like <code>Conv5_3</code> , <code>Conv4_3</code>, for the base network <code>Conv8_2</code>,  <code>Conv9_2</code>,  <code>Conv10_2</code> for the added features layers.</p>
<p>What does this <strong>_2, _3</strong> notation mean in the convolution layers representation?</p>
<p>I have seen the same notations being used in the <a href=""https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/"" rel=""nofollow noreferrer"">SSD model description page</a>, where base network VGG16 change to ResNET50 and used notations like <strong>Conv5_x, Conv4_x</strong>.</p>
<p>What does this <strong>_x</strong> means for the convolution layer notation?
<a href=""https://i.sstatic.net/0klbUipC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0klbUipC.png"" alt=""enter image description here"" /></a></p>
<p>(note): The <a href=""https://pytorch.org/vision/main/models/generated/torchvision.models.detection.ssd300_vgg16.html"" rel=""nofollow noreferrer"">SSD model</a> and <a href=""https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html"" rel=""nofollow noreferrer"">VGG16 model</a> (till considered as base network in SSD) have same layers (see below), but resulted different output feature maps (<code>torchinfo.summary(model,(1, 3, 300, 300))</code> used)<a href=""https://i.sstatic.net/MB306cup.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MB306cup.png"" alt=""enter image description here"" /></a> VGG16 each layer output feature map<a href=""https://i.sstatic.net/FyJzQmDV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FyJzQmDV.png"" alt=""enter image description here"" /></a>, SSD each layer output feature map <a href=""https://i.sstatic.net/jrNGaWFd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jrNGaWFd.png"" alt=""enter image description here"" /></a></p>
","2024-04-30 11:13:15","1","Question"
"78405109","78404199","","<p>One potential way to handle the problem of imbalanced data for sentiment analysis and avoid a biased model in the future is to try LLMs like <a href=""https://www.mdpi.com/2076-3417/13/17/9766"" rel=""nofollow noreferrer"">GPT</a> or <a href=""https://www.sciencedirect.com/science/article/abs/pii/S0952197623011831"" rel=""nofollow noreferrer"">BERT</a> to generate random text for undersampled classes.</p>
","2024-04-29 20:45:32","1","Answer"
"78404705","","How to multiply matrices and exclude elements based on masking?","<p>I have the following input matrix</p>
<pre><code>inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.7980, 0.0000],
        [1.0000, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.0000, 1.0000]])
</code></pre>
<p>and indices of the elements that I want to exclude (<strong>in this example they are the zero elements, but they can be any value</strong>)</p>
<pre><code>mask_indices = torch.tensor(
[[7, 2],
[2, 6]])
</code></pre>
<p>How can I exclude these elements from the multiplication with the following matrix:</p>
<pre><code>my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696],
        [0.4414, 0.2969, 0.8317]])
</code></pre>
<p>That is, instead of multiplying it including these values (zeros in this example):</p>
<pre><code>a = torch.mm(inp_tensor, my_tensor)
print(a)
tensor([[1.7866, 2.5468, 1.6330],
        [2.2041, 2.5388, 2.3315]])
</code></pre>
<p>I want to exclude the (zero) elements (and the corresponding rows of <code>my_tensor</code>), <strong>so they will not participate in the computational graph</strong>:</p>
<pre><code>inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.6524, 0.6057, 0.3725, 0.7980]]) # remove the elements based on the indices (the zeros here)

my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696]]) # remove the corresponding zero elements rows

b = torch.mm(inp_tensor, my_tensor)
print(b)
&gt;&gt;&gt; tensor([[1.7866, 2.5468, 1.6330]])

inp_tensor = torch.tensor([[1.0000, 0.1115, 0.6524, 0.6057, 0.3725, 1.0000]]) # remove the elements based on the indices (the zeros here)

my_tensor = torch.tensor(
        [
        [0.8823, 0.9150, 0.3829],                
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.4414, 0.2969, 0.8317]])  # remove the corresponding zero elements rows

c = torch.mm(inp_tensor, my_tensor)
print(c)
&gt;&gt;&gt; tensor([[2.2041, 2.5388, 2.3315]])
print(torch.cat([b,c]))
&gt;&gt;&gt; tensor([[1.7866, 2.5468, 1.6330],
        [2.2041, 2.5388, 2.3315]])
</code></pre>
<p>I need this to be efficient (i.e., no <code>for loops</code>), as my tensors are quite large, and also to maintain the gradient (i.e., if I call <code>optimizer.backward()</code> that the relevant parameters from the computational graph be updated)</p>
<p>Note that each of the rows of <code>inp_tensor</code> have the same number of elements to be removed (e.g., zero elements in this example). Hence, each of the rows of <code>mask_indices</code> will also have the same number of elements (e.g., 2 in this example).</p>
","2024-04-29 19:07:12","2","Question"
"78404199","","The number of samples of each class in my dataset is not equal","<p>I hope you are doing well.</p>
<p>I am currently using a dataset that contains 3 classes with distributions of 15%, 31%, and 52% for each class.</p>
<p>I wanted to ask whether I need to balance the number of samples for each class for the sentiment analysis project or not.</p>
<p>One of the considerations I had in mind is that it might be easier to classify positive sentences, so the distribution of positive class labels in the dataset is low. Or, due to the difficulty of identifying neutral comments, there should be more samples in the dataset.</p>
<p>For this reason, I am confused about whether I should equalize the number of samples for each class or use the same distribution.</p>
<p>My dataset is available at the following link: <a href=""https://huggingface.co/datasets/Khedesh/MirasOpinion"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/Khedesh/MirasOpinion</a></p>
<p>Please help me use the dataset with the appropriate class frequency percentage.</p>
","2024-04-29 17:04:47","0","Question"
"78401954","78401834","","<p>The problem might be in this line:</p>
<pre><code>loss = criterion(outputs, labels)
</code></pre>
<p>You should replace <code>labels</code> with <code>labels_onehot</code> if you are using one-hot encoding for your labels:</p>
<pre><code>loss = criterion(outputs, labels_onehot)
</code></pre>
","2024-04-29 10:08:23","1","Answer"
"78401834","","Calculating MSE loss on two tensors of the same shape","<p>I am trying to do a classification task but due to reasons I needed to delete the softmax and replace the loss module from cross entropy to MSE now to create a one hot tensor for the labels (target) I do the following:</p>
<pre><code>        labels_onehot = nn.functional.one_hot(labels, num_classes=10).float()
</code></pre>
<p>but when i try to calculate the loss an exception is thrown</p>
<pre><code>Cell In[13], line 121
    116 print(&quot;Labels one-hot shape:&quot;, labels_onehot.shape)
    118 loss = criterion(outputs, labels_onehot)
--&gt; 121 loss = criterion(outputs, labels)
    122 loss.backward()
    123 optimizer.step()

File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-&gt; 1518     return self._call_impl(*args, **kwargs)

File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535, in MSELoss.forward(self, input, target)
    534 def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
--&gt; 535     return F.mse_loss(input, target, reduction=self.reduction)

File /opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3328, in mse_loss(input, target, size_average, reduce, reduction)
   3325 if size_average is not None or reduce is not None:
   3326     reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3328 expanded_input, expanded_target = torch.broadcast_tensors(input, target)
   3329 return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))

File /opt/conda/lib/python3.10/site-packages/torch/functional.py:73, in broadcast_tensors(*tensors)
     71 if has_torch_function(tensors):
     72     return handle_torch_function(broadcast_tensors, tensors, *tensors)
---&gt; 73 return _VF.broadcast_tensors(tensors)

RuntimeError: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1 ```

I tried printing the shapes and they both were of the same shape and I cannot see why the exceptions were thrown.
</code></pre>
","2024-04-29 09:49:19","1","Question"
"78401749","78401518","","<p>By not using loops I presume you mean avoiding nested loops like this:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

def adjacency_matrix_to_list_with_loops(adj_matrix):
    num_nodes = adj_matrix.shape[0]
    adj_list = []
    for i in range(num_nodes):
        neighbors = []
        for j in range(num_nodes):
            if adj_matrix[i][j] == 1:
                neighbors.append(j)
        adj_list.append(neighbors)
    return adj_list
</code></pre>
<p>If you don't wish to use an additional library like <code>networkx</code> then you will need to use at least one loop to iterate over the matrix nodes.</p>
<p>The result of <code>torch.nonzero</code> is a tensor with shape <code>(n, 1)</code> (where <code>n</code> is the number of non-zero elements). The <code>.squeeze()</code> method removes the singleton dimension, resulting in a tensor of shape <code>(n,)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import torch

def adjacency_matrix_to_list(adj_matrix):
    num_nodes = adj_matrix.shape[0]
    adj_list = []
    for i in range(num_nodes):
        # Find indices of non-zero elements in the i-th row
        neighbors = torch.nonzero(adj_matrix[i]).squeeze()
        adj_list.append(neighbors.tolist())
    return adj_list
</code></pre>
<p>The performance of this method will depend on the size of the graph and the sparsity of the adjacency matrix. For very large graphs, the <code>networkx</code> solution may be a more optimized approach.</p>
","2024-04-29 09:33:57","0","Answer"
"78401518","","I want to get adjacency list with pytorch geo. Is there any method in pytorch do that or anything to convert adjacency matrix but optimized, not loop","<p>I want to get adjacency list with pytorch geo. Is there any method in pytorch to do that or anything to convert adjacency matrix but optimized, not loop?</p>
<p>Like this :</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import networkx as nx


def adjacency_matrix_to_list(adj_matrix):
    # Créer un graphe à partir de la matrice d'adjacence
    graph = nx.from_numpy_matrix(adj_matrix)

    # Convertir le graphe en liste d'adjacence
    adj_list = nx.to_dict_of_lists(graph)

    return adj_list
</code></pre>
","2024-04-29 08:48:07","1","Question"
"78401104","78389728","","<p>You need to distinguish between <strong>Run Code</strong> and <strong>Run Python File</strong>.</p>
<p><a href=""https://i.sstatic.net/ED5eFuCZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ED5eFuCZ.png"" alt=""enter image description here"" /></a></p>
<p>One of them is a <em>Code Runner</em> (Run Code) extension and the other is the official extension <strong>Python</strong> (Run Python File).</p>
<p><em>Code Runner</em> is not affected by <code>Python: Select Interpreter</code>, it simply executes scripts using the command <code>python -u name.py</code>. So it doesn't necessarily use the python environment you chose for vscode.</p>
","2024-04-29 07:27:03","0","Answer"
"78400280","78396679","","<p>Is there any progress dealing with the issue?</p>
<p>I am experiencing the same.
When I try to import torch, it just raises error saying that shm.dll or one of its dependencies could not be loaded.</p>
<p>Could this be connected to the CUDA version the NVIDIA driver is compiled with?
The display driver on my computer has CUDA version 12.3 (from nvidia-smi log),
but the pytorch package installed on my computer supports CUDA version 12.1.</p>
<p>If this is the case, where and how can I get NVIDIA driver that is compatiable with pytorch package? The NVIDIA driver site doesn't have options for CUDA version.</p>
","2024-04-29 02:52:27","0","Answer"
"78398810","78398311","","<p>Your <code>fastai</code> is too old: the current version is 2.7.15 while yours is 1.0.60.</p>
<p>1.0.60 seems to have been written for Python 2, whose <code>collections</code> <a href=""https://docs.python.org/2.7/library/collections.html#collections.Sized"" rel=""nofollow noreferrer"">had <code>Sized</code></a>.</p>
<p>The simple solution is to update.</p>
","2024-04-28 16:07:12","2","Answer"
"78398576","78398026","","<p><code>step</code> is a <em>method</em> of the <code>optimizer</code>, so you need to <em>call</em> it to have the parameters updated by the optimizer (based on the gradients calculated by the loss function when you did <code>loss.backward()</code>):</p>
<pre><code>optimizer.step()  # note the () after optimizer.step
</code></pre>
","2024-04-28 14:52:54","2","Answer"
"78398311","","AttributeError: module 'collections' has no attribute 'Sized' when trying to load a pickled model","<p>I am trying to load a pretrained model but I'm hitting an error when I do. AttributeError: module 'collections' has no attribute 'Sized'</p>
<pre><code>from fastai import *
from fastai.vision import *
from matplotlib.pyplot import imshow
import numpy as np
import matplotlib.pyplot as plt
from skimage.transform import resize
from PIL import Image
learn = load_learner(&quot;&quot;, &quot;model.pkl&quot;)
</code></pre>
<p>These are the version I'm using.</p>
<pre><code>torch                     1.11.0                 
torchvision               0.12.0                  
python                    3.10.14   
fastai                    1.0.60  

</code></pre>
<p>Can someone help me fix this problem?</p>
<pre><code>File c:\Users\lib\site-packages\fastai\basic_train.py:620, in load_learner(path, file, test, tfm_y, **db_kwargs)
    618 state = torch.load(source, map_location='cpu') if defaults.device == torch.device('cpu') else torch.load(source)
    619 model = state.pop('model')
--&gt; 620 src = LabelLists.load_state(path, state.pop('data'))
    621 if test is not None: src.add_test(test, tfm_y=tfm_y)
    622 data = src.databunch(**db_kwargs)

File c:\Users\lib\site-packages\fastai\data_block.py:578, in LabelLists.load_state(cls, path, state)
    576 &quot;Create a `LabelLists` with empty sets from the serialized `state`.&quot;
    577 path = Path(path)
--&gt; 578 train_ds = LabelList.load_state(path, state)
    579 valid_ds = LabelList.load_state(path, state)
    580 return LabelLists(path, train=train_ds, valid=valid_ds)

File c:\Users\lib\site-packages\fastai\data_block.py:690, in LabelList.load_state(cls, path, state)
    687 @classmethod
    688 def load_state(cls, path:PathOrStr, state:dict) -&gt; 'LabelList':
    689     &quot;Create a `LabelList` from `state`.&quot;
--&gt; 690     x = state['x_cls']([], path=path, processor=state['x_proc'], ignore_empty=True)
    691     y = state['y_cls']([], path=path, processor=state['y_proc'], ignore_empty=True)
...
--&gt; 298     if not isinstance(a, collections.Sized) and not getattr(a,'__array_interface__',False):
    299         a = list(a)
    300     if np.int_==np.int32 and dtype is None and is_listy(a) and len(a) and isinstance(a[0],int):

AttributeError: module 'collections' has no attribute 'Sized'
</code></pre>
","2024-04-28 13:24:35","1","Question"
"78398026","","ANN training in pytorch giving me unchanged lossfunction","<p>i am learning pytorch. when I have run my code in jupyter cell the loss function is remaining unchanged. It should be either raised or get down. why is this happening?</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
class Model(nn.Module):
    def __init__(self,in_features=4,h1=8,h2=9,out_features=3):
        #how many layers:
        #intiate the inherrited class of nn.Module
        
        super().__init__()
        #Input layer(4 feature)--&gt; hiddenlayer 1 neural net---&gt;hiddenlayer2 neural net--&gt;output(3 classes of Iris dataset)
        #fully connected layer. I could edit the layers here for example first layer is connected with  h1, h1 is connected with h2 
        # then h2 is connecte with out_Features
        #Alternatively i can use  all values of in_features,hidden layers value and out features  loaded in init parameter
        self.fc1=nn.Linear(in_features,h1)
        self.fc2=nn.Linear(h1,h2)
        self.out=nn.Linear(h2,out_features)
        
    #propagate method  here start  foroward propagation  
    def forward(self,x):
        x=F.relu(self.fc1(x))
        x=F.relu(self.fc2(x))
        x=self.out(x)
        return x
    torch.manual_seed(32)
    model=Model()
    df=pd.read_csv('iris.csv')
    df.tail()
    X=df.drop('target',axis=1).values
    y=df['target'].values
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=33)


    X_train=torch.FloatTensor(X_train)
    X_test=torch.FloatTensor(X_test)

    y_train=torch.LongTensor(y_train)

    y_test=torch.LongTensor(y_test)
    criterion=nn.CrossEntropyLoss()
    optimizer=torch.optim.Adam(model.parameters(),lr=0.01)
    epoch=100
    #for tracking loss  lets make it a empty list and then make the loss
    losses=[]
    for i in range(epoch):


   #forward and get a prediction
    y_pred=model.forward(X_train)#passing x_train  to forward function which actually applies features in fully connected neural net and activation function applied on those
    #measuring loss between predicted y, and actual y which is y_train. Using CrossEntropyloss, so we dont need to one hot encoding here
    loss=criterion(y_pred,y_train)
    #append the loss into losses for tracking losses in each epoch completion
    losses.append(loss.item())
    # we print this performance every 10 epoch completion
    if i%10==0:
        print(f'epoch{i} and loss is :{loss}')
    
    #Back propagation
    
    optimizer.zero_grad() #resetting the gradient since it accumulates in every epoch
    #print('Optimizer check',optimizer.zero_grad())
    loss.backward()#adjusting the paramter
   
    optimizer.step# updting the weight and bias
</code></pre>
<p>output:</p>
<pre><code>epoch0 and loss is :1.1507114171981812
epoch10 and loss is :1.1507114171981812
epoch20 and loss is :1.1507114171981812
epoch30 and loss is :1.1507114171981812
epoch40 and loss is :1.1507114171981812
epoch50 and loss is :1.1507114171981812
epoch60 and loss is :1.1507114171981812
epoch70 and loss is :1.1507114171981812
epoch80 and loss is :1.1507114171981812
epoch90 and loss is :1.1507114171981812
</code></pre>
","2024-04-28 11:39:15","1","Question"
"78397788","78397119","","<p>As first step you should try a simple hello world python script to see if python works as expected in your system. It looks like fine but it doesnt hurts to check.</p>
<p>Then you can try runinng your scprit just by</p>
<pre><code>python  myscript.py
</code></pre>
<p>to do this you need to navigate to the location your scirpt is located
in your case</p>
<pre><code>cd /mnt/d/Python_Project/TransUNet_baseline/train_model
</code></pre>
<p>This helps because in model training it is highly likely that your training script calls diffrent files and again it is highly possible that paths of those files given as relative paths. In this condition python thinks starting point of path as your current working directory.</p>
<p>You need to change your current working directory to your script location (with cd command given above).</p>
<p>This might not cause an error if files opened with write/read, python might have created those files and tried to read each data inside (in this case none)  so it cant train anything. This is just a guess by the inforamtion provided. It might be a different problem.</p>
","2024-04-28 10:06:51","0","Answer"
"78397119","","Installing Python as a Python interpreter in WSL to run deep learning programs without any response","<p>When I use WSL as a Python interpreter to run a deep learning program, there is no response and no error after running for a period of time. <a href=""https://i.sstatic.net/2fJngHHM.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Has anyone encountered a similar situation before? I didn't do anything about the WSL Python interpreter.</p>
","2024-04-28 05:05:07","0","Question"
"78396679","","Pytorch import issue with shm.dll","<p>I have an issue with importing torch, and error dealing with shm.dll or its dependencies came up. I would appreciate any help on this! Thank you in advance!</p>
<p>I have tried to use both the CPU and CUDA versions, but neither work and both come up with this error. <a href=""https://i.sstatic.net/mdTkpcTD.png"" rel=""nofollow noreferrer"">This is the error that I get</a>
<a href=""https://i.sstatic.net/JfW7i052.png"" rel=""nofollow noreferrer"">This is the dependency walker view</a></p>
","2024-04-27 23:46:22","0","Question"
"78396330","78247259","","<p>In case of yolov9.c or e models, you have to use train_dual.py instead of train.py</p>
","2024-04-27 20:31:05","0","Answer"
"78394535","78393551","","<p>PyTorch 2.2.2 is not supported as of vllm 0.4.1. You can build vllm from source with a custom PyTorch version or wait for the next release of vllm as mentioned <a href=""https://github.com/vllm-project/vllm/issues/3987#issuecomment-2048723520"" rel=""nofollow noreferrer"">here</a>.</p>
<p>You can install vllm 0.4.1 with pytorch 2.2.1 as mentioned in the release <a href=""https://github.com/vllm-project/vllm/releases"" rel=""nofollow noreferrer"">notes</a>.</p>
<p>I have tested it with the following steps:</p>
<pre><code>conda create -n vllm_test python=3.11 
conda activate vllm_test
pip install torch==2.2.1
pip install vllm
</code></pre>
","2024-04-27 09:37:54","1","Answer"
"78393551","","How does one use vllm with pytorch 2.2.2 and python 3.11?","<h1>Title: How does one use vllm with pytorch 2.2.2 and python 3.11?</h1>
<p>I'm trying to use the vllm library with pytorch 2.2.2 and python 3.11. Based on the GitHub issues, it seems vllm 0.4.1 supports python 3.11.</p>
<p>However, I'm running into issues with incompatible pytorch versions when installing vllm. The github issue mentions needing to build from source to use pytorch 2.2, but the pip installed version still uses an older pytorch.</p>
<p>I tried creating a fresh conda environment with python 3.11 and installing vllm:</p>
<pre class=""lang-bash prettyprint-override""><code>$ conda create -n vllm_test python=3.11
$ conda activate vllm_test
(vllm_test) $ pip install vllm
...
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
vllm 0.4.1 requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.
</code></pre>
<p>I also tried installing pytorch 2.2.2 first and then vllm:</p>
<pre class=""lang-bash prettyprint-override""><code>(vllm_test) $ pip install torch==2.2.2
(vllm_test) $ pip install vllm
...
Building wheels for collected packages: vllm
  Building wheel for vllm (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Building wheel for vllm (pyproject.toml) did not run successfully.
  │ exit code: 1
</code></pre>
<p>Can someone clarify what versions of vllm, pytorch and python work together currently? Is there a recommended clean setup to use vllm with the latest pytorch 2.2.2 and python 3.11?</p>
<p>I've tried creating fresh conda environments, but still run into version conflicts. Any guidance on the right installation steps would be much appreciated. Thanks!</p>
<p>ref: <a href=""https://github.com/vllm-project/vllm/issues/2747"" rel=""nofollow noreferrer"">https://github.com/vllm-project/vllm/issues/2747</a></p>
","2024-04-27 01:34:19","0","Question"
"78392429","","How can we capture update done with optimizer.step() in an elegant way?","<p>I want to implement a method to monitor in Tensorboard the update-to-data ratio during training with PyTorch, following an idea mentioned in Karpathy's video. I've come up with a solution, but I'm looking for a more elegant and configurable approach.</p>
<p>The current implementation directly modifies the training loop as follows:</p>
<pre><code>for step, batch in data_loader:
    x, y = batch
    optimizer.zero_grad()
    for name, param in model.named_parameters():
        if param.requires_grad and &quot;weight&quot; in name:
            param.data_before_step = param.data.clone()
    output = model(x)
    loss = loss_fn(output, y)
    loss.backward()
    optimizer.step()
    lr_scheduler.step()
    for name, param in model.named_parameters():
        if hasattr(param, &quot;data_before_step&quot;):
            update = param.data - param.data_before_step
            update_to_data = (update.std() / param.data_before_step.std()).log10().item()
            summary_writer.add_scalar(f&quot;Update:data ratio {name}&quot;, update_to_data, epoch * len(data_loader) + step)
            param.data_before_step = param.data.clone()
</code></pre>
<p>However, this approach adds code directly within the training loop, which can clutter the code and if we want to make it configurable, if-else statements are needed which clutter the code even more.</p>
<p>I've also explored using PyTorch hooks to achieve this. I've successfully implemented a hook to track gradients:</p>
<pre><code>class GradToDataRatioHook:
    def __init__(self, name, param, start_step, summary_writer):
        self.name = name
        self.param = param
        self.summary_writer = summary_writer
        self.grads = []
        self.grads_to_data = []
        self.param.update_step = start_step

    def __call__(self, grad):
        self.grads.append(grad.std().item())
        self.grads_to_data.append((grad.std() / (self.param.data.std() + 1e-5)).log10().item())
        self.summary_writer.add_scalar(f&quot;Grad {self.name}&quot;, self.grads[-1], self.param.update_step)
        self.summary_writer.add_scalar(f&quot;Grad:data ratio {self.name}&quot;, self.grads_to_data[-1], self.param.update_step)
        self.param.update_step += 1
</code></pre>
<p>However, implementing a similar hook to capture updates seems tricky. As far as I understand, <code>param.register_hook(...)</code> registers the hook, which is called when the gradient is calculated, i.e., before <code>optimizer.step()</code> is called. While the gradient and learning rate provide a direct value of an update for standard SGD, modern optimizers like Adam make the update process a bit more complicated. I'm seeking a solution that captures updates in an optimizer-agnostic way, preferably with PyTorch hooks. However, any suggestions or alternative approaches would be also greatly appreciated.</p>
","2024-04-26 18:32:22","1","Question"
"78391918","78328401","","<p>Downgrade <code>mkl package</code> to an earlier version. It should work.</p>
","2024-04-26 16:40:52","1","Answer"
"78391474","78385716","","<p>Thanks to @FlyingTeller, I was able to understand that <strong>the problem</strong> doesn't actually <strong>comes from</strong> Pytorch but <strong>Yolov5</strong>.</p>
<p>Reseting the backend didn't actually work ( instead of plotting, it printed &lt;Figure size widthxheight with 1 Axes&gt; )</p>
<p>So instead of resetting the backend to <code>module://matplotlib_inline.backend_inline</code> <strong>I put the following line in my notebook <code>%matplotlib inline</code></strong> after loading the yolov5 model and it solved the issue</p>
","2024-04-26 15:22:08","1","Answer"
"78391141","78102942","","<p>Your issue might be that you are using Python 3.11 or a higher version. The pygame 2.1.0 package is not supported on it. You can download Python 3.10, and that should resolve the problem</p>
","2024-04-26 14:26:49","2","Answer"
"78390559","","WinError 126: The specified module could not be found","<p>I am trying to use torch and ultralytics library using pip but I am getting this error</p>
<pre><code> Error loading &quot;C:\Users\Acer\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\lib\shm.dll&quot; or one of its dependencies.
</code></pre>
<p>Please, help me. I am over 6 yours trying to solve it.</p>
","2024-04-26 12:47:14","1","Question"
"78389728","","ModuleNotFoundError: No module named 'torch' when running Python file within Conda environment in VSCode","<p>I'm encountering an issue while trying to run a Python file in Visual Studio Code (VSCode) within a Conda environment. The error message states &quot;ModuleNotFoundError: No module named 'torch'&quot;. What's puzzling is that VSCode's interpreter recognizes the 'torch' module without any errors (the <code>import torch</code> statement is highlighted in green). However, when I try to run the code using the &quot;Run Code&quot; feature in VSCode, I encounter this error. Interestingly, running the Python file from the terminal (<code>python train.py</code>) or using the &quot;Run Python File&quot; option in VSCode works perfectly fine.</p>
<p><a href=""https://i.sstatic.net/FyJ4LCLV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FyJ4LCLV.png"" alt=""enter image description here"" /></a></p>
<p>Here are some additional details:</p>
<ul>
<li>The interpreter in VSCode is correctly set to ...\anaconda3\python.exe.</li>
<li>I installed the libraries using the following commands:
<code>conda install pytorch torchvision torchaudio cpuonly -c pytorch</code>
<code>pip install torch torchvision torchaudio</code></li>
<li>Both <code>pip list</code> and <code>conda list</code> show that the necessary libraries are installed within the Conda environment.</li>
<li>When I run <code>where python</code>, I get the following paths:
<ul>
<li><code>C:\Users\giova\anaconda3\python.exe</code></li>
<li><code>C:\Users\giova\AppData\Local\Microsoft\WindowsApps\python.exe</code></li>
</ul>
</li>
<li>Running <code>where python3</code> returns <code>C:\Users\giova\AppData\Local\Microsoft\WindowsApps\python3.exe</code>. I'm unsure whether I should also have 'python3' within the Anaconda environment, but I don't think so, because as I mentioned running from terminal <code>python train.py</code> perfectly works. Only clicking &quot;Run Code&quot; (play button) in VSCode gives <code>No module named 'torch'</code></li>
</ul>
<p>Could uninstalling and reinstalling VSCode be a solution?</p>
<p>I'm seeking insights into why this discrepancy occurs and potential solutions to resolve it. Any ideas would be greatly appreciated!</p>
","2024-04-26 10:13:00","0","Question"
"78388537","78385716","","<p>This is related to the matplotlib backend being changed by yolo, see <a href=""https://github.com/pytorch/pytorch/issues/82857"" rel=""nofollow noreferrer"">this github issue</a>. The solution should be to reset the matpltolib backend after loading:</p>
<pre><code>onnx_path = &quot;my_weights.onnx&quot;
yolo_path = &quot;lib/yolov5/&quot;


# Save backend
b = plt.get_backend()
    
torch.hub.load(yolo_path, 'custom', path=onnx_path, source='local') 

#Reset backend
matplotlib.use(b)

video_reader = VideoReader(str(src_file))

# wait for thread to read
while not video_reader.is_ready():
    waiting += 1
    time.sleep(1)

while(video_reader.is_ready()):
   frame = video_reader.frame

   #cv2.imshow('image',frame)
   #cv2.waitKey(0)


   plt.imshow(frame)
   plt.axis('off')
   plt.show()
</code></pre>
","2024-04-26 06:09:08","2","Answer"
"78388432","78384821","","<p>Don't store the noise you generated.</p>
<p>Store the noise that is the difference between clean subject and noisy subject. <em>Those</em> values you can safely subtract.</p>
<p>When you apply noise to the picture, and it's made of integers, you will get either integer overflow/underflow/wraparound (200 + 200 = 400 = 144, mod 256), or saturation (200 + 200 = 255, clipped). That is the source of the differences you see.</p>
<p>The &quot;effective&quot; noise you added (and calculated by subtracting) will look weird. Where the source image is bright, the noise's values cannot be very positive. In dark regions, the noise cannot be very negative.</p>
<p>You might want to work with numbers that aren't clipped/saturated. Floats are a better candidate for this.</p>
<p>Also consider gamma compression. Your network might learn that you added synthetic noise. It could learn to distinguish real-noisy pictures from fake-noisy pictures. The (assumed gaussian) noise in real images is gamma-compressed along with the &quot;signal&quot;. If you add (gaussian) noise to a gamma-compressed image, then in linear space, the noise appears no longer gaussian.</p>
<p>Remember that <em>lossy</em> image compression is <em>lossy</em>. Since you seem to care about exact pixel values, you should want to use lossless compressions only.</p>
","2024-04-26 05:40:06","2","Answer"
"78387261","78182135","","<p>Solved installing:</p>
<p>pip3 install torch==1.13.1 &amp;
pip3 install torchvision==0.13.1</p>
","2024-04-25 21:10:33","1","Answer"
"78387025","78382319","","<p>Found the solution! I'm writing step by step about my experience and what solved my issue in case anybody else is stuck on the same issue.</p>
<p>For MacOS, the command <code>pip install signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall</code> won't work. For MacOS, run the following commands instead(Thanks to the author <a href=""https://github.com/patrick-kidger/signatory/issues/43#issuecomment-1036471498"" rel=""nofollow noreferrer"">amandlek</a>):</p>
<pre><code>brew install llvm

export PATH=&quot;/opt/homebrew/opt/llvm/bin:$PATH&quot;

export LDFLAGS=&quot;-L/opt/homebrew/opt/llvm/lib&quot;

export CPPFLAGS=&quot;-I/opt/homebrew/opt/llvm/include&quot;

MACOSX_DEPLOYMENT_TARGET=13.4.1 CC=clang CXX=clang++ pip install signatory==1.2.6.1.9.0 --no-binary signatory
</code></pre>
<p>Note that, here <strong>13.4.1</strong> is my MacOS version. The above lines of codes should successfully install signatory without any error. If not, read further.</p>
<p>Now in my case, I got an error after running the first line of code: <code>zsh: command not found: brew</code> So I had to install <strong>brew</strong> first on my Mac following this <a href=""https://mac.install.guide/homebrew/zsh-command-not-found-brew"" rel=""nofollow noreferrer"">link</a>. In summary, I had to run the following 3 lines of codes sequentially to install <strong>brew</strong>:</p>
<pre><code>/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;
 
echo 'eval $(/opt/homebrew/bin/brew shellenv)' &gt;&gt; /Users/$USER/.zprofile
 
eval $(/opt/homebrew/bin/brew shellenv)
</code></pre>
<p>After running the above commands, MUST restart the device (which I didn't at first and I think that's why the error was still there) and then run the 5 lines of codes mentioned at the beginning.</p>
<p>So if the code using the <strong>brew</strong> doesn't work, try restarting your device and then again run only the <code>MACOSX_DEPLOYMENT_TARGET=13.4.1 CC=clang CXX=clang++ pip install signatory==1.2.6.1.9.0 --no-binary signatory</code> command, that should successfully install signatory!</p>
<p>Note: It's a good and safe practice to do all of these inside a virtual environment.</p>
","2024-04-25 20:05:20","0","Answer"
"78385716","","Pytorch and Matplotlib interfering","<p>I'm facing a weird bug with Matplotlib and torch in my jupyter notebook. If I run with this torch.hub.load line the plt.imshow will simply not display anything (even tho frame is a correct image). If I comment this line the plt.imshow works.</p>
<p>Whether this torch.hub.load line is commented or not cv2.imshow will work.</p>
<pre><code>onnx_path = &quot;my_weights.onnx&quot;
yolo_path = &quot;lib/yolov5/&quot;


torch.hub.load(yolo_path, 'custom', path=onnx_path, source='local') 

video_reader = VideoReader(str(src_file))

# wait for thread to read
while not video_reader.is_ready():
    waiting += 1
    time.sleep(1)

while(video_reader.is_ready()):
   frame = video_reader.frame

   #cv2.imshow('image',frame)
   #cv2.waitKey(0)


   plt.imshow(frame)
   plt.axis('off')
   plt.show()
</code></pre>
<p>It seems i'm missing something but I don't see it. Any help is appreciated :)</p>
","2024-04-25 15:32:52","1","Question"
"78384821","","How do I add reversible noise to the MNIST dataset using PyTorch?","<p>I would like to add reversible noise to the MNIST dataset for some experimentation.</p>
<p>Here's what I am trying atm:</p>
<pre><code>import torchvision.transforms as transforms
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
from PIL import Image
import torchvision

def display_img(pixels, label = None):
    plt.imshow(pixels, cmap=&quot;gray&quot;)
    if label:    
        plt.title(&quot;Label: %d&quot; % label)
    plt.axis(&quot;off&quot;)
    plt.show()

class NoisyMNIST(torchvision.datasets.MNIST):
    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):
        super(NoisyMNIST, self).__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)

    def __getitem__(self, index):
        img, target = self.data[index], self.targets[index]
        img = Image.fromarray(img.numpy(), mode=&quot;L&quot;)

        if self.transform is not None:
            img = self.transform(img)
        
        # add the noise
        noise_level = 0.3
        noise = self.generate_safe_random_tensor(img) * noise_level
        noisy_img = img + noise
        
        return noisy_img, noise, img, target

    def generate_safe_random_tensor(self, img):
        &quot;&quot;&quot;generates random noise for an image but limits the pixel values between -1 and 1&quot;&quot;&quot; 
       
        min_values = torch.clamp(-1 - img, max=0)
        max_values = torch.clamp(1 - img, min=0)
       
        return torch.rand(img.shape) * (max_values - min_values) + min_values



# Define transformations to apply to the data
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert images to tensors
    transforms.Normalize((0.1307,), (0.3081,)),
])

train_dataset = NoisyMNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = NoisyMNIST(root='./data', train=False, download=True, transform=transform)

np_noise = train_dataset[img_id][1]
np_data = train_dataset[img_id][0]



display_img(np_data_sub_noise, 4)

</code></pre>
<p>Ideally, this would give me the regular MNIST dataset along with a noisy MNIST images and a collection of the noise that was added. Given this, I had assumed I could subtract the noise from the noisy image and go back to the original image, but my image operations are not reversible.</p>
<p>Any pointers or code snippets would be greatly appreciated. Below are the images I currently get wit my code:</p>
<p>Original image:</p>
<p><a href=""https://i.sstatic.net/YsX1R.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YsX1R.png"" alt=""enter image description here"" /></a></p>
<p>With added noise:</p>
<p><a href=""https://i.sstatic.net/s77Ls.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/s77Ls.png"" alt=""enter image description here"" /></a></p>
<p>And with the noise subtracted for the image with noise:</p>
<p><a href=""https://i.sstatic.net/cMmJV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cMmJV.png"" alt=""enter image description here"" /></a></p>
","2024-04-25 12:55:51","1","Question"
"78383539","78375284","","<p>Go to the file torch_numpy\ufuncs.py, and change the loop containing name with the following loop:</p>
<pre><code>for name in _binary:
    ufunc = getattr(_binary_ufuncs_impl, name)
    ufunc_name = name  
    vars()[ufunc_name] = deco_binary_ufunc(ufunc)
</code></pre>
<p>And this one too :</p>
<pre><code>for name in _unary:
    ufunc = getattr(_unary_ufuncs_impl, name)
    #vars()[name] = deco_unary_ufunc(ufunc)
    ufunc_name = name  # Définir une variable avec le nom de l'ufunc
    vars()[ufunc_name] = deco_binary_ufunc(ufunc)
</code></pre>
","2024-04-25 09:16:18","0","Answer"
"78382319","","""ERROR: Could not build wheels for signatory, which is required to install pyproject.toml-based projects"" on Mac OS Ventura","<p>[<strong>Update:</strong>  I followed the solution mentioned in the GitHub issue <a href=""https://github.com/patrick-kidger/signatory/issues/43"" rel=""nofollow noreferrer"">#43</a> but it didn't work for me! Still causing the same error.]</p>
<p>I have PyTorch version 1.9.0 and I'm trying to install signatory version 1.2.6 on my Mac OS Ventura. I'm running the following command (reference: <a href=""https://I%20have%20PyTorch%20version%201.9.0%20and%20I%27m%20trying%20to%20install%20signatory%20version%201.2.6%20on%20my%20Mac%20OS%20Ventura.%20I%27m%20running%20the%20following%20command(%20reference:%20https://signatory.readthedocs.io/en/latest/pages/usage/installation.html#usage-install-from-source)%20on%20the%20terminal%20creating%20a%20virtual%20environment:%20pip%20install%20signatory==1.2.6.1.9.0%20--no-cache-dir%20--force-reinstall%20And%20that%20is%20giving%20me%20the%20following%20result:"" rel=""nofollow noreferrer"">signatory installation</a>) on the terminal creating a virtual environment:</p>
<pre><code>pip install signatory==1.2.6.1.9.0 --no-cache-dir --force-reinstall
</code></pre>
<p>And that is giving me the following result:</p>
<pre><code>    Downloading signatory-1.2.6.1.9.0.tar.gz (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 1.7 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Building wheels for collected packages: signatory
  Building wheel for signatory (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [111 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.9-x86_64-cpython-38
      creating build/lib.macosx-10.9-x86_64-cpython-38/signatory
      copying src/signatory/signature_inversion_module.py -&gt; buil
      ...
      ...
      ...
                raise RuntimeError(message) from e
      RuntimeError: Error compiling objects for extension
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for signatory
  Running setup.py clean for signatory
Failed to build signatory
ERROR: Could not build wheels for signatory, which is required to install pyproject.toml-based projects

</code></pre>
<p>I've searched on Google but found no reference to this <code>ERROR: Could not build wheels for signatory, which is required to install pyproject.toml-based projects</code>.
Any suggestions would be highly appreciated! Thanks.</p>
","2024-04-25 04:56:30","0","Question"
"78377178","78377056","","<p>A convolution with kernel size <code>3x1</code> is not a 1D conv, it's a 2D conv:</p>
<p><a href=""https://i.sstatic.net/HMTNv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HMTNv.png"" alt=""enter image description here"" /></a></p>
<pre><code>conv = nn.Conv2d(1,64,(3,1))
maxpool = nn.MaxPool2d((3,1))
</code></pre>
<p>Look at an inference with a single channel <code>180x11</code> input:</p>
<pre><code>&gt;&gt;&gt; maxpool(conv(torch.rand(1,1,180,11))).shape
torch.Size([1, 64, 59, 11])
</code></pre>
<p><em>This matches the shape of the <strong>&quot;Conv. layer 1&quot;</strong> output shown in the figure above.</em></p>
","2024-04-24 09:04:27","0","Answer"
"78377056","","How to get 3D tensor from 2D tensor using only conv1d layer in Pytorch?","<p>I am a newbie in ML. I am trying to implement a model from article &quot;Swimming Style Recognition and Lap Counting Using a Smartwatch and Deep Learning&quot; (<a href=""https://doi.org/10.1145/3341163.3347719"" rel=""nofollow noreferrer"">https://doi.org/10.1145/3341163.3347719</a>). Input of the model consist of 11 channels windowed data with size of 180. But after first conv layer and max pooling they have tensor with consist of 11 layers and window size equals 59, but there is also another dimension with 64 feature maps. But authors used only conv1d with kernel size 3x1.</p>
<p>I am failed to implement such kernel using nn.Conv1d. How can I do that?</p>
","2024-04-24 08:47:09","0","Question"
"78376791","78364395","","<p>Yet another PyTorch-only solution would be using <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html"" rel=""nofollow noreferrer""><code>expand()</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

c = 4
x = torch.tensor([[1,2,3], [4,5,6]]) 
y = x[:, :, None].expand(-1, -1, c)
print(y)
# &gt;&gt;&gt; tensor([[[1, 1, 1, 1],
#              [2, 2, 2, 2],
#              [3, 3, 3, 3]],
#
#             [[4, 4, 4, 4],
#              [5, 5, 5, 5],
#              [6, 6, 6, 6]]])
</code></pre>
<p>Note that (see <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html"" rel=""nofollow noreferrer"">documentation</a>):</p>
<blockquote>
<p>Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor</p>
</blockquote>
<p>This means that, although the values are now repeated <code>c</code> times, you still will <em>not</em> need <code>c</code> times as much memory (or rather, <code>c+1</code> times as much memory, as <code>x</code> still exists, as well): <code>y</code> will just be a <em>view</em> into <code>x</code>, i.e. <code>y</code> will be a &quot;reinterpretation&quot; of the memory of <code>x</code>. The latter could be …</p>
<ul>
<li>good for you if <code>y</code> is used in a read-only fashion, as it potentially saves a lot of memory (depending on the the size and number of repetitions),</li>
<li>bad for you if you also want to change the values in <code>y</code>.</li>
</ul>
","2024-04-24 07:51:48","0","Answer"
"78376725","78364395","","<p>You can use a chain of <a href=""https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html"" rel=""nofollow noreferrer"">repeat_interleave</a> and <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html"" rel=""nofollow noreferrer"">unfold</a> on the input tensor:</p>
<pre><code>tensor.repeat_interleave(c, dim=1).unfold(dimension=1, size=c, step=c)
</code></pre>
<p><strong>Example:</strong></p>
<pre><code>In [137]: tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])

In [138]: c = 4

In [139]: tensor.repeat_interleave(c, dim=1).unfold(dimension=1, size=c, step=c)
Out[139]: 
tensor([[[1, 1, 1, 1],
         [2, 2, 2, 2],
         [3, 3, 3, 3]],

        [[4, 4, 4, 4],
         [5, 5, 5, 5],
         [6, 6, 6, 6]]])

In [140]: tensor.repeat_interleave(c, dim=1).unfold(dimension=1, size=c, step=c).shape
Out[140]: torch.Size([2, 3, 4])
</code></pre>
","2024-04-24 07:39:34","0","Answer"
"78376537","","Positional encoding for VIsion transformer","<p>why the positional encoding is (1,patch,emb) size, it should be (batch_size,patch,emb) in general
even in the pytorch github code <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"" rel=""nofollow noreferrer"">https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py</a>  they are defining<br />
self.pos_embedding = nn.Parameter(torch.empty(<em>1</em>, seq_length, hidden_dim).normal_(std=0.02))  # from BERT</p>
<p>can anyone help me, what should I use as pos_encoding in my code</p>
<p>self.pos_embedding = nn.Parameter(torch.empty(<em>batch_size</em>, seq_length, hidden_dim).normal_(std=0.02))</p>
<p>is it correct?</p>
","2024-04-24 07:07:20","0","Question"
"78376438","","How to specify the specific node connection/data flow in the tensorflow or pytorch?","<p>The neurons in the tensorflow were often assumed to be fully connected layers, i.e.</p>
<pre><code>model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 1), activation='relu', input_shape=(10, 1, 1)), # layer 1
    tf.keras.layers.Dense(32, activation='relu'), # layer 2
    tf.keras.layers.Dense(32, activation='relu'), # layer 3
}
</code></pre>
<p>Is it possible to specify the connection between the layers? i.e. the 2nd neuron in the layer 2 is connected to only the 3rd neuron in layer 3, with directed graphs.</p>
<pre><code>[(2,3),(3,3)], [(2,2),(3,1)], 
</code></pre>
<p>or even the non trivial back flow</p>
<pre><code>[(3,3),(2,3)], [(2,3),(3,5)],  
</code></pre>
<p>where the connection such as</p>
<pre><code>[(1,2),(2,3)], 
</code></pre>
<p>did not exist, i.e. the weight was always zero and can not be trained.</p>
<p>How to do it in tensorflow or pytorch?</p>
","2024-04-24 06:49:49","1","Question"
"78375819","78356026","","<pre><code>left  = torch.tensor([[0.9, 0.8],[0.3, 0.0],[0.6, 0.9],[0.7, 0.0],[0.6, 0.8],[0.6, 0.2],[0.6, 0.2]])
mask  = torch.tensor([ True,  True, False,  True, False, False,  True])
right = torch.tensor([[ 1.,  3.],[ 1.,  5.],[ 7.,  0.],[11., 13.],[17., 19.],[21., 1. ],[ 1., 13.]])
    
c1 = torch.tensor([1,3,5])
c2 = torch.tensor([11, 13])
    
### Function start here
right_in_c1 = right.view(*right.shape, *([1]*len(c1.shape))) == c1.view(*([1]*len(right.shape)), *c1.shape)
right_in_c1 = right_in_c1.view(right.shape[0], -1, torch.prod(torch.tensor(c1.shape))).any(dim= -1).all(dim= 1)
    
right_in_c2 = right.view(*right.shape, *([1]*len(c2.shape))) == c2.view(*([1]*len(right.shape)), *c2.shape)
right_in_c2 = right_in_c2.view(right.shape[0], -1, torch.prod(torch.tensor(c2.shape))).any(dim= -1).all(dim= 1)
    
final_mask = torch.logical_or(right_in_c1, right_in_c2)
    
left[torch.logical_and(mask, final_mask)] = torch.tensor([0, 1]).float()
left[torch.logical_and(mask, ~final_mask)] = torch.tensor([1, 0]).float()
### Function end here
    
left
</code></pre>
<p>The output should be:</p>
<pre><code>tensor([[0.0000, 1.0000],
        [0.0000, 1.0000],
        [0.6000, 0.9000],
        [0.0000, 1.0000],
        [0.6000, 0.8000],
        [0.6000, 0.2000],
        [1.0000, 0.0000]])
</code></pre>
","2024-04-24 03:18:23","1","Answer"
"78375284","","torch error : NameError name 'name' is not defined","<p>I'm currently attempting to create an executable using PyInstaller, but I've encountered an error : NameError: name 'name' is not defined caused by the line of code below.</p>
<pre><code>model = lp.Detectron2LayoutModel('lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config',extra_config=[&quot;MODEL.ROI_HEADS.SCORE_THRESH_TEST&quot;, 0.8],label_map={0: &quot;Text&quot;, 1: &quot;Title&quot;, 2: &quot;List&quot;, 3: &quot;Table&quot;, 4: &quot;Figure&quot;}) 
</code></pre>
<p><a href=""https://i.sstatic.net/ks08l.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ks08l.png"" alt=""enter image description here"" /></a></p>
<p>Can you provide guidance on resolving this issue</p>
","2024-04-23 22:51:17","2","Question"
"78373015","78372815","","<p>The code adds up the losses from 100 minibatches: <code>running_loss += loss</code>. Every 100 minibatches (<code>(i + 1) % 100 == 0</code>), you need to divide <code>running_loss</code> by 100 in order to get the average value. Then, the code resets <code>running_loss</code> (<code>running_loss=0</code>), before starting to add up the losses again for the next 100 minibatches.</p>
<p>There is a mistake in the code; it should divide by &quot;100&quot;, not &quot;10, 000&quot;, because you are accumulating 100 values each time.</p>
","2024-04-23 14:11:03","1","Answer"
"78372815","","Print statistics in train","<p>To train cifar100 dataset, I find this function train, while I'm newer in Pytorch, I would like to understand the value 10000, because when I change it, the loss change</p>
<pre><code>def train(net,trainloader,epochs,use_gpu = True):
    net.train()
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    print(f&quot;Training {epochs} epoch(s) w/ {len(trainloader)} batches each&quot;)

    # Train the network
    for epoch in range(epochs):  # loop over the dataset multiple times
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            images, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()
            outputs = net(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print statistics
            running_loss += loss.item()
            if i % 100 == 99:  # print every 100 mini-batches
                print(&quot;[%d, %5d] loss: %.3f&quot; % (epoch + 1, i + 1, running_loss / 10000))
                running_loss = 0.0
</code></pre>
","2024-04-23 13:37:59","0","Question"
"78371698","78329328","","<p>You will get a model in .safetensors format if you save the model using the following code:
model.save_pretrained('folder/').</p>
<p>And you will get a .bin format model if you save the model using the following code:
torch.save(model.state_dict(),'folder/pytorch_model.bin'.format(epoch)).</p>
<p>Alternatively, you can use
model.save_pretrained(output_dir, safe_serialization=False).</p>
<p>When you downgrade the transformers library, it will automatically save the model in a .bin file.</p>
","2024-04-23 10:42:16","0","Answer"
"78371218","78371164","","<p>As usual, asking the question is sufficient to find the solution.</p>
<p>Upgrading to pip-24.0 with</p>
<pre><code> python.exe -m pip install --upgrade pip
</code></pre>
<p>is sufficient, now the cache is recognised.</p>
","2024-04-23 09:21:26","0","Answer"
"78371164","","Keep pytorch with CUDA in pip cache","<p>I frequently need to install torch with CUDA in different virtual environments. Conda is not a good option for various reasons, so I need to install via pip. I do this following the instructions from <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">https://pytorch.org/</a></p>
<pre><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre>
<p>This leads to a 2.5GB download which is fine once, but gets exhausting quickly. To my understanding, pip should by default cache downloads (<a href=""https://pip.pypa.io/en/stable/topics/caching/"" rel=""nofollow noreferrer"">https://pip.pypa.io/en/stable/topics/caching/</a>).</p>
<p>However, using the command above, torch does not land in the cache; if I reinstall in another venv, it redownloads torch, and if I check with pip cache list, torch is not there.</p>
<p>I tried creating environment variables like PIP_DOWNLOAD_CACHE or PIP_FIND_LINKS, as suggested in some other topics, but those don't seem to help either.</p>
<p>Also, I figured that the --index-url parameter might prohibit using the cache. However, if I pip install torch without --index-url, it just downloads a different version.</p>
<p>Python version is 3.10, pip 23.0.1. Any advice?</p>
","2024-04-23 09:14:48","2","Question"
"78370920","78359307","","<p>The cause of the error was the wrong setup of the <code>pred_to_mAP</code>. Instead of</p>
<pre><code>pred_to_mAP = [
        dict(
            boxes=torch.tensor(box, dtype=torch.float32),
            scores=score,
            labels=label
        )   for box, label, score in zip(pred_boxes_upscaled, 
                                         pred_labels.clone().detach(), 
                                         pred_scores.clone().detach())
        ]
</code></pre>
<p>it should be</p>
<pre><code>pred_to_mAP = [
        dict(
            boxes=torch.stack([torch.tensor(box) for box in pred_boxes_upscaled]
                             ).astype(torch.float32).to(device),
            scores=pred_scores.to(device),
            labels=pred_labels.to(device)
        ) 
        ]
</code></pre>
<ul>
<li>What led to this mistake is my misunderstanding of <a href=""https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html"" rel=""nofollow noreferrer"">the provided example in the documentation</a>, it exhibits only one bounding box in the <code>preds</code> and one bounding box in the <code>target</code> and I was confused how to apply the metric &quot;MeanAveragePrecision&quot; on multiple bboxes per image at once.
I have seen an example of multi-boxes <a href=""https://github.com/Lightning-AI/torchmetrics/issues/794"" rel=""nofollow noreferrer"">here</a>.</li>
</ul>
","2024-04-23 08:34:12","0","Answer"
"78370028","78369381","","<p>This should be equivalent to your function without using a for loop</p>
<pre><code>def cat_aggregate(x, index):
    index_count = torch.bincount(index)
    fill_count = index_count.max() - index_count
    # fill_zeros = torch.zeros_like(x[0]).repeat(fill_count.sum(),1) ## &lt;- Only support 2D tensor
    fill_zeros = torch.zeros_like(x[0]).repeat(fill_count.sum(),*([1]*(len(x.shape)-1))) ## &lt;- change this to make the function takes in arbitrary shape
    fill_index = torch.range(0, fill_count.shape[0]-1).repeat_interleave(fill_count)
    index_ = torch.cat([index, fill_index], dim = 0)
    x_ = torch.cat([x, fill_zeros], dim = 0)
    # x_ = x_[torch.argsort(index_)].view(index_count.shape[0], index_count.max(), -1) ## &lt;- Only support 2D tensor
    x_ = x_[torch.argsort(index_)].view(index_count.shape[0], index_count.max(), *x.shape[1:]) ## &lt;- change this to make the function takes in arbitrary shape
    return x_
</code></pre>
<p>Output:</p>
<pre><code>tensor([[[  0,   0],
         [  0,   0],
         [  0,   0],
         [  0,   0]],

        [[  7,  70],
         [  8,  80],
         [  9,  90],
         [  0,   0]],

        [[ 10, 100],
         [  0,   0],
         [  0,   0],
         [  0,   0]],

        [[  5,  50],
         [  6,  60],
         [ 11, 110],
         [ 12, 120]]])
</code></pre>
","2024-04-23 05:23:59","0","Answer"
"78369381","","Group PyTorch feature tensors according to labels by concatenation","<p>I'm working on a batchable, loop and recursion free, PyTorch utility <code>concat_aggregate</code> for grouping rows of an input tensor <code>x</code> according to labels given by an <code> index</code> tensor. It should pad rows so that the resulting tensor is rectangular. For example,</p>
<pre><code>x = torch.tensor([[5, 50], [6, 60], [7, 70], [8, 80], [9, 90], [10, 100], [11, 110], [12, 120]])
index = torch.tensor([3, 3, 1, 1, 1, 2, 3, 3])
concat_aggregate(x, index)
</code></pre>
<p>should output:</p>
<pre><code>torch.tensor([
    [[0, 0], [0, 0], [0, 0], [0, 0]],
    [[7, 70], [8, 80], [9, 90], [0, 0]],
    [[10, 100], [0, 0], [0, 0], [0, 0]],
    [[5, 50], [6, 60], [11, 110], [12, 120]]
])
</code></pre>
<p>I hacked my way to this function:</p>
<pre><code>def cat_aggregate(x: torch.Tensor, index: torch.Tensor) -&gt; torch.Tensor:
    # Number of groups and the number of features in each row of x
    num_groups = index.max().item() + 1
    num_features = x.size(1)
    # Compute the maximum number of elements in any group
    group_sizes = torch.zeros(num_groups, dtype=torch.long, device=x.device)
    group_sizes.index_add_(0, index, torch.ones_like(index, dtype=torch.long))
    # Prepare the output tensor, padded with zeros
    max_num_elements = group_sizes.max()
    result = torch.zeros(num_groups, max_num_elements, num_features, dtype=x.dtype, device=x.device)
    # Positions to fill in the result tensor
    positions = group_sizes.clone().fill_(0)  # Current fill position in each group
    # Fill the tensor
    for i in range(x.size(0)):
        group_id = index[i]
        result[group_id, positions[group_id]] = x[i]
        positions[group_id] += 1
    return result
</code></pre>
<p>which returns the correct results for 1 and 2D tensors. But, it requires iterating over <code>x.size(0)</code>, making it at least linear in the length of <code>x</code>. I'm not sure if what I have is idiomatic. Does anyone here see any possible efficiency/complexity improvements or an obvious way to extend it to 2D tensors? I'm surprised such a function is missing from the PyTorch API.</p>
","2024-04-23 00:30:38","0","Question"
"78366952","78299639","","<p>Using <code>.transfer_batch_to_device</code> solved it:</p>
<pre class=""lang-py prettyprint-override""><code>class PlotCallback(Callback):
    def on_train_epoch_end(self, trainer: L.Trainer, model: Model) -&gt; None:
        loader = model.predict_dataloader()
        for batch in loader:
            batch = model.transfer_batch_to_device(batch, model.device, 0)
            model.predict_step(batch)
        
        ... # save figure to wandb
</code></pre>
","2024-04-22 14:26:04","1","Answer"
"78366939","78366460","","<blockquote>
<p>[...] it doesnt give good results at all</p>
</blockquote>
<p>Is that with the validation data, or the training data?</p>
<p>If the train loss doesn't go down, it might point to an issue in the learning pipeline. I haven't noticed any issues with the code you posted, so the problem might be elsewhere. I would first confirm that the model is able to learn by reducing the training set size to just a few samples, and ensuring that your train loss decreases (and that the train <em>score</em> is very high). This would demonstrate that the model is able to learn something.</p>
<p>After confirming the model can learn, if you give the model more samples its train loss will be a bit higher and the train score will go down a bit, but you should start to see that the validation score improves.</p>
<p>Some quick checks:</p>
<ul>
<li>Start with <code>Adam</code>; it's a good default optimiser</li>
<li>The loss should be suitable for regression, such as MSE or similar</li>
<li>The input data should be scaled</li>
<li>Might be worth having no dropout initially. You can add some dropout back in later depending on the extent to which the model overfits.</li>
</ul>
<p>Your input image dimensions are around 680 x 680? Try increasing the kernel size of the input convolutional layer in order to capture more spatial information (I think 5x5 and 7x7 are typical at the input).</p>
<blockquote>
<p>i have applied a log scale to the target prediction to reduce this effect to attempt to improve learning but not sure how much it helped.</p>
</blockquote>
<p>To help discern whether things have improved over the previous run, set the <code>np</code> and <code>torch</code> random seeds and print out the metrics of at least the final epoch. That gives you a reference value to compare between two runs. When running such tests, I wouldn't go for too many epochs, otherwise you can't iterate as fast towards a good solution.</p>
","2024-04-22 14:24:26","1","Answer"
"78366460","","PyTorch model for regression problem with 4 images per sample with time gap between them","<p>I am using a dataset where each sample corresponds to 4 images taken at a known delay from each other and each set of 4 images has a target prediction that is a number (not classification). I currently have made the model below but it doesnt give good results at all. any advice ?</p>
<pre><code>class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(8)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout1 = nn.Dropout(p=0.25)
        
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(16)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout2 = nn.Dropout(p=0.25)
        
        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(32)
        self.pool3 = nn.MaxPool2d(kernel_size=5, stride=2)
        self.dropout3 = nn.Dropout(p=0.25)
        
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28800, 512)
        self.dropout4 = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(512, 1)  # Single output

    def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.dropout1(x)
        
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        x = self.dropout2(x)
        
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        x = self.dropout3(x)
        
        x = self.flatten(x)
        x = torch.relu(self.fc1(x))
        x = self.dropout4(x)
        
        x = self.fc2(x)  # Output layer, no activation function for regression
        return x
</code></pre>
<p>Also, the target prediction value is often very small and sometimes much larger such from around 1e-9 to 1e2. i have applied a log scale to the target prediction to reduce this effect to attempt to improve learning but not sure how much it helped.</p>
","2024-04-22 13:04:42","-1","Question"
"78366312","78364395","","<p>I guess the most fastforward way to do this is using the <code>einops</code> module. It provides a lot of tensor manipulations by making the transformations clearly visible. Here:</p>
<pre class=""lang-py prettyprint-override""><code>from einops import repeat
X = torch.tensor([[1, 2, 3], [4, 5, 6]])
Y = repeat(X, &quot;a b -&gt; a b c&quot;, c=4)
</code></pre>
<p>Output:</p>
<pre class=""lang-bash prettyprint-override""><code>tensor([[[1,1,1,1],
         [2,2,2,2],
         [3,3,3,3]],

         [4,4,4,4],
         [5,5,5,5],
         [6,6,6,6]]])
</code></pre>
","2024-04-22 12:39:02","2","Answer"
"78365932","77906691","","<p>I guess the reason is that you use <code>handle.wait()</code> before <code>print(f&quot;Process {rank}: async check&quot;)</code>. <code>handle.wait()</code> will block the process until the allreduce finish, which synchronizes rank0 and rank1. I think only when you put <code>handle.wait()</code> after <code>print(f&quot;Process {rank}: async check&quot;)</code>, you can expect ‘Process 0: async check’ should be printed before ‘Process 1: begin aysnc all-reduce’.</p>
","2024-04-22 11:31:58","0","Answer"
"78364395","","How can I add a dimension to Torch tensor with values of first two dimensions repeated in the new dimension","<p>I have a Tensor of shape [a,b]. How can I reshape it to [a,b,c] such that all the values along the new dimensions repeat values of the i,j entry of the original tensor of shape [a,b]?</p>
<p>ex: a = 2, b= 3</p>
<pre><code>tensor([[1,2,3],  
       [4,5,6]]) 
</code></pre>
<p><em><strong>Now to reshape with c = 4 to get shape [2,3,4]</strong></em></p>
<p>result:</p>
<pre><code>tensor([[[1,1,1,1],
         [2,2,2,2],
         [3,3,3,3]],

         [4,4,4,4],
         [5,5,5,5],
         [6,6,6,6]]])
</code></pre>
","2024-04-22 06:45:17","0","Question"
"78362941","78359357","","<p>I don't think there is an option to change that. This behavior occurs because both the training and validation loops call <code>log_metrics</code> at different points in time, (even though epoch and step may still overlap). This leads to distinct entries in the CSV file. You can see in the <a href=""https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/fabric/loggers/csv_logs.py#L227"" rel=""nofollow noreferrer"">source file</a>, the metrics being appended to the <em>list</em>, and <a href=""https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/fabric/loggers/csv_logs.py#L246"" rel=""nofollow noreferrer"">then written</a> to the file with the <a href=""https://docs.python.org/3/library/csv.html#csv.DictWriter"" rel=""nofollow noreferrer""><code>csv.DictWriter</code></a>.</p>
","2024-04-21 19:41:13","0","Answer"
"78362903","78361756","","<p>As long as you use differentiable built-in operators, Autograd will track your operations, and a backward call will result in gradient computation.</p>
<p>In your case, you have the following two operations:</p>
<pre><code>C = g(B)
Z = A*C
</code></pre>
<p>So we can summarize the computation graph with:</p>
<pre><code>dL/dB &lt;------\    
  B   -----\  \ 
            \ dC/dB 
             \  \ &lt;--- dL/dC ----\
              -&gt; g(B) = C  ----\  \
                                \ dZ/dC
                                 \  \ &lt;--- dL/dZ ---
                                  -&gt; A * C = Z
                                 /  /
                                / dZ/dA
  A   -------------------------/  /
dL/dA &lt;--------------------------/
</code></pre>
<p>Your input gradients <code>A.grad</code> and <code>B.grad</code> correspond to <code>dL/dB</code> and <code>dL/dA</code>, respectively. While, your model gradient will be given by <code>dL/dC</code> which is actually <code>dL/dg</code> (by <code>g</code>, we refer to the parameters for the <code>&quot;g&quot;</code> model). This quantity will be computed with the chain rule as <code>dL/dC = dL/dz * dZ/dC</code>.</p>
<p>You can read more in another question: <a href=""https://stackoverflow.com/questions/69367939/understanding-backpropagation-in-pytorch/69369471#69369471""><em>Understanding backpropagation in PyTorch</em></a>.</p>
","2024-04-21 19:27:59","1","Answer"
"78361906","78353279","","<p>When you change from torch.inference_mode() to torch.no_grad(), you need to Delete the runtime and restart the session, I've got the same error here and I fixed it.</p>
","2024-04-21 14:23:38","0","Answer"
"78361756","","Whether there is any need to modify the backward function in pytorch?","<p>Recently I have been working on self-defined models with self-defined backward function (since the forward process is not implemented via pytorch AD). Say if I have a model of which the forward function outputs two tensors <code>A</code> and <code>B</code>. The backward function defines the gradients (<code>dA/dP</code> and <code>dB/dP</code>) of parameter <code>P</code> and the gradients of the whole loss will be composed of <code>dA/dP</code> and <code>dB/dP</code>.</p>
<p>Now I want to define a new differentiable function, which is <code>f(A, B) = A * g(B)</code> (multiply, not convolution), therefore the gradient of <code>f(A, B)</code> will be <code>(dA/dP) * g(B) + A * g'(B) * (dB/dP)</code>.</p>
<p>So I figure, since <code>dA/dP</code> and <code>dB/dP</code> are already defined, <code>f(A, B)</code> should be directly differentiable, and there is no need to modify the backward function. My question is that since I am not sure about the above, I wonder whether this is correct and whether I need to modify the backward function (and the forward function, even).</p>
","2024-04-21 13:37:47","0","Question"
"78361625","78361473","","<p>Usually you do the matching by checking if the IoU between the predicted box and GT box is above a given threshold. There is also some Non Max Suppression to deal with multiples predicted boxes corresponding to one true box.</p>
","2024-04-21 12:55:53","0","Answer"
"78361473","","How to match predicted bounding boxes to ground-truth bounding boxes","<p>I have finetuned &quot;<strong>fasterrcnn_resnet50_fpn</strong>&quot; model from PyTorch for an object detection task, the model has some <em><code>False Negatives</code></em> and therefore in some images in the validation dataset <em>the number of the predicted bboxes</em> is less than <em>the number of the ground-truth bboxes</em>. In this case, how to match each <em>predicted bbox</em> to <em>its corresponding ground-truth bbox</em>?</p>
","2024-04-21 12:02:15","0","Question"
"78360929","78354455","","<p>Very interesting, took me a while to figure out a solution! <br>A possible improvement would be to vectorize the cumulative product.</p>
<p>Start by concatenating a <code>1</code> at the front and expand row-wise, this will be the output tensor:</p>
<pre><code>&gt;&gt;&gt; m = torch.cat([torch.ones(1),array])[None].repeat(N,1)
tensor([[1.0000, 0.1000, 0.2000, 0.0500, 0.3000, 0.2000],
        [1.0000, 0.1000, 0.2000, 0.0500, 0.3000, 0.2000],
        [1.0000, 0.1000, 0.2000, 0.0500, 0.3000, 0.2000],
        [1.0000, 0.1000, 0.2000, 0.0500, 0.3000, 0.2000],
        [1.0000, 0.1000, 0.2000, 0.0500, 0.3000, 0.2000],
        [1.0000, 0.1000, 0.2000, 0.0500, 0.3000, 0.2000]])
</code></pre>
<p>To apply <a href=""https://pytorch.org/docs/stable/generated/torch.cumprod.html"" rel=""nofollow noreferrer""><code>torch.cumprod</code></a> along <code>dim=1</code>, we need to set the lower triangle to <code>1</code>:</p>
<pre><code>&gt;&gt;&gt; m = m.triu(1) + torch.ones_like(m).tril(0)
tensor([[1.0000, 0.1000, 0.2000, 0.0500, 0.3000, 0.2000],
        [1.0000, 1.0000, 0.2000, 0.0500, 0.3000, 0.2000],
        [1.0000, 1.0000, 1.0000, 0.0500, 0.3000, 0.2000],
        [1.0000, 1.0000, 1.0000, 1.0000, 0.3000, 0.2000],
        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000],
        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])
</code></pre>
<p>To get the desired results copy the upper triangle, transpose it, and add to  <code>m</code>.</p>
<p>All in all, it comes down to:</p>
<pre><code>def gen_matrix(array):
    N = len(array) + 1
    m = torch.cat([torch.ones(1),array])[None].repeat(N,1)
    m = m.triu(1) + torch.ones_like(m).tril(0)
    m = m.cumprod(1)
    return m.triu() + m.triu(1).T
</code></pre>
","2024-04-21 08:46:48","0","Answer"
"78359357","","PyTorch Lightning CSVLogger: Why are training and validation losses on different lines?","<p>I have a question about the PyTorch Lightning framework's CSVLogger that has been bugging me for a couple of weeks already.</p>
<p>When I try to log the training and validation losses in their respective training_step and validation_step functions, it seems that the resulting CSV file logs both metrics on separate lines in the metrics.csv file. The file looks like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Epoch</th>
<th>train_loss</th>
<th>val_loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.01</td>
<td>null</td>
</tr>
<tr>
<td>0</td>
<td>null</td>
<td>0.02</td>
</tr>
<tr>
<td>1</td>
<td>0.005</td>
<td>null</td>
</tr>
<tr>
<td>1</td>
<td>null</td>
<td>0.01</td>
</tr>
<tr>
<td>2</td>
<td>0.01</td>
<td>null</td>
</tr>
<tr>
<td>2</td>
<td>null</td>
<td>0.02</td>
</tr>
</tbody>
</table></div>
<p>It also shows a step number that I've omitted here, though it's the same for each unique epoch.</p>
<p>Is there any way to place these in a single line in the CSV, using the built-in CSVLogger? I couldn't find anything about this online nor in the documentation.</p>
<hr/>
<p>The following code produces the problem described above:</p>
<pre class=""lang-py prettyprint-override""><code>
import torch
from torch.nn import functional as F
from torch.utils.data import TensorDataset
import lightning as pl
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
features, target = iris.data, iris.target

train_features, val_features, train_target, val_target = train_test_split(features, target, test_size=0.2)

train_features = torch.tensor(train_features).float()
val_features = torch.tensor(val_features).float()
train_target = torch.tensor(train_target).long()
val_target = torch.tensor(val_target).long()

dm = pl.LightningDataModule.from_datasets(
    train_dataset=TensorDataset(train_features, train_target),
    val_dataset=TensorDataset(val_features, val_target),
    batch_size=5,
)

class Model(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(4, 3)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.layer(x)
        loss = F.cross_entropy(y_hat, y)
        self.log(&quot;train_loss&quot;, loss, prog_bar=True, on_step=False, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.layer(x)
        loss = F.cross_entropy(y_hat, y)
        self.log(&quot;val_loss&quot;, loss, prog_bar=True, on_step=False, on_epoch=True)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.02)

    def forward(self, x):
        return self.layer(x)

model = Model()
trainer = pl.Trainer(max_epochs=10)
trainer.fit(model, dm)
</code></pre>
<p>PyTorch Lightning version: 2.2.2</p>
","2024-04-20 18:46:57","2","Question"
"78359340","78358637","","<p>In your proposed version, two problems would indicate your <code>biases</code> aren't getting any gradient. First, you are using <code>torch.no_grad</code> which means no gradient computation is permitted, so in this scope, all <code>requires_grad</code> flags are set to <code>False</code> by design. Even removing that, you are deliberately bypassing gradient computation because the parameters are overwritten through their <code>data</code> attribute which means the gradients are not tracked!</p>
<p><strong>Let alone the fact you are accumulating the biases, <em>ie.</em> after 10 iterations, you would have <code>params += 10*biases</code> which seems incorrect. In other words, in your implementation, there is no &quot;resetting&quot; of the parameters...</strong></p>
<p>Now, of course, you probably did that to overcome the scary <em>&quot;Leaf variable was used in an in-place operation&quot;</em> error. This happens when you try to operate on a parameter in place, so this would fail:</p>
<pre><code>param += self.biases[task_id]
</code></pre>
<p>Having it out of place makes no sense since <code>param</code> is just a scope variable, the underlying parameter remains unchanged.</p>
<p>After thinking about it, it seems the only way to make the gradient track back to the added bias, is to apply that layer manually via the <a href=""https://pytorch.org/docs/stable/nn.functional.html"" rel=""nofollow noreferrer"">functional</a> approach. Instead of modifying the parameters on every iteration, you do it just before applying it (which fixes the issue highlighted above in bold).</p>
<p>Here is a possible implementation which ensures gradient computation on the biases:</p>
<pre><code>class MetaModelWithBias(nn.Module):
    def __init__(self, meta_model, num_tasks):
        super(MetaModelWithBias, self).__init__()
        self.meta_model = meta_model
        self.biases = nn.ParameterList([
              nn.Parameter(torch.randn(1)) for _ in range(num_tasks)])

    def forward(self, x, task_id):
        output = self.meta_model(x, self.biases[task_id])
        return output

class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc = nn.Linear(10, 1)

    def forward(self, x, bias):
        return F.linear(x, self.fc.weight+bias, self.fc.bias+bias)
</code></pre>
","2024-04-20 18:41:03","0","Answer"
"78359307","","How to use MeanAveragePrecision metric from torchmitrics.detection on an object detection model","<p>I have finetuned &quot;<strong>fasterrcnn_resnet50_fpn</strong>&quot; model from PyTorch for an <em>object detection task</em>, then I wanted to calculate <code>mAP</code> metric for the trained model on a validation dataset. I used <code>MeanAveragePrecision</code> from <code>torchmetrics.detection</code>.</p>
<p>The function that uses the trained model for inference looks as follows:</p>
<pre><code>@torch.no_grad
def generate_bboxes_on_one_img(image, model, device):
    model.to(device)
    model.eval()
    x = [image.to(device)]
    pred_boxes, pred_labels, pred_scores = model(x)[0].values()
    return pred_boxes, pred_labels, pred_scores
</code></pre>
<p>To be able to use the previous function on a dataLoader, I set up the dataset instance and the dataLoader as follows:</p>
<pre><code>def collate_fn(batch):
    return list(zip(*batch))

val_dataset = VisDroneDataset(val_images_path, val_annotations_df, transforms=val_transform)
val_data_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)
</code></pre>
<p>Then I wrote the following code to calculate mAP between ground-truths and predictions on each image</p>
<pre><code>mAP = MeanAveragePrecision(iou_type=&quot;bbox&quot;)
mAP.to(device)

for image, target in val_data_loader:
    original_boxes, original_labels, image_idx, _, _ = target[0].values()
    model.eval()
    x = [img.to(device) for img in image]
    preds_boxes, preds_labels, preds_scores = model(x)[0].values()

    image_PIL = val_dataset.get_image(image_idx)
    upscaled_image, pred_boxes_upscaled, labels = get_inverse_transform(image[0], 
                                                                    pred_boxes, 
                                                                    pred_labels, 
                                                                    *image_PIL.size)
    pred_to_mAP = [
            dict(
                boxes=torch.tensor(box, dtype=torch.float32),
                scores=score,
                labels=label
            )   for box, label, score in zip(pred_boxes_upscaled, pred_labels.clone().detach(), pred_scores.clone().detach())
            ]
            
    gt_to_mAP = [
            dict(
                boxes=original_boxes,
                labels=original_labels
            )   for box, label in zip(test_image_gt_bboxes, test_image_gt_labels)
            ]
    mAP.update(pred_to_mAP, gt_to_mAP)
    pprint(mAP.compute())

    break
</code></pre>
<p>I got the following error: <code>ValueError: Expected argument preds and target to have the same length, but got 100 and 127</code>.</p>
<p>I don't understand why <em>preds</em> and <em>target</em> should have the same length.</p>
<p>I read the documentation and it was not that helpful. Please help me with understanding how <code>MeanAveragePrecision</code> works!</p>
","2024-04-20 18:29:37","0","Question"
"78358637","","How to manually add bias to model parameters and make this bias trainable for gradient backpropagation and updates","<p>I have some tasks from different domains, and there's a base model called <code>MetaModel</code> that can perform regression. I want to add a <strong>task-specific bias</strong> to the parameters of the base model <code>MetaModel</code> when training different tasks. Currently, I'm using the method shown in the code below, but during gradient backpropagation, the gradients of <code>MetaModelWithBias.biases</code> are always <code>None</code>. I'd like to know how to update the parameters of <code>MetaModelWithBias.biases</code> to make them learnable.</p>
<p>The key codes for the task-specific bias are below:</p>
<pre><code>for param in self.meta_model.parameters():
     param.data += self.biases[task_id]
</code></pre>
<p>The complete codes:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim

class MetaModelWithBias(nn.Module):
    def __init__(self, meta_model, num_tasks):
        super(MetaModelWithBias, self).__init__()
        self.meta_model = meta_model
        self.biases = nn.ParameterList([nn.Parameter(torch.randn(1)) for _ in range(num_tasks)])

    def forward(self, x, task_id):
        with torch.no_grad():
            for param in self.meta_model.parameters():
                param.data += self.biases[task_id]
        output = self.meta_model(x)
        return output

class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc = nn.Linear(10, 1)

    def forward(self, x):
        return self.fc(x)



# random some data
num_tasks = 5
input_size = 10
num_samples = 100
num_tasks = 5
X = torch.randn(num_samples, input_size)
task_ids = torch.randint(0, num_tasks, (num_samples,))

# create model
meta_model = MetaModel()
meta_model_with_bias = MetaModelWithBias(meta_model, num_tasks)

# training
num_epochs=10
optimizer = optim.SGD([
    {'params': meta_model_with_bias.meta_model.parameters()},
    {'params': meta_model_with_bias.biases.parameters() }
], lr=0.01)
for epoch in range(num_epochs):
    outputs =0
    for i in range(num_samples):
        task_id = task_ids[i]
        output = meta_model_with_bias(X[i].unsqueeze(0), task_id)
        outputs+=output

    criterion = nn.MSELoss()
    targets = torch.randn(1, 1)
    loss = criterion(outputs, targets)

    print('\tMetaModelWithBias biases before backward',[parms for parms in meta_model_with_bias.biases],[parms.grad for parms in meta_model_with_bias.biases])
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print('\tMetaModelWithBias biases after backward',[parms for parms in meta_model_with_bias.biases],[parms.grad for parms in meta_model_with_bias.biases])

    print('meta_model_with_bias params')
    for name, parms in meta_model_with_bias.named_parameters():
        print('--&gt;name:', name, '--&gt;grad_requirs:', parms.requires_grad,
              ' --&gt;parameter:', parms,
              ' --&gt;grad_value:', parms.grad)

</code></pre>
<p>Then I get:</p>
<pre><code>    MetaModelWithBias biases before backward [Parameter containing:
tensor([-0.2255], requires_grad=True), Parameter containing:
tensor([-2.2697], requires_grad=True), Parameter containing:
tensor([-0.7426], requires_grad=True), Parameter containing:
tensor([0.0925], requires_grad=True), Parameter containing:
tensor([-0.0564], requires_grad=True)] [None, None, None, None, None]
    MetaModelWithBias biases after backward [Parameter containing:
tensor([-0.2255], requires_grad=True), Parameter containing:
tensor([-2.2697], requires_grad=True), Parameter containing:
tensor([-0.7426], requires_grad=True), Parameter containing:
tensor([0.0925], requires_grad=True), Parameter containing:
tensor([-0.0564], requires_grad=True)] [None, None, None, None, None]
</code></pre>
<p>thanks for your time!</p>
","2024-04-20 14:47:47","0","Question"
"78358623","78324549","","<p>Simple RNN below that accepts data as <code>(sequence_length, n_features)</code> or <code>(batch_size, sequence_length, n_features)</code>. It steps through the entire sequence and returns the outputs and hidden states for each step (it also stores them as attributes which you can access). Is this the sort of functionality you were after? No pruning, but you could add that in like in your original code.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size=4, output_size=2, activation='tanh', batch_first=True):
        super().__init__()
        
        #Onyl support batch_first=True (as per OP's test data)
        assert batch_first, 'This model assumes batch_first=True for simplicity'

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.activation_fn = getattr(torch.nn.functional, activation)
        
        self.Wxh = nn.Linear(self.input_size, self.hidden_size)
        self.Whh = nn.Linear(self.hidden_size, self.hidden_size)
        self.Why = nn.Linear(self.hidden_size, output_size)
        
    def forward(self, x):
        x = x.clone()
        
        x_ndim_orig = x.ndim
        
        #If it's 2D, assume that means (sequence_length, n_features,)
        # and prepend batch
        if x.ndim == 2:
            print('X.ndim is 2 | Assuming X.shape is (sequence_length, n_features)')
            x = x.unsqueeze(dim=0)
        elif x.ndim == 3:
            print('X.ndim is 3 | Assuming X.shape is (batch_size, sequence_length, n_features)')
        
        #Record the hidden state and y at each step for input x
        hidden_states = []
        outputs = []
        
        batch_size, sequence_len, n_features = x.shape
        assert self.input_size == n_features, f'Expected input features size of {self.input_size}'
        
        #Initialise hidden_state to 0, and step through the sequence recurrently
        hidden_state = torch.zeros(batch_size, self.hidden_size)
        for frame_idx in range(sequence_len):
            frame = x[:, frame_idx, :] #(batch, n_features) for this timestep
            
            hidden_state = self.activation_fn(
                self.Wxh(frame) + self.Whh(hidden_state)
            )
            output = self.activation_fn(self.Why(hidden_state))
            
            #Record the hidden state and y for this frame
            hidden_states.append(hidden_state)
            outputs.append(output)
        
        #Stack into (batch_size, sequence_length, output_size/hidden_size)
        # Available as attributes
        self.outputs = torch.stack(outputs, dim=1)
        self.hidden_states = torch.stack(hidden_states, dim=1)
        
        #Optionally drop the batch dim that we added
        if x_ndim_orig == 2:
            self.outputs, self.hidden_states = self.outputs[0], self.hidden_states[0]
        
        return self.outputs, self.hidden_states

</code></pre>
<p>Test the shapes:</p>
<pre class=""lang-py prettyprint-override""><code>#Input:  (sequence_length=12, n_features=4)
#Output: (sequence_length=12, hidden_size)
x = torch.rand(12, 4)
outputs, hidden_states = SimpleRNN(input_size=4)(x)
print(hidden_states.shape)

#Input: (batch_size=32, sequence_length=12, n_features=4)
#Output: (batch_size=32, sequence_length=12, hidden_size)
x = torch.rand(32, 12, 4)
outputs, hidden_states = SimpleRNN(input_size=4)(x)
print(hidden_states.shape)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>X.ndim is 2 | Assuming X.shape is (sequence_length, n_features)
torch.Size([12, 4])

X.ndim is 3 | Assuming X.shape is (batch_size, sequence_length, n_features)
torch.Size([32, 12, 4])
</code></pre>
<p>The RNN is untested, and is meant to illustrate how you can do the recurrence inside the class &amp; store the hidden states.</p>
","2024-04-20 14:43:03","0","Answer"
"78358348","78358188","","<p>It seems the PIL.Image.fromarray function only takes in a set of datatypes for conversion (as can be seen in <a href=""https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.fromarray"" rel=""nofollow noreferrer"">https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.fromarray</a>).</p>
<p>For your case, I tried out converting the torch array to <code>np.uint32</code> and it worked.</p>
<p>You can do this with :</p>
<pre><code>import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
from IPython.display import display
def tensor_to_pil(image_tensor):
    print(image_tensor[0].shape)
    plt.figure()
    plt.imshow(image_tensor[0].cpu().squeeze().numpy(), cmap='gray')
    plt.show()
    # print((image_tensor[0].cpu().squeeze().numpy()))
    if image_tensor.shape[1] == 1:
        pil_image = Image.fromarray(image_tensor[0].cpu().squeeze().numpy().astype(np.uint32))
    else:
        # This seems to be an error in the code where 'error' is not defined
        # error
        pil_image = Image.fromarray(image_tensor[0].permute(1, 2, 0).cpu().numpy())
    print(&quot;yes&quot;)
    display(pil_image)
    print(&quot;yes&quot;)
</code></pre>
","2024-04-20 13:14:25","0","Answer"
"78358188","","Why do PIL and plt.imshow display different images when using the same tensor in Python?","<p>I am trying to convert a PyTorch tensor to a PIL image and display it using both matplotlib.pyplot and PIL. However, I am noticing that the images displayed by plt.imshow and PIL's display() function look different from each other.</p>
<p>imshow</p>
<p><a href=""https://i.sstatic.net/X0qrB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/X0qrB.png"" alt=""enter image description here"" /></a></p>
<p>PIL</p>
<p><a href=""https://i.sstatic.net/7JEZu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7JEZu.jpg"" alt=""enter image description here"" /></a></p>
<p>Below is the function I am using to perform the conversion and display the images:</p>
<pre><code>import matplotlib.pyplot as plt
from PIL import Image

def tensor_to_pil(image_tensor):
    print(image_tensor[0].shape)
    plt.figure()
    plt.imshow(image_tensor[0].cpu().squeeze().numpy(), cmap='gray')

    if image_tensor.shape[1] == 1:
        pil_image = Image.fromarray(image_tensor[0].cpu().squeeze().numpy(), &quot;L&quot;)
    else:
        # This seems to be an error in the code where 'error' is not defined
        # error
        pil_image = Image.fromarray(image_tensor[0].permute(1, 2, 0).cpu().numpy())
    print(&quot;yes&quot;)
    display(pil_image)
    print(&quot;yes&quot;)


</code></pre>
<p>Tensor Dimensions: The tensor I am using is a grayscale image (1 color channel) , image_tensor variable have dimension torch.Size([1,1,640,640])</p>
<p>What could be causing the difference in how plt.imshow and PIL display the same image tensor? Is there any additional processing I need to do to align their outputs?</p>
","2024-04-20 12:23:38","0","Question"
"78357078","78355676","","<p>I prefer to have a separate <code>requirements.txt</code> for installing Python packages rather than specific <code>pip install</code> commands in the <code>Dockerfile</code>.</p>
<p>🗎 <code>Dockerfile</code> (Upgrading <code>pip</code> is not necessary but it silences a warning message.)</p>
<pre><code>FROM python:3.10.12

COPY requirements.txt .

RUN pip install --upgrade pip &amp;&amp; \
    pip install -r requirements.txt
</code></pre>
<p>🗎 <code>requirements.txt</code></p>
<pre><code>torch==2.2.1
torchaudio==2.2.1
numpy==1.26.4
</code></pre>
<p><a href=""https://i.sstatic.net/2ap9d.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2ap9d.png"" alt=""enter image description here"" /></a></p>
","2024-04-20 05:25:16","1","Answer"
"78356949","","A100 Problem: CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call","<p>I have 4 A100 80G GPU. I always meet error when I use</p>
<pre><code>device_map=&quot;auto&quot;
</code></pre>
<p>The error is :</p>
<pre><code>CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p>But when I set it to <code>cpu</code> or <code>cuda:0</code>. The error will be disappear .</p>
<p>Can anyone explain why?</p>
<p>Here is my code from Llama3 official one:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map=&quot;auto&quot;,
)

messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a pirate chatbot who always responds in pirate speak!&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors=&quot;pt&quot;
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids(&quot;&lt;|eot_id|&gt;&quot;)
]

outputs = model.generate(
    input_ids,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))
</code></pre>
","2024-04-20 04:02:08","0","Question"
"78356491","78349947","","<p><code>unpack_sequence()</code> also removes the padding, so the sequences are no longer padded to the same length, as you've said. <code>unpacked_lstm_out</code> is a <code>list</code> of length <code>batch_size</code>, where each element is shaped <code>(sample's sequence length, hidden_size)</code>.</p>
<blockquote>
<p><code>output = self.fc1(unpacked_lstm_tensor[:,-1,:])</code></p>
</blockquote>
<p>I think it should be:</p>
<pre class=""lang-py prettyprint-override""><code>output_n = torch.stack([seq[-1, :] for seq in unpacked_lstm_out], dim=0)

output = self.fc1(output_n)
</code></pre>
<p>This pulls out the final frame from each sequence in the unpacked <code>list</code>, and stacks them into a tensor shaped <code>(batch_size, hidden_size)</code>.</p>
<blockquote>
<p>To account for the variable lengths of the sequences in the last batch (since the data runs out)</p>
</blockquote>
<p>Perhaps you could drop those shorter sequences? You'll lose a bit of the tail data, but it'll mean you can avoid handling variable sequence lengths. Alternatively, you could write a custom data sampler that batches equally-sized sequences together (example below). In both cases, you can use regular tensors (rather than packed sequences), which are simpler and work seamlessly with other <code>torch.nn</code> layers.</p>
<hr />
<p>Code I've previously used to draw batches from a dataset, where each batch has sequences of the same length. For example, the first batch might be <code>(batch_size, sequences that all have length 5)</code>, and the next random batch could be <code>(batch_size, sequences that all have length 13)</code>. <code>SameLengthsBatchSampler</code> yields the indices of the samples to use, not the samples themselves. It's supplied to the <code>batch_sampler=</code> parameter of <code>DataLoader()</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data import Sampler

#Batch sampler: yields (B, sample indices where each sample has same seq_length).
class SameLengthsBatchSampler(Sampler):
    def __init__(self, sentences, batch_size, drop_last=False):
        lengths = [len(sentence) for sentence in sentences]
        unique_lengths, counts = np.unique(lengths, return_counts=True)
        
        #Only consider sequence lengths where count &gt;= batch_size
        unique_lengths = unique_lengths[counts &gt;= batch_size]
        counts = counts[counts &gt;= batch_size]
        
        same_lens_dict = {}
        for length in unique_lengths:
            same_lens_dict[length] = np.argwhere(lengths == length).ravel()
        
        self.same_lens_dict = same_lens_dict #samples organised by sequence len
        self.unique_lengths = unique_lengths
        self.batch_size = batch_size
        self.drop_last = drop_last
    
    def __len__(self):
        for i, _ in enumerate(self.__iter__()):
            pass
        return i
        
    def __iter__(self):
        for seq_len in self.unique_lengths[torch.randperm(len(self.unique_lengths))]:
            #All samples with this length
            sample_indices = torch.tensor(self.same_lens_dict[seq_len])
            shuffled_ixs = sample_indices[torch.randperm(len(sample_indices))]
        
            #Split tensor into batch-sized tensors
            indices_per_batch = torch.split(shuffled_ixs, self.batch_size)
            
            if self.drop_last and len(indices_per_batch[-1]) &lt; self.batch_size:
                indices_per_batch = indices_per_batch[:-1]
            
            if False: #print batch details
                print('sequence_length={} | yielding {} samples over {} batches'.format(
                    seq_len, len(sample_indices), len(indices_per_batch)
                ))
            
            #yield over the batch indices
            yield from indices_per_batch

#
# Batch data
#
batch_size = 32

train_loader = DataLoader(
    train_dataset,
    batch_sampler=SameLengthsBatchSampler(trn_sentences, batch_size)
)

val_loader = DataLoader(
    val_dataset,
    batch_sampler=SameLengthsBatchSampler(val_sentences, batch_size)
)
</code></pre>
<p>Some discussion of this type of functionality is available <a href=""https://discuss.pytorch.org/t/tensorflow-esque-bucket-by-sequence-length/41284"" rel=""nofollow noreferrer"">here</a>.</p>
","2024-04-19 23:00:50","0","Answer"
"78356205","78355963","","<p><strong>Short answer: The <code>torch.Tensor</code> constructor is overloaded to do the same thing as both <code>torch.tensor</code> and <code>torch.empty</code>.</strong></p>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.empty.html"" rel=""nofollow noreferrer"">torch.empty</a> returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument size. So when you call <code>torch.Tensor</code> with <code>torch.Size</code> it is expected behaviour to get seemingly random (uninitialized) data with shape defined by the <code>torch.Size</code> object.</p>
<p>There is a <a href=""https://discuss.pytorch.org/t/what-is-the-difference-between-tensor-and-tensor-is-tensor-going-to-be-deprecated-in-the-future/17134/6?u=harsanyidani"" rel=""nofollow noreferrer"">great comment</a> about this on pytorch forum by a developer, which also explains <strong>why is it this way</strong>:</p>
<blockquote>
<p>Our torch.Tensor constructor is overloaded to do the same thing as both torch.tensor and torch.empty. We thought this overload would make code confusing, so we split torch.Tensor into torch.tensor and torch.empty. So @yxchng yes, to some extent, torch.tensor works similarly to torch.Tensor (when you pass in data). @ProGamerGov no, neither should be more efficient than the other. It’s just that the torch.empty and torch.tensor have a nicer API than our legacy torch.Tensor constructor.</p>
</blockquote>
<p>You are right that this should be documented, however it is <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" rel=""nofollow noreferrer"">mentioned</a> that</p>
<blockquote>
<p>To create a tensor with pre-existing data, use torch.tensor().</p>
</blockquote>
<p>Similar question: <a href=""https://stackoverflow.com/questions/51911749/what-is-the-difference-between-torch-tensor-and-torch-tensor"">What is the difference between torch.tensor and torch.Tensor?</a></p>
","2024-04-19 21:19:01","1","Answer"
"78356026","","How to assign values to certain rows in a tensor based on some condition?","<p>Give tensor left[N,2], right[N,2], mask[1,N], I want a fast method to assign a row value to left based on some condition of the values of right, applying a mask to both, for instance:</p>
<p>left =</p>
<pre><code>[[0.9, 0.8],
 [0.3, 0.0],
 [0.6, 0.9],
 [0.7, 0.0],
 [0.6, 0.8],
 [0.6, 0.2],
 [0.6, 0.2]]
</code></pre>
<p>mask =</p>
<pre><code>[ True,  True, False,  True, False, False,  True]
</code></pre>
<p>right =</p>
<pre><code>[[ 1.,  3.],
 [ 1.,  5.],
 [ 7.,  0.],
 [11., 13.],
 [17., 19.],
 [21., 1. ],
 [ 1., 13.]]
</code></pre>
<p>and, <code>c1 = [1,3,5]</code> and <code>c2 = [11, 13]</code>. So, filtering both left and right by the <code>mask</code>, if a row in <code>right</code> has both it's entries in either <code>c1</code> or both in <code>c2</code>, then the corresponding row in left should be [0, 1] if not then [1, 0]. It is applied in the forward method, so I want a fast efficient way to do it, other than a for loop.</p>
<p>so it should be something like this, except this does not work:</p>
<pre><code>left[mask] = torch.tensor([0, 1]) if (right[mask][0] in c1 and right[mask][1] in c1) or ( right[mask][0] in c2 and right[mask][1] in c2) else torch.tensor([1,0])
</code></pre>
","2024-04-19 20:21:21","1","Question"
"78355963","","I don't understand the behavior of torch.Tensor when passing a torch.Size object into it","<p><strong>Background</strong>
I'm trying to stick within the torch framework to ensure that if the data structures worked with are in the GPU then it is all in the GPU and vice versa so that I don't mix host and device level variables.</p>
<p><strong>The Problem</strong>
So I want to define a variable, or a small vector containing dimensional values. I have a <code>torch.Tensor</code> containing data, let us call it <code>data</code>. So if I write <code>data.shape</code> it returns</p>
<pre><code>torch.Size([1, 2000, 3000])
</code></pre>
<p>I want to store this information in another <code>torch.Tensor</code> object so I write:</p>
<pre><code>dimensional_tensor = torch.Tensor(data.shape)
</code></pre>
<p>The problem is that it doesn't store those values, instead it generates a <code>Tensor</code> object with what looks like pseudorandom numbers with the same shape as indicated in <code>data.shape</code>, i.e, I get this output if I write <code>dimensional_tensor</code>:</p>
<pre><code>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
</code></pre>
<p>It looks like a zero tensor from the output but if I write <code>torch.unique(dimensional_tensor)</code> it will yield non-zero elements.</p>
<p>If I write</p>
<pre><code>dimensional_tensor = torch.Tensor(list(data.shape))
</code></pre>
<p>or even</p>
<pre><code>dimensional_tensor = torch.tensor(data.shape)
</code></pre>
<p>then it does what I want. What is up with that?</p>
<p>I'm using pytorch 2.1.0. I don't know how to look into the python source code of this and figure out why this is happening. I suspect it may be because torch is binary level code.</p>
<p><strong>Summary</strong></p>
<p><strong>Expected Result</strong>
As I mentioned above I expect the call <code>torch.Tensor(data.shape)</code> to yield a <code>torch.Tensor</code> containing the numerical values of the <code>data.shape</code> call.</p>
<p><strong>Actual Result</strong>
A <code>torch.Tensor</code> object with dimension <code>data.shape</code> seemingly filled with pseudorandom numbers.</p>
<p>I don't have a problem with this if this is the way it <em>should</em> be. I merely want to understand the rationale behind this behavior and I'm unable to find any documentation that would support/explain such behavior. Perhaps this is a bug?</p>
<p>The problem I have with this is that it is undefined and apparently not supported in the documentation which may lead to risk of breaking the code when switching to future updates of pytorch if I would write applications that rely on this.</p>
","2024-04-19 20:03:39","3","Question"
"78355676","","OSError: libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory","<p>I needed to have Python <code>torchaudio</code> library installed for my application which is packaged into a Docker image.</p>
<p>I am able to do this easily on my EC2 instance easily:</p>
<pre><code>pip3 install torchaudio
python3
Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torchaudio
&gt;&gt;&gt; torchaudio.__version__
'2.2.1+cu121'
</code></pre>
<p>But not through my Dockerfile, here's what I have in my Dockerfile:</p>
<pre><code>RUN pip3 install --target=/opt/prod/lib/python3.8/site-packages torchaudio
</code></pre>
<p>but when I entered into the docker container started from this image:</p>
<pre><code>&gt;&gt;&gt; import torchaudio
/opt/prod/lib/python3.8/site-packages/torchaudio/_internal/module_utils.py:99: UserWarning: Failed to import soundfile. 'soundfile' backend is not available.
  warnings.warn(&quot;Failed to import soundfile. 'soundfile' backend is not available.&quot;)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/opt/prod/lib/python3.8/site-packages/torchaudio/__init__.py&quot;, line 1, in &lt;module&gt;
    from torchaudio import (  # noqa: F401
  File &quot;/opt/prod/lib/python3.8/site-packages/torchaudio/_extension.py&quot;, line 135, in &lt;module&gt;
    _init_extension()
  File &quot;/opt/prod/lib/python3.8/site-packages/torchaudio/_extension.py&quot;, line 105, in _init_extension
    _load_lib(&quot;libtorchaudio&quot;)
  File &quot;/opt/prod/lib/python3.8/site-packages/torchaudio/_extension.py&quot;, line 52, in _load_lib
    torch.ops.load_library(path)
  File &quot;/opt/prod/lib/python3.8/site-packages/torch/_ops.py&quot;, line 852, in load_library
    ctypes.CDLL(path)
  File &quot;/opt/prod/python3.8/lib/python3.8/ctypes/__init__.py&quot;, line 373, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory
</code></pre>
","2024-04-19 18:47:00","0","Question"
"78354455","","Generating a specific matrix in PyTorch by cumulative products without for loops","<p>In pytorch, I would like to generate a specific NxN matrix starting from a (N-1)-element array, following a specific order of cumulative products along the elements of the array to populate each row and column of the matrix.</p>
<p>Starting from the array: <a href=""https://i.sstatic.net/wyZE7.png"" rel=""nofollow noreferrer"">array</a>
This should be the output matrix: <a href=""https://i.sstatic.net/vq7gF.png"" rel=""nofollow noreferrer"">matrix</a></p>
<p><strong>I would like to do this without python for loops, in order to avoid significant slowdowns in the forward() function of my model</strong></p>
<p>Here is a working example of this, using a for loop:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import matplotlib.pyplot as plt

def gen_matrix(array):
    N = array.shape[0] + 1
    m = torch.ones(N,N)
    for i in range(N):
        m[i   ,i+1:] = torch.cumprod(array[i:],0)
        m[i+1:,i   ] = torch.cumprod(array[i:],0)
    return m

example_array = torch.tensor([0.1, 0.2, 0.05, 0.3, 0.2])
output_matrix = gen_matrix(example_array)

plt.figure()
plt.imshow(output_matrix)
plt.colorbar()
</code></pre>
<p>Is it possible to substitute the loop with some pytorch method?</p>
<p>I tried to look for a suitable method, however I'm still new to pytorch and couldn't find a way to achieve this without slicing and looping on every row. The output matrix is symmetric as it is equal to its transpose, so I can generate half of it using a combination of <code>.T</code> and <code>.tril()</code>, but I still end up looping over either rows or columns.</p>
","2024-04-19 14:35:46","1","Question"
"78354276","78346857","","<p>Not sure there's a solid solution here.</p>
<p>The typical workaround for GLIBC issues in Conda Forge ecosystem to support deployments to outdated systems is to include a <code>sysroot_linux-64</code> dependency, a minimal repacking of system libraries. However, the problem is that they only cover 2.17 and 2.28. Moreover, one often cannot reliably mix Conda Forge packages with those from <code>main</code> channel and <a href=""https://anaconda.org/pyg/repo"" rel=""nofollow noreferrer"">the <code>pyg</code> channel builds</a> prioritize <code>main</code>, <code>pytorch</code> and <code>nvidia</code>. So, that seems like a dead end.</p>
<p>Personally, I'm very reluctant to recommend <a href=""https://stackoverflow.com/a/78350882/570918"">manipulating LD_LIBRARY_PATH</a> - you basically end up with a software environment that is not transparent and can undermine reproducibility.</p>
<p>I'd probably try running in a container with a modern Linux distro that includes a newer GLIBC. You can still quickly bootstrap the Python environment with Micromamba in a <a href=""https://micromamba-docker.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">micromamba-docker</a> container (e.g., <code>mambaorg/micromamba:jammy-cuda-12.1.1</code> has GLIBC 2.35).</p>
<p>In my opinion, this would be the least hacky/most reproducible approach, and that is usually high value for scientific researchers. Even if your HPC admins don't let people run Docker, they do usually allow Singularity, which can run Docker images.</p>
<p>Otherwise, the all-Conda approach should use the following channel configuration, as this is what the official <code>pyg</code> channel uses when building <code>pytorch-sparse</code> and <code>pyg</code>:</p>
<p><strong>environment.yaml</strong></p>
<pre><code>name: pyg
channels:
  - main
  - pytorch
  - nvidia
  - conda-forge
dependencies:
  - python=3.10        # adjust as needed
  - pyg
  - pytorch-sparse
  - pytorch-cuda=12.1  # adjust as needed
</code></pre>
","2024-04-19 14:07:59","0","Answer"
"78353279","","pytorch error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd","<p>trying to finetune a model but gives this error when training it.
here's the Jupyter cell that gives me this error:</p>
<pre><code>training_args = transformers.TrainingArguments(
      per_device_train_batch_size=1,
      gradient_accumulation_steps=4,
      num_train_epochs=1,
      learning_rate=2e-4,
      fp16=True,
      save_total_limit=3,
      logging_steps=1,
      output_dir=&quot;experiments&quot;,
      optim=&quot;paged_adamw_8bit&quot;,
      lr_scheduler_type=&quot;cosine&quot;,
      warmup_ratio=0.05,
)

trainer = transformers.Trainer(
    model=model,
    train_dataset=data,
    args=training_args,
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)
)


model.config.use_cache = False
trainer.train()

</code></pre>
<p>i tried changing <code>torch.inference_mode()</code> with <code>torch.no_grad()</code> earlier in the code but that doesn't work, tried to change some parameters, which doesn't work too, and when the usecache is =True it says the list is out of tuple which might be expected?
also tried insantiating the model again before optimizing, that doesn't work too. Keep in mind this code used to work, which makes me think if there's an update on the libraries that i'm using or maybe on the model that im training?</p>
","2024-04-19 11:20:05","0","Question"
"78350882","78346857","","<blockquote>
<p>I either need <code>GLIBC-2.29</code> or <code>GLIBCXX-3.4.29</code></p>
</blockquote>
<p>No: you need <em>both</em> up-to-date GLIBC <em>and</em> up-to-date <code>libstdc++.so.6</code>. <code>GLIBC</code> and <code>GLIBCXX</code> have ~nothing to do with each other.</p>
<blockquote>
<p>I downloaded GLIBC-2.29 binary, configure it with a local prefix, built it and installed it. But I didn't know where to go from there.</p>
</blockquote>
<p>Read <a href=""https://stackoverflow.com/a/851229/50617"">this</a> answer.</p>
<hr />
<p>For <code>GLIBCXX</code>, the answer is much simpler. The <a href=""https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html"" rel=""nofollow noreferrer"">ABI page</a> tells you that you need <code>libstdc++.so.6</code> from GCC-11.0. So download a build of that, put <code>libstdc++.so.6</code> into some directory and point <code>LD_LIBRARY_PATH</code> or <code>-rpath</code> to that directory.</p>
<p>Note: <code>miniconda</code> may set <code>LD_LIBRARY_PATH</code> somewhere you don't expect. My advice is to get the binary working to the point where it successfully loads <code>torch</code> outside of <code>miniconda</code>, and once that works, make <code>miniconda</code> work as a separate step.</p>
","2024-04-19 00:49:55","0","Answer"
"78349947","","Issue when padding and packing sequences in LSTM networks using PyTorch","<p>I'm trying to make a simple lstm neural network. I've got time series data which I am splitting into sequences and batches using Pytorch's <code>Dataset</code> and <code>DataLoader</code>. To account for the variable lengths of the sequences in the last batch (since the data runs out), I use padding and packing.</p>
<p>I'm using collate_fn in the dataloader, which looks like this:</p>
<pre><code>def collate_data(batch):
    sequences, targets = zip(*batch)
    
    lens = [len(seq) for seq in sequences]
    print(f&quot;Lens before padding: {lens}&quot;)

    padded_seq = pad_sequence(sequences=sequences,batch_first=True,
    padding_value=float(9.99e10))

    print(f&quot;Lens after padding: {[len(seq) for seq in padded_seq]}&quot;)

    padded_targets = pad_sequence(sequences=targets,batch_first=True,
    padding_value=float(9.99e10))

    packed_batch=pack_padded_sequence(padded_seq,lengths=lens,batch_first=True,\
    enforce_sorted=False)

    print(f&quot;Packed batch lengths: {packed_batch.batch_sizes}&quot;)

    return packed_batch, padded_targets
</code></pre>
<p>My issue is when I try to unpack the values in the forward method of my neural network. My forward method looks like this:</p>
<pre><code> def forward(self,x ):
        lstm = self.lstm
        batch_size = self.batch_size

        h0 = torch.zeros(self.num_layers,batch_size,self.hidden_size,)   
        c0 = torch.zeros(self.num_layers,batch_size,self.hidden_size,)

        packed_lstm_out, (hn,cn) = lstm(x, (h0,c0))
        
        print(f&quot;lstm_out size: {packed_lstm_out.data.size}&quot;)        
        unpacked_lstm_out = unpack_sequence(packed_sequences=packed_lstm_out,)        
        print(f&quot;Unpacked lengths: {[len(seq) for seq in unpacked_lstm_out]}&quot;)

        unpacked_lstm_tensor = torch.stack(unpacked_lstm_out,dim=0).float().\
        requires_grad_(True)

        print(unpacked_lstm_tensor.shape)

        output = self.fc1(unpacked_lstm_tensor[:,-1,:])

        return output
</code></pre>
<p>However I am getting an error when I try to use <code>torch.stack(unpacked_lstm_out, dim=0)</code> since the sizes are different. This is only occurring on the last batch, which should be padded.</p>
<p>I've added print statements, which outputs this, for the last batch:</p>
<pre><code>Lens before padding: [10, 10, 10, 10, 10, 10, 10, 9, 8, 7, 6, 5]
Lens after padding: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
Packed batch lengths: tensor([12, 12, 12, 12, 12, 11, 10,  9,  8,  7])
lstm_out size: torch.Size([105, 16])
Unpacked lengths: [10, 10, 10, 10, 10, 10, 10, 9, 8, 7, 6, 5]
</code></pre>
<p>My understanding is that the issue occurs when I use <code>pack_padded_sequence()</code>, but I don't know how to fix it or why it occurs.</p>
<p>Does anyone know how to fix this issue so that all the tensors are the same size after unpacking them in the forward function?</p>
","2024-04-18 19:35:21","0","Question"
"78349359","78329495","","<p>Another way to solve your problem is using a <strong>minimum</strong> function: If you think of it, joining a series of boolean values with <code>and</code> is the same as finding their minimum: if any value is <code>0</code>/<code>False</code>, the result will be <code>0</code>/<code>False</code>, otherwise the result will be <code>1</code>/<code>True</code>.</p>
<p>In your case, as you want to propagate/accumulate the result, you can use the <a href=""https://pytorch.org/docs/stable/generated/torch.cummin.html"" rel=""nofollow noreferrer""><code>cummin()</code></a> function:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

x = torch.tensor([[False, True, False], [True, False, True]])

print(x.cummin(dim=0).values)
# tensor([[False,  True, False],
#         [False, False, False]])
print(x.cummin(dim=1).values)
# tensor([[False, False, False],
#         [ True, False, False]])
</code></pre>
<ul>
<li>The advantage over <code>cumprod()</code>: your result will again be boolean, no need for an explicit conversion (also, the calculation might be a bit cheaper/faster, but this I didn't check).</li>
<li>The disadvantage compared to <code>cumprod()</code>: you will additionally need to access the <code>values</code> attribute of the result, as Pytorch's <code>cummin()</code> (in contrast to e.g. the one of Pandas) returns the result as a named tuple <code>(values, indices)</code>.</li>
</ul>
","2024-04-18 17:35:54","0","Answer"
"78348484","78348466","","<p>The error:</p>
<pre><code>Expected type 'Optional[(str) -&gt; bool]', got 'bool' instead
</code></pre>
<p>says that <code>is_valid_file</code> an optional field. But If you are passing something it must be a <strong>callable</strong> that takes one argument(which is the path of the file) and returns a boolean(which decides whether the file is corrupted or not).</p>
<p>Quoting from the docs:</p>
<blockquote>
<p><code>is_valid_file</code> (<code>callable</code>, optional) – A function that takes path of
an Image file and check if the file is a valid file (used to check of
corrupt files)</p>
</blockquote>
<p>To solve this you could write a function like this to check whether the  image is broken or not.</p>
<pre><code>from PIL import Image

def is_valid_image_file(path: str) -&gt; bool:
    try: 
        img = Image.open(path)
        img.verify()
     except Exception: 
        return False
     else:
        return True

train_dataset = torchvision.datasets.ImageFolder(root=&quot;SmallSet/Train&quot;, is_valid_file=is_valid_image_file)
</code></pre>
","2024-04-18 14:59:56","2","Answer"
"78348466","","Expected type 'Optional[(str) -> bool]', got 'bool' instead","<p>I am writing a code for some Nueral Network, and I am trying to load images.</p>
<pre><code>train_dataset = torchvision.datasets.ImageFolder(root=&quot;SmallSet/Train&quot;, is_valid_file=True)
</code></pre>
<p>Some of my images are corrupted so I added <code>is_valid_file=True</code></p>
<p>, but I get the error <code>Expected type 'Optional[(str) -&gt; bool]', got 'bool' instead </code></p>
<p>How can I fix this?</p>
","2024-04-18 14:57:20","1","Question"
"78348098","78225920","","<p>I think the issue lies with num_workers.</p>
<p>Can you reduce the number of workers used by the DataLoader ?</p>
<p>Also, try removing pin_memory=True that has caused issues for me before.</p>
<pre><code>&gt; NUM_WORKERS = 2
&gt; 
&gt; train_dataloader, test_dataloader, class_names =
&gt; data_setup.create_dataloaders(
&gt;     train_dir=train_dir,
&gt;     test_dir=test_dir,
&gt;     transform=manual_transforms,
&gt;     batch_size=BATCH_SIZE,
&gt;     num_workers=NUM_WORKERS  # Adjust the number of workers here )
</code></pre>
","2024-04-18 14:07:16","1","Answer"
"78347882","78199621","","<p>I have been successfully run. Environment follows:
cuda 11.8
python 3.10.13
pytorch 2.1.1
causal_conv1d 1.1.1
mamba-ssm 1.2.0.post1</p>
<pre><code>pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118
pip install causal_conv1d==1.1.1
pip install mamba-ssm==1.2.0.post1
</code></pre>
<p>If you try to run vision mamba, you could copy mamba-ssm dir in vim to conda env site-package dir.</p>
<pre><code>cp -rf mamba-1p1p1/mamba_ssm /opt/miniconda3/envs/mamba/lib/python3.10/site-packages
</code></pre>
","2024-04-18 13:39:21","0","Answer"
"78347092","","Timm throwing HuggingFace Hub not installed error when HFhub is installed","<p>I'm trying to create the <a href=""https://huggingface.co/timm/convit_base.fb_in1k"" rel=""nofollow noreferrer"">convit_base.fb_in1k</a> model via timm. When I call <code>timm.create_model('convit_base.fb_in1k', pretrained=True)</code>, I get a <code>RuntimeError: Hugging Face hub model specified but package not installed. Run 'pip install huggingface_hub'.</code> error. I have huggingface_hub installed.</p>
<p>There seem to be various suggestions around the web to update other packages, all of which I have updated. I'm running python 3.8 with <code>huggingface-hub==0.22.2</code>, <code>timm==0.9.16</code>, <code>tokenizers==0.15.2</code>, <code>transformers==4.39.3</code>, <code>safetensors==0.4.3</code>, <code>torch==2.2.2</code>, <code>torchvision==0.17.2</code>. The full pip freeze list can be found <a href=""https://pastebin.com/GneMtx3u"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I'm not sure how to fix this error.</p>
","2024-04-18 11:33:38","2","Question"
"78346857","","How to install GLIBCXX_3.4.29 in Miniconda?","<p><strong>Background:</strong> I work on a server where I am not a <code>sudo</code>-er. I work with Miniconda, the portable version of Anaconda and install all my libraries there.<br />
Now, I am working on a project that requires me to install either <a href=""https://github.com/pyg-team/pyg-lib"" rel=""nofollow noreferrer""><code>pyg-lib</code></a> or <a href=""https://github.com/rusty1s/pytorch_sparse"" rel=""nofollow noreferrer""><code>torch-sparse</code></a>. And I did install them using their <code>pip</code> commands.</p>
<p><strong>The problem:</strong> Both <code>pyg-lib</code> and <code>torch-sparse</code> depend on their choice of <code>GLIBC</code> or <code>GLIBCXX</code>, but my system doesn't have them. I either need <code>GLIBC-2.29</code> or <code>GLIBCXX-3.4.29</code>.</p>
<p>I am a CS major, doing a Ph.D. in machine learning. I can go through elaborate installation steps, but I would prefer a simpler solution. I am willing to do anything to get my project running. @stackoverflow community, please help me get this installed properly in my local directory without <code>sudo</code>-ing anything.</p>
<p><strong>What I tried:</strong> I downloaded <code>GLIBC-2.29</code> binary, configure it with a local prefix, built it and installed it. But I didn't know where to go from there.</p>
","2024-04-18 10:53:50","2","Question"
"78343858","78338857","","<p>You better use <a href=""https://github.com/albumentations-team/albumentations"" rel=""nofollow noreferrer"">albumentation</a>  for image augmentation, you can provide both image and mask together to the augmentation function.</p>
<p>Otherwise you can generate randomly the input parameter (like a rotation angle) and then call the deterministic functional function corresponding to it, like F.rotate in this case.</p>
<p>I think <a href=""https://pytorch.org/blog/extending-torchvisions-transforms-to-object-detection-segmentation-and-video-tasks/"" rel=""nofollow noreferrer"">torchvision.transforms.v2</a> can handle it too.</p>
","2024-04-17 21:33:16","0","Answer"
"78343436","78225920","","<p>The main reason the <code>next(iter(train_dataloader)</code> call is slow is due to multiprocessing - or to the pittfalls of multiprocessing. When <code>num_workers &gt; 0</code>, the call to <code>iter(train_dataloader)</code> will fork the main Python process (the current script), which means that any time-consuming code that occurs during import before the call to <code>iter(...)</code>, such as any kind of file loading that happens in global scope (!), will cause an extra slow down. That is, extra on top of the process creation time and on top of the serialization and deserialization of data that needs to happen when <code>next(iter(...))</code> is called.</p>
<p>You can verify this by adding <code>time.sleep(5)</code> in global scope anywhere before calling <code>next(iter(train_dataloader))</code>. You'll then see that the call will be 5 sec slower than it already was.</p>
<p>Unfortunately, I don't know how to fix this for the torch DataLoader, apart from either (1) set <code>num_workers=0</code>, or (2) make sure you don't have time-consuming code during the import of the main script, or (3) don't use the torch DataLoader, but use the HuggingFace dataset interfaces.</p>
<p>Update: There does not seem to be a work-around here. If you have the following code (in the same script):</p>
<pre><code>dataloader = create_dataloader(...)  # similar to the OPs code
for x in dataloader:
     ...
</code></pre>
<p>or also if you initialized the dataloader in some other module and use something like</p>
<pre><code>from other_module import dataloader
a, b = next(iter(dataloader))
</code></pre>
<p>then the fork (that is triggered by starting to iterate) will cause re-initialization of the dataloader (and its underlying datasets, reading everything from disk again). So, it appears that it only makes sense to use <code>num_workers=1</code> (or higher) if data actually needs to be downloaded from remote servers. If all data is already on the localhost, then, as I understand it, it never makes sense to set <code>num_workers=1</code> (or higher) in this API. (I'm not totally sure here, since I'm not familiar with the underlying torch implementation. Conceivably it could also make sense when the <code>transform</code> method is much slower than the serialization/deserialization part of the code.)</p>
","2024-04-17 19:38:06","2","Answer"
"78343062","78342706","","<p>The error has nothing much to do with <code>torch.nn.Module</code> (or any superclass/subclass of it for that matter). It's due to how attribute look-up works in Python classes.</p>
<p>As you've overridden the <code>__getattr__</code> special method in <code>MyWrapper</code> class, when you do <code>self.instance</code> inside <code>__getattr__</code>, it's getting into an infinite recursive situation to get the attribute named <code>instance</code> as it's looking into the <code>__getattr__</code> of the current object's (<code>self</code>) class (<code>MyWrapper</code>) again (and again) and failing.</p>
<p><strong>Fix:</strong></p>
<p>You can take help from the fact that Python allows you to use superclass's <code>__getattr__</code> method (easily accessible using the <code>super</code> method). So if we use superclass's <code>__getattr__</code> to get the <code>instance</code> resolution correctly, then we can still use <code>getattr</code> to get the next <code>name</code> lookup. For example:</p>
<pre class=""lang-py prettyprint-override""><code>
    In [259]: class MyWrapper(torch.nn.Module):
         ...:     def __init__(self, instance):
         ...:         super().__init__()
         ...:         self.instance = instance
         ...: 
         ...:     def __getattr__(self, name):
         ...:         instance = super().__getattr__(&quot;instance&quot;)
         ...:         return getattr(instance, name)
         ...:         
    
    In [260]: # Your failing example - now working
         ...: net = torch.nn.Linear(12, 12)
         ...: net.test_attribute = &quot;hello world&quot;
         ...: b = MyWrapper(net)
    
    In [261]: print(b.test_attribute)
    hello world

</code></pre>
","2024-04-17 18:14:58","1","Answer"
"78342902","78342736","","<p>Assuming the number of sliced elements remains constant across rows, you can create an arrangement tensor and shift it by the per-row starting index:</p>
<pre><code>&gt;&gt;&gt; idx = torch.tensor([5,10])
&gt;&gt;&gt; idx_ = torch.arange(5,)[None]+idx[:,None]
tensor([[ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14]])
</code></pre>
<p>Then expand <code>idx_</code> such that it has the same last dimension size as <code>partial_arr</code>:</p>
<pre><code>&gt;&gt;&gt; idx_ = idx_[...,None].expand(-1,-1,partial_arr.size(-1)) 
# shaped torch.Size([2, 5, 3])
</code></pre>
<p>Finally, gather the values using <a href=""https://pytorch.org/docs/stable/generated/torch.gather.html"" rel=""nofollow noreferrer""><code>torch.gather</code></a>:</p>
<pre><code>&gt;&gt;&gt; partial_arr.gather(1,idx_).shape
tensor([[[8, 3, 1],
         [2, 4, 6],
         [4, 4, 5],
         [2, 8, 6],
         [3, 7, 0]],

        [[3, 6, 7],
         [5, 7, 4],
         [1, 5, 4],
         [4, 5, 3],
         [7, 1, 2]]])
</code></pre>
","2024-04-17 17:42:41","1","Answer"
"78342736","","Collecting varying element indices from a tensor across multiple dimensions","<p>Assume I got the following tensor:</p>
<pre><code>arr = torch.randint(0, 9, (100, 50, 3))
</code></pre>
<p>What I want to achieve is collecting, for example, 2 elements of that tensor, let's start with collecting the 6th and 56th one:</p>
<pre><code>indices = torch.tensor([5, 55])
partial_arr = arr[indices]
</code></pre>
<p>This gives me an array of  shape</p>
<pre><code>torch.Size([2, 50, 3])
</code></pre>
<p>Now, let's assume that from the first element, I want to collect the elements 5 through 10</p>
<pre><code>first_result = partial_arr[0, 5:10]
</code></pre>
<p>and from the second element, the elements from 10 to 15:</p>
<pre><code>second_result = partial_arr[1, 10:15]
</code></pre>
<p>Since I want everything in one tensor, I can do:</p>
<pre><code>final_result = torch.cat([first_result, second_result])
</code></pre>
<p>How can I achieve the final result only with one operation on the first tensor: <code>arr = torch.randint(0, 9, (100, 50, 3))</code> ?</p>
","2024-04-17 17:12:39","0","Question"
"78342706","","Pythons `__getattr__` + `torch.nn.Module` yields infinite recursion","<p>I wrote a simple wrapper to add special methods to a given PyTorch neural network.
While the implementation below works well for general objects like strings, lists etc. I get a <code>RecursionError</code> when applying it to a <code>torch.nn.Module</code>. It seems that in the latter case the call to <code>self.instance</code> inside the <code>__getattr__</code> method is unsuccessful, so it falls back to <code>__getattr__</code> again, leading to the infinite loop (I also tried <code>self.__dict__['instane']</code> without luck).</p>
<p>I assume that this behaviour stems from the implementations of the <code>__getattr__</code> and <code>__setattr__</code> methods <code>torch.nn.Module</code> but after inspecting their implementations I still don't see how.</p>
<p>I would like to understand in detail what is going on and how to fix the error in my implementation.</p>
<p>(I am aware of the similar question in <a href=""https://stackoverflow.com/q/56497789/12894006"">link</a> but it does not answer my question.)</p>
<p>Here is a minimal implementation to recreate the my situation.</p>
<pre><code>import torch

class MyWrapper(torch.nn.Module):
    def __init__(self, instance):
        super().__init__()
        self.instance = instance

    def __getattr__(self, name):
        print(&quot;trace&quot;, name)
        return getattr(self.instance, name)

# Working example
obj = &quot;test string&quot;
obj_wrapped = MyWrapper(obj)
print(obj_wrapped.split(&quot; &quot;)) # trace split\n ['test', 'string']

# Failing example
net = torch.nn.Linear(12, 12)
net.test_attribute = &quot;hello world&quot;
b = MyWrapper(net)

print(b.test_attribute) # RecursionError: maximum recursion depth exceeded
b.instance # RecursionError: maximum recursion depth exceeded
</code></pre>
","2024-04-17 17:07:22","2","Question"
"78339541","78338107","","<p>What you are trying to do is get <code>out</code> such that:</p>
<pre><code>out[b][k][n] = A[i][B[b][k][n]][n]
</code></pre>
<p>To use <a href=""https://pytorch.org/docs/stable/generated/torch.gather.html"" rel=""nofollow noreferrer""><code>torch.gather</code></a>, you indeed have to have the same number of dimensions. You can do so by expanding an extra singleton dimension on B to have a shape of <code>(b, k, n)</code>.</p>
<p>Here is a minimal example:</p>
<pre><code>A = torch.rand(b,m,n)
B = torch.randint(0,m,(b,k))
</code></pre>
<p>Expand <code>B</code>:</p>
<pre><code>&gt;&gt;&gt; B_ = B[:,:,None].expand(-1,-1,A.size(-1))
</code></pre>
<p>Gather values from <code>A</code>:</p>
<pre><code>&gt;&gt;&gt; A.gather(1,B_)
</code></pre>
","2024-04-17 08:38:09","0","Answer"
"78339198","78338986","","<p>There should be no need to use the pre-release nightly build for cuda 11.8 and python 3.12, at least there seems to be installation candidates in <a href=""https://download.pytorch.org/whl/cu118"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/cu118</a> for python 3.12:</p>
<p><a href=""https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp312-cp312-linux_x86_64.whl#sha256=e46a40d6a1055a4a4ee8c8ac5a8dfb2c70b7382a00c411b0e9f2c86029b6efc4"" rel=""nofollow noreferrer"">torch-2.2.0+cu118-cp312-cp312-linux_x86_64.whl</a><br />
<a href=""https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp312-cp312-win_amd64.whl#sha256=183b17fced6d344cd93a385a0c5f98e3f31abd254b0aed4741e921115d8de7a8"" rel=""nofollow noreferrer"">torch-2.2.0+cu118-cp312-cp312-win_amd64.whl</a></p>
<p>You can specify the index url to search for packages directly in your requirements.txt.</p>
<p>You can make your requirements.txt like this:</p>
<pre><code>--index-url https://download.pytorch.org/whl/cu118
--extra-index-url=https://pypi.org/simple
torch
torchvision
torchaudio
&lt;other packages&gt;
</code></pre>
","2024-04-17 07:44:23","0","Answer"
"78338986","","Is there a way I can install a pytorch version that is not in pip library on an Azure deployment?","<p>I have a flask application that uses pytorch at one point.
However I cannot seem to deploy this successfully due to pytorch versions that are only available pip install from URL</p>
<p>The only version of pytorch with Cuda enabled that seems to work with my python version of 3.12 is one that I found here: <a href=""https://stackoverflow.com/a/77597368/24421235"">https://stackoverflow.com/a/77597368/24421235</a>
and is downloaded with:
pip3 install --pre torch torchvision torchaudio --index-url <a href=""https://download.pytorch.org/whl/nightly/cu118"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/nightly/cu118</a></p>
<p>This means locally my application works completely fine however when trying to deploy my web app with a requirements.txt the version that azure tries to install is non existent</p>
<p><a href=""https://i.sstatic.net/1Q1WT.png"" rel=""nofollow noreferrer"">attempting local pip install of the version shown by pip requirements still gives error</a></p>
<p>Is there a way around this?</p>
<p>github repo <a href=""https://github.com/jamiemitch121/Flask_Image_Creation_Site"" rel=""nofollow noreferrer"">https://github.com/jamiemitch121/Flask_Image_Creation_Site</a></p>
","2024-04-17 07:06:37","0","Question"
"78338857","","How to use PyTorch transform to apply the same transform on the input/output image pairs?","<p>I want to build a deep learning model by PyTorch that makes some image enhancements. Input and output of the model are the same size images.</p>
<p>I want to use PyTorch's <code>torchvision.transforms</code> library for data augmentation. The nature of many useful transforms are random, so I do not know how to apply the same transform on the input/output pairs. I mean, the same transform that applies on the input image, must be applied on the corresponding output image.</p>
","2024-04-17 06:44:09","0","Question"
"78338107","","pytorch indexing multi-dimension tensor with another multi-dimension tensor","<p>In pytorch, I have a tensor A with shape [b, m, n] and another tensor B with shape [b, k]. I want to index A with B. So the result tensor should have a shape [b, k, n].</p>
<p>I tried to do some search but got no luck. torch.index_select or torch.take can only take 1d index tensor. torch.gather requires input tensor and index tensor to have same shapes.</p>
","2024-04-17 02:47:52","0","Question"
"78337993","78337711","","<p>I do not understand what you mean by &quot;training inference&quot;, and overall, the question needs more clarity.</p>
<p>As I understand from a quick look at the paper, they provide you with a special algorithm to compute derivatives of low-rank matrices.</p>
<p>In Torch, you can define a custom function with a forward and backward method using <code>torch.autograd.Function</code> as succinctly described in <a href=""https://gist.github.com/Hanrui-Wang/bf225dc0ccb91cdce160539c0acc853a"" rel=""nofollow noreferrer"">this gist</a>. Refer to <a href=""https://pytorch.org/docs/stable/notes/extending.html"" rel=""nofollow noreferrer"">official documentation</a> for an extensive overview.</p>
<p>This class should handle the gradient computation step while the optimizer will be instantiated with <code>[X]</code> as the list of variable to optimize on.</p>
","2024-04-17 01:58:51","0","Answer"
"78337711","","What would be a proper way of implementing this riemann gradient?","<p>So here’s a toy example from this paper,<a href=""https://arxiv.org/abs/2103.14974"" rel=""nofollow noreferrer"">Automatic differentiation for Riemannian optimization on low-rank matrix and tensor-train manifolds</a>:</p>
<pre><code>import torch
import torch.nn as nn

def f(X):
    return torch.sum(X**2)

def g(delta_U, delta_V, U, V, f):
    perturbed_matrix = U @ delta_V.t() + delta_U @ V.t()
    return f(perturbed_matrix)

def compute_riemannian_gradient(X):
    U, S, V = torch.svd(X)
    delta_U = U @ torch.diag(S)
    delta_V = torch.zeros_like(V)
    delta_U.requires_grad_(True)
    delta_V.requires_grad_(True)
    perturbed_value = g(delta_U, delta_V, U, V, f)
    perturbed_value.backward()

    return delta_U.grad, delta_V.grad

def apply_gauge_conditions(delta_U, delta_V, V):
    delta_V -= V @ (V.t() @ delta_V)
    return delta_U, delta_V

def riemannian_gradient(X):
    U, _, V = torch.svd(X)
    delta_U, delta_V = compute_riemannian_gradient(X)
    delta_U, delta_V = apply_gauge_conditions(delta_U, delta_V, V)
    return delta_U @ V.t() + U @ delta_V.t()

X = torch.randn(5, 3)
y = X**2 + 0.1*torch.randn_like(X)
rgrad = riemannian_gradient(X)

for i in range(10):
    rgrad = riemannian_gradient(X)
    X = X - 0.01*rgrad
    # X = retraction(X, rgrad, 0.01)
    print(f(X))
</code></pre>
<p>So as you can see, in the training inference, I don’t need the gradient of X, or [U, S, V]. Instead, I need the gradient from delta_U and delta_V to update X. Therefore I'm not able to simply loop through the parameters registered in parameters if I want to integrate this piece of code into torch.optimizer module.</p>
<p>My question is what’s the proper way of implementing this optimizing algorithm in optim.step() function when the weight X is updated by gradients from other parameters?</p>
","2024-04-16 23:30:42","0","Question"
"78337397","","How can I speed up my training time for a python-chess bot using ppo?","<p>I am attempting to build a chess bot which learns using Proximal Policy Optimization. I am currently using the python-chess library (<a href=""https://python-chess.readthedocs.io/en/latest/index.html#"" rel=""nofollow noreferrer"">https://python-chess.readthedocs.io/en/latest/index.html#</a>) as the environment where my agent plays games against itself and learns. The issue I am facing is extremely slow games for training. With a move limit of 200 per game, my bot can play 1 game against itself in about 1 second. This 1 second also includes the PPO part of the training which takes on average 0.01 seconds with a GPU.</p>
<p>I am using PyTorch, so I have already moved all tensors to the GPU. I have not found any other ways beyond this to speed up the execution time.</p>
<p>I would like to get the execution time down for playing the games to around 0.5 or less seconds per game, but I haven't been able to find a way to accomplish this.</p>
<p>I would greatly appreciate the feedback and help if anyone knows of a possible solution.</p>
","2024-04-16 21:27:24","0","Question"
"78337099","78331905","","<p>The issue you're running into in <code>outputs.backward(gradient=identity)</code> is because <code>identity</code> is the wrong shape.</p>
<p><em>usually</em> the argument to gradient should be the same shape as the tensor you are calling <code>backward</code> on. I say usually because technically <code>a.backward(gradient=b)</code> computes <code>b^T @ Jacobian(a)</code> so it depends on the shape of the Jacobian, but <em>usually</em> when you're calling <code>backward</code> on the output of a model you want the same shape.</p>
<p>This is the standard way to call <code>backward</code> on a vector output:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(32, 4)
model = nn.Linear(4, 2)
y = model(x)

input_grad = torch.ones_like(y) # create vector of ones the same shape as y
y.backward(input_grad)
model.weight.grad

&gt; tensor([[ 1.1243,  2.3456,  2.3656, -8.8384],
          [ 1.1243,  2.3456,  2.3656, -8.8384]])
</code></pre>
<p>If you want to mask parts of your output from the gradient calculation, you can set the relevant <code>input_grad</code> values to <code>0</code>.</p>
<p>For example, this masks the second column of the output tensor:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(32, 4)
model = nn.Linear(4, 2)
y = model(x)

input_grad = torch.ones_like(y) # create vector of ones the same shape as y
input_grad[:,1] = 0. # mask the second column of the output

y.backward(input_grad)

# masking changes gradient output
model.weight.grad

&gt; tensor([[ 1.1243,  2.3456,  2.3656, -8.8384],
          [ 0.0000,  0.0000,  0.0000,  0.0000]])
</code></pre>
<p>This example only backprops through the first four items in the batch:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(32, 4)
model = nn.Linear(4, 2)
y = model(x)

input_grad = torch.ones_like(y) # create vector of ones the same shape as y
input_grad[4:] = 0. # only backprop through the first 4 items in the batch

y.backward(input_grad)

# masking changes gradient output
model.weight.grad

&gt; tensor([[-0.2436,  0.8189, -0.1244, -0.4814],
          [-0.2436,  0.8189, -0.1244, -0.4814]])
</code></pre>
<p>To backprop from a specific slice, take the slice and use a ones tensor of the same shape:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(32, 4)
model = nn.Linear(4, 2)
y = model(x)

y_slice = y[7] # grab specific batch item

input_grad = torch.ones_like(y_slice) # create vector of ones the same shape as y_slice

y_slice.backward(input_grad)

model.weight.grad

&gt; tensor([[-0.1745, -1.1161, -0.8109, -0.6540],
          [-0.1745, -1.1161, -0.8109, -0.6540]])
</code></pre>
<p>Now to your example of computing the gradients of different classes with respect to the input, you can do something like this:</p>
<pre class=""lang-py prettyprint-override""><code># inputs must have `requires_grad=True` if you want to backprop into them
x = torch.randn(32, 4, requires_grad=True)

# model has two output classes
model = nn.Linear(4, 2)

# y is size `(32, 2)` for 32 batch items and 2 classes
y = model(x)

grads = []

for batch_idx in range(x.shape[0]): # iterate over batch items
    for class_idx in range(y.shape[1]): # iterate over classes
        y_slice = y[batch_idx, class_idx] # get specific output value

        # compute grad with retain graph
        # note that the second argument must be `x`, you cannot 
        # pass a slice of `x` because it was not used in the 
        # compute graph that produced `y`
        grad = torch.autograd.grad(y_slice, x, grad_outputs=torch.ones_like(y_slice), retain_graph=True)
        
        # grad output is a tulple of shape `(grad,)
        # grad[0] grabs the actual grad tensor
        # the grad tensor is the same shape of `x`, but due to backproping from y_slice,
        # all items except `batch_idx` are zero.
        # `grad[0][batch_idx]` gets the gradient specifically of item `batch_idx` wrt `class_idx`
        grad = grad[0][batch_idx]
        
        # save tuple of batch_idx, class_idx, grad
        grads.append((batch_idx, class_idx, grad))
</code></pre>
<p>For a fun exercise, run the code above and look at the gradients for each class:</p>
<pre class=""lang-py prettyprint-override""><code>print([i[-1] for i in grads if i[1]==0])
print([i[-1] for i in grads if i[1]==1])
</code></pre>
<p>You'll notice the gradient values are the same for each input item with respect to a given class. Think about why this makes sense.</p>
","2024-04-16 20:03:18","1","Answer"
"78335144","78295146","","<p>I've followed your steps and was able to regenerate the error -
<a href=""https://i.sstatic.net/woGmB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/woGmB.png"" alt=""enter image description here"" /></a></p>
<p>Now to remove the error, I tried installing the required pytorch and cuda version from the links given on the official website -
[https://pytorch.org/get-started/previous-versions/]</p>
<p>for my windows system the command will be -</p>
<pre><code>pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>Results -</p>
<pre><code>(pixray-env) PS C:\Users\...\Desktop\VS01\..\test&gt; python --version   
Python 3.8.10                                                                         
(pixray-env) PS C:\Users\..\Desktop\VS01\..\test&gt; pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
Looking in links: https://download.pytorch.org/whl/torch_stable.html
Collecting torch==1.9.0+cu102
  Downloading https://download.pytorch.org/whl/cu102/torch-1.9.0%2Bcu102-cp38-cp38-win_amd64.whl (1440.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 GB 3.0 MB/s eta 0:00:00
Collecting torchvision==0.10.0+cu102
</code></pre>
<p>Visit the link and find the suitable command by scrolling down and finding your requirement.</p>
<p>Also, sometimes you need to check if your system is using the correct python version or not, use the command <code>python --version</code> once before running the pytorch installation</p>
","2024-04-16 14:04:15","0","Answer"
"78334935","78334582","","<p>The answer is &quot;sort of&quot;. Integer tensors can be used in loss functions such as in loss functions for categorical outcomes (see <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer"">here</a> for an example). However, they cannot be used as a parameter to be optimized. That's because derivatives with respect to integers are not well defined. The derivative captures the effect of an infinitesimal change in the input on the output. But infinitesimal changes are not applicable to integers; the smallest possible change is 1 (or -1).</p>
","2024-04-16 13:34:59","1","Answer"
"78334582","","Can a tensor with dtype uint8 be used for a loss function, which will later call '.backward()'?","<p>I attempted to calculate the loss between a tensor with dtype <code>float32</code> and another with dtype <code>uint8</code>.</p>
<p>Since the loss function performs automatic type promotion, I didn't make a type conversion explicitly at first.</p>
<p>Here's the code:</p>
<pre><code>import torch
import torch.nn as nn

a = torch.randn(3, 3, dtype=torch.float32, requires_grad=True)
b = torch.randint(0, 256, (3, 3), dtype=torch.uint8)
loss = nn.MSELoss()(a, b)
print(loss.dtype)
loss.backward()
</code></pre>
<p>The output:</p>
<pre class=""lang-none prettyprint-override""><code>torch.float32
</code></pre>
<p>From my point of view, that means the auto type promotiom works as expected.</p>
<p>However, an error occurs:</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File &quot;/root/.../test.py&quot;, line 8, in &lt;module&gt;
    loss.backward()
  File &quot;/root/.../python3.8/site-packages/torch/_tensor.py&quot;, line 522, in backward
    torch.autograd.backward(
  File &quot;/root/.../python3.8/site-packages/torch/autograd/__init__.py&quot;, line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Found dtype Byte but expected Float
</code></pre>
<p>then I changed the line about loss:</p>
<pre><code># loss = nn.MSELoss()(a, b)
loss = nn.MSELoss()(a, b.to(torch.float32))
</code></pre>
<p>It works.</p>
<p>But why?</p>
<p>I also noticed that in the definition of 'b', I can't set <code>requires_grad=True</code>, since it will raise an error:</p>
<blockquote>
<p>RuntimeError: Only Tensors of floating point and complex dtype can require gradients</p>
</blockquote>
<p>I think maybe there's some connection with my main problem, so I'm including it here.</p>
<p>So, can a tensor with dtype uint8 be used for a loss function, which will later call <code>.backward()</code>?</p>
","2024-04-16 12:36:04","0","Question"
"78331905","","Efficient way to compute gradients separately w.r.t each class in Pytorch","<p>I am trying to compute the gradients of a Pytorch image classifier model <strong>separately</strong> with respect to each class, for example</p>
<pre><code>outputs = net(inputs)[0] # assuming we only consider the first sample of the batch
grads = [torch.autograd.grad(outputs[i], inputs, retain_graph=True) 
         for i in range(len(outputs))]
</code></pre>
<p>However, in <code>torch.autograd.grad</code> documentation, it states
'''
Note that in nearly all cases setting this option (retain_graph) to True is not needed and often can be worked around in a much more efficient way
'''</p>
<h3></h3>
<p>Bing AI has suggested to use</p>
<pre><code>identity = torch.eye(len(outputs))
outputs.backward(gradient=identity)
</code></pre>
<p>But it obviously does not work</p>
<pre><code>RuntimeError: Mismatch in shape: grad_output[0] has a shape of torch.Size([9, 9]) and outputs has a shape of torch.Size([9]).
</code></pre>
<p>(Here the image classifer has 9 classes)</p>
<h3></h3>
<p>I am therefore wondering whether there is a more efficient way in this case, and if so, how to implement it?</p>
<p>Thanks for any helps!</p>
","2024-04-16 03:01:03","1","Question"
"78331811","78331713","","<p>Your prediction is the wrong size. The prediction tensor should be of size <code>(bs, n_classes)</code>. Since you have three classes, your prediction should be of shape <code>(bs, 3)</code>.</p>
<p>If your prediction is shape <code>(bs, 1)</code>, the softmax of the class dimension will return 1. for every value. No matter what your model does, the output of shape <code>(bs, 1)</code> will be interpreted as predicting class 0 with 100% confidence due to the softmax.</p>
<p>To predict three classes, your output should be of shape <code>(bs, 3)</code>. With the correct shape, the loss weights work as predicted:</p>
<pre class=""lang-py prettyprint-override""><code>bs = 32
n_classes = 3
preds = torch.randn(bs, n_classes)
targs = torch.randint(0, high=n_classes, size=(bs,))
weights = torch.tensor([1., 10., 10.])
loss = nn.CrossEntropyLoss(weight=weights)
loss(preds, targs)
&gt; tensor(1.5492)
</code></pre>
","2024-04-16 02:23:32","1","Answer"
"78331713","","cross entropy loss and torch weights mismatch","<p>My targets are primarily class 0, less frequently class 1 or 2
Trying to do cross entropy loss with class weights
The following code</p>
<pre><code>weights = torch.tensor([1., 10, 10.]).to(device)
lossfn = nn.CrossEntropyLoss(weight=weights) 
pred = model(input1, input2)
target = labelarray.type(torch.LongTensor).to(device) 
loss = lossfn(pred, target)
</code></pre>
<p>produces the following error</p>
<p><strong>RuntimeError: weight tensor should be defined either for all 1 classes or no classes but got weight tensor of shape: [3]</strong></p>
<p>B (batch size) is 128;</p>
<p>pred is ~ torch.Size([B, 1])</p>
<p>target ~ torch.Size([B]) =</p>
<p>([0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,
0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 2, 1, 0, 0, 2, 1, 2, 1, 0, 0, 0, 2, 0, 0,
0, 0, 0, 0, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')</p>
<p>Clearly, my target has 3 classes , the error suggests pytorch sees just 1 class</p>
","2024-04-16 01:37:05","0","Question"
"78331651","78322538","","<p>I figured this out:</p>
<ol>
<li>The discrepancy was because my application was built into a docker image with all necessary dependencies, not really directly being run on the GPU instance, to see if <code>torch</code> is installed in the docker image, I needed to keep the app running (so the docker instance is up and I can enter), using this command to enter into the docker <code>docker exec -it container_id /bin/sh</code> and then run commands there to see if <code>torch</code> is installed;</li>
<li>To install <code>torch</code> module, instead of doing so via UserData in my .yaml template which is used to install/run on the GPU instance, I should be packaging <code>torch</code> into my docker image instead;</li>
</ol>
<p>Just in case if anyone else runs into this issue in the future, they might find this post somewhat helpful;</p>
<p>Thanks</p>
","2024-04-16 01:08:51","0","Answer"
"78329867","78329495","","<p>The logical <code>and</code> corresponds to a product in binary terms. You can use <a href=""https://pytorch.org/docs/stable/generated/torch.cumprod.html"" rel=""nofollow noreferrer""><code>cumprod</code></a> for that:</p>
<pre><code>&gt;&gt;&gt; x.cumprod(dim=0).bool()
tensor([[False,  True, False],
        [False, False, False]])

&gt;&gt;&gt; x.cumprod(dim=1).bool()
tensor([[False, False, False],
        [ True, False, False]])
</code></pre>
","2024-04-15 16:35:41","2","Answer"
"78329767","78329483","","<p>When you call <code>loss.backward()</code> it will go backwards through the graph over all parameters and calculate the gradients for each trainable parameter and accumulate it in <code>parameter.grad</code> (but not change the parameter itself. That is done with <code>optimizer.step()</code>). So unless you call <code>metric.backward()</code>, <code>metric</code> will not affect the gradients. In fact you can calculate as many metrics as you want this way without affecting the gradients. This answer might also be helpful: <a href=""https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/2"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/2</a></p>
","2024-04-15 16:15:49","0","Answer"
"78329495","","what is the equivalent of numpy accumulate ufunc in pytorch","<p>In numpy, I can do the following:</p>
<pre><code>&gt;&gt;&gt; x = np.array([[False, True, False], [True, False, True]])
&gt;&gt;&gt; z0 = np.logical_and.accumulate(x, axis=0)
&gt;&gt;&gt; z1 = np.logical_and.accumulate(x, axis=1)
</code></pre>
<p>This returns the following:</p>
<pre><code>&gt;&gt;&gt; z0
array([[False,  True, False],
       [False, False, False]])

&gt;&gt;&gt; z1
array([[False, False, False],
       [ True, False, False]])
</code></pre>
<p>What is the equivalent of this ufunc operation in pytorch?</p>
","2024-04-15 15:26:35","2","Question"
"78329483","","how does loss.backward() calculated in pytorch lightning","<p>I understand that when <code>training_step()</code> returns loss, the code for automatic optimization
(<a href=""https://lightning.ai/docs/pytorch/stable/common/optimization.html#automatic-optimization"" rel=""nofollow noreferrer"">link</a>) takes care of the loss.backward()</p>
<p>Can someone tell me what would be difference in the loss.backward() automatic optimization, for the following two scenarios for training_step():</p>
<p>Scenario 1:</p>
<pre><code>def training_step(self, batch: list,epochidx):
     x,y = batch
     output = model(x)
     loss = self.loss_func(output,y)

     return loss

</code></pre>
<p>Scenario 2:</p>
<pre><code>def training_step(self, batch: list,epochidx):
     x,y = batch
     output = model(x)
     loss = self.loss_func(output,y)
     metric = self.metric(output,y)

     train_log = {&quot;loss&quot;:loss,&quot;metric&quot;:metric}

     return train_log

</code></pre>
<p>What my worry is that loss.backward() in the 2nd scenario does backward for both loss and metric instead of just loss.</p>
<p>I opened the pytorch-lightning files in my conda environment to understand how the automatic optimization is happening if I send a dictionary instead of a Tensor but it didn't lead to much.</p>
<p>Any help/hint is appreciated. Thanks!</p>
","2024-04-15 15:25:31","0","Question"
"78329328","","converting safe.tensor to pytorch bin files","<p>I fine-tuned my transformer model with HuggingFace. It has provided me a <code>model.safetensor</code> file for later use.</p>
<p>I want to plug-in the model to a old framework which takes <code>pytorch.bin</code> files only. I wonder how can I <strong>downgrade</strong> my model to fit the framework.</p>
<p>I am not sure what is the best way to do such moodel format conversion.</p>
<p>Thanks for your help.</p>
","2024-04-15 15:04:10","2","Question"
"78329223","78328887","","<p>I could not figure out a way to do it with self.log in training_step. However you can aggregate them yourself.</p>
<pre><code>def MyModel(pl.LightningModule):
    def __init__(self, ...):
        ...
        self._training_batch_losses = []

    ...

    def training_step(self, batch, batch_idx):
        # get batch loss
        ...
        self._training_batch_losses.append(batch_loss)
        return batch_loss

    def on_train_epoch_end(self):
        epoch_median = torch.stack(self._training_batch_losses).median()
        self.log(&quot;train_loss_epoch&quot;, epoch_median)
        self._training_batch_losses.clear()
</code></pre>
<p>It works for validation as well.</p>
","2024-04-15 14:46:04","0","Answer"
"78328985","78326120","","<p>Why I would suggest is first flatten your original image,resulting in a (800*800, 1500) vector <code>v</code>.</p>
<p>If this fits in memory, I would to the scalar product then argmax of a section of v with itself, consuming no additional memory than the chunk of v used (although it might even be a reference to v used in the scalar product so no more memory used). then you only store the argmax indices in a (800*800) shape vector.</p>
<pre><code>v: torch.Tensor = ...
v = torch.flatten(v)

chunk_size: int = 50 # arbitrary chunk size

argmax_flat_indices = []
for cursor in range(0, len(v), chunk_size):
    # Here compute the cosine sim between v and v[cursor: cursor + chunk_size]
    # Do also the argmax along dim 1
    chunk_argmax_indices = ...

    argmax_flat_indices.append(chunk_argmax_indices)
    

argmax_flat_indices = torch.cat(argmax_flat_indices)

#Then compute back original indices
...
</code></pre>
<p>Would this fit you ?</p>
","2024-04-15 14:08:07","0","Answer"
"78328887","","How to use median as the reduction function for loss in Pytorch Lightning Trainer?","<p>I want to log the median loss every epoch in Pytorch Lightning. I tried <code>self.log(..., reduce_fx=torch.median)</code> which gave me error <code>lightning_fabric.utilities.exceptions.MisconfigurationException: Only `self.log(..., reduce_fx={min,max,mean,sum})` are supported. If you need a custom reduction, please log a `torchmetrics.Metric` instance instead. Found: &lt;function median at 0x7f53bbd59fc0&gt;</code></p>
<p>So I implement a subclass of torchmetrics.Metric, and pass that as reduce_fx:</p>
<pre><code>import torch
from torchmetrics import Metric

class MedianMetric(Metric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.add_state(&quot;median&quot;, default=torch.Tensor(0))
    
    def update(self, batch_losses):
        self.median = torch.median(self._input_format(batch_losses))

    def compute(self):
        return self.median.float()
    
    def __name__(self):
        return &quot;median&quot;
</code></pre>
<p>in my LightningModule:</p>
<pre><code>class MyModel(pl.LightningModule):
    def __init__(self, ...):
        self._red = MedianMetric()
        ...
    ...
    def training_step(self, batch, batch_idx):
        loss, _ = self.calculate_metrics_and_loss(batch, batch_idx, metric=False)
        self.log(&quot;train_loss&quot;, loss, on_epoch=True, prog_bar=True, reduce_fx=self._red)
        return loss
</code></pre>
<p>Now I get <code>TypeError: MedianMetric.update() takes 2 positional arguments but 3 were given</code>. I think it wants me to put preds and inputs as the arguments to update, but then I'm re-calculating the loss once in training_step and then a second time in MedianMetric.compute, which doesn't seem right. Is there an easy way to keep track of median loss per epoch using Lightning?</p>
","2024-04-15 13:50:58","0","Question"
"78328631","78328539","","<p>Every model with &quot;token-classification&quot; tag should be usable with pipeline : <a href=""https://huggingface.co/models?pipeline_tag=token-classification"" rel=""nofollow noreferrer"">https://huggingface.co/models?pipeline_tag=token-classification</a></p>
","2024-04-15 13:10:55","0","Answer"
"78328539","","Huggingface pipeline available models","<p>I'm working with Huggingface in Python to make inference with specific LLM text generation models. So far I used pipelines like this to initialize the model, and then insert input from a user and retrieve the response:</p>
<pre><code>import torch
from transformers import pipeline
print(torch.cuda.is_available())

generator = pipeline('text-generation', model='gpt2', device=&quot;cuda&quot;)
#Inference code
</code></pre>
<p>However, when I change <code>gpt2</code> with <code>google/gemma-2b-it</code> or some other models, it might ask for authentication or directly it thwors an error indicating it´s not available from <code>pipeline()</code>.</p>
<p>I know some models need specific tokenizers and dependencies, but, is there any way to list all available models from <code>pipeline()</code>? And is there any way I can use other models inside <code>pipeline()</code> with all its dependencies without needing to import or use them inside the script?</p>
","2024-04-15 12:54:43","0","Question"
"78328401","","libtorch_cpu.so: undefined symbol: iJIT_IsProfilingActive","<p>I have created this Conda environment:</p>
<pre class=""lang-bash prettyprint-override""><code>conda env create -f environment.yml
</code></pre>
<p>The <code>environment.yml</code> file:</p>
<pre class=""lang-yaml prettyprint-override""><code>name: deep3d_pytorch
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.6
  - pytorch=1.6.0
  - torchvision=0.7.0
  - numpy=1.18.1
  - scikit-image=0.16.2
  - scipy=1.4.1
  - pillow=6.2.1
  - pip
  - ipython=7.13.0
  - yaml=0.1.7
  - pip:
    - matplotlib==2.2.5
    - opencv-python==3.4.9.33
    - tensorboard==1.15.0
    - tensorflow==1.15.0
    - kornia==0.5.5
    - dominate==2.6.0
    - trimesh==3.9.20
</code></pre>
<p>I activate the Conda environment. But even a simple statement like <code>python -c &quot;import torch; print(torch.__version__)&quot;</code> to get the PyTorch version throws the <code>undefined symbol</code> error:</p>
<pre class=""lang-bash prettyprint-override""><code>(deep3d_pytorch) m3@i7:~/repos/Deep3DFaceRecon_pytorch&gt; python -c &quot;import torch; print(torch.__version__)&quot;
Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/m3/anaconda3/envs/deep3d_pytorch/lib/python3.6/site-packages/torch/__init__.py&quot;, line 189, in &lt;module&gt;
    from torch._C import *
ImportError: /home/m3/anaconda3/envs/deep3d_pytorch/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_IsProfilingActive
</code></pre>
<p>I believe the PyTorch installed by Conda is broken. But the Conda logs are all fine. Does anyone have a clue or hint? I'm receiving the <code>undefined symbol</code> error on both my local machine and on Google Colab.</p>
<h1>Update: minimal env</h1>
<p>Even a minimal Environment like below, would throw similar errors:</p>
<pre class=""lang-bash prettyprint-override""><code>conda create -n minimal_pytorch python=3.6 pytorch torchvision torchaudio -c pytorch
source activate minimal_pytorch &amp;&amp; python -c &quot;import torch; print(torch.__version__)&quot;
</code></pre>
<p>A similar <code>undefined symbol</code> error is thrown:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/envs/minimal_pytorch/lib/python3.6/site-packages/torch/__init__.py&quot;, line 197, in &lt;module&gt;
    from torch._C import *  # noqa: F403
ImportError: /usr/local/envs/minimal_pytorch/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent
</code></pre>
<h2>Python version</h2>
<p>When Python version is omitted while creating the environment:</p>
<pre class=""lang-bash prettyprint-override""><code>conda create -n minimal_pytorch python pytorch torchvision torchaudio -c pytorch
</code></pre>
<p>The error is resolved:</p>
<pre><code>source activate minimal_pytorch &amp;&amp; python -c &quot;import torch; print(torch.__version__)&quot;
</code></pre>
<p>PyTorch version is received without any error:</p>
<pre><code>2.2.2
</code></pre>
","2024-04-15 12:29:49","4","Question"
"78328351","78287979","","<p>You defined your conv layer to output a single layer, while in the original implementation, it outputs <code>64</code>, <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L184"" rel=""nofollow noreferrer"">here</a>. That's where the error comes from, a subsequent batch normalization layer expects <code>64</code>, not <code>1</code>.</p>
","2024-04-15 12:22:30","0","Answer"
"78328334","78328271","","<p>Dataloader adds a batch dimension, it is one of the purposes  of the dataloader. And most pytorch function/layers expect a batched input too.</p>
<p>The issue there is your <code>__getitem__</code> function that should return only ONE sample, not the whole dataset. You need to use the <code>idx</code> argument and return something like <code>data[idx,:,:]</code> and <code>labels[idx, :]</code> instead of the whole data, labels arrays. Otherwise each batch will contain 64 copies of the whole dataset instead of 64 samples, which is of course not what you want.</p>
<p>By the way, you probably do not need the <code>length</code> argument at it is already contained in the shape of your labels or data (this is the first dimension, 10000).</p>
","2024-04-15 12:19:23","1","Answer"
"78328316","78211345","","<p>I had the same issue.
And resolved it by adding <code>&quot;torch&quot;</code> to packages in my setup.py file and also you need to make <code>base='console'</code>. <br>
My old code(didn`t work): <br></p>
<pre><code>from cx_Freeze import setup, Executable

# Dependencies are automatically detected, but it might need
# fine tuning.
build_options = {'packages': [], 'excludes': [], 'include_files': ['settings.ini','design.ui']}

import sys
base = 'Win32GUI' if sys.platform=='win32' else None

executables = [
    Executable('ozvuchator.py', base=base, target_name = 'Ozvuchator')
]

setup(name='test',
      version = '1',
      description = 'no',
      options = {'build_exe': build_options},
      executables = executables)
</code></pre>
<p>My fixed code:</p>
<pre><code>from cx_Freeze import setup, Executable


build_options = {'packages': [&quot;torch&quot;], 'excludes': [], 'include_files': ['settings.ini','design.ui']}

base = 'console'

executables = [
    Executable('ozvuchator.py', base=base)
]

setup(name='test',
      version = '1',
      description = 'no',
      options = {'build_exe': build_options},
      executables = executables)
</code></pre>
","2024-04-15 12:16:01","0","Answer"
"78328271","","Pytorch Dataloader adding a batch dimension","<p>I think this question was already asked a few times but I am yet to find a good answer here.</p>
<p>So I have a Pytorch Dataset that is made from 2 numpy arrays.</p>
<p>The following are the dimensions.</p>
<p>features = [10000, 450, 28] numpy array. dim_0 = the number of sample, dim_1 = time series, dim_2 = features. Basically I have a data that is 450 frames long, where each frame contains 28 features and I have 10000 samples.</p>
<p>label = [10000,450] numpy array. dim_0 = number of samples, dim_1 = label per each frame.</p>
<p>The assignment is that I need to do a classification for each frame.</p>
<p>I created a Pytorch custom Dataset and Dataloader using the following function.</p>
<pre><code>label_length = label.size
label = torch.from_numpy(label)
features = torch.from_numpy(features)

train_dataset = Dataset(label, features, label_length)

train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
</code></pre>
<p>As expected, the train_dataloader.dataset.data returns a tensor of size [10000,450,28]
Great! Now just need to take the batches from the 10000 sample and loop!
So I run a code like below - assume that the optimizers/loss function are all set.</p>
<pre><code>train_loss = 0
EPOCHS = 3
for epoch_idx in range(EPOCHS):
    for i, data in enumerate(train_dataloader):
        inputs, labels = data
        print(inputs.size())
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
</code></pre>
<p>But I get this error:</p>
<blockquote>
<p>ValueError: LSTM: Expected input to be 2D or 3D, got 4D instead</p>
</blockquote>
<p>When I checked the dimension of inputs, it gave  [64 x 10000 x 450 x 28]</p>
<p>Why does dataloader add this dimension of batch? (I understand per documentation it is supposed to do it, but I think it should take 64 samples out of 10000 and create batches and loop over each batch?</p>
<p>I think I am making a mistake somewhere but cannot pin point what I am doing wrong...</p>
<p>EDIT: This is my simple Dataset class</p>
<pre><code>class Dataset(torch.utils.data.Dataset):
    def __init__(self, label, data, length):
        self.labels = label
        self.data = data
        self.length = length

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        # need to create tensor
        #data = torch.from_numpy(self.data)
        #labels = torch.from_numpy(self.labels).type(torch.LongTensor)
        data = self.data
        labels = self.labels
        return data, labels
</code></pre>
","2024-04-15 12:07:05","0","Question"
"78326933","78326643","","<p><strong>TL;DR</strong> Using <code>nn.Parameter</code> <strong>(1)</strong> will both register the tensor in the parameters and in the state dictionary of the model while <code>register_buffer</code> <strong>(3)</strong> will only do the latter. On the contrary, using no wrapper <strong>(2)</strong> will do neither of those two, this shouldn't be adopted if you are trying to learn these parameters. Lastly using <code>register_parameter</code> is the same as <code>nn.Parameter</code>.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""></th>
<th style=""text-align: center;"">method</th>
<th style=""text-align: center;""><a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict"" rel=""nofollow noreferrer""><code>state_dict</code></a></th>
<th style=""text-align: center;""><a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters"" rel=""nofollow noreferrer""><code>parameters</code></a></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;""><a href=""https://pytorch.org/docs/stable/tensors.html"" rel=""nofollow noreferrer""><code>nn.Parameter</code></a></td>
<td style=""text-align: center;"">✅</td>
<td style=""text-align: center;"">✅</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;""><a href=""https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html"" rel=""nofollow noreferrer""><code>torch.Tensor</code></a></td>
<td style=""text-align: center;"">❌</td>
<td style=""text-align: center;"">❌</td>
</tr>
<tr>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;""><a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer"" rel=""nofollow noreferrer""><code>register_buffer</code></a></td>
<td style=""text-align: center;"">✅</td>
<td style=""text-align: center;"">❌</td>
</tr>
<tr>
<td style=""text-align: center;"">3b</td>
<td style=""text-align: center;""><code>register_buffer(persistent=False)</code></td>
<td style=""text-align: center;"">❌</td>
<td style=""text-align: center;"">❌</td>
</tr>
<tr>
<td style=""text-align: center;"">4</td>
<td style=""text-align: center;""><a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_parameter"" rel=""nofollow noreferrer""><code>register_parameter</code></a></td>
<td style=""text-align: center;"">✅</td>
<td style=""text-align: center;"">✅</td>
</tr>
</tbody>
</table></div>
<hr />
<h3>(1) <code>nn.Parameter</code></h3>
<blockquote>
<p><code>nn.Parameter</code> is a subclass of <code>torch.Tensor</code>, they have a very special property when used with <code>nn.Module</code>s [...] they are automatically added to the list of its parameters. <em>Assigning a Tensor doesn’t have such an effect</em>. - docs</p>
</blockquote>
<p>The tensor is wrapped with a <code>nn.Parameter</code> which means it will be registered both in the state dictionary of the instance and the parameter list:</p>
<pre><code>class Model1(nn.Module):
    def __init__(self): 
        super().__init__()
        self.matrix = nn.Parameter(torch.zeros(197, 192))
</code></pre>
<p>Here we define an instance <code>model1</code> and test if the <code>parameters</code> generator is not empty, then look at its state dictionary:</p>
<pre><code>&gt;&gt;&gt; any(True for _ in model1.parameters())
True # not empty

&gt;&gt;&gt; model1.state_dict()
OrderedDict([('matrix', ...)]) # not empty
</code></pre>
<h3>(2) <code>torch.Tensor</code></h3>
<p>In that case, the tensor is just an attribute of the class but has no special treatment and will not be considered a parameter of the model, <em>ie.</em> it won't be traversed to fetch parameters or state:</p>
<pre><code>class Model2(nn.Module):
    def __init__(self): 
        super().__init__()
        self.matrix = torch.zeros(197, 192)
</code></pre>
<p>Same here, we can test whether the parameters list and dictionary are empty or not:</p>
<pre><code>&gt;&gt;&gt; any(True for _ in model2.parameters())
False # empty

&gt;&gt;&gt; model2.state_dict()
OrderedDict() # empty
</code></pre>
<h3>(3) <code>nn.Module.register_buffer</code></h3>
<blockquote>
<p>This is typically used to register a buffer that <strong>should not</strong> be considered a model parameter. [...] Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module’s <code>state_dict</code>. - docs</p>
</blockquote>
<p>Buffers can be accessed as attributes using given names.</p>
<p>We register the tensor in the module buffer:</p>
<pre><code>class Model3(nn.Module):
    def __init__(self): 
        super().__init__()
        self.register_buffer('matrix', torch.zeros(197, 192))
</code></pre>
<p>In this case, notice how it differs, the parameters are empty and the state dictionary is not:</p>
<pre><code>&gt;&gt;&gt; any(True for _ in model3.parameters())
False # empty

&gt;&gt;&gt; model3.state_dict()
OrderedDict([('matrix', ...)]) # not empty
</code></pre>
<p>By default, the parameter will appear in the state but when the argument <code>persistent</code> is set to <code>False</code>, it won't appear anymore:</p>
<pre><code>self.register_buffer('matrix', torch.zeros(197, 192), persistent=False)
</code></pre>
<p>This will lead to an empty <em>dict</em>:</p>
<pre><code>&gt;&gt;&gt; model3.state_dict()
OrderedDict() # empty
</code></pre>
<h3>(4) <code>nn.Module.register_parameter</code></h3>
<blockquote>
<p>Adds a parameter to the module. The parameter can be accessed as an attribute using the given name. - docs</p>
</blockquote>
<p>We register the tensor as a parameter of the model, this is the same as <code>nn.Parameter</code> but the difference is the parameter name can be defined programmatically:</p>
<pre><code>class Model4(nn.Module):
    def __init__(self): 
        super().__init__()
        self.register_parameter('matrix', 
            nn.Parameter(torch.zeros(197, 192)))
</code></pre>
<p>Check on the instance:</p>
<pre><code>&gt;&gt;&gt; any(True for _ in model4.parameters())
True # not empty

&gt;&gt;&gt; model4.state_dict()
OrderedDict([('matrix', ...)])
</code></pre>
<hr />
<p><em>Note</em>: if you set a <code>nn.Parameter</code> with <code>requires_grad</code> to <code>False</code>, that parameter will not be trained even though <strong>it will appear</strong> in the parameters list!</p>
","2024-04-15 08:08:48","4","Answer"
"78326643","","What is the difference between register_parameter(requires_grad=False) and register_buffer in PyTorch?","<p>During deep learning Training, the update of declared values with this variable is done under with torch.no _grad().</p>
<p>When learning with ddp, it's the process of obtaining the average for each batch sample at each gpu, so no synchronization is required. Is there a difference between the two? register_buffer performs worse. It's not like gradient is flowing, so I don't know why the performance difference is significant.</p>
<p>example in model init</p>
<ol>
<li>self.matrix = nn.Parameter(torch.zeros(197, 192).cuda(), requires_grad=False)</li>
<li>self.matrix = torch.zeros(197, 192).cuda()</li>
<li>self.register_buffer('matrix', torch.zeros(197, 192))</li>
</ol>
<p>1 and 3 are same performence, but 2 is different</p>
","2024-04-15 07:12:52","1","Question"
"78326120","","Per-pixel cosine similarity between features of two images","<p>We are given two 3D matrices HxWxC where H,W are the dimensions of the 2D images and C the per-pixel features. We want to calculate the arg-maximum cosine similarity of each pixel feature of the first image with each pixel feature of the second image, or equivalently we need an HxW array that stores the pixel coordinates of the second image that has the maximum cosine similarity for every pixel in the first image.</p>
<p>For small enough H,W and C this can be easily and very efficiently computed with
<code>torch.nn.CosineSimilarity() </code>
after vectorization where you can get an HxWxHxW matrix, and then compute the argmax for the last two rows.</p>
<p>If though, H and W are bigger (specifically let's say we have an 800x800 image) and the feature dimension C is 1500, the previous solution is not memory efficient. Even trying vectorizing per column and using only one for-loop requires more than enough memory.</p>
<p>So, the question is, is there an efficient way timewise and memorywise to compute the positions of the maximum per-pixel cosine similarity for two images in GPU?</p>
<p>Thank you!</p>
","2024-04-15 04:36:19","1","Question"
"78324875","78284866","","<p>It is more efficient applying it on the <code>collate_fn</code> as the tokenizer accepts a list of texts (the batch), specially if the sequences need to be padded at a fixed length, you would be processing batch-wise</p>
","2024-04-14 18:37:04","1","Answer"
"78324614","78323859","","<p>If <code>z</code> is the desired output tensor, then you will have to allocate <code>BxCxN</code> in memory one way or another. An alternative solution is to expand <code>x</code> and <code>y</code> and <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"" rel=""nofollow noreferrer""><code>scatter</code></a> values into a zero tensor:</p>
<pre><code>&gt;&gt;&gt; x, y = x.expand(-1,C,-1), y.expand(-1,C,-1)
&gt;&gt;&gt; z = torch.zeros(B,C,N).scatter_(1,y,x).sum(-1)
</code></pre>
<p>You can check for yourself, but this approach seems to take less memory.</p>
<hr />
<p>Edit: If you are looking to reduce <code>N</code> afterward, then no need for <code>C</code>. Since you were using one-hot-encodings, a standard scatter operation without reduction will suffice. Also, the extra singletons are not needed, so assuming <code>x</code> and <code>y</code> are both <code>BxN</code>:</p>
<pre><code>&gt;&gt;&gt; z = torch.zeros(B,C).scatter_(1,y,x)
</code></pre>
","2024-04-14 17:06:22","1","Answer"
"78324549","","How to keep track of hidden states for different input shapes","<p>I defined a RNN &quot;by hand&quot;, composed of multiple linear layers with pruned connections.</p>
<p>To keep track of the hidden states, I have a variable <code>next_hidden_states</code> in which I save the hidden states at time t, to re-use them at time t+1. This variable is of size <code>(batch_size, N)</code>.</p>
<p>During my training/evaluation, I would like to be able to evaluate the model for inputs with batch size (train the agent) or without batch size (run an episode in the environment). This is usually possible for classic pytorch modules as the batch size is implicit...</p>
<p>I thought about giving the <code>next_hidden_states</code> as an argument and output of the network, but it is quite inelegant.</p>
<p><strong>Edit</strong></p>
<p>Here is a minimal version of my code</p>
<pre><code>import numpy as np

import torch
import torch.nn.utils.prune as prune
import torch.nn as nn


class BrainRNN(nn.Module):
    def __init__(self, activation=torch.sigmoid, batch_size=8):
        super(BrainRNN, self).__init__()
        self.n_neurons = 3*4
        self.activation = activation
        self.batch_size = batch_size
        self.reset_hidden_states()

        # Create the input layer
        self.input_layer = nn.Linear(4, 4)

        # Create forward hidden layers
        self.hidden_layers = nn.ModuleList([])
        new_layer = nn.Linear(4,4)
        mask = np.ones((4,4))-np.eye(4)
        prune.custom_from_mask(new_layer, name='weight', mask=torch.tensor(mask.T)) # delete fictive connections
        self.hidden_layers.append(new_layer)

        # Create the backward weights
        self.recurrent_layers = nn.ModuleList([]) # recurrent_layers[i](hidden_states) = layer j&gt;i to i

        new_layer = nn.Linear(self.n_neurons, 4, bias=False) # no bias for backward connection
        mask = np.zeros((12,4))
        mask[1,0] = 1
        prune.custom_from_mask(new_layer, name='weight', mask=torch.tensor(mask.T)) # delete fictive connections
        self.recurrent_layers.append(new_layer)

        # Create the output layer
        self.output_layer = nn.Linear(4,4)

    def forward(self, x):
        next_hidden_states = torch.empty(x.shape[0], self.n_neurons) if x.dim() &gt; 1 else torch.empty(self.n_neurons)
        skips = [] # list of current states for skip connections

        # Input layer
        x = self.activation(self.input_layer(x) + self.recurrent_layers[0](self.hidden_states))
        next_hidden_states[...,[0,1,2,3]] = x

        # Hidden layers
        x = self.hidden_layers[0](x)
        x = self.activation(x)
        next_hidden_states[...,[4,5,6,7]] = x

        # Output layer
        x = self.output_layer(x) # no activation nor recurrent/skip connection for the last one
        
        self.hidden_states = next_hidden_states

        return x

    def reset_hidden_states(self, hidden_states=None):
        if self.batch_size &gt; 0:
            self.hidden_states = nn.init.normal_(torch.empty(self.n_neurons), std=1).repeat(self.batch_size,1) # same hidden states for all batches
        else:
            self.hidden_states = nn.init.normal_(torch.empty(self.n_neurons), std=1)

nn = BrainRNN()
nn(torch.zeros(8,4)) # works well
nn(torch.zeros(4)) # shape issue at next_hidden_states[...,[0,1,2,3]] = x
</code></pre>
<p>where there are 3 layers of 4 nodes each, with a recurrent connexion between hidden layer and input layer, and some pruned connections.</p>
<p>The aim is to be able, if <code>nn = BrainRNN(...)</code>, to evaluate <code>nn(torch.zeros((B,4)))</code> as well as <code>nn(torch.zeros(4))</code>.
Ideally, I would like to reproduce the behavior of classic nn.Modules, but I don't really know how to do so while saving the states...</p>
","2024-04-14 16:46:00","0","Question"
"78323859","","Broadcast pytorch array across channels based on another array","<p>I have two arrays, <code>x</code> and <code>y</code>, with the same shape (<code>B 1 N</code>).</p>
<p><code>x</code> represents data and <code>y</code> represents which class (from <code>1</code> to <code>C</code>) each datapoint in <code>x</code> belongs to.</p>
<p>I want to create a new tensor <code>z</code> (with shape <code>B C</code>) where</p>
<ol>
<li>the data in <code>x</code> are partitioned into channels based on their classes in <code>y</code></li>
<li>and summed over <code>N</code></li>
</ol>
<p>I can accomplish this if I use a one-hot encoding. However, for large tensors (especially with a large number of classes), PyTorch's one-hot encoding quickly uses up all memory on the GPU.</p>
<p>Is there a more memory-efficient way to do this broadcasting without explicitly allocating a <code>B C N</code> tensor?</p>
<p>Here's an MWE of what I'm after:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

B, C, N = 2, 10, 1000

x = torch.randn(B, 1, N)
y = torch.randint(low=0, high=C, size=(B, 1, N))

one_hot = torch.nn.functional.one_hot(y, C)  # B 1 N C
one_hot = one_hot.squeeze().permute(0, -1, 1)  # B C N

z = x * one_hot  # B C N
z = z.sum(-1)  # B C
</code></pre>
","2024-04-14 12:59:45","0","Question"
"78322538","","ModuleNotFoundError: No module named 'torch' on AWS Batch GPU instance","<p>I have a job that runs on AWS Batch on a GPU instance, my application uses torch, i.e.</p>
<pre><code>import torch
</code></pre>
<p>The Compute Environment has only one GPU instance, I was able to confirm that <code>torch</code> is available there by connecting to the instance via AWS Console and ran:</p>
<pre><code>sh-4.2$ python3
Python 3.7.16 (default, Aug 30 2023, 20:37:53)
[GCC 7.3.1 20180712 (Red Hat 7.3.1-15)] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; print(torch.__version__)
1.13.1+cu117
&gt;&gt;&gt;
</code></pre>
<p>However when I submit my Batch job, it fails saying</p>
<pre><code> ModuleNotFoundError: No module named 'torch'
</code></pre>
<p>I looked into this and found that:
My AWS Batch job definition has this Command:</p>
<pre><code>  CoolGPUJobDefinition:
    DependsOn: ComputeRole
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      ContainerProperties:
        Command:
          - &quot;/opt/prod/bin/python3&quot;
          - &quot;/opt/prod/bin/start.py&quot;
</code></pre>
<p>From the stacktrace of my application, it shows:</p>
<pre><code>File &quot;/opt/prod/lib/python3.8/site-packages/cool_service/slowfast/utils/distributed.py&quot;, line 9, in &lt;module&gt;
import torch
ModuleNotFoundError: No module named 'torch'
</code></pre>
<p>But when I tried to <code>ls -a</code> in the <code>/opt</code> directory of this GPU instance, it didn't even have <code>prod</code>:</p>
<pre><code>sh-4.2$ pwd
/opt
sh-4.2$ ls -a
.  ..  aws  containerd  nvidia
</code></pre>
<p>Somehow there's a disconnect from what I can see in the GPU instance and how AWS Batch runs my application on this GPU instance.</p>
<p>I'd like to understand:</p>
<ol>
<li>Why this disconnect/discrepancy?</li>
<li>How could I resolve this module not found error in this case?</li>
</ol>
<p>Thanks!</p>
","2024-04-14 02:31:57","0","Question"
"78321818","78319557","","<p>I <strong>think</strong> this happens because you're moving tensors around without updating the autograd engine.</p>
<p>Consider this example:</p>
<pre><code>D = 5
x = torch.ones((D,), device=&quot;cuda&quot;)
a = 2 * torch.ones((D,), device=&quot;cuda&quot;, requires_grad=True)
q = a # keep a reference to a
b = 3 * torch.ones((D,), device=&quot;cuda&quot;, requires_grad=True)
y = a*x + b
s = y.sum()


a = a.cpu()
a = a.cuda()

s.backward(inputs=[a])
print(a.grad) # --&gt; None
s.backward(inputs=[q])
print(a.grad) # --&gt; None
print(q.grad) # --&gt; tensor([1., 1., 1., 1., 1.], device='cuda:0')

</code></pre>
<p>So, when you write <code>a = a.cpu()</code> or <code>a = a.cuda()</code>, you get a copy of <code>a</code> when a move to different device memory occurs. However, the underlying graph for back-propagation is not automatically updated to reflect the new address of the tensor in the new memory type. You can see that this is true because computing the gradient with respect to <code>q</code>, which refers to the original location of <code>a</code> works as expected.</p>
<p>In your case, when you call <code>model.cpu()</code> or <code>model.cuda()</code>, the same will happen to alll <code>nn.Parameter</code> objects that are attributes of <code>model</code>. Since <code>self.temp_weight</code> is not updated to reflect the new location of <code>model.weight</code>, autograd behaves unexpectedly.</p>
<p>However, note that this is just a part of the full picture, because if you try to save a reference to a <code>nn.Parameter</code> in <code>model</code>, like</p>
<pre><code>q = model.weight
</code></pre>
<p>it doesn't work. So clearly, there's something else going on in addition to the logic I've outlined above.</p>
<p><strong>EDIT:</strong></p>
<p>Wrapping <code>self.temp_weight</code> in a nn.Parameter is enough to solve this:</p>
<pre><code>model.temp_weight = nn.Parameter(model.weight * model.weight_mul)
</code></pre>
<p><strong>EDIT2:</strong></p>
<p>Wrapping in nn.Module:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()

    def forward(self, w1, w2):
        return w1 * w2


class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.weight_mul = nn.Parameter(2*torch.ones(D,), requires_grad=True)
        self.weight = nn.Parameter(3*torch.ones(D,), requires_grad=True)

    def forward(self, x):
        x = x * self.temp_weight(self.weight, self.weight_mul)
        return x

model.temp_weight = MyModule()
</code></pre>
","2024-04-13 19:41:03","0","Answer"
"78320418","78317978","","<p>You can compute the batched p-norm with <a href=""https://pytorch.org/docs/stable/generated/torch.cdist.html"" rel=""nofollow noreferrer""><code>torch.cdist</code></a>, it operates between <code>x1</code> of shape <code>B×P×M</code> and <code>x2</code> of shape <code>B×R×M</code>, returning a tensor shaped <code>B×P×R</code>. Which means the common dimension <code>M</code> is the one reduced. First, unsqueeze one singleton dimension on both inputs to turn them 3D, then apply the function:</p>
<pre><code>&gt;&gt;&gt; torch.cdist(a[None], b[None]).shape
&gt;&gt;&gt; torch.Size([1, 1600, 128])
</code></pre>
","2024-04-13 11:27:42","1","Answer"
"78320397","","RuntimeError: Library cublas64_12.dll is not found or cannot be loaded. While using WhisperX diarization","<p>I was trying to use whisperx to do speaker diarization. I did it sucessfully on google colab but I'm encountering this error while tyring to transcribe the audio file.</p>
<p><code>Traceback (most recent call last): File &quot;D:\Programming\Python\Projects\Conversation-Analyser\Conversation Analyser\Classes\diarization.py&quot;, line 42, in &lt;module&gt; diarize() File &quot;D:\Programming\Python\Projects\Conversation-Analyser\Conversation Analyser\Classes\diarization.py&quot;, line 40, in diarize result = model.transcribe(audio, batch_size=batch_size) File &quot;D:\Programming\Python\Projects\Conversation-Analyser\.venv\lib\site-packages\whisperx\asr.py&quot;, line 194, in transcribe language = language or self.detect_language(audio) File &quot;D:\Programming\Python\Projects\Conversation-Analyser\.venv\lib\site-packages\whisperx\asr.py&quot;, line 252, in detect_language encoder_output = self.model.encode(segment) File &quot;D:\Programming\Python\Projects\Conversation-Analyser\.venv\lib\site-packages\whisperx\asr.py&quot;, line 86, in encode return self.model.encode(features, to_cpu=to_cpu) RuntimeError: Library cublas64_12.dll is not found or cannot be loaded </code></p>
<p>I'm did <code>pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118</code></p>
<p>I'm trying to do speaker diarization. While at the transcription phase, I'm encountering this error. THis is the code:
<code>   model = whisperx.load_model(&quot;large-v2&quot;, device, compute_type=compute_type, download_root=model_dir) result = model.transcribe(audio, batch_size=batch_size)</code></p>
","2024-04-13 11:18:29","2","Question"
"78319557","","Backward(inputs=) doesn’t work when the model is moved between devices","<pre><code>import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.weight_mul = nn.Parameter(torch.randn(D,))
        self.weight = nn.Parameter(torch.randn(D,))

    def forward(self, x):
        x = x * self.temp_weight
        return x

D = 5

x = torch.randn(D,).cuda()
model = Model()
model.cuda()
model.temp_weight = model.weight * model.weight_mul
model.cpu(); model.cuda()
output = model(x)
output.sum().backward(inputs=[model.weight, model.weight_mul])

print(model.weight.grad)
print(model.weight_mul.grad)
</code></pre>
<p>To my surprise, the <code>.grad</code> are None. There are two ways to get the <code>backward()</code> to work. One is to remove the <code>inputs</code> to <code>backward()</code>, the other is to remove <code>model.cpu(); model.cuda()</code>. But why?</p>
","2024-04-13 04:47:52","1","Question"
"78318431","78314572","","<p>Your model is too big or your input is too big. You do not have much choice. Use a smaller model or use smaller inputs. 2448x2448x3 is usually a very big array for most networks. If you work with images they often take images like 224x224 or 512x512 as input, so you need to resize or do tiling.</p>
","2024-04-12 19:41:55","3","Answer"
"78317978","","PyTorch L2-norm between 2 tensors of different shapes","<p>I have 2 tensors in PyTorch:</p>
<pre><code>a.shape, b.shape
# (torch.Size([1600, 2]), torch.Size([128, 2]))
</code></pre>
<p>I want to compute L2-norm distance between each of the 128 values in 'b' having 2-dim values from all 1600 values in 'a'. Currently, I have an inefficient for loop to do it for each values in b as follows:</p>
<pre><code># Need to compute l2-norm squared dist b/w each b from a-
l2_dist_squared = list()

for bmu in bmu_locs:
    l2_dist_squared.append(torch.norm(input = a.to(torch.float32) - b, p = 2, dim = 1))

l2_dist_squared = torch.stack(l2_dist_squared)

# l2_dist_squared.shape
# torch.Size([128, 1600])
</code></pre>
<p>Is there a better way to do as a one liner?</p>
","2024-04-12 17:54:44","0","Question"
"78317551","78315964","","<p>If you look at the model description by printing it, you will see the fully connected classifier layer as a key name of <code>&quot;head&quot;</code>, not <code>&quot;heads&quot;</code>. The following code works on my end:</p>
<pre><code>for parameter in pretrained_vit.parameters():
    parameter.requires_grad = False
pretrained_vit.head = nn.Linear(in_features=192, out_features=10)
pretrained_vit(torch.rand(1,3,224,224)).mean().backward()
</code></pre>
<p>I recommend using <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.requires_grad_"" rel=""nofollow noreferrer""><code>nn.Module.requires_grad_</code></a> instead of setting the attribute yourself on each tensor parameter. Keep in mind, with your current code, the whole model will be frozen, including the classifier layer, as such you might want to unfreeze that layer:</p>
<pre><code>pretrained_vit.requires_grad_(False)
pretrained_vit.head = nn.Linear(in_features=192, out_features=10)
pretrained_vit.head.requires_grad_(True)
</code></pre>
","2024-04-12 16:16:00","1","Answer"
"78317274","78316874","","<p>You will notice the second returned element is in fact <code>None</code> when <code>need_weights=False</code>.</p>
","2024-04-12 15:25:35","1","Answer"
"78317211","78316485","","<p>I don't believe you can get away with <code>torch.unique</code> because it won't work per column. Instead of iterating over <code>dim=1</code> you could construct three mask tensors to check for <code>-1</code>, <code>0</code>, and <code>1</code> values, respectively. To compute the resulting column mask, you can get away with some basic logic when combining the masks:</p>
<p>Considering you only check on the last timestep, focus on that and flatten the spatial dimensions:</p>
<pre><code>x_ = x[-1].flatten(1)
</code></pre>
<p>The three masks to identify <code>-1</code>, <code>0</code>, and <code>1</code> conditions can be obtained with: <code>x_ == -1</code>, <code>x_ == 0</code>, and <code>x_ == 1</code>, respectively. Combine them with <a href=""https://pytorch.org/docs/stable/generated/torch.logical_or.html"" rel=""nofollow noreferrer""><code>torch.logical_or</code></a></p>
<pre><code>mask = (x_ == -1).logical_or(x_ == 0).logical_or(x_ == 1)
</code></pre>
<p>Finally, check that all elements are <code>True</code> across rows:</p>
<pre><code>keep_indices = mask.all(dim=1)
</code></pre>
","2024-04-12 15:14:58","0","Answer"
"78316874","","torch.nn.MultiheadAttention with need_weights=False still returns a tuple of length two, is it expected","<p>For an object of <code>nn.MultiheadAttention</code> class when I use the forward method with <code>need_weights=False</code>, it still returns a tuple of length two. Is it expected? I was expecting it to only return the <code>attn_output</code>.</p>
<p>See the toy example below:</p>
<pre><code>import torch
import torch.nn as nn

# Create an instance of MultiheadAttention
embed_dim = 64
num_heads = 8
multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)

query = torch.randn(10, 5, embed_dim)  # (seq_len, batch_size, embed_dim)
key = torch.randn(10, 5, embed_dim)
value = torch.randn(10, 5, embed_dim)

# Call the forward method
output = multihead_attn(query, key, value, need_weights=False)
len(output)

# I should do
attn_output = output[0]
</code></pre>
","2024-04-12 14:18:59","0","Question"
"78316541","78315374","","<p>No, that is not possible.</p>
<p>According to the documentation, CLIP is designed for image-text similarity and zero-shot image classification, which are tasks completely different from image generation. For any given image, the model classifies it using a list of provided prompts, which defines the classes. The image must always be used as the input.</p>
<p>It seems you might be looking for a different type of model architecture. There are several text-to-image model architectures available, most notably the diffusion models such as <a href=""https://huggingface.co/docs/diffusers/using-diffusers/sdxl"" rel=""nofollow noreferrer"">Stable Diffusion</a>.</p>
","2024-04-12 13:20:54","1","Answer"
"78316485","","Optimization of pytorch function to eliminate for loop","<p>lately I have been developing a function capable of dealing with tensors with dimension:</p>
<p>torch.Size([51, 265, 23, 23])</p>
<p>where the first dim is time, the second is pattern and the last 2 are pattern size.</p>
<p>Each individual pattern can have a maximum of 3 states: [-1,0,1], and it is considered 'alive'
meanwhile a pattern is 'dead' in all other cases where it doesn't have all 3 states.</p>
<p>my objective is to filter all the dead patterns by checking the last row (last time step) of the tensor.</p>
<h1>My current implementation (that works) is:</h1>
<pre><code>def filter_patterns(tensor_sims):

   # Get the indices of the columns that need to be kept
   keep_indices = torch.tensor([i for i in 
   range(tensor_sims.shape[1]) if 
   tensor_sims[-1,i].unique().numel() == 3])

   # Keep only the columns that meet the condition
   tensor_sims = tensor_sims[:, keep_indices]

   print(f'Number of patterns: {tensor_sims.shape[1]}')
   return tensor_sims
</code></pre>
<p>Unfortunately I'm not able to get rid of the for loop.</p>
<p>I tried to play around with the torch.unique() function and with the parameter dim, I tried reducing the dimensions of the tensor and flattening, but nothing worked.</p>
<h1>Found Solution (thanks to the answer):</h1>
<pre><code>def filter_patterns(tensor_sims):
   # Flatten the spatial dimensions of the last timestep
   x_ = tensor_sims[-1].flatten(1)

   # Create masks to identify -1, 0, and 1 conditions
   mask_minus_one = (x_ == -1).any(dim=1)
   mask_zero = (x_ == 0).any(dim=1)
   mask_one = (x_ == 1).any(dim=1)

   # Combine the masks using logical_and
   mask = 
   mask_minus_one.logical_and(mask_zero).logical_and(mask_one)

   # Keep only the columns that meet the condition
   tensor_sims = tensor_sims[:, mask]

   print(f'Number of patterns: {tensor_sims.shape[1]}')
   return tensor_sims
</code></pre>
<p>the new implementation is extremely faster.</p>
","2024-04-12 13:05:41","1","Question"
"78315964","","Loading pre-trained weights properly in Pytorch","<p>I would like to perform transfer learning by loading a pretrained vision transformer model, modify its last layer and training it with my own data.</p>
<p>Hence, I am loading my dataset perform the typical transformation similar to the ImageNet, then, load the model, disable the grad from all its layer remove the last layer and add a trainable one using the number of classes of my dataset. My code could look like as follows:</p>
<pre><code>#retrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, &quot;DEFAULT&quot; means best available
#pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)
pretrained_vit = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True).to(device)

for parameter in pretrained_vit.parameters():
    parameter.requires_grad = False

pretrained_vit.heads = nn.Linear(in_features=192, out_features=len(class_names)).to(device)
optimizer(torch.optim.Adam(params=pretrained_vit.parameters(), ... )
loss_fn = torch.nn.CrossEntropyLoss()

esults = engine.train(model=pretrained_vit, ..., ... )
</code></pre>
<p>When I am using <code>torchvision.models.ViT_B_16_Weights.DEFAULT</code> then the code works smoothly and I can run my code without any problem. However, when I am using instead the <code>deit_tiny_patch16_224</code> and I set the <code>requires_grade = False</code>  then I got the following error:</p>
<pre><code>Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>When the varialble is set to True, the code works smoothly but ofc the training is really bad since I had a very small amount of pictures. How, can I set properly the <code>deit_tiny_patch16_224</code> parametes to <code>parameter.requires_grad = False</code>?</p>
<p>Is there an issue with the way I am loading the pre-trained weights?</p>
","2024-04-12 11:27:08","1","Question"
"78315374","","ValueError: You have to specify pixel_values","<p>Is it possible to generate an image using the CLIP model without giving a reference image? I tried to follow the documentation and came up with this:</p>
<pre><code>import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image

# Load CLIP model and processor
model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

prompts = [
    &quot;a cat&quot;,
]

for i, prompt in enumerate(prompts):
    with torch.no_grad():
        outputs = model(prompt)
        image_features = outputs.pixel_values

    # Convert image features to image
    image = Image.fromarray(image_features[0].numpy())

    image.save(f&quot;generated_image_{i}.png&quot;)
</code></pre>
<p>but I get this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;clip.py&quot;, line 20, in &lt;module&gt;
    outputs = model(**inputs)
  File &quot;/Users/x/.pyenv/versions/clip/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/Users/x/.pyenv/versions/clip/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/Users/x/.pyenv/versions/clip/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py&quot;, line 1110, in forward
    vision_outputs = self.vision_model(
  File &quot;/Users/x/.pyenv/versions/clip/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/Users/x/.pyenv/versions/clip/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/Users/x/.pyenv/versions/clip/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py&quot;, line 847, in forward
    raise ValueError(&quot;You have to specify pixel_values&quot;)
</code></pre>
<p>Docs: <a href=""https://huggingface.co/docs/transformers/en/model_doc/clip"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/en/model_doc/clip</a></p>
","2024-04-12 09:36:02","0","Question"
"78314572","","How to solve ' OutOfMemoryError: CUDA out of memory' in pytorch?","<p>In Colab I am predicting array of 2448x2448 with 7 classes with trained model(input= (2448, 2448, 3) and output= (2448, 2448, 7).</p>
<pre><code>for idx in range(len(test_dataset)):

image, gt_mask = test_dataset[idx]
image_vis = test_dataset_vis[idx][0].astype('uint8')
x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)
# Predict test image
pred_mask = best_model(x_tensor)
pred_mask = pred_mask.detach().squeeze().cpu().numpy()
# Convert pred_mask from `CHW` format to `HWC` format
pred_mask = np.transpose(pred_mask,(1,2,0))
# Get prediction channel corresponding to foreground
pred_urban_land_heatmap = pred_mask[:,:,select_classes.index('urban_land')]
pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)
# Convert gt_mask from `CHW` format to `HWC` format
gt_mask = np.transpose(gt_mask,(1,2,0))
gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)
cv2.imwrite(os.path.join(sample_preds_folder, f&quot;sample_pred_{idx}.png&quot;), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])

visualize(
    original_image = image_vis,
    ground_truth_mask = gt_mask,
    predicted_mask = pred_mask,
    pred_urban_land_heatmap = pred_urban_land_heatmap
)
</code></pre>
<p>But I get</p>
<blockquote>
<p>OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 357.06 MiB is free. Process 224843 has 14.40 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 344.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (<a href=""https://pytorch.org/docs/stable/notes/cuda.html#environment-variables"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/notes/cuda.html#environment-variables</a>)</p>
</blockquote>
<p>I have GPU of 15 GBs in colab, but when I reach to this line</p>
<pre><code>pred_mask = best_model(x_tensor)
</code></pre>
<p>the allocated memory of GPU spikes to top in the allocation graph.</p>
","2024-04-12 06:37:36","1","Question"
"78313047","78312158","","<p>Perhaps you could add an additional penalty to the loss that incentivises higher variance for feature 2.</p>
<p>One way of doing this is to directly incorporate <code>sigma_feature2</code> into the loss term, e.g. <code>loss_total = loss_original + lambda * 1 / sigma_feature2**2</code>, where <code>sigma_feature2</code> is the the variance parameter of the latent representation for feature 2.</p>
<p>Alternatively, you could add an activation loss term that looks at the output activations for feature 2, and adds <code>lambda * 1 / feature2_activations.var()</code> to the loss term.</p>
<p>Both methods above obtain a measure of the degree of variance related to feature 2, and calculate <code>1 / variance</code> before adding it to the loss. This means that when the variance is low, the loss increases and the model compensates by learning to increase the variance for that feature.</p>
<p><code>lambda</code> would need to be tuned to achieve a balance between accurate and normally-distributed reconstructions for feature 2 (small <code>lambda</code>), whilst still getting your desired level of variance for that feature (larger <code>lambda</code>).</p>
","2024-04-11 21:12:07","1","Answer"
"78312158","","Give more weights to some input features for variational auto encoder","<p>I made a variation auto encoder to augment data.</p>
<p>It is working fine, but I would like some columns in the augmented data to have more variations  than the others.</p>
<p>For example, I actually have this data generated:</p>
<pre><code>100  16 2.6
105  16.6 2.7
110  16.7 2.8
</code></pre>
<p>You will tell me: it is normal, you don't have enough real input data in a big range for the second column around 105 value for the first column. That's true but my second column is important and here is what looks like my input data important :</p>
<pre><code>100 16 2.5
110 20 3.5
120 30 3.7
130 40 4 
200 80 7 
.....
</code></pre>
<p>I would like give more weight to the second column to vary in a full  range of the real input data from 1 to 100, and have a result like this:</p>
<pre><code>100 20  3
105 50  3.5
110 80  6
....
</code></pre>
<p>EDIT:
I tried it ... not really good :</p>
<pre><code>class customLoss(nn.Module):
    def __init__(self):
        super(customLoss, self).__init__()
        self.mse_loss = nn.MSELoss(reduction=&quot;sum&quot;)
    
    def forward(self, x_recon, x, mu, logvar):
        loss_MSE = self.mse_loss(x_recon, x)
        #   Kullback-Leibler (KL) divergence
        loss_KLD = -0.5 * torch.sum(1. + logvar - mu.pow(2) - logvar.exp())
        # Variance penalty for feature 2
        variance_penalty = torch.mean(logvar[:, 2])  # Average logvar for feature 2

        loss_original = loss_MSE + loss_KLD - variance_penalty * 2.
        return loss_original
</code></pre>
","2024-04-11 17:39:18","-4","Question"
"78311839","78311435","","<p>What you could do is clip the indices such that they do not go above <code>B.size(1)</code> using a <a href=""https://pytorch.org/docs/stable/generated/torch.scatter.html"" rel=""nofollow noreferrer"">scattering operation</a>, in this case, the last element will override the other (only the 2nd <code>2</code> will be kept). You also have specialized functions to accumulate via summation or reduce to the maximum value. Let's try this:</p>
<pre><code>torch.zeros(2,6).scatter_(1, C.clip(0,5), A)
tensor([[ 0.0000,  0.4821,  0.0915, -0.1870,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0704,  1.3817,  0.0000,  2.1717]])
</code></pre>
<p>But this won't work all the time because indices that are over the max length will be placed at the end. A solution might be to concatenate an additional buffer column to account for the undesired values and then trim the tensor at the end to discard those:</p>
<pre><code>torch.zeros(2,6+1).scatter_(1, C.clip(0,6), A)[:,:-1]
tensor([[ 0.0000,  0.4821,  0.0915, -0.1870,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0704,  1.3817,  0.0000,  2.1717]])
</code></pre>
<p>If you want to reduce your scattering differently, you can use <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_"" rel=""nofollow noreferrer""><code>torch.scatter_reduce_</code></a> and specify the <code>reduce</code> argument:</p>
<blockquote>
<p><code>reduce</code> (<code>str</code>) – the reduction operation to apply for non-unique indices (<code>&quot;sum&quot;</code>, <code>&quot;prod&quot;</code>, <code>&quot;mean&quot;</code>, <code>&quot;amax&quot;</code>, <code>&quot;amin&quot;</code>)</p>
</blockquote>
<p>For example to get the maximum:</p>
<pre><code>torch.zeros(2,7).scatter_reduce_(1, C.clip(0,6), A, reduce='amax')[:,:-1]
</code></pre>
","2024-04-11 16:34:35","1","Answer"
"78311435","","projecting values of tensor A into tensor B at indices C (pytorch)","<p>Given:</p>
<pre><code>A = tensor([[ 0.4821, -0.3484,  0.0915, -0.1870],
            [ 1.3817,  0.3011,  1.0704,  2.1717]])

B = torch.zeros(2,6)

C =  torch.tensor([[1,2,2,3], [3,7,2,5]]) (same shape of A)
</code></pre>
<p>I want to replace values in B by A at indices C where &lt; 6 <code>(B.size(-1))</code></p>
<pre><code>-&gt; B =[[0, 0.4821, 0.0915, -0.1870, 0, 0],
       [0, 0, 1.0704, 1.3817, 0, 2.1717]]
</code></pre>
<p>Notice that: there are two 2 in the first row of C at the second and third position in A. Here I want to get the max (or sum if you think it's more possible to do)</p>
","2024-04-11 15:20:43","0","Question"
"78310411","78297385","","<p>The batch size is batch size per device. The CUDA OOM error is most likely because a batch size of 256 is too big. Trying a smaller batch size like 32 or 64 will solve the issue. The effective batch size of your code will be <code>batch_size_per_device x num_nodes x num_gpus</code></p>
","2024-04-11 12:41:05","1","Answer"
"78308332","78289901","","<p>One potential solution I found later on using .children():</p>
<pre><code>for layer in tm.children():
    print(layer.state_dict())
</code></pre>
","2024-04-11 05:27:59","0","Answer"
"78307385","78295146","","<p>This is due to a compatibility issue between PyTorch and Python, the &quot;Could not find a version that satisfies the requirement&quot; refers to the currently installed Python version not being compatible (because too recent) with PyTorch version <code>1.9.0</code>. PyTorch releases try to keep up with newer Python versions, but they cannot maintain older PyTorch versions to work with newly released Python versions. It turns out, that back when <code>1.9.0</code> was released, PyTorch was not providing support for Python <code>3.8</code> (probably because it was not released yet!).</p>
<p>So you have two options:</p>
<ul>
<li>either downgrade Python to something compatible with PyTorch <code>1.9.0</code>. <a href=""https://github.com/pixray/pixray/issues/80#issuecomment-1426258790"" rel=""nofollow noreferrer"">According to the author</a> of the <em>pixray</em> repository you linked, Python <code>3.7</code> should be working.</li>
<li>Or you can upgrade the PyTorch version: with your current Python <code>3.8</code>, you can safely install the latest PyTorch <code>2.2.2</code>. In that case, you risk breaking other dependencies in the installation.</li>
</ul>
<p>I recommend going with the first option as it is the safest if you are just trying to run their code.</p>
","2024-04-10 22:21:57","1","Answer"
"78307369","78307251","","<p>When you call <code>.float()</code> on a tensor, you return a new object.</p>
<p>When you run <code>print(xq_train_tensor.type())</code> at the end of your code, you are referencing the old object, which is still double type.</p>
<p>If you want the variable <code>xq_train_tensor</code> to be updated to float type, you need to reassign the variable itself.</p>
<p>ie <code>xq_train_tensor = xq_train_tensor.float()</code></p>
<p>If you want to use the list iteration format, you can save the new float tensors to a new list. However, this won't update the <code>xq_train_tensor</code> varaible, as it still points to the old tensor.</p>
<pre class=""lang-py prettyprint-override""><code>output_tensors = []
for i, tensor in enumerate(flat_tensor_list):
    output_tensors.append(tensor.float())

for i, tensor in enumerate(output_tensors):
    print(f&quot;{i}: {tensor.type()}&quot;)
</code></pre>
","2024-04-10 22:18:31","0","Answer"
"78307251","","How to convert the tensor type of a list of pytorch tensors using a for loop","<p>I'm trying to convert the type of tensors from DoubleTensor to FloatTensor. However, it seems like the tensors aren't being converted in my code. How can I convert the tensors using a for loop?</p>
<pre><code>train_tensorset = [xq_train_tensor,y_train_tensor]
val_tensorset = [xq_val_tensor,y_val_tensor]
test_tensorset = [xq_test_tensor,y_test_tensor]

tensor_list = [train_tensorset,val_tensorset,test_tensorset]
flat_tensor_list = list(itertools.chain.from_iterable(tensor_list))
print(f&quot;Num tensors:{len(flat_tensor_list)}&quot;)

for i, tensor in enumerate(flat_tensor_list):
    tensor = flat_tensor_list[i].float()
    print(f&quot;{i}: {tensor.type()}&quot;) 

print(xq_train_tensor.type())
</code></pre>
<p>To verify that the tensors are being converted I check the type of <code>xq_train_tensor</code>, which is a part of <code>flat_tensor_list</code>. This was the output of the code above:</p>
<pre><code>Num tensors:6
0: torch.FloatTensor
1: torch.FloatTensor
2: torch.FloatTensor
3: torch.FloatTensor
4: torch.FloatTensor
5: torch.FloatTensor
xq_train_tensor: torch.DoubleTensor
</code></pre>
<p>Even though all items in <code>flat_tensor_list</code> are being converted in the for loop, it doesn't seem like the tensors are actually being converted.</p>
","2024-04-10 21:40:15","0","Question"
"78305088","78305085","","<p>From <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.ndim.html"" rel=""nofollow noreferrer"">PyTorch</a> the number of dimensions of a tensor is stored in the <code>.ndim</code> attribute of the tensor, or you can call the <code>.dim()</code> function.</p>
<p>If you have</p>
<pre><code>t = torch.rand((2,3,4,5))
</code></pre>
<p>you get the number of dimensions with</p>
<pre><code>t.ndim 
# 4
</code></pre>
<p>or</p>
<pre><code>t.dim()
# 4
</code></pre>
","2024-04-10 14:18:44","2","Answer"
"78305085","","How do I get the number of dimensions (aka order or degree) of a tensor in PyTorch?","<p>If I have a PyTorch tensor such as</p>
<pre class=""lang-py prettyprint-override""><code>t = torch.rand((2,3,4,5))
</code></pre>
<p>how do I get the number of dimensions of this tensor? In this case, it would be 4.</p>
","2024-04-10 14:18:16","1","Question"
"78302959","78302924","","<p><code> torch.meshgrid(x, y)</code> is used to create a grid of coordinates based on two 1-dimensional tensors, <a href=""https://pytorch.org/docs/stable/generated/torch.meshgrid.html"" rel=""nofollow noreferrer"">More Here</a></p>
<p><code>x</code> represents the values along the <code>rows</code> and <code>y</code> represents the values along the <code>columns</code></p>
<p>If you want to see expected just swap x and y</p>
<pre><code>import torch

x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5, 6])


torch.meshgrid(y, x)
</code></pre>
<p>You get#</p>
<pre><code>tensor([[1, 2, 3],
        [1, 2, 3],
        [1, 2, 3]])
tensor([[4, 4, 4],
        [5, 5, 5],
        [6, 6, 6]])
</code></pre>
<p>More like</p>
<pre><code>  y/x  1   2   3
       ___________
    4 | 1 | 2 | 3 |
       -----------
    5 | 1 | 2 | 3 |
       -----------
    6 | 1 | 2 | 3 |
       -----------
</code></pre>
<p><code>y</code> are rows and <code>x</code> are col. Each cell contains a pair of coordinates <code>(x, y)</code></p>
","2024-04-10 08:14:04","3","Answer"
"78302948","78302924","","<p>It all depends on the indexing mode you use: <code>ij</code> or <code>xy</code> (default is <code>ij</code>):</p>
<pre><code>&gt;&gt;&gt; torch.meshgrid(x, y, indexing='xy')
(tensor([[1, 2, 3],
         [1, 2, 3],
         [1, 2, 3]]),
 tensor([[4, 4, 4],
         [5, 5, 5],
         [6, 6, 6]]))
</code></pre>
<p>Here is a visualization of the two mesh grids:</p>
<p><a href=""https://i.sstatic.net/uuTOA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uuTOA.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>If you are not interested in this structure and prefer the expected result, you can use a combination of stack, transpose and repeat:</p>
<pre><code>torch.hstack((x[:,None], y[:,None])).T[...,None].repeat(1,1,3)
tensor([[[1, 1, 1],
         [2, 2, 2],
         [3, 3, 3]],

        [[4, 4, 4],
         [5, 5, 5],
         [6, 6, 6]]])
</code></pre>
","2024-04-10 08:10:38","2","Answer"
"78302933","78300152","","<p>To remove the last layer without interfering with the model's inference, you can simply replace the fully connected layer with the identity function, <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Identity.html"" rel=""nofollow noreferrer""><code>nn.Identity</code></a>:</p>
<pre><code>model.fc = nn.Identity()
</code></pre>
","2024-04-10 08:07:27","0","Answer"
"78302924","","Pytorch meshgrid","<p>I'm currently using torch 1.12 for my thesis on neural A star algorithm and i'm not quite sure what meshgrid does. For example why</p>
<pre><code>x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5, 6])
torch.meshgrid(x, y)
</code></pre>
<p>return</p>
<pre><code>tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]])
tensor([[4, 5, 6], [4, 5, 6], [4, 5, 6]])
</code></pre>
<p>and not</p>
<pre><code>tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]])
tensor([[4, 4, 4], [5, 5, 5], [6, 6, 6]])
</code></pre>
","2024-04-10 08:04:01","1","Question"
"78301534","78301477","","<p>The pruning utility in Pytorch acts as a masking wrapper on the layer that receives the pruning. This means you still have access to the original model weights and the network size remains unchanged, if not larger because of the initialization of a mask for each pruned tensor.</p>
<p>If you look at the documentation page for <a href=""https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured"" rel=""nofollow noreferrer""><code>prune.global_unstructured</code></a>:</p>
<blockquote>
<p>Modifies modules in place by:</p>
<ul>
<li><p>adding a named buffer called <code>name+'_mask'</code> corresponding to the binary
mask applied to the parameter name by the pruning method.</p>
</li>
<li><p>replacing the parameter name by its pruned version, while the original
(unpruned) parameter is stored in a new parameter named <code>name+'_orig'</code>.</p>
</li>
</ul>
</blockquote>
<p>Here is a minimal example to show that the unpruned weights are still accessible:</p>
<pre><code>net = nn.Sequential(OrderedDict(
    f1=nn.Linear(10, 5),
    f2=nn.Linear(5, 1)))

pruned = ((net.f1, 'weight'),)
prune.global_unstructured(pruned, prune.L1Unstructured, amount=0.8)
</code></pre>
<p>Then you can access the pruned weights:</p>
<pre><code>&gt;&gt;&gt; net.f1.weight
tensor([[0.0000, 0.0000, 0.0000, -0.0000, 0.0000],
        [0.0000, 0.3599, 0.0000, -0.0000, 0.4034]])
</code></pre>
<p>The original unpruned weight:</p>
<pre><code>&gt;&gt;&gt; net.f1.weight_orig
Parameter containing:
tensor([[ 0.1312,  0.1105,  0.0910, -0.2650,  0.3439],
        [ 0.0412,  0.3599,  0.2040, -0.2672,  0.4034]])
</code></pre>
<p>And the pruning mask:</p>
<pre><code>&gt;&gt;&gt; net.f1.weight_mask
tensor([[0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 1.]])
</code></pre>
","2024-04-09 23:38:24","0","Answer"
"78301477","","Pruned model size is the same of non-pruned model [PyTorch]","<p>I'm trying to implement model pruning on PyTorch with a ResNet18. Given an instance of ResNet18, I run the following code to load a pre-trained model, prune it and save the pruned model:</p>
<pre><code>def random_unstructured_pruning(pruning_rate: float, device, log_file):
    trained_model=retrieve_file(folder=&quot;./models&quot;, file_name='trained_model.pth')
    model=ResNet18(num_classes=10, input_channels=1).to(device)
    model.load_state_dict(torch.load(trained_model))
    
    modules_list=filter(lambda x: isinstance(x[1], (nn.Conv2d, nn.Linear, nn.BatchNorm2d)), model.named_modules())
    modules_list = map(lambda x: (x[1], 'weight'), modules_list)
    modules_list=tuple(modules_list)
 
  
    prune.global_unstructured(modules_list, pruning_method=prune.L1Unstructured, amount=0.8)
    for module in modules_list:
        prune.remove(module[0], module[1])
        
        
        
    pruning_rate_str= &quot;{:02d}&quot;.format(int(pruning_rate * 10))
    path=f&quot;{model_saving_path}pruned_{pruning_rate_str}.pth&quot;
    # 
    torch.save(model.state_dict(), f&quot;{path}&quot;)
</code></pre>
<p>In the end of the above function, the .pth file has the same dimension of the file that I load at the beginning while I expect it to be smaller since I'm pruning 80% of the weights.</p>
<p>Can somebody explain me why does it happen? What am I wrong?
Thank you!!</p>
<p>I think that the problem is in the saving part of the function, it seems that I'm saving always the same model that I re-load at the beginning and the pruning is not effective.</p>
","2024-04-09 23:14:30","0","Question"
"78300416","78300315","","<p>Standard dropout samples a random mask with every forward pass. This is a basic example:</p>
<pre class=""lang-py prettyprint-override""><code>class Dropout(nn.Module):
    def __init__(self, p):
        super().__init__()
        self.p = p
        
    def get_mask(self, x):
        mask = torch.rand(*x.shape)&lt;=self.p
        return mask
        
    def forward(self, x):
        if self.training:
            mask = self.get_mask(x)
            x = x * mask
        return x
</code></pre>
<p>If you want a custom dropout, you can implement your own logic in <code>get_mask</code></p>
","2024-04-09 18:20:46","1","Answer"
"78300315","","Custom dropout in PyTorch?","<p>I’m looking to implement a custom dropout in PyTorch — in the sense that I’d like to pass a mask of some sort, and have the corresponding neurons be “dropped out”, rather than dropping out random neurons with a specific probability. Something like:</p>
<pre><code>Nn.Dropout(inputs, mask = [0, 1, 0, …]
</code></pre>
<p>I can’t seem to find anything in pytorch’s documentation or forums, so any and all help would be appreciated.</p>
","2024-04-09 17:57:32","-1","Question"
"78300152","","Value for in_feature not changing","<p>I am making a deep learning model using resnet for multiclass classification and in it i am facing the problem of overfitting. To fix it i tried to reduce the layers of resnet and i am facing this error</p>
<blockquote>
<p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (65536x1 and 2048x1000)</p>
</blockquote>
<p>my code is</p>
<pre><code>class Flowers(nn.Module):
def __init__(self):
    super().__init__()
    self.classifier=nn.Sequential(
        nn.Flatten(),nn.Linear(in_features=1,out_features=102),
        )
def forward(self,x):
    x=self.classifier(x)
    return x
torch.manual_seed(42)
model = torch.hub.load(&quot;pytorch/vision&quot;, &quot;resnet50&quot;, weights=&quot;IMAGENET1K_V2&quot;)
list(model.modules())
my_model = nn.Sequential(*list(model.children())[:12])
my_model.add_module('fc', Flowers())
</code></pre>
<p>it will be huge help if anyone can help me with the overfitting problem in general.</p>
","2024-04-09 17:18:52","0","Question"
"78299837","78299815","","<p>There is no Torch wheel for the platform you're on:</p>
<pre><code>FROM python:3.12.0-alpine
</code></pre>
<p>Alpine Linux uses <a href=""https://musl.libc.org/"" rel=""nofollow noreferrer"">musl libc</a> C standard library, not the more common glibc, and that means you can't use the regular <a href=""https://github.com/pypa/manylinux"" rel=""nofollow noreferrer""><code>manylinux</code></a> wheels that most packages, <a href=""https://pypi.org/project/torch/#files"" rel=""nofollow noreferrer"">Pytorch included</a>, use.</p>
<p>To fix this, use the regular (glibc-backed) Python container:</p>
<pre><code>FROM python:3.12.0
</code></pre>
<p>In general, <a href=""https://pythonspeed.com/articles/alpine-docker-python/"" rel=""nofollow noreferrer"">Alpine is not a great choice for Python</a>.</p>
","2024-04-09 16:15:17","4","Answer"
"78299815","","Docker Could not find a version that satisfies the requirement","<p>I am trying to create a docker container of a flask server.</p>
<p>I get this error</p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement torch~=2.2.2 (from versions: none)
ERROR: No matching distribution found for torch~=2.2.2</p>
</blockquote>
<p>those are my files:</p>
<ol>
<li>requirements.txt</li>
</ol>
<pre><code>passlib~=1.7.4
pymongo~=3.10.1
Flask~=3.0.2
jsonschema~=4.19.2
torch~=2.2.2
torchvision~=0.17.2
pandas~=2.1.4
pyproj~=3.6.1
fiona~=1.9.5
geopandas~=0.14.3
pillow~=10.2.0
fonttools~=4.51.0
pytest~=7.4.4
</code></pre>
<ol start=""2"">
<li>Dockerfile</li>
</ol>
<pre><code># syntax=docker/dockerfile:1

FROM python:3.12.0-alpine
LABEL authors=&quot;Daniel Ben-Avi, Dean Avram&quot;

WORKDIR /wastewise

COPY requirements.txt requirements.txt
RUN pip install -r requirements.txt

COPY . .
CMD [ &quot;python3&quot;, &quot;-m&quot; , &quot;flask&quot;, &quot;run&quot;, &quot;--host=0.0.0.0&quot;]
</code></pre>
<p>please help me</p>
","2024-04-09 16:10:49","0","Question"
"78299639","","PyTorch Lightning inference after each epoch","<p>I'm using pytorch lightning, and, after each epoch, I'm running inference on a small dataset to produce a figure that I monitor with weight &amp; biases.</p>
<p>I thought the natural way to do that was to use a Callback with a <code>on_train_epoch_end</code> method that generates the plot. The latter method needs to run some inference, therefore I wanted to use <code>trainer.predict</code>. Yet, when doing this, I get the error below, so I guess it's not the intented way to do that.</p>
<p>Minimal reproducible example:</p>
<pre class=""lang-py prettyprint-override""><code>import lightning as L
from lightning.pytorch.callbacks import Callback

import torch
from torch.utils.data import DataLoader
from torch import nn, optim

class Model(L.LightningModule):
    def __init__(self):
        super().__init__()
        self.f = nn.Linear(10, 1)
        
    def training_step(self, batch, *args):
        out = self(batch)
        return out.mean() ** 2
    
    def forward(self, x):
        return self.f(x)[:, 0]

    def train_dataloader(self):
        return DataLoader(torch.randn((100, 10)))
    
    def predict_dataloader(self):
        return DataLoader(torch.randn((100, 10)))
    
    def predict_step(self, batch):
        return self(batch)
    
    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=1e-3)
        return optimizer
    
class CallbackExample(Callback):
    def on_train_epoch_end(self, trainer: L.Trainer, model: Model) -&gt; None:
        loader = model.predict_dataloader()
        trainer.predict(model, loader)
        
        ... # save figure to wandb

model = Model()
callback = CallbackExample()
trainer = L.Trainer(max_epochs=2, callbacks=callback, accelerator=&quot;mps&quot;)

trainer.fit(model)
</code></pre>
<pre class=""lang-bash prettyprint-override""><code>File ~/Library/Caches/pypoetry/virtualenvs/novae-ezkWKrh6-py3.9/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:233, in _LoggerConnector.metrics(self)
    231 &quot;&quot;&quot;This function returns either batch or epoch metrics.&quot;&quot;&quot;
    232 on_step = self._first_loop_iter is not None
--&gt; 233 assert self.trainer._results is not None
    234 return self.trainer._results.metrics(on_step)

AssertionError: 
</code></pre>
<p>What is the most natural and elegant way to do it?</p>
","2024-04-09 15:41:27","1","Question"
"78299001","78298942","","<p>You need to transfer your model and data to GPU to benefit from GPU computation. First you should check that you indeed have access to a CUDA device.</p>
<pre><code>device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

model.train(True)
model.to(device) # Transfer model to device
for i in range(n_train):
    start = time.time()
    for iter in range(num_of_epochs):
        train_loss = []
        for x, f, y in train_dataloader:
            # Zero your gradients for every batch!
            optimizer.zero_grad()

            # Transfer data to device
            x, f = x.to(device), f.to(device)

            # Make predictions for this batch
            out = model(x, f)
</code></pre>
","2024-04-09 13:59:12","-1","Answer"
"78298942","","With little networks with few parameters pytorch seems to be very slow in training speed respect to tensorflow. Am i missing something?","<p>I am currently working with small structured networks and even if Pytorch seems to be the obvious choice due to its high flexibility, it still drops a lot in terms of training speed compared to TensorFlow.
As I could see after 10 training sessions, PyTorch seems to double in training time. Am I doing something wrong? Is there a way to improve the computational time of Pytorch?</p>
<p>I've made a Google Colab to reproduce the same network in both PyTorch/TensorFlow to compare the two models.</p>
<pre><code>import torch.nn as nn
import torch
import time
from torch.utils.data import DataLoader,TensorDataset
import numpy as np

## Parameters
learning_rate = 0.0005
num_of_epochs = 300
batch_size = 128

## Create Data
xx=torch.arange(40*640*60, dtype=torch.float32).view(640*60, 40)
ff=torch.arange(1*640*60, dtype=torch.float32).view(640*60, 1)
out=torch.arange(1*640*60, dtype=torch.float32).view(640*60, 1)

## Define the model
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.lin = nn.Linear(40, 1, False)
        self.par = nn.Parameter(torch.Tensor(1))

    def forward(self, x, f):
        return torch.add(self.lin(x),self.par*f)

model = Model()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
loss_fun = torch.nn.MSELoss()

dataset = TensorDataset(xx,ff,out)
train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
</code></pre>
<hr />
<pre><code>%%time
## Training
model.train(True)
for i in range(n_train):
    start = time.time()
    for iter in range(num_of_epochs):
        train_loss = []
        for x, f, y in train_dataloader:
            # Zero your gradients for every batch!
            optimizer.zero_grad()

            # Make predictions for this batch
            out = model(x, f)

            # Compute the loss and its gradients
            loss = loss_fun(out, y)
            loss.backward()

            # Adjust learning weights
            optimizer.step()
            train_loss.append(loss.item())
        train_loss = np.mean(train_loss)
    end = time.time()
    exe_time['pytorch'].append(end-start)
</code></pre>
<p>Full code can be found <a href=""https://colab.research.google.com/drive/1L1x6pv-duokURbRhxGjf8m18B-rgwqkO?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
","2024-04-09 13:50:05","1","Question"
"78297737","78297706","","<p>The simplest way to have an explicit structure when using a sequential architecture is to define all layers (including any linear or  flattening layers) in a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"" rel=""nofollow noreferrer""><code>nn.Sequential</code></a> module:</p>
<pre><code>class NNM(nn.Sequential):
    def __init__(self, num_features, num_hidden):
        super().__init__(
            nn.Linear(num_features, num_hidden),
            nn.Sigmoid(),
            nn.Linear(num_hidden, 1))

        self.saved_parameters = []

    def save_parameters(self):
        self.saved_parameters.append(copy.deepcopy(self.state_dict()))
</code></pre>
<p>Then your instance <a href=""https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/container.py#L328"" rel=""nofollow noreferrer""><code>repr</code></a>esentation will be explicit:</p>
<pre><code>&gt;&gt;&gt; NNM(28, 100)
NNM(
  (0): Linear(in_features=28, out_features=100, bias=True)
  (1): Sigmoid()
  (2): Linear(in_features=100, out_features=1, bias=True)
)
</code></pre>
","2024-04-09 10:18:30","1","Answer"
"78297706","","Saving Pytorch model with specified activation function","<p>I am trying to save my Pytorch model with its activation function used.</p>
<p>Here is a simple example</p>
<pre><code># define class for neural network
class NNM(nn.Module):
    def __init__(self, num_features, num_hidden):
        super(NNM, self).__init__()
        self.fc1 = nn.Linear(num_features, num_hidden)
        self.fc2 = nn.Linear(num_hidden, 1)
        
        self.saved_parameters = []
        
    def forward(self, x):
        x = torch.sigmoid(self.fc1(x))
        return self.fc2(x)
    
    def save_parameters(self):
        self.saved_parameters.append(copy.deepcopy(self.state_dict()))

# created model a few lines later

model = NNM(28, 100)
</code></pre>
<p>Here, the list <code>saved_parameters</code> and the function <code>save_parameters</code> will allow to save model's parameters in a list a specified location of training.</p>
<p>Using <code>model.eval()</code> just shows model's layers (input-hidden and hidden-output).</p>
<pre><code>NNM(
  (fc1): Linear(in_features=28, out_features=100, bias=True)
  (fc2): Linear(in_features=100, out_features=1, bias=True)
)
</code></pre>
<p>What I need is something that integrates the used activation function.</p>
<pre><code>NNM(
  (fc1): Linear(in_features=28, out_features=100, bias=True)
  (fc2): Linear(in_features=100, out_features=1, bias=True)
  (act_func): Sigmoid(fc1-fc2)
)
</code></pre>
<p>Or a simpler solution to save the activation function with related layers in a dictionary.</p>
","2024-04-09 10:12:41","0","Question"
"78297464","78295969","","<p>I solved this problem myself later by adding the following</p>
<pre><code>queue.view(C, K).detach()
</code></pre>
<p>operation. One thing I think I learned is that you need to block all the &quot;roads&quot; in the loss feedback.</p>
","2024-04-09 09:31:40","0","Answer"
"78297385","","DeepSpeed Lightning refusing to parallelize layers even when setting to stage 3","<p>I want to come up with a very simple Lightning example using DeepSpeed, but it refused to parallelize layers even when setting to stage 3.</p>
<p>I'm just blowing up the model by adding FC layers in the hope they get distributed to the different GPU (6 in total)</p>
<p>But I'm ending up with</p>
<blockquote>
<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate
2.00 MiB (GPU 3; 15.00 GiB total capacity; 14.00 GiB already allocated; 5.25 MiB free; 14.00 GiB reserved in total by PyTorch) If
reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb
to avoid fragmentation.  See documentation for Memory Management and
PYTORCH_CUDA_ALLOC_CONF</p>
</blockquote>
<p>Therefore I guess the layers are only put to a single GPU.</p>
<p>The full code is available <a href=""https://gist.github.com/romeokienzler/26752e93c00500becb3a83df7311cc9b"" rel=""nofollow noreferrer"">here</a>, but this is a gist of it:</p>
<p>Blowing up the model with 18000 layers:</p>
<pre><code>class TelModel(L.LightningModule):
    def __init__(self):
        super().__init__()
        embed_dim = 512
        component_list = [
                nn.Linear(512, embed_dim)
        #] + [nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True) for _ in range(n_layers)] + [
        ] + [nn.Linear(embed_dim, 512) for _ in range(n_layers)] + [
                nn.Linear(embed_dim, 512)
        ]
        self.net = torch.nn.Sequential(*component_list)
</code></pre>
<p>Initializing DeepSpeed:</p>
<pre><code>tel_model = TelModel()
train_ds = RandomDataset(100)
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)
trainer = L.Trainer(accelerator=&quot;gpu&quot;, devices=6, strategy=&quot;deepspeed_stage_3&quot;, precision=32)
trainer.fit(tel_model, train_loader)
</code></pre>
<p>And finally, I run it like this:</p>
<blockquote>
<p>deepspeed lightning-deepspeed-tel.py</p>
</blockquote>
","2024-04-09 09:18:32","0","Question"
"78297310","","No module named 'torch._six' to import queue","<p>I'm trying to apply a RCAN model from the github <a href=""https://github.com/sanghyun-son/EDSR-PyTorch"" rel=""nofollow noreferrer"">https://github.com/sanghyun-son/EDSR-PyTorch</a> . I am using pytorch 2.0.1. I am getting the error:</p>
<pre><code>ModuleNotFoundError: No module named 'torch._six'
</code></pre>
<p>when trying to import:</p>
<pre><code>from torch._six import queue
</code></pre>
<p>Any suggestions on how to resolve this without going down to torch 1.0 ?</p>
<p>I have tried the following alternatives:</p>
<pre><code>from six import queue
from multiprocessing import Queue as queue
from multiprocessing.queues import Queue as queue
</code></pre>
<p>all of which raise new errors. I am not sure what function I could even use to replace queue if it is deprecated since I am not very familiar with pytorch or the model code.</p>
","2024-04-09 09:04:05","0","Question"
"78296954","78296533","","<p>By default, <a href=""https://en.wikipedia.org/wiki/Bessel%27s_correction"" rel=""nofollow noreferrer"">Bessel's correction</a> is used when computing the standard deviation in PyTorch with <code>torch.std</code>. Up until <code>1.13</code>, this parameter was called <a href=""https://pytorch.org/docs/1.13/generated/torch.std.html?highlight=std#torch.std"" rel=""nofollow noreferrer""><code>&quot;unbiased&quot;</code></a>, but in <code>2.0</code>, it was renamed to <a href=""https://pytorch.org/docs/2.0/generated/torch.std.html?highlight=std#torch.std"" rel=""nofollow noreferrer""><code>&quot;correction&quot;</code></a>. To replicate the behavior of a batch normalization layer, you should not use the default correction:</p>
<ul>
<li><p>versions <code>1.*</code>:</p>
<pre><code>(input-input.mean(dim=0))/input.std(dim=0, unbiased=False)
</code></pre>
</li>
<li><p>versions <code>2.*</code>:</p>
<pre><code>(input-input.mean(dim=0))/input.std(dim=0, correction=0)
</code></pre>
</li>
</ul>
","2024-04-09 08:08:44","1","Answer"
"78296533","","pytorch BatchNorm1D result different from manually implementation","<p>I read the BatchNorm1D doc in pytorch, but I get the different result:</p>
<pre><code>input  = torch.tensor([[1,2,3],[4,5,6]])
</code></pre>
<p>1.using BatchNorm1D</p>
<pre><code>m  = nn.BatchNorm1d(3)
output = m(input)
</code></pre>
<p>output:</p>
<pre><code>[[-1.0, -1.0, -1.0],[ 1.0, 1.0, 1.0]]
</code></pre>
<p>2.hard coding</p>
<pre><code>output = (input-input.mean(dim=0))/input.std(dim=0)
</code></pre>
<p>output:</p>
<pre><code>[[-0.7071, -0.7071, -0.7071],[0.7071, 0.7071, 0.7071]]
</code></pre>
<p>But they get the different result, why?</p>
","2024-04-09 06:40:22","0","Question"
"78295969","","Pytorch error :Trying to backward through the graph a second time","<p>I am a beginner in Pytorch and encountered the following error while attempting to replicate the Moco V1 model using Python. Using repain_graph=True should be meaningless. Both the encoder and momentum_encoder have been tested and there are no issues with them under direct training.</p>
<p>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</p>
<p>Pseudocode of the original paper
<a href=""https://i.sstatic.net/IqEqa.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<pre><code>def train(train_loader, encoder, momentum_encoder, device, epochs, m):
    torch.autograd.set_detect_anomaly(True)
    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)
    encoder.apply(xavier_init_weights)
    encoder.to(device)
    momentum_encoder.apply(xavier_init_weights)
    momentum_encoder.to(device)

    queue = Memory_queue(4)
    criterion = nn.CrossEntropyLoss()
    for batch_idx, (datas, _) in enumerate(train_loader, 1):
        encoder.eval()
        momentum_encoder.eval()
        datas = datas.to(device)
        q = encoder(datas)
        queue.put(q)

    for epoch in range(epochs):
        encoder.train()
        for batch_idx, (datas, _) in enumerate(train_loader, 1):  # 调整起始值为1
            encoder_optimizer.zero_grad()
            encoder.zero_grad()

            datas = add_noise(datas).to(device)
            datas_key1 = add_noise(datas).to(device)

            q = encoder(datas)
            k = momentum_encoder(datas_key1)

            k = k.detach()
            N, C = q.shape

            l_pos = torch.squeeze(torch.bmm(q.view(N, 1, C), k.view(N, C, 1)).to(device), dim=2)

            temp_queue = torch.cat(queue.get_all(), dim=0)
            K, _ = temp_queue.shape

            l_neg = torch.mm(q.view(N, C), temp_queue.view(C, K)).to(device)

            logits = torch.cat([l_pos, l_neg], dim=1).to(device)
            labels = torch.zeros(N).long().to(device)
            loss = criterion(logits, labels)

            loss.backward()
            encoder_optimizer.step()

            update_model_ema(encoder, momentum_encoder, 0.5)
            queue.put(q)
</code></pre>
<p>after trying</p>
<p>loss.backward(retain_graph=True)</p>
<p>this error appeared</p>
<p>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 32]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!</p>
","2024-04-09 03:26:33","0","Question"
"78295874","78283209","","<p>There's a step in <a href=""https://pytorch.org/audio/stable/installation.html"" rel=""nofollow noreferrer"">https://pytorch.org/audio/stable/installation.html</a>, that if you're installing it from <a href=""https://pytorch.org/audio/stable/installation.html"" rel=""nofollow noreferrer"">pre-built binaries</a> you still have to install the FFMPEG libs.  It's an important step disguised as a note, below is a that note quoted... revisit the installation page as that has links to the FFMPEG libs that you need to install:</p>
<blockquote>
<p>NOTE</p>
<p>This software was compiled against an unmodified copies of FFmpeg, with the specific rpath removed so as to enable the use of system
libraries. The LGPL source can be downloaded from the following
locations: n4.1.8 (license), n5.0.3 (license) and n6.0 (license).</p>
</blockquote>
","2024-04-09 02:35:50","0","Answer"
"78295568","78042907","","<p><code>num_workers=0</code> doesn't work for me. So, I removed the <code>num_workers</code> parameter, then it worked fine!</p>
","2024-04-09 00:12:10","0","Answer"
"78295199","78291257","","<p>Assuming you <a href=""https://stackoverflow.com/questions/78295146/no-matching-distribution-found-for-torch-1-9-0cu102"">now</a> have access to your GPU, this can be checked with:</p>
<pre><code>&gt;&gt;&gt; torch.backends.mps.is_available()
&gt;&gt;&gt; torch.backends.mps.is_built()
</code></pre>
<p>You can perform inference on that device instead of the CPU:</p>
<pre><code>device = torch.device('mps')
pipe = pipe.to(device)
</code></pre>
","2024-04-08 21:50:30","1","Answer"
"78295146","","No matching distribution found for torch==1.9.0+cu102","<p>I am trying to use <a href=""https://github.com/pixray/pixray"" rel=""nofollow noreferrer"">pixray</a> on macOS</p>
<p>but it looks like the dependencies mentioned in the requirements.txt aren't updated. These are the steps I followed:</p>
<ul>
<li><p>created a pyenv virtual env:
Python 3.8.10</p>
</li>
<li><p>Cloned the repo:
git clone <a href=""https://github.com/pixray/pixray.git"" rel=""nofollow noreferrer"">https://github.com/pixray/pixray.git</a> --recursive</p>
</li>
<li><p>cd pixray</p>
</li>
<li><p>pip install -r requirements.txt</p>
</li>
</ul>
<p>but now I get this:</p>
<pre><code>  Running command git clone -q https://github.com/pixray/aphantasia /private/var/folders/mn/tn1xcsr133z5ql6x9tctkd040000gr/T/pip-req-build-pux3s766
  WARNING: Did not find branch or tag '7e6b3bb', assuming revision or ref.
  Running command git checkout -q 7e6b3bb
ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2)
ERROR: No matching distribution found for torch==1.9.0+cu102

</code></pre>
<p>requirements.txt</p>
<pre><code>### THIS IS THE SAME AS REQUIREMENTS.TXT but with a different version of cuda (102) for cog

# these are minimal requirements for just the VQGAN drawer

-f https://download.pytorch.org/whl/torch_stable.html
torch==1.9.0+cu102
torchvision==0.10.0+cu102
torchtext==0.10.0

numpy==1.19.4
tqdm==4.49.0
matplotlib==3.3.4
braceexpand==0.1.7
colorthief==0.2.1
einops==0.3.2
imageio==2.9.0
ipython==7.28.0
kornia==0.6.2
omegaconf==2.1.1
Pillow==8.3.2
PyYAML==5.4.1
scikit_learn==1.0
scikit-image==0.18.3
torch_optimizer==0.1.0
torch-tools==0.1.5

# can use CompVis/taming-transformers when https://github.com/CompVis/taming-transformers/pull/81 is merged
git+https://github.com/bfirsh/taming-transformers.git@7a6e64ee
git+https://github.com/openai/CLIP
git+https://github.com/pvigier/perlin-numpy@6f077f8

# diffvg: these are minimal requirements for just the pixeldraw / linedraw etc drawers

# DO THIS: &quot;git clone https://github.com/pixray/diffvg &amp;&amp; cd diffvg &amp;&amp; git submodule update --init --recursive &amp;&amp; CMAKE_PREFIX_PATH=$(pyenv prefix) DIFFVG_CUDA=1 python setup.py install&quot;
cmake==3.21.3
cssutils==2.3.0
svgpathtools==1.4.2


# fft: these are IN ADDITION to the core requirements_vqgan.txt

lpips
sentence_transformers
opencv-python
PyWavelets==1.1.1
git+https://github.com/fbcotter/pytorch_wavelets

# main aphantasia library
git+https://github.com/pixray/aphantasia@7e6b3bb

# slip
timm

# resmem loss
resmem
</code></pre>
<p>where should i make a change? Should I update the python version or the library version inside the requirements.txt?</p>
","2024-04-08 21:33:58","2","Question"
"78293740","78293333","","<p>Assuming that you really want to keep optimizing your model using different optimizers, your issue is that your problem is convex.</p>
<p>Convexity means that you have a single minimum which is also the global minimum. It also means that the gradients become smaller and smaller as you approach the minimum point. In the convex setting, stepping towards the gradient is a reasonable idea.</p>
<p>So, when you start with SGD, you approach the minimum and the gradients become small. Then, the other algorithms which make a sub-optimal step (when the problem is convex) don't mess up too much since the gradients are already small.</p>
<p>On the other hand, if you start with say Adam, you start with making bad steps, which you later fix with SGD since for convex problems, SGD will converge to the minimum regardless of the starting point</p>
","2024-04-08 15:49:37","1","Answer"
"78293706","78293333","","<p>The issue is that you keep the same model for all regressions, meaning that when the first optimization ends, you will proceed with the next one using a trained model. It happens that the learning rate is only working for SGD (it seems too large for the other two optimizers), so to summarize both cases:</p>
<ul>
<li><p>if you start with the other two, the model will not perform well and the model won't fit the points. On the third training (SGD), the model will be trained properly and fit the points.</p>
</li>
<li><p>if you start with SGD it will train the model and the subsequent two trainings will not change the weights much leading to a similar performance</p>
</li>
</ul>
<p>Instead, you should reset your model and define optimizer specific learning rates:</p>
<pre><code>optimizers = dict(    
    RMSprop=(torch.optim.RMSprop, 0.3),
    SGD=(torch.optim.SGD, 0.01),
    Adam=(torch.optim.Adam, 0.5),
)

for optimizer_name, (klass,lr) in optimizers.items():
    model = copy.deepcopy(model_)
    optimizer = klass(model.parameters(), lr=lr)
    ## Proceed with your training loop
</code></pre>
<p>With the above setup, you will get the following fitting, <em>regardless of the order of execution</em>.</p>
<p><a href=""https://i.sstatic.net/sq8wz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sq8wz.png"" alt=""enter image description here"" /></a></p>
","2024-04-08 15:43:36","2","Answer"
"78293542","78293449","","<p>You can use the &quot;meta&quot; device to create a &quot;fake tensor&quot; without allocating any memory and using flops for computation.</p>
<p>See the following minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn

conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)
conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)
conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)

x = torch.zeros([1, 1, 8192, 8192], device=&quot;meta&quot;)

y = conv1(x)
y = conv2(y)
y = conv3(y)

print(y.shape)
</code></pre>
<p>Here are some more notes on the &quot;fake tensors&quot;: <a href=""https://pytorch.org/torchdistx/latest/fake_tensor_and_deferred_init.html"" rel=""nofollow noreferrer"">https://pytorch.org/torchdistx/latest/fake_tensor_and_deferred_init.html</a></p>
<p>I hope this helps!</p>
","2024-04-08 15:19:31","0","Answer"
"78293449","","How can I calculate the shape of a convolution layer automatically in Pytorch?","<p>I want to calculate the output dimension of any Pytorch convolution layer automatically.
When creating bigger CNN's, the calculation of the output of the last conv. layer gets pretty tedious.
Is there a way to do that automatically on the fly without using nn.LazyLinear()?</p>
<p>Code example:</p>
<pre><code>self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)
self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)
self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)

self.feature_dim = [ADD CODE HERE]

self. fc1 = nn.Linear(self.feature_dim, 256)
</code></pre>
<p>Thanks, the help is highly appreciated!</p>
<p>I have tried using the formula <code>[(W−K+2P)/S]+1</code> and implement it in the code but doing that for every layer is almost more work to implement than to calculate it simply by hand.
Is there a uniform way to read the output shape from Pytorch directly?</p>
<p><strong>EDIT:</strong>
Using this 2 functions one can extract the shape of the output at any point during the forward path. <a href=""https://pytorch.org/docs/stable/generated/torch.numel.html"" rel=""nofollow noreferrer"">torch.numel()</a> gives the total number of elements of the given tensor.</p>
<pre><code>self.flat_features = self._get_conv_output((1, 84, 84))

def _get_conv_output(self, shape):
    with torch.no_grad():
        input = torch.rand(1, *shape)
        output = self._forward_features(input)
        return output.numel()

def _forward_features(self, x):
    x = F.relu(self.conv1(x))
    x = F.relu(self.conv2(x))
    x = F.relu(self.conv3(x))
    return x
</code></pre>
","2024-04-08 15:04:18","0","Question"
"78293333","","The running order of optimizers impacts predictions in PyTorch","<p>I am running a linear regression for many optimizers. I noticed that if the SGD is activated first, the others have good accuracy. Otherwise, Adam and RMSprop present terrible adjustments.</p>
<pre><code># Generate synthetic data
X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=15)

X = torch.from_numpy(X_numpy.astype(np.float32))
y = torch.from_numpy(y_numpy.astype(np.float32))
y = y.view(y.shape[0], 1)

# Define the model
n_samples, n_features = X.shape
input_size = n_features
output_size = 1
model = nn.Linear(input_size, output_size)

# Define learning rate
learning_rate = 0.01

# Define criteria
criterion = nn.MSELoss()

# Define different optimizers
optimizers = {    
    &quot;Adam&quot;: torch.optim.Adam(model.parameters(), lr=learning_rate),
    &quot;RMSprop&quot;: torch.optim.RMSprop(model.parameters(), lr=learning_rate),
    &quot;SGD&quot;: torch.optim.SGD(model.parameters(), lr=learning_rate),
}

# Training loop for each optimizer
num_epochs = 100
predictions = {}
for optimizer_name, optimizer in optimizers.items():
    print(f&quot;Optimizer: {optimizer_name}&quot;)
    predictions[optimizer_name] = []
    for epoch in range(num_epochs):        
        y_predicted = model(X)
        loss = criterion(y_predicted, y)
        loss.backward()
        optimizer.step() #update wights
        optimizer.zero_grad() #zero the gradients
    predictions[optimizer_name] = model(X).detach().numpy()

# Plotting predictions with different colors
plt.figure(figsize=(10, 6))
plt.plot(X_numpy, y_numpy, 'ro', label='Original Data')
for optimizer_name, prediction in predictions.items():
    plt.plot(X_numpy, prediction, label=optimizer_name)
plt.legend()
plt.show()
</code></pre>
<p>The above code generates the predictions:</p>
<p><a href=""https://i.sstatic.net/2iF4L.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2iF4L.png"" alt=""enter image description here"" /></a></p>
<p>If I run SGD first, the following happens:</p>
<pre><code>optimizers = {    
    &quot;Adam&quot;: torch.optim.Adam(model.parameters(), lr=learning_rate),
    &quot;RMSprop&quot;: torch.optim.RMSprop(model.parameters(), lr=learning_rate),
    &quot;SGD&quot;: torch.optim.SGD(model.parameters(), lr=learning_rate),
}
</code></pre>
<p><a href=""https://i.sstatic.net/p7Lsn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/p7Lsn.png"" alt=""enter image description here"" /></a></p>
<p>Why does it happen?</p>
","2024-04-08 14:45:19","1","Question"
"78291257","","stabilityai/stable-cascade takes 7+ hours to generate an image","<p>I am using this model: <a href=""https://huggingface.co/stabilityai/stable-cascade"" rel=""nofollow noreferrer"">https://huggingface.co/stabilityai/stable-cascade</a></p>
<pre><code>from diffusers import StableCascadeCombinedPipeline

print(&quot;LOADING MODEL&quot;)
pipe = StableCascadeCombinedPipeline.from_pretrained(&quot;stabilityai/stable-cascade&quot;, variant=&quot;bf16&quot;, torch_dtype=torch.bfloat16)
print(&quot;MODEL LOADED&quot;)

prompt = &quot;a lawyer&quot;
pipe(
    prompt=prompt,
    negative_prompt=&quot;&quot;,
    num_inference_steps=10,
    prior_num_inference_steps=20,
    prior_guidance_scale=3.0,
    width=1024,
    height=1024,
).images[0].save(&quot;cascade-combined2.png&quot;)
</code></pre>
<p>The model is loaded almost instantly but the next part took 7 plus hours.</p>
<pre><code>Loading pipeline components...: 100%|█████████████████████████████████████████| 5/5 [00:00&lt;00:00,  9.73it/s]
Loading pipeline components...: 100%|█████████████████████████████████████████| 6/6 [00:00&lt;00:00, 11.08it/s]
MODEL LOADED
  0%|                                                                                | 0/20 [00:00&lt;?, ?it/s]  0%|                                                                                | 0/20 [04:10&lt;?, ?it/s]
</code></pre>
<p>I am using</p>
<p>Apple M2 Pro (32 GB)</p>
<p>Python 3.10.2</p>
<p>Is there anything I can do to speed this up? Becuase I would like to generate maybe 50 images and that doesn't seem possible at the current speed.</p>
<p><strong>Edit:</strong></p>
<pre><code>pipe = StableCascadeCombinedPipeline.from_pretrained(&quot;stabilityai/stable-cascade&quot;, variant=&quot;bf16&quot;, torch_dtype=torch.float32)

device = torch.device('mps')
pipe.to(device)

prompt = &quot;a football&quot;
pipe(
    prompt=prompt,
    negative_prompt=&quot;&quot;,
    num_inference_steps=10,
    prior_num_inference_steps=20,
    prior_guidance_scale=3.0,
    width=1024,
    height=1024,
).images[0].save(&quot;cascade-combined2.png&quot;)
</code></pre>
","2024-04-08 08:40:16","0","Question"
"78290676","78289901","","<p>You can access its weights normally through <code>tm.parameters()</code> just like any <code>nn.Module()</code></p>
","2024-04-08 06:51:15","0","Answer"
"78290206","78118974","","<p>So, I've found a way how to convert *ckpt to my own *npz model.
First, in <a href=""https://github.com/aqlaboratory/openfold/blob/main/train_openfold.py"" rel=""nofollow noreferrer"">train_openfold.py</a> I added:</p>
<pre><code>def convert_to_pt(ckpt_path, output_path):

checkpoint = torch.load(ckpt_path)
model_state_dict = checkpoint['state_dict']

adjusted_state_dict = {}
for key in model_state_dict.keys():
    adjusted_state_dict[key.replace('model.', &quot;&quot;, 1)] = model_state_dict[key]

torch.save(adjusted_state_dict, output_path)
print(f&quot;Converted checkpoint '{ckpt_path}' to PyTorch state dict '{output_path}'.&quot;)
</code></pre>
<p>and after <code>if(args.resume_from_ckpt):</code></p>
<pre><code>convert_to_pt(args.resume_from_ckpt, &quot;/npz/model.pt&quot;)
print(&quot;Checkpoint converted to pt format&quot;)
</code></pre>
<p>Then,
I used generated *.pt in <a href=""https://github.com/aqlaboratory/openfold/blob/main/scripts/convert_of_weights_to_jax.py"" rel=""nofollow noreferrer"">convert_of_weights_to_jax.py</a> for creating *.npz.
I used model_1 instead of model_1_ptm.</p>
","2024-04-08 04:32:07","1","Answer"
"78290081","78277279","","<p>From the source code, tensorflow v2.2.3 is the last version that contains <code>tf.Summary</code> export</p>
<p><a href=""https://github.com/tensorflow/tensorflow/blob/v2.2.3/tensorflow/python/__init__.py#L199"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.2.3/tensorflow/python/__init__.py#L199</a></p>
<p>The paper is from 2019, therefore I would suggest you to limit package versions before 2019 to avoid possible issues.</p>
","2024-04-08 03:37:05","0","Answer"
"78289901","","TypeError: 'GraphModule' object is not subscriptable (Access Weights for .onnx ML model in Pytorch)","<p>I have a popular .onnx ML model for weather forecasting and am trying to convert this to PyTorch for finetuning. I use the following code to convert this:</p>
<pre><code>import os
import numpy as np
import onnx
from onnx import numpy_helper
import onnxruntime as ort
from onnx2torch import convert

model_24 = onnx.load('pangu_weather_24.onnx')
tm = convert(model_24) #Convert onnx model to torch
</code></pre>
<p>From here, I want to access the weights of the model in the 'tm' object but I can't seem to find any resources online for this.</p>
<p>Trying to subscript it with tm[0] reveals the following error:</p>
<pre><code>TypeError: 'GraphModule' object is not subscriptable
</code></pre>
<p>and the dict for this object via 'tm.<strong>dict</strong>' is even more confusing (pasted in the image).</p>
<p>The normal methods online for accessing a PyTorch weight matrix also reveal the same error that the graphmodule is not subscriptable</p>
","2024-04-08 02:13:29","0","Question"
"78287979","","attention map for an image","<p>I am new to pytorch. I want to use imagenet images to understand how much each pixel contributes to the gradient. For this, I am trying to construct attention maps for my images. However, while doing so, I am encountering the following error:</p>
<pre><code>&lt;ipython-input-64-08560ac86bab&gt;:2: UserWarning: To copy  construct from a tensor, it is recommended to use    sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than   torch.tensor(sourceTensor).
  images_tensor = torch.tensor(images, requires_grad=True)
  &lt;ipython-input-64-08560ac86bab&gt;:3: UserWarning: To copy     construct from a tensor, it is recommended to use     sourceTensor.clone().detach() or   sourceTensor.clone().detach().requires_grad_(True), rather than    torch.tensor(sourceTensor).
  labels_tensor = torch.tensor(labels)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most   recent call last)
&lt;ipython-input-65-49bfbb2b28f0&gt; in &lt;cell line: 20&gt;()
 18     plt.show()
 19 
---&gt; 20 show_attention_maps(X, y)

9 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py  in batch_norm(input, running_mean, running_var, weight, bias,  training, momentum, eps)
   2480         _verify_batch_size(input.size())
   2481 
-&gt; 2482     return torch.batch_norm(
   2483         input, weight, bias, running_mean,     running_var, training, momentum, eps, torch.backends.cudnn.enabled
   2484     )

RuntimeError: running_mean should contain 1 elements not 64
</code></pre>
<p>I have tried changing the image size in preprocessing and changing the model to resnet152 instead of resnet18. My understanding from the research I have done is that the batchnorm in the first layer expects input size 1, but I have 64. I am not sure how that can be changed.</p>
<p>My code is here:</p>
<pre><code>model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)
import torch.nn as nn
new_conv1 = nn.Conv2d(15, 1, kernel_size=1, stride=1, padding=112)     
nn.init.constant_(new_conv1.weight, 1)
model.conv1 = new_conv1
model.eval()

for param in model.parameters():
    param.requires_grad = False

def show_attention_maps(X, y):
X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)
y_tensor = torch.LongTensor(y)
attention = compute_attention_maps(X_tensor, y_tensor, model)
attention = attention.numpy()

N = X.shape[0]
for i in range(N):
    plt.subplot(2, N, i + 1)
    plt.imshow(X[i])
    plt.axis('off')
    plt.title(class_names[y[i]])
    plt.subplot(2, N, N + i + 1)
    plt.imshow(attention[i], cmap=plt.cm.gray)
    plt.axis('off')
    plt.gcf().set_size_inches(12, 5)
plt.suptitle('Attention maps')
plt.show()

show_attention_maps(X, y)

def compute_attention_maps(images, labels, model):
    images_tensor = torch.tensor(images, requires_grad=True)
    labels_tensor = torch.tensor(labels)
    predictions = model(images_tensor.unsqueeze(0))
    criterion = torch.nn.CrossEntropyLoss()
    loss = criterion(predictions, labels_tensor)
    model.zero_grad()
    loss.backward()
    gradients = images_tensor.grad
    attention_maps = torch.mean(gradients.abs(), dim=1)
    return attention_maps
</code></pre>
<p>Thank you very much in advance.</p>
<p>Edit: I changed my question because I was able to solve my previous problem by changing the resnet's conv1 (in line 3 of my code provided) and I am still trying to compute attention maps.</p>
","2024-04-07 13:33:57","-1","Question"
"78287938","78279823","","<h3>How does a hook work?</h3>
<p>A hook allows you to execute a specific function - referred to as a &quot;callback&quot; - when a particular action has been performed. In this case, you are expecting <code>self.get_attention</code> to be called once the <code>forward</code> function of <code>module</code> has been accessed. To give a minimal example of how a hook would look like. I define a simple class on which you can register new callbacks through <code>register_hook</code>, then when the instance is called (via <code>__call__</code>), all hooks will be called with the provided arguments:</p>
<pre><code>class Obj:
    def __init__(self):
        self.hooks = []
    
    def register_hook(self, hook):
        self.hooks.append(hook)

    def __call__(self, x, y):
        print('instance called')
        for hook in self.hooks:
            hook(x, y)
</code></pre>
<p>First, implement two hooks for demonstration purposes:</p>
<pre><code>def foo(x, y):
    print(f'foo called with {x} and {y}')
def bar(x, _):
    print(f'bar called with {x}')
</code></pre>
<p>And initialize an instance of <code>Obj</code>:</p>
<pre><code>obj = Obj()
</code></pre>
<p>You can register a hook and call the instance:</p>
<pre><code>&gt;&gt;&gt; obj.register_hook(foo)
&gt;&gt;&gt; obj('yes', 'no')
instance called
foo called with yes and no
</code></pre>
<p>You can add hooks on top and call again to compare, here both hooks are triggered:</p>
<pre><code>&gt;&gt;&gt; obj.register_hook(bar)
&gt;&gt;&gt; obj('yes', 'no')
instance called
foo called with yes and no
bar called with yes
</code></pre>
<hr />
<h3>Using hooks in PyTorch</h3>
<p>There are two primary hooks in PyTorch: forward and backward. You also have pre- and post-hooks. Additionally there exists hooks on other actions such as <code>load_state_dict</code>...</p>
<ul>
<li><p>To attach a hook on the forward process of a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html"" rel=""noreferrer""><code>nn.Module</code></a>, you should use <a href=""https://register_forward_hook"" rel=""noreferrer""><code>register_forward_hook</code></a>, the argument is a callback function that expects <code>module</code>, <code>args</code>, and <code>output</code>. This callback will be triggered on every forward execution.</p>
</li>
<li><p>For backward hooks, you should use <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook"" rel=""noreferrer""><code>register_full_backward_hook</code></a>, the registered hook expects three arguments: <code>module</code>, <code>grad_input</code>, and <code>grad_output</code>. As of recent PyTorch versions, <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_backward_hook"" rel=""noreferrer""><code>register_backward_hook</code></a> has been deprecated and <strong>should not be used</strong>.</p>
</li>
</ul>
<p>One side effect here is that you are registering the hook with <code>self.get_attention</code> and <code>self.get_attention_gradient</code>. The function passed to the register handler is <em>not unbound to the class instance</em>! In other words, on execution, these will be called without the <code>self</code> argument like:</p>
<pre><code>self.get_attention(module, input, output)
self.get_attention_gradient(module, grad_input, grad_output)
</code></pre>
<p>This will fail. <a href=""https://stackoverflow.com/questions/1015307/how-to-bind-an-unbound-method-without-calling-it"">A simple way</a> to fix this is to wrap the hook with a lambda when you register it:</p>
<pre><code>module.register_forward_hook(
    lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs))
</code></pre>
<p>All in all, your class could look like this:</p>
<pre><code>class Routine:
    def __init__(self, model, attention_layer_name):
        self.model = model

        for name, module in self.model.named_modules():
            if attention_layer_name in name:
                module.register_forward_hook(
                    lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs))
                module.register_full_backward_hook(
                    lambda *args, **kwargs: Routine.get_attention_gradient(self, *args, **kwargs))

        self.attentions = []
        self.attention_gradients = []

    def get_attention(self, module, input, output):
        self.attentions.append(output.cpu())

    def get_attention_gradient(self, module, grad_input, grad_output):
        self.attention_gradients.append(grad_input[0].cpu())

    def __call__(self, input_tensor):
        self.model.zero_grad()
        output = self.model(input_tensor)
        loss = output.mean()
        loss.backward()
</code></pre>
<p>When initialized with a single linear layer model:</p>
<pre><code>routine = Routine(nn.Sequential(nn.Linear(10,10)), attention_layer_name='0')
</code></pre>
<p>You can call the instance, this will first trigger the forward hook with (because of <code>self.model(input_tensor)</code>, and then the backward hook (because of <code>loss.backward()</code>).</p>
<pre><code>&gt;&gt;&gt; routine(torch.rand(1,10, requires_grad=True))
</code></pre>
<p>Following your implementation, your forward hook is caching the output of the <code>&quot;attention_layer_name&quot;</code> layer in <code>self.attentions</code>.</p>
<pre><code>&gt;&gt;&gt; routine.attentions
[tensor([[-0.3137, -0.2265, -0.2197,  0.2211, -0.6700, 
          -0.5034, -0.1878, -1.1334,  0.2025,  0.8679]], grad_fn=&lt;...&gt;)]
</code></pre>
<p>Similarly for the <code>self.attention_gradients</code>:</p>
<pre><code>&gt;&gt;&gt; routine.attentions_gradients
[tensor([[-0.0501,  0.0393,  0.0353, -0.0257,  0.0083,  
           0.0426, -0.0004, -0.0095, -0.0759, -0.0213]])] 
</code></pre>
<hr />
<p>It is important to note that the cached outputs and gradients will remain in <code>self.attentions</code> and <code>self.attentions_gradients</code> and get appended on every execution of <code>Routine.__call__</code>.</p>
","2024-04-07 13:15:54","9","Answer"
"78287442","78287387","","<p>Unlike <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to"" rel=""nofollow noreferrer""><code>nn.Module.to</code></a>, <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.to.html"" rel=""nofollow noreferrer""><code>torch.Tensor.to</code></a> doesn't work in place.
<br>So you must assign the result back to a variable:</p>
<pre><code>expanded_tensor = expanded_tensor.to(self.device)
</code></pre>
","2024-04-07 10:15:00","-1","Answer"
"78287387","","Can't get torch model to run on GPU even though it's a recognized device","<p>I'm using resnet18 for for grounded image semgmentation but when I pass the input to the model I get this torch error:</p>
<p><code>RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor</code></p>
<p>I got the weights from here:</p>
<p>'https://download.pytorch.org/models/resnet18-5c106cde.pth'</p>
<p>I assume the weights are trained on nvidia hardware because why wouldn't they be, so that leaves out my input tensor as being processed on my CPU (r7 1600x), but that's strange because I treid to make sure everything is being run on my nvidia GPU (rtx 3060) which is recognized (<code>torch.cuda.is_available() == True</code>).</p>
<p>Here's the code I think is relevant</p>
<pre><code>
class Masking:
    def __init__(
            self, 
            device_handle:Provider='cuda:0',  #&lt;--------------------------
            classifier:Module=None,
            face_parser:Module=None,
            num_classes=19
    ) -&gt; None:
        self.device = device(device_handle)
        self.classifier = classifier
        self.face_parser = face_parser
        self.num_classes = num_classes
        self.provided_image = None 

        print(&quot;is cuda available:  &quot;, torch.cuda.is_available())  # True

    def startup_model(self):
        self.classifier.to(self.device)  #&lt;-----------------------------
        self.classifier.load_state_dict(self.face_parser)
        self.classifier.eval()

    def preprocess_image(self, image_path):
        to_tensor = transforms.Compose([ 
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])

        self.provided_image = Image.open(image_path)
        w, h = self.provided_image.size
        interpolated_image = self.provided_image.resize((w, h), Image.BILINEAR)
        composed_image = to_tensor(interpolated_image)
        expanded_tensor = unsqueeze(composed_image, 0)
        expanded_tensor.to(self.device) #&lt;--------------------------------
        #expanded_tensor.to(&quot;cuda:0&quot;) # doesn't work either
        #expanded_tensor.cuda() # no dice
        out = self.classifier(expanded_tensor)[0]  # &lt;&lt;&lt;&lt;&lt; ERROR

        return out


    masker = Masking(
        'cuda:0',
        BiSeNet(n_classes=19),
        load('path/to/my/models/79999_iter.pth', device('cuda:0')) #&lt;---------------------
    )

</code></pre>
<p>....Then I run the methods I defined above etc</p>
","2024-04-07 09:54:21","-1","Question"
"78286355","","I can't find pytorch and cudnn version for CUDA 12.4. Which versions can I download and use?","<p>nvidia-smi output says CUDA 12.4 as follows.</p>
<p><a href=""https://i.sstatic.net/Jjfbv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jjfbv.png"" alt=""enter image description here"" /></a>
I downloaded and installed this as CUDA toolkit</p>
<p><a href=""https://i.sstatic.net/285tF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/285tF.png"" alt=""enter image description here"" /></a></p>
<p>and downloaded cudnn top one:
<a href=""https://i.sstatic.net/gASML.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gASML.png"" alt=""enter image description here"" /></a></p>
<p>There is no selection for 12.4. Because of this i downloaded pytorch for CUDA 12.1.
<a href=""https://i.sstatic.net/bP82A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bP82A.png"" alt=""enter image description here"" /></a>
I transferred cudnn files to CUDA folder. Bin folder added to path. And results:</p>
<p><a href=""https://i.sstatic.net/ln5Co.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ln5Co.png"" alt=""enter image description here"" /></a>
I bought a computer to work with CUDA but I can't run it.</p>
","2024-04-07 01:21:21","3","Question"
"78285836","78285817","","<p>You can call <code>detach</code> on the mask tensor to remove it from the gradient chain.</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.randn(1, 3, requires_grad=True)

mask = torch.tensor([[1., 0., 0.]], requires_grad=True)

mask_no_grad = mask.detach()

b = a * mask_no_grad
print(b)
&gt; tensor([[0.3871, 0.0000, -0.0000]], grad_fn=&lt;MulBackward0&gt;)
</code></pre>
","2024-04-06 20:47:52","1","Answer"
"78285817","","How to mask a tensor without losing the gradient?","<p>I have a tensor</p>
<pre><code>import torch
a = torch.randn(1, 3, requires_grad=True)
print('a: ', a)
&gt;&gt;&gt; a:  tensor([[0.0200, 1.00200, -4.2000]], requires_grad=True)
</code></pre>
<p>And a mask</p>
<pre><code>mask = torch.zeros_like(a)
mask[0][0] = 1
</code></pre>
<p>I want to mask my tensor <code>a</code> without propagating the gradients to my mask tensor (in my real case it has a gradient). I tried to the following</p>
<pre><code>with torch.no_grad():
    b = a * mask
    print('b: ', b)
    &gt;&gt;&gt; b:  tensor([[0.0200, 0.0000, -0.0000]])
</code></pre>
<p>But it removes the gradient entirely from my tensor. What is the correct way to do it?</p>
","2024-04-06 20:38:59","0","Question"
"78285237","78284866","","<p>Neither. You want to tokenize your entire dataset in batch prior to training. Tokenizing during training slows it down, and is wasteful if you're doing multiple epochs (you will tokenize the same items multiple times).</p>
<p>You should tokenize your entire dataset first. Then do batching and padding in your collate function.</p>
<p>It sounds like you're using huggingface tokenizers - you should also use huggingface datasets and the collate functions they provide.</p>
","2024-04-06 17:12:03","3","Answer"
"78284866","","What is the best function/stage to use tokenizer in Pytorch's data processing?","<p>I am subclassing <code>torch.utils.data.Dataset</code> and writing a collate function to be passed to Dataloader's <code>dataset</code> and <code>collate_fn</code> argument respectively.</p>
<p>Between Dataset's <code>__getitem__</code> or <code>collate_fn</code> I was wondering what would be the best function to use tokenizer (huggingface's FastTokenizer) in (or is there another better option out of these two that I don't know of?)</p>
<p><code>collate_fn</code> seems to me as the best option since I can use tokenizer to tokenize and adding padding for the full batch together, which should also help with tokenization speed. But I am not sure if it can cause any problems, in future if not now as some of my teammates start using/customizing my code.</p>
","2024-04-06 15:14:45","1","Question"
"78284767","78284705","","<p>I would say that the main requirement for the custom loss function is that it has to differentiable, so the autograd can propagate error through it.</p>
<p>The best way of it is to inherit from torch's <code>nn.Module</code> and make sure that you don't break the computational graph, for example:</p>
<pre class=""lang-py prettyprint-override""><code>class MyLoss(nn.Module):
    def __init__(self, reduction=&quot;mean&quot;):
        super().__init__()
        self.reduction = reduction

    def forward(self, input, target):
        # Some dummy loss:
        loss = target - input

        # Return mean value of the whole batch:
        if self.reduction == &quot;mean&quot;:
            return loss.mean()
        else:
            # Otherwise return raw (no reduction) loss values:
            return loss
</code></pre>
<p>I don't know what kind of loss you are trying to implement, but the error basically tells you that some of the tensors used in loss computing has <code>requires_grad=False</code> so autograd doesn't know how to compute grads for that particular tensor.</p>
","2024-04-06 14:37:25","-2","Answer"
"78284705","","Some details when writing your own loss function in pytorch","<p>When I write my own loss function code, I need to calculate the loss value for each pixel of the tensor data type image, and then add all the loss values ​​to calculate the average.But when I run the code, it shows an error like this:</p>
<pre class=""lang-py prettyprint-override""><code>element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>Therefore, I would like to ask, if you want to write the code of the loss function yourself, is it not possible to use the method of calculating the loss value for individual pixels separately?</p>
<p>Any help and suggestions will be greatly appreciated.</p>
","2024-04-06 14:16:34","-1","Question"
"78284478","78282213","","<p>I just had the same problem.
I am training an NLP with my own data.</p>
<p>The data I provided to the model was parsed incorrectly. Instead of tokenizing words or sentences I had tokenized the whole text and saved it as a single token.</p>
<p>Just check the data you are providing and you should be able to see the problem.</p>
","2024-04-06 13:05:32","0","Answer"
"78283209","","The torchaudio backend is empty","<p>I am trying to read m4a audio file using torchaudio.load() but i go this following error</p>
<pre><code>torchaudio.load(&quot;1.m4a&quot;)
</code></pre>
<pre><code>RuntimeError                              Traceback (most recent call last)
Cell In[6], line 1
----&gt; 1 torchaudio.load(&quot;1.m4a&quot;)

File c:\Users\ASUS\Desktop\AI-projects\Voice-Gender-Classification\voiceenv\Lib\site-packages\torchaudio\_backend\utils.py:204, in get_load_func.&lt;locals&gt;.load(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)
    118 def load(
    119     uri: Union[BinaryIO, str, os.PathLike],
    120     frame_offset: int = 0,
   (...)
    126     backend: Optional[str] = None,
    127 ) -&gt; Tuple[torch.Tensor, int]:
    128     &quot;&quot;&quot;Load audio data from source.
    129 
    130     By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with
   (...)
    202             `[channel, time]` else `[time, channel]`.
    203     &quot;&quot;&quot;
--&gt; 204     backend = dispatcher(uri, format, backend)
    205     return backend.load(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)

File c:\Users\ASUS\Desktop\AI-projects\Voice-Gender-Classification\voiceenv\Lib\site-packages\torchaudio\_backend\utils.py:116, in get_load_func.&lt;locals&gt;.dispatcher(uri, format, backend_name)
    114     if backend.can_decode(uri, format):
    115         return backend
--&gt; 116 raise RuntimeError(f&quot;Couldn't find appropriate backend to handle uri {uri} and format {format}.&quot;)

RuntimeError: Couldn't find appropriate backend to handle uri 1.m4a and format None.

</code></pre>
<p>so I checked the torchaudio backend and i got an empty list</p>
<pre><code>import torchaudio
print(torchaudio.list_audio_backends())
</code></pre>
<pre><code>[]
</code></pre>
<p>I tried installing ffmpeg and librosa and PySoundFile... but nothing worked for me...</p>
<p>I just want it to read the audiofile properly so that i can conitnue with my project</p>
","2024-04-06 04:08:09","3","Question"
"78283046","78282837","","<p>You're getting that error because you're passing the input tensor &quot;too early.&quot; Calling <code>Net()</code> semantically is &quot;model creation&quot; when the model object is created and initialized, but your model doesn't exist yet! Model execution just right after model creation will be:</p>
<p><code>Net()(X_train[:5])</code></p>
<p>But usually, we do it in 2 steps:</p>
<pre class=""lang-py prettyprint-override""><code># Create model
model = Net()
# Now you can call model multiple times...
model(X_train[:5])
</code></pre>
","2024-04-06 02:38:26","0","Answer"
"78283018","78282862","","<p>As of version 2.2.0, torch does not yet support <code>compile</code> with Python 3.12.</p>
<p>Check <a href=""https://github.com/pytorch/pytorch/issues/120233"" rel=""noreferrer"">pytorch/pytorch#120233</a> issue for more details.</p>
","2024-04-06 02:15:19","5","Answer"
"78282873","78282837","","<p>You are passing your input to the model constructor. You need to instantiate the model before calling the <code>forward</code> method.</p>
<pre class=""lang-py prettyprint-override""><code>model = Net()
out = model(torch.randn(8, 3))
</code></pre>
","2024-04-06 00:51:33","0","Answer"
"78282862","","python- pytorch.compile() giving runtime error saying Dynamo is not supported on python 3.12+","<p>I'm trying to run this block of code in my local LLM.</p>
<pre class=""lang-py prettyprint-override""><code>if compile:
    print(&quot;compiling the model&quot;)
    unoptimized_model = model
    model = torch.compile(model)
</code></pre>
<p>And this is the error i get:</p>
<p>Pls help me fix this</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
File &quot;c:\\Users\\abul4\\OneDrive\\Desktop\\LLM\\train.py&quot;, line 180, in \&lt;module\&gt;
model = torch.compile(model)
^^^^^^^^^^^^^^^^^^^^
File &quot;C:\\Users\\abul4\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\__init_\_.py&quot;, line 1801, in compile
raise RuntimeError(&quot;Dynamo is not supported on Python 3.12+&quot;)
RuntimeError: Dynamo is not supported on Python 3.12+
</code></pre>
","2024-04-06 00:46:07","1","Question"
"78282837","","Unusual Error When Using nn.Sequential in Model Class in PyTorch","<p>Just creating a simple neural network here to try out nn.Sequential, for some reason i'm getting this error</p>
<p>Here is the code:</p>
<pre><code># Create Model

class Net(nn.Module):
    def __init__(self):
        super().__init__()

        # Define Network

        self.stack = nn.Sequential(
            nn.Linear(in_features=3, out_features=8),

            nn.ReLU(),

            nn.Linear(in_features=8, out_features=8),

            nn.ReLU(),

            nn.Linear(in_features=8, out_features=1),

            nn.Sigmoid(),
        )

    def forward(self, x):
        # Define Forward Pass

        return self.stack(x)


# Instance Of Model

model = Net(X_train[:5])

</code></pre>
<p>Error Here:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-32-f947c74336f3&gt; in &lt;cell line: 31&gt;()
     29 # Instance Of Model
     30 
---&gt; 31 model = Net(X_train[0])

TypeError: Net.__init__() takes 1 positional argument but 2 were given

</code></pre>
<p>Here I tried to test if my model worked by inputting the first 5 values of my training data, instead of getting 5 numbers between 0 and 1, ex. [0.1, 0.2, 0.43, 0.67, .78] I get the error above.</p>
","2024-04-06 00:27:30","-1","Question"
"78282414","78281918","","<p>From the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"" rel=""nofollow noreferrer"">documentation</a></p>
<p>A unidirectional LSTM model gives three outputs:</p>
<ul>
<li><code>output</code>, the main output, size <code>(L, N, H_out)</code></li>
<li><code>h_n</code>, the hidden state, size <code>(num_layers, N, H_out)</code></li>
<li><code>c_n</code>, the cell state, size <code>(num_layers, N, H_cell)</code></li>
</ul>
<p>The <code>proj_size</code> argument changes the <code>H_out</code> of the model. If <code>proj_size&gt;0</code>, <code>H_out=proj_size</code>, otherwise <code>H_out=hidden_size</code>. <code>proj_size</code> also changes the dimension of the hidden state weight, <code>W_hi</code>.</p>
<p>This feature is motivated by the <a href=""https://arxiv.org/pdf/1402.1128.pdf"" rel=""nofollow noreferrer"">paper</a> linked in the documentation.</p>
<p>The paper found that using a smaller hidden size with a projection to match the cell size gave better parameter-adjusted performance.</p>
<p>To show the difference:</p>
<pre class=""lang-py prettyprint-override""><code># without projection
lstm_kwargs = {
    'input_size' : 64,
    'hidden_size' : 512,
    'num_layers' : 3,
    'batch_first' : True
}

lstm1 = nn.LSTM(**lstm_kwargs)

[(k, v.shape) for k,v in lstm1.state_dict().items()]

&gt; [('weight_ih_l0', torch.Size([2048, 64])),
 ('weight_hh_l0', torch.Size([2048, 512])),
 ('bias_ih_l0', torch.Size([2048])),
 ('bias_hh_l0', torch.Size([2048])),
 ('weight_ih_l1', torch.Size([2048, 512])),
 ('weight_hh_l1', torch.Size([2048, 512])),
 ('bias_ih_l1', torch.Size([2048])),
 ('bias_hh_l1', torch.Size([2048])),
 ('weight_ih_l2', torch.Size([2048, 512])),
 ('weight_hh_l2', torch.Size([2048, 512])),
 ('bias_ih_l2', torch.Size([2048])),
 ('bias_hh_l2', torch.Size([2048]))]

x = torch.randn(8, 12, 64)
x1, (h1, c1) = lstm1(x)

x1.shape
&gt; torch.Size([8, 12, 512])

h1.shape
&gt; torch.Size([3, 8, 512])

c1.shape
&gt; torch.Size([3, 8, 512])
</code></pre>
<pre class=""lang-py prettyprint-override""><code># with projection
lstm_kwargs = {
    'input_size' : 64,
    'hidden_size' : 512,
    'num_layers' : 3,
    'batch_first' : True
}

lstm2 = nn.LSTM(proj_size=256, **lstm_kwargs)

[(k, v.shape) for k,v in lstm2.state_dict().items()]

&gt; [('weight_ih_l0', torch.Size([2048, 64])),
 ('weight_hh_l0', torch.Size([2048, 256])),
 ('bias_ih_l0', torch.Size([2048])),
 ('bias_hh_l0', torch.Size([2048])),
 ('weight_hr_l0', torch.Size([256, 512])),
 ('weight_ih_l1', torch.Size([2048, 256])),
 ('weight_hh_l1', torch.Size([2048, 256])),
 ('bias_ih_l1', torch.Size([2048])),
 ('bias_hh_l1', torch.Size([2048])),
 ('weight_hr_l1', torch.Size([256, 512])),
 ('weight_ih_l2', torch.Size([2048, 256])),
 ('weight_hh_l2', torch.Size([2048, 256])),
 ('bias_ih_l2', torch.Size([2048])),
 ('bias_hh_l2', torch.Size([2048])),
 ('weight_hr_l2', torch.Size([256, 512]))]

x = torch.randn(8, 12, 64)
x1, (h1, c1) = lstm1(x)

x1.shape
&gt; torch.Size([8, 12, 256])

h1.shape
&gt; torch.Size([3, 8, 256])

c1.shape
&gt; torch.Size([3, 8, 512])
</code></pre>
<p>Note that with projection, we have additional weight matrices for the projection, and the output/hidden sizes are changed. The cell size is not.</p>
<p>To your question about the output size, typically you would use another layer on top of the LSTM to predict your exact output.</p>
","2024-04-05 21:27:10","2","Answer"
"78282213","","pytorch: IndexError: index out of range in self","<p>I'm following this <a href=""https://github.com/bernhard-pfann/lad-gpt/"" rel=""nofollow noreferrer"">github code</a> to try and run the model using my own chats. I was able to fix a few things that initially didn't work for me (regex, encoding while leading the txt file)</p>
<p>I'm having 764 unique tokens in my file and I get this error when I run the run.py train --update function.</p>
<pre><code>Loaded existing model to continue training.
Parameters to be optimized: 4831966

Traceback (most recent call last):
  File &quot;E:\VS Code Projects\.venv\codes\new\lad-gpt\run.py&quot;, line 20, in &lt;module&gt;
    main()
  File &quot;E:\VS Code Projects\.venv\codes\new\lad-gpt\run.py&quot;, line 15, in main
    train.model_training(args.update)
  File &quot;E:\VS Code Projects\.venv\codes\new\lad-gpt\src\train.py&quot;, line 55, in model_training
    train_loss = estimate_loss(model, train_data)
  File &quot;E:\VS Code Projects\.venv\lib\site-packages\torch\utils\_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;E:\VS Code Projects\.venv\codes\new\lad-gpt\src\utils.py&quot;, line 23, in estimate_loss
    logits, loss = model(X, Y)
  File &quot;E:\VS Code Projects\.venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;E:\VS Code Projects\.venv\codes\new\lad-gpt\src\model.py&quot;, line 150, in forward
    tok_emb = self.token_embedding(idx)                     # (B, T, C)
  File &quot;E:\VS Code Projects\.venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;E:\VS Code Projects\.venv\lib\site-packages\torch\nn\modules\sparse.py&quot;, line 162, in forward
    return F.embedding(
  File &quot;E:\VS Code Projects\.venv\lib\site-packages\torch\nn\functional.py&quot;, line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
<p>what changes do I need to do to make this work?</p>
<p>I tried looking into the nn.embedding, vocab size etc but the code uses len(vocab) which eliminates any error with the embedding function. the embedding size is at 256 and i've tried both a higher and lower value and I still have the same error. I don't completely understand what's the error to proceed further.</p>
","2024-04-05 20:27:02","0","Question"
"78281918","","What's the difference with `hidden size` and `proj_size` in PyTorch LSTM?","<p>I recently starting exploring LSTMs in PyTorch and I don't quite understand the difference between using <code>hidden_size</code> and <code>proj_size</code> when trying to define the output size of my LSTM?</p>
<p>For context, I have an input size of 5, sequence length of 30 and want to have an output size of 2, two outputs of sequence length 30 each. Should I just set my <code>hidden_size</code> to 2 or would it be better to use <code>proj_size=2</code> so that I can tune the <code>hidden_size</code> hyper-parameter?</p>
","2024-04-05 19:06:18","1","Question"
"78280921","78279924","","<p>Gradients are computed on leaf tensors, which in this case are the <code>torch.arange</code> tensors themselves. You need to access the <code>grad</code> attribute of those tensors.</p>
<p>When you iterate over the items in the <code>torch.arange</code> tensors, you are getting a view of the tensor, which doesn't have the grad.</p>
<p>This works for what you're trying to do:</p>
<pre class=""lang-py prettyprint-override""><code>w_1_values = torch.arange(+2,+4,0.1, requires_grad=True)
w_2_values = torch.arange(-2,+4,0.1, requires_grad=True)

for w_1_value  in w_1_values:
    for w_2_value  in w_2_values:
        l_value = l(w_1_value, w_2_value)
        l_value.backward()

print(w_1_values.grad)
print(w_2_values.grad)
</code></pre>
","2024-04-05 15:27:46","2","Answer"
"78279924","","pytorch: autograd with tensors generated by arange","<p>I want to compute the gradient of a function in several points. However, If I use tensors generated with <code>torch.arange</code> the gradient is not computed. Instead, using classical tensors it works. Why?</p>
<pre><code>import torch
from torch import tensor

def l(w_1,w_2):
    return w_1*w_2

w_1 = tensor(3., requires_grad=True)
w_2 = tensor(5., requires_grad=True)
l_v = l(w_1, w_2)
l_v.backward()
print(l_v.item(), w_1.grad, w_2.grad) # HERE WORKS OK
#############
for w_1_value  in torch.arange(+2,+4,0.1, requires_grad=True):
    for w_2_value  in torch.arange(-2,+4,0.1, requires_grad=True):
        print(w_1_value, w_2_value)
        l_value = l(w_1_value, w_2_value)
        l_value.backward()
        print(l_value.item(), w_1_value.grad, w_2_value.grad) # HERE I GET NONE ON GRAD VALUES
        
</code></pre>
","2024-04-05 12:36:30","0","Question"
"78279823","","How exactly the forward and backward hooks work in PyTorch","<p>I am trying to understand how exactly code-wise the hooks operate in <code>PyTorch</code>. I have a model and I would like to set a forward and backward hook in my code. I would like to set a hook in my model after a specific layer and I guess the easiest way is to set a hook to this specific <code>module</code>. This introductory <a href=""https://youtu.be/syLFCVYua6Q?t=1207"" rel=""noreferrer"">video</a> warns that the backward module contains a bug, but I am not sure if that is still the case.</p>
<p>My code looks as follows:</p>
<pre><code>def __init__(self, model, attention_layer_name='desired_name_module',discard_ratio=0.9):
  self.model = model
  self.discard_ratio = discard_ratio
  for name, module in self.model.named_modules():
    if attention_layer_name in name:
        module.register_forward_hook(self.get_attention)
        module.register_backward_hook(self.get_attention_gradient)

  self.attentions = []
  self.attention_gradients = []

def get_attention(self, module, input, output):
  self.attentions.append(output.cpu())

def get_attention_gradient(self, module, grad_input, grad_output):
  self.attention_gradients.append(grad_input[0].cpu())

def __call__(self, input_tensor, category_index):
  self.model.zero_grad()
  output = self.model(input_tensor)
  loss = ...
  loss.backward()
</code></pre>
<p>I am puzzled to understand how code-wise the following lines work:</p>
<pre><code>module.register_forward_hook(self.get_attention)
module.register_backward_hook(self.get_attention_gradient)
</code></pre>
<p>I am registering a hook to my desired module, however, then, I am calling a function in each case without any input. My question is <code>Python</code>-wise, how does this call work exactly? How the arguments of the <code>register_forward_hook</code> and <code>register_backward_hook</code> operate when the function it's called?</p>
","2024-04-05 12:15:14","12","Question"
"78278630","78042907","","<p>I got the same error. I think I found the culprit in my case. The <code>DataLoader</code> was trying to process data in parallel, because I specified <code>worker=1</code>, which initiates parallel computing on one worker. If <code>worker=0</code> is set, the initialization is non-parallel on one device. At least this is how I understood it from <a href=""https://github.com/pytorch/pytorch/issues/21092"" rel=""nofollow noreferrer"">this post</a>.</p>
<p>As for your issue: I see no point where you set your data to be on GPU. Can you check this with <code>print(x.device)</code>, for your two tensors ? What about after using DataLoader ? Are <code>print(train_dataloader.data.device)</code> and <code>print(train_dataloader.target.device)</code> (or in the case of no separation <code>print(train_dataloader.dataset.device)</code>) also on &quot;cuda&quot; ?</p>
","2024-04-05 08:24:18","1","Answer"
"78278302","78277541","","<p>You could define C, add it to an optimizer (<em>eg.</em> <a href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"" rel=""nofollow noreferrer""><code>SGD</code></a>), and then minimize some distance (<em>eg.</em> <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html"" rel=""nofollow noreferrer""><code>MSELoss</code></a>) between <code>Ai = Bi @ C</code> at every iteration. Here is a minimal example:</p>
<pre><code>C = torch.rand(m,n)
optimizer = torch.optim.SGD([C], lr=0.1)

for a, b in zip(A,B):
    loss = F.mse(a, b@C)
    optimizer.zero_grad()    
    loss.backward()
    optimizer.step()
</code></pre>
","2024-04-05 07:14:28","1","Answer"
"78278187","78277541","","<p>If you know <code>A</code> and <code>C</code>, you can calculate <code>B</code> as <code>B = A@torch.linalg.pinv(C)</code></p>
","2024-04-05 06:47:22","2","Answer"
"78277541","","PyTorch how to factor a matrix until get 1 matrix that doesn't change?","<p>I'm having a matrix A, I'd like to factor it as A = BC while knowing that C is some matrix which isn't changed over multiple samples of A. How do I find B using PyTorch? A is a known matrix.</p>
<pre><code>A1 = B1 C
A2 = B2 C
A3 = B3 C
...
</code></pre>
","2024-04-05 03:02:56","-1","Question"
"78277414","78276910","","<p>The tensor dtype depends on what you intend to do with it.</p>
<p>&quot;number crunching&quot; layers like <code>nn.Linear</code>, <code>nn.Conv2d</code>, etc expect a <code>torch.float32</code> input, or <code>torch.float16</code> for half precision training.</p>
<p>&quot;number lookup&quot; layers like <code>nn.Embedding</code> expect the input to be <code>torch.int</code> or <code>torch.long</code>.</p>
<p>So long as the dtype is compatible with the layer that will be processing it, you're good.</p>
","2024-04-05 02:00:05","0","Answer"
"78277279","","AttributeError: module 'tensorflow' has no attribute 'Summary'","<p>When running this code:</p>
<pre><code>def scalar_summary(self, tag, value, step):
        &quot;&quot;&quot;Log a scalar variable.&quot;&quot;&quot;
        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])
        self.writer.add_summary(summary, step)
</code></pre>
<p>I get the error message:
<code>AttributeError: module 'tensorflow' has no attribute 'Summary'. Did you mean: 'summary'?</code></p>
<p>I am trying to run the code from <a href=""https://github.com/InhwanBae/ENet-SAD_Pytorch/blob/master/utils/tensorboard.py"" rel=""nofollow noreferrer"">https://github.com/InhwanBae/ENet-SAD_Pytorch/blob/master/utils/tensorboard.py</a> on Google colabs. I am trying to train the ENet-SAD model using the CULane dataset.</p>
","2024-04-05 00:51:51","0","Question"
"78277035","78276910","","<p>Typically you would use a long tensor for one-hot-encodings.
<br>Evidence of that can be seen with <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html"" rel=""nofollow noreferrer""><code>F.one_hot</code></a> which returns a <code>LongTensor</code>.</p>
","2024-04-04 23:10:04","0","Answer"
"78276910","","Which dtype for one-hot encoded features when converting them into pytorch tensors?","<p>The title explains most of my problem. I have a dataset with both categorical and quantitative features. My question is if it's best to assign the type <code>torch.float32</code> to the one-hot encoded features, which means that I can create one tensor for both the quantitative and the categorical (OH encoded) features, or if I should use <code>torch.bool</code> for the one-hot features, since they are all either 1 or 0.</p>
<p>If I were to use <code>torch.bool</code> it would complicate the creation of the model since I would need to create 2 &quot;pathways&quot;. I'm new to this so I don't know if using <code>torch.float32</code> would cause any issues or not.</p>
","2024-04-04 22:27:43","0","Question"
"78276187","78275784","","<p>As the error message suggests, the <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.ToPILImage.html"" rel=""nofollow noreferrer""><code>ToPILImage</code></a> transform operates on tensors that are either 2D <code>(H,W)</code> or 4D <code>(C, H, W)</code>. This means you have to iterate over the batch elements and apply the transform:</p>
<pre><code>imgs = [transform(t) for t in y_hat]
</code></pre>
<p>Alternatively, you can use <a href=""https://pytorch.org/vision/main/generated/torchvision.utils.make_grid.html"" rel=""nofollow noreferrer""><code>torchvision.utils.make_grid</code></a> to construct a grid from a list of tensors:</p>
<pre><code>img = transform(make_grid(y_hat))
</code></pre>
<p>The convenient <a href=""https://pytorch.org/vision/stable/generated/torchvision.utils.save_image.html"" rel=""nofollow noreferrer""><code>torchvision.utils.save_image</code></a> utility function is there to combine <code>make_grid</code>, the <code>PIL.Image</code> conversion, and saving to file system in one call:</p>
<pre><code>save_image(y_hat, 'pred.jpg')
</code></pre>
","2024-04-04 19:23:20","1","Answer"
"78275784","","""RuntimeError: Cannot access data pointer of Tensor that doesn't have storage"" when using PyTorch vmap","<p>Let's say I have a segmentation model (<code>model</code>) and I want to batch transform its predictions to pillow images. And, for simplicity, let's say everything is done on CPU (no GPU involved).</p>
<p>If I do:</p>
<pre><code>import torch
from torchvision.transforms import ToPILImage

transform = ToPILImage()
model.eval()
for i, (x, y) in enumerate(dataloader):
    y_hat = torch.sigmoid(model(x))  # returns a tensor (batch_size, 1, H, W)
    y_hat = (y_hat &gt; 0.5).float()
    img = transform(y_hat)
</code></pre>
<p>I get:</p>
<pre><code>ValueError: pic should be 2/3 dimensional. Got 4 dimensions.
</code></pre>
<p>Fair enough. Let me try using <code>vmap</code> to transform it as a batch:</p>
<pre><code>import torch
from torchvision.transforms import ToPILImage

transform = ToPILImage()
batch_transform = torch.func.vmap(transform)
model.eval()
for i, (x, y) in enumerate(dataloader):
    y_hat = torch.sigmoid(model(x))  # returns a tensor (batch_size, 1, H, W)
    y_hat = (y_hat &gt; 0.5).float()
    img = batch_transform(y_hat)
</code></pre>
<p>That produces the following error:</p>
<pre><code>RuntimeError: Cannot access data pointer of Tensor that doesn't have storage
</code></pre>
<p>Why does this behave this way? Does it have anything to do with the function I've chosen to vmap? I've followed the pattern that's in the <a href=""https://pytorch.org/docs/stable/generated/torch.func.vmap.html"" rel=""nofollow noreferrer"">documentation</a> and this should work. How can I perform this operation to a batch of images?</p>
","2024-04-04 18:05:09","1","Question"
"78275587","78272186","","<p>The short answer is the model internals are caching previous time-steps to avoid re-computing things unnecessarily. Do a word search for <code>cache</code> in their codebase and see what comes up.</p>
<p>Longer answer:</p>
<p>Imagine you do autoregressive generation naively. You start with an input of length <code>5</code>. You do a forward pass, generate a new token. You concatenate the new token to your input sequence. Now you have a sequence of length <code>6</code>. You start again, doing a full forward pass with an input of size <code>6</code>, predict a new token, and repeat.</p>
<p>This is extremely inefficient. If your transformer is using upper triangular masking (standard for causal language models), your activations at sequence position <code>n</code> only depend on the previous tokens <code>0, ... n-1</code>. This means you can compute the activations for that token once, cache them, and re-use the cached values for computing new tokens.</p>
<p>When computing a new token, you only need to reference previous token values for the attention operation (feed forward, norm, etc do not operate across tokens). So really you only need to cache the <code>k</code> and <code>v</code> activations from previous timesteps - hence the term KV cache.</p>
<p>When you run inference on a new timestep, you reference the cached KV values from old tokens for the attention steps. Then you update the KV cache with KV values from the new token.</p>
<p>This means you don't have to run a full forward pass on the full expanding sequence every time.</p>
","2024-04-04 17:26:53","1","Answer"
"78274606","78255992","","<p>I confused myself by reading the error message incorrectly.  Per the maintainer of torch_geometric - the inputs are meant to be float, and <em>not</em> long type:</p>
<p><a href=""https://github.com/pyg-team/pytorch_geometric/discussions/9135#discussioncomment-8998822"" rel=""nofollow noreferrer"">https://github.com/pyg-team/pytorch_geometric/discussions/9135#discussioncomment-8998822</a></p>
<pre><code>x = torch.tensor([[1,0],[2,4],[5,7]], dtype=torch.float)  
ei = torch.tensor([[1, 1],[0,2]], dtype=torch.float)
</code></pre>
","2024-04-04 14:36:27","0","Answer"
"78273900","78273851","","<p>you can use <code>*tensor.shape</code> to expand the values as parameters</p>
<pre><code>tensor1 = torch.Tensor(x,y)
tensor2 = torch.Tensor(k, *tensor1.shape)
</code></pre>
","2024-04-04 12:37:36","2","Answer"
"78273889","78199810","","<p>Firstly, you should extract significant keywords from the questions. Then, you can perform document ranking based on these keywords from data sources like websites, reports, or Wikipedia using methods like BM25 or frequency-based approaches. Alternatively, you can use context-based ranking using embeddings. Finally, you can utilize a Q&amp;A model based on transformers to address the questions effectively.</p>
","2024-04-04 12:35:33","0","Answer"
"78273851","","How can I create a new tensor with shape ( k, x.shape), where x is another tensor?","<p>Having tensor <code>x</code>, I want to create a tensor <code>y</code> of the shape <code>(k, x.shape)</code>. However, <code>y = torch.empty((k, x.shape))</code> does not work, since <code>(k, x.shape)</code> is a tuple of type <code>(int, torch.Size)</code>. Is there no possibility of achieving this?</p>
<p>Naturally, I could use:</p>
<pre><code>y = torch.empty((k, x.shape[0], x.shape[1], x.shape[2], x.shape[3]))
</code></pre>
<p>But this requires knowing the number of dimensions in <code>x</code>...</p>
","2024-04-04 12:28:00","0","Question"
"78273826","78273684","","<p>According to your comment, the maximum CUDA version supported by your GPU is <code>10.1</code>. You are using a PyTorch version shipped with a version <code>11.8</code> CUDA toolkit, which is higher than <code>10.1</code>.</p>
<p>Therefore, you must install a Pytorch version compiled with a CUDA toolkit version lower or equal to <code>10.1</code>. For your information, the latest PyTorch version that follows this requirement is <a href=""https://pytorch.org/get-started/previous-versions/#v181"" rel=""nofollow noreferrer""><code>1.8.1</code></a> (one of them is compiled with a CUDA toolkit version of <code>10.1</code>):</p>
<pre><code>pip install torch==1.8.1+cu101 torchvision==0.9.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>Alternatively, you can update your CUDA driver, if your GPU allows it. In that case, your maximum supported CUDA version will be raised and you should be able to run on a PyTorch version <code>&gt; 1.8.1</code>. Of course, this will depend on your new driver version.</p>
","2024-04-04 12:23:31","2","Answer"
"78273684","","I am getting this error whenever I run program with pytorch","<blockquote>
<p>The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: <a href=""http://www.nvidia.com/Download/index.aspx"" rel=""nofollow noreferrer"">http://www.nvidia.com/Download/index.aspx</a> Alternatively, go to: <a href=""https://pytorch.org"" rel=""nofollow noreferrer"">https://pytorch.org</a> to install a PyTorch version that has been compiled with your</p>
</blockquote>
<p>I expect to run the program but instead it starts using CPU which becomes a lot slower for execution.</p>
","2024-04-04 11:58:09","-3","Question"
"78272186","","Understanding Change in Output Tensor Shape during Causal Inference in Gemma Model's MLP Block","<p>I am printing the shape of the output tensor of the MLP block during causal inference of Gemma model for a given input. What I observe is that during first token generation, the shape is <em>(batch_size, input_seq_length, hidden_size)</em>, but from the subsequent token generations, the shape changes to <em>(batch_size, 1, hidden_size)</em>. For example, consider a given input sequence of length 5 and a desired output length of 2:</p>
<p><a href=""https://i.sstatic.net/VaGYM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VaGYM.png"" alt=""enter image description here"" /></a></p>
<p>Why does this happen?  My understanding is that during the first token inference, the model processes the entire input sequence through a Gemma_Decoder block, generating a <code>&lt;SOS&gt;</code> (Start of Sentence) token while obtaining token embeddings for each input sequence. However, for subsequent token generations, it only utilizes the last token generated to produce a new token, retrieving information about previous tokens through the kv cache built over time during inference.</p>
<p>I would love to understand it in more depth, so if anyone can provide with links to resources, it would be of great help.</p>
","2024-04-04 06:50:44","0","Question"
"78270162","78270035","","<p><code>Tot Alloc</code> and <code>Tot Freed</code> show the total amount of memory allocated or freed over the memory snapshot. They are accumulated stats.</p>
<p><code>Cur Usage</code> shows how much memory is currently being used by your process. <code>Peak Usage</code> shows the highest amount of memory used at a single time.</p>
<p><code>Peak Usage</code> is the main value you should be concerned about - you will get a cuda memory error if this value tries to exceed your GPU's memory.</p>
<p>Additionally, the cuda memory profiler profiles cuda memory, not system memory. The values shown have nothing to do with system/CPU memory.</p>
","2024-04-03 19:46:45","3","Answer"
"78270035","","Pytorch CUDA Allocated memory is going into 100's of GB","<p>I am trying to get inference from HuggingFace Transformer model running using Pytorch Framework. I have a GPU instance running and when I am checking the cuda memory summary, I find that <strong>allocated memory (Total Allocation) is increasing by 100's of GB's</strong> with each inference e.g. <em>after 2nd inference Allocated memory (Total Allocation)  was 19GB, with 3rd inference Allocated memory (Total Allocation)  was 205GB</em>. This total allocation is freed up. The memory maps don't show any anomalous pattern. Current usage and peak usage from nearly constant. My inference sagemaker instance has 128GB of CPU memory only and 24 GB of GPU memory.</p>
<p><a href=""https://i.sstatic.net/AkjAm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AkjAm.jpg"" alt=""enter image description here"" /></a></p>
<p>So, I have three queries/concerns:</p>
<ol>
<li>How is it possible that total allocation memory is more than sagemaker instance on which inference is running.</li>
<li>How do i control this anomalous behaviour?</li>
<li>Is this a concern that I need to rectify, as the inference seems to be running fine.</li>
</ol>
","2024-04-03 19:22:53","0","Question"
"78266394","78266223","","<p>You can construct a mask depending on the norm of <code>A</code> along the last dimension and use <a href=""https://pytorch.org/docs/stable/generated/torch.where.html"" rel=""nofollow noreferrer""><code>torch.where</code></a> to assemble to desired tensor:</p>
<pre><code>&gt; torch.where(A.norm(dim=-1, keepdim=True).bool(), A, B)
tensor([[[[0., 0., 1.],
          [1., 2., 1.],
          [0., 1., 0.]]],


        [[[2., 0., 0.],
          [0., 1., 1.],
          [1., 1., 1.]]]])
</code></pre>
","2024-04-03 08:43:18","0","Answer"
"78266223","","Replace the row with a norm of 0 in the tensor with the corresponding row in another tensor","<p>I now have a pytorch tensor A of dimensions (2000, 1, 360, 3). I'm trying to find all indexes with norms of 0 in the last dimension of this tensor. And replace these positions with the values of the corresponding positions in another tensor B (the same dimension as A).</p>
<p>Example (A, B: (2, 1, 3, 3))</p>
<pre><code>A = [[[[0, 0, 0],  # norm == 0
       [1, 2, 1],
       [0, 1, 0]]],
     [[[2, 0, 0],
       [0, 0, 0],  # norm == 0
       [1, 1, 1]]]]
B = [[[[0, 0, 1],
       [1, 1, 1],
       [0, 1, 0]]],
     [[[1, 0, 0],
       [0, 1, 1],
       [2, 1, 1]]]]
</code></pre>
<p>Expected result:</p>
<pre><code>new_A = [[[[0, 0, 1],   # &lt;-- replaced
           [1, 2, 1],
           [0, 1, 0]]],
         [[[2, 0, 0],
           [0, 1, 1],   # &lt;-- replaced
           [1, 1, 1]]]]
</code></pre>
","2024-04-03 08:10:55","0","Question"
"78265535","78259244","","<p>It's because you didn't call <code>eval()</code> function. The <code>dropout</code> layer is a stocastic process. In each run the outputs will be randomly set to 0, based on your preset probabilities. Unless you call <code>eval()</code> function to disable this behavior, I bet your models will output different results after each run even if they are using the same device.</p>
","2024-04-03 06:05:27","0","Answer"
"78260163","78259907","","<p>If you replace your classifier with the identity function, you will see what the problem is:</p>
<pre><code>model.classifier = nn.Identity()
model(torch.rand(2,3,512,512)).shape
torch.Size([2, 492032])
</code></pre>
<p>The <code>in_features</code> of your classifier linear layer should be <code>492032</code>, not <code>1280</code>. Beside, if you compare with the source code of <code>SqueezeNet</code>, you will see that <code>model.classfier</code> does not contain a linear layer but a convolutional layer followed by a pooling layer, <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/squeezenet.py#L81-L83"" rel=""nofollow noreferrer"">line 81</a>:</p>
<pre><code>final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)
self.classifier = nn.Sequential(
   nn.Dropout(p=dropout), 
   final_conv, 
   nn.ReLU(inplace=True), 
   nn.AdaptiveAvgPool2d((1, 1)))
</code></pre>
<p>Considering the kernel size is of shape <code>1x1</code>, it acts as a linear layer. You could therefore replace your implementation with the following code:</p>
<pre><code>model.classifier = nn.Sequential(
    nn.Dropout(p=dropout), 
    nn.Conv2d(512, len(class_names), kernel_size=1),
    nn.ReLU(inplace=True), 
    nn.AdaptiveAvgPool2d((1, 1)))
</code></pre>
","2024-04-02 09:12:04","0","Answer"
"78259907","","(Pytorch) mat1 and mat2 shapes cannot be multiplied (212992x13 and 1280x3)","<p>I am trying to do transfer learning on Pytorch pretrained models with custom dataset.
Presently, I am getting an error as
<code>mat1 and mat2 shapes cannot be multiplied (212992x13 and 1280x3) </code>
during training the custom model.</p>
<p>When I try using efficient net, the below code works and it trains successfully, but when I use models like squeeze net I get an error</p>
<p>Works:</p>
<pre><code>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
model = torchvision.models.efficientnet_b0(weights=weights).to(device)
</code></pre>
<p>Does not work:</p>
<pre><code>weights = torchvision.models.SqueezeNet1_0_Weights.DEFAULT
model = torchvision.models.squeezenet1_0(weights=weights).to(device)
</code></pre>
<p>Train:</p>
<pre><code>auto_transforms = weights.transforms()
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=auto_transforms, batch_size=32)

for param in model.features.parameters():
    param.requires_grad = False #Freeze layers

torch.manual_seed(42)
output_shape = len(class_names)
model.classifier = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True),
    torch.nn.Linear(in_features=1280,
                    out_features=output_shape,
                    bias=True)).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

#ERROR DURING TRAIN
results = engine.train(model=model, train_dataloader=train_dataloader, test_dataloader=test_dataloader, optimizer=optimizer, loss_fn=loss_fn, epochs=100, device=device)
</code></pre>
<p>The training image size is 512x512</p>
<p>To make sure that this is not a problem of transforms, I have used autotransforms but still the problem persists.</p>
<p>Although there exists a similar topic <a href=""https://stackoverflow.com/questions/72724452/mat1-and-mat2-shapes-cannot-be-multiplied-128x4-and-128x64"">mat1 and mat2 shapes cannot be multiplied (128x4 and 128x64)</a>, it is based completely on creating a new Sequential model, whereas I am trying to use transfer learning on pretrained model.</p>
","2024-04-02 08:24:45","-1","Question"
"78259244","","torch DataParallel model predict same data is different between single GPU or CPU","<h2>problem</h2>
<p>I have use torch.nn.DataParallel to use multi GPU train model, but I notice, when I use two cuda to calculate result , it is different from use single cuda or cpu. It is stange, and I don`t know why, please tell me how to fix it or explain it.</p>
<h2>system</h2>
<p>PyTorch: 2.0.0+cu117</p>
<p>CUDA Version: 12.0</p>
<h2>result</h2>
<p>You can use follow code to reproduce  this problem， and it will print like this, Multi-card results are very different from single-card results or CPU results.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>multi GPU</th>
<th>single GPU</th>
<th>CPU</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.436291307</td>
<td>0.307008564</td>
<td>0.307096988</td>
</tr>
<tr>
<td>0.618099451</td>
<td>0.502333581</td>
<td>0.498412549</td>
</tr>
<tr>
<td>-0.230154008</td>
<td>-0.209866986</td>
<td>-0.206241921</td>
</tr>
<tr>
<td>-0.556713521</td>
<td>-0.558704913</td>
<td>-0.555095673</td>
</tr>
<tr>
<td>-0.30781576</td>
<td>-0.209164187</td>
<td>-0.211295918</td>
</tr>
<tr>
<td>-0.410598308</td>
<td>-0.512709379</td>
<td>-0.513415456</td>
</tr>
<tr>
<td>0.298149496</td>
<td>0.474029034</td>
<td>0.468837589</td>
</tr>
<tr>
<td>-0.270193249</td>
<td>-0.148075178</td>
<td>-1.52E-01</td>
</tr>
<tr>
<td>-0.05058343</td>
<td>-0.017191991</td>
<td>-0.019832365</td>
</tr>
<tr>
<td>0.619232118</td>
<td>6.59E-01</td>
<td>6.56E-01</td>
</tr>
<tr>
<td>-0.036778659</td>
<td>-0.135650381</td>
<td>-0.132507876</td>
</tr>
</tbody>
</table></div>
<h3>code</h3>
<pre class=""lang-py prettyprint-override""><code>
import torch
import torchvision

def load_data(num_gpus):
    transforms = torchvision.transforms.Compose([
    torchvision.transforms.Resize(256),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                                                 ])
#     dataset = torchvision.datasets.ImageFolder(root='datasets/cifar-10-python.tar.gz', transform=transforms)
    dataset = torchvision.datasets.CIFAR10('datasets', train=True,  transform=transforms,download=True)

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=64,
        shuffle=False,
        num_workers=4*num_gpus
                                                )
    return dataloader



model = torchvision.models.resnet50(pretrained=False)
model = model.cuda()
model = torch.nn.parallel.DataParallel(model, device_ids=[0,1], dim=0)

dataloader = load_data(2)

m_s = model.module.state_dict()

model1 = torchvision.models.resnet50(pretrained=False)
model1.load_state_dict(m_s)


model2 = torchvision.models.resnet50(pretrained=False)
model2.load_state_dict(m_s)
model2 = model2.to('cuda:2')

for images, labels  in dataloader:
    break


multi_gpu = model(images.cuda())
cpu_predict = model1(images)
s_gpu = model2(images.to('cuda:2'))

a = multi_gpu.reshape(-1).cpu()
b = cpu_predict.reshape(-1)
c = s_gpu.reshape(-1).cpu()

for i,j,k in zip(a,b,c):
    print(i.item(),j.item(),k.item())
</code></pre>
","2024-04-02 06:05:53","0","Question"
"78257177","78256167","","<p>Taking your desired output</p>
<pre><code>word 2: pxhs[1, 0, 131] * pxhs[1, 1, 132] * pxhs[1, 2, new_word_token_ids].sum()
</code></pre>
<p>The idea is to split up the computation in two parts: the <code>token_list</code> probabilities (<code>pxhs[1, 0, 131] * pxhs[1, 1, 132]</code>) and the <code>new_word_token_ids</code> probabilities (<code>pxhs[1, 2, new_word_token_ids].sum()</code>).</p>
<p>I'm assuming you have access to a <code>n_words</code>-length <code>seq_lens</code> tensor that stores the index of the first padding token for each sequence. I also assume all objects are tensors, and that the maximum sequence length (<code>max_len</code>) below is not too long (otherwise the for-loop will be a bottleneck).</p>
<p>I'm using log probabilities for numerical stability; you should be able to convert back without issue.</p>
<pre class=""lang-py prettyprint-override""><code>n = len(word_token_list)
max_len = seq_lens.max()
seq_idxs = torch.arange(n)
log_pxhs = pxhs.log()

# Step 1:
# Compute the probability of the sequences in `word_token_list`
# (clever indexing could also vectorize this at the expense of intelligibility)
log_p_x = torch.zeros(n)
log_p_x = torch.zeros(n)
for i in range(max_len):
    tok_idxs = word_token_list[:, i]
    log_p_x += log_pxhs[seq_idxs, i, tok_idxs] * (i &lt; seq_lens)

# Step 2:
# Compute the probability of the `new_word_token_ids` at the end of the sequence
v = pxhs.shape[2]
v_mask = torch.isin(torch.arange(v), new_word_token_ids, assume_unique=True)
# mask pxhs to only include nonzero values for `new_word_token_ids`
p_new = pxhs * v_mask
# index those probabilities for the end of the sequence
p_new_given_x = p_new[seq_idxs, seq_lens].sum(1)

# Step 3: compute the final log-probability
print(log_p_x + p_new_given_x.log())
</code></pre>
<p>I didn't benchmark things but I think it should lead to a substantial speedup.</p>
","2024-04-01 18:35:50","0","Answer"
"78256322","78220369","","<p>First, to clarify, the traditional encoder-decoder is encoder + cross-attention + decoder (like the EncoderDecoderModel from Huggingface), and from my understand the ICAE paper you mentioned is basically a LLaVA-like design (correct me if I'm wrong), where you concatenate the encoder's output -- projected image features for LLaVA or compressed language features for ICAE -- with the normal prompt for a LLM. There is no cross attention.</p>
<p>If you just want to test out the idea quickly, I would recommend you to start with ICAE's codebase so you can get results quick, reliable, and can benchmark them. Or, if you really want to set up your own code, you will need to go in to the OPT's source code and write the corresponding code to 1. use the encoder of your choice to encode/compress the context and optionally 2. use your choice of projection method to project the output of encoder and 3. prepend the projected output (one token in your case) to OPT's normal token sequences. So I would imagine that in the forward() in the OPT source <a href=""https://github.com/huggingface/transformers/blob/c9f6e5e35156e068b227dd9b15521767f6afd4d2/src/transformers/models/opt/modeling_opt.py#L727"" rel=""nofollow noreferrer"">code</a> you can add another parameter for your projected BERT cls token, then do something like <code>inputs_embeds=torch.cat([projected_cls, inputs_embeds], dim=1)</code>, and adjust the attention mask accordingly. The huggingface pipeline is only for high-level usage and doesn't suffice your need.</p>
","2024-04-01 15:27:18","1","Answer"
"78256222","78210297","","<p>So, the solution was to save model and it's weights by using save_pretrained not by torch.save()</p>
","2024-04-01 15:06:01","2","Answer"
"78256167","","Indexing into torch tensor with variable length indices along an axis","<p>I'm trying to compute the word probabilities of a list of tokenized words according to a language model, and I need some fancy indexing.</p>
<p>My inputs, illustrated with toy example below:</p>
<ul>
<li>token_list: n_words x max_tokenization_length (e.g., three words where the max tokenization length is 3)</li>
<li>pxhs: n_words x (max_tokenization_length + 1) x |vocabulary|, (e.g. three words, four sets of logits for 3+1 tokens, and dimension 1000 vocab)</li>
<li>next_word_token_ids: list of tokens that constitute a new word (e.g., all tokens that start with a space character).</li>
</ul>
<pre><code>pxhs = torch.rand((3,4,1000))

pad_token_id = tokenizer.pad_token_id
word_token_list = [
    [120, pad_token_id, pad_token_id],
    [131, 132, pad_token_id],
    [140, 141, 142],
]

new_word_token_ids = [0,1,2,3,5]
</code></pre>
<p>Desired output is a length 3 list of word probabilities computed as follows:</p>
<pre><code>word 1: pxhs[0, 0, 120] * pxhs[0, 1, new_word_token_ids].sum()
word 2: pxhs[1, 0, 131] * pxhs[1, 1, 132] * pxhs[1, 2, new_word_token_ids].sum()
word 3: pxhs[2, 0, 140] * pxhs[2, 1, 141] * pxhs[2, 2, 142] * pxhs[2, 3, new_word_token_ids].sum()
</code></pre>
<p>In practice, I want to index by replacing the first pad_token_id with the new word token ids, and then nothing (this doesn't work as an index, just illustrating):</p>
<pre><code>actual_idx = [
    [[120], new_word_token_ids, [None], [None]],
    [[131], [132], new_word_token_ids, [None]],
    [[140], [142], [143], new_word_token_ids],
]
</code></pre>
<p>I wrote a very slow function that does this:</p>
<pre><code>all_word_probs = []
for word_tokens, word_probs in zip(token_list, pxhs):
    counter=0
    p_word=1
    while (counter &lt; len(word_tokens) and 
            word_tokens[counter] != tokenizer.pad_token_id):
        p_word = p_word * word_probs[counter, word_tokens[counter]]
        counter+=1
    new_word_prob = word_probs[counter, new_word_tokens].sum()
    p_word = p_word * new_word_prob
    all_word_probs.append(p_word)
</code></pre>
<p>I need something faster, thanks in advance for your help!</p>
","2024-04-01 14:55:24","1","Question"
"78255992","","PyTorch Geometric SAGEConv - Expected scalar type Long, but found Float?","<p>I am trying to implement graph neural networks from the torch_geometric library of model types.  I am receiving an error: &quot;RuntimeError: expected scalar type Long but found Float&quot; in this line of the SAGEConv module:</p>
<pre><code>&quot;(My Path)\Python310\lib\site-packages\torch_geometric\nn\dense\linear.py&quot;, line 147, in forward
    F.linear(x, self.weight, self.bias)
RuntimeError: expected scalar type Long but found Float
</code></pre>
<p>I realize that there are similar questions on Stack Overflow, but I don't think that there is clear guidance regarding how to troubleshoot effectively in torch_geometric.  I tried to reduce my problem to the simplest code possible:</p>
<p>First, I import SAGEConv:</p>
<pre><code>import torch
from torch_geometric.nn import SAGEConv
sconv = SAGEConv((-1, -1), 64)
</code></pre>
<p>Then, I create very basic node and edge tensors for the graph:</p>
<pre><code>x = torch.tensor([[1,0],[2,4],[5,7]]) # Three Node Graph; Two &quot;features&quot; per node
ei = torch.tensor([[1, 1],[0,2]]) # Edge Index Matrix - Node 1 to Node 0 and Node 1 to Node 2
</code></pre>
<p>Finally, I call my SAGEConv layer</p>
<pre><code>sconv(x, ei)
&gt;&gt;&gt; (...) RuntimeError: expected scalar type Long but found Float
</code></pre>
<p>I can't wrap my head around this because both &quot;x&quot; and &quot;ei&quot; variables are LongTensor type:</p>
<pre><code>x.type()
&gt;&gt;&gt; 'torch.LongTensor'
ei.type()
&gt;&gt;&gt; 'torch.LongTensor'
</code></pre>
<p>This has been driving me a little crazy.  Any assistance with finding what I've done wrong would be extremely appreciated.  In case of a version issue, here is my pip freeze for the packages:</p>
<pre><code>torch==2.0.1
torch_geometric==2.5.0
</code></pre>
<p>Edit 1:</p>
<p>I upgraded both <code>torch</code> and <code>torch_geometric</code> to the most recent versions and the error still exists.  Though the message is now &quot;expected m1 and m2 to have the same dtype, but got: __int64 != float&quot;</p>
","2024-04-01 14:18:15","-1","Question"
"78254720","78254492","","<p>The error message indicates that the object you are interacting with does not have a <code>shape</code> attribute and is an instance of <a href=""https://pillow.readthedocs.io/en/stable/reference/Image.html"" rel=""nofollow noreferrer""><code>PIL.Image</code></a>. What you can do is either avoiding to convert by to image with <code>Image.from_array</code> and convert directly to <code>Tensor</code> (check the array has the correct type though), or you can use <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html"" rel=""nofollow noreferrer""><code>ToTensor</code></a> or <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.functional.to_tensor.html"" rel=""nofollow noreferrer""><code>tf.to_tensor</code></a>:</p>
<pre><code>img_prime = tf.to_tensor(img_prime)
</code></pre>
","2024-04-01 09:46:07","0","Answer"
"78254690","78253997","","<p>If you look into the source code of <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py#L160"" rel=""nofollow noreferrer""><code>VisionTransformer</code></a>, you will notice in <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py#L235-L243"" rel=""nofollow noreferrer""><strong>this section</strong></a> that <code>self.heads</code> is a sequential layer, not a linear layer. By default, it only contains a single layer <code>head</code> corresponding to the final classification layer. To overwrite this layer, you can do:</p>
<pre><code>heads = self.vit_b_16.heads
heads.head = nn.Linear(heads.head.in_features, num_classes)
</code></pre>
","2024-04-01 09:39:19","2","Answer"
"78254492","","'Image' object has no attribute 'shape'","<p>I am trying to implement prime augmentation on my dataset. Previously I was getting error raise ValueError('pic should be 2/3 dimensional. Got {} dimensions.'.format(pic.ndim))
ValueError: pic should be 2/3 dimensional. Got 4 dimensions. The first print gives the shape: img_prime shape: torch.Size([3, 3, 320, 320]), so I removed the first element which is probably the batch size, Then I converted the tensor array to numpy and transposed so that the shape becomes (320,320,3) i.e in a (H,W,C) format.</p>
<pre><code>class PrimeAugment:
    def __init__(self, prime_module):
        self.prime_module = prime_module

    def __call__(self, img, mask):
        img = transforms.ToTensor()(img).unsqueeze(0)  # Convert PIL image to tensor and add batch dimension
        img_prime = self.prime_module(img)  # Apply PRIME augmentations
        print(&quot;img_prime shape:&quot;, img_prime.shape)  # Print the shape of img_prime
        img_prime = img_prime[0, :, :, :]
        img_prime = img_prime.detach().cpu().numpy().transpose(1, 2, 0)
        img_prime = (img_prime * 255).astype(np.uint8)  # Convert data type to uint8
        print(&quot;shape after remove:&quot;, img_prime.shape)
        img_prime = Image.fromarray(img_prime)  # Convert back to PIL image
        return img_prime, mask
</code></pre>
<p>But now I am getting an error:</p>
<pre><code>    params.update({&quot;cols&quot;: kwargs[&quot;image&quot;].shape[1], &quot;rows&quot;: kwargs[&quot;image&quot;].shape[0]})
AttributeError: 'Image' object has no attribute 'shape'
</code></pre>
<p>Whole trace of the error:</p>
<pre><code>Original Traceback (most recent call last):
  File &quot;/home/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py&quot;, line 308, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/home/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;/home/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 51, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;/home/Crack-PRIME-final3/tool/dataset.py&quot;, line 225, in __getitem__
    image_store, mask_store = image_mask_transformation(image, mask, self.img_trans, self.aug_trans)
  File &quot;/home/Crack-PRIME-final3/tool/dataset.py&quot;, line 180, in image_mask_transformation
    transformed = img_norm(image=image)
  File &quot;/home/anaconda3/envs/myenv/lib/python3.9/site-packages/albumentations/core/transforms_interface.py&quot;, line 118, in __call__
    return self.apply_with_params(params, **kwargs)
  File &quot;/home/anaconda3/envs/myenv/lib/python3.9/site-packages/albumentations/core/transforms_interface.py&quot;, line 125, in apply_with_params
    params = self.update_params(params, **kwargs)
  File &quot;/home/anaconda3/envs/myenv/lib/python3.9/site-packages/albumentations/core/transforms_interface.py&quot;, line 175, in update_params
    params.update({&quot;cols&quot;: kwargs[&quot;image&quot;].shape[1], &quot;rows&quot;: kwargs[&quot;image&quot;].shape[0]})
AttributeError: 'Image' object has no attribute 'shape'
</code></pre>
<p>from dataset.py:</p>
<pre><code>179     img_norm = A.Normalize(config.MEAN, config.STD,  p=1.0)
180     transformed = img_norm(image=image)
181     image = transformed[&quot;image&quot;]
182
</code></pre>
","2024-04-01 08:51:59","0","Question"
"78253997","","vision transformers: RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x1000 and 768x32)","<p>I am trying to do Regression on the vision transformers model and I cannot replace the last layer of classification with the regression layer</p>
<pre><code>class RegressionViT(nn.Module):
    def __init__(self, in_features=224 * 224 * 3, num_classes=1, pretrained=True):
        super(RegressionViT, self).__init__()
        self.vit_b_16 = vit_b_16(pretrained=pretrained)
        # Accessing the actual output feature size from vit_b_16
        self.regressor = nn.Linear(self.vit_b_16.heads[0].in_features, num_classes * batch_size)

    def forward(self, x):
        x = self.vit_b_16(x)
        x = self.regressor(x)
        return x


# Model
model = RegressionViT(num_classes=1)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)

criterion = nn.MSELoss()  # Use appropriate loss function for regression
optimizer = optim.Adam(model.parameters(), lr=0.0001)

</code></pre>
<p>I get this error when I try to initialize and run the model</p>
<pre><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x1000 and 768x32)
</code></pre>
<p>The problem is that there is a mismatch between the regression layer and the <strong>vit_b_16</strong> model layer, what would be the correct way to solve this issue</p>
","2024-04-01 06:33:32","-2","Question"
"78253819","78211526","","<p>I was able to recreate your problem on Databricks with the following cluster:</p>
<ul>
<li>Runtime: <em>14.1 ML (includes Apache Spark 3.5.0, GPU, Scala 2.12)</em></li>
<li>Worker Type: <em>Standard_NC16as_T4_v3 / Standard_NC6s_vs</em></li>
<li>Driver Type: <em>Standard_NC16as_T4_v3 / Standard_NC6s_vs</em></li>
</ul>
<p>And then building on top of all the answers here already I was able to overcome your problem by the following:</p>
<ol>
<li>Upgrade your transformers library via: <code>!pip install -–upgrade git+https://github.com/huggingface/transformers</code></li>
<li>Upgrading your torch version via: <code>!pip install -–upgrade torch torchvision</code></li>
<li>Upgrading your accelerate version via: <code>!pip install -–upgrade accelerate</code></li>
<li>Using a specific version of the datasets library via: <code>!pip install datasets==2.16.0</code></li>
</ol>
<blockquote>
<p>I'm not sure if it matters but the order I used of the commands above are: <code>4 &gt;&gt; 1 &gt;&gt; 3 &gt;&gt; 2</code></p>
</blockquote>
<p>This makes your problem go away and works on both <code>transformers.Trainer</code> and also <code>SFTTrainer</code> that I saw in your article imported but never used.</p>
","2024-04-01 05:32:40","1","Answer"
"78253733","78250811","","<p>For short: Nope, each output channel take some information from all input channels, and the number of them is exactly what you defined, no more, no less.</p>
<p>For detail, look at convolution this way:</p>
<ul>
<li>For each pixel (or latent pixel), you have a feature vector (across the channel dimension), then you can simply think it as a Linear layer, every output channel take some information from all input channels. This is true for kernel size of 1.</li>
<li>For larger kernel size, you can flatten the input local feature map to a feature vector (both the height and width to the channel dimension). This is exactly what is happening with current implementation of convolution: strided-trick to flatten overlapped feature maps and doing matmul with a matrix-flatten kernel.</li>
</ul>
<p>I personally find this view easier to understand the dense convolution in deep learning, as mostly what is taught in school usually focus on single channel and the sliding property of the kernel, not about how the channel dimension work on aggregating the information.</p>
","2024-04-01 04:56:59","0","Answer"
"78253235","78211526","","<p>The ecosystem is a bit shaky,
try</p>
<pre><code>pip install git+https://github.com/huggingface/transformers --upgrade
pip install git+https://github.com/huggingface/peft --upgrade
</code></pre>
","2024-04-01 00:40:13","0","Answer"
"78251371","78242743","","<p>Try this:</p>
<pre><code>self.flatten = nn.Flatten()
self.linear1 = nn.Linear(in_features=hidden_units*7*7, out_features=output_shape))

def forward(self, x:torch.Tensor): 
x = self.block_1(x)
x = self.block_2(x)
x = self.flatten(x)
x = self.linear1(x)
return x
</code></pre>
<p>If you don't want to add flatten layer to your model, you can simply do it in forward function:</p>
<pre><code>def forward(self, x:torch.Tensor):
x = self.block_1(x)
x = self.block_2(x)
x = x.view(x.size(0), -1)
x = self.linear1(x)
return x
</code></pre>
","2024-03-31 12:55:27","0","Answer"
"78250917","77912547","","<p>I've used with success this in one of the <code>CMakeLists.txt</code> file:</p>
<pre><code>list(APPEND CMAKE_PREFIX_PATH C:\\ThePath)
</code></pre>
","2024-03-31 09:52:54","1","Answer"
"78250811","","The meaning of an out_channel in nn.Conv2d pytorch","<p>I was studying convolutional neural networks, and creating a module.
While using nn.conv2d, I can't figure out exactly how to specify my out_channels.</p>
<p>I understand that it is the number of filters that are going to be used.
For a black and white picture, (meaning in_channels to be 1), if I put 8 output_channels, does it mean that I will get 8 different feature maps?</p>
<p>Also if I increase my input_channel to 3, let's say RGB, does this mean that I will get total of 24 (3*8 = 24) feature maps, 8 red, 8 green, 8 black?</p>
","2024-03-31 09:14:33","0","Question"
"78247724","78211526","","<p>Upgrading to torch-2.1.2 will resolve that error.</p>
","2024-03-30 11:26:00","0","Answer"
"78247495","78247234","","<p>You need to pass the height and width as follows:</p>
<pre class=""lang-py prettyprint-override""><code>output_image = pipe(
    prompt,
    image,
    mask,
    height,
    width,
    #strength=noise,
    guidance_scale=cfg
)
</code></pre>
<p>If you check the source code - the <a href=""https://github.com/huggingface/diffusers/blob/f0c81562a43c183f856d7fda2b68cabc86d6a4df/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py#L324"" rel=""noreferrer"">height</a> and <a href=""https://github.com/huggingface/diffusers/blob/f0c81562a43c183f856d7fda2b68cabc86d6a4df/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py#L325"" rel=""noreferrer"">width</a> defaults to 512.</p>
","2024-03-30 09:57:02","6","Answer"
"78247259","","Attribute error of train.py file while training YOLOv9-e","<p>When starting the training of yolov9-e or yolov9-c I always get this error:</p>
<pre><code>`your text`Traceback (most recent call last):
           File &quot;/content/yolov9/train.py&quot;, line 634, in &lt;module&gt;
           main(opt)
           File &quot;/content/yolov9/train.py&quot;, line 528, in main
           train(opt.hyp, opt, device, callbacks)
           File &quot;/content/yolov9/train.py&quot;, line 304, in train
           loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size
           File &quot;/content/yolov9/utils/loss_tal.py&quot;, line 168, in __call__
           pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in           feats], 2).split(
           File &quot;/content/yolov9/utils/loss_tal.py&quot;, line 168, in &lt;listcomp&gt;
           pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in  feats], 2).split(
           AttributeError: 'list' object has no attribute 'view'
</code></pre>
<p>I am trying to train it using the code snippet from the roboflow platform. Based on the videos I watched nobody got this error when training these two models using the basic code snipped of roboflow.</p>
<p>I tried clearing the GPU memory with:</p>
<pre><code>import torch
import os

torch.cuda.empty_cache()
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
</code></pre>
","2024-03-30 08:23:52","0","Question"
"78247234","","Stable Diffusion pipe always outputs 512*512 images regardless of the input resolution","<p>I'm making an inpainting app and I'm almost getting the desired result except the pipeline object outputs a 512*512 image no matter what resolution I pass in. I'm running this on the CPU, it's the onnx-converted, AMD-friendly version of stable diffusion.</p>
<p>Here's the code I think is relevant:</p>
<pre><code>class CustomDiffuser:
    def __init__(self, provider:Literal['CPUExecutionProvider', 'DmlExecutionProvider']='CPUExecutionProvider'):

        self.pipe_text2image = None
        self.pipe_inpaint = None
        self.image = None
        self.sam = None
        self.provider = provider


    def load_model_for_inpainting(
            self, 
            path: str = '../stable_diffusion_onnx_inpainting', 
            safety_checker=None
    ):
        self.pipe_inpaint = OnnxStableDiffusionInpaintPipeline.from_pretrained(path, provider=self.provider, revision='onnx', safety_checker=safety_checker)        


    def inpaint_with_prompt(
            self, 
            image: cv2.typing.MatLike | Image.Image, 
            mask: cv2.typing.MatLike | Image.Image,
            height: int, 
            width: int,             
            prompt: str = '', 
            negative: str = '',
            steps: int = 10, 
            cfg: float =  7.5,
            noise: float = 0.75
    ):

        pipe = self.pipe_inpaint

        image = image.resize((width, height))
        mask = mask.resize((width, height))

        output_image = pipe(
            prompt,
            image,
            mask,
            #strength=noise,
            guidance_scale=cfg
        )

        return output_image

  
diffuser = CustomDiffuser('CPUExecutionProvider')
    
diffuser.load_model_for_inpainting('C:/path/to/repository/stable_diffusion_onnx_inpainting')

output = diffuser.inpaint_with_prompt(
    Image.open(image_path),
    Image.fromarray(headless_selfie_mask.astype(np.uint8)),
    576, #height first 
    384,                
    'a picture of a man dressed in a darth vader costume, full body shot, front view, light saber',
    ''
)
 
</code></pre>
","2024-03-30 08:11:45","1","Question"
"78245883","78245568","","<p>Check the docs - <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer"" rel=""nofollow noreferrer"">transformer class</a>, <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer"" rel=""nofollow noreferrer"">transformer decoder</a></p>
<p>For an unbatched (2 dim) input where <code>src = (S, E)</code> and <code>tgt = (T, E)</code>, the output will be of shape <code>(T, E)</code>.</p>
<p>In the transformer decoder layer, the first argument is <code>tgt</code> which defines the output size.</p>
<p>Since you define your <code>tgt</code> param <code>L</code> as <code>torch.randn(1, d)</code>, your transformer decoder output will be of size <code>(1, d)</code>.</p>
<p>This has nothing to do with broadcasting, this is just the input/output mechanics of the transformer layer.</p>
","2024-03-29 20:46:27","0","Answer"
"78245686","78245489","","<p>A 1d conv with a kernel size of 1 accomplishes this:</p>
<pre class=""lang-py prettyprint-override""><code>B = 10
F = 20
F_desired = 17
D = 64

x = torch.randn(B, F, D)

reduction1 = nn.Conv1d(F, F_desired, 1)
x1 = reduction1(x)
print(x1.shape)
&gt; torch.Size([10, 17, 64])
</code></pre>
<p>You could also do a linear layer, provided you permute the axes:</p>
<pre class=""lang-py prettyprint-override""><code>reduction2 = nn.Linear(F, F_desired)
x2 = reduction2(x.permute(0,2,1)).permute(0,2,1)
print(x2.shape)
&gt; torch.Size([10, 17, 64])
</code></pre>
<p>Note that if your convolution kernel is size <code>1</code>, these are actually equivalent operations</p>
<pre class=""lang-py prettyprint-override""><code>reduction2.weight.data = reduction1.weight.squeeze().data
reduction2.bias.data = reduction1.bias.data

x2 = reduction2(x.permute(0,2,1)).permute(0,2,1)
print(torch.allclose(x1,x2, atol=1e-6))
&gt; True
</code></pre>
","2024-03-29 19:54:39","0","Answer"
"78245568","","Understanding batching in pytorch models","<p>I have following model which forms one of the step in my overall model pipeline:</p>
<pre><code>import torch
import torch.nn as nn

class NPB(nn.Module):
    def __init__(self, d, nhead, num_layers, dropout=0.1):
        super(NPB, self).__init__()
            
        self.te = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d, nhead=nhead, dropout=dropout, batch_first=True),
            num_layers=num_layers,
        ) 

        self.t_emb = nn.Parameter(torch.randn(1, d))
        
        self.L = nn.Parameter(torch.randn(1, d)) 

        self.td = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=d, nhead=nhead, dropout=dropout, batch_first=True),
            num_layers=num_layers,
        ) 

        self.ffn = nn.Linear(d, 6)
    
    def forward(self, t_v, t_i):
        print(&quot;--------------- t_v, t_i -----------------&quot;)
        print('t_v: ', tuple(t_v.shape))
        print('t_i: ', tuple(t_i.shape))

        print(&quot;--------------- t_v + t_i + t_emb -----------------&quot;)
        _x = t_v + t_i + self.t_emb
        print(tuple(_x.shape))

        print(&quot;--------------- te ---------------&quot;)
        _x = self.te(_x)
        print(tuple(_x.shape))
        
        print(&quot;--------------- td ---------------&quot;)
        _x = self.td(self.L, _x)
        print(tuple(_x.shape))

        print(&quot;--------------- ffn ---------------&quot;)
        _x = self.ffn(_x)
        print(tuple(_x.shape))

        return _x
</code></pre>
<p>Here <code>t_v</code> and <code>t_i</code> are inputs from earlier encoder blocks. I pass them as shape of <code>(4,256)</code>, where <code>256</code> is number of features and <code>4</code> is batch size. <code>t_emb</code> is temporal embedding.  <code>L</code> represents learned matrix representing the embedding of the query. I tested this module block with following code:</p>
<pre><code>t_v = torch.randn((4,256))
t_i = torch.randn((4,256))
npb = NPB(d=256, nhead=8, num_layers=2)
npb(t_v, t_i)
</code></pre>
<p>It outputted:</p>
<pre><code>=============== NPB ===============
--------------- t_v, t_i -----------------
t_v:  (4, 256)
t_i:  (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- te ---------------
(4, 256)
--------------- td ---------------
(1, 256)
--------------- ffn ---------------
(1, 6)
</code></pre>
<p>I was expecting the output should be of shape <code>(4,6)</code>, 6 values for each sample in the batch of size <code>6</code>. But the output was of size <code>(1,6)</code>. After a lot of tweaking, I tried changing <code>t_emb</code> and <code>L</code> shape from <code>(1,d)</code> to <code>(4,d)</code>, since I did not wanted all sampled to share these variables (through broadcasting:</p>
<pre><code>self.t_emb = nn.Parameter(torch.randn(4, d)) # [n, d] = [4, 256]     
self.L = nn.Parameter(torch.randn(4, d)) 
</code></pre>
<p>This gives desired output of shape (4,6:</p>
<pre><code>--------------- t_v, t_i -----------------
t_v:  (4, 256)
t_i:  (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- te ---------------
(4, 256)
--------------- td ---------------
(4, 256)
--------------- ffn ---------------
(4, 6)
</code></pre>
<p>I have following doubts:</p>
<p><strong>Q1.</strong> Exactly why changing <code>L</code> and <code>t_emb</code> shape from <code>(1,d)</code> to <code>(4,d)</code> worked? Why it did not work with <code>(1,d)</code> through broadcasting?<br />
<strong>Q2.</strong> Am I doing batching right way or the output is artificially correct while under the hood its doing something different than what I am expecting (predicting 6 values for each sample in the batch of size 4)?</p>
","2024-03-29 19:24:58","0","Question"
"78245489","","Mapping a higher dimension tensor into a lower one: (B, F, D) -> (B, F-n, D) in PyTorch","
<p>I have a tensor of embeddings that I want to reduce into a smaller number of embeddings. I am working in a batched environment. The tensor shape is B, F, D where B is the number of items in batch, F is the number of embeddings and D is the dimension. I want to learn a reduction to B, F-n, D.</p>
<p>e.g.</p>
<pre class=""lang-py prettyprint-override""><code>import torch

B = 10
F = 20
F_desired = 17
D = 64

x = torch.randn(B, F, D)
# torch.Size([50, 20, 64])

reduction = torch.?

y = reduction(x)

print(y.shape)
# torch.Size([50, 20, 64])

</code></pre>
<p>I think a 1x1 convolution would make sense here, but not sure how to confirm it was actually doing what I expected? So would love to hear if it's the right approach / if there are better approaches</p>
<pre class=""lang-py prettyprint-override""><code>reduction = torch.nn.Conv1d(
    in_channels=F,
    out_channels=F_desired,
    kernel_size=1,
)
</code></pre>
","2024-03-29 19:06:03","0","Question"
"78245304","77762546","","<p>You should download wheel file directly. And use <code>pip install xxx.whl</code>.</p>
<p>I build a web tool, you can find and download specific version of pytorch.</p>
<p><a href=""https://install.pytorch.site/?device=CUDA+10.0"" rel=""nofollow noreferrer"">https://install.pytorch.site/?device=CUDA+10.0</a></p>
","2024-03-29 18:14:57","0","Answer"
"78245146","78243727","","<p>Is it necessary to use multiple GPUs or is that just a workaround for memory constraints?</p>
<p>On a single GPU, you can reduce memory overhead by pre-allocating the output array and breaking the matmul operation into chunks:</p>
<pre class=""lang-py prettyprint-override""><code>def chunked_matmul(a, b, n_rows, n_cols):
    assert a.shape[1] == b.shape[0]
    assert a.dtype == b.dtype
    
    rows = a.shape[0]
    cols = b.shape[1]
    c = torch.zeros(a.shape[0], b.shape[1], dtype=a.dtype)
    
    for row in range(0, rows, n_rows):
        a_chunk = a[row:row+n_rows]
        for col in range(0, cols, n_cols):
            b_chunk = b[:, col:col+n_cols]
            
            result = torch.mm(a_chunk, b_chunk)
            c[row:row+n_rows, col:col+n_cols] += result
            
    return c
</code></pre>
<p>Note that pytorch uses different matmul algorithms under the hood, so there may be numeric issues depending on the matmul chunksize. For example:</p>
<pre class=""lang-py prettyprint-override""><code>dim1 = 512
dim2 = 512
dim3 = 512

a = torch.randn(dim1, dim2)
b = torch.randn(dim2, dim3)

c1 = torch.mm(a,b)
c2 = chunked_matmul(a, b, 8, 8)
c3 = chunked_matmul(a, b, 128, 128)

(c1 - c2).abs().max()
&gt; tensor(9.1553e-05)

(c1 - c3).abs().max()
&gt; tensor(0.)
</code></pre>
","2024-03-29 17:34:04","1","Answer"
"78245017","78243727","","<p>I tried the following approach which can solve the first problem in the problem detail to some extent, but does not completely resolve it.</p>
<pre class=""lang-py prettyprint-override""><code>import torch

a1 = torch.randn(15000, 30000).cuda(0)
a2 = torch.randn(15000, 30000).cuda(1)
b1 = torch.randn(30000, 30000).cuda(0)
b2 = b1.cuda(1)

# create a empty tensor first,
# then directly use it to save the computation result,
# but its maximum memory usage on a single GPU is still high
c = torch.empty(30000, 30000).cuda(0)
c[:15000] = torch.mm(a1,b1)
c[15000:] = torch.mm(a2,b2).to(0)
</code></pre>
<p>UPDATE: this code can reduce the maximum mem usage on a single GPU when using multiple GPUs (here 2 GPUs used):</p>
<pre class=""lang-py prettyprint-override""><code>import torch

# assuming a1 and a2 are parts of a big matrix
a1 = torch.randn(15000, 30000).cuda(0)
a2 = torch.randn(15000, 30000).cuda(1)
b1 = torch.randn(30000, 15000).cuda(0)
b2 = torch.randn(30000, 15000).cuda(1)

c = torch.empty(30000, 30000).cuda(0)
c[:15000, :15000] = torch.mm(a1,b1)
c[:15000, 15000:] = torch.mm(a1.cuda(1),b2)
c[15000:, :15000] = torch.mm(a2,b1.cuda(1))
c[15000:, 15000:] = torch.mm(a2,b2)
</code></pre>
","2024-03-29 17:07:14","0","Answer"
"78244849","78242721","","<p>You need to change</p>
<p><code>attention = MultiheadAttention(embed_dim=1536, num_heads=4)</code></p>
<p>to</p>
<p><code>attention = MultiheadAttention(embed_dim=1536, num_heads=4, batch_first=True)</code></p>
<p>The default behavior of <code>batch_first=False</code> is making the computation think your query batch size doesn't match your k/v batch size.</p>
","2024-03-29 16:26:32","1","Answer"
"78243757","78196316","","<p>I believe the issue is that the CUDA toolkit version of your Pytorch installation is not compatible with the compute capability of your GPU. If you look into the <a href=""https://en.wikipedia.org/wiki/CUDA#GPUs_supported"" rel=""nofollow noreferrer"">CUDA support table</a>, you see that the <code>RTX 6000 Ada</code> has a compute capability version of <code>8.9</code>. This compute version only supports CUDA SDK versions <strong>above</strong> <code>11.8</code>.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Compute Capability (CUDA SDK support vs. Microarchitecture)</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;""><a href=""https://i.sstatic.net/tm7vV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tm7vV.png"" alt=""enter image description here"" /></a></td>
</tr>
</tbody>
</table></div>
<p>However, the CUDA toolkit version you have installed - according to the image you are using - is <code>11.3.0</code> which is lower than the minimum supported version by a <code>RTX 6000 Ada</code>. In other words, you should use an image that comes with a higher CUDA toolkit version.</p>
<p>I looked at the available images, from release version <a href=""https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-22-09.html"" rel=""nofollow noreferrer""><code>22-09</code></a>, and the shipped CUDA toolkit is equal to or higher than <code>11.8</code>. The first image that has this requirement is <code>1.13.0a0+d0d6b1f</code>.</p>
","2024-03-29 12:29:48","2","Answer"
"78243727","","How to do a simple large matrix multiplication on multiple GPUs in PyTorch? I have wrote some simple codes, but works not well","<p>I want to use multiple GPUs to do matrix multiplication, like <code>torch.mm(a, b)</code>, to reduce memory usage on a single GPU.</p>
<p>Here is the code working on a single GPU:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

a = torch.randn(30000, 30000).cuda(1)
b = torch.randn(30000, 30000).cuda(1)
c = torch.mm(a, b)

# during this process, the maximum memory usage is 10491 MB.
</code></pre>
<p>Here is the code working on two GPUs:</p>
<pre class=""lang-py prettyprint-override""><code>import torch 

# assuming `a1` and `a2` are parts of a big matrix
a1 = torch.randn(15000, 30000).cuda(0)
a2 = torch.randn(15000, 30000).cuda(1)
b1 = torch.randn(30000, 30000).cuda(0)
b2 = b1.cuda(1)

c1 = torch.mm(a1,b1)
c2 = torch.mm(a2,b2).to(0)
# for now, the result `c1` and `c2` is on GPU 0
# the maximun memory usage on GPU 1 is 7059 MB
# the maximum memory usage on GPU 0 is 8777 MB, bigger than 1 because the result is on it

c = torch.concat([c1, c2], dim=0)
# OOM because concat is not in-place
</code></pre>
<p>Therefore, if we can make the concat operation in-place, seems it would work as expected? Or should I move <code>c1</code> and <code>c2</code> to CPU memory first and then cat them, then move the cated result to GPU?</p>
<p>I have also tried tensor parallelism provided by PyTorch 2.2:</p>
<pre class=""lang-py prettyprint-override""><code>import torch  
import torch.distributed as distributed 
import os
from torch.distributed._tensor import init_device_mesh, Shard, distribute_tensor
from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel
from visualize_sharding import visualize_sharding

mesh = init_device_mesh(&quot;cuda&quot;, (2,))
rank = distributed.get_rank()

big_tensor_1 = torch.randn(3, 2)
big_tensor_2 = torch.randn(2, 6)

print(&quot;big_tensor_1&quot;, big_tensor_1)

my_dtensor_1 = distribute_tensor(big_tensor_1, mesh, [Shard(dim=0)]) 
my_dtensor_2 = distribute_tensor(big_tensor_2, mesh, [Shard(dim=1)]) 

# visualize_sharding(my_dtensor_1, header=&quot;my_dtensor_1&quot;)

c = torch.mm(my_dtensor_1, my_dtensor_2)
print(&quot;c: &quot;, c)
</code></pre>
<p>But everything would run twice because the command was <code>python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 tmp.py</code>, so there would be two <code>big_tensor_1</code> randomly generated, how can I modify the code to make it run once with two processes?</p>
<p>Everthing I tried is listed in the problem details.</p>
","2024-03-29 12:20:41","0","Question"
"78243131","78242743","","<p>You can define your linear layer separately from the classifier as a standalone layer:</p>
<pre><code>self.linear = nn.Linear(in_features=hidden_units*7*7, 
                        out_features=output_shape))
</code></pre>
<p>Then in the forward function, the equivalent implementation would be:</p>
<pre><code>def forward(self, x: torch.Tensor):
    x = self.block_1(x)
    x = self.block_2(x)
    x = x.flatten(1)
    x = self.linear(x)
    return x
</code></pre>
","2024-03-29 10:04:57","0","Answer"
"78242956","78237958","","<p>What you are looking for is to gather values along two axes:</p>
<pre><code>out[b, i, j] = x[b, index[b,i], index[b,j]]
</code></pre>
<p>There are no functions for this out of the box, you need to work around it. Compare your setup with the use case of <a href=""https://pytorch.org/docs/stable/generated/torch.gather.html"" rel=""nofollow noreferrer""><code>torch.gather</code></a>, here x is only indexed on a single axis: <code>dim=1</code>:</p>
<pre><code>out[b,i] = x[b, index[b,i]]
</code></pre>
<p>So what you want to do is flatten <code>x</code>, and the indices accordingly. Here is a basic setup:</p>
<pre><code>x = torch.rand(B,N,N)
indices = torch.randint(0,N,(B,N))
</code></pre>
<p>You can easily get the flattened indices with:</p>
<pre><code>findex = indices.repeat_interleave(N,1)*N + indices.repeat(1,N)
</code></pre>
<p>Then simply flatten the <code>(N,N)</code> dimensions of <code>x</code> and apply the indexing on <code>dim=1</code> using <code>findex</code>:</p>
<pre><code>x.flatten(1).gather(1,findex).view(B,N,N)
</code></pre>
","2024-03-29 09:29:07","0","Answer"
"78242816","78242316","","<p>The <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html"" rel=""nofollow noreferrer""><code>nn.MSELoss</code></a> loss computes the L2 distance between the two inputs point-wise. Additionally, the reduction parameter dictates whether to average or sum the resulting tensor, in that case, it will apply the reduction <em>over all dimensions</em>. Here is a comparison:</p>
<pre><code>&gt;&gt;&gt; x_t = torch.rand((32, 4, 100))
&gt;&gt;&gt; x_est = torch.rand((32, 4, 100))
</code></pre>
<p>With <code>nn.MSELoss</code>:</p>
<pre><code>&gt;&gt;&gt; loss(x_t, x_est)
</code></pre>
<p>With builtin Tensor methods:</p>
<pre><code>&gt;&gt;&gt; (x_t-x_est).pow(2).mean()
</code></pre>
","2024-03-29 08:53:47","0","Answer"
"78242743","","how to convert nn.Sequential code to nn.Linear","<p>I am new to deep learning, and I came across this while learning.</p>
<p>Is there a way to convert the nn.Sequential() functions here to nn.Linear(), because of how flexible it is to use nn.Linear() functions.</p>
<pre><code>class FashionMNISTModelV2(nn.Module):
def __init__(self, input_shape: int, hidden_units: int, output_shape: int):
    super().__init__()
    self.block_1 = nn.Sequential(
        nn.Conv2d(in_channels=input_shape, 
                  out_channels=hidden_units, 
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.Conv2d(in_channels=hidden_units, 
                  out_channels=hidden_units,
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2,
                     stride=2)
    )
    self.block_2 = nn.Sequential(
        nn.Conv2d(hidden_units, hidden_units, 3, padding=1),
        nn.ReLU(),
        nn.Conv2d(hidden_units, hidden_units, 3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2)
    )
    self.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features=hidden_units*7*7, 
                  out_features=output_shape)
    )

def forward(self, x: torch.Tensor):
    x = self.block_1(x)
    x = self.block_2(x)
    x = self.classifier(x)
    return x
</code></pre>
","2024-03-29 08:33:19","0","Question"
"78242721","","RuntimeError with PyTorch's MultiheadAttention: How to resolve shape mismatch?","<p>I'm encountering an issue regarding the input shape for PyTorch's MultiheadAttention. I have initialized MultiheadAttention as follows:
<code>attention = MultiheadAttention(embed_dim=1536, num_heads=4)</code></p>
<p>The input tensors have the following shapes:</p>
<ul>
<li><strong>query.shape</strong> is <strong>torch.Size([1, 1, 1536])</strong></li>
<li><strong>Both key.shape</strong> and <strong>value.shape</strong> are <strong>torch.Size([1, 23, 1536])</strong></li>
</ul>
<p>However, when attempting to use these inputs, I encounter the following error:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
Cell In[15], line 1
----&gt; 1 _ = cal_attn_weight_embedding(attention, top_j_sim_video_embeddings_list)

File ~/main/reproduct/choi/make_embedding.py:384, in cal_attn_weight_embedding(attention, top_j_sim_video_embeddings_list)
    381 print(embedding.shape)
    383 # attention
--&gt; 384 output, attn_weights = attention(thumbnail, embedding, embedding)
    385 # attn_weight shape: (1, 1, j+1)
    387 attn_weights = attn_weights.squeeze(0).unsqueeze(-1)  # shape: (j+1, 1)

File ~/anaconda3/envs/choi_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/anaconda3/envs/choi_venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1205, in MultiheadAttention.forward(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)
   1191     attn_output, attn_output_weights = F.multi_head_attention_forward(
   1192         query, key, value, self.embed_dim, self.num_heads,
...
   5281     # TODO finish disentangling control flow so we don't do in-projections when statics are passed
   5282     assert static_k.size(0) == bsz * num_heads, \
   5283         f&quot;expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}&quot;

RuntimeError: shape '[1, 4, 384]' is invalid for input of size 35328
</code></pre>
<p>Why am I encountering this error?</p>
<p>The main execution environment is as follows:</p>
<ul>
<li>Ubuntu 20.04</li>
<li>Anaconda 1.7.2</li>
<li>Python 3.8.5</li>
<li>VSCode 1.87.2</li>
<li>PyTorch 2.0.1</li>
</ul>
<p>Thank you for your cooperation in advance.</p>
","2024-03-29 08:27:55","1","Question"
"78242443","78204360","","<blockquote>
<p>I’m getting the same error in the colab notebook for the NLP Course: Lesson 3 - Fine-tuning model with the Trainer API. In order to get around it you can:</p>
<ol>
<li>Run pip install accelerate -U in a cell</li>
<li>In the top menu click Runtime → Restart Runtime</li>
<li>Do not rerun any cells with !pip install in them</li>
<li>Rerun all the other code cells and you should be good to go!</li>
</ol>
</blockquote>
<blockquote>
<p>On a side note, be sure to turn on a GPU for this notebook by clicking Edit → Notebook Settings → GPU type - from the top menu. This was the next thing that got me : )</p>
</blockquote>
<p>Source: <a href=""https://discuss.huggingface.co/t/trainingargument-does-not-work-on-colab/43372/6"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/trainingargument-does-not-work-on-colab/43372/6</a></p>
","2024-03-29 07:10:28","-1","Answer"
"78242316","","Ask nn.MSELoss() calculation mechnism in pytorch framework","<p>I want to ask that when calculating MSE loss about time-sequence data shaped like (minibatch, feature, sequence length) in pytorch by using <code>nn.MSELoss()</code> with <code>reduction=&quot;mean&quot;</code>,  average just targets on minibatch? or also implicitly about time sequence?</p>
<p>To confirm the calculation result to check what I have questioned above, I printed out below code</p>
<pre><code>nn.MSELoss() #reduction='mean' is default
x_t = torch.ones((32, 4, 100)) # [minibatch size, feature size, time sequence length]
x_est = torch.ones((32, 4, 100)) * 2

loss_result = loss(x_t, x_est)
print(loss_result)
</code></pre>
<pre><code>&gt;&gt;&gt; tensor(1.)
</code></pre>
","2024-03-29 06:31:28","0","Question"
"78240012","78239906","","<p>I think there may be an issue with your formula for calculating the dequantized output.</p>
<pre><code>import numpy as np

# Original values
activation = np.array([1, 2, 3, 4])
weight = np.array([5, 6, 7, 8])

# Quantization parameters
bit = 16  # Desired bit precision
min_val = min(np.min(activation), np.min(weight))
max_val = max(np.max(activation), np.max(weight))

# Calculate scale factor
scale_factor = (2 ** (bit - 1) - 1) / max(abs(min_val), abs(max_val))

# Quantize activation and weight values
quantized_activation = np.round(activation * scale_factor).astype(np.int16)
quantized_weight = np.round(weight * scale_factor).astype(np.int16)

# Dequantize activation and weight values
dequantized_activation = quantized_activation / scale_factor
dequantized_weight = quantized_weight / scale_factor

# Print values
print(&quot;Original activation:&quot;, activation)
print(&quot;Original weight:&quot;, weight)
print(&quot;Minimum value:&quot;, min_val)
print(&quot;Maximum value:&quot;, max_val)
print(&quot;Scale factor:&quot;, scale_factor)
print(&quot;Quantized activation:&quot;, quantized_activation)
print(&quot;Quantized weight:&quot;, quantized_weight)
print(&quot;Dequantized activation:&quot;, dequantized_activation)
print(&quot;Dequantized weight:&quot;, dequantized_weight)

---------------------------------------------------------

Original activation: [1 2 3 4]
Original weight: [5 6 7 8]
Minimum value: 1
Maximum value: 8
Scale factor: 4095.875
Quantized activation: [ 4096  8192 12288 16384]
Quantized weight: [20479 24575 28671 32767]
Dequantized activation: [1.00003052 2.00006104 3.00009156 4.00012207]
Dequantized weight: [4.99990844 5.99993896 6.99996948 8.        ]

</code></pre>
<p>Calculate output:</p>
<pre><code>output = np.sum(dequantized_activation * dequantized_weight)
print(&quot;Dequantized output:&quot;, output) # 70.00183110125477
</code></pre>
","2024-03-28 17:38:40","0","Answer"
"78239969","78239756","","<p>Per the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer"" rel=""nofollow noreferrer"">docs</a>, the <code>name</code> argument must be a string, and the <code>tensor</code> argument must be a pytorch tensor.</p>
<p>If you have a dict of buffers, you could consider using a dedicated <code>nn.Module</code> for that purpose. Something like this:</p>
<pre class=""lang-py prettyprint-override""><code>class BufferDict(nn.Module):
    def __init__(self, input_dict):
        super().__init__()
        for k,v in input_dict.items():
            self.register_buffer(k, v)
            
input_dict = {'a' : torch.randn(4), 'b' : torch.randn(5)}

bd = BufferDict(input_dict)
bd.state_dict()
&gt; OrderedDict([('a', tensor([ 0.1908,  1.6965, -0.3710,  0.4551])),
               ('b', tensor([-0.6943, -0.0534,  0.1779,  1.3607, -0.2236]))])
</code></pre>
","2024-03-28 17:30:43","3","Answer"
"78239906","","How to manually dequantize the output of a layer and requantize it for the next layer in Pytorch?","<p>I am working on school project that requires me to perform manual quantization of each layer of a model. Specifically, I want to implement manually:</p>
<blockquote>
<p>Quantized activation, combined with quantized weight A - layer A -
quantized output - dequantized output - requantized output, combined
with quantized weight B - layer B - ...</p>
</blockquote>
<p>I know Pytorch already have a quantization function, but that function is limited to int8. I would like to perform quantization from bit = 16 to bit = 2, and then compare their accuracy.</p>
<p>The issue I encountered is that after quantization, the output of a layer is multi-magnitude larger (with bit = 16), and I don't know how to dequantize it back. I am performing the quantization with the same min and max of both activation and weight. So here is an example:</p>
<pre><code>Activation = [1,2,3,4]
Weight = [5,6,7,8]
Min and max across activation and weight = 1, 8
Expected, non-quantized output = 70

Quantize with bit = 16
Quantized activation = [-32768, -23406, -14044, -4681]
Quantized weight = [4681, 14043, 23405, 32767]
Quantized output = -964159613
Dequantize output with min = 1, max = 8 = -102980
</code></pre>
<p>The calculation makes sense to me, because the output involves multiplying activations and weigths, their magnitude increase is also multiplied together. If I perform dequantization once with the original min and max, it is reasonable to have a much larger output.</p>
<p>How does Pytorch handle dequantization? I attempted to locate the quantization of Pytorch, but I could not locate it. How to dequantize the output?</p>
","2024-03-28 17:17:53","1","Question"
"78239756","","register_buffer a dict object in PyTorch","<p>I thought this was a simple question but I couldn't find an answer.</p>
<p>I want a member variable of a pytorch module to be saved/loaded with model state_dict. I can do that in <strong>init</strong> with the following line.</p>
<pre><code>        self.register_buffer('loss_weight', torch.tensor(loss_weight))
</code></pre>
<p>But what if loss_weight is a dict object? Is it allowed? If so, how can I convert it to a tensor?</p>
<p>When tried, I got an error &quot;Could not infer dtype of dict.&quot;</p>
","2024-03-28 16:46:48","0","Question"
"78239566","","In GPytorch is there any way to initialize a VNNGP model with faiss-cpu and then set it to cuda?","<p>As faiss-gpu does not support windows i am forced to use faiss-cpu (which i would like do to as initializing my model with it takes about 4 minutes compared to the 38 minutes it takes with scikit-learn) I run in to the an error after getting through the first training epoch:</p>
<blockquote>
<p>RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)</p>
</blockquote>
<p>as it looked to me like the NNVariationalStrategy class uses the device of its inducing_points attribute to set the device on other tensors it creates i tried to set
<code>model.variational_strategy.inducing_points = train_x</code> after setting <code>train_x = train_x.cuda()</code> and also separately i tried setting <code>model.variational_strategy.nn_util.to('cuda:0')</code>.
From what i can tell in both cases the inducing_points tensor gets set to cuda, but im not sure if it does much else as <code>model.variational_strategy.current_training_indices.device</code> for example will still return <code>device(type='cpu')</code></p>
<p>I have also unsuccessfully tried to find where the RuntimeError comes from, hoping i can see exactly which indexed tensor its talking about.</p>
<p>Any help or insight would be much appreciated.</p>
","2024-03-28 16:12:03","1","Question"
"78238259","78237761","","<p>If the warnings do not affect your code execution or performance, you can choose to ignore them.</p>
<pre><code>import warnings
warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)
</code></pre>
","2024-03-28 12:40:55","0","Answer"
"78237958","","Rearrange 2D tensors in a batch Torch","<p>Let us have an initial_tensor of size (batch_size, N, N) and a tensor of indexes (batch_size, N), specifying the new order of elements in each 2D tensor in a batch. The goal is to re-arrange the elements of tensors in the batch according to the index tensor to obtain a target tensor.</p>
<p>Currently I am able to do it on CPU using the following loop:</p>
<pre><code>    for batch in range(batch_size):
        old_ids = indexes[batch]

        for i in range(N):
            for j in range(N):
                target[batch][i][j] = initial_tensor[batch][old_ids[i]][old_ids[j]]
</code></pre>
<p>I am looking for an equivalent vector solution to get rid of CPU utilisation.</p>
<p>I tried various options of utilisation of scattering and slicing, but could not figure out the equivalent for the loop.</p>
","2024-03-28 11:47:51","0","Question"
"78237830","78237761","","<p>I found a solution that is not based on importing tensorflow (which I dont use) but allows muting <a href=""https://stackoverflow.com/a/40871012/9659620"">through environment variables</a>:</p>
<p>Simply add this line to the beginning of your script.</p>
<pre class=""lang-py prettyprint-override""><code>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL
</code></pre>
<p>This should work for any <code>tensorflow &gt; 1.14</code></p>
","2024-03-28 11:26:03","0","Answer"
"78237761","","import torch.utils.tensorboard causes tensorflow warnings","<p>As stated <a href=""https://stackoverflow.com/questions/57547471/does-torch-utils-tensorboard-need-installation-of-tensorflow"">here</a> tensorboard is part of tensorflow but does not depend on it. One can use it in pytorch such as</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.tensorboard import SummaryWriter
</code></pre>
<p>It is, however, annoying that this import causes a long trace of tensorflow related warnings</p>
<pre class=""lang-bash prettyprint-override""><code>2024-03-28 12:11:43.296359: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.                                                                              
2024-03-28 12:11:43.331928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-28 12:11:43.970865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
</code></pre>
<p>I wonder if this is necessary and / or how they can be (safely?) muted.</p>
","2024-03-28 11:13:26","0","Question"
"78229534","78228880","","<p>The batch size in DDP is batch size <em>per GPU</em>. With four GPUs, your true batch size is <code>8*4=32</code>. When your batch size is larger, you have fewer batches per epoch, and therefore fewer gradient updates. For your case, you would need 4 epochs of DDP training to have the same number of parameter update steps as one epoch of single GPU training.</p>
<p>DDP averages gradients between nodes, so you don't need to worry about changing the learning rate (this would not be the case if gradients were summed).</p>
","2024-03-27 05:36:08","0","Answer"
"78228880","","Questions about batchsize and learning rate settings for DDP and single-card training","<p>Questions about batchsize and learning rate settings for DDP and single-card training</p>
<p>Single-card network training, batchsize = 8, learning rate = 10e-4</p>
<p>Now it is changed to DDP single machine multi-card (one node 4 GPUs) training
Like the following solution:</p>
<pre><code>import torch.distributed as dist
dist.init_process_group(backend='nccl', init_method=init_method, world_size=args.nprocs, rank=local_rank)
</code></pre>
<p>At this time, if the batch size is set to 8 and the learning rate is set to 10e-4, the training time of DPP is less than single card. However, looking at the loss curve, the number of epochs required for convergence is obviously much greater than that of a single card.</p>
<p>I would like to ask, if the batch size remains unchanged in DDP, does the learning rate need to be increased? how to increased?</p>
<p>If I want to keep the same training effect as a single card, whether to set the batchsize to (8 / gpu numbers)?</p>
<p>I have tried setting batchsize on DDP to 2, which is the batchsize/gpu number of a single card, but there is no conclusion yet.</p>
","2024-03-27 01:11:17","0","Question"
"78228475","78228322","","<p>In the video, the person is using Linux. From your code, I can see you are using Windows. The path in Linux and Windows works different ways, so you have to tweak the folder path that works with Windows. In Windows the path works with backslash <code>'\'</code>, and in Linux the path works with forward slash <code>'/'</code>.</p>
","2024-03-26 22:27:37","-1","Answer"
"78228322","","I am trying to build an AI image classifier in Python using a youtube guide. When I run my program (unfinished) it does not open up the image","<p>I am trying to build an AI image classifier using a youtube guide for a school project. Here is the link: <a href=""https://www.youtube.com/watch?v=oEKg_jiV1Ng&amp;t=727s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=oEKg_jiV1Ng&amp;t=727s</a></p>
<p>At this stage, I am not done, but when I run my main.py I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\xxx\xx\xx\xx\newai\main.py&quot;, line 19, in &lt;module&gt;
    for img_path in os.listdir(os.path.join(dir_, category)):
NotADirectoryError: [WinError 267] The directory name is invalid: 'C:/xx/xx/xx/xx/newai\\Data\\Blue-Squares\\BlueSquare (1).jpg'
</code></pre>
<p>I also get this, but I don't think it matters much to the project personally. (Maybe it does, I just assume since it works in the video it should still work as the video is recent.) :</p>
<pre><code>:\XX\XX\XX\XX\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\XX\XX\XX\XX\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
</code></pre>
<p>On the video, when he runs it at this stage it prints the keys and runs fine.</p>
<p>Here is my full code:</p>
<pre><code>from img2vec_pytorch import Img2Vec
import os
from PIL import Image

# prepare data

img2vec = Img2Vec()

data_dir = 'C:/XX/XX/XX/XX/newai'
train_dir = os.path.join(data_dir, r'Data', r'Blue-Squares')
val_dir = os.path.join(data_dir, r'Data', r'Red-Triangles')

data = {}

for j, dir_ in enumerate([train_dir, val_dir]):
    features = []
    labels = []
    for category in os.listdir(dir_):
        for img_path in os.listdir(os.path.join(dir_, category)):
            img_path_ = os.path.join(dir_, category, img_path)
            img = Image.open(img_path_)

            img_features = img2vec.get_vec(img)

            features.append(img_features)
            labels.append(category)

    data[['training_data', 'validation_data'][j]] = features
    data[['training_labels', 'validation_labels'][j]] = labels


print(data.keys())

# train model

# test performance

# save the model

</code></pre>
<p>I tried:
Copy and pasting the youtubers code and using exactly that, swapping out paths, Changing folders, Changing image names, changing how its set up, googling errors, etc. I know the image isn't a directory, so I understand that, I just don't get what to change. Any feedback would be greatly appreciated.</p>
","2024-03-26 21:44:09","-2","Question"
"78227198","78225952","","<p>This is a numerics issue.</p>
<p>When you create the torch tensor of <code>lst</code>, the values are automatically cast to fp32, which loses precision. You can improve precision by explicitly forming the tensor as fp64, but there will still be some variance</p>
<pre class=""lang-py prettyprint-override""><code>lst = [0.0014, -0.0306,  0.0005,  0.0011,  0.0012,  0.0022,  0.0017,  0.0011,
          0.0017,  0.0011,  0.0012,  0.0017,  0.0014,  0.0015,  0.0010,  0.0006,
          0.0006,  0.0004,  0.0009,  0.0007,  0.0008,  0.0007,  0.0013,  0.0013,
          0.0015,  0.0023,  0.0007]

x = torch.tensor([lst], dtype=torch.float64)

x.sum()
&gt; 2.6020852139652106e-18

sum(lst)
&gt; 3.2526065174565133e-19

(x @ torch.ones(x.shape[::-1], dtype=torch.float64)).item()
&gt; 3.2526065174565133e-19

(x @ torch.ones(x.shape, dtype=torch.float64).T).item()
&gt; 2.4936649967166602e-18
</code></pre>
","2024-03-26 17:31:03","1","Answer"
"78227019","78221650","","<p>It depends a lot on the architecture of the two models, but you can do something like this:</p>
<pre class=""lang-py prettyprint-override""><code>class MyVIT(nn.Module):
    def __init__(self, embedding, vit_model):
        super().__init__()
        self.embedding = embedding
        self.vit_model = vit_model

    def forward(self, x):
        x = self.embedding(x)
        x = self.vit_model(x)
        return x
</code></pre>
<p>In this example, <code>embedding</code> would be your custom embedding, and <code>vit_model</code> would be all the trained layers of the vit model except the embedding. Depending on how the vit model is structured, you may need to hack into it to extract the non-embedding layers in a way that allows you to simply pass an input to them.</p>
","2024-03-26 17:00:40","0","Answer"
"78225952","","Pytorch sum problem (possibly floating point)","<p>I have a simple Python test to get the sum of several items in a list that I am putting in a torch Tensor:</p>
<pre class=""lang-py prettyprint-override""><code>lst = [0.0014, -0.0306,  0.0005,  0.0011,  0.0012,  0.0022,  0.0017,  0.0011,
          0.0017,  0.0011,  0.0012,  0.0017,  0.0014,  0.0015,  0.0010,  0.0006,
          0.0006,  0.0004,  0.0009,  0.0007,  0.0008,  0.0007,  0.0013,  0.0013,
          0.0015,  0.0023,  0.0006]

LEN=27

trch = torch.Tensor([lst])


print('--------------------------------------------------------------')

print(trch.sum(1, keepdim=True))
print(sum(lst))
print(trch @ torch.ones((LEN,1)))
print(torch.mm( trch , torch.ones((LEN,1))))


trch_sum= 0
for num in lst:
    trch_sum += num
print(trch_sum)
</code></pre>
<p>and I get the following (reasonable) results:</p>
<pre><code>tensor([[-0.0001]])
-9.999999999999912e-05
tensor([[-9.9999e-05]])
tensor([[-9.9999e-05]])
-9.999999999999972e-05
</code></pre>
<p>Changing the last number of the list to 0.0007 significantly impacts the results, however.</p>
<pre><code>tensor([[-9.3132e-10]])
9.215718466126788e-19
tensor([[1.1642e-09]])
tensor([[1.1642e-09]])
3.2526065174565133e-19
</code></pre>
<p>I appreciate that this is a floating point situation, but is there a way to improve it?</p>
","2024-03-26 14:11:55","0","Question"
"78225920","","Why next(iter(train_dataloader)) takes long execution time in PyTorch","<p>I am trying to load a local dataset with images (around 225 images in total) using the following code:</p>
<pre><code># Set the batch size
BATCH_SIZE = 32 

# Create data loaders
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(
  train_dir=train_dir,
  test_dir=test_dir,
  transform=manual_transforms, # use manually created transforms
  batch_size=BATCH_SIZE
)

# Get a batch of images
image_batch, label_batch = next(iter(train_dataloader)) # why it takes so much time? what can 
      I do about it?
</code></pre>
<p>My question concerns the last line of the code and the iteration in the <code>train_dataloader</code> which takes long execution time. Why is this the case? I have only 225 images.</p>
<p><strong>Edit:</strong></p>
<p>The code for the dataloader can be found in the following <a href=""https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py"" rel=""nofollow noreferrer"">link</a>.</p>
<pre><code>import os

from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import pdb

NUM_WORKERS = os.cpu_count()

def create_dataloaders(
  train_dir: str, 
  test_dir: str, 
  transform: transforms.Compose, 
  batch_size: int, 
  num_workers: int=NUM_WORKERS
):
# Use ImageFolder to create dataset(s)
train_data = datasets.ImageFolder(train_dir, transform=transform)
test_data = datasets.ImageFolder(test_dir, transform=transform)

# Get class names
class_names = train_data.classes

# Turn images into data loaders
train_dataloader = DataLoader(
  train_data,
  batch_size=batch_size,
  shuffle=True,
  num_workers=num_workers,
  pin_memory=True,
)
test_dataloader = DataLoader(
  test_data,
  batch_size=batch_size,
  shuffle=False, # don't need to shuffle test data
  num_workers=num_workers,
  pin_memory=True,
)

return train_dataloader, test_dataloader, class_names
</code></pre>
","2024-03-26 14:04:35","2","Question"
"78225911","78211526","","<p>The <code>itemsize</code> attribute doesn't seem to have documentation in PyTorch 2.0.x, which suggests that it doesn't exist in that version. (Try searching &quot;itemsize&quot; in the docs for <a href=""https://pytorch.org/docs/2.0/search.html?q=itemsize"" rel=""nofollow noreferrer"">versions 2.0</a> and <a href=""https://pytorch.org/docs/2.2/search.html?q=itemsize"" rel=""nofollow noreferrer"">version 2.2</a> yourself.) The probable reason for this is that one of the packages you're using assumes a Pytorch version greater than the one you're using.</p>
<p>Moreover, if you look at the Medium article author's own notebook for this tutorial <a href=""https://www.kaggle.com/code/dassum/finetune-phi-2-on-custom-dataset?source=post_page-----fb60abdeba07--------------------------------"" rel=""nofollow noreferrer"">here</a>, you will see that pip install errors out, suggesting their code isn't runnable.</p>
<p>The best solution I can propose (given that the medium author didn't seem to provide a list of requirement versions) is to create a conda environment which will try to resolve the various package dependencies for you, or to reach out to the author for a precise requirements file so that you can replicate their work. If they cannot do this for you, then don't trust their results.</p>
","2024-03-26 14:03:03","1","Answer"
"78224666","78223872","","<p>I would guess the error is raised by something other than <code>h</code>, perhaps the color! Check whether <code>data.y</code> is a GPU tensor, in that case you can give it the same treatment as <code>h</code> by calling detach/cpu/numpy on it.</p>
","2024-03-26 10:41:56","1","Answer"
"78223985","78220238","","<p>The default <code>collate_fn</code> would behave like stacking up the each keys of your dictionary. Assuming that batch_size=N, each element of <code>target['key']</code> would be [N, ...]. <strong>(Recommended behavior for future compatibility, memory allocations, and optimization on browsing datasets.)</strong></p>
<p>If you really need to pop out the values on your dictionary, try below code.</p>
<pre><code>def my_collate_fn(batch_sample):
    image = []
    target = []
    for sample in batch_sample:
        # The `sample` is returns of your __getitem__()
        image.append(sample[0])
        target.append(sample[1])

    return (torch.stack(image).contiguous(),
            target) # The `target` is no longer a tesnor as the dict cannot be stacked itself.
# --- on your later DataLoader init --- #
loader = torch.utils.data.DataLoader(collate_fn=my_collate_fn, **kwargs)
</code></pre>
<p>BTW, mentioned with above reasons, I suggest you use the default <code>collate_fn</code> instead.</p>
","2024-03-26 08:48:44","0","Answer"
"78223872","","I have used detach().clone().cpu().numpy() but still raise TypeError: can't convert cuda:0 device type tensor to numpy","<p>bug occur at this function line 7</p>
<pre><code>def visualize_embedding(h, color, epoch=None, loss=None):
    plt.figure(figsize=(7,7))
    plt.xticks([])
    plt.yticks([])
    h = h.detach().clone().cpu().numpy()
    print(type(h))
    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=&quot;Set2&quot;)
    if epoch is not None and loss is not None:
        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)
    plt.show()
</code></pre>
<p>error:</p>
<pre><code>&lt;class 'numpy.ndarray'&gt;
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[17], line 21
     19 loss, h = train(data)
     20 if epoch % 10 == 0:
---&gt; 21     visualize_embedding(h, color=data.y, epoch=epoch, loss=loss)
     22     time.sleep(0.3)

Cell In[16], line 16
     14 h = h.detach().clone().cpu().numpy()
     15 print(type(h))
---&gt; 16 plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=&quot;Set2&quot;)
     17 if epoch is not None and loss is not None:
     18     plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)

File c:\Users\polyu\Documents\RA\hkjc_dm\hkjc_dm\model\src\venvModel4\lib\site-packages\matplotlib\pyplot.py:3684, in scatter(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)
   3665 @_copy_docstring_and_deprecators(Axes.scatter)
   3666 def scatter(
   3667     x: float | ArrayLike,
   (...)
   3682     **kwargs,
   3683 ) -&gt; PathCollection:
-&gt; 3684     __ret = gca().scatter(
   3685         x,
   3686         y,
...
   1030     return self.numpy()
   1031 else:
-&gt; 1032     return self.numpy().astype(dtype, copy=False)

TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
</code></pre>
<p>h is already ndarray, why it still gives me the convert cuda tensor error? By the way h is the representation of shape [batch_size, 2]</p>
","2024-03-26 08:28:55","0","Question"
"78222658","78196316","","<p>The <code>Segmentation Fault</code> generally comes from the unexpected memory access on native C.</p>
<p>It can be caused from Graphic Driver, Pytorch version, CUDA and cuDNN version compatibility, etc...</p>
<p>If all the compatibility are checked, try to investigate your GPU memory allocation, like memory leaking or OOM. Most of <code>Segmentation Fault</code> is caused from GPU OOM in my case as Pytorch sometimes cannot catch the out of memory.</p>
","2024-03-26 02:29:11","3","Answer"
"78221817","78221540","","<p>Amazon SageMaker and Bedrock are services you could use.
This link will tell you about the differences (in a nutshell Bedrock isn't as flexible but handles the GPU work for you);
<a href=""https://repost.aws/questions/QURQ0DJ5oPSUyyaLv0jjS4vw/bedrock-vs-sagemaker"" rel=""nofollow noreferrer"">https://repost.aws/questions/QURQ0DJ5oPSUyyaLv0jjS4vw/bedrock-vs-sagemaker</a></p>
<p>If you want to save money, you could spin up the GPUs in EC2 or use EKS or Kubernetes and Docker to run your GPUs.
The more DIY you go, the cheaper you can make things but the more work you'll have to put in to managing the GPU cluster.</p>
<p>You could still use Ray in SageMaker.
<a href=""https://aws.amazon.com/blogs/machine-learning/orchestrate-ray-based-machine-learning-workflows-using-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/orchestrate-ray-based-machine-learning-workflows-using-amazon-sagemaker/</a></p>
<p>You could likewise use Horovod.
<a href=""https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/</a></p>
","2024-03-25 21:14:28","0","Answer"
"78221650","","Custom patch embedding layer for pre-trained Vision transformers","<p>I am looking for a way to use a custom patch embedding layer for vanilla ViT. I want the rest of the ViT from the pre-trained model. Is there a way to do it using Pytorch?</p>
<p>I can load a pre-trained model or can write the ViT code from scratch. But I want something where I can use the weights of the model for the layers after the patch embeddings.</p>
<p>Thanks,</p>
","2024-03-25 20:38:32","0","Question"
"78221540","","How can I train on AWS cloud GPUs using pytorch lightning?","<p>I am currently doing a project and coding up an ML model using pytorch lightning. The dataset I am training on is reasonably large, and hence infeasible to train on my local GPU. For this reason, I am thinking of using AWS cloud GPUs for training. I've heard a few terms thrown around, e.g. SageMaker, ray lightning, Docker but beyond that I'm not entirely sure where to start/which is the best for my use case.</p>
<p>I guess my question is: if I want to do multi-node cloud GPU training using pytorch lightning, what libraries/frameworks/tools should I be looking at?</p>
<p>I have researched different libraries/frameworks, but am currently unsure of which ones being the best for my use case</p>
","2024-03-25 20:12:24","-1","Question"
"78221053","78220979","","<p>You want your scripted function to contain only model operations - ie no preprocessing, i/o, device transfer, etc.</p>
<p>Dataloading/preprocessing logic should be separate from model logic. For example, tokenization should not occur within the model code.</p>
<p>Once logic is separated, additional functions can be added by implementing them in the <code>module</code> class and adding the <code>@torch.jit.export</code>. By default, torch scripting will compile the <code>forward</code> method and any other methods that include the <code>@torch.jit.export</code> decorator. See this example from the pytorch <a href=""https://pytorch.org/docs/stable/jit.html#torch.jit.export"" rel=""nofollow noreferrer"">docs</a></p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

class MyModule(nn.Module):
    def implicitly_compiled_method(self, x):
        return x + 99

    # `forward` is implicitly decorated with `@torch.jit.export`,
    # so adding it here would have no effect
    def forward(self, x):
        return x + 10

    @torch.jit.export
    def another_forward(self, x):
        # When the compiler sees this call, it will compile
        # `implicitly_compiled_method`
        return self.implicitly_compiled_method(x)

    def unused_method(self, x):
        return x - 20

# `m` will contain compiled methods:
#     `forward`
#     `another_forward`
#     `implicitly_compiled_method`
# `unused_method` will not be compiled since it was not called from
# any compiled methods and wasn't decorated with `@torch.jit.export`
m = torch.jit.script(MyModule())
</code></pre>
","2024-03-25 18:27:12","0","Answer"
"78221001","78216943","","<p>Since the GRU is moving both ways, the first chunk of the hidden state represents the forward direction, and the second chunk the backward direction.</p>
<p>As a result, the first chunk represents the last output, and the second chunk represents the first output.</p>
<pre class=""lang-py prettyprint-override""><code>bs = 1
sl = 4
input_size = 3
hidden_size = 5
bidir=True
n_layers=1

input_ids = torch.randn((bs, sl, input_size))
gru = nn.GRU(
            input_size=input_size, 
            hidden_size=hidden_size, 
            bidirectional=bidir, 
            num_layers=n_layers, 
            batch_first=True
        )

o, h = gru(input_ids)

(o[:,-1][:, :hidden_size] == h[0]).all()
&gt; tensor(True)

(o[:,0][:, hidden_size:] == h[1]).all()
&gt; tensor(True)
</code></pre>
","2024-03-25 18:12:48","1","Answer"
"78220979","","How to add Attribute and use after building the model?","<p>I’ve built a neural network model and I’d like to incorporate custom functions, encodeImage and encodeText , for pre-processing data. Ideally, I want these functions to be callable both during model definition and after training (post-build). However, including them directly within the model definition restricts their use to before Just-In-Time (JIT) compilation. Calls made after model building result in the functions being undefined</p>
<pre><code># The Custom Attributes I wan to add in the Model
    def encode_image(self, image):
      return self.visual(image.type(self.dtype))

    def encode_text(self, text):
      x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

      x = x + self.positional_embedding.type(self.dtype)
      x = x.permute(1, 0, 2)  # NLD -&gt; LND
      x = self.transformer(x)
      x = x.permute(1, 0, 2)  # LND -&gt; NLD
      x = self.ln_final(x).type(self.dtype)

      # x.shape = [batch_size, n_ctx, transformer.width]
      # take features from the eot embedding (eot_token is the highest number in each sequence)
      x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection

      return x
  # Image Classifier Neural Network
  class ImageClassifier(nn.Module):
      def __init__(self, n_qubits, n_layers, encode_image):
          super().__init__()
          self.model = nn.Sequential(
              qlayer,
              ClassicalLayer(2)
          )
  
      def forward(self, x):
          result = self.model(x)
          return result
</code></pre>
","2024-03-25 18:07:50","0","Question"
"78220960","78219991","","<p>What exactly do you mean by &quot;batch of indices&quot;?</p>
<p>The problem with something like <code>torch.where(condition)</code> is each item in the batch has a different number of elements where <code>condition=True</code>. This means you can't have a batch-wise application of <code>where</code>, given that the outputs for each item in the batch will be of different size.</p>
<p>The default behavior of <code>where</code> is to put out a tuple of tensors, one for each axis, showing all the index tuples where <code>condition=True</code>. The output indices are flattened to deal with the irregular size issues. You use the outputs to get indices by batch if needed.</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(16, 32, 64)
indices = torch.where(x&gt;0)
print(indices)
&gt; (tensor([ 0,  0,  0,  ..., 15, 15, 15]),
&gt;  tensor([ 0,  0,  0,  ..., 31, 31, 31]),
&gt;  tensor([ 4,  5,  7,  ..., 61, 62, 63]))

index_tensor = torch.stack(indices)
# for example, select outputs from the first item in the batch
index_tensor[:, index_tensor[0] == 0]
</code></pre>
<p>You can also use additional arguments in <code>torch.where</code> to return a tensor the same shape as the input. For example</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.randn(16, 32, 64)
x1 = torch.where(x&gt;0, 1, 0) # fills 1 where x&gt;0, 0 elsewhere

# shape is retained 
x.shape == x1.shape
&gt; True

x2 = torch.where(x&gt;0, x, float('-inf')) # returns `x` with -inf where x&lt;0
</code></pre>
","2024-03-25 18:04:22","0","Answer"
"78220658","78219448","","<p>Your conv dimensions don't line up. I've replaced <code>hidden_units</code> with your value 16 for clarity</p>
<pre class=""lang-py prettyprint-override""><code>self.cnn = nn.Sequential(
    nn.Conv2d(19, 16, kernel_size=3, padding=1),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(19, 32, kernel_size=3, padding=1),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(kernel_size=2, stride=2)
)
</code></pre>
<p>The first conv takes in a 19 channel input and produces a 16 channel output.</p>
<p>Your second conv expects a 19 channel input, but gets a 16 channel input, throwing the error.</p>
<p>You probably want something like</p>
<pre class=""lang-py prettyprint-override""><code>self.cnn = nn.Sequential(
    nn.Conv2d(19, hidden_units, kernel_size=3, padding=1),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(hidden_units, hidden_units * 2, kernel_size=3, padding=1),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(kernel_size=2, stride=2)
)
</code></pre>
<p>Also some errata on CNN design:</p>
<p>MaxPool layers have fallen out of style in favor of adding stride to the convolution (conv with stride 2 produces the same size reduction as conv with stride 1 + max pool). Nothing strictly wrong with having explicit max pool layers but worth noting.</p>
<p>You also definitely want to add an adaptive max pool layer after your conv layer. Without adaptive pooling, your model only works for a fixed input size (required to match up with the input size for the final linear layer). Adding an adaptive pooling layer before the final linear layer makes your model compatible with most image sizes.</p>
","2024-03-25 17:04:43","0","Answer"
"78220369","","Encoder-Decoder with Huggingface Models","<p>I want to create an Encoder-Decoder Model using the following structure:</p>
<ul>
<li>Bert-base-uncased for encoding the input (<a href=""https://huggingface.co/google-bert/bert-base-uncased"" rel=""nofollow noreferrer"">https://huggingface.co/google-bert/bert-base-uncased</a>)</li>
<li>Linear layer for connecting the two models using the CLS token of Bert as input</li>
<li>OPT-125M for decoding using the output of the linear layer as input (<a href=""https://huggingface.co/facebook/opt-125m"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/opt-125m</a>)</li>
</ul>
<p>I want to do this to basically implement the idea I read about in the In-Context Autoencoder paper and test it out myself (<a href=""https://arxiv.org/abs/2307.06945"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2307.06945</a>)</p>
<p>I would like to do this with the huggingface library using PyTorch as it helps to minimize the programming efforts a lot and because I do not know where I would even get the raw implementations of the OPT-125M or BERT model and how to implement them by hand. Also the optimization of huggingface plays a big role to try it on a normal desktop-PC.</p>
<p>My problem is that the OPT-125M model uses a tokenizer for inputs and I am not able to bypass this.</p>
<p>Does anyone know of a way to directly input the output of the linear layer into OPT-125M without encoding it, or a different way of implementing it other than huggingface which is also as performant?</p>
<p>This is the skeleton code that I have already written which produces an error because of the wrong input to OPT:</p>
<pre><code>from transformers import BertTokenizer, BertModel, AutoModelForCausalLM
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
OPT = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-125m&quot;)
import torch
from torch import nn

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertModel.from_pretrained('bert-base-uncased')

    def forward(self, input_text):
        inputs = self.tokenizer(input_text, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)
        outputs = self.model(**inputs)
        return outputs.last_hidden_state[:, 0, :]  # CLS token embeddings

class LinearTransformation(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearTransformation, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.linear(x)

class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        self.model = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-125m&quot;)

    def forward(self, x):
        # Assuming x is prepared correctly for the OPT model
        output = self.model(input_ids=x)
        return output

class BertOptPipeline(nn.Module):
    def __init__(self):
        super(BertOptPipeline, self).__init__()
        self.encoder = Encoder()
        self.linear_transformation = LinearTransformation(768, 512)
        self.decoder = Decoder()

    def forward(self, input_text):
        encoded = self.encoder(input_text)
        transformed = self.linear_transformation(encoded)
        print(transformed.shape)
        # Further processing may be needed here to match the decoder's input requirements
        decoded = self.decoder(transformed)
        return decoded

pipeline = BertOptPipeline()
input_text = &quot;thank you for your help&quot;
output = pipeline(input_text)
</code></pre>
<p>Thanks for your help!</p>
","2024-03-25 16:07:03","1","Question"
"78220238","","iteration over torch DataSet not loading multiple targets","<p>I am trying load a dataset from files and train an AI model on it.
For some reason when I use <code>for iamges, targets in dataloader</code> in my main it loads the targets like:</p>
<pre><code>{
    'image_id':[all image ids],
    'keypoints':[all keypoint lists],
    'labels':[all label lists],
    'boxes':[all bboxes]
}
</code></pre>
<p>instead of</p>
<pre><code>[
    {
        'image_id':image_id of first sample,
        'keypoints':[list of keypoints of first sample],
        'labels':[list of labels of first sample],
        'boxes':bbox of first sample
    },
...
    {
        'image_id':image_id of fourth sample,
        'keypoints':[list of keypoints of fourth sample],
        'labels':[list of labels of fourth sample],
        'boxes':bbox of fourth sample
    }
]
</code></pre>
<p>This is my <code>__getitem__</code> function:</p>
<pre><code>def __getitem__(self, idx):
    annotation = self.annotations[idx]
    image_id = annotation['image_id']
    file_name = annotation['file_name']
    image_path = f&quot;{self.images_dir}/{file_name}&quot;
    image = Image.open(image_path).convert(&quot;RGB&quot;)
    bbox = np.array(annotation['bbox'])
    keypoints = np.array([[ann[&quot;x&quot;],ann[&quot;y&quot;]] for ann in annotation[&quot;keypoints&quot;]])
    labels = np.array([kp_num[ann[&quot;name&quot;]] for ann in annotation[&quot;keypoints&quot;]])
    target = {
        &quot;image_id&quot;:image_id,
        &quot;keypoints&quot;: torch.tensor(keypoints, dtype=torch.float32),
        &quot;labels&quot;: torch.tensor(labels, dtype=torch.int64),
        &quot;boxes&quot;:torch.tensor(bbox, dtype=torch.int)
    }
    
    if self.transform:
        image = self.transform(image)

    return image, target
</code></pre>
<p>I hoped for it to return a list of targets but it returns a dict of lists. I tried putting target in the return statement in a list with one element but it just returned a list with a single entry with all info instead of a list with batch_size many targets.
I am using the torch.utils.data DataLoader class.</p>
<p>EDIT:
solved it, I just implemented a custom_collate_fn like this:</p>
<pre><code>def custom_collate_fn(batch):
    images = [item[0] for item in batch]
    targets = [item[1] for item in batch] 
    return images, targets
</code></pre>
","2024-03-25 15:42:50","0","Question"
"78219991","","Use torch.where() to a batch of arrays","<p>I am using pytorch and I want to apply a simple torch.where(array &gt; 0) to a batch of arrays without using a loop, how could I do this code using torch functions?</p>
<pre><code>def batch_node_indices(states_batch):
    batch_indices = []
    for state in states_batch:
        node_indices = torch.where(state &gt; 0)[0].detach().cpu().numpy()
        batch_indices.append(node_indices)
    return batch_indices
</code></pre>
<p>I tried different torch functions but I wasn't successful. I want the method to return a batch of arrays that each array contains the indices where the state array is bigger than 0.</p>
","2024-03-25 15:02:38","0","Question"
"78219448","","Given groups=1, weight of size [16, 32, 3, 3], expected input[42, 19, 224, 224] to have 32 channels, but got 19 channels instead","<p>I am trying to build a custom model for image classification. This is the code.</p>
<pre><code>class CustomModel(nn.Module):
    def __init__(self, input_channels, hidden_units, output_classes):
        super(CustomModel, self).__init__()
        
        # Global feature extraction layers
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling
        
        # Local feature extraction layers
        self.local_conv = nn.Conv2d(input_channels, hidden_units, kernel_size=3, padding=1)
        
        # Convolutional neural network
        self.cnn = nn.Sequential(
            nn.Conv2d(19, hidden_units, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(19, hidden_units * 2, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        # Fully connected layer
        self.fc = nn.Linear(19 * 2 * 56 * 56, output_classes)

    def forward(self, x):
        # Global feature extraction
        global_features = self.global_pool(x)
        global_features
        global_features = global_features.view(global_features.size(0), -1)
        global_features = global_features.unsqueeze(-1).unsqueeze(-1)  # Expand dimensions to match local features
        global_features = global_features.expand(-1, -1, x.size(2), x.size(3))  # Expand to match spatial dimensions
        
        # Local feature extraction
        local_features = self.local_conv(x)
        local_features = F.relu(local_features)
        
        # Concatenate global and local features
        combined_features = torch.cat((global_features, local_features), dim=1)
        
        # CNN processing
        cnn_output = self.cnn(combined_features)
        
        # Flatten for fully connected layer
        cnn_output = cnn_output.view(cnn_output.size(0), -1)
        
        # Fully connected layer
        output = self.fc(cnn_output)
        return output
</code></pre>
<pre><code>#custom_model_1 = CustomModel(input_channels = 3 , hidden_units = 16 , output_classes = len(class_names) )
input_channels = 3
hidden_units = 16
output_classes = len(class_names) # this is 75 btw.
custom_model = CustomModel(input_channels, hidden_units, output_classes)

</code></pre>
<p>when I try to train the model I am getting this error.</p>
<pre><code>Given groups=1, weight of size [16, 32, 3, 3], expected input[42, 19, 224, 224] to have 32 channels, but got 19 channels instead
</code></pre>
<p>Why is this happening? I have a hunch that the input dimension are not matching to the conv layers. Not sure where I am missing. Please help!</p>
","2024-03-25 13:29:48","0","Question"
"78218686","78216943","","<p>look at whole <code>o</code> closely:</p>
<p>(tensor([[[-0.1934, -0.2133,  0.0361,  0.2266,  0.2379, <strong>-0.0954,  0.1876,-0.3175,  0.3742, -0.6895</strong>],<br />
[-0.2852, -0.0441, -0.1567, -0.0664,  0.2387, -0.3890, -0.1220,-0.0149,  0.1201, -0.3516],<br />
[-0.1148,  0.1436, -0.1213,  0.4508,  0.1348, -0.4948,  0.0236,0.2680, -0.1538,  0.0195],<br />
[<strong>-0.4600,  0.0463, -0.5190, -0.0478,  0.0284</strong>,  0.1085,  0.0334,-0.0956,  0.2880, -0.1574]]], grad_fn=),</p>
<pre><code>tensor([[[-0.4600,  0.0463, -0.5190, -0.0478,  0.0284]],  
        [[-0.0954,  0.1876, -0.3175,  0.3742, -0.6895]]],grad_fn=&lt;StackBackward0&gt;))
</code></pre>
<p>Explanation: backward pass process input from tail to head, finished near <em>first</em> token, so it's final hidden state associated with one.</p>
","2024-03-25 11:17:29","0","Answer"
"78218478","78215347","","<p>I think the link is incorrect, try to download the dataset with that :</p>
<pre><code>https://github.com/Tony-Y/pytorch_warmup/blob/master/examples/emnist/download.py
</code></pre>
<p>And then change your code with :</p>
<pre><code>import torchvision
from torchvision import transforms

# Update the path to where you've manually placed the EMNIST dataset
root_dir = &quot;./path/to/your/emnist&quot;  # Change this to the actual path

trainset = torchvision.datasets.EMNIST(root=root_dir,
                                   split=&quot;letters&quot;,
                                   train=True,
                                   download=False,  # Set to False since you already downloaded it 
                                   transform=transforms.ToTensor())
</code></pre>
","2024-03-25 10:41:34","2","Answer"
"78217090","78183172","","<p>You can also run the training using Pytorch Estimator and can have full control on which library to use. Please refer the below example to use pytorch estimator and install any additional libraries using requirements.txt</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/training/smart_sifting/Text_Classification_BERT/Train_text_classification.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/training/smart_sifting/Text_Classification_BERT/Train_text_classification.ipynb</a></p>
","2024-03-25 04:36:57","1","Answer"
"78216943","","Why the output and hidden state of last layer returned by GRU are not the same","<p>Okay, here is the document of Pytorch, GRU return 2 variables:</p>
<ul>
<li><code>output</code> has a shape of <code>(N,L,D∗Hout)(N,L,D∗Hout​)</code>  containing the output features <code>(h_t)</code> from the last layer of the GRU, for each t.</li>
<li><code>hidden</code> <code>(D∗num_layers,N,Hout​)</code> containing the final hidden state for the input sequence.</li>
</ul>
<p>So as I understand, if I have GRU one layer and unidirectional then when I get the last time step of output, it will be the hidden state returned by GRU. I have checked this by code, and it is correct, but when I use bidirectional then the hidden state and the last time step just the same in the forward pass, the backward pass is different? Does GRU apply additional layer to the backward?</p>
<pre><code>input_ids = torch.randn((1,4,3))
gru = nn.GRU(input_size=3, hidden_size=5, bidirectional=True, num_layers=1, batch_first=True)
o, h = gru(input_ids)

print(o[:,-1,:]) # get the last time step of the last layer hidden state
print(torch.cat([h[0::2, :, :], h[1::2,:,:]], dim=-1)) # concat forward and backward hidden state 

tensor([[-0.2308,  0.0597, -0.4346, -0.3713,  0.2811,  0.2582, -0.0627,  0.3114,
         -0.3813,  0.0590]], grad_fn=&lt;SliceBackward0&gt;)

tensor([[[-0.2308,  0.0597, -0.4346, -0.3713,  0.2811,  0.3513, -0.0135,
           0.1894, -0.4211,  0.0058]]], grad_fn=&lt;CatBackward0&gt;)
</code></pre>
<p>Edit:</p>
<p>I found this post: <a href=""https://discuss.pytorch.org/t/missing-or-conflicting-documentations-between-versions/166075/2"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/missing-or-conflicting-documentations-between-versions/166075/2</a></p>
","2024-03-25 03:25:19","2","Question"
"78216575","78213696","","<p>In the repository you linked, there is a zip file called <em>Source code</em> which contains <em>/models</em> and some other helper modules. I was able to load the model in Colab by downloading the zip, expanding it to a directory in my Google Drive called <em>yolov7</em>, moving <em>yolov7-tiny.pt</em> to this directory and then running the following:</p>
<pre class=""lang-py prettyprint-override""><code>from google.colab import drive
drive.mount('/content/drive')

import torch
import onnx
import sys

sys.path.append('/content/drive/My Drive/yolov7')
model = torch.load('/content/drive/My Drive/yolov7/yolov7-tiny.pt')
</code></pre>
","2024-03-25 00:28:53","0","Answer"
"78216357","78199621","","<p>I solved the issue simply uninstalling <code>causal-conv1d</code> after the rebuilding of <code>mamba</code> following the istruction of this <a href=""https://github.com/state-spaces/mamba/issues/40#issuecomment-1849095898"" rel=""nofollow noreferrer"">answer</a>.</p>
","2024-03-24 22:38:04","0","Answer"
"78215878","78215078","","<p>It looks like you have:</p>
<ul>
<li><code>yTrue_yHat_allBatches_tensorSub</code> shaped (2, 15)</li>
<li><code>interceptXY_data_allBatches[:, :, :-1]</code> shaped (2, 15, 5)</li>
</ul>
<p>If you want to multiply them to get a resulting shape of (2, 5), then you need to make the first one into (2, 1, 15) using <code>.unsqueeze(dim=1)</code>. Then you can use <a href=""https://pytorch.org/docs/stable/generated/torch.bmm.html"" rel=""nofollow noreferrer""><code>torch.bmm()</code></a> or the <code>@</code> operator to multiply (2, 1, 15) into (2, 15, 5), yielding a result shaped (2, 1, 5). Finally, <code>.squeeze</code> the result to drop the singleton dimension and get (2, 5).</p>
<pre><code>y_yhat_allBatches_matmulX_allBatches =\
    torch.bmm(yTrue_yHat_allBatches_tensorSub.unsqueeze(dim=1),
              interceptXY_data_allBatches[:, :, :-1]
             ).squeeze()
</code></pre>
<p>More compact notation using the <code>@</code> operator:</p>
<pre class=""lang-py prettyprint-override""><code>y_yhat_allBatches_matmulX_allBatches =\
    (yTrue_yHat_allBatches_tensorSub.unsqueeze(dim=1) @ interceptXY_data_allBatches[:, :, :-1]).squeeze()
</code></pre>
","2024-03-24 19:44:16","1","Answer"
"78215686","78209363","","<p>Your model maps the input of shape <code>(4, 6)</code> to <code>(4, 12)</code> in the first linear layer, then to <code>(4, 3)</code> in the second layer.</p>
<p>If you want the output to be of shape <code>(4, 3, 3)</code>, you need to have the second layer output <code>(4, 3*3)</code>, then reshape.</p>
<pre class=""lang-py prettyprint-override""><code>n_problems = 3
classes_per_problem = 3

model = nn.Linear(6, n_problems*classes_per_problem)

x = torch.randn(4, 6)
x1 = model(x)
bs, _ = x1.shape
x1 = x1.reshape(bs, classes_per_problem, n_problems)

y = torch.randint(high=classes_per_problem, size=(bs, n_problems))
loss_function = nn.CrossEntropyLoss()

loss = loss_function(x1, y)
</code></pre>
","2024-03-24 18:39:21","1","Answer"
"78215347","","Load EMNIST dataset from within the Pytorch","<p>I'm working on <strong>EMNIST</strong> dataset and want to load it from PyTorch, but it returns a strange error as:</p>
<blockquote>
<p>RuntimeError: File not found or corrupted.</p>
</blockquote>
<p>Here's how i have tried to load the dataset:</p>
<pre><code>trainset = torchvision.datasets.EMNIST(root=&quot;emnist&quot;,
                                   split=&quot;letters&quot;,
                                   train=True,
                                   download=True,
                                   transform=transforms.ToTensor())
</code></pre>
<p>What might be wrong?</p>
","2024-03-24 16:53:53","1","Question"
"78215320","78203005","","<p>I encountered something similiar and found this explanation on Github
<a href=""https://github.com/DeepLabCut/DeepLabCut/issues/2465"" rel=""nofollow noreferrer"">https://github.com/DeepLabCut/DeepLabCut/issues/2465</a></p>
<blockquote>
<p>I think the reason that Colab now uses 1% of the GPU is because Google
made an update to CUDA 12.2 recently. Tensorflow-2.10.0 automatically
installs on Colab, so it needs CUDA 11.2 and cuDNN 8.1 (according to
the Linux GPU section here). A temporary solution is to connect to a
GPU runtime -&gt; click tools -&gt; command palette -&gt; type in and select
'use fallback runtime'. But this will only work until early Jan
unfortunately.</p>
</blockquote>
<p>The ulitmate solution that worked for those in the thread is  to run</p>
<blockquote>
<ul>
<li>!apt update &amp;&amp; apt install cuda-11-8</li>
</ul>
</blockquote>
<p>before running other code in Colab.</p>
","2024-03-24 16:45:19","0","Answer"
"78215078","","broadcasting tensor matmul over batches","<p>how can i find dot product of each batch response and X data.</p>
<pre><code>y_yhat_allBatches_matmulX_allBatches = torch.matmul(yTrue_yHat_allBatches_tensorSub, interceptXY_data_allBatches[:, :, :-1])
</code></pre>
<p>expected shape of <code>y_yhat_allBatches_matmulX_allBatches</code> should be 2 by 5. where each row is for specific batch</p>
<p><code>yTrue_yHat_allBatches_tensorSub.shape</code> = <code>[2, 15]</code> where rows batch (1&amp;2) and columns = size of response (15)</p>
<p><code>interceptXY_data_allBatches[:, :, :-1].shape = torch.Size([2, 15, 5])</code> for 15 observations by 5 features for 2 batches</p>
<p>please see full reproducible code</p>
<pre><code>#define dataset
nFeatures_withIntercept = 5
NObservations = 15
miniBatches = 2
interceptXY_data_allBatches = torch.randn(miniBatches, NObservations, nFeatures_withIntercept+1) #+1 Y(response variable)

#random assign beta to work with
beta_holder = torch.rand(nFeatures_withIntercept)

#y_predicted for each mini-batch
y_predBatchAllBatches = torch.matmul(interceptXY_data_allBatches[:, :, :-1], beta_holder)

#y_true - y_predicted for each mini-batch
yTrue_yHat_allBatches_tensorSub = torch.sub(interceptXY_data_allBatches[..., -1], y_predBatchAllBatches)
y_yhat_allBatches_matmulX_allBatches = torch.matmul(yTrue_yHat_allBatches_tensorSub, interceptXY_data_allBatches[:, :, :-1])
</code></pre>
","2024-03-24 15:26:11","1","Question"
"78215054","78205960","","<p>You haven't mentioned what PyTorch code you're running.</p>
<p>However, back when Gensim had <em>both</em> a plain, pure-Python implementation of its word2vec routines, and the alternate Cython- and BLAS- optimized path, it was typical for the plain-Python to run 80-120 times <em>slower</em>.</p>
<p>If the PyTorch code you're running is pure-Python, and further might be mainly instructional in its purpose – without even Python efficiency considered, or perhaps without any attempt to use multiple threads/processes – then a slowdown from &quot;under a minute&quot; to &quot;several hours&quot; would be plausible.</p>
","2024-03-24 15:20:22","0","Answer"
"78214179","78214004","","<p>You can use an inplace operation like <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.fill_.html"" rel=""nofollow noreferrer""><code>fill_</code></a>:</p>
<pre><code>tensor.fill_(3.0)
</code></pre>
","2024-03-24 10:35:49","1","Answer"
"78214004","","Single value tensor set value?","<p>So if I have a tensor with a single value I can access the element with:</p>
<pre><code>tensor.item()
</code></pre>
<p>But how can I change it?</p>
<p>Will:</p>
<pre><code>tensor.item() = 3.0
</code></pre>
<p>work, I don't think so.</p>
<pre><code>tensor[0] = 3.0
</code></pre>
<p>Isn't an option either for a single value tensor.</p>
<p>I can like divide the tensor by itself and then multiply it by the value I need to set but this looks like an ugly hack (as well as creating a new tensor).</p>
","2024-03-24 09:30:15","0","Question"
"78213696","","after loading a pretrained pytorch .pt model file: ModuleNotFoundError: No module named 'models'","<p>I downloaded one of the pretrained yolo models from the link:
<a href=""https://github.com/WongKinYiu/yolov7/releases"" rel=""nofollow noreferrer"">https://github.com/WongKinYiu/yolov7/releases</a></p>
<p>In this case, yolov7-tiny.pt is downloaded.
Then tried to run the code to load the model and convert it to onnx file:</p>
<pre><code>import torch
import onnx

model = torch.load('./yolo_custom/yolov7-tiny.pt')
input_shape = (1, 3, 640, 640)
torch.onnx.export(model, torch.randn(input_shape), 'yolov7-tiny.onnx', opset_version=11)
</code></pre>
<p>An error occurs on</p>
<pre><code>model = torch.load('./yolo_custom/yolov7-tiny.pt')
</code></pre>
<p>and the error message is:</p>
<pre><code>ModuleNotFoundError: No module named 'models'
</code></pre>
<p>The issue is reproducible even on Colab. Is there anything wrong on the steps?</p>
","2024-03-24 07:14:29","0","Question"
"78213672","78210261","","<p>Here is my solution</p>
<pre class=""lang-py prettyprint-override""><code>import os
import datetime

from diffusers import DiffusionPipeline
import torch

if __name__ == &quot;__main__&quot;:
    output_dir = &quot;output_images&quot;
    os.makedirs(output_dir, exist_ok=True)

    pipe = DiffusionPipeline.from_pretrained(
        # https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,
        torch_dtype=torch.float16,
        use_safetensors=True,
        variant=&quot;fp16&quot;,
    )
    pipe.to(&quot;cuda&quot;)
    # enabling xformers for memory efficiency
    pipe.enable_xformers_memory_efficient_attention()

    prompt = &quot;Extreme close up of a slice a lemon with splashing green cocktail, alcohol,  healthy food photography&quot;

    images = pipe(
                prompt=prompt,
                negative_prompt='',
                width=1024,                                     # Width of the image in pixels.
                height=1024,                                    # Height of the image in pixels.
                guidance_scale=guidance_scale,                  # How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt).
                num_inference_steps=num_inference_steps,        # Amount of inference steps performed on image generation.
                num_images_per_prompt = 1,

    ).images
    timestamp = datetime.datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)
    image_path = os.path.join(output_dir, f&quot;output_{timestamp}.jpg&quot;)
    images[0].save(image_path)

    print(f&quot;Image saved at: {image_path}&quot;)
</code></pre>
","2024-03-24 07:07:47","0","Answer"
"78213233","78211526","","<p>if you use transformers version latest(v4.39.1), plz try to downgrade to v4.38.2.
I solved it that way.</p>
","2024-03-24 02:49:08","0","Answer"
"78212483","78211309","","<p>Your <code>nn.Sequential</code> setup doesn't make sense. <code>nn.Sequential</code> runs the model modules in the order listed. Yours:</p>
<pre class=""lang-py prettyprint-override""><code>        self.hidden_layer = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_size),
            nn.Linear(input_size, hidden_size),
            nn.Linear(input_size, hidden_size),

            nn.ReLU()
        )
</code></pre>
<p>Has linear layers back to back, which is redundant since the composition of two linear layers is still a linear layer. Your sizes don't line up. The first layer maps an input of size <code>input_size</code> to <code>hidden_size</code>, but your second layer expects the input to be of size <code>input_size</code>. This works for you currently because you are using the same size for input and hidden, but this will throw an error if that is ever not the case.</p>
<p>You want something like this:</p>
<pre class=""lang-py prettyprint-override""><code>self.hidden_layer = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.BatchNorm1d(hidden_size),
    nn.Linear(hidden_size, hidden_size),
    nn.ReLU(),
    nn.BatchNorm1d(hidden_size)
)
</code></pre>
<p>That example has two blocks of linear/relu/batchnorm. You can add more if you want.</p>
<p>Your <code>forward</code> method is also weird.</p>
<p>First, make sure <code>nn.Flatten</code> is doing what you expect. Check the input/output shapes to be sure.</p>
<p>Second, you apply the same block of layers three times. If you want more layers, you should add them to the <code>nn.Sequential</code> block instead of passing different activations through the same layers 3 times.</p>
","2024-03-23 21:07:19","0","Answer"
"78212238","78210297","","<p>You don't have to worry about file endings in this case.</p>
<p><code>torch.save</code> basically writes the input to a zip file (it also works for arbitrary python objects, not just weights). The <code>.pt</code> file ending is just convention.</p>
<p>Huggingface saves model weights to a directory, usually of the form <code>model_name.hf/</code>.</p>
<p>The error you are seeing is because <code>AutoModelForCTC.from_pretrained(&quot;model.pt&quot;)</code> results in huggingface code looking for a directory named <code>model.pt</code>, when no such directory exists.</p>
<p>When you download the model the first time, the weights are saved to cache, usually <code>~/.cache/huggingface/hub</code>.</p>
<p>If you want to explicitly save the model weights to a different directory, you can run:</p>
<pre><code>model = AutoModelForCTC.from_pretrained(model_name, config=config)
model.save_model(&quot;path_to_save&quot;) 
</code></pre>
","2024-03-23 19:47:39","1","Answer"
"78212126","78203794","","<p>Found the fix myself. Turns out you can just go into the ??create_vision_model and then the ??add_head and put them as the model class inside of the &quot;initialize&quot; function inside of the handler.py ; you should end up with something like this:</p>
<pre><code>state_dict = torch.load(model_pt_path, map_location=self.device)
head = None
concat_pool = True
pool = True
lin_ftrs = None
ps = 0.5
first_bn = True
bn_final = False
lin_first = False
y_range = None
init = nn.init.kaiming_normal_
arch = resnet50
n_out = 9
pretrained = True
cut = None
n_in = 3
custom_head = None
# self.model = MyVisionModel()
meta = model_meta.get(arch, _default_meta)
model = arch(pretrained=pretrained)
body = create_body(model, n_in, pretrained, ifnone(cut, meta['cut']))
nf = num_features_model(nn.Sequential(*body.children())) if custom_head is None else None
if head is None:
    head = create_head(nf, n_out, concat_pool=concat_pool, pool=pool,
                       lin_ftrs=lin_ftrs, ps=ps, first_bn=first_bn, bn_final=bn_final, lin_first=lin_first,
                       y_range=y_range)
self.model = nn.Sequential(body, head)
self.model.load_state_dict(state_dict)
self.model.to(self.device)
self.model.eval()

logger.debug(&quot;Model file {0} loaded successfully&quot;.format(model_pt_path))
self.initialized = True
</code></pre>
","2024-03-23 19:12:25","0","Answer"
"78211526","","PyTorch: AttributeError: 'torch.dtype' object has no attribute 'itemsize'","<p>I am trying to follow this article on medium <a href=""https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07"" rel=""nofollow noreferrer"">Article</a>.</p>
<p>I had a few problems with it so the remain chang eI did was to the <code>TrainingArguments</code> object I added <code>gradient_checkpointing_kwargs={'use_reentrant':False},</code>.</p>
<p>So now I have the following objects:</p>
<pre class=""lang-py prettyprint-override""><code>peft_training_args = TrainingArguments(
    output_dir = output_dir,
    warmup_steps=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    max_steps=100, #1000
    learning_rate=2e-4,
    optim=&quot;paged_adamw_8bit&quot;,
    logging_steps=25,
    logging_dir=&quot;./logs&quot;,
    save_strategy=&quot;steps&quot;,
    save_steps=25,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=25,
    do_eval=True,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={'use_reentrant':False},
    report_to=&quot;none&quot;,
    overwrite_output_dir = 'True',
    group_by_length=True,
)

peft_model.config.use_cache = False

peft_trainer = transformers.Trainer(
    model=peft_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=peft_training_args,
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
</code></pre>
<p>And when I call <code>peft_trainer.train()</code> I get the following error:</p>
<pre><code>AttributeError: 'torch.dtype' object has no attribute 'itemsize'
</code></pre>
<p>I'm using Databricks, and my pytorch version is <code>2.0.1+cu118</code></p>
","2024-03-23 16:05:27","2","Question"
"78211345","","ImportError: cannot import name 'NP_SUPPORTED_MODULES' from 'torch._dynamo.utils'","<pre><code>try:
    import os
    from bs4 import BeautifulSoup as bs
    import xlsxwriter
    import sys,traceback
    import time,threading
    import PySimpleGUI as sg
    import requests
    import easyocr
    import certifi
    from datetime import datetime
    from pymongo import MongoClient
    from bson.objectid import ObjectId
except:
    sys.stderr = open('../error.log', 'a+',encoding='utf-8')
    traceback.print_exc()
    sys.stderr.close()

reader = easyocr.Reader(['en']) # this needs to run only once to load the model into memory
</code></pre>
<p>Traceback</p>
<pre><code>(most recent call last):
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\osos_adanaorganize_scraper.py&quot;, line 9, in &lt;module&gt;
    import easyocr
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\easyocr\__init__.py&quot;, line 1, in &lt;module&gt;
    from .easyocr import Reader
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\easyocr\easyocr.py&quot;, line 3, in &lt;module&gt;
    from .recognition import get_recognizer, get_text
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\easyocr\recognition.py&quot;, line 6, in &lt;module&gt;
    import torchvision.transforms as transforms
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\torchvision\__init__.py&quot;, line 6, in &lt;module&gt;
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\osos-scraper-_win_application\lib\torchvision\models\__init__.py&quot;, line 2, in &lt;module&gt;
    from .convnext import *
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\osos-scraper-_win_application\lib\torchvision\models\convnext.py&quot;, line 8, in &lt;module&gt;
    from ..ops.misc import Conv2dNormActivation, Permute
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\torchvision\ops\__init__.py&quot;, line 23, in &lt;module&gt;
    from .poolers import MultiScaleRoIAlign
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\torchvision\ops\poolers.py&quot;, line 10, in &lt;module&gt;
    from .roi_align import roi_align
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\torchvision\ops\roi_align.py&quot;, line 4, in &lt;module&gt;
    import torch._dynamo
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\torch\_dynamo\__init__.py&quot;, line 2, in &lt;module&gt;
    from . import allowed_functions, convert_frame, eval_frame, resume_execution
  File &quot;D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\env\lib\site-packages\torch\_dynamo\allowed_functions.py&quot;, line 30, in &lt;module&gt;
    from .utils import hashable, is_safe_constant, NP_SUPPORTED_MODULES
ImportError: cannot import name 'NP_SUPPORTED_MODULES' from 'torch._dynamo.utils' (D:\Developing\W.S-PROJECT\osos.adanaorganize.org.tr\osos-scraper-_win_application\lib\torch\_dynamo\utils.pyc)
</code></pre>
<p>when I am running the script in vs code is working.
but when I am creating an exe using cx_freeze, it is successfully created.
After I am running exe then gives me this error.</p>
","2024-03-23 15:02:11","0","Question"
"78211309","","Simple NN aiming to learn a of non-linear equations can't converge","<p>I've got a bunch of equations using sums, multiplication and min(x,0) or max(x,0) that yield a result (one output, 18 inputs).</p>
<p>I'm trying to have an NN model in pytorch learn these so I generate quick results.</p>
<p>I generated 30k random X-Y pairs in excel (just using RND()*100-50 for X and calculating Y).
I uploaded the pairs with pandas and wrote an NN with ReLu (which I hoped would handle the non-linearity). Here's the net:</p>
<pre><code>class MyModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.flatten = nn.Flatten()  # Flatten input data
        self.hidden_layer = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_size),
            nn.Linear(input_size, hidden_size),
            nn.Linear(input_size, hidden_size),

            nn.ReLU()
        )
        self.output_layer = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.flatten(x)
        x = self.hidden_layer(x)
        x = self.hidden_layer(x)
        x = self.hidden_layer(x)
        output = self.output_layer(x)
        return output
</code></pre>
<p>sizes are 18 for inputs and hidden layer and 1 for output.</p>
<p>Can't converge, left with quite a big error. Thought that'd be a simple task for an NN, to learn that set of equations, there's no noise or anything. What can I do to make this work?</p>
","2024-03-23 14:50:27","0","Question"
"78210297","","How to convert pretrained hugging face model to .pt and run it fully locally?","<p>I'm attempting to convert this <a href=""https://huggingface.co/UrukHan/wav2vec2-russian"" rel=""nofollow noreferrer"">model</a> in .pt format. It's working fine for me so i dont want to fine-tune it. How can i export it to .pt and run interface?</p>
<p>I tried using this to convert to .pt:</p>
<pre><code>from transformers import AutoConfig, AutoProcessor, AutoModelForCTC, AutoTokenizer, Wav2Vec2Processor
import librosa
import torch



# Define the model name
model_name = &quot;UrukHan/wav2vec2-russian&quot;

# Load the model and tokenizer
config = AutoConfig.from_pretrained(model_name)
model = AutoModelForCTC.from_pretrained(model_name, config=config)
processor = Wav2Vec2Processor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Save the model as a .pt file
torch.save(model.state_dict(), &quot;model.pt&quot;)

# Save the tokenizer as well if needed
tokenizer.save_pretrained(&quot;model-tokenizer&quot;)
</code></pre>
<p>but unfortunately its not running the interface :</p>
<pre><code>model = AutoModelForCTC.from_pretrained(&quot;model.pt&quot;)
processor = AutoProcessor.from_pretrained(&quot;model.pt&quot;)


# Perform inference with the model
FILE = 'here is wav.wav'
audio, _ = librosa.load(FILE, sr = 16000)
audio = list(audio)
def map_to_result(batch):
  with torch.no_grad():
    input_values = torch.tensor(batch, device=&quot;cpu&quot;).unsqueeze(0) #, device=&quot;cuda&quot;
    logits = model(input_values).logits
  pred_ids = torch.argmax(logits, dim=-1)
  batch = processor.batch_decode(pred_ids)[0]
  return batch
map_to_result(audio)
print(map_to_result(audio))


model.eval()
</code></pre>
<p>And encountered an error:
`model.pt is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'</p>
<p>`</p>
","2024-03-23 09:18:49","-1","Question"
"78210261","","How to configure inference settings to generate images with the Stable Diffusion XL pipeline?","<p>I'm working with the Stable Diffusion XL (SDXL) model from Hugging Face's diffusers library and I want to set this inference parameters :</p>
<ul>
<li>width: Width of the image in pixels.</li>
<li>height: Height of the image in pixels.</li>
<li>steps: Amount of inference steps performed on image generation.</li>
<li>cfg_scale: How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt).</li>
</ul>
<p>Here's a minimal example of my current implementation:</p>
<pre><code>import os
import datetime

from diffusers import DiffusionPipeline
import torch

if __name__ == &quot;__main__&quot;:
    output_dir = &quot;output_images&quot;
    os.makedirs(output_dir, exist_ok=True)

    pipe = DiffusionPipeline.from_pretrained(
        # https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,
        torch_dtype=torch.float16,
        use_safetensors=True,
        variant=&quot;fp16&quot;,
    )
    pipe.to(&quot;cuda&quot;)
    # enabling xformers for memory efficiency
    pipe.enable_xformers_memory_efficient_attention()

    prompt = &quot;Extreme close up of a slice a lemon with splashing green cocktail, alcohol,  healthy food photography&quot;

    images = pipe(prompt=prompt).images
    timestamp = datetime.datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)
    image_path = os.path.join(output_dir, f&quot;output_{timestamp}.jpg&quot;)
    images[0].save(image_path)

    print(f&quot;Image saved at: {image_path}&quot;)
</code></pre>
<p>How Can I set the inference parameters?</p>
","2024-03-23 09:03:52","0","Question"
"78209852","78208603","","<p>It's not entirely clear why they do this. It was implemented that way for some reason a long time ago and now it's backward incompatible to change it. You can find discussion of this <a href=""https://github.com/pytorch/pytorch/issues/2159"" rel=""nofollow noreferrer"">here</a>. The transpose operation doesn't add any overhead so there's no performance incentive to change it.</p>
","2024-03-23 05:35:26","0","Answer"
"78209813","77802368","","<p>It seems that MultiStepLR is invalid in LightningModule.I met the same error and fixed it by replace MultiStepLR by a ReduceLROnPlateau.</p>
<pre><code>optim_conf = model.configure_optimizers()
monitor = &quot;val_loss&quot;
a,b = optim_conf
scheduler0 = ReduceLROnPlateau(a[0])
b[0]=scheduler0
</code></pre>
<p>where a is a list of Adam optimizer,b is also a list and b[0] is MultiStepLR.
Hope this could help.</p>
","2024-03-23 05:10:26","0","Answer"
"78209637","78209351","","<p>I think I figured out the answer, but I'm not 100% sure (due to the fact that I'm pretty new at this stuff). Please feel free to correct me.</p>
<p>I think my original question was based on a misunderstanding of <a href=""https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf"" rel=""nofollow noreferrer"">the original word2vec paper</a>: I don't think you're supposed to drop common words from the vocabulary altogether, I think you're supposed to omit  them <em>when you're generating the training pairs</em> (in my case, skipgrams). The words stay in the vocabulary, but you (ahem) skip them when you're making the training data with some probability.</p>
<p>So the answer (according to the paper) is option 1.</p>
","2024-03-23 03:14:54","0","Answer"
"78209363","","How to make a neural network with multiple outputs (and multiple classes) using pytorch?","<p>I am working on a multi-output (i.e &gt; 1 output target) multi-class (i.e &gt; 1 class) (I believe this is also called a multi-task problem).
For example, my train_features_data is of shape (4, 6) (i.e three rows/examples and 6 columns/features), and my train_target_data is of shape (4, 3) (i.e 4 rows/examples and 3 columns/targets). For each target I have three different classes (-1, 0, 1).</p>
<p>I define an example model architecture (and data) for this problem like so:</p>
<pre><code>import pandas as pd
from torch import nn 
from logging import log
import torch
feature_data = {
    'A': [1, 2, 3, 4],
    'B': [5, 6, 7, 8],
    'C': [9, 10, 11, 12],
    'D': [13, 14, 15, 16],
    'E': [17, 18, 19, 20],
    'F': [21, 22, 23, 24]
}

target_data = {
    'Col1': [1, -1, 0, 1],
    'Col2': [-1, 0, 1, -1],
    'Col3': [-1, 0, 1, 1]
}

# Create the DataFrame
train_feature_data = pd.DataFrame(feature_data) 
train_target_data = pd.DataFrame(target_data)
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

# create the model
class MyModel(nn.Module):
  def __init__(self, inputs=6, l1=12, outputs=3):
      super().__init__()
      self.sequence = nn.Sequential(
        nn.Linear(inputs, l1),
        nn.Linear(l1, outputs),
        nn.Softmax(dim=1)
    )
      
  def forward(self, x):
      x = self.sequence(x)
      return x
    
x_train = torch.tensor(train_feature_data.to_numpy()).type(torch.float)
model = MyModel(inputs = 6, l1 = 12, outputs = 3).to(device)
model(x_train.to(device=device))
</code></pre>
<p>When I pass my train data into the model (i.e when i call model(x_train.to(device=device))), I get back an array of shape (4, 3).</p>
<p>By following this resource <a href=""https://www.learnpytorch.io/02_pytorch_classification/#83-creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model"" rel=""nofollow noreferrer"">resource</a>, my expectation was that I would get a shape of like (4, 3, 3) whereby the first axis (i.e 4) is the number of examples in my features and targets data, the second axis (i.e the middle 3) represents the logits (or in this case because I have a softmax function, this will be the predicted probabilities) of each example (and this would be 3 because I have three classes), while the third axis (or rightmost 3 value in the shape) represents the number of outputs/columns I have in my train_target_data.</p>
<p>Can someone please provide some guidance on what I'm doing incorrectly here (if my approach is wrong) and how to go about fixing it. Thanks.</p>
","2024-03-23 00:28:47","0","Question"
"78209351","","Subsampling when training word embeddings","<p>NLP newbie here with a question about word embeddings. As a learning exercise, I'm trying to train my own set of word embeddings based on word2vec. I have a corpus of english sentences that I've downloaded and cleaned and I think I have a decent grasp of how the training is supposed to work, but there's something I still don't really understand.</p>
<p>As one might imagine, the corpus contains many more instances of common words like 'the', 'and', and so on. The word frequency distribution is a fairly extreme power law, which makes sense. My question is this: what are the best practices to deal with this when I'm generating samples to train the word embeddings?</p>
<p>I can see a few of options:</p>
<ol>
<li>When I'm generating training samples, do some sort of probabilistic sampling based on the frequency of the input token in the dataset. My newbie intuition is that this makes some sense, but I'm not 100% sure how the sampling should work.</li>
<li>With some probability, drop the most common words from the vocabulary altogether and don't learn embeddings for them at all. I've seen some guidance on the web (and in <a href=""https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf"" rel=""nofollow noreferrer"">the original word2vec paper</a>) that recommends doing this and just treating them as an OOV token when looking up the embedding, but it just feels ... weird. After all, I do want to have an embedding for the word 'the', even if it appears very frequently.</li>
<li>Just power through and live with the fact that I'm going to have a lot more training samples for the word 'the' than the word 'persnickety'. This will make a training epoch take a lot longer.</li>
</ol>
<p>Can anyone give me some guidance here? How do people usually deal with this kind of imbalance?</p>
","2024-03-23 00:23:27","0","Question"
"78208603","","Why are the dimension of the weight in torch.nn.functional.linear (out,in) instead of (in,out)","<p>In the documentation of torch.nn.functional.linear (<a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html</a>), the dimensions of the weight input are (out_features, in_features) then the wight matrix is transposed when computing the output: y=xA^T+b. Why are they doing this instead of taking a matrix W of dimensions (in_features, out_features) and doing y=xW+b?</p>
<p>By doing y=xW+b the dimensions will match and so I cannot find a clear reason for the above.</p>
","2024-03-22 20:01:42","0","Question"
"78208596","78205810","","<p>You've already converted <code>y</code>; the problem with <code>x</code> is that it's not a purely numerical array. You could convert <code>x</code> to a <a href=""https://numpy.org/doc/stable/user/basics.rec.html"" rel=""nofollow noreferrer"">NumPy structured array</a>,</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; ak.to_numpy(x)
array([(  0., 0. ), (100., 0.1), ( 20., 0.2), ( 30., 0.3), (  4., 0.4)],
      dtype=[('MET_pt', '&lt;f8'), ('MET_phi', '&lt;f8')])
</code></pre>
<p>which can then be viewed and reshaped to get the array that you want:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; ak.to_numpy(x).view(&quot;&lt;f8&quot;).reshape(-1, 2)
array([[  0. ,   0. ],
       [100. ,   0.1],
       [ 20. ,   0.2],
       [ 30. ,   0.3],
       [  4. ,   0.4]])
</code></pre>
<p>but this relies strongly on the fact that all of the fields are the same type, <code>&quot;&lt;f8&quot;</code> (doubles). If you had a mix of floating-point numbers and integers (charge?), or numbers of different bit-widths, then this wouldn't work.</p>
<p>Here's a better method: break up <code>x</code> (or the original <code>arr</code>) into its two fields, first.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; x[&quot;MET_pt&quot;]
&lt;Array [0, 100, 20, 30, 4] type='5 * float64'&gt;
&gt;&gt;&gt; x[&quot;MET_phi&quot;]
&lt;Array [0, 0.1, 0.2, 0.3, 0.4] type='5 * float64'&gt;
</code></pre>
<p>What you want to do is interleave these so that you get one value from <code>&quot;MET_pt&quot;</code>, followed by one value from <code>&quot;MET_phi&quot;</code>, then the next value from <code>&quot;MET_pt&quot;</code>, and so on. If you first put the values in length-1 lists, which is a reshaping (can be done in Awkward or NumPy, with the same syntax),</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; x[&quot;MET_pt&quot;, :, np.newaxis]
&lt;Array [[0], [100], [20], [30], [4]] type='5 * 1 * float64'&gt;
&gt;&gt;&gt; x[&quot;MET_phi&quot;, :, np.newaxis]
&lt;Array [[0], [0.1], [0.2], [0.3], [0.4]] type='5 * 1 * float64'&gt;
</code></pre>
<p>then what you want is to concatenate each of these length-1 lists from the first array with each of the length-1 lists from the second array. That is, you want to concatenate them, not concatenation at <code>axis=0</code>, but concatenation at <code>axis=1</code>, the first level deep of lists (see <a href=""https://awkward-array.org/doc/main/reference/generated/ak.concatenate.html"" rel=""nofollow noreferrer"">ak.concatenate</a> or <a href=""https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html"" rel=""nofollow noreferrer"">np.concatenate</a>).</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; np.concatenate((x[&quot;MET_pt&quot;, :, np.newaxis], x[&quot;MET_phi&quot;, :, np.newaxis]), axis=1)
&lt;Array [[0, 0], [100, 0.1], ..., [30, ...], [4, 0.4]] type='5 * 2 * float64'&gt;
</code></pre>
<p>Now you can pass it to Torch.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; torch.tensor(np.concatenate((
...     x[&quot;MET_pt&quot;, :, np.newaxis], x[&quot;MET_phi&quot;, :, np.newaxis]
... ), axis=1))
tensor([[  0.0000,   0.0000],
        [100.0000,   0.1000],
        [ 20.0000,   0.2000],
        [ 30.0000,   0.3000],
        [  4.0000,   0.4000]], dtype=torch.float64)
</code></pre>
","2024-03-22 19:59:27","1","Answer"
"78207182","78206233","","<p>Unable to reproduce. I ran the code you provided and the loss term has <code>requires_grad=True</code>.</p>
<pre class=""lang-py prettyprint-override""><code>class Cbow(nn.Module):
    def __init__(self, vocab_size, hctx_len, emb_size):
        super().__init__()
        self.proj = nn.Linear(in_features=vocab_size, out_features=emb_size, bias=False)
        self.hidden = nn.Linear(in_features=emb_size, out_features=vocab_size, bias=False)

    def forward(self, x):
        # x: (num_batches, 2*hctx_len, vocab_size)

        embs = self.proj(x)               # (num_batches, 2*hctx_len, emb_size)
        means = embs.mean(dim=1)          # (num_batches, emb_size)

        sim = self.hidden(means)            # (num_batches, vocab_size)
        out = torch.softmax(sim, dim=1)     # (num_batches, vocab_size)
        # breakpoint()

        return out
    
vocab_size = 64
hctx_len = 128
emb_size = 128
bs = 12

model = Cbow(vocab_size, hctx_len, emb_size)

x = torch.randn(bs, 2*hctx_len, vocab_size)
y = torch.randint(0, vocab_size, (bs,))

out = model(x)
loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(out, y)
print(loss.requires_grad)
&gt; True
</code></pre>
<p>Additionally, <code>CrossEntropyLoss</code> includes the softmax operation - applying a softmax in the model before the loss is incorrect.</p>
","2024-03-22 15:12:58","-1","Answer"
"78206652","78205810","","<p>One approach is to convert it to a list of dictionaries using <code>to_list()</code>, and then read out the numerical values. Converting it directly using <code>to_numpy()</code> seems to result in the keys being tied up in the dtypes, which is why I opted for <code>to_list()</code>.</p>
<pre><code>#Read out the values from each dictionary entry in arr.to_list()
arr_dicts = arr.to_list()
arr_dict_values = [list(arr_dict.values()) for arr_dict in arr_dicts]

#To numpy
arr_np = np.array(arr_dict_values)

#To float32 tensor. Could supply &quot;arr_np&quot; or &quot;arr_dict_values&quot; here.
arr_t = torch.tensor(arr_dict_values).float()

#Slice out X and y tensors
x_t = arr_t[:, 0:2]
y_t = arr_y[:, 2]
</code></pre>
","2024-03-22 13:45:53","1","Answer"
"78206233","","Intermediate results in the forward method of my torch neural net have requires_grad=False","<p>I have defined a small neural net in PyTorch, and tried to train it. However, the loss did not have requires_grad set - that was curious. I set a breakpoint in the forward method of my neural net, and none of the intermediate variables created (embs, means, sim, out) had requires_grad set.</p>
<p>Here is my code:</p>
<pre><code>class Cbow(nn.Module):
    def __init__(self, vocab_size, hctx_len, emb_size):
        super().__init__()
        self.proj = nn.Linear(in_features=vocab_size, out_features=emb_size, bias=False)
        self.hidden = nn.Linear(in_features=emb_size, out_features=vocab_size, bias=False)

    def forward(self, x):
        # x: (num_batches, 2*hctx_len, vocab_size)

        embs = self.proj(x)               # (num_batches, 2*hctx_len, emb_size)
        means = embs.mean(dim=1)          # (num_batches, emb_size)

        sim = self.hidden(means)            # (num_batches, vocab_size)
        out = torch.softmax(sim, dim=1)     # (num_batches, vocab_size)
        # breakpoint()

        return out


def fit_model_layers(model, train_loader, epochs, lr=0.01):
    model.train()

    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    optimizer.zero_grad()

    for e in range(epochs):
      running_loss, num_batches = 0, 0
      for x, y in train_loader:
          out = model(x)

          # calculate loss
          loss = loss_fn(out, y)                # loss.requires_grad = False !!!!!!
          running_loss += loss.item()
          num_batches += 1

          # backprop + optimization step
          loss.backward()
          optimizer.step()
          optimizer.zero_grad()

      print(f'Epoch {e+1} loss: {running_loss / num_batches}')
</code></pre>
<p>I don't quite understand what I do wrong. In the optimization example from PyTorch (<a href=""https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html</a>) intermediate results in forward() all have requires_grad set.</p>
<p>What could be wrong?</p>
","2024-03-22 12:28:42","0","Question"
"78205960","","Very long training times in pyTorch compared to Gensim","<p>I have trained a word2vec model using the Brown corpus with gensim as follows:</p>
<pre><code>model = gensim.models.Word2Vec(brown.sents(),min_count = 5,
                              vector_size = 30, window = 5, negative=5) 
</code></pre>
<p>This took just a couple of seconds to train.</p>
<p>Then I tried to build a word2vec model using pyTorch. The training takes more than 5 hours according to my estimation.</p>
<p>I was expecting a speed difference since Gensim's word2vec uses optimized C routines. However, I was not expecting this much difference.</p>
<p>Is it normal for the training to take this much time with pyTorch? For both cases I'm not using any GPU's. What should I expect the running time of PyTorch for this task on a standard 4-cpu Mac?</p>
","2024-03-22 11:37:50","0","Question"
"78205955","78203005","","<p>Google Colab's free version operates on a dynamic and undisclosed usage limit system, designed to manage access to computational resources like GPUs and TPUs. These limits, including runtime durations, availability of certain GPU types, and cooldown periods between sessions, can vary over time and are not transparently communicated to users. This approach helps prioritize resources for interactive use and moderate the consumption of long-running computations.</p>
<p>Frequent or heavy use can lead to shortened runtime durations, more frequent disconnections, and extended cooldown periods before reconnecting to a GPU, ranging from hours to days or even weeks. Google monitors the usage of individual accounts as well as related accounts, adjusting limits to prevent abuse of the system. Users facing restrictions receive generic messages about usage limits without specific reasons for disconnections or access denial, and there's no straightforward way for users to track their own usage. This opacity maintains Google's control over resource allocation and prevents users from circumventing the system's restrictions.</p>
","2024-03-22 11:36:52","0","Answer"
"78205810","","Converting from awkward arrays into torch arrays","<p>Note: I am using awkward version 1.10.3.</p>
<p>So, the general overview is that I have a set of data that is in awkward arrays, and I want to be able to pass this data to a simple feedforward pytorch model. I believe that pytorch doesn't natively handle awkward arrays so I am planning on converting the data to either torch or numpy arrays before passing through to the model. I should also note that whilst the data is stored in awkward arrays, at this point the data is <em>not</em> jagged.</p>
<p>Here is an example of the input data and of what I am looking for:</p>
<pre><code>import awkward as ak
import numpy as np
import torch


arr = ak.Array({&quot;MET_pt&quot; : [0.0, 100.0, 20.0, 30.0, 4.0],
                 &quot;MET_phi&quot; : [0, 0.1, 0.2, 0.3, 0.4],
                 &quot;class&quot; : [0, 1, 0, 1, 0]})

# These are my input features
x = arr[['MET_pt', 'MET_phi']]
# These are my class labels
y = arr['class']
#
## Here would be the code converting to torch tensors 
# 
x_torch = torch.tensor([[0, 0], [100, 0.1], [20, 0.2], [30, 0.3], [4, 0.4]])

y_torch = torch.tensor([0, 1, 0, 1, 0])
</code></pre>
<p>However, I cannot find an easy way to convert x from the awkward arrays to the torch arrays.
I can easily convert y to torch tensors by simply doing:</p>
<pre><code>torch.tensor(y)
&gt; tensor([0, 1, 0, 1, 0])
</code></pre>
<p>But I am unable to do this for the x array:</p>
<pre><code>torch.tensor(x)
&gt; TypeError: object of type 'Record' has no len()
</code></pre>
<p>This lead me to the idea of converting to numpy arrays first:</p>
<pre><code>torch.tensor(ak.to_numpy(x))
&gt; TypeError: can't convert np.ndarray of type numpy.void. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool
</code></pre>
<p>But as you can see this doesn't work either.</p>
<p>I think the problem lies in the fact that the ak.to_numpy() function converts the x array to:</p>
<pre><code>ak.to_numpy(x)
&gt; array([(  0., 0. ), (100., 0.1), ( 20., 0.2), ( 30., 0.3), (  4., 0.4)],
      dtype=[('MET_pt', '&lt;f8'), ('MET_phi', '&lt;f8')])
</code></pre>
<p>where I want it to convert like:</p>
<pre><code>ak.to_numpy(x)

&gt; [[0, 0], [100, 0.1], [20, 0.2], [30, 0.3], [4, 0.4]]
</code></pre>
<p>Is there anyway of converting an N-dim non-jagged awkward array such as x into the format shown immediately above? Or is there a smarter way to convert directly to torch tensors?</p>
<p>Sorry if this is a stupid question! Thanks!</p>
","2024-03-22 11:11:18","2","Question"
"78204360","","Upgrading accelerate while using Trainer class","<p>I am facing an issue whilst using Trainer class with Pytorch on Google Colab as it demands accelarate&gt;=0.21.0 even though I have updated all the requirements, is there any alternative to it?</p>
<p>&quot;Using the <code>Trainer</code> with <code>PyTorch</code> requires <code>accelerate&gt;=0.21.0</code>: Please run <code>pip install transformers[torch]</code> or <code>pip install accelerate -U</code>&quot;.
I have tried both the above suggestions but none worked. I don't know what am I doing wrong or how to proceed?</p>
","2024-03-22 06:18:05","0","Question"
"78203794","","How do I export my fastai resnet50/vision_learner trained model into torchserve?","<p>My goal is to deploy a model I trained with Fastai into Torchserve. I was following <a href=""https://aws.amazon.com/blogs/opensource/deploy-fast-ai-trained-pytorch-model-in-torchserve-and-host-in-amazon-sagemaker-inference-endpoint/"" rel=""nofollow noreferrer"">this tutorial</a> but got stuck on the part where he created the model class for pytorch.</p>
<p>He mentions that to run our model in Torchserve, and we need the following:</p>
<ol>
<li>A model class</li>
<li>The weights exported from pytorch (a pth file)</li>
<li>A handler</li>
</ol>
<p>Out of these, I get two: the weights and the handler. However, where I'm stuck is in the model class. He created one class file, but I have no idea where he got the <strong>DynamicUnet</strong> to use as a base for the class or how he mixed that class with <strong>unet_learner</strong> to create a custom pytorch model class. Can you help me build a model class for a model trained under the learner <strong>vision_learner</strong> and the pre-trained model of <strong>resnet50</strong>?</p>
","2024-03-22 02:55:25","-1","Question"
"78203424","78203314","","<p>You need to:</p>
<ol>
<li>Throw an exception in a new cell (ie 1/0)</li>
<li>Delete variables</li>
<li>Run <code>torch.cuda.empty_cache()</code></li>
</ol>
<p>The reason for <code>1</code> is when you have an error in a jupyter notebook, jupyter saves the error state to allow for use of the jupyter debugger. The error state holds references to GPU variables involved in the error, which prevents them from being cleared from memory. Throwing a new exception clears the old error state.</p>
","2024-03-22 00:11:52","0","Answer"
"78203314","","Avoid restarting jupyter notebook when encountering cuda out of memory exception?","<p>I am using pytorch and jupyter notebook. Frequently I'll encounter cuda out of memory and need to restart the notebook. How can I avoid needing to restart the whole notebook? I tried del a few variables but it didn't change anything.</p>
","2024-03-21 23:32:00","0","Question"
"78203005","","Google Colab can't use the GPU","<p>I'm kinda new to Google colab and have taken the Colab pro to train my neural nets but when computing the code I see that only the system RAM is used and the GPU Ram isn't used.</p>
<p>Is there any settings to force the code to be computed on the GPU?</p>
<p>Thanks for your help</p>
<p>I tried this to force the code to use the GPU but it still doesn't work:</p>
<p>for iteration in range(n_iterations):
with tf.device('/GPU:0'):</p>
","2024-03-21 21:54:27","-2","Question"
"78202853","78202800","","<p>OK for now I went for this, but if someone has a way to do it without creating a <code>K×L</code> matrix, I'll accept that instead.</p>
<pre class=""lang-py prettyprint-override""><code>def reverse_index(a, reverse_indices):
        &quot;&quot;&quot;
        assuming all elements in a are also in reverse_indices
        Args:
            a : Tensor, dim (K,)
            reverse_indices: Tensor, dim (L,)
        Returns:
            indices: so that ``reverse_indices[indices] = a``
        &quot;&quot;&quot;
        a = a[torch.isin(a, reverse_indices)]
        correspondance = (a[:, None] == reverse_indices[None, :]).astype(int)
        indices = correspondance.argmax(dim=1) #works cause all of them are 0 and one is 1
        return indices
</code></pre>
","2024-03-21 21:21:57","0","Answer"
"78202800","","pytorch: reverse indexing","<p>I have 2 torch tensors</p>
<pre><code>a : Tensor, dim (K,)
reverse_indices: Tensor, dim (L,)
</code></pre>
<p>Where all values in <code>reverse_indices</code> are unique and sorted (but all values of <code>a</code> are not necessarily in <code>reverse_indices</code>)</p>
<p>I am looking for an efficient way to obtain the tensor</p>
<pre><code>indices: Longtensor, dim (M,)
</code></pre>
<p>so that</p>
<pre><code>reverse_indices[indices] = a[torch.isin(a, reverse_indices)]
</code></pre>
<p>(note that <code>reverse_indices</code> is <strong>not</strong> the output of <code>torch.unique</code>, otherwise the result would be easy to get by just passing the right result to <code>torch.unique</code>).</p>
","2024-03-21 21:08:04","0","Question"
"78202782","78196998","","<p><strong>I reshaped the encoded word vector from (45, 1) to (1,45)</strong></p>
<p>if input size is (1,45) and batch_size = 2:</p>
<pre><code>size of weight matrix = output_features x input_features = 3x45

bias vector size = output_features = 3


         input x       weight transposed            bias
y = [ [1,2,3,...,45],  * [ [1, 2, 3],     +    [ [b1, b2, b3],
      [3,2,1,...,45]]      [2, 2, 3],            [b1, b2, b3] ]
                           [3, 2, 3],
                           [.  .  .],
                           [45,45,45] ]

         2x45       *        45x3
                  
                   2x3                   +           2x3
                          
</code></pre>
","2024-03-21 21:02:50","0","Answer"
"78200662","78200460","","<p>The reason is that <a href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" rel=""nofollow noreferrer""><code>transforms.ToTensor</code></a> is a <strong>class</strong>, so calling it will return an instance of that transformation, it expects the transformation parameters (if any) and initializes it. You can call the instance with a tensor and that will perform the operation. The correct way to use it is:</p>
<pre><code>transform = transforms.ToTensor()
e = transform(c)
</code></pre>
<p>In case you only want to call the transformation straight away, you can use its functional form: <a href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.functional.to_tensor.html"" rel=""nofollow noreferrer""><code>transforms.functional.to_tensor</code></a>:</p>
<pre><code>e = transforms.functional.to_tensor(c)
</code></pre>
","2024-03-21 14:26:07","1","Answer"
"78200460","","ToTensor() can not convert an image to tensor, and meet a TypeError?","<p>I meet a torchvision.transforms.ToTensor(img). When I directly input an image(128*128), it's wrong. But when I rename the function ttensor = transforms.ToTensor(), it works!
Just like this:</p>
<pre><code>import numpy as np
import torch
from torchvision import transforms

c = np.random.randn(128,128)
d = transforms.ToTensor([c])
# wrong,
# TypeError: __init__() takes 1 positional argument but 2 were given
ttensor = transforms.ToTensor()
e = ttensor(c)
# right
</code></pre>
<p>I don't know how to explain this. I want to know why a new variable can be in effect.</p>
<pre><code>ttensor = transforms.ToTensor()
</code></pre>
","2024-03-21 13:55:57","1","Question"
"78199810","","How to use GPT2 as a Question-Answering System (What to put in context?)","<p>I have tried to implement <a href=""https://github.com/openai/gpt-2"" rel=""nofollow noreferrer"">GPT2</a> as a question answering system with Pytorch. I copied their example code for <a href=""https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering"" rel=""nofollow noreferrer"">how to do this</a> into a separate python file and let it run. The code works, however the answer I get to the specified question is the context I have provided it with. The documentation of the pipeline() function I use here says specifying &quot;context&quot; is necessary for the function to <a href=""https://huggingface.co/transformers/v4.6.0/_modules/transformers/pipelines/question_answering.html"" rel=""nofollow noreferrer"">run</a>. So basically I need to give the system the answer beforehand, in order for it to be able to give the answer to me. Rather useless. I had hoped to find a way to use what the model has learnt from other dataset(s) it has been pre-trained on to generate an answer to my question. I could not find any version of question-answering with GPT2 that did not rely on specified context. Is there any way to generate an answer based on the question without giving it the answer beforehand?<br />
Or should I just use e.g. the entirety of Wikipedia as context?</p>
<p>If it helps someone, the code I currently am using:</p>
<pre><code>from transformers import AutoTokenizer, GPT2ForQuestionAnswering
import torch
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2-large&quot;)

model = GPT2ForQuestionAnswering.from_pretrained(&quot;openai-community/gpt2-large&quot;)

question, text = &quot;Who was Jim Henson?&quot;, &quot;Jim Henson was a nice puppet&quot;

inputs = tokenizer(question, text, return_tensors=&quot;pt&quot;)

with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]

# target is &quot;nice puppet&quot;
target_start_index = torch.tensor([14])
target_end_index = torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss

question_answerer = pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)
question_answerer = question_answerer(question=question, context = text)
print(question_answerer)
</code></pre>
<p>and I got</p>
<pre><code>Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at openai-community/gpt2-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. 
{'score': 0.05718426778912544, 'start': 0, 'end': 28, 'answer': 'Jim Henson was a nice puppet'}
</code></pre>
","2024-03-21 12:02:21","0","Question"
"78199621","","TypeError: causal_conv1d_fwd(): incompatible function arguments","<p>I'm having this error with <code>mamba-ssm</code> module:</p>
<pre><code>TypeError: causal_conv1d_fwd(): incompatible function arguments. The following argument types are supported:     1. (arg0: torch.Tensor, arg1: torch.Tensor, arg2: Optional[torch.Tensor], arg3: bool) -&gt; torch.Tensor  Invoked with: tensor([[[ 0.4762, -0.0323, -0.8240,  ...
</code></pre>
<p>while I run this test code to see if <code>mamba-ssm</code> works:</p>
<pre><code>import torch
from mamba_ssm import Mamba
batch, length, dim = 2, 64, 16
x = torch.randn(batch, length, dim).to(&quot;cuda&quot;)
model = Mamba(
    # This module uses roughly 3 * expand * d_model^2 parameters
    d_model=dim, # Model dimension d_model
    d_state=16,  # SSM state expansion factor
    d_conv=4,    # Local convolution width
    expand=2,    # Block expansion factor
).to(&quot;cuda&quot;)
y = model(x)
assert y.shape == x.shape
</code></pre>
<p>I've tried to follow this <a href=""https://github.com/state-spaces/mamba/issues/40#issuecomment-1849095898"" rel=""nofollow noreferrer"">guide</a> but nothing changed. Both <code>nvidia-smi</code> and <code>nvcc -V</code> works and this are their outputs:</p>
<pre><code>Thu Mar 21 12:20:23 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   52C    P0    N/A /  N/A |     11MiB /  4096MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1605      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A      2186      G   /usr/lib/xorg/Xorg                  4MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Sep_21_10:33:58_PDT_2022
Cuda compilation tools, release 11.8, V11.8.89
Build cuda_11.8.r11.8/compiler.31833905_0
</code></pre>
<p>Thank you in advance for any replies.</p>
","2024-03-21 11:34:45","-1","Question"
"78198644","78197780","","<p>You are describing something similar to a GAN optimization approach, where <code>A</code> would be the generator, and <code>B</code> the discriminator. So it's good to compare how it is done with GANs in such a framework as PyTorch. You can't separate two gradient signals with a single backward pass. You must have two backward passes.</p>
<pre><code>|&lt;------------------- L1 
|&lt;---------•••••••••• L2
x ---&gt; A ---&gt; B ---&gt; y 
</code></pre>
","2024-03-21 09:05:29","0","Answer"
"78197780","","Locally blocking gradient update for nested neural network","<p>I have two neural networks in torch that are nested and I am computing multiple losses across the output with respect to different parameters. Below is a simple case</p>
<pre><code># two neural networks
&gt;&gt;&gt; A = nn.Linear(10,10)
&gt;&gt;&gt; B = nn.Linear(10,1)

# dummy input
&gt;&gt;&gt; x = torch.rand(1,10, requires_grad=True)

# nested computation
&gt;&gt;&gt; y = B(A(x))

# evaluate two separate Loss functions on the output
&gt;&gt;&gt; Loss1 = f(y)
&gt;&gt;&gt; Loss2 = g(y)

# evaluate backprop through both losses
&gt;&gt;&gt; (Loss1+Loss2).backward()
</code></pre>
<p>I would like for Loss1 to track the gradient changes of network A and B together, but would like Loss2 to only track the changes with respect to network A. I know I can compute this by breaking the computation into two back propagation steps like</p>
<pre><code># two neural networks
&gt;&gt;&gt; A = nn.Linear(10,10)
&gt;&gt;&gt; B = nn.Linear(10,1)

# dummy input
&gt;&gt;&gt; x = torch.rand(1,10, requires_grad=True)

# nested computation
&gt;&gt;&gt; y = B(A(x))

# evaluate first loss function
&gt;&gt;&gt; Loss1 = f(y)

# evaluate backprop through first loss
&gt;&gt;&gt; Loss1.backward()

# disable gradient computation on B
&gt;&gt;&gt; B.requires_grad_(False)

# nested computation
&gt;&gt;&gt; y = B(A(x))

# evaluate second loss function
&gt;&gt;&gt; Loss2 = g(y)

# evaluate backprop through second loss
&gt;&gt;&gt; Loss2.backward()
</code></pre>
<p>I am do not like this approach as it requires multiple backpropagation computations through the nested neural networks. Is there a way to mark the second loss to not update network B? I am thinking something similar to <code>g(y).detach()</code> however this also removes the gradients with respect to network A.</p>
","2024-03-21 05:55:10","0","Question"
"78196998","","PyTorch matrix multiplication shape error: ""RuntimeError: mat1 and mat2 shapes cannot be multiplied""","<p>I'm new to PyTorch and creating a multi-output linear regression model to color words based on their letters. (This will help people with grapheme-color synesthesia have an easier time reading.) It takes in words and outputs RGB values. Each word is represented as a vector of 45 floats [0,1], where (0, 1] represents letters and 0 represents that no letter exists in that place. The output for each sample should be a vector [r-value, g-value, b-value].</p>
<p>I'm getting</p>
<blockquote>
<p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (90x1 and 45x3)</p>
</blockquote>
<p>when I try to run my model in the training loop.</p>
<p>Looking at extant Stack Overflow posts, I think this means that I need to reshape my data, but I don't know how/where to do so in a way that would solve this problem. Especially considering that I don't know where that 90x1 matrix came from.</p>
<p><strong>My Model</strong></p>
<p>I started simple; multiple layers can come after I can get a single layer to function.</p>
<pre><code>class ColorPredictor(torch.nn.Module):
    #Constructor
    def __init__(self):
        super(ColorPredictor, self).__init__()
        self.linear = torch.nn.Linear(45, 3, device= device) #length of encoded word vectors &amp; size of r,g,b vectors
        
    # Prediction
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        y_pred = self.linear(x)
        return y_pred
</code></pre>
<p><strong>How I'm loading my data</strong></p>
<pre><code># Dataset Class
class Data(Dataset):
    # Constructor
    def __init__(self, inputs, outputs):
        self.x = inputs # a list of encoded word vectors
        self.y = outputs # a Pandas dataframe of r,g,b values converted to a torch tensor
        self.len = len(inputs)
    
    # Getter
    def __getitem__(self, index):
        return self.x[index], self.y[index]
    
    # Get number of samples
    def __len__(self):
        return self.len
</code></pre>
<pre><code># create train/test split
train_size = int(0.8 * len(data))
train_data = Data(inputs[:train_size], outputs[:train_size])
test_data = Data(inputs[train_size:], outputs[train_size:])
</code></pre>
<pre><code># create DataLoaders for training and testing sets
train_loader = DataLoader(dataset = train_data, batch_size=2)
test_loader = DataLoader(dataset = test_data, batch_size=2)
</code></pre>
<p><strong>The testing loop, where the error occurs</strong></p>
<pre><code>for epoch in range(epochs):
    # Train
    model.train() #training mode
    for x,y in train_loader:
        y_pred = model(x) #ERROR HERE
        loss = criterion(y_pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
      
</code></pre>
<p><strong>Error Traceback</strong>
<a href=""https://i.sstatic.net/2Ojco.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2Ojco.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/X45n4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/X45n4.png"" alt=""enter image description here"" /></a></p>
<h1>New Attempt:</h1>
<p>Changed the 45x1 input tensor to a 2x45 input tensor, with the second column being all zeros. This works for the first run through the train_loader loop, but during the second run through the train_loader loop I get another matrix multiplication error, this time for matrices of sizes 90x2 and 45x3.</p>
","2024-03-21 01:00:23","0","Question"
"78196720","78196427","","<p>This is called a fold operation, <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#fold"" rel=""nofollow noreferrer""><code>nn.Fold</code></a> and <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.fold.html#torch.nn.functional.fold"" rel=""nofollow noreferrer""><code>F.fold</code></a> are made for this purpose. If you look at the documentation, it reads:</p>
<blockquote>
<p>Combines an array of sliding local blocks into a large containing tensor.
<br>Consider a batched input tensor containing sliding local blocks, <em>e.g.</em>, patches of images, of shape <code>(N,C×∏(kernel_size),L)</code>, where:</p>
<ul>
<li><code>N</code> is batch dimension,</li>
<li><code>C×∏(kernel_size)</code> is the number of values within a block (a block has <code>∏(kernel_size)</code> spatial locations each containing a <code>C</code>-channeled vector),</li>
<li><code>L</code> is the total number of blocks.</li>
</ul>
<p>This is exactly the same specification as the output shape of <code>Unfold</code>. This operation combines these local blocks into the large output tensor of shape <code>(N,C,output_size[0],output_size[1],…)</code> by summing the overlapping values.</p>
</blockquote>
<p>In your case, your input tensor is shaped <code>(B//s2,D2//s1,s1,s2)</code>. To get to the specs, you have <code>L = B//s2 * D2//s1</code> and kernel <code>∏(kernel_size) = s1 * s2</code>. Since the function expects <code>L</code> in last position you will need to do some permutation before flattening:</p>
<pre><code>y_ = y.permute(2,3,1,0).reshape(s1*s2,-1)
</code></pre>
<p>Now <code>y_</code> is shaped <code>(s1*s2, B//s2*D2//s1)</code>. Finally you can apply the fold:</p>
<pre><code>F.fold(y_, output_size=(D2,B), kernel_size=(s1,s2), stride=(s1,s2))
</code></pre>
","2024-03-20 23:19:46","2","Answer"
"78196427","","Given a few block matrices, get the overall large matrix","<p>In short, we are given a 4D tensor <code>y</code> of shape <code>( B // s2, D2 // s1, s1, s2)</code>, where <code>y[i,j,...]</code> represents a matrix of shape (s1,s2). These are the block matrices used to construct the overall large matrix of shape (D2, B), and there are <code>(B//s2) * (D2 //s1)</code> such block matrices in total. Here we assume all the numbers involved are integers. I am clear on how to do it using for loops:</p>
<pre><code># y shape ( B // s2, D2 // s1, s1, s2)
result = torch.zeros(D2, B)
for i in range(D2 // s1):
    for j in range(B // s2):
         result[i * s1: (i + 1) * s1, j * s2: (j + 1) * s2] = y[j,i, ...]

</code></pre>
<p>I know the assignment can be done in parallel. Can we use pytorch built-in functions to eliminate the two for loops?</p>
","2024-03-20 21:54:35","1","Question"
"78196316","","PyTorch Segementation Fault (core dumped) when moving Pytorch tensor to GPU","<p>I have a machine with RTX 6000 ADA GPUs.</p>
<p>We used to have CUDA version 11.x and I used the following image:
<code>nvcr.io/nvidia/pytorch:21.04-py3</code>
(I use PyTorch 1.x).</p>
<p>However, it seems that drivers on our machine were updated to the following -</p>
<p>Driver information from nvidia-smi command:</p>
<p><code>NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2</code></p>
<p>Now I keep getting:
<code>Segmentation fault (core dumped)</code>
Whenever I try to move a tensor to cuda device.</p>
<p>I assume it's a CUDA version issue so I tried updating the image to:
<code>nvcr.io/nvidia/pytorch:23.02-py3</code>
that supports newer CUDA version, but the issue remains the same. Here is an example program that causes the issue:</p>
<pre><code>import torch

tensor_cpu = torch.tensor([1, 2, 3, 4, 5])

print(&quot;Tensor device before moving to CUDA:&quot;, tensor_cpu.device)

if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)          
    tensor_cuda = tensor_cpu.to(device)    
    print(&quot;Tensor device after moving to CUDA:&quot;, tensor_cuda.device)
else:
    print(&quot;CUDA is not available. Cannot move tensor to CUDA.&quot;)

</code></pre>
<p>And the output is:</p>
<pre><code>Tensor device before moving to CUDA: cpu
Segmentation fault (core dumped)
</code></pre>
<p>How can I fix it? What is the correct image (with PyTorch 1.x) that I should use with my GPU?</p>
<p>Or is the issue be related to something else?</p>
","2024-03-20 21:25:44","0","Question"
"78193822","78192905","","<p>What you are noticing has actually nothing to do with <code>tqdm</code>, but rather with the inner workings of PyTorch (in particular, the <code>DataLoader</code>'s <code>num_workers</code> attribute) and Python's underlying <code>multiprocessing</code> framework. Here is a minimum working example that should reproduce your problem:</p>
<pre class=""lang-py prettyprint-override""><code>from contextlib import suppress
from multiprocessing import set_start_method
import torch
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
print(&quot;torch version:&quot;, torch.__version__)

class DummyData(Dataset):
    def __len__(self): return 256
    def __getitem__(self, i): return i

def main():
    for batch in tqdm(DataLoader(DummyData(), batch_size=16, num_workers=4)):
        pass  # Do something
    
if __name__ == &quot;__main__&quot;:
    # Enforce &quot;spawn&quot; method (e.g. on Linux) for subprocess creation to
    # reproduce problem (suppress error for reruns in same interpreter)
    with suppress(RuntimeError): set_start_method(&quot;spawn&quot;)
    main()
</code></pre>
<p>If you run this piece of code, you should see your PyTorch version number be printed exactly 4 times, messing up your <code>tqdm</code> progress bar. It is not a coincidence that this number is the same as <code>num_workers</code> (which you can easily check by changing this number).</p>
<p>What happens is the following:</p>
<ul>
<li>If <code>num_workers</code> is &gt; 0, then subprocesses are launched for the workers.</li>
<li>On Windows and macOS, these subprocesses are launched with the &quot;spawn&quot; method by default (on Linux, one can enforce this method to reproduce your observation, which I have done with <code>set_start_method()</code>).</li>
<li>The &quot;spawn&quot; method will launch your main script once for each subprocess, executing all the lines that are not guarded by an <code>if __name__ == &quot;__main__&quot;:</code> block. This includes your <code>print()</code> calls on top of the script.</li>
</ul>
<p>The behavior is documented <a href=""https://pytorch.org/docs/stable/data.html#platform-specific-behaviors"" rel=""nofollow noreferrer"">here</a>, along with potential mitigations. The one that would work for you, I guess, is:</p>
<blockquote>
<p>Wrap most of you main script’s code within <code>if __name__ == '__main__':</code> block, to make sure it doesn’t run again</p>
</blockquote>
<p>So, either</p>
<ol>
<li>move the <code>print()</code> calls to the beginning of your <code>if __name__ == '__main__':</code> block,</li>
<li>move the <code>print()</code> calls to the beginning of your <code>main()</code> function, or</li>
<li>remove the <code>print()</code> calls.</li>
</ol>
<p>Alternatively, but this is probably not what you want, you can set <code>num_workers=0</code>, which will disable the underlying use of <code>multiprocessing</code> altogether (but in this way you will also lose the benefits of parallelization). Note that you should probably also move other function calls (such as <code>load_data()</code>) into the <code>if __name__ == '__main__':</code> block or into the <code>main()</code> function to avoid multiple unintended executions.</p>
","2024-03-20 13:42:55","1","Answer"
"78192905","","Unexpected printouts interfere with tqdm progress bar in PyTorch training run","<p>I am trying to understand how the progress bar using <code>tqdm</code> works exactly. I have some code that looks as follows:</p>
<pre><code>import torch
import torchvision
print(f&quot;torch version: {torch.__version__}&quot;)
print(f&quot;torchvision version: {torchvision.__version__}&quot;)

load_data()
manual_transforms = transforms.Compose([])
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders()

# them within the main function I have placed the train function that exists in the `engine.py` file
def main():

      results = engine.train(model=model,
        train_dataloader=train_dataloader,
        test_dataloader=test_dataloader,
        optimizer=optimizer,
        loss_fn=loss_fn,
        epochs=5,
        device=device)
</code></pre>
<p>and the <code>engine.train()</code> function includes the following code <code>for epoch in tqdm(range(epochs)):</code> then, the training for each batch takes place to visualize the progress of the training. Each time the tqdm runs for each step it prints also the following statements:</p>
<pre><code>print(f&quot;torch version: {torch.__version__}&quot;)
print(f&quot;torchvision version: {torchvision.__version__}&quot;)
</code></pre>
<p>So finally, my question is why this is happening. How does the main function have access to these global statements and how can avoid printing everything in each loop?</p>
","2024-03-20 11:27:57","1","Question"
"78192809","78185340","","<p>Have you checked the input shape of <strong>detectron2_model.onnx</strong>?</p>
<p>You can try and adjust this script to find the shape of the .onnx model and <em><strong>match your input against it</strong></em>,</p>
<pre><code>import onnx
onnx_model = onnx.load(&quot;detectron2_model.onnx&quot;)
for i in onnx_model.graph.input:
    print(i.name)
i_shape = onnx_model.graph.input[0].type.tensor_type.shape.dim
print([d.dim_value for d in i_shape])
</code></pre>
<p><strong>References:</strong></p>
<ol>
<li><a href=""https://onnx.ai/onnx/intro/python.html"" rel=""nofollow noreferrer"">https://onnx.ai/onnx/intro/python.html</a></li>
<li><a href=""https://stackoverflow.com/questions/56734576/find-input-shape-from-onnx-file"">Find input shape from onnx file</a></li>
</ol>
","2024-03-20 11:10:35","0","Answer"
"78192348","78192110","","<p>I figure it out. I converted the tensors to float32 and now it makes sense.</p>
<pre><code>NUMPY CPU--- 1.626929759979248 seconds ---, float64
PYTORCH CPU--- 1.3480534553527832 seconds ---, torch.float64
PYTORCH GPU--- 4.0684168338775635 seconds ---, torch.float64

NUMPY CPU--- 2.4209649562835693 seconds ---, float32
PYTORCH CPU--- 0.6886072158813477 seconds ---, torch.float32
PYTORCH GPU--- 0.2805318832397461 seconds ---, torch.float32
</code></pre>
","2024-03-20 09:59:00","0","Answer"
"78192110","","pytorch gpu slower performance in linear algebra","<p>I am making a silly benchmark between numpy and pytorch (cpu + gpu). I can't seem to understand with the GPU is so much slower.
To avoid the overhead between moving arrays back and froth from cpu to gpu, the time command is only in the linalg part.
Any tips?</p>
<pre><code>import torch
import numpy as np
import time
import os

os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;

print(f&quot;Is CUDA supported by this system?  {torch.cuda.is_available()}&quot;)
print(f&quot;CUDA version: {torch.version.cuda}&quot;)

# Storing ID of current CUDA device
cuda_id = torch.cuda.current_device()
print(f&quot;ID of current CUDA device: {torch.cuda.current_device()}&quot;)
   
print(f&quot;Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}&quot;)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)


size=10000
real=1
A=np.random.rand(size,size)
b=np.random.rand(size,1)
start_time = time.time()
for t in range(real):
    x_np=np.linalg.solve(A,b)
print(&quot;NUMPY CPU--- %s seconds ---&quot; % (time.time() - start_time))

A = torch.from_numpy(A)
b = torch.from_numpy(b)
start_time = time.time()
for t in range(real):
    x_torch = torch.linalg.solve(A, b)
print(&quot;PYTORCH CPU--- %s seconds ---&quot; % (time.time() - start_time))

A = A.to(device)
b = b.to(device)
start_time = time.time()
for t in range(real):
    x_torch = torch.linalg.solve(A, b)
print(&quot;PYTORCH GPU--- %s seconds ---&quot; % (time.time() - start_time))
</code></pre>
<p>The results are the following (I only do one iteration).</p>
<pre><code>Is CUDA supported by this system?  True
CUDA version: 12.1
ID of current CUDA device: 0
Name of current CUDA device: NVIDIA GeForce RTX 3060
NUMPY CPU--- 1.6754064559936523 seconds ---
PYTORCH CPU--- 1.3463587760925293 seconds ---
PYTORCH GPU--- 3.8940138816833496 seconds ---
</code></pre>
","2024-03-20 09:20:00","0","Question"
"78190838","78190591","","<p>You could identify the rows for which the value in 0 is different from the previous in 1 and discard the other:</p>
<pre><code>mask = matrix[1:,0]!=matrix[:-1,1]
# tensor([False,  True])

true = torch.tensor([True])

out = torch.column_stack([matrix[torch.cat((true, mask)), 0],
                          matrix[torch.cat((mask, true)), 1],
                         ])
</code></pre>
<p>Variant:</p>
<pre><code>mask = torch.cat((torch.tensor([True]),
                  matrix[1:,0]!=matrix[:-1,1]))
# tensor([ True, False,  True])

out = torch.column_stack([matrix[mask, 0],
                          matrix[mask.roll(-1), 1],
                         ])
</code></pre>
<p>Output:</p>
<pre><code>tensor([[1, 3],
        [4, 5]])
</code></pre>
<p>More complex example:</p>
<pre><code># input             # values to keep (X)
tensor([[1, 2],     #    X   -
        [2, 3],     #    -   -
        [3, 4],     #    -   X
        [0, 0],     #    X   X
        [5, 6],     #    X   -
        [6, 9]])    #    -   X

# output
tensor([[1, 4],
        [0, 0],
        [5, 9]])
</code></pre>
","2024-03-20 04:19:41","2","Answer"
"78190622","78190525","","<p>I think you're misunderstanding how <code>vector_to_parameters</code> works. Look at the source code:</p>
<pre class=""lang-py prettyprint-override""><code>def vector_to_parameters(vec: torch.Tensor, parameters: Iterable[torch.Tensor]) -&gt; None:
    r&quot;&quot;&quot;Convert one vector to the parameters

    Args:
        vec (Tensor): a single vector represents the parameters of a model.
        parameters (Iterable[Tensor]): an iterator of Tensors that are the
            parameters of a model.
    &quot;&quot;&quot;
    # Ensure vec of type Tensor
    if not isinstance(vec, torch.Tensor):
        raise TypeError('expected torch.Tensor, but got: {}'
                        .format(torch.typename(vec)))
    # Flag for the device where the parameter is located
    param_device = None

    # Pointer for slicing the vector for each parameter
    pointer = 0
    for param in parameters:
        # Ensure the parameters are located in the same device
        param_device = _check_param_device(param, param_device)

        # The length of the parameter
        num_param = param.numel()
        # Slice the vector, reshape it, and replace the old data of the parameter
        param.data = vec[pointer:pointer + num_param].view_as(param).data

        # Increment the pointer
        pointer += num_param
</code></pre>
<p><code>vector_to_parameters</code> assigns the values in <code>vec</code> to the parameters in <code>parameters</code>. This updates the <code>data</code> values in <code>parameters</code>, but there is no autograd link between <code>vec</code> and the updated parameters.</p>
<p>When you do a forward pass, you use the parameters in the model state dict and the gradients flow back to those parameters.</p>
<p>For example, when you run the following code, you get gradients populated at <code>model.linear.weight.grad</code>, because that is a leaf parameter of the computation. <code>vparams</code> has no involvement in the computation, so there's no gradient chain back to <code>vparams</code>.</p>
<pre class=""lang-py prettyprint-override""><code>vector_to_parameters(vparams, model.parameters())

input_data = torch.randn(1, 10)
output = model(input_data)
target = torch.randn(1, 1)

loss = loss_function(output, target)
loss.backward()

print(model.linear.weight.grad)
</code></pre>
","2024-03-20 02:42:42","0","Answer"
"78190615","78190580","","<p>You can try torch.randint function. for sample data with a range 0 to n with size m for example.</p>
<pre><code>sample = torch.randint(low=0, high=n, size(m,))
</code></pre>
","2024-03-20 02:40:17","0","Answer"
"78190591","","How to merge rows of pytorch matrix with same element?","<p>For example, N*2 input <code>matrix = torch.tensor([[1, 2], [2, 3],[4, 5]])</code> ,
the expected output matrix is <code>torch.tensor([[1, 3],[4, 5]])</code>, since the first and second rows share the same element 2.</p>
<p>How can this be achieved? Thanks in advance.</p>
","2024-03-20 02:30:10","1","Question"
"78190580","","Is there any method in Pytorch as the ""numpy.random.choice"" function in NumPy?","<p>Is there any method in Pytorch as the same as the &quot;numpy.random.choice&quot; function in NumPy?</p>
","2024-03-20 02:26:17","0","Question"
"78190525","","Reparameterizing a model in PyTorch","<p>I am trying to optimize the parameters of a simple model which is implemented using the PyTorch library. For the purpose of optimization, I would like to use a different representation of the parameters than that which is specified by the model class. I would like, in particular, to represent my parameters as a single vector (rather than two vectors as in this example).</p>
<p>I can convert from <code>model.parameters()</code> (which is an <code>Iterable</code>) to the desired vector representation using <code>parameters_to_vector</code> from <code>torch.nn.utils.convert_parameters</code>. However, when I try to label this vector as a leaf (with <code>detach</code> and <code>requires_grad_</code>), and use it to populate the original parameters of the model object with <code>vector_to_parameters</code>, then it appears that the computation graph is not made aware of what is happening.</p>
<pre><code>#!/usr/bin/python3
import torch
import torch.nn as nn
from torch.nn.utils.convert_parameters import *

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(10, 1)

    def forward(self, x):
        return self.linear(x)

model = SimpleModel()

for name, param in model.named_parameters():
    print(name, param.size())
# this prints:
## linear.weight torch.Size([1, 10])
## linear.bias torch.Size([1])

loss_function = nn.MSELoss()

vparams = parameters_to_vector(model.parameters()).detach().clone().requires_grad_(True)

# populate model.parameters() from vparams
vector_to_parameters(vparams, model.parameters())

input_data = torch.randn(1, 10)
output = model(input_data)
target = torch.randn(1, 1)

loss = loss_function(output, target)
# loss.backward()  ## if we do this, then vparams.grad is None

## this one works, but we wanted to use vparams:
# vgrads = torch.autograd.grad(loss, model.linear.weight)[0]   

## this gives an error:
vgrads = torch.autograd.grad(loss, vparams)[0]
## &quot;One of the differentiated Tensors appears to not have been used in the graph.&quot;
</code></pre>
<p>I also tried performing the vector slices manually, but this doesn't fix the error. E.g.:</p>
<pre><code>model.linear.weight.data.copy_(vparams[0:10].view_as(model.linear.weight.data))
</code></pre>
<p>or</p>
<pre><code>model.linear.weight = nn.Parameter(vparams[0:10].view_as(model.linear.weight.data))
</code></pre>
<p>I'm somewhat new to PyTorch, but I've read that it is possible with PyTorch to compute gradients through slices, so it seems that what I'm attempting should be possible.</p>
<p>Am I missing something about the <code>torch.nn.Parameter</code> class which is used by PyTorch models? Are members of this class required to be &quot;leaves&quot; in the computation graph? Here is a smaller example that omits the <code>nn.Module</code> subclass and just tries to create a <code>nn.Parameter</code> object from a &quot;slice&quot;:</p>
<pre><code>import torch
import torch.nn as nn

a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

#b = nn.Parameter(a[1]) # &quot;RuntimeError: One of the differentiated Tensors appears to not have been used in the graph.&quot;
#b = torch.Tensor(a[1]) # &quot;IndexError: slice() cannot be applied to a 0-dim tensor.&quot;
b = a[1] # works

g = torch.autograd.grad(b, a)[0]
</code></pre>
<p>From the error message, it seems that PyTorch is not able to differentiate through an <code>nn.Parameter</code> initialization. Is there a way to fix this?</p>
","2024-03-20 02:12:02","0","Question"
"78190520","78186288","","<p>You need to explicitly move the model and the model inputs to the GPU.</p>
<p>You can run <code>nvidia-smi</code> to verify things are running on the GPU.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BartTokenizer, BartForConditionalGeneration

model_name = 'facebook/bart-base'
device = 'cuda'

def load_model():
    # load the pretrained bart model
    model = BartForConditionalGeneration.from_pretrained(model_name)
    model.to(device)
    # load the tokenizer
    tokenizer = BartTokenizer.from_pretrained(model_name)
    return model, tokenizer

def calculate_text_embeddings(text, model, tokenizer):
    # tokenize the text
    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=512, truncation=True)
    # generate the embedding
    with torch.no_grad():
        outputs = model.encoder(input_ids=inputs.input_ids.to(device), attention_mask=inputs.attention_mask.to(device))
    # get the last hidden state
    last_hidden_states = outputs.encoder_last_hidden_state
    
    return last_hidden_states.cpu()
</code></pre>
","2024-03-20 02:10:41","0","Answer"
"78190372","78189428","","<p>My two cents is trying to share the model across multiple processes which are all trying to use the same GPU will be a massive headache with minimal performance improvements.</p>
<p>You want a setup where each model you want to deploy has a container image for just that model that loads the container-specific model on startup. Each model will have a separate inference queue. ie if you want to deploy <code>Model_A</code> and <code>Model_B</code>, you have <code>Image_A</code> running <code>Model_A</code> consuming from <code>Queue_A</code> and <code>Image_B</code> running <code>Model_B</code> consuming from <code>Queue_B</code>.</p>
<p>Trying to run multiple models from the same queue/container is easier at first but becomes a nightmare once you start to scale.</p>
<p>Regardless of your inference framework, there's always going to be some cold start delay getting the container going, but you should only need to load a model once for the lifetime of the inference container.</p>
<p>Since you're using GPU inference, you want to make sure your inference containers are doing batch inference whenever possible.</p>
<p>You might also find <a href=""https://www.auroria.io/running-pytorch-models-for-inference-using-fastapi-rabbitmq-redis-docker/"" rel=""nofollow noreferrer"">this article</a> useful.</p>
","2024-03-20 01:12:21","1","Answer"
"78189648","78024121","","<p>I'm not a Python expert...I've learned and forgotten Python twice...but I guess I'm on to round 3.</p>
<p>After removing existing pytorch installations with <code>pip uninstall torch torchvision torchaudio</code> and <code>pip cache purge</code>, and learning that CUDA 12.1 is the only version of CUDA 12 supported by looking <a href=""https://download.pytorch.org/whl/"" rel=""nofollow noreferrer"">here</a>, I ran <code>pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121</code> and finally CUDA worked with pytorch.</p>
","2024-03-19 21:07:19","3","Answer"
"78189462","78189428","","<p>Yes, there are ways to share a PyTorch model across multiple processes without creating copies.</p>
<p><strong>torch.multiprocessing</strong> and <strong>model.share_memory_()</strong>:</p>
<p>This method utilizes the <strong>torch.multiprocessing</strong> module from PyTorch.
You can call <strong>model.share_memory_()</strong> on your model to allocate shared
memory for its parameters. This allows all processes to access the
same model parameters in shared memory, avoiding redundant copies.
This approach is efficient for training a model in parallel across
multiple CPU cores.</p>
<p>Some resources for further exploration: <a href=""https://www.geeksforgeeks.org/getting-started-with-pytorch/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/getting-started-with-pytorch/</a></p>
","2024-03-19 20:25:12","1","Answer"
"78189428","","Is there a way to share a PyTorch model across multiple processes without using multiple copies?","<p>I have a custom PyTorch model that bottlenecks my application due to how it is currently used.</p>
<p>The application is a web server built in Flask that receives job submissions for the PyTorch model to process. Due to the processing time of each job, I use Celery to handle the computation, where Flask queues the tasks for Celery to execute.</p>
<p>Each job consists of loading the PyTorch model from the disk, moving the model and data to a GPU, and making a prediction on the data submitted. However, loading the model takes around 6 seconds. In many instances, that is a magnitude or two larger than prediction time.</p>
<p>Thus, is it possible to load the model and move it to a GPU on server startup (specifically when the Celery worker starts), avoiding the time needed to load the model and copy it to the GPU every job? Ideally, I'd want to load the model and copy it to every available GPU on server startup, leaving each Celery job to choose an available GPU and copy the data over. Currently, I only have one GPU, so a multi-GPU solution is not a requirement at the moment, but I'm planning ahead.</p>
<p>Further, the memory constraints of the model and data allow for only one job per GPU at a time, so I have a single Celery worker that processes jobs sequentially. This could reduce the complexity of the solution due to avoiding multiple jobs attempting to use the model in shared memory at the same time, so I figured I'd mention it.</p>
<p>As of this moment, I am using PyTorch's multiprocessing package with the <code>forkserver</code> start method, but I've had trouble determining exactly how this works and if it behaves in the way I prefer. If you have any input on my configuration or suggestion for a solution, please leave a comment! I'm open to efficiency suggestions, as I intend to scale this solution. Thank you!</p>
","2024-03-19 20:19:54","1","Question"
"78189220","78187211","","<p>I don't see a hidden state issue. You're using a weird setup which is probably hindering learning.</p>
<p>First we want a proper dataset/dataloader</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch.nn.functional as F

class RandDataset(Dataset):
    def __init__(self, sequence_length):
        self.sequence_length = sequence_length
    
    def __len__(self):
        return 10000 # data is generated so length is arbitrary 
    
    def __getitem__(self, idx):
        sequence = torch.rand(self.sequence_length)
        labels = torch.ones_like(sequence)
        labels[1:] = sequence[:-1] &lt; sequence[1:]
        
        sequence = sequence[None,:,None] # shape (1, sequence_length, 1)
        labels = labels[None,:] # shape (1, sequence_length)
        
        return sequence, labels

def collate_fn(batch):
    sequences = torch.cat([i[0] for i in batch])
    labels = torch.cat([i[1] for i in batch])
    return sequences, labels

dataset = RandDataset(1000)
dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
</code></pre>
<p>Now the model. Add an input projection layer, remove the unnecessary relu after the lstm module.</p>
<pre class=""lang-py prettyprint-override""><code>class LSTMModel(nn.Module):
    def __init__(self, d_in, d_proj, d_hidden, n_layers):
        super().__init__()
        
        self.input_layer = nn.Linear(d_in, d_proj)
        self.lstm = nn.LSTM(input_size=d_proj, hidden_size=d_hidden, num_layers=n_layers, batch_first=True)
        self.output_layer = nn.Linear(d_hidden, 1)
        
    def forward(self, x):
        x = self.input_layer(x)
        x = F.relu(x)
        
        x, _ = self.lstm(x)
        
        x = self.output_layer(x)
        return x

model = LSTMModel(1, 32, 64, 2)
</code></pre>
<p>Now we set up training. We're predicting a binary output, so we want to use <code>BCEWithLogitsLoss</code> rather than <code>MSELoss</code>. Using MSE for categorical prediction doesn't make sense.</p>
<pre class=""lang-py prettyprint-override""><code>device = 'cuda'

opt = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_function = nn.BCEWithLogitsLoss()

epochs = 1

model.to(device);

for epoch in range(epochs):
    for i, batch in enumerate(dataloader):
        seqs, labs = batch
        
        seqs = seqs.to(device)
        labs = labs.to(device)
        
        preds = model(seqs)
        loss = loss_function(preds.reshape(-1), labs.reshape(-1))
        
        if i%10==0:
            print(f'{loss.item():.3f}')
        
        opt.zero_grad()
        loss.backward()
        opt.step()
</code></pre>
<p>After training, test performance</p>
<pre class=""lang-py prettyprint-override""><code>model.eval()
seq, lab = dataset[0]

pred = model(seq.to(device)).cpu()

pred = (torch.sigmoid(pred)&gt;0.5).float().squeeze()
lab = lab.squeeze()

acc = (pred == lab).float().mean()
print(acc)
&gt; tensor(0.9990)
</code></pre>
","2024-03-19 19:36:00","1","Answer"
"78187477","78186908","","<p>Assuming that <code>x_i</code> has the same shape as <code>x_aug</code>, and <code>mask_i</code> has the same shape as <code>mask_aug</code>. No broadcasting is required for those. However, you said <code>ws[:, i]</code> was 1D of shape <code>(320,)</code>. That means you need to unsqueeze two dimensions, no more.</p>
<p>A double indexing with <code>None</code> should work:</p>
<pre><code>x_aug += ws[:, i][:,None,None] * x_i
mask_aug += ws[:, i][:,None,None] * mask_i
</code></pre>
","2024-03-19 14:37:06","0","Answer"
"78187274","78187218","","<p>I think you have a problem in your imports and are currently importing Dataset from another package. This should resolve your error :</p>
<pre><code>from torch.utils.data import Dataset
</code></pre>
","2024-03-19 14:06:52","0","Answer"
"78187218","","Converting Pandas DataFrame structure into Pytorch Dataset","<p>Have a question regarding Pytorch framework that I started to use recently (while always used keras/tf in the past).</p>
<p>So I would like to convert simple pandas <code>DataFrame</code> to the pytorch <code>Dataset</code>.</p>
<pre><code>import pandas as pd

df = pd.DataFrame(np.array([[1, 2], [4, 5], [7, 8]]), columns=['A', 'B'])
</code></pre>
<p>For applying desirable transformation, I create custom Dataset class and inherit from the pytorch Dataset in the following way:</p>
<pre><code>from datasets import Dataset

class CustomDataset(Dataset):
    def __init__(self, src_file):
        df = pd.read_csv(src_file)
        self.A = df['A']
        self.B = df['B']

    def __len__(self):
        return len(self.A)

    def __getitem__(self, idx):
        A_py = self.A.iloc[idx]
        B_py = self.B.iloc[idx] 
        return A_py, B_py 
</code></pre>
<p>When I try to execute the code doing:</p>
<pre><code>data = CustomDataset('src_file')
data
</code></pre>
<p>receiving an error of the following kind:</p>
<pre><code>AttributeError: 'CustomDataset' object has no attribute '_info'
</code></pre>
<p>What is wrong here and how should I change my approach?</p>
","2024-03-19 14:00:13","0","Question"
"78187211","","PyTorch LSTM not using hidden layer","<p>I am using PyTorch's LSTM api, but have a bit of an issue. I'm using an LSTM for a dummy AI model. The task of the model is to return 1 if the previous number is less than the current one.</p>
<p>So for an array like <code>[0.7, 0.3, 0.9, 0.99]</code>, the expected outputs are <code>[1.0, 0.0, 1.0, 1.0]</code>. The first output should be <code>1.0</code> no matter what.</p>
<p>I designed the following network to try this problem:</p>
<pre class=""lang-py prettyprint-override""><code># network.py

import torch

N_INPUT = 1
N_STACKS = 1
N_HIDDEN = 3

LR = 0.001


class Network(torch.nn.Module):

    # params: self
    def __init__(self):
        super(Network, self).__init__()

        self.lstm = torch.nn.LSTM(
            input_size=N_INPUT,
            hidden_size=N_HIDDEN,
            num_layers=N_STACKS,
        )

        self.linear = torch.nn.Linear(N_HIDDEN, 1)
        self.relu = torch.nn.ReLU()

        self.optim = torch.optim.Adam(self.parameters(), lr=LR)
        self.loss = torch.nn.MSELoss()

    # params: self, predicted, expecteds
    def backprop(self, xs, es):

        # perform backprop
        self.optim.zero_grad()
        l = self.loss(xs, torch.tensor(es))
        l.backward()
        self.optim.step()

        return l

    # params: self, data (as a python array)
    def forward(self, dat):

        out, _ = self.lstm(torch.tensor(dat))

        out = self.relu(out)
        out = self.linear(out)

        return out
</code></pre>
<p>And I am calling this from this file:</p>
<pre class=""lang-py prettyprint-override""><code># main.py

import network

import numpy as np

# create a new network
n: network.Network = network.Network()


# create some data
def rand_array():

    # a bunch of random numbers
    a = [[np.random.uniform(0, 1)] for i in range(1000)]

    # now, our expected value is 0 if the previous number is greater, and 1 else
    expected = [0.0 if a[i - 1][0] &gt; a[i][0] else 1.0 for i in range(len(a))]
    expected[0] = 1.0  # make the first element always just 1.0

    return [a, expected]


# a bunch of random arrays
data = [rand_array() for i in range(1000)]

# 100 epochs
for i in range(100):
    for i in data:

        pred = n(i[0])
        loss = n.backprop(pred, i[1])
        print(&quot;Loss: {:.5f}&quot;.format(loss))
</code></pre>
<p>Now, when I run this program, I'm just getting a loss around <code>0.25</code>, and it isn't really changing once it gets there. I think the model is just picking the average value of <code>0</code> and <code>1</code> (<code>0.5</code>) for each input.</p>
<p>This leads me to the belief that the model can't see the previous data; the data is just random numbers (the expected output is based on these random numbers, though), and the model can't remember what happened before.</p>
<p>What is my issue?</p>
","2024-03-19 13:59:08","0","Question"
"78186908","","RuntimeError: output with shape [320, 320, 3] doesn't match the broadcast shape [320, 320, 320, 320, 3]","<p>I am trying to implement an augmentation function to my images and masks, I have defined the augmentations like below:</p>
<pre><code>if config.AUG == &quot;PRIMEAugmentation&quot;:
    augmentations = [autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y, translate_x, translate_y]
</code></pre>
<p>and the function is like below:</p>
<pre><code>import torch
from torch.distributions import Dirichlet, Beta

class PRIMEAugmentation:
    def __init__(self, mixture_width=3, mixture_depth=-1):
        self.mixture_width = mixture_width
        self.mixture_depth = mixture_depth

    def __call__(self, x, mask):
        x = torch.from_numpy(x)
        mask = torch.from_numpy(mask)
        ws = Dirichlet(torch.ones(self.mixture_width)).sample((x.shape[0],))
        m = Beta(torch.ones(1), torch.ones(1)).sample().expand(x.shape[0], 1, 1, 1)

        x_aug = torch.zeros_like(x)
        mask_aug = torch.zeros_like(mask)
        for i in range(self.mixture_width):
            x_i = x.clone()
            mask_i = mask.clone()
            for d in range(self.mixture_depth):
                op = torch.randint(len(self.augmentations), size=(x.shape[0],)).tolist()
                x_i, mask_i = self.augmentations[op](x_i, mask_i)
            x_aug += ws[:, i].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(1) * x_i.unsqueeze(1)
            mask_aug += ws[:, i].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(1) * mask_i.unsqueeze(1)

        mixed = (1 - m) * x + m * x_aug.sum(dim=1)
        mixed_mask = (1 - m) * mask + m * mask_aug.sum(dim=1)
        return mixed.numpy(), mixed_mask.numpy()
</code></pre>
<p>and I have called it like the following way:</p>
<pre><code>augmenter_PRIMEAugmentation = aug_lib_new.PRIMEAugmentation()

import os

def image_mask_transformation(image,mask,img_trans,aug_trans=False):
    transformed = img_trans(image=image, mask=mask)
    image = transformed[&quot;image&quot;]
    mask = transformed[&quot;mask&quot;]

    if aug_trans in augmenter_list:
        image,mask = eval('augmenter_'+aug_trans)(image, mask)
</code></pre>
<p>but I am getting an error:</p>
<pre><code>     x_aug += ws[:, i].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(1) *
 x_i.unsqueeze(1) RuntimeError: output with shape [320, 320, 3] doesn't
 match the broadcast shape [320, 320, 320, 320, 3]
</code></pre>
","2024-03-19 13:14:50","0","Question"
"78186288","","Does Pytorch automaticly use the GPU","<p>I'm new to using torch and calculate on huge amount of data. I want to create embeddings for an large text corpus and write my embedding function, it works well but it seems pretty slow, so I'm not sure if it really uses the GPU or does still uses the CPU.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BartTokenizer, BartForConditionalGeneration

model_name = 'facebook/bart-base'

def load_model():
    # load the pretrained bart model
    model = BartForConditionalGeneration.from_pretrained(model_name)
    # load the tokenizer
    tokenizer = BartTokenizer.from_pretrained(model_name)
    return model, tokenizer

def calculate_text_embeddings(text, model, tokenizer):
    # tokenize the text
    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=512, truncation=True)
    # generate the embedding
    with torch.no_grad():
        outputs = model.encoder(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)
    # get the last hidden state
    last_hidden_states = outputs.encoder_last_hidden_state
    
    return last_hidden_states

    return embeddings
</code></pre>
<p><code>torch.cuda.is_available()</code> returns true</p>
<p>I want to calculate the embeddings on the GPU using CUDA.</p>
","2024-03-19 11:37:54","0","Question"
"78186057","78185614","","<p>tqdm is both the name of the library and the name of the main class contained inside. You should simply change at the start of your file</p>
<pre><code>import tqdm
</code></pre>
<p>to:</p>
<pre><code>from tqdm import tqdm
</code></pre>
<p>This can be confusing for beginners but is often the case in Python for packages with one main class.</p>
<p>You should always try to reduce your buggy code to a minimal reproducible example before posting.</p>
","2024-03-19 11:00:33","0","Answer"
"78185694","78185619","","<pre><code>for x in shared_model_parameters:
        x.to(device='cuda')
</code></pre>
<p>Does not modify x in-place; instead, it returns a new tensor that has been moved to the specified device. You need to assign the value to a variable and pass that variable to the next step:</p>
<pre><code>shared_model_parameters_cuda = [x.to(device='cuda') for x in shared_model_parameters]

w_diff = torch.tensor(0., device=self.device)
for w, w_t in zip(self.model.parameters(), shared_model_parameters_cuda):
    w_diff += torch.pow(torch.norm(w - w_t), 2)
</code></pre>
","2024-03-19 10:04:44","0","Answer"
"78185619","","Why these two loss function are not the same?","<p>This is a loss function of a personalized federated learning framework. When lambda equals 0, all clients train locally. Otherwise they adjust their parameters based on the similarity to the global model.</p>
<p>Version 1, server returns the global_model to every client:</p>
<pre><code>    def mtl_loss_fn(self, logits, labels, shared_model):
        sample_loss_fn = torch.nn.CrossEntropyLoss()
        mean_batch_term = sample_loss_fn(logits, labels)

        shared_model = shared_model.to(device='cuda')

        w_diff = torch.tensor(0., device=self.device)
        for w, w_t in zip(self.model.parameters(), shared_model.parameters()):
            w_diff += torch.pow(torch.norm(w - w_t), 2)

        prox_term = 0.5 * self.lam * w_diff
        # print(f&quot;mean batch term: {mean_batch_term}, prox_term: {prox_term}&quot;)
        return mean_batch_term + prox_term
</code></pre>
<p>Version 2, server returns the parameters of the global_model to every client:</p>
<pre><code>    def mtl_loss_fn(self, logits, labels, shared_model_parameters):
        sample_loss_fn = torch.nn.CrossEntropyLoss()
        mean_batch_term = sample_loss_fn(logits, labels)

        for x in shared_model_parameters:
            x.to(device='cuda')

        w_diff = torch.tensor(0., device=self.device)
        for w, w_t in zip(self.model.parameters(), shared_model_parameters):
            w_diff += torch.pow(torch.norm(w - w_t), 2)

        prox_term = 0.5 * self.lam * w_diff
        # print(f&quot;mean batch term: {mean_batch_term}, prox_term: {prox_term}&quot;)
        return mean_batch_term + prox_term
</code></pre>
<p>Version 1 works well, but version 2 turns into completely local training no matter what lambda is.</p>
<p>I don't know why, they look the same to me..</p>
","2024-03-19 09:54:34","0","Question"
"78185614","","TypeError while unpacking train data loader in pytorch","<p>I'm trying to train a CNN model on an image dataset that has an extension <code>.npy</code>, but I get a TypeError in the train loop.</p>
<h3>Imports</h3>
<pre class=""lang-py prettyprint-override""><code>import torch
from torchvision import datasets, transforms
import torch.nn as nn
import torch.optim as optim
import tqdm
</code></pre>
<h3>CNN model</h3>
<pre class=""lang-py prettyprint-override""><code>class CNN(nn.Module):
    def __init__(self) -&gt; None:
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)

        self.fc1 = nn.Linear(64*64*64, 128)
        self.fc2 = nn.Linear(128, 3)

        self.relu = nn.ReLU()

        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(p=0.2)
    
    def forward(self, image: torch.Tensor) -&gt; torch.Tensor:
        image = self.pool(self.relu(self.conv1(image)))
        image = self.pool(self.relu(self.conv2(image)))
        image = self.pool(self.relu(self.conv3(image)))
        image = image.view(-1, 64*64*64)
        image = self.dropout(self.relu(self.fc1(image)))
        image = self.fc2(image)
        return image
</code></pre>
<h3>Hyperparameters</h3>
<pre class=""lang-py prettyprint-override""><code>train_path = 'dataset/train'
val_path = 'dataset/val'
num_epochs = 11
num_classes = 3
batch_size = 64
learning_rate = 0.002
train_test_ratio = 0.9

device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
</code></pre>
<h3>Loading train data</h3>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

def npy_loader(path):
    sample = torch.from_numpy(np.load(path))
    return sample

transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])
dataset = datasets.DatasetFolder(
    root=train_path,
    loader=npy_loader,
    extensions='.npy',
    transform=transform
)

trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
</code></pre>
<h3>Training</h3>
<pre class=""lang-py prettyprint-override""><code>model = CNN()
model.to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()
model.train()
for epoch in range(num_epochs):
    train_loss = 0.0
    for data, target in tqdm(trainloader):
        data = data.to(device)
        target = target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * data.size(0)
    train_loss = train_loss/len(trainloader.dataset)
    print(f'Epoch: {epoch+1} \tTraining Loss: {train_loss:.6f}')
</code></pre>
<h3>Error</h3>
<p><code>TypeError: 'module' object is not callable</code> in line <code>for data, target in tqdm(trainloader):</code></p>
<p>I tried coding <code>MyDataset</code> a custom Dataset class, but that did not solve my error.</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self, np_file_paths, transform=None):
        self.files = np_file_paths
        self.transform = transform
    
    def __getitem__(self, index):
        x = np.load(self.files[index])
        x = torch.from_numpy(x).float()
        if self.transform is not None:
            image = self.transform(image)
        return x
    
    def __len__(self):
        return len(self.files)
</code></pre>
","2024-03-19 09:53:36","0","Question"
"78185428","78184358","","<p>According to this <a href=""https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with/61034368#61034368"">SO thread</a> and this <a href=""https://discuss.pytorch.org/t/pytorch-finds-cuda-despite-nvcc-not-found/166754"" rel=""nofollow noreferrer"">forum thread</a>, you are not required to have <code>nvcc</code> installed on your local machine since PyTorch is shipped with its own CUDA library. The only requirement is that you have a CUDA driver installed on your device and it supports the CUDA version you installed via Pytorch.</p>
<p>I would assume that <code>conda install cudatoolkit</code> installs a standalone CUDA toolkit, but is independent of PyTorch. Following the <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">installation page</a>, you should instead use <code>conda install pytorch::pytorch-cuda</code>. That way you install PyTorch with CUDA support.</p>
","2024-03-19 09:20:27","1","Answer"
"78185340","","finding the input size for detectron2 model to convert to onnx","<p>I am trying to convert this detectron 2 model to onnx format but facing issues with the dummy input shape. here is my code:</p>
<pre><code>import torch
import cv2
from detectron2.config import get_cfg
from detectron2.modeling import build_model
from detectron2.checkpoint import DetectionCheckpointer
from detectron2 import model_zoo

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)
cfg.MODEL.DEVICE = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

model = build_model(cfg)
model.eval()
model.to(cfg.MODEL.DEVICE)

DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)

dummy_input = torch.randn(1, 3, 224, 224, device=cfg.MODEL.DEVICE)

print('input: ',dummy_input.shape)

input_names = [&quot;actual_input_1&quot;]
output_names = [&quot;output1&quot;]

torch.onnx.export(model, dummy_input, &quot;detectron2_model.onnx&quot;, verbose=True,
                  input_names=input_names, output_names=output_names)

print(&quot;Detectron2 model has been successfully exported to detectron2_model.onnx&quot;)
</code></pre>
<p>Error:</p>
<pre><code>
input:  torch.Size([1, 3, 224, 224])
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-10-569a1c2752b7&gt; in &lt;cell line: 33&gt;()
     31 output_names = [&quot;output1&quot;]
     32 
---&gt; 33 torch.onnx.export(model, dummy_input, &quot;detectron2_model.onnx&quot;, verbose=True,
     34                   input_names=input_names,
     35                   output_names=output_names)

16 frames
/content/detectron2/detectron2/modeling/meta_arch/rcnn.py in &lt;listcomp&gt;(.0)
    225         Normalize, pad and batch the input images.
    226         &quot;&quot;&quot;
--&gt; 227         images = [self._move_to_current_device(x[&quot;image&quot;]) for x in batched_inputs]
    228         images = [(x - self.pixel_mean) / self.pixel_std for x in images]
    229         images = ImageList.from_tensors(

IndexError: too many indices for tensor of dimension 3
</code></pre>
<p>I want the code to work and find the optimal dummy input shape to convert to onnx format. Your help is appreaciated. Thanks in advance.</p>
","2024-03-19 09:07:09","1","Question"
"78184358","","How does the pytorch CUDA version and CUDA actually works?","<p>Here is my conda environment's package list:</p>
<p><img src=""https://i.sstatic.net/uaSzY.png"" alt=""MY PACKAGE LIST"" /></p>
<p>As you can see, I have not installed cudatoolkit and the nvcc comand is not usable. But I do have installed pytorch in CUDA version.</p>
<p>However, when I import torch in python and check <code>torch.cuda.is_available()</code>, it returns Ture.</p>
<p>I even run this test script:</p>
<pre><code>import torch
from torch import nn
from torch.nn import Module
from torch.optim.lr_scheduler import LambdaLR


class TestNet(Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.linear = nn.Linear(10,10)

    def forward(self, x):
        return self.linear(x)
    

if __name__==&quot;__main__&quot;:
    if torch.cuda.is_available():
        device = &quot;cuda&quot;
    else:
        device = &quot;cpu&quot;
    print(f&quot;Using device {device}&quot;)
    test_samples = torch.rand([32,10]).to(device)
    gt_matrix = torch.eye(10).to(device)
    target = torch.matmul(test_samples, gt_matrix)

    model = TestNet().to(device)

    optimizer = torch.optim.SGD(model.parameters(), lr=1)
    criterion = nn.MSELoss()
    scheduler = LambdaLR(optimizer, lr_lambda=lambda x: min(x, 24)/24)

    for epoch in range(128):
        logits = model(test_samples)
        loss = criterion(logits, target)
        learning_rate = optimizer.param_groups[0][&quot;lr&quot;]

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

        print(f&quot;Epoch {epoch+1}/{24}, loss {loss.item()}, lr {learning_rate}&quot;)
    
    print(&quot;Learned matrix:&quot;)
    print(model.state_dict()[&quot;linear.weight&quot;])
</code></pre>
<p>And it runs successfully.</p>
<p>So I am curious about how pytorch CUDA version actually works? Does it need a pre-installed CUDA toolkit or not? Besides, what is the difference between installing CUDA by <code>conda install cudatoolkit</code> , <code>conda install cuda</code> and even installing by graphical installer?</p>
","2024-03-19 05:18:07","0","Question"
"78183643","78182788","","<p>You are correct, you are passing <code>x.flatten(1)</code> as an input even though <code>y</code> - let alone <code>y.flatten(1)</code> - was computed from <code>x</code>, not <code>x.flatten(1)</code>. Instead, you could avoid the flattening with something like this:</p>
<pre><code>def jacobian(y, x):
    jacobian = []
    for i in range(x.numel()//len(x)):
        v = torch.zeros_like(y.flatten(1))
        v[:, i] = 1.
        dy_dx, *_ = torch.autograd.grad(y, x, v.view_as(x), 
                      retain_graph=True, create_graph=True, allow_unused=True)
        jacobian.append(dy_dx)
    jacobian = torch.stack(jacobian, dim=1)
    return jacobian
</code></pre>
<p>So after calling the function, you can flatten <code>d1</code>, <code>d2</code>, and <code>d3</code> together. Here is a minimal example:</p>
<pre><code>x = torch.rand(1,3,2,2, requires_grad=True)
J = jacobian(x**2, x)

&gt; J.flatten(-3)
tensor([[[1.2363, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.2386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 1.0451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 1.4160, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.4090, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7642, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3041, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1995]]],
       grad_fn=&lt;ViewBackward0&gt;)
</code></pre>
<p>From here, I guess you can compute the trace you're looking for with <a href=""https://pytorch.org/docs/stable/generated/torch.trace.html"" rel=""nofollow noreferrer""><code>torch.trace</code></a>.</p>
<pre><code>&gt; [j.trace() for j in J.flatten(-3)]
[tensor(7.6128, grad_fn=&lt;TraceBackward0&gt;)]
</code></pre>
<p>Keep in mind you can also use the builtin <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian"" rel=""nofollow noreferrer""><code>jacobian</code></a> function:</p>
<pre><code>J = torch.autograd.functional.jacobian(lambda x: g(t, x), x)
</code></pre>
<p>However, you will need to reshape the result:</p>
<pre><code>k = x.numel()//len(x)

&gt; [j.trace() for j in J.view(len(x), k, k)]
[tensor(7.6128)]
</code></pre>
","2024-03-19 00:16:40","1","Answer"
"78183172","","how to train a model in sagemaker via transformers library?","<p>Based on this example here, <a href=""https://aws.amazon.com/blogs/machine-learning/fine-tune-and-host-hugging-face-bert-models-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/fine-tune-and-host-hugging-face-bert-models-on-amazon-sagemaker/</a>,
for training a HuggingFace estimator is used and i am assuming , once the command to start training is issued ( see below) , the hugging face library , downloads the model , distilbert-base-cased in this case. and runs the train.py file. instead of using hugging face, can i set up the training in sagemaker with just transformers library?</p>
<pre><code>huggingface_estimator = HuggingFace(entry_point='train.py',
                            source_dir='./scripts',
                            instance_type='ml.p3.2xlarge',
                            instance_count=1,
                            role=role,
                            transformers_version='4.6.1',
                            pytorch_version='1.7.1',
                            py_version='py36',
                            hyperparameters = hyperparameters)
</code></pre>
<p>start training command</p>
<pre><code>/opt/conda/bin/python train.py --epochs 10 --model_name distilbert-base-cased --token_name distilbert-base-cased--train_batch_size 1024

</code></pre>
<p>I have run the example , provided in the link. but this uses hugging face library , how to set up the training without using hugging face library.</p>
","2024-03-18 21:48:21","1","Question"
"78183024","","How to solve RuntimeError: Couldn't find appropriate backend to handle uri dataset/data/0.wav and format None","<p>The Problem is if I try to run <code>metadata = torchaudio.info(path)</code> I get the error message <code>RuntimeError: Couldn't find appropriate backend to handle uri dataset/data/0.wav and format None.</code> And if I run <code>print(str(torchaudio.list_audio_backends()))</code> it returns an empty list</p>
<p>I looked at both the documentation and similar questions, like those <a href=""https://stackoverflow.com/questions/78097861/how-to-solve-runtimeerror-couldnt-find-appropriate-backend-to-handle-uri-in-py/78103260#78103260"">How to solve RuntimeError: Couldn&#39;t find appropriate backend to handle uri in python</a>, <a href=""https://superuser.com/questions/1819222/how-to-install-sox-for-pytorch-audio/1819866#1819866"">https://superuser.com/questions/1819222/how-to-install-sox-for-pytorch-audio/1819866#1819866</a> and <a href=""https://stackoverflow.com/questions/62543843/cannot-import-torch-audio-no-audio-backend-is-available"">cannot import torch audio &#39; No audio backend is available.&#39;</a>. According to them I just need sox and libsox. None of the install commands from the answeres helped me.</p>
<p>I have installed both sox and libsox-dev. Here are the Versions:</p>
<ul>
<li><code>pip show torchaudio</code> -&gt; <code>... Version: 2.2.1 ...</code></li>
<li><code>sox --version</code> -&gt; <code>sox:      SoX v14.4.2</code></li>
<li><code>ldd $(which sox) | grep libsox</code> -&gt; <code>libsox.so.3 =&gt; /lib/x86_64-linux-gnu/libsox.so.3</code></li>
</ul>
<p>I got no idea what's wrong and would appreciate any help.</p>
<p>Ps. I am using Ubuntu</p>
","2024-03-18 21:09:29","0","Question"
"78182788","","How to generate jacobian of a tensor-valued function using torch.autograd?","<p>Computing the jacobian of a function f : R^d -&gt; R^d is not too hard:</p>
<pre><code>def jacobian(y, x):
    k, d = x.shape
    jacobian = list()
    
    for i in range(d):
        v = torch.zeros_like(y)
        v[:, i] = 1.
        dy_dx = torch.autograd.grad(y, x, grad_outputs = v, retain_graph = True, create_graph = True, allow_unused = True)[0]  # shape [k, d]
        jacobian.append(dy_dx)
    jacobian = torch.stack(jacobian, dim = 1).requires_grad_()
    return jacobian
</code></pre>
<p>Above, <code>jacobian</code> is invoked with <code>y = f(x)</code>. However, now I have a function <code>g = g(t, x)</code>, where <code>t</code> is a <code>torch.tensor</code> of shape <code>k</code> and <code>x</code> is a <code>torch.tensor</code> of shape <code>(k, d1, d2, d3)</code>. The result of <code>g</code> is again a <code>torch.tensor</code> of shape <code>(k, d1, d2, d3)</code></p>
<p>I've tried to use my already existing <code>jacobian</code> function. What I did was</p>
<pre><code>y = g(t, x)
x = x.flatten(1)
y = y.flatten(1)
jacobian(y, x)
</code></pre>
<p>The problem is that all the time <code>dy_dx</code> is <code>None</code>. The only explanation I have for this is that most probably the dependency graph is broken after the <code>flatten(1)</code> call.</p>
<p>So, what can I do here? I should remark that what I actually want to compute is the divergence. That is, the trace of the jacobian. If there is a more performant solution for that specific case available, I'd be interested in that one.</p>
","2024-03-18 20:11:24","0","Question"
"78182288","78180700","","<p>Missing keys error means your model architecture is trying to find weights that don't exist in the file you are loading. Unexpected keys means the file you are loading contains keys the model isn't looking for. The problem is likely that your model architecture changed at some point after saving weights, and some of your state dict keys got renamed.</p>
","2024-03-18 18:18:53","0","Answer"
"78182135","","module 'torch' has no attribute 'SymInt","<p>I wanted to test the &quot;segment-anything&quot; package, but ran into a problem when initializing the modules. When I initialize them, an error appears that torch does not have the SymInt attribute.
Unworked line: <code>from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor</code>
Final problem text: &quot;module 'torch' has no attribute 'SymInt'&quot;</p>
<p>I tried different available versions of torchvision and torch.</p>
","2024-03-18 17:51:31","1","Question"
"78182012","78181866","","<p>It looks like the weights file <code>best.pt</code> was trained using the Ultralytics package.  Give the following a try.</p>
<pre><code>from ultralytics import YOLO

model = YOLO('best.pt')
input_image = Image.open('input_image.jpg')
output = model(input_image)
print(output)
</code></pre>
","2024-03-18 17:30:25","1","Answer"
"78181866","","Simple Torch Model Test: ModuleNotFoundError: No module named 'ultralytics.yolo'","<p>I have a model, <code>best.pt</code>, that I'd like to run. It takes an image as input, and outputs a string.</p>
<p>I have <code>ultralytics</code>, <code>torch</code> and <code>torchvision</code> installed.</p>
<p>My code is simple:</p>
<pre><code>import torch
from PIL import Image

# Load the pre-trained model
model = torch.load('best.pt')
# Load the input image
input_image = Image.open('input_image.jpg')
# Pass the image through the model
output = model(input_image)
# Print the output
print(output)
</code></pre>
<p>The result is as follows:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/fares/project/model/main.py&quot;, line 5, in &lt;module&gt;
    model = torch.load('best.pt')
            ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/serialization.py&quot;, line 1026, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/serialization.py&quot;, line 1438, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/serialization.py&quot;, line 1431, in find_class
    return super().find_class(mod_name, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'ultralytics.yolo'
</code></pre>
<p>What am I doing wrong?</p>
","2024-03-18 17:04:49","1","Question"
"78180700","","Load pre-training parameters trained on a single GPU on multi GPUS on a single machine","<p>I tried to load the pre-training parameters trained by a single GPU on a single machine with multiple GPUs, but errors such as Missing keys and Unexpected keys occurred.</p>
<pre><code>backbone_cfg = dict(
            embed_dim=embed_dim,
            depths=depths,  
            num_heads=num_heads, 
            window_size=window_size,
            ape=False,
            drop_path_rate=0.3,
            patch_norm=True,
            use_checkpoint=False,
            frozen_stages=frozen_stages
        )

        self.backbone = SwinTransformer(**backbone_cfg)
</code></pre>
<pre><code>def init_weights_multiGPUs(self, pretrained=None):
             if pretrained is not None:
                if dist.get_rank() == 0:
                    self.backbone.load_state_dict(torch.load(pretrained))
                dist.barrier()
</code></pre>
<p>pretrained is a pre-training parameters path.</p>
<p>I have tried the methods mentioned below but the problem is still the same</p>
<pre><code>def init_weights_multiGPUs(self, pretrained = None) :
        print(f'== Load encoder backbone on multiGPUs from: {pretrained}')
        if isinstance(self.backbone, torch.nn.parallel.DistributedDataParallel):
            self.backbone = self.backbone.module
        self.backbone.load_state_dict(torch.load(pretrained, map_location='cuda:{}'.format(torch.cuda.current_device())))
</code></pre>
","2024-03-18 13:51:11","0","Question"
"78179104","78143186","","<p>pipe = DiffusionPipeline.from_pretrained(
model,
torch_dtype=torch.float16,
variant='fp16',use_safetensors=True,
)</p>
","2024-03-18 09:20:26","0","Answer"
"78178758","78178701","","<p>In the code snippet above, pil_mask is created as a PIL Image, then converted to a NumPy array (pil_mask_np). You can now use pil_mask_np wherever you need it in your code.</p>
<pre><code> import torch
 import numpy as np
 import PIL.Image as Image
    
 def identity(x_tensor, pil_mask, _):
    return x_tensor

 augmentations = [
   (identity, 1.0)
]

    pil_mask = Image.new('L', (256, 256), color=0)
pil_mask_np = np.array(pil_mask)

    class PRIMEAugModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.augmentations = augmentations
        self.num_transforms = len(augmentations)


    def forward(self, x, mask_t):
        x_tensor = torch.from_numpy(x)
        aug_x = torch.zeros_like(x_tensor)
        for i in range(self.num_transforms):
            fn, weight = self.augmentations[i]
            if fn.__name__ == 'identity':
                aug_x += fn(x_tensor, pil_mask_np, _) * mask_t[:, i] * weight
            else:
                aug_x += fn(x_tensor, pil_mask_np) * mask_t[:, i] * weight
        return aug_x
</code></pre>
","2024-03-18 08:09:53","0","Answer"
"78178701","","NameError: name 'pil_mask' is not defined","<p>This is my code. I have defined various operations like this:</p>
<pre><code>def identity(pil_img, pil_mask, _):
    return pil_img, pil_mask

def autocontrast(pil_img, pil_mask, _):
    return ImageOps.autocontrast(pil_img), pil_mask


def equalize(pil_img, pil_mask, _):
    return ImageOps.equalize(pil_img), pil_mask



def rotate(pil_img, pil_mask, level):
    degrees = int_parameter(level, min_max_vals.rotate.max)
    if np.random.uniform() &gt; 0.5:
        degrees = -degrees
    return pil_img.rotate(degrees, resample=Image.BILINEAR), pil_mask.rotate(degrees, resample=Image.BILINEAR)
</code></pre>
<p>like the above.</p>
<p>Now I want to use the PRIME augmentation (PRImitives of Maximum Entropy) :</p>
<p>but I am getting an error:</p>
<pre><code>    aug_x += fn(x_tensor, pil_mask, _) * mask_t[:, i] * weight
NameError: name 'pil_mask' is not defined
</code></pre>
<pre><code>and this is the PRIME code:

augmentations = [
    (identity, 1.0)
    ]
class PRIMEAugModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.augmentations = augmentations
        self.num_transforms = len(augmentations)


    def forward(self, x, mask_t):
        x_tensor = torch.from_numpy(x)
        aug_x = torch.zeros_like(x_tensor)
        for i in range(self.num_transforms):
            fn, weight = self.augmentations[i]
            if fn.__name__ == 'identity':
                aug_x += fn(x_tensor, pil_mask, _) * mask_t[:, i] * weight
            else:
                aug_x += fn(x_tensor, pil_mask) * mask_t[:, i] * weight
        return aug_x
</code></pre>
<p>I am confused where and how should I define PIL_mask</p>
","2024-03-18 07:54:52","0","Question"
"78177397","78177067","","<p>The <code>T</code> stands for the transpose operation. For reasons that aren't entirely clear, pytorch stores the transpose of the weight matrix for linear layers. You can find some discussion of this <a href=""https://github.com/pytorch/pytorch/issues/2159"" rel=""nofollow noreferrer"">here</a>, but it seems to be a legacy thing.</p>
<pre class=""lang-py prettyprint-override""><code>layer = nn.Linear(64, 128)
layer.weight.shape
&gt; torch.Size([128, 64]) # we would expect (64, 128) but we get the transpose (128, 64)

x = torch.randn(8, 64) # random input

# nn.Linear computes `xA^T + b`
((x@layer.weight.T) + layer.bias == layer(x)).all()
&gt; tensor(True)
</code></pre>
","2024-03-18 00:01:57","3","Answer"
"78177067","","What is T in nn.Linear equation?","<p>I'm trying to understand the PyTorch's <code>nn.Linear</code>. I get that it applies a linear transformation to the input, but in the docs they specify the equation being used as <code>y= xA^T + b</code>.</p>
<p>This reminds my of <code>y = (x[1] * w[1]) + (x[2] * w[2]) + b</code>. Is that at all what's happening here?</p>
<p>Also, here is my current understanding of the variables in this equation, is this correct?</p>
<p><code>x</code> = input
<code>A</code> = weight (I think)
<code>T</code> = Not sure
<code>b</code> = bias</p>
","2024-03-17 21:46:51","0","Question"
"78176769","78132075","","<p>Generally speaking, you want to load and preprocess your data on the CPU and only move to the GPU right before passing to the model.</p>
<p>This is because GPU memory is limited. You don't want to use GPU memory storing data that isn't being used.</p>
<p>Maybe there are niche scenarios where you have preprocessed tensors saved and you can load directly to the GPU. However, this would require loading from disk to the GPU (to avoid having the full dataset sitting on GPU memory), which is rather slow. Doing your data processing on CPU allows you to load your dataset into memory and transfer data from CPU memory to GPU memory which is much faster.</p>
<p>For the case you're describing, it sounds like you're bottlenecked on i/o loading dataset files from disk. In this scenario, it would be best to have your dataset load/preprocess on the CPU, then move to GPU after batching in your dataloader.</p>
","2024-03-17 20:10:06","2","Answer"
"78175879","78165875","","<p><code>x</code> cannot be <code>None</code>. It has to be a tensor that is the same shape and on the correct device (of the current process). I suspect this is because <code>broadcast</code> internally does a <code>copy_</code>. For some reason, an empty tensor also does not work. Instead, I just created a tensor with all zeros.</p>
<pre class=""lang-py prettyprint-override""><code>from accelerate.utils import broadcast

x = torch.zeros(*final_shape, device=accelerator.device)
if accelerator.is_local_main_process:
    x = &lt;do_some_computation&gt;
    x = broadcast(x)
print(x.shape)
</code></pre>
","2024-03-17 15:42:57","1","Answer"
"78173387","78132075","","<p><code>num_workers&gt;0</code> allows parallelization on CPU, not on GPU. Generally, the best practice is to perform the data loading and preprocessing across multiple processes (using <code>num_workers</code>) on the CPU.</p>
<p>After the batch is collated by the <code>DataLoader</code>, transfer the entire batch to the GPU in your training loop.</p>
<p>For training on multiple GPUs you can use PyTorch's <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noreferrer"">distributed data parallel (DDP)</a> or data parallel (DP) wrappers to distribute the model and data across the GPUs.</p>
","2024-03-16 21:46:51","0","Answer"
"78173348","78173209","","<p>The Python Imaging Library doesn't support <code>exr</code> files, <a href=""https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"" rel=""nofollow noreferrer"">ref</a>. The only function available for saving images in Torchvision is <a href=""https://pytorch.org/vision/main/generated/torchvision.utils.save_image.html"" rel=""nofollow noreferrer""><code>save_image</code></a>, <a href=""https://github.com/pytorch/vision/blob/main/torchvision/utils.py#L150"" rel=""nofollow noreferrer"">it uses PIL</a> to save the file. As far back as the first documented version (<a href=""https://pytorch.org/vision/0.8/search.html?q=save_float_image%20&amp;check_keywords=yes&amp;area=default"" rel=""nofollow noreferrer""><code>0.8</code></a>), there was never a &quot;save_float_image&quot; function.</p>
<p>What you may use instead is <a href=""https://github.com/imageio/imageio"" rel=""nofollow noreferrer""><code>imageio</code></a> (<code>pip install imageio</code>):
You can get a reference by looking at the <a href=""https://github.com/imageio/imageio/blob/085ce272ae33e9ee37da43d03bcca35bbb5d571d/tests/test_freeimage.py#L693-695"" rel=""nofollow noreferrer"">unit test</a> for saving an <code>exr</code> file. You can save to disk by first transposing and converting to numpy array:</p>
<pre><code>arr = tensor.permute(1,2,0).numpy()
imageio.imwrite('out_path.exr', arr)
</code></pre>
<p>Where <code>out_path</code> has the <code>.exr</code> extension file, otherwise you must use the option: <code>extension=&quot;.exr&quot;</code></p>
<hr />
<p>Here is a minimal reproducible example:</p>
<pre><code>import os
os.environ[&quot;OPENCV_IO_ENABLE_OPENEXR&quot;] = &quot;1&quot;

tensor = torch.rand(3,100,150)
arr = tensor.permute(1,2,0).numpy()
imageio.imwrite('out_path.exr', arr)
</code></pre>
","2024-03-16 21:22:46","2","Answer"
"78173209","","How to save an `exr` from a pytorch tensor in Python?","<p>Previously, there was a function <code>torchvision.utils.save_float_image</code> with which it was possible to store an <code>.exr</code> file from a pytorch tensor. This function is gone in the current relase (0.17). Now, there is only the function <code>torchvision.utils.save_image</code> (<a href=""https://pytorch.org/vision/stable/generated/torchvision.utils.save_image.html#torchvision.utils.save_image"" rel=""nofollow noreferrer"">https://pytorch.org/vision/stable/generated/torchvision.utils.save_image.html#torchvision.utils.save_image</a>). But when I try to execute</p>
<pre><code>with open(os.path.join('folder/', `foo.exr`), &quot;wb&quot;) as fout:
    save_image(tensor, fout)
</code></pre>
<p>I'm receiving the error &quot;unknown file extension *.exr&quot;. So, what can we do now?</p>
","2024-03-16 20:26:42","0","Question"
"78170194","78168388","","<p>As far as I understand, you don't need a special function to achieve such behavior. If you assign the existing tensor to a new variable, both variable names in that scope will be pointing to the same tensor.</p>
<pre><code>x = torch.arange(5)
&gt; tensor([0, 1, 2, 3, 4])

y = x # assign to y

# change y[0] which is x[0]
y[0] = -1 

x
&gt; tensor([-1,  1,  2,  3,  4])
</code></pre>
","2024-03-16 00:17:45","0","Answer"
"78168649","78168447","","<p>I have figured out the problem. The model object has internal variables that are not parameters (weights). I have to set those variables to use mps_device explicitly. Specifying model.to(mps_device) is not sufficient.</p>
<pre><code>def forward(self, x):
    # Start with empty network output and cell state to initialize the sequence
    c_0 = torch.zeros(self.layer_size, BATCH_SIZE, self.hidden_size).requires_grad_().to(mps_device)
    h_0 = torch.zeros(self.layer_size, BATCH_SIZE, self.hidden_size).requires_grad_().to(mps_device)

    # Iterate over all sequence elements across all sequences of the mini-batch
    #print(x.size())
    out, (h_t, c_t) = self.lstm(x, (h_0.detach(), c_0.detach()))
    
    # Final output layer
    return self.sig(self.fc(out[-1]))
</code></pre>
","2024-03-15 17:14:55","3","Answer"
"78168513","78087355","","<p>There are a lot of ways you could approach this problem, but the best way is you use a same convolution, with this you are guaranteed to have the same output dimensions after the operation, you dont have to try and force layers to conform to the dimensions of other layers.</p>
<pre><code>class CustomConBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True, device=&quot;cuda&quot;):
        super(CustomConBlock, self).__init__()
        self.kernel_size = kernel_size
        self.padding = (self.kernel_size - 1) // 2  # this is what you are missing
        self.conv1 = nn.Conv2d(in_channels, out_channels, self.kernel_size, stride, self.padding, bias=bias).to(device)
        self.conv2 = nn.Conv2d(out_channels, out_channels, self.kernel_size, stride, self.padding, bias=bias).to(device)
        self.relu = nn.ReLU().to(device)
        self.batchnorm = nn.BatchNorm2d(out_channels).to(device)
        self.maxpool = nn.MaxPool2d(2, 2).to(device)

    def forward(self, input):
        # perform your network forward pass here or whatever you want
        # now all the conv layers will have the appropriate dimensions 
        
        return None
</code></pre>
","2024-03-15 16:50:00","2","Answer"
"78168447","","Placeholder storage has not been allocated on MPS device","<p>I understand I need to allocate both the input tensor and the model parameters to the mps device in order for PyTorch to use my Mac M1 GPU for training.  I did just that and it still gave me this error message:</p>
<p>*Placeholder storage has not been allocated on MPS device!
*
Here is a snippet of my code.</p>
<pre><code>if torch.backends.mps.is_available():
    mps_device = torch.device(&quot;mps&quot;)
    x = torch.ones(1, device=mps_device)
    print(x)
else:
    print(&quot;MPS device not found&quot;)


model = LSTMModel(input_size=197,
                  hidden_size=HIDDEN_UNITS,
                  output_size=1,layer_size=2)

# Transfer the model to the GPU
model = model.to(mps_device)


# iterate over the training data
    for i in range(100000):
        # send the input/labels to the GPU
        
        next_batch = next(train_generate)
        inputs = torch.from_numpy(next_batch[0]).float().to(mps_device)
        labels = torch.from_numpy(next_batch[1][-1]).float().to(mps_device)

        
        with torch.set_grad_enabled(True):
            outputs = model(inputs)
            loss = loss_function(outputs, labels)

            # backward
            loss.backward()
            optimizer.step()
</code></pre>
<p>I followed documentation of how to use M1 GPU for PyTorch but it didn't work.</p>
","2024-03-15 16:39:36","5","Question"
"78168388","","Is it possible to delete an element from a pytorch tensor referentially?","<p>Currently, I'm trying to delete an element in a vector corresponding to an index in pytorch, however, it keeps creating an extra copy with the methods I'm currently using.</p>
<p>This makes sense under the hood; the only way I'm thinking the memory address could remain the same is if the tensor was implemented like a linkedlist.</p>
<p>That being said, is it possible to delete an element from a PyTorch tensor referentially. i.e is it possible to delete an element from the tensor such that the tensor with the deleted index and the old tensor have the same memory address.</p>
<p>Methods I've already tried (but involved creating a copy/new memory address) involve (pseudocode)...</p>
<ol>
<li>new_data = torch.cat((data[:i], data[i+1:))</li>
<li>data[torch.LongTensor(indices_to_keep)] where indices_to_keep = [index for index in range(data.shape[0]) if index != i]</li>
</ol>
","2024-03-15 16:28:58","1","Question"
"78165875","","How to broadcast a tensor from main process using Accelerate?","<p>I want to do some computation in the main process and broadcast the tensor to other processes. Here is a sketch of what my code looks like currently:</p>
<pre class=""lang-py prettyprint-override""><code>from accelerate.utils import broadcast

x = None
if accelerator.is_local_main_process:
    x = &lt;do_some_computation&gt;
    x = broadcast(x)  # I have even tried moving this line out of the if block
print(x.shape)
</code></pre>
<p>This gives me following error:
<code>TypeError: Unsupported types (&lt;class 'NoneType'&gt;) passed to `_gpu_broadcast_one` . Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` s hould be passed.</code></p>
<p>Which means that <code>x</code> is still <code>None</code> and is not really being broadcasted. How do I fix this?</p>
","2024-03-15 09:30:29","2","Question"
"78164797","","Could not find a version that satisfies the requirement llama-index-finetuning-cross-encoders","<p>I'm trying to run this Llama Index <a href=""https://docs.llamaindex.ai/en/stable/examples/finetuning/cross_encoder_finetuning/cross_encoder_finetuning.html"" rel=""nofollow noreferrer"">How to Finetune a cross-encoder using LLamaIndex</a>.
But I cannot install <code>llama-index-finetuning-cross-encoders</code> package.</p>
<p>I tried this code</p>
<pre><code>%pip install llama-index-finetuning-cross-encoders
</code></pre>
<p>But I get these error messages:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement llama-index-finetuning-cross-encoders (from versions: none)
ERROR: No matching distribution found for llama-index-finetuning-cross-encoders
</code></pre>
<p>How can I install this package?</p>
","2024-03-15 05:12:29","0","Question"
"78162549","","Loading a PyTorch model into my Flask app, throws this error- ModuleNotFoundError: No module named 'models'","<p>The project aims to detect objects from an image and generates an audio file in which the audio plays what the object really is. For instance, if the object contains a car, the audio file contains a audio saying 'CAR'. But the problem here is that when I try to run my Flask application I run into this error:</p>
<pre><code>PS E:\ObjRec&gt; python app.py
Loading YOLOv5 model...
Traceback (most recent call last):
  File &quot;E:\ObjRec\app.py&quot;, line 11, in &lt;module&gt;
    model = torch.load('best.pt')
            ^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Admin\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\serialization.py&quot;, line 1026, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Admin\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\serialization.py&quot;, line 1438, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Admin\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\serialization.py&quot;, line 1431, in find_class
    return super().find_class(mod_name, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'models'
</code></pre>
<p>This is my <code>app.py</code> :</p>
<pre><code>from flask import Flask, request, jsonify
import torch
from torchvision import transforms
from PIL import Image
import io

app = Flask(__name__)

print('Loading YOLOv5 model...')
model = torch.load('best.pt')
model.eval()

preprocess = transforms.Compose([
    transforms.Resize((416, 416)),
    transforms.ToTensor(),
])

@app.route('/detect', methods=['POST'])
def detect_objects():
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400
    image_file = request.files['image']
    image_bytes = image_file.read()
    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    image = preprocess(image)
    results = model(image.unsqueeze(0)) 
    labels = results.names
    
    return jsonify({'labels': labels}), 200

if __name__ == '__main__':
    app.run(debug=True)

</code></pre>
<p>I couldn't proceed further without loading my model into the project. Also I'm not sure whether the procedure I'm following is correct.</p>
<p><a href=""https://i.sstatic.net/KRX2P.png"" rel=""nofollow noreferrer"">This is the actual file paths.</a></p>
<p>I tried to <code>pip install models</code> and again ran into metadata-generation-failed error:</p>
<pre><code>PS E:\ObjRec&gt; pip install models
Collecting models
  Using cached models-0.9.3.tar.gz (16 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─&gt; [8 lines of output]
      Traceback (most recent call last):
        File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt;
        File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 34, in &lt;module&gt;
        File &quot;C:\Users\Admin\AppData\Local\Temp\pip-install-8yqo_00s\models_16df2befc82545e7a9071610e1043cde\setup.py&quot;, line 25, in &lt;module&gt;
          import models
        File &quot;C:\Users\Admin\AppData\Local\Temp\pip-install-8yqo_00s\models_16df2befc82545e7a9071610e1043cde\models\__init__.py&quot;, line 23, in &lt;module&gt;
          from base import *
      ModuleNotFoundError: No module named 'base'
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─&gt; See above for output.

note: This is an issue with the package mentioned above, not pip.
</code></pre>
<p>No idea how to proceed further from here!</p>
","2024-03-14 18:00:00","0","Question"
"78158073","78156534","","<p>Have you try import <code>twoD_predict</code> to the file you want to load the model? The pickled version still requires things to replicate on how to recreate the structure of your model (e.g. you have to load the definition of the model to your current file). If you don't want to do that, considering export your model in TorchScript format.</p>
","2024-03-14 04:14:31","0","Answer"
"78157953","78115094","","<p>I fixed this problem.</p>
<p><a href=""https://i.sstatic.net/hIz91.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>The reason for the labeling error is that the directory order of the images read is sorted by 1,10,100... instead of 1,2,3... instead of 1,2,3... So when I didn't use Dataloader, it would predict the image for label 2 to be 14. I used a function that exchanges the Key and values in the dictionary and then backtracks through the predicted values to find the Key, and I get the correct result. This way may not be the best or the official solution.</p>
<p><a href=""https://i.sstatic.net/ETwMb.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","2024-03-14 03:24:31","0","Answer"
"78157395","78157143","","<p>You can create a one-dimensional ctypes array object from pointer and shape. It implements the buffer protocol, so it can be converted to a one-dimensional tensor which is finally reshaped.</p>
<p>The code at the end shows that <code>x</code> and <code>y</code> share the same memory.</p>
<pre><code>import torch
import ctypes
from math import prod

# It additionally needs the ctypes type as torch type
def as_tensor(pointer, shape, torch_type):
    arr = (pointer._type_ * prod(shape)).from_address(
        ctypes.addressof(pointer.contents))
    
    return torch.frombuffer(arr, dtype=torch_type).view(*shape)

shape = (2, 3, 4)
x = torch.zeros(shape)

p = ctypes.cast(x.data_ptr(), ctypes.POINTER(ctypes.c_float))

y = as_tensor(p, shape, torch.float)

print(y)  # Print created tensor

x[1,1,0] = 3.  # Modify original

print(y)  # Print again
</code></pre>
<p>Output:</p>
<pre><code>tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [3., 0., 0., 0.],
         [0., 0., 0., 0.]]])
</code></pre>
","2024-03-13 23:45:11","5","Answer"
"78157143","","How do I cast a raw pointer to a pytorch tensor of a specific shape?","<p>I get a raw pointer from a C++ library which I would like to interpret (in a &quot;<code>reinterpret_cast</code>-like fashion) as a pytorch tensor of a specific shape. Since the code is executed in a performance critical section, I really want to make sure that no heap allocations and/or copy operations are performed.</p>
<p>Here is what I got right now:</p>
<pre><code>def as_tensor(pointer, shape):
    return torch.from_numpy(numpy.array(numpy.ctypeslib.as_array(pointer, shape = shape)))

shape = (2, 3, 4)
x = torch.zeros(shape)

p = ctypes.cast(x.data_ptr(), ctypes.POINTER(ctypes.c_float))
y = as_tensor(p, shape)
</code></pre>
<p>Is it really necessary to cast to a numpy array before? And I'm also not 100% sure if the call to <code>numpy.array(...)</code> doesn't copy the content of what the <code>as_array()</code> call is pointing to.</p>
","2024-03-13 22:26:00","1","Question"
"78156534","","PyTorch and Pickle Error: AttributeError: Can't get attribute ""Class_name"" on <module '__main__' from '/load_model.py'>","<p>I have created a neural network model using PyTorch for a time-series forecasting problem. I have saved the model using Pickle.
When loading the model to check on test data, it is throwing an attribute error.
Below, I'm providing the relevant parts of my code:</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class twoD_predict(nn.Module):

  def __init__(self):

    super().__init__()

  def forward(self,x):
  ...

  def train(self,
        epochs = 100):
  ...

obj1 = twoD_predict()

obj1.train()
</code></pre>
<p>I saved the model using Pickle as follows:</p>
<pre><code>import pickle
filename = (f&quot;{column_names[0]}.sav&quot;)
pickle.dump(obj1, open(filename, 'wb'))
</code></pre>
<p>However, when I try to load the model with the following code:</p>
<pre><code>import pickle
if __name__ == '__main__':
    with open(&quot;model.sav&quot;, 'rb') as file:
        model = pickle.load(file)
</code></pre>
<p>I encounter the error: <code>&quot;AttributeError: Can't get attribute &quot;twoD_predict&quot; on &lt;module '__main__' from '/load_model.py'&gt;&quot;</code></p>
<p>Can anyone please help?</p>
","2024-03-13 19:58:48","0","Question"
"78155590","78147725","","<p>You've tagged this with pytorch, so I'll give the pytorch answer.</p>
<p>Pytorch data utils has a <code>Dataset</code> and a <code>DataLoader</code>. tl;dr, the <code>Dataset</code> handles loading a single example, while the <code>DataLoader</code> handles batching and any bulk processing.</p>
<p>The <code>Dataset</code> has two methods, <code>__len__</code> for determining the number of items in the dataset and <code>__getitem__</code> for loading a single item.</p>
<pre class=""lang-py prettyprint-override""><code>class MyDataset(Dataset):
    def __init__(self):
        ...

    def __len__(self):
        ...

    def __getitem__(self, index):
        ...
</code></pre>
<p>The <code>DataLoader</code> is passed a list of outputs from the <code>Dataset</code> (ie <code>batch_input = [dataset.__getitem__(i) for i in idxs]</code>). The batch input is sent to the <code>collate_fn</code> of the <code>DataLoader</code>.</p>
<pre class=""lang-py prettyprint-override""><code>def my_collate_fn(batch):
    ...

dataloader = DataLoader(my_dataset, batch_size, collate_fn=my_collate_fn)
</code></pre>
<p>In terms of thinking about what to do where, the <code>Dataset</code> should handle loading single examples. The <code>Dataset</code> will be called in parallel, so tasks that are CPU-bound should go in the <code>Dataset</code>. Loading from disk (if applicable) is also typically done in the <code>Dataset</code>.</p>
<p>The <code>collate_fn</code> handles converting a list of outputs from your <code>Dataset</code> into whatever format your model wants. Since the <code>DataLoader</code> deals with a batch of data, it can be more efficient to apply batch processing steps. Stacking tensors, padding to length, generating masks or other bulk tensor ops work well in the <code>collate_fn</code>.</p>
<p>In general, think of the <code>Dataset</code> as running multi-process on single examples, while the <code>DataLoader</code> running a single-process on a batch of examples.</p>
","2024-03-13 16:59:21","2","Answer"
"78152950","78152760","","<p>The <a href=""https://pytorch.org/docs/stable/generated/torch.matmul.html"" rel=""nofollow noreferrer""><code>matmul</code></a> operator works like this: <code>(*, i, j) @ (*, j, k) = (*, i, k)</code>. <br>So in your case, no need to transpose <code>A</code>, simply <code>A@B</code>.</p>
<p>If you prefer, you can use <a href=""https://pytorch.org/docs/stable/generated/torch.einsum.html"" rel=""nofollow noreferrer""><code>torch.einsum</code></a> to show the explicit expression:</p>
<pre><code>torch.einsum('bij,bjk-&gt;bik', A, B)
</code></pre>
<p>Note: <a href=""https://pytorch.org/docs/stable/generated/torch.bmm.html"" rel=""nofollow noreferrer""><code>torch.bmm</code></a> works the same way as <a href=""https://pytorch.org/docs/stable/generated/torch.matmul.html"" rel=""nofollow noreferrer""><code>matmul</code></a> but does not broadcast: <code>A.bmm(B)</code>.</p>
","2024-03-13 10:19:08","1","Answer"
"78152817","78152760","","<p><code>torch.bmm</code> is what your need, although <code>torch.matmul</code> should be equivalent in your case. I think you should recheck your computation.</p>
","2024-03-13 10:00:39","1","Answer"
"78152760","","Pytorch - do matrix multiplications from slices of 2 tensors","<p>If there are 2 tensors of the following sizes.</p>
<pre><code>A = [N x L x T]

B = [N x T x K]
</code></pre>
<p>Then I would like to do a <code>matrix multiplication</code> of slices from the 2 tensors. like below.</p>
<p><code>matmul_slice = A[0,:,:] @ B[0,:,:] = [L x T] @ [T x K] = [L x K]</code></p>
<p>Then I would like to do it <code>N</code> times along the <code>dimension = 0</code>.
So that I end up with the final matrix with size <code>[N,L,K]</code></p>
<p>I <strong>do not</strong> want to use loop over N since it <em>slows</em> down the computation. I have been playing around with <code>torch.matmul</code> and <code>einsum</code>, but I cannot get the correct answer.</p>
<p>How can I achieve this in a compact way?</p>
","2024-03-13 09:52:30","0","Question"
"78151471","","Neural Style Transfer Unsatisfactory Results","<p>I am trying to use neural style transfer(NST) to visualize the results of remodelling of a home's interiors and how it will look as per the user's style image. While the code I use gives me good results most of the time, it occasionally gives me noisy and irrelevant results. What can be the cause of this phenomenon?</p>
<p><a href=""https://i.sstatic.net/krOeW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/krOeW.jpg"" alt=""This is an image where the NST has produced decent results and is able to transfer the stye quite well"" /></a></p>
<p>The above is an image where the NST has produced decent results and is able to transfer the stye quite well</p>
<p><a href=""https://i.sstatic.net/LDvws.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LDvws.jpg"" alt=""This is another situation where the NST process has failed terribly and is only outputting noisy results"" /></a></p>
<p>This is another situation where the NST process has failed terribly and is only outputting noisy results</p>
<p>Is there any technical name for this occurrence? Technical papers and articles on why this occurs and how to solve this will be greatly appreciated.</p>
","2024-03-13 05:43:05","0","Question"
"78150899","78150886","","<p>The second one should be the right statement. You should change to this</p>
<pre><code>plt.imshow(img.squeeze().numpy(), cmap=&quot;gray&quot;)
</code></pre>
","2024-03-13 02:08:54","0","Answer"
"78150886","","Issues between PyTorch DataLoader and Matplotlib's Imshow for Image Classification Task","<p>I am currently working on a binary classification task involving image data. To begin, it is essential for me to inspect my dataset. However, I have encountered an issue with the <code>DataLoader</code>.</p>
<p>On the official PyTorch website, there is written like this</p>
<pre><code>training_data = datasets.FashionMNIST(
    root=&quot;data&quot;,
    train=True,
    download=True,
    transform=ToTensor()
)

labels_map = {
    0: &quot;T-Shirt&quot;,
    1: &quot;Trouser&quot;,
    2: &quot;Pullover&quot;,
    3: &quot;Dress&quot;,
    4: &quot;Coat&quot;,
    5: &quot;Sandal&quot;,
    6: &quot;Shirt&quot;,
    7: &quot;Sneaker&quot;,
    8: &quot;Bag&quot;,
    9: &quot;Ankle Boot&quot;,
}
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(labels_map[label])
    plt.axis(&quot;off&quot;)
    plt.imshow(img.squeeze(), cmap=&quot;gray&quot;)
plt.show()
</code></pre>
<p>When they set <code>training data</code>, they transformed data type to tensor. And they just use imshow(matplotlib). But when i try this process on my own, the error <code>TypeError: pic should be PIL Image or ndarray. Got &lt;class 'torch.Tensor'&gt;</code> bother me.</p>
<p>When I ask this to GPT4, it said &quot;PyTorch and matplotlib are compatible.&quot; However, when I inquired again with my code provided, it mentioned, &quot;You need to convert the PyTorch tensor to a NumPy array before using imshow.&quot; Which one is the accurate statement?</p>
","2024-03-13 02:03:19","0","Question"
"78150721","78150478","","<p>Update - think I solved it myself. Kind of dumb, but I just upgrade the Python version in my Dockerfile. I'm sure this results in something breaking, but I haven't run into it yet.</p>
<pre><code>FROM pytorch/pytorch

RUN conda update -n base -c defaults conda &amp;&amp; \
    conda install -y python=3.11 &amp;&amp; \
    conda update --all --yes

CMD [&quot;python&quot;, &quot;--version&quot;]
</code></pre>
","2024-03-13 00:48:48","0","Answer"
"78150556","78147543","","<p>I am not sure you could overwrite the modules when they are loaded in. What you can do though is wrap the <code>nn.Module</code> with a function that will go through the module tree and replace <code>nn.Conv2d</code> with another layer implementation (for example here <code>nn.Identity</code>). The only trick is the fact child layers can be identified by compound keys. For example <code>models.layer1[0].conv2</code> has keys <code>&quot;layer1&quot;</code>, <code>&quot;0&quot;</code>, and finally <code>&quot;conv2&quot;</code>.</p>
<p>Gather the <code>nn.Conv2d</code> and split their compound keys:</p>
<pre><code>convs = []
for k, v in model.named_modules():
    if isinstance(v, nn.Conv2d):
        convs.append(k.split('.'))
</code></pre>
<p>Build a recursive function to get a sub module from a compound key:</p>
<pre><code>inspect = lambda m, k: inspect(getattr(m, k[0]), k[1:]) if len(k)&gt;1 else m
</code></pre>
<p>Finally, you can iterate over the submodules and replace the layer:</p>
<pre><code>for k in convs:
    setattr(inspect(model, k), k[-1], nn.Identity())
</code></pre>
<p>You will see all <code>nn.Conv2d</code> layers (whatever their depth) will be replaced:</p>
<pre><code>&gt;&gt;&gt; model.layer1[0].conv2
Identity()
</code></pre>
<hr />
<p>If you want to access the parameters of the conv layer you are about to replace, you can check <a href=""https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/conv.py#L103-L112"" rel=""nofollow noreferrer"">its attributes</a>:</p>
<pre><code>keys = 'in_channels', 'out_channels', 'kernel_size', \
       'stride', 'padding', 'dilation', 'groups', \
       'bias', 'padding_mode'

for k in convs:
    parent = inspect(model, k)
    conv = getattr(parent, k[-1])
    setattr(parent, k[-1], nn.Conv2d(**{k: getattr(conv,k) for k in keys}))
</code></pre>
","2024-03-12 23:42:31","2","Answer"
"78150478","","Installing PyTorch with Python version 3.11 in a Docker container","<p>I see on the official PyTorch page that PyTorch supports Python versions 3.8 to 3.11.</p>
<p>When I actually try to install PyTorch + CUDA in a Python 3.11 Docker image, it seems unable to find CUDA drivers, e.g.</p>
<pre><code>FROM python:3.11.4
RUN --mount=type=cache,id=pip-build,target=/root/.cache/pip \
    pip install torch torchaudio
ENV PATH=&quot;/usr/local/nvidia/bin:${PATH}&quot; \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=all
</code></pre>
<p>Then, inside the container, I see that <code>torch.version.cuda</code> is <code>None</code></p>
<p>Compare this to</p>
<pre><code>FROM pytorch/pytorch
RUN --mount=type=cache,id=pip-build,target=/root/.cache/pip \
    pip install torchaudio
ENV PATH=&quot;/usr/local/nvidia/bin:${PATH}&quot; \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=all
</code></pre>
<p>Inside the container I see that <code>torch.version.cuda</code> is <code>12.1</code></p>
<p>PyTorch claims they're compatible with Python 3.11, has anybody actually been able to use PyTorch+CUDA with Python 3.11?</p>
<p>Tried running Docker images with Python 3.11.4</p>
<p>Tried running the Conda docker image and installing pytorch, but kept getting errors that the images couldn't be found</p>
","2024-03-12 23:14:46","1","Question"
"78150239","78149421","","<p>I had input of shape <code>[#batches, height, width] = [4,180,320]</code>.  I wanted to unfold them in sequence of <code>p</code> smaller patches of shape <code>h_p x w_p</code> yielding tensor of shape <code>4 x p x h_p x w_p</code>. Notice that to cover all <code>h x w = 180 x 320</code> elements using patch of size <code>h_p x w_p = 64 x 64</code>, I will need <code>p = 3 x 5 = 15</code> patches:<br />
<a href=""https://i.sstatic.net/W4VwA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/W4VwA.png"" alt=""enter image description here"" /></a></p>
<p>So, I added padding of 6 rows on both sides, top and bottom. Rest of the code I have explained in comments:</p>
<pre><code>patch_size = (64,64)
input = torch.randn(4,180,320)

# Padding 6 rows on top and bottom, to make up total padding of 12 rows, 
# so that our frame will become of size 192 x 320 and we can fit 3
# kernels of size 64 x 64 vertically
input = f.pad(input, pad=(0,0,6,6))
print(input.shape) # [4,192,320]

# add additional dimension indicating single channel
input = input.unsqueeze(1) # [4,1,192, 320]
print(input.shape)

# unfold with both stride and kernel size of 64 x 64
unfold = torch.nn.Unfold(kernel_size=patch_size, stride=(64,64))
unfolded = unfold(input)
print(unfolded.shape) # [4, 4096, 15] 
# 4 for batch size
# 4096 = 64 x 64 elements in one patch
# 15 = we can fit 15 patches of size 64 x 64 in frame of size 192 x 329

# reshape result to desired size
# size(0) = 4 = batch size
# -1 to infer p or number of patches, by our calculations it will be 15
# *patch_size = 64 x 64
unfolded = unfolded.view(unfolded.size(0),-1,*patch_size) 
print(unfolded.shape) # [4, 15, 64, 64]
</code></pre>
<p>This correctly output:</p>
<pre><code>torch.Size([4, 192, 320])
torch.Size([4, 1, 192, 320])
torch.Size([4, 4096, 15])
torch.Size([4, 15, 64, 64]
</code></pre>
","2024-03-12 21:57:53","2","Answer"
"78149836","78139855","","<h2>In Short</h2>
<p>The main &quot;error&quot; is from <code>pooled_sentence = torch.max(last_hidden_state, dim=1)</code>. Step back think a little what the <code>torch.max</code> is doing and what are you &quot;pooling&quot;.</p>
<hr />
<h2>In Long</h2>
<p>Depending on your longest sentence in the batch your token length will be different, so when you do a <code>torch.max</code>, since the max token is different, you'll get different sizes.</p>
<pre><code>
texts = [&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;]
last_hidden_state = model(input_ids=tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True).input_ids).last_hidden_state

second_sentence_embeddings_1 = last_hidden_state[1]


texts = [&quot;hello world&quot;, &quot;foo bar&quot;]
last_hidden_state = model(input_ids=tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True).input_ids).last_hidden_state

second_sentence_embeddings_2 = last_hidden_state[1]

second_sentence_embeddings_1.shape, second_sentence_embeddings_2.shape

</code></pre>
<p>[out]:</p>
<pre><code>(torch.Size([10, 768]), torch.Size([4, 768]))
</code></pre>
<h3>First, solve the differing batch size outputs by using <code>pipeline</code></h3>
<pre><code>from transformers import pipeline

pipe = pipeline(task=&quot;feature-extraction&quot;, model=&quot;google/mt5-base&quot;, framework=&quot;pt&quot;)

# See https://github.com/huggingface/transformers/issues/20404
pipe.model = pipe.model.encoder 

hello_world = pipe([&quot;hello world&quot;, &quot;foo bar&quot;], return_pt=True)

batch_mode = pipe([&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;], return_pt=True)


assert hello_world[1] == batch_mode[1]
</code></pre>
<h3>But how do I get a vector of 768 dimensions per sentence?</h3>
<p>TL;DR:</p>
<pre><code>from transformers import MT5EncoderModel, AutoTokenizer
from transformers import FeatureExtractionPipeline


class LuigiThePlumber(FeatureExtractionPipeline):
    def postprocess(self, model_outputs):
        # If you just want it to return a torch.return_types.max, 
        # instead of plain tensor use 
        # `return torch.max(model_outputs.last_hidden_state, dim=1)`
        return torch.max(model_outputs.last_hidden_state, dim=1).values

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

pipe = LuigiThePlumber(task=&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, framework=&quot;pt&quot;)

# See https://github.com/huggingface/transformers/issues/20404
pipe.model = pipe.model.encoder 


out = pipe([&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;])

print(out[0].shape, out[1].shape, out[2].shape)
</code></pre>
<p>[out]:</p>
<pre><code>torch.Size([1, 768]) torch.Size([1, 768]) torch.Size([1, 768])
</code></pre>
<h3>Then, when didn't my method work but the pipeline does?</h3>
<p>It's because when you have a batch, there's an attention mask that's being computed when sentences that are shorter than the max length of the batch is padded to the max. So when you try to extract the last hidden state, you'll have to account for the attention mask.</p>
<p>When you use <code>pipeline</code> it standardizes the batch size you put into the dataset. And usually it's set to 1.</p>
<h3>But I really want to do batch_size larger than 1!</h3>
<p>First, see:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/65083581/how-to-compute-mean-max-of-huggingface-transformers-bert-token-embeddings-with-a"">How to compute mean/max of HuggingFace Transformers BERT token embeddings with attention mask?</a></li>
<li><a href=""https://www.philschmid.de/custom-inference-huggingface-sagemaker"" rel=""nofollow noreferrer"">https://www.philschmid.de/custom-inference-huggingface-sagemaker</a></li>
</ul>
<p>Then you'll have to take in the attention mask when computing the max-pooling.</p>
<pre><code>from transformers import MT5EncoderModel, AutoTokenizer

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

texts = [&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;]

encoded_input = tokenizer(texts, padding=True, return_tensors='pt')
model_output = model(input_ids=encoded_input.input_ids)
attention_mask = encoded_input['attention_mask']


def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


def max_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = torch.where(attention_mask==0, -1e-9, 0.).unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.max(token_embeddings-input_mask_expanded, 1).values


mean_pooling(model_output, attention_mask).shape, max_pooling(model_output, attention_mask).shape
</code></pre>
<p>[out]:</p>
<pre><code>(torch.Size([3, 768]), torch.Size([3, 768]))
</code></pre>
<p><em><strong>Q: But are the value of the pooled embeddings with different batch the same?</strong></em></p>
<p>A:</p>
<pre><code>from transformers import MT5EncoderModel, AutoTokenizer

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

texts = [&quot;hello world&quot;, &quot;foo bar&quot;, &quot;this is a foo bar sentence&quot;]

encoded_input = tokenizer(texts, padding=True, return_tensors='pt')
model_output = model(input_ids=encoded_input.input_ids)
attention_mask = encoded_input['attention_mask']


def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


def max_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = torch.where(attention_mask==0, -1e-9, 0.).unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.max(token_embeddings-input_mask_expanded, 1).values


x = max_pooling(model_output, attention_mask)

text = [&quot;hello world&quot;, &quot;foo bar&quot;]

encoded_input = tokenizer(texts, padding=True, return_tensors='pt')
model_output = model(input_ids=encoded_input.input_ids)
attention_mask = encoded_input['attention_mask']

y = max_pooling(model_output, attention_mask)

# Lets check all values in the &quot;embeddings&quot; of &quot;hello World&quot; in both batch sizes.
assert all(v for v in x[0] == y[0])
</code></pre>
","2024-03-12 20:21:06","1","Answer"
"78149543","78149533","","<p>What finally worked for me was:</p>
<pre><code>pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/index.html
</code></pre>
","2024-03-12 19:13:00","0","Answer"
"78149533","","ERROR: Could not build wheels for mmcv-full, which is required to install pyproject.toml-based projects","<p>python = 3.10</p>
<p>pytorch = 2.2.1+cu118</p>
<p>pip install mmcv-full</p>
<pre><code>    running install_scripts
      C:\Users\omras\miniconda3\envs\transseg1\lib\site-packages\wheel\bdist_wheel.py:109: RuntimeWarning: Config variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect
        if get_flag(&quot;Py_DEBUG&quot;, hasattr(sys, &quot;gettotalrefcount&quot;), warn=(impl == &quot;cp&quot;)):
      error: [Errno 2] No such file or directory: 'LICENSE'
  ERROR: Failed building wheel for mmcv-full
  Running setup.py clean for mmcv-full
Failed to build mmcv-full
ERROR: Could not build wheels for mmcv-full, which is required to install pyproject.toml-based projects
</code></pre>
","2024-03-12 19:11:19","0","Question"
"78149421","","Unfolding tensor containing image into patches","<p>I have a batch of size <code>4</code> of size <code>h x w = 180 x 320</code> single channel images. I want to unfold them series of <code>p</code> smaller patches of shape <code>h_p x w_p</code> yielding tensor of shape <code>4 x p x h_p x w_p</code>. If <code>h</code> is not divisible for <code>h_p</code>, or <code>w</code> is not divisible for <code>w_p</code>, the frames will be 0-padded. I tried following to achieve this:</p>
<pre><code>import torch
tensor = torch.randn(4, 180, 320)
patch_size = (64, 64) #h_p = w_p = 64
unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size, padding=0)
unfolded = unfold(tensor)
print(unfolded.shape)
</code></pre>
<p>It prints:</p>
<pre><code>torch.Size([16384, 10])
</code></pre>
<p>What I am missing here?</p>
<p><strong>PS:</strong></p>
<p>I guess I have found the solution myself which I have posted below. I am yet to evaluate it fully. But let me know if you find it wrong or poor in any sense, may be performance</p>
","2024-03-12 18:49:39","1","Question"
"78147744","78104756","","<p>Scheduling successive workoads on multi GPU environments can be quite a pain, especially for workloads with multi-phase dependencies. If you are still encountering challenges around it, Run:ai is an alternative that helps companies manage workloads efficiently and schedule them automatically on clusters. We can chat on Linkedin if you think it might be useful</p>
","2024-03-12 13:58:39","-3","Answer"
"78147725","","What should collator do exactly?","<p>Suppose we have an audio classification task (AudioMNIST).</p>
<p>My pipeline and other pipelines I’ve seen consist of the next steps:</p>
<ol>
<li>Read the dataset (the data samples).</li>
<li>Do the base transforms (merge the audio channels, change the bitrate, etc).</li>
<li>Split the dataset into the train one, the test one, etc.</li>
<li>Do the main transforms (different for the train and the test) such as the augmentation.</li>
<li>Batch (along with the sampling).</li>
<li>Pad/Truncate the batch samples.</li>
<li>Do the forward pass with the batch.</li>
<li>&lt;…&gt;</li>
</ol>
<p>I saw the scheme:</p>
<ul>
<li>Dataset or a subclass - pp. 1., 2., 3., 4.</li>
<li>Collator - p. 6.</li>
</ul>
<p>Either:</p>
<ul>
<li>Dataset or a subclass - p. 1.</li>
<li>somebody else - pp. 2., 3., 4.</li>
<li>Collator - p. 6.</li>
</ul>
<p>Or:</p>
<ul>
<li>Dataset or a subclass - p. 1.</li>
<li>somebody else - p. 3.</li>
<li>Collator - pp. 2., 4., 6.</li>
</ul>
<p>What should the collator do and what shouldn’t? (The main question.)
What is the correct scheme?</p>
","2024-03-12 13:56:08","1","Question"
"78147543","","How to overwrite nn.conv2d","<p>I want to overwrite nn.conv2d so prepared models such as resnet, alexnet etc. can use it without changing the all nn.conv2ds in the model manually.</p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import models
from torch import nn

class replace_conv2d(nn.Module):
      # other codes

nn.conv2d = replace_conv2d # what I want to do
model = models.resnet18()
</code></pre>
<p>so resnet18 will use the replace_conv2d class instead of nn.conv2d</p>
","2024-03-12 13:31:21","1","Question"
"78145911","78143253","","<p>That's odd, <code>pip3 install torchrl</code> works on my end.
The issues seems to come from audio/text/vision which have been installed previously
Does</p>
<pre><code>!pip3 install torchrl torchvision torchaudio torchtext torchdata -U
</code></pre>
<p>Solves your issue? You could also consider removing a couple of these if you're not using them</p>
","2024-03-12 09:22:03","0","Answer"
"78143253","","Can't install torchrl into Google Colab after torch 2.2.1","<p>I am using RL environments in Google Colab with the <code>torchrl</code> library. The following code used to work:</p>
<pre><code>!pip3 install torchrl
</code></pre>
<p>Now, I get the following error from running that line at the beginning of the notebook:</p>
<pre><code>Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, tensordict, torchrl
  Attempting uninstall: triton
    Found existing installation: triton 2.1.0
    Uninstalling triton-2.1.0:
      Successfully uninstalled triton-2.1.0
  Attempting uninstall: torch
    Found existing installation: torch 2.1.0+cu121
    Uninstalling torch-2.1.0+cu121:
      Successfully uninstalled torch-2.1.0+cu121

ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.
torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.
torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.
torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.

</code></pre>
<p>The installation also runs much more slowly. I think that the issue has something to do with the fact that torch 2.2.1 was released recently, but I am not sure how to resolve it (running <code>!pip3 install torch==2.1.0</code> first does not work as it is uninstalled for some reason). Any insight is appreciated. Thanks so much!</p>
","2024-03-11 20:26:43","0","Question"
"78143186","","How to run juggernaut model in local","<p>I want to run fine-tuned stable diffusion models in my local pc using python. For example juggernaut: <a href=""https://huggingface.co/RunDiffusion/Juggernaut-XL-v9"" rel=""nofollow noreferrer"">https://huggingface.co/RunDiffusion/Juggernaut-XL-v9</a></p>
<p>This is my code (it works with stable-diffusion-xl-base-1.0):</p>
<pre><code>import random
from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline
import torch
import gc
import time

# for cleaning memory
gc.collect()
torch.cuda.empty_cache()

start_time = time.time()

model = &quot;RunDiffusion/Juggernaut-XL-v9&quot;
pipe = DiffusionPipeline.from_pretrained(
    model,
    torch_dtype=torch.float16,
)

pipe.to(&quot;cuda&quot;)

prompt = (&quot;a portrait of male as a knight in middle ages, masculine looking, battle in the background, sharp focus, highly detailed, movie-style lighting, shadows&quot;)
seed = random.randint(0, 2**32 - 1)

generator = torch.Generator(&quot;cuda&quot;).manual_seed(seed)
image = pipe(prompt=prompt, generator=generator, num_inference_steps=1)
image = image.images[0]
image.save(f&quot;output_images/{seed}.png&quot;)

end_time = time.time()

total_time = end_time - start_time
minutes = int(total_time // 60) 
seconds = int(total_time % 60) 

print(f&quot;Took: {minutes} min {seconds} sec&quot;)
print(f&quot;Saved to output_images/{seed}.png&quot;)

</code></pre>
<p>But I am getting:</p>
<blockquote>
<p>OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory</p>
</blockquote>
<p>Maybe because of python, cuda versions. I'm dropping down my libraries versions:</p>
<p>Python 3.9.0</p>
<p>PyTorch: 2.2.0+cu118</p>
<p>CUDA : 11.8</p>
<p>Diffusers: 0.26.3</p>
<p>Transformers: 4.38.1</p>
","2024-03-11 20:12:01","1","Question"
"78142012","","ONNX export of Seq2Seq model - issue with decoder input length","<p>I have a simple Seq2Seq model trained according to &quot;Attention is all you need&quot; and implemented using PyTorch. The model works fine. I decided to export it to ONNX. I exported the encoder and decoder separately. When using the ONNX model, the encoder works fine. However, the decoder only works for the same length of input sequence for which it was exported. For all other lengths, it ends with an error:
<code>The input tensor cannot be reshaped to the requested shape. Input shape:{2,1,300}, requested shape:{1,20,15}</code>
The embedding size is 300.
I don't think this is a problem with dynamic axes, as I set them correctly after the first failure. I tried to solve the problem by using a constant input length for the decoder and applying a mask, but this resulted in nonsensical output. Thank you in advance for any tips.</p>
","2024-03-11 16:05:01","0","Question"
"78141648","78139855","","<p>Can you try to set the model to evaluation mode by calling <code>model.eval()</code>. Normally model is set to train mode when it's initialized in which the dropout (randomly) and normalization is activate, we need to deactivate it in evaluation mode.</p>
","2024-03-11 15:07:37","0","Answer"
"78141144","78082292","","<p>If I run through the batch one by one or by creating a batch, I can get the same numerical results from the neural network. I then have to reshape the output to make it fit the expected output. I find it weird that PyTorch Geometric does not do this automatically. I don't know if this is the &quot;correct&quot; way of doing it. However, this seems to be the best alternative/solution.</p>
<pre><code># setup example
batch_size = 3
num_nodes = 3
memory = np.zeros(batch_size, dtype=object)

# fill memory
for i in range(batch_size):
    memory[i] = random_pyg_graph(num_nodes=num_nodes)

# define model
CNN = DeepNetworkGCN()

# test for single PyG
for i in range(len(memory)):
    output = CNN.forward(memory[i])
    print(output)
# tensor([[-0.1082],
#         [-0.1337],
#         [-0.1323]], grad_fn=&lt;AddmmBackward0&gt;)
# tensor([[-0.0894],
#         [-0.0903],
#         [-0.0789]], grad_fn=&lt;AddmmBackward0&gt;)
# tensor([[-0.1073],
#         [-0.1131],
#         [-0.1131]], grad_fn=&lt;AddmmBackward0&gt;)

# Create batch and do forward pass.
output = CNN.forward(Batch.from_data_list(memory[:]))
print(output)

# tensor([[-0.1082],
#         [-0.1337],
#         [-0.1323],
#         [-0.0894],
#         [-0.0903],
#         [-0.0789],
#         [-0.1073],
#         [-0.1131],
#         [-0.1131]], grad_fn=&lt;AddmmBackward0&gt;)

print(output.reshape(batch_size, num_nodes))

# tensor([[-0.1082, -0.1337, -0.1323],
#         [-0.0894, -0.0903, -0.0789],
#         [-0.1073, -0.1131, -0.1131]], grad_fn=&lt;ViewBackward0&gt;)
</code></pre>
","2024-03-11 13:51:26","0","Answer"
"78140896","78140043","","<p>Permute the axes of your tensor:</p>
<pre><code>x = x.permute(0,3,1,2)
</code></pre>
","2024-03-11 13:18:09","1","Answer"
"78140544","78139464","","<p>The error is caused because the parameter is moved to <code>&lt;param&gt;_orig</code> and the masked value is stored alongside it.
When the SyncDataCollector takes the params and buffers out and puts them on &quot;meta&quot; device to create a stateless policy, these additional values are ignored because they're not parameters anymore (and hence not caught by the call to <code>&quot;to&quot;</code>).</p>
<p>What you can do as a fix is to call</p>
<pre><code>policy_module.module[0].weight = policy_module.module[0].weight.detach()
</code></pre>
<p>before creating the collector.
That should be ok because the <code>weight</code> attribute will be recomputed during the next forward call anyway.</p>
<p>TorchRL should maybe handle better the deepcopy, although in this case the error is caused by a tensor requiring gradients at a place where it shouldn't. IMO the pruning methods should compute the <code>&quot;weight&quot;</code> during forward call (as they do) but then prune</p>
","2024-03-11 12:14:53","1","Answer"
"78140043","","How do i change the input shape of a pytorch resnet50 model before training to 224, 224, 3 from 3, 224, 224","<p>How do i change the input shape of the pytorch resnet50 model before training on my dataset</p>
<p>I faced an error when i converted the trained model to .tflite format to use in a flutter app, which basically wanted me to change the input tensor of my model to 1, 224, 224, 3 from 1, 3, 224, 224.</p>
","2024-03-11 10:47:20","0","Question"
"78140017","78135427","","<p>Each situation is unique. But here are some pointers:</p>
<ol>
<li>You only want to use enough parallelism that there is no oversubscription.</li>
<li>Parallelism involves overhead from thread creation, sending jobs to different workers, etc. If the overhead is too much then it may be better to do serial calculation</li>
<li>Vectorization often furnishes benefits. This is where your objective function gets sent a lot of parameter vectors at once. If you can efficiently deal with this (including implementing your own parallisation), then this may be better than differential_evolution doing the parallel computation.</li>
<li>Sometimes it's better to parallelise within your own objective function than to parallelise the solver.</li>
<li>If bottle neck is computation think of implementing your objective function in Cython/C. However, for matrix multiplication of large matrices you need to be using BLAS.</li>
</ol>
","2024-03-11 10:42:51","0","Answer"
"78139855","","Why do I get different embeddings when I perform batch encoding in huggingface MT5 model?","<p>I am trying to encode some text using HuggingFace's mt5-base model. I am using the model as shown below</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import MT5EncoderModel, AutoTokenizer

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

def get_t5_embeddings(texts):
    last_hidden_state = model(input_ids=tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True).input_ids).last_hidden_state
    pooled_sentence = torch.max(last_hidden_state, dim=1)
    return pooled_sentence[0].detach().numpy()
</code></pre>
<p>I was doing some experiments when I noticed that the same text had a low cosine similarity score with itself. I did some digging and realized that the model was returning very different embeddings if I did the encoding in batches. To validate this, I ran a small experiment that generated embeddings for <code>Hello</code> and a list of 10 <code>Hello</code>s incrementally. and checking the embeddings of the <code>Hello</code> and the first <code>Hello</code> in the list (both of which should be same).</p>
<pre class=""lang-py prettyprint-override""><code>for i in range(1, 10):
    print(i, (get_t5_embeddings([&quot;Hello&quot;])[0] == get_t5_embeddings([&quot;Hello&quot;]*i)[0]).sum())
</code></pre>
<p>This will return the number of values in the embeddings that match each other.
This was the result:</p>
<pre><code>1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27
</code></pre>
<p>Every time I run it, I get mismatches if the batch size is more than 768.</p>
<p>Why am I getting different embeddings and how do I fix this?</p>
","2024-03-11 10:14:53","1","Question"
"78139837","78139177","","<p>Why don't you want to use the <code>retain_graph=True</code>? Your case can be solved using different optimizers and <code>.zero_grad()</code> on <code>unet</code> before calling <code>.backward()</code> the second time to optimize <code>unet</code>. It should be like:</p>
<pre><code>ten_c.zero_grad() # model can call .zero_grad() 
loss_tenc.backward(retain_graph=True)
ten_c_optimizer.step() # or equivalence with scaler.

unet.zero_grad() # model can call .zero_grad() 
loss_unet.backward()
unet_optimizer.step() # or equivalence with scaler.

# zero_grad() both for safety
ten_c.zero_grad()
unet.zero_grad()
</code></pre>
<p>If you want to use just one optimizer, then calculate the gradient of <code>ten_c</code> first then freeze the weight of <code>ten_c</code> and <code>zero_grad()</code> the <code>unet</code> before calling the second <code>.backward()</code>:</p>
<pre><code>ten_c.zero_grad() # model can call .zero_grad() 
loss_tenc.backward(retain_graph=True)
for p in ten_c.parameters():
    p.requires_grad = False # freeze the weight so the gradient will not be updated on the second `.backward()`

unet.zero_grad() # model can call .zero_grad() 
loss_unet.backward() # Only calculate gradient for unet as ten_c have been freeze

for p in ten_c.parameters():
    p.requires_grad = True

optimizer.step() # or equivalence with scaler.


# zero_grad() both for safety
ten_c.zero_grad()
unet.zero_grad()
</code></pre>
","2024-03-11 10:12:14","0","Answer"
"78139513","78139177","","<p>Considering both components are connected to <code>model_pred</code>, you could backpropagate a single time by summing both loss terms together:</p>
<pre><code>loss_tenc = loss.mean()
loss_unet = (loss * mask).mean()

scaler.scale(loss_tenc + loss_unet).backward()
</code></pre>
","2024-03-11 09:15:12","0","Answer"
"78139464","","How to solve deepcopy error of a pruned model in pytorch","<p>I am trying to build a RL model, where my actor network has some pruned connections.
When using the data collector SyncDataCollector from torchrl, the deepcopy fails (see error below).</p>
<p>This seems to be due to the pruned connections, which sets the pruned layers with gradfn (and not requires_grad=True) as suggested in <a href=""https://stackoverflow.com/questions/56590886/how-to-solve-the-run-time-error-only-tensors-created-explicitly-by-the-user-gr"">this post</a>.</p>
<p>Here is an example of code I would like to run, where SyncDataCollector attempts a deepcopy of the model,</p>
<pre><code>device = torch.device(&quot;cpu&quot;)

model = nn.Sequential(
    nn.Linear(1,5),
    nn.Linear(5,1)
)
mask = torch.tensor([1,0,0,1,0]).reshape(-1,1)
prune.custom_from_mask(model[0], name='weight', mask=mask)


policy_module = TensorDictModule(
    model, in_keys=[&quot;in&quot;], out_keys=[&quot;out&quot;]
)

env = FlyEnv()

collector = SyncDataCollector(
    env,
    policy_module,
    frames_per_batch=1,
    total_frames=2,
    split_trajs=False,
    device=device,
)
</code></pre>
<p>And here is a minimal example producing the error</p>
<pre><code>import torch
from torch import nn
from copy import deepcopy

import torch.nn.utils.prune as prune

device = torch.device(&quot;cpu&quot;)

model = nn.Sequential(
    nn.Linear(1,5),
    nn.Linear(5,1)
)
mask = torch.tensor([1,0,0,1,0]).reshape(-1,1)
prune.custom_from_mask(model[0], name='weight', mask=mask)

new_model = deepcopy(model)
</code></pre>
<p>where the error is</p>
<pre><code>RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001
</code></pre>
<p>I tried to remove the pruning with <code>prune.remove(model[0], 'weight')</code> and then setting <code>model[0].requires_grad_()</code>, which fixes the result but then all the weights are trained...</p>
<p>I think it might work to mask the pruned weights &quot;manually&quot;, by masking them before each forward pass, but it does not seem efficient (nor elegant).</p>
","2024-03-11 09:07:35","0","Question"
"78139451","78137887","","<p>This requires a little code inspection but you can easily find the implementation if you look in the right places. Let us start with your snippet.</p>
<ul>
<li><p>The <code>my_forward_wrapper</code> function is a function generator that defines <code>my_forward</code> and returns it. This implementation is overwriting the implementation of the last block attention layer <code>blocks[-1].attn</code> of the loaded model <code>&quot;deit_small_distilled_patch16_224&quot;</code>.</p>
<pre><code>model.blocks[-1].attn.forward = my_forward_wrapper(model.blocks[-1].attn)
</code></pre>
</li>
<li><p>What the <code>x</code> corresponds to is the output of the previous block. To understand, you can dive into the source code of <a href=""https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/deit.py"" rel=""nofollow noreferrer"">timm</a>. The model loaded in the script is <a href=""https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/deit.py#L295-L302"" rel=""nofollow noreferrer""><code>deit_small_distilled_patch16_224</code></a> which returns a <a href=""https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/deit.py#L29"" rel=""nofollow noreferrer""><code>VisionTransformerDistilled</code></a> instance. The blocks are defined in the <a href=""https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L507-521"" rel=""nofollow noreferrer""><code>VisionTransformer</code></a> class. There are <code>n=depth</code> blocks defined sequentially. The default block definition is given by <a href=""https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L123"" rel=""nofollow noreferrer""><code>Block</code></a> in which attn is implemented by <a href=""https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L57"" rel=""nofollow noreferrer""><code>Attention</code></a>, the details are given here:</p>
<pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    B, N, C = x.shape
    qkv = self.qkv(x) \
              .reshape(B, N, 3, self.num_heads, self.head_dim) \
              .permute(2, 0, 3, 1, 4)
    q, k, v = qkv.unbind(0)
    q, k = self.q_norm(q), self.k_norm(k)

    if self.fused_attn:
        x = F.scaled_dot_product_attention(
            q, k, v,
            dropout_p=self.attn_drop.p if self.training else 0.,
        )
    else:
        q = q * self.scale
        attn = q @ k.transpose(-2, -1)
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        x = attn @ v

    x = x.transpose(1, 2).reshape(B, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x
</code></pre>
<p>While the implementation - that you provided - overwriting it is:</p>
<pre><code>def my_forward(x):
    B, N, C = x.shape
    qkv = attn_obj.qkv(x) \
            .reshape(B, N, 3, attn_obj.num_heads, C // attn_obj.num_heads) \
            .permute(2, 0, 3, 1, 4)
    q, k, v = qkv.unbind(0)

    attn = (q @ k.transpose(-2, -1)) * attn_obj.scale
    attn = attn.softmax(dim=-1)
    attn = attn_obj.attn_drop(attn)
    attn_obj.attn_map = attn
    attn_obj.cls_attn_map = attn[:, :, 0, 2:]

    x = (attn @ v).transpose(1, 2).reshape(B, N, C)
    x = attn_obj.proj(x)
    x = attn_obj.proj_drop(x)
    return x
</code></pre>
<p>The idea is that the attention map is being cached as an attribute to the attention layer with <code>attn_obj.attn_map = attn</code>, such that it can be inspected after inference.</p>
</li>
</ul>
","2024-03-11 09:04:38","1","Answer"
"78139177","","Backprop two networks with different loss without retain_graph=True?","<p>I have two networks in sequence that perform an expensive computation.</p>
<p>The loss objective for both is the same, except for the second network's loss I want to apply a mask.</p>
<p>How to achieve this without using retain_graph=True?</p>
<pre><code># tenc          - network1
# unet          - network2

# the work flow is input-&gt;tenc-&gt;hidden_state-&gt;unet-&gt;output


params = []
params.append([{'params': tenc.parameters(), 'weight_decay': 1e-3, 'lr': 1e-07}])
params.append([{'params': unet.parameters(), 'weight_decay': 1e-2, 'lr': 1e-06}])
optimizer = torch.optim.AdamW(itertools.chain(*params), lr=1, betas=(0.9, 0.99), eps=1e-07, fused = True, foreach=False)
scheduler = custom_scheduler(optimizer=optimizer, warmup_steps= 30, exponent= 5, random=False)
scaler = torch.cuda.amp.GradScaler() 


loss = torch.nn.functional.mse_loss(model_pred, target, reduction='none')
loss_tenc = loss.mean()
loss_unet = (loss * mask).mean()

scaler.scale(loss_tenc).backward(retain_graph=True)
scaler.scale(loss_unet).backward()
scaler.unscale_(optimizer)

scaler.step(optimizer)
scaler.update()

scheduler.step()
optimizer.zero_grad(set_to_none=True)
</code></pre>
<p>The loss_tenc should only optimize tenc parameters, and the loss_unet only unet. I may have to use two different optimizers if necessary, but I grouped them into one here for simplicity.</p>
","2024-03-11 08:09:23","0","Question"
"78137887","","This code runs perfectly but I wonder what the parameter 'x' in my_forward function refers to","<p>refering to the attention maps in VIT transformers example in: <a href=""https://github.com/huggingface/pytorch-image-models/discussions/1232?sort=old"" rel=""nofollow noreferrer"">https://github.com/huggingface/pytorch-image-models/discussions/1232?sort=old</a></p>
<p>This code runs perfectly but I wonder what the parameter 'x' in my_forward function refers to. and How and where in the code the x value is passed to the function my_forward.</p>
<pre><code>def my_forward(x):
        B, N, C = x.shape

        qkv = attn_obj.qkv(x).reshape(
            B, N, 3, attn_obj.num_heads, C // attn_obj.num_heads).permute(2, 0, 3, 1, 4)
        # make torchscript happy (cannot use tensor as tuple)
        q, k, v = qkv.unbind(0) 
</code></pre>
","2024-03-11 00:05:50","-1","Question"
"78136587","78021371","","<p>To handle this in-place you can simply flatten <code>x</code>. You should <em>ravel</em> the indices such that they can index <code>x</code> when flattened. First, gather the indices, then index <code>x.flatten()</code> (fyi. <strong>not</strong> a copy).</p>
<pre><code>indices = torch.tensor([i*m+j for i,r in enumerate(list_of_indices) for j in r])
&gt; tensor([ 6,  7,  9, 24, 25, 26, 27, 32, 35])

x.flatten()[indices] = -1
&gt; tensor([[ 0,  1,  2,  3],
          [ 4,  5, -1, -1],
          [ 8, -1, 10, 11],
          [12, 13, 14, 15],
          [16, 17, 18, 19],
          [20, 21, 22, 23],
          [-1, -1, -1, -1],
          [28, 29, 30, 31],
          [-1, 33, 34, -1]])
</code></pre>
<p>You can also use <a href=""https://pytorch.org/docs/stable/generated/torch.scatter.html"" rel=""nofollow noreferrer""><code>torch.scatter_</code></a>, but in that case it is slightly longer to write:</p>
<pre><code>x.flatten().scatter_(0,indices,value=-1).view_as(x)
</code></pre>
","2024-03-10 16:15:40","3","Answer"
"78135427","","multiprocessing slows down when using torch","<p>I am using <code>differential_evolution</code> from <code>scipy</code> with <code>workers</code> to parallel the calculations. And I switch to <code>pytorch</code> from <code>numpy</code> to speed up the code.</p>
<pre><code>from torch.multiprocessing import set_start_method,Pool
if __name__ == '__main__':
    #device = get_device()
    device = torch.device('cpu') # testing with cpu
    num_workers=int(sys.argv[1])
    set_start_method(&quot;spawn&quot;,force=True)
    pool=Pool(num_workers)
    results = differential_evolution(likelihood, seed=np.random.seed(0),workers=pool.map,
                                             callback=print_de, bounds=bounds, maxiter=1500,
                                             disp=True,recombination=0.1,mutation=(0.9,1),
                                             constraints=NonlinearConstraint(positive_definite, lb=0, ub=np.inf),
                                             popsize=25,
                                             polish=False
                                             )
</code></pre>
<p>In my own laptop (m1 macbook) this works fine either using <code>torch.multiprocessing</code> or <code>multiprocessing</code> (simply set <code>workers=int(sys.argv[1])</code>).</p>
<p>When I test this in HPC with 256 cores in 1 node, it slows down a lot. Using <code>torch.multiprocessing</code> is faster than <code>multiprocessing</code>, but still one iteration is much slower than when I don't do parallel.</p>
<p>When I use <code>top</code>, I can see the correct number of <code>python</code> instances are running, but the cpu usage is more than 100%, some even with 1000% per <code>python</code>, can this be the problem? When I use <code>numpy</code> the cpu usage is almost 100%.</p>
<p>How can I solve the problem?</p>
","2024-03-10 09:52:51","0","Question"
"78134333","78133882","","<p>You are running out of RAM memory. You can check it in another terminal by running <code>watch free -h</code>, while running <code>pip install torch</code> in another terminal.</p>
<p>To fix it, I would recommend you create a swap file. Swap files act as additional RAM memory, but it's a bit slow as it runs from the hard drive. However they are useful when you want to allocate a memory peak, as it is your case. The following example shows how to create a 5GB swap file:</p>
<pre><code>sudo fallocate -l 1G ~/swapfile
sudo dd if=/dev/zero of=~/swapfile bs=1024 count=1048576
sudo chmod 600 ~/swapfile
sudo mkswap ~/swapfile
sudo swapon ~/swapfile
</code></pre>
<p>Keep in mind that t2.micro instances are very limited. Be mindful of what you do and consider upgrading to a larger instance if needed.</p>
","2024-03-10 00:10:06","3","Answer"
"78134287","78134230","","<p>My guess is that you're running out of memory. Sometimes the operating system is configured to automatically kill memory-intensive processes when the system is close to running out, and when this happens, there wouldn't necessarily be an error message printed to the terminal (although such a message would be recorded in a system log somewhere).</p>
<p>Note that batch size would affect GPU memory usage, but not necessarily CPU memory usage. Also note that memory usage can <a href=""https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662"" rel=""nofollow noreferrer"">grow over time</a> as more data has to be copied between data loader processes.</p>
<p>My first step to debug this would either be to find the relevant system logs, or to just monitor memory usage while the program is running (e.g. with <code>htop</code> or something).</p>
","2024-03-09 23:43:47","0","Answer"
"78134230","","PyTorch training script abruptly stops without any warning or error","<p>I am trying to run a training script in PyTorch for a model with 3dCNN and LSTM, but it abruptly stops after the 1st or 2nd batch. The script was earlier written for just the 3DCNN.</p>
<p>What could be the reason for this?</p>
<p>There is no error or warning messages. I have checked for all system exit statements, but none of them seem to be responsible. I have adjusted batch-size and checked, in case of a memory shortage issue as well.</p>
<p>The training completes the first batch but the model seems to not return any output in the second batch, causing the script to abruptly stop. How to possibly find what is causing this issue?</p>
<p>This is the architecture of the model I am trying to train:</p>
<pre><code>def __init__(self, num_classes):
    super(ConvLSTM, self).__init__()
    self.conv_layer1 = self._make_conv_layer(3, 64, (1, 2, 2), (1, 2, 2))
    self.conv_layer2 = self._make_conv_layer(64, 128, (2, 2, 2), (2, 2, 2))
    self.conv_layer3 = self._make_conv_layer(128, 256, (2, 2, 2), (2, 2, 2))
    self.conv_layer4 = self._make_conv_layer(256, 256, (2, 2, 2), (2, 2, 2))

    self.lstm = nn.LSTM(input_size=256, hidden_size=512, num_layers=1, batch_first=True)

    self.fc5 = nn.Linear(512, 512)
    self.fc5_act = nn.ELU()
    self.fc6 = nn.Linear(512, num_classes)

def _make_conv_layer(self, in_c, out_c, pool_size, stride):
    conv_layer = nn.Sequential(
        nn.Conv3d(in_c, out_c, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm3d(out_c),
        nn.ELU(),
        nn.MaxPool3d(pool_size, stride=stride, padding=0)
    )
    return conv_layer

def forward(self, x):
    # print(f&quot;input {x.size()}&quot;)
    x = self.conv_layer1(x)
    x = self.conv_layer2(x)
    x = self.conv_layer3(x)
    x = self.conv_layer4(x)
    
    x = x.permute(0, 2, 1, 3, 4).contiguous()
    batch_size, seq_len, input_size, height, width = x.size()
    x = x.view(batch_size, seq_len, -1)
    
    lstm_out, _ = self.lstm(x)
    lstm_out = lstm_out[:, -1, :]

    x = self.fc5(lstm_out)
    x = self.fc5_act(x)
    x = self.fc6(x)
    return x
</code></pre>
","2024-03-09 23:09:24","-1","Question"
"78134059","78133963","","<p>If you look carefully at the stack trace of the error raised by <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset"" rel=""nofollow noreferrer""><code>TensorDataset</code></a>, you will see:</p>
<blockquote>
<p><code>assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)</code></p>
</blockquote>
<p>This means all tensors must have the same size along the first dimension (the dataset size).</p>
<p>In your case, <strong>one</strong> <code>4x64x64</code> tensor corresponds to <strong>one</strong> 64x64 tensor, in other words, the inputs to TensorDataset must be shaped <code>(1,4,64,64)</code> and <code>(1,64,64)</code> for the input and output, respectively. Therefore you need to add an extra dimension on both (with <a href=""https://stackoverflow.com/questions/69797614/indexing-a-tensor-with-none-in-pytorch""><code>None</code> indexing</a> or <a href=""https://pytorch.org/docs/stable/generated/torch.unsqueeze.html"" rel=""nofollow noreferrer""><code>unsqueeze</code></a>):</p>
<pre><code>x = torch.rand(4,64,64)
y = torch.rand(64,64)

dataset = TensorDataset(x[None], y[None])
</code></pre>
","2024-03-09 21:55:27","0","Answer"
"78134048","77829919","","<p>I just encountered this problem. I looked into the for iopath files and found the culprit:</p>
<pre><code>def log_event(self, topic: Optional[str] = None):
    if b_tmetry_available and self._enabled:

        # Sample the current event.
        if not self._sample_record():
            return

        if topic is None:
            topic = self.DEFAULT_TOPIC

        for writer in self._writers:
            writer.writeRecord(topic, self._evt)
    del self._evt
    self._evt = SimpleEventRecord()
</code></pre>
<p>The last two lines should be under the conditional (they are not):</p>
<pre><code>if b_tmetry_available and self._enabled:
</code></pre>
<p>When I did <code>conda intall</code> it gave me the version of iopath with this bug, but as of writing this the latest version on GitHub has this bug corrected.
<a href=""https://github.com/facebookresearch/iopath/blob/main/iopath/common/event_logger.py"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/iopath/blob/main/iopath/common/event_logger.py</a></p>
<p>I noticed the import block as well:</p>
<pre><code>try:
    from tmetry.simpleevent import SimpleEventRecord
    from tmetry.writer import TmetryWriter

    b_tmetry_available = True
except ImportError:
    b_tmetry_available = False
</code></pre>
<p>But <code>tmetry</code> is an internal tool used by the developers. So the fix, as per the latest version on GitHub, is that it should not execute those last two lines.</p>
<p>That said, I've done <code>conda update iopath</code> and it still gave me the broken version. I ended up fixing it manually for now.</p>
","2024-03-09 21:47:47","2","Answer"
"78133976","77963476","","<p>This should work with the latest mlx: <code>pip install -U mlx</code></p>
<pre class=""lang-bash prettyprint-override""><code>&gt;&gt;&gt; import mlx.core as mx
&gt;&gt;&gt; mask = mx.array([True, False])
&gt;&gt;&gt; mx.where(mask, mx.array(float(&quot;-inf&quot;)), mx.array(0.0))
array([-inf, 0], dtype=float32)
</code></pre>
","2024-03-09 21:14:03","1","Answer"
"78133963","","TensorDataSet ""size mismatch between tensors""","<p>I have 4 matrices of size 64x64 that were stacked (Torch.Stack) to create a size of [4,64,64] and are meant to be the inputs for my TensorDataSet. I have 1 matrix of 64x64 that is meant to be output for my TensorDataSet. When I load these into the TensorDataSet(inputs,outputs), I get the size mismatch.</p>
<p>If I take 1 input and 1 output each with size 64x64 the TensorDataSet will accept this. However, I want to pass in 4 input values that correspond to 1 output value. For example the first value in the [0,0] position of each input has a relationship to the [0,0] position of the output.</p>
<p>I tried using squeeze methods and didn't have any success.</p>
","2024-03-09 21:11:18","-2","Question"
"78133882","","Running out of RAM while doing pip install on AWS EC2","<p>I created EC2 instance (t2.micro, 1 GiB of RAM, 30 GiB of Disk).
I connected to the instance then successfully installed flask as</p>
<pre><code>pip3 install flask
</code></pre>
<p>But when I try to install PyTorch as</p>
<pre><code>pip3 install torch
</code></pre>
<p>it fails on the last moment:</p>
<pre><code>Collecting torch
  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 754.9/755.5 MB 32.8 MB/s eta 0:00:01Killed
</code></pre>
","2024-03-09 20:45:28","1","Question"
"78132768","78107838","","<p>I see you've already found out the problem, but this answer could still be helpful to others.&quot;</p>
<p>First count how many parameters each layer of your model has:</p>
<ul>
<li><code>Linear(in_features=N, out_features=M)</code> has <code>MxN</code> weights;</li>
<li><code>Flatten</code> and <code>ReLU</code> have no weights;</li>
<li>Both <code>BatchNorm1d(C)</code> and <code>BatchNorm2d(C)</code> have <code>2xC</code> weights (2 per channel);</li>
<li>Both <code>Conv2d</code> and <code>ConvTranspose2d(in_features=N, out_features=M, kernel_size=K, ...)</code> have <code>M</code> different filters, each of size <code>NzKxK</code>. Each filter has also (by default) its own bias (another weight). Therefore, for <code>Conv2d</code> layers, you end up with <code>MxNxKxK + M</code> weights.</li>
</ul>
<p>If you compute the total number of weights of your model, you should end up with <code>8,599,888,384 + 2048 x dim_code + 8,604,081,155 + 2048 x dim_code = 17,203,969,539 + 4,096 x dim_code</code> different parameters (I hope I dind't miscalculate anything! I leave the computations in the bottom of the answer). This is even larger than most recent LLMs, such as Mistral7B (which, as the name itself suggests, has around <code>7B</code> parameters).</p>
<p>Now, considering that PyTorch defaults to a <code>float32</code> data type for tensors, your model needs more than <code>17B x 32bit = 64 GiB</code> of RAM. This calculation disregards the contribution from the <code>4,096 x dim_code</code> term, assuming that <code>dim_code</code> is relatively small in comparison. Therefore, make sure your machine has enough RAM (if you are using CPU as device, or enough VRAM, if you use a GPU).</p>
<h2>Number of parameters calculation</h2>
<p>Encoder has <code>8,599,888,384 + 2048 x dim_code</code>:</p>
<ul>
<li><code>enc_conv0</code> has:
<ul>
<li><code>Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)</code> has
<code>3x64x3x3+64 = 1,792</code> weights;</li>
<li><code>BatchNorm2d(64)</code> has <code>2x64 = 128</code> weights;</li>
<li><code>Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)</code> has
<code>128x64x3x3+128 = 73,856</code> weights;</li>
<li><code>BatchNorm2d(128)</code> has <code>2x128 = 256</code> weights;</li>
</ul>
</li>
<li><code>enc_conv1</code> has:
<ul>
<li><code>Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)</code> has
<code>256x128x3x3+256 = 295,168</code> weights;</li>
<li><code>BatchNorm2d(256)</code> has <code>2x256 = 512</code> weights;</li>
<li><code>Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)</code> has <code>512x256x3x3+512 = 1,180,160</code> weights;</li>
<li><code>BatchNorm2d(512)</code> has <code>2x512 = 1,024</code> weights</li>
</ul>
</li>
<li><code>enc_fc</code> has:
<ul>
<li><code>Linear(in_features=512*64*64, out_features=4096)</code> has <code>512x64x64x4096 = 8,589,934,592</code> weights;</li>
<li><code>BatchNorm1d(4096)</code> has <code>2x4096 = 8,192</code> weights;</li>
<li><code>Linear(in_features=4096, out_features=2048)</code> has <code>4096x2048 = 8,388,608</code> weights;</li>
<li><code>BatchNorm1d(2048)</code> has <code>4096</code> weights;</li>
<li><code>Linear(in_features=2048, out_features=d)</code> has <code>2048xd</code> weights;</li>
</ul>
</li>
</ul>
<p>Decoder has <code>8,604,081,155 + 2048 x dim_code</code> weights:</p>
<ul>
<li><code>dec_fc</code> has:
<ul>
<li><code>Linear(in_features=dim_code, out_features=2048)</code> has <code>dim_code x 2048</code> weights;</li>
<li><code>BatchNorm1d(2048)</code> has <code>2x2048 = 4,096</code> weights;</li>
<li><code>Linear(in_features=2048, out_features=4096)</code> has <code>2048x4096 = 8,388,608</code> weights;</li>
<li><code>BatchNorm1d(4096)</code> has <code>2x4096 = 8,192</code> weights;</li>
<li><code>Linear(in_features=4096, out_features=512*64*64)</code> has <code>4096x512x64x64 = 8,589,934,592</code> weights;</li>
<li><code>BatchNorm1d(512*64*64)</code> has <code>2x512x64x64 = 4,194,304</code> weights;</li>
</ul>
</li>
<li><code>dec_conv0</code> has:
<ul>
<li><code>ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, padding=1)</code> has <code>256x512x3x3 + 256 = 1,179,904</code> weights;</li>
<li><code>BatchNorm2d(256)</code> has <code>512</code> weights;</li>
<li><code>ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)</code> has <code>128x256x3x3 + 128 = 295,040</code> weights;</li>
<li><code>BatchNorm2d(128)</code> has <code>2x128 = 256</code> weights;</li>
</ul>
</li>
<li><code>dec_conv1</code> has :
<ul>
<li><code>ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)</code> has <code>64x128x3x3 + 64 = 73,792</code> weights;</li>
<li><code>BatchNorm2d(64)</code> has <code>2x64 = 128</code> weights;</li>
<li><code>ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=3, padding=1)</code> has <code>3x64x3x3 + 3 = 1,731</code> weights.</li>
</ul>
</li>
</ul>
","2024-03-09 14:57:19","0","Answer"
"78132075","","Is it better to store CUDA or CPU tensors that are loaded by torch DataLoader?","<p>I am working on a project where I aim to train a <code>PyTorch</code> model on <strong>multiple GPUs</strong>.</p>
<p>My input data is stored in separate files for each training example, and during preprocessing, I save them using the <code>torch.save</code> method to <code>.pt</code> files. Later, I load these files using <code>DataLoader</code>, where I want to set <code>num_workers &gt; 0</code> to <strong>speed up</strong> the process. However, it seems that <code>num_workers</code> can only be set to <code>&gt;0</code> when the input data is on CPU.</p>
<p>My question is: Should I save <code>CUDA tensors</code> already and just use <code>num_workers=0</code>, or should I store <code>CPU tensors</code>, set <code>num_workers &gt; 0</code>, and then move the batch as a whole to GPU?</p>
<p>I'm uncertain which approach would be more efficient for training speed (time) on multiple GPUs. Any insights or best practices on this matter would be greatly appreciated.</p>
","2024-03-09 10:58:02","0","Question"
"78131346","78130978","","<p>So, both cases are all before the <code>backward()</code> function. Then it doesn't matter where you place it. Actually you can put it before calling <code>backward()</code> and <code>step()</code> or after as long as not between <code>backward()</code> and <code>step()</code>. It actually depends on the codding style of the person who wrote it.</p>
<p>Normally If you have only one optimization then people tend to put the <code>opt.zero_grad()</code> at the beginning of the loop. But if your model is a bit complex with more than one optimization. Then I tend to do something like:</p>
<pre><code>encoder_opt.zero_grad()
encoder_loss = ...
encoder_loss.backward()
encoder_opt.step()

decoder_opt.zero_grad()
decoder_loss = ...
decoder_loss.backward()
decoder_opt.step()

</code></pre>
<p>to make the code easier to read and make it clearer I guess.</p>
<p>About your experiment, I don't think that putting it in different lines make it produce different result. As your model weight is initialized as well as updated differently when you re-train your model. You can set a specific seed for the reproducibility.</p>
","2024-03-09 05:35:25","2","Answer"
"78130978","","Exactly where in the training loss loop should zero_grad() be used? Does doing it before or after calculating the loss change things?","<p>So I know you need to zero out the gradients before a backwards pass, because the reason for that is obvious. I'm confused about where to add zero_grad() otherwise though, as I've seen examples put it either at the start of the loop or just before loss.backward(), and I'm unable to tell which is correct, or if there's much of a difference at all.</p>
<p>I did try this and noticed a change in all of my accuracy calculations though which has me sort of curious as to what the reason might be, and if it's significant at all.</p>
","2024-03-09 01:38:12","0","Question"
"78128803","78127851","","<p>You can always consider the two spatial dimensions of <code>som</code> as a single flattened dimension. Considering your previous <a href=""https://stackoverflow.com/a/78126805/6331369"">question</a>, it is preferable to work with <code>dist_l2.argmin(1)</code> (flattened indices) rather than <code>row</code> and <code>col</code> (unraveled indices). Let's write the intermediate tensors:</p>
<pre><code># expand batch-wise to (512, 1600, 84)
_som = som.view(1,-1,z.size(-1)).expand(len(z),-1,-1)

# expand z on dim=1 to match som
_z = z[:,None].expand(-1,40*40,-1)

# L2((512, 1600, 84), (512, 1, 84)) = (512, 1600, 1)
dist_l2 = torch.cdist(_som, z[:,None])[:,:,0]

# indices, shape reduced to (512,)
arg = dist_l2.argmin(1)
</code></pre>
<p>That way you can get all <code>som[row[i], col[i]]</code> just with <code>_som[range(len(arg)),arg]</code>. In vectorized form, you compute all iterations of the loop by introducing an extra dimension (of size <code>40*40</code>). Additionally we expand a singleton on <code>dim=1</code>:</p>
<pre><code>som_arg = _som[range(len(arg)),arg][:,None]
</code></pre>
<p>So the L2 distance between <code>som[r, c]</code> and <code>som[row[i], col[i]])</code> corresponds to <code>torch.cdist(_som, som_arg)</code>:</p>
<pre><code># shape (512, 1600, 84) x (512, 1, 84) -&gt; (512, 1600, 1)
l2_dist = torch.cdist(_som, som_arg)

# shape (512, 1600, 1)
neigh_dist = torch.exp(-l2_dist) / (2.0 * torch.pow(neighb_rad, 2))

# shape (512, 1600, 84) accumulated batch-wise -&gt; (1600, 84)
out = (lr * neigh_dist * (_z - _som)).sum(0)
</code></pre>
<p>Finally, you can reshape to the desired squared form: <code>out.view(40,40,-1)</code>.</p>
","2024-03-08 15:43:04","1","Answer"
"78128663","78128662","","<p>Currently, <code>numpy</code> <a href=""https://github.com/numpy/numpy/issues/19808"" rel=""nofollow noreferrer"">does not support bfloat16</a>**. One work-around is to upcast the tensor from half-precision to single-precision before making the conversion:</p>
<pre class=""lang-py prettyprint-override""><code>x.float().numpy()
</code></pre>
<p>The Pytorch maintainers are <a href=""https://github.com/pytorch/pytorch/issues/90574"" rel=""nofollow noreferrer"">also considering</a> adding a <code>force=True</code> option to the <code>Tensor.numpy</code> method to this automatically.</p>
<p>** although <a href=""https://github.com/numpy/numpy/issues/19808#issuecomment-1477207638"" rel=""nofollow noreferrer"">that may change</a> thanks to <a href=""https://github.com/jax-ml/ml_dtypes"" rel=""nofollow noreferrer"">work</a> by @jakevdp</p>
","2024-03-08 15:18:42","4","Answer"
"78128662","","Converting Pytorch bfloat16 tensors to numpy throws TypeError","<p>When you try to convert a Torch bfloat16 tensor to a numpy array, it throws a <code>TypeError</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

x = torch.Tensor([0]).to(torch.bfloat16)
x.numpy()  # TypeError: Got unsupported ScalarType BFloat16

import numpy as np
np.array(x)  # same error
</code></pre>
<p>Is there a work-around to make this conversion?</p>
","2024-03-08 15:18:42","3","Question"
"78127885","78127740","","<p><code>x[&lt;whatever&gt;]</code> creates a <em><strong>new</strong></em> Tensor, which you can verify with</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; x=torch.rand(3,3)
&gt;&gt;&gt; type(x[0])
&lt;class 'torch.Tensor'&gt;
</code></pre>
<p>For your test with <code>id</code>, it is unclear what you would expect. But it might be worth to look at <a href=""https://docs.python.org/3/library/functions.html#id"" rel=""nofollow noreferrer"">the docs on the id function</a></p>
<blockquote>
<p>id(object)<br />
Return the “identity” of an object. This is an integer which is guaranteed to be unique and constant for this object during its lifetime. <strong>Two objects with non-overlapping lifetimes may have the same id()</strong> value.</p>
<p>CPython implementation detail: This is the address of the object in memory.</p>
<p>Raises an auditing event builtins.id with argument id.</p>
</blockquote>
<p>Basically, you create new objects with each <code>[]</code> operator and there is no guarantee, that the results of two statements like <code>x[0][0]</code> will result in the same <code>id</code> (=reside at the same memory adress in case of CPython)</p>
","2024-03-08 13:08:12","3","Answer"
"78127851","","Find neighborhood for torch tensor","<p>I am trying to implement a Self-Organizing Map where for a given input sample, the best matching unit/winning unit is chosen based on (say) L2-norm distance between the SOM and the input. The winning unit/BMU (som[x, y]) has the smallest L2 distance from the given input (z):</p>
<pre><code># Input batch: batch-size = 512, input-dim = 84-
z = torch.randn(512, 84)

# SOM shape: (height, width, input-dim)-
som = torch.randn(40, 40, 84)

print(f&quot;BMU row, col shapes; row = {row.shape} &amp; col = {col.shape}&quot;)
# BMU row, col shapes; row = torch.Size([512]) &amp; col = torch.Size([512])
</code></pre>
<p>For clarity, for the first input sample in the batch &quot;z[0]&quot;, the winning unit is &quot;som[row[0], col[0]]&quot;-</p>
<pre><code>z[0].shape, som[row[0], col[0]].shape
# (torch.Size([84]), torch.Size([84]))
</code></pre>
<p><code>torch.norm((z[0] - som[row[0], col[0]]))</code> is the smallest L2 distance between z[0] and all other som units except row[0] and col[0].</p>
<pre><code># Define initial neighborhood radius and learning rate-
neighb_rad = torch.tensor(2.0)
lr = 0.5

# To update weights for the first input &quot;z[0]&quot; and its corresponding BMU &quot;som[row[0], col[0]]&quot;-
for r in range(som.shape[0]):
    for c in range(som.shape[1]):
        neigh_dist = torch.exp(-torch.norm(input = (som[r, c] - som[row[0], col[0]])) / (2.0 * torch.pow(neighb_rad, 2)))
        som[r, c] = som[r, c] + (lr * neigh_dist * (z[0] - som[r, c]))
</code></pre>
<p>How can I implement the code for:</p>
<ol>
<li>updating weights for all units around each BMU without the 2 for loops (and)</li>
<li>do it for all of the inputs &quot;z&quot; (here, z has 512 samples)</li>
</ol>
","2024-03-08 13:01:39","0","Question"
"78127740","","In torch, why do I get two different results when I use id(tensor[x][y]) consecutively?","<pre><code>x=torch.rand(3,3)
id(x[0][0])
1186163119824
id(x[0][0])
1186163118464
id(x[0,0])
1186163118464
id(x[0,0])
1186163118464
</code></pre>
<p>I understand the storage structure of lists in python, and I understand shallow and deep copies, but I'm not familiar with tensors in pytorch.Is there a bug in the id function?</p>
","2024-03-08 12:41:56","1","Question"
"78127465","78127371","","<p>The <a href=""https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/dropout.py#L20-L21"" rel=""nofollow noreferrer""><code>__repr__</code></a> method of <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html"" rel=""nofollow noreferrer""><code>nn.Dropout</code></a> doesn't output that kind of information. That's why it will show the same whatever the mode the layer is on.</p>
<p>It doesn't mean it isn't applied though, you can check for yourself!<br>
You can check the mode of your sublayer or parent layer with the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#:%7E:text=Boolean%20represents%20whether%20this%20module%20is%20in%20training%20or%20evaluation%20mode."" rel=""nofollow noreferrer""><code>training</code></a> attribute:</p>
<pre><code>&gt;&gt;&gt; model = nn.Dropout(0.5)
&gt;&gt;&gt; model.training
True

&gt;&gt;&gt; model.eval()
Dropout(p=0.5, inplace=False)

&gt;&gt;&gt; model.training 
False
</code></pre>
<hr />
<p>The reason why the mode is not propagated to your child layers is because you are using lists which means the layers are not registered as child modules. Instead, you should wrap your lists with <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html"" rel=""nofollow noreferrer""><code>nn.ModuleList</code></a> for <code>encoder_layers</code> and <code>decoder_layers</code>.</p>
","2024-03-08 11:45:18","1","Answer"
"78127371","","model.eval() disables only dropout layers defined in outer class in Pytorch","<p>I am trying to reproduce the original transformer for machine translation in PyTorch.</p>
<pre class=""lang-py prettyprint-override""><code>class Transformer(nn.Module):

    def __init__(self, vocab_size_in, vocab_size_out, embedding_dim, n_heads, key_dim, value_dim, ffn_dim, n=10000,
                 eps=1e-5, padding_token_index=0, p_drop=0.1, n_encoder_layers=1, n_decoder_layers=1):
        super(Transformer, self).__init__()

        # parameters
        self.key_dim = key_dim
        self.n_heads = n_heads
        self.embedding_dim = embedding_dim
        self.eps = eps
        self.ffn_dim = ffn_dim
        self.padding_token_index = padding_token_index
        self.vocab_size_in = vocab_size_in

        # Embedding layers encoder
        self.embedding_layer_enc = EmbeddingLayer(vocab_size_in, embedding_dim, n)
        self.dropout_enc = nn.Dropout(p_drop)

        # Encoder layers
        self.encoder_layers = [EncoderLayer(embedding_dim, key_dim, value_dim, ffn_dim, n_heads, p_drop, eps)] * n_encoder_layers

        # Embedding layers decoder
        self.embedding_layer_dec = EmbeddingLayer(vocab_size_out, embedding_dim)
        self.dropout_dec = nn.Dropout(p_drop)

        # Decoder layers
        self.decoder_layers = [DecoderLayer(embedding_dim, key_dim, value_dim, ffn_dim, n_heads, p_drop, eps)] * n_decoder_layers

        # Linear output layer
        self.output_linear = nn.Linear(embedding_dim, vocab_size_out)

    def forward(self, input, target):
        # ...
</code></pre>
<p>As you can see there are two dropout layers defined here. Moreover, I have further dropout layers in <code>EncoderLayer</code> and <code>DecoderLayer</code>.</p>
<pre class=""lang-py prettyprint-override""><code>class EncoderLayer(nn.Module):

    def __init__(self, embedding_dim=512, key_dim=512, value_dim=512, ffn_dim=512, n_heads=8, p_drop=0.1, eps=1e-5):
        super().__init__()

        self.multi_head = MultiHeadAttentionLayer(n_heads, embedding_dim, key_dim, value_dim)
        self.dropout_multi_head = nn.Dropout(p_drop)
        self.norm_multi_head = LayerNormalization(embedding_dim, eps)
        self.FFN_in = nn.Linear(embedding_dim, ffn_dim)
        self.FFN_out = nn.Linear(ffn_dim, embedding_dim)
        self.dropout_FFN = nn.Dropout(p_drop)
        self.norm_FFN = LayerNormalization(embedding_dim, eps)

    def forward(self, source, mask=None):
        multi_head_out = self.multi_head(source, mask)  # shape = (n_sentences, len_sentence, embedding_dim)
        multi_head_out = self.dropout_multi_head(multi_head_out)
        multi_head_norm = self.norm_multi_head(source + multi_head_out)
        ffn_in = self.FFN_in(multi_head_norm)
        ffn_in = F.relu(ffn_in)
        ffn_out = self.FFN_out(ffn_in)
        ffn_out = self.dropout_FFN(ffn_out)
        enc_out = self.norm_FFN(multi_head_norm + ffn_out)

        return enc_out
</code></pre>
<p>I am testing my code in evaluation mode. The forward step takes source and target sequences of indices and outputs a tensor of probabilities for each word in the output sequence. So far, the testing function is simple, I just wanted to make sure when I input the same thing the output stays the same:</p>
<pre><code>def translate_sentence(model, source, target, max_num_words=200):
        for i in range(max_num_words):
            model.eval()

            with torch.no_grad():

                output = model(source, target)
                print(output)
</code></pre>
<p>However, that does not happen. As suggested, I added:</p>
<pre class=""lang-py prettyprint-override""><code>print(model.dropout_enc.training)
print(model.encoder_layers[0].dropout_multi_head.training)
</code></pre>
<p>In order to check if dropout layers are active or not, and the output is:</p>
<pre><code>False
True
</code></pre>
<p>Therefore, <code>model.eval()</code> disables dropout layers defined in the <code>__init__</code> of Transformer, but not in its sublayers. Any idea on how to solve?</p>
<hr />
<p>Solved: I had to use <code>nn.ModuleList</code>.</p>
","2024-03-08 11:29:08","0","Question"
"78127355","78107928","","<p>If you try <code>print(model)</code>, you should get a description of the model, something like this:</p>
<pre><code>MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): lora.Linear(
            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
...
</code></pre>
<p>If lora is mentioned, you can see that LoRa is already applied to the base model. The MistralForCausalLM class does not know anything about LoRa, so you can't call &quot;merge_and_unload&quot; on it.</p>
<p>PeftModel does have the <code>merge_and_unload</code> method, so you need to use that:</p>
<pre><code># merge base + LoRa models and save the model

from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import sys
import torch

device_map = {&quot;&quot;: 0}
lora_dir = &quot;mistralai-my-lora-finetuning&quot;
base_model_name = &quot;mistralai/Mistral-7B-Instruct-v0.2&quot;
tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
model = AutoPeftModelForCausalLM.from_pretrained(lora_dir, device_map=device_map, torch_dtype=torch.bfloat16)


model = model.merge_and_unload()

output_dir = &quot;output/my_merged_model&quot;
model.save_pretrained(output_dir)
</code></pre>
","2024-03-08 11:24:41","0","Answer"
"78126805","78126632","","<p>You can easily extend this operation to batches by expanding your <code>som</code> tensor:</p>
<pre><code>_som = som.view(1,-1,z.size(-1)).expand(len(z),-1,-1)

# L2((512, 1600, 84), (512, 1, 84)) = (512, 1600, 1)
dist_l2 = torch.cdist(_som, z[:,None])[:,:,0]

# both are shaped (512,)
row, col = torch.unravel_index(dist_l2.argmin(1), (40,40))
</code></pre>
<p><em>Note: <a href=""https://pytorch.org/docs/2.2/generated/torch.unravel_index.html#torch-unravel-index"" rel=""nofollow noreferrer""><code>torch.unravel_index</code></a> is available from PyTorch version <strong>2.2</strong>, if you don't have access to this version, you may resort to <a href=""https://discuss.pytorch.org/t/how-to-do-a-unravel-index-in-pytorch-just-like-in-numpy/12987/3"" rel=""nofollow noreferrer"">this</a> user-made implementation.</em></p>
","2024-03-08 09:51:04","1","Answer"
"78126632","","Find winning unit between 2 torch tensors of different shapes","<p>I am trying to implement a Self-Organizing Map where for a given input sample, the best matching unit/winning unit is chosen based on (say) L2-norm distance between the SOM and the input. To implement this, I have:</p>
<pre><code># Input batch: batch-size = 512, input-dim = 84-
z = torch.randn(512, 84)

# SOM shape: (height, width, input-dim)-
som = torch.randn(40, 40, 84)

# Compute L2 distance for a single sample out of 512 samples-
dist_l2 = np.linalg.norm((som.numpy() - z[0].numpy()), ord = 2, axis = 2)

# dist_l2.shape
# (40, 40)

# Get (row, column) index of the minimum of a 2d np array-
row, col = np.unravel_index(dist_l2.argmin(), dist_l2.shape)

print(f&quot;BMU for z[0]; row = {row}, col  = {col}&quot;)
# BMU for z[0]; row = 3, col  = 9
</code></pre>
<p>So for the first input sample of 'z', the winning unit in SOM has the index: (3, 9). I can put this in a for loop iterating over all 512 such input samples, but that is very inefficient.</p>
<p>Is there an efficient vectorized PyTorch manner to compute this for the entire batch?</p>
","2024-03-08 09:19:26","0","Question"
"78126372","78126282","","<p>There are <a href=""https://github.com/huggingface/transformers/issues/14336#issuecomment-964049795"" rel=""noreferrer"">2 possible solutions</a>:</p>
<p>Saving manually:</p>
<pre class=""lang-py prettyprint-override""><code>from diffusers import StableDiffusionPipeline

model = StableDiffusionPipeline.from_pretrained(
    &quot;CompVis/stable-diffusion-v1-4&quot;,
    use_auth_token=True,
)

model.save_pretrained(&quot;./my_model_directory/&quot;)  # only needed first run
model = StableDiffusionPipeline.from_pretrained(&quot;./my_model_directory/&quot;)
</code></pre>
<p>Cache dir:</p>
<pre class=""lang-py prettyprint-override""><code>from diffusers import StableDiffusionPipeline

model = StableDiffusionPipeline.from_pretrained(
    &quot;CompVis/stable-diffusion-v1-4&quot;,
    cache_dir=&quot;./my_model_directory/&quot;,
    use_auth_token=True,
)
</code></pre>
","2024-03-08 08:29:11","8","Answer"
"78126282","","How to prevent repeated downloading with HuggingFace","<h4>Description:</h4>
<p>I am confused on how the installation of the packages are performed. Currently I was working on a StableDiffusion model and every-time I run the code its again and again downloading files which are 3 to 4 Gigs big.</p>
<h4>Code:</h4>
<p>This is the code I was trying to run at first:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import autocast
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    &quot;CompVis/stable-diffusion-v1-4&quot;, 
    use_auth_token=True
).to(&quot;cuda&quot;)

prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
with autocast(&quot;cuda&quot;):
    image = pipe(prompt)[&quot;sample&quot;][0]  
    
image.save(&quot;astronaut_rides_horse.png&quot;)
</code></pre>
<h4>Issue:</h4>
<p>When I run the code the following appears in my shell:</p>
<pre class=""lang-bash prettyprint-override""><code>Fetching 16 files:   0%|                                                             | 0/16 [00:00&lt;?, ?it/s]
vae/diffusion_pytorch_model.safetensors:   0%|                                   | 0.00/335M [00:00&lt;?, ?B/s]
unet/diffusion_pytorch_model.safetensors:   0%|                                 | 0.00/3.44G [00:00&lt;?, ?B/s]
safety_checker/model.safetensors:   0%|                                         | 0.00/1.22G [00:00&lt;?, ?B/s]
text_encoder/model.safetensors:   0%|                                            | 0.00/492M [00:00&lt;?, ?B/s]
</code></pre>
<p>and this happens each and everytime I run the code.</p>
<h4>What I tried?</h4>
<p>I tried installing and cloning the whole git repo. (I honestly don't know why I did that even though I know it wasn't gonna affect a thing!) Also I tried searching for many forums for this issue but not even a single clue, maybe its because of my in-experienced approach.</p>
","2024-03-08 08:09:49","3","Question"
"78126184","78126160","","<p><code>loss.backward()</code> does not return anything. This is why <code>print(loss.backward())</code> returns <code>None</code>.</p>
<p>You want to use <code>print(loss)</code> instead</p>
","2024-03-08 07:47:39","0","Answer"
"78126160","","Getting None from loss in neural network despite tensors being leaf","<p>I checked all the tensors and input parameters, they were all leaf, according to the code below,</p>
<pre><code>def train_step(w1,b1):
    print(&quot;w=&quot;,w1)
    trainable_variables = [w1,b1]
    optimizer = torch.optim.SGD(trainable_variables, lr=learning_rate)
    loss = Variable(loss2_function(), requires_grad = True)
    print(loss.backward())
    with torch.no_grad():
        w1 -=(learning_rate * w1.grad)
        b1 -= (learning_rate * b1.grad)
        w1.grad.zero_()
        b1.grad.zero_()
    optimizer.step()
    optimizer.zero_grad()
</code></pre>
<p>I still get none, and even with the change in learning rate, weight and bias, the network still does not work, please guide me.</p>
","2024-03-08 07:42:18","0","Question"
"78125246","78123358","","<p>I would suggest two options:</p>
<ol>
<li><p>update the <code>__getitem__</code> method of your <code>Dataset</code> to look up a single row from the table by index</p>
</li>
<li><p>use the <a href=""https://huggingface.co/docs/datasets/"" rel=""nofollow noreferrer"">datasets</a> library with the <a href=""https://huggingface.co/docs/datasets/en/tabular_load#databases"" rel=""nofollow noreferrer"">sql interface</a></p>
</li>
</ol>
","2024-03-08 02:16:51","1","Answer"
"78124975","78123358","","<p>An option would be to have a &quot;pagination_control&quot; table from which the limit and offset can be obtained.</p>
<p>So two primary columns (e.g. <code>p_limit</code> and <code>p_offset</code>).</p>
<p>This could then facilitate a flexible approach in that the limits and offset could be altered to suit. However, to facilitate the next block/group/set of rows. After a selection has been extracted an update of the &quot;paginataion_control&quot; table could then ready it for the next block/group/set of sequences by adding the limit to the offset.</p>
<p><strong>Demo</strong></p>
<p>You may wish to consider the following demo:-</p>
<pre><code>/* Cleanup demo environment just in case */
DROP TABLE IF EXISTS `table`;
DROP TABLE IF EXISTS pagination_control;
/* Create the core table */
CREATE TABLE IF NOT EXISTS `table` (id INTEGER PRIMARY KEY, sequences TEXT, classification TEXT, other_if_any TEXT DEFAULT 'ooops');
/* load the core table with some 300000 rows (after doing playing around) */
WITH 
    /* Create a CTE (temp table) with the core sequence indentifiers */
    /* note very little knowledge of DNA */
    sequences(seq) AS (
        SELECT 'A' UNION ALL SELECT 'B' UNION ALL SELECT 'C' UNION ALL SELECT 'G' UNION ALL SELECT 'T'
    ),
    /* create CTE with groups of indentifiers (which could potentially compress the stored sequences (intended just as a hint)) */
    /* i.e. all permutations (3125) of 5 identifiers */
    grouped_sequences_by_5 AS (
    SELECT DISTINCT 
        groupof5.seq ||s2.seq||s3.seq||s4.seq||s5.seq AS groupof5
    FROM sequences AS groupof5 
        JOIN sequences AS s2 
        JOIN sequences AS s3 
        JOIN sequences AS s4 
        JOIN sequences AS s5
    )
    ,
    /* Create another CTE of random rows (300000 rows) */
    ready_to_insert(sequences,classification) AS (
        SELECT 
            (SELECT groupof5 FROM grouped_sequences_by_5 ORDER BY random() LIMIT 1 ),
            'CAT'||(abs(random()) % 10)
        UNION ALL SELECT 
            (SELECT groupof5 FROM grouped_sequences_by_5 ORDER BY random() LIMIT 1 ),
            'CAT'||(abs(random()) % 10)
        FROM ready_to_insert LIMIT 300000
    )
INSERT INTO `table` (sequences,classification) SELECT * FROM ready_to_insert;
SELECT * FROM `table`;

/*----------------------------------------*/
/* Now demonstrate the pagination_control */
CREATE TABLE IF NOT EXISTS pagination_control (id INTEGER PRIMARY KEY,p_limit,p_offset);
/* initialise pagination table */
INSERT OR REPLACE INTO pagination_control VALUES(1,1000,0);
SELECT * FROM `table` ORDER BY id LIMIT (SELECT p_limit FROM pagination_control) OFFSET (SELECT p_offset FROM pagination_control);
/* Always update after selection to ready for next block */
UPDATE pagination_control SET p_offset = p_offset + p_limit;
SELECT * FROM `table` ORDER BY id LIMIT (SELECT p_limit FROM pagination_control) OFFSET (SELECT p_offset FROM pagination_control);
/* Always update after selection to ready for next block (again) */
UPDATE pagination_control SET p_offset = p_offset + p_limit;
/* optional to alter e.g. set blocks to 500 rows per block */
UPDATE pagination_control SET p_limit = 500;
/* and so on */
SELECT * FROM `table` ORDER BY id LIMIT (SELECT p_limit FROM pagination_control) OFFSET (SELECT p_offset FROM pagination_control);
UPDATE pagination_control SET p_offset = p_offset + p_limit;
SELECT * FROM `table` ORDER BY id LIMIT (SELECT p_limit FROM pagination_control) OFFSET (SELECT p_offset FROM pagination_control);
UPDATE pagination_control SET p_offset = p_offset + p_limit;
/* Cleanup the demo environment */
DROP TABLE IF EXISTS pagination_control;
DROP TABLE IF EXISTS `table`;
</code></pre>
<ul>
<li>so first some 300000 rows are inserted (obviously just to demonstrate the pagination_control)
<ul>
<li><p>you may wish to consider how grouping sequences could be used to reduce the overall data e.g. in the above every 5 bytes could be represented by 2 bytes (the id(rowid) of the <code>grouped_sequences_by_5</code> CTE IF it were a made permanent table).</p>
</li>
<li><p>e.g. Result 1 could be:-</p>
</li>
<li><p><a href=""https://i.sstatic.net/86t9E.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/86t9E.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p><em><strong>Note</strong></em> the above apparently random selection of the group of 5 sequences isn't actually random throughout due to just the single value being calculated just the once. As the actual underlying data is just for demonstration this <em><strong>error in the demo</strong></em> has been left asis.</p>
</li>
</ul>
</li>
</ul>
<p>However, it is after the above that the pagination_control table is demonstrated itself.</p>
<p>First it is created with the two core columns (<code>p_limit</code> and <code>p_offset</code>), the id column just used to maintain the single row.</p>
<p>The following SELECT (Result 2) demonstrates how the pagination_control is used to determine the selected rows.</p>
<p><a href=""https://i.sstatic.net/VBdLB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VBdLB.png"" alt=""enter image description here"" /></a></p>
<p>The following UPDATE, which should typically immediately follow the SELECT, shows how the pagination table can be prepared for the next SELECT (Result 3).</p>
<p><a href=""https://i.sstatic.net/Jnn7a.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jnn7a.png"" alt=""enter image description here"" /></a></p>
<p>After this 2nd SELECT an UPDATE changes the LIMIT from 1000 to 500. The 3rd SELECT (Result 4) and the 4th SELECT (Result 5) then grab then next block of 500.</p>
<p><a href=""https://i.sstatic.net/Uvv0L.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Uvv0L.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/jGQco.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jGQco.png"" alt=""enter image description here"" /></a></p>
<p>Of course you can easily manipulate the &quot;pagination_control&quot; table and have very flexible control e.g. to reset you could update it so that <code>p_limit</code> is 1000 and <code>p_offset</code> is 0 to rerun with 1000 blocking/paging factor.</p>
","2024-03-08 00:13:51","1","Answer"
"78123358","","What is the best approach to train a pytorch model over large dataset stored in a database?","<p>I have a large dataset, and for convenience, I put it in an sqlite database. It has about 270k rows, each row has 10_000 bp long DNA sequence. It's impossible to load the entire dataset at once, let alone train a model (yeah, I tried, and my laptop's GPU ain't running / making any noise).</p>
<p>So currently I am running a loop. In each iteration, I am selecting paged data using offset and limit (say 500 sequences at a time) from the database, and training on the model on those 500 sequences with, say, 10 epochs.</p>
<p>The relevant part of my code:</p>
<pre class=""lang-py prettyprint-override""><code>my_offset = 0
my_limit = 500
my_batch_size = 100

some_model = MyModel()

db_page_number = 0

# pagination
while db_page_number &lt; 100:
  db_page_number += 1
  my_offset += my_limit

  query = f&quot;SELECT sequences, classification FROM table ORDER BY id LIMIT {my_limit} OFFSET {my_offset}&quot;
  paged_data = pd.read_sql_query(query)
  
  x, y = get_x_y_from(paged_data)
  train_dataset = torch.utils.data.TensorDataset(x, y)
  train_loader = DataLoader(train_dataset, batch_size=my_batch_size)
  for epoch in range(0, 10):
    # ... typical pytorch code...
    for data in train_loader:
      x1, y1 = data
      predicted_outputs = self.pytorch_model(x1)  # predict output from the model
      train_loss = self.loss_function(predicted_outputs, y1)  # calculate loss for the predicted output
      train_loss.backward()  # back propagate the loss
      optimizer.step()  # adjust params based on the calculated gradients
      # ... typical pytorch code...

</code></pre>
<p>Here, I am doing the pagination manually (the outer most while loop). I managed to run my code, but I wonder what is the best practice? Maybe using pandas, or some other library for for pagination part? I'm open to suggestions.</p>
","2024-03-07 17:36:15","0","Question"
"78121936","78119926","","<p>You can do something like this</p>
<pre class=""lang-py prettyprint-override""><code>import torch

x = torch.tensor([[4, 3, 1, 4, 2],
                  [0, 0, 2, 3, 4],
                  [4, 4, 3, 0, 3]]).float() # float required for later ops

k = torch.tensor([1, 0, 0]).long()

# set 0 to -1, ie [1, -1, -1]
k_sign = k + (-1 * (k==0).float())

# flip sign for rows where we want the smallest nonzero index
x_signed = x * k_sign.unsqueeze(1)

# fill zeros with -inf
x_filled = x_signed.masked_fill(x==0, float('-inf'))

# grab topk index of each row
_, output = x_filled.topk(1, dim=1)

output = output.squeeze()

output
&gt; tensor([0, 2, 2])
</code></pre>
","2024-03-07 13:58:45","4","Answer"
"78121649","78114412","","<p>It looks like, somehow, another file is missing (<em>one of its dependencies</em>). Starting from a <strong>new environment</strong> could remove unnecessary constraints. Inspired from <a href=""https://stackoverflow.com/a/77859957/12846804"">this answer</a>, with all requirements at once, not one part then add the other modules:</p>
<pre><code>conda create -n env_torch pytorch=2.2.0 torchvision=0.17.0 torchaudio=2.2.0 numpy pandas matplotlib seaborn scikit-learn pyedflib
activate env_torch 
</code></pre>
<p>The conda <a href=""https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf"" rel=""nofollow noreferrer"">cheat sheet</a> is your best friend. Other things I looked up:</p>
<ul>
<li>similar issue with pip, not conda: <a href=""https://stackoverflow.com/q/65931555/12846804"">While importing torch WinError 126 The specified module could not be found</a>,</li>
<li><a href=""https://stackoverflow.com/q/54845807/12846804"">What is the purpose of the c flag in the &quot;conda install&quot; command</a></li>
</ul>
","2024-03-07 13:09:44","0","Answer"
"78121040","78119974","","<p>I don't know why the other answer was accepted, as there is a fundamental misunderstanding in the way you constructed the model in the question! And the other answer does not take this into account.<br />
When you define a model and its input and output sizes, you still only consider one sample. You don't use <code>batch_size</code> for scaling the output. When you then give a batch of input data into the model, <code>PyTorch</code> handles the batch internally and the model gets evaulated on each sample in parallel.</p>
<p>You can look at an official <a href=""https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"" rel=""nofollow noreferrer"">PyTorch tutorial</a>, where they built a model for data on the <code>Fashion MNIST</code> dataset. Each image in this dataset is <code>(28x28x1)</code> pixels (greyscale), and there are 10 different classes to predict. Notice the first and last layer:</p>
<blockquote>
<p>nn.Linear(28*28, 512)<br />
....<br />
nn.Linear(512, 10)</p>
</blockquote>
<p>where the input is the image pixels <code>28*28</code> and the output is <code>10</code> numbers for 10 classes. You can then use <code>SoftMax</code> or <code>categorical_crossentropy</code> for prediction. There is no information on batch sizes in the model itself, as the model doesn't need that.</p>
<p>Most of the time it is no problem to have the last batch a bit smaller than the others. If your batch size is <code>32</code>, but the last batch is only <code>15</code> samples, the model will just get 15 samples and labels, do the prediction and compare the 15 results to the 15 labels for the last batch.<br />
If for some reason you need all batches to be exactly the same size (e.g. for a stateful <code>LSTM</code>), then you can use <code>DataLoader</code> with <code>drop_last=True</code>. But most of the time, it is not needed and you just hide data from the model if you use it.</p>
<p>Using the <code>DataLoader</code> is still a good idea, because they can efficiently handle loading your data on CPU, while the model will train on GPU.</p>
","2024-03-07 11:36:39","1","Answer"
"78120372","78119974","","<p>I suggest you to use a Pytorch <code>DataLoader</code> for loading data batch by batch instead of doing it manually. In this regard, PyTorch provides a simple solution for this using the <code>drop_last</code> parameter in the <code>DataLoader</code>. When set to True, it drops the last incomplete batch, ensuring that all batches are of the specified size except for the last one.
The <code>Dataloader</code> is a wrapper for the torch <code>Dataset</code>, you can find more info <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"" rel=""nofollow noreferrer"">here</a></p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
from torch.utils.data import DataLoader

X = torch.Tensor(...)  # your features
y = torch.Tensor(...)  # your labels

dataset = 

# Create a DataLoader with drop_last=True
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

model = ...

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Training loop
num_epochs = 5
for epoch in range(num_epochs):
    total_loss = 0
    for batch_x, batch_y in dataloader:
        optimizer.zero_grad()
        y_pred = model(batch_x)
        loss = criterion(y_pred, batch_y)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    average_loss = total_loss / len(dataloader)
    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}')
</code></pre>
","2024-03-07 09:59:38","1","Answer"
"78119974","","How to train NN in batches with odd examples size?","<p>I am a newbie in the NN field and I am doing some training with pytorch.<br />
I decided to make a simple vanilla NN.<br />
I used a personal dataset i had with 2377 numerical features and 6277 examples.</p>
<p>My first try was to make the NN predict each single example, so the pseudocode would look like</p>
<pre><code>for i in range(...):
    X = ... # features
    y = ... # outcome
    y_pred = model(X[i])
    loss = criterion(y_pred, y)

    y_pred.size # [1,1]
    y.size # [1,1]
</code></pre>
<p>This took about 10 seconds per epoch and i decided to improve it using mini batches.</p>
<p>So i define the batch size at the beginning and the NN in Pytorch is defined like this</p>
<pre><code>batch_size = 30
n_inputs = X.size[1] #2377

## 2 hidden layers
model = nn.Sequential(
    nn.Linear(n_inputs, 1024),
    nn.ReLU(),
    nn.Linear(1024, 512),
    nn.ReLU(),
    nn.Linear(512, 356),
    nn.ReLU(),
    nn.Linear(356, batch_size),
    nn.ReLU(),
)
</code></pre>
<p>And then I do the training in batches</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(5):
    totalloss = 0  
    permutation = torch.randperm(X.size()[0])
    for i in range(0, X.size()[0], batch_size):
        optimizer.zero_grad()
        indices = permutation[i:i+batch_size]
        batch_x, batch_y = x[indices], y[indices]

        ypred = model(batch_x)
        loss = criterion(ypred, batch_y) 
        totalloss += loss.item()
        
        ## update the weights
        loss.backward()
        optimizer.step()
</code></pre>
<p>Now the problem is that my NN always outputs 100 values <strong>but</strong> the last batch size can vary.<br />
In fact, if i choose 100 as batch size the last batch will be made of 77 examples (6277%100).</p>
<p>I am sure there is a way around this problem, and that there is a mistake in my structure, but i cannot see it.</p>
<p>Can you help me generalize the training in batch to work with any number of examples and batch size?</p>
","2024-03-07 08:58:57","1","Question"
"78119926","","How to get index of different top-k at each row in a 2D tensor in Pytorch?","<p>Given:</p>
<ul>
<li>a positive integer tensor A: (batch_size, N) in which zero is the smallest value. For example:</li>
</ul>
<pre><code>tensor([[4, 3, 1, 4, 2],
        [0, 0, 2, 3, 4],
        [4, 4, 3, 0, 3]])

</code></pre>
<p>I want get the index of different k of k-th largest value at each row?</p>
<ul>
<li>k is a list of <code>batch_size</code> elements are chosen randomly in which its values only express only 2 cases:
first is largest (so k = 1) second is the smallest but ignore zero, e.g. if the row is [2,3,4,0] so the smallest index is 0 (value 2). (with possibility = 0.7 for largest and 0.3 for smallest)</li>
</ul>
<p>With the example above, <code>if k = [1,0,0]</code> (1 means get largest, 0 mean smallest) then the output indices will be</p>
<p><code>output = [0, 2, 2]</code> the correspond values are <code>[4, 2,3]</code></p>
<p>Notes: please vectorize these calculations.</p>
","2024-03-07 08:49:20","0","Question"
"78118974","","How to use *.ckpt file as a model in OpenFold?","<p>I've trained the OpenFold model https://github.com/aqlaboratory/openfold and it geterated checkpoint file (*ckpt) (Pytorch Lighntning).</p>
<p>Explain me please, how can I use *.ckpt files for the prediction by <code>run_pretrained_openfold.py</code>? Or perhaps I need to somehow convert this to another format first?</p>
<p><code>python3 run_pretrained_openfold.py \ fasta_dir \ data/pdb_mmcif/mmcif_files/ \ --uniref90_database_path data/uniref90/uniref90.fasta \ --mgnify_database_path data/mgnify/mgy_clusters_2018_12.fa \ --pdb70_database_path data/pdb70/pdb70 \ --uniclust30_database_path data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \ --output_dir ./ \ --bfd_database_path data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \ --model_device &quot;cuda:0&quot; \ --jackhmmer_binary_path lib/conda/envs/openfold_venv/bin/jackhmmer \ --hhblits_binary_path lib/conda/envs/openfold_venv/bin/hhblits \ --hhsearch_binary_path lib/conda/envs/openfold_venv/bin/hhsearch \ --kalign_binary_path lib/conda/envs/openfold_venv/bin/kalign \ --config_preset &quot;model_1_ptm&quot; \ **--openfold_checkpoint_path openfold/resources/openfold_params/finetuning_ptm_2.pt**</code></p>
<p>If I use this:<code> --openfold_checkpoint_path /checkpoints/14my.ckpt</code> I got this error message:</p>
<p><code>raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format( RuntimeError: Error(s) in loading state_dict for AlphaFold:                                                                     Missing key(s) in state_dict: &quot;aux_heads.tm.linear.weight&quot;, &quot;aux_heads.tm.linear.bias&quot;.</code></p>
<p>##################################################</p>
<p>I resolved the error by using  <code>--config_preset &quot;model_1&quot;</code> instead of <code>model_1_ptm</code>.</p>
<p>However now I have pdb where the coordinates of amino acids are calculated incorrectly. I use &quot;manual optimization mode&quot; because I am testing my own optimizer, the standard Adam generally gives normal results.</p>
<p>Is it a problem with Manual Optimization of Lightning or what?</p>
<p>Peptide after Manual Optimization:
<a href=""https://i.sstatic.net/PLTEw.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Standard Adam and Automatic Optimization:
<a href=""https://i.sstatic.net/ks1Y1.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","2024-03-07 05:36:12","-1","Question"
"78118932","78118772","","<p>The below code resulted in what I want.
But if anyone has a more efficient solution, please feel free to answer.</p>
<pre class=""lang-py prettyprint-override""><code>for edge_index_class in torch.unique(batch.edge_index_class):
    # Find indices where edge_index_class matches
    indices = (batch.edge_index_class == edge_index_class).nonzero(as_tuple=True)[0]
    
    # Extract corresponding edge_index and n_id
    # edge_index = batch.edge_index[:, indices]
    n_id = torch.unique(batch.n_id[batch.edge_index[:, indices]])
    
    tmp_filled[n_id] = tmp[int(edge_index_class.item())][n_id]


tmp_filled[1624]
# tensor([0.6071, 0.9668, 0.9829, 0.1886], device='cuda:0')

tmp[3][1624]
# tensor([0.6071, 0.9668, 0.9829, 0.1886], device='cuda:0')
</code></pre>
","2024-03-07 05:19:42","1","Answer"
"78118772","","How to get the values from the list of tensors by matching indices in pytorch?","<p>I have a question about calling the values from the list of tensors with multiple indices.<br />
Although I think that there are similar questions such as <a href=""https://stackoverflow.com/questions/75504084/select-multiple-indices-in-an-axis-of-pytorch-tensor/75505948#75505948"">here</a>, I couldn't completely use it.</p>
<p>I have a dataset comprising the 4-dimensional features for about 108,000 nodes and their links.</p>
<pre class=""lang-py prettyprint-override""><code>tmp = []
for _ in range(4):
    tmp.append(torch.rand((107940, 4), dtype=torch.float).to(device))

tmp
# [tensor([[0.9249, 0.5367, 0.5161, 0.6898],
#         [0.2189, 0.5593, 0.8087, 0.9893],
#         [0.4344, 0.1507, 0.4631, 0.7680],
#         ...,
#         [0.7262, 0.0339, 0.9483, 0.2802],
#         [0.8652, 0.3117, 0.8613, 0.6062],
#         [0.5434, 0.9583, 0.3032, 0.3919]], device='cuda:0'),
# tensor([...], device='cuda:0'),
# tensor([...], device='cuda:0'),
# tensor([...], device='cuda:0')]
</code></pre>
<pre class=""lang-py prettyprint-override""><code># batch.xxx: factors in the batch from the graph
# Note that batch.edge_index[0] is the target node and batch.edge_index[1] is the source node.
# If you need more information, please see the Pytorch Geometric data format.

print(batch.n_id[batch.edge_index])
print(batch.edge_index_class)

#tensor([[10231,  3059, 32075, 10184,  1187,  6029, 10134, 10173,  6521,  9400,
#         14942, 31065, 10087, 10156, 10158, 26377, 85009,   918,  4542, 10176,
#         10180,  6334, 10245, 10228,  2339,  7891, 10214, 10240, 10041, 10020,
#          7610, 10324,  4320,  5951,  9078,  9709],
#        [ 1624,  1624,  6466,  6466,  6779,  6779,  7691,  7691,  8655,  8655,
#         30347, 30347, 32962, 32962, 34435, 34435,  3059,  3059, 32075, 32075,
#          1187,  1187,  6029,  6029, 10173, 10173,  6521,  6521,  9400,  9400,
#         31065, 31065, 10087, 10087, 10158, 10158]], device='cuda:0')
#tensor([3., 3., 2., 2., 0., 0., 3., 3., 2., 2., 0., 0., 2., 2., 2., 2., 3., 3.,
#        2., 2., 0., 0., 0., 0., 3., 3., 2., 2., 2., 2., 0., 0., 2., 2., 2., 2.],
#       device='cuda:0')
</code></pre>
<p>In this case, I want the new tensor that contains the feature values matched to the edge_index_class.<br />
For example, <code>tmp_filled</code> will have the 1624, 10231, and 3059th values from the fourth dataset in <code>tmp</code> because they are labeled with <code>edge_index_class</code> as 3.
Similarly, 6466, 32075, and 10184th values in the third dataset in <code>tmp</code> will go into the same index in <code>tmp_filled</code>.</p>
<p>To do this, I tried the code as below:</p>
<pre class=""lang-py prettyprint-override""><code>for k in range(len(batch.edge_index_class)):
    tmp_filled[batch.n_id[torch.unique(batch.edge_index)]] = tmp[int(batch.edge_index_class[k].item())][batch.n_id[torch.unique(batch.edge_index)]]

tmp_filled
# tensor([[0., 0., 0., 0.],
#        [0., 0., 0., 0.],
#        [0., 0., 0., 0.],
#        ...,
#        [0., 0., 0., 0.],
#        [0., 0., 0., 0.],
#        [0., 0., 0., 0.]], device='cuda:0')
</code></pre>
<p>But it returned the wrong result.</p>
<pre class=""lang-py prettyprint-override""><code>tmp_filled[1624]
# tensor([0.3438, 0.5555, 0.6229, 0.7983], device='cuda:0')

tmp[3][1624]
# tensor([0.6895, 0.3241, 0.1909, 0.1635], device='cuda:0')
</code></pre>
<p>When I need the <code>tmp_filled</code> data to consist of (107940 x 4) format, how should I correct my code?</p>
<p>Thank you for reading my question!</p>
","2024-03-07 04:26:08","0","Question"
"78117380","78111173","","<p>I figured it out. It is because my config.properties was wrong. This is correct.</p>
<pre><code>models={\
  &quot;prompt_injection_model&quot;: {\
    &quot;1.0&quot;: {\
        &quot;defaultVersion&quot;: true,\
        &quot;marName&quot;: &quot;prompt_injection_model.mar&quot;,\
        &quot;minWorkers&quot;: 2,\
        &quot;maxWorkers&quot;: 5,\
        &quot;batchSize&quot;: 128,\
        &quot;maxBatchDelay&quot;: 20,\
        &quot;responseTimeout&quot;: 60\
    }\
  }\
}
</code></pre>
","2024-03-06 20:52:33","0","Answer"
"78116763","78115234","","<p>You need to pass a tensor of ones if you want to backprop non-scalar values</p>
<pre class=""lang-py prettyprint-override""><code>import torch 

def P(x, A):
    x = x.unsqueeze(1)  # Convert to column vector
    vector = torch.matmul(A, x)
    denom = (vector.transpose(0, 1) @ vector).squeeze()
    P_matrix = (vector @ vector.transpose(0, 1)) / denom
    return P_matrix.squeeze()

A = torch.tensor([[1.0, 0.5], [0.5, 1.3]], dtype=torch.float32)
x = torch.tensor([1.0, 2.0], dtype=torch.float32, requires_grad=True)
h = torch.tensor([2.0, -1.0], dtype=torch.float32)

Pxh = torch.matmul(P(x, A), h)

Pxh.backward(torch.ones_like(Pxh))

x.grad
&gt; tensor([ 0.4853, -0.2427])
</code></pre>
","2024-03-06 18:50:32","1","Answer"
"78116027","78056849","","<p>To complement the answer of <a href=""https://stackoverflow.com/a/78063717/6331369"">@Karl</a>, here is an alternative solution that doesn't require detaching tensors. It uses <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html"" rel=""nofollow noreferrer""><code>torch.autograd.functional.jacobian</code></a> to compute and extract the gradient. Here again, we compute <code>dy/dx</code> once and use its computation via the chain rule.</p>
<pre><code># compute dy/dx
dy_dx = jacobian(slow_fun, x)

# compute dz1/dx
dz1_dy = jacobian(lambda y: y**2, y)
dz1_dx = dy_dx*dz1_dy

# compute dz2/dx
dz2_dy = jacobian(lambda y: y.sqrt(), y)
dz2_dx = dy_dx*dz2_dy
</code></pre>
","2024-03-06 16:39:51","0","Answer"
"78115234","","Checking derivative tensor in Pytorch","<p>In <a href=""https://math.stackexchange.com/questions/4561173/derivative-tensor-of-fracaxx-top-ax-top-aa-x-with-a-symmetric-positiv"">this</a> question on Math StackExchange people are discussing the derivative of a function <code>f(x) = Axx'A / (x'AAx)</code> where <code>x</code> is a vector and <code>A</code> is a symmetric, positive semi-definite square matrix.</p>
<p>The derivative of this function at a point <code>x</code> is a tensor. And when &quot;applied&quot; to another vector <code>h</code> it is a matrix. The answers under that post differ in terms of expressions for this matrix, so I would like to check them numerically using <code>Pytorch</code> or <code>Autograd</code>.</p>
<p>Here is my attempt with Pytorch</p>
<pre><code>import torch 

def P(x, A):
    x = x.unsqueeze(1)  # Convert to column vector
    vector = torch.matmul(A, x)
    denom = (vector.transpose(0, 1) @ vector).squeeze()
    P_matrix = (vector @ vector.transpose(0, 1)) / denom
    return P_matrix.squeeze()

A = torch.tensor([[1.0, 0.5], [0.5, 1.3]], dtype=torch.float32)
x = torch.tensor([1.0, 2.0], dtype=torch.float32, requires_grad=True)
h = torch.tensor([2.0, -1.0], dtype=torch.float32)

Pxh = torch.matmul(P(x, A), h)

# compute gradient 
Pxh.backward()
</code></pre>
<p>But this doesn't work. What am I doing wrong?</p>
<h1>JAX</h1>
<p>I am also happy with a Jax Solution. I tried <code>jax.grad</code> but does not work.</p>
","2024-03-06 14:37:08","0","Question"
"78115094","","Issue about PyTorch, predicting without utilizing a DataLoader return distinct predictions compared to employing a DataLoader","<p>I try to predict a single image without using <code>Dataloader</code>, but I get a weird result.</p>
<p><img src=""https://i.sstatic.net/v5m0W.png"" alt=""Terminal Result"" /></p>
<p>This image is the result of my prediction.
With <code>Dataloader</code>, predicted results are consistent with labels.
However, when reading a single image and making a prediction, the resulting label might differ from the expected one, yet the prediction itself remains accurate. For instance, the model predicts all labels as 14, and label 3 may be 25.</p>
<p>I'm new to <code>Pytorch</code>, and confused about this issue. Is this mandatory to apply Dataloader to predict?</p>
<p>The following is my main code:</p>
<pre><code>data_transforms = {
    'train':
    transforms.Compose([
    transforms.Resize(256),
    transforms.RandomRotation(45),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.1),
    transforms.RandomGrayscale(p=0.025),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'valid': transforms.Compose([transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}
   
def loop_prediction(): # wrong label
    correct_count = 0
    size = 10
    for i in range(size):
        # random get a name from './flower_data/valid/{random_number}/*.jpg'
        rand_int = random.randint(2, 3)
        img_file_name = random.choice(os.listdir(f'./flower_data/valid/{rand_int}'))
        img_file = f'./flower_data/valid/{rand_int}/{img_file_name}'
        img = Image.open(img_file)
        # read a image and change to tensor
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        img = transform(img)
        img = img.unsqueeze(0)
        # print(img.shape)
        model_ft.eval()
        with torch.no_grad():
            output = model_ft(img.cuda())

            _, preds_tensor = torch.max(output, 1)
            preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(
                preds_tensor.cpu().numpy())  #

        print('Label', rand_int, ' ', 'Predict:', preds)
        if preds + 1 == rand_int:
            correct_count += 1
   
def batch_prediction(): # correct label
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in
                      ['train', 'valid']}
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in
                   ['train', 'valid']}
    dataiter = iter(dataloaders['valid'])
    images, labels = next(dataiter)
    model_ft.eval()
    print(images.shape, labels.shape)
    if train_on_gpu:
        output = model_ft(images.cuda())
    else:
        output = model_ft(images)
    _, preds_tensor = torch.max(output, 1)
    preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())
    print('Label:', labels, 'Predict:', preds)
</code></pre>
<p>I want to find a way to predict a single image without <code>Dataloader</code> in <code>Pytorch</code>, and get the correct prediction label</p>
<p><a href=""https://i.sstatic.net/VcfVq.png"" rel=""nofollow noreferrer"">Terminal Result</a></p>
","2024-03-06 14:15:23","-1","Question"
"78114492","78107838","","<p>So the issue was the size of the model, once I tried to make smaller one all the problems disappeared.</p>
","2024-03-06 12:46:06","1","Answer"
"78114412","","import torch: How to fix OSError WinError 126, error loading fbgemm.dll or dependencies","<p>I installed the modules below:</p>
<pre><code>conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 -c pytorch
</code></pre>
<p>Then installed numpy, pandas, matplotlib, seaborn, sickit-learn, pyedflib in this environment. Yet upon <code>import</code> it seems some files are missing:</p>
<pre><code>OSError                                   Traceback (most recent call last)
Cell In[3], line 1
----&gt; 1 import torch
      2 from torch import nn
      3 import numpy as np

File d:\anaconda3\envs\RN\lib\site-packages\torch\__init__.py:141
    139                 err = ctypes.WinError(ctypes.get_last_error())
    140                 err.strerror += f' Error loading &quot;{dll}&quot; or one of its dependencies.'
--&gt; 141                 raise err
    143     kernel32.SetErrorMode(prev_error_mode)
    146 def _preload_cuda_deps(lib_folder, lib_name):

OSError: [WinError 126] can't find this module. Error loading &quot;d:\anaconda3\envs\RN\lib\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.
</code></pre>
<p>I reinstalled torch, but I can't solve it.
And I even checked the file 'fbgemm.dll', it is there.</p>
","2024-03-06 12:32:10","9","Question"
"78114404","78111985","","<p>Make sure operations pipe() and tensors reside on GPU by using .to(device) with device defined as device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</p>
","2024-03-06 12:30:46","0","Answer"
"78113207","78112985","","<p>In <a href=""https://pytorch.org/vision/0.8/models.html"" rel=""nofollow noreferrer""><code>torch.models</code></a>, the function to initialize a <strong>ViT-B-16</strong> architecture is not <strike><code>vit_B_16</code></strike> but <a href=""https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html"" rel=""nofollow noreferrer""><code>vit_b_16</code></a>, lowercase &quot;b&quot;.</p>
<p>As <a href=""https://stackoverflow.com/users/21767810/simeonovich"">@simeonovich</a> and <a href=""https://stackoverflow.com/users/349130/dr-snoopy"">@Dr. Snoopy</a> discussed in the comments, you have on the other hand <a href=""https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights"" rel=""nofollow noreferrer""><code>ViT_B_16_Weights</code></a> (capital letters) referring to the weight <em>enum</em>, it refers to the different pretrained_weights: <code>IMAGENET1K_V1</code> (also <code>DEFAULT</code>), <code>IMAGENET1K_SWAG_E2E_V1</code>, and <code>IMAGENET1K_SWAG_LINEAR_V1</code>.</p>
<p>So in summary:</p>
<ul>
<li><p><a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py#L621"" rel=""nofollow noreferrer""><code>torchvision.models.vit_b_16</code></a> (<em>function</em>): returns a <a href=""https://pytorch.org/vision/main/models/vision_transformer.html"" rel=""nofollow noreferrer""><code>VisionTransformer</code></a>;</p>
</li>
<li><p><a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py#L351"" rel=""nofollow noreferrer""><code>torchvision.models.ViT_B_16_Weights</code></a> (<em>enum</em>): is an enum of weights.</p>
</li>
</ul>
","2024-03-06 09:32:29","1","Answer"
"78112985","","module 'torchvision.models' has no attribute 'ViT_B_16_Weights'","<p>I am unable to import ViT_B_16 from torchvision.</p>
<p>I have the below code:</p>
<pre><code>import torch
import torchvision

from torch import nn
from torchvision import transforms

pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT 

pretrained_vit = torchvision.models.vit_B_16(weights=pretrained_vit_weights).to(device)
</code></pre>
<p>and the error is:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
      1 # 1. Get pretrained weights for ViT-Base
----&gt; 2 pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT
      3 
      4 # 2. Setup a ViT model instance with pretrained weights
      5 pretrained_vit = torchvision.models.vit_B_16(weights=pretrained_vit_weights).to(device)

AttributeError: module 'torchvision.models' has no attribute 'ViT_B_16_Weights'

resolve the error of module 'torchvision.models' has no attribute 'ViT_B_16_Weights'
</code></pre>
","2024-03-06 08:56:25","0","Question"
"78111985","","training time seems excessive for training a stable diffusion v1-4 model, given the hardware and hyperparameters","<p>I am training a model using stable-diffusion-v1-4, with around 4900 training dataset size. I use the following to create a pipe. It is run on cuda with Nvidia GPU of 8GB, even though I tried it on google colab the time to train a batch size of 16 wirh number of inference=25 is around 7 minutes, much higher if I increase this number, so for one full training of one epoc it takes around 34 hours which seems very excessive. I am wondering if I am doing something wrong here or is this the best I can hope for given the hardware which is a 8GB GPU on my personal cmputer? All the delay is for this line of code :</p>
<pre><code>        with autocast():
            outputs = pipe(combined_meanings, tokenized_prompts=batch_tokenized_prompts, num_inference_steps=num_steps)[&quot;images&quot;]
</code></pre>
<p>I need to keep the model to sd-v1-4.</p>
<pre><code>from diffusers import StableDiffusionPipeline
model_id = &quot;CompVis/stable-diffusion-v1-4&quot;
pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=&quot;.../models/ldm/stable-diffusion-v1/&quot;)
</code></pre>
<p>I then train run the model using pipe and create associated functions as follows:</p>
<pre><code># Define function to pre-tokenize prompts
def pre_tokenize_prompts(train_dataset, tokenizer):
    &quot;&quot;&quot;
    Pre-tokenize prompts for training.

    Args:
        train_dataset: The training dataset.
        tokenizer: The CLIP tokenizer.

    Returns:
        list: Pre-tokenized prompts.
    &quot;&quot;&quot;
    tokenized_prompts = []
    for entry in train_dataset:
        meanings = [str(entry['meaning'])]
        max_length = 77
        stride = 5
        for start in range(0, len(meanings), stride):
            end = min(start + max_length, len(meanings))
            segment = ' '.join(meanings[start:end])
            tokens = tokenizer.tokenize(segment)
            tokenized_prompts.extend(tokens)
    return tokenized_prompts
        


# Define function to train model
def train(epochs, batch_size, lr, num_steps, train_dataset, device, dataset_length):
    &quot;&quot;&quot;
    Train the model.

    Args:
        epochs (int): Number of epochs.
        batch_size (int): Batch size.
        lr (float): Learning rate.
        num_steps (int): Number of inference steps.
        train_dataset: The training dataset.
        device: The device for computations.
        dataset_length (int): Length of the training dataset.
    &quot;&quot;&quot;
    # Instantiate the CLIP tokenizer
    tokenizer = CLIPTokenizer.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
    # Pre-tokenize prompts
    tokenized_prompts = pre_tokenize_prompts(train_dataset, tokenizer)
    # Define the optimizer
    optimizer = create_optimizer(pipe, lr)
    for epoch in range(epochs):
        for i in range(0, len(train_dataset), batch_size):
            batch = train_dataset[i:i + batch_size]
            meanings = [str(entry['meaning']) for entry in batch]
            images = [entry['image'] for entry in batch]

            # Convert list of meanings to a single string
            combined_meanings = ' '.join(meanings)

            # Zero the gradients
            optimizer.zero_grad()

            # Use pre-tokenized prompts
            start_idx = i * len(tokenized_prompts)
            end_idx = (i + batch_size) * len(tokenized_prompts)
            batch_tokenized_prompts = tokenized_prompts[start_idx:end_idx]
                          
        
            # Print the meanings for each batch
            print('combined meanings: ', combined_meanings)
            
            # Forward pass
            start_time = time.time()
            
            with autocast():
                outputs = pipe(combined_meanings, tokenized_prompts=batch_tokenized_prompts, num_inference_steps=num_steps)[&quot;images&quot;]
            
</code></pre>
","2024-03-06 05:23:08","0","Question"
"78111173","","torchserve : batch_size is always 1 even config.properties specify other value","<p>I think my torchserve loaded config.properties correctly because the number of worker is 2 as I set. But the batch_size is 1 instead of 20.</p>
<p>Anyone has an idea what might go wrong ? Thanks !</p>
<p>I have checked and torchserve load config.properties correctly, alas it ignored the batch_size and max_batch_delay specified in config.properties.</p>
<p>Here is my config.properties for the reference</p>
<pre><code>inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
log_file=/ml_server/logs/torchserve.log
default_workers_per_model=2
number_of_netty_threads=32
job_queue_size=1000
batch_size=20
max_batch_delay=10
</code></pre>
<p>Below is the log, worker with batchSize: 1</p>
<pre><code>ml-server  | 2024-03-06T00:11:11,091 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: _model, batchSize: 1
ml-server  | 2024-03-06T00:11:11,091 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: _model, batchSize: 1
</code></pre>
","2024-03-06 00:19:31","0","Question"
"78110765","78104467","","<p><code>transforms</code> are expected to take as an input one data point (an image in this case) and return a single transformed data point,thus patching an image using a custom <code>transform</code> and returning a list of patches is not possible for now.</p>
<p>A possible solution is to provide a custom implementation for the <code>collate_fn</code> function and pass it as an argument to the <code>DataLoader</code> class.</p>
<p>The <code>collate_fn</code> function takes as an input a list of tuples (the first element of the tuple is the data point and the second is the label),and returns a tuple of tow tensors,the first tensor represents a batch of images and the second one represents the corresponding labels.</p>
<p>Below you find a possible implementation of the functionality that you want :</p>
<pre class=""lang-python prettyprint-override""><code>def make_paches(
    img : torch.Tensor,
    patch_width : int,
    patch_height : int
) -&gt; list[torch.Tensor]:

    patches = img \
        .unfold(1,patch_width,patch_width) \
        .unfold(2,patch_height,patch_height) \
        .flatten(1,2) \
        .permute(1,0,2,3)

    patches = list(patches)
    return patches

def collate_fn(batch : list[tuple[torch.Tensor, int]]) -&gt; tuple[torch.Tensor, torch.Tensor]:
    
    new_x = []
    new_y = []
    
    for x, y in batch:
        patches = make_paches(x, 224, 224)
        new_x.extend(patches)
        new_y.extend([y for _ in range(len(patches))])

    new_x = torch.stack(new_x)
    new_y = torch.tensor(new_y)
    
    return new_x,new_y
</code></pre>
<pre class=""lang-python prettyprint-override""><code>dataset = datasets.ImageFolder(root=&quot;&lt;your-path&gt;&quot;, transform=transform)
            
dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
</code></pre>
","2024-03-05 21:57:25","2","Answer"
"78110673","78107838","","<p>Here's an updated version of your training code incorporating the suggestions:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

criterion = nn.MSELoss()  # Use Mean Squared Error for image reconstruction
print('crit')

autoencoder = Autoencoder().to(device)
print('deviced')
</code></pre>
<p>If the problem persists, try addressing the points mentioned above and let me know if you encounter any specific errors or if you have additional details about the issue.</p>
","2024-03-05 21:34:06","-2","Answer"
"78110552","78089532","","<p>I resolved the problem by replacing the following line</p>
<pre><code>self.log('learning_rate', lr)
</code></pre>
<p>with</p>
<pre><code>self.log('learning_rate', lr, on_step=False, on_epoch=True, prog_bar=True)
</code></pre>
","2024-03-05 21:07:19","1","Answer"
"78110096","78097242","","<p>Turns out I haven't added commands like <code>--output</code>, <code>--device</code>, etc. that can be found in the <code>file_builder.py</code> source (in <code>parser.add_argument(...)</code>). There were no instructions on how to use this file, so I figured it out myself.</p>
","2024-03-05 19:20:44","0","Answer"
"78108642","78103531","","<p>I have created Http trigger function with runtime stack python. I have added some sample code about Pandas and Torch below:</p>
<p><strong>function code:</strong></p>
<pre><code>import logging
import azure.functions as func
import torch
import pandas as pd

def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    # Sample data for demonstration
    data = {'A': [1, 2, 3, 4, 5], 'B': [6, 7, 8, 9, 10]}
    df = pd.DataFrame(data)

    # Create a PyTorch tensor from pandas DataFrame
    tensor = torch.tensor(df.values)

    # Perform a simple operation using PyTorch
    result = tensor.sum().item()

    # Construct response message
    if result:
        return func.HttpResponse(f&quot;The sum of all elements in the tensor is: {result}&quot;, status_code=200)
    else:
        return func.HttpResponse(
            &quot;This HTTP triggered function executed successfully. However, there was an issue calculating the result.&quot;,
            status_code=200
        )

</code></pre>
<p><strong>requirement.txt:</strong></p>
<pre><code>azure-functions
torch
sentence-transformers
pandas
</code></pre>
<ul>
<li>The code runs successfully in local environment. check below:</li>
</ul>
<p><strong>Output:</strong>
<img src=""https://i.imgur.com/gj1uSrE.png"" alt=""enter image description here"" /></p>
<p>while deploying the above function into azure portal i am also getting same error.</p>
<p><img src=""https://i.imgur.com/fofhB7k.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have tried another approach like publishing the function into azure by using this <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-cli-powershell?tabs=windows%2Cazure-cli%2Cbrowser"" rel=""nofollow noreferrer"">DOC</a>.</li>
</ul>
<p><strong>Deployment status:</strong></p>
<p><img src=""https://i.imgur.com/i0Ln7AR.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/ZB7liZ5.png"" alt=""enter image description here"" /></p>
<ul>
<li>I have tried with zip deployment method also, check this <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/deployment-zip-push?tryIt=true&amp;source=docs#code-try-0"" rel=""nofollow noreferrer"">DOC</a></li>
</ul>
<pre><code>az functionapp deployment source config-zip -g &lt;resource_group&gt; -n &lt;app_name&gt; --src &lt;zip_file_path&gt;
</code></pre>
<p><img src=""https://i.imgur.com/lnAvxfG.png"" alt=""enter image description here"" /></p>
<ul>
<li>While deploying the function into azure by using zip deployment ensure that basic authentication have enabled and until completed the deployment check the basic authentication. check below for enabling.</li>
</ul>
<p><img src=""https://i.imgur.com/VGvfoHj.png"" alt=""enter image description here"" /></p>
","2024-03-05 14:59:12","0","Answer"
"78107928","","'MistralForCausalLM' object has no attribute 'merge_and_unload""","<p>I finetuned (or further pretrained) the Model OpenChat (a Mistral 7B finetuning) on my own data. This worked well and the inference produces nice results. Now I want to merge the adapter weights with the original model, to quantize the model in a further step. The issue is that calling model.merge_and_unload() produces the error:</p>
<pre><code> &quot;AttributeError: 'MistralForCausalLM' object has no attribute 'merge_and_unload&quot;.
</code></pre>
<p>Is there a way to fix this or another method to merge my weight adapters with the original model?</p>
<p>Here is my code:</p>
<p><strong>Training</strong></p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = &quot;openchat/openchat-3.5-1210&quot;
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={&quot;&quot;:0})


from peft import prepare_model_for_kbit_training

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)




from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,],
    lora_dropout=0.05,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;
)

model = get_peft_model(model, config)


data_train = ...



import transformers
from trl import SFTTrainer
from transformers import TrainingArguments


tokenizer.pad_token = tokenizer.eos_token

trainer = transformers.Trainer(
    model=model,
    train_dataset=data_train,
    args=transformers.TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=2,
        #eval_steps=100,
        logging_dir=&quot;./logs&quot;,
        #max_steps=10,   
        num_train_epochs=1,  
        #evaluation_strategy=&quot;steps&quot;,   
        logging_strategy=&quot;steps&quot;,           
        learning_rate=2e-4,        
        fp16=True,
        logging_steps=5 ,
        save_total_limit=3,
        output_dir=&quot;outputs&quot;,
        optim=&quot;paged_adamw_8bit&quot;
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model.config.use_cache = False

trainer.train()

trainer.save_model(&quot;pretrained_model&quot;)
tokenizer.save_pretrained(&quot;pretrained_model&quot;)
</code></pre>
<p><strong>Merging</strong></p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

from peft import prepare_model_for_kbit_training

from peft import (
    LoraConfig,
    PeftConfig,
    PeftModel,
    get_peft_model,
    prepare_model_for_kbit_training
)

model_id = &quot;openchat/openchat-3.5-1210&quot;
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16
)


model_name = &quot;pretrained_model&quot;

config = PeftConfig.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    quantization_config=bnb_config,
    device_map=&quot;auto&quot;,
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name)


model.merge_and_unload()
</code></pre>
","2024-03-05 13:03:24","1","Question"
"78107838","","I cannot put PyTorch model to device (.to(device))","<p>So I was writing my first ever autoencoder, here is the code (it can be a little bit goofy, but I believe I written all of it right):</p>
<pre><code>class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        
        self.flatten = nn.Flatten()
        
        self.enc_conv0 = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(64),

            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(128)
        )
        
        self.enc_conv1 = nn.Sequential(
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(256),

            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(512)
        )
        
        self.enc_fc = nn.Sequential(
            nn.Linear(in_features=512*64*64, out_features=4096),
            nn.ReLU(),
            nn.BatchNorm1d(4096),
            
            nn.Linear(in_features=4096, out_features=2048),
            nn.ReLU(),
            nn.BatchNorm1d(2048),
            
            nn.Linear(in_features=2048, out_features=dim_code)
        )
        
        self.dec_fc = nn.Sequential(
            nn.Linear(in_features=dim_code, out_features=2048),
            nn.ReLU(),
            nn.BatchNorm1d(2048),
            
            nn.Linear(in_features=2048, out_features=4096),
            nn.ReLU(),
            nn.BatchNorm1d(4096),
            
            nn.Linear(in_features=4096, out_features=512*64*64),
            nn.ReLU(),
            nn.BatchNorm1d(512*64*64)
        )
        
        self.dec_conv0 = nn.Sequential(
            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=(3,3), padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(256),
            
            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3,3), padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(128),
        )
        
        self.dec_conv1 = nn.Sequential(
            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(3,3), padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            
            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=(3,3), padding=1)
        )

    def forward(self, x):
        e0 = self.enc_conv0(x)
        e1 = self.enc_conv1(e0)
        latent_code = self.enc_fc(self.flatten(e1))
        
        d0 = self.dec_fc(latent_code)
        d1 = self.dec_conv0(d0.view(-1, 512, 64, 64))
        reconstruction = self.dec_conv1(d1)

        return reconstruction, latent_code
</code></pre>
<p>And then I was preparing to train it with the next cell of code:</p>
<pre><code>`device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

criterion = nn.BCELoss()
print('crit')

autoencoder = Autoencoder().to(device)
print('deviced')`
</code></pre>
<p>Cell prints:
cuda
'crit'</p>
<p>And then just stalks infinitely, filling the RAM and CPU at its full (im doing everything on kaggle notebook). And I dont get why. :(</p>
<p>Tried to launch the same notebook in Google colab instead of Kaggle, but it just crashed with error about trying to allocate resources that are not accessable.</p>
<p>Also I thought the issue could had something to do with the first line after initiation of a class, so I replaced</p>
<pre><code>def __init__(self):
        super().__init__()
</code></pre>
<p>with</p>
<pre><code>def __init__(self):
        super(Autoencoder, self).__init__()
</code></pre>
<p>like I saw in some tutorials (honestly I don't know what this lines do, it just written in every other similar cases)
But it also didnt worked</p>
","2024-03-05 12:47:03","0","Question"
"78107272","78104467","","<p>A possible solution to patch an image tensor using <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html#torch-tensor-unfold"" rel=""nofollow noreferrer""><code>torch.Tensor.unfold</code></a>:</p>
<pre><code>class Patch(nn.Module):
    def __init__(self, patch_size):
        super().__init__()
        self.patch_size = patch_size

    def forward(self, x):
        b, c, h, w = x.shape
        ph, pw = self.patch_size
        out = x.unfold(-2, ph, ph).unfold(-1, pw, pw)
        out = out.contiguous().view(b, c, -1, ph, pw).permute(0,2,1,4,3)
        return out
</code></pre>
<p><a href=""https://i.sstatic.net/C584t.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C584t.jpg"" alt=""enter image description here"" /></a></p>
<p><sup><em>source: <a href=""https://en.wikipedia.org/wiki/Main_Page#/media/File:The_One_Bel_Air_House_by_Wallace_Lin.jpg"" rel=""nofollow noreferrer"">The One Bel Air House by Wallace Lin</a></em></sup></p>
","2024-03-05 11:10:54","2","Answer"
"78106375","78105436","","<p>In case your model is a (custom) PyTorch model, you can leverage the <code>PyTorchModelHubMixin</code> class available in the <code>huggingface_hub</code> Python library. It is a minimal class which adds <code>from_pretrained</code> and <code>push_to_hub</code> capabilities to any <code>nn.Module</code>, along with download metrics.</p>
<pre><code>import torch
import torch.nn as nn
from huggingface_hub import PyTorchModelHubMixin


class MyModel(nn.Module, PyTorchModelHubMixin):
    def __init__(self, config: dict):
        super().__init__()
        self.param = nn.Parameter(torch.rand(config[&quot;num_channels&quot;], config[&quot;hidden_size&quot;]))
        self.linear = nn.Linear(config[&quot;hidden_size&quot;], config[&quot;num_classes&quot;])

    def forward(self, x):
        return self.linear(x + self.param)

# create model
config = {&quot;num_channels&quot;: 3, &quot;hidden_size&quot;: 32, &quot;num_classes&quot;: 10}
model = MyModel(config=config)

# save locally
model.save_pretrained(&quot;my-awesome-model&quot;, config=config)

# push to the hub
model.push_to_hub(&quot;my-awesome-model&quot;, config=config)

# reload
model = MyModel.from_pretrained(&quot;username/my-awesome-model&quot;)
</code></pre>
<p>Here is a link to the huggingface docs, explaining how to push pytorch model to the huggingface hub.</p>
<p><a href=""https://huggingface.co/docs/hub/en/models-uploading#upload-a-pytorch-model-using-huggingfacehub"" rel=""nofollow noreferrer"">https://huggingface.co/docs/hub/en/models-uploading#upload-a-pytorch-model-using-huggingfacehub</a></p>
","2024-03-05 08:44:30","1","Answer"
"78105436","","Convert PyTorch Model to Hugging Face model","<p>I have looked at a lot resources but I still have issues trying to convert a PyTorch model to a hugging face model format. I ultimately want to be able to use inference API with my custom model.</p>
<p>I have a &quot;model.pt&quot; file which I got from fine-tuning the Facebook Musicgen medium model (<a href=""https://github.com/chavinlo/musicgen_trainer"" rel=""nofollow noreferrer"">The Git repo I used to train / Fine tune the model is here</a>). I want to upload this to the hugging face hub so i can use this with inference API. How can I convert the <code>.pt</code> model to files/model that can be used on hugging face hub? I tried looking at other posts but there is no clear answer, or it is poorly explained.</p>
<p>Any help / guidance would be greatly appreciated 🙏</p>
<p>This is the code I have right now that is not working:</p>
<pre><code>import torch
from transformers import MusicgenConfig, MusicgenModel
from audiocraft.models import musicgen
import os

os.mkdir('models')

state_dict = musicgen.MusicGen.get_pretrained('facebook/musicgen-medium', device='cuda').lm.load_state_dict(torch.load('NEW_MODEL.pt'))

config = MusicgenConfig.from_pretrained('facebook/musicgen-medium')
model = MusicgenModel(config)
model.load_state_dict(state_dict)

model.save_pretrained('/models')

loaded_model = MusicgenModel.from_pretrained('/models')
</code></pre>
","2024-03-05 05:09:24","2","Question"
"78104756","","How to Properly Manage GPU Memory Between Successive PyTorch Training Phases using accelerate?","<p>I'm encountering a challenging issue with GPU memory not being released properly between successive training phases in PyTorch, leading to CUDA out of memory errors.</p>
<p>My project involves fine-tuning a model in two consecutive phases:</p>
<ul>
<li>first on a FP (Further pretraining Phase) dataset,</li>
<li>and then on an SFT (Supervised Fine-tuning) dataset.</li>
</ul>
<p>The code structure is as follows:</p>
<pre><code>from transformers import AutoModelForCausalLM

# Model and data loader initialization
model = AutoModelForCausalLM.from_pretrained(args.model)  # fig.1
fp_data_loader = data_loader(fp_dataset)
sft_data_loader = data_loader(sft_dataset)

# First phase of training
fp_model, fp_loss = train_loop(model, fp_data_loader) #fig.2
fp_model.module.save_pretrained(checkpoint_dir)

# Attempt to release GPU memory
del model, fp_data_loader,
# del fp_model
# fp_model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)
gc.collect()
torch.cuda.empty_cache()  #fig.3

# Second phase of training
sft_model, sft_loss = train_loop(fp_model, sft_data_loader)

</code></pre>
<p><a href=""https://i.sstatic.net/UEStv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UEStv.png"" alt=""enter image description here"" /></a><br />
fig.1: when model is on gpu</p>
<p><a href=""https://i.sstatic.net/99bTI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/99bTI.png"" alt=""enter image description here"" /></a><br />
fig.2: during training</p>
<p><a href=""https://i.sstatic.net/jz0jC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jz0jC.png"" alt=""enter image description here"" /></a><br />
fig.3: after empty_cache (capture issue: ignore gpu0's 77762/81920, it is 57524/81920)</p>
<p>My expectation was that the gpu allocation of fig.1 would be like after empty_cache, but there is quite a lot of gpu memory allocated as in fig.3</p>
<p>Despite explicitly deleting the model and data loader used in the first phase and calling gc.collect() and torch.cuda.empty_cache(), the GPU memory does not seem to be fully released. As a result, when initiating the second training phase, I'm faced with a CUDA out of memory error.</p>
<p>I also wrap my model, optimizer, and data loaders with accelerator.prepare() for mixed precision and distributed training, which might be relevant.</p>
<p>Has anyone faced similar issues or has suggestions on ensuring that GPU memory is properly released between training phases? I've considered completely restarting the process between phases but would prefer a cleaner solution if possible.</p>
<p>PS: This question was machine translated from Korean, apologies if there is awkward language.</p>
","2024-03-05 00:35:59","2","Question"
"78104467","","How to load a batch of images of and split them into patches on the fly with PyTorch>","<p>I want to load a batch of images of different resolutions and split them into non-overlapping patches of equal sizes on the fly to feed them to a Resnet18 model, is there an existing transform class in PyTorch that does this, if not how do I implement my own class.</p>
<p>Here's the code:</p>
<pre class=""lang-py prettyprint-override""><code>transform = transforms.Compose([
    ImageResizer(), # Custom class to resize the image to the next multiple of 224 (takes as input PIL image and returns PIL image) 
    #Patch(patch_size=(224, 224)), # Custom class to divide the image into patches of 224x224 (takes as input PIL image and returns a list of PIL images)
    transforms.ToTensor(),
])

dataset = ImageFolder(root=&quot;&lt;path&gt;&quot;, transform=transform)

batch_size = 32
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
</code></pre>
<p>Here's how my ImageResizer code looks:</p>
<pre class=""lang-py prettyprint-override""><code>class ImageResizer:
    &quot;&quot;&quot;
    A class to resize the image to the next multiple of 224, so that the images can be divided into 224x224 patches later.
    &quot;&quot;&quot;

    def __init__(self):
        pass

    def get_new_dimensions(width : int, height : int, patch_height : int = 224, patch_width : int = 224):
        &quot;&quot;&quot;
        Get the new dimensions of the image after resizing.

        Parameters:
        - width: The width of the image.
        - height: The height of the image.
        - patch_height: The height of the patch.
        - patch_width: The width of the patch.

        Returns:
        - new_height: The new height of the image.
        - new_width: The new width of the image.
        &quot;&quot;&quot;
    
        width_coef = int(np.round(width / patch_width).astype(np.int32))
        height_coef = int(np.round(height / patch_height).astype(np.int32))

        new_width = width_coef * patch_width
        new_height = height_coef * patch_height

        return new_width, new_height

    def __call__(self, image):
        &quot;&quot;&quot;
        Resize the given image to the next multiple of 224.

        Parameters:
        - image: an image of type pillow.

        Returns:
        - resized_image: The resized image of type pillow.
        &quot;&quot;&quot;

        width, height = image.size

        new_width, new_height = ImageResizer.get_new_dimensions(width, height)

        # Resize the image
        resized_image = image.resize((new_width, new_height))

        return resized_image
</code></pre>
","2024-03-04 22:56:18","0","Question"
"78104190","78102820","","<p>The issue is here</p>
<pre class=""lang-py prettyprint-override""><code>for _, batch in enumerate(loader):
    ...
    grad = loss.backward()
    ...
    for param in theta:
        grad.append(param.grad)
    ...
</code></pre>
<p><code>backward</code> does not return a value. When you run <code>grad = loss.backward()</code>, you are assigning <code>grad = None</code>. Later, you attempt to append values to <code>None</code> via <code>grad.append(param.grad)</code>, hence the error.</p>
","2024-03-04 21:37:52","0","Answer"
"78104180","78081304","","<p>The discrepancy is caused by dropout in the encoder layer. You can fix this by passing <code>dropout=0.0</code> to <code>TransformerEncoderLayer</code></p>
<pre class=""lang-py prettyprint-override""><code>encoder = nn.TransformerEncoder(
    nn.TransformerEncoderLayer(
        d_model=input_size,
        nhead=1,
        batch_first=True,
        dropout=0.0
    ),
    num_layers=1,
    norm=None,
)
</code></pre>
","2024-03-04 21:35:13","1","Answer"
"78104019","78102820","","<p>You defined <code>grad</code> the following way in your code:</p>
<pre><code>        grad = loss.backward()
</code></pre>
<p>You are getting this error because <a href=""https://pytorch.org/docs/stable/_modules/torch/autograd.html#backward"" rel=""nofollow noreferrer""><code>torch.Tensor.backward</code></a> returns precisely <code>None</code>.</p>
","2024-03-04 20:54:45","0","Answer"
"78103531","","Failed to upload azure functions app from vscode when using pytorch","<p>I am trying to upload a function app, that contains pytorch, from vscode, but I fail. The output says the following:</p>
<pre><code>3:51:18 PM CartProvider: Running pip install...
3:51:25 PM CartProvider: Done in 7 sec(s).
3:51:25 PM CartProvider: [18:51:18+0000] Collecting azure-functions
3:51:25 PM CartProvider: [18:51:18+0000]   Downloading azure_functions-1.18.0-py3-none-any.whl (173 kB)
3:51:25 PM CartProvider: [18:51:19+0000] Collecting torch
3:51:25 PM CartProvider: [18:51:19+0000]   Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)
3:51:25 PM CartProvider: &quot;2024-03-04 18:51:25&quot;|ERROR|[18:51:18+0000] Collecting azure-functions
3:51:25 PM CartProvider: [18:51:18+0000]   Downloading azure_functions-1.18.0-py3-none-any.whl (173 kB)
3:51:25 PM CartProvider: [18:51:19+0000] Collecting torch
3:51:25 PM CartProvider: [18:51:19+0000]   Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB) | Exit code: 137 | Please review your requirements.txt | More information: https://aka.ms/troubleshoot-python
3:51:27 PM CartProvider: /opt/Kudu/Scripts/starter.sh oryx build /tmp/zipdeploy/extracted -o /home/site/wwwroot --platform python --platform-version 3.10.4 -p packagedir=.python_packages/lib/site-packages
3:51:31 PM CartProvider: Deployment failed.
</code></pre>
<p>Here is my requirements.txt file:</p>
<p><code>azure-functions torch sentence-transformers pandas </code></p>
<p>it's supposed to have every dependency in a newline, but I'm new to this.</p>
<p>My code is a simple http trigger with an input binding, the function is given a list of product names, and it returns the most semantically similar products within the input database. The code works perfectly when testing in a local environment.</p>
<p>Please help :)</p>
<p>Below is the function_app.py code:
<a href=""https://i.sstatic.net/kMvhT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kMvhT.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/stIW0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/stIW0.png"" alt=""enter image description here"" /></a></p>
","2024-03-04 19:05:21","0","Question"
"78103260","78097861","","<p>On my Windows 11 machine with:</p>
<ul>
<li>
<pre class=""lang-text prettyprint-override""><code>C:\Users\Foo&gt;python3 --version
Python 3.12.2
</code></pre>
</li>
<li>
<pre class=""lang-text prettyprint-override""><code>C:\Users\Foo&gt;pip3 --version
pip 24.0 from C:\Users\Foo\PathToPython312\site-packages\pip (python 3.12)
</code></pre>
</li>
<li>
<pre class=""lang-text prettyprint-override""><code>C:\Users\Foo&gt;pip3 show torch torchvision torchaudio PySoundFile
WARNING: Package(s) not found: torch torchvision torchaudio PySoundFile
</code></pre>
<p>No previous installs of <code>torch</code>, <code>torchvision</code>, <code>torchaudio</code> or <code>PySoundFile</code></p>
</li>
</ul>
<hr />
<p>I opened command prompt (not as administrator) and ran:</p>
<pre class=""lang-text prettyprint-override""><code>pip3 install torch torchvision torchaudio
</code></pre>
<p>Which installed:</p>
<pre class=""lang-text prettyprint-override""><code>torch              2.2.1
torchaudio         2.2.1
torchvision        0.17.1
</code></pre>
<p><strong>Note:</strong> several other dependency packages were installed along with the packages above.</p>
<p>I then ran <code>python3 ./test.py</code>:</p>
<pre class=""lang-python prettyprint-override""><code># ./test.py

import torchaudio
print(str(torchaudio.list_audio_backends()))
</code></pre>
<p>Which output an empty list:</p>
<pre class=""lang-text prettyprint-override""><code>[]
</code></pre>
<p>So I then ran:</p>
<pre class=""lang-text prettyprint-override""><code>pip3 install PySoundFile
</code></pre>
<p>Which installed:</p>
<pre class=""lang-text prettyprint-override""><code>PySoundFile        0.9.0.post1
</code></pre>
<p>I then re-ran <code>python3 ./test.py</code> which output:</p>
<pre class=""lang-text prettyprint-override""><code>['soundfile']
</code></pre>
<hr />
<p>I suggest uninstalling all related packages:</p>
<pre class=""lang-text prettyprint-override""><code>pip3 uninstall torch torchvision torchaudio PySoundFile
</code></pre>
<p>You'll probably also want to uninstall <code>soundfile</code>, since it's in the list of commands you tried.</p>
<p>I would then inspect the output of <code>pip3 list</code> and ensure the packages are no longer there. Then follow the steps I took above, in order, and see if you can get <code>['soundfile']</code> as output.</p>
<p><strong>Note:</strong> I do not use anaconda, so if the steps above do not resolve your issue then I'd say there's a good chance that it's causing the problem.</p>
","2024-03-04 18:02:57","8","Answer"
"78102942","","Why I am getting an error while trying to install gym[all]?","<p>I am trying to install <code>gym[all]</code>, but I am getting this error message every time:</p>
<pre><code>Collecting pygame==2.1.0 (from gym[all])
  Using cached pygame-2.1.0.tar.gz (5.8 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─&gt; [77 lines of output]
      
      
      WARNING, No &quot;Setup&quot; File Exists, Running &quot;buildconfig/config.py&quot;
      Using WINDOWS configuration...
</code></pre>
<p>And after all the output, the message is this:</p>
<pre><code>  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed
</code></pre>
<p>If I try to install <code>box2d-py</code> on Google Colab, here is the error that I get:</p>
<pre><code>Building wheels for collected packages: box2d-py
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Building wheel for box2d-py (setup.py) ... error
  ERROR: Failed building wheel for box2d-py
  Running setup.py clean for box2d-py
Failed to build box2d-py
ERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects
</code></pre>
<p>Does anybody know what is going on here?</p>
<p>Thanks!</p>
<p>I have already installed Visual Studio Build Tools. I have a similar error when trying to install on Google Colab. My Python version is <code>Python 3.11.3</code>.</p>
","2024-03-04 17:00:41","1","Question"
"78102820","","Extracting the gradient during PyTorch fit functions","<p>I am trying to train a neural network until the L2-norm of its gradient is within 10e-3 of 0; therefore, my code includes defining the parameters and gradients that are computed during the fit process. I keep hitting snags that make me think I am not getting at the parameters or gradient correctly.</p>
<p>Here is my code:</p>
<pre><code>def get_theta(self):
    theta = self.parameters().detach().cpu
    return theta

def J_loss(self, xb, yb):
    #forward returns x so here it will return x on GPU
    #return cross_entropy result of xb and yb on GPU
    return F.cross_entropy(self.forward(xb.to(device)), yb.to(device))

def fit(self, loader, epochs = 1999):
    norm2Gradient = 1
    while norm2Gradient &gt;10e-3 and epochs &lt;2000:
        #grad = []
        for _, batch in enumerate(loader):
            x, y = batch['x'], batch['y']
            #computes f.cross_entropy loss of (xb,yb) on GPU 
            loss = self.J_loss(x,y) 
            #print(&quot;loss:&quot;, loss)
            #computes new gradients
            grad = loss.backward()
            #print(&quot;grad:&quot;,grad)
            print(&quot;grad?&quot;,grad)
            #takes one step along new gradients to decrease the loss; updates parameters 
            self.optimizer.step()  
            #captures new parameters
            theta = self.parameters()
            print(&quot;theta:&quot;,theta)
            #collects gradient along new parameters
            for param in theta:
                grad.append(param.grad)
            #computes gradient norm
            norm2Gradient = torch.linalg.norm(grad)
            sumNorm2Gradient += norm2Gradient.detach().cpu
            #clears out old gradients  
            self.optimizer.zero_grad()
    return sumNorm2Gradient
</code></pre>
<p>The current error message, &quot;AttributeError: 'NoneType' object has no attribute 'append'&quot; occurs at the line:</p>
<pre><code>grad.append(param.grad)
</code></pre>
<p>Additionally, the print out of the variable &quot;grad&quot; is &quot;None&quot;. I have combed through documentation trying to figure out what each line is doing in the code and how to extract the gradient and parameters. How do I correctly get at the gradient?</p>
","2024-03-04 16:40:06","0","Question"
"78102720","78102637","","<p>The pytorch SGD implementation is actually independent of the batching!
It only uses the gradients that were calculated and stored in the parameters <code>.grad</code> attribute in the backward pass.
So the batch size used for calculations and the batch size used for optimization are decoupled.</p>
<p>You can now either:</p>
<p>a) Put all your samples as one big batch through your model by setting the batchsize to the dataset size or</p>
<p>b) Accumulate the gradients for many smaller batches before doing a single step of the optimizer (Pseudo-code):</p>
<pre><code>model = YourModel()
data = YourDataSetOrLoader()
optim = torch.optim.SGD(model.parameters())
for full_batch_step in range(100)
   #this sets the accumulated gradient to zero
   optim.zero_grad()
   for batch in data:
      f=model(data)
      # this adds the gradient wrt to the parameters for the current datapoint to the model paramters
      f.backward()
  # now after we summed the gradient for all samples, we do a GD step.
  optim.step()
   
</code></pre>
","2024-03-04 16:25:11","2","Answer"
"78102637","","How to Implement Full Batch Gradient Descent with Nesterov Momentum in PyTorch?","<p>I'm working on a machine learning project in PyTorch where I need to optimize a model using the full batch gradient descent method. The key requirement is that the optimizer should use all the data points in the dataset for each update. My challenge with the existing torch.optim.SGD optimizer is that it doesn't inherently support using the entire dataset in a single update. This is crucial for my project as I need the optimization process to consider all data points to ensure the most accurate updates to the model parameters.</p>
<p>Additionally, I would like to retain the use of Nesterov momentum in the optimization process. I understand that one could potentially modify the batch size to equal the entire dataset, simulating a full batch update with the SGD optimizer. However, I'm interested in whether there's a more elegant or direct way to implement a true Gradient Descent optimizer in PyTorch that also supports Nesterov momentum.</p>
<p>Ideally, I'm looking for a solution or guidance on how to implement or configure an optimizer in PyTorch that meets the following criteria:</p>
<ul>
<li>Utilizes the entire dataset for each parameter update (true Gradient Descent behavior).</li>
<li>Incorporates Nesterov momentum for more efficient convergence.</li>
<li>Is compatible with the rest of the PyTorch ecosystem, by subclassing torch.optim.Optimizer</li>
</ul>
","2024-03-04 16:10:18","0","Question"
"78102449","78100922","","<p>You can achieve this using PyTorch's torch.split function along with list comprehension to split the array according to the provided indices.</p>
<pre><code>import torch

input_array = torch.arange(20)
splits = [1, 2, 5, 10]

result = [input_array.split(split) for split in splits]
print(tuple(result))
</code></pre>
","2024-03-04 15:37:27","0","Answer"
"78101869","78088764","","<p>See that you have a very defined model you want to train: Y = a*X.</p>
<p>So your model architecture is just the parameter <code>a</code>, and the forward pass is just multiplying that parameter with the inputs.</p>
<p>Also, that model works in a regression format (the output is a continuous value), so cross-entropy (normally used for multi-class classification) is not the correct choice, instead a Mean Squared Error would make more sense (you want predicted outputs to be as close to the real ones).</p>
<p>Also also, the exercise asks you to use only the provided X and Y. In such case, and given that the model is so simple that only one value of <code>a</code> is the optimal solution (i.e. it is a convex optimization problem), you are better off just doing a single optimization step.</p>
<p>The following implements that:</p>
<pre class=""lang-py prettyprint-override""><code>from torch.nn import Parameter
from torch.optim import Adam


class FindParameter(Module):
    def __init__(self):
        super(FindParameter, self).__init__()
        self.a = Parameter(data=1.0)

    def forward(self, input):
        return self.a * input


X = ...
Y = ...


model = FindParameter()
loss_f = MSELoss()
optimizer = Adam(model.parameters(), lr=1e-3)

epochs = 100

for epoch in epochs:
    optimizer.zero_grad()
    y_pred = model(X)

    loss = loss_f(y_pred, Y)
    loss.backward()
    optimizer.step()
</code></pre>
","2024-03-04 14:14:57","1","Answer"
"78101580","78088764","","<p>We show in our R package 'cito' (which simplifies the training of NN) how you can use NN to optimize arbitrary functions (the only requirement is that they must be differentiable and written in Torch). Your question is similar to our <a href=""https://citoverse.github.io/cito/articles/D-Advanced_custom_loss_functions.html#example-3-using-cito-for-optimization-active-learning"" rel=""nofollow noreferrer"">example</a>:</p>
<p>Simulate some data from a linear model (true slope = 2, true sigma = 0.4)</p>
<pre><code>library torch
library(cito)  
X = runif(200)
Y = 2*X + rnorm(200, sd = 0.4)
df = data.frame(X = X, Y = Y)
</code></pre>
<p>Function we want to optimize (linear model):</p>
<pre><code>Xt = torch_tensor(matrix(X))
Yt = torch_tensor(matrix(Y))

model_lm = function(par) {
  pred = Xt$matmul(par[,1,drop=FALSE])
  loss = -torch::distr_normal(pred, scale = torch::torch_exp(par[,2,drop=FALSE]))$log_prob(Yt)
  return(loss$mean())
}
</code></pre>
<p>The actual loss function:</p>
<pre><code>custom_loss = function(pred, true, ...) {
  if(nrow(pred) &gt; 1) return(torch_zeros(1L)) # disable loss calculation
  loss = model_lm(pred)
  return(loss)
}
</code></pre>
<p>X and Y values don't matter, number of columns in Y has to match the number of parameters we want to optimize, here two column, one for the slope parameter and one for the sigma parameter. We feed noise to our neural network and it predicts the two parameter (similar to the generator in a GAN):</p>
<pre><code>noise = matrix(runif(300*5), 300, 5)
noise_y = matrix(runif(300*2), 300, 2)
df = data.frame(y1 = noise_y[,1], y2 = noise_y[,2], noise)
</code></pre>
<p>Fit the final model:</p>
<pre><code>m = dnn(cbind(y1, y2)~., data = df, loss = custom_loss, batchsize = 1L, epochs = 20L, verbose = FALSE) 
</code></pre>
<p>Results:</p>
<pre><code># Effect:
mean(predict(m)[,1])
#&gt; [1] 2.02776
# SD
mean(exp(predict(m)[,2]))
#&gt; [1] 0.3864421
</code></pre>
","2024-03-04 13:28:24","0","Answer"
"78101386","78081304","","<p>I think the problem is that you are not selecting the encoder in eval mode. If you add <code>encoder.eval()</code> just before <code>out1000 = encoder(src, src_key_padding_mask=padding_mask)</code> it should do it correctly</p>
","2024-03-04 12:53:22","1","Answer"
"78101018","78100922","","<p>Another possible option would be to <em>slice</em> the tensor first, then <a href=""https://pytorch.org/docs/stable/generated/torch.split.html"" rel=""nofollow noreferrer""><code>split</code></a> it :</p>
<pre><code>import torch

t = torch.arange(20)

splits = [1, 2, 5, 10]

out = torch.split(t[: sum(splits)], splits)
</code></pre>
<p>Output :</p>
<pre><code>(tensor([0]),
 tensor([1, 2]),
 tensor([3, 4, 5, 6, 7]),
 tensor([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17]))
</code></pre>
","2024-03-04 11:48:11","2","Answer"
"78100998","78100922","","<p>You could use <a href=""https://pytorch.org/docs/stable/generated/torch.tensor_split.html"" rel=""nofollow noreferrer""><code>tensor_split</code></a> on the cumulated sum of the <code>splits</code> (e.g. with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html"" rel=""nofollow noreferrer""><code>np.cumsum</code></a>), excluding the last chunk:</p>
<pre><code>import torch
import numpy as np

t = torch.arange(20)
splits = [1,2,5,10]

t.tensor_split(np.cumsum(splits).tolist())[:-1]
</code></pre>
<p>Output:</p>
<pre><code>(tensor([0]),
 tensor([1, 2]),
 tensor([3, 4, 5, 6, 7]),
 tensor([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17]),
)
</code></pre>
","2024-03-04 11:45:10","3","Answer"
"78100983","78100922","","<p>I used numpy for my example but it might work with tensors as well.</p>
<pre><code>import numpy as np

array = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19])

def split(array, split_list: list[int]):
    if sum(split_list) &gt; len(array):
        return None
    tup = ()
    for i in split_list:
        tup = tup + (array[:i].tolist(),)
        array = array[i:]
    return tup

print(split(array, [1, 2, 5]))
</code></pre>
<p>And i got this as output:</p>
<pre><code>([0], [1, 2], [3, 4, 5, 6, 7])
</code></pre>
<p>I hope this is what you meant?</p>
","2024-03-04 11:41:49","0","Answer"
"78100922","","pytorch split array by list of indices","<p>I want to split a torch array by a list of indices.</p>
<p>For example say my input array is <code>torch.arange(20)</code></p>
<pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19])
</code></pre>
<p>and my list of indices is <code>splits = [1,2,5,10]</code></p>
<p>Then my result would be:</p>
<pre><code>(tensor([0]),
 tensor([1, 2]),
 tensor([3, 4, 5, 6, 7]),
 tensor([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17]))
</code></pre>
<p>assume my input array is always long enough to bigger than the sum of my list of indices.</p>
","2024-03-04 11:30:09","0","Question"
"78099713","78088466","","<p>Specifying the whole network is out of scope of a single answer, but generally you want something like this:</p>
<ol>
<li>Use a Resnet or vision transformer as the encoder</li>
<li>Use the encoder to map the input down to a latent tensor</li>
<li>Reshape latent tensor as needed</li>
<li>Use <code>ConvTranspose3d</code> layers to upsample latent tensor to desired output size</li>
</ol>
<p>You can do a UNet-like setup where you have skip connections between encoder layers and decoder layers, you would just need a projection layer to map the encoder activations into a shape compatible with the decoder activations.</p>
","2024-03-04 08:02:04","1","Answer"
"78098170","78092916","","<p>What do you mean there is no batch size? The documentation you linked explicitly says the input is of shape <code>(N, C_in, H_in, W_in)</code> and the output is of shape <code>(N, C_out, H_out, W_out)</code>, where <code>N</code> is the batch size.</p>
","2024-03-03 22:27:44","0","Answer"
"78098167","78092370","","<p>I think you should review the basics of transformers. Your question is hard to answer because it embeds a number of incorrect assumptions. I will attempt to address:</p>
<ol>
<li>The state dict is just a dictionary of model weights. Nothing more or less</li>
<li>You are trying to find attention values. These are activations, not weights. Activations must be computed from a specific input</li>
<li>There is no &quot;attention passed to the decoder&quot;. In encoder-decoder transformers, the decoder model computes attention between the decoder inputs and the output activations of the encoder model. There is no &quot;attention value&quot; passed to the decoder. The decoder is passed the final activations of the encoder model. The decoder uses the final encoder activations to compute different attention values in each layer of the decoder model</li>
<li>Attention values are not the same length as the input. Attention values are of shape <code>(L1, L2)</code> where <code>L1</code> is the length of the encoder sequence and <code>L2</code> is the length of the decoder sequence.</li>
</ol>
","2024-03-03 22:25:50","1","Answer"
"78098048","78097965","","<p>The problem is that you are indexing twice, currently <code>mask2</code> operates only after <code>mask1</code> has been applied. Why not define <code>mask2</code> unmasked (<em>wrt</em> <code>mask1</code>)?</p>
<pre><code>&gt;&gt;&gt; mask = (values &gt; 0)*(~torch.greater(saved_values, values))
</code></pre>
<p>That way you combine both masks and only require a single masking:</p>
<pre><code>&gt;&gt;&gt; result[mask] = values[mask]
tensor([0.0000, 0.5000, 0.9900, 0.0000])
</code></pre>
<hr />
<p>In case you <em>can only</em> / <em>only want to</em> compute <code>mask2</code> after having masked the tensors with <code>mask1</code>, you are still able to construct the resulting mask by indexing with <a href=""https://pytorch.org/docs/stable/generated/torch.nonzero.html"" rel=""nofollow noreferrer""><code>nonzero</code></a> indices:</p>
<pre><code>&gt;&gt;&gt; mask = torch.zeros_like(mask1)
&gt;&gt;&gt; mask[mask1.nonzero()[:,0]] = mask2
</code></pre>
<p>A cleaner implementation uses <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"" rel=""nofollow noreferrer""><code>torch.scatter_</code></a>:</p>
<pre><code>&gt;&gt;&gt; torch.zeros_like(mask1).scatter_(0, mask1.nonzero()[:,0], mask2)
</code></pre>
","2024-03-03 21:41:31","1","Answer"
"78097965","","Assign values via two subsequent masking operations in pytorch","<p>I have generated two different masks based on values:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

values = torch.tensor([0, 0.5, 0.99, 0.87])
saved_values = values + torch.tensor([0.1, -0.4, 0, 0.1])
result = torch.zeros_like(values)

mask1 = values &gt; 0
mask2 = ~torch.greater(saved_values[mask1], values[mask1])
</code></pre>
<p>Now I have tested if I am able to grep the data with the help of the masks:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; result[mask1][mask2]
tensor([0., 0.])
</code></pre>
<p>seems to work, so I used broadcasting for testing further:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; result[:] = 5
&gt;&gt;&gt; result[mask1][mask2]
tensor([5., 5.])
</code></pre>
<p>Seems to work also, So I tested finally the values with the masks:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; values[mask1][mask2]
tensor([0.5000, 0.9900])
</code></pre>
<p>Seems to work as well, so I try to assign the values based on the masks:</p>
<pre class=""lang-py prettyprint-override""><code>result = torch.zeros_like(values)
result[mask1][mask2] = values[mask1][mask2]
</code></pre>
<p>Doesn't throw an error so I assumed it works, checked twice:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; result[mask1][mask2]
tensor([0., 0.])
&gt;&gt;&gt; result
tensor([0., 0., 0., 0.])
</code></pre>
<p>It seems the values are not saved properly due to some reference issues.</p>
<p>How can I achieve the wanted behavior?</p>
","2024-03-03 21:12:22","0","Question"
"78097861","","How to solve RuntimeError: Couldn't find appropriate backend to handle uri in python","<p>I want to work with audiofiles in pytorch.</p>
<p>If I try running this line: <code>metadata = torchaudio.info(SAMPLE_WAV_PATH)</code> i get the error message <code>RuntimeError: Couldn't find appropriate backend to handle uri _assets\steam.wav and format None</code></p>
<p>the internet told me to try <code>str(torchaudio.list_audio_backends())</code> to see if I have soundfile installed. It appears I don't, because it returns an empty list.</p>
<p>So I tried installing soundfile. Here is a list of commands i tried:</p>
<ul>
<li><code>conda install pytorch torchvision torchaudio -c pytorch</code></li>
<li><code>conda install -c conda-forge pysoundfile</code></li>
<li><code>pip install soundfile</code></li>
<li><code>pip install PySoundFile</code></li>
</ul>
<p>none of it made a difference. According to the anaconda navigator I have both pysoundfile and soundfile installed, but I still get the same error message. I searched for similar problems and found questions like this <a href=""https://stackoverflow.com/questions/62543843/cannot-import-torch-audio-no-audio-backend-is-available"">cannot import torch audio &#39; No audio backend is available.&#39;</a> But the only answeres there say I should install soundfile</p>
<p>If somebody got an idea whats wrong I would appreciate some help, thanks in advance.</p>
<p><strong>EDIT</strong> I am using windows 11</p>
","2024-03-03 20:30:40","11","Question"
"78097519","78079501","","<p>I was able to install pytorch by running the command</p>
<p><code>conda install nvidia/label/cuda-12.1.0::NAMEHERE --solver=libmamba</code></p>
<p>for each of the offending packages.</p>
<p>for example, you could run</p>
<p><code>conda install nvidia/label/cuda-12.1.0::libcublas --solver=libmamba</code></p>
<p>and the next time you run the original command, another package error will show. After a few packages, the installation will run through.</p>
","2024-03-03 18:32:10","0","Answer"
"78097340","78096910","","<p>A quick Google search gave me <a href=""https://maxpumperla.com/learning_ray/ch_05_tune/"" rel=""nofollow noreferrer"">https://maxpumperla.com/learning_ray/ch_05_tune/</a> , which suggests adding this to the beginning of your notebook (and running it):</p>
<pre class=""lang-bash prettyprint-override""><code>! pip install &quot;ray[tune]==2.2.0&quot;
! pip install &quot;hyperopt==0.2.7&quot;
! pip install &quot;bayesian-optimization==1.3.1&quot;
! pip install &quot;tensorflow&gt;=2.9.0&quot;
</code></pre>
<p>You can try these lines or something similar. You may want to ask the author of <a href=""https://youtube.com/watch?v=YBJd8BQWK8Q"" rel=""nofollow noreferrer"">https://youtube.com/watch?v=YBJd8BQWK8Q</a> on what <code>! pip install</code> command they use.</p>
","2024-03-03 17:32:01","1","Answer"
"78097242","","'NoneType' object has no attribute 'seek'"" related to Conda env","<p>I tried to run a Python file (not mine) and this occurred:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/fname/.conda/envs/ptsne/lib/python3.7/site-packages/torch/serialization.py&quot;, line 348, in _check_seekable
    f.seek(f.tell())
AttributeError: 'NoneType' object has no attribute 'seek'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;file_builder.py&quot;, line 29, in &lt;module&gt;
    map_location=dev)
  File &quot;/home/fname/.conda/envs/ptsne/lib/python3.7/site-packages/torch/serialization.py&quot;, line 771, in load
    with _open_file_like(f, 'rb') as opened_file:
  File &quot;/home/fname/.conda/envs/ptsne/lib/python3.7/site-packages/torch/serialization.py&quot;, line 275, in _open_file_like
    return _open_buffer_reader(name_or_buffer)
  File &quot;/home/fname/.conda/envs/ptsne/lib/python3.7/site-packages/torch/serialization.py&quot;, line 260, in __init__
    _check_seekable(buffer)
  File &quot;/home/fname/.conda/envs/ptsne/lib/python3.7/site-packages/torch/serialization.py&quot;, line 351, in _check_seekable
    raise_err_msg([&quot;seek&quot;, &quot;tell&quot;], e)
  File &quot;/home/fname/.conda/envs/ptsne/lib/python3.7/site-packages/torch/serialization.py&quot;, line 344, in raise_err_msg
    raise type(e)(msg)
AttributeError: 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.
</code></pre>
<p>I'm using <code>file_builder.py</code> from <a href=""https://github.com/Academich/reaction_space_ptsne"" rel=""nofollow noreferrer"">this repository</a>.</p>
<p>Is that solely because of Conda or is something wrong with that file?  How to fix it?</p>
<p>I haven't tried anything yet, as I have no idea what to begin with.</p>
","2024-03-03 17:05:46","0","Question"
"78096910","","ModuleNotFoundError: No module named 'ray.tune.suggest'","<p>I have installed ray, and I am trying to import</p>
<p><code>from ray.tune.suggest.hyperopt import HyperOptSearch</code></p>
<p>but I keep receiving</p>
<p><code>ModuleNotFoundError: No module named 'ray.tune.suggest' </code></p>
<p>I checked the documentation but didn`t see anything that can give me a clue about this problem.</p>
<p>Any idea?</p>
","2024-03-03 15:29:31","0","Question"
"78096668","78096276","","<p>The <code>BilinearInterpolation</code> layer below performs scaling whilst preserving gradient flow. It wraps <code>F.interpolate</code>, and the gradient function at the output is <code>&lt;UpsampleBilinear2DBackward0&gt;</code>.</p>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>&quot;grad_fn&quot; of z_scaled is: &lt;UpsampleBilinear2DBackward0 object...&gt;
</code></pre>
<p><a href=""https://i.sstatic.net/64s2t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/64s2t.png"" alt=""enter image description here"" /></a></p>
<p>Interpolation layer:</p>
<pre class=""lang-py prettyprint-override""><code>class BilinearInterpolation(nn.Module):
    def __init__(self, scale_factor):
        super(BilinearInterpolation, self).__init__()
        self.scale_factor = scale_factor

    def forward(self, x):
        batch_size, channels, height, width = x.size()
        new_height = int(height * self.scale_factor)
        new_width = int(width * self.scale_factor)

        # Perform bilinear interpolation
        interpolated = F.interpolate(x, size=(new_height, new_width), mode='bilinear', align_corners=True)
        return interpolated
</code></pre>
<hr />
<p>Reproducible example:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn
import torch.nn.functional as F

class BilinearInterpolation(nn.Module):
    def __init__(self, scale_factor):
        super(BilinearInterpolation, self).__init__()
        self.scale_factor = scale_factor

    def forward(self, x):
        batch_size, channels, height, width = x.size()
        new_height = int(height * self.scale_factor)
        new_width = int(width * self.scale_factor)

        # Perform bilinear interpolation
        interpolated = F.interpolate(x, size=(new_height, new_width), mode='bilinear')
        return interpolated

#
# Test data
#
import numpy as np
xx, yy = np.meshgrid(*[np.linspace(-1, 1)] * 2)
z = np.sin(xx)**2 + np.cos(yy)**2
z = torch.tensor(z).float()
z = z[None, None, ...]
z.requires_grad = True

#View original data
import matplotlib.pyplot as plt
plt.contourf(z[0, 0, :, :].detach(), cmap='YlGnBu')
plt.text(x=9, y=42, s=f'original tensor\n{list(z.shape)}', fontweight='bold')

#Scale
scale_factor = 2
z_scaled = BilinearInterpolation(scale_factor=scale_factor)(z)

#View scaled data
plt.contourf(z_scaled[0, 0, ...].detach(), cmap='YlGnBu', zorder=0)
plt.text(x=28, y=91, s=f'{scale_factor}x interpolated tensor\ndims={list(z_scaled.shape)}', fontweight='bold')
plt.xlabel('x')
plt.ylabel('y')
plt.gcf().set_size_inches(5, 5)

print('&quot;grad_fn&quot; of z_scaled is:', z_scaled.grad_fn)
</code></pre>
","2024-03-03 14:21:32","2","Answer"
"78096276","","How can I make interpolation function that works with torch gradient","<p>Are there any methods for making Down/Upscaling function that has gradient flow?
I want to make this because of backpropagation for training Downscaling Factor Generation &amp; Faster RCNN.</p>
<p>I'm designing a computer vision deep learning flow with Pytorch.</p>
<p>Preliminary:</p>
<p>It's consists of Two parts : Preprocessing and Vision Task.
As Preprocessing part, I created a downscaling factor generator model and an image Down/Upscaling module.
In the Vision Task part, I forward the preprocessed image and to do the object detections.</p>
<p>Train flow looks like this :: img input -&gt; Downscaling Factor Generation(img forward) -&gt; img downscaling with downscaling factor -&gt; img upscaling with 1/downscaling factor -&gt; Vision Task(Faster RCNN, upscaled img) -&gt; step backward.</p>
<p>Question:</p>
<p>As i was designing this flow, i got stuck at the Down/Upscaling module.
I tried to use Pytorch's F.interpolate(img, scale_factor,,,) function, but this blocked the flow of gradients.
I put original image and downscaling factor(from model w/ gradeint) into F.interpolate function, but grad_fn is disappearing.</p>
<p>I tried to made custom interpolation function like this,</p>
<pre class=""lang-py prettyprint-override""><code>def bilinear_interpolate(self, img, scale_factor):
    print('img, scale_factor :',img,scale_factor)
    n, c, h, w = img.size()
    new_h, new_w = int(h * scale_factor), int(w * scale_factor) 
    device = img.device

    h_scale = torch.linspace(0, h-1, new_h, device=device)
    w_scale = torch.linspace(0, w-1, new_w, device=device)

    grid_h, grid_w = torch.meshgrid(h_scale, w_scale)

    h_floor = grid_h.floor().long()
    h_ceil = h_floor + 1
    h_ceil = h_ceil.clamp(max=h-1)

    w_floor = grid_w.floor().long()
    w_ceil = w_floor + 1
    w_ceil = w_ceil.clamp(max=w-1)
    print('h_floor,h_floor, h_ceil, w_floor, w_ceil :',h_floor, h_ceil, w_floor, w_ceil)

    tl = img[:, :, h_floor, w_floor]
    tr = img[:, :, h_floor, w_ceil]
    bl = img[:, :, h_ceil, w_floor]
    br = img[:, :, h_ceil, w_ceil]

    h_frac = grid_h - h_floor.to(device)
    w_frac = grid_w - w_floor.to(device)

    # bilinear interpolation
    top = tl + (tr - tl) * w_frac
    bottom = bl + (br - bl) * w_frac
    interpolated_img = top + (bottom - top) * h_frac

    return interpolated_img
</code></pre>
<p>but it doesn't work because of int/float transformation and sort of variable assignments.</p>
","2024-03-03 12:19:31","-1","Question"
"78095347","78088966","","<p><a href=""https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html#torch_geometric.nn.conv.SAGEConv"" rel=""nofollow noreferrer"">SAGEConv</a></p>
<p>You must ensure that the shape of <code>edge_index</code> you enter equals (2, |E|).</p>
<p>For instance, if the adjacency matrix for your particular graph in your particular problem type looks like this (|V|, |V|):</p>
<pre class=""lang-py prettyprint-override""><code>idx   0  1  2
0   [[0, 1, 1], 
1    [1, 0, 1], 
2    [1, 1, 0]]
</code></pre>
<p>It should be reshaped to look like this (2, |E|):</p>
<pre class=""lang-py prettyprint-override""><code>idx   0  1  2  3  4  5
0   [[0, 0, 1, 1, 2, 2],
1    [1, 2, 0, 2, 0, 1]]
</code></pre>
<p>If not, you need first alter the tensor's form as follows before making a forward pass:</p>
<pre class=""lang-py prettyprint-override""><code>class GNN(torch.nn.Module):
    def __init__(self, num_nodes, hidden_channels):
        super().__init__()
        self.num_nodes = num_nodes
        self.conv1 = SAGEConv(in_channels=2, out_channels=hidden_channels, aggr='mean')

    def forward(self, adj, coord):
        &quot;&quot;&quot;
        Args:
            adj (torch.Tensor): Adjacency matrix of shape (B x V x V), where B is the batch size and V is the number of nodes.
            coord (torch.Tensor): Coordinate matrix of shape (B x V x 2)
        &quot;&quot;&quot;
        # reshape your tensor
        edge_index = adj.reshape(-1, self.num_nodes).eq(1).nonzero().t().contiguous()  # B x V x V -&gt; (B*V) x V -&gt; 2 x |E|
        # send it to SAGEConv
        x = self.conv1(coord.view(-1, 2), edge_index)  # e.g. my node features shape is (|V|, F_in = 2)
        return x
</code></pre>
","2024-03-03 06:33:43","0","Answer"
"78094398","77924496","","<p>You may not be using the right <code>python</code>.</p>
<p>Most venvs usually have <code>[virtual-env]/bin/python</code> and <code>[virtual-env]/bin/python3</code>.</p>
<p>Try activating the shell with <code>python3</code> or running your test with <code>python3</code> if you're not doing so now.</p>
<p>Then setup aliases if you want.</p>
","2024-03-02 21:44:38","0","Answer"
"78094267","78092923","","<p>This is a typical question when it comes to CNN, there are many posts alike where users encounter the same type of error originating from this.</p>
<blockquote>
<p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (<code>i</code>x<code>j</code> and <code>k</code>x<code>l</code>)</p>
</blockquote>
<p>I will provide a canonical answer here.</p>
<p>When working with <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"" rel=""nofollow noreferrer""><code>nn.Conv2d</code></a>, intermediate tensors will be four-dimensional: <code>(b, c, h, w)</code>. Convolutions work spatially, they move across the tensor across the height and width dimensions (for 2D convs). The number of output channels is determined by the number of filters in the convolution layer which operates independently of each other. You can read more about convolution layers and sizes in: <a href=""https://stackoverflow.com/questions/65554032/understanding-convolutional-layers-shapes/65612711#65612711""><em><strong>Understanding convolutional layers shapes</strong></em></a>.</p>
<p>When it comes to CNN architectures, you have to accommodate for a change in dimensionality when moving from the convolutional part (feature extractor) to the fully connected layers (classifier). In the general case, the tensor shape is going from 4D to 2D. This requires some form of spatiality reduction: either</p>
<ul>
<li>via flattening, leading to a shape of <code>(b, c*h*w)</code>. This can be done using <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"" rel=""nofollow noreferrer""><code>nn.Flatten</code></a>;</li>
<li>or using a pooling operation such as a maximum <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"" rel=""nofollow noreferrer""><code>nn.MaxPool2d</code></a> or average pooling <a href=""https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html"" rel=""nofollow noreferrer""><code>nn.AdaptiveAvgPool2d</code></a>, resulting in a reduced shape of <code>(b, c, h', w')</code>. If <code>h'</code> and <code>w'</code> are not singletons, a flattening operation is still necessary.</li>
</ul>
<p>Ultimately the output shape of the last convolution layer depends on two things: the input shape and the number and sizes of convolutions preceding it.
The above error refers to a shape mismatch between the output of the CNN and the shape expected by the linear layer. <code>i</code> is the batch size, <code>j</code> is the <em>actual flattened feature length</em>, <code>k</code> is the <code>in_features</code> of the first linear layer, and <code>l</code> is its <code>out_features</code>. So in case you get this error, you already know which <code>in_features</code> to use!</p>
<p>To anticipate this error and avoid throwing it while debugging the architecture, another way to determine <code>in_features</code> is by truncating the model (removing all linear layers) and performing inference with dummy data. Observing the output shape of that inference will inform you of the <code>in_features</code> to adopt.</p>
<pre><code>&gt;&gt;&gt; CNNclf().net(torch.rand(3,1,100,100)).shape # adapt with your input shape
torch.Size([3, 64, 7, 7])
</code></pre>
<p>Therefore the spatial dimension is <code>7x7</code> and the channel count is <code>64</code>, so the feature dimension is <code>64*7*7 = 3136</code>. In this case, the first linear layer must be initialized as <code>nn.Linear(3136, 20, bias=True)</code>.</p>
<p>Since <a href=""https://pytorch.org/docs/1.8.0/generated/torch.nn.LazyLinear.html?highlight=lazylinear"" rel=""nofollow noreferrer"">version 1.8</a>, there exists a class <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html"" rel=""nofollow noreferrer""><code>nn.LazyLinear</code></a> which infers the <code>in_features</code> automatically at runtime (during the first inference of the model). In this case, no need to perform the dummy inference yourself, simply use <code>nn.LazyLinear(20, bias=True)</code></p>
","2024-03-02 20:47:29","4","Answer"
"78093583","78091661","","<p>Here is a simple code snippet that adds one simple linear layer on top of a sentence transformer:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from sentence_transformers import SentenceTransformer

class SentenceTransformerWithLinearLayer(torch.nn.Module):
    def __init__(self, transformer_model_name):
        super(SentenceTransformerWithLinearLayer, self).__init__()
        
        # Load the sentence transformer model
        self.sentence_transformer = SentenceTransformer(transformer_model_name)
        last_layer_dimension = self.sentence_transformer.get_sentence_embedding_dimension()
        
        # New linear layer with 16 output dimensions
        self.linear = torch.nn.Linear(last_layer_dimension, 16)

    def forward(self, x):
        # Pass the input through the sentence transformer
        x = self.sentence_transformer.encode(x, convert_to_numpy=False).unsqueeze(0)
        
        # Pass through the linear layer
        x = self.linear(x)
        return x
</code></pre>
<p>This can than be used similarly to a simple sentence transformer. In this example I loaded the <code>all-mpnet-base-v2</code> model as the base sentence transformer. The input of <code>&quot;Hello world&quot;</code> is passed through the sentence transformer and then the linear layer, resulting in a 16 dimensional vector.</p>
<pre class=""lang-py prettyprint-override""><code>model = SentenceTransformerWithLinearLayer(&quot;all-mpnet-base-v2&quot;)
output = model.forward(&quot;Hello world&quot;)
</code></pre>
<p>This vector can then be used in a loss function e.g. a MSELoss</p>
<pre class=""lang-py prettyprint-override""><code>loss_function = torch.nn.MSELoss()

...
expected = ...
loss = loss_function(output, expected)
loss.backward()
...
</code></pre>
","2024-03-02 17:02:21","1","Answer"
"78093040","78083956","","<p>I think you can implement a Batch Sampler to choose which data point will be yield for your dataset via <code>__getitem__</code></p>
<pre><code>class NegativeSampler:

  def __init__(self, positive_idx, negative_idx):
     
    self.positive_idx = positive_idx
    self.negative_idx = negative_idx 

  def __iter__(self): # this function will return index for your custom dataset ```__getitem__(self, idx)```
    
    for i in range(n_batch):
      positive_idx_batch = random.sample(self.positive_idx, batch_size)
      negative_idx_batch = []

      for pos_idx in positive_idx_batch:
        negative_idx_batch.append()
    
    
      yield positive_idx_batch + negative_idx_batch  

</code></pre>
","2024-03-02 14:27:34","1","Answer"
"78092923","","How to compute the parameters in the CNN classifier?","<p>I have implemented the CNN model for mnist. I was able understand how to compute the parameters and shapes for different layers of CNN but I wanted to understand how determine the <code>in_features</code> and <code>out_features</code> in classifier part, specifically the <code>nn.Linear()</code>. Also, how to select <code>in_channels</code>, <code>out_channels</code> in <code>nn.Conv2d</code>?</p>
<pre><code>class CNNclf(nn.Module):
def __init__(self):
    super().__init__()
    self.net = nn.Sequential(
        nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3),
        nn.ReLU(),
        nn.MaxPool2d((2, 2), stride=2),
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),
        nn.ReLU(),
        nn.MaxPool2d((2, 2), stride=3),
        nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3),
        nn.ReLU(),
        nn.MaxPool2d((2, 2), stride=2))
    self.clf = nn.Sequential(
        nn.Flatten(),
        nn.Linear(64, 20, bias=True),
        nn.ReLU(),
        nn.Linear(20, 10, bias=True))

def forward(self, x):
    x = self.net(x)
    x = self.clf(x)
    return x
</code></pre>
","2024-03-02 13:52:26","0","Question"
"78092916","","Whether is batch_size default to None in Conv2d of PyTorch.nn？","<p>I found there is no <code>batch_size</code> in <code>torch.nn.Conv2d</code> (<a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a>):</p>
<pre><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, 
dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
</code></pre>
<p>I don't know whether it is same as keras.Conv2D？</p>
","2024-03-02 13:49:57","-1","Question"
"78092370","","Understanding state_dict() in nn.Transformer of PyTorch","<p>I am trying to interpret the transformer model. This is the structure of my model:</p>
<pre><code>self.src_mask = None
self.pos_encoder = PositionalEncoding(feature_size)
self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=nhead, dropout=dropout)
self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)   
self.decoder = nn.Linear(feature_size, output_size)
</code></pre>
<p>This is an encoder-only transformer model, with a linear decoder layer (please correct me if I'm wrong). I am trying to find the attention passed to the decoder from the encoder.</p>
<p>I tried using state_dict() to find the attention. These are the keys for the last encoder layer:</p>
<pre><code>- transformer_encoder.layers.2.self_attn.in_proj_weight
- transformer_encoder.layers.2.self_attn.in_proj_bias
- transformer_encoder.layers.2.self_attn.out_proj.weight
- transformer_encoder.layers.2.self_attn.out_proj.bias
- transformer_encoder.layers.2.linear1.weight
- transformer_encoder.layers.2.linear1.bias
- transformer_encoder.layers.2.linear2.weight
- transformer_encoder.layers.2.linear2.bias
- transformer_encoder.layers.2.norm1.weight
- transformer_encoder.layers.2.norm1.bias
- transformer_encoder.layers.2.norm2.weight
- transformer_encoder.layers.2.norm2.bias
</code></pre>
<p>I am wondering which of them is the attention passed to the decoder? As I understand, the attention should be the same length as the input. I tried looking for the documentation but couldn't find anything useful.</p>
<p>Thanks for the help!</p>
","2024-03-02 10:58:39","0","Question"
"78092239","78092146","","<p>You should use <a href=""https://pytorch.org/docs/stable/generated/torch.cdist.html"" rel=""nofollow noreferrer""><code>torch.cdist</code></a>:</p>
<pre><code>&gt;&gt;&gt; torch.cdist(A[:,None],B)
</code></pre>
","2024-03-02 10:18:15","2","Answer"
"78092146","","How to calculate euclidean distance between 2D and 3D tensors in Pytorch","<p>Given:</p>
<ul>
<li>a tensor A has shape <code>(batch_size, dim)</code></li>
<li>a tensor B has shape <code>(batch_size, N, dim)</code></li>
</ul>
<p>I want to calculate euclidean distance between each row in A and the correspond row in B which has shape <code>(N, dim)</code></p>
<p>The expected result has shape <code>(batch_size, N)</code></p>
","2024-03-02 09:40:08","1","Question"
"78092036","78091091","","<p>This can be achieved using the <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"" rel=""nofollow noreferrer""><code>scatter_add</code></a> operation. The trick is to flatten <code>mapping</code> and repeat the values of <code>input</code> to match <code>mapping</code>'s indices layout.</p>
<p>First define the helper tensors <code>src</code>, <code>index</code>, and <code>out</code>:</p>
<pre><code>&gt;&gt;&gt; reps = torch.tensor([len(x) for x in mapping])
&gt;&gt;&gt; src = input.repeat_interleave(reps)
tensor([0, 1, 1, 1, 2, 2, 3, 3])

&gt;&gt;&gt; index = torch.tensor([r for x in mapping for r in x])
tensor([1, 0, 2, 4, 0, 3, 1, 2])

&gt;&gt;&gt; out = torch.zeros(max(index)+1, dtype=src.dtype)
tensor([0, 0, 0, 0, 0])
</code></pre>
<p>Applying <code>scatter_add</code> on 1D tensors is equivalent to accumulating the following element-wise: <code>out[index[i]] += src[i]</code>. So it leaves us with:</p>
<pre><code>&gt;&gt;&gt; out.scatter_add(dim=0, index=index, src=src)
tensor([3, 3, 4, 2, 1])
</code></pre>
","2024-03-02 09:04:05","1","Answer"
"78091916","78085841","","<p>A slightly more compact version uses <a href=""https://pytorch.org/docs/stable/generated/torch.any.html"" rel=""nofollow noreferrer""><code>torch.any</code></a> to check if the row is padded and avoids broadcasting the tensors to different shapes:</p>
<pre><code>&gt;&gt;&gt; mask = B.any(dim=-1)                 # (bs, N)
&gt;&gt;&gt; A *= mask.unsqueeze(-1)              # (bs, N, dim)
&gt;&gt;&gt; output = A.sum(1)/mask.sum(1, True)  # (bs, dim)
</code></pre>
","2024-03-02 08:20:16","1","Answer"
"78091896","78084246","","<p>This is due to an error in your <code>while</code> condition</p>
<pre class=""lang-py prettyprint-override""><code>def fit(self, loader, epochs = None):
    #loss_mean = []
    norm2Gradient = 1
    while norm2Gradient &lt;10e-3  and epochs &lt;2000:
        ... 
    return grad
</code></pre>
<p>Since <code>norm2Gradient = 1</code>, the condition <code>norm2Gradient &lt;10e-3</code> evals to <code>False</code> and the <code>while</code> loop never executes. The function then tries to <code>return grad</code> when the <code>grad</code> variable has not been assigned. This triggers the error.</p>
<p>That said, there is another issue with your approach. Your gradient tensors will be of different shapes, so you can't string them together in a list and compute the L2 norm of them. You probably want to compute the L2 norm of each gradient tensor individually, then compute the average.</p>
","2024-03-02 08:12:45","0","Answer"
"78091885","78078945","","<p>It sounds like what you want is to have these embeddings computed on the fly during training. The best approach for this would be to move the model computation outside of the <code>__getitem__</code> function and into the training loop.</p>
<p>The <code>__getitem__</code> method should be used for singular tasks that are disk bound or CPU bound. Computing the embeddings is GPU bound and should be done in batch.</p>
<p>Best practice would be to do something like:</p>
<ol>
<li>use the <code>__getitem__</code> method to return the necessary data to compute <code>anchor_embedding</code> and other quantities used later on</li>
<li>use the <code>collate_fn</code> of your <code>DataLoader</code> to batch the inputs for computing <code>anchor_embedding</code></li>
<li>in your training loop, use <code>model</code> to compute <code>anchor_embedding</code> and other quantities in batch</li>
</ol>
","2024-03-02 08:06:58","1","Answer"
"78091661","","How to apply a linear layer atop a sentence transformer","<p>Hey there I am trying to create a basic Sentence Transformer model for few shot learning, however while fitting I observed that the changes made to the model are miniscule because the model has been trained on 1B+ pairs whereas I train it on around 40 pairs per epochs, to deal with this problem I decided to apply a linear layer on top of the sentence transformer in order to learn the embeddings corresponding to a specific data set.
However there seems to be no forward function for the sentence transformers. Their is an alternative with the model.encode() method but it does not change the model parameters.
So summarizing I want to create a network that does a forward pass on the sentence transformer, then on the linear layer and then finally get a loss which can be used across the model.
Any help would be useful.
Thank you.</p>
","2024-03-02 06:20:46","0","Question"
"78091297","78082292","","<p>In PyTorch if you're generating a dataset on the fly it's common practice to use an <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"" rel=""nofollow noreferrer"">IterableDataset</a>. It only requires an <code>__iter__</code> instead of <code>__len__</code> and <code>__getitem__</code>, which should fit your requirements for your reinforcement learning problem.</p>
<p>Example taken from <a href=""https://discuss.pytorch.org/t/how-to-use-dataloader-with-iterabledataset/179242/2"" rel=""nofollow noreferrer"">here</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import IterableDataset

class DataStream1(IterableDataset):

    def __init__(self) -&gt; None:
        super().__init__()
        self.size_input = 4
        self.size_output = 2

    def generate(self):
        while True:
            x = torch.rand(self.size_input)
            y = torch.rand(self.size_output)
            yield x, y

    def __iter__(self):
        return iter(self.generate())

dataset = DataStream1()

train_loader = DataLoader(dataset=dataset)

for i, data in enumerate(train_loader):
    print (i, data)
</code></pre>
<p>If you end up needing to put multiple <code>IterableDatasets</code> together at some point, check out <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.ChainDataset"" rel=""nofollow noreferrer"">ChainDatasets</a>.</p>
","2024-03-02 02:46:18","2","Answer"
"78091091","","How to efficiently map one tensor to another with one-to-multiple relations in PyTorch?","<p>Suppose I have a tensor <code>input</code> and an irregular 2d array <code>mapping</code> where <code>mapping[i]</code> contains the list of indices of <code>output</code> that <code>input[i]</code> should be mapped to, how to obtain tensor <code>output</code> such that <code>output[j]</code> equals to the sum of all the entries in <code>input</code> that is mapped to it, without looping? <code>input</code> and <code>output</code> are of different sizes.</p>
<p>For example:<br />
<code>input = [0,1,2,3]</code><br />
<code>mapping = [[1], [0,2,4], [0,3], [1,2]]</code><br />
<code>output = [1+2, 0+3, 1+3, 2, 1] = [3,3,4,2,1]</code></p>
","2024-03-02 00:46:24","0","Question"
"78089532","","How to print learning rate per epoch with pytorch lightning?","<p>I am having a problem with printing (logging) learning rate per epoch in pytorch lightning (PL). TensorFlow logs the learning rate at default. As PL guide suggested, I wrote the following code:</p>
<pre><code>class FusionNetModule(pl.LightningModule):
...
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr_rate)
        lr_scheduler = {'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),
                        'name': 'expo_lr'}
        return [optimizer], [lr_scheduler]

    def on_validation_epoch_end(self):
        # Log the learning rate.
        lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[0]
        self.log('learning_rate', lr)
        ...
...

    # Learning Rate Logger
    lr_logger = LearningRateMonitor(logging_interval='epoch')
    trainer = pl.Trainer(
        logger=True,
        max_epochs=epochs,
        accelerator=&quot;gpu&quot;,
        devices=[gpu_id],
        callbacks=[lr_logger, early_stopping, checkpoint_callback, metric_logger, progressbar],
        default_root_dir=model_path)

</code></pre>
<p>But I didn't get the learning rate in the training log.</p>
<pre><code>Epoch 101: 100%|#| 23/23 [00:06&lt;00:00,  3.43it/s, v_num=102, val_loss=0.988, val_acc=0.768, train_loss=0.965, train_acc=0.752]
</code></pre>
<p>Any hint or clue for logging lr would be appreciated.</p>
","2024-03-01 17:39:55","2","Question"
"78088966","","How to add edge attributes for GrapheSage Link Prediction","<p>I'm new to graph neural networks and I'm attempting to perform link prediction (binary classification), but I'm struggling to understand how to incorporate edge attributes into my SAGEConv layer. The documentation states that SAGEConv doesn't support edge attributes, but I'm unsure if there's a workaround to include edge attributes. Below is the code I'm practicing with, which I found on Medium. I would greatly appreciate any assistance you can provide.</p>
<pre><code>import torch.nn.functional as F
class GNN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super().__init__()
        self.conv1 = SAGEConv(hidden_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
    def forward(self, x: Tensor, edge_index: Tensor) -&gt; Tensor:
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x
# Our final classifier applies the dot-product between source and destination
# node embeddings to derive edge-level predictions:
class Classifier(torch.nn.Module):
    def forward(self, x_user: Tensor, x_movie: Tensor, edge_label_index: Tensor) -&gt; Tensor:
        # Convert node embeddings to edge-level representations:
        edge_feat_user = x_user[edge_label_index[0]]
        edge_feat_movie = x_movie[edge_label_index[1]]
        # Apply dot-product to get a prediction per supervision edge:
        return (edge_feat_user * edge_feat_movie).sum(dim=-1)

class Model(torch.nn.Module):
    def __init__(self, hidden_channels):
        super().__init__()
        # Since the dataset does not come with rich features, we also learn two
        # embedding matrices for users and movies:
        self.movie_lin = torch.nn.Linear(20, hidden_channels)
        self.user_emb = torch.nn.Embedding(data[&quot;user&quot;].num_nodes, hidden_channels)
        self.movie_emb = torch.nn.Embedding(data[&quot;movie&quot;].num_nodes, hidden_channels)
        # Instantiate homogeneous GNN:
        self.gnn = GNN(hidden_channels)
        # Convert GNN model into a heterogeneous variant:
        self.gnn = to_hetero(self.gnn, metadata=data.metadata())
        self.classifier = Classifier()
    def forward(self, data: HeteroData) -&gt; Tensor:
        x_dict = {
          &quot;user&quot;: self.user_emb(data[&quot;user&quot;].node_id),
          &quot;movie&quot;: self.movie_lin(data[&quot;movie&quot;].x) + self.movie_emb(data[&quot;movie&quot;].node_id),
        } 
        # `x_dict` holds feature matrices of all node types
        # `edge_index_dict` holds all edge indices of all edge types
        x_dict = self.gnn(x_dict, data.edge_index_dict)
        pred = self.classifier(
            x_dict[&quot;user&quot;],
            x_dict[&quot;movie&quot;],
            data[&quot;user&quot;, &quot;rates&quot;, &quot;movie&quot;].edge_label_index,
        )
        return pred
        
model = Model(hidden_channels=64)```
</code></pre>
","2024-03-01 16:00:50","2","Question"
"78088764","","Finding the coefficient ""a"" of a linear equation of the form Y = aX","<p>Suppose I have two sets, X and Y, and some preknown coefficient &quot;a&quot;.</p>
<p>Set Y depends on X and &quot;a&quot; as follows:
Y = aX</p>
<pre><code>import numpy as np

a = 2
X = np.random.randint(1, 10, 10)
Y = a * X

print(f&quot;{X = }&quot;)
print(f&quot;{Y = }&quot;)

&gt;&gt;&gt; X = array([9, 2, 7, 8, 8, 9, 5, 8, 4, 1])
&gt;&gt;&gt; Y = array([18,  4, 14, 16, 16, 18, 10, 16,  8,  2])
</code></pre>
<p>Task:
Using PyTorch toolkit find the coefficient &quot;a&quot; using only X and Y</p>
<p>What I tried to do and what the problem was:</p>
<p>I am trying to pass X[i] to a model and expect to get the coefficient a_pred from it. Then I find y_pred = a_pred * X[i]. Finally I compare y[i] and y_pred.
Obviously, the model does not &quot;see&quot; the relationship between a_pred, &quot;a&quot;, y_pred and Y[i]. Hence the question: what should be the architecture of the model so that it can find the correct coefficient of &quot;a&quot;?</p>
<p>I'm just starting to learn PyTorch, and unfortunately, so far, I haven't found an unambiguous answer to this question. So far, it seems to me that the solution to this problem must be somehow related to GAN, but how exactly I unfortunately do not know</p>
<p>My current code:</p>
<pre><code>import numpy as np
from torch.nn import Module, Linear, ReLU, Sequential, CrossEntropyLoss
from torch.optim import Adam
from torch import from_numpy

from random import randint


class FindParameter(Module):
    def __init__(self):
        super(FindParameter, self).__init__()
        self.layers = Sequential(
            Linear(1, 10),
            ReLU(),
            Linear(10, 100),
            ReLU(),
            Linear(100, 10),
            ReLU(),
            Linear(10, 1),
        )

    def forward(self, input):
        return self.layers(input)


a = 5.0
x = []
y = []
train_dataset_size = 10000
for i in range(train_dataset_size):
    x.append(randint(0, 10000))
    y.append(x[i] * a)

X = [from_numpy(np.array([c], dtype=np.float32)) for c in x]
Y = [from_numpy(np.array([c], dtype=np.float32)) for c in y]
A = from_numpy(np.array([a], dtype=np.float32))

epochs = range(100)

model = FindParameter()
loss_f = CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=1e-3)


for epoch in epochs:
    for x, y in zip(X, Y):
        optimizer.zero_grad()
        a_pred = model(x)
        y_pred = a_pred * x

        loss = loss_f(y_pred, y)
        loss.backward()
        optimizer.step()
</code></pre>
","2024-03-01 15:22:42","1","Question"
"78088713","77937339","","<p>Anyone else who stumbles upon this problem: this nasty message seems to indicate wrong input shape to <code>nn.Linear</code>. This is a <a href=""https://github.com/pytorch/pytorch/issues/119161"" rel=""nofollow noreferrer"">known bug</a> in pytorch.</p>
","2024-03-01 15:15:59","0","Answer"
"78088466","","Encoder - Decoder neural network architecture with different input and output size","<p>I am trying to figure out what would be a good architecture for neural network that takes projections (2D images) from different angles and creates volume consisting of 2D slices (CT-like).</p>
<p>So for example:</p>
<ul>
<li>Input [180,100,100] -&gt; 180 projections of image 100x100 pixels.</li>
<li>Output [100,100,100] -&gt; Volume of size 100x100x100 (100 slices of 2D images)</li>
</ul>
<p>I have ground truth volumes.</p>
<p>I came up with the idea of using ResNet as Encoder. But I'm not really sure how to implement Decoder and what model would be a good choice for this kind of problem. I did think of U-net architecture, but output dimension is different, so I've abandoned this idea.</p>
<p>I am using PyTorch.</p>
","2024-03-01 14:32:11","0","Question"
"78087355","","Sum up tensor of different shape","<p>So i am doing some new work on the Forward Forward algorithm using ConvNet, here is the original paper <a href=""https://arxiv.org/pdf/2212.13345.pdf"" rel=""nofollow noreferrer"">forward forward </a> when performing prediction i will need to sum up the goodness score for each layer, the problem is each of this scores have different dimension since the Convolution operation has been performed on the data, here is my code sample.</p>
<pre><code>def predict(self, x):
        goodness_score_per_label = []
        for label in range(self.output_dim):
            # perform one hot encoding#
            print('label:', label, x.shape)
            encoded = overlay_y_on_x(x, label)
            goodness = []
            for idx, layer in enumerate(self.layers):
                encoded = layer(encoded)
                print('encoded:', encoded.shape)
                goodness += [encoded.pow(2).mean(1)]
                print('goodness:', len(goodness), goodness[idx].shape)
            goodness_score_per_label += [sum(goodness).unsqueeze(1)]
        goodness_score_per_label = torch.cat(goodness_score_per_label, 1)
        return goodness_score_per_label.argmax(1)
</code></pre>
<p>here is the tensor dimension,</p>
<pre><code>encoded: torch.Size([50000, 6, 14, 14])
goodness: 1 torch.Size([50000, 14, 14])
encoded: torch.Size([50000, 16, 7, 7])
goodness: 2 torch.Size([50000, 7, 7])
encoded: torch.Size([50000, 120, 3, 3])
</code></pre>
<p>i am getting the error from this line <code>goodness_score_per_label += [sum(goodness).unsqueeze(1)]</code> what is the best way of handling this problem?</p>
","2024-03-01 11:18:06","0","Question"
"78087089","78076239","","<p>To solve this problem, we need three ideas:</p>
<ul>
<li><p>The gradients of the outputs with respect the the parameters is the Jacobian of the network wrt the parameters. <a href=""https://pytorch.org/functorch/stable/generated/functorch.jacrev.html"" rel=""nofollow noreferrer"">https://pytorch.org/functorch/stable/generated/functorch.jacrev.html</a></p>
</li>
<li><p>We can functionalize a pytorch model, that is transform a model into a function of its parameters <a href=""https://pytorch.org/functorch/nightly/generated/functorch.functionalize.html"" rel=""nofollow noreferrer"">https://pytorch.org/functorch/nightly/generated/functorch.functionalize.html</a></p>
</li>
<li><p>Pytorch can vectorize over many operations using vmap <a href=""https://pytorch.org/functorch/stable/generated/functorch.vmap.html"" rel=""nofollow noreferrer"">https://pytorch.org/functorch/stable/generated/functorch.vmap.html</a></p>
</li>
</ul>
<p>This is all part <code>functorch</code> / <code>torch.func</code>.</p>
<p>Putting it all together, this does the same as your code:</p>
<pre><code># extract the parameters and buffers for a funcional call
params = {k: v.detach() for k, v in net.named_parameters()}
buffers = {k: v.detach() for k, v in net.named_buffers()}

def one_sample(sample):
    # this will calculate the gradients for a single sample
    # we want the gradients for each output wrt to the parameters
    # this is the same as the jacobian of the network wrt the parameters

    # define a function that takes the as input returns the output of the network
    call = lambda x: torch.func.functional_call(net, (x, buffers), sample)
    
    # calculate the jacobian of the network wrt the parameters
    J = torch.func.jacrev(call)(params)
    
    # J is a dictionary with keys the names of the parameters and values the gradients
    # we want a tensor
    grads = torch.cat([v.flatten(1) for v in J.values()],-1) 
    return grads

# no we can use vmap to calculate the gradients for all samples at once
grads2 = torch.vmap(one_sample)(X.flatten(1))

print(torch.allclose(grads,grads2))
</code></pre>
<p>It <em>should</em> run in parallel, you should try it out for bigger models etc, I did not benchmark it.</p>
<p>This is also related, to for example <a href=""https://stackoverflow.com/questions/50175711/pytorch-gradient-of-output-w-r-t-parameters"">Pytorch: Gradient of output w.r.t parameters</a> (which tbh doesn't have a great answer), and pytorch.org/tutorials/intermediate/per_sample_grads.html which shows some of the functions within torch.func for calculating the per sample gradients.</p>
","2024-03-01 10:25:48","1","Answer"
"78086883","78085996","","<p>Seems a version compatibility problem.</p>
<p>I would try to pin version of python to 3.10 and tensorflow/keras seeking stability.</p>
<pre><code>conda create -n my_env python=3.10
conda activate my_env
python -m pip install tensorflow==2.15.0 keras==2.15.0
</code></pre>
<p>And try to run your program with this setup</p>
","2024-03-01 09:49:50","-1","Answer"
"78085996","","import error on transformers and tenserflow","<pre><code>RuntimeError: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):
module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'
</code></pre>
<pre><code>sentiment_analysis = pipeline(&quot;sentiment-analysis&quot;, model=&quot;ProsusAI/finbert&quot;)
@app.route('/news', methods=['POST'])
def analyze_news_sentiment():
    data = request.json
    news_text = data.get('news_text')

    # Perform sentiment analysis
    result = sentiment_analysis(news_text)

    return jsonify(sentiment=result[0]['label'])
</code></pre>
<p>how to solve this error</p>
","2024-03-01 06:53:23","-1","Question"
"78085841","","How to reduce mean ignore padded rows in 3D tensor","<p>I have and a 3D tensor <code>A</code> has shape <code>(batch_size, N, dim)</code> and a 3D tensor <code>B</code> has shape <code>(batch_size, N, 2)</code>. In which <code>B</code> has some padding row to fill to N (which is not a zero vector, as it already passed in some functions). To know which row is padded, I have to look up tensor <code>B</code>, if row <code>k</code> is padded, the value at the <code>k</code>-row in tensor <code>B</code> is <code>[0, 0]</code>. I want to filter out these padded rows in <code>A</code> before calculating the mean.</p>
<p>After reducing A to its mean along <code>dim=1</code>, the result has a shape of <code>(batch_size, dim)</code>.</p>
<p>Edit: I figured out one solution. Any other solution is welcome!</p>
<pre><code># squeeze to 2D
A = A.view(-1, A.size(-1))
B = B.view(-1, B.size(-1))

# mask out padded row
mask = torch.sum(B, dim=-1)
mask[mask!=0] = 1

# the point is I need to assign padded row in A by zero
A = mask.unsqueeze(-1)*A

# calculate the real size of each cluster. 
mask = mask.view(batch_size, -1)
batch_cluster_size = torch.sum(mask, dim=-1, keepdim=True)

# convert back to original shape
A = A.view(batch_size, -1, A.size(-1))

output = torch.sum(A, dim=1)/batch_cluster_size

</code></pre>
","2024-03-01 06:16:01","0","Question"
"78084968","78072430","","<p>You don't need to use <code>conda install</code> if you meet version conflicts. You can install the <code>.whl</code> file manually.</p>
<p>You can find the .whl files here.</p>
<p><a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noreferrer"">torch, torchvision, torchaudio...</a></p>
<p><a href=""https://download.pytorch.org/whl/torch/"" rel=""nofollow noreferrer"">torch</a></p>
<ol>
<li>Download the .whl file with the version you want.</li>
<li>Install the .whl file using pip:</li>
</ol>
<pre><code>pip3 install your_package.whl
</code></pre>
","2024-03-01 00:11:36","0","Answer"
"78084257","78078811","","<p>You can't backprop through the index values of a slice. You can only backprop through the tensor being sliced.</p>
<p>If you want selection to be learned, you need to have a &quot;soft&quot; selection method. You can do this with the gumbel softmax reparameterization trick</p>
<pre class=""lang-py prettyprint-override""><code>e = torch.arange(10) # the tensor we want to select from
logits = torch.randn(e.shape, requires_grad=True) # selection logits, predicted by model

soft_selection = F.softmax(logits, dim=0) # soft selection weights

selection_index = soft_selection.argmax() # index we want to select
hard_selection = torch.zeros_like(logits) # selection mask
hard_selection[selection_index] = 1. # add 1 at selection index

# this passes the soft_selection through the hard_selection tensor
hard_selection = hard_selection - soft_selection.detach() + soft_selection

selection = e * hard_selection # selection via multiplication 

selection.backward(gradient=torch.ones_like(selection)) # example backprop

assert logits.grad is not None # gradients have been passed through
</code></pre>
","2024-02-29 21:02:01","1","Answer"
"78084246","","Extracting the parameters and gradient norm used to fit data in PyTorch","<p>ORIGINAL CODE</p>
<pre><code>def get_theta(self):
    theta = self.parameters().detach().cpu
    return theta

def get_norm2Gradient(self):
    theta = get_theta(self)
    loss = loss(self, xb, yb)
    grad = loss.backward()
    for param in theta:
                grad.append(param.grad)
    #computes gradient norm
    norm2Gradient = torch.linalg.norm(grad)
    return norm2Gradient

def fit(self, loader, epochs = 2000):
    norm2Gradient = 1
    while norm2Gradient &lt;10e-3 and epochs &lt;2000:
        for _, batch in enumerate(loader):
            x, y = batch['x'], batch['y']
            #computes f.cross_entropy loss of (xb,yb) on GPU 
            loss = self.loss(x,y) 
            #print(&quot;loss:&quot;, loss)
            loss = loss.mean()
            #print(&quot;loss mean:&quot;, loss)
            #clears out old gradients  
            self.optimizer.zero_grad()
            #calculates new gradients
            grad = loss.backward()
            print(&quot;grad:&quot;,grad)
            #takes one step along new gradients to decrease the loss
            self.optimizer.step()  
            #captures new parameters
            theta = self.parameters()
            print(&quot;theta:&quot;,theta)
            #collects gradient along new parameters
            for param in theta:
                grad.append(param.grad)
            #computes gradient norm
            norm2Gradient = torch.linalg.norm(grad)
    return grad
</code></pre>
<p>CURRENT QUESTION and CODE (corrected per Karl's 3/2/2024 feedback)</p>
<p>I am trying to extract values that are computed during the fit function of PyTorch: the parameters themselves; and an L-2 norm of the gradient. Here is my code for these objectives.</p>
<pre><code>def get_theta(self):
    theta = self.parameters().detach().cpu
    return theta

def fit(self, loader, epochs = 2000):
    norm2Gradient = 1
    while norm2Gradient &gt;10e-3 and epochs &lt;2000:
        for _, batch in enumerate(loader):
            x, y = batch['x'], batch['y']
            #computes f.cross_entropy loss of (xb,yb) on GPU 
            loss = self.loss(x,y) 
            #print(&quot;loss:&quot;, loss)
            loss = loss.mean()
            #print(&quot;loss mean:&quot;, loss)
            #clears out old gradients  
            self.optimizer.zero_grad()
            #calculates new gradients
            grad = loss.backward()
            print(&quot;grad:&quot;,grad)
            #takes one step along new gradients to decrease the loss
            self.optimizer.step()  
            #captures new parameters
            theta = self.parameters()
            print(&quot;theta:&quot;,theta)
            #collects gradient along new parameters
            for param in theta:
                grad.append(param.grad)
            #computes gradient norm
            norm2Gradient = torch.linalg.norm(grad)
            sumNorm2Gradient += norm2Gradient.detach().cpu
    return sumNorm2Gradient
</code></pre>
<p>Here is the reoccurring error message</p>
<pre><code>AttributeError: 'NoneType' object has no attribute 'append'
</code></pre>
<p>It occurs at this line of the code.</p>
<pre><code>grad.append(param.grad) 
</code></pre>
<p>I printed the grad variable out, and it says &quot;None.&quot;</p>
<p>My intention was to capture the gradient with the following line of code.</p>
<pre><code>grad = loss.backward()
</code></pre>
<p>What's the better way to do it that gets at the gradient being computed during the fit function?</p>
<p>Similarly: Does this line capture the parameters?</p>
<pre><code>theta = self.parameters()
</code></pre>
<p>Thank you!</p>
","2024-02-29 20:59:48","0","Question"
"78084194","78072453","","<p>The two models have different <code>forward</code> logic. If you inspect the result of <code>nn.Sequential(*list(model.children())[:-1])</code>, you'll see you're putting a <code>ModuleList</code> into a <code>Sequential</code> block which doesn't make sense. The specific error is trying to call the <code>forward</code> method of <code>ModuleList</code> which isn't implemented.</p>
<p>You need to adapt your logic to each model architecture.</p>
","2024-02-29 20:48:40","0","Answer"
"78083956","","How to use balanced sampler for torch Dataset/Dataloader","<p>My simplified Dataset looks like:</p>
<pre><code>class MyDataset(Dataset):
    def __init__(self) -&gt; None:
        super().__init__()
        self.images: torch.Tensor[n, w, h, c]   # n images in memmory - specific use case
        self.labels: torch.Tensor[n, w, h, c]   # n images in memmory - specific use case
        self.positive_idx: List                 # positive 1 out of 10000 negative
        self.negative_idx: List
        
    def __len__(self):
        return 10000 # fixed value for training
        
    def __getitem__(self, idx):
        return self.images[idx], self.labels[idx]
    

ds = MyDataset()
dl = DataLoader(ds, batch_size=100, shuffle=False, sampler=...)   
# Weighted Sampler? Shuffle False because I guess the sampler should process shuffling.
</code></pre>
<p>What is the most &quot;torch&quot; way of balancing the sampling for Dataloader so the batch will be constructed as 10 positive + 90 random negative in each epoch and in case of not enough positive duplicating the possible ones?</p>
<p>For the purpose of this exercise I'm not implementing augmenting for increasing sample size of positives.</p>
","2024-02-29 19:56:22","0","Question"
"78082292","","Pytorch Geometric graph batching not using DataLoader for Reinforcement learning","<p>I am using PyTorch Geometric to create a reinforcement learning algorithm, and I would therefore like to avoid using the inbuilt DataLoader as I generate data/observations on the go. However, I am encountering an issue when passing a batch of PyTorch Geometric Graphs. I have a numpy memory array with PyG graphs. I pick from this memory and try to push it through the neural network (NN).</p>
<p>Pushing a single graph through the NN seems to work fine. I get a representation for each node. However, when using a batch, issues arise. Normally, I can create a tensor of the numpy_array batch. However, PyTorch cannot do this as it cannot handle the PyG datatype. I, therefore, create a Batch using PyTorch Geometric's inbuilt functionality. It goes through the neural network; however, the output dimension seems weird. It seems that the graphs are combined into a single object and then passed through as a single graph. However, I was expecting an output of <code>[batch_size, n_nodes]</code> not <code>[batch_size * n_nodes]</code>. I was wondering if I am doing this correctly or not. Is there a better way of handling this to avoid the dimensionality issue? I do not trust that I can just split the output array every n_nodes in the array.</p>
<p>One option is to use a for-loop to push each individual graph into the forward pass, however this is quite inefficient. Perhaps there is a simple setting that I am missing? I have included a working example.</p>
<pre class=""lang-py prettyprint-override""><code>import torch as T
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import GCNConv
from torch_geometric.data import Batch
from torch_geometric.data import Data
import numpy as np


class DeepNetworkGCN(nn.Module):
    def __init__(self, lr=0.001, input_dims=[1], fc1_dims=128, fc2_dims=128, out_dims=[1]):
        super(DeepNetworkGCN, self).__init__()

        # CNN part of network
        self.GCNconv1 = GCNConv(*input_dims, fc1_dims)
        self.GCNconv2 = GCNConv(fc1_dims, fc2_dims)

        # conform to output dimension
        self.fc1 = nn.Linear(fc2_dims, *out_dims)

        self.optimizer = optim.Adam(self.parameters(), lr=lr)
        self.loss = nn.MSELoss()
        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')
        self.to(self.device)

    def forward(self, state):
        # Process graph data using GCN layers
        x = self.GCNconv1(state.x, state.edge_index)
        x = F.relu(x)
        x = self.GCNconv2(x, state.edge_index)

        # Final fully connected layer
        out = self.fc1(x)

        return out


def random_pyg_graph(num_nodes=3):  
    # random node features
    node_features = T.randint(0, 5, (num_nodes, 1), dtype=T.float)

    # random edge features
    edge_features = T.randn(num_nodes, num_nodes)

    # random edge indices
    edge_index = T.randint(0, num_nodes, (2, num_nodes * 2))

    # Remove self-loops
    edge_index = edge_index[:, edge_index[0] != edge_index[1]]

    # graph
    graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)

    return graph_data


# setup example
batch_size = 3
memory = np.zeros(batch_size, dtype=object)

# fill memory
for i in range(batch_size):
    memory[i] = random_pyg_graph()

# define model
CNN = DeepNetworkGCN()

# test for single PyG
output = CNN.forward(memory[0])
print(output)
# output 1 for each node e.g.
# tensor([[0.3770],
#        [0.6119],
#        [0.2014]], grad_fn=&lt;AddmmBackward0&gt;)

# test for numpy.ndarray
# FAILS! # FAILS! # FAILS!
# output = CNN.forward(memory[:]) # FAILS!
# FAILS! # FAILS! # FAILS!

# Create batch and do forward pass.
output = CNN.forward(Batch.from_data_list(memory[:]))
print(output)
# output dimension is weird. ( n_nodes*batch_size).
# tensor([[ 0.0173],
#         [ 0.0316],
#         [ 0.0282],
#         [ 0.0147],
#         [-0.0201],
#         [-0.0264],
#         [ 0.0147],
#         [-0.0084],
#         [ 0.0021]], grad_fn=&lt;AddmmBackward0&gt;)


</code></pre>
","2024-02-29 14:45:12","3","Question"
"78081999","78078811","","<p>As is, the operation is not differentiable.
Think about the definition of the gradient:</p>
<pre><code>                     e[:(d+eps)] - e[:(d+eps)]
    df/dd  = limit       ------------
           eps-&gt;0              eps
</code></pre>
<p>So you would need to a define what it means to take &quot;a little bit more of the e then d&quot;.</p>
<p>A common trick to do this is to soften the expression:
Instead of indexing you could multiply by a mask that is almost 1 for values below d and almost zero for values above d, with a smooth transition in between.
But this will always result in values in f that are not actually in e, but some interpolation or weighted values.</p>
<p>What do you want to achieve in the end?
Maybe there is another way that retains gradients?</p>
","2024-02-29 14:00:05","0","Answer"
"78081304","","Issue with Padding Mask in PyTorch Transformer Encoder","<p>I'm encountering an issue with the padding mask in PyTorch's Transformer Encoder. I'm trying to ensure that the values in the padded sequences do not affect the output of the model. However, even after setting the padded values to zeros in the input sequence, I'm still observing differences in the output.</p>
<p>Here's a simplified version of my code:</p>
<pre><code>import torch as th
from torch import nn

# Data
batch_size = 2
seq_len = 5
input_size = 16
src = th.randn(batch_size, seq_len, input_size)

# Set some values to a high value
src[0, 2, :] = 1000.0
src[1, 4, :] = 1000.0

# Generate a padding mask
padding_mask = th.zeros(batch_size, seq_len, dtype=th.bool)
padding_mask[0, 2] = 1
padding_mask[1, 4] = 1

# Pass the data through the encoder of the model
encoder = nn.TransformerEncoder(
    nn.TransformerEncoderLayer(
        d_model=input_size,
        nhead=1,
        batch_first=True,
    ),
    num_layers=1,
    norm=None,
)
out1000 = encoder(src, src_key_padding_mask=padding_mask)

# Modify the input data so that the masked vector does not affect
src[0, 2, :] = 0.0
src[1, 4, :] = 0.0

# Pass the modified data through the model
out0 = encoder(src, src_key_padding_mask=padding_mask)

# Check if the results are the same
assert th.allclose(
    out1000[padding_mask == 0],
    out0[padding_mask == 0],
    atol=1e-5,
)
</code></pre>
<p>Despite setting the padded values to zeros in the input sequence, I'm still observing differences in the output of the Transformer Encoder. Could someone please help me understand why this might be happening? How can I ensure that the values in the padded sequences do not affect the output of the model?</p>
","2024-02-29 12:08:16","2","Question"
"78081284","78074899","","<p>I commented refiner and decreased <code>num_inference_steps=10</code> and it worked. But image quality was low. So need to optimize code playing around with these parameters.</p>
","2024-02-29 12:05:54","1","Answer"
"78080142","78080061","","<p>I think this happens to be very pretty much what the <code>collate_fn</code> from the dataloader does:</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data.dataloader import default_collate

default_collate(mention_indices)
</code></pre>
<p>For a longer explanation: In Pytorch, the <code>Dataset</code> might return a dictionary for each sample. For simplicity, let's say we have a dictionary with two keys, each of which has a tensor of dimension <code>D</code> as value. In order to perform efficient batching, the Dataloader first samples many of these dictionaries, obtaining a list of B dictionaries. Then the collate function is in charge to convert the list of dictionaries into a single dictionary. In this example the dictionary would have 2 keys, each of which of dimension <code>(B, D)</code>.</p>
","2024-02-29 09:18:43","1","Answer"
"78080061","","What is an efficient way for merge list of same key dictionaries which value is tensor [Pytorch]","<p>Is there any way more efficient than this way?</p>
<pre><code>mention_inputs = defaultdict(list)

        for idx in mention_indices:
            mention_input, _ = ...
            for key,value in mention_input.items(): # value is a tensor has shape (dim,)
                mention_inputs[key].append(value)
        
        mention_inputs = {key:torch.stack(value) for key, value in mention_inputs.items()}
</code></pre>
","2024-02-29 09:02:36","0","Question"
"78079501","","Trying to install Pytorch in an Anaconda env but it is throwing an error related to libculas","<p>I am on Ubuntu 20.04 so I can run ROS1
I am trying to follow the README in this repo: <a href=""https://github.com/SYSU-STAR/H2-Mapping/tree/main?tab=readme-ov-file"" rel=""nofollow noreferrer"">https://github.com/SYSU-STAR/H2-Mapping/tree/main?tab=readme-ov-file</a>
I set up my conda env in bash</p>
<pre><code>conda env create -f h2mapping.yaml
conda activate h2mapping 
</code></pre>
<p>Then I attempt to install PyTorch (I need cuda for the following steps in the README):</p>
<pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia --solver=libmamba
</code></pre>
<p>It gives me the following error:</p>
<pre><code>Channels:
 - pytorch
 - nvidia
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: failed

LibMambaUnsatisfiableError: Encountered problems while solving:
  - package pytorch-cuda-12.1-ha16c6d3_5 requires libcublas &gt;=12.1.0.26,&lt;12.1.3.1, but none of the providers can be installed

Could not solve for environment specs
The following packages are incompatible
├─ libcublas 11.10.3.66.*  is requested and can be installed;
└─ pytorch-cuda 12.1**  is not installable because it requires
   └─ libcublas &gt;=12.1.0.26,&lt;12.1.3.1 , which conflicts with any installable versions previously reported.

</code></pre>
<p>Any thoughts on what to do differently? I have tried uninstalling and reinstalling cuda, I have tried downloading anaconda without having cuda installed, and I have tried reinstalling anaconda. I have also tried adding pointers to where cuda is installed to my bashrc, and I have also tried getting rid of any pointers to my Cuda install as well. Every time, I get the same error. Any thoughts?</p>
","2024-02-29 07:18:50","-1","Question"
"78078945","","Is it efficient to pass model into a custom dataset to run model inference during training for sampling strategy?","<p>I'm trying to design a training flow for sampling samples during training.</p>
<p>My data look like this:</p>
<pre><code>defaultdict(list,
        {'C1629836-28004480': [0, 5, 6, 12, 17, 19, 28],
         'C0021846-28004480': [1, 7, 15],
         'C0162832-28004480': [2, 9],
         'C0025929-28004480': [3, 10, 30],
         'C1515655-28004480': [4],
         ...
        }
</code></pre>
<p>where key is label and value is list of data index</p>
<p>I custom dataset class in which my <code>__getitem__(self, idx)</code> function need to calculate distance between an anchor (which is chosen randomly) and other data points. It looks like this:</p>
<pre><code>def __getitem__(self, idx):
    item_label = self.labels[idx] # C1629836-28004480
    item_data = self.data[item_label] # [0, 5, 6, 12, 17, 19, 28]

    anchor_index = random.sample(item_data,1)
    mentions_indices = [idx for idx in item_data if idx != anchor_index]
    
    with torch.no_grad():
        self.model.eval()
        anchor_input = ...
        anchor_embedding = self.model.mention_encoder(anchor_input)

        for idx in mention_indices: 
        ...
</code></pre>
<p>Another way to prevent from passing the model into custom dataset is to  run inference inside the <code>training_step</code> function during training.</p>
<p>But I read somewhere that, using dataset and dataloader to prepare data to feed into model might save the training time, as they have parallel mechanism or something like that.</p>
<p>But in fact, I need to compute these kind of distance base on the latest state of weight of my model during training, is this parallel mechanism ensure that? Though in python variable is reference variable instead of value variable.</p>
<p>So which way is more professional and correct?</p>
<p>Edit:</p>
<p>I did both approaches and the second approach much faster than the first approach.</p>
","2024-02-29 04:57:29","-1","Question"
"78078811","","Gradients for selection from array operation in PyTorch","<p>This is a followup question from <a href=""https://stackoverflow.com/q/78072628/6997665"">here</a>. I have obtained a tensor, say, <code>d</code> with gradients. Now I have another tensor array, say <code>e</code> from which I need to pick the first <code>d</code> elements. MWE below.</p>
<pre><code>import torch

a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.])
c = torch.tensor([6.])
d = a.min(b).min(c)

e = torch.arange(10)
f = e[:d]  # Throws error &quot;TypeError: only integer tensors of a single element can be converted to an index&quot;
</code></pre>
<p>Based on the answer <a href=""https://discuss.pytorch.org/t/typeerror-only-integer-tensors-of-a-single-element-can-be-converted-to-an-index/45641/2"" rel=""nofollow noreferrer"">here</a>, the following line works.</p>
<pre><code>f = e[:d.to(dtype=torch.long)]
</code></pre>
<p>However, the gradients are lost. Is there someway I can pass the gradients or this operation is not differentiable at all? Many thanks.</p>
","2024-02-29 04:07:18","1","Question"
"78077661","78077447","","<p>If you have a horizontal kernel size of <code>3</code> and want the same behavior as <code>padding='same'</code>, the padding set manually must be symmetrical and equal to <code>1</code> on both ends:</p>
<pre><code>pad = layers.ZeroPadding2D(padding=(1,0))

conv = layers.Conv2D(filters, 
                     kernel_size=(3, 1), 
                     strides=int(1/s), 
                     padding='valid', 
                     data_format='channels_last')
</code></pre>
<p>Then <code>conv(pad(keras_tensor))</code> will have a shape of <code>(128, 1024, 1, 2)</code>.</p>
","2024-02-28 21:17:58","0","Answer"
"78077447","","Different 2D Convolution results between PyTorch and Keras","<p>I am trying to find the equivalent keras representation of the following PyTorch line:</p>
<pre><code>conv1 = nn.Conv2d(int(filters*s), filters, kernel_size=(3, 1), stride=int(1/s), padding=(1, 0))
</code></pre>
<p>Here is the full code</p>
<pre><code>import torch
from torch import nn

import torch
import tensorflow as tf
import numpy as np

from keras import initializers
from tensorflow.keras import layers

# constants between both libraries
filters = 2
s = 1

# Set the seed for reproducible set of numbers between keras and pytorch
torch.manual_seed(0)
np.random.seed(0)
tf.random.set_seed(0)

# Create a tensor in PyTorch of shape (128, 2, 1024, 1)
pytorch_tensor = torch.rand((128, 2, 1024, 1))

# Convert the PyTorch tensor to a NumPy array
numpy_array = pytorch_tensor.numpy()

# Reshape the NumPy array to the desired shape for Keras (128, 1024, 1, 2)
numpy_array = np.transpose(numpy_array, (0, 2, 3, 1))

# Create a tensor in Keras (TensorFlow) from the NumPy array
keras_tensor = tf.constant(numpy_array)

# PyTorch zeropad and convolution 
pytorch_conv1 = nn.Conv2d(int(filters*s), filters, kernel_size=(3, 1), stride=int(1/s), padding=(1, 0))(pytorch_tensor)

print(f&quot;Pytorch values: {pytorch_conv1}&quot;)

# Keras zeropad and convolution 
keras_zeropad = layers.ZeroPadding2D(padding=((0,0),(1,0)))(keras_tensor)
keras_conv1 = layers.Conv2D(filters, kernel_size=(3, 1), strides=int(1/s), padding='valid', data_format='channels_last')(keras_zeropad)

print(f&quot;Keras tensor values: {keras_conv1}&quot;)
</code></pre>
<p>I thought the issue was padding since PyTorch and Keras handle it differently. Namely, inside Keras’ <code>Conv2D</code> layer the padding can only be set to <code>valid</code> or <code>same</code>. Instead I am using the <code>ZeroPadding2D</code> layer before the <code>Conv2D</code> so asymmetric padding can be done in Keras. However, despite this I still cannot get the output data to match.</p>
<p>I wrote the following code which generates the same input tensor for PyTorch and Keras so I know the data is the same. I’ve also tried initializing the weights and bias's the same way but the data still doesn’t match.</p>
<p>I’ve seen posts about using the PyTorch weights in Keras to achieve the same results but I would like to avoid that for my application.</p>
<p><strong>Update:</strong>
Changing the Keras Conv2D layer to the following still produces vastly different results compared to PyTorch. The padding and shape are equal, however Keras produces new data every time. I’ve looked into kernel initializers but nothing has produced data that matches PyTorch.</p>
<pre><code>keras_zeropad = layers.ZeroPadding2D(padding=(1,0))(keras_tensor)
keras_conv1 = layers.Conv2D(filters, kernel_size=(3, 1), strides=int(1/s), padding='valid', data_format='channels_last')(keras_zeropad)

print(f&quot;Keras tensor values: {keras_conv1}&quot;)
print(f&quot;Keras tensor shape: {keras_conv1.shape}&quot;)
</code></pre>
<p>For readability the input tensor shape was changed to (1,2,4,1). Here is the PyTorch and Keras data that should match.</p>
<pre><code>Pytorch values: tensor([[[[-0.1842],
          [-0.2021],
          [-0.3707],
          [-0.2296]],

         [[ 0.2727],
          [ 0.0941],
          [ 0.0927],
          [ 0.1981]]]], grad_fn=&lt;ConvolutionBackward0&gt;)
PyTorch tensor shape: torch.Size([1, 2, 4, 1])
Keras tensor values: [[[[-0.57757664  0.40883008]]

  [[ 0.21837917  0.6590299 ]]

  [[ 0.28047863  0.3622663 ]]

  [[ 0.2800742   0.2882987 ]]]]
Keras tensor shape: (1, 4, 1, 2)
</code></pre>
","2024-02-28 20:38:20","0","Question"
"78077402","78077221","","<p>You seem to be trying to pass the output from a CNN layer directly into the quantum layer without ensuring the dimensions are compatible.</p>
<p>Try adjusting the dimensions of the input tensor before passing it to the quantum layer. Maybe, add a layer before <code>qlayer</code> in your <code>ImageClassifier</code> which flattens or somehow processes the output of the previous layers to match the expected input size of the quantum layer.</p>
","2024-02-28 20:29:26","0","Answer"
"78077221","","Stuck in Dimension problem in building QNN with pytorch","<h4>I'm attempting to build a Quantum Neural Network (QNN) using PyTorch and PennyLane. However, I'm encountering a dimension error specifically when defining the quantum layer.</h4>
<p>I have successfully set up my PyTorch and PennyLane environment, but when I try to define the quantum layer using PennyLane, I receive a dimension error. I suspect this might be due to a mismatch in the dimensions of my input data and the expected input shape of the quantum layer.</p>
<h2>My Code:</h2>
<pre><code># Get data 
train = datasets.MNIST(root=&quot;data&quot;, download=True, train=True, transform=ToTensor())
dataset = DataLoader(train, 32)
n_qubits = 2
dev = qml.device(&quot;default.qubit&quot;, wires=n_qubits)

@qml.qnode(dev)
def qnode(inputs, weights_0, weight_1):
    print(inputs)
    qml.RX(inputs[0], wires=0)
    qml.RX(inputs[1], wires=1)
    qml.Rot(*weights_0, wires=0)
    qml.RY(weight_1, wires=1)
    qml.CNOT(wires=[0, 1])
    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))
weight_shapes = {&quot;weights_0&quot;: 3, &quot;weight_1&quot;: 1}
qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)
print(qlayer)
class ImageClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(qlayer,
            nn.Conv2d(1, 32, (3, 3)),
            nn.ReLU(),
            nn.Conv2d(32, 64, (3, 3)),
            nn.ReLU(),
            nn.Conv2d(64, 64, (3, 3)),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * (28 - 6) * (28 - 6), 10)
        )

    def forward(self, x):
        result = self.model(x)
        return result
# Instance of the neural network, loss, optimizer
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
# Instance of the neural network, loss, optimizer
clf = ImageClassifier().to('cpu')
opt = Adam(clf.parameters(), lr=1e-3)
loss_fn = nn.CrossEntropyLoss()

# Training flow 
if __name__ == &quot;__main__&quot;:
    for epoch in range(1):  # train for 10 epochs
        for batch in dataset:
            X, y = batch
            X, y = X.to('cpu'), y.to(device)
            yhat = clf(X)
            loss = loss_fn(yhat, y)

            # Apply backprop 
            opt.zero_grad()
            loss.backward()
            opt.step()

        print(f&quot;Epoch:{epoch} loss is {loss.item()}&quot;)
</code></pre>
<h2>The Error I found:</h2>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-84-a98a57a9f607&gt; in &lt;cell line: 9&gt;()
     12             X, y = batch
     13             X, y = X.to('cpu'), y.to(device)
---&gt; 14             yhat = clf(X)
     15             loss = loss_fn(yhat, y)
     16 

10 frames
/usr/local/lib/python3.10/dist-packages/pennylane/qnn/torch.py in &lt;listcomp&gt;(.0)
    427 
    428         if len(x.shape) &gt; 1:
--&gt; 429             res = [torch.reshape(r, (x.shape[0], -1)) for r in res]
    430 
    431         return torch.hstack(res).type(x.dtype)

RuntimeError: shape '[896, -1]' is invalid for input of size 28
</code></pre>
","2024-02-28 19:51:40","0","Question"
"78076303","78021371","","<p>It's because the <code>list_of_indices</code> is a ragged <code>list</code> (i.e. it contains empty nested <code>[]</code>s and <code>list</code>s of different <code>len</code>gths), so if we include a function that returns a <code>tensor</code> the same <code>shape</code> as <code>x</code> where <code>1</code>s are the <code>indices</code> from <code>list_of_indices</code> (and <code>0</code>s are the indices not in <code>list_of_indices</code>), then we can just input that into <code>torch.where</code> indexing <code>x</code>:</p>
<pre><code>def get_indices_from_list(list_of_indices):
    def fill_list(f):
        _f = torch.zeros(4).long(); _f[f] = 1
        return _f
    return torch.stack([fill_list(i) for i in list_of_indices])

x[torch.where(get_indices_from_list(list_of_indices) == 1)] = -1
print(x)
</code></pre>
<p>Outputs:</p>
<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5, -1, -1],
        [ 8, -1, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19],
        [20, 21, 22, 23],
        [-1, -1, -1, -1],
        [28, 29, 30, 31],
        [-1, 33, 34, -1]])
</code></pre>
","2024-02-28 16:57:23","0","Answer"
"78076239","","How to efficiently calculate gradients of all outputs with respect to parameters?","<p>I have a relatively simple requirement but surprisingly this does not seem to be straightforward to implement in pytorch. Given a neural network with $P$ parameters that outputs a vector of length $Y$ and a batch of $B$ data inputs, I would like to calculate the gradients of the outputs with respect to the model's parameters.</p>
<p>In other words, I would like the following function:</p>
<pre><code>def calculate_gradients(model, X):
    &quot;&quot;&quot;
    Args:
        nn module with P parameters in total that outputs a tensor of size (B, Y).
        torch tensor of shape (B, .).

    Returns:
        torch tensor of shape (B, Y, P)
    &quot;&quot;&quot;
    # function logic here
</code></pre>
<p>Unfortunately, I don't currently see an obvious way of calculating this efficiently, especially without aggregating over the data or target dimensions. A minimal working example below involves looping over input and target dimensions, but surely there is a more efficient way?</p>
<pre><code>import torch
from torchvision import datasets, transforms
import torch.nn as nn

###### SETUP ######

class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h = self.fc1(x)
        pred = self.fc2(self.relu(h))
        return pred
    
train_dataset = datasets.MNIST(root='./data', train=True, download=True, 
                            transform=transforms.Compose(
                                [transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (0.5,))
        ]))

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)

X, y = next(iter(train_dataloader))  # take a random batch of data

net = MLP(28*28, 20, 10)  # define a network


###### CALCULATE GRADIENTS ######
def calculate_gradients(model, X):
    # Create a tensor to hold the gradients
    gradients = torch.zeros(X.shape[0], 10, sum(p.numel() for p in model.parameters()))

    # Calculate the gradients for each input and target dimension
    for i in range(X.shape[0]):
        for j in range(10):
            model.zero_grad()
            output = model(X[i])
            # Calculate the gradients
            grads = torch.autograd.grad(output[j], model.parameters())
            # Flatten the gradients and store them
            gradients[i, j, :] = torch.cat([g.view(-1) for g in grads])
            
    return gradients

grads = calculate_gradients(net, X.view(X.shape[0], -1))
</code></pre>
<p><strong>Edit:</strong>
I ran some quick benchmarks of Felix Zimmermann's solution which does indeed provide some nice speedups for this toy problem on my machine.</p>
<pre><code>import time

start = time.time()
for _ in range(1000):
    grads = calculate_gradients(net, X.view(X.shape[0], -1))
end = time.time()
print('Loop solution', end - start)

start = time.time()
for _ in range(1000):
    params = {k: v.detach() for k, v in net.named_parameters()}
    buffers = {k: v.detach() for k, v in net.named_buffers()}
    grads2 = torch.vmap(one_sample)(X.flatten(1))
end = time.time()
print('Vmap solution', end - start)
</code></pre>
<p>Which outputs</p>
<pre><code>Loop solution 8.408899307250977
Vmap solution 2.355229139328003
</code></pre>
<p>Note that the performance gains are likely to be much greater in more realistic settings with larger batches on GPUs.</p>
","2024-02-28 16:47:27","2","Question"
"78074899","","OutOfMemoryError: CUDA out of memory. Both in local machine and google colab","<p>I am trying to use stable diffusion xl model to generate images. But after installing and painfully matching version of python, pytorch, diffusers, cuda versions I got this error:</p>
<p><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 857.06 MiB is free. Process 43684 has 13.91 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 602.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation. </code></p>
<p>Now it may seem obvious to get higher GPU Memory but!!! I have tried this on my local computer with NVIDIA GEFORCE FTX 3060 6GB. And also in Google Colab with 15 GB of VRAM!</p>
<p>I have tried every solution in stackoverflow, github and still can't fix this issue.
Solutions I have tried:</p>
<ol>
<li>I am not training the model here. When training batch_size was 1.</li>
<li>Added these environment variables:
PYTHONUNBUFFERED=1;PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256</li>
<li>Resize image to 512x512</li>
<li>I have read somewhere that I need to downgrade pytorch version to 1.8 because of RTX 3060 GPU and Cuda version 11.3. But can't install pytorch version 1.8 : <code>Could not find a version that satisfies the requirement torch==1.8.1</code></li>
</ol>
<p>Here is my python code:</p>
<pre><code>
from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline
import torch
import gc

#for cleaning memory
gc.collect()
del variables
torch.cuda.empty_cache()

model = &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;
pipe = DiffusionPipeline.from_pretrained(
    model,
    torch_dtype=torch.float16,
)
pipe.to(&quot;cuda&quot;)
pipe.load_lora_weights(&quot;model/&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;)

refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(
    &quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;,
    torch_dtype=torch.float16,
)
refiner.to(&quot;cuda&quot;)


prompt = &quot;a portrait of maha person 4k, uhd&quot;

for seed in range(1):
    generator = torch.Generator(&quot;cuda&quot;).manual_seed(seed)
    image = pipe(prompt=prompt, generator=generator, num_inference_steps=25)
    image = image.images[0]
    image.save(f&quot;output_images/{seed}.png&quot;)
    image = refiner(prompt=prompt, generator=generator, image=image)
    image = image.images[0]
    image.save(f&quot;images_refined/{seed}.png&quot;)
</code></pre>
","2024-02-28 13:41:01","1","Question"
"78074237","78073954","","<p>You can use a combination of the view method and broadcasting:</p>
<pre><code>result = A.view(16, 1, 1, 1) * B
</code></pre>
<p>The view method creates a tensor with shape <strong>[ [[[a_1]]], [[[a_2]]], ..., [[[a_16]]] ]</strong>. Then broadcasting takes effect during the multiplication.</p>
<p>Example:</p>
<blockquote>
<p>A = [1,2,3] (shape = (3,) )</p>
<p>B =
[[[1,1],[1,1]],
[[1,1],[1,1]],
[[1,1],[1,1]]] (shape = (3,2,2) )</p>
<p>A.view(3,1,1)*B -&gt;
[[[1,1],[1,1]],
[[2,2],[2,2]],
[[3,3],[3,3]]] (shape = (3,2,2) )</p>
</blockquote>
","2024-02-28 12:02:26","3","Answer"
"78074000","78072628","","<p>The problem with your code is the line <code>d = torch.min(torch.tensor([a, b, c]))</code></p>
<p>When you compute <code>torch.tensor([a, b, c])</code>, you create a new tensor that has no computational graph to the <code>a</code>, <code>b</code> or <code>c</code> tensors. For example:</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.])
c = torch.tensor([6.])
d = torch.tensor([a,b,c])
d.requires_grad
&gt; False
</code></pre>
<p>The solution is to use the <code>min</code> function with the input tensors themselves.</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.])
c = torch.tensor([6.])
d = a.min(b).min(c)
d.requires_grad
&gt; True
</code></pre>
<p>Note that the gradient of the <code>min</code> function is <code>1</code> for the min value and <code>0</code> for all other values. This means that you will lose gradient signal if the value you want to backprop through is not the min.</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.])
d = a.min(b)
d.backward()
a.grad
&gt; tensor([1.])

a = torch.tensor([6.], requires_grad=True)
b = torch.tensor([5.])
d = a.min(b)
d.backward()
a.grad
&gt; tensor([0.])
</code></pre>
","2024-02-28 11:20:53","3","Answer"
"78073954","","How to multiply element-wise two tensors with partially matching dimensions?","<p>Say I have two tensors, tensor A with Size [16] and tensor B with size [16,1,30,30].</p>
<p>I want to multiply them element-wise.</p>
<p>The following code works, but I was wondering if there is another way:</p>
<pre><code>result = A[..., None, None, None] * B
</code></pre>
<p>perhaps something which looks cleaner.</p>
","2024-02-28 11:13:11","0","Question"
"78073195","78072594","","<p>If you are processing 5,000 images, you don't want to pay the start-up time for 5,000 <code>convert</code> processes. You would be better to use:</p>
<pre><code>mogrify -format JPEG *.png
</code></pre>
<p>and then you only pay one process start-up cost. However, that only uses a single CPU core, and will probably overflow your command-line length, I mean the <code>ARG_MAX</code> parameter. So you will do better to use <strong>GNU Parallel</strong> which will use all your cores in parallel. Depending on your CPU and your RAM and your image sizes, you will likely want a command like this:</p>
<pre><code>find . -name &quot;*.png&quot; -print0 | parallel -0 -n 64 magick mogrify -format JPEG {}
</code></pre>
<p>That runs <code>find</code> to identify all the PNG files and send the list to <strong>GNU Parallel</strong> with null-termination so spaces do not upset you. Then <strong>GNU Parallel</strong> will run as many <code>mogrify</code> processes as you have CPU cores and pass each process 64 files to convert, which means you will only pay 1 process start-up for every 64 files. It will keep starting new jobs , each processing 64 images as each previous job exits, thereby keeping all your CPU cores busy.</p>
<p>You may need to tweak the numbers and maybe add <code>--eta</code> or <code>--bar</code> to get a progress bar so you can watch them <em>&quot;whoosh&quot;</em> past. Examples <a href=""https://stackoverflow.com/a/51822265/2836621"">here</a> and <a href=""https://stackoverflow.com/a/70519968/2836621"">here</a>.</p>
<hr />
<p>If you cannot install <strong>GNU Parallel</strong>, at least start each of your <code>convert</code> processes in the background, and then add a <code>wait</code> after every 4 jobs or so. Something very approximately like this, (untested):</p>
<pre><code>i=0
for file in *.png
    do convert &quot;$file&quot; &quot;$(basename &quot;$file&quot; .png).jpg&quot; &amp;
    ((i=i+1))
    [ i % 4 -eq 0 ] &amp;&amp; wait
done
</code></pre>
<hr />
<p>Likewise with your PIL/Pillow code, you should consider multiprocessing like <a href=""https://stackoverflow.com/a/59181995/2836621"">this</a>.</p>
<hr />
<p>As regards the poor results, I suspect JPEG's <em>&quot;chroma subsampling&quot;</em> may have affected your results. You could either turn off chroma-subsampling, or try converting with <code>-quality 90</code>.</p>
","2024-02-28 09:09:36","0","Answer"
"78072998","78072379","","<p>You did not explain why you wanted to save both the local and global representations. in many cases (imho) it is redundant. But, it may be so that it isn't in your case. Anyhow, I have had cases where this helped me compare the performance of models trained on different representations and gave insightson how different aspects of the data were captured by the models. So kudos to that!.</p>
<p>Here is how I went about doing this (adapt it to how you want to save them)</p>
<pre><code>import os
import torch
from torch_geometric.data import InMemoryDataset

class LocalRepresentationDataset(InMemoryDataset):
    def __init__(self, root, transform=None, pre_transform=None):
        super(LocalRepresentationDataset, self).__init__(root, transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self):
        return []

    @property
    def processed_file_names(self):
        return ['local_data.pt']

    def download(self):
        pass

    def process(self):
        data_list = [...] #(PLUGG IN YOU LOCAL REPRESENTATION HERE)
        data, slices = self.collate(data_list)
        torch.save((data, slices), self.processed_paths[0])

class GlobalRepresentationDataset(InMemoryDataset):
    def __init__(self, root, transform=None, pre_transform=None):
        super(GlobalRepresentationDataset, self).__init__(root, transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self):
        return []

    @property
    def processed_file_names(self):
        return ['global_data.pt']

    def download(self):
        pass

    def process(self):
        data_list = [...] #(PLUGG IN YOU GLOBAL REPRESENTATION HERE)
        data, slices = self.collate(data_list)
        torch.save((data, slices), self.processed_paths[0])

local_processed_dir = 'processed/local'
global_processed_dir = 'processed/global'

os.makedirs(local_processed_dir, exist_ok=True)
os.makedirs(global_processed_dir, exist_ok=True)

local_dataset = LocalRepresentationDataset(local_processed_dir)
global_dataset = GlobalRepresentationDataset(global_processed_dir)
</code></pre>
","2024-02-28 08:38:38","0","Answer"
"78072916","78071359","","<p>Using <a href=""https://pytorch.org/docs/stable/generated/torch.load.html"" rel=""nofollow noreferrer""><code>torch.load</code></a> with <code>map_location</code> set to a cuda device will load the state to the desired device. However, it is likely that when <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict"" rel=""nofollow noreferrer""><code>load_state_dict</code></a> is called, the tensors are copied to the model's CPU, including if the model is on the CPU. If you test your code without the <code>model.to(device)</code>, you will notice your model is, in fact, not on the GPU. The state was on the GPU, but the model never was. You can read more <a href=""https://discuss.pytorch.org/t/torch-load-does-not-map-to-gpu-as-advertised/74637/4"" rel=""nofollow noreferrer"">here</a>.</p>
<pre><code>model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location=device))
</code></pre>
<p>This means you are required to use the <code>model.to(device)</code>.</p>
<p>You can prevent that by first transferring the model to the Cuda device, and then loading your state. But overall the number of transfers to the GPU is the same, 2: 1st with your initialized model, then 2nd the state.</p>
<pre><code>model = TheModelClass(*args, **kwargs).to(device)
model.load_state_dict(torch.load(PATH, map_location=device))
</code></pre>
<p>You should be able to reduce the number of transfers by first loading the state on the CPU, then transfer the model to the GPU:</p>
<pre><code>model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location='cpu'))
model.to(device)
</code></pre>
","2024-02-28 08:24:17","1","Answer"
"78072867","78063823","","<p>I found the error, and it has nothing to do with the way of computing MSE. After Cp_train, there is an intermediate variable from which you compute Cp_train_predicted. I sort that variable to plot it, but it does not have to be changed to calculate Cp_train_predicted. That was the error, I was using the sorted variable to calculate my predictions. Such a little error. Anyway, thank you all for your replies! :D</p>
","2024-02-28 08:15:53","1","Answer"
"78072656","78072628","","<p>You need to make sure that 'b' and 'c' are also tensors with the same properties as your 'a' (just set the gradient to 'false').</p>
<p>Adjusted code looks like this:</p>
<pre><code>import torch

a = torch.tensor([4.], requires_grad=True)
b = torch.tensor([5.], requires_grad=False)  # ensure b is a tensor
c = torch.tensor([6.], requires_grad=False)  # ensure c is also a tensor
d = torch.min(torch.cat((a, b, c)))  # concatenate tensors and compute minimum

# use backward to compute gradients with respect to a
d.backward()

print(a.grad)  # this should print the gradient of a
</code></pre>
","2024-02-28 07:36:45","-1","Answer"
"78072644","78072430","","<p>Create a virtual environment.</p>
<blockquote>
<p>Please ensure that you have met the prerequisites below (e.g., numpy)</p>
</blockquote>
<blockquote>
<p>To install the PyTorch binaries, you will need to use one of two supported package managers: Anaconda or pip. Anaconda is the recommended package manager as it will provide you all of the PyTorch dependencies in one, sandboxed install, including Python.</p>
</blockquote>
<p>Complete guide here:
<strong><a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a></strong></p>
<p><a href=""https://i.sstatic.net/BLl0i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BLl0i.png"" alt=""enter image description here"" /></a></p>
","2024-02-28 07:33:02","1","Answer"
"78072628","","Pass gradients through min function in PyTorch","<p>I have a variable, say, <code>a</code> which has some gradient associated with it from some operations before. Then I have integers <code>b</code> and <code>c</code> which have no gradients. I want to compute the minimum of <code>a</code>, <code>b</code>, and <code>c</code>. MWE is as given below.</p>
<pre><code>import torch

a = torch.tensor([4.], requires_grad=True)  # As an example I have defined a leaf node here, in my program I have an actual variable with gradient
b = 5
c = 6
d = torch.min(torch.tensor([a, b, c]))  # d does not have gradient associated
</code></pre>
<p>How can I write this differently so that the gradient from <code>a</code> flows through to <code>d</code>? Thanks.</p>
","2024-02-28 07:29:56","2","Question"
"78072594","","Error using pytorch-fid for datasets containing 5000 images","<p>I am trying to calculate the FID between 2 datasets, using pytorch-fid. The GitHub link for the code is: <a href=""https://github.com/mseitzer/pytorch-fid"" rel=""nofollow noreferrer"">https://github.com/mseitzer/pytorch-fid</a></p>
<p><strong>Attempt 1:</strong>
I started off small, with 2800 images, all 512×512, in each dataset. Unfortunately my generated images were PNGs, and the real images were JPGs. So I cd-ed into my generated image folder, ran the following bash script (which uses ImageMagick), and converted all the PNGs into JPGs:</p>
<pre><code>for file in *.png
    do convert &quot;$file&quot; &quot;$(basename &quot;$file&quot; .png).jpg&quot;
done
</code></pre>
<p>With these, I managed to get a FID of 143.6, which I acknowledge is pretty bad.</p>
<p><strong>Attempt 2:</strong>
I thought increasing the number of images might lower the FID, so I tried again with 5000 images in each dataset. The bash script I wrote above was taking very long to run. So I switched to the Python script below:</p>
<pre><code>import os
directory = &lt;PATH TO MY DIRECTORY&gt;

files = os.listdir(directory)


# Then you rename the files
for file_name in files:
    # You give the full path of the file
    old_name = os.path.join(directory, file_name)

    # You CHANGE the extension
    new_name = old_name.replace('.jpg', '.png')
    os.rename(old_name, new_name)
</code></pre>
<p>I later realized this only changes the file extension to '.jpg', and not the file format. This obviously did not work, and I ended up getting the following error:</p>
<pre><code>RuntimeError: Trying to resize storage that is not resizable
</code></pre>
<p><strong>Attempt 3:</strong>
I went back to my generated image folder, reversed the code from Attempt 2 (renamed all the JPGs back into PNGs) and ran the script below using Pillow to actually change the file format. Keep in mind, both datasets now have 5000, 512×512, JPG images.</p>
<pre><code>from PIL import Image
import os

path = &lt;PATH TO MY DIRECTORY&gt;
files=os.listdir(path)

for file in files:
    if file.endswith(&quot;.png&quot;):
        img = Image.open(path+file)
        #print(img)
        file_name, file_ext = os.path.splitext(file)
        img.save('&lt;PATH TO OUTPUT DIRECTORY&gt;{}.jpg'.format(file_name))
</code></pre>
<p>But I am still getting the runtime error from Attempt 2. What could be wrong? Is there an issue with the PIL JPG conversion? Should I revert to ImageMagick? Please help!</p>
<p><strong>Edit:</strong>
I made a silly mistake. I overlooked the fact that a couple of my real images weren't 512×512. I am now able to get a FID of 131.02. That's still pretty bad, but I will continue trying to improve it.</p>
<p>Also, thanks for suggesting multiprocessing. I used it and my conversion process was way faster than before.</p>
","2024-02-28 07:23:38","0","Question"
"78072453","","Unable to extract features using huggingface swin transformer model","<p>I need to use swin transfomer as a backbone for my SVM model. I need to extract the features first, therefore I am trying to remove the last layer and train the model. This is a sample executable code.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np

HUB_URL = &quot;SharanSMenon/swin-transformer-hub:main&quot;
MODEL_NAME = &quot;swin_tiny_patch4_window7_224&quot;
model = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True)
model = nn.Sequential(*list(model.children())[:-1])          #--&gt; If you comment this line, the code is executing
for param in model.parameters():
    param.requires_grad = False


dummy_tensor = torch.randn(32, 3, 224, width)

# Print the shape of the dummy tensor
model(dummy_tensor)
</code></pre>
<p><a href=""https://i.sstatic.net/j7MhL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/j7MhL.png"" alt=""enter image description here"" /></a></p>
<p>What am I missing, I tried the same process with resnet50 and it worked fine. Why am I not able to remove the last layer and get only the output features? what should I do? any suggestions?</p>
","2024-02-28 06:52:31","0","Question"
"78072430","","Unable to Install torch for Python 3.7 - 3.11","<p>I am trying to install pytorch on linux based remote system. But I am getting the same error with every version of python from 3.7 to 3.11.</p>
<p>Command Used for installation</p>
<p>pip3 install torch torchvision torchaudio --index-url <a href=""https://download.pytorch.org/whl/cpu"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/cpu</a></p>
<p>Following is the the error:</p>
<p>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch</p>
<p>Conda command used:
conda install pytorch torchvision torchaudio cpuonly -c pytorch</p>
<p>Following is the error:
PackagesNotFoundError: The following packages are not available from current channels:</p>
<ul>
<li>torchvision</li>
<li>torchaudio</li>
</ul>
","2024-02-28 06:49:04","1","Question"
"78072379","","Process and raw directory in Pytorch Geometric","<p>I have two graph representations for the same image file. One for local information and another for global.</p>
<p>In pytorch geometric, the processed data is stored in the processed directory. Is it possible to save the two different representations to different processed directories? What would be the best way to save and get the processed data in this case?</p>
<p>I use this for facial feature extraction and classification problems. while the global information is for overall features, local features extract region-specific information. I have tried using a DataList but am not sure of how works in the case of 'process directory' as given in Pytorch geometric.</p>
","2024-02-28 06:38:43","1","Question"
"78071359","","PyTorch: redundancy between map_location and .to(device)?","<p>The <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-on-cpu-load-on-gpu"" rel=""nofollow noreferrer"">PyTorch documentation on torch.save and torch.load</a> contains the following section:</p>
<h2>Save on CPU, Load on GPU</h2>
<p>Save:</p>
<pre><code>torch.save(model.state_dict(), PATH)
</code></pre>
<p>Load:</p>
<pre><code>device = torch.device(&quot;cuda&quot;)
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location=&quot;cuda:0&quot;))  # Choose whatever GPU device number you want
model.to(device)
</code></pre>
<blockquote>
<p>When loading a model on a GPU that was trained and saved on CPU, set
the map_location argument in the torch.load() function to
cuda:device_id. This loads the model to a given GPU device. Next, be
sure to call model.to(torch.device('cuda')) to convert the model’s
parameter tensors to CUDA tensors.</p>
</blockquote>
<p>Isn't calling <code>.to(device)</code> in this example redundant since calling <code>load</code> with <code>map_location</code> must have already placed it in the GPU?</p>
<p>Moreover, the text says &quot;Next, be sure to call model.to(torch.device('cuda')) to convert the model’s parameter tensors to CUDA tensors&quot;. But wasn't that conversion already done during <code>load</code> with <code>map_location</code>?</p>
","2024-02-28 00:42:30","2","Question"
"78070053","78052337","","<p>problem was this line :</p>
<pre><code>x_flattened = x_padded.view(x_padded.size(0), -1) 
</code></pre>
<p>had to use this :</p>
<pre><code>x_flattened = x_padded.view(x_padded.size(1), -1) 
</code></pre>
","2024-02-27 19:19:34","0","Answer"
"78068452","78067070","","<p>There is very little context in your question. For example, I don't know which other dependencies you use. However, I encountered the same error before, and with me it was caused by a <code>botocore</code> dependency which caused a conflict with <code>urllib3&gt;=2.0.0</code>. So, you may want to check your dependencies. Or even better, try <code>urllib3&lt;2</code>.</p>
<p>See also: <a href=""https://stackoverflow.com/questions/76414514/cannot-import-name-default-ciphers-from-urllib3-util-ssl-on-aws-lambda-us"">&quot;cannot import name &#39;DEFAULT_CIPHERS&#39; from &#39;urllib3.util.ssl_&#39;&quot; on AWS Lambda using a layer</a></p>
","2024-02-27 14:52:32","0","Answer"
"78068250","78065633","","<p>By default, if an integer <code>i</code> is provided as an argument to <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.to.html"" rel=""nofollow noreferrer""><code>torch.Tensor.to</code></a>, it will consider the <code>i</code>-th cuda device. Here is a test:</p>
<pre><code>&gt;&gt;&gt; torch.rand(0).to(0).device
device(type='cuda', index=0)

&gt;&gt;&gt; torch.rand(0, device=0).device
device(type='cuda', index=0)
</code></pre>
<p>Which means <code>.to(0)</code> will be same as <code>.to('cuda:0')</code>, <code>to(torch.device('cuda'))</code>, or even <code>.cuda()</code>, which defaults to the first device <em>ie.</em> <code>cuda:0</code>.</p>
","2024-02-27 14:20:05","1","Answer"
"78067070","","ImportError: cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_'","<p>How can I solve this error, I'm trying to load dataset locally on jupyter notebook.</p>
<pre><code>path=os.path.join(&quot;C:\\Users\\Adeel\\fashion-product-images-small&quot;)

from datasets import load_dataset
print(path)
fashion = load_dataset(
   path,
    split=&quot;train&quot;
)
fashion
</code></pre>
<p>I try to load dataset but on Jupyter notebook, it gives the following error:</p>
<blockquote>
<p>ImportError: cannot import name 'DEFAULT_CIPHERS' from
'urllib3.util.ssl_'</p>
</blockquote>
","2024-02-27 11:07:43","0","Question"
"78066911","78066450","","<p>This snippet should take less on the ground that it does fewer operations.</p>
<pre><code>p=0.5
D=[19000,19000]
torch.rand(D)&gt;p
</code></pre>
<p>As D increases, the mean approaches p.</p>
","2024-02-27 10:44:33","1","Answer"
"78066450","","Create a binary pytorch tensor with n% ones","<p>I want to create a binary tensor with 0 and 1s only, but the number of 1s should be n%.</p>
<p>I am using this code, but it takes 7+ seconds and has 19926MB of memory.</p>
<p>Is there any faster way to do that?</p>
<pre><code>import torch
import time
start_time = time.time()
def create_random_binary_tensor(shape, percentage, device):
    size = shape[0] * shape[1]
    num_ones = int(size * (percentage / 100.0))
    
    # Create a tensor filled with zeros
    tensor = torch.zeros(size, dtype=torch.float32, device=device)
    
    # Set random indices to ones until the desired number of ones is reached
    indices = torch.randperm(size, device=device)[:num_ones]
    tensor[indices] = 1
    
    # Reshape the tensor to the desired shape
    tensor = tensor.view(shape)
    
    return tensor

# Set the shape and percentage of ones
shape = (19000, 19000)
percentage = 0.5

# Check if CUDA is available and use it if possible
device = torch.device(&quot;cuda:2&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Create the random binary tensor
tensor = create_random_binary_tensor(shape, percentage, device)

# Print the tensor
execution_time = time.time() - start_time
print(&quot;Execution Time: {:.4f} seconds&quot;.format(execution_time))

</code></pre>
","2024-02-27 09:33:32","1","Question"
"78065633","","How does model.to(rank) work if rank is an integer? (DistributedDataParallel)","<p>I was looking at the basic implementation of DDP:</p>
<pre><code>class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def demo_basic(rank, world_size):
    print(f&quot;Running basic DDP example on rank {rank}.&quot;)
    setup(rank, world_size)

    # create model and move it to GPU with id rank
    model = ToyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(rank)
    loss_fn(outputs, labels).backward()
    optimizer.step()

    cleanup()


def run_demo(demo_fn, world_size):
    mp.spawn(demo_fn,
             args=(world_size,),
             nprocs=world_size,
             join=True)
</code></pre>
<p>Just wondering how PyTorch knows which GPU to put the model on just based off of rank? Usually we specify a torch.device() object to a model. How does Pytorch interpret it when the to() function is provided an integer?</p>
","2024-02-27 06:54:07","0","Question"
"78065575","78065138","","<p>The error is likely coming from an out of bounds index. Since you're adding a label, my guess is somewhere in the code the class index of the new label is trying to index into something sized for the old number of labels.</p>
","2024-02-27 06:42:44","0","Answer"
"78065185","78065138","","<p><strong>Yes , it is possible to add data during training time , it dynamically updates during training !</strong>
Check the inputs providing to training model, if encoded well or mapped to your model performance ?
<strong>CUDA Error -</strong> occurring due to exceeding the dataset range or accessing elements beyond dataset size.</p>
","2024-02-27 04:41:18","0","Answer"
"78065138","","Can I change the size of input data during training?","<p>I am trying to do active learning and bump into the error:</p>
<pre><code>RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. 
</code></pre>
<p>I think the reason of this error is because that I add labeled data during training.
Is it possible to add data to dataset during training?</p>
<p>I have tried using just CPU.
But it's too slow.</p>
","2024-02-27 04:19:48","0","Question"
"78063823","","MSE loss with PyTorch","<p>I have some trained models with their corresponding MSE, which has been computed using the nn.MSELoss function of PyTorch. <a href=""https://i.sstatic.net/ZAzwJ.png"" rel=""nofollow noreferrer"">You can see the plot here.</a></p>
<p>When I test the models and I calculate the MSE without the built-in function, I get <a href=""https://i.sstatic.net/rnzEw.png"" rel=""nofollow noreferrer"">the plot that can be seen here.</a>. It has a lot of peaks and it is no longer smooth.</p>
<p>To calculate the MSE, I am using this formula (with numpy as np):</p>
<pre><code>mse = np.mean((Cp_train - Cp_train_predicted)**2)
</code></pre>
<p>where <code>Cp_train</code> is my ground truth vector and <code>Cp_train_predicted</code>is my array of predictions.</p>
<p>As far as I now, it should give the same results as if I had used the built-in function in training. Did I misunderstand the internal working of MSELoss?</p>
","2024-02-26 20:56:56","1","Question"
"78063779","78063474","","<p>If I'm understanding the problem correctly, you are computing the cross entropy loss of a vector of size <code>(N)</code> or <code>(1, N)</code> (ie a single item, not a batch). In this case, there is only one expected loss value. <code>CrossEntropyLoss</code> produces one value per item in the batch.</p>
<pre class=""lang-py prettyprint-override""><code>loss = nn.CrossEntropyLoss(reduction='none')

n_classes = 5
batch_size = 3
input = torch.randn(batch_size, n_classes)
target = torch.empty(batch_size, dtype=torch.long).random_(n_classes)
output = loss(input, target)
output.shape
&gt; torch.Size([3]) # output is one value for each item in the batch

batch_size = 1
input = torch.randn(batch_size, n_classes)
target = torch.empty(batch_size, dtype=torch.long).random_(n_classes)
output = loss(input, target)
output.shape
&gt; torch.Size([1]) # output is one value for each item in the batch
</code></pre>
<p>Cross entropy is computed as the negative log prob of the expected class. Since there is only one target class per item, there is only one loss value per item.</p>
<pre class=""lang-py prettyprint-override""><code>loss = nn.CrossEntropyLoss(reduction='none')

n_classes = 5
batch_size = 1
input = torch.randn(batch_size, n_classes)
target = torch.empty(batch_size, dtype=torch.long).random_(n_classes)

loss_value = loss(input, target)

log_probs = nn.functional.log_softmax(input, dim=-1)

torch.allclose(-log_probs[:, target], loss_value) # CE loss is the same as negative log prob of expected class 
&gt; True
</code></pre>
","2024-02-26 20:46:38","1","Answer"
"78063717","78056849","","<p>You can decouple the nested function using chain rule. However, there will be some differences due to numerical issues.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import time

def slow_fun(x):
    A = x*torch.ones((1000,1000))
    B = torch.matrix_exp(1j*A)
    return torch.real(torch.trace(B))

# baseline z1
x = torch.tensor(1.0, requires_grad = True)
y = slow_fun(x)
z1 = y**2
z1.backward()
dz1_dx = x.grad
print(dz1_dx)
&gt; tensor(-1648274.2500)

# baseline z2
x = torch.tensor(1.0, requires_grad = True)
y = slow_fun(x)
z2 = torch.sqrt(y)
z2.backward()
dz2_dx = x.grad
print(dz2_dx)
&gt; tensor(-13.1979)

# compute just dy/dx
x = torch.tensor(1.0, requires_grad = True)
y = slow_fun(x)
y.backward()
dy_dx = x.grad

# detach y to prevent full backprop
y1 = y.detach()
y1.requires_grad = True
z1 = y1**2
z1.backward()
dz1_dy = y1.grad
# compute gradient with chain rule
dz1_dx = dz1_dy * dy_dx
print(dz1_dx)
&gt; tensor(-1672148.5000)

# detach y to prevent full backprop
y2 = y.detach()
y2.requires_grad = True
z2 = torch.sqrt(y2)
z2.backward()
dz2_dy = y2.grad
# compute gradient with chain rule
dz2_dx = dz2_dy * dy_dx
print(dz2_dx)
&gt; tensor(-13.1980)
</code></pre>
","2024-02-26 20:34:36","0","Answer"
"78063595","78037360","","<p>This could be happening due to few issues -</p>
<ol>
<li>The request is timing out beyond the 60s limit in SageMaker Hosting.</li>
<li>SageMaker uses /PING and /Invocations to respond, the fact that you are seeing 500 errors on /Ping tells me that you might not be reaching the container successfully.</li>
</ol>
<p>To debug this issue, I would advise you to 1/ Try local Inference within your SageMaker notebook instance( or ec2) to run the container and check for any issues. 2/ Add debug logs into your container code( model.py) to see where the issue is coming from. The error in CW is very generic and the issue cannot be determined.</p>
","2024-02-26 20:07:37","0","Answer"
"78063474","","Accessing the N values of PyTorch's Cross-Entropy loss function","<p>I am trying to access specific values of PyTorch's Cross-Entropy loss function (torch.nn.functional.cross_entropy) that I believe are being calculated when the input is a vector of length N. I would like access to the vector of N individual loss values; not the mean or sum or whathaveyou as I believe is what is being returned.</p>
<p>From looking at the documentation, I tried setting &quot;reduction = None&quot;; however, it still returns a scalar value. The function's default setting is to return the mean.</p>
<p>Here is the error message:</p>
<pre><code>Exception has occurred: IndexError

slice() cannot be applied to a 0-dim tensor.

IndexError: slice() cannot be applied to a 0-dim tensor.
</code></pre>
<p>Here's is the code snippet leading up to the error:</p>
<pre><code>tMinusOne_loss = burnIn_model.loss(combined_tMinusOne_X, combined_tMinusOne_Y)

print(&quot;tMinusOne_loss:&quot;, tMinusOne_loss)

tMinusOne_first_loss = tMinusOne_loss[ :len(combined_tMinusOne_X_first)]
</code></pre>
<p>Here is what is printed out from the print line right before the error occurs:</p>
<pre><code>tMinusOne_loss: tensor(0.3171, grad_fn=&lt;NllLossBackward0&gt;)
</code></pre>
<p>Thank you!</p>
","2024-02-26 19:40:05","1","Question"
"78062849","78061733","","<p>One side-effect of this operation is that it creates a copy of <code>self.gammas</code>. Consider the following script:</p>
<pre><code>betas = torch.ones(2, 3)
gammas = betas
# gammas = gammas.unsqueeze(2).squeeze(2)

betas.data = torch.clamp(betas.data, min=2.0)

assert torch.all(gammas == 2.0)
</code></pre>
<p>This code will run successfully. However, if you uncomment line three, <code>gammas</code> will no longer point to the same object as <code>betas</code>, so the increase of the values in <code>betas</code> will not be reflected by <code>gammas</code>.</p>
","2024-02-26 17:34:20","0","Answer"
"78061773","78057322","","<p>Self resolved.</p>
<p>The functionality of the old torch.fft corresponds to the new <strong>torch.fft.fft, fft2</strong>, or <strong>fftn</strong>.</p>
<p>The important thing is the value of signal_ndim in torch.fft, i.e., how many dimensions of FFT you want to perform.
To prepare, we first converse Real (3, 4, 2) -&gt; Complex (3,4)</p>
<pre><code>a = torch.view_as_complex(a)
</code></pre>
<p>Then,</p>
<pre><code># If signal_ndim=1 (1D FFT)
torch.fft.fft(a)

# If signal_ndim=2 (2D FFT)
torch.fft.fft2(a)

# If signal_ndim=k (k&gt;=3)
torch.fft.fftn(a, dim=k)
</code></pre>
","2024-02-26 14:46:45","-1","Answer"
"78061733","","Why could a no-op change the optimization result?","<p>I'm modifying a part of a fairly large tool that uses PyTorch to optimize some parameters. I noticed that if I add this line to the code, the results are getting worse:</p>
<pre><code>self.gammas = self.gammas.unsqueeze(2).squeeze(2)
</code></pre>
<p><code>self.gammas</code> is a tensor that at this point only contains zeros, and will later be optimized.</p>
<p>Unfortunately, I'm not able to condense the project into a small example, but are there general reasons why this no-op might have an impact on the optimization process?</p>
<p>I've made sure that the process is deterministic: If I run the code multiple times both with an without the modification, the results are consistent.</p>
","2024-02-26 14:40:31","0","Question"
"78061357","78060302","","<p>Considering <a href=""https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare"" rel=""nofollow noreferrer""><code>Accelerate.prepare</code></a> <a href=""https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1171"" rel=""nofollow noreferrer"">iterates over each passed object individually</a>, calling it twice will have the same effect. The expected list of argument should contain <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer""><code>data.DataLoader</code></a>, <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html"" rel=""nofollow noreferrer""><code>nn.Module</code></a>, <a href=""https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer"" rel=""nofollow noreferrer""><code>optim.Optimizer</code></a>, or <a href=""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"" rel=""nofollow noreferrer""><code>optim.lr_scheduler.LRScheduler</code></a>, its purpose is to transfer to the desired device and cast to the desired data type.</p>
","2024-02-26 13:37:52","1","Answer"
"78061257","78058636","","<p>One of your packages seems to be corrupt. You can try cleaning the packages from cache and reinstalling pytorch:</p>
<pre><code>conda clean -p
</code></pre>
","2024-02-26 13:24:43","5","Answer"
"78060354","78056709","","<p>I was unable to reproduce, same as Karl. Also increased the size to check for some RAM/bandiwdth issue. Still, found nothing wrong:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
import torch
from time import time


tensor = torch.FloatTensor(200_000, 16_000).uniform_() &gt; 0.8 # random 1's and 0's

all_timings = []
all_sizes = np.arange(1600, 16001, 1600)
for size in all_sizes:
    timings = []
    for _ in range(5):
        start_time = time()
        tensor[:, :size].sum(dim=0)
        timings.append(time() - start_time)

    all_timings.append(min(timings))

plt.plot(all_sizes, all_timings)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/XWa5V.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XWa5V.png"" alt=""enter image description here"" /></a></p>
<p>Timings are increasing linearly, as expected.</p>
","2024-02-26 10:51:20","0","Answer"
"78060302","","Can I call prepare() separately on multiple models or should it be a single call when using accelerator with pytorch?","<p>I'm training two deep learning models in tandem, say <code>model1</code> and <code>model2</code> using pytorch. I'm using <code>accelerator</code> to handle distributed training. When calling the <code>prepare()</code> function, can I call it separately as in the below code?</p>
<pre><code>model1, dataloader, optimizer = accelerator.prepare(model1, dataloader, optimizer)
if some_condition:
    model2 = acceletator.prepare(model2)
</code></pre>
<p>Or do I have to call <code>prepare()</code> only once, as in the below code?</p>
<pre><code>if not some_condition:
    model1, dataloader, optimizer = accelerator.prepare(model1, dataloader, optimizer)
else:
    model1, model2, dataloader, optimizer = acceletator.prepare(model1, model2, dataloader, optimizer)
</code></pre>
","2024-02-26 10:43:43","3","Question"
"78059111","77824012","","<p>From the pytorch documentation: <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html</a></p>
<p>This layer uses statistics computed from input data in both training and evaluation modes.</p>
<p><a href=""https://i.sstatic.net/9TgLc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9TgLc.png"" alt=""layernorm formu"" /></a></p>
<p>The E[x] and Var[x] are calculated on every input tensor.
Only the γ and β are fixed at training time.</p>
<p>Thus what you are observing is the correct and expected behavior.</p>
","2024-02-26 07:08:55","1","Answer"
"78058636","","CondaVerificationError when installing PyTorch","<p>I am trying to install pytorch using either of below command and I got a lot of error. I am using windows CPU only.</p>
<pre><code>conda install pytorch::pytorch

or 
conda install pytorch torchvision torchaudio cpuonly -c pytorch

</code></pre>
<p>some of the error are</p>
<pre><code>CondaVerificationError: The package for pytorch located at C:\Users\test\miniconda3\pkgs\pytorch-2.2.1-py3.10_cpu_0
appears to be corrupted. The path 'Lib/site-packages/torchgen/static_runtime/gen_static_runtime_ops.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at C:\Users\test\miniconda3\pkgs\pytorch-2.2.1-py3.10_cpu_0
appears to be corrupted. The path 'Lib/site-packages/torchgen/static_runtime/generator.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at C:\Users\test\miniconda3\pkgs\pytorch-2.2.1-py3.10_cpu_0
appears to be corrupted. The path 'Lib/site-packages/torchgen/utils.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at C:\Users\test\miniconda3\pkgs\pytorch-2.2.1-py3.10_cpu_0
appears to be corrupted. The path 'Lib/site-packages/torchgen/yaml_utils.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at C:\Users\test\miniconda3\pkgs\pytorch-2.2.1-py3.10_cpu_0
appears to be corrupted. The path 'Scripts/convert-caffe2-to-onnx-script.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at C:\Users\test\miniconda3\pkgs\pytorch-2.2.1-py3.10_cpu_0
appears to be corrupted. The path 'Scripts/convert-onnx-to-caffe2-script.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at C:\Users\test\miniconda3\pkgs\pytorch-2.2.1-py3.10_cpu_0
appears to be corrupted. The path 'Scripts/torchrun-script.py'
specified in the package manifest cannot be found.
</code></pre>
","2024-02-26 04:31:06","3","Question"
"78057938","78057322","","<p>In old Pytorch versions, for complex numbers the last dimension was used to store real and imaginary part in a normal float tensor.</p>
<p>Since 1.7ish, you can use complex types for tensors and these are used in torch.fft.fft.</p>
<p>You can use <code>torch.view_as_complex</code>, for example, for the conversion Real (3, 4, 2) -&gt; Complex (3,4) and  <code>view_as_real</code> to back an old-style real tensor of shape (3, 4, 2):</p>
<pre><code>F = torch.fft.fftn(torch.view_as_complex(a))
F_asreal = torch.view_as_real(F)
</code></pre>
<p>This should match an 'old' <code>torch.fft call</code></p>
","2024-02-25 22:42:11","0","Answer"
"78057322","","How to make the old torch.fft and the new torch.fft.fft compatible in Pytorch?","<p>How can I get a value that represents the same meaning from the new torch.fft.fft, with torch.fft that we could use until v1.7.1?</p>
<p>The documentation is here.</p>
<p><a href=""https://pytorch.org/docs/1.7.1/generated/torch.fft.html?highlight=torch%20fft"" rel=""nofollow noreferrer"">https://pytorch.org/docs/1.7.1/generated/torch.fft.html?highlight=torch%20fft</a></p>
<p><a href=""https://pytorch.org/docs/2.0/generated/torch.fft.fft.html?highlight=torch+fft#torch.fft.fft"" rel=""nofollow noreferrer"">https://pytorch.org/docs/2.0/generated/torch.fft.fft.html?highlight=torch+fft#torch.fft.fft</a></p>
<p>Specifically, I am having trouble with the following 3D input (assuming a 2D image).</p>
<pre><code>    a = torch.tensor([[[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0]],
                      [[2.0, 3.0], [4.0, 5.0], [6.0, 7.0], [8.0, 9.0]],
                      [[3.0, 4.0], [5.0, 6.0], [7.0, 8.0], [9.0,10.0]]])
    # print(torch.fft(a,signal_ndim=2,normalized=False)) #old
</code></pre>
<p>As the document says, torch.fft.fft accepts the deepest dimension of the input data as [complex], while the old torch.fft seems to accept [real,imag]. (This is the reason why the assumed data and the input dimension are off by one.)</p>
<p>Appendix:</p>
<p>For 2D input (assuming 1D data), the following code succeeded. It may be of help to us.</p>
<pre><code>#old @Pytorch v1.7.1
    b = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
    print(b.shape)
    print(torch.fft(b,signal_ndim=1,normalized=False))
    
#new @Pytorch v1.7.1
import torch.fft
    b = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
    b = b[:,0]+1j*b[:,1]
    b = torch.unsqueeze(b,1)
    print(b)
    print(b.shape)
    print(torch.fft.fft(b, dim=0))

# With v1.7.1 (example), we can use torch.fft.fft as well, 
# so we do not need to prepare two environments.
# However, we cannot use it at the same time as the old torch.fft 
# because the module names conflict.

</code></pre>
<p>Output</p>
<pre><code>#old
torch.Size([3, 2])
tensor([[ 9.0000, 12.0000],
        [-4.7321, -1.2679],
        [-1.2679, -4.7321]])

#new
tensor([[1.+2.j],
        [3.+4.j],
        [5.+6.j]])
torch.Size([3, 1])
tensor([[ 9.0000+12.0000j],
        [-4.7321-1.2679j],
        [-1.2679-4.7321j]])

</code></pre>
<p>Will appreciate any help...;; Thanks,</p>
","2024-02-25 19:12:30","0","Question"
"78056849","","Calculating two gradients in pytorch and reusing an intermediate gradient","<p>Suppose we have a function f whose gradient is slow to compute, and two functions <code>g1</code> and <code>g2</code> whose gradient is easy to compute. In <code>pytorch</code>, how can I calculate the gradients of <code>z1 = g1(f(x))</code> and <code>z2 = g2(f(x))</code> with respect to x, without having to calculate the gradient of f twice?</p>
<p>Example:</p>
<pre><code>import torch
import time

def slow_fun(x):
    A = x*torch.ones((1000,1000))
    B = torch.matrix_exp(1j*A)
    return torch.real(torch.trace(B))

x = torch.tensor(1.0, requires_grad = True)
y = slow_fun(x)
z1 = y**2
z2 = torch.sqrt(y)

start = time.time()
z1.backward(retain_graph = True)
end = time.time()
print(&quot;dz1/dx: &quot;, x.grad)
print(&quot;duration: &quot;, end-start, &quot;\n&quot;)

x.grad = None
start = time.time()
z2.backward(retain_graph = True)
end = time.time()
print(&quot;dz2/dx: &quot;, x.grad)
print(&quot;duration: &quot;, end-start, &quot;\n&quot;)
</code></pre>
<p>This prints</p>
<pre><code>dz1/dx:  tensor(-1673697.1250)
duration:  1.5571658611297607

dz2/dx:  tensor(-13.2334)
duration:  1.3989012241363525
</code></pre>
<p>so calculating <code>dz2/dx</code> takes about as long as calculating <code>dz1/dx</code>.</p>
<p>The calculating of <code>dz2/dx</code> could be sped up if <code>pytorch</code> would store <code>dy/dx</code> during the calculation of <code>dz1/dx</code>, and then reuse that result during the calculation of <code>dz2/dx</code>.</p>
<p>Is there a mechanism built into <code>pytorch</code> to achieve such a behavior?</p>
","2024-02-25 16:53:34","4","Question"
"78056828","78056762","","<p>Since your classification layer is a single linear layer that takes the reconstruction predictions as input and outputs the class predictions, the network has to find a hard balance between reconstruction quality and linear separability of the predicted images.</p>
<p>If you want to keep this architecture, you should try to give different weights to the reconstruction loss and classification loss, something like: <code>loss = recon_loss + 10*class_loss</code>.</p>
<p>You can try adding an activation, another layer and a softmax on top of the current linear classification layer for a better classification. However, it's probably better to change the architecture and generate the classification and the reconstruction predictions with different network branches from the latent representations, similarly to this: <a href=""http://tech.octopus.energy/timeserio/_images/MNIST.svg"" rel=""nofollow noreferrer"">http://tech.octopus.energy/timeserio/_images/MNIST.svg</a>.</p>
","2024-02-25 16:46:06","1","Answer"
"78056762","","PyTorch correct implementation of classification on an autoencoder","<p><strong>EDIT: embarrassingly my error was shuffling the data only and not the labels.</strong></p>
<p>I was given an assignment to create an lstm autoEncoder in pytorch to reconstruct mnist images.
next the assignment asked to modify the network to also allow for classification of the reconstructed images, an important part is that it should do the 2 tasks at the same time, reconstruction and classification of the reconstructed image, so the network should train on both the losses at the same time.</p>
<p>my implementation of the auto encoder is in this format:</p>
<pre><code>def __init__(self, input_size, hidden_size, num_layers, output_size, epochs, optimizer, learning_rate, grad_clip, batch_size):
    super(AE, self).__init__()
    self.encoder = Encoder(input_size, hidden_size, num_layers)
    self.decoder = Decoder(input_size, hidden_size, num_layers, output_size)
    self.epochs = epochs
    self.optimizer = optimizer
    self.learning_rate = learning_rate
    self.grad_clip = grad_clip
    self.batch_size = batch_size
    self.criterion = nn.MSELoss()
    self.losses = []
</code></pre>
<p>the forward and the train methods work fine and when i run the network on the mnist datat set i get a fairly well reconstructed images with MSE loss averaging at around 1e-6.</p>
<p>I introduced the classifying elemnt in a separate class:</p>
<pre><code>class AeWithClassifier(AE):
    def __init__(self, input_size, hidden_size, num_layers, output_size, epochs, optimizer, learning_rate, grad_clip, batch_size, num_classes):
        super(AeWithClassifier, self).__init__(input_size, hidden_size, num_layers, output_size, epochs, optimizer, learning_rate, grad_clip, batch_size)

        self.classifier = nn.Sequential(
            nn.Linear(output_size*output_size, num_classes))
        self.classifier_criterion = nn.CrossEntropyLoss()
</code></pre>
<p>the methods are pretty straight forward but I will provide them:</p>
<pre><code>def forward(self, x):
    predictions = super().forward(x)
    classifier_predictions = self.classifier(predictions.reshape(-1, 28*28))
    return predictions, classifier_predictions
</code></pre>
<pre><code>def train(self, x, y):
        losses = []
        optimizer = self.optimizer(self.parameters(), lr=self.learning_rate)

        for epoch in range(self.epochs):
            cur_loss = 0
            batch_idx = 0
            for batch_idx, x_batch in enumerate(x):
                x_batch = x_batch.to(device)
                y_batch = y[batch_idx*self.batch_size:(batch_idx+1)*self.batch_size]
                optimizer.zero_grad()
                predictions, classifier_predictions = self.forward(x_batch)

                recon_loss = self.criterion(predictions, x_batch)
                class_loss = self.classifier_criterion(classifier_predictions, y_batch)
                cur_loss = loss = recon_loss + class_loss

                loss.backward()
                nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)
                optimizer.step()
            losses.append(cur_loss.item())
            print(f'Epoch: {epoch+1}/{self.epochs}, Loss: {cur_loss.item()}')
        self.losses = losses
</code></pre>
<p>as you can see i calculated the loss as the sum of the loss of the reconstruction and the loss of the classification and then I use torch to perform grad calculation and optimization.</p>
<p>however in this format despite the fact that the network still reconstructs the images it fails to classify them properly with cross entropy loss doesn't decrease below ~2.3</p>
<p>Am I doing something wrong in the construction of the network? or is the problem in the training itself?
I tried weighing the loss differently in order for the network to focus more on the classification task but it still doesn't improve at all.</p>
","2024-02-25 16:26:55","-2","Question"
"78056709","","PyTorch tensor.sum() performance drops with large tensors vs. NumPy","<p><code>tensor.sum()</code> performance drops once my tensor exceeds a certain size. Why is that?</p>
<pre><code>import torch

tensor = torch.FloatTensor(200_000, 2_000).uniform_() &gt; 0.8 # random 1's and 0's
tensor[:, :1000].sum(dim=0)
tensor[:, :2000].sum(dim=0) # 2x wider but 20x slower
</code></pre>
<p>Profiling:</p>
<pre><code>import time
import torch

start = time.time()
tensor[:, :1000].sum(dim=0)
end = time.time()
print(end - start) # 0.69s

start = time.time()
tensor[:, :2000].sum(dim=0)
end = time.time()
print(end - start) # 20s
</code></pre>
<p>NumPy doesn't appear to share this limitation:</p>
<pre><code>import numpy as np

start = time.time()
np.array(tensor[:, :2000]).sum(axis=0)
end = time.time()
print(end - start) # 0.40s
</code></pre>
","2024-02-25 16:16:47","1","Question"
"78056590","78056502","","<p>The error you're getting is because the output of your first layer (<code>fcs</code>) has dimension <code>N_HIDDEN</code> (which is 10), while the hidden layers in <code>fch</code> have input dimension <code>N_INPUT</code> (which is 2).</p>
<p>To fix this, you have to ensure that the input size for all layers matches the output size of the previous layer. In your code:</p>
<pre><code>class FCN(nn.Module):
    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):
        super().__init__()
        activation = nn.Tanh
        self.fcs = nn.Sequential(
            nn.Linear(N_INPUT, N_HIDDEN),
            activation()
        )
        self.fch = nn.Sequential(*[
            nn.Sequential(
                nn.Linear(N_HIDDEN, N_HIDDEN),  # Adjust input size to N_HIDDEN
                activation()
            ) for _ in range(N_LAYERS - 1)
        ])
        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)  # Output layer

    def forward(self, x):
        x = self.fcs(x)
        x = self.fch(x)
        x = self.fce(x)
        return x
</code></pre>
<p>Finally, to get good performance you should play with the hidden size (not just between 2 and 10, you can also try 100 or 1000), the number of layers (start with 1 or 2, not 8) and the learning rate of the optimizer.</p>
","2024-02-25 15:41:45","1","Answer"
"78056502","","Changing the number of hidden layers in my NN results in an error","<p>As the title says, if I change the number of hidden layers in my pytorch neural network to be anything different from the amount of input nodes it returns the error below.</p>
<blockquote>
<p>RuntimeError: mat1 and mat2 shapes cannot be multiplied (380x10 and 2x10)</p>
</blockquote>
<p>I think that the architecture is incorrectly coded but I am relatively new to pytorch and neural networks so I can't spot the mistake. Any help is greatly appreciated, I've included the code below</p>
<pre><code>class FCN(nn.Module):

def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):
    super().__init__()
    activation = nn.Tanh
    self.fcs = nn.Sequential(*[
        nn.Linear(N_INPUT, N_HIDDEN),
        activation()])
    self.fch = nn.Sequential(*[
                  nn.Sequential(*[
                      nn.Linear(N_INPUT, N_HIDDEN),
                      activation()]) for _ in range(N_LAYERS-1)])
    self.fce = nn.Linear(N_INPUT, N_HIDDEN)

def forward(self, x):

    x = self.fcs(x)
    x = self.fch(x)
    x = self.fce(x)
    
    return x


torch.manual_seed(123)

pinn = FCN(2, 2, 10, 8)
</code></pre>
<p>If the pinn architecture is defined as <code>pinn = FCN(2, 2, 2, 8)</code> no errors are returned but neural network does not perform well.</p>
<p>Other information:</p>
<ul>
<li>the input is a matrix tensor with a batch size of 380</li>
</ul>
<p>Please let me know if you need anymore information and thank you!</p>
","2024-02-25 15:17:38","1","Question"
"78056171","78045174","","<p>A 7 second epoch sounds really small. I feel like at that point you might be running into some sort of pcie bandwidth/latency limit. (Basically your <code>.to(&quot;cuda&quot;)</code> is probably really slow.)
You could try 2 benchmarks to narrow this down.</p>
<ol>
<li>where you skip everything except the <code>.to(&quot;cuda&quot;)</code></li>
<li>where you use the first batch and have the training part without <code>.to(&quot;cuda&quot;)</code></li>
</ol>
<p>1 -</p>
<pre class=""lang-py prettyprint-override""><code>    for i, data in enumerate(train_loader, 1):
        img, label = data
        img = Variable(img).to(&quot;cuda&quot;)
        label = Variable(label).to(&quot;cuda&quot;)
</code></pre>
<p>2 -</p>
<pre class=""lang-py prettyprint-override""><code>    running_loss = 0.0
    running_acc = 0.0
    img, label = next(iter(train_loader))
    img = Variable(img).to(&quot;cuda&quot;)
    label = Variable(label).to(&quot;cuda&quot;)
    for i, data in enumerate(train_loader, 1):
        out = model(img)
        loss = criterion(out, label)  # loss
        running_loss += loss.item() * label.size(0) 
        _, pred = torch.max(out, 1) 
        num_correct = (pred == label).sum() 
        # accuracy = (pred == label).float().mean()
        running_acc += num_correct.item() 
        optimizer.zero_grad() 
        loss.backward()
        optimizer.step()

</code></pre>
<p>it could also be a data loading limit, but I don't think that should be the case because MNIST should fit in ram. You could also use cProfile (part of stdlib) or another profiler to check the timing in more detail.</p>
","2024-02-25 13:46:55","0","Answer"
"78055526","78053224","","<p>This is called an <a href=""https://en.wikipedia.org/wiki/Outer_product"" rel=""nofollow noreferrer"">outer operation</a>, given a single tensor shaped <code>(*, i)</code>, you are looking to compute an operation cross-element-wise resulting in a tensor shaped <code>(*,i,i)</code>. You can achieve this by thoughtfully unsqueezing extra dimensions on the initial tensor.</p>
<p>In your first example, you have a shape of <code>(i,)</code>, if you unsqueeze dimensions twice two different ways: one as <code>(i,1)</code> and the other <code>(1,i)</code>, then applying an element-wise operator (such as <code>__sub__</code>, <code>__add__</code>, etc...) between the two will perform the outer operation: the result will be shaped <code>(i,i)</code>.</p>
<p>In your case:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 3, 4, 7])
&gt;&gt;&gt; x[None]-x[:,None]
tensor([[ 0,  2,  3,  6],
        [-2,  0,  1,  4],
        [-3, -1,  0,  3],
        [-6, -4, -3,  0]])
</code></pre>
<p>You always need to unsqueeze around the dimension of interest, <em>ie.</em> on the last and before-last dimensions. So in the 4D case:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[[[6,5,1], [2, 5, 3]], [[1, 4, 8], [8, 6, 4]]]])
&gt;&gt;&gt; x[...,None,:]-x[...,None]
tensor([[[[[ 0, -1, -5],
           [ 1,  0, -4],
           [ 5,  4,  0]],

          [[ 0,  3,  1],
           [-3,  0, -2],
           [-1,  2,  0]]],


         [[[ 0,  3,  7],
           [-3,  0,  4],
           [-7, -4,  0]],

          [[ 0, -2, -4],
           [ 2,  0, -2],
           [ 4,  2,  0]]]]])
</code></pre>
","2024-02-25 09:55:59","1","Answer"
"78053841","78053086","","<p>In the example below, I first solve the ODE using a standard solver <code>scipy.integrate.solve_ivp</code>. The solution is used to train the network, as it gives us a target <code>y</code> for each <code>t</code>. The net will learn parameters such that given <code>t</code> and <code>y0</code>, it will closely match the reference <code>y</code>.</p>
<p>After training, you can supply <code>t</code> and <code>y0</code> to the network, and it will output the estimated solution <code>y_hat</code> for each <code>t</code>.</p>
<p>Note that this example is somewhat minimal - you'd usually want to be evaluating the model on samples it hasn't seen (a validation set), otherwise it might just be memorising the training data without being able to generalise to unseen <code>t</code> (though maybe it's not an issue for your use-case).</p>
<pre class=""lang-py prettyprint-override""><code>Net comprises 501 parameters
[epoch  100/ 500] loss: 0.00044
[epoch  200/ 500] loss: 0.00015
[epoch  300/ 500] loss: 0.00011
[epoch  400/ 500] loss: 0.00008
[epoch  500/ 500] loss: 0.00004
</code></pre>
<p><a href=""https://i.sstatic.net/KfHXP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KfHXP.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch import nn
from torch import optim

import numpy as np
import matplotlib.pyplot as plt

#Input data
n_samples = 100
t_array = np.linspace(0, 2, num=n_samples)
y0 = 1.0

#ODE function
# dy/dt + 2y + t = 0 --&gt; dy/dt = -(2y + t)
def dy_dt(t, y):
    return -(2 * y + t)

#Solve using scipy
# The solution will be used to train the neural network
from scipy.integrate import solve_ivp
solved = solve_ivp(dy_dt, [t_array.min(), t_array.max()], np.array([y_0]), t_eval=t_array)
solved_y = solved.y.ravel()

plt.plot(t_array, solved_y, color='cadetblue', linewidth=3, label='RK45 solver')
plt.xlabel('t')
plt.ylabel('y')
plt.gcf().set_size_inches(8, 3)

#
# Define the ODE net
#
class ODENet(nn.Module):
    def __init__(self, model_size=20, output_dim=1, activation=nn.ReLU):
        super().__init__()
        
        self.map_inputs = nn.Sequential(nn.Linear(2, model_size), activation())
        
        self.hidden_mapping = nn.Sequential(
            nn.Linear(model_size, model_size),
            activation()
        )
        
        self.output = nn.Linear(model_size, output_dim)
    
    def forward(self, x):
        # t, y0 = x[:, 0], x[:, 1]
        mapped_inputs = self.map_inputs(x)
        hidden = self.hidden_mapping(mapped_inputs)
        y_hat = self.output(hidden)
        return y_hat
    
print('Net comprises', sum([p.numel() for p in ODENet().parameters()]), 'parameters')

#Define the loss
# could alternatively pick from PyTorch's provided losses
def mse_loss(pred, target):
    return torch.mean((pred - target) ** 2)

#
# Create model
#
torch.manual_seed(0) #reproducible results

model = ODENet()
optimizer = optim.NAdam(model.parameters())

#Prepare the input data
# Convert to tensors
t_tensor = torch.Tensor(t_array).float().reshape(-1, 1)
y0_tensor = torch.Tensor([y0] * len(t_array)).float().reshape(-1, 1)
solved_y_tensor = torch.Tensor(solved_y).float()

# Combine inputs into a single matrix to make manpulation more compact
t_y0 = torch.cat([t_tensor, y0_tensor], dim=1)

#Scale the input features
# Will help the net's convergence, though not always abs necessary
t_y0 = (t_y0 - t_y0.mean(dim=0)) / (t_y0.std(dim=0) + 1e-10)

#
#Train model
#
n_epochs = 500
for epoch in range(n_epochs):
    model.train()
    
    y_hat = model(t_y0).flatten()

    #Loss, derivative, and step optimizer
    optimizer.zero_grad()
    loss = mse_loss(y_hat, solved_y_tensor)
    loss.backward()
    optimizer.step()
    
    #Print losses
    if ((epoch + 1) % 100) == 0 or (epoch + 1 == n_epochs):
        print(
            f'[epoch {epoch + 1:&gt;4d}/{n_epochs:&gt;4d}]',
            f'loss: {loss.item():&gt;7.5f}'
        )

#Get the final predictions, and overlay onto the solver's solution
model.eval()
with torch.no_grad():
    predictions = model(t_y0)
    
plt.plot(t_array, predictions, color='sienna', linewidth=3, linestyle=':', label='ODENet')
plt.legend()

#Optional formatting
[plt.gca().spines[spine].set_visible(False) for spine in ['right', 'top']]
plt.gca().spines['bottom'].set_bounds(t_array.min(), t_array.max())
plt.gca().spines['left'].set_bounds(-0.75, 1)
</code></pre>
","2024-02-24 20:13:16","1","Answer"
"78053550","78052918","","<ol>
<li><p>GAE is still Google Cloud (Google Cloud consists of multiple products/services). I assume you're asking if you should switch to maybe Google Compute Engine (GCE) or Cloud Run</p>
</li>
<li><p>See if you can find out where exactly the bottleneck is by</p>
<p>a) Go to logs explorer - <a href=""https://console.cloud.google.com/logs/"" rel=""nofollow noreferrer"">https://console.cloud.google.com/logs/</a></p>
<p>b) Find an entry that seems to have taken a long time. If you mouse over the time, a menu should popup and the first entry should be 'view trace details'. Click on it and it will give you a breakdown of the calls to internal APIs and how long each one took. This might help you figure out where your bottleneck is and if it's something you can fix</p>
<p>c) Also check how often new instances are being started (your logs will tell you if a visit kicked off a new instance). This can help you figure out if you should increase the number of min or max instances you need.</p>
</li>
</ol>
","2024-02-24 18:36:10","0","Answer"
"78053521","78052918","","<p>Two suggestions to try:</p>
<ol>
<li>Don't load the model during instance creation.  Instead load it at the first request that needs it.  This is <a href=""https://stackoverflow.com/questions/55228492/spacy-on-gae-standard-second-python-exceeds-memory-of-largest-instance"">described in more detail here</a>.</li>
<li>You might need more memory.  For my ML models, I use GAE flexible with this instance specification:</li>
</ol>
<pre><code>resources:
  cpu: 2
  memory_gb: 8.0
  disk_size_gb: 20
</code></pre>
","2024-02-24 18:27:13","0","Answer"
"78053224","","Element differences of tensor with each other along spatial axises (Pytorch)","<p>I want to get the difference of a channel to all other channels along the spatial axes (1, 2). The lengths of c_deltas and c_in should be equal.</p>
<pre><code>in_shape = [bs, y_dim, x_dim, c_in]
out_shape = [bs, y_dim, x_dim, c_in, c_deltas]
</code></pre>
<p>Here are some examples (1d and 4d) with the expected results:</p>
<pre><code># 1d example; in.size=(4), out.size=(4, 4)
in_matrix = [1, 3, 4, 7]
out_matrix = [[0, 2, 3, 6], [-2, 0, 1, 4], [-3, -1, 0, 3], [-6, -4, -1, 0]]

# 4d example; in.size=(1, 2, 2, 3), out.size=(1, 2, 2, 3, 3)
in_matrix=[[[[6,5,1], [2, 5, 3]], [[1, 4, 8], [8, 6, 4]]]]
out_matrix =  [[[[[0, -1, -5], [1, 0, -4], [5, 4, 0]],
                 [[0, 3, 1], [-3, 0, -2], [-1, 2, 0]]],
                [[[0, 3, 7], [-3, 0, 4], [-7, -4, 0]],
                 [[0, -2, -4], [2, 0, -2], [4, 2, 0]]]]]
</code></pre>
<p>I was able to find <em>some</em> similar questions; however, I was unable to cleanly generalize them for my use e.g., <a href=""https://discuss.pytorch.org/t/compute-difference-of-each-element-in-a-tensor-with-each-other/44087/2"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/compute-difference-of-each-element-in-a-tensor-with-each-other/44087/2</a></p>
<p>Is there an efficient way to implement this?</p>
","2024-02-24 17:02:59","0","Question"
"78053086","","solving an ODE using neural networks","<p>I want to solve this ODE using neural nets. du/dt + 2u + t = 0 with initial condition u(0)=1 and t is between 0 to 2.
I want to use pytorch and automatic differentiation method to solve this equation. but I don't know how can I calculate du/dt in pytorch.
I want to define loss function as below and minimize it to find optimum weights and biases of the neural net.
u_hat is an approximate solution of ODE which is substituted with neural network.
R = du_hat/dt + 2*u_hat + t.
loss function = sum(R<sub>i</sub>^2).
loss function is the sum of R<sub>i</sub> which is calculated in the points t = 0, 0.5, 1, 1.5, 2.</p>
<p>I don't know how can I write the code in pytorch.</p>
","2024-02-24 16:16:51","1","Question"
"78053013","78052337","","<p>You can multiply a matrix p×q with a matrix q×r and the result will be a matrix p×r.</p>
<p>More explicitly:</p>
<p>You can multiply a matrix (p rows, q columns) with a matrix (q rows, r columns) and the result will have (p rows, r columns).</p>
<p>Notice how the number of <em><strong>columns</strong></em> of the matrix on the left must equal the number of <em><strong>rows</strong></em> of the matrix on the right.</p>
<p><a href=""https://en.wikipedia.org/wiki/Matrix_multiplication"" rel=""nofollow noreferrer"">Wikipedia has a nice picture to visualise this.</a></p>
<p>Examples:</p>
<ul>
<li>(8×10) times (10×12) gives (8x12);</li>
<li>(8×10) times (10×10) gives (8x10);</li>
<li>(8×8) times (8×10) gives (8x10);</li>
<li>(10×8) times (8×8) gives (10x8);</li>
<li>(8×10) times (8×8) doesn't work.</li>
</ul>
<p>So there must have been an error in your reasoning if you thought you had to multiply these two matrices in that order.</p>
<p>Ask yourself these questions:</p>
<ul>
<li>What do these two matrices represent?</li>
<li>Why do you want to multiply them?</li>
<li>If you want to multiply them, how did you choose which matrix should be on the left and which on the right?</li>
<li>What would the resulting matrix represent?</li>
</ul>
<h2>Important note: matrix multiplication, function composition</h2>
<h4>Applying operations right-to-left</h4>
<p>In mathematics, function composition is always written &quot;right-to-left&quot;. If you have a vector x, and you apply function f, then you apply function g, then the result is written</p>
<p>g(f(x)) or (g∘f)(x)</p>
<p>note how g appears on the left and f on the right although f is applied first.</p>
<p>Matrices can represent linear functions, and matrix multiplication corresponds to function composition, so if matrix A represents linear function f and matrix B represents linear function g, and column vector X represents vector x, then g(f(x)) must be calculated as BAX, not ABX.</p>
","2024-02-24 15:37:52","0","Answer"
"78052918","","GAE is very slow loading a sentence transformer","<p>I'm using Google App Engine to host a website using Python and Flask.</p>
<p>I need to add text similarity functionality, using sentence_transformers.  In requirements.txt, I add a dependency to the cpu version of torch:</p>
<pre><code>torch @ https://download.pytorch.org/whl/cpu/torch-2.2.1%2Bcpu-cp311-cp311-linux_x86_64.whl 
sentence-transformers==2.4.0
</code></pre>
<p>When I add these statements to the main.py file:</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
</code></pre>
<p>the GAE instance creation time degrades from &lt; 1 sec to &gt; 20 sec.</p>
<p>Performance improves if I save the model to a directory in the project and use:</p>
<pre><code>model = SentenceTransformer('./idp_web_server/model')
</code></pre>
<p>but it is still over 15 sec.  (Removing the statement for model creation reduces instance creation time to 4 sec).  Going from an F4 instance (2.4 GHZ, with automatic scaling) to a B8 instance (4.8 MHZ, basic scaling) instance does not improve performance, so, it seems to be IO bound.  Running the app locally on my machine (2.4 GHz), the model creation takes only 1.7 sec, i.e., is 5 to 10 times faster.</p>
<p>Can this be improved?  Should I move to Google Cloud instead of GAE?</p>
","2024-02-24 15:03:22","0","Question"
"78052896","78052275","","<p>Not commenting much on SHAP below, but I have some thoughts on potential alternatives. Example code at the end.</p>
<blockquote>
<p>It takes forever to finish, since the dataset contains 950 samples, I have tried to do it with only 1 sample and it takes long enough [...] Should I try other methods?</p>
</blockquote>
<p>Since SHAP is taking so long, I think it's worth considering other techniques if you think they can provide useful information which you can iterate on more quickly.</p>
<p>One approach is to run permutation importance tests (example code at end). Start by training a 'good' reference model, and getting the model's reconstruction and reconstruction error using the original data. Then, for each <code>feature_i</code></p>
<ul>
<li>Shuffle <code>feature_i</code></li>
<li>Record the model's reconstruction and recon error, and calculate the difference with the original result.</li>
<li>You could optionally normalise the results by the sum of all changes, so that you get scores that sum to 1 across all features.</li>
</ul>
<p>This information will allow you to plot feature vs. change in recon, or feature vs. change in recon error. The first plot tells you how each feature impacts the model's output, and can be viewed as an approximation of SHAP (though I view it as a distinct and useful method in its own right). The second plot tells you how each feature impacts reconstruction accuracy. This method is relatively fast as you only need to train the model once.</p>
<p>A limitation of this method is that if features are highly correlated, permutation tests can underestimate or miss a feature's importance (SHAP doesn't). There are ways of mitigating this, such as assessing correlations in advance and removing or grouping related ones.</p>
<p>An alternative way of assessing feature importance for an autoencoder is to record the latent representation of each sample. You can run a mutual information analysis to see the strength of association between a feature and the latent space representation. Some features might explain more of the compressed representation than others, suggesting a relative importance.</p>
<p>Other techniques could look at the size of the weight learnt for each feature (perhaps in combination with a sparsity penalty), or activation sizes.</p>
<p>For any given method, consider running it on just a portion of the dataset in order to save time, or training for only a few epochs. The results will be more approximate, but may be good enough for assessing relative feature importances.</p>
<p>To minimise overfitting, you might want to run the fitting on part of the data, and then get your recons and recon errors using an unseen validation sample.</p>
<hr />
<p>The code below trains an autoencoder on petal features and runs a permutation test on the features. In this example some the features were highly correlated, and since I didn't handle that I'm not going to rely on the results below. The figures are just illustrative of what the code does.</p>
<p><a href=""https://i.sstatic.net/4HQk5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4HQk5.png"" alt=""enter image description here"" /></a></p>
<p>Imports and prepare data</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#
# Load data
#
from sklearn.datasets import load_iris
from sklearn.preprocessing import QuantileTransformer
from sklearn.model_selection import train_test_split

X, y  = load_iris(return_X_y=True, as_frame=True)
y.name = 'species'
X = pd.concat([X, y.to_frame()], axis=1)
n_features = X.shape[1]

trn_val_ix, tst_ix = train_test_split(range(len(X)), test_size=0.1, random_state=0)
trn_ix, val_ix = train_test_split(trn_val_ix, test_size=0.2, random_state=0)

X_trn, X_val, X_tst = X.iloc[trn_ix], X.iloc[val_ix], X.iloc[tst_ix]

#To numpy arrays, and scale
scaler = QuantileTransformer(output_distribution='uniform', n_quantiles=10, random_state=0).fit(X_trn.values)
X_trn_a, X_val_a, X_tst_a = [scaler.transform(data.values) for data in [X_trn, X_val, X_tst]]

# To tensors
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader

X_trn_t, X_val_t, X_tst_t = [torch.Tensor(data).float()
                             for data in [X_trn_a, X_val_a, X_tst_a]]
</code></pre>
<p>Define a simple autoencoder and a training loop:</p>
<pre class=""lang-py prettyprint-override""><code>#
#Define a simple autoencoder
#
def make_autoencoder(latent_dim_size=3, hidden_size=5):
    activation = nn.Tanh
    encoder = nn.Sequential(
        nn.Linear(n_features, n_features),
        activation(),
        nn.Linear(n_features, hidden_size),
        activation(),
        nn.Linear(hidden_size, latent_dim_size),
    )

    decoder = nn.Sequential(
        activation(),
        nn.Linear(latent_dim_size, hidden_size),
        activation(),
        nn.Linear(hidden_size, n_features),
        activation(),
        nn.Linear(n_features, n_features)
    )

    autoencoder = nn.Sequential(encoder, decoder)
    return autoencoder

print('Model size:', sum([p.numel() for p in make_autoencoder().parameters()]))

@torch.no_grad()
def eval_metric(model, loader):
    model.eval()

    cum_rmse_pct = 0
    for X_minibatch in loader:
        output = model(X_minibatch)
        rmse_pct = (output - X_minibatch).norm(dim=1) / X_minibatch.norm(dim=1) * 100
        cum_rmse_pct += rmse_pct.sum()
    
    return (cum_rmse_pct / loader.dataset.shape[0]).item()

def train(model, loader, optimiser, n_epochs=1, loss_fn=nn.functional.mse_loss):
    metrics = {'train_loss': [], 'train_metric': [], 'val_metric': []}
    
    for epoch in range(n_epochs):
        model.train()
        cum_loss = 0
        
        for minibatch, X_minibatch in enumerate(loader):
            output = model(X_minibatch)
            
            loss = loss_fn(output, X_minibatch)
            optimiser.zero_grad()
            loss.backward()
            optimiser.step()
            
            cum_loss += loss.item() * len(X_minibatch)
        
        #Record metrics
        metrics['train_loss'].append(cum_loss / loader.dataset.shape[0])
        metrics['train_metric'].append(eval_metric(autoencoder, train_loader))
        metrics['val_metric'].append(eval_metric(autoencoder, val_loader))
        
        #Print epoch average loss
        if (epoch + 1) % 20 == 0 or (epoch == n_epochs - 1):
            print(
                f'[epoch {epoch + 1:&gt;3d}][minibatch {minibatch + 1:&gt;3d}/{len(loader):&gt;3d}]',
                f'train loss {metrics[&quot;train_loss&quot;][-1]:&gt;6.3f} |',
                f'train metric {metrics[&quot;train_metric&quot;][-1]:&gt;6.2f} | '
                f'val metric {metrics[&quot;val_metric&quot;][-1]:&gt;6.2f}'
            )
    
    return metrics
</code></pre>
<p>Train the model. Calling it good at ~13% reconstruction error.</p>
<pre class=""lang-py prettyprint-override""><code>#Register optimiser and define data loaders
batch_size = 4
n_epochs = 200

torch.manual_seed(0)
autoencoder = make_autoencoder()
optimiser = optim.NAdam(autoencoder.parameters())
# optimiser = optim.SGD(autoencoder.parameters(), lr=1e-3, momentum=0.9)

train_loader = DataLoader(X_trn_t, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(X_val_t, batch_size=batch_size, num_workers=2)

history = train(autoencoder, train_loader, optimiser, n_epochs=n_epochs)

f, ax = plt.subplots(figsize=(10, 3))
ax.plot(history['train_loss'], linestyle='--', label='loss')
ax2 = ax.twinx()
ax2.plot(history['train_metric'], label='train metric')
ax2.plot(history['val_metric'], label='val metric')

ax.set_xlabel('epoch')
ax.set_ylabel('loss')
ax2.set_ylabel('rmse %')
f.legend()
</code></pre>
<p><a href=""https://i.sstatic.net/ayOWw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ayOWw.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>[epoch  20][minibatch  27/ 27] train loss  0.021 | train metric  26.33 | val metric  41.81
[epoch  40][minibatch  27/ 27] train loss  0.016 | train metric  23.17 | val metric  36.84
[epoch  60][minibatch  27/ 27] train loss  0.006 | train metric  13.34 | val metric  17.87
[epoch  80][minibatch  27/ 27] train loss  0.006 | train metric  12.97 | val metric  16.94
[epoch 100][minibatch  27/ 27] train loss  0.005 | train metric  12.60 | val metric  16.59
[epoch 120][minibatch  27/ 27] train loss  0.005 | train metric  12.32 | val metric  16.10
[epoch 140][minibatch  27/ 27] train loss  0.005 | train metric  11.80 | val metric  15.49
[epoch 160][minibatch  27/ 27] train loss  0.004 | train metric  10.89 | val metric  14.62
[epoch 180][minibatch  27/ 27] train loss  0.004 | train metric  10.52 | val metric  14.28
[epoch 200][minibatch  27/ 27] train loss  0.003 | train metric   9.63 | val metric  13.54
</code></pre>
<p>On that trained model, run permutation tests for each feature, and plot the results. Plotted are the model's drop in performance, and a bar plot of normalised results (which can be interpreted as feature importances). These results are shown at the start of this example.</p>
<p>Permutation tests:</p>
<pre class=""lang-py prettyprint-override""><code>rng = np.random.default_rng(0)

#Model's val score before permutation
unpermuted_rmse_pct = history['val_metric'][-1]

n_repeats = 50 #number of trials per feature
permutation_metrics = np.empty([n_features, n_repeats])

#Shuffle each feature in turn, and get model's score
for col_idx, col_name in enumerate(X_val.columns):
    X_val_perm = X_val_t.clone()
    
    for repeat in range(n_repeats):
        shuffled_ixs = rng.permutation(len(X_val))
        X_val_perm[:, col_idx] = X_val_t[shuffled_ixs, col_idx]
        
        val_loader = DataLoader(X_val_perm, batch_size=batch_size, shuffle=True)
        permutation_metrics[col_idx, repeat] = eval_metric(autoencoder, val_loader)

#Convert to change in score compared to unpermuted data
permutation_df = pd.DataFrame(permutation_metrics.T, columns=X_val.columns) - unpermuted_rmse_pct
</code></pre>
<p>Plotting:</p>
<pre class=""lang-py prettyprint-override""><code>#Box plot of change in score
import seaborn as sns
permutation_melt = permutation_df.melt(var_name='feature', value_name='permuted_rmse_pct')
sns.boxplot(permutation_melt, y='feature', x='permuted_rmse_pct')
ax = sns.stripplot(permutation_melt, y='feature', x='permuted_rmse_pct', marker='.', color='tab:red')

ax.set_xlabel('drop in performance')
ax.set_ylabel('permuted feature')
ax.figure.set_size_inches(8, 2.5)
plt.show()

#Bar chart of feature importances
normalised_scores = permutation_df.mean(axis=0) / permutation_df.mean(axis=0).sum() #scores 0-1
ax = sns.barplot(normalised_scores, color='tab:purple')
ax.tick_params(axis='x', rotation=45)
ax.set_xlabel('feature')
ax.set_ylabel('feature importance')
ax.figure.set_size_inches(4, 3)
</code></pre>
","2024-02-24 14:56:48","2","Answer"
"78052337","","PYTORCH - TENSORS issue - Mat1 and mat2 shapes cannot be multiplied (8x10 and 8x8)","<p>[I know this question comes a lot, but I could not find any answer matching my use case]</p>
<p>[edit: please answer this question if you know about Pytorch, I know about matrix multiplication, this is not the issue here]</p>
<p>I feed N = 10 inputs made out of 8 floats to a 8 input layer</p>
<p>but I get that error</p>
<pre><code>Mat1 and mat2 shapes cannot be multiplied (8x10 and 8x8) 
</code></pre>
<p>What am I missing ?</p>
<p>I generate random data in my dataset</p>
<pre><code>class MyDataset(Dataset):
    def __init__(self):
        self.data = []
        self.input_size = 0
        for i in range(0,10):
            label = random.randint(0, 1)
            data = [random.uniform(0.0, 1.0) for _ in range(8)]
            self.data.append((data , label))
            self.input_size = len(data ) if self.input_size &lt; len(encoded_text) else self.input_size
  
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        text, label = self.data[idx]
        return text, label
</code></pre>
<p>This is my model</p>
<pre><code>class MyModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MyModel, self).__init__()
        self.input = nn.Linear(input_size,hidden_size)
        self.hidden = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # padding is there as original dataset does not have full 8 floats inputs
        x_padded = pad_sequence(x, batch_first=True, padding_value=0).float()  
        output = self.input(x_padded) &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR
        return torch.sigmoid(output)

def train_model(model, train_loader, criterion, optimizer, num_epochs):
    for epoch in range(num_epochs):
        for inputs, labels in train_loader:
            outputs = model(inputs)
</code></pre>
<p>The initialisation part</p>
<pre><code>if __name__ == &quot;__main__&quot;:
    hidden_size = 8  # hidden size 
    output_size = 1  # binary classification 
    learning_rate = 0.001
    num_epochs = 10
    
    dataset = MyDataset()
    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)
    
    input_size = dataset.input_size
    
    model = MyModel(input_size, hidden_size, output_size)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    train_model(model, train_loader, criterion, optimizer, num_epochs)`
</code></pre>
<p>This is a really simple beginner use case, nothing fancy</p>
<p>thanks for your help</p>
<p>[edit: the x input is made out of 8 tensors of 10 values but it should be 10 tensors of 8 values ]</p>
<p><a href=""https://i.sstatic.net/l0uBG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l0uBG.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/vx54Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vx54Y.png"" alt=""enter image description here"" /></a></p>
","2024-02-24 12:08:14","2","Question"
"78052275","","Feature Importance of a Pytorch AutoEncoder","<p>I need to get from my Pytorch AutoEncoder the importance it gives to each input variable. I am working with a tabular data set, no images.</p>
<p>My AutoEncoder is as follows:</p>
<pre><code>class AE(torch.nn.Module):
    def __init__(self, input_size, hidden_layer, latent_layer):
        super().__init__()

        self.encoder = torch.nn.Sequential(
            torch.nn.Linear(input_size, hidden_layer),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_layer, latent_layer)
        )

        self.decoder = torch.nn.Sequential(
            torch.nn.Linear(latent_layer, hidden_layer),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_layer, input_size)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
</code></pre>
<p>To save unnecessary information, I simply call the following function to get my model:</p>
<pre><code>average_loss, model, train_losses, test_losses = fullAE(batch_size=128, input_size=genes_tensor.shape[1],
                                 learning_rate=0.0001, weight_decay=0,
                                 epochs=50, verbose=False, dataset=genes_tensor, betas_value=(0.9, 0.999), train_dataset=genes_tensor_train, test_dataset=genes_tensor_test)
</code></pre>
<p>Where &quot;model&quot; is a trained instance of the previous AutoEncoder:</p>
<pre><code>model = AE(input_size=input_size, hidden_layer=int(input_size * 0.75), latent_layer=int(input_size * 0.5)).to(device)
</code></pre>
<p>Well now I need to get the importance given by that model to each input variable in my original &quot;genes_tensor&quot; dataset, but I don't know how. I have researched how to do it and found a way to do it with shap software:</p>
<pre><code>e = shap.DeepExplainer(model, genes_tensor)

shap_values = e.shap_values(
    genes_tensor
)

shap.summary_plot(shap_values,genes_tensor,feature_names=features)
</code></pre>
<p>The problem with this implementation is the following: 1) I don't know if what I am actually doing is correct. 2) It takes forever to finish, since the dataset contains 950 samples, I have tried to do it with only 1 sample and it takes long enough. The result using a single sample is as follows:</p>
<p>I have seen that there are other options to obtain the importance of the input variables like Captum, but Captum only allows to know the importance in Neural Networks with a single output neuron, in my case there are many.</p>
<p>The options for AEs or VAEs that I have seen on github do not work for me since they use concrete cases, and especially images always, for example:</p>
<p><a href=""https://github.com/peterparity/PDE-VAE-pytorch"" rel=""nofollow noreferrer"">https://github.com/peterparity/PDE-VAE-pytorch</a></p>
<p><a href=""https://github.com/FengNiMa/VAE-TracIn-pytorch"" rel=""nofollow noreferrer"">https://github.com/FengNiMa/VAE-TracIn-pytorch</a></p>
<p>Is my shap implementation correct?</p>
<p>Edit:</p>
<p>I have run the shap code with only 4 samples and get the following result:</p>
<p><a href=""https://i.sstatic.net/wzcDJ.png"" rel=""nofollow noreferrer"">shap with 4 samples</a></p>
<p>I don't understand why it's not the typical shap summary_plot plot that appears everywhere.</p>
<p>I have been looking at the shap documentation, and it is because my model is multi-output by having more than one neuron at the output.</p>
","2024-02-24 11:46:59","1","Question"
"78050166","77864227","","<p>I believe you might not have used a good dataset of images with good captions and an unique token. <em>Ryan</em> looks like it might be something know to the model from before. Also one way to check its effect would be to generate two Images with same seed, one with LoRA and one without.</p>
","2024-02-23 20:59:33","0","Answer"
"78048828","","How 'torch.nn.Functional.grid_sample' module actually works in the case of 2D gray scale image?","<p>I write an example PyTorch code for wrapping a gray-scale image with a certain transformation.</p>
<pre><code>import torch
import numpy as np

# Gray Scale Image
image = torch.tensor([[[1, 2, 3, 4],
                       [5, 6, 7, 8],
                       [9, 10, 11, 12],
                       [13, 14, 15, 16]]]
                      ).unsqueeze(0).float()

# Define a simple grid with some shifts and rotations
grid_x, grid_y = torch.meshgrid(torch.arange(4), torch.arange(4))
grid_x = grid_x.float()
grid_y = grid_y.float()
new_locs = torch.stack([grid_x + 0.2 * torch.sin(grid_y), grid_y - 0.1 * torch.cos(grid_x)], dim=2).unsqueeze(0).float()

# Warp the image using grid_sample
import torch.nn.functional as F
warped_image = F.grid_sample(image, new_locs, align_corners=True, mode='nearest')
</code></pre>
<p>This code is written following the documentation of <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html"" rel=""nofollow noreferrer"">grid_sample</a> package. But I am not able to properly understand what <code>F.grid_sample</code> actually do. My first confusion is, while I was debugging the code I noticed that the values inside the <code>new_locs</code> variable contain values between [-1,1]. This makes me confused because I am not able to understand how the pixel location of the new output can be negative. In the case of standard image processing, we normally represent the top left pixel location as (0,0), thus all the other pixel locations are positive, meaning there is no negative pixel in that case. Secondly, I am also confused regarding the purpose of interpolation mode, in the above case nearest interpolation. Why do we need interpolation in <code>F.grid_sample</code>? I would really appreciate if anyone can help me understanding the <code>F.grid_sample</code> module better.</p>
","2024-02-23 16:31:36","0","Question"
"78045233","78042907","","<p>To see where the error comes from run it on cpu by setting the <code>device = torch.device(&quot;cpu&quot;)</code>. After solving the issue you can switch the device back to cuda</p>
","2024-02-23 04:48:45","0","Answer"
"78045210","78045174","","<p>Try setting a bigger <code>batch_size</code> in <code>DataLoader</code> (e.g. 16 or 32). By default, <code>batch_size</code> is going to be 1, which quite slow.</p>
","2024-02-23 04:39:58","0","Answer"
"78045174","","google colab A100 is much slower than my local Nvidia RTX 3070, why?","<p>Learning pytorch here.  Got the following code from a book.  It runs fast on my Nvidia RTX3070 (every epoch took 3.6 seconds), but on Google colab, even if I choose the run time to be A100, each epoch took ~7 seconds.</p>
<p>Any idea why?
Thanks in advance!</p>
<pre><code>import torch
from torch import nn, optim
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision import transforms

torch.manual_seed(1)
batch_size = 128 
learning_rate = 1e-2 
num_epoches = 10 

train_dataset = datasets.MNIST(
    root='./data', 
    train=True, 
    transform=transforms.ToTensor(), 
    download=True)  

test_dataset = datasets.MNIST(
    root='./data',
    train=False, 
    transform=transforms.ToTensor())

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)


class Cnn(nn.Module):
    def __init__(self, in_dim, n_class):  # 28x28x1
        super(Cnn, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_dim, 6, 3, stride=1, padding=1),  # 28 x28
            nn.ReLU(True),
            nn.MaxPool2d(2, 2),  # 14 x 14
            nn.Conv2d(6, 16, 5, stride=1, padding=0),  # 10 * 10*16
            nn.ReLU(True),
            nn.MaxPool2d(2, 2))  # 5x5x16

        self.fc = nn.Sequential(
            nn.Linear(400, n_class)) #120),  # 400 = 5 * 5 * 16
            #nn.Linear(120, 84),
            #nn.Linear(84, n_class))

    def forward(self, x):
        out = self.conv(x)
        out = out.view(out.size(0), 400)  # 400 = 5 * 5 * 16,
        out = self.fc(out)
        return out

model = Cnn(1, 10).to('cuda') 

print(model)

# loss和optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)
import time
start = time.time()

for epoch in range(num_epoches):
    print('epoch {}'.format(epoch + 1))
    print('*' * 10)
    running_loss = 0.0
    running_acc = 0.0
    for i, data in enumerate(train_loader, 1):
        img, label = data
        img = Variable(img).to(&quot;cuda&quot;)
        label = Variable(label).to(&quot;cuda&quot;)
        out = model(img)
        loss = criterion(out, label)  # loss
        running_loss += loss.item() * label.size(0) 
        _, pred = torch.max(out, 1) 
        num_correct = (pred == label).sum() 
        # accuracy = (pred == label).float().mean()
        running_acc += num_correct.item() 
        optimizer.zero_grad() 
        loss.backward()
        optimizer.step()

    print('Train Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(
        epoch + 1, running_loss / (len(train_dataset)), running_acc / (len(
            train_dataset))))
    print(f&quot;took {time.time() - start}&quot;)
    start = time.time()


torch.save(model.state_dict(), './cnn.pth')
</code></pre>
","2024-02-23 04:23:23","0","Question"
"78044370","78037880","","<p>The issue was caused by using <a href=""https://huggingface.co/microsoft/graphcodebert-base/blob/main/config.json"" rel=""nofollow noreferrer"">max_position_embeddings</a> of size <code>514</code> from <code>graphcodebert</code> config:</p>
<pre><code>max_length = llm.config.max_position_embeddings
inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding=True)
</code></pre>
<p>while in fact, the <code>512</code>, which is standard for <a href=""https://huggingface.co/google-bert/bert-base-uncased/blob/main/config.json"" rel=""nofollow noreferrer"">BERT</a> models allowed Tokenizer to produce valid outputs.</p>
<p>Few notes on debugging process:</p>
<ul>
<li>Set environment value <code>export CUDA_LAUNCH_BLOCKING=1</code></li>
<li>Increase logging by adding <code>--log-verbose</code> to the Triton IS command: <code>tritonserver --model-repository /opt/triton_models/ --log-verbose=1</code></li>
<li>Look for the <strong>first</strong> exception, which in my case was:
<pre><code>../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [523,0,0], thread: [31,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.
Traceback (most recent call last):
  File &quot;/opt/triton_models/feature_based_pwsh_classifier/1/script_embeddings.py&quot;, line 140, in compute_code_embeddings
    outputs = llm(**inputs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py&quot;, line 828, in forward
    embedding_output = self.embeddings(
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py&quot;, line 130, in forward
    position_embeddings = self.position_embeddings(position_ids)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
</li>
<li>It turns out that once <code>CUDA error</code> is thrown, the model enters undetermined state and <strong>any</strong> following CUDA Tensor operation will produce <code>CUDA error: device-side assert triggered</code>, heavily polluting the logs</li>
<li>Manually collect offending <code>text</code> and replicate failing operations in local Jupyter Notebook on a CPU, which caused:
<pre><code>IndexError: index out of range in self
</code></pre>
</li>
</ul>
<p>Helpful discussion:</p>
<ul>
<li><a href=""https://discuss.pytorch.org/t/solved-assertion-srcindex-srcselectdimsize-failed-on-gpu-for-torch-cat/1804/9"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/solved-assertion-srcindex-srcselectdimsize-failed-on-gpu-for-torch-cat/1804/9</a></li>
</ul>
","2024-02-22 22:48:31","0","Answer"
"78044065","78043284","","<p>One solution I see is to use a reduce function to distribute the values from <code>spec_x</code> at indices given by <code>pitch</code>. The <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"" rel=""nofollow noreferrer""><code>torch.scatter</code></a> function seems complex to set up but all you need to do is make sure that</p>
<ol>
<li><p>All three tensors (<code>z</code>, <code>src</code>, and <code>index</code>) have the same number of dimensions;</p>
</li>
<li><p>The indexing tensor (<code>index</code>) has values smaller than the dimension size of the output tensor (<code>z</code>) at the scattering dimension (<code>dim</code>).</p>
</li>
</ol>
<p>To accommodate for the dimension different, we can unsqueeze and expand all three tensors. The output tensor <code>z</code> intermediate shape is <code>(B,C,H,T)</code>:</p>
<pre><code>&gt;&gt;&gt; z = torch.zeros(B,C,H,T)
&gt;&gt;&gt; index = pitch[:,None,None].expand_as(z)
&gt;&gt;&gt; src = spec_x[:,None].expand_as(z)
</code></pre>
<p>The scattering operation will be applied on <code>dim=1</code> (dimension indexed by integers between <code>[0, 255[</code>). In pseudo-code, that corresponds to:</p>
<pre><code># z[b][index[b][c][h][t]][h][t] = src[b][c][h][t]
</code></pre>
<p>The first step is to scatter the values:</p>
<pre><code>&gt;&gt;&gt; o = z.scatter(dim=1, index=index, src=src) 
</code></pre>
<p>A trick to get the correct average computed is to apply the same operation but on a tensor of ones of the same shape as <code>src</code>:</p>
<pre><code>&gt;&gt;&gt; count = z.scatter(dim=1, index=index, src=torch.ones_like(src)) 
</code></pre>
<p>Then simply sum <code>o</code> and <code>count</code> over their last two dimensions and divide <code>o</code> by the counts:</p>
<pre><code>&gt;&gt;&gt; out = o.sum(dim=(-1,-2)) / count.sum(dim=(-1,-2))
</code></pre>
<p>You may notice that the output tensor is not of the desired shape, you can fix that by repeating the hidden state dimension since all values are equal row-wise:</p>
<pre><code>&gt;&gt;&gt; out[:,:,None].repeat(1,1,H)
</code></pre>
","2024-02-22 21:21:24","1","Answer"
"78043538","78033871","","<p>I was able to replicate this issue and then solved it by changing the runtime type to T4 GPU. You can do this by going to the settings in the top right corner if you are using colab. See image.
<a href=""https://i.sstatic.net/QiX26.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QiX26.png"" alt=""enter image description here"" /></a></p>
","2024-02-22 19:19:22","0","Answer"
"78043284","","Removing the for loops for mean calculation with batch with pytorch","<pre><code>B = spec_x.size(0)
H = spec_x.size(1)
T = spec_x.size(2)

# Initialize x tensor with zeros
z = torch.zeros(B, 256, H).to(pitch.device)

# Iterate over each batch element
for b in range(B):
    # Iterate over each pitch index
    for i in range(256):
        # Mask spec_x where pitch equals i
        masked_spec_x = spec_x[b].masked_select(pitch[b] == i)
        
        # Compute mean along the time dimension
        mean_spec_x = torch.mean(masked_spec_x, dim=0)
        
        # Assign the mean to the corresponding position in x
        z[b, i] = mean_spec_x
</code></pre>
<p>The above code has 2 tensors, spec_x, and pitch. pitch is B T, it's a 2D tensor and it tells us an index from 0 to 255 corresponding to the pitch of the spectrogram at each frame.</p>
<p>The goal is to build tensor z which is B, 256, H, where H is the hidden size of spec_x.</p>
<pre><code>z[b][i] = average of spec_x[b] where pitch == i
</code></pre>
<p>The above code works, but it's very slow because of the loops, I'm just not sure if there's a way to remove the loops using pytorch built ins.</p>
<p>Thanks!</p>
","2024-02-22 18:26:13","0","Question"
"78043076","78028879","","<p>I finally found the source of this issue.</p>
<p>Model is calling a function that has <code>@torch.jit.script</code> decorator:
<a href=""https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/lib/infer_pack/commons.py#L108"" rel=""nofollow noreferrer"">https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/lib/infer_pack/commons.py#L108</a></p>
<p>Removing this decorator, <code>torch.jit.trace</code> no longer add an <code>intimplicit</code> operator in the trace and CoreML tools could succesfully convert the model.</p>
","2024-02-22 17:47:53","0","Answer"
"78042907","","RuntimeError: CUDA error - Initialization error in PyTorch DataLoader worker process","<p>I am encountering a RuntimeError in my PyTorch code while using a DataLoader for training. The error occurs in the worker process, and the traceback points to a CUDA initialization error. The specific message is:</p>
<pre><code>RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py&quot;, line 308, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 51, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;&lt;ipython-input-38-b0835867e243&gt;&quot;, line 8, in __getitem__
    return self.X[index], self.Y[index]
RuntimeError: CUDA error: initialization error
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Code snippet where the error occurs
for batch, (X, Y) in enumerate(train_dataloader): &lt;---- error here 
    print(f&quot;Batch: {batch+1}&quot;)
    print(f&quot;X shape: {X.shape}&quot;)
</code></pre>
<p>I am using PyTorch for a deep learning project and is only occurring when I try to iterate over the data loader. The error seems to be related to CUDA initialization, but I'm not sure how to address it.</p>
<p>I'm running on Google Colab with:<br />
PyTorch version: 2.1.0+cu121 <br />
GPU model: Tesla T4</p>
<p>I set the device with the following bit of code:
<code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</code>
and it does grab the GPU. I'm handing the device to tensors on initialization, which happens in a test/train split function.</p>
<p>I tried setting TORCH_USE_CUDA_DSA with <code>os.environ[&quot;TORCH_USE_CUDA_DSA&quot;] = &quot;1&quot;</code>, but didn't manage to get anything working from that.
What does the error mean/why does it happen? It works fine when my torch device is set to 'cpu', so it's not a problem with the overall data ingestion/training.</p>
<p>The data class is pretty simple:</p>
<pre class=""lang-py prettyprint-override""><code>class Data(Dataset):
    def __init__(self, X, Y):
        self.X = X
        self.Y = Y
        self.len = len(X)

    def __getitem__(self, index):
        return self.X[index], self.Y[index]

    def __len__(self):
        return self.len
</code></pre>
<p>I set up the data loader with the use of the GPU in mind:</p>
<pre class=""lang-py prettyprint-override""><code>batch_size = 256

train_data = Data(X_train, Y_train)
train_dataloader = DataLoader(
  dataset=train_data, 
  batch_size=batch_size, 
  shuffle=True,
  pin_memory=True
)


# Check it's working
for batch, (X, Y) in enumerate(train_dataloader): &lt;--- error
    print(f&quot;Batch: {batch+1}&quot;)
    print(f&quot;X shape: {X.shape}&quot;)
    print(f&quot;y shape: {Y.shape}&quot;)
    print(f&quot;X type: {X.dtype}&quot;)
    print(f&quot;Y type: {Y.dtype}&quot;)
    break
</code></pre>
<p>Weirdly enough, when I remove the num_workers and pin_memory params, it works. These are valuable for improving training runtime, but maybe I'm misusing them? Not sure what the deal is there.</p>
<p>Is there a way to set TORCH_USE_CUDA_DSA?</p>
","2024-02-22 17:14:55","2","Question"
"78042556","78042365","","<blockquote>
<p>I believe the issue stems from params being a generator rather than a dictionary</p>
</blockquote>
<p>You are correct, the object you have is a generator while the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict"" rel=""nofollow noreferrer""><code>load_state_dict</code></a> method expects a <em>dict</em>. The generator contains the parameters as an <em>array</em> of parameters while the dictionary contains each layer name - parameters <em>associations</em>. Converting a generator to a dictionary is not possible because a generator has no information about which parameters correspond to which layer, ie. in general you wouldn't be able to assign each parameter to the correct layer of the target model.</p>
","2024-02-22 16:22:05","0","Answer"
"78042478","78042273","","<p>You have a typo in your code, the instance shouldn't be called when appended to the module lists. To answer your question, yes both sub-networks will share the same weights since you appended a unique instance and not two.</p>
<pre><code>shared_conv = Convolution_layers(1,1,3)
self.subnetwk_1 = nn.ModuleList([shared_conv])
self.subnetwk_2 = nn.ModuleList([shared_conv])
</code></pre>
<p>What does <em>&quot;be able to create the basic convolution block only once&quot;</em> mean? If you are looking to have the two sub-networks share the same architecture but with separate weights, then you need to initialize <strong>two</strong> layers:</p>
<pre><code>self.subnetwk_1 = nn.ModuleList([Convolution_layers(1,1,3)])
self.subnetwk_2 = nn.ModuleList([Convolution_layers(1,1,3)])
</code></pre>
<p>If you want separate sub-networks but share their arguments, you can use keyword arguments and pass a unique dictionary multiple times to the init function:</p>
<pre><code>params = dict(in=1, out=1, kernel=3)
self.subnetwk_1 = nn.ModuleList([Convolution_layers(*params)])
self.subnetwk_2 = nn.ModuleList([Convolution_layers(*params)])
</code></pre>
<p>Or depending on the complexity of your initialization, make use of a helper function, maybe that's what you meant by &quot;<code>self.basic_conv()</code>&quot;  in your code snippet:</p>
<pre><code>class Network_Model(nn.Module):
  def __init__(self):
    super(Network_Model, self).__init__()
    self.subnetwk_1 = nn.ModuleList([self.basic_conv()])
    self.subnetwk_2 = nn.ModuleList([self.basic_conv()])

  def basic_conv(self):
      return Convolution_layers(1, 1, 3)
</code></pre>
","2024-02-22 16:12:17","0","Answer"
"78042365","","Error Loading State Dictionary in PyTorch: TypeError Encountered","<p>I'm encountering an issue while trying to load the state dictionary in PyTorch. Here's a breakdown of my situation:</p>
<ol>
<li><p>I have a model named model_fg</p>
</li>
<li><p>During training, I return its parameters using <code>model_fg.parameters()</code>.</p>
</li>
<li><p>I use the Adam optimizer in training with the following line of code:</p>
<p><code>state = optim.Adam(params, lr=lr)</code>
where params is of type <code>&lt;class 'generator'&gt;</code>.</p>
</li>
<li><p>In the evaluation phase, I attempt to load the trained parameters using:</p>
<pre><code>model.load_state_dict(trained_params)
</code></pre>
</li>
</ol>
<p>But I encoutere the error:</p>
<pre><code>TypeError: Expected state_dict to be dict-like, got &lt;class 'generator'&gt;
</code></pre>
<p>I believe the issue stems from params being a generator rather than a dictionary. I'm considering converting the generator to a dictionary, but I'm unsure how to proceed.</p>
<p>How to address this issue?</p>
","2024-02-22 15:57:20","0","Question"
"78042273","","If an instance of a nn.module inheriting object is called by 2 different sequential layers, are weights shared between them?","<p>Apologies if the terminology in the title is strange or incorrect, I am trying to refer to the following scenario:</p>
<p>As a minimal example, I define a network as follows:</p>
<pre><code>class Convolution_Layers(nn.Module):
  def __init__(self, in, out, kernel):
    super(Convolution_Layers, self).__init__()
  
    self.conv2d = nn.Conv2d(in_channels=in, out_channels=out, kernel_size=kernel)

    self.conv2d_layers = nn.Sequential(
      self.conv2d,
      nn.ReLU,
    )

  forward(self,x):
    return self.conv2d_layers(x)
    

class Network_Model(nn.Module):
  def __init__(self):
    super(Network_Model, self).__init__()
    self.basic_conv = Convolution_layers(1,1,3)

    self.subnetwk_1 = nn.ModuleList().append([self.basic_conv])
    self.subnetwk_2 = nn.ModuleList().append([self.basic_conv])

  def forward(self,x1,x2):
    out1, out2 = x1, x2
    for l in self.subnetwk_1:
      out1 = l(x1)
    for l in self.subnetwk_2:
      out2 = l(x2)
    return out1,out2
</code></pre>
<p>I would like to know if this would result in the weights in subnetwork 1 and 2 being shared, since they come from the same instance of Convolution layers.</p>
<p>Ideally I would like to have the weights be separate, but be able to create the basic convolution block only once, and then re-use it elsewhere. There may be a better way of accomplishing this.</p>
","2024-02-22 15:41:48","0","Question"
"78042086","78041279","","<p>The GPU RAM consumption usually only depends on the model weights and batch size, not on the dataset size.</p>
<p>At each training iteration, the model weights are loaded and the output is computed for all the data points in a batch, it doesn't matter if the data has 1 or 1 million points. You can try with 1 data point to see if anything changes (it shouldn't).</p>
<p>Some strategies you might try to reduce memory consumption are quantization and LoRA <a href=""https://pytorch.org/blog/finetune-llms/"" rel=""nofollow noreferrer"">https://pytorch.org/blog/finetune-llms/</a>.</p>
","2024-02-22 15:17:20","0","Answer"
"78042078","78041844","","<p>There is an attribute for all <code>torch.Tensor</code>s called <code>is_nested</code>, but it's not documented sadly. It's only mentioned in the <a href=""https://pytorch.org/docs/2.2/torch.compiler_faq.html#frequently-asked-questions"" rel=""nofollow noreferrer"">FAQ</a></p>
<pre><code>&gt; nt.is_nested
True
</code></pre>
<pre><code>&gt; a.is_nested
False
</code></pre>
","2024-02-22 15:15:43","1","Answer"
"78041844","","How can I differentiate between a PyTorch Tensor and a nested tensor?","<p>Recently, PyTorch introduced the <a href=""https://pytorch.org/docs/stable/nested.html"" rel=""nofollow noreferrer"">nested tensor</a>. However, if I create a nested tensor, e.g.,</p>
<pre class=""lang-py prettyprint-override""><code>import torch

a = torch.randn(20, 128)
nt = torch.nested.nested_tensor([a, a], dtype=torch.float32)
</code></pre>
<p>and then look at its class type, it shows:</p>
<pre class=""lang-py prettyprint-override""><code>type(nt)
torch.Tensor
</code></pre>
<p>i.e., the class type is just a regular PyTorch <code>Tensor</code>. So, <code>type(nt) == torch.Tensor</code> and <code>isinstance(nt, torch.Tensor)</code> will both return <code>True</code>.</p>
<p>So, my question is, is there a way to differentiate between a regular tensor and a nested tensor?</p>
<p>One way I can think of is that the <code>size</code> method to nested tensors (currently) works differently to that for regular tensors in that it requires an argument otherwise it raises a <code>RuntimeError</code>. So, a solution might be:</p>
<pre><code>def is_nested_tensor(nt):
    if not isinstance(nt, torch.Tensor):
        return False

    try:
        # try calling size without an argument
        nt.size()
        return False
    except RuntimeError:
        return True

    return False
</code></pre>
<p>but is there something simpler that doesn't rely on something like the <code>size</code> method not changing in the future?</p>
","2024-02-22 14:41:10","1","Question"
"78041279","","High GPU RAM Usage Training Large Language Model with Small Dataset on A100","<p>I'm encountering an issue with excessive GPU RAM consumption while training a large language model on a relatively small dataset. Despite using only 200 rows of data, the training process consumes around 40 GB of RAM on an Nvidia A100 GPU, which seems disproportionately high for the dataset size.</p>
<p>Environment:</p>
<ol>
<li>Model: vilsonrodrigues/falcon-7b-instruct-sharded (a variant of a
large language model with 7 billion parameters)</li>
<li>Dataset Size: 200 rows</li>
<li>GPU: Nvidia A100</li>
<li>Framework: PyTorch &amp; Google Colab,</li>
<li>Transformers library by Hugging Face (specify version)</li>
<li>Training Configuration:</li>
<li>Batch size: 1
<ul>
<li>Gradient accumulation steps: 16</li>
<li>Mixed precision (FP16) enabled</li>
</ul>
</li>
</ol>
<p><strong>Code: <a href=""https://colab.research.google.com/drive/1TNra_fwJbQ9M3FsFB1z8sniOxLzkDJfo?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1TNra_fwJbQ9M3FsFB1z8sniOxLzkDJfo?usp=sharing</a></strong></p>
<p>Issue:
The training consumes around 40 GB of GPU RAM, which seems excessive for the small size of the dataset and the training configuration used. I've already employed strategies such as reducing the batch size, enabling mixed precision training, and using gradient accumulation to manage memory usage, but the issue persists.</p>
<pre><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 316.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 240.38 MiB is free. Process 36185 has 15.54 GiB memory in use. Of the allocated memory 15.19 GiB is allocated by PyTorch, and 43.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre>
<p>Questions:</p>
<ul>
<li>Are there any recommended strategies to further reduce GPU RAM usage for training large language models on small datasets?</li>
<li>Could there be potential misconfigurations or inefficiencies in my training setup that I might be overlooking?</li>
<li>Is there a way to optimize the utilization of the A100 GPU for such training tasks to prevent excessive memory usage?</li>
</ul>
","2024-02-22 13:22:24","-2","Question"
"78040219","78039340","","<p>A shape can have a single dimension set to <code>-1</code>, as this means that &quot;all other data is put here&quot;.</p>
<p>This is a small pytorch example for you to understand the <code>-1</code> notation in tensor shapes.</p>
<pre class=""lang-py prettyprint-override""><code>tensor = torch.random((4,4,4)) # random values of shape (4, 4, 4)
tensor.reshape((-1, 4)) # has 4 values in last dimension, all others are placed into first dimension -&gt; (16, 4)
</code></pre>
<p>Usually the batch size is the first dimension. So the interpretation of your output shape <code>(-1, 4)</code> is the following: The network does not know the input batch size in advance, so the first output dimension (=batch size) is also unknown (if you feed in batches of size 32, the output batch size will be 32, too). However, the output for each single element is known and is given with <code>(4,)</code>.</p>
","2024-02-22 10:34:07","0","Answer"
"78039340","","ONNX output shape is -1","<p>I am trying to run ONNX object detection model in C++. so, I have loaded my own model to C++ code and I have checked output dimensions, it was [-1, 4]</p>
<p>so, when I got output tensor size, the size is weird.</p>
<p>I don't know why the dimensions is weird.</p>
<pre><code>std::vector&lt;int64_t&gt; outputDims = outputTensorInfo.GetShape();
    std::cout &lt;&lt; &quot;Output Dimensions: &quot; &lt;&lt; outputDims &lt;&lt; std::endl;
</code></pre>
","2024-02-22 08:20:27","1","Question"
"78038293","78037811","","<p>There are many way to solve your problems. For the <code>cosine similariry</code> case, the output should already been in range <code>[-1,1]</code>, now you can choose to clip all the values that is smaller than <code>0</code> to be <code>0</code> (<strong>recommended</strong>), like:</p>
<pre><code>x = torch.clamp(x, 0, 1)
</code></pre>
<p>or scale them to be in range <code>[0,1]</code> (<strong>not rcommended</strong>):</p>
<pre><code>x = (x + 1)/2
</code></pre>
<p>For the case of <code>euclidian distance</code>, your approach is right. If you want the threshold &quot;harder&quot;, consider:</p>
<pre><code>x = torch.sigmoid(alpha * x) ## with alpha &gt; 1 make the result more aggresive toward 0 and 1
</code></pre>
<p>Or because <code>x</code> being the distance, which make <code>x &gt;= 0</code> in all case, you can use any exponential function to calculate similarity, for example:</p>
<pre><code>x = self.sm(out1, out2)
x = torch.exp( - alpha * x) ## alpha &gt; 0 
</code></pre>
<p>A thing to note is that you should not use <code>sigmoid</code> in the <code>cosine similarity</code> case.</p>
","2024-02-22 03:40:06","0","Answer"
"78037880","","CUDA error: device-side assert triggered on tensor.to(device='cuda')","<p>An ML Model is running under Triton Inference Server on a GPU instance group and after a certain amount of successful inferences starts throwing the exception:
<code>CUDA error: device-side assert triggered</code></p>
<p>With <code>export CUDA_LAUNCH_BLOCKING=1</code> the stacktrace points to <code>{key: val.to(device=COMPUTE_DEVICE) for key, val in inputs.items()}</code>:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/opt/triton_models/feature_based_pwsh_classifier/1/script_embeddings.py&quot;, line 129, in compute_code_embeddings
    inputs = {key: val.to(device=COMPUTE_DEVICE) for key, val in inputs.items()}
  File &quot;/opt/triton_models/feature_based_pwsh_classifier/1/script_embeddings.py&quot;, line 129, in &lt;dictcomp&gt;
    inputs = {key: val.to(device=COMPUTE_DEVICE) for key, val in inputs.items()}
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<p>Here is a simplified form of the problematic code:</p>
<pre class=""lang-py prettyprint-override""><code>max_length = llm.config.max_position_embeddings

# inputs is a dict with keys: [input_ids, attention_mask]
inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding=True)

# Move the inputs to the CUDA device
inputs = {key: val.to(device=COMPUTE_DEVICE) for key, val in inputs.items()}

with torch.no_grad():
    outputs = llm(**inputs)
</code></pre>
<p>Where:</p>
<ul>
<li><code>COMPUTE_DEVICE</code> is <code>torch.device('cuda')</code></li>
<li><code>llm</code> and <code>tokenizer</code> are loaded via <code>transformers</code> library from <a href=""https://huggingface.co/microsoft/graphcodebert-base/"" rel=""nofollow noreferrer"">Graph-CodeBERT</a></li>
<li>once the exception occurs, all following InderenceRequests yield error, and the Triton Server needs to be restarted</li>
<li>the <code>inputs</code> looks valid with: <code>dtype:torch.int64, size:(1, xxx), device:cpu, has_NAN:False, has_inf:False</code></li>
<li>GPU VRAM is usually under 20% when the exception occurs</li>
</ul>
<p>Help and recommendation are appreciated!</p>
","2024-02-22 00:59:40","0","Question"
"78037811","","Set the range of pairwise distance and cosine similarity between 0 and 1","<p>I write a BiLSTM-Siamese Network to measure the string similarities using pairwise distance and cosine similarities with the detail as follows:</p>
<pre><code>class SiameseNetwork(nn.Module):
    def __init__(self, num_layers, dropout, weight_matrix, vocabs, similarity_measure):
        super(SiameseNetwork, self).__init__()        
        self.lstm_network = BiLSTM(num_layers, weight_matrix, vocabs)
        self.fc_drop = nn.Dropout(p = dropout)
        self.similarity_measure = similarity_measure
        if self.similarity_measure == 'euclidean_distance':
            self.sm = nn.PairwiseDistance(p=2)
        else:
            self.sm = nn.functional.cosine_similarity
        
    def forward(self, input1, input2):
        output1 = self.lstm_network(input1)
        output2 = self.lstm_network(input2)
        
        out1 = self.fc_drop(output1)
        out2 = self.fc_drop(output2)
        
        x = self.sm(out1, out2)
        if self.similarity_measure == 'euclidean_distance':
            x = 1-x  # The larger the x value is, the more similar the strings are.      
        x = torch.sigmoid(x)

        return x
</code></pre>
<p>I used the torch.sigmoid to make the similarity degree between 0 and 1. However, the sigmoid makes the same string pair’s similarities, not 1. Hence, I need to know how to make the range of the similarity degree in the range 0-1 using the pairwise distance and cosine similarity. 0 if the string pairs are dissimilar and 1 if the string pairs are similar. Any help would be greatly appreciated. Thank you!</p>
","2024-02-22 00:26:07","0","Question"
"78037360","","AWS SageMaker: No Response Received from invoke_endpoint (Timeout)","<p>I am experimenting with the AWS SageMaker Model Hosting service and have been following this tutorial <a href=""https://tudorvladstefan.medium.com/deploying-pytorch-models-in-aws-sagemaker-7c48d6308df1"" rel=""nofollow noreferrer"">https://tudorvladstefan.medium.com/deploying-pytorch-models-in-aws-sagemaker-7c48d6308df1</a></p>
<p>The public repository can be found here
<a href=""https://github.com/tdrvlad/pytorch-sagemaker-deployment"" rel=""nofollow noreferrer"">https://github.com/tdrvlad/pytorch-sagemaker-deployment</a></p>
<p>I have published my model.tar.gz file onto S3 as instructed, and successfully deployed the model on SageMaker (my endpoint has the &quot;InService&quot; status shown. All these were done by running</p>
<pre><code>python sagemaker_deployment.py
</code></pre>
<p>However, when I tried to run an inference with</p>
<pre><code>python predict.py
</code></pre>
<p>I never received anything back as my response, until I received the 60 seconds timeout error</p>
<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: 
</code></pre>
<p>As a debug attempt, I went onto CloudWatch --&gt; log groups --&gt; log stream (for the target endpoint), in which shows the following logs:</p>
<pre><code>2024-02-21T22:04:38,699 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1708553078
</code></pre>
<pre><code>2024-02-21T22:04:43,698 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.178.2:41038 &quot;GET /ping HTTP/1.1&quot; 500 0
</code></pre>
<p>Anyone has idea about what these logs mean? Do they represent errors? If so, could they be the reason I didn't receive back any response from the invocation, or can there be other causes?</p>
","2024-02-21 22:09:19","1","Question"
"78037330","78035295","","<p>I see two causes for your observation:</p>
<p>(1) If you are using image augmentation, it is usually only active for training data, not for validation data. If this is the case, it is likely that your data loaders are too slow during training (as they perform additional augmentation) compared to validation loaders (skipping augmentation). This would make training on the validation set faster.</p>
<p>If possible, increase your workers so that your training loader becomes faster. Check your GPU utilization (<code>nvidia-smi</code> command). If utilization is below 90%, the loaders are too slow (or too few) for your GPU when augmentation slows the workers down.</p>
<p>(2) To state the obvious: The time needed for 1 epoch is 10 times shorter, if the validation data is only a tenth of the training data(!)</p>
","2024-02-21 22:02:38","0","Answer"
"78037293","78037249","","<p>Note, a <code>Sequential</code> layer is just a way to bundle multiple feed-forward layers into &quot;one&quot;. This means, you dont need to pass your data to each layer explicitly (in contrast what I did below). I rewrote your example without <code>Sequential</code> layers so that you see what happens underneath. Doing so makes it easy to access the layer outputs / inputs and change them according to your needs. Of course you could re-arange your <code>Sequential</code> bundles to make a split where you need to access the <code>x</code> for your &quot;function&quot;.</p>
<pre class=""lang-py prettyprint-override""><code>class CNN(Module):
    def __init__(self) -&gt; None:
        super(CNN, self).__init__()
        self.conv1 = Conv2d(in_channels=2, out_channels=6, kernel_size=5)
        self.relu1 = ReLU(inplace=True)
        self.maxpool1 = MaxPool2d(kernel_size=2, stride=2)
        self.flatten = Flatten()
        self.linear1 = Linear(256, 120)
        self.linear2 = Linear(120, 84)
        self.linear3 = Linear(84, 10)

    def forward(self, image):
        x = self.conv1(image)
        x = x * 2 - 123  # arbitrary stuff
        x = self.relu1(x)
        x = self.maxpool(x)
        x = self.flatten(x)  # shorter than your reshaping
        x = linear1(x)
        x = linear2(x)
        x = linear3(x)
        return x
</code></pre>
","2024-02-21 21:52:36","1","Answer"
"78037249","","PyTorch: how do I add an arbitrary function between layers?","<p><strong>!!! I am just starting to understand PyTorch !!!</strong></p>
<p>Assume that the model has the following architecture:</p>
<pre><code>(conv1): Conv2d(2, 6, kernel_size=(5, 5), stride=(1, 1))
(pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
(conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
(fc1): Linear(in_features=256, out_features=120, bias=True)
(fc2): Linear(in_features=120, out_features=84, bias=True)
(fc3): Linear(in_features=84, out_features=10, bias=True)
</code></pre>
<p>What should I do to add some MyFunction between conv1 and pool layers, for example?</p>
<p>Here is my current code:</p>
<pre><code>class CNN(Module):
    def __init__(self) -&gt; None:
        super(CNN, self).__init__()
        self.cnn_layer = Sequential(
            Conv2d(in_channels=2, out_channels=6, kernel_size=5),
            # MyFunction here
            ReLU(inplace=True),
            MaxPool2d(kernel_size=2, stride=2),
        )
        self.linear_layers = Sequential(
            Linear(256, 120), Linear(120, 84), Linear(84, 10)
        )

    def forward(self, image):
        image = self.cnn_layer(image)

        image = image.view(-1, 4 * 4 * 16)
        image = self.linear_layers(image)
        return image
</code></pre>
","2024-02-21 21:43:18","2","Question"
"78036165","78034992","","<p>Your <code>adjacency_matrix</code> isn't updated because it's not a <code>nn.Parameter</code>.</p>
<p>Your <code>subdiagonal_block</code> isn't updated because it's not used in your forward pass.</p>
","2024-02-21 17:57:22","2","Answer"
"78035597","78022923","","<p>Here are some tricks to speed up the process.</p>
<h2>Caching</h2>
<p>If the dataset is not changing very often, we can focus on new sentence only and store a map from sentences to tensors.</p>
<h2>DataLoading</h2>
<p>Properly identify operations that do not require the model, and put them in the DataLoader. I find the <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer"">pytorch DataLoader</a> effective to dispatch data loading operation on multiple workers, speeding up the training / inference loop.</p>
<p>Another solution is to prepare batch in advance, compute tokenization for these batches and store tokenized batch somewhere.</p>
<p>Paired with parallelization techniques, it avoid using workers to load data, worker would only process data into the model.</p>
<h2>Compilation</h2>
<p>Use <code>model.compile</code> (available in torch^2.0) to speed up the model.</p>
<h2>Parallelization</h2>
<p>If you own multiple GPU or even a single GPU, you can try to parallelize inference. You can look the processing usage of the GPU and see if you extract more juice from it. You can do as follow to parallelize on a single machine (it saves 10s on my machine on 5000 sentences, originally took 55s for the same process):</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel

import multiprocessing as mp
import numpy as np
import time


def load_model(model_package: str, device: str):
    return (
        AutoTokenizer.from_pretrained(model_package),
        AutoModel.from_pretrained(model_package, return_dict=False).to(device),
    )


def embed_sentences(model, tokenizer, sentences: list[str], device=&quot;cpu&quot;):
    inputs = tokenizer(
        sentences, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=512
    ).to(device)
    outputs = model(**inputs)
    return outputs[1].detach().cpu().numpy()


def worker_main(
    model_package: str, device: str, input_queue: mp.Queue, output_queue: mp.Queue
):
    tokenizer, model = load_model(model_package, device)
    while True:
        batch = input_queue.get()
        if batch is None:
            return
        outputs = embed_sentences(model, tokenizer, batch, device)
        output_queue.put((batch, outputs))


def storage_main(output_queue: mp.Queue, output_fp: str, total: int):
    tic = None
    pbar = tqdm(range(total))
    with open(output_fp, &quot;wb&quot;) as stream:
        while True:
            output = output_queue.get()
            if tic is None:
                tic = time.time()

            if output is None:
                tac = time.time()
                print(f&quot;elapsed time {tac - tic}&quot;)
                break

            sentences, vectors = output
            for sentence, vector in zip(sentences, vectors):
                np.save(stream, sentence)
                np.save(stream, vector)
                pbar.update(1)


def dispatch_sentences_on_workers(
    all_sentences: list[str],
    model_package: str,
    num_workers: int,
    device: str,
    batch_size: int,
    output_fp: str,
):
    input_queue = mp.Queue()
    output_queue = mp.Queue()

    # start processes
    worker_processes = [
        mp.Process(
            target=worker_main, args=(model_package, device, input_queue, output_queue)
        )
        for _ in range(num_workers)
    ]
    storage_process = mp.Process(
        target=storage_main, args=(output_queue, output_fp, len(all_sentences))
    )
    for p in worker_processes:
        p.start()
    storage_process.start()

    # put process data
    for i in range(0, len(all_sentences), batch_size):
        input_queue.put(all_sentences[i : i + batch_size])

    # stop worker processes
    for _ in worker_processes:
        input_queue.put(None)
    for p in worker_processes:
        p.join()

    # stop storage process
    output_queue.put(None)
    storage_process.join()


dispatch_sentences_on_workers(
    [your_sentences...],
    &quot;bert-base-uncased&quot;,
    2,
    &quot;cuda:0&quot;,
    1,
    &quot;test.npy&quot;,
)
</code></pre>
<p>Some other things to investigate:</p>
<ul>
<li>Kubeflow</li>
<li>Quantization, it will reduce model size, and may allow you to process bigger batches or parallelize more.</li>
</ul>
","2024-02-21 16:30:01","0","Answer"
"78035295","","PyTorch, validation step is considerably faster if I train on the validation data, why?","<p>I am training a FCN model, I have two dataloaders train_loader and val_loader. As you can see in the code below, I made the model train on the validation data. I did this to debug a problem I had where switching between the two dataloaders would case the iteration time to increase tenfold from the first loop. I obviously can't train the model on the validation data, but why does it work like this?</p>
<p>The dataset is loaded in another class as a ConcatDataset which merges several ImageFolders, and made into dataloaders with a batch_size = 32, num_workers = os.cpu_count(), persistent_workers = True, pin_memory = True</p>
<p>This is my code:</p>
<pre><code>if __name__ == &quot;__main__&quot;:
    from multiprocessing import freeze_support
    freeze_support()
    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

    model = FCN_resnet50().to(device)

    loss_fn = torch.nn.MSELoss().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)


    datasets = universal_fake_detect.Datasets(&quot;&lt;Path to data&gt;&quot;, (0.8, 0.1, 0.1))
    train_loader = datasets.training()
    val_loader = datasets.validation()

    # tb_writer = SummaryWriter()

    best_eval_loss = float(&quot;inf&quot;)

    for epoch_index in range(10):

        model.train(True)
        running_loss = 0.0
        train_loss = 0.0

        for i, (inp, lab) in enumerate(train_loader):
            print(&quot;Training iteration:&quot;, i)
            lab = lab.view(-1, 1, 1, 1).expand(-1, 1, 224, 224).float()
            inputs, labels = inp.to(device), lab.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 1000 == 9:
                print(&quot;  batch {} loss: {}&quot;.format(i + 1, running_loss/(i+1)))
                tb_x = epoch_index * len(train_loader) + i + 1

        train_loss = running_loss / len(train_loader)
        print(&quot;Training loss&quot;, train_loss)


        # Evaluation
        model.eval()
        running_loss = 0.0

        for i, (inp, lab) in enumerate(val_loader):
            print(&quot;val iteration:&quot;, i)
            lab = lab.view(-1, 1, 1, 1).expand(-1, 1, 224, 224).float()
            inputs, labels = inp.to(device), lab.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        avg_val_loss = running_loss / len(val_loader)
        print(&quot;Validation loss:&quot;, avg_val_loss)
        if avg_val_loss &lt; best_eval_loss:
            best_eval_loss = avg_val_loss
            torch.save(model.state_dict(), &quot;../../model/FCN_test_model.pth&quot;)


    print(&quot;finished training&quot;)
</code></pre>
<p>I have tried a lot of different settings on the parameters for the dataloaders, but nothing seems to help there.</p>
","2024-02-21 15:50:18","1","Question"
"78034992","","Pytorch custom network doesn't update the weights matrix","<p>I have written this simple custom network for a linear classifier on MNIST.</p>
<p>The kick is that the model operates trough a <em>global adjacency matrix</em> of the entire network to perform the calculations. The matrix is almost all zeroes, with only the bottom left block being non zero.</p>
<p>The model itself is very basic, only two layers, without any non linearity. The problem is that in the learning process the <strong>adjacency matrix does not get updated</strong>, so the model does not learn, I don't know why. I have tested my training loop on more standard architectures and all works fine (I am using SGD with cross entropy loss), so the problem must be in how I have specified the class of the network. For me it is crucial to operate trough this global adjacency matrix, and I would like to understand where the problem is, and how to make it work.</p>
<pre><code>class Simple_Direct_Network_Adjacency_Matrix_Implementation_Dim2(nn.Module):
def __init__(self, input_dim , middle_dim, output_dim):
    super().__init__()
    self.input_dim = input_dim
    _ = middle_dim #This is an hack: we want dim 2 now, so this input to the class gets ignored
    self.output_dim = output_dim
    self.total_dim = self.input_dim + self.output_dim

    self.subdiagonal_block = nn.Parameter(torch.empty(self.output_dim, self.input_dim))
    nn.init.normal_(self.subdiagonal_block , mean=0 , std=0.1)

    self.adjacency_matrix = self.make_subdiagonal_matrix().requires_grad_(requires_grad=True)


def make_subdiagonal_matrix(self):
    over_block = torch.zeros(self.input_dim, self.input_dim)
    side_block = torch.zeros(self.total_dim, self.output_dim)

    matrix = torch.cat((over_block , self.subdiagonal_block), 0)
    matrix = torch.cat((matrix, side_block), 1)

    return matrix

def forward(self, batch_of_inputs):
    # Flatten the batch of input images
    flat_inputs = batch_of_inputs.view(-1 , batch_of_inputs.size(0))

    # Append zeros to match
    flat_inputs_total = torch.cat((flat_inputs, torch.zeros(self.output_dim , flat_inputs.size(1))), dim=0)

    # Perform matrix multiplication
    y_total_final = torch.mm(self.adjacency_matrix , flat_inputs_total)

    # Extract logits
    logits = y_total_final[-self.output_dim: , :].t()

    return logits
</code></pre>
<p>Note that I have also tryed omitting the requires grad, and nothing changes, I don't know if it is necessary. Also note that the matrix of parameters, the one specified with <code>nn.Parameter()</code> also doesn't change. Note also that moving the construction of the adjacency matrix inside the forward function also seems to not solve the problem..</p>
","2024-02-21 14:55:42","0","Question"
"78033978","78033894","","<p>From <a href=""https://pytorch.org/docs/stable/type_info.html"" rel=""nofollow noreferrer"">Pytorch docs</a>:</p>
<blockquote>
<p>eps | ... |
The smallest representable number such that 1.0 + eps != 1.0.</p>
</blockquote>
<p>As in your example <code>2.8438</code> &gt; <code>2</code> * <code>1.0</code>, it has <a href=""https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation"" rel=""nofollow noreferrer"">exponent</a> greater than the exponent of <code>1.0</code>, so <code>2.8438 + eps</code> != <code>2.8438</code> doesn't hold.</p>
<p>Update. For an arbitrary number, we probably can shift <code>eps</code> by the exponent of that number as:</p>
<pre><code>import torch
import math

def anyEps(n):
    if n == 0:
        return torch.finfo(torch.bfloat16).eps
    pow2 = 2**math.floor(math.log(abs(n),2))
    return torch.finfo(torch.bfloat16).eps * pow2

test = torch.Tensor([2.8438]).to(torch.bfloat16)
delta = anyEps(test)
test - delta == test
# tensor([False])
</code></pre>
","2024-02-21 12:22:21","1","Answer"
"78033894","","pytorch bfloat16 epsilon subtraction results in the same result","<p>I am trying to sweep some parameters in a very narrow range of values and I use bfloat16 epsilong as the step size to avoid issues such <code>a-b==a</code> for very small <code>b</code>, however I chanced upon this situation:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
test = torch.Tensor([2.8438]).to(torch.bfloat16)
delta=torch.finfo(torch.bfloat16).eps
test - delta == test
Out[1]: tensor([True])
</code></pre>
<p>I thought the purpose of the epsilon is to precisely avoid this kind of issues. What am I doing wrong? How can I get the next smallest number? At the moment I am using:</p>
<pre class=""lang-py prettyprint-override""><code>if test - delta == test:
    test = test - delta*2
else:
    test = test - delta
</code></pre>
<p>And this works, but I am concerned my understanding of the epsilon is not correct..?</p>
","2024-02-21 12:11:20","2","Question"
"78033871","","While using Seq2SeqTrainingArguments function, This error is displayed: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`","<p>I am trying to run the <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb#scrollTo=IreSlFmlIrIm"" rel=""nofollow noreferrer"">Google Colab notebook</a>. Every step is well explained and easy to understand. But I have encountered a problem. while trying to run a specific part of code :</p>
<pre><code>batch_size = 16
model_name = model_checkpoint.split(&quot;/&quot;)[-1]

args = Seq2SeqTrainingArguments(
    output_dir=&quot;output_model_T5-small&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
</code></pre>
<p>I am getting the following error:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-36-791f6c1591ac&gt; in &lt;cell line: 10&gt;()
      8 model_name = model_checkpoint.split(&quot;/&quot;)[-1]
      9 
---&gt; 10 args = Seq2SeqTrainingArguments(
     11     output_dir=&quot;output_model_T5-small&quot;,
     12     evaluation_strategy=&quot;epoch&quot;,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1829         if not is_sagemaker_mp_enabled():
   1830             if not is_accelerate_available():
-&gt; 1831                 raise ImportError(
   1832                     f&quot;Using the `Trainer` with `PyTorch` requires `accelerate&gt;={ACCELERATE_MIN_VERSION}`: &quot;
   1833                     &quot;Please run `pip install transformers[torch]` or `pip install accelerate -U`&quot;

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`
</code></pre>
<p>I have tried both the given solutions, &quot;pip install transformers[torch]&quot; and &quot;pip install accelerate -U&quot; but still I got the same error.</p>
<p>Even if I find the version of my accelerate</p>
<pre><code>import accelerate
print(accelerate.__version__)
</code></pre>
<p>the output is <strong>0.27.2</strong></p>
","2024-02-21 12:07:54","0","Question"
"78033047","77947786","","<p>The problem was related to the fact that either <code>pytorch</code> or <code>pytorch-scatter</code> were installed via different channels like <code>conda-forge</code> and <code>pytorch</code>. I solved the problem by creating a fresh environment, and including <code>pytorch</code> from <code>pytorch</code> channel and <code>pytorch-scatter</code> from <code>pyg</code> channel. Everything now is working perfectly.</p>
","2024-02-21 09:54:42","0","Answer"
"78032588","78032535","","<p>You should read the <a href=""https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"" rel=""nofollow noreferrer"">basic pytorch documentation</a>. You have to change the <code>forward</code> method of the model to include the use of <code>fc1</code>.</p>
<p>When you run <code>res50.fc1 = nn.Linear(51, 43)</code>, all you're doing is assigning <code>nn.Linear</code> to the attribute <code>fc1</code>. You haven't changed the model. How do you expect the model to know what <code>fc1</code> is or when it is supposed to be used?</p>
<p>Although for your case, what you actually want to do is replace <code>fc</code> entirely. The <code>fc</code> layer predicts classes for the original task the model was trained on. It makes no sense to feed those values into a new layer predicting new classes. You are asking the model to predict your classes by taking a weighted average of the old classes.</p>
<p>You need to replace the <code>fc</code> layer entirely and use the model's 2048 size latent representation to predict your output.</p>
<p>In this case you can simply assign <code>res50.fc = nn.Linear(2048, 43)</code>. This works (as opposed to the <code>fc1</code> case) because the model's <code>forward</code> method already uses <code>fc</code>.</p>
","2024-02-21 08:43:58","2","Answer"
"78032535","","Pretrained model return old output even after adding a new layer","<p>I have a pretrained model that I have used from this repository (<a href=""https://github.com/ViTAE-Transformer/RSP"" rel=""nofollow noreferrer"">https://github.com/ViTAE-Transformer/RSP</a>). The model is ResNet50 with a final FC layer that outputs 51 classes. Since I want to solve my problem which has only 43 classes, I have added another FC layer with an output of 43. I have also frozen the weights of all layers except the last 2 FC layers (The one I added and the one that came with model) to fine tune those 2 layers only. Only issue is when I make the forward pass I end up getting 51 classes which is what the original model predicts instead of 43 (almost as if the last layer was not registed by the model). Any idea what I am doing wrong ?</p>
<pre><code># Load path to RSP Scene Recognition repository in order to load pretrained model
sys.path.append(&quot;/home/imantha/workspace/RemSens_SSL/RSP/Scene Recognition&quot;)
from models.resnet import resnet50

# Load Model and Pretrained weights
path_to_weights = &quot;pretrain_weights/rsp-aid-resnet-50-e300-ckpt.pth&quot;
res50 = resnet50(num_classes = 51)
res50_state = torch.load(path_to_weights)
res50.load_state_dict(res50_state[&quot;model&quot;])

# To Fintune
# Freeze everything !!!
for param in res50.parameters():
    param.requires_grad = False

# Unfreeze last layer as we want to finetune it too 
res50.fc.weight.requires_grad = True
res50.fc.bias.requires_grad = True

# Add last layer which will also have .requires_grad = True
res50.fc1 = nn.Linear(51, 43)

# some code to get load dataset and data loader
# ...

# Loop through data loader and make forward pass
for X, y in train_loader:
    yhat = res50(X)
    print(f&quot;yhat.shape : {yhat.shape} , y.shape : {y.shape}&quot;)
    break

&gt;&gt;&gt; yhat.shape : torch.Size([64, 51]) , y.shape : torch.Size([64, 43])
</code></pre>
<p>But if you look at all the layers in the model it looks fine,</p>
<pre><code>print(res50)

&gt;&gt;&gt;
...
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=51, bias=True)
  (fc1): Linear(in_features=51, out_features=43, bias=True)
</code></pre>
<p>However if I follow this approach it seems to work.</p>
<pre><code>new_model = nn.Sequential(
    res50,
    nn.Linear(51,43)
)

for X, y in train_loader:
    yhat = new_model(X)
    print(f&quot;yhat.shape : {yhat.shape} , y.shape : {y.shape}&quot;)
    break

&gt;&gt;&gt;
yhat.shape : torch.Size([64, 43]) , y.shape : torch.Size([64, 43])
</code></pre>
<pre><code>print(new_model)
&gt;&gt;&gt;
...
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=51, bias=True)
  )
  (1): Linear(in_features=51, out_features=43, bias=True)
)

</code></pre>
<p>Any idea why the previous approach didnt work !</p>
","2024-02-21 08:32:19","0","Question"
"78031545","78031380","","<p>I have come up with this vertorization version, which is not optimized in term of memory because of the use of .clone()</p>
<pre><code>output_ids_para = output_ids.clone()

## Equivalent to the reverse second condition of the if statement,
## that's why there is &quot;~&quot; notation below.
mask = (output_ids != 0) * (output_ids != 1) * (output_ids != 2)

## Replace with a value that does not appear in input_ids
output_ids_para[~mask] = vocab_size + 9999 

## Parallelize the comparison (first condition of the if statement + torch.where())
input_ids_expand = input_ids.unsqueeze(-1).expand(batch_size, input_len, output_len)
output_ids_expand = output_ids_para.unsqueeze(1).expand(batch_size, input_len, output_len)
indices_i, values, indices_k = torch.where(input_ids_expand == output_ids_expand) 

## Handle the duplicated values and indices
stacked, idx, counts =  torch.unique(torch.stack((indices_i, indices_k), dim = 1), dim =0, sorted=True, return_inverse=True, return_counts=True)
_, ind_sorted = torch.sort(idx, stable=True)
cum_sum = counts.cumsum(0)
cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1]))
first_indicies = ind_sorted[cum_sum]

indices_i, indices_k = indices_i[first_indicies], indices_k[first_indicies]
values = values[first_indicies]

## The newly assigned output_ids
output_ids_para[indices_i, indices_k] = vocab_size + values

## Return the value that does not satisfy the second condition of the if statement
output_ids_para[~mask] =  output_ids[~mask] 

output_ids_para
</code></pre>
","2024-02-21 04:22:37","1","Answer"
"78031380","","How to vectorize this 2 loops in Pytorch (Difficult)","<p>How to vectorize this:</p>
<pre><code>vocab_size = 20
batch_size = 2
input_len = 5
output_len = 10
input_ids = torch.randint(0, vocab_size, (batch_size, input_len))
output_ids = torch.randint(0, vocab_size, (batch_size, output_len))

print(input_ids)
print(output_ids)

tensor([[ 0,  8,  7, 12,  8],
        [14, 15,  9,  7, 10]])
tensor([[ 2, 8,  3, 15,  2, 19,  7,  1, 19,  8],
        [10,  8,  0,  7, 16,  0,  6,  2, 16, 13]])

</code></pre>
<p>Basically, the new value in output_ids gonna be batch_size + index of the k_th of that value in the input_ids because that value might appear multiple times in the input_ids and output_ids as well. So if that value appears for the second times in the output_ids, its' gonna be replace by vocab_size + the 2nd index of that value in the input_ids (Though my code above only get the first appearance). I change the value of output like an example (the 21 and 24 at the first row in the output)</p>
<p>This is what I want:</p>
<pre><code>#%%
for i in range(batch_size):
    for k, value in enumerate(output_ids[i]):
        if value in input_ids[i] and value not in [0, 1, 2]: # mean that I will ignore values 0, 1, 2
            output_ids[i][k] = vocab_size + torch.where(input_ids[i] == value)[0][0]
output_ids

tensor([[ 2, 21,  3, 15,  2, 19, 22,  1, 19, 24],
        [24,  8,  0, 23, 16,  0,  6,  2, 16, 13]])
</code></pre>
","2024-02-21 03:16:17","0","Question"
"78028924","78025895","","<p>To generate an output with the model you can simply call the model on an input <code>X</code>:</p>
<pre><code>y_pred = model(X)
</code></pre>
<p>An example implementation of your script could be:</p>
<pre><code>torch.manual_seed(42)

N = 1000  # number of samples
D = 2     # Input dimension
C = 1     # Output dimension
lr = 1e-1 # learning rate

X = torch.rand(N, D)                      # 1000 numbers of 2 dims
y = torch.sum(X, axis=-1).reshape(-1, C)  # This is summing X rows and reshaping it to 1 output dimension

print(f&quot;X.shape: {X.shape}, y.shape:{y.shape}&quot;)
print(f&quot;X[:5]: {X[:5]}&quot;)
print(f&quot;y[:5]: {y[:5]}&quot;)

model = torch.nn.Sequential(torch.nn.Linear(D, C))
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

print(&quot;\nTraining model&quot;)
for i in range(500):
    y_pred = model(X)
    loss = criterion(y_pred, y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 50 == 0:
        print(f&quot;Epoch: {i+1:&lt;5} loss: {loss.item()}&quot;)

print(&quot;\nTesting trained model on new random numbers&quot;)
for i in range(5):
    X = torch.rand(1, D)
    y_pred = model(X)
    print(f&quot;{X[:, 0].item():.2f} + {X[:, 1].item():.2f} = {X.sum().item():.2f}, predicted: {y_pred.item():.2f}&quot;)

print(f&quot;\nModel learned weights and biases\n{model.state_dict()}&quot;)
</code></pre>
<p>This is the output of the above implementation:</p>
<pre><code>X.shape: torch.Size([1000, 2]), y.shape:torch.Size([1000, 1])
X[:5]: tensor([[0.8823, 0.9150],
        [0.3829, 0.9593],
        [0.3904, 0.6009],
        [0.2566, 0.7936],
        [0.9408, 0.1332]])
y[:5]: tensor([[1.7973],
        [1.3422],
        [0.9913],
        [1.0502],
        [1.0740]])

Training model
Epoch: 1     loss: 1.1976375579833984
Epoch: 51    loss: 0.013810846023261547
Epoch: 101   loss: 1.6199066521949135e-05
Epoch: 151   loss: 2.2864621485041425e-07
Epoch: 201   loss: 1.5489435289950393e-09
Epoch: 251   loss: 1.0740366929162803e-11
Epoch: 301   loss: 5.607882254811229e-14
Epoch: 351   loss: 4.732325513424554e-18
Epoch: 401   loss: 1.3877788466974007e-20
Epoch: 451   loss: 1.3877788466974007e-20

Testing trained model on new random numbers
0.29 + 0.47 = 0.77, predicted: 0.77
0.15 + 0.45 = 0.60, predicted: 0.60
0.57 + 0.48 = 1.05, predicted: 1.05
0.31 + 0.65 = 0.96, predicted: 0.96
0.37 + 0.22 = 0.59, predicted: 0.59

Model learned weights and biases
OrderedDict([('0.weight', tensor([[1., 1.]])), ('0.bias', tensor([2.3689e-09]))])
</code></pre>
<p>Note: I changed the learning rate from <code>1e-2</code> to <code>1e-1</code> so that the model optimization converges to a low training loss.</p>
<p>Also, you can verify that the trained model has weights <code>[[1., 1.]]</code> and bias <code>[2.3689e-09]</code>, which means that the model has learned to add two numbers as <code>y_pred = x_0 * 1 + x_1 * 1 + 2.3689e-09</code>.</p>
","2024-02-20 16:26:03","1","Answer"
"78028879","","CoreML Tools: RuntimeError: PyTorch convert function for op 'intimplicit' not implemented","<p>I am trying to <code>coremltools.converters.convert</code> a traced PyTorch model and I got an error:
<code>PyTorch convert function for op 'intimplicit' not implemented</code></p>
<p>I am trying to convert a RVC model from <a href=""https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI.git"" rel=""nofollow noreferrer"">github</a>.</p>
<p>I traced the model with <code>torch.jit.trace</code> and CoreML conversion fails. So I traced down the problematic part to the WN layer :
<a href=""https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/lib/infer_pack/modules.py#L188"" rel=""nofollow noreferrer"">https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/lib/infer_pack/modules.py#L188</a></p>
<pre class=""lang-py prettyprint-override""><code>import torch
import coremltools as ct
from infer.lib.infer_pack.modules import WN

model = WN(192, 5, dilation_rate=1, n_layers=16, gin_channels=256, p_dropout=0)
model.remove_weight_norm()
model.eval()

test_x = torch.rand(1, 192, 200)
test_x_mask = torch.rand(1, 1, 200)
test_g = torch.rand(1, 256, 1)

traced_model = torch.jit.trace(model,
  (test_x, test_x_mask, test_g),
  check_trace = True)

x = ct.TensorType(name='x', shape=test_x.shape)
x_mask = ct.TensorType(name='x_mask', shape=test_x_mask.shape)
g = ct.TensorType(name='g', shape=test_g.shape)

mlmodel = ct.converters.convert(traced_model,
    inputs=[x, x_mask, g])
</code></pre>
<p>I got an error <code>RuntimeError: PyTorch convert function for op 'intimplicit' not implemented.</code></p>
<p>How could I modify the <code>WN::forward</code> so it does not generate an <code>intimplicit</code> operator so CoreML conversion could proceed?</p>
<p>Thanks</p>
<p>David</p>
","2024-02-20 16:21:06","0","Question"
"78028594","78025204","","<p>Basically the issue is that the environment has a global done state but there is a different reward key for each group.</p>
<p><code>RewardSum(reset_keys=[&quot;_reset&quot;] * len(env.group_map.keys())</code> should work</p>
","2024-02-20 15:40:12","1","Answer"
"78028189","78025970","","<p><strong>This is my initial answer before you deleted and later edited <a href=""https://stackoverflow.com/revisions/78025970/1"">your post</a></strong>. It answer how to index <code>A</code> with <code>B</code> using values from <code>C</code> <em>under a condition</em> <code>M</code>.</p>
<hr />
<p>You can achieve this by combining <a href=""https://pytorch.org/docs/stable/generated/torch.scatter.html"" rel=""nofollow noreferrer""><code>torch.scatter</code></a> and <a href=""https://pytorch.org/docs/stable/generated/torch.where.html"" rel=""nofollow noreferrer""><code>torch.where</code></a>:</p>
<p>We start with a minimal setup:</p>
<pre><code>&gt;&gt;&gt; A = torch.rand(2,8)
&gt;&gt;&gt; B = torch.randint(0, A.size(-1), size=(len(A), 5))
&gt;&gt;&gt; C = torch.rand_like(B.float())
&gt;&gt;&gt; M = torch.ones_like(A).bool()
</code></pre>
<p>Following your example <em>&quot;except index 3 and 5 for all rows&quot;</em>, we set both <code>M[:,3]</code> and <code>M[:,5] = False</code>. Then we can perform the indexing with <code>torch.scatter</code> (we can't <code>scatter_</code> directly on <code>A</code> because we do not know if a particular index is <em>valid</em> or not. So we instead do it out of place:</p>
<pre><code>&gt;&gt;&gt; O = torch.zeros_like(A).scatter_(dim=1, index=B, src=C)
</code></pre>
<p>Tensor <code>O</code> acts as a buffer <em>ie.</em> as if all rows were valid. The above line is equivalent to <code>O[j, B[b,j]] = C</code>, in pseudo-code. Then you can combine this with the initial tensor <code>A</code> based on the mask <code>M</code> using <a href=""https://v"" rel=""nofollow noreferrer""><code>torch.where</code></a>:</p>
<pre><code>&gt;&gt;&gt; O.where(M, A) # equivalent to torch.where(M, O, A)
</code></pre>
","2024-02-20 14:35:41","1","Answer"
"78026471","78026404","","<p><code>timm.list_models()</code> returns a complete list of available models in timm:</p>
<pre><code>import timm

timm.list_models()
</code></pre>
<p>More info about how you can list models can be found <a href=""https://timm.fast.ai/models#List-of-models-supported-by-timm"" rel=""nofollow noreferrer"">here</a>. Similar resources can be found on the <a href=""https://huggingface.co/docs/timm/quickstart#list-models-with-pretrained-weights"" rel=""nofollow noreferrer"">HuggingFace website</a>.</p>
<p>Specifically, the <code>transformers.TimBackbone</code> class checks if the string you pass is in <code>timm.list_models()</code>. Check the related code <a href=""https://github.com/huggingface/transformers/blob/ff76e7c2126ab26e5722f54640d44cab7e3dfdd4/src/transformers/models/timm_backbone/modeling_timm_backbone.py#L53"" rel=""nofollow noreferrer"">here</a>.</p>
","2024-02-20 10:16:45","1","Answer"
"78026404","","list of supported backbones models","<p>I'm running the following example:</p>
<pre><code>from transformers import TimmBackboneConfig, TimmBackbone

# Initializing a timm backbone
configuration = TimmBackboneConfig(&quot;resnet50&quot;)

# Initializing a model from the configuration
model = TimmBackbone(configuration)

# Accessing the model configuration
configuration = model.config
</code></pre>
<p>from <a href=""https://huggingface.co/docs/transformers/main/en/main_classes/backbones"" rel=""nofollow noreferrer"">here</a></p>
<p>Now I want to use other backbone model from the supported list in the same page,</p>
<ul>
<li>BEiT</li>
<li>BiT</li>
<li>ConvNet</li>
<li>ConvNextV2</li>
<li>DiNAT</li>
<li>DINOV2</li>
<li>FocalNet</li>
<li>MaskFormer</li>
<li>NAT</li>
<li>ResNet</li>
<li>Swin Transformer</li>
<li>Swin Transformer v2</li>
<li>ViTDet</li>
</ul>
<p>where can I find the dictionary from this list, I mean what is the string that I should use for each of the model?</p>
<p>For example:</p>
<ul>
<li>&quot;ResNet&quot; -&gt; 'resnet50'</li>
<li>&quot;BEiT&quot; -&gt; ???</li>
</ul>
<p>I have tried to search for it in the some of the links in the page and in google but didn't find this information anywhere.</p>
","2024-02-20 10:07:59","0","Question"
"78025970","","How to asign a tensor of value to another tensor of value by a tensor of indices (Pytorch)","<p>Given:</p>
<ul>
<li>a tensor A of zeroes has shape: (batch_size, vocab_size), lets say (16, 10000)</li>
<li>a tensor B is tensor of indices has shape: (batch_size, seq_len), lets say (16, 20)</li>
<li>a tensor C is tensor of value has shape: (batch_size, seq_len), -&gt; (16, 20)</li>
</ul>
<ol>
<li><p>Now I want to replace values in A by C at indices B. Like: A[B] = C</p>
</li>
<li><p>I want to replace values in A by C at indices B except some indices. For example. B = [[0, 1, 2, 3], [0, 1, 2, 4]] Replace the values of A by C at indices B except index 3 and 5 (apply for all rows) -&gt; Now B can't expressed by a tensor because it doesn't have equal dim after filter. Something like this: A[B[valid_indices]] = C[valid_indices]</p>
</li>
</ol>
<p>I tried use for loop but it cost 2 inner loop and take too long.</p>
<pre><code>for i,row in enumerate(probs): 
            valid_indices = torch.tensor([idx[0] for idx in enumerate(encoder_input_ids[i]) if idx[1] not in [vocab['&lt;pad&gt;'],vocab['&lt;unk&gt;'], vocab['&lt;/s&gt;']]])
            valid_ids = torch.tensor([idx[0] for idx in enumerate(encoder_input_ids[i]) if idx[1] not in [vocab['&lt;pad&gt;'],vocab['&lt;unk&gt;'], vocab['&lt;/s&gt;']]])
            # print(valid_ids)
            # value = probs_c[i][valid_indices]
            # probs[i][tmp] = value #probs_c[i]
</code></pre>
","2024-02-20 09:07:19","0","Question"
"78025895","","MLP to learn addition","<p>I want to train a neural net to add two numbers. Using some examples, I've got to here and have achieved a low loss. I want to generate new numbers and pass them to the model to see what the outcomes are, and start to give it two numbers to see what it predicts. Here's what I have:</p>
<pre><code>torch.manual_seed(42)

N = 1000  # number of samples
D = 2     # Input dimension
C = 1     # Output dimension
lr = 1e-2 # learning rate

X = torch.rand(N, D)                      # 1000 numbers of 2 dims
y = torch.sum(X, axis=-1).reshape(-1, C)  # This is summing X rows and reshaping it to 1 output dimension

# print(X\[:50\])

# print(y\[:50\])

model = torch.nn.Sequential(torch.nn.Linear(D, C))

criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

for i in range(500):
    y_pred = model(X)
    loss = criterion(y_pred, y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 50 == 0:
        print(i)
        print('---')
        print(loss)

idx = torch.tensor(\[3, 4\])
for i in range(20):
    n = model.generate(idx, 50)
    print(n)
</code></pre>
<p>I am not able to generate from this model and am not sure how.</p>
","2024-02-20 08:52:38","0","Question"
"78025631","77962011","","<p>All opensource models are loaded into cpu memory by default. You need to manually call <code>pipe = pipe.to(&quot;cuda:0)</code> or <code>pipe = pipe.cuda()</code> to run it on your GPU. You also need to transfer all your other tensors which take part in the calculation to the same GPU device if you want to finetune a model.</p>
","2024-02-20 08:03:31","1","Answer"
"78025204","","Define rewardsum instance for pettingzooenv in torchrl","<p>I'd like to define <code>rewardsum()</code> instance for pettingzoo env wrapper in torchrl.</p>
<p>here is the definition of my env:</p>
<pre><code>from torchrl.envs.libs.pettingzoo import PettingZooEnv
from torchrl.envs.utils import MarlGroupMapType

env = PettingZooEnv(
    task=&quot;mpe/simple_spread_v3&quot;,
    parallel=False,
    use_mask=True, # Must use it since one player plays at a time
    group_map=None # # Use default for AEC (one group per player)
)
</code></pre>
<p>what I am trying to do:</p>
<pre><code>
env = TransformedEnv(
    env,
    RewardSum(),
    
)

check_env_specs(env)
</code></pre>
<p>here is the error i get (split into multiple lines):</p>
<pre><code>ValueError: Could not match the env reset_keys ['_reset'] with 
            the in_keys [('agent_0', 'reward'), ('agent_1', 'reward'),
            ('agent_2', 'reward')].
            Please make sure that these have the same length.
</code></pre>
<p>I am expecting the sum of the rewards of all agents in my env to be accessible via <code>[&quot;next&quot;, &quot;episode_reward&quot;]</code> which are default access keys defined by torchrl.</p>
","2024-02-20 06:36:52","0","Question"
"78024930","78024121","","<p><strong>Solution - find and uninstall non-CUDA torch installations!</strong></p>
<p>In my case, the issue was caused by another system-wide installation of torch. Before I started using Anaconda, I was under the silly impression that I can get away with using a vanilla Python. Whatmore, I was using a mixture of system-wide and user-installed packages - a nice recipe for disaster.</p>
<p><em>@talonmies offered a great tip: printing out the output of <code>torch.cuda.get_arch_list()</code>. In my case, it was an empty list <code>[]</code>, indicating a wrong torch library was loaded.</em></p>
<p>I ran pip list using regular Python outside of the Anaconda environment and lo and behold, I had the non-cuda torch installed! I have used pip to uninstall everything that had to do with torch/pytorch (and noticed some weird ~orch leftovers that I forcefully purged) - and then all was good.</p>
<p>Lesson learned - when doing complex things with Python (e.g. ML) always start with a virtual environment (Anaconda, Miniconda, VirtualEnv, what have you) or you will regret it later.</p>
","2024-02-20 05:20:55","3","Answer"
"78024782","78024207","","<p>tl;dr use both, they do different things</p>
<p>Preprocessing normalization and batchnorm are both doing normalization operations, but they serve different functions.</p>
<p>Preprocessing normalization is done because giving the model &quot;nice and reasonable&quot; numbers has better numerical stability. It generally works well to normalize inputs to have mean 0, variance 1. The normalization transform does this for your inputs with the per-channel mean and variance values.</p>
<p>Note that if you want to use a pretrained model, you need to use the same normalization parameters as the training data for that model.</p>
<p>Batchnorm is a normalization applied per layer in the model. Batchnorm tracks the mean and variance of activations and uses those values to normalize them. Batchnorm typically also has learnable parameters to shift and scale the activations (ie learning the right mean and variance).</p>
<p>Strictly speaking batchnorm is optional, but it tends to improve the model.</p>
","2024-02-20 04:20:58","0","Answer"
"78024207","","Data Normalisation in transformation then Batch Normalisation in ResNet50 pytorch","<p>I have a question regarding normalization.My current approach for training using pytorch ResNet50 on my image dataset is as follows:</p>
<p><strong>First step:</strong> I calculate the mean and standard deviation of my entire dataset,then I use the following code for normalization of my images in the ImageFolder of pytorch:-</p>
<pre><code>  data_transform = transforms.Compose([
    transforms.Resize(input_size),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.1,0.1,0.1),
    transforms.ToTensor(),
    transforms.Normalize([0.554, 0.450, 0.343],[0.231, 0.241, 0.241]),
  ]) 
data = datasets.ImageFolder(root=data_directory, transform=data_transform)
</code></pre>
<p><strong>second step:</strong> I use ResNet50 in Pytorch for training which does batch normalization in its architecture, as shown in the below figure:-</p>
<p><a href=""https://i.sstatic.net/PlOGz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PlOGz.png"" alt=""enter image description here"" /></a></p>
<p>My questions are:</p>
<p>Q1) Do I need both normalizations or one of these is normalization is more than enough?</p>
<p>Q2) what will be the reason for choosing that normalization approach??</p>
","2024-02-20 00:34:29","0","Question"
"78024121","","Cannot convince Pytorch to install with Cuda (Windows 11)","<p>I am trying to install PyTorch with Cuda using Anaconda3, on Windows 11:</p>
<ul>
<li>My GPU is RTX <strong>3060</strong>.</li>
<li>My conda environment is Python <strong>3.10.13</strong>.</li>
<li>nvidia-smi outputs Driver Version: <strong>551.23</strong>, CUDA Version: <strong>12.4</strong>.</li>
</ul>
<p>What I tried:</p>
<ul>
<li>Following the instructions on <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a>, picking CUDA 12.1 (and 11.8)</li>
<li>Reading through a bunch of posts on <a href=""https://discuss.pytorch.org/"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/</a> (including <a href=""https://discuss.pytorch.org/t/torch-cuda-is-available-gives-false/197333"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/torch-cuda-is-available-gives-false/197333</a>)</li>
<li>Reading through a bunch of posts on SO (e.g. <a href=""https://stackoverflow.com/questions/73250147/pytorch-cuda-is-not-available"">PyTorch: CUDA is not available</a>)</li>
</ul>
<p>Many articles say that I don't need to have a independently installed CUDA so I uninstalled the system-wide version - but it predictably didn't make a difference. No matter what I try, <code>torch.cuda.is_available()</code> is always false.</p>
<p>I think my conclusion is that the nVidia driver must match the version of CUDA, but for me it's counterintuitive. I've always trained myself to install the latest and greatest nVidia driver (I use my machine for gaming as much as I want to use it for ML) - is my understanding correct, and the only way to get PyTorch to work with CUDA is to <em>downgrade</em> my GPU driver from 551.23 to something that supports CUDA 12.1?</p>
","2024-02-19 23:56:12","1","Question"
"78023090","78016973","","<p>Just loop over the parameters and compare them with torch.<a href=""https://pytorch.org/docs/stable/generated/torch.allclose.html"" rel=""nofollow noreferrer"">allclose</a>. I used DistilBertModel for the answer, please use the respective classes from your example that are also mentioned in the comments:</p>
<pre><code>import torch
from transformers import  DistilBertModel

# AutoModelForCausalLM in your case
base_model =  DistilBertModel.from_pretrained(&quot;distilbert/distilbert-base-uncased&quot;)
# PeftModel.merge_and_unload() in your case
finetuned_model = DistilBertModel.from_pretrained(&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;)

for base_param, finetuned_param in zip(base_model.named_parameters(), finetuned_model.named_parameters()):
  if not torch.allclose(base_param[1], finetuned_param[1]):
    print(base_param[0])
</code></pre>
<p>Output:</p>
<pre><code>embeddings.word_embeddings.weight
embeddings.position_embeddings.weight
embeddings.LayerNorm.weight
embeddings.LayerNorm.bias
transformer.layer.0.attention.q_lin.weight
transformer.layer.0.attention.q_lin.bias
transformer.layer.0.attention.k_lin.weight
transformer.layer.0.attention.k_lin.bias
transformer.layer.0.attention.v_lin.weight
transformer.layer.0.attention.v_lin.bias
transformer.layer.0.attention.out_lin.weight
transformer.layer.0.attention.out_lin.bias
transformer.layer.0.sa_layer_norm.weight
transformer.layer.0.sa_layer_norm.bias
transformer.layer.0.ffn.lin1.weight
transformer.layer.0.ffn.lin1.bias
transformer.layer.0.ffn.lin2.weight
transformer.layer.0.ffn.lin2.bias
transformer.layer.0.output_layer_norm.weight
transformer.layer.0.output_layer_norm.bias
...
transformer.layer.5.attention.q_lin.weight
transformer.layer.5.attention.q_lin.bias
transformer.layer.5.attention.k_lin.weight
transformer.layer.5.attention.k_lin.bias
transformer.layer.5.attention.v_lin.weight
transformer.layer.5.attention.v_lin.bias
transformer.layer.5.attention.out_lin.weight
transformer.layer.5.attention.out_lin.bias
transformer.layer.5.sa_layer_norm.weight
transformer.layer.5.sa_layer_norm.bias
transformer.layer.5.ffn.lin1.weight
transformer.layer.5.ffn.lin1.bias
transformer.layer.5.ffn.lin2.weight
transformer.layer.5.ffn.lin2.bias
transformer.layer.5.output_layer_norm.weight
transformer.layer.5.output_layer_norm.bias
</code></pre>
","2024-02-19 19:27:21","0","Answer"
"78022923","","Reducing Runtime of BERT Embedding Extraction in PyTorch","<p>I'm using BERT from Hugging Face's Transformers library in PyTorch to extract embeddings for text data, aiming to integrate these embeddings into a machine learning pipeline. Despite the success in capturing rich textual features, the extraction process significantly slows down my pipeline, especially when processing large datasets. I'm looking for targeted advice to optimize this specific phase of my workflow.</p>
<p>Below is the minimal code snippet representing my current approach:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)

def extract_embeddings(text):
    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, max_length=512)
    outputs = model(**inputs)
    embeddings = outputs[0][:,0,:].detach()
    return embeddings

# Example usage
text = &quot;Here is some example text to encode&quot;
embeddings = extract_embeddings(text)
</code></pre>
<p>Given this context, my question is specific:</p>
<ol>
<li><strong>How can I optimize the runtime of the BERT embedding extraction process in PyTorch for large datasets?</strong></li>
</ol>
<p>I'm particularly interested in any PyTorch-specific techniques or practices that can help speed up this operation, such as adjustments to batch size, use of PyTorch DataLoader for efficient batching, or model inference optimizations that do not compromise the quality of the embeddings.</p>
","2024-02-19 18:50:10","0","Question"
"78022307","78020235","","<p>You're misinterpreting the post. It doesn't say that looping inference increases GPU utilization - it posits looping inference as a test to see if the bottleneck is loading data. It's a test, not a solution.</p>
<p>Looping inference loads data once, then runs inference multiple times, allowing you to see the GPU performance without i/o overhead. If GPU performance improves when looping inference, it indicates an i/o bottleneck.</p>
","2024-02-19 16:54:50","2","Answer"
"78022287","78018487","","<p>Autocast doesn't transform the weights of the model, so weight grads will have the same dtype as the weights. You can try manually calling <code>.half()</code> on the model to change this. I'm not sure if there's a way to compute grads in fp16 while keeping the weights in fp32.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

torch.set_default_device('cuda')

model = nn.Linear(8,1)
opt = torch.optim.SGD(model.parameters(), lr=1e-3)

x = torch.randn(12, 8, dtype=torch.float16)

with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):
    output = model(x)
    loss = output.mean()
loss.backward()
    
print(model.weight.grad.dtype)
# &gt; torch.float32

opt.zero_grad()

model.half()

with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):
    output = model(x)
    loss = output.mean()
loss.backward()
    
print(model.weight.grad.dtype)
# &gt; torch.float16
</code></pre>
<p>Additionally, some ops have numerical stability issues when computed in fp16. To avoid this, pytorch autocasts certain opts to fp32. You can find the full list <a href=""https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float32"" rel=""nofollow noreferrer"">here</a>.</p>
<p>In your case, MSE loss (and really the <code>pow</code> function) autocast to fp32. This won't change the weight grad dtype in the example above, but worth noting if you see fp32 cropping up other places.</p>
","2024-02-19 16:51:37","2","Answer"
"78022020","78021309","","<p>I assume that your input shape is <code>(height, width, n_channels)</code> and the expected output shape is <code>(patches_n_rows, patches_n_cols, patch_size, patch_size, n_channels)</code>.</p>
<ol>
<li><p>To make patches overlap, the step should be the size of the patch minus the size of the overlap.</p>
</li>
<li><p>Then we calculate the width that needs to be padded. If not padded, the excess width of the image is <code>(width - overlap) % step</code>, where <code>overlap</code> is the size of the overlap. If the excess width is not 0, the width to be padded is <code>step - excess_width</code>. The calculation method for the height to be padded is the same.</p>
</li>
<li><p>Next, according to the documentation, the parameter <code>pad</code> for <code>F.pad</code> should be <code>(last dim left pad, last dim right pad, 2nd to last dim left pad, 2nd to last dim right pad, ...)</code>. Here, it should be <code>(0, 0, 0, padding_width, 0, padding_height)</code>.</p>
</li>
<li><p>After padding the image and <code>unfold</code> the first two axes, we will get the shape <code>(patches_n_rows, patches_n_cols, n_channels, patch_size, patch_size)</code> (<code>unfold</code> always puts new dimension at the last) and need to apply <code>moveaxis</code> once finally.</p>
</li>
</ol>
<p>In summary, the code is as follows:</p>
<pre><code>def create_patches(image, patch_size=1024, overlap=100):
    height, width, _ = image.shape
    step = patch_size - overlap
    padding_width = (step - (width - overlap) % step) % step
    padding_height = (step - (height - overlap) % step) % step
    image = F.pad(image, (0, 0, 0, padding_width, 0, padding_height))
    return (
        image
        .unfold(0, patch_size, step)
        .unfold(1, patch_size, step)
        .moveaxis(2, -1)
    )
</code></pre>
<p>Test:</p>
<pre><code>&gt;&gt;&gt; image = torch.rand(2160, 3840, 4)
&gt;&gt;&gt; patches = create_patches(image)
&gt;&gt;&gt; patches.shape
torch.Size([3, 5, 1024, 1024, 4])
&gt;&gt;&gt; 
&gt;&gt;&gt; from itertools import pairwise
&gt;&gt;&gt; # overlap detection for each row
&gt;&gt;&gt; all(
...     torch.equal(left[:, -100:], right[:, :100])
...     for i in range(patches.size(0))
...     for left, right in pairwise(patches[i])
... )
True
&gt;&gt;&gt; # overlap detection for each column
&gt;&gt;&gt; all(
...     torch.equal(up[-100:], down[:100])
...     for j in range(patches.size(1))
...     for up, down in pairwise(patches[:, j])
... )
True
</code></pre>
","2024-02-19 16:05:48","2","Answer"
"78021660","78021659","","<p>The solution that work for me is from <a href=""https://huggingface.co/LiheYoung/depth-anything-large-hf/discussions/1"" rel=""nofollow noreferrer"">here</a></p>
<p>Just install transformers from source by:</p>
<pre><code>pip install -q git+https://github.com/huggingface/transformers.git
</code></pre>
<p>instead (Quick tour recommendation):</p>
<pre><code>!pip install transformers datasets
</code></pre>
<p>additionally you may need to install PIL by:</p>
<pre><code>pip install pillow
</code></pre>
","2024-02-19 15:10:09","0","Answer"
"78021659","","Running DepthEstimationPipeline example from huggingface.co","<p>getting the following error when running</p>
<pre><code>depth_estimator = pipeline(task=&quot;depth-estimation&quot;, model=&quot;LiheYoung/depth-anything-base-hf&quot;)
</code></pre>
<blockquote>
<p>ValueError: The checkpoint you are trying to load has model type
<code>depth_anything</code> but Transformers does not recognize this
architecture. This could be because of an issue with the checkpoint,
or because your version of Transformers is out of date.</p>
</blockquote>
","2024-02-19 15:10:09","0","Question"
"78021371","","Pytorch advanced indexing with list of lists as indices","<p>Here is some python code to reproduce my issue:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

n, m = 9, 4

x = torch.arange(0, n * m).reshape(n, m)
print(x.shape)
print(x)
# torch.Size([9, 4])
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11],
#         [12, 13, 14, 15],
#         [16, 17, 18, 19],
#         [20, 21, 22, 23],
#         [24, 25, 26, 27],
#         [28, 29, 30, 31],
#         [32, 33, 34, 35]])

list_of_indices = [
    [],
    [2, 3],
    [1],
    [],
    [],
    [],
    [0, 1, 2, 3],
    [],
    [0, 3],
]
print(list_of_indices)

for i, indices in enumerate(list_of_indices):
    x[i, indices] = -1

print(x)
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5, -1, -1],
#         [ 8, -1, 10, 11],
#         [12, 13, 14, 15],
#         [16, 17, 18, 19],
#         [20, 21, 22, 23],
#         [-1, -1, -1, -1],
#         [28, 29, 30, 31],
#         [-1, 33, 34, -1]])
</code></pre>
<p>I have a list of list of indices. I want to set the indices in <code>x</code> to a specific value (here <code>-1</code>) using the indices in <code>list_of_indices</code>. In this list, each sublist correspond to a row of <code>x</code>, containing the indices to set to <code>-1</code> for this row. This can be easily done using a for-loop, but I feel like pytorch would allow to do that much more efficiently.</p>
<p>I tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>x[torch.arange(len(list_of_indices)), list_of_indices] = -1
</code></pre>
<p>but it resulted in</p>
<pre class=""lang-py prettyprint-override""><code>IndexError: shape mismatch: indexing tensors could not be broadcast together with shapes [9], [9, 0]
</code></pre>
<p>I tried to find people having the same problem, but the number of questions about indexing tensors is so large that I might have missed it.</p>
","2024-02-19 14:22:54","1","Question"
"78021309","","Create images patches with overlap using torch.unfold","<p>I would like to split an images into smaller images of 1024x1024. I would like to have an overlap of 100 pixels in the top/bottom and left/right of the patches.</p>
<p><img src=""https://i.sstatic.net/tDc9P.png"" alt=""enter image description here"" /></p>
<p>I am using padding and torch.unfold to create even sized patches.</p>
<pre><code>def create_patches(image, patch_size):
    # Pad right and bottom to fit patch_size
    padding_right = patch_size - image.shape[2]%patch_size
    padding_bottom = patch_size - image.shape[1]%patch_size
    image = F.pad(image, (0, 0, padding_right, padding_bottom))
    return image.unfold(0, 3, 3).unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)
</code></pre>
<p>How can I make the patches to overlap?</p>
","2024-02-19 14:14:46","1","Question"
"78021259","78017563","","<p>The <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html?highlight=pil_to_tensor#torchvision.transforms.functional.pil_to_tensor"" rel=""nofollow noreferrer""><code>PILToTensor</code></a> operator doesn't cast the tensor and keeps the same dtype. You may instead use <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.functional.to_tensor.html?highlight=to_tensor#torchvision.transforms.functional.to_tensor"" rel=""nofollow noreferrer""><code>ToTensor</code></a> instead which is generally preferred:</p>
<pre><code>image_transformations = Compose([
    v2.Resize((224, 224)),
    v2.ToTensor(),
    v2.Normalize(mean=[0.485,  0.456,  0.406], std=[0.229,  0.224,  0.225]),
])
</code></pre>
","2024-02-19 14:06:25","1","Answer"
"78020235","","understanding looping real inference call","<p>I was looking for a solution for an issue I was having with my interface speed. I saw this <a href=""https://forums.developer.nvidia.com/t/inference-time-hugging-face-detr/274721"" rel=""nofollow noreferrer"">answer</a> online but I don't understand what the solution was. The person is using a hugging face model with pytorch and the solution was to loop the real inference call to increase the GPU utilization. This was caused by a bottleneck from accessing the data. Can I get some help understanding how to implement this solution?
Thanks</p>
","2024-02-19 11:15:23","0","Question"
"78019217","78015382","","<p>You can reduce the inner loops and conditions as follows:</p>
<pre><code>import torch

dilation = 3
nbd_size = 5
knn_key = torch.randint(0, 30, (64, 12, 198, 100))

dilated_keys = torch.zeros((knn_key.shape[0], knn_key.shape[1], knn_key.shape[2], nbd_size), dtype=torch.int64)

for i in range(knn_key.shape[0]):
    for j in range(knn_key.shape[1]):
        for k in range(knn_key.shape[2]):
            key = knn_key[i, j, k]
            indices = torch.nonzero(key % dilation == k % dilation).squeeze()
            selected_indices = indices[:nbd_size]
            dilated_keys[i, j, k] = key[selected_indices]
</code></pre>
","2024-02-19 08:23:24","1","Answer"
"78018487","","How to save memory using half precision while keeping the original weights in single?","<p>I'm trying to save memory while training a model that uses single precision weights by doing the calculations in half precision.</p>
<p>I tried using autocast, and the model does prediction in half precision as it should.
However the gradient produced is still in single precision.
This ruins both performance and memory savings.
Is there any way to instruct torch to calculate grads in half precision and use those to update the original single precision weights?</p>
<pre><code>import torch

class KekNet (torch.nn.Module):
    def __init__(self):
        super(KekNet, self).__init__()
        self.layer1 = torch.nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dtype=torch.float32)
    
    def forward(self, x, features=False):
        return self.layer1(x)

device = torch.device(&quot;cuda&quot;)


# HALF-DATA AUTOCAST

net = KekNet().to(device)

loss_l2  = torch.nn.MSELoss(reduction='none')
g_params = [{'params': net.parameters(), 'weight_decay': 0}]
optimizerG = torch.optim.RMSprop(g_params, lr=3e-5, alpha=0.99, eps=1e-07, weight_decay=0)
schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max=300)

X = torch.randn((40,3,555,555), dtype=torch.float16, device =device)

with torch.autocast(device_type='cuda', dtype=torch.float16):
    Y_h=net(X)

Y = torch.randn_like(Y_h)
loss = loss_l2(Y_h, Y).mean()

loss.backward()

print(f&quot;-autocast\r\ndata precision: {X.dtype}\r\npred precision: {Y_h.dtype}\r\ngrad precision: {net.layer1.weight.grad.dtype}\r\n&quot;)

optimizerG.step()
schedulerG.step()
</code></pre>
<p>results in following:</p>
<pre><code>data precision: torch.float16
pred precision: torch.float16
grad precision: torch.float32
</code></pre>
","2024-02-19 05:26:05","-1","Question"
"78018036","78005416","","<p>I was running the code on reboot using crontab from the sudo user. Somehow my Python from sudo user could not find Pytorch. After switching from <code>sudo crontab -e</code> to only <code>crontab -e</code> I solved the issue for Pytorch.</p>
","2024-02-19 01:48:06","0","Answer"
"78017563","","Using make_grid to preview a dataset is generating exceptions","<p>I am trying to display images using <code>make_grid()</code> and cannot seem to get it to work.</p>
<p>Here is my code:</p>
<pre><code>class CustomDS(torch.utils.data.Dataset):
    def __init__(self, pil_objs, transform=None):
        self.pil_objs = pil_objs
        self.transform = transform

    def __len__(self):
        return len(self.pil_objs)

    def __getitem__(self, idx):
        pil_img = self.pil_objs[idx]
        if self.transform:
            pil_img = self.transform(pil_img)
        return pil_img

image_transformations = Compose([
    v2.Resize((224, 224)),
    v2.PILToTensor(),
    v2.Normalize(mean=[0.485,  0.456,  0.406], std=[0.229,  0.224,  0.225]),
])

image_dataset = CustomDS(pil_objs, image_transformations)
image_dataloader = DataLoader(image_dataset, batch_size=IN_BATCH_SIZES, shuffle=False, num_workers=4)

first_batch = next(iter(image_dataloader))
grid = make_grid(first_batch, nrow=8)
to_pil = ToPILImage()(grid)
to_pil.show()
</code></pre>
<p>I get the following error:</p>
<pre><code>AttributeError: 'NoneType' object has no attribute 'seek'
AttributeError: 'JpegImageFile' object has no attribute 'load_seek'

During handling of the above exception, another exception occurred:

AttributeError  Traceback (most recent call last)
Cell In[47], line 10
&gt;    5     plt.show()
&gt;    7 # Assuming you have already defined your DataLoader as 'image_dataloader'
&gt;    8 
&gt;    9 # Get the first batch from the DataLoader
---&gt; 10 first_batch = next(iter(image_dataloader))
</code></pre>
<p>Additional notes</p>
<ul>
<li>the pil_objs is a list of 4,000+ Pil Image Objects</li>
<li><code>len(image_dataloader.dataset)</code>  returns the correct number of images</li>
<li><code>len(image_dataloader)</code> returns the number of batches based on the batch_size and qty of images</li>
</ul>
<p>Any help would be most appreciated</p>
","2024-02-18 21:59:14","1","Question"
"78017271","78017072","","<p>Pytorch tracks parameters through specific constructor classes. It has no visibility into arbitrary lists.</p>
<p>To track your list of modules, you need to wrap it in a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"" rel=""nofollow noreferrer"">nn.Sequential</a> or <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html"" rel=""nofollow noreferrer"">nn.ModuleList</a></p>
<pre class=""lang-py prettyprint-override""><code>class Q_function(nn.Module):

    def __init__(self, input_size=3, hidden_size=5, num_layers=3, learning_rate=0.01):
        super(Q_function, self).__init__()
        self.input_size = input_size
        self.layers = []
        for i in num_layers:
            self.layers.append(nn.Linear(input_size, hidden_size)) 
            self.append(nn.ReLU())
        self.layers.append(nn.Linear(hidden_size,1))
        self.layers = nn.ModuleList(self.layers)
    
    def forward(self, x):
        out = self.layers[0](x)
        for lay in range(1,len(self.layers)):
            out = self.layers[lay](out)
        return out
</code></pre>
<p>That said, there are also a number of errors in your model code. You probably want something like this:</p>
<pre class=""lang-py prettyprint-override""><code>class Q_function(nn.Module):

    def __init__(self, input_size=3, hidden_size=5, num_layers=3, learning_rate=0.01):
        super(Q_function, self).__init__()
        self.input_size = input_size
        self.layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]
        for i in range(num_layers-1):
            self.layers.append(nn.Linear(hidden_size, hidden_size)) 
            self.layers.append(nn.ReLU())
        self.layers.append(nn.Linear(hidden_size,1))
        self.layers = nn.Sequential(*self.layers)
    
    def forward(self, x):
        x = self.layers(x)
        return x
</code></pre>
","2024-02-18 20:21:11","2","Answer"
"78017198","78015750","","<p>Padding does not add dimensions to a tensor but adds elements to an existing dimension. For example: Say you have a vector shaped <code>(3,)</code> with values <code>[1, 2, 3]</code> and want to multiply it by a tensor shaped <code>(2, 3)</code> If you just 0-pad it with 2 elements, you will get a tensor shaped <code>(5,)</code> with values <code>[1, 2, 3, 0, 0]</code>, which will be no good to operate with the <code>(2,3)</code> tensor.</p>
<p>You have two options for this:</p>
<ol>
<li>Repeat the tensor across a new dimension. You can use <code>torch.repeat</code> or the more efficient <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand"" rel=""nofollow noreferrer""><code>torch.expand</code></a> to get the tensor</li>
</ol>
<pre><code>[[1, 2, 3],
 [1, 2, 3]]
</code></pre>
<p>which you can then operate with any other <code>(2, 3)</code> shaped tensor.</p>
<ol start=""2"">
<li>The most efficient and common way to do this is to convert your <code>(3,)</code> tensor to a <code>(1, 3)</code> tensor. This you can do by using <a href=""https://pytorch.org/docs/stable/generated/torch.unsqueeze.html"" rel=""nofollow noreferrer""><code>unsqueeze()</code></a> method. This will add a new dimension of size 1. Now your <code>(1, 3)</code> tensor can operate with any tensor shaped <code>(2, 3)</code>. I suggest you take a look at <a href=""https://pytorch.org/docs/stable/notes/broadcasting.html#general-semantics"" rel=""nofollow noreferrer"">broadcast semantics</a> for more info.</li>
</ol>
","2024-02-18 19:50:16","2","Answer"
"78017072","","Why does my dynamic neural network have 0 parameters?","<p>I have defined the following neural network:</p>
<pre><code>class Q_function(nn.Module):

    def __init__(self, input_size=3, hidden_size=5, num_layers=3, learning_rate=0.01):
        super(Q_function, self).__init__()
        self.input_size = input_size
        self.layers = []
        for i in num_layers:
            self.layers.append(nn.Linear(input_size, hidden_size)) 
            self.append(nn.ReLU())
        self.layers.append(nn.Linear(hidden_size,1))
    
    def forward(self, x):
        out = self.layers[0](x)
        for lay in range(1,len(self.layers)):
            out = self.layers[lay](out)
        return out
</code></pre>
<p>When I run:</p>
<pre><code>net = Q_function()
list(net.parameters())
</code></pre>
<p>I get the output as an empty list <code>[]</code>. Can someone explain why the network has no parameters? How to register the parameters? How to fix this issue?</p>
","2024-02-18 19:14:17","0","Question"
"78016973","","Check the difference in pretrained and Finetuned model","<p>So I am finetuning a pretrained LLaMa2 model. I want to check if the model that I have finetuned is different from the original. I want to check the difference between base_model and model. Is there a way to check if there is a difference in weights or parameters after training?</p>
<pre class=""lang-py prettyprint-override""><code>from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Path to your saved model in Google Drive
model_path_in_drive = '/content/drive/MyDrive/Mod/llama-2-7b-miniguanaco'

# Reload model in FP16 and merge it with LoRA weights
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)

# Load your PeftModel from the saved checkpoint in Google Drive
model = PeftModel.from_pretrained(base_model, model_path_in_drive)
model = model.merge_and_unload()
#mark_only_lora_as_trainable(lora_model)
# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;
</code></pre>
<p>I tried some methods in this <a href=""https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c"" rel=""nofollow noreferrer"">article</a> but it didn't help me at all.</p>
","2024-02-18 18:44:56","1","Question"
"78016739","78014539","","<p>Since you said its a 5 class problem, your output tensor should have the shape [1,5]. Do a torch.max on the output tensor.</p>
<pre><code>#This is assuming that the num of classes is in dim 1
_, pred = torch.max(scores, dim=1) 
</code></pre>
<p>The first output will be the value and the second - which is what we need - will be the index of the most probable class. Next, do a direct comparison with your Ground Truth label.</p>
<pre><code>if pred == labels:
   correct_predictions+=1
</code></pre>
<p>Then, average it out for your entire val/train set.</p>
<p>This is just a skeleton; you may have to change it to suit your particular setting.</p>
","2024-02-18 17:34:00","0","Answer"
"78016546","78016395","","<p>As discussed in the comments - the issue is your images have an alpha channel. You can modify the <code>read_image</code> function to remove the alpha channel from the input images as follows:</p>
<pre class=""lang-py prettyprint-override""><code>image = read_image(f'{self.dir}/{self.images[index]}', mode=ImageReadMode.RGB)
</code></pre>
<p>For other modes, you can check the <a href=""https://pytorch.org/vision/stable/generated/torchvision.io.ImageReadMode.html#torchvision.io.ImageReadMode"" rel=""nofollow noreferrer"">ImageReadMode class</a>.</p>
<p><em><strong>Update1:</strong></em></p>
<p>For the new error - according to the <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>ToTensor class converts a PIL Image or ndarray to tensor and scale the values accordingly.</p>
</blockquote>
<p>But here you are providing <code>tensor</code> as an input instead of the required PIL image or ndarray.</p>
<p>To resolve this you may use the <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.ToPILImage.html"" rel=""nofollow noreferrer"">ToPILImage</a> method.</p>
<p><em><strong>Update2:</strong></em></p>
<p>For the error: <code>TypeError: '_SingleProcessDataLoaderIter' object is not subscriptable</code></p>
<p>Check how to <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#iterate-through-the-dataloader"" rel=""nofollow noreferrer"">Iterate through a DataLoader</a> from the PyTorch tutorials.</p>
<p>Also, you may try using a for loop as well to iterate as follows:</p>
<pre><code>for images in trainloader:
    # Process images here
    break # update this break statement as per your requirement
</code></pre>
","2024-02-18 16:37:04","0","Answer"
"78016395","","Validation data without targets","<p>I have a validation dataset of images to be classified by my CNN model. I want to load these images using <code>pytorch</code>. <code>torchvision.datasets.ImageFolder()</code> function doesn't work, since there are no targets, because the dataset is unclassified. I'm assuming that I need to write a custom dataset class, that I would later put in <code>torch.utils.data.DataLoader()</code>. I've searched online, but I'm still not really understanding how the class should look like.</p>
<p>I've tried this</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import Dataset
from torchvision.io import read_image
import os


class Dset(Dataset):
    def __init__(self, dir: str, transform=None) -&gt; None:
        self.transform = transform
        self.images = os.listdir(dir)
        self.dir = dir
    
    def __getitem__(self, index: int) -&gt; torch.Tensor:
        image = read_image(f'{self.dir}/{self.images[index]}')
        if self.transform is not None:
            image = self.transform(image)
        return image

    def __len__(self) -&gt; int:
        return len(self.images)
</code></pre>
<p>But after this cell (all images are in <code>.data/</code>)</p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import transforms

batch_size = 64
transform = transforms.Compose([transforms.Grayscale(), transforms.ToTensor()])
data = Dset('data', transform=transform)
trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)

images, labels = iter(trainloader)
</code></pre>
<p>I am encountering this error: <code>TypeError: Input image tensor permitted channel values are [1, 3], but found 4</code></p>
<p><strong>Update</strong></p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import Dataset
from torchvision.io import read_image, ImageReadMode
import os


class Dset(Dataset):
    def __init__(self, dir: str, transform=None) -&gt; None:
        self.transform = transform
        self.images = os.listdir(dir)
        self.dir = dir
    
    def __getitem__(self, index: int) -&gt; torch.Tensor:
        image = read_image(f'{self.dir}/{self.images[index]}', mode=ImageReadMode.RGB)
        if self.transform is not None:
            image = self.transform(image)
        return image

    def __len__(self) -&gt; int:
        return len(self.images)
</code></pre>
<p>The error was caused by the alpha channel in the images.</p>
<p>After fixing that, I'm encountering this: <code>TypeError: pic should be PIL Image or ndarray. Got &lt;class 'torch.Tensor'&gt;</code></p>
<p><strong>Update 2</strong></p>
<pre class=""lang-py prettyprint-override""><code>from torchvision import transforms

batch_size = 64
transform = transforms.Compose(
[transforms.ToPILImage(), transforms.Resize((512, 512)),
transforms.Grayscale(), transforms.ToTensor()]
)
data = Dset('data', transform=transform)
trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)

images = iter(trainloader)[0]
</code></pre>
<p><code>torchvision.transforms.ToTensor</code> only converts PIL Image or <code>numpy.ndarray</code> to tensor.</p>
<p>Last line results in: <code>TypeError: '_SingleProcessDataLoaderIter' object is not subscriptable</code></p>
","2024-02-18 15:48:19","1","Question"
"78016213","78014487","","<p>Colab currently only provides an older generation of TPUs which is not compatible with recent JAX or PyTorch releases. It’s possible that may change in the future, but I don’t know of any official timeline of when that might happen. In the meantime, you can access recent-generation TPUs via Kaggle or Google Cloud.</p>
","2024-02-18 14:57:54","0","Answer"
"78015750","","Pytorch: Adding a dimension to a tensor through padding","<p>I have given different tensors where some have only 2 dimensions and some 3 dimensions. The first 2 dimensions are always matching.
I want them all to be of the same shape for further processing. Example:</p>
<p>tensor a: (1, 10)
tensor b: (1, 10, 15)</p>
<p>Now my approach was to pad 'tensor a' with zeros to the shape of 'tensor b' without changing any information.</p>
<p>In the following snippet <code>feature_tensor</code> can be seen as <code>tensor a</code> and the <code>padding_reference_tensor</code> as <code>tensor b</code>.</p>
<p>For the padding i use torch.nn.functional.pad.</p>
<pre><code>if feature_tensor.dim() == padding_reference_tensor.dim():
   padding_number = padding_reference_tensor.size()[2] - feature_tensor.size()[2]

elif feature_tensor.dim() &lt; padding_reference_tensor.dim():
   padding_number = padding_reference_tensor.size()[2] - 1

feature_tensor = F.pad(feature_tensor, (0, padding_number), &quot;constant&quot;, 0)
</code></pre>
<p>The case that <code>feature_tensor.dim() &gt; padding_reference_tensor.dim()</code> can be ignored for now.</p>
<p>Now i'd like <code>feature_tensor</code> to be from (1,10) to (1,10,15) but instead it is (1,24).</p>
<p>I understand why that happened but how do I add efficiently a dimension to <code>feature_tensor</code> when it is necessary?</p>
<p>Thanks a lot!</p>
","2024-02-18 12:32:58","0","Question"
"78015382","","Is there a dilated k-nearest neighbour solution available fast execution?","<p>I am implementing the <code>dilated k-nearest neighbors algorithm</code>. The algorithm unfortunately has nested loops. The presence of loops severely hampers the execution speed.</p>
<pre><code>import torch
dilation=3
nbd_size=5
knn_key=torch.randint([0,30,(64,12,198,100)])


dilated_keys=torch.zeros([knn_key.shape[0],knn_key.shape[1],knn_key.shape[2],nbd_size])

for i in range(knn_key.shape[0]):
    for j in range(knn_key.shape[1]):
        for k in range(knn_key.shape[2]):
            list_indices=[]
            while (len(list_indices))&lt;nbd_size:
                for l in range(knn_key.shape[3]):
                    if knn_key[i][j][k][l]%dilation==k%dilation:
                        list_indices.append(knn_key[i][j][k][l])
                        if (len(list_indices))&gt;=nbd_size:
                            break
            list_indices_tensor=torch.tensor(list_indices)
            dilated_keys[i][j][k]=list_indices_tensor
</code></pre>
<p>The variable <code>knn_key</code> stores the <code>100</code> nearest neighbours among the 1000 data points originally available. The <code>dilated_keys</code> stores the <code>nbd_size=5</code> selected indices of the neighbours that are used after applying dialation filter. Any help to use broadcasting solution to remove the three nested loops will be highly helpful.</p>
","2024-02-18 10:35:12","0","Question"
"78014539","","How do I calculate the accuracy of my Vision Transformer?","<p>I'm new to PyTorch, and want to find the accuracy of each epoch. I know that accuracy is # of correct predictions / the total samples, but I don't know how to integrate this into my code.:</p>
<pre><code>for epoch in range(1):
epoch_losses = []
model.train()
for step, (inputs, labels) in enumerate(train_dataloader):
    optimizer.zero_grad()
    inputs, labels = inputs.to(device), labels.to(device)
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    epoch_losses.append(loss.item())
if epoch % 1 == 0:  # For every epoch
    print(f&quot;&gt;&gt;&gt; Epoch {epoch+1} train loss: &quot;, np.mean(epoch_losses))
    epoch_losses = []
    model.eval()
    epoch_losses = []

    for step, (inputs, labels) in enumerate(test_dataloader):
        # Unpack the tuple
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        loss = criterion(outputs, labels)
        epoch_losses.append(loss.item())
    print(f&quot;&gt;&gt;&gt; Epoch {epoch+1} test loss: &quot;, np.mean(epoch_losses))
</code></pre>
","2024-02-18 04:04:12","1","Question"
"78014487","","How can I use PyTorch 2.2 with Google Colab TPUs?","<p>I'm having trouble getting PyTorch 2.2 running with TPUs on Google Colab. I'm getting an error about a JAX bug, but I'm confused about this because I'm not doing anything with JAX.</p>
<p>My setup process is very simple:</p>
<pre><code>!pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html
</code></pre>
<p>And then</p>
<pre><code>import torch
import torch_xla.core.xla_model as xm
</code></pre>
<p>which gives the error</p>
<pre><code>/usr/local/lib/python3.10/dist-packages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')
 This a JAX bug; please report an issue at https://github.com/google/jax/issues
  _warn(f&quot;cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report &quot;
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
</code></pre>
<p>Then trying</p>
<pre><code>t1 = torch.tensor(100, device=xm.xla_device())
t2 = torch.tensor(200, device=xm.xla_device())
print(t1 + t2)
</code></pre>
<p>gives the error</p>
<pre><code>2 frames
/usr/local/lib/python3.10/dist-packages/torch_xla/runtime.py in xla_device(n, devkind)
    121 
    122   if n is None:
--&gt; 123     return torch.device(torch_xla._XLAC._xla_get_default_device())
    124 
    125   devices = xm.get_xla_supported_devices(devkind=devkind)

RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: No ba16c7433 device found.
</code></pre>
","2024-02-18 03:36:44","1","Question"
"78011213","78008119","","<p>To fix the problem, I had to assign the model to gpu before passing into parameter.</p>
<p>before:</p>
<pre><code>peft_trainer = Trainer(
    model=peft_model.to(device),
    args=peft_training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
)
</code></pre>
<p>after:</p>
<pre><code>peft_model= peft_model.to(device)
peft_trainer = Trainer(
    model=peft_model,
    args=peft_training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
)
</code></pre>
","2024-02-17 06:57:02","1","Answer"
"78008932","78007663","","<p>You are calculating the memory required for just the weights of the model - this is a fraction of the total memory required for training.</p>
<p>When you train the model, you also have memory allocation for the model's activations, gradients, and optimizer state.</p>
<p>Pytorch doesn't &quot;reserve more memory than needed&quot; - you just need that much for what you are trying to do.</p>
<p>To reduce the memory required for fine-tuning, you can look into the following:</p>
<ul>
<li>use mixed precision training</li>
<li>reducing batch size and using gradient accumulation</li>
<li>gradient checkpointing</li>
<li>fine-tuning only the final layer of the model</li>
<li>using efficient fine-tuning methods like LORA</li>
</ul>
<p>Since you are using Huggingface, <a href=""https://huggingface.co/docs/transformers/v4.18.0/en/performance"" rel=""nofollow noreferrer"">this</a> will be useful</p>
","2024-02-16 17:03:19","1","Answer"
"78008438","78008233","","<p><code>normal_</code> fills with values drawn from a normal distribution, but that doesn't mean that the resulting tensor represents a normal or even a valid probability distribution.</p>
<p>E.g. <code>[0.2, 0.2, 0.2, 0.2, 0.2]</code> is an valid uniform distribution. If you had used <code>torch.empty(10).uniform_()</code>, you would not get a tensor that represents a uniform distribution.</p>
<p>For computing the KL divergence, each value in the tensor should represent the <em>probability</em> of that index occurring, not merely a sample from the said distribution (as in your example).</p>
<p>In your code, you could make the probabilities sum to 100%:</p>
<p><code>F.kl_div( (input_1/input_1.sum() ).log(), input_2 / input_2.sum(), reduction='batchmean')</code></p>
<p>which would give a positive result.</p>
","2024-02-16 15:33:46","5","Answer"
"78008233","","Why KL divergence is negative in Pytorch?","<p>I'm trying to get the KL divergence between 2 distributions using Pytorch, but the output is often negative which <a href=""https://stats.stackexchange.com/questions/335197/why-kl-divergence-is-non-negative"">shouldn't be the case</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch 
import torch.nn.functional as F

x_axis_kl_div_values = []
for epoch in range(200):
    # each epoch generates 2 different distributions
    input_1 = torch.empty(10).normal_(mean=torch.randint(1,50,(1,)).item(),std=0.5).unsqueeze(0)
    input_2 = torch.empty(10).normal_(mean=torch.randint(1,50,(1,)).item(),std=0.5).unsqueeze(0)

    kl_divergence = F.kl_div(input_1.log(), input_2, reduction='batchmean')
    x_axis_kl_div_values.append(kl_divergence.item())

x_axis_kl_div_values 
&gt;&gt;&gt; 
[324.4713134765625,
 -69.10758972167969,
 -92.42606353759766,
</code></pre>
<p>From the Pytorch forum I found <a href=""https://discuss.pytorch.org/t/kl-divergence-produces-negative-values/16791/1"" rel=""nofollow noreferrer"">this</a> that mentions that their issue was that the inputs were not proper distributions, which is not the case in my code as I'm creating a normal distribution. From <a href=""https://stackoverflow.com/questions/62806681/pytorch-kldivloss-loss-is-negative"">this SO thread</a> it seems like their issue was that the <code>nn.KLDivLoss</code> expects the input to be log-probabiltie, but again, I did that in my code. So I'm not sure what I'm missing</p>
","2024-02-16 14:58:47","1","Question"
"78008119","","Huggingface transformer train function throwing Device() received an invalid combination of arguments","<p>I was trying to train a model with peft qLora training. Lora config and peft training args are like below:</p>
<pre class=""lang-py prettyprint-override""><code>lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=[
        &quot;q_proj&quot;,
        &quot;k_proj&quot;,
        &quot;v_proj&quot;,
        &quot;o_proj&quot;,
        &quot;gate_proj&quot;,
        &quot;up_proj&quot;,
        &quot;down_proj&quot;,
        &quot;lm_head&quot;,
    ],
    bias=&quot;none&quot;,
    lora_dropout=0.05,  # Conventional
    task_type=&quot;CAUSAL_LM&quot;,
)
peft_model = get_peft_model(original_model, 
                            lora_config)

output_dir = f'./peft-bn-mistral-training-{str(int(time.time()))}'

peft_training_args = TrainingArguments(
    output_dir=output_dir,
    auto_find_batch_size=True,
    learning_rate=1e-3, # Higher learning rate than full fine-tuning.
    num_train_epochs=1,
    logging_steps=1,
    max_steps=1    
)
device = torch.device(&quot;cuda:0&quot;)
peft_trainer = Trainer(
    model=peft_model.to(device),
    args=peft_training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
)
peft_trainer.train()
</code></pre>
<p>The code is resulting error like this:</p>
<pre class=""lang-py prettyprint-override""><code>TypeError                               Traceback (most recent call last)

&lt;ipython-input-46-b47531775ae7&gt; in &lt;cell line: 1&gt;()
----&gt; 1 peft_trainer.train()
      2 
      3 peft_model_path=&quot;./peft-bn-mistral-checkpoint-local&quot;
      4 
      5 peft_trainer.model.save_pretrained(peft_model_path)
   1326             current_device_index = current_device.index if isinstance(current_device, torch.device) else current_device
   1327 
-&gt; 1328             if torch.device(current_device_index) != self.device:
   1329                 # if on the first device (GPU 0) we don't care
   1330                 if (self.device.index is not None) or (current_device_index != 0):

TypeError: Device() received an invalid combination of arguments - got (NoneType), but expected one of:
 * (torch.device device)
      didn't match because some of the arguments have invalid types: (!NoneType!)
 * (str type, int index)
</code></pre>
<p>I tried tweaked difference settings of introducing the device argument to model, but it consistently results the error above.
Note that i used <code>BitsAndBytesConfig</code> module from <code>transformers</code> for tokenizer.
TIA</p>
","2024-02-16 14:39:55","2","Question"
"78007663","","Pytorch reserving way more data than needed","<p>I'm trying to finetune a sentencetransformer.
The issue is that I'm running into an OOM-error (I'm using google-cloud to train the model).</p>
<p>I keep getting that pytorch reserves ~13GB (theres ~14GB) available thus theres no room for any batch.</p>
<p>If I try to calcuate the actual memory used, it's around 1.3GB</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import models
model_name = &quot;alexandrainst/scandi-nli-large&quot;
word_embedding_model = models.Transformer(model_name, max_seq_length=512)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
model.to(&quot;cuda&quot;)

param_size = 0
for param in model.parameters():
    param_size += param.nelement() * param.element_size()
buffer_size = 0
for buffer in model.buffers():
     buffer_size += buffer.nelement() * buffer.element_size()
size_all_mb = (param_size + buffer_size) / 1024 ** 2
print('model size: {:.3f}MB'.format(size_all_mb)) # ~1300
torch.cuda.memory_reserved()/(1024**2) # ~1300
</code></pre>
<p>I have tried to call <code>torch.cuda.empty_cache()</code> and set <code>os.environ[&quot;PYTORCH_CUDA_ALLOC_CONF&quot;] = &quot;max_split_size_mb:128&quot;</code> but the same error occurs.</p>
<p>Isn't there a way to don't make pytorch reserve memory (or atleast reduce it) but just use the memory which is needed?</p>
","2024-02-16 13:24:34","0","Question"
"78006570","77954041","","<p>Your two GPU cards have 24 GB of GPU memory each, right?</p>
<p>That would be a total of 48 GB. Mixtral, as you load it in your code above, requires around 100 GB of GPU memory. device_map=&quot;auto&quot; is loading everything, that does not fit into the GPU memory, in to your RAM. And that is making your inference slow.</p>
<p>On their model card, you'll find code to load the model in Lower precision (8-bit &amp; 4-bit). This will reduce the GPU memory needed by a lot and harms the quality only by a little:
<a href=""https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"" rel=""nofollow noreferrer"">https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1</a></p>
<p>In a discussion thread on their model card I found this:</p>
<ul>
<li>In 4-bits -&gt; 180 trillion bits, that's 22.5GB of VRAM required</li>
<li>in 8-bits -&gt; 45GB of VRAM</li>
<li>in half-precision -&gt; 90GB of VRAM required</li>
</ul>
","2024-02-16 10:12:28","2","Answer"
"78005662","77936766","","<p>You can use the <strong>.map</strong> function in the dataset to append the embeddings. I suggest you run this on <strong>GPU</strong> instead of <strong>CPU</strong> since nos of rows is very high.</p>
<p>Please try running the code below.</p>
<pre><code>import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModel

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;CPU&quot;)

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;)
model = AutoModel.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;, device_map = device)

# Load the dataset
ds = Dataset.from_file('training.arrow') #this is already tokenized

# Convert tokenized sequences to tensor
inputs = torch.tensor(ds['input_ids']).to(device)

# Reduce batch size
batch_size = 4

def get_embeddings(data):

    # Convert tokenized sequences to tensor
    input_ids =  torch.tensor(data['input_ids']).to(device)

    # Pass tokenized sequences through the model with reduced batch size
    with torch.no_grad():
        outputs = model(input_ids, output_hidden_states=True)
    
    hidden_states = outputs.hidden_states
    embeddings = hidden_states[-1]

    return {'embeddings' : embeddings.detach().cpu()}

# Extract embeddings
ds = ds.map(get_embeddings, batched=True, batch_size=batch_size)
ds
</code></pre>
","2024-02-16 07:15:13","1","Answer"
"78005416","","""No module named 'torch'"" using crontab","<p>My Python code works fine if I run it from the command line by:</p>
<pre><code> $ python3 detect_people_main.py
</code></pre>
<p>Now I want to run the code from crontab using:</p>
<pre><code>@reboot sleep 30 &amp;&amp; /usr/bin/python3 /home/pip/Downloads/yolov7/detect_people_main.py &gt;&gt; /home/pip/Downloads/yolov7/logfile.log 2&gt;&amp;1 &amp;
</code></pre>
<p>while printing the logs in <code>logfile.log</code>. But the code does not work from crontab. And the log is :</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/pip/Downloads/yolov7/detect_people_main.py&quot;, line 6, in &lt;module&gt;
    import torch
ModuleNotFoundError: No module named 'torch'
</code></pre>
<p>I am using Jetson NANO to run the code. Any suggestions to solve the issue?</p>
","2024-02-16 06:03:56","0","Question"
"78005039","78004983","","<p>Yes, that is the case.</p>
<p>You can see how <code>in_proj_weight</code> is used in the <a href=""https://github.com/pytorch/pytorch/blob/4eefe7285a4d105650d02a02ada73ebb782e0b2d/torch/nn/functional.py#L4846"" rel=""nofollow noreferrer"">_in_projection_packed</a> function</p>
<pre><code>projection weights for q, k and v, packed into a single tensor. Weights
are packed along dimension 0, in q, k, v order.
</code></pre>
","2024-02-16 03:25:52","1","Answer"
"78004983","","PyTorch MultiHeadAttention implementation","<p>In Pytorch's MultiHeadAttention <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention"" rel=""nofollow noreferrer"">implementation</a>, regarding in_proj_weight, is it true that the first embed_dim elements correspond to the query, the next embed_dim elements correspond to the key, and the final embed_dim elements correspond to the value? Just confirming.</p>
<p><a href=""https://stackoverflow.com/questions/58532911/why-is-the-input-size-of-the-multiheadattention-in-pytorch-transformer-module-15/78004965#78004965"">This</a> is a question asked in the same context, but doesn't answer my specific question</p>
","2024-02-16 02:59:59","0","Question"
"78000846","77997783","","<p>Your image is a 16-bit, greyscale PNG. You can see that with <code>exiftool</code>:</p>
<pre><code>exiftool -BitDepth -ColorType image.png
</code></pre>
<p><strong>Output</strong></p>
<pre><code>Bit Depth                       : 16
Color Type                      : Grayscale
</code></pre>
<p>It opens in <code>I</code> mode within PIL, i.e. as 32-bit signed integers:</p>
<pre><code>from PIL import Image

im = Image.open('image.png')
print(im)
print(im.mode)
</code></pre>
<p>That prints:</p>
<pre><code>&lt;PIL.PngImagePlugin.PngImageFile image mode=I size=494x727&gt;
I
</code></pre>
<p>An easy way to make it 8-bit unsigned is convert to Numpy array, shift right 8 bits and convert to unsigned 8-bit:</p>
<pre><code># Make &quot;im&quot; into Numpy array, shift right 8 bits, convert to uint8
na = (np.array(im)&gt;&gt;8).astype(np.uint8)
</code></pre>
<p>Now you can make that back into PIL Image with:</p>
<pre><code>pilImage = Image.fromarray(na)
pilImage.show()
</code></pre>
<p><a href=""https://i.sstatic.net/G6t6k.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/G6t6k.jpg"" alt=""enter image description here"" /></a></p>
","2024-02-15 12:40:15","1","Answer"
"77999946","77999462","","<p>It is difficult to see from this snippet of code. I have had a similar issue before with custom models. When your custom model consists of multiple parts, I have found that it is sometimmes necesary to move the individual parts to the device so lets say you have model A that combines an LSTM model B with a CNN model C, than you do</p>
<pre><code>B.to(device)
C.to(device)
</code></pre>
<p>Instead of <code>A.to(device)</code>
. It is worth a try if this is the case for you. Otherwise you need to debug by leaving out certain groups of tensors/models to find which part of the code introduces the cpu tensor</p>
","2024-02-15 10:16:55","0","Answer"
"77999462","","PyTorch RuntimeError: Device Mismatch in Custom Training Loop Despite Consistent Device Usage","<p>I'm working on a custom PyTorch model for a text-to-audio generation task using a diffusion model architecture. My setup involves processing text with a BERT model to generate embeddings, which are then used to condition audio generation. Despite explicitly moving all tensors and models to the same device (cuda if available, otherwise cpu), I'm encountering a RuntimeError indicating a device mismatch during training.</p>
<p>Here's a simplified version of my setup:</p>
<pre><code>import torch
import torch.nn as nn
import librosa
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel

# Custom dataset class
class TextAudioDataset(Dataset):
    def __init__(self, audio_dir, text_dir, tokenizer, bert_model, device):
        self.audio_dir = audio_dir
        self.text_dir = text_dir
        self.files = os.listdir(audio_dir)
        self.tokenizer = tokenizer
        self.bert_model = bert_model.to(device)
        self.device = device

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        # Code to load and process audio and text files
        # Returns text_embeddings.to(self.device), audio_tensor.to(self.device)

# Custom model, including a U-Net with FiLM layers conditioned on text embeddings
class DiffusionModel(nn.Module):
    # Model initialization and methods

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)
dataset = TextAudioDataset('./wavs', './transcripts', tokenizer, bert_model, device)
dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

# Model and training setup
diffusion_model = DiffusionModel(...).to(device)
optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=1e-4)

def train(model, dataloader, optimizer, epochs=10, device=device):
    model.train()
    for epoch in range(epochs):
        for i, (text_embeddings, audio) in enumerate(dataloader):
            text_embeddings = text_embeddings.to(device)
            audio = audio.to(device)
            # Forward pass, loss calculation, backward pass, and optimizer step

train(diffusion_model, dataloader, optimizer, epochs=10, device=device)

</code></pre>
<p>Despite moving all tensors (text_embeddings, audio) and models (bert_model, diffusion_model) to the intended device, I'm still getting the following error during training:</p>
<pre><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
</code></pre>
<p>I've double-checked that every tensor and model is explicitly moved to device before any operation that involves them. Could there be any part of my setup or approach that I'm overlooking which could cause this device mismatch issue?</p>
","2024-02-15 09:00:43","0","Question"
"77998841","77993164","","<p>As mentioned in the warning:</p>
<pre><code>    UserWarning: 
    Found GPU9 NVIDIA GeForce GT 710 which is of cuda capability 3.5.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability supported by this library is 3.7.

</code></pre>
<p>The main issue was that torch was trying to run code on the GT710 and due to it being not supported, the program crashed. To fix this I added an environment variable:</p>
<pre><code>export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8
</code></pre>
<p>So torch just ignores the GT710 now and it runs perfectly fine for me now.</p>
","2024-02-15 06:49:02","1","Answer"
"77997834","77997783","","<p>The individual image conversion can be handled by using
<code>png.point(lambda x: x / 256)</code>
This is documented here: <a href=""https://github.com/python-pillow/Pillow/issues/6765"" rel=""nofollow noreferrer"">https://github.com/python-pillow/Pillow/issues/6765</a>, where it's shown that PIL has issues with PNGs. However, the issue of it not carrying through the neural network is still an issue, but better suited for another question.</p>
","2024-02-15 00:04:26","0","Answer"
"77997783","","Issue converting a PNG image to a JPG image using PIL","<p>I'm working with X-ray images to build up a neural network, using Pytorch. After some fumbling, I found my main issue was on initial loading of the images.
For example, I start with this image: <a href=""https://i.sstatic.net/hhI4D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hhI4D.png"" alt=""Image of a PNG X-ray"" /></a></p>
<p>When I try to load it using the ImageFolder, and then DataLoader, it shows as:
<a href=""https://i.sstatic.net/SBjl7.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SBjl7.jpg"" alt=""Failed JPG-PNG conversion, it is just a white box."" /></a></p>
<p>After looking through it, I've realised that it is PIL.show() causing the issue (and the internal PNG-JPG conversion), but for the life of me, I can't get it to work. I've tried PIL's .convert() method, with no luck, as well as the steps for copying an alpha mask onto a background as shown <a href=""https://stackoverflow.com/questions/9166400/convert-rgba-png-to-rgb-with-pil?noredirect=1&amp;lq=1"">here</a>, I've tried changing the background colour to black as well, and nothing, everything just degrades into the white box. I've tried it on other X-rays in the files as well, and it's the same result.</p>
<p>When I tried to convert an image using <a href=""https://png2jpg.com/"" rel=""nofollow noreferrer"">PNGtoJPG.com</a>, I got this image: <a href=""https://i.sstatic.net/BD62H.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BD62H.jpg"" alt=""PNGtoJPG conversion, incorrectly"" /></a>, but FreeConverter.com worked fine.</p>
<p>I know it must be something with the transparency of the original image, but at this point, I've tried everything I can find, and nothing.</p>
<p>For reference, this is the conversion code I've been testing on:</p>
<pre><code>png = Image.open(&quot;0001_1297860395_01_WRI-L1_M014.png&quot;) #with the correct file path
    png.load()
    alpha = png.split()[-1]
    alpha= alpha.convert('RGBA')
    background = Image.new(&quot;RGB&quot;, png.size, (0, 0, 0))
    background.paste(png, alpha)
    background.save('./foo3.jpg', 'JPEG', quality=80)

</code></pre>
<p>The images load fine when using cv2, but since I need to use a custom transform, the method needs to convert into cv2.</p>
<p>Any help would be greatly appreciated, I'm at a loss!</p>
","2024-02-14 23:48:10","0","Question"
"77997747","77997000","","<p>You should look at the documentation section on <a href=""https://pytorch.org/vision/0.9/transforms.html#functional-transforms"" rel=""nofollow noreferrer"">functional transforms</a> which allow you to specify transform variables to deal with this exact issue.</p>
<pre class=""lang-py prettyprint-override""><code>import torchvision.transforms.functional as TF
import random

def my_segmentation_transforms(image, segmentation):
    if random.random() &gt; 0.5:
        angle = random.randint(-30, 30)
        image = TF.rotate(image, angle) # rotate image
        segmentation = TF.rotate(segmentation, angle) # rotate segmentation mask with same angle
    # more transforms ...
    return image, segmentation
</code></pre>
","2024-02-14 23:33:44","0","Answer"
"77997101","77997000","","<p>You could try concatenating the image and mask along the channel dimension, running the transform, and then splitting the result back into two tensors. Below assumes the image and mask are shaped <code>channels x height x width</code>.</p>
<pre><code>...

if self.transform is not None:
    #Concatenate along channel dimension.
    # Assuming dim=0 is the channel dimension (not the batch dim)
    image_and_mask = torch.cat([image, mask], dim=0) 
 
    #Transform together
    transformed = self.transform(image_and_mask)
    
    #Slice the tensors out
    image = transformed[:image.shape[0], ...]
    mass_mask = transformed[image.shape[0]:, ...]

...
</code></pre>
","2024-02-14 20:39:04","2","Answer"
"77997000","","Can't apply same transform to image and mask for data-augmentation","<p>I'm trying to train a U-Net model build with pytorch. For that case, I built the dataset and applied transformations for data augmentation in both image and mask. The situation is that i want to apply the same transformation to both, that meaning, if I rotate the image by an amount of degrees I want the mask to be rotated the same amount of degrees and therein lies my problem. The image and the mask aren´t rotated by the same amount.</p>
<p>I leave the code bellow:</p>
<p><strong>Dataset</strong></p>
<pre><code>import torch
from torch.utils.data import Dataset
import os

class INBreastDataset2012(Dataset):
    def __init__(self, dict_dir, transform=None):
        self.dict_dir = dict_dir
        self.data = os.listdir(self.dict_dir)
        self.transform = transform



    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        dict_path = os.path.join(self.dict_dir, self.data[index])
        patient_dict = torch.load(dict_path)
        image = patient_dict['image'].unsqueeze(0)
        mass_mask = patient_dict['mass_mask'].unsqueeze(0)
        mass_mask[mass_mask &gt; 1.0] = 1.0


        if self.transform is not None:
            image = self.transform(image)
            mass_mask = self.transform(mass_mask)
            
        
        return image, mass_mask

</code></pre>
<hr />
<p><strong>&quot;Trainging&quot;(isn't really training at this point, just visualization of the information brought by the dataloader)</strong></p>
<pre><code>from dataset import INBreastDataset2012
from torchvision.transforms import v2 as T
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader

train_dir = r'directory\of\training images and masks'
test_dir = r'directory\of\testing images and masks'

train_transform = T.Compose(
        [
            T.RandomRotation(degrees=35, expand=True, fill=255.0),
            T.RandomHorizontalFlip(p=0.5),
            T.RandomVerticalFlip(p=0.5),

        ]
    )

train_data = INBreastDataset2012(train_dir,transform=train_transform)
test_data = INBreastDataset2012(test_dir)

train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)

plt.figure(figsize=(12,12))
for i, (imagen,mascara) in enumerate(train_dataloader):
    ax = plt.subplot(2,4,i+1)
    ax.title.set_text(f'imagen {i+1}')
    plt.imshow(imagen.squeeze(), cmap='gray')
    ax = plt.subplot(2,4,i+3)
    ax.title.set_text(f'mascara de imagen {i+1}')
    plt.imshow(mascara.squeeze(), cmap='gray')
    if i == 1:
        break
</code></pre>
<p><strong>Result</strong>
<a href=""https://i.sstatic.net/tFqmz.png"" rel=""nofollow noreferrer"">Result transformation of images and masks</a></p>
<p>I will also add that I've tried with albumentations and torchvision.transforms v1. In examples of pytorch and youtube videos they seem to be doing the same as me.</p>
<p>I someone could help me to see what I'm doing wrong or have a solution to ensuring that the transformations are the same is going to be greatly appreciated.</p>
<p>If any extra information is needen please ask. Is my first post so I may have missed something.
Thank you in advance</p>
","2024-02-14 20:12:23","0","Question"
"77995327","77994888","","<p>The short answer is that you want <code>DataSet[bool]</code> to be a subtype of <code>DataSet[int]</code>, because <code>bool</code> is a subtype of <code>int</code>. By default, generic types are <em>invariant</em>, because the assumption is that the type will be mutable.</p>
<p>Compare tuples and lists. Tuples are covariant, because you can't modify them. The only thing you can do is read <em>from</em> them, so if you need a <code>tuple[int]</code> value, a tuple of any subclass of <code>int</code> will do. Lists, on the other hand, are invariant, because you don't necessarily know if you will be reading <em>from</em> the list or writing <em>to</em> the list. (If reading <code>int</code>s, you can take a <code>list[bool]</code> in place of <code>list[int]</code>; if writing <code>bool</code>s, you can take a <code>list[int]</code> instead a <code>list[bool]</code>. But in general, neither is substitutable for the other.)</p>
<p>Contravariance doesn't really come up with traditional containers, but does when talking about function types. A function <code>f</code> is substitutable for a function <code>g</code> if <code>f</code> accepts <em>all</em> the same arguments (but possibly  more) that <code>g</code> does, and returns <em>some</em> of the same arguments (but possibly fewer) than <code>g</code> could. We describe this as saying that function types are <em>contravariant</em> in their argument(s) and <em>covariant</em> in their return types.</p>
","2024-02-14 15:14:15","0","Answer"
"77994888","","Can we provide an example and motivation on when to use covariant and contravariant?","<p>I have a background in machine/deep learning but I aspire to be a good software engineer as well.</p>
<p>I have some troubles finding real use cases of covariant/contravariant (partly because this is a new concept for me and the initial learning curve is difficult).</p>
<p>I would like a concrete motivation and example on when covariant/contravariant is used, in particular, I would appreciate the example to be such that if covariant/contravariant is not applied, then the application would be buggy/not type safe.</p>
<p>To start, I know PyTorch's <code>Dataset</code> and <code>DataLoader</code> is parametrized by a covariant type:</p>
<pre class=""lang-py prettyprint-override""><code>class Dataset(Generic[T_co]):
    r&quot;&quot;&quot;An abstract class representing a :class:`Dataset`.

    All datasets that represent a map from keys to data samples should subclass
    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
    data sample for a given key. Subclasses could also optionally overwrite
    :meth:`__len__`, which is expected to return the size of the dataset by many
    :class:`~torch.utils.data.Sampler` implementations and the default options
    of :class:`~torch.utils.data.DataLoader`. Subclasses could also
    optionally implement :meth:`__getitems__`, for speedup batched samples
    loading. This method accepts list of indices of samples of batch and returns
    list of samples.

    .. note::
      :class:`~torch.utils.data.DataLoader` by default constructs a index
      sampler that yields integral indices.  To make it work with a map-style
      dataset with non-integral indices/keys, a custom sampler must be provided.
    &quot;&quot;&quot;

    def __getitem__(self, index) -&gt; T_co:
        raise NotImplementedError(&quot;Subclasses of Dataset should implement __getitem__.&quot;)

    # def __getitems__(self, indices: List) -&gt; List[T_co]:
    # Not implemented to prevent false-positives in fetcher check in
    # torch.utils.data._utils.fetch._MapDatasetFetcher

    def __add__(self, other: 'Dataset[T_co]') -&gt; 'ConcatDataset[T_co]':
        return ConcatDataset([self, other])

    # No `def __len__(self)` default?
    # See NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]
    # in pytorch/torch/utils/data/sampler.py
</code></pre>
<p>I wonder if someone can come up with convincing example of why a <code>Dataset</code> needs to be covariant.</p>
","2024-02-14 14:07:02","0","Question"
"77993236","77992977","","<p>All tensors are contiguous 1D data lists in memory. What differs is the interface PyTorch provides us with to access them. This all revolves around the notion of <strong>stride</strong>, which is the way this data is navigated through. Indeed, on a higher level, we prefer to reason our data in higher dimensions by using tensor <strong>shapes</strong>. The following example and description are still valid for higher-dimensional tensors.</p>
<p>The permutation operator offers a way to change how you access the tensor data by <em>seemingly</em> changing the order of dimensions. Permutations return a view and do not require a copy of the original tensor (as long as you do not make the data contiguous), in other words, the permuted tensor shares the <strong>same</strong> underlying data.</p>
<p>At the user interface, permutation reorders the dimensions, which means the way this tensor is indexed changes depending on the order of dimensions supplied to the <a href=""https://pytorch.org/docs/stable/generated/torch.permute.html"" rel=""nofollow noreferrer""><code>torch.Tensor.permute</code></a> method.</p>
<p>Take a simple 3D tensor example: <code>x</code> shaped <code>(I=3,J=2,K=2)</code>. Given <code>i&lt;I</code>, <code>j&lt;J</code>, and <code>k&lt;K</code>, <code>x</code> could naturally be accessed via <code>x[i,j,k]</code>. Concerning the underlying data being accessed, since the stride of <code>x</code> is <code>(JK=4, J=2, 1)</code>, then <code>x[i,j,k]</code> corresponds to <code>_x[i*JK+j*J+1]</code> where <code>_x</code> is the underlying data of <code>x</code>. By &quot;corresponds to&quot;, it means the data array associated with tensor <code>x</code> is being accessed with the index <code>i*JK+j*J+1</code>.</p>
<p>If you now were to permute your dimensions, say <code>y = x.permute(2,0,1)</code>, then the underlying data would remain the same (in fact <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html"" rel=""nofollow noreferrer""><code>data_ptr</code></a> would yield the same pointer) but the interface to <code>y</code> is different! We have <code>y</code> with a shape of <code>(K,I,J)</code> and accessing <code>y[i,j,k]</code> translate to <code>x[k,i,j]</code> ie. <code>dim=2</code> move to the front and <code>dim=0,1</code> moved to the back... After permutation the stride is no longer the same, <code>y</code> has a stride of <code>(IJ, I, 1)</code> so <code>y[i,j,k]</code> corresponds to <code>_x[i*IJ+j*I+1]</code>.</p>
<p>To read more about views and strides, refer to <a href=""https://stackoverflow.com/questions/42479902/what-does-view-do-in-pytorch"">this</a>.</p>
","2024-02-14 09:27:48","3","Answer"
"77993164","","RuntimeError: CUDA error: no kernel image is available for execution on the device for cuda 11.8 and torch 2.0.0","<p>I wanted to use meta-llama/Llama-2-13b-chat-hf, but I am having this error:</p>
<pre><code>RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.

For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<p>The output of nvidia-smi is:</p>
<pre><code>| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
</code></pre>
<p>NVCC:</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation

Built on Wed_Sep_21_10:33:58_PDT_2022

Cuda compilation tools, release 11.8, V11.8.89

Build cuda_11.8.r11.8/compiler.31833905_0
</code></pre>
<p>Torch Version:</p>
<pre><code>torch==2.0.0+cu118
torchaudio==2.0.1+cu118
torchvision==0.15.1+cu118
</code></pre>
<p>Transformers Version:</p>
<pre><code>transformers==4.37.2
</code></pre>
<p>I have a some 2080ti and a 710 and am using Ubuntu 16.</p>
<p>I also got this in my output:</p>
<pre><code>Found GPU9 NVIDIA GeForce GT 710 which is of cuda capability 3.5.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability supported by this library is 3.7.
</code></pre>
<p>I was downloading torch versions from <a href=""https://pytorch.org/get-started/previous-versions/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I tried building from the source as well but it gave me the same output.</p>
<p>The output from bitsandbytes:</p>
<pre><code>
++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++
COMPILED_WITH_CUDA = True
COMPUTE_CAPABILITIES_PER_GPU = ['7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '3.5']
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running a quick check that:
    + library is importable
    + CUDA function is callable


WARNING: Please be sure to sanitize sensible info from any such env vars!

SUCCESS!
Installation was successful!

</code></pre>
<p>I ran this code to test torch</p>
<pre><code>import torch
import sys
print('A', sys.version)
print('B', torch.__version__)
print('C', torch.cuda.is_available())
print('D', torch.backends.cudnn.enabled)
device = torch.device('cuda')
print('E', torch.cuda.get_device_properties(device))
print('F', torch.tensor([1.0, 2.0]).cuda())
</code></pre>
<p>It gave me this output:</p>
<pre><code>A 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]
B 2.0.0+cu118
C True
D True
    UserWarning: 
    Found GPU9 NVIDIA GeForce GT 710 which is of cuda capability 3.5.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability supported by this library is 3.7.
    
  warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))
E _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=11019MB, multi_processor_count=68)
F tensor([1., 2.], device='cuda:0')
</code></pre>
<p>What should I do to fix this?</p>
","2024-02-14 09:16:46","0","Question"
"77993048","77987416","","<p>Sort by row = False</p>
<pre class=""lang-py prettyprint-override""><code>from torch_geometric.utils import sort_edge_index

sorted_edge_index = sort_edge_index(edge_index, num_nodes=self.num_nodes, sort_by_row=False)
x = self.graph_sage(coord.view(-1, 2), sorted_edge_index)
</code></pre>
<p><a href=""https://github.com/pyg-team/pytorch_geometric/discussions/8908"" rel=""nofollow noreferrer"">https://github.com/pyg-team/pytorch_geometric/discussions/8908</a></p>
","2024-02-14 08:59:18","0","Answer"
"77992977","","How does tensor permutation work in PyTorch?","<p>I'm struggling with understanding the way <code>torch.permute()</code> works. In general, how exactly is an n-D tensor permuted? An example with explaination for a 4-D or higher dimension tensor is highly appreciated.</p>
<p>I've search across the web but did not find any clearly explaination.</p>
","2024-02-14 08:44:02","2","Question"
"77992278","77992011","","<p>🗎 <code>Dockerfile</code></p>
<pre><code>FROM pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime

COPY requirements.txt .

RUN pip3 install -r requirements.txt
</code></pre>
<p>I don't know what other packages and versions you require, but you have mentioned <code>opencv-python==4.7.0.68</code>.</p>
<p>🗎 <code>requirements.txt</code></p>
<pre><code>opencv-python==4.7.0.68
</code></pre>
<p>Build and check that correct package version is available in running container.</p>
<p><a href=""https://i.sstatic.net/mxi4B.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mxi4B.png"" alt=""enter image description here"" /></a></p>
","2024-02-14 05:45:52","1","Answer"
"77992011","","How to resolve conflicts with conda packages","<p>I have this Dockerfile and I want to install all the Python dependencies which include opencv-python 4.7.0.68 in the requirements.txt file</p>
<pre><code># PyTorch image
...

RUN pip install -r  requirements.txt
</code></pre>
<p>However, I am getting this error and I can't seem to resolve the issue except not installing the opencv-python 4.7.0.68</p>
<pre><code>55.09 Installing collected packages: voluptuous, tensorboard-plugin-wit, sentencepiece, library, easygui, tensorboard-data-server, safetensors, regex, pyasn1-modules, protobuf, opencv-python, oauthlib, multidict, markdown, lightning-utilities, grpcio, ftfy, frozenlist, entrypoints, cachetools, async-timeout, absl-py, yarl, requests-oauthlib, huggingface-hub, google-auth, aiosignal, torchmetrics, tokenizers, google-auth-oauthlib, diffusers, aiohttp, accelerate, transformers, timm, tensorboard, altair, pytorch-lightning, open-clip-torch
56.38     Found existing installation: protobuf 3.20.3
56.39     Uninstalling protobuf-3.20.3:
58.77       Successfully uninstalled protobuf-3.20.3
58.87   Attempting uninstall: opencv-python
58.87     Found existing installation: opencv-python 4.6.0
58.87 ERROR: Cannot uninstall opencv-python 4.6.0, RECORD file not found. Hint: The package was installed by conda.
</code></pre>
<p>Any idea?</p>
","2024-02-14 03:59:29","0","Question"
"77991715","77991685","","<p>Typically you want to set it to <code>True</code> for train. This allows the model to see more diverse batches across different epochs. At test time, it does not really matter.</p>
<p>I would say the reason for your low test loss is most likely unrelated to the <code>shuffle</code> parameter.</p>
","2024-02-14 01:33:17","0","Answer"
"77991685","","Should the PyTorch validation and test loaders shuffle the data?","<p>In PyTorch should you shuffle the validation and test datasets?</p>
<p>According to this <a href=""https://discuss.pytorch.org/t/shuffle-true-or-shuffle-false-for-val-and-test-dataloaders/143720"" rel=""nofollow noreferrer"">pytorch discussion thread</a> I believe the answer is no, it should be False.</p>
<p>The <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"" rel=""nofollow noreferrer"">official docs/tutorial</a> states <em>While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting</em>; which suggest just training should be <code>shuffle=True</code> However their immediate example has both set to True.</p>
<p>Any guidance would be helpful.</p>
<p>The reason I ask is I have mine set to True; but my test accuracy is terrible.</p>
<pre><code>Epoch: 1 | Train Loss: 2.1650 | Train Acc: 0.4683 | Test Loss: 6.2742 | Test Acc: 0.0349
Epoch: 2 | Train Loss: 1.2008 | Train Acc: 0.6624 | Test Loss: 6.9259 | Test Acc: 0.0359
Epoch: 3 | Train Loss: 0.9474 | Train Acc: 0.7325 | Test Loss: 7.3948 | Test Acc: 0.0365
Epoch: 4 | Train Loss: 0.7897 | Train Acc: 0.7808 | Test Loss: 7.7263 | Test Acc: 0.0352
Epoch: 5 | Train Loss: 0.6799 | Train Acc: 0.8186 | Test Loss: 8.0628 | Test Acc: 0.0377
Epoch: 6 | Train Loss: 0.5953 | Train Acc: 0.8374 | Test Loss: 8.3828 | Test Acc: 0.0351
Epoch: 7 | Train Loss: 0.5269 | Train Acc: 0.8647 | Test Loss: 8.5854 | Test Acc: 0.0351
Epoch: 8 | Train Loss: 0.4693 | Train Acc: 0.8791 | Test Loss: 8.9052 | Test Acc: 0.0337
Epoch: 9 | Train Loss: 0.4234 | Train Acc: 0.8955 | Test Loss: 9.1290 | Test Acc: 0.0354
</code></pre>
<pre><code>train_ds = ImageFolder(
    root=TRAIN_DIR,
    transform=pretrained_weights.transforms(),
)

test_ds = ImageFolder(
    root=TEST_DIR,
    transform=pretrained_weights.transforms(),
)

train_dl = DataLoader(
    dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True
)

test_dl = DataLoader(
    dataset=test_ds, batch_size=BATCH_SIZE, shuffle=True
)
</code></pre>
","2024-02-14 01:20:48","0","Question"
"77990796","77989807","","<p>There was a problem with python 3.12. Now I use python 3.11. Details here:</p>
<pre><code>Ultralytics YOLOv8.1.13 🚀 Python-3.11.5 torch-2.2.0+cu118 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)
Setup complete ✅ (16 CPUs, 15.9 GB RAM, 213.6/232.3 GB disk)

OS                  Windows-10-10.0.19045-SP0
Environment         Windows
Python              3.11.5
Install             pip
RAM                 15.95 GB
CPU                 AMD Ryzen 7 3700X 8-Core Processor
CUDA                11.8

matplotlib          ✅ 3.8.2&gt;=3.3.0
numpy               ✅ 1.26.4&gt;=1.22.2
opencv-python       ✅ 4.9.0.80&gt;=4.6.0
pillow              ✅ 10.2.0&gt;=7.1.2
pyyaml              ✅ 6.0.1&gt;=5.3.1
requests            ✅ 2.31.0&gt;=2.23.0
scipy               ✅ 1.12.0&gt;=1.4.1
torch               ✅ 2.2.0+cu118&gt;=1.8.0
torchvision         ✅ 0.17.0+cu118&gt;=0.9.0
tqdm                ✅ 4.66.2&gt;=4.64.0
psutil              ✅ 5.9.8
py-cpuinfo          ✅ 9.0.0
thop                ✅ 0.1.1-2209072238&gt;=0.1.1
pandas              ✅ 2.2.0&gt;=1.1.4
seaborn             ✅ 0.13.2&gt;=0.11.0
</code></pre>
","2024-02-13 20:50:37","0","Answer"
"77990778","77984185","","<p>Forgot to reset in the training loop:</p>
<pre><code>inp = torch.empty(0, 10)
lab = torch.empty(0, dtype=torch.int64)
</code></pre>
","2024-02-13 20:46:49","0","Answer"
"77989807","","NotImplementedError YOLO ultralitycs","<p>When launching neural network training, I get this error:</p>
<pre><code>NotImplementedError                       Traceback (most recent call last)
Cell In[1], line 5
      1 from ultralytics import YOLO
      3 model = YOLO('yolov8m.pt')
----&gt; 5 results = model.train(data='dataset_yolo8/data.yaml', epochs=100, imgsz=640) #, model='yolov8m.pt')

File c:\Users\Mugmazoid\AppData\Local\Programs\Python\Python312\Lib\site-packages\ultralytics\engine\model.py:601, in Model.train(self, trainer, **kwargs)
    598             pass
    600 self.trainer.hub_session = self.session  # attach optional HUB session
--&gt; 601 self.trainer.train()
    602 # Update model and cfg after training
    603 if RANK in (-1, 0):

File c:\Users\Mugmazoid\AppData\Local\Programs\Python\Python312\Lib\site-packages\ultralytics\engine\trainer.py:208, in BaseTrainer.train(self)
    205         ddp_cleanup(self, str(file))
    207 else:
--&gt; 208     self._do_train(world_size)

File c:\Users\Mugmazoid\AppData\Local\Programs\Python\Python312\Lib\site-packages\ultralytics\engine\trainer.py:322, in BaseTrainer._do_train(self, world_size)
    320 if world_size &gt; 1:
    321     self._setup_ddp(world_size)
--&gt; 322 self._setup_train(world_size)
    324 nb = len(self.train_loader)  # number of batches
    325 nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs &gt; 0 else -1  # warmup iterations
...
PythonTLSSnapshot: registered at ..\aten\src\ATen\core\PythonFallbackKernel.cpp:162 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at ..\aten\src\ATen\functorch\DynamicLayer.cpp:494 [backend fallback]
PreDispatch: registered at ..\aten\src\ATen\core\PythonFallbackKernel.cpp:166 [backend fallback]
PythonDispatcher: registered at ..\aten\src\ATen\core\PythonFallbackKernel.cpp:158 [backend fallback]  
</code></pre>
<p>All file paths are correct. Here's the entire code:</p>
<pre><code>from ultralytics import YOLO

model = YOLO('yolov8m.pt')

results = model.train(data='dataset_yolo8/data.yaml', epochs=100, imgsz=640, batch = 40) #, model='yolov8m.pt')
</code></pre>
<p>What could be the issue and how to fix it?</p>
","2024-02-13 17:25:59","0","Question"
"77987416","","Why does LSTM Aggregation in PyG need to sort edge_index?","<p><em>Hello, I have used <a href=""https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html"" rel=""nofollow noreferrer"">GraphSAGE</a> to do Node embedding. The function I chose to use for aggregate is <a href=""https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.LSTMAggregation.html#torch_geometric.nn.aggr.LSTMAggregation"" rel=""nofollow noreferrer"">LSTM</a> with the library of <a href=""https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html"" rel=""nofollow noreferrer"">PyG</a> for Graph Neural Network, the arguments it needs are follow:</em></p>
<p><strong>Input 1: Node features (|V|, F_in)</strong> - Here I use Node coordinates x-y in 2D plane (V x 2) and already normalized into the range of [0, 1] e.g.</p>
<pre><code>          x         y
0  0.374540  0.598658
1  0.950714  0.156019
2  0.731994  0.155995
</code></pre>
<p><strong>Input 2: Edge indices (2, |E|)</strong> - Adjacency matrix (V x V) but retrieves only the edge into (2, |E|)  from the original adjacency matrix I have e.g.</p>
<pre><code>idx   0  1  2
0   [[0, 1, 1], 
1    [1, 0, 1], 
2    [1, 1, 0]]
</code></pre>
<p>Above we have a shape (V x V) with 6 edges in the graph. We had to transform it a bit to accommodate PyG's use of shape (2, |E|) and I'd like to call it <code>edge_index</code> where edges is (0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1).</p>
<pre><code>idx   0  1  2  3  4  5
0   [[0, 0, 1, 1, 2, 2],
1    [1, 2, 0, 2, 0, 1]]
</code></pre>
<p><strong>Output: Node features (|V|, F_out)</strong> - Similar to Node coordinates, but they are not in 2D anymore, they are in a new embedding dimension with F_out dimensions.</p>
<p>My problem is that when using the LSTM aggregator it is forced to sort <code>edge_index</code> (Edge indices in input2) otherwise it will show an error saying <code>ValueError: Can not perform aggregation since the 'index' tensor. is not sorted.</code></p>
<p>So I have to do <a href=""https://pytorch.org/docs/stable/generated/torch.sort.html"" rel=""nofollow noreferrer"">sorting</a> gives it with the following command:</p>
<pre class=""lang-py prettyprint-override""><code># inside def __init__()
self.graph_sage=SAGEConv(in_channels=2, out_channels=hidden_dim, aggr='lstm')

# inside def forward()
sorted_edge_index, _ = torch.sort(edge_index, dim=1)  # for LSTM
x = self.graph_sage(coord.view(-1, 2), sorted_edge_index)  # using GraphSAGE
</code></pre>
<p>The <code>sorted_edge_index</code> tensor will look like this after sorting.</p>
<pre><code>idx   0  1  2  3  4  5
0   [[0, 0, 1, 1, 2, 2],
1    [0, 0, 1, 1, 2, 2]]
</code></pre>
<p>I noticed that in my full-mesh graph of 3 nodes connected, when I sorted it, the edges could be reinterpreted as (0, 0), (0, 0), (1, 1), (1, 1), (2, 2), (2, 2) which made me curious. And my questions are the following 2 things.</p>
<ol>
<li>Why does LSTM need to sort the <code>edge_index</code>?</li>
<li>After I sort <code>edge_index</code> like this, how will my model know which nodes are connected? Because all the original edge relationship pairs are gone. It's like sending edge pairs that don't exist in the graph as input. Will this be a disadvantage?</li>
</ol>
<p>I have tried doing the above and it ran fine. But I have some doubts and hope someone knowledgeable will help clarify things for a beginner like me. And I sincerely hope this question can be useful to other students studying GNN as well.</p>
","2024-02-13 11:04:01","0","Question"
"77986786","77962011","","<p>Ok I'll just post more info here, in case anyone else ends up here:</p>
<ul>
<li>The model needs loading form disk to GPU. This takes CPU and time</li>
<li>You need enough GPU VRAM.</li>
<li>On a bigger GPU take away the <code>load_in_8bit</code> so you can use all the memory and compute.</li>
</ul>
","2024-02-13 09:25:06","0","Answer"
"77984292","77984185","","<p>You shouldn't be concatenating inputs/outputs together. To implement gradient accumulation, you call <code>backward</code> every iteration as usual, but stagger the optimizer step.</p>
<pre class=""lang-py prettyprint-override""><code>accum_iter = 4 # ie to accumulate 4 batches

for i, data in enumerate(trainloader):
    inputs, labels = data
    
    outputs = net(inputs)
    
    loss = criterion(outputs, labels)
    
    loss = loss / accum_iter # normalize loss for accumulation
    
    loss.backward()
    
    if ((i + 1) % accum_iter == 0) or (i + 1 == len(data_loader)):
        # optimizer step every `accum_iter` iterations
        optimizer.step()
        optimizer.zero_grad()
</code></pre>
","2024-02-12 20:57:48","0","Answer"
"77984185","","Pytorch gradient accumulation implementation results in inplace operation error?","<p>Does anyone know why I am getting this error?</p>
<blockquote>
<p>RuntimeError: one of the variables needed for gradient computation has
been modified by an inplace operation: [torch.FloatTensor [84, 10]],
which is output 0 of AsStridedBackward0, is at version 2524; expected
version 2523 instead. Hint: the backtrace further above shows the
operation that failed to compute its gradient. The variable in
question was changed in there or anywhere later. Good luck!</p>
</blockquote>
<p>My code is here. I am taking the pytorch train a classifier tutorial as the simplified example
import torch
import torchvision
import torchvision.transforms as transforms
transform = transforms.Compose(
[transforms.ToTensor(),
transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</p>
<pre><code>batch_size = 4

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()



import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
inp = torch.empty(0, 10)
lab = torch.empty(0, dtype=torch.int64)

with torch.autograd.set_detect_anomaly(True):
    for epoch in range(2):  # loop over the dataset multiple times
        for i, data in enumerate(trainloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data

            outputs = net(inputs)
            inp = torch.cat((inp, outputs), dim=0)
            lab = torch.cat((lab, labels), dim=0)
            if i % 101 == 100:
                # zero the parameter gradients
                optimizer.zero_grad()
                # forward + backward + optimize
                loss = criterion(inp, lab)
                loss.backward(retain_graph=True)
                optimizer.step()
                print(loss)


print('Finished Training')
</code></pre>
","2024-02-12 20:33:28","0","Question"
"77981396","77977069","","<p><code>Conv2d</code> layers have no <code>in_features</code> or <code>out_features</code> arguments. Instead, the layer has <code>in_channels</code> and <code>out_channels</code> arguments. Additionaly, the second <code>Linear</code> layer in the <code>Decoder</code> must have <code>in_features</code> and <code>out_features</code> provided according to your input size.</p>
<pre><code>import torch
from torch import nn

latent_dims=4

class Encoder(nn.Module):

    def __init__(self):
        super().__init__()

        #input Nx1x28x28
        self.encoder = nn.Sequential(
             nn.Conv2d(in_channels=1, out_channels=16,kernel_size=(3,3), stride=2, padding=1), # out=16x14X14
             nn.ReLU(),
             nn.Conv2d(in_channels=16,out_channels=32,kernel_size=(3,3) ,stride=2,padding=1), #  out 32x7x7
             nn.ReLU(),
            # #nn.Conv2d(input_channel=32,out_channels=64,kernel_size=(7,7) ,stride=2,padding=1) #   64x1x1  , non considerare questo layer    
             nn.Flatten(),
             nn.Linear(in_features=32*7*7, out_features=latent_dims)
        )

    def forward(self, x):
        x = self.encoder(x)  
        return x

class Decoder(nn.Module):

    def __init__(self):
        super().__init__()

        self.decoder= nn.Sequential(
            nn.Linear(in_features=latent_dims, out_features=32*7*7),
            nn.Unflatten(dim=1, unflattened_size=(32, 7, 7)), #Unflatten for transpose conv
            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1),
            nn.Linear(in_features=32*7*7, out_features=32*7*7),  # Provide a value for out_features
            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.decoder(x)  
        return x

class Autoencoder(nn.Module):

    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()

    def forward(self, x):
        latent = self.encoder(x)
        recon = self.decoder(latent)
        return recon
    
autoencoder = Autoencoder()
</code></pre>
","2024-02-12 12:18:52","0","Answer"
"77980261","77979992","","<p>If you load the checkpoint you linked to (<code>hifigan_libritts100360_generator0p5.pt</code>) then you will see that the archive contains a single key/value pair: <code>&quot;generator&quot;</code> and is assigned to the state dictionary of the checkpoint itself.</p>
<pre><code>&gt;&gt;&gt; pt = torch.load('hifigan_libritts100360_generator0p5.pt')
&gt;&gt;&gt; pt.keys()
dict_keys(['generator'])
</code></pre>
","2024-02-12 08:51:38","1","Answer"
"77979992","","Export state_dict checkpoint from .pt model PyTorch","<p>I'm new to PyTorch and the whole model/AI programming.</p>
<p>I have a library that needs a checkpoint in the form of a <em>state_dict</em> from a model to run.</p>
<p>I've the <code>.pt</code> model (more preciously the <a href=""https://github.com/NVIDIA/radtts"" rel=""nofollow noreferrer"">radtts pre-trained model</a>) and I need to extract the dictionary for the checkpoint.</p>
<p>From what I understand from the <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended"" rel=""nofollow noreferrer"">PyTorch documentation</a> I should be able to load the model and save the <em>state_dict</em> with <code>torch.save(model.state_dict(), PATH)</code></p>
<p>My problem is, first of all, is it correct? How do I load the model on PyTorch to extract the <em>state_dict</em>?</p>
","2024-02-12 07:55:47","0","Question"
"77979126","77977525","","<p>Look at the <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention"" rel=""nofollow noreferrer"">source code</a> for the multihead attention module.</p>
<p>The weights learned are the projection layers for the query, key, and value inputs, in projection bias, out projection weight, and out projection bias.</p>
","2024-02-12 02:48:20","1","Answer"
"77977525","","What are the gradients computed when doing backpropagation (training) in nn.MultiheadAttention?","<p>This is my understanding of how training process works inside <code>nn.MultiheadAttention</code>. Lets ignore position encoding and lets only focus on <code>Q</code> case.</p>
<p><code>batch = 1,num_heads = 2, seq_len = 5, problem_dim = 4.</code></p>
<p><code>word_embedding = [5,4]</code>
<code>q_weight = [4x4]</code>
<code>Q = word_embedding*q_weight</code></p>
<p>Assume,</p>
<pre><code> class MultiHeadAttentionModel(nn.Module):
        def __init__(self, problem_dim, num_heads):
            super().__init__()
            self.multihead_attn = nn.MultiheadAttention(embed_dim=problem_dim,num_heads=num_heads,batch_first=True)
        
        def forward(self, query, key, value):
            attn_output, attn_output_weights = self.multihead_attn(query, key, value)
            return attn_output, attn_output_weights

model = MultiHeadAttentionModel(problem_dim=problem_dim, num_heads=num_heads)
model.eval()          &lt;---------------- forward pass
attn_output, attn_output_weights = model(Q, K, V)
attn_output.backward() &lt;--------------- training (backward pass)

final_linear_weight = model.multihead_attn.out_proj.weight
</code></pre>
<p>There is final linear transformation of <code>output = (softmax(Q.dot(K_trans).dot(V))*final_linear_weight</code> <em>Ignore the scalings for now</em></p>
<p>My question is, is <code>final_linear_weight</code> the only weight that is being learned during the training phase?</p>
","2024-02-11 16:39:19","0","Question"
"77977069","","Autoencoder with nn.Sequential","<p>i wrote this code, in order to implement an autoencoder with nn.sequential module, but i have an error:</p>
<pre><code>latent_dims=4

class Encoder(nn.Module):

    def __init__(self):
        super().__init__()

        #input Nx1x28x28
        self.encoder = nn.Sequential(
             nn.Conv2d(in_features=1,out_features=16,kernel_size=(3,3), stride=2, padding=1), # out=16x14X14
             nn.ReLU(),
             nn.Conv2d(in_features=16,out_features=32,kernel_size=(3,3) ,stride=2,padding=1), #  out 32x7x7
             nn.ReLU(),
            # #nn.Conv2d(input_channel=32,out_channels=64,kernel_size=(7,7) ,stride=2,padding=1) #   64x1x1  , non considerare questo layer    
             nn.Flatten(),
             nn.Linear(in_features=32*7*7, out_features=latent_dims)
        )

    def forward(self, x):
        x = self.encoder(x)  
        return x

class Decoder(nn.Module):

    def __init__(self):
        super().__init__()
        
        self.decoder= nn.Sequential(
            nn.Linear(in_features=latent_dims, out_features=32*7*7),
            nn.Unflatten(dim=1, unflatten_size=(32, 7, 7)), #Unflatten for transpose conv
            nn.ConvTranspose2d(in_features=32, out_features=16, kernel_size=3, stride=2, padding=1),
            nn.Linear(),
            nn.ConvTranspose2d(in_features=16, out_features=1, kernel_size=3, stride=2, padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.decoder(x)  
        return x

class Autoencoder(nn.Module):

    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()

    def forward(self, x):
        latent = self.encoder(x)
        recon = self.decoder(latent)
        return recon
    
autoencoder = Autoencoder()
</code></pre>
<pre><code>error: 
TypeError                                 Traceback (most recent call last)
Input In [19], in &lt;cell line: 50&gt;()
     47         recon = self.decoder(latent)
     48         return recon
---&gt; 50 autoencoder = Autoencoder()
     52 device = torch.device(&quot;cuda:0&quot; if use_gpu and torch.cuda.is_available() else &quot;cpu&quot;)
     53 autoencoder = autoencoder.to(device)

Input In [19], in Autoencoder.__init__(self)
     40 def __init__(self):
     41     super(Autoencoder, self).__init__()
---&gt; 42     self.encoder = Encoder()
     43     self.decoder = Decoder()

TypeError: __init__() missing 2 required positional arguments: 'in_features' and 'out_features'
</code></pre>
<p>i tried many things but it continues to not working, i think there is a logic problem</p>
<p>i tried to write just the numbers in the conv layers, but after i had no sense(maybe) error in unflatten, i tried to remove some layers to see if they were the problem, but nothing</p>
","2024-02-11 14:18:45","0","Question"
"77973266","77970659","","<h1>Problem</h1>
<p>You instantiated a wrong AutoModel class compared to the pipeline you declared.</p>
<p>The model is intended for Sequence classification, not fill-mask, so the responses of the model do not work correctly because the pipeline expects different output. That is the reason why you see dimension related problem.</p>
<h1>Solution</h1>
<p>Move to a AutoModelForMaskedLM Model and your code will work correctly</p>
<pre><code>from transformers import AutoModelForMaskedLM
from transformers import AutoTokenizer
from transformers import pipeline

modeltemp = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-cased&quot;, num_labels=5)
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

unmasker = pipeline('fill-mask', model=modeltemp, tokenizer=tokenizer)
unmasker(&quot;Hello I drive a red [MASK].&quot;)

&gt;&gt; [{'score': 0.18619821965694427,
  'token': 14013,
  'token_str': 'Ferrari',
  'sequence': 'Hello I drive a red Ferrari.'},
 {'score': 0.1289202719926834,
  'token': 13439,
  'token_str': 'BMW',
  'sequence': 'Hello I drive a red BMW.'},
 {'score': 0.0779753103852272,
  'token': 12110,
  'token_str': 'Honda',
  'sequence': 'Hello I drive a red Honda.'},
 {'score': 0.0691772997379303,
  'token': 21064,
  'token_str': 'Mustang',
  'sequence': 'Hello I drive a red Mustang.'},
 {'score': 0.055530842393636703,
  'token': 14062,
  'token_str': 'Harley',
  'sequence': 'Hello I drive a red Harley.'}]
</code></pre>
","2024-02-10 13:35:17","0","Answer"
"77972651","77971895","","<p>This is called automatic broadcasting, it corresponds to arguments of an operator being automatically expanded to be of equal dimension sizes. That can only happen if (a) the operator supports it <em>and</em> (b) the tensors are &quot;broadcastable&quot; (<a href=""https://pytorch.org/docs/stable/notes/broadcasting.html#general-semantics"" rel=""nofollow noreferrer"">ref</a>):</p>
<blockquote>
<ul>
<li>Each tensor has at least one dimension.</li>
<li>When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.</li>
</ul>
</blockquote>
<p>In the first case <code>a</code> is shaped <code>(50,1)</code> while weight is <code>(1,10)</code> so the expanded shapes will be <code>(50,1*10)</code> and <code>(1*50,10)</code> and the operation results in a shape of <code>(50,10)</code>. In the second case you have <code>(50,10)</code> and <code>(1,10)</code>, so the 2nd tensor expands <code>dim=0</code> to to <code>50</code> resulting in the same shape.</p>
<p>See the <a href=""https://numpy.org/doc/stable/user/basics.broadcasting.html"" rel=""nofollow noreferrer"">NumPy documentation</a> for details on broadcasting behaviors.</p>
","2024-02-10 10:29:18","1","Answer"
"77972607","77972379","","<p>What is not clear in your question is whether you want symmetry on one model (&quot;enforce symmetry in my <strong>model</strong>&quot;) or between two (implied when you introduce <code>model1</code> and <code>model2</code>).</p>
<p>If you want to enforce symmetry on a single model, you can minimize some distance between <code>model(x,y)</code> and <code>model(y,x)</code>? In that case you only require one model, and one optimizer and
enforce that property on <code>model</code>, and not on two distinct models.</p>
<p>Besides, I don't clearly see what you would get by having <code>model1</code> and <code>model2</code> distinct: <code>model1</code> would <code>model2</code> be &quot;symmetric&quot;... <code>model2</code> defined as <code>x,y: model1(y,x)</code> would suffice too no? Maybe there is more to it and you indeed want two models?</p>
","2024-02-10 10:15:32","1","Answer"
"77972379","","How to effectively share parameters between two Pytorch models","<p>I want to enforce symmetry in my model like <code>f(x,y) = f(y,x)</code> and would like to achieve it by defining two models <code>model1(x,y)</code> and <code>model2(y,x)</code> but let them have same architecture and parameters. I could share their parameters by</p>
<pre class=""lang-py prettyprint-override""><code>for name, param1 in model1.named_parameters():
    if name in model2.state_dict() and param1.shape == model2.state_dict()[name].shape:
      model2.state_dict()[name].data = param1.data
</code></pre>
<p>Does it make sense to share parameters this way? Do I have to define two optimizers for each model? Does</p>
<pre class=""lang-py prettyprint-override""><code>optimizer = optim.Adam(list(model1.parameters()) + list(model2.parameters()))  # Combine parameters
</code></pre>
<p>make sense since the models share parameters, assuming one of my losses is</p>
<pre class=""lang-py prettyprint-override""><code>criterion(model1(x,y), model2(y,x))
</code></pre>
<p>I would appreciate a minimal working example supposing my models have dynamic layer configurations.</p>
","2024-02-10 08:42:54","0","Question"
"77971895","","Why is the result different?","<p>I'm training a model about regression problem. the &quot;x&quot; size = (50), sz = 10, weights size is (1,10). After trying this two method to resize x, I get the same matrix of x.</p>
<pre><code>a = x.view(50, -1)
b = x.expand(sz, len(x)).t()
print(a*weight == b*weight)
result: True
</code></pre>
<p>But why I get different loss figure?</p>
","2024-02-10 04:32:09","0","Question"
"77971334","77951682","","<p>To resolve the issue I needed to carefully match versions of<code>cuda</code>, <code>pytorch</code> and <code>onnxruntime</code> provided by the <code>tritonserver</code> docker image with the Python packages of <code>torch</code> and <code>onnxruntime-gpu</code> installed manually. Here is the process in details:</p>
<ul>
<li>Understand what version of CUDA is currently supported by the <code>onnxruntime-gpu</code> by visiting <a href=""https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements"" rel=""nofollow noreferrer"">onnx cuda execution provider</a>. In my case it was <code>cuda==12.2</code></li>
<li>Navigate to the <a href=""https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel-24-01.html#rel-24-01"" rel=""nofollow noreferrer"">Triton IS release notes</a> and look for the <code>Container Version</code> with the matching cuda version from prior step. In my case it was <code>tritonserver:23.10-py3</code></li>
<li>Navigate to the <a href=""https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html"" rel=""nofollow noreferrer"">Triton IS version matrix</a> to retrieve the version of PyTorch included with the Triton IS Docker Image. In my case <code>torch 2.1</code></li>
</ul>
<p>Base on the collected versions, update the environment. In my case it is the Docker image with following changes:</p>
<pre><code>FROM --platform=linux/amd64 nvcr.io/nvidia/tritonserver:23.10-py3

RUN pip install transformers
RUN pip install torch==2.1

# https://onnxruntime.ai/docs/install/
# https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements
RUN pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/
</code></pre>
<p>NOTE: if your build environment has no access to the Azure repo: <a href=""https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/"" rel=""nofollow noreferrer"">https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/</a>  then retrieve and install the files manually from: <a href=""https://aiinfra.visualstudio.com/PublicPackages/_artifacts/feed/onnxruntime-cuda-12"" rel=""nofollow noreferrer"">https://aiinfra.visualstudio.com/PublicPackages/_artifacts/feed/onnxruntime-cuda-12</a> (make sure to correct <code>cuda-12</code> for your version)</p>
","2024-02-09 23:23:07","2","Answer"
"77970659","","Huggingface pipeline error: IndexError: too many indices for tensor of dimension 2","<p>Anyone know why I get that error when I run the pipeline?</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer
from transformers import pipeline

modeltemp = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=5)
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

unmasker = pipeline('fill-mask', model=modeltemp, tokenizer=tokenizer)
unmasker(&quot;Hello I drive a red [MASK].&quot;)
</code></pre>
<p>If I directly have the pipeline take in bert-base-cased, everything works. But if I first load bert-base-cased using AutoModel, it doesn't work.</p>
","2024-02-09 20:20:41","1","Question"
"77969618","77969343","","<p>Do the following:</p>
<ol>
<li>Add a new row to your embedding matrix, randomly initialized. This row represents your new vocab item</li>
<li>Create a training dataset of inputs that use the new vocab item (make sure you've updated tokenizers/preprocessing/etc to produce the new vocab integer)</li>
<li>Freeze all the non-embedding parameters of the model</li>
<li>Fine-tune the model with the new dataset. Make sure to zero the gradients on all the old embeddings (ie only update the new embedding) to prevent the old embeddings from being trained</li>
</ol>
","2024-02-09 16:43:55","1","Answer"
"77969595","77966203","","<p>It looks like your code is building a list of every skip connection that isn't reset between batches</p>
<pre class=""lang-py prettyprint-override""><code>class convolutionalEncoder(torch.nn.Module):
    
    class conv_block(torch.nn.Module):
 
        ...
 
    def __init__(self, device) -&gt; None:
        super(convolutionalEncoder, self).__init__()
        self.conv_block_list = []
        self.skip_conn = []
        for i in range(num_skip_conn):
            self.conv_block_list.append(self.conv_block(filters[i], device))
 
    def forward(self, X):
 
        for i in range(num_skip_conn):
            X = self.conv_block_list[i](X)
            self.skip_conn.append(X)
 
        return self.skip_conn
</code></pre>
<p><code>self.skip_conn</code> should just be a normal list, not an attribute that is retained between batches. What is happening is the tensors for every single batch are added to <code>self.skip_conn</code>, essentially storing the entire dataset in that list until you oom.</p>
<p>Just replace it with a new normal list every time</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(self, X):
 
        skip_conn = []
        for i in range(num_skip_conn):
            X = self.conv_block_list[i](X)
           skip_conn.append(X)
 
        return skip_conn
</code></pre>
","2024-02-09 16:38:35","1","Answer"
"77969343","","How to add a new item in the embeddings vocabulary?","<p>Imagine you have trained a model containing an Embedding layer.</p>
<p>Your model performs well and you're happy with your embedding.</p>
<p>Then, suddenly, you want to add a new item in your vocabulary.
In other words you want to compute the embedding of this new item.</p>
<p>Basically an Embedding layer is a lookup table, used to turn positive integers into dense vectors of fixed size, and now you want to consider a new integer that was not there during the training.</p>
<p>How can you do this <strong>without</strong> retraining the model from scratch?</p>
<p>Does it make sense (after some matrix shape adjustement) to relaunch the training freezing all the parameters except the ones used to embed the new item?</p>
","2024-02-09 15:55:26","0","Question"
"77968613","77968270","","<p><code>torchtext</code> is supported till <code>python 3.11</code></p>
<p><strong><a href=""https://pypi.org/project/torchtext/"" rel=""nofollow noreferrer"">https://pypi.org/project/torchtext/</a></strong></p>
<p>You need to downgrade to <code>python 3.11</code> or install <code>python 3.11</code> separately to work with <code>torchtext</code></p>
<p><a href=""https://i.sstatic.net/No9Ue.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/No9Ue.png"" alt=""enter image description here"" /></a></p>
","2024-02-09 13:53:13","0","Answer"
"77968270","","AttributeError: module 'torchtext.data' has no attribute 'Field' not fixed with torchtext.legacy","<p>I am trying to use torchtext.data.Field() from torchtext but it is not working in any way I have tried.</p>
<p>I have tried with
<code>from torchtext.legacy import data</code> but it does not seem to work. I have also tried to install another version of torchtext (currently have 0.16.2) by running <code>pip install torchtext==0.10.0</code> but I get this error message in the terminal</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torchtext==0.10.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.16.2)
ERROR: No matching distribution found for torchtext==0.10.0
</code></pre>
<p>Has anyone experienced this error before?</p>
<p>I am using a conda environment with Python version 3.12.1 and my torch version is 2.2.0.</p>
","2024-02-09 12:53:18","1","Question"
"77967236","77931982","","<p>There is <code>torch._dynamo.reset()</code>
<a href=""https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html</a></p>
<p>Which is recommended to use when using a different mode.</p>
<p>There was also <code>torch._dynamo.reset_code_caches</code> recently added:
<a href=""https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/__init__.py#L83"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/__init__.py#L83</a></p>
","2024-02-09 09:53:38","2","Answer"
"77966203","","Pytorch accumulates tensors from the previous batch(es)","<p>I have been trying to implement a trasnformer-UNet hybrid model for image segmentation. Whenever I try to train the model, it keeps running out of memory. Initially, I thought this was due to the size of the model and tried to decrease parameters like batch size, number of attention heads, number of transformer layers etc.</p>
<p>All these steps only delayed the inevitable, running out of memory. I even tried to use cloud GPUs, but still no luck. Here's a screenshot of pytorch's memory snapshot tool:</p>
<p><img src=""https://i.sstatic.net/lTsq8.png"" alt=""enter image description here"" /></p>
<p>(I do empty the cache by calling <code>torch.cuda.empty_cache</code>)</p>
<p>I suspect that the some tensors are being retained because I use a list to implement the skip connection (the necessary tensors are appended to the list).</p>
<pre><code>class convolutionalEncoder(torch.nn.Module):
    
    class conv_block(torch.nn.Module):
 
        def __init__(self, out_channels, device) -&gt; None:
            super().__init__()
            self.conv = LazyConv2d(out_channels= out_channels, kernel_size= 2,
                                   stride= (2, 2), padding= 'valid', device= device)
            
 
        def forward(self, X):
            global relu
 
            X = self.conv(X)
            X = relu(X)
 
            return X
 
    def __init__(self, device) -&gt; None:
        super(convolutionalEncoder, self).__init__()
        self.conv_block_list = []
        self.skip_conn = []
        for i in range(num_skip_conn):
            self.conv_block_list.append(self.conv_block(filters[i], device))
 
    def forward(self, X):
 
        for i in range(num_skip_conn):
            X = self.conv_block_list[i](X)
            self.skip_conn.append(X)
 
        return self.skip_conn
</code></pre>
<p><a href=""https://pastebin.com/xJeVjUNC"" rel=""nofollow noreferrer"">Here's a link to the code</a></p>
<p><a href=""https://drive.google.com/file/d/13EZ29fyZsNk7Kthe2j7t1ztVWKDtU_K2/view?usp=sharing"" rel=""nofollow noreferrer"">Here's a link to the pickle file (memory snapshot dump)</a></p>
","2024-02-09 05:44:35","-3","Question"
"77963646","77959410","","<p>Like you guessed, the issue is with the computational graph that gets created when you do backpropagation.</p>
<p>Let me explain the above point:</p>
<p>When you initialize a tensor in pytorch, it usually signals that the operations you perform on them should be tracked. When you do a forward pass, the functions for backward prop are set up and the graph is set.</p>
<p>In case 2, you are deleting the tensor and hence the entire process is reset -- the computation graph is reset. In case 3, you are clearly resetting the parameters.</p>
<p>The output tensor and the model parameters are connected to the graph.</p>
<p>If you want to clearly visualize where the TBackward0 function is, use torchviz to visualize the computational graph.</p>
","2024-02-08 17:22:04","-1","Answer"
"77963476","","How do I do masked fill in mlx?","<p>I want to implement masked_fill in mlx but it doesn't play nice with float('-inf')</p>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html</a></p>
<p>I'm trying to use mlx.core.where for this</p>
<pre><code>masked_tensor = mlx.core.where(mask, mlx.core.array(float('-inf')), mlx.core.array(0))
</code></pre>
<p>but for the mask</p>
<pre><code>array([[False, False, True, True],                                                                                                                                                      
       [False, False, True, True],                                                                                                                                                     
       [False, False, True, True],                                                                                                                                                    
       [False, False, True, True]], dtype=bool) 
</code></pre>
<p>this returns</p>
<pre><code>array([[nan, nan, -inf, -inf],                                                                                                                                              
       [nan, nan, -inf, -inf],                                                                                                                                                
       [nan, nan, -inf, -inf],                                                                                            
       [nan, nan, -inf, -inf]], dtype=float32)   
</code></pre>
<p>which is not what I want. Ideally it would return</p>
<pre><code>array([[0, 0, -inf, -inf],                                                                                                                                              
       [0, 0, -inf, -inf],                                                                                                                                                
       [0, 0, -inf, -inf],                                                                                            
       [0, 0, -inf, -inf]], dtype=float32)  
</code></pre>
<p>help</p>
","2024-02-08 16:55:27","0","Question"
"77962167","77961619","","<blockquote>
<p>shouldn't this be better? because has more layers(depth)</p>
</blockquote>
<p>More layers means the model is more complex and has more capacity, but that doesn't mean it will perform better. Deeper models can struggle to converge, resulting in both a lower train and validation score compared to a simpler one. I think this is what's happening in your case.</p>
<p>Try training the larger model for more epochs and with a different learning rate. More epochs give the model more time to adapt. I'd start with a smaller learning rate, and see how the model responds to larger rates. Change one thing at a time and observe its effect.</p>
<p>If it converges fine, it can still go too far and <em>overfit</em>, which means it'll score highly on the training set but less well on the validation set compared to a simpler model. This will be exacerbated if the dataset is
relatively small and non-diverse.</p>
","2024-02-08 13:43:41","6","Answer"
"77962011","","Transformer pipeline with 'accelerate' not using gpu?","<p>My transformers pipeline does not use cuda.</p>
<p>code:</p>
<pre><code> from transformers import pipeline, Conversation

# load_in_8bit: lower precision but saves a lot of GPU memory
# device_map=auto: loads the model across multiple GPUs
chatbot = pipeline(&quot;conversational&quot;, model=&quot;BramVanroy/GEITje-7B-ultra&quot;, model_kwargs={&quot;load_in_8bit&quot;: True}, device_map=&quot;auto&quot;)
</code></pre>
<p>Testing for cuda works just fine:</p>
<pre><code>import torch

print(torch.cuda.is_available())
</code></pre>
<p>Which prints <code>True</code></p>
<p>I have a project with these libs:</p>
<pre><code>[tool.poetry.dependencies]
python = &quot;^3.11&quot;
transformers = &quot;^4.37.2&quot;
torch = {version = &quot;^2.2.0+cu121&quot;, source = &quot;pytorch&quot;}
torchvision = {version = &quot;^0.17.0+cu121&quot;, source = &quot;pytorch&quot;}
accelerate = &quot;^0.26.1&quot;
bitsandbytes = &quot;^0.42.0&quot;

[[tool.poetry.source]]
name = &quot;pytorch&quot;
url = &quot;https://download.pytorch.org/whl/cu121&quot;
priority = &quot;supplemental&quot;
</code></pre>
<p>What am I missing?</p>
","2024-02-08 13:20:01","0","Question"
"77961619","","Why does ResNet101 have less accuracy than ResNet50 in classification of sport-celebrity dataset?","<p>I trained two different type of ResNet model from <code>torchvision.models</code> which is ResNet50 with <code>DEFAULT</code> weight and ResNet101 with <code>DEFAULT</code> weight too</p>
<p>but the results of training is really weird, the train accuracy and test accuracy of ResNet50 model is 89 and 85 respectively and for ResNet101 is 34, 28 !</p>
<p>what is wrong?</p>
<p>I froze the entire models and just trained the FC layer which is modified to have 4 output(equals to length of the classes)</p>
<p>I used 5 epochs for both.</p>
<p>why ResNet101 is worse than ResNet50??</p>
<p>shouldn't this be better? because has more layers(depth)</p>
","2024-02-08 12:23:07","2","Question"
"77960847","77960500","","<p>Did you already try using image from local instead from URL?</p>
<p>You also could try using smaller image sample or smaller tokens.</p>
","2024-02-08 10:19:07","0","Answer"
"77960500","","Cuda out of memory when running pretrained model","<p>I am new to the world of pytorch and I used searches and a couple of other source to get rid of the CUDA memory error without luck, perhaps anyone here has a solution.</p>
<p>I have the following code and want to simply run it:</p>
<pre><code>from PIL import Image

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration

model_id = &quot;llava-hf/llava-1.5-13b-hf&quot;

prompt = &quot;USER: &lt;image&gt;\nWhat are these?\nASSISTANT:&quot;
image_file = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;

model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
).to(0)

processor = AutoProcessor.from_pretrained(model_id)

raw_image = Image.open(requests.get(image_file, stream=True).raw)
inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)

output = model.generate(**inputs, max_new_tokens=200, do_sample=False)
print(processor.decode(output[0][2:], skip_special_tokens=True))



</code></pre>
<p>if I start the program I immediately get the standard CUDA out of memory error.</p>
<pre><code>+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070 Ti     Off | 00000000:01:00.0 Off |                  N/A |
| 30%   57C    P0              34W / 285W |      0MiB / 12282MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
</code></pre>
<p>Is it possible that the graphic card is really to weak? I could not imagine that as it takes about 20 sec to run the script with CPU? Tried everything with batch sizes clearing cache rebooting. Does anybody know or can point me in the right direction to get the pretrained model running?</p>
","2024-02-08 09:21:30","0","Question"
"77960376","77960180","","<p>The <a href=""https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html"" rel=""nofollow noreferrer"">documentation</a> gives clear examples that this achieves the effect that you want. I would assume that the maintainers are capable of writing proper test cases to cover this.</p>
<p>And quoting from <a href=""https://discuss.pytorch.org/t/how-to-compute-gradients-and-do-back-propagation-by-a-skipping-way/83531/2"" rel=""nofollow noreferrer"">this answer from albanD</a>:</p>
<blockquote>
<p>Yes doing with torch.no_grad() will prevent gradient from flowing back all the way to moduleA.</p>
</blockquote>
<p>where moduleA produces output that is processed by moduleB (which is executed within <code>no_grad</code>).</p>
<p>Alternatively: use <code>.detach()</code> on <code>proxy_target</code>. That detaches the tensor from the computation graph and no gradient can flow back. But I'm assuming using <code>no_grad</code> is cheaper since the computational graph will not be stored at all in this case.</p>
<p><strong>NOTE:</strong> Setting <code>requires_grad</code> to <code>False</code> manually on an intermediate module <a href=""https://discuss.pytorch.org/t/what-will-happen-if-i-set-requires-grad-false/54877"" rel=""nofollow noreferrer"">will let the gradients pass to earlier modules still</a>. This is different to using <code>no_grad</code> and then checking if the output tensor has <code>requires_grad</code> set to <code>True</code>.</p>
<hr />
<p>But if you still don't feel comfortable then do a little test:</p>
<ol>
<li>Check if <code>proxy_target.requires_grad</code> is <code>False</code></li>
<li>Return only <code>proxy_target</code> and create the rest in the forward function as new tensors (they will 100% not have any impact on the gradient). Store the full model weights before and after your pipeline, run for one iteration and check if there are any differences in the weights.</li>
</ol>
","2024-02-08 08:59:42","1","Answer"
"77960180","","Stopping gradient flow for a multiple headed Pytorch module","<p>I have a multiple headed model in Pytorch, similar to this one:</p>
<pre><code>class Net(nn.Module):
    def __init__(self):
         super(Net, self).__init__()
    
         self.backbone = Backbone()
         self.proxyModule = ProxyModule()

    def forward(self, x):
         backbone_output = self.backbone(x)
         proxy_target = transform_to_targets(backbone_output)
         proxy_output = self.proxyModule(backbone_output)
         return backbone_output, proxy_target, proxy_output 

net = Net()
x,y = get_some_data()
backbone_output, proxy_target, proxy_output = net(x)
backbone_loss = Loss(backbone_output, y)
proxy_loss = Loss(proxy_output, proxy_target)
total_loss = backbone_loss + proxy_loss
optimizer.zero_grad()
loss.backward()
optimizer.step()
</code></pre>
<p>Basically, I want to update the backbone model, and the proxy model via the same loss. However, I do not want that Backbone module gets updated via the operation <code>proxy_target = transform_to_targets(backbone_output)</code>. The purpose in there is only to generate output variables for the proxyModule. That is similar to a Q-learning scenario, actually.</p>
<p>I have the following modification in mind but I am not sure if that would work as I expect:</p>
<pre><code>class Net(nn.Module):
    def __init__(self):
         super(Net, self).__init__()
    
         self.backbone = Backbone()
         self.proxyModule = ProxyModule()

    def forward(self, x):
         backbone_output = self.backbone(x)

         with torch.set_grad_enabled(False):
              proxy_target = transform_to_targets(backbone_output)

         proxy_output = self.proxyModule(backbone_output)
         return backbone_output, proxy_target, proxy_output 

net = Net()
x,y = get_some_data()

optimizer.zero_grad()
with torch.set_grad_enabled(True):
    backbone_output, proxy_target, proxy_output = net(x)
    backbone_loss = Loss(backbone_output, y)
    proxy_loss = Loss(proxy_output, proxy_target)
    total_loss = backbone_loss + proxy_loss
    optimizer.zero_grad()
    loss.backward()
optimizer.step()
</code></pre>
<p>The difference is now I have put the line <code>proxy_target = transform_to_targets(backbone_output)</code> into the context manager where I set the gradient calculation to false. Lately the Autograd mechanism in Pytorch became more complex, so I cannot be sure if this would provide the intended effect.</p>
","2024-02-08 08:22:09","0","Question"
"77960052","","torch.cuda.OUtOfMemoryError: CUDA out of memory. Tried to allocate xxx MiB","<p>This is the error I got when I use pytorch to train my nerf model, which shows during the backward pass:</p>
<pre><code>Traceback (most recent call last):
  File &quot;train_dmsr.py&quot;, line 133, in &lt;module&gt;
    train()
  File &quot;train_dmsr.py&quot;, line 64, in train
    total_loss.backward()
  File &quot;/home/sue/anaconda3/envs/DM-NeRF/lib/python3.7/site-packages/torch/_tensor.py&quot;, line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File &quot;/home/sue/anaconda3/envs/DM-NeRF/lib/python3.7/site-packages/torch/autograd/__init__.py&quot;, line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 11.51 GiB total capacity; 9.94 GiB already allocated; 304.25 MiB free; 10.17 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre>
<p>I have tried to reduce the batchsize, but the error still exists, and it still shows the same amount of memory allocation (Tried to allocate 288.00 MiB).</p>
<p>I am not sure if it is because my GPU (11GB) is really out of memory or my GPU memory is not utilized.</p>
<p>I am also wondering if I should user a gpu server with larger memory or if there is any other solution to the Out of Memory Error?</p>
<p>Any suggestions are appreciated! Thanks!</p>
","2024-02-08 07:58:28","1","Question"
"77959410","","Pruning nn.Linear weights inplace causes unexpected error, requires slightly weird workarounds. Need explanation","<h2>This fails</h2>
<pre class=""lang-py prettyprint-override""><code>import torch

def test1():  
  layer = nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test1()
</code></pre>
<p>with error</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-3-bb36a010bd86&gt; in &lt;cell line: 10&gt;()
      8     x = 5 - torch.sum(layer(torch.ones(90)))
      9     x.backward()
---&gt; 10 test1()
     11 # and this works as well
     12 

2 frames
/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    249     # some Python versions print out the first line of a multi-line function
    250     # calls in the traceback and some print out the last line
--&gt; 251     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    252         tensors,
    253         grad_tensors_,

RuntimeError: Function TBackward0 returned an invalid gradient at index 0 - got [10, 90] but expected shape compatible with [10, 100]
</code></pre>
<h2>This works</h2>
<pre class=""lang-py prettyprint-override""><code>import torch

def test2():  
  layer = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  del x    #main change
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test2()
</code></pre>
<h2>and this works as well</h2>
<pre class=""lang-py prettyprint-override""><code>import torch
def test3():  
  layer = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  layer.weight = torch.nn.Parameter(layer.weight)   #main change
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test3()
</code></pre>
<p>I encountered this when trying to implement a paper on model pruning (Temporal Neuron Variance Pruning). I believe this has something to do with the autograd graph, but I have am not sure what exactly is going on. I've already seen the link on pruning and got my code working using the 3rd snippet. I am now trying to figure out why 1 and 2 did not work. Is there some explanation for why these almost identical code snippets work or fail?</p>
<h2>Major points I'd like to figure out -</h2>
<ol>
<li>what is <code>TBackward0</code></li>
<li>where is it defined</li>
<li>where is the runtime error raised</li>
<li>why is the compatibility with the old shape expected - especially when the grad has been modified correctly (I am assuming I have edited the tensors correctly because cases 2, 3 work)</li>
<li>can I change something else (other than the 2 working cases) to make this work ?</li>
</ol>
","2024-02-08 05:17:32","4","Question"
"77957584","77957522","","<p>I found the solution. It was previously reported <a href=""https://github.com/sthalles/SimCLR/issues/40"" rel=""nofollow noreferrer"">here</a> and recently <a href=""https://github.com/sthalles/SimCLR/issues/60"" rel=""nofollow noreferrer"">here</a>. However, the code was still not updated. It seems this line:</p>
<pre><code>labels = torch.cat([torch.arange(self.args.batch_size) for i in range(self.args.n_views)], dim=0)
</code></pre>
<p>is dependant on <code>batch_size</code> and was causing the issue. The above line needs to be replaced with either: <code>labels = torch.cat([torch.arange(features.shape[0]//2) for i in range(self.args.n_views)], dim=0)</code></p>
<p>OR: <code>labels = torch.cat([torch.arange(features.shape[0]//self.args.n_views) for i in range(self.args.n_views)], dim=0)</code></p>
<p>Based on that change, the correct loss function would be:</p>
<pre><code>def info_nce_loss(self, features):

        labels = torch.cat([torch.arange(features.shape[0]//2) for i in range(self.args.n_views)], dim=0)
        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
        labels = labels.to(self.args.device)

        features = F.normalize(features, dim=1)

        similarity_matrix = torch.matmul(features, features.T)
        # assert similarity_matrix.shape == (
        #     self.args.n_views * self.args.batch_size, self.args.n_views * self.args.batch_size)
        # assert similarity_matrix.shape == labels.shape

        # discard the main diagonal from both: labels and similarities matrix
        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.args.device)
        labels = labels[~mask].view(labels.shape[0], -1)
        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)
        # assert similarity_matrix.shape == labels.shape

        # select and combine multiple positives
        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)

        # select only the negatives the negatives
        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)

        logits = torch.cat([positives, negatives], dim=1)
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.args.device)

        logits = logits / self.args.temperature
        return logits, labels
</code></pre>
","2024-02-07 19:57:00","1","Answer"
"77957522","","Shape mismatch when implementing Info NCE loss","<p>I'm trying to implement the Info NCE loss of this <a href=""https://arxiv.org/pdf/1807.03748.pdf"" rel=""nofollow noreferrer"">paper</a> with my own image dataset. I'm following the implementation from this <a href=""https://github.com/sthalles/SimCLR/tree/master"" rel=""nofollow noreferrer"">repo</a> and using the following code:</p>
<pre><code>def info_nce_loss(self, features):

        labels = torch.cat([torch.arange(self.args.batch_size) for i in range(self.args.n_views)], dim=0)
        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
        labels = labels.to(self.args.device)

        features = F.normalize(features, dim=1)

        similarity_matrix = torch.matmul(features, features.T)
        # assert similarity_matrix.shape == (
        #     self.args.n_views * self.args.batch_size, self.args.n_views * self.args.batch_size)
        # assert similarity_matrix.shape == labels.shape

        # discard the main diagonal from both: labels and similarities matrix
        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.args.device)
        labels = labels[~mask].view(labels.shape[0], -1)
        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)
        # assert similarity_matrix.shape == labels.shape

        # select and combine multiple positives
        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)

        # select only the negatives the negatives
        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)

        logits = torch.cat([positives, negatives], dim=1)
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.args.device)

        logits = logits / self.args.temperature
        return logits, labels
</code></pre>
<p>to train my model in a self-supervised manner. I was using <code>batch_size</code> of 32 with the above loss function in my code and everything was working fine. But when I change the <code>batch_size</code> to any other number for instance, 256, I get the following error:</p>
<pre><code>The shape of the mask [512, 512] at index 0 does not match the shape of the indexed tensor [2, 2] at index 0. 
</code></pre>
<p>The error originates at this line:</p>
<pre><code>labels = labels[~mask].view(labels.shape[0], -1)
</code></pre>
<p>I tried resizing my images but that didn't help either. Any idea on what could be the issue here?</p>
","2024-02-07 19:46:25","1","Question"
"77955502","77950487","","<p>The error suggests that <code>filename</code> is missing the &quot;.jpg&quot; extension. A simple fix is to add the extension manually if it's always the same format:</p>
<p><code>filename = self.df.loc[index, 'image_name'] + &quot;.jpg&quot;</code></p>
","2024-02-07 14:18:05","-1","Answer"
"77955060","77954817","","<p>I added comments that helped me to better understand what happens here.</p>
<p>tldr: Option 2 (or 3) are preferred, as this does not apply the same gradient twice.</p>
<h3>OPTION 1</h3>
<pre class=""lang-py prettyprint-override""><code>optA = torch.optim.Adam(params=netA.parameters())  # contains shared fc2
optB = torch.optim.Adam(params=netB.parameters())  # contains shared fc2
optA.step()  # applies 1 grad to fc1_A, 2 grads to fc2_AB
optB.step()  # applies 1 grad to fc1_B, 2 grads to fc2_AB
# applies the fc2_AB grads twice! Thus doubling the learning rate for fc2_AB
</code></pre>
<h3>OPTION 2</h3>
<pre class=""lang-py prettyprint-override""><code>opt = torch.optim.Adam(
    params=(
        list(netA.parameters())  # contains netA.fc1, netA/B.fc2
        + list(netB.parameters())[:1]  # contains netA.fc1
    )
)
opt.step()  # applies 1 grad to fc1_A, 1 grad to fc1_B, 2 grads to fc2_AB
# does not double the learning rate for fc2_AB
</code></pre>
<h3>OPTION 3</h3>
<pre class=""lang-py prettyprint-override""><code>optA = torch.optim.Adam(params=netA.parameters())  # contains shared fc2
optB = torch.optim.Adam(params=netB.parameters())  # contains shared fc2
optA.step()  # applies 1 grad to fc1_A, 2 grad to fc2_AB
optA.zero_grad()  # avoids the same gradient to be applied twice!
optB.step()  # applies 1 grad to fc1_B, 2 (ZERO!) grads to fc2_AB
# applies the fc2_AB grads twice! Thus doubling the learning rate for fc2_AB
</code></pre>
<p>Option 1 and 3 are identical and preferred, however, option 3 is slower as an unnecessary zero-gradient is applied.</p>
","2024-02-07 13:15:07","0","Answer"
"77954817","","joint or seperate optimizers when sharing weights between networks","<p>I am wondering, what are the side effects of these two scenarios.</p>
<ol>
<li><code>netA</code> and <code>netB</code> share weights but have one optimizer each</li>
<li><code>netA</code> and <code>netB</code> share weights but use a single optimizer</li>
</ol>
<p>The results are very similar, but not equal.</p>
<p>Consider this example</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

torch.manual_seed(0)

share_fc2 = True
batch_size = 4
channels = 2

training_data_A = torch.rand((batch_size, channels))
training_data_B = torch.rand((batch_size, channels))


class Net(torch.nn.Module):
    &quot;&quot;&quot;Minimal network&quot;&quot;&quot;

    def __init__(self) -&gt; None:
        super().__init__()
        self.fc1 = nn.Linear(channels, channels, bias=False)
        self.fc2 = nn.Linear(channels, channels, bias=False)

    def forward(self, x):
        return self.fc2(self.fc1(x))


netA = Net().requires_grad_()
netB = Net().requires_grad_()

if share_fc2:
    # replace fc2 by netA.fc2
    netB.fc2 = netA.fc2

lossA = netA(training_data_A).mean()
lossB = netA(training_data_A).mean()

lossA.backward()
lossB.backward()
</code></pre>
<h3>Option 1</h3>
<pre class=""lang-py prettyprint-override""><code># OPTION 1 (independent run, seed used)
optA = torch.optim.Adam(params=netA.parameters())  # contains shared fc2
optB = torch.optim.Adam(params=netB.parameters())  # conteins shared fc2
optA.step()
optB.step()

print(list(netA.parameters()))
print(list(netB.parameters()))
</code></pre>
<h4>output</h4>
<p><a href=""https://i.sstatic.net/t7YR1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t7YR1.png"" alt=""enter image description here"" /></a></p>
<h3>Option 2</h3>
<pre class=""lang-py prettyprint-override""><code>opt = torch.optim.Adam(
    params=(
        list(netA.parameters())  # contains netA.fc1, netA/B.fc2
        + list(netB.parameters())[:1]  # contains netA.fc1
    )
)
opt.step()
print(list(netA.parameters()))
print(list(netB.parameters()))

</code></pre>
<h4>output</h4>
<p><a href=""https://i.sstatic.net/JoQKR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JoQKR.png"" alt=""enter image description here"" /></a></p>
<h3>Questions</h3>
<ul>
<li>are the differences due to Adam's internal parameter adjustments and thus can be ignored</li>
<li>Are the differences due to mathematics in gradient calculation and application?</li>
<li>Is one option better than the other?</li>
</ul>
","2024-02-07 12:42:07","0","Question"
"77954041","","Inference of Mixtral 8x7b on multiple GPUs with pipeline","<p>I run Mixtral 8x7b on two GPUs (RTX3090 &amp; A5000) with pipeline. I can load the model in GPU memories, it works fine, but inference is very slow. When I run <code>nvidia-smi</code>, there is not a lot of load on GPUs. But the motherboard RAM is full (&gt;128Gb) and a CPU reach 100% of load. I feel that the model is loaded in GPU, but inference is done in the CPU.</p>
<p>Here is my code:</p>
<pre><code>from transformers import pipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import time

import torch
from accelerate import init_empty_weights, load_checkpoint_and_dispatch

t1= time.perf_counter()

model_id = &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=&quot;auto&quot;)

t2= time.perf_counter()
print(f&quot;Loading tokenizer and model: took {t2-t1} seconds to execute.&quot;)
# Create a pipeline
code_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)

t3= time.perf_counter()
print(f&quot;Creating piepline: took {t3-t2} seconds to execute.&quot;)


# Generate code for an input string
while True:
  print(&quot;\n=========Please type in your question=========================\n&quot;)
  user_content = input(&quot;\nQuestion: &quot;) # User question
  user_content.strip()
  t1= time.perf_counter()
  generated_code = code_generator(user_content, pad_token_id=tokenizer.eos_token_id, max_new_tokens=20)[0]['generated_text']
  t2= time.perf_counter()
  print(f&quot;Inferencing using the model: took {t2-t1} seconds to execute.&quot;)
  print(generated_code)
</code></pre>
<p>Any idea why inference is so long (&gt;300s)</p>
","2024-02-07 10:43:27","0","Question"
"77953397","77952758","","<p>You can use <a href=""https://pytorch.org/docs/stable/generated/torch.where.html"" rel=""nofollow noreferrer""><code>torch.where</code></a> to construct the desired tensor based on a condition mask of the same shape as <code>z</code>:</p>
<pre><code>&gt;&gt;&gt; mask = x.norm(dim=(2,3), keepdim=True) &gt; y.norm(dim=(2,3), keepdim=True) 
&gt;&gt;&gt; torch.where(mask.repeat(1,1,H,W), x, y)
</code></pre>
","2024-02-07 09:03:37","0","Answer"
"77953039","77952758","","<p>You can do it by creating a boolean mask for your condition:</p>
<pre><code>import torch

x = torch.rand(20, 30, 40, 50)
y = torch.rand(20, 30, 40, 50)

B, C, H, W = x.size()
z = torch.zeros_like(x)

x_norm = torch.norm(x, dim=(2, 3))
y_norm = torch.norm(y, dim=(2, 3))

condition = x_norm &gt;= y_norm  # Create a boolean tensor indicating the condition

# Use the condition to assign values to z without loops
z[condition] = x[condition]
z[~condition] = y[~condition]
</code></pre>
<p>Though the question does not clarify that, I am assuming that <code>x</code> and <code>y</code> have the same shape.</p>
","2024-02-07 07:52:05","1","Answer"
"77952831","77951682","","<p>This is most likely just an installation issue.</p>
<p>The error you are getting</p>
<pre><code>There's no data transfer registered for copying tensors from DeviceType:1 to DeviceType:0
</code></pre>
<p>means that <code>ort</code> is trying to copy data from the GPU (<code>DeviceType:1</code>, the torch tensor you explicitly initialized on <code>cuda:0</code>) to the CPU (<code>DeviceType:0</code>, the <code>InferenceSession</code>).</p>
<p>As 1) this transfer should be possible and 2) the <code>InferenceSession</code> should be on the GPU in the first place, what is probably happening is that your redundant installation of <code>onnxruntime</code> <em>after</em> <code>onnxruntime-gpu</code> in Dockerfile is messing up the dependencies. I.e. your code is using the <code>onnxruntime</code> installation which does not have GPU support.</p>
<p>Add <code>print(session.get_providers())</code> to confirm that your session is defaulting to just <code>CPUExecutionProvider</code>, and try rebuilding the container without the unnecessary library installation.</p>
","2024-02-07 07:10:35","0","Answer"
"77952758","","Merging two tensors with channel norm","<p>I want to merge 3-D tensors with channel norm.</p>
<p>My approach is as below :</p>
<pre><code>B, C, H, W = x.size()
z = torch.zeros_like(x)

x_norm = torch.norm(x, dim=(2,3))
y_norm = torch.norm(y, dim=(2,3))

for b in range(B):
    for c in range(C): 
        if x_norm[b,c] &gt;= y_norm[b,c]:
            z[b,c] = x[b,c]
        else:
            z[b,c] = y[b,c]
    
</code></pre>
<p>But this method is too slow because of uses the two for loop ...</p>
<p>How can I modify the code to process faster?</p>
","2024-02-07 06:54:42","0","Question"
"77951682","","ONNX Runtime: io_binding.bind_input causing ""no data transfer from DeviceType:1 to DeviceType:0""","<p>I am using Nvidia Triton Inference Server and ONNX model for inference on a GPU instance.
The Dockerfile, containing the environment, inference server and models contains following <code>from/pip</code> lines:</p>
<pre><code>FROM --platform=linux/amd64 nvcr.io/nvidia/tritonserver:23.12-py3

RUN pip install torch transformers onnx onnxruntime-gpu onnxruntime
</code></pre>
<p>the <code>model.py</code> for the Triton Inference Server has been simplified to following:</p>
<pre class=""lang-py prettyprint-override""><code>import onnxruntime as ort
import torch
import numpy as np

session = ort.InferenceSession(&quot;path/to/onnx.model&quot;, providers=[&quot;CUDAExecutionProvider&quot;, &quot;CPUExecutionProvider&quot;])

...

io_binding = session.io_binding()
pt_script_embeddings = torch.rand(
    size=(100, 2010), dtype=torch.float32, device=&quot;cuda:0&quot;
).contiguous()

io_binding.bind_input(
    name=&quot;np_script_embeddings&quot;,
    device_type=&quot;cuda&quot;,
    device_id=0,
    element_type=np.float32,
    shape=tuple(pt_script_embeddings.shape),
    buffer_ptr=pt_script_embeddings.data_ptr(),
)

logit_output_shape = (100, 2)
logit_output = torch.empty(logit_output_shape, dtype=torch.float32, device='cuda:0').contiguous()
io_binding.bind_output(
    name=&quot;np_logits&quot;,
    device_type=&quot;cuda&quot;,
    device_id=0,
    element_type=np.float32,
    shape=tuple(logit_output.shape),
    buffer_ptr=logit_output.data_ptr()
)

session.run_with_iobinding(io_binding)
outputs = logit_output.cpu().numpy()
</code></pre>
<p>Unfortunately, the error below is triggered at the line <code>io_binding.bind_input</code> causing me a lot of grief:</p>
<pre><code>RuntimeError: Error when binding input: There's no data transfer registered for copying tensors from Device:[DeviceType:1 MemoryType:0 DeviceId:0] to Device:[DeviceType:0 MemoryType:0 DeviceId:0]
</code></pre>
<p>Note: articles reviewed before the SO submission:</p>
<ul>
<li><a href=""https://github.com/microsoft/onnxruntime/issues/10286"" rel=""nofollow noreferrer"">https://github.com/microsoft/onnxruntime/issues/10286</a></li>
<li><a href=""https://github.com/microsoft/onnxruntime/issues/10327"" rel=""nofollow noreferrer"">https://github.com/microsoft/onnxruntime/issues/10327</a></li>
<li><a href=""https://onnxruntime.ai/docs/api/python/api_summary.html"" rel=""nofollow noreferrer"">https://onnxruntime.ai/docs/api/python/api_summary.html</a></li>
</ul>
","2024-02-07 01:03:25","1","Question"
"77950487","","How to load a .csv and Image dataset in kaggle?","<p>I've been trying Binary Classification with PyTorch on the competition called SIIM-ISIC Melanoma Classification (<a href=""https://www.kaggle.com/competitions/siim-isic-melanoma-classification"" rel=""nofollow noreferrer"">https://www.kaggle.com/competitions/siim-isic-melanoma-classification</a>) but I've had some problems on how to combine the images and labels. I've been trying to implement a class for loading and merging the images and their labels but for some reason the same error appeared every time I tried to run the line train[5], with 5 or another index:</p>
<p><em>FileNotFoundError: [Errno 2] No such file or directory: '/kaggle/input/siim-isic-melanoma-classification/jpeg/train/ISIC_0074311'</em></p>
<p>I can assure everyone that the paths are correctly copied and that the images are in those folders.</p>
<p><strong>The folders structure is:</strong>
<a href=""https://i.sstatic.net/9HfAi.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><strong>train.csv:</strong>
image_name
patient_id
sex
age_approx
anatom_site_general_challenge
diagnosis
benign_malignant
target (0 or 1)</p>
<p><strong>train folder in jpeg folder:</strong>
ISIC_0015719.jpg
ISIC_0052212.jpg
ISIC_0068279.jpg
ISIC_0074268.jpg
...</p>
<p><strong>My code</strong>:</p>
<pre><code>import os
import pandas as pd
from PIL import Image
import torch
import torch.utils.data
from PIL import Image
</code></pre>
<pre><code>class LoadDataset(torch.utils.data.Dataset):
    
    def __init__(self, csv_path, image_folder, transform = None):
        self.df           = pd.read_csv(csv_path)
        self.image_folder = image_folder
        self.transform    = transform

    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, index):
        filename = self.df.loc[index, 'image_name']
        label    = self.df.loc[index, 'target']
        image    = Image.open(os.path.join(self.image_folder, filename))

        if self.transform is not None:
            image = self.transform(image)
            
        return image, label
</code></pre>
<pre><code>train = LoadDataset('/kaggle/input/images1-isic2020/train.csv', '/kaggle/input/images1-isic2020/train/train')
</code></pre>
<pre><code>train[5]
</code></pre>
<p><strong>Does anyone have any ideas on how to solve it or another option other than an image generator to obtain each image with its label?</strong></p>
","2024-02-06 19:49:57","-1","Question"
"77950239","77949486","","<p>I'm assuming <code>x</code> in this case is your <code>dataset</code> tensor which is of shape <code>(3,1)</code>.</p>
<p>You want to combine the outputs with <code>torch.stack</code>, which stacks tensors together along a new axis.</p>
<pre class=""lang-py prettyprint-override""><code>def brownian_motion(x, t, dt):
    k = int(t / dt)
    path = [x]
    path.append(x)

    for i in range(k):
        xi = torch.normal(torch.zeros(x.shape), torch.ones(x.shape))
        x += np.sqrt(dt) * xi # you may want to replace this line with `x = path[-1] + np.sqrt(dt) * xi`
        path.append(x)
        
    return torch.stack(path).squeeze()

data = torch.normal(0, 1, size = (1, 3))
dataset = torch.tensor(data.T).float()

out = brownian_motion(dataset, 0.02, 0.01)
out.shape
&gt; torch.Size([4, 3])
</code></pre>
<p>To explain the final line:</p>
<ul>
<li>we have <code>path</code> which is a list of tensors of shape <code>(3, 1)</code></li>
<li>we compute <code>torch.stack(path)</code>, which stacks the tensors in <code>path</code> along a new axis, giving a tensor of shape <code>(k+2, 3, 1)</code>. Note that this is <code>k+2</code> rather than your desired <code>k+1</code> because the input <code>x</code> tensor is added to <code>path</code> twice (<code>path = [x]</code>, <code>path.append(x)</code>) - not sure if this is intended or a bug.</li>
<li>we call <code>.squeeze()</code> on the stack result to remove the unit axis at the end, giving a result of size <code>(k+2, 3)</code></li>
</ul>
","2024-02-06 18:58:19","0","Answer"
"77950074","77949486","","<p>It is possible using <a href=""https://pytorch.org/docs/stable/generated/torch.cat.html"" rel=""nofollow noreferrer""><code>torch.cat</code></a>, that concatenates tensor along a dimension. You also do not need to append <code>x</code> to path since it's already initiated with x inside.</p>
<p>In addition, you should not use in-place operators, since your tensors will share the same memory (resulting in a list of tensors with identical values, and you will not be able to track down the step by step motion). Instead, you can copy the tensor or create a new one.</p>
<p>Here is my version of your method returning a <code>(len(dataset), k+1)</code> tensor.</p>
<pre class=""lang-py prettyprint-override""><code>def brownian_motion(x, t, dt):
    k = int(t / dt)
    path = [x]

    for _ in range(k):
        xi = torch.normal(torch.zeros(x.shape), torch.ones(x.shape))
        path.append(path[-1] + numpy.sqrt(dt) * xi)

    return torch.cat(path, dim=-1)
</code></pre>
","2024-02-06 18:29:32","0","Answer"
"77949749","77947467","","<p>The <code>Conv3d</code> function expects a 5 dimensional input of shape <code>(batch_size, channels, D, H, W)</code>. Your inputs have a single channel, so you need to add a unit dimension. Your input should be of shape <code>(batch_size, 1, 193, 229, 193)</code></p>
","2024-02-06 17:33:22","0","Answer"
"77949486","","Pytorch: Appending tensors like a list","<p>I have a dataset with random data like the following:</p>
<pre><code>data = torch.normal(0, 1, size = (1, 3))
dataset = torch.tensor(data.T).float()
print(dataset)
#tensor([[-2.1445],
#        [-1.3322],
#        [-0.6355]])
</code></pre>
<p>And now I want to simulate Brownian motions which are started in the 1-dimensional points given by the dataset up to a certain time <code>t</code> with a step size <code>dt</code>. In the example above, the desired output should look like:</p>
<pre><code>#tensor([[-2.1445, -2.1035, -2.1022],
#        [-1.3322, -1.3121, -1.3210],
#        [-0.6355, -0.6156, -0.5999]])
</code></pre>
<p>for <code>dt = .01</code> and <code>t = .02</code>. That is, the first dimension corresponds to <code>t = 0</code>, the second to <code>t = .01</code>, the third to <code>t = .02</code> and so on in general. Here is what I tried:</p>
<pre><code>def brownian_motion(x, t, dt):
    k = int(t / dt)
    path = [x]
    path.append(x)

    for i in range(k):
        xi = torch.normal(torch.zeros(x.shape), torch.ones(x.shape))
        x += numpy.sqrt(dt) * xi
        path.append(x)
</code></pre>
<p>However, this gives me a list of several tensors of size 3. But what I need want to return is a tensor of size (k + 1, 3) (assuming the dataset size is 3). How can I do that?</p>
","2024-02-06 16:50:06","0","Question"
"77949421","77947396","","<p>Answer is found.
In Keras you should avoid using loops, instead pass whole dataset in <a href=""https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call"" rel=""nofollow noreferrer"">operator()</a>.
In PyTorch you should use tensors for speedup, not numpy.</p>
<p>Btw, PyTorch is still faster than Keras, but only 8 times faster with new fixes.</p>
<p>All the above is demonstated in updated repository, feel free to check out.</p>
","2024-02-06 16:40:04","0","Answer"
"77948430","77921048","","<p>What piece of code do you use to call the <code>model.forward()</code> function?<br />
Could it be that you reinitialize the model after each epoch?<br />
Also, your <code>imgs</code> param is not used in the code you provided.</p>
<p>When I initialize the model and call <code>model.forward()</code> like this...</p>
<pre><code>model = Model()
epochs = 4
for epoch in range(epochs):
  print(f&quot;Epoch {epoch+1}:&quot;)
  model.forward(None)
  print(&quot;--&quot;)
</code></pre>
<p>...I get the following output that you also expect to come: In the first epoch, the ones are added because both tensors contain only zeros. In the following they are not added because the zero values are not equal to the one values.</p>
<pre><code>Epoch 1:
tensor([[0., 0., 0.]])
zeros. 
--
Epoch 2:
tensor([[1., 1., 1.]])
--
Epoch 3:
tensor([[1., 1., 1.]])
--
Epoch 4:
tensor([[1., 1., 1.]])
--
</code></pre>
","2024-02-06 14:22:39","0","Answer"
"77947786","","An error occurred with torch_scatter while executing a graph generation model","<p>I'm having an error while executing the EDGE diffusion model for graph generation. In particular the error is related to <code>torch_scatter</code>. Here the details:</p>
<pre><code>  Traceback (most recent call last):
  File &quot;/Users/turex/Desktop/graph-generation-EDGE/train.py&quot;, line 3, in &lt;module&gt;
    from diffusion.utils import add_parent_path, set_seeds
  File &quot;/Users/turex/Desktop/graph-generation-EDGE/diffusion/__init__.py&quot;, line 2, in &lt;module&gt;
    from .diffusion_base import *
  File &quot;/Users/turex/Desktop/graph-generation-EDGE/diffusion/diffusion_base.py&quot;, line 5, in &lt;module&gt;
    from torch_scatter import scatter
  File &quot;/Users/turex/Desktop/graph-generation-EDGE/myenv/lib/python3.10/site-packages/torch_scatter/__init__.py&quot;, line 16, in &lt;module&gt;
    torch.ops.load_library(spec.origin)
  File &quot;/Users/turex/Desktop/graph-generation-EDGE/myenv/lib/python3.10/site-packages/torch/_ops.py&quot;, line 933, in load_library
    ctypes.CDLL(path)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ctypes/__init__.py&quot;, line 374, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: dlopen(/Users/turex/Desktop/graph-generation-EDGE/myenv/lib/python3.10/site-packages/torch_scatter/_version_cpu.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev
</code></pre>
<p>I've tried to install different versions of both <code>torch</code> and <code>torch_scatter</code>, but I cannot understand the problem.</p>
","2024-02-06 12:42:53","1","Question"
"77947467","","Error: Given groups=1, weight of size [32, 1, 3, 3, 3], expected input[1, 4, 193, 229, 193] to have 1 channels, but got 4 channels instead","<p><code>RuntimeError: Given groups=1, weight of size [32, 1, 3, 3, 3], expected input[1, 4, 193, 229, 193]</code> to have 1 channel, but got 4 channels instead.</p>
<p>Working on 3D Convolutional neural network for classification of Parkinson's disease.</p>
<p>I think this portion of the code has some mistake:</p>
<pre><code># Load the dataset using DataLoader
batch_size = 4

# Load the dataset with batch loading (adjust path as needed)
dataset = CustomDataset(root_dir=r'D:\PD\PD25_16bit', transform=ToTensor())
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define the 3D CNN model
class CNN3D(nn.Module):
    def __init__(self, num_channels=1):
        super(CNN3D, self).__init__()
        self.conv1 = nn.Conv3d(num_channels, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 48 * 57 * 48 // 4, 128)  # Adjust the linear layer input size
        self.fc2 = nn.Linear(128, 3)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, (64 * 48 * 57 * 48 // 4))  # Adjust the view to handle batch size of 4
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
</code></pre>
<p>The dimension of each 3D file(NIFTI) is 193,229,193. When I changed the batch size the model consider it to be number of channel.</p>
","2024-02-06 11:49:15","0","Question"
"77947396","","Keras LSTM is extremely slow in comparison to PyTorch LSTM","<p>I am using CPU for small LSTM model that needs to be realtime.
I encountered an issue with Keras LSTM, the minimal model takes about 70ms to make ONE prediction.
Same model made with PyTorch takes about 0.5ms to make prediction.</p>
<p>It's 130 times difference. Is it a bug or am I doing smth wrong?</p>
<p>I've created a really simple repo to recreate this issue:
<a href=""https://github.com/Al-Garifov/lstm_issue_demo/tree/master"" rel=""nofollow noreferrer"">https://github.com/Al-Garifov/lstm_issue_demo/tree/master</a></p>
<p>I've tried &quot;unroll&quot; option in Keras LSTM = no difference.</p>
","2024-02-06 11:36:22","-2","Question"
"77946335","77944804","","<p>They are referring to the methods only accessible in the forward method. In the scope of the <code>forward</code> method, <code>self</code> is conventionally named <code>ctx</code> and is provided as an <a href=""https://pytorch.org/docs/stable/autograd.html#context-method-mixins"" rel=""nofollow noreferrer"">object mixin</a>. There are <em>four</em> special functions available on that mixin: <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.mark_dirty.html"" rel=""nofollow noreferrer""><code>mark_dirty</code></a>, <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html"" rel=""nofollow noreferrer""><code>mark_non_differentiable</code></a>, <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.save_for_backward.html"" rel=""nofollow noreferrer""><code>save_for_backward</code></a>, and <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html"" rel=""nofollow noreferrer""><code>set_materialize_grads</code></a>.</p>
","2024-02-06 08:46:06","3","Answer"
"77944804","","PyTorch methods that can only be called from `forward`","<p>In the <a href=""https://pytorch.org/docs/stable/notes/extending.html"" rel=""nofollow noreferrer"">docs</a> for extending <code>torch.autograd</code>:</p>
<blockquote>
<p>...please refer to the docs of <a href=""https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function"" rel=""nofollow noreferrer"">Function</a> to find descriptions of
useful methods that can be called only from <code>forward()</code>.</p>
</blockquote>
<p>But following that link, it's not clear to me what they're referring to. Any ideas?</p>
","2024-02-06 01:25:44","1","Question"
"77943300","77939542","","<p>You should read how indexing works in pytorch/numpy and other similar libraries.</p>
<p>You have a tensor of shape <code>(2, 11938)</code>.</p>
<p>When you index with <code>X_train[0:2]</code>, you get a tensor of size <code>(2, 11938)</code>. You don't need to index if you want all the rows.</p>
<p>When you index with <code>X_train[0:2][batch:batch+BATCH_SIZE]</code>, you're indexing the result of <code>X_train[0:2]</code> with <code>[batch:batch+BATCH_SIZE]</code>. This means you are still indexing the first axis.</p>
<p>If you want a chunk of size <code>BATCH_SIZE</code> from the second axis, you need to index the second axis, ie <code>X_train[:, batch:batch+BATCH_SIZE]</code>.</p>
","2024-02-05 18:48:22","2","Answer"
"77943271","77936766","","<p>You can use the <code>map</code> function to compute embeddings and save them in the same dataset</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import DataCollatorWithPadding

collator = DataCollatorWithPadding(tokenizer, padding=True, return_tensors='pt')

def embed(batch):
    inputs = collator({'input_ids' : batch['input_ids']})
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        
    hidden_states = outputs.hidden_states
    embeddings = hidden_states[-1]
    return {'embeddings' : embeddings.detach().cpu()}

ds1 = ds1.map(embed, batched=True, batch_size=4)
</code></pre>
","2024-02-05 18:42:57","0","Answer"
"77940911","77939004","","<p>You need to call <code>weights_initialization</code> method after <code>net</code> object is created. Also, set the <code>bias</code> inside the linear layers to <code>False</code> when manually adding <code>bias</code> values. There were also a few other fixes in the following codeL</p>
<pre><code>import torch
import torch.nn as nn


class NeuralNet(nn.Module):
    def __init__(self):    
        super().__init__()  
        self.fc1 = nn.Linear(in_features=1, out_features=2, bias=False)
        self.output = nn.Linear(in_features=2, out_features=1, bias=False)
        self.bias1 = torch.tensor([[2.]])
        self.bias2 = torch.tensor([[3.]])
        

    def act(self, x):
        return x**2

    def forward(self, x):
        x = self.act(self.fc1(x) + self.bias1)
        x = self.act(self.output(x) + self.bias2)

        return x

    def weights_initialization(self):
        with torch.no_grad():
            self.fc1.weight.copy_(torch.tensor([[0.2],
                                                [0.3]]))
            self.output.weight.copy_(torch.tensor([[1.5, 2.5]]))
    

net = NeuralNet()
net.weights_initialization()
input_data = torch.tensor([[5.]])
output = net(input_data)
print(output)
</code></pre>
","2024-02-05 12:24:22","0","Answer"
"77940162","77939979","","<p>I had the wrong PyTorch version installed. I needed to uninstall PyTorch version 2.2.0+cpu and then used</p>
<p><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121</code></p>
<p>for installation. Now, it successfully detects the CUDA cores.</p>
<p>Reference: <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">PyTorch - Get Started Locally</a></p>
","2024-02-05 10:20:30","2","Answer"
"77939979","","Unable to Detect NVIDIA RTX 4080 Laptop GPU for CUDA in Python","<p><strong>Description:</strong></p>
<p>I'm attempting to utilize my NVIDIA RTX 4080 laptop GPU for training and running a GPT model in Python using PyTorch. However, despite having a GPU, my code consistently evaluates to using the CPU.</p>
<p>Here's a simplified version of my code:</p>
<pre><code>import torch

if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)
else:
    device = torch.device(&quot;cpu&quot;)
</code></pre>
<p>Even after ensuring that my laptop's hybrid graphics mode is set to &quot;Discrete&quot; and rebooting the system, the code still detects and uses the CPU.</p>
<p>Are there any additional steps I should take to make Python recognize and utilize my laptop's GPU for CUDA operations?</p>
<p>Any help or suggestions on how to resolve this issue would be greatly appreciated. Thank you!</p>
<p><strong>Additional Information:</strong></p>
<ul>
<li>Laptop Model: MSI Vector GP68HX 12VH</li>
<li>GPU: NVIDIA GForce RTX 4080 laptop GPU</li>
<li>Operating System: Win 11 Pro</li>
<li>Python Version: 3.12.0</li>
<li>PyTorch Version: 2.2.0+cpu</li>
</ul>
","2024-02-05 09:51:24","0","Question"
"77939542","","How to slice data in pytorch tensor?","<p>I've put my data to the pytorch tensor and now i want to split into the batches size 64. I got the following code:</p>
<pre><code>batch = 0
BATCH_SIZE = 64
X_train = x_scaled.to(device)
y_train = y_scaled.to(device)

for batch in range(0,len(X_train[0]),BATCH_SIZE):
   ### Training
   model.train() # train mode is on by default after construction
   # 1. Forward pass
   y_pred = model(X_train[0:2][batch:batch+BATCH_SIZE])

</code></pre>
<p>The shape of the tensor is: <code>torch.Size([2, 11938])</code>. And i want to slice it into [2,64]. However it do not slice properly and gives an error: <code>mat1 and mat2 shapes cannot be multiplied (2x11938 and 2x64)</code></p>
<p>What i want:</p>
<pre><code>tensor([[0.0000, 0.0002, 0.0004, 0.0005, 0.0007, 0.0009, 0.0011, 0.0013, 0.0014,
    0.0016, 0.0018, 0.0018, 0.0020, 0.0022, 0.0023, 0.0025, 0.0027, 0.0029,
    0.0029, 0.0031, 0.0032, 0.0034, 0.0036, 0.0038, 0.0040, 0.0041, 0.0043,
    0.0045, 0.0047, 0.0049, 0.0051, 0.0052, 0.0054, 0.0056, 0.0058, 0.0060,
    0.0061, 0.0061, 0.0063, 0.0065, 0.0067, 0.0069, 0.0070, 0.0072, 0.0074,
    0.0076, 0.0078, 0.0079, 0.0081, 0.0083, 0.0083, 0.0085, 0.0087, 0.0088,
    0.0090, 0.0092, 0.0094, 0.0094, 0.0096, 0.0097, 0.0099, 0.0101, 0.0103,
    0.0105],[0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684, 0.0684,
    0.0703, 0.0703, 0.0703, 0.0684, 0.0684, 0.0703, 0.0703, 0.0703, 0.0703,
    0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703,
    0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703,
    0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703,
    0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703,
    0.0703, 0.0703, 0.0712, 0.0712, 0.0712, 0.0712, 0.0712, 0.0712, 0.0712,
    0.0712]], device='cuda:0', dtype=torch.float64)
</code></pre>
<p>What i get:</p>
<pre><code>tensor([[0.0000e+00, 1.8038e-04, 3.6076e-04,  ..., 9.9964e-01, 9.9982e-01,
         1.0000e+00],
        [6.8395e-02, 6.8395e-02, 6.8395e-02,  ..., 5.7695e-01, 5.7695e-01,
         5.7695e-01]], device='cuda:0', dtype=torch.float64)
</code></pre>
<p>How can i slice the torch tensor to the requiered shape?</p>
","2024-02-05 08:34:38","2","Question"
"77939004","","manually assigning weights, biases and activation function in pytorch","<p>I'm trying to build a neural network with nn.module in pytorch. I want to implement custom weights, biases and activation function.
with input value=5 and first layer weights= [[0.2, 0.3]] and second layer weights= [[1.5],[2.5]], first layer bias= 2 and second layer bias=3 and activation function y=x^2 the output value should obtain 2220.765625 but my code doesn't calculate this value. could you please help me to correct this code?</p>
<pre><code>import torch
import torch.nn as nn


class NeuralNet(nn.Module):
    def __init__(self):    
        super().__init__()  
        self.fc1 = nn.Linear(in_features=1, out_features=2)
        self.output = nn.Linear(in_features=2, out_features=1)
        self.bias1 = torch.tensor([[2.]])
        self.bias2 = torch.tensor([[3.]])
        

    def act(self, x):
        return x**2

    def forward(self, x):
        x = self.act(self.fc1(x)) + self.bias1 
        x = self.act(self.output(x)) + self.bias2

        return x

    def weights_initialization(self):
        with torch.no_grad():
            self.fc1.weight.copy_(torch.tensor([[0.2, 0.3]]))
            self.output.weight.copy_(torch.tensor([[1.5],
                                                   [2.5]]))
 

net = NeuralNet()
input_data = torch.tensor([[5.]])
output = net(input_data)
print(output)
</code></pre>
","2024-02-05 06:34:09","-1","Question"
"77938218","77937339","","<p>I got it guys! I needed to wrap my inputs and labels!! Here is the part of the code that's missing:</p>
<pre><code>for inputs, labels in zip(x_train, y_labels[:train_split]):                                                                                                                                                          
    inputs = torch.tensor([[inputs]])  # Need to wrap inpu
    labels = torch.tensor([[labels]])  # Need to wrap labe
    y_pred = model(inputs)
</code></pre>
","2024-02-05 01:16:30","2","Answer"
"77937339","","PyTorch ArrayRef invalid index problem with linear nn","<p>I have a headache and following code, that doesn't let me sleep at night:</p>
<pre><code>1 import torch
  2 import torch.nn as nn
  3 import torch.optim as optim
  4 import numpy as np
  6 samples = torch.linspace(0, 100,100) # GENERATE THE SET
  7 train_split = int(len(samples)*0.8)
  8 x_train, x_test = samples[:train_split], samples[train_split:]
  9 y_labels = 2*samples-4 # define the function
 10 y_labels += torch.tensor(np.random.normal(0, 5, len(samples))) # ADD NOISE
 13 class NeuralNetwork(nn.Module):
 14     def __init__(self):
 15         super().__init__()
 16         self.fc1 = nn.Linear(1, 1)
 17     def forward(self, x):
 18         return self.fc1(x)
 19 model = NeuralNetwork()
 20 loss_func = nn.MSELoss()
 21 optimizer = optim.Adam(model.parameters(), lr=0.001)
 22 num_epochs = 50
 23 for epoch in range(num_epochs):
 24     for inputs, labels in zip(x_train, y_labels[:train_split]):                                                                                                                                         
 25      
 26         y_pred = model(inputs) # File &quot;.../torch/nn/modules/linear.py&quot;, line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: ArrayRef: invalid index Index = 18446744073709551615; Length = 0
 27         loss = loss_func(y_pred, labels)
 28         optimizer.zero_grad()
 29         loss.backward()
 30         optimizer.step()
 31         print(&quot;Epoch %d  - loss: %.4f%&quot; % (epoch, loss))


</code></pre>
<p>It is a simple one layer thing, and I'm doing it intiuitevly so no need to call me a goof.
I run it on couple machines, no difference...</p>
<p>P.S. ANY SUGGESTIONS ON FURTHER IMPROVEMENTS IN THE PROCESS OF WRITING THOSE BEASTS WOULD BE APPRECIATED!!!</p>
","2024-02-04 19:40:19","1","Question"
"77936949","77932146","","<p>Rather than implementing this via a <code>for</code> loop, you should use JAX's built-in <a href=""https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html"" rel=""nofollow noreferrer""><code>scatter</code></a> operator. The most convenient interface for this is the <a href=""https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"" rel=""nofollow noreferrer""><code>Array.at</code></a> syntax. If I understand your goal correctly, it might look something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import jax.numpy as jnp
import numpy as np

# Generate some data
num_batches = 4
n = 10
d = 3
x = np.random.randn(n, d)
ind = np.random.randint(low=0, high=num_batches, size=(n,))

#Compute the result with jax.lax.scatter
result = jnp.zeros((num_batches, d)).at[ind].add(x)
print(result.shape)
# (4, 3)
</code></pre>
","2024-02-04 17:50:39","1","Answer"
"77936766","","Mapping embeddings to labels in PyTorch/Huggingface","<p>I am currently working on a project where I am using a pre-trained transformer model to generate embeddings for DNA sequences (some have a '1' label and some have a '0' label). I'm trying to map these embeddings back to their corresponding labels in my dataset, but I'm encountering an IndexError when attempting to do so. I think it has to do with the fact that I am batching since I'm running out of memory.</p>
<p>Here is the code I'm working with:</p>
<pre><code>from datasets import Dataset
from transformers import AutoTokenizer, AutoModel
import torch

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;)
model = AutoModel.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;)

# Load the dataset
ds1 = Dataset.from_file('training.arrow') #this is already tokenized

# Convert tokenized sequences to tensor
inputs = torch.tensor(ds1['input_ids']).to(torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;))

# Reduce batch size
batch_size = 4

# Pass tokenized sequences through the model with reduced batch size
with torch.no_grad():
    outputs = model(input_ids=inputs[:batch_size], output_hidden_states=True)

# Extract embeddings
hidden_states = outputs.hidden_states
embeddings1 = hidden_states[-1]

</code></pre>
<p>Here is the information about the size of the output embeddings and the original dataset:</p>
<pre><code>embeddings1.shape
torch.Size([4, 86, 1280])


ds1
Dataset({
    features: ['labels', 'input_ids', 'attention_mask'],
    num_rows: 22535512
})
</code></pre>
<p>I'm having a hard time figuring out how to map the labels back to the output embeddings, especially with the big discrepancy with the sizes. As you can see, I have 22million sequences, I would like a an embedding for each sequence.</p>
<p>My plan is to use these embeddings for downstream prediction using another model.
I have already split my data into train, test, and val, but does it make more sense to get the embeddings for a label1 dataset and label0 dataset and then combine and then split into train/test, so I don't have to worry about the mapping of the labels?</p>
","2024-02-04 17:06:58","0","Question"
"77934105","","Modifying Vision Transformer (ViT) Model in timm for Custom Head in PyTorch","<p>I am working with the Vision Transformer (ViT) model using PyTorch and the timm library. My goal is to modify the ViT model to replace the default classification head with a custom head that takes the mean of all tokens and adds a new classification layer.</p>
<p>deafult summary of the ViT model in timm is ending like this:</p>
<pre><code>       LayerNorm-247             [-1, 197, 768]           1,536
        Identity-248                  [-1, 768]               0
         Dropout-249                  [-1, 768]               0
          Linear-250                 [-1, 1000]         769,000
VisionTransformer-251                 [-1, 1000]               0

</code></pre>
<p>To remove the last layers so far I have coded:</p>
<pre><code>class VisionTransformerWithoutHead(nn.Module):
    
    def __init__(self, model_name):
        super(VisionTransformerWithoutHead, self).__init__()

        # Load the ViT model
        vit_model = timm.create_model(model_name, pretrained=True)

        # Remove the final layers
        self.features = nn.Sequential(*list(vit_model.children())[:-1])

    def forward(self, x):
        # Forward pass through the modified model
        output = self.features(x)
        return output
</code></pre>
<p>Summary is now ending with:</p>
<pre><code>       LayerNorm-247             [-1, 196, 768]           1,536
        Identity-248             [-1, 196, 768]               0
         Dropout-249             [-1, 196, 768]               0
</code></pre>
<p>It reduced the number of the tokens 197 to 196 and it seems to remove the class token.I would like to understand why this is happening and if there's a way to remove only the last layers while preserving the class token.</p>
","2024-02-03 23:22:43","0","Question"
"77933951","77933852","","<p>Yes, you are right, unfortunately, there is no such an option.</p>
<p>I think upgrading pytorch-CUDA from 11.0 to 11.2 is less critical then downgrading the <code>pytorch</code> version. In the first case it is minor version change <code>11.0 -&gt; 11.2</code>, in a last - it's (almost) major: <code>1.10.0 -&gt; 1.9.1</code></p>
","2024-02-03 22:21:34","0","Answer"
"77933852","","Installing Torchaudio for PyTorch 1.10.0 with CUDA 11.0","<p>On my Ubuntu 18.04 machine I have a virtual environment that contains <code>pytorch=1.10.0=cuda110py38hf84197b_0</code>. My CUDA version is 11.0, which I've checked by running <code>nvidia-smi</code>. I would like to install torchaudio.</p>
<p>I've attempted this using <code>mamba install torchaudio=0.10.0 -c pytorch</code>. However, this tries to upgrade the cuda build of my pytorch from 11.0 to 11.2. Similarly, if I try installing torchaudio=0.9.1, mamba wants to downgrade my pytorch version from 1.10.0 to 1.9.1.</p>
<p>This is an old machine with several-year-old code that relies on these old packages. Ideally, I would be able to install torchaudio with modifying as few packages/cuda drivers as possible. Is there a way to install torchaudio for pytorch 1.10.0 cuda build 11.0? I've checked the <a href=""https://pytorch.org/get-started/previous-versions/"" rel=""nofollow noreferrer"">official pytorch releases page</a> and it seems like this option isn't even listed (so maybe I've already answered my own question, but I'm trying here as well).</p>
<p>For reference, here is a basic environment containing my pytorch-necessary packages. Thank you.</p>
<pre><code>name: torchbase
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - _libgcc_mutex=0.1=conda_forge
  - _openmp_mutex=4.5=1_llvm
  - bzip2=1.0.8=h7f98852_4
  - ca-certificates=2023.11.17=hbcca054_0
  - cffi=1.15.0=py38h3931269_0
  - cudatoolkit=11.0.3=h15472ef_9
  - cudnn=8.2.1.32=h86fa8c9_0
  - future=0.18.2=py38h578d9bd_4
  - icu=68.2=h9c3ff4c_0
  - ld_impl_linux-64=2.36.1=hea4e1c9_2
  - libblas=3.9.0=12_linux64_mkl
  - libcblas=3.9.0=12_linux64_mkl
  - libffi=3.4.2=h7f98852_5
  - libgcc-ng=11.2.0=h1d223b6_11
  - libiconv=1.16=h516909a_0
  - liblapack=3.9.0=12_linux64_mkl
  - libnsl=2.0.0=h7f98852_0
  - libprotobuf=3.18.1=h780b84a_0
  - libstdcxx-ng=11.2.0=he4da1e4_11
  - libuuid=2.32.1=h7f98852_1000
  - libxml2=2.9.12=h72842e0_0
  - libzlib=1.2.11=h36c2ea0_1013
  - llvm-openmp=12.0.1=h4bd325d_1
  - magma=2.5.4=h4a2bb80_2
  - mkl=2021.4.0=h8d4b97c_729
  - nccl=2.11.4.1=h96e36e3_0
  - ncurses=6.2=h58526e2_4
  - ninja=1.10.2=h4bd325d_1
  - numpy=1.22.3=py38h99721a1_2
  - openssl=1.1.1o=h166bdaf_0
  - pip=23.3.2=pyhd8ed1ab_0
  - pycparser=2.21=pyhd8ed1ab_0
  - python=3.8.12=hb7a2778_2_cpython
  - python_abi=3.8=2_cp38
  - pytorch=1.10.0=cuda110py38hf84197b_0
  - pytorch-gpu=1.10.0=cuda110py38h5b0ac8e_0
  - readline=8.1=h46c0cb4_0
  - setuptools=60.1.1=py38h578d9bd_0
  - sleef=3.5.1=h9b69904_2
  - tbb=2021.5.0=h4bd325d_0
  - tk=8.6.11=h27826a3_1
  - typing_extensions=4.0.1=pyha770c72_0
  - wheel=0.37.1=pyhd8ed1ab_0
  - xz=5.2.5=h516909a_1
  - zlib=1.2.11=h36c2ea0_1013
  - zstd=1.5.1=ha95c52a_0
</code></pre>
","2024-02-03 21:43:23","0","Question"
"77933348","77933173","","<p>You're doing a lot of weird stuff here that breaks the gradient chain.</p>
<p>When you call <code>.item()</code> on a tensor, you convert it to a scalar python value, which removes any gradient tracking.</p>
<p>Adding <code>tensor.requires_grad_(True)</code> only tracks gradients from that point onward. Since you computed that tensor by calling <code>.item()</code> at various points, there's no way for the gradient to propagate back.</p>
<p>Also the gradient isn't computed until you call <code>.backward()</code> on your loss, so no matter what your statement <code>print(&quot;Gradient of tensor:&quot;, tensor.grad)</code> in the forward pass will always return <code>None</code>.</p>
<p>Overall, it looks like your model is doing feature engineering within the model itself (ie the <code>salary</code> and <code>debt</code> calculations) in addition to the actual modeling part. You want to separate these.</p>
<p>An additional minor point, it looks like your model is creating a fixed length vector (<code>tensor = torch.tensor([x[6], x[7], x[8], x[10], x[11], x[12], self.mathematical_modeling(x).item()])</code>). If that is the case, there is no need to use a RNN, you can just use a MLP.</p>
","2024-02-03 19:00:32","0","Answer"
"77933183","77911898","","<p>If you have multiple CUDA versions installed, assuming that the newer one (12.0) is a default one. And you need to run a script that depends on old CUDA (11.8) version, simply set/update the following env vars before starting your python script:</p>
<pre class=""lang-bash prettyprint-override""><code>#!/bin/bash

export CUDA_PATH=/usr/local/cuda-11.8
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-11.8/lib64
export PATH=${PATH}:/usr/local/cuda-11.8/bin

python my_py-cuda118_script.py
</code></pre>
","2024-02-03 18:04:31","0","Answer"
"77933173","","Why isn't all the weight of the model updated?","<p>I'm making a model that predicts grades. It is a model that calculates with RNN using the value calculated through mathematical modeling. However, from the weight used for mathematical modeling to the weight of RNN, everything has not been updated. As a result of checking, the gradients of each weight are all being output as None.</p>
<p>I wonder why the weight isn't being updated.</p>
<p>This code is modified.</p>
<pre><code>class Expector(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers):
    super(Expector, self).__init__()

    self.weight_0 = nn.Parameter(torch.tensor([0.2]))
    self.weight_1 = nn.Parameter(torch.tensor([0.8, 0.7, 0.6, 0.5])) #homeownership weight
    self.weight_2 = nn.Parameter(torch.tensor([0.5 for i in range(0, 13)])) #loan_purpose weight

    #interesst_rate_expector
    self.num_features = 13
    self.linear_1 = torch.nn.Linear(self.num_features, self.num_features*2)
    self.linear_2 = torch.nn.Linear(self.num_features*2, self.num_features*4)
    self.linear_3 = torch.nn.Linear(self.num_features*4, self.num_features*8)
    self.linear_out = torch.nn.Linear(self.num_features*8, 1)

    #loan_rating_expector
    self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
    self.fc = nn.Linear(hidden_size, input_size)

  def interest_rate_expector(self, x):
    x = [x[i] for i in range(len(x)-1)]
    for i in [0,4,9,10,11]:
      if x[i] &gt; 0:
        x[i] = math.log(x[i])
    input = torch.tensor(x).to(DEVICE)
    out1 = self.linear_1(input)
    out1 = torch.nn.functional.softplus(out1)
    out2 = self.linear_2(out1)
    out2 = torch.nn.functional.softplus(out2)
    out3 = self.linear_3(out2)
    out3 = torch.nn.functional.softplus(out3)
    logits = self.linear_out(out3)
    interest_rate = torch.sigmoid(logits)

    return interest_rate

  def mathematical_modeling(self, x):
    salary = x[4] / 12 if x[4] != 0 else 300000
    debt = x[4] * (1 / x[5]) if x[5] != 0 else 0
  
    interest_rate = float(self.interest_rate_expector(x))

    if x[2] &gt; 4:
        repayment = (x[0] * interest_rate * ((1 + interest_rate) ** x[1])) / ((1 + interest_rate) ** x[1]) - 1
        result = (debt * self.weight_0[0] + repayment) / (salary * x[3])
    else:
        result = (debt * self.weight_0[0] + x[0] * x[1]) / (salary * x[3])

    return torch.tensor(result)

  def forward(self, x):
    x[3] = torch.matmul(one_hot_encoding1(x[3]), self.weight_1.view(-1, 1))
    x[7] = torch.matmul(one_hot_encoding2(x[7]), self.weight_2.view(-1, 1))

    tensor = torch.tensor([x[6], x[7], x[8], x[10], x[11], x[12], self.mathematical_modeling(x)])

    tensor = tensor.unsqueeze(0)
    output, _ = self.rnn(tensor.to(DEVICE))
    final_output = self.fc(output)
    output_probabilities = F.softmax(final_output, dim=1)
    
    return output_probabilities
</code></pre>
<ol>
<li>I have explicitly set requests_grad=True.</li>
<li>The mathematical_modeling function attempted to return the tensor.</li>
<li>I've adjusted the learning rate.</li>
</ol>
<p>Despite these efforts, the gradient is not updated, and when I check, I only see the phrase Gradient of sensor: None. Please help me.</p>
","2024-02-03 18:02:18","0","Question"
"77932146","","Jax Implementation of function similar to Torch's 'Scatter'","<p>For graph learning purposes, I am trying to implement a global sum batching function, that takes as inputs batched graph representations 'x' of size (n x d) and a corresponding vector of batches (n x 1). I then want to compute the sum over all graph representations for each batch. Here is a graphical representation: <a href=""https://i.sstatic.net/4f1pX.png"" rel=""nofollow noreferrer"">torch's scatter function</a></p>
<p>This is my current attempt:</p>
<pre><code>def global_sum_pool(x, batch):
    graph_reps = []
    i = 0
    n = jnp.max(batch)
    while True:
        ind = jnp.where(batch == i, True, False).reshape(-1, 1)
        ind = jnp.tile(ind, x.shape[1])
        x_ind = jnp.where(ind == True, x, 0.0)
        graph_reps.append(jnp.sum(x_ind, axis=0))
        if i == n:
            break
        i += 1
    return jnp.array(graph_reps)
</code></pre>
<p>I get the following exception on the line <code>if i == n</code>:</p>
<pre><code>jax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..
The error occurred while tracing the function make_step at /venvs/jax_env/lib/python3.11/site-packages/equinox/_jit.py:37 for jit. 
</code></pre>
<p>I understand this is due to the fact that at compile time, Jax does not a priori know the max value of the 'batch' array and hence cannot allocate memory. Does anyone know a workaround or different implementation?</p>
","2024-02-03 12:53:17","1","Question"
"77932011","77928493","","<p>A YOLOv8 model can be converted to the TF.js format as comes from the official Ultralytics <a href=""https://docs.ultralytics.com/modes/export/?h=export"" rel=""nofollow noreferrer"">docs</a>:</p>
<pre><code>from ultralytics import YOLO

model = YOLO('best.pt')
model.export(format='tfjs', imgsz=640, half=False, int8=False)
</code></pre>
<p>However, the TensorFlow version required for this process can be an issue. The current <a href=""https://github.com/ultralytics/ultralytics/issues/5161#issuecomment-1817762594"" rel=""nofollow noreferrer"">working solution</a> (at least for Google Colab) is to re-install it as follows:</p>
<pre><code>!pip install tensorflow==2.13.0rc0
</code></pre>
<p>And then run the model export. The content of the resulting <code>/yolov8n_web_model</code> folder:</p>
<pre><code>/content/yolov8n_web_model

group1-shard1of4.bin
group1-shard2of4.bin
group1-shard3of4.bin
group1-shard4of4.bin
metadata.yaml
model.json
</code></pre>
","2024-02-03 12:09:31","0","Answer"
"77931982","","Using torch.compile twice on a model on the same machine, is there a cache of optimized operations?","<p>I'm using torch.compile to compile a torch model by:</p>
<pre><code>self.model = torch.load(saved_model_path, map_location=self.device).to(self.device)
self.model.eval()

self.model.half()

# Configure hidet to use tensor core and enable tuning
hidet.torch.dynamo_config.use_tensor_core(True)
hidet.torch.dynamo_config.search_space(2)
self.model = torch.compile(self.model, backend=&quot;hidet&quot;)
</code></pre>
<p>I ran this on a remote machine a first time. Lots of optimization operations were being skipped because they weren't supported:</p>
<pre><code>[2024-02-03 12:48:24,408] torch._dynamo.convert_frame: [WARNING]     raise NotImplementedError(&quot;\n&quot;.join(lines))
[2024-02-03 12:48:24,408] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='hidet' raised:
[2024-02-03 12:48:24,408] torch._dynamo.convert_frame: [WARNING] NotImplementedError: The following modules/functions are not supported by hidet yet:
[2024-02-03 12:48:24,408] torch._dynamo.convert_frame: [WARNING]   torch.nn.AvgPool3d
</code></pre>
<p>but many others were. After a while, the optimizations were done and inference was performed.</p>
<p>Then, I ran it again on the same machine, but I only saw the warnings regarding the skipped operations. Why is that? Is there some sort of cache, so that optimizations are not performed again for the same model?</p>
","2024-02-03 11:57:16","0","Question"
"77931502","77924304","","<p>The reason of this problem is lib d2l has already loaded in my PyTorch environment, but this d2l lib has no attribute 'train_ch3', so I solved this problem by downloading the source code, and covered the original d2l lib code in my PyTorch environment.</p>
","2024-02-03 09:13:04","0","Answer"
"77930518","77918545","","<p>Answering my own question, I found a workaround involving some brute force where we try to connect repeatedly. This took a few tries, but it finally worked. I still do not know why this is happening, but for now, this temporary fix enables the model to be downloaded to the local PyTorch Cache. The code is as follows:</p>
<pre><code># DEFINE THE MODEL
# our model will predict the bounding boxes and class scores for each object in the image
# We will use a pre-trained Faster R-CNN model available from torchvision
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import time

# load a model pre-trained on COCO
# Since I get a connection reset error, I have to implement the following workaround
def retry_open_connection():
    max_retries = 5
    retries = 0
    while retries &lt; max_retries:
        try:
            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
            return model
        except Exception as e:
            print(f&quot;Error: {e}&quot;)
            print(&quot;Retrying...&quot;)
            retries += 1
            time.sleep(1)  # Wait for 1 second before retrying

model = retry_open_connection()
</code></pre>
","2024-02-03 00:49:35","0","Answer"
"77930298","77929684","","<p>There isn't enough information in the question to determine, but here is my best guess.</p>
<p>It looks like your input is a batch of items of shape <code>(32, 1)</code>, which is sent to a <code>(32, 32)</code> linear layer. You need the input size of the linear layer to match the number of features in the input, which in this case is <code>1</code>. Your layer should be <code>nn.Linear(1, 32)</code>.</p>
<p>The matmul happens between the last dimension of the input and the first dimension of the linear layer. The first dimension of the input is the batch size, which is irrelevant to the matmul operation.</p>
","2024-02-02 23:13:15","1","Answer"
"77929684","","Why does pytorch give an error related to matrix multiplication?","<p>I have the following class for a neural network:</p>
<pre><code>class NN_test(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden1 = nn.Linear(32, 32)
        self.act1 = nn.ReLU()
        self.hidden2 = nn.Linear(32, 8)
        self.act2 = nn.ReLU()
        self.output = nn.Linear(8, 1)
        self.act_output = nn.Sigmoid()
    def forward(self, x):
        x = self.act1(self.hidden1(x))
        x = self.act2(self.hidden2(x))
        x = self.act_output(self.output(x))
        return x

model = NN_test()
model = model.to(torch.float64)
</code></pre>
<p>Also the optimizer and the loss counting function, as well as the learning cycle inside which I split the batch into 32 pieces and transfer them to the neural network.</p>
<pre><code># Create loss function
loss_fn = nn.L1Loss()

# Create optimizer
optimizer = torch.optim.SGD(params=model.parameters(), # optimize newly created model's parameters
                            lr=0.01)
torch.manual_seed(42)

BATCH_SIZE = 32

# Set the number of epochs 
epochs = 1000 
# Put data on the available device
# Without this, error will happen (not all model/data on device)
X_train = x_scaled.to(device)
X_test = x_scaled_test.to(device)
y_train = y_scaled.to(device)
y_test = y_scaled_test.to(device)



for epoch in range(epochs):
    for batch in range(0,len(X_train),BATCH_SIZE):
        ### Training
        model.train() # train mode is on by default after construction

        # 1. Forward pass
        y_pred = model(X_train[batch:batch+BATCH_SIZE])

        # 2. Calculate loss
        loss = loss_fn(y_pred, y_train[batch:batch+BATCH_SIZE])

        # 3. Zero grad optimizer
        optimizer.zero_grad()

        # 4. Loss backward
        loss.backward()

        # 5. Step the optimizer
        optimizer.step()

        ### Testing
        model.eval() # put the model in evaluation mode for testing (inference)
        # 1. Forward pass
        with torch.inference_mode():
            test_pred = model(X_test[batch:batch+BATCH_SIZE])
        
            # 2. Calculate the loss
            test_loss = loss_fn(test_pred, y_test[batch:batch+BATCH_SIZE])

        if epoch % 100 == 0:
            print(f&quot;Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}&quot;)
</code></pre>
<p>This code gives the following error:
<code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x1 and 32x32)</code></p>
<p>Yet i'm pretty sure that 32x1 matrix is multipliable by 32x32 matrix, so what's going on here.</p>
","2024-02-02 20:26:24","0","Question"
"77928493","","How to use/convert best.pt model of YOLOv8s to js","<p>I trained a YOLOv8s model on custom dataset in google colab, I have the best.pt file and I want to make the model work on web app using javascript, I saw it can be done using TensorFlow.js but I havn't seen how to do it.</p>
<p>Can someone please refrence me to a source that explain how can I convert it to something that can be run on the browser using javascript. Thank you.</p>
","2024-02-02 16:26:18","0","Question"
"77926901","77924496","","<p>You have either</p>
<ul>
<li>not installed torch in the environment you thought you had</li>
<li>or you are not using the environment you think you use.</li>
</ul>
<p>I made you a screenshot where you can follow the very same steps to activate an run torch, maybe it helps you to find the error?</p>
<p><a href=""https://i.sstatic.net/0Prh8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0Prh8.png"" alt=""enter image description here"" /></a></p>
","2024-02-02 12:15:45","0","Answer"
"77924644","77924521","","<p>This can be done provided the offset/chunksize is the same for all rows.</p>
<pre class=""lang-py prettyprint-override""><code>def index_function(data, start_index, chunksize, dim):
    index_tensor = torch.stack([torch.arange(i, i+chunksize) for i in start_index])
    result = data.gather(dim, index_tensor)
    return result

data = torch.tensor([[ 1.,  2.,  3.,  4.,  5.],
                     [ 6.,  7.,  8.,  9., 10.],
                     [11., 12., 13., 14., 15.]])

start_idx = torch.tensor([0, 3, 1]) # start index tensor must be int, not float
index_function(data, start_idx, 2, 1)

&gt;tensor([[ 1.,  2.],
         [ 9., 10.],
         [12., 13.]])

start_idx = torch.tensor([0, 2, 1])
index_function(data, start_idx, 3, 1)

&gt;tensor([[ 1.,  2.,  3.],
         [ 8.,  9., 10.],
         [12., 13., 14.]])
</code></pre>
","2024-02-02 04:14:47","2","Answer"
"77924521","","Selecting different ranges of values from each row in a 2D Tensor","<p>Let's say there's a 2D Tensor:</p>
<pre class=""lang-py prettyprint-override""><code>data = tensor([[ 1.,  2.,  3.,  4.,  5.],
               [ 6.,  7.,  8.,  9., 10.],
               [11., 12., 13., 14., 15.]])
</code></pre>
<p>There are also two 1D Tensors for start and end indices of values in each row to be selected:</p>
<pre class=""lang-py prettyprint-override""><code>start = tensor([0., 3., 1.])
end = start + 2               # end is always at a +2 offset from start
</code></pre>
<p>Is there a way to select <code>start[i]: end[i]</code> for the <code>i</code>th row in <code>data</code> that does <em>not</em> involve iterating over <code>data</code>?
For the above example, the expected output is:</p>
<pre><code>tensor([[ 1.,  2.],
        [ 9., 10.],
        [12., 13.]])
</code></pre>
","2024-02-02 03:24:57","1","Question"
"77924496","","Installed torch in conda env and activated the env but got No module named torch","<p>I successfully installed torch in conda env, but when I'm tring to use torch, I got <code>No module named torch</code></p>
<p>I used <code>pip list | grep torch</code>, and got:</p>
<ul>
<li>torch              2.1.1</li>
<li>torchaudio         2.1.1</li>
<li>torchvision        0.16.1</li>
</ul>
<p>and also, I deactivated the conda env and then activated it again to make sure I use the env.
But when I tried to <code>import torch</code>, I still got <code>No module named torch</code>.</p>
<p>Could anyone can help to solve this problem?</p>
","2024-02-02 03:18:18","0","Question"
"77924304","","AttributeError: module 'd2l.torch' has no attribute 'train_ch3'","<p>I encountered an issue while working on a deep learning code in Python. Specifically, when trying to call the 'train_ch3' function from the d2l.torch module, I encountered an AttributeError. I have confirmed that the 'train_ch3' function is indeed present in my d2l.torch module, but for some reason, it cannot be found in my code.</p>
<p>Here is how I import the d2l.torch module in my code:</p>
<pre><code>import torch
from torch import nn
from d2l import torch as d2l
</code></pre>
<p>Here is the error message (by VSCode):</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[16], line 2
      1 num_epochs = 10
----&gt; 2 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)

AttributeError: module 'd2l.torch' has no attribute 'train_ch3'
</code></pre>
<p>Here is my project directory:
<a href=""https://i.sstatic.net/gwKNs.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I attempted to use the 'train_ch3' function from the 'd2l.torch' module in my Python deep learning code. I expected the function to be recognized and executed without issues since I confirmed its existence in the module. However, I encountered an AttributeError. Despite ensuring the correct version and file path, the problem persists. I expected a successful invocation of 'train_ch3' without any attribute errors, but the actual result was an AttributeError indicating that the module 'd2l.torch' has no attribute 'train_ch3'.</p>
","2024-02-02 01:54:07","0","Question"
"77923434","77923078","","<p>I think you're confusing masked language modeling (MLM) with causal language modeling (CLM).</p>
<p>MLM is used for models like BERT. For MLM training, a percentage of input tokens are replaced with [MASK] tokens, and the model predicts the true value of the [MASK] tokens using the other tokens in the sequence.</p>
<p>CLM is used for autoregressive models like GPT. CLM doesn't mask input tokens (ie there is no [MASK] token at all), but instead uses masking in attention layers. Attention masking prevents tokens from attending to subsequent tokens, forcing each token to only use information from previous tokens.</p>
<p>For your use case of generating sequences, you want to use a CLM approach.</p>
<p>For inference, you necessarily need to predict one token at a time, but you can use KV caching to speed things up and reduce redundant computation.</p>
","2024-02-01 21:19:15","1","Answer"
"77923078","","Training torch.TransformerDecoder with causal mask","<p>I use torch.TransformerDecoder to generate a sequence, where each next token depends on itself and first 2 tokens [CLS] and first predicted one.
So, steps of execution on inference, that i need:</p>
<ol>
<li>Start from sequence [CLS], [MASK], add positional embeddings and generate first predict, taking into consideration itself and [CLS] token (just causal mask)<a href=""https://i.sstatic.net/IUwrJ.png"" rel=""nofollow noreferrer"">enter image description here</a></li>
<li>Put few (i know the number based on first predict) [MASK] tokens to sequence [CLS], [PRED1].</li>
<li>Suppose have sequence [CLS], [PRED1], [MASK], [MASK], [MASK], [MASK].</li>
<li>Add position embeddings to them and compute tokens based on [CLS], [PRED1] and itself, so i suppose that attention mask should look like: <a href=""https://i.sstatic.net/kHFx7.png"" rel=""nofollow noreferrer"">enter image description here</a></li>
<li>Do predict. Doing it, i see that [PRED1] is correct, one class after is also correct (output of first [MASK] after [PRED1]) and all other masks are just repeated of last predicted (output of first [MASK]). Look like this: [PRED1], [PRED2], [PRED2], [PRED2], [PRED2].</li>
</ol>
<p>Steps of sequential proces:</p>
<ol>
<li>the same</li>
<li>put only one mask token</li>
<li>predict, so we get sequence [CLS], [PRED1], [PRED2]</li>
<li>only then add new mask token [CLS], [PRED1], [PRED2], [MASK] and than do predict -&gt; [CLS], [PRED1], [PRED2], [PRED3], where 3 is correct.</li>
<li>So if i repeat till the end it works.</li>
</ol>
<p>But i need parallel processing of all prediction starting from the second.</p>
<p>How it was trained:</p>
<ol>
<li>In Dataset i get [CLS], [PRED1], [PRED2], [PRED3], [MASK]; also mask of what i going to predict masked_mask = [0, 0, 0, 0, 1] - the last one; and mask of what i see (it's always two first positions 1 + masked_mask) mask = [1, 1, 0, 0, 0] + masked_mask = [1, 1, 0, 0, 1]. (also i add paddings for batch training but it's also masked in mask by zero)</li>
<li>In model i add positional embeddings to the input sequence and than i build causal_mask for prediction. i do it this way: cusal_mask = torch.tril(mask.unsqueeze(-1) * mask.unsqueeze(-2)) == 0
<a href=""https://i.sstatic.net/W3cJI.png"" rel=""nofollow noreferrer"">enter image description here</a>
so for my example this mask is:
<a href=""https://i.sstatic.net/r67bA.png"" rel=""nofollow noreferrer"">enter image description here</a></li>
</ol>
<p>It's not convinient to show the code, that's why i tried to explain it. If there are some assumptions why it's not working, thanks!</p>
<p>i don't know what to do</p>
","2024-02-01 20:08:33","1","Question"
"77922995","77920451","","<p>As per <a href=""https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"" rel=""nofollow noreferrer"">its documentation</a>, <code>torch.compile()</code> uses JIT (just in time) compilation based on the provided data the function should run on. This is then cached and reused if another call of the &quot;compiled&quot; function is compatible with the already compiled one. So there is no real way to force compilation without explicitly calling the &quot;compiled&quot; function with similar data as the compilation depends on the shape/type of the data.</p>
<p>Regarding your second function. <code>test2</code> does not return anything but only changes the referenced numpy-array. Afaik, this is not the intended use for <code>torch.compile()</code>. Data may be copied to the CUDA, which would not change a referenced numpy-array that is copied. I assume this leads to the <code>compile()</code> function not really compiling <code>test2</code>.</p>
","2024-02-01 19:53:21","1","Answer"
"77921740","77910481","","<p>The short answer is yes absolutely they do/should, so it should be more a question of figuring out where in the process something is mis-configured...</p>
<p>Let's check some external factors first:</p>
<ul>
<li>On the client side, I'm always suspicious of Python for threading/multiprocessing to be honest... You have checked by <code>print()</code>ing <strong>before</strong> <code>requests.post()</code> that this test setup actually <em>initiates</em> the API requests concurrently, right?</li>
<li>API Gateway seems a very unlikely culprit - processing API requests at scale is what it does</li>
<li>Your test looks quite short - I wonder is the Lambda Function scaling quickly enough to process requests concurrently before it finishes? I'd maybe check you don't have a Lambda concurrency limit =1 set or something? Maybe can just stub out the Lambda SageMaker calls with a <code>sleep()</code> to check whether this is behaving as expected.</li>
</ul>
<p>If those are eliminated and your SageMaker endpoint itself is definitely serializing requests, then it's time to dig deeper. To re-configure your endpoint's serving stack, you'll probably need to know what it's using:</p>
<p>If you <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models"" rel=""nofollow noreferrer"">deployed a custom PyTorch model</a> for the first time, you're probably using the <a href=""https://github.com/aws/deep-learning-containers/blob/master/available_images.md#sagemaker-framework-containers-sm-support-only"" rel=""nofollow noreferrer"">AWS DLC for SageMaker PyTorch Inference</a> which is based on <a href=""https://pytorch.org/serve/"" rel=""nofollow noreferrer"">TorchServe</a> under the hood. If you're using the <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html"" rel=""nofollow noreferrer"">SageMaker Hugging Face bindings</a> it might be the Hugging Face DLC of which at least several versions are based on <a href=""https://github.com/awslabs/multi-model-server"" rel=""nofollow noreferrer"">AWS MMS server</a> (not 100% sure about recent versions). If you tuned/deployed a model from <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html"" rel=""nofollow noreferrer"">SageMaker JumpStart</a> or did some other custom thing, it might even be <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/triton.html"" rel=""nofollow noreferrer"">Triton Inference Server</a> or <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html"" rel=""nofollow noreferrer"">DJL Serving</a>.</p>
<p>If you're not sure, try to track down your model's container image URI to figure out which: Find your <code>Model</code> in the <a href=""https://console.aws.amazon.com/sagemaker/home?region=#/models"" rel=""nofollow noreferrer"">SageMaker &gt; Inference &gt; Models console</a> and you should be able to see it. If you don't know which <code>Model</code> it is, can go through SageMaker &gt; Inference &gt; Endpoints first and follow the trail through from Endpoint to Endpoint Configuration to Model.</p>
<p>Once the serving stack is identified, relevant questions might include:</p>
<ul>
<li>Is the front-end web server configured with enough workers/threads to handle incoming requests concurrently?</li>
<li>Is the back-end inference side configured with enough workers/threads to process inferences concurrently?</li>
<li>Is there any explicit request batching set up to concat concurrent requests into mini-batches for more efficient communication with the GPU(s)? This comes with a low-volume latency penalty (wait for XYZms after first req comes in, for any others to batch it with) so may be disabled by default.</li>
</ul>
<p><code>p3.2xlarge</code> is a single-GPU machine and I see <a href=""https://pytorch.org/serve/configuration.html#other-properties"" rel=""nofollow noreferrer"">TorchServe configs default</a> to <code>nGPUs</code> worker per model - so maybe that's your issue and adding an environment variable of <code>TS_DEFAULT_WORKERS_PER_MODEL</code> of (say) 2 might resolve it? But I believe that'd load extra copies of your model into GPU memory... So might be better to configure inference batching instead?</p>
<p>So, long story short, it should work but if something is blocking it within your SageMaker endpoint itself - then might require digging deeper into the (mostly open-source) containerized serving infrastructure that you're using?</p>
<p>You might find some more help in the AWS ML blogs on:</p>
<ul>
<li><a href=""https://aws.amazon.com/blogs/machine-learning/benchmark-and-optimize-endpoint-deployment-in-amazon-sagemaker-jumpstart/"" rel=""nofollow noreferrer"">Benchmark and optimize endpoint deployment in Amazon SageMaker JumpStart</a></li>
<li><a href=""https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/"" rel=""nofollow noreferrer"">Best practices for load testing Amazon SageMaker real-time inference endpoints</a></li>
</ul>
","2024-02-01 16:10:45","0","Answer"
"77921048","","How to use the input of first iter to init variable in module?","<p>I want to use something related to input to init the property in my module, before first iter it is init to zeros in <strong>init</strong>()</p>
<pre><code>class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

        self.test_var = torch.zeros((1,3))

    def forward(self, imgs):
  
        print(self.test_var)
        if torch.equal( self.test_var, torch.zeros(self.test_var.shape)):
            print('zeros. ')
            var = torch.tensor([1,1,1])
            self.test_var = var + self.test_var
</code></pre>
<p>It should change the value of test_var to [1,1,1] after 1st iter, but I only find the output is
<a href=""https://i.sstatic.net/EAR6r.png"" rel=""nofollow noreferrer"">enter image description here</a>
It means the change of test_var is invalid, and it is set to zero every iter.
I'm confused.
I guess maybe something wrong with <strong>DataParalle</strong>l, because when I set  CUDA_VISIEBLE_DEVICES=0, the question can be fixed.</p>
","2024-02-01 14:38:14","0","Question"
"77920892","77918117","","<p>It seems you missed move inputs to device. Also, after the process done you have to move your data to cpu again to predict part, <a href=""https://numpy.org/doc/stable/user/basics.interoperability.html"" rel=""nofollow noreferrer"">since numpy only supports CPU</a></p>
<pre><code>import numpy as np
import torch.nn as nn
import torch

x_values = [i for i in range(11)]
x_train = np.array(x_values,dtype=np.float32)
x_train = x_train.reshape(-1,1)


y_values = [2*i+1 for i in x_values]
y_train = np.array(y_values,dtype=np.float32)
y_train = y_train.reshape(-1,1)


class MyLinearRegressionModel(nn.Module):
    def __init__(self,input_dim,output_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim,output_dim)

    def forward(self,x):
        out = self.linear(x)
        return out
input_dim = 1
output_dim = 1

model = MyLinearRegressionModel(input_dim,output_dim)
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)
print(device)

criterion = nn.MSELoss()
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)
epochs = 1000

for epoch in range(epochs):
    epoch += 1
    inputs = torch.from_numpy(x_train).to(device)
    labels = torch.from_numpy(y_train).to(device)
    optimizer.zero_grad()
    
    ## Addition 1: Moved inputs to device here
    inputs = inputs.to(device)
    
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    
    if epoch % 50 == 0:
        print(&quot;epoch %d, loss %f&quot; % (epoch, loss))

## First converts numpy array to torch tensor and moved to device, after your process data moved again on cpu (since numpy only supports cpu) and convert to np array again
predict = model(torch.from_numpy(x_train).to(device).requires_grad_()).data.cpu().numpy()
print(predict)
</code></pre>
","2024-02-01 14:15:36","0","Answer"
"77920451","","How to log compile errors from torch.compile","<p>I'm new to torch and I try to speedup some mathematical computations using <code>torch.compile</code>.
I'm using pytorch: <strong>pytorch 2.2.0 py3.11_cpu_0</strong> from Anaconda.
The end goal is to parallelize my code on a GPU or TPU, but for starters I compile it to a C-Kernel.</p>
<p>Given the following self contained example in python:</p>
<pre><code>import numpy as np
import torch


def test(foo, bar):
    return np.count_nonzero(foo) + np.count_nonzero(bar)

def test2(foo, bar):
    for clause in foo:
        for i in range(0, bar.shape[1]):
            res = np.logical_xor(clause, bar[i])
            if np.count_nonzero(res) == 1:
                bar[i] |= res
    return bar


foo = np.random.choice([True]*5 + [False]*5, size=(10,10))
bar = np.random.choice([True] + [False]*10, size=(10,10))

torch._logging.set_logs(output_code=True)

compiled = torch.compile(test)
compiled(foo, bar)
</code></pre>
<p>This code works and produces a logfile with translated python code and a C Kernel.
But if I replace the compile call to
<code>compiled = torch.compile(test2)</code>
to call my other function, it does not log anything.</p>
<p>How can I check the compiled code or even assure that compile did something?</p>
<p>Furthermore, the compilation seems to be triggered lazily and only when I execute the compiled function. How can I force to compile it earlier?</p>
","2024-02-01 13:10:38","1","Question"
"77919957","77919632","","<p>To acquire all you need you have to go over the whole tensor. The most efficient should therefore be to use <code>argsort</code> afterwards limited to <code>n</code> entries.</p>
<pre><code>&gt;&gt;&gt; x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])
&gt;&gt;&gt; x.argsort(dim=0, descending=True)[:n]
[2, 4, 0, 5]
</code></pre>
<p>Sort it again to get <code>[0, 2, 4, 5]</code> if you need the ascending order of indices.</p>
","2024-02-01 11:50:30","4","Answer"
"77919632","","How to find the indexes of the first $n$ maximum values of a tensor?","<p>I know that <code>torch.argmax(x, dim = 0)</code> returns the index of the first maximum value in <code>x</code> along dimension <code>0</code>. But is there an efficient way to return the indexes of the first <code>n</code> maximum values? If there are duplicate values I also want the index of those among the <code>n</code> indexes.</p>
<p>As a concrete example, say <code>x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])</code>. I would like a function</p>
<pre><code>generalized_argmax(xI torch.tensor, n: int)
</code></pre>
<p>such that
<code>generalized_argmax(x, 4)</code>
returns <code>[0, 2, 4, 5]</code> in this example.</p>
","2024-02-01 11:05:23","2","Question"
"77918545","","URLError: <urlopen error [WinError 10054] An existing connection was forcibly closed by the remote host>","<p>I'm currently following along the PyTorch Tutorial <a href=""https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"" rel=""nofollow noreferrer"">here</a>. As I call fasterrcnn_resnet50_fpn(), I'm getting the following error:</p>
<p>URLError: &lt;urlopen error [WinError 10054] An existing connection was forcibly closed by the remote host&gt;</p>
<p>I first checked the website<a href=""https://download.pytorch.org/models/resnet50-0676ba61.pth"" rel=""nofollow noreferrer"">text</a> where the call was trying to download the weights from and indeed there's a connection reset error. I ran windows diagnostics and found that the remote device won't accept the connection. I tried downloading the weights manually and loading the weight path directly but I couldn't get the call to stop downloading the weights from the website.</p>
<pre><code># load a model pre-trained on COCO
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights_path='./fasterrcnn_resnet50_fpn_coco-258fb6c6.pth', )

# replace the classifier with a new one, that has num classes which is 2 
num_classes = 2 # 1 class (person) + background

# get number of input features for the classifier
in_features = model.roi_heads.box_predictor.cls_score.in_features

# replace the pre-trained head with a new one
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
</code></pre>
","2024-02-01 08:07:46","0","Question"
"77918213","77916868","","<p>Basically, with vectorizing, the idea is to add any of the iterables as an additional dimension of your vector (using <code>view</code>) to utilize broadcasting, and then reshape (<code>view</code>) back to your original dimensions.</p>
<pre><code>input_reshaped = input.view(num_samples, 1, input.size(1), input.size(2), input.size(3))
doutput_reshaped = doutput.view(num_samples, 1, doutput.size(1), doutput.size(2), doutput.size(3))

temp_conv2d = torch.nn.functional.conv2d(input_reshaped, doutput_reshaped, stride=dilation, padding=padding, dilation=stride, groups=groups).squeeze()

cut_conv2d = temp_conv2d[:, :, :kernel_size, :kernel_size].permute(0, 2, 3, 1)
grad_w += cut_conv2d.sum(dim=0)
</code></pre>
","2024-02-01 07:01:13","0","Answer"
"77918117","","THE following code error : Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu","<p>The data and model are both on GPU, How can i change code to aviod this error?</p>
<h1>Here is the code!!!</h1>
<pre><code>import numpy as np
import torch.nn as nn
import torch

x_values = [i for i in range(11)]
x_train = np.array(x_values,dtype=np.float32)
x_train = x_train.reshape(-1,1)


y_values = [2*i+1 for i in x_values]
y_train = np.array(y_values,dtype=np.float32)
y_train = y_train.reshape(-1,1)


class MyLinearRegressionModel(nn.Module):
    def __init__(self,input_dim,output_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim,output_dim)

    def forward(self,x):
        out = self.linear(x)
        return out
input_dim = 1
output_dim = 1

model = MyLinearRegressionModel(input_dim,output_dim)
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
**model.to(device)**
print(device)

criterion = nn.MSELoss()   # 指定损失函数
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)
epochs = 1000

for epoch in range(epochs):
    epoch +=1
  **  inputs = torch.from_numpy(x_train).to(device)
    labels = torch.from_numpy(y_train).to(device)**
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs,labels)
    loss.backward()
    optimizer.step()
    if epoch % 50 == 0:
        print(&quot;epoch %d ,loss %f&quot; % (epoch ,loss))


predict = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()
print(predict)

</code></pre>
<p>When i ran these code above ,it gave this error, Please help change code to solve this problem.</p>
<p>&quot;RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!&quot;</p>
","2024-02-01 06:38:33","0","Question"
"77917867","77916678","","<p>Like suggested in the comments, you may need to play around a bit with your hyperparameters: Learning Rate, Optimizer Choice, Kernel parameters, etc. In regards to how many epochs to train for: There is no magic number; a suggestion will be to track your training and validation loss curves to monitor the training.</p>
<p>When training loss keeps decreasing but your validation loss starts increasing, it is usually a sign of overfitting and an indication to stop training or tinker with learning rate.</p>
","2024-02-01 05:22:51","0","Answer"
"77917516","77884413","","<p>I get it. Because onnx file is bigger than 2G.
But another question , if onnx model larger than 2G, Then how to do ?</p>
","2024-02-01 03:07:38","0","Answer"
"77917184","77912311","","<p>So you could have a process where you generate a set of weights, then use the <code>torch.nn.functional.conv1d</code> function to apply the convolution using those weights. But I see an issue with this approach.</p>
<p>For a standard <code>conv1d</code> operation, your input will be of shape <code>(batch_size, in_channels, i*W)</code> and your weight will be of shape <code>(out_channels, in_channels, k*W)</code>. The issue is your input has a batch dimension, but your weights don't.</p>
<p>Your desired approach, as you describe it, would want a different set of weights for each <code>(x,y)</code> vector pair in the batch.</p>
<p>Given that all convolutions are a strict subset of matrix-vector multiplication, you're probably better off just concatenating <code>[x,y]</code> and putting them through a linear layer.</p>
","2024-02-01 00:51:42","0","Answer"
"77917036","77916868","","<p>Figured it out. Not sure if it's possible to remove the last for loop, but this already speeds it up 100x for me, so good enough.</p>
<pre class=""lang-py prettyprint-override""><code>for s in range(num_samples):
            temp_input = input[s, :, :, :].unsqueeze(1)
            temp_doutput = doutput[s, :, :, :].unsqueeze(1)
            
            temp_conv2d = torch.nn.functional.conv2d(temp_input, temp_doutput, stride=dilation, padding=padding, dilation=stride, groups=groups).squeeze()
            cut_conv2d = torch.permute((temp_conv2d[:, :, :kernel_size, :kernel_size]), (1, 0, 2, 3))
            grad_w[:, :, :, :] += cut_conv2d
</code></pre>
","2024-01-31 23:55:31","0","Answer"
"77916868","","How can I vectorize my custom pytorch conv2d operation?","<p>I have a backwards pass of a custom conv2d layer that I think can be vectorized.</p>
<pre class=""lang-py prettyprint-override""><code>for s in range(num_samples):
    for c in range(num_channels):
        temp_input = input[s, c, :, :].unsqueeze(0).unsqueeze(0)
        temp_doutput = doutput[s, :, :, :].unsqueeze(1)
        temp_conv2d = torch.nn.functional.conv2d(temp_input, temp_doutput, stride=dilation, padding=padding, dilation=stride, groups=groups).squeeze(0).squeeze(0)
        cut_conv2d = temp_conv2d[:, :kernel_size, :kernel_size]
        grad_w[:, c, :, :] += cut_conv2d
</code></pre>
<p>The reason I think it can be vectorized is that if it wasn't vectorized, with an input of 50000 images of size 512x512, with the current configuration, it would have to perform billions of conv2d functions for just a single epoch. I'm guessing that's not right, but I can't think of a way to vectorise this any more.</p>
","2024-01-31 23:01:33","1","Question"
"77916678","","Why would the loss of my neural network not be decreasing? What is wrong with my current set up? (PyTorch)","<p>I am sort of new to deep learning and have created neural nets for CIFAR-10 and MNIST datasets. I wanted to try a larger dataset with a different end goal, so I chose the Country211 dataset from Pytorch. I created the neural net (I used three convolutional layers because the input for flattening would have been massive otherwise), but it seems like the loss printed barely decreases. Am I just not running it long enough? Or is there something fundamentally wrong with my NN?</p>
<p>My model is below:</p>
<pre><code>import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Use CUDA device
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

print(device)

# Define transform to Normalize images (input is PIL image)
transform = transforms.Compose([transforms.ToTensor(), 
                                transforms.Resize((300, 300)),
                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = 64

# Define training and test set
trainset = torchvision.datasets.Country211(root='./data', split='train',
                                        transform=transform, download=True)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=0)

testset = torchvision.datasets.Country211(root='./data', split='test',
                                          download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                        shuffle=False, num_workers=0)

# classes = ()

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 8, 5)
        self.pool = nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(8, 12, 5)
        self.conv3 = nn.Conv2d(12, 16, 5)
        self.fc1 = nn.Linear(16 * 34 * 34, 4096)
        self.fc2 = nn.Linear(4096, 1024)
        self.fc3 = nn.Linear(1024, 211)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x
</code></pre>
<p>My training loop is below:</p>
<pre><code>net = Net()
net.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)
epochs = 6

for epoch in range(10):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0
</code></pre>
<p>My output of loss is here:</p>
<pre><code>[1,  2000] loss: 5.352
[1,  4000] loss: 5.351
[1,  6000] loss: 5.350
[2,  2000] loss: 5.322
[2,  4000] loss: 5.320
[3,  2000] loss: 5.276
[3,  4000] loss: 5.272
[3,  6000] loss: 5.258
[4,  2000] loss: 5.211
[4,  4000] loss: 5.197
[4,  6000] loss: 5.212
[5,  4000] loss: 5.114
[5,  6000] loss: 5.140
</code></pre>
<p>Thank you in advance for any help!</p>
","2024-01-31 22:07:02","0","Question"
"77915461","77912919","","<p>The dataloader is created from the dataset, which is created by the k-fold split.</p>
<p>What you're asking for - <code>I want to use the dataloaders defined outside the fold loop</code> - doesn't make sense. The dataloaders have a fixed dataset split. Using k-fold requires you to create different splits. If you want to do k-fold cross validation, you have to create different dataset splits.</p>
<p>Pseudocode below:</p>
<pre class=""lang-py prettyprint-override""><code>dataset = ...

for k in range(n_folds):
    train_dataset, valid_dataset = split_dataset(dataset)

    train_dataloader = ... # create from train_dataset
    valid_dataloader = ... # create from valid_dataset

    train_epoch(train_dataloader, valid_dataloader, ...)
</code></pre>
","2024-01-31 17:50:53","1","Answer"
"77914099","77910806","","<p>To make the DDP code work when running in a notebook, you must include:</p>
<p><code>%%writefile ddp.py</code> at the top of the DDP code.
To run the code, and train the model, in another cell call:
<code>!python -W ignore ddp.py</code></p>
","2024-01-31 14:25:19","0","Answer"
"77912919","","use k-fold cross validation with pytorch","<p>I am training to add k-fold cross validation to my script ,after reading some documentations it says that the training loop should be inside the fold loop
but what I didn't understande is the that the dataloaders should be inside the the fold loop also but in my case it's not
so If I want to use the dataloaders defined outside the fold loop and call them from the inside
how can I do that ?
these are the functions
def get_train_utils(opt, model_parameters):</p>
<pre><code>            data augmentation 
             ...........
   train_loader = torch.utils.data.DataLoader(train_data,
                                           batch_size=opt.batch_size,
                                           shuffle=(train_sampler is None),
                                           num_workers=opt.n_threads,
                                           pin_memory=True,
                                           sampler=train_sampler,
                                           worker_init_fn=worker_init_fn)
return return (train_loader, train_sampler, train_logger, train_batch_logger,
        optimizer, scheduler)
</code></pre>
<p>and</p>
<pre><code>def get_val_utils(opt):
   data augmentation 
   ........
   val_loader = torch.utils.data.DataLoader(val_data,
                                         batch_size=(opt.batch_size //
                                                     opt.n_val_samples),
                                         shuffle=False,
                                         num_workers=opt.n_threads,
                                         pin_memory=True,
                                         sampler=val_sampler,
                                         worker_init_fn=worker_init_fn,
                                         collate_fn=collate_fn)
 return val_loader, val_logger
</code></pre>
<p>and the training and validation loop are defined in another function</p>
<pre><code>def main_worker(index, opt):
 other code 
     
if not opt.no_train:
    (train_loader, train_sampler, train_logger, train_batch_logger,
     optimizer, scheduler) = get_train_utils(opt, parameters)
    if opt.resume_path is not None:
        opt.begin_epoch, optimizer, scheduler = resume_train_utils(
            opt.resume_path, opt.begin_epoch, optimizer, scheduler)
        if opt.overwrite_milestones:
            scheduler.milestones = opt.multistep_milestones
if not opt.no_val:
    val_loader, val_logger = get_val_utils(opt)

if opt.tensorboard and opt.is_master_node:
    from torch.utils.tensorboard import SummaryWriter
    if opt.begin_epoch == 1:
        tb_writer = SummaryWriter(log_dir=opt.result_path)
    else:
        tb_writer = SummaryWriter(log_dir=opt.result_path,
                                  purge_step=opt.begin_epoch)
else:
    tb_writer = None

prev_val_loss = None

   for i in range(opt.begin_epoch, opt.n_epochs + 1):
    if not opt.no_train:
        if opt.distributed:
            train_sampler.set_epoch(i)
        current_lr = get_lr(optimizer)
        train_epoch(i, train_loader, model, criterion, optimizer,# 
                    opt.device, current_lr, train_logger,
                    train_batch_logger, tb_writer, opt.distributed)
      
        if i % opt.checkpoint == 0 and opt.is_master_node:
            save_file_path = opt.result_path / 'save_{}.pth'.format(i)
            save_checkpoint(save_file_path, i, opt.arch, model, optimizer,
                            scheduler)


    if not opt.no_val:
      prev_val_loss = val_epoch(i, val_loader, model, criterion,#
                                  opt.device, val_logger, tb_writer,
                                  opt.distributed)
    
    if not opt.no_train and opt.lr_scheduler == 'multistep':
        scheduler.step()
    elif not opt.no_train and opt.lr_scheduler == 'plateau':
        scheduler.step(prev_val_loss)
</code></pre>
","2024-01-31 11:23:10","0","Question"
"77912547","","How can I set CMAKE_PREFIX_PATH in visual studio 2022?","<p>I want to use Pytorch c++ in Visual studio 2022 ,that is why I need to set CMAKE_PREFIX_PATH in Visual studio ,but I can't it.I tried to do it following the tutorial <a href=""https://pytorch.org/cppdocs/installing.html"" rel=""nofollow noreferrer"">https://pytorch.org/cppdocs/installing.html</a> ,but i failed .For some reason it doesn't work.I haven't errors , but my cmake doesn't see the way to libtorch</p>
<p>I want gain the opportunity use Pytorch c++</p>
","2024-01-31 10:27:03","1","Question"
"77912311","","pytorch convolutional layer with weights being a learnable function of the input","<p>I would like to define a pytorch model where some of the layers are convolutional layers, whose weights are an explicit function of the input (with learnable parameters).</p>
<p>Example: let's say we have 1 channel and 1 minibatch of 1.
Then given a 1-dimensional input vector <code>x</code> of size n, I can pass it into a 1-dimensional convolutional layer <code>nn.Conv1d(1, 1, k, bias = False)</code> to obtain an output vector of size n-k+1. If I wrap my convolution layer into a model, the model has now k learnable parameters (the weights of the 1-dimensional convolutional layer).</p>
<p>Now let's say that my input is actually a pair of 1-dimensional vectors <code>(x, y)</code> where <code>y</code> has size m.
I would like y to &quot;control&quot; the weights of the convolutional layer applied to x, in the sense that the weights of the convolutional layer are the vector <code>A * y</code> where A is a k-by-m matrix of learnable parameters.</p>
<p>Problem: I am confused on how to define the <code>nn.Module</code> class to satisfy the above requirements.
Let's say I start with a very simple model which just applies a 1-dimensional convolutional layer:</p>
<pre><code>import torch
import torch.nn.functional as F

class Conv1dModel(torch.nn.Module):
    def __init__(self, kernel_size):
        super(Conv1dModel, self).__init__()
        self.conv1d = torch.nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = kernel_size)

    def forward(self, x):
        output = self.conv1d(x)
        return output
</code></pre>
<p>Then I would like to modify the weight of the convolutional layer, which I can access with <code>self.conv1d.weights</code>.
But here's my confusion: I think that I can (should?) only modify module parameters (those access via <code>self.xyz</code>) in the initialization method <code>__init__</code>, but on the other hand in order to set <code>weights = Linear(m,k)(y)</code> I need the input vector y, which is not available at initialization time.</p>
<p>How do I solve this conundrum?</p>
","2024-01-31 09:51:11","0","Question"
"77912013","77908932","","<p>It seems that PyTorch does the <code>shared_axis</code> operation as default. We can check with the following code:
TensorFlow:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
x = tf.keras.layers.PReLU(shared_axes=(1,2))
x.build((None, 10, 10, 3))  # initialize the weights to an image-like input shape
print(x.weights[0].shape)  # will print 'TensorShape([1, 1, 3])'

# if we omit the shared_axes parameter we get:
x = tf.keras.layers.PReLU()
x.build((None, 10, 10, 3))  # initialize the weights to an image-like input shape
print(x.weights[0].shape)  # will print 'TensorShape([10, 10, 3])'
</code></pre>
<p>PyTorch:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn
x = nn.modules.activation.PReLU()
print(x.weight.shape)  # will print 'torch.Size([1])'

# to get one parameter per filter we have to set it
x = nn.modules.activation.PReLU(3)
print(x.weight.shape)  # will print 'torch.Size([3])'
</code></pre>
<p>Hope that helps to understand the differences. You can also initialize the torch version with a list to get the second version of the TF code.<br />
One additional difference is that TensorFlow initializes with <code>0.0</code>, while PyTorch initializes with <code>0.25</code>. To get the exact same initialization as TensorFlow, do:</p>
<pre class=""lang-py prettyprint-override""><code>f = #number of filters of the previous layer
x = nn.modules.activation.PReLU(f, init=0.0)
</code></pre>
","2024-01-31 09:03:16","1","Answer"
"77911898","","Pytorch reports CUDA version mismatches","<p>I have both 11.8 and 12.0 CUDA version. But when I run the code, there comes the following error:</p>
<pre><code>RuntimeError:
The detected CUDA version (12.0) mismatches the version that was used to compile
PyTorch (11.8). Please make sure to use the same CUDA versions.
</code></pre>
<p>How can I solve it??</p>
<p>I have checked if CUDA 11.8 successfully installed using cmd command, and it turned out to be ok. Also, the system path is correctly set.</p>
","2024-01-31 08:43:05","0","Question"
"77910806","","Weird PyTorch Multiprocessing Error Where Main Loop Is Not Defined In __main__ | Kaggle","<p>The following PyTorch code for single-node multi-GPU training with DDP seen here:</p>
<p><a href=""https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-series/multigpu.py"" rel=""nofollow noreferrer"">https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-series/multigpu.py</a></p>
<p>Gives the following error when running in a Kaggle environment with two GPU T4 accelerators:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/opt/conda/lib/python3.10/multiprocessing/spawn.py&quot;, line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File &quot;/opt/conda/lib/python3.10/multiprocessing/spawn.py&quot;, line 126, in _main
    self = reduction.pickle.load(from_parent)
AttributeError: Can't get attribute 'main' on &lt;module '__main__' (built-in)&gt;
Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/opt/conda/lib/python3.10/multiprocessing/spawn.py&quot;, line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File &quot;/opt/conda/lib/python3.10/multiprocessing/spawn.py&quot;, line 126, in _main
    self = reduction.pickle.load(from_parent)
AttributeError: Can't get attribute 'main' on &lt;module '__main__' (built-in)&gt;
---------------------------------------------------------------------------
ProcessExitedException                    Traceback (most recent call last)
Cell In[11], line 104
     95 if __name__ == &quot;__main__&quot;:
     96 #     import argparse
     97 #     parser = argparse.ArgumentParser(description='simple distributed training job')
   (...)
    100 #     parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')
    101 #     args = parser.parse_args()
    103     world_size = torch.cuda.device_count()
--&gt; 104     mp.spawn(main, args=(world_size, 5, 10, 32), nprocs=world_size)
</code></pre>
<p>Any Information is appreciated.</p>
","2024-01-31 04:24:49","1","Question"
"77910543","77910031","","<p>The <code>==</code> operator is always element-wise, even when broadcasting. You can get your desired result by aggregating with <code>all</code> along the last axis.</p>
<pre class=""lang-py prettyprint-override""><code>a = torch.tensor([[1,2],[1,2],[1,3]]) 
b = torch.tensor([1,2])

output1 = a == b
&gt;tensor([[ True,  True],
         [ True,  True],
         [ True, False]])

output2 = output1.all(-1)
&gt;tensor([ True,  True, False])
</code></pre>
<p>Also if it matters for your use case, you can use reductions like <code>all</code> with <code>keepdim=True</code> to retain the number of dims.</p>
<pre class=""lang-py prettyprint-override""><code>output2 = output1.all(-1) # shape (3,)
&gt;tensor([ True,  True, False])

output2 = output1.all(-1, keepdim=True) # shape (3, 1)
&gt;tensor([[ True],
         [ True],
         [False]])
</code></pre>
","2024-01-31 02:45:05","0","Answer"
"77910481","","AWS sagemaker endpoints dont accept concurrent calls?","<p>I have a pytorch model deployed on p3.2xlarge, connected to lambda and API gateway.</p>
<p>When multiple requests are sent to it, based on logs, it accepts them sequentially.
There is no concurrency, but Shouldn't endpoints be able to handle like 200 calls concurrently?</p>
<p>Do we need to set something up?</p>
<p>Please let me know.</p>
<p>This is a sample concurrent call:</p>
<pre><code>import threading
import requests

def send_request():
    data = {&quot;image&quot;: encoded_image}
    response = requests.post(&quot;https://idr263lxri.something&quot;, json=data)
    print(response.text)

threads = []
for _ in range(8):
    thread = threading.Thread(target=send_request)
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join()
</code></pre>
<p>I tried various concurrent calling and they are handled sequentially.</p>
<p>I used various concurrency calls.</p>
","2024-01-31 02:23:31","1","Question"
"77910031","","How do I compare 2-d tensor with 1-d tensor in Pytorch?","<p>Example of what I want to compare these two:</p>
<pre><code>torch.tensor([[1,2],[1,2],[1,3]]) == torch.tensor([1,2])
</code></pre>
<p>I want this output:</p>
<pre><code>[True, True, False]
</code></pre>
<p>But instead the broadcasting gets me:</p>
<pre><code>tensor([[ True,  True],
        [ True,  True],
        [ True, False]])
</code></pre>
","2024-01-30 23:42:49","0","Question"
"77909561","77900830","","<p>It's hard to say why it might be in the tutorial, but usually you have <code>(batch_number, **your_data)</code> shape as input in your network, output in case of classification usually has <code>(batch_number, number_of_classes)</code>, and you're right that in that case you should use dim=1(or recommended way use even dim=-1 because you can have more complicated output, for example - <code>(batch_number, some_more_data, ..., number_of_classes)</code> ) to get model confidence along dim which sum to 1, but sometimes in architecture of deep network might reshape dimension of the data for some purpose then you can check in what dimension <code>number_of_classes</code> is</p>
<p>and other part of question the difference between of argmax and softmax is that first one returns the confidences along number of classes, the second one returns the one class index with the highest confidence for each sample, usually you apply <code>softmax</code> and then <code>argmax</code> in order to get final class index</p>
<p>Hope it helps</p>
","2024-01-30 21:39:35","1","Answer"
"77909164","77906649","","<p>The first embeddings (input + position) are the first layer of the model. These embeddings are used to map tokens to vectors.</p>
<p>The second set of embeddings (<code>encoder_last_hidden_state</code>) are the outputs of the final layer in the model's encoder.</p>
<p>These embeddings are supposed to be different.</p>
","2024-01-30 20:13:06","0","Answer"
"77909142","77908384","","<p>I don't know about your specific error, but your tensors are the wrong shape for the <code>conv2d</code> function.</p>
<p>From the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html"" rel=""nofollow noreferrer"">documentation</a>, <code>conv2d</code> expects the following tensor shapes:</p>
<ul>
<li><code>input</code> - <code>(batch_size, in_channels, i*H, i*W)</code></li>
<li><code>weight</code> - <code>(out_channels, in_channels / groups, k*H, k*W)</code></li>
<li><code>bias</code> - None or <code>(out_channels)</code></li>
<li><code>stride</code> - integer or tuple of shape <code>(s*H, s*W)</code></li>
<li><code>padding</code> - string value, number, or tuple of shape <code>(pad*H, pad*W)</code></li>
<li><code>dilation</code> - integer or tuple <code>(d*H, d*W)</code></li>
<li><code>groups</code> - integer</li>
</ul>
<p>Your tensors have the shape:</p>
<ul>
<li><code>input</code> - <code>(2, 5, 9)</code> (missing a dimension)</li>
<li><code>weight</code> - <code>(2, 3, 7)</code> (missing a dimension</li>
<li><code>bias</code> - <code>(2,)</code> (correct number of dimensions)</li>
<li><code>stride</code> - <code>[1]</code> - should be an integer or a tuple of two values</li>
<li><code>padding</code> - <code>[0]</code> - should be a string, integer or tuple of two values</li>
<li><code>dilation</code> - <code>[1]</code> - should be a integer or a tuple of two values</li>
</ul>
<p><code>conv2d</code> work as expected when the correct tensor shapes are passed</p>
<pre class=""lang-py prettyprint-override""><code>batch_size = 2
in_channels = 8
out_channels = 16
input_size = 32
kernel_size = 3
stride = 1 # or (1, 1)
padding = 0 # or (0, 0)
dilation = 1 # (1, 1)

torch.nn.functional.conv2d(
        input=torch.randn(batch_size, in_channels, input_size, input_size), 
        weight=torch.randn(out_channels, in_channels, kernel_size, kernel_size), 
        bias=torch.randn(out_channels),
        stride=stride, 
        padding=padding, 
        dilation=dilation
                        )
</code></pre>
<p>The error message comes from <a href=""https://github.com/pytorch/pytorch/blob/d0627cc2af41dbfccf4d74b8ef9fcebc2a89bb78/torch/csrc/lazy/core/shape_inference.cpp#L85"" rel=""nofollow noreferrer"">shape_inference.cpp</a>. My guess is the incorrect input shapes are causing something to be dispatched to the wrong shape checker.</p>
","2024-01-30 20:08:18","0","Answer"
"77908932","","TensorFlow to PyTorch: x=tf.keras.layers.PReLU(shared_axes=[1,2])(x)","<p>How to write this line on PyTorch?
tf.keras.layers.PReLU(shared_axes=[1,2])(x)</p>
<p>It is important to have shared_axes=[1,2]</p>
<p>I found this solution somewhere, but Imnot sure if it is correct:
nn.PReLU(num_parameters=1)</p>
<p>Thanks in advance everyone!</p>
","2024-01-30 19:25:18","0","Question"
"77908384","","I think I found a bug in torch.nn.functional.conv2d()","<p>conv2d() returns an error saying that output_padding is wrong, yet conv2d() doesn't have an output_padding attribute.
Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>my_input = tensor([[[ 1.42e+00,  2.63e+00,  1.38e+00,  8.38e-01, -2.40e-01,  3.47e-01,
           9.39e-01, -1.70e+00,  9.41e-01],
         [ 1.48e+00,  1.59e+00,  8.10e-01, -5.31e-01, -3.23e+00,  3.47e+00,
           5.13e+00,  8.14e+00,  1.18e+00],
         [ 6.23e-01,  4.88e+00,  3.44e+00, -2.56e+00, -2.04e+00,  4.17e+00,
          -3.82e+00,  1.94e-01,  2.85e-01],
         [ 2.13e+00,  3.15e-01, -4.79e-01,  1.69e+00,  1.51e+00, -2.41e+00,
           4.35e+00, -1.04e+00,  3.94e+00],
         [ 1.36e-01,  8.97e-02, -1.96e-01,  8.24e-02,  2.17e+00,  1.37e+00,
          -3.01e-01,  3.12e+00,  6.68e-01]],

        [[ 3.04e+00,  6.06e-01, -9.81e-01, -1.61e+00, -2.45e-02,  1.83e+00,
           1.65e+00,  5.01e+00,  1.51e+00],
         [-2.03e-01,  1.09e+00, -2.80e+00, -6.62e-01,  2.44e-01,  7.15e+00,
           1.63e+00,  7.22e-01, -2.20e+00],
         [ 3.40e+00,  6.09e-01, -9.68e+00,  8.82e+00,  3.99e-03, -5.73e+00,
           9.31e+00,  3.73e+00,  1.90e+00],
         [-8.22e-01, -3.69e+00, -1.51e+00,  1.07e+01, -3.73e+00,  3.88e+00,
           2.16e+00, -2.46e+00,  3.51e+00],
         [-1.77e-01, -2.54e+00,  4.76e+00,  3.92e+00, -2.39e+00,  3.02e+00,
           5.19e+00, -1.17e+00,  7.10e-01]]], dtype=torch.float64)

my_weight = tensor([[[ 0.41,  0.10,  0.73,  0.21, -0.35, -0.35, -1.60],
         [ 0.27,  0.58,  0.30, -0.14,  0.04,  0.33,  1.88],
         [ 0.95,  1.99, -2.01, -0.02,  0.59, -1.88, -1.05]],

        [[ 0.87,  0.19, -0.96, -0.22,  0.31,  0.15,  0.02],
         [-0.28,  0.91, -1.56,  0.13,  0.48,  2.22, -0.81],
         [ 1.03,  1.01, -0.97,  0.80, -0.19, -2.13,  1.62]]],
       dtype=torch.float64)

my_bias = tensor([-1.94, -0.15], dtype=torch.float64)

torch.nn.functional.conv2d(input=my_input, weight=my_weight, bias=my_bias, stride=[1], padding=[0], dilation=[1])
</code></pre>
<p>The last line returns this:</p>
<p><code>RuntimeError: expected output_padding to be a single integer value or a list of 1 values to match the convolution dimensions, but got output_padding=[0, 0]</code></p>
<p>The issue is that conv2d does NOT have an attribute output_padding, and if I try to add it, like this:
<code>torch.nn.functional.conv2d(input=my_input, weight=my_weight, bias=my_bias, stride=[1], padding=[0], dilation=[1], output_padding=[0])</code></p>
<p>I get this error:</p>
<pre><code> * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)
      didn't match because some of the keywords were incorrect: output_padding
 * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)
      didn't match because some of the keywords were incorrect: output_padding
</code></pre>
<p>This clearly shows, that there is an error in the conv2d() program. Can someone that is a bit more knowledgeable in pytorch confirm this?</p>
","2024-01-30 17:44:00","0","Question"
"77906691","","Processes get blocked though using async all-reduce in torch.distributed","<p>I am trying to use asynchronous all-reduce in torch.distributed, which is introduced in <a href=""https://pytorch.org/docs/stable/distributed.html#synchronous-and-asynchronous-collective-operations"" rel=""nofollow noreferrer"">PyTorch Docs</a>. However, I found the processes still get blocked thought I set async_op=True. Where did I go wrong?</p>
<p>I copied the example code provided by Docs, adding some sleep and print commands to check if it is blocking.</p>
<pre><code>import torch
import torch.distributed as dist
import os
import time

rank = int(os.getenv('RANK', '0'))
dist.init_process_group(
        backend='nccl',
        world_size=2,
        rank=rank,
        )

output = torch.tensor([rank]).cuda(rank)
if rank == 1:
       time.sleep(5)

s = torch.cuda.Stream()
print(f&quot;Process {rank}: begin aysnc all-reduce&quot;, flush=True)
handle = dist.all_reduce(output, async_op=True)
# Wait ensures the operation is enqueued, but not necessarily complete.
handle.wait()
# Using result on non-default stream.
print(f&quot;Process {rank}: async check&quot;)
with torch.cuda.stream(s):
    s.wait_stream(torch.cuda.default_stream())
    output.add_(100)
if rank == 0:
    # if the explicit call to wait_stream was omitted, the output below will be
    # non-deterministically 1 or 101, depending on whether the allreduce overwrote
    # the value after the add completed.
    print(output)
</code></pre>
<p>output:</p>
<pre><code>Process 0: begin aysnc all-reduce
Process 1: begin aysnc all-reduce
Process 1: async check
Process 0: async check
tensor([101], device=‘cuda:0’)
</code></pre>
<p>I expect ‘Process 0: async check’ should be printed before ‘Process 1: begin aysnc all-reduce’.</p>
","2024-01-30 13:25:47","0","Question"
"77906649","","Why token embedding different from the embedding by the BartForConditionalGeneration model","<p>Why both the embeddings are different even when i generate them using same BartForConditionalGenration model?</p>
<p>First embedding is generated by combining token embedding and positional embedding from</p>
<pre><code>embed_pos = modelBART.model.encoder.embed_positions(input_ids.input_ids)
inputs_embeds = modelBART.model.encoder.embed_tokens(input_ids.input_ids)
</code></pre>
<p>The Second embedding by the model via</p>
<pre><code>output = modelBART(input_ids.input_ids)
print(&quot;\n\n output: \n\n&quot;,output.encoder_last_hidden_state)
</code></pre>
<p>Shouldn't the embedding by first and second be same? What to do so that difference of the embedding from first and second be zero?</p>
","2024-01-30 13:18:34","0","Question"
"77903272","77903155","","<p>First of all, in PyTorch you do need to use <code>matmul()</code> for matrix multiplication. (I assume you are talking about multiplication even though your example uses <code>+</code>)</p>
<pre class=""lang-py prettyprint-override""><code>print(torch.matmul(X,  Y))
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>tensor([[229,  82],
        [149, 111]])
</code></pre>
<p>Second, this has nothing to do with broadcasting. Broadcasting is when you you have an operation that requires two tensors to be of compatible shape (usually the same) and they are not but it is possible to <em>broadcast</em> one of them to an equivalent shape so they are compatible.</p>
<p>An example from the <a href=""https://pytorch.org/docs/stable/notes/broadcasting.html"" rel=""nofollow noreferrer"">broadcasting documentation</a>:</p>
<pre class=""lang-py prettyprint-override""><code>x=torch.empty(5,1,4,1)
y=torch.empty(  3,1,1)
(x+y).size()
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>torch.Size([5, 3, 4, 1])
</code></pre>
<p>Another example would be adding an additional outer dimension to X in your example:</p>
<pre class=""lang-py prettyprint-override""><code>X = torch.tensor([[[1,5,2,7],[8,2,5,3]]])
Y = torch.tensor([[2,9],[11,4],[9,2],[22,7]])

print(X.shape, Y.shape)
</code></pre>
<pre class=""lang-none prettyprint-override""><code>torch.Size([1, 2, 4]) torch.Size([4, 2])
</code></pre>
<p>But <code>matmul()</code> still works as Y is broadcasted to a (1,4,2) tensor (by prepending the so called batch-dimension) leading to a (1,2,2) tensor:</p>
<pre class=""lang-py prettyprint-override""><code>print(torch.matmul(X,  Y).shape)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>torch.Size([1, 2, 2])
</code></pre>
","2024-01-30 00:14:00","0","Answer"
"77903155","","Pytorch broadcasting not working as expected","<p>I am in the early stages of learning Pytorch for deep learning and have come across something I don't understand. I have written a very simple script to just make sure I fully understand the broadcasting mechanism, but I am getting an error that I find confusing.</p>
<pre class=""lang-py prettyprint-override""><code>import torch

X = torch.tensor([[1,5,2,7],[8,2,5,3]])
Y = torch.tensor([[2,9],[11,4],[9,2],[22,7]])

print(X.shape, Y.shape)
</code></pre>
<p>outputs</p>
<pre><code>&gt;&gt;&gt; torch.Size([2, 4]) torch.Size([4, 2])
</code></pre>
<p>But when I try to execute a basic mathematical operation on these tensors, where I would expect the broadcasting mechanism to bring them to the same size, I get the following error.</p>
<pre><code>print(X + Y)
</code></pre>
<p>outputs</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-7-e4a642f73c42&gt; in &lt;cell line: 1&gt;()
----&gt; 1 X + Y

RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1
</code></pre>
<p>All the explanations I have seen say that the matrices simply need to be compatible for matrix multiplication. which to my knowledge in this case they are.</p>
<p>X = 2x4
Y = 4x2</p>
<p>The amount of rows to amount of columns are the same so I don't understand the error.</p>
","2024-01-29 23:27:32","0","Question"
"77900830","","Why use dim=0 when using torch.softmax() for getting predictions probabilities?","<p>I was watching a tutorial, when he want to calculate the probabilities of a predictions from logits it use softmax with dim=0 why? isn't dim=0 means take softmax across the rows?
so shouldn't we use dim=1? like when we want to get the class id we use torch.argmax(**, dim=1)
because every row is representing the probability of different classes for <strong>one sample</strong> so why not use dim=1?</p>
<p>what's the differences between these two(when we're getting class id using argmax and when getting probabilities using softmax)?</p>
<p>I read some answers on the other questions but I didn't understand it</p>
","2024-01-29 15:33:25","1","Question"
"77899701","77884077","","<p>Using the <a href=""https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula"" rel=""nofollow noreferrer"">Sherman-Morrison formula</a>, you are not calculating the covariance of the gradient vectors, but it's inverse. If that is the goal, I think it is much more efficient just to calculate the covariance matrix first (just sum up the outer products), and then take its inverse at the end.</p>
<p>Side note, that as <code>u == v == diff</code> and <code>A</code> is a symmetric matrix for your call to <code>sherman_morrison_update</code>, so <code>vT@A == Au.T</code>. This can save you some computation, although not as much as just taking the inverse at the end at once.</p>
","2024-01-29 12:24:28","0","Answer"
"77897283","77893929","","<p>As the comment you received says the easiest way to achieve a higher utilization rate of the A100 GPU would be increasing the batch-size.</p>
<p>It is important to keep in mind that the basic problem here is that you are testing a really simple model consisting of just a few layers and training on data that is too really simple, not that far from a toy dataset. Fashion-MNIST was created to be a drop-in replacement for the original MNIST dataset, it is more complex than it but its images are still grayscale and 28x28 pixels. In order to see a higher utilization rate of the GPU and, thus, a higher differential in training performance compared with your local you should try using a more complex model and dataset. Increasing the complexity of both the model and the data will make the differences quite easy to understand.</p>
","2024-01-29 03:21:30","0","Answer"
"77893929","","How do I use more of the GPU RAM in google colab?","<p>Im working on this deep learning project in pytorch where I have 2 fully connected neural networks and I need to train then test them. But when I run the code in google colab it is not much faster than running it on my CPU on my PC. I have colab pro btw. It is also using 0.6 out of the 40GB GPU RAM of the A100 GPU.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim


device = torch.device(&quot;cuda:0&quot;)
# Define transform
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load FashionMNIST dataset
trainset = torchvision.datasets.FashionMNIST('./data', download=True, train=True, transform=transform)
testset = torchvision.datasets.FashionMNIST('./data', download=True, train=False, transform=transform)

# Create data loaders
trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(testset, batch_size=1  , shuffle=False, num_workers=2)

# Define constant for classes
classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')




# Define the fully connected neural network
class FCNN(nn.Module):
    def __init__(self, num_layers=1):
        super(FCNN, self).__init__()
        self.num_layers = num_layers
        self.fc_layers = nn.ModuleList()
        if self.num_layers == 1:
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
        elif self.num_layers == 2:
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
            self.fc_layers.append(nn.Linear(1024, 1024))
        self.output_layer = nn.Linear(1024, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        for layer in self.fc_layers:
            x = nn.functional.relu(layer(x))
        x = self.output_layer(x)
        return x

# Modify the train function to move inputs and labels to the GPU
def train(net, criterion, optimizer, epochs=15):
    for epoch in range(epochs):
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()

            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 2000 == 1999:
                print('[%d, %5d] loss: %.2f' %
                      (epoch + 1, i + 1, running_loss / 2000))
                running_loss = 0.0

# Define function to test accuracy
def test(net):
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy: %d %%' % (
            100 * correct / total))

# Main function
if __name__ == &quot;__main__&quot;:
    # Define the network
    net1 = FCNN(num_layers=1)
    net2 = FCNN(num_layers=2)
    net2.to(device)

    # Define loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.0)
    optimizer2 = optim.SGD(net2.parameters(), lr=0.001, momentum=0.0)

    # Train and test network with 1 FC layer
    #print(&quot;Training network with 1 layer...&quot;)
    #train(net1, criterion, optimizer1)
    #test(net1)

    # Train and test network with 2 FC layers
    print(&quot;Training network with 2 layers...&quot;)
    train(net2, criterion, optimizer2)
    test(net2)
</code></pre>
<p>tried using different GPUS in google colab</p>
<p>tried adding this line to always use CUDA cores:</p>
<pre><code>device = torch.device(&quot;cuda:0&quot;)
</code></pre>
<p>and had the network use the device:</p>
<pre><code>device = torch.device(&quot;cuda:0&quot;)
</code></pre>
","2024-01-28 07:11:15","0","Question"
"77893103","77890452","","<p>You can check <a href=""https://stackoverflow.com/a/72949970/8031146"">this answer</a>.</p>
<p>However, I won't recommend putting this in the <code>requirements.txt</code>. CUDA version depends on the GPU and the driver version, so it would be better to ask users to manually install pytorch and cuda according to their environment.</p>
","2024-01-27 22:51:55","1","Answer"
"77892935","77855742","","<p>I had the same problem and this is what I found for the conversion of model:</p>
<p><a href=""https://github.com/xenova/transformers.js?tab=readme-ov-file#convert-your-models-to-onnx"" rel=""nofollow noreferrer"">Convertor</a></p>
<p>Download from repo <code>convert.py</code> and <code>requirements.txt</code> from <a href=""https://github.com/xenova/transformers.js/tree/main/scripts"" rel=""nofollow noreferrer"">scripts</a>, put it to some folder.</p>
<ol>
<li>Create a virtual environment by running <code>python -m venv .venv</code></li>
<li>Activate it by running <code>source .venv/bin/activate</code></li>
<li>Install requirements by <code>pip install -r requirements.txt</code></li>
<li>Run a script for converting a model, in my case it was: <code>python convert.py --quantize --model_id BAAI/bge-small-en-v1.5</code></li>
</ol>
","2024-01-27 21:51:23","0","Answer"
"77891352","77885918","","<p>You have to re-initialize the optimizer with the new model -- the model_finetune object. Currently, as I see it in your code, it seems to still use the optimizer which is initialized with your old model weights -- model.parameters().</p>
","2024-01-27 13:22:35","1","Answer"
"77890452","","How to install cuda pytorch requirements","<p>I am making a public project. I am adding <code>requirements.txt</code> to it. but I don't know how its possible to add the pytorch <code>cuda</code> installation to <code>requirements.txt</code>.</p>
<ol>
<li>with what line in <code>requirements.txt</code> this can be done?</li>
<li>will this answer work for any os?(windows, linux and iOs). note many macbooks recently support <code>mps</code> device instead of <code>cuda</code> device. so is this line also suitable for <code>mps</code> devices or not?</li>
<li>if its not possible to make gpu devices enabled for pytorch('cuda' or 'mps'), so what's the solution for automatic installing requirements of pytorch projects?</li>
</ol>
","2024-01-27 08:01:05","0","Question"
"77889448","77885055","","<p>The error is caused by trying to batch images of different sizes together. You need to add a resize transformation to your <code>transforms.Compose</code>  block prior to <code>transforms.ToTensor()</code> to make all the input images the same size. If you check the <a href=""https://pytorch.org/vision/stable/transforms.html"" rel=""nofollow noreferrer"">documentation</a> there's a couple ways to do it.</p>
<p>wrt <code>AdaptiveAvgPool2d</code>, that serves a different purpose. Older CNNs that used max pooling with a fixed window size could only process a specific input size, say a 224x224 image. If you used a different input image size, your pooled output would be either too large or too small for the linear layers after the CNN encoder. <code>AdaptiveAvgPool2d</code> pools a tensor to a specific desired output size. This allows the encoder to process any input image size and still have the right pooled size for the final linear layers.</p>
","2024-01-26 23:19:11","4","Answer"
"77885918","","Why finetuning MLP model on a small dataset, still keeps the test accuracy same as pre-trained weights?","<p>I have designed a simple MLP model trained on 6k data samples.</p>
<pre><code>class MLP(nn.Module):
    def __init__(self,input_dim=92, hidden_dim = 150, num_classes=2):
        super().__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim
        #self.softmax = nn.Softmax(dim=1)

        self.layers = nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, self.num_classes),

        )

    def forward(self, x):
        x = self.layers(x)
        return x
</code></pre>
<p>and the model has been instantiated</p>
<pre><code>model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)

optimizer = Optimizer.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
criterion = nn.CrossEntropyLoss()
</code></pre>
<p>and the hyperparameters:</p>
<pre><code>num_epoch = 300   # 200e3//len(train_loader)
learning_rate = 1e-3
batch_size = 64
device = torch.device(&quot;cuda&quot;)
SEED = 42
torch.manual_seed(42)
</code></pre>
<p>My implementation mostly follows <a href=""https://stackoverflow.com/questions/71199036/pytorch-nn-not-as-good-as-sklearn-mlp"">this question</a>. I save the model as pre-trained weights <code>model_weights.pth</code>.</p>
<p>The accuracy of <code>model</code> on the test dataset is <code>96.80%</code>.</p>
<p>Then, I have another 50 samples (in <code>finetune_loader</code>) that I am trying to fine-tune the model on these 50 samples:</p>
<pre><code>model_finetune = MLP()
model_finetune.load_state_dict(torch.load('model_weights.pth'))
model_finetune.to(device)
model_finetune.train()
# train the network
for t in tqdm(range(num_epoch)):
  for i, data in enumerate(finetune_loader, 0):
    #def closure():
      # Get and prepare inputs
      inputs, targets = data
      inputs, targets = inputs.float(), targets.long()
      inputs, targets = inputs.to(device), targets.to(device)
      
      # Zero the gradients
      optimizer.zero_grad()
      # Perform forward pass
      outputs = model_finetune(inputs)
      # Compute loss
      loss = criterion(outputs, targets)
      # Perform backward pass
      loss.backward()
      #return loss
      optimizer.step()     # a

model_finetune.eval()
with torch.no_grad():
    outputs2 = model_finetune(test_data)
    #predicted_labels = outputs.squeeze().tolist()

    _, preds = torch.max(outputs2, 1)
    prediction_test = np.array(preds.cpu())
    accuracy_test_finetune = accuracy_score(y_test, prediction_test)
    accuracy_test_finetune
    
    Output: 0.9680851063829787
</code></pre>
<p>The accuracy remains the same as before fine-tuning the model to 50 samples, I checked, and the output probabilities are also the same.</p>
<p>What could be the reason? Am I making some mistakes in the code for fine-tuning?</p>
","2024-01-26 11:32:05","-1","Question"
"77885055","","use AdaptiveAvgPool2d for CNN in pytorch","<p>I try to train a CNN with different size images but it throws an error
<code>ValueError: expected sequence of length 1200 at dim 2 (got 1069)</code>
when I convert them to tensor.</p>
<p>I think it is because of the different size images. But I don't want to resize the images. How can I use AdaptiveAvgPool2d?</p>
<p>Here is my code:</p>
<pre><code>def makeData():
    data = []
    # cat
    for i in range(1, 11):
        data.append({&quot;path&quot;: &quot;images/cat_&quot; + str(i) + &quot;.jpeg&quot;, &quot;label&quot;: 1})
    # not cat
    for i in range(1, 11):
        data.append({&quot;path&quot;: &quot;images/not_cat_&quot; + str(i) + &quot;.jpeg&quot;, &quot;label&quot;: 0})
    return data

def loadImage(path):
    return Image.open(path).convert('RGB')

class MyDataset(Data.Dataset):
    def __init__(self, data):
        self.data = data
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  
        ])
    def __getitem__(self, idx):
        img = loadImage(self.data[idx][&quot;path&quot;])
        img = self.transform(img)
        label = torch.FloatTensor([self.data[idx][&quot;label&quot;]])
        return img, torch.FloatTensor(label)

    def __len__(self):
        return len(self.data)
    
def collate_fn(batch_list):
    data, labels = [], []
    for item in batch_list:
        data.append(item[0].tolist())
        labels.append(item[1].tolist())
    return torch.FloatTensor(data), torch.FloatTensor(labels)


loader = Data.DataLoader(MyDataset(makeData()), 2, True, collate_fn=collate_fn)
   
</code></pre>
<p>And here is my code of the model:</p>
<pre><code>class CatModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1)
        self.act1 = nn.ReLU()
        self.drop1 = nn.Dropout(0.3)
 
        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=1, padding=1)
        self.act2 = nn.ReLU()
        self.pool2 = nn.AdaptiveAvgPool2d((16, 16))
 
        self.flat = nn.Flatten()
 
        self.fc3 = nn.Linear(8192, 512)
        self.act3 = nn.ReLU()
        self.drop3 = nn.Dropout(0.5)
 
        self.fc4 = nn.Linear(512, 1)
 
    def forward(self, x):
        # input 3x32x32, output 32x32x32
        x = self.act1(self.conv1(x))
        x = self.drop1(x)
        # input 32x32x32, output 32x32x32
        x = self.act2(self.conv2(x))
        # input 32x32x32, output 32x16x16
        x = self.pool2(x)
        # input 32x16x16, output 8192
        x = self.flat(x)
        # input 8192, output 512
        x = self.act3(self.fc3(x))
        x = self.drop3(x)
        # input 512, output 1
        x = self.fc4(x)
        return nn.Sigmoid()(x)
</code></pre>
<p>I added AdaptiveAvgPool2d layers and few dropout and linear layers.</p>
","2024-01-26 08:47:14","1","Question"
"77884811","77881247","","<p>According to <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"" rel=""nofollow noreferrer"">torch.nn.Conv2d</a>, the input for the convolutional layer should be in the format <strong>(N, C, H, W)</strong> or <strong>(C, H, W)</strong>, commonly known as the channel-first mode. Therefore, it is necessary to permute (swap axes) from (N H W C) to (N C H W) or from (H W C) to (C H W).</p>
<pre class=""lang-py prettyprint-override""><code>    # for (N C H W)
    image = sample['image'][i].squeeze().to(device).float()
    image = image.permute(0, 3, 1, 2) # (N H W C) -&gt; (N C H W)
    # for (C H W)
    image = sample['image'][i].squeeze().to(device).float()
    image = image.permute(2, 0, 1) # (H W C) -&gt; (C H W)
</code></pre>
","2024-01-26 07:59:21","1","Answer"
"77884413","","When I convert a PyTorch model to onnx, I get many files, why?","<p>when I convert a torch model to onnx, I get many files like you can see in the images below:</p>
<p><a href=""https://i.sstatic.net/jO4m3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jO4m3.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/WYFks.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WYFks.png"" alt=""enter image description here"" /></a></p>
<p>No errors were reported during conversion. And my code is:</p>
<pre><code>model_path = &quot;./saved_models/emotion_model_temp/&quot;
config = BertConfig.from_json_file(model_path)  # 加载bert模型配置信息
config.num_labels=3

device = 'cuda'
input_ids_np = torch.from_numpy(np.zeros([1, 128], dtype=np.int64))
token_type_ids_np = torch.from_numpy(np.zeros([1, 128], dtype=np.int64))
attention_mask_np = torch.from_numpy(np.zeros([1, 128], dtype=np.int64))
input_ids_tf = input_ids_np.type(torch.int64).to(device)
token_type_ids_tf = token_type_ids_np.type(torch.int64).to(device)
attention_mask_tf = attention_mask_np.type(torch.int64).to(device)
feature_wav = torch.from_numpy(np.array(np.zeros((1,313, 128, 1))))
feature_wav_tf = feature_wav.type(torch.float32).to(device)

model = EmotionModel.from_pretrained(model_path, config=config).to(&quot;cuda&quot;)
model.load_state_dict(torch.load(os.path.join(model_path, &quot;pytorch_model.bin&quot;),map_location=&quot;cuda&quot;))
print(&quot;####  加载模型完成&quot;)
model.eval()

onnx_name = &quot;./temp/temp.onnx&quot;
torch.onnx.export(model,  # model being run
                  (input_ids_tf,token_type_ids_tf,attention_mask_tf,feature_wav_tf),  # model input (or a tuple for multiple inputs)
                  onnx_name,  # where to save the model
                  opset_version=11,  # the ONNX version to export the model to
                  input_names=['input_ids', 'token_type_ids', 'attention_mask', 'feature_wav'],
                  output_names=['logits'],
                  dynamic_axes={&quot;input_ids&quot;: {0: &quot;batch_size&quot;},  # 批处理变量
                                &quot;token_type_ids&quot;: {0: &quot;batch_size&quot;},
                                &quot;attention_mask&quot;: {0: &quot;batch_size&quot;},
                                &quot;feature_wav&quot;: {0: &quot;batch_size&quot;},}
                  )
</code></pre>
<p>Does anyone know what happened to my onnx conversion？</p>
","2024-01-26 06:01:53","0","Question"
"77884077","","Make Sherman-Morrison update more efficient","<p>I need to compute the covariance matrix of the parameter gradients taken on the points of a subset of the CIFAR10 dataset. For that I have this code:</p>
<pre><code>from torch.func import functional_call, vmap, grad

model1 = LogisticModel().to(device)

def loss_fn(predictions, targets):
  loss = nn.CrossEntropyLoss()
  return loss(predictions, targets)

def compute_loss(params, buffers, sample, target):
  batch = sample.unsqueeze(0)
  targets = target.unsqueeze(0)

  predictions = functional_call(model1, (params, buffers), (batch,))
  loss = loss_fn(predictions, targets)
  return loss

ft_compute_grad = grad(compute_loss)
ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))

def sherman_morrison_update(A, u, v):
  vT = v.T
  Au = A @ u

  alpha = 1/(1 + vT@Au)
  A = A - alpha*torch.outer(Au, vT@A)
  return A

testloader1 = DataLoader(test_dataset, batch_size = 512)
params = {k: v.detach() for k, v in model1.named_parameters()}
buffers = {k: v.detach() for k, v in model1.named_buffers()}
w = 0

p_covs = {p:torch.eye(q.flatten().shape[0]).to(device) for p,q in param_grads.items()}
param_grad_mean = {p:torch.zeros(q.flatten().shape[0]).to(device) for p,q in param_grads.items()}

for x,y in tqdm(testloader1):
  param_grads = ft_compute_sample_grad(params, buffers, x.to(device), y.to(device))
  for p,q, mean in zip(param_grads.values(), p_covs, param_grad_mean):
    for p_grad in p:
      w += 1
      diff = p_grad.flatten() - param_grad_mean[mean]
      param_grad_mean[mean] += diff / w
      p_covs[q] = sherman_morrison_update(A=p_covs[q], u=diff, v= diff)
</code></pre>
<p>Now, this is very inefficient and doing this in every iteration is quite time taking.
Also, we can't really take the gradients of the parameters at all points at once because that causes memory issues (thus I shifted to Sherman-Morrison).</p>
<p>Is there a way to make it more efficient? A better implementation of Sherman-Morrison? Anything else?</p>
","2024-01-26 03:37:37","0","Question"
"77881834","77881635","","<p>To log training loss every epoch, set <code>logging_strategy='epoch'</code>.</p>
<p>Now I get:</p>
<pre><code>{'loss': 7.1773, 'learning_rate': 4.2857142857142856e-05, 'epoch': 1.0, 'step': 160}
{'eval_loss': 6.232218265533447, 'eval_f1': 0.20766773162939295, 'eval_runtime': 1.2916, 'eval_samples_per_second': 30.97, 'eval_steps_per_second': 30.97, 'epoch': 1.0, 'step': 160}
{'loss': 6.3841, 'learning_rate': 3.571428571428572e-05, 'epoch': 2.0, 'step': 320}
{'eval_loss': 5.86290979385376, 'eval_f1': 0.2006269592476489, 'eval_runtime': 1.3634, 'eval_samples_per_second': 29.339, 'eval_steps_per_second': 29.339, 'epoch': 2.0, 'step': 320}
{'loss': 5.5212, 'learning_rate': 2.857142857142857e-05, 'epoch': 3.0, 'step': 480}
{'eval_loss': 5.343527793884277, 'eval_f1': 0.24319419237749546, 'eval_runtime': 1.29, 'eval_samples_per_second': 31.008, 'eval_steps_per_second': 31.008, 'epoch': 3.0, 'step': 480}
{'loss': 4.7184, 'learning_rate': 2.1428571428571428e-05, 'epoch': 4.0, 'step': 640}
{'eval_loss': 5.131855487823486, 'eval_f1': 0.23588039867109634, 'eval_runtime': 1.3336, 'eval_samples_per_second': 29.993, 'eval_steps_per_second': 29.993, 'epoch': 4.0, 'step': 640}
{'loss': 4.0205, 'learning_rate': 1.4285714285714285e-05, 'epoch': 5.0, 'step': 800}
{'eval_loss': 4.972315788269043, 'eval_f1': 0.22551928783382788, 'eval_runtime': 1.2714, 'eval_samples_per_second': 31.462, 'eval_steps_per_second': 31.462, 'epoch': 5.0, 'step': 800}
{'loss': 3.5411, 'learning_rate': 7.142857142857143e-06, 'epoch': 6.0, 'step': 960}
{'eval_loss': 4.964015960693359, 'eval_f1': 0.23100303951367776, 'eval_runtime': 1.2783, 'eval_samples_per_second': 31.292, 'eval_steps_per_second': 31.292, 'epoch': 6.0, 'step': 960}
{'loss': 3.2564, 'learning_rate': 0.0, 'epoch': 7.0, 'step': 1120}
{'eval_loss': 4.895078182220459, 'eval_f1': 0.22585438335809802, 'eval_runtime': 1.3362, 'eval_samples_per_second': 29.935, 'eval_steps_per_second': 29.935, 'epoch': 7.0, 'step': 1120}
{'train_runtime': 81.2849, 'train_samples_per_second': 13.779, 'train_steps_per_second': 13.779, 'total_flos': 73700199874560.0, 'train_loss': 4.945595060076032, 'epoch': 7.0, 'step': 1120}]
</code></pre>
","2024-01-25 17:22:07","0","Answer"
"77881635","","logging training and validation loss per epoch using huggingface trainer in pytorch to assess bias-variance tradeoff","<p>I'm fine-tuning a transformer model for text classification in Pytorch using huggingface Trainer. I would like to log both the training and the validation loss for each epoch of training. This is so that I can assess when the model starts to overfit to the training data (i.e. the point at which training loss keeps decreasing, but validation loss is stable or increasing, the bias-variance tradeoff).</p>
<p>Here are my training arguments of the huggingface trainer:</p>
<pre><code>training_arguments = TrainingArguments(
            output_dir = os.path.join(MODEL_DIR, f'{TODAYS_DATE}_multicls_cls'),
            run_name = f'{TODAYS_DATE}_multicls_cls',
            overwrite_output_dir=True,
            evaluation_strategy='epoch',
            save_strategy='epoch',
            num_train_epochs=7.0,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            optim='adamw_torch',
            learning_rate=LEARNING_RATE
</code></pre>
<p>My training arguments of the huggingface trainer are set to evaluate every epoch, as desired, but my training loss is computed every 500 steps by default. You can see this in the log history of <code>trainer.state</code> after training:</p>
<pre><code>{'eval_loss': 6.346338748931885, 'eval_f1': 0.2146690518783542, 'eval_runtime': 1.2777, 'eval_samples_per_second': 31.306, 'eval_steps_per_second': 31.306, 'epoch': 1.0, 'step': 160}
{'eval_loss': 5.505970001220703, 'eval_f1': 0.23817863397548159, 'eval_runtime': 1.5768, 'eval_samples_per_second': 25.367, 'eval_steps_per_second': 25.367, 'epoch': 2.0, 'step': 320}
{'eval_loss': 5.21959114074707, 'eval_f1': 0.2233676975945017, 'eval_runtime': 1.3016, 'eval_samples_per_second': 30.732, 'eval_steps_per_second': 30.732, 'epoch': 3.0, 'step': 480}
{'loss': 6.1108, 'learning_rate': 2.767857142857143e-05, 'epoch': 3.12, 'step': 500}
{'eval_loss': 5.014569282531738, 'eval_f1': 0.24625623960066553, 'eval_runtime': 1.3961, 'eval_samples_per_second': 28.652, 'eval_steps_per_second': 28.652, 'epoch': 4.0, 'step': 640}
{'eval_loss': 5.090881824493408, 'eval_f1': 0.2212643678160919, 'eval_runtime': 1.2708, 'eval_samples_per_second': 31.477, 'eval_steps_per_second': 31.477, 'epoch': 5.0, 'step': 800}
{'eval_loss': 4.950728416442871, 'eval_f1': 0.23750000000000002, 'eval_runtime': 1.298, 'eval_samples_per_second': 30.816, 'eval_steps_per_second': 30.816, 'epoch': 6.0, 'step': 960}
{'loss': 3.8989, 'learning_rate': 5.357142857142857e-06, 'epoch': 6.25, 'step': 1000}
{'eval_loss': 4.940125465393066, 'eval_f1': 0.24444444444444444, 'eval_runtime': 1.4609, 'eval_samples_per_second': 27.38, 'eval_steps_per_second': 27.38, 'epoch': 7.0, 'step': 1120}
{'train_runtime': 80.7323, 'train_samples_per_second': 13.873, 'train_steps_per_second': 13.873, 'total_flos': 73700199874560.0, 'train_loss': 4.81386468069894, 'epoch': 7.0, 'step': 1120}
</code></pre>
<p>How can I set the training arguments to log the training loss every epoch, just like my validation loss? There is no equivalent parameter to <code>evaluation_strategy=epoch</code> for training in training arguments.</p>
","2024-01-25 16:48:10","0","Question"
"77881247","","""PyTorch Conv2d error: Expected 3 channels, got 128 for [1, 128, 128, 3]. Help needed!""","<p>I am encountering an issue while working with a PyTorch convolutional neural network. The error message I'm receiving is:</p>
<p>Given groups=1, weight of size [8, 3, 5, 5], expected input[1, 128, 128, 3] to have 3 channels, but got 128 channels instead</p>
<p>Context:</p>
<p>Model Architecture:</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.cnn_model = nn.Sequential(
        nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 3, stride = 5),
        nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 2, stride = 5))
        
        self.fc_model = nn.Sequential(
        nn.Linear(in_features = 256, out_features = 120),
        nn.Tanh(),
        nn.Linear(in_features = 120, out_features = 84),
        nn.Tanh(),
        nn.Linear(in_features = 84, out_features = 1))
        
    def forward(self, x):
        x = self.cnn_model(x)
        x = x.view(x.size(0), -1)
        x = self.fc_model(x)
        x = F.sigmoid(x)
        
        return x
</code></pre>
<p>Data Loading:</p>
<pre><code>class MRI(Dataset):
    def __init__(self):
        # Load images and labels
        tumor = []
        path_tumor = '/kaggle/input/brain-tumor/Dataset/Yes_Data/*.jpg'
        for f in glob.iglob(path_tumor):
            img = cv2.imread(f)
            img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_AREA)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            tumor.append(img)

        healthy = []
        path_healthy = '/kaggle/input/brain-tumor/Dataset/No_data/*.jpg'
        for f in glob.iglob(path_healthy):
            img = cv2.imread(f)
            img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_AREA)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            healthy.append(img)

        # Convert lists to numpy arrays
        healthy = np.array(healthy)
        tumor = np.array(tumor)
        
        # Create labels
        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)
        healthy_label = np.zeros(healthy.shape[0], dtype=np.float32)

        # Concatenate images and labels
        images = np.concatenate((tumor, healthy), axis=0)
        labels = np.concatenate((tumor_label, healthy_label), axis=0)
        self.images = images
        self.labels = labels

    def __getitem__(self, index):
        sample = {'image': self.images[index], 'label': self.labels[index]}
        return sample

    def __len__(self):
        return self.images.shape[0]

    def normalize(self):
        self.images = (self.images / 255.0).astype(np.float32)
</code></pre>
<p>Error Context:</p>
<pre><code>model.eval()
outputs = []
y_true = []

with torch.no_grad():
    for sample in dataloader:
        for i in range(sample['image'].size(0)):
            image = sample['image'][i].squeeze().to(device).float()
            label = sample['label'][i].to(device)

            y_hat = model(image)
            outputs.append(y_hat.cpu().detach().numpy())
            y_true.append(label.cpu().detach().numpy())

</code></pre>
<p>I have tried every debugging method out there, printing and checking the shape<a href=""https://i.sstatic.net/LDvT3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LDvT3.png"" alt=""Debugging"" /></a></p>
","2024-01-25 15:46:44","0","Question"
"77880818","77837531","","<blockquote>
<p><code>cov_matrix = torch.tensor([[sigmas[0]**2, 0], [0, sigmas[1]**2]], requires_grad=True)</code></p>
</blockquote>
<p>This line actually breaks the gradient flow as explained in <a href=""https://discuss.pytorch.org/t/does-creating-a-new-tensor-break-autograd/119555/2"" rel=""nofollow noreferrer"">here</a>. To make the gradient flow back to your parameters, change that line to the following <code>cov_matrix = torch.diag(torch.stack(sigmas).pow(2))</code></p>
","2024-01-25 14:41:29","0","Answer"
"77880775","77880286","","<p>A common &quot;problem&quot; of importing tensorflow is that it tries to occupy as much GPU memory as it can get. This is something you can turn off manually as described <a href=""https://stackoverflow.com/a/57992246/9659620"">here</a>.</p>
<p>Being restricted in memory might leave the dataloader unable to pin gpu mem for faster access and training would be slower.  Since you have inference here, it is not clear to me how this could effect your inference speed, to be honest. But still, such connection seems plausible to me.</p>
<p>I hope this info useful as you further try to understand the situation.</p>
<p>Also, are you sure that your time measurement is accurate?</p>
<p>Note: It might be better to not import tensorflow at all. For example, tools like tensorboard are also available in pytorch directly and you dont get any interference!</p>
","2024-01-25 14:36:53","1","Answer"
"77880463","77865273","","<p>Try to transfer the weight to cpu first and then save the weight. Or if you can, save each model to hard drive and save the path to each weight. If you copy the weight directly from GPU, sometime the unused one will not be handled by garbage collector, and the new one is still stay on gpu, which will take up space.</p>
","2024-01-25 13:47:04","0","Answer"
"77880286","","Is the order of importing tensorflow and torch relevant?","<p>I am using a PyTorch model to run inference on an image, using a standard CNN model.</p>
<p>I have noticed that the order in which I import the <code>tensorflow</code> and the <code>torch</code> packages has an effect on the time the inference is taking.</p>
<p>When importing <code>tensorflow</code> first, the inference takes ~200ms. When importing it after <code>torch</code>, the inference time increases up to 800ms.</p>
<pre><code># import tensorflow # &lt;-- model inference takes 200ms
import torch
# import tensorflow # &lt;-- model inference takes 800ms
</code></pre>
<p>The inference also takes 800ms when <code>tensorflow</code> is not imported add all.</p>
<p>Coming from other programming languages, I assume that the import of the <code>tensorflow</code> package has side effects which affect what the <code>torch</code> import does. Is this assumption correct?</p>
<p>My notebook is running in the context of a custom Docker container based on <code>nvcr.io/nvidia/tensorflow:22.09-tf2-py3</code>. In addition, these packages were installed <code>pip install torch==1.12.1+cu113 torchaudio==0.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113</code></p>
<p>Update: What is really confusing about this is that the <code>tensorflow</code> package is not even used in my code. I simply had it left in my notebook. When I noticed that, I removed the import, which changed the inference duration.</p>
<p>Is this a known behaviour?</p>
","2024-01-25 13:20:56","2","Question"
"77879917","77872605","","<p>You need to ensure that gradients are enabled only for the <strong>parameters</strong> of the last layer. Replace <code>model_vgg.classifier[-1].requires_grad = True</code> with the below code snippet</p>
<pre><code>for param in model_vgg.classifier[-1].parameters():
    param.requires_grad = True
</code></pre>
","2024-01-25 12:17:01","1","Answer"
"77878585","77875016","","<p>I figured out how to do it as follows:</p>
<pre><code>from torch.utils.tensorboard import SummaryWriter
%load_ext tensorboard
from torch import nn


from torch import nn

class LinearRegressionModel(nn.Module):

'''
Torch Module class.
Initializes weight randomly and gets trained via train method.
'''
def __init__(self, optimizer):
    super().__init__()   #The super call delegates the function call to the parent class, which is nn.
    self.optimizer = optimizer

    # Initialize Weights and Bias
    self.weights = nn.Parameter(
        torch.randn(1, len(X_train[1]), dtype=torch.float),
        requires_grad=True) #main use case is to tell autograd to begin recording operations on a Tensor tensor .
                            #autograd is PyTorch’s automatic differentiation engine that powers neural network training.
    self.bias = nn.Parameter(
        torch.randn(1, len(X_train[1]), dtype=torch.float),
        requires_grad=True
        )
'''
Now, our goal is to optimize these weights via backpropagation —
for that we need to setup our linear layer, consisting of the regression formula:
'''
def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        return (self.weights * x + self.bias).sum(axis=1)

'''
The trainModel method will help us perform backpropagation and weight adjustment:
'''
def trainModel(
        self,
        epochs: int,
        X_train: torch.Tensor,
        X_test: torch.Tensor,
        y_train: torch.Tensor,
        y_test: torch.Tensor,
        lr: float
        ):
    '''
    Trains linear model using pytorch.
    Evaluates the model against test set for every epoch.
    '''
    torch.manual_seed(42)
    # Create empty loss lists to track values
    self.train_loss_values = []
    self.test_loss_values = []

    loss_fn = nn.L1Loss()

    if self.optimizer == 'SGD':
        optimizer = torch.optim.SGD(
            params=self.parameters(),
            lr=lr
            )
    elif self.optimizer == 'Adam':
        optimizer = torch.optim.Adam(
            params=self.parameters(),
            lr=lr
            )
for epoch in range(epochs):
        self.train()
        y_pred = self(X_train)
        loss = loss_fn(y_pred, y_train)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Set the model in evaluation mode
        self.eval()
        with torch.inference_mode():
            self.evaluate(X_test, y_test, epoch, loss_fn, loss, epochs)

def evaluate(self, X_test, y_test, epoch_nb, loss_fn, train_loss, epochs):
    '''
    Evaluates current epoch performance on the test set.
    '''
    writer = SummaryWriter('runs/working_directory')
    test_pred = self(X_test)
    test_loss = loss_fn(test_pred, y_test.type(torch.float))
    if epoch_nb % 10 == 0:
        self.train_loss_values.append(train_loss.detach().numpy())
        self.test_loss_values.append(test_loss.detach().numpy())
        print(f&quot;Epoch: {epoch_nb} - MAE Train Loss: {train_loss} - MAE Test Loss: {test_loss} &quot;)
        #writer.add_scalar(&quot;train_loss x epoch&quot;, train_loss)
        writer.add_scalar(&quot;test_loss x epoch&quot;, test_loss, epoch_nb)
    if epoch_nb &gt;= epochs:
        writer.close()

# train a model using Adam optimizer and 0.001 learning rate for 500 epochs:
adam_model = LinearRegressionModel('Adam')
adam_model.trainModel(500, X_train, X_test, y_train, y_test, 0.001)

tensorboard  --logdir=runs`enter code here`
</code></pre>
","2024-01-25 08:50:47","0","Answer"
"77878274","77877283","","<p>In The <code>DonutOCRDataset</code> object for training, the <code>root_dir</code> that you pass is <code>dataset/train</code>. Then, in <code>load_data()</code>, you are looking for subdirectories within this directory (at <code>if os.path.isdir(folder_path)</code>), which do not seem to exist in your directory structure. So the if condition is probably never satisfied, and <code>self.data</code> in your dataset object remains an empty list, giving a length of zero. Removing the for loop within <code>load_data()</code> should resolve the issue.</p>
","2024-01-25 07:53:40","0","Answer"
"77877283","","Pytorch dataset - len(train_dataset) returns zero","<p>I am trying to create a custom dataset and dataloader in pytorch, to finetune a DONUT model. For context, my dataset is organised as follows:</p>
<pre><code>dataset/
├── train/
│   ├── image1.jpg
│   ├── image2.jpg
│   ├── metadata.jsonl
│   └── ...
├── validation/
│   ├── image1.jpg
│   ├── image2.jpg
│   ├── metadata.jsonl
│   └── ...
└── ...
</code></pre>
<p>I have already written my custom dataset class:</p>
<pre><code>class DonutOCRDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, transform):
        self.root_dir = root_dir
        self.transform = transform
        self.data = self.load_data()
    
    def load_data(self):
        data = []
        for folder in os.listdir(self.root_dir):  # Use self.root_dir here
            folder_path = os.path.join(self.root_dir, folder)
            if os.path.isdir(folder_path):
                metadata_path = os.path.join(folder_path, 'metadata.jsonl')
                with open(metadata_path, 'r') as f:
                    metadata = [json.loads(line) for line in f]
                data.extend([(item[&quot;file_name&quot;], item[&quot;ground_truth&quot;]) for item in metadata])
        return data

    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        img_path, label = self.data[idx]
        img_path_full = os.path.join(self.root_dir, img_path)
        print(f&quot;Loading image: {img_path_full}&quot;)
        img = Image.open(img_path_full).convert('RGB')

        if self.transform:
            img = self.transform(img)

        return img, label
</code></pre>
<p>and below I tried to define my transformations, instantiate dataset and dataloader:</p>
<pre><code># Define root_dir
root_dir = r'C:\Users\Company\Documents\.....\240111_donut_1\dataset'


# Define your transformation
transform = transforms.Compose([
    transforms.Resize((640, 460)),
    transforms.ToTensor(),
])

# Instantiate the dataset
train_dataset = DonutOCRDataset(os.path.join(root_dir, 'train'), transform=transform)
val_dataset = DonutOCRDataset(os.path.join(root_dir, 'validation'), transform=transform)

batch_size = 8
train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False)
val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)
</code></pre>
<p>However, i later found out when i printed len(train_dataset) and len(val_dataset), they both return 0.</p>
<p>Anyone knows whats wrong with my code?</p>
","2024-01-25 02:47:55","0","Question"
"77876707","77876566","","<pre><code>'bhld,lrd-&gt;bhlr'
</code></pre>
<p>First arg is 4d, 2nd is 3d, result is 4d</p>
<p>'bh' passes thru unchanged. 'r' also. 'ld' dimensions are matched, with multiplication, and sum of products on
'd'.</p>
<p>It terms of a broadcasted sum of products I think the equivalent is (not tested)</p>
<pre><code>(A[:,:,:,None,:] * B[None, None, :,:,:]).sum(axis=-1)
</code></pre>
<p>With matmul, put 'd' of <code>B</code> 2nd to the last.  Ensure that the first 3 dimensions broadcast.</p>
<pre><code>A[:,:,:,None,:] @ B.transpose(0,2,1)[None, None, :,:,:]
</code></pre>
<p>Same sum on 'd', 'r' is shared among all 3</p>
<pre><code>'bhrd,lrd-&gt;bhlr'
</code></pre>
","2024-01-24 23:08:30","1","Answer"
"77876566","","Clarification on einsum equation","<p>I came across some code on Huggingface (in a self-attention module) that uses <code>torch.einsum</code>, which I'm not too familiar with and would like some help interpreting. I've looked through <a href=""https://stackoverflow.com/questions/55894693/understanding-pytorch-einsum"">this list of basic operations and their implementations in NumPy/PyTorch</a>. The inputs are a 4D tensor and a 3D tensor.</p>
<p>This is the (explicit) einsum string:</p>
<pre><code>'bhld,lrd-&gt;bhlr'
</code></pre>
<p>(Another einsum string used is similar:</p>
<pre><code>'bhrd,lrd-&gt;bhlr')
</code></pre>
<p>What does this mean/how else could this be implemented without using einsum? E.g., the second tensor must be transposed so that d is the first dimension.</p>
","2024-01-24 22:23:59","0","Question"
"77876425","77760620","","<p>It seems that there is an issue between Jupyter and SetFit!
You will see the values printed once you run your code directly in the terminal.</p>
<p>Sample run from the terminal here:</p>
<pre><code>{'embedding_loss': 0.3245, 'learning_rate': 9.090909090909091e-08, 'epoch': 0.0}                                                                                                                                                                                                                                                                                                                                            | 0/2200 [00:00&lt;?, ?it/s]
    {'embedding_loss': 0.035, 'learning_rate': 1.1111111111111113e-05, 'epoch': 1.0}                                                                                                                                                                                                                                                                                                                                                                     
    {'eval_embedding_loss': 0.1738, 'learning_rate': 1.1111111111111113e-05, 'epoch': 1.0}                                                                                                                                                                                                                                                                                                                                                               
    {'embedding_loss': 0.0005, 'learning_rate': 0.0, 'epoch': 2.0}                                                                                                                                                                                                                                                                                                                                                                                       
    {'eval_embedding_loss': 0.181, 'learning_rate': 0.0, 'epoch': 2.0}                                                                                                                                                                                                                                                                                                                                                                                   
    {'train_runtime': 156.4973, 'train_samples_per_second': 112.462, 'train_steps_per_second': 14.058, 'epoch': 2.0}                                                                                                                                                                                                                                                                                                                                     
    100%|███████████████████| 2200/2200 [02:34&lt;00:00, 14.23it/s]
    Batches: 100%|███████████████████| 46/46 [00:01&lt;00:00, 44.09it/s]
    ***** Running evaluation *****
    Batches: 100%|███████████████████| 12/12 [00:00&lt;00:00, 55.14it/s]
    Metrics: {'accuracy': 0.6539509536784741, 'precision': 0.6951538195187225, 'recall': 0.6539509536784741, 'f1': 0.6658552263536266}
</code></pre>
","2024-01-24 21:50:36","0","Answer"
"77875502","77875298","","<p><code>@torch.no_grad()</code> is a function decorator that wraps the entire function, so everything that happens inside the function is done without gradient tracking.</p>
<p><code>with torch.no_grad()</code> removes gradient tracking for code within the <code>with torch.no_grad()</code> block.</p>
<p>You can use <code>@torch.no_grad()</code> when you want everything in the function to happen without grad tracking, or use <code>with torch.no_grad()</code> when you only want to remove grad tracking from a specific part of the function.</p>
","2024-01-24 18:39:51","7","Answer"
"77875298","","What is the difference between '''@torch.no_grad()''' and '''with torch.no_grad()'''","<pre><code>@torch.no_grad()
def evalarc(model, dataloader):
    val_loss = 0.0 
    for images, labels in tqdm(dataloader):
        .
        .
        .
</code></pre>
<p>vs.</p>
<pre><code>def evalarc(model, dataloader):
    val_loss = 0.0 
    with torch.no_grad():
        for images,labels in tqdm(dataloader):
            .
            .
            .
</code></pre>
<p>What is the difference between the two? Will there be any change if I switch between the two?</p>
","2024-01-24 18:02:13","2","Question"
"77875016","","How to implement TensorBoard from inside PyTorch module class","<p>I have the following PyTorch nn module subclass. How can I get the benefit of tensorboard visualization? I'm new to the subject.</p>
<pre><code>from torch import nn

class LinearRegressionModel(nn.Module):
  
    def __init__(self, optimizer):
        super().__init__()   
        self.optimizer = optimizer

        # Initialize Weights and Bias
        self.weights = nn.Parameter(
            torch.randn(1, 13, dtype=torch.float),
            requires_grad=True) 
        self.bias = nn.Parameter(
            torch.randn(1, 13, dtype=torch.float),
            requires_grad=True
            )

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
            return (self.weights * x + self.bias).sum(axis=1)

    def trainModel(
            self,
            epochs: int,
            X_train: torch.Tensor,
            X_test: torch.Tensor,
            y_train: torch.Tensor,
            y_test: torch.Tensor,
            lr: float
            ):

        torch.manual_seed(42)
       
        self.train_loss_values = []
        self.test_loss_values = []

        loss_fn = nn.L1Loss()

        if self.optimizer == 'SGD':
            optimizer = torch.optim.SGD(
                params=self.parameters(),
                lr=lr
                )
        elif self.optimizer == 'Adam':
            optimizer = torch.optim.Adam(
                params=self.parameters(),
                lr=lr
                )
</code></pre>
","2024-01-24 17:17:20","0","Question"
"77874054","77746203","","<p>In a torch_geometric.data Data object, <code>num_nodes</code> is a type of property that is calculated according to the size of <code>x</code>. For example, in your graph <code>x</code>=[<strong>33</strong>, 401], and <code>num_nodes</code> is naturely 33.</p>
<p>You can change <code>data.x</code> and <code>data.edge_index</code> by assigning values as you show, but this is not efficient and often won't work if your graph dataset reads from a local file.
I suggest you define and apply your <code>torch_geometric.transforms</code> to modify the original graph. There is a detailed answer here:</p>
<p><a href=""https://stackoverflow.com/questions/77369606/how-to-add-a-new-attribute-to-a-torch-geometric-data-data-object-element/77523155#77523155"">How to add a new attribute to a torch_geometric.data Data object element?</a></p>
","2024-01-24 15:00:49","0","Answer"
"77873610","77869946","","<p>Here is one way to rearrange the flattened tensor back into the original shape while keeping the order:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

flattened = torch.tensor([0,0,0,4,0,0,0,8]) # flattened tensor
kernel_size = 2
input_h = 2  
input_w = 4

unflatten = []
for i in range(0, len(flattened), input_w):
    unflatten.append(flattened[i:i+input_w])

result = torch.stack(unflatten)
print(result)
</code></pre>
<p>The key ideas:</p>
<ul>
<li>Iterate over the flattened tensor in chunks of size <code>input_w</code> (the original input width)</li>
<li>Append those chunks as separate tensors to create a list</li>
<li>Stack that list back into a 2D tensor</li>
</ul>
<p>This takes advantage of slicing the flattened tensor and stitching together the pieces.</p>
<p>The output with the example data is:</p>
<pre><code>tensor([[0, 0, 0, 4],
        [0, 0, 0, 8]])
</code></pre>
","2024-01-24 13:54:43","0","Answer"
"77872605","","Training the VGG16 Model for Image Classification using PyTorch","<p>I am using PyTorch for image classification.</p>
<p>I coded the following train function that worked with a simple linear model:</p>
<pre><code>criterion = nn.CrossEntropyLoss()
def train(model, dataloader, epoch):
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
running_loss, running_acc = 0., 0.
loss_history = []
accuracy_history = [](data_train):.2f}%&quot;)

for i in range(1, epoch + 1):
  model.train()
  for inputs, targets in dataloader:
      inputs, targets = inputs.to(device), targets.to(device)
      outputs = model(inputs)
      loss = criterion(outputs, targets)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      preds = torch.argmax(outputs, 1)
      running_loss += loss.item()
      running_acc += torch.sum(preds == targets).item()

  print(f&quot;[TRAIN epoch {i}] Loss: {running_loss/len(data_train):.2f} Acc: {100 * running_acc/len
 
</code></pre>
<p>I have the pre-trained VGG16 model and I want to change the weights of its last layer:</p>
<pre><code>model_vgg = models.vgg16(weights='DEFAULT')
model_vgg.classifier[6] = nn.Linear(4096, 2)

for param in model_vgg.parameters():
    param.requires_grad = False
model_vgg.classifier[-1].requires_grad = True

train(model_vgg, train_loader, 2)
</code></pre>
<p>However, when training it I get the following error:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)

&lt;timed eval&gt; in &lt;module&gt;

&lt;ipython-input-27-1f64686a5cfd&gt; in train(model, dataloader, epoch)
     39           loss = criterion(outputs, targets)
     40           optimizer.zero_grad()
---&gt; 41           loss.backward()
     42           optimizer.step()
     43           preds = torch.argmax(outputs, 1)

/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
--&gt; 251     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    252         tensors,
    253         grad_tensors_,

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>How do I fix this?</p>
","2024-01-24 11:21:50","0","Question"
"77871789","77871684","","<p>Is this code you wrote? It appears that not all seven paramters are being utilized in your loop. In your backprogation. only <code>sout_sur_layer_output</code> seems to be trains in your model. Perhaps you could debug step by step to monitor which paramters are actually being used.</p>
","2024-01-24 09:22:49","0","Answer"
"77871684","","Some of my learnable parameters are not being updated","<p>I am trying to build a Spiking Neural Network (SNN) from scratch using PyTorch. In the network, I have defined 7 learnable parameters as such:</p>
<pre><code>class SNN(nn.Module):
    def __init__(self,window_length,beta,dims):
        super(SNN,self).__init__()
        self.window_length=window_length
        self.beta=beta
        self.input_size=dims[0]
        self.hidden_size=dims[1]
        self.output_size=dims[2]
        
        self.a=nn.Parameter(torch.ones(1,requires_grad=True))
        self.W0=nn.Parameter(torch.ones(self.input_size,requires_grad=True))
        self.W1=nn.Parameter(torch.ones(self.hidden_size,requires_grad=True))
        self.W2=nn.Parameter(torch.ones(self.output_size,requires_grad=True))
        self.th0=nn.Parameter(torch.ones(self.input_size,requires_grad=True))
        self.th1=nn.Parameter(torch.ones(self.hidden_size,requires_grad=True))
        self.th2=nn.Parameter(3*torch.ones(self.output_size,requires_grad=True))

    def Layer(self,x):
        #I have three of such methods (for my three layers)
        n_samples=x.shape[0]
        window_length=self.window_length
        layer_size=self.hidden_size
        
        mems_sur_layer_output=torch.zeros(n_samples,layer_size,window_length)
        sout_sur_layer_output=torch.zeros(n_samples,layer_size,window_length)
        
        for i in range(layer_size):            
            sout_sur=torch.zeros(n_samples,window_length)
            mems_sur=torch.zeros(n_samples,window_length)
            mem=torch.zeros(n_samples)
            mem_sur=torch.zeros(n_samples)
            
            for time_step in range(window_length):
                mems_sur[:,time_step]=(self.beta*mem_sur+self.W0[i]*torch.sum(x[:,:,time_step],dim=[1]))
                #W0 is replaced with W1 and W2 in the two other layers
                
                x1=mems[:,time_step]-self.th0[i]
                #th0 is replaced with th1 and th2 in the two other layers
                s_sur=torch.sigmoid(x1)
                sout_sur[:,time_step]=s_sur

                mem_sur=mems_sur[:,time_step]*(1-s_sur)
        
            mems_sur_layer_output[:,i,:]=mems_sur
            sout_sur_layer_output[:,i,:]=sout_sur   

        return sout_sur_layer_output

    def Rate_Decode(self,x):
        return x/self.a

    def forward(self,x):
        output=self.Layer(x)
        #Layer() is called 3 times with the variable 'output' being the output of each call and the input of the next.
        return self.Rate_Decode(output)
</code></pre>
<p>When the model finishes training, only parameters <code>a</code> and <code>th2</code> are being updated.</p>
<p>I am building a simple network that adds two numbers together with a simple architecture (2x10x1). Any possible reasons on why this is happening?</p>
","2024-01-24 09:04:51","0","Question"
"77871127","77861105","","<p>As <code>torch.compile</code> was first released in PyTorch 2.0, most of it was still experimental and wasn't documented very thoroughly.</p>
<p>I have absolutely no idea why your implementation doesn't work, but you get the desired improvement by using <code>torch_tensorrt.compile</code> instead of <code>torch.compile</code>.</p>
<pre class=""lang-py prettyprint-override""><code>    input_shape = (8192, 3, 32, 32)
    inputs = [torch.randn(input_shape).to(device)]

    if use_tensorrt:
        model = torch_tensorrt.compile(
            model,
            inputs=inputs,
            workspace_size = 20 &lt;&lt; 30,
            enabled_precisions = {torch.float},
        )
</code></pre>
<p>On my system with the same library versions, this yields a 3x improvement</p>
<pre><code>&gt;&gt;&gt; Without TensorRT
&gt;&gt;&gt; Using CUDA.
&gt;&gt;&gt; PyTorch FPS: 12.75
&gt;&gt;&gt; With TensorRT
&gt;&gt;&gt; Using CUDA.
&gt;&gt;&gt; PyTorch FPS: 42.96
</code></pre>
<p>If I had to guess, it probably has something to do with <code>torch.compile</code> failing with the options provided, and defaulting to some other IR.</p>
","2024-01-24 07:12:20","1","Answer"
"77870974","77865949","","<p>Your input is of shape <code>(8,10)</code>. Since you're putting this directly into the MLP, I'm assuming the first dimension is the batch dimension and you won't be adding another batch dimension on top of the first two. If you do, this solution still works, just need some axis munging.</p>
<p>You can pack all the layer weights into a single tensor, index into them, then batch matmul.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import timeit
import math

torch.manual_seed(42)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

class MultiLinear(nn.Module):
    def __init__(self, in_features, out_features, n_replicas, bias=True, device=None, dtype=None):
        # this is mostly copied from pytorch nn.Linear
        factory_kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # important note: pytorch nn.Linear weights are of shape (out_features, in_features) 
        # for weird transpose reasons. This variant has in_features first
        self.weight = nn.Parameter(torch.empty((n_replicas, in_features, out_features), **factory_kwargs))
        
        if bias:
            self.bias = nn.Parameter(torch.empty(n_replicas, out_features, **factory_kwargs))
        else:
            self.register_parameter('bias', None)
            
        self.reset_parameters()
        
    def reset_parameters(self) -&gt; None:
        # this is also copied from pytorch nn.Linear
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, input, index_tensor):
        # input is a 2d float tensor
        # index_tensor is a 1d bool tensor
        
        weight = self.weight[index_tensor]
        bias = self.bias[index_tensor]
        
        output = torch.bmm(input[:,None,:], weight).squeeze() + bias
        return output
    
class MultiMLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, n_replicas):
        super().__init__()
        self.fc1 = MultiLinear(input_size, hidden_size, n_replicas)
        self.fc2 = MultiLinear(hidden_size, output_size, n_replicas)
        
    def forward(self, x, index_tensor):
        x = F.relu(self.fc1(x, index_tensor))
        x = self.fc2(x, index_tensor)
        return x
    
# arbitrary parameters
input_size = 10
hidden_size = 20
output_size = 5
n_replicas = 3

input_data = torch.rand(size=(8, input_size), device=device)
mlp_index = torch.tensor([0, 1, 0, 1, 0, 2, 0, 2], device=device) # note this version skips the extra axis you add

model = MultiMLP(input_size, hidden_size, output_size, n_replicas)
model.to(device)

# note - the first use of the model causes memory allocation to the GPU that is 
# much slower than subsequent calls. this impacts your previous benchmarks.
# calling the model once outside the time test fixes this
result = model(input_data, mlp_index)

def time_test():
    result = model(input_data, mlp_index)
    return result

update_time = timeit.timeit(time_test, number=20000)
print(f&quot;Execution time: {update_time} seconds&quot;)
</code></pre>
<p>On my GPU, the test times are:</p>
<ul>
<li>Baseline: 1.01s</li>
<li>First Update: 4.30s</li>
<li>Second Update: 4.46s</li>
<li>Third Update: 10.1s</li>
<li>My Version: 2.85s</li>
</ul>
<p>With all that said, you also need some way of making the different weights learn different things. If the outputs of the various models all go into the same loss, there's no reason to use an approach like this over a standard MLP.</p>
","2024-01-24 06:35:07","1","Answer"
"77870197","77864704","","<p>Original paper applied Dropout to the Sub-Layer (Multi Head Attention) before Residual Connection and Layer Normalization. This is called <strong>Post Normalization</strong>.</p>
<blockquote>
<p>dropout to the output of each sub-layer, <strong>before</strong> it is added to the sub-layer input (x) and (layer) normalized.</p>
</blockquote>
<p>However, recent approach is <strong>Pre Normalization</strong> where LayerNorm is applied to the input x into the sub-layer as explained in <a href=""https://youtu.be/kCc8FmEb1nY?t=5729"" rel=""noreferrer"">Let's build GPT: from scratch, in code, spelled out.</a></p>
<blockquote>
<p>Very few details about the Transformer have changed in the last five years, but there is something slightly departs from the original paper. You see that Add and Norm is applied <strong>after</strong> the transformation (Multi Head Attention). But now it is more common to apply LayerNorm before the transformation, so there is a reshuffling of the Layer Norm. This is called <strong>pre-norm formulation</strong> and that is the one we are going to implement as well.</p>
</blockquote>
<p>This is proposed in <a href=""https://arxiv.org/pdf/2002.04745.pdf"" rel=""noreferrer"">On Layer Normalization in the Transformer Architecture</a>.</p>
<p><a href=""https://i.sstatic.net/GXxYV.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/GXxYV.png"" alt=""enter image description here"" /></a></p>
<p>The Annotated Transformer is also following this approach.</p>
","2024-01-24 02:03:00","5","Answer"
"77869986","77869443","","<p>You have a capital <code>X</code> in the <code>forward</code> method, while the function expects lowercase <code>x</code>. You must have a tensor with shape (2, 20) assigned to variable capital <code>X</code>.</p>
","2024-01-24 00:39:18","2","Answer"
"77869946","","How to transmute a flattened tensor/array back to original size while keeping specific order","<p>I have a flattened pytorch tensor that represents the indices for reverting a MaxPool2d operation, after getting the gradients for the backwards pass. The issue is that the size changes based on the kernel_size and the original input height/width before the MaxPool forward pass.
For example, lets say I have an input of size 2x4 with a kernel_size of 2x2 (kernel always square, and stride always same as kernel):</p>
<p><code>input1 = tensor([[1,2,5,6], [3,4,7,8]])</code>
For input1, after the forward pass, the indices will be [3,3](indexing from 0).
In the backwards pass, I then, after flattening and adding the gradient, end up with a tensor like this:
tensor([0,0,0,(gradient of 4),0,0,0,(gradient of 8)])</p>
<p>The issue is that I now need to transform the tensor back into the original shape while keeping the number order. Using something simple like tensor.view(input_height,input_width) doesn't work because the order of the numbers is messed up:
<code>input1.view(2,4) = tensor([[1,2,3,4],  [5,6,7,8]])</code></p>
<p>I've tried things like chunking it into groups of input_height/input_width but then I have issues making it work for different sizes. I think there is an easy solution and I just lack the pytorch or numpy skills to figure it out :(</p>
<p>Edit:
Ok so I've tried <code>input1.view(2,4)</code> which as I've explained doesn't have the right order. And getting the right order isn't possible through permute or similar functions. I've tried chunking this way:
<code>input1.view(-1, 2)</code> which splits the tensor into chunks of 2, but then I can't stack them back up properly into multiple columns. I've searched everywhere and I don't know how to progress. I've even (to no avail) asked chatGPT lol.</p>
","2024-01-24 00:20:11","0","Question"
"77869443","","Create custom model class with python","<p>I have an example class as follows:</p>
<pre><code>class MLP(nn.Module):
    # Declare a layer with model parameters. Here, we declare two fully
    # connected layers
    def __init__(self):
        # Call the constructor of the `MLP` parent class `Module` to perform
        # the necessary initialization. In this way, other function arguments
        # can also be specified during class instantiation, such as the model
        # parameters, `params` (to be described later)
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # Hidden layer
        self.out = nn.Linear(256, 10)  # Output layer

    # Define the forward propagation of the model, that is, how to return the
    # required model output based on the input `X`
    def forward(self, X):
        # Note here we use the funtional version of ReLU defined in the
        # nn.functional module.
        return self.out(torch.relu(self.hidden(X)))
</code></pre>
<p>Calling the class is like this:
<code>net = MLP() net(X)</code></p>
<p>Now, I need to create a similar class and function for a model with 4 layers:</p>
<p>Layers  Configuration   Activation Function
fully connected input size 128, output size 64  ReLU
fully connected input size 64, output size 32   ReLU
dropout probability 0.5 -
fully connected input size 32, output size 1    Sigmoid</p>
<p>I need to pass the following assertion:</p>
<pre><code>model = Net()

assert model.fc1.in_features == 128
assert model.fc1.out_features == 64
assert model.fc2.in_features == 64
assert model.fc2.out_features == 32
assert model.fc3.in_features == 32
assert model.fc3.out_features == 1

x = torch.rand(2, 128)
output = model.forward(x)
assert output.shape == (2, 1), &quot;Net() is wrong!&quot;
</code></pre>
<p>Here is what I have so far:</p>
<pre><code>class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
                
        
        self.fc1 = nn.Linear(128, 64)
        self.fc2 = nn.Linear(64, 32)
        self.dropout = nn.Dropout(p=0.5)
        self.fc3 = nn.Linear(32, 1)
        

    def forward(self, x):
        return self.fc3(torch.sigmoid(self.dropout(self.fc2(torch.relu(self.fc1(torch.relu(X)))))))
       
</code></pre>
<p>But I'm getting an error:</p>
<pre><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x20 and 128x64)
</code></pre>
<p>How to resolve it?</p>
","2024-01-23 21:44:52","2","Question"
"77869223","77869081","","<p>The issue was solved by configuring the model pad toked ID</p>
<pre><code>model.config.pad_token_id=50256
</code></pre>
","2024-01-23 20:55:14","2","Answer"
"77869081","","gpt2 tokenizer issue ( AssertionError: Cannot handle batch sizes > 1 if no padding token is defined )","<p>I am trying to train gpt2 on an IMDB Sentimental dataset for a classification task.</p>
<p>The dataset looks like the following:</p>
<p><a href=""https://i.sstatic.net/vZGhT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vZGhT.png"" alt=""enter image description here"" /></a></p>
<p>my code is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>    import pandas as pd
    from sklearn.model_selection import train_test_split
    from torch.utils.data import DataLoader, TensorDataset
    from transformers import GPT2Tokenizer, GPT2ForSequenceClassification , AdamW
    import numpy as np
    import torch

    df = pd.read_csv('data/IMDB.csv')
    x = df['text'].tolist()
    y = df['label'].tolist()
    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
    tokenizer.add_special_tokens({'pad_token':'&lt;|endoftext|&gt;'})
    tokenizer.padding_side=&quot;left&quot;
    tokenized_text = [tokenizer.encode(text, truncation=True, padding='max_length', max_length=128) for text in x]
    
    # Step 3: Split the dataset into training and testing sets
    x_train, x_test, y_train, y_test = train_test_split(tokenized_text, y, test_size=0.2, random_state=42)
    # Convert tokens to PyTorch tensors
    x_train_tensors = torch.tensor(x_train)
    y_train_tensors = torch.tensor(y_train)
    x_test_tensors = torch.tensor(x_test)
    y_test_tensors = torch.tensor(y_test)
    
    # Create DataLoader for training and testing sets
    train_dataset = TensorDataset(x_train_tensors, y_train_tensors)
    test_dataset = TensorDataset(x_test_tensors, y_test_tensors)
    
    batch_size =8
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    model = GPT2ForSequenceClassification.from_pretrained(&quot;gpt2&quot;, num_labels=2)  # Assuming binary classification
    # Move the model to the appropriate device
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
    # Training loop
    num_epochs = 3
    for epoch in range(num_epochs):
        model.train()
        for inputs, labels in train_dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
    
            optimizer.zero_grad()
            outputs = model(inputs, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
    
        # Evaluation on the test set
        model.eval()
        with torch.no_grad():
            correct = 0
            total = 0
            for inputs, labels in test_dataloader:
                inputs, labels = inputs.to(device), labels.to(device)
    
                outputs = model(inputs)
                predictions = torch.argmax(outputs.logits, dim=1)
    
                total += labels.size(0)
                correct += (predictions == labels).sum().item()
    
            accuracy = correct / total
            print(f&quot;Epoch {epoch + 1}, Test Accuracy: {accuracy:.4f}&quot;)

</code></pre>
<p>I got the following error at <code>outputs = model(inputs, labels=labels)</code>:</p>
<blockquote>
<p>AssertionError: Cannot handle batch sizes &gt; 1 if no padding token is
defined.</p>
</blockquote>
<p>I already defined the no padding token so I have no idea why this assertion error appears.</p>
","2024-01-23 20:27:47","1","Question"
"77867395","77866054","","<p>You can use <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html"" rel=""nofollow noreferrer""><code>torch.nn.LazyLinear</code></a> instead of the <code>torch.nn.Linear</code> module.</p>
<p>The advantage of <code>LazyLinear</code> is that the value of the <code>in_features</code> argument is inferred <em>lazily</em> after the first forward pass, from the shape of the last dimension of the input to the module. In your case, that would be the output of the dropout layer.</p>
<p>Essentially, you would need:</p>
<pre><code>self.fc = torch.nn.LazyLinear(out_features=2)
</code></pre>
<p>FWIW, the weight and bias would also be initialized after the first forward pass (they start as uninitialized).</p>
","2024-01-23 15:20:47","1","Answer"
"77866335","77865949","","<p>This is a fun question. When playing around with your Code, I found that the execution time of the network itself is minimal compared to any reshaping or data mangling (which explains your strange observations).</p>
<p>Given that your &quot;real&quot; problem has a much larger sample size (and the time reshaping can be diminished compared to execution time), I suggest this third way:</p>
<pre class=""lang-py prettyprint-override""><code>input_data = torch.rand(size=(8, input_size))
mlp_index = torch.tensor([2, 1, 0, 1, 0, 2, 0, 2])

# make batches per model that can be executed at once
input_1 = input_data[(mlp_index == 0)[:,0]]
input_2 = input_data[(mlp_index == 1)[:,0]]
input_3 = input_data[(mlp_index == 2)[:,0]]

# execute batches
out_1 = mlp1.forward(input_1)
out_2 = mlp2.forward(input_2)
out_3 = mlp3.forward(input_3)

# stitching result
res = torch.empty((8, output_size))
res[(mlp_index == 0)[:,0]] = out_1
res[(mlp_index == 1)[:,0]] = out_2
res[(mlp_index == 2)[:,0]] = out_3

</code></pre>
<p>If your network is really that small (and your samples are really so few), indexing your data takes longer than feeding it through the network...</p>
<h1>Edit analysis of depth and speed</h1>
<p>Note, that I introduced a <code>depth</code> parameter.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import timeit

torch.manual_seed(42)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)


class MLP0(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, depth):
        super(MLP0, self).__init__()
        self.depth = depth
        self.first = nn.Linear(input_size, hidden_size)
        for i in range(2, depth-1):
            # generate (depth - 2) hidden layer
            setattr(self, f&quot;fc{i}&quot;, nn.Linear(hidden_size, hidden_size))
        self.last = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = F.relu(self.first(x))
        for i in range(2, depth-2):
            # execute hidden layer
            x = F.relu(getattr(self, f&quot;fc{i}&quot;)(x))
        x = self.last(x)
        return x


# arbitrary parameters
input_size = 10
hidden_size = 20
output_size = 5
depth = 200

mlp1 = MLP0(input_size, hidden_size, output_size, depth)
mlp1.to(device)
mlp2 = MLP0(input_size, hidden_size, output_size, depth)
mlp2.to(device)
mlp3 = MLP0(input_size, hidden_size, output_size, depth)
mlp3.to(device)

input_data = torch.rand(size=(8, input_size), device=device)
mlp_index = torch.tensor([0, 1, 0, 1, 0, 2, 0, 2], device=device).unsqueeze(1)

def baseline():
    &quot;&quot;&quot;no fair comparison as this only executes model one. Sole purpose as a baseline as this cannot be topped.&quot;&quot;&quot;
    result = mlp1.forward(input_data)
    return result


def first_update():
    out_1 = mlp1.forward(input_data)
    out_2 = mlp2.forward(input_data)
    out_3 = mlp3.forward(input_data)
    result = torch.where(
        mlp_index == 0,
        out_1,
        torch.where((mlp_index != 0) &amp; (mlp_index != 2), out_2, out_3),
    )
    return result


def second_update():
    result = torch.where(
        mlp_index == 0,
        mlp1(input_data),
        torch.where(
            (mlp_index != 0) &amp; (mlp_index != 2), mlp2(input_data), mlp3(input_data)
        ),
    )
    return result


def third_update():
    # make batches per model that can be executed at once
    # execute batches
    out_1 = mlp1.forward(input_data[(mlp_index == 0)[:, 0]])
    out_2 = mlp2.forward(input_data[(mlp_index == 1)[:, 0]])
    out_3 = mlp3.forward(input_data[(mlp_index == 2)[:, 0]])

    out = torch.zeros(size=(8, output_size), device=device)
    out[(mlp_index == 0)[:, 0]] = out_1
    out[(mlp_index == 1)[:, 0]] = out_2
    out[(mlp_index == 2)[:, 0]] = out_3
    return out


baseline_time = timeit.timeit(baseline, number=200)
print(f&quot;Execution time: {baseline_time} seconds&quot;)

first_update_time = timeit.timeit(first_update, number=200)
print(f&quot;Execution time: {first_update_time} seconds&quot;)

second_update_time = timeit.timeit(second_update, number=200)
print(f&quot;Execution time: {second_update_time} seconds&quot;)

third_update_time = timeit.timeit(third_update, number=200)
print(f&quot;Execution time: {third_update_time} seconds&quot;)
</code></pre>
<p>result is</p>
<pre><code>Execution time: 1.3142005730001074 seconds
Execution time: 4.0445914549995905 seconds
Execution time: 3.7615038879998792 seconds
Execution time: 3.5695767729998806 seconds
</code></pre>
<p>Fully connected layer have massive amount of parameters (compared to CNN), but are very fast to calculate. The forward pass through such layers is simply a matrix multiplication. The size of this matrix (at scales that we have considered here) does not influence the duration it takes to multiply two of them. That is the reason why your attempts to increase the computational efforts did not succeed as you only increased the size of the hidden weight matrix. What takes time, however, is to wait for another multiplication to finish. Therefore I introduced a depth parameter. When the depth is increased it becomes computational cheaper to pre-select the important rows by indexing, as you can see in my example.</p>
","2024-01-23 12:35:33","1","Answer"
"77866054","","variable in_features for internal torch layers","<p>I have created a convolutional network using pytorch and i want to optimize its' hyperparameters, e.g  kernel_size, in_channels, out_channels of each layer dropout_rate etc. The problem is i cannot find a way to parametrize the in_features of the final linear layer which means i have to hard code it every time according to the input size and the internal architecture. Is there a way to define the in_features of the fc layer to be equal to the output of the dropout layer
?</p>
<pre><code>class Conv_v0(torch.nn.Module):

    def __init__(self):
        super(Conv_v0, self).__init__()

        self.conv1 = torch.nn.Conv1d(in_channels=4, out_channels=3, kernel_size=17)
        self.activation = torch.nn.ReLU()
        self.maxpool = torch.nn.MaxPool1d(kernel_size=5)
        
        self.dropout = torch.nn.Dropout(p=0.5) 
        #in_features = 108 for 200kb, 588 for 1kb, 1188 for 2kb
        self.fc = torch.nn.Linear(in_features=1188, out_features=2)
        #self.sigmoid = torch.nn.Sigmoid() will not be used since its intergraded in BCEWithLogitsLoss()

    def forward(self, x):
        x= x.permute(0, 2, 1)
        x = self.conv1(x)
        x = self.activation(x)
        x = self.maxpool(x)
        
        # Reshape the output of the max pooling layer before passing it to the fully connected layer
        x = x.view(x.size(0), -1)
        
        #print(&quot;Size after reshaping:&quot;, x.size())
        x=self.dropout(x)
        x = self.fc(x)
        #x = self.sigmoid(x)
        return x
</code></pre>
<p>I have tried initializing  in_features=0 and changing it accordingly on the forward function but i am afraid this way the weigths of the final layer are initialized in every forward pass thus no learning is achieved</p>
<pre><code>class Conv2(nn.Module):
    def __init__(self, out_channels1, kernel_size1, out_channels2, kernel_size2, dropout_rate):
        super(Conv2, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=4, out_channels=out_channels1, kernel_size=kernel_size1)

        self.conv2 = nn.Conv1d(in_channels = out_channels1, out_channels = out_channels2, kernel_size=kernel_size2)
        self.dropout = torch.nn.Dropout(p=dropout_rate)
        self.fc = torch.nn.Linear(in_features=0, out_features=2)

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = F.max_pool1d(torch.tanh(self.conv1(x)), 2)
        x = F.max_pool1d(torch.tanh(self.conv2(x)), 2)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)

        # Dynamically set the in_features for the fc layer
        if self.fc.in_features == 0:
            self.fc.in_features = x.size(1)
            self.fc = nn.Linear(in_features=x.size(1), out_features=2)  

        out = self.fc(x)
        return out
</code></pre>
","2024-01-23 11:46:35","1","Question"
"77865949","","How to efficiently apply different MLP´s on different areas of my input tensor in pytorch","<p>Given input tensor of size <code>input(8,10)</code> I got three MLP´s(1,2,3) that all have the input size of 10. Furthermore I have a an index tensor <code>mlp_index(8)</code> which determines the <code>mlp</code> I want to apply onto a certain row in my input. For Example if <code>mlp_index[0] = 2</code>, then the second MLP should be applied onto input[0]. I wrote a minimal example to showcase the problem, and two different ways of dealing with the problem efficiently. However, as you can see, applying just one MLP to the whole input is still significantly faster.</p>
<p>Question: Is there a more efficient way of dealing with that problem?</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import timeit

torch.manual_seed(42)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

class MLP0(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP0, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# arbitrary parameters
input_size = 10
hidden_size = 20
output_size = 5

mlp1 = MLP0(input_size, hidden_size, output_size)
mlp1.to(device)
mlp2 = MLP0(input_size, hidden_size, output_size)
mlp2.to(device)
mlp3 = MLP0(input_size, hidden_size, output_size)
mlp3.to(device)

input_data = torch.rand(size=(8, input_size), device=device)
mlp_index = torch.tensor([0, 1, 0, 1, 0, 2, 0, 2], device=device).unsqueeze(1)


def baseline():
    result = mlp1.forward(input_data)
    return result


def first_update():
    out_1 = mlp1.forward(input_data)
    out_2 = mlp2.forward(input_data)
    out_3 = mlp3.forward(input_data)
    result = torch.where(mlp_index == 0, out_1, torch.where((mlp_index!=0) &amp; (mlp_index!=2), out_2 , out_3))
    return result


def second_update():
    result = torch.where(mlp_index == 0, mlp1(input_data), torch.where((mlp_index!=0) &amp; (mlp_index!=2), mlp2(input_data) , mlp3(input_data)))
    return result


def third_update():
    # make batches per model that can be executed at once
    input_1 = input_data[(mlp_index == 0)[:,0]]
    input_2 = input_data[(mlp_index == 1)[:,0]]
    input_3 = input_data[(mlp_index == 2)[:,0]]

    # execute batches
    out_1 = mlp1.forward(input_1)
    out_2 = mlp2.forward(input_2)
    out_3 = mlp3.forward(input_3)

    out = torch.zeros(size=(8, output_size), device=device)
    out[(mlp_index == 0)[:,0]] = out_1
    out[(mlp_index == 1)[:,0]] = out_2
    out[(mlp_index == 2)[:,0]] = out_3
    return out

    

baseline_time = timeit.timeit(baseline, number=20000)
print(f&quot;Execution time: {baseline_time} seconds&quot;)

first_update_time = timeit.timeit(first_update, number=20000)
print(f&quot;Execution time: {first_update_time} seconds&quot;)

second_update_time = timeit.timeit(second_update, number=20000)
print(f&quot;Execution time: {second_update_time} seconds&quot;)

third_update_time = timeit.timeit(third_update, number=20000)
print(f&quot;Execution time: {third_update_time} seconds&quot;)

</code></pre>
<p>Ouput:</p>
<pre><code>Execution time: 1.5391472298651934 seconds
Execution time: 2.1761511098593473 seconds
Execution time: 2.233237884938717 seconds
Execution time: 6.252682875841856 seconds
</code></pre>
","2024-01-23 11:29:16","3","Question"
"77865273","","RuntimeError: CUDA out of memory when .append?","<p>I have 50 clients but I would like to train each 5 clients seperately and save their weights in <code>w_locals</code> after that I would like to save all the weights in <code>w_totals</code></p>
<pre><code>w_totals = []
for iter in range(epochs):
  
    w_locals = [w_glob for i in range(client_5)]
    
    for idxs_task in range(task_num):
        
        dataset = load_data() 
        
        for client_idx in range(client_5):
                   
            dataloader_train = load_data_train()
            w = train(copy.deepcopy(net_glob).to(device), dataloader_train) 
            w_locals[client_idx] = copy.deepcopy(w)
                      
        
        w_totals.append(copy.deepcopy(w_locals))
</code></pre>
<p>I'm sure that the error come when I add this line <code>w_totals.append(copy.deepcopy(w_locals))</code>
So how can I resolve the problem and</p>
","2024-01-23 09:40:55","0","Question"
"77864818","77860679","","<p>EDIT: this solution works if the class is a subclass of NamedTuple because the JIT tracer doesn't know how to handle return types other than <code>torch.Tensor</code> <code>tuple</code> <code>list</code> or <code>dict</code> (or the deprecated <code>torch.Variable</code>) in <code>torch.nn.Module</code>'s <code>forward</code> method.</p>
<p>I've been thinking about this for a while and I think the best solution is simply to wrap the original tensor in some kind of wrapper class, then have the original tensor be accessible via a <code>tensor_wrapper.tensor</code> attribute. I.e. something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import typing

import torch

class TensorWrapper(typing.NamedTuple):
    tensor: torch.Tensor

    @property
    def foo(self) -&gt; torch.Tensor:
        return self.tensor[..., 0:1]

    ...  # other methods
</code></pre>
<p>Only downside is that one can no longer directly access the tensor, but I suppose it's better practice anyway since turning a <code>torch.Tensor</code> into a subclassed tensor like <code>MyTensor</code> probably copies memory (including the graph) and incurs extra overhead.</p>
","2024-01-23 08:16:20","0","Answer"
"77864704","","Annotated Transformer - Why x + DropOut(Sublayer(LayerNorm(x)))?","<p>Please clarify if the <a href=""https://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks"" rel=""nofollow noreferrer"">Annotated Transformer</a> Encoder LayerNorm implementation is correct.</p>
<p><a href=""https://i.sstatic.net/8rLZkm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8rLZkm.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Transformer paper</a> says the output of the sub layer is <code>LayerNorm(x + Dropout(SubLayer(x)))</code>.</p>
<p><a href=""https://i.sstatic.net/bl9MS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bl9MS.png"" alt=""enter image description here"" /></a></p>
<p><code>LayerNorm</code> should be applied <strong>after</strong> the <code>DropOut(SubLayer(x))</code> as per the paper:</p>
<p><a href=""https://i.sstatic.net/HrYPa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HrYPa.png"" alt=""enter image description here"" /></a></p>
<p>However, the <a href=""https://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks"" rel=""nofollow noreferrer"">Annotated Transformer</a> implementation does <code>x + DropOut(SubLayer(LayerNorm(x)))</code> where <code>LayerNorm</code> is applied <strong>before</strong> <code>Sublayer</code>, which is the other way around.</p>
<pre><code>class SublayerConnection(nn.Module):
    &quot;&quot;&quot;
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    &quot;&quot;&quot;

    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        &quot;Apply residual connection to any sublayer with the same size.&quot;
        return x + self.dropout(sublayer(self.norm(x)))   # &lt;--- LayerNorm before SubLayer
</code></pre>
","2024-01-23 07:49:43","3","Question"
"77864576","77864342","","<p>First, the example</p>
<pre class=""lang-py prettyprint-override""><code>def f(x,y):
   return x 
</code></pre>
<p>Autograd tracks operations performed. Your function only returns the input tensor <code>x</code>. Since no operations are performed, there is no gradient to compute. When you compute the first derivative, you get a tensor <code>[1., 1.]</code>, which is the gradient of <code>x</code> with respect to itself. Since no computations have been performed, the gradient tensor is not part of any computation graph. As a result, the <code>requires_grad</code> attribute of the first derivative is zero. When you then try to compute the second derivative, your <code>z</code> value is the first derivative tensor which has no grad attribute, raising the error.</p>
<p>Second, the example of</p>
<pre class=""lang-py prettyprint-override""><code>def f(x,y):
    return x*y 
</code></pre>
<p>The first derivative of <code>z</code> wrt <code>x</code> is <code>y</code>. For computational simplicity, <code>MulBackward</code> populates the grad tensor with the data/computational graph of <code>y</code>. This means the first derivative tensor does not have <code>x</code> in it's computational graph. You can verify this by removing the <code>allow_unused</code> parameter.</p>
<pre class=""lang-py prettyprint-override""><code>def f(x,y):
    return x*y 
x = torch.tensor([3.0,2.0], requires_grad=True)
y = torch.tensor([2.0,3.0], requires_grad=True)
z = f(x,y)

dx = torch.autograd.grad(z, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]
dx2 = torch.autograd.grad(dx, x, grad_outputs=torch.ones_like(x))[0]
</code></pre>
<p>Computing <code>dx2</code> will raise the error <code>One of the differentiated Tensors appears to not have been used in the graph</code>. There is no chain of operations linking <code>dx</code> to <code>x</code>. As a result, there is no derivative to be computed.</p>
<p>If you try compute the second derivative with a function that retains <code>x</code> in the first derivative compute chain, things will work.</p>
<pre class=""lang-py prettyprint-override""><code>def f(x,y):
    return x.pow(y )
x = torch.tensor([3.0,2.0], requires_grad=True)
y = torch.tensor([2.0,3.0], requires_grad=True)
z = f(x,y)

dx = torch.autograd.grad(z, x, grad_outputs=torch.ones_like(x), create_graph=True)[0]
dx2 = torch.autograd.grad(dx, x, grad_outputs=torch.ones_like(x))[0]
</code></pre>
","2024-01-23 07:20:57","2","Answer"
"77864342","","Why Pytorch autograd returns 'None' and throws error without variable exponentiated","<p>I have some strange behavior I do not understand from Pytorch autograd, while trying to compute the second partial derivative with respect to <code>x</code>. The code</p>
<pre><code>def f(x,y):
   return x**1 
x = torch.tensor([3.0,2.0], requires_grad=True)
y = torch.tensor([2.0,3.0], requires_grad=True)
z = f(x,y)
# differentiate z with respect to x twice
for _ in range(2):
    dx = torch.autograd.grad(z, x, grad_outputs=torch.ones_like(x), create_graph=True, allow_unused=True)[0]
    z = dx
print(z)
</code></pre>
<p>as expected, gives</p>
<pre><code>tensor([0., 0.])
</code></pre>
<p>However, without exponentiating x to 1, and multiplying by <code>y</code> as follows</p>
<pre><code>def f(x,y):
    return x*y 
x = torch.tensor([3.0,2.0], requires_grad=True)
y = torch.tensor([2.0,3.0], requires_grad=True)
z = f(x,y)
# differentiate z with respect to x twice
for _ in range(2):
    dx = torch.autograd.grad(z, x, grad_outputs=torch.ones_like(x), create_graph=True, allow_unused=True)[0]
    z = dx
print(z)
</code></pre>
<p>we get the output</p>
<pre><code>None
</code></pre>
<p>Worst of all, just having the function return x as follows:</p>
<pre><code>def f(x,y):
   return x 
x = torch.tensor([3.0,2.0], requires_grad=True)
y = torch.tensor([2.0,3.0], requires_grad=True)
z = f(x,y)
# differentiate z with respect to x twice
for _ in range(2):
    dx = torch.autograd.grad(z, x, grad_outputs=torch.ones_like(x), create_graph=True, allow_unused=True)[0]
    z = dx
print(z)
</code></pre>
<p>Throws the following error:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
Cell In[59], line 9
  7 # differentiate z with respect to x twice
  8 for _ in range(2):
----&gt; 9     dx = torch.autograd.grad(z, x, grad_outputs=torch.ones_like(x), create_graph=True, allow_unused=True)[0]
 10     z = dx
 11 print(z)

 File ~/miniconda3/envs/randomvenv/lib/python3.8/site-packages/torch/autograd/__init__.py:394, in grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)
390     result = _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(
391         grad_outputs_
392     )
393 else:
--&gt; 394     result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
395         t_outputs,
396         grad_outputs_,
397         retain_graph,
398         create_graph,
399         t_inputs,
400         allow_unused,
401         accumulate_grad=False,
402     )  # Calls into the C++ engine to run the backward pass
403 if materialize_grads:
404     result = tuple(
405         output
406         if output is not None
407         else torch.zeros_like(input, requires_grad=True)
408         for (output, input) in zip(result, t_inputs)
409     )

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>Does anyone understand what is going on?</p>
","2024-01-23 06:24:13","1","Question"
"77864227","","I use Diffusers to train LoRA. Training images are my photos, but the result image is not like me","<p>Here is my training code.</p>
<pre><code>from accelerate.utils import write_basic_config
write_basic_config()

import os

os.environ[&quot;MODEL_NAME&quot;] = &quot;runwayml/stable-diffusion-v1-5&quot;
os.environ[&quot;INSTANCE_DIR&quot;] = &quot;/notebooks/me_photos&quot;
os.environ[&quot;OUTPUT_DIR&quot;] = &quot;/notebooks/me_model_1_22&quot;
script_path = &quot;/notebooks/diffusers/examples/dreambooth/train_dreambooth_lora.py&quot;

!accelerate launch {script_path} \
  --pretrained_model_name_or_path={os.environ[&quot;MODEL_NAME&quot;]} \
  --instance_data_dir={os.environ[&quot;INSTANCE_DIR&quot;]} \
  --output_dir={os.environ[&quot;OUTPUT_DIR&quot;]} \
  --instance_prompt=&quot;a photo of Ryan&quot; \
  --resolution=512 \
  --train_batch_size=1 \
  --learning_rate=2e-6 \
  --max_train_steps=2400 \
  --gradient_checkpointing \
  --use_8bit_adam \
  --with_prior_preservation \
  --prior_loss_weight=1.0 \
  --class_data_dir=&quot;/notebooks/faces_prior_preservation&quot; \
  --class_prompt=&quot;a photo of person face&quot;
</code></pre>
<p>Here is the code to produce image.</p>
<pre><code>from diffusers import DiffusionPipeline
import torch

# Initialize logging
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()


pipe = DiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, 
                                         torch_dtype=torch.float16, 
                                         use_safetensors=True, 
                                         variant=&quot;fp16&quot;)
pipe.to(&quot;cuda&quot;)


pipe.load_lora_weights(&quot;/notebooks/me_model_1_22&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot; , adapter_name=&quot;me&quot;)

active_adapters = pipe.get_active_adapters()
active_adapters

logger.info(f&quot;LoRA {active_adapters} loaded successfully.&quot;)


# Generate an image
prompt = &quot;a photo of Ryan&quot;
lora_scale= 1
image = pipe(
    prompt, num_inference_steps=30, cross_attention_kwargs={&quot;scale&quot;: lora_scale}
).images[0]

# Save the image
output_path = &quot;/notebooks/image_of_me2.png&quot;
image.save(output_path)
logger.info(f&quot;Image saved at {output_path}&quot;)

# Clean up
del image
torch.cuda.empty_cache()


</code></pre>
","2024-01-23 05:52:20","-2","Question"
"77863605","77863169","","<p>By default, pytorch expects you to call <code>backward</code> on a scalar value. This is why if you call <code>y.backward()</code>, you get the error <code>grad can be implicitly created only for scalar outputs</code>.</p>
<p>One solution is to aggregate <code>y</code> with a <code>sum</code> operation, which doesn't affect downstream gradients.</p>
<pre class=""lang-py prettyprint-override""><code>x = np.linspace(-np.pi, np.pi, 100) 
x = torch.tensor(x, requires_grad=True) 
y = torch.sin(x)
loss = y.sum()
loss.backward()
</code></pre>
<p>You can also call <code>backward</code> on a vector value if you provide an upstream gradient vector.</p>
<pre class=""lang-py prettyprint-override""><code>x = np.linspace(-np.pi, np.pi, 100) 
x = torch.tensor(x, requires_grad=True) 
y = torch.sin(x)
grad_vector = torch.ones_like(y)
y.backward(grad_vector)
</code></pre>
<p>Both methods pass the assertions you provided.</p>
","2024-01-23 01:31:23","0","Answer"
"77863169","","Backpropagation and gradient descent with python","<p>I am new to gradient descent and I'm completely lost on the exercise below. The first part is an explanation with a simple example. Here is that example:</p>
<p>When training the model, we want to find parameters (denoted as Θ
) that minimize the total loss across all training examples:</p>
<p>Θ=argminΘ 𝐿(Θ).
To do this, we will iteratively reduce the error by updating the parameters in the direction that incrementally lowers the loss function. This algorithm is called gradient descent. The most naive application of gradient descent consists of taking the derivative of the loss function. Let us see how to do this.</p>
<p>As a toy example, say that we are interested in differentiating the function  𝑦=2𝐱⊤𝐱
with respect to the column vector  𝐱
. To start, let us create the variable x and assign it an initial value.</p>
<p>Here is the code:</p>
<pre><code>x = torch.arange(4.0)
x.requires_grad_(True) 
x.grad 
y = 2 * torch.dot(x, x)
y.backward()
x.grad
#checking if gradient calculated correctly
x.grad == 4 * x
</code></pre>
<p>And now, based on the above I have to solve this:</p>
<p>Let  𝑓(𝑥)=sin(𝑥). Plot  𝑓(𝑥)  and and  𝑑𝑓(𝑥)𝑑𝑥, where the latter is computed without exploiting that  𝑓′(𝑥)=cos(𝑥).</p>
<p><code>x = np.linspace(-np.pi, np.pi, 100) x = torch.tensor(x, requires_grad=True) y = torch.sin(x)</code></p>
<p>...and now what?</p>
<p>I tried:</p>
<p><code>y.backward() x.grad</code></p>
<p>but I'm getting an error that y is not a scalar value.</p>
<p>I need to pass these assertions:
<code>assert torch.allclose(x.grad[10].float(), torch.Tensor([-0.8053]), rtol=1e-2) assert torch.allclose(x.grad[50].float(), torch.Tensor([0.9995]), rtol=1e-2) </code></p>
","2024-01-22 22:39:18","1","Question"
"77863153","77861879","","<p>According to the second issue in the <a href=""https://github.com/openclimatefix/graph_weather/issues/82"" rel=""nofollow noreferrer"">repo</a> (4 days back), you can simply change the feature shape vector from <code>78</code> to <code>102</code> dimensions:</p>
<pre class=""lang-py prettyprint-override""><code>features = torch.randn((2, len(lat_lons), 102))
</code></pre>
","2024-01-22 22:35:37","1","Answer"
"77861969","77858593","","<p>The pytorch <code>DataLoader</code> class has a <code>collate_fn</code> that processes dataset items into a batch. Using the example from the <a href=""https://pytorch.org/docs/stable/data.html"" rel=""nofollow noreferrer"">pytorch documentation</a>, it works like this:</p>
<pre class=""lang-py prettyprint-override""><code>for indices in batch_sampler:
    yield collate_fn([dataset[i] for i in indices])
</code></pre>
<p>If you don't pass a <code>collate_fn</code>, pytorch automatically uses <code>default_collate</code>. The behavior of <code>default_collate</code> depends on the types from you dataset, defined <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.default_collate"" rel=""nofollow noreferrer"">here</a>.</p>
","2024-01-22 18:26:24","0","Answer"
"77861937","77854708","","<p>You don't have a gradient chain between your model and your <code>box_loss</code>.</p>
<p>The model outputs <code>pred</code>. <code>pred</code> is the variable that allows you to backprop back into your weights.</p>
<p>When you create bounding boxes (<code>boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])</code>), you break the gradient chain and can no longer backprop.</p>
<p>I see the model output <code>cls</code> (which might have a gradient chain, I can't tell) is added to <code>scores</code>. But then you add the <code>scores</code> terms to <code>coords</code> (<code>coords[:, 0] = torch.tensor(scores)</code>) which also breaks the gradient chain.</p>
<p>As a result, the <code>pred_boxes</code> tensor you pass to your loss function has no way of backproping into the model.</p>
","2024-01-22 18:19:44","0","Answer"
"77861879","","RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 78 but got size 102 for tensor number 1 in the list","<p>I am trying to try the example of this library <a href=""https://github.com/openclimatefix/graph_weather?tab=readme-ov-file"" rel=""nofollow noreferrer"">graph_weather</a></p>
<pre><code>import torch
from graph_weather import GraphWeatherForecaster
from graph_weather.models.losses import NormalizedMSELoss

lat_lons = []
for lat in range(-90, 90, 1):
    for lon in range(0, 360, 1):
        lat_lons.append((lat, lon))
model = GraphWeatherForecaster(lat_lons)

features = torch.randn((2, len(lat_lons), 78))

out = model(features)
criterion = NormalizedMSELoss(lat_lons=lat_lons, feature_variance=torch.randn((78,)))
loss = criterion(out, features)
loss.backward()
</code></pre>
<p>When I get to the line of code <code>out = model(features)</code> I got this error message:</p>
<pre><code>RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 78 but got size 102 for tensor number 1 in the list.
</code></pre>
<p>the</p>
<pre><code>len(lat_lons)
64800
features.shape
torch.Size([2, 64800, 78])
</code></pre>
","2024-01-22 18:09:37","0","Question"
"77861642","77861305","","<p>Just use <code>.data</code> attribute</p>
<pre><code>print(&quot;Weight:&quot;, net.fc1.weight[0].data)
</code></pre>
","2024-01-22 17:28:32","1","Answer"
"77861374","77861305","","<pre><code>weight_without_grad = net.fc1.weight[0].clone().requires_grad_(False)
print(&quot;Weight:&quot;, weight_without_grad)
</code></pre>
<p>This clones the tensor and print it. another way is using <code>detach()</code> but I wont suggest because as the name suggests it detaches the tensor</p>
","2024-01-22 16:36:35","1","Answer"
"77861305","","How to remove grad_fn when printing neural network weights","<p>I have a neural net that uses nn.Linear connections between layers. When printing the weights between input and hidden layers with the code below:</p>
<pre><code>print(&quot;Weight:&quot;, net.fc1.weight[0])
</code></pre>
<p>The print out looks like this:</p>
<pre><code>Weight: tensor([ 0.0375,  0.1901,  0.0787,  0.2476,  0.0740,  0.2848, -0.2852, -0.0864,
     0.1827,  0.1384], grad_fn=&lt;SelectBackward0&gt;)
</code></pre>
<p>Is there a way to stop printing out the <code>grad_fn=&lt;SelectBackward0&gt;</code> and just print the weights out like this:</p>
<pre><code>Weight: tensor([ 0.0375,  0.1901,  0.0787,  0.2476,  0.0740,  0.2848, -0.2852, -0.0864,
 0.1827,  0.1384])
</code></pre>
","2024-01-22 16:23:32","0","Question"
"77861105","","Inference speed isn't improved with tensor-rt compared to regular cuda","<p>I'm trying to use the tensor-rt framework to enhance the inference speed of my deep learning model. I've created a very simple python code to test tensor-rt with pytorch.</p>
<pre><code>import torch
import argparse
import time
import numpy as np
import torch_tensorrt

# Define a simple PyTorch model
class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.relu1 = torch.nn.ReLU()
        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.relu2 = torch.nn.ReLU()
        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = torch.nn.Linear(64 * 16 * 16, 512)
        self.relu3 = torch.nn.ReLU()
        self.fc2 = torch.nn.Linear(512, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool(x)
        x = x.view(-1, 64 * 16 * 16)
        x = self.fc1(x)
        x = self.relu3(x)
        x = self.fc2(x)
        return x

def compute(use_tensorrt=False):
    force_cpu = False
    useCuda = torch.cuda.is_available() and not force_cpu
    if useCuda:
        print('Using CUDA.')
        dtype = torch.cuda.FloatTensor
        ltype = torch.cuda.LongTensor
        device = torch.device(&quot;cuda:0&quot;)
    else:
        print('No CUDA available.')
        dtype = torch.FloatTensor
        ltype = torch.LongTensor
        device = torch.device(&quot;cpu&quot;)

    model = MyModel()

    input_shape = (8192, 3, 32, 32)

    if use_tensorrt:
        model = torch.compile(
            model,
            backend=&quot;torch_tensorrt&quot;,
            options={
                &quot;truncate_long_and_double&quot;: True,
                &quot;precision&quot;: dtype,
                &quot;workspace_size&quot; : 20 &lt;&lt; 30
            },
            dynamic=False,
        )

    model = model.to(device)
    model.eval()

    num_iterations = 100
    total_time = 0.0
    with torch.no_grad():
        input_data = torch.randn(input_shape).to(device).type(dtype)
        #warmup
        for i in range(100):
            output_data = model(input_data)

        for i in range(num_iterations):
            start_time = time.time()
            output_data = model(input_data)
            end_time = time.time()
            total_time += end_time - start_time
    pytorch_fps = num_iterations / total_time
    print(f&quot;PyTorch FPS: {pytorch_fps:.2f}&quot;)

if __name__ == &quot;__main__&quot;:
    print(&quot;Without TensorRT&quot;)
    compute()
    print(&quot;With TensorRT&quot;)
    compute(use_tensorrt=True)
</code></pre>
<p>Unfortunately, when I run this code, I get approximately the same FPS with tensor-rt and without, which is ~14.2, even with a significant warmup. Does anyone know what could be the issue ? Is there something I'm missing ?</p>
<p>Here are some more information about my setup:</p>
<p>libraries:</p>
<pre><code>torch 2.0.1
torch_tensorrt 1.4.0
</code></pre>
<p>GPU:</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Cuda compilation tools, release 11.5, V11.5.119
Build cuda_11.5.r11.5/compiler.30672275_0
</code></pre>
","2024-01-22 15:49:58","1","Question"
"77860679","","JIT tracable PyTorch Tensor with meaningful properties","<h2>The Scenario</h2>
<p>I have a PyTorch module which outputs a tensor of shape <code>(batch_size, 3, 512, 7)</code>. The final dimension (7) is a tensor of meaningfully interpretable values which can be used in loss calculation.</p>
<p>I can access these values using slicing, and have successfully done so in the past like in the following examples</p>
<pre class=""lang-py prettyprint-override""><code>foo = tensor[..., 0:1]
target_foo = target_tensor[..., 0:1]
some_loss_function(foo, target_foo)
</code></pre>
<p>or</p>
<pre class=""lang-py prettyprint-override""><code>bar = tensor[..., 1:3]
target_bar = target_tensor[..., 1:3]
some_other_loss_function(bar, target_bar)
</code></pre>
<p>At first the code base was littered with these slicings. Since the slicing is not very human readable, unless one has specific knowledge of the how the network internally works, I have instead created my own tensor class which defines the various features neatly.</p>
<pre class=""lang-py prettyprint-override""><code>class MyTensor(torch.Tensor):

    @property
    def foo(self):
        return self[..., 0:1]

    @property
    def bar(self):
        return self[..., 1:3]
</code></pre>
<p>At some point in the model I have a module which wraps a resulting tensor in <code>MyTensor</code> during the fowrard pass</p>
<pre class=""lang-py prettyprint-override""><code>class MyModule(torch.nn.Module):

    def forward(self, x: torch.Tensor) -&gt; MyTensor:
        # do something to x
        x = x * 2  # this is purely exemplary, might be some network layers
        return MyTensor(x)
</code></pre>
<p>In a corresponding loss module I can now simply write</p>
<pre class=""lang-py prettyprint-override""><code>class MyLoss(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.foo_loss_function: callable  # some arbitrary loss function for foo
        self.bar_loss_function: callable  # some arbitrary loss function for bar

    def forward(self, x: MyTensor, target: MyTensor):
        foo_loss = self.foo_loss_function(x.foo, target.foo)
        bar_loss = self.bar_loss_function(x.bar, target.bar)
        return foo_loss + bar_loss
</code></pre>
<p>To me this is <em>much</em> more readable, and I would like to keep it this way if possible.</p>
<h2>The problem</h2>
<p>When now tracing the model containing a module like <code>MyModule</code> above, I get the following warning:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
model = MyModule()
some_input = torch.rand((32, 3, 512, 7))
torch.jit.trace(model, some_input)
</code></pre>
<blockquote>
<p>TracerWarning: torch.Tensor results are registered as constants in the trace.
You can safely ignore this warning if you use this function to create tensors
out of constant variables that would be the same every time you call this
function. In any other case, this might cause the trace to be incorrect.
return MyTensor(x)</p>
</blockquote>
<p>I understand why I'm getting this warning, the tracer apparently has no knowledge of how to dynamically assign values from the original tensor <code>x</code> to this <code>MyTensor</code> tensor. In principle, no operation should even be necessary since all I'm doing is trying to add some ease of use accessability properties to an existing tensor, but apparently the tracer doesn't know what to do here.</p>
<p>Has anyone run into an issue similar to this? Is there a way to keep the readability of using an approach like the one described above using <code>MyTensor</code> whilst still being able to trace the model? Any help would be much appreciated.</p>
","2024-01-22 14:44:28","1","Question"
"77860446","77859639","","<p>It's unavoidable due to the way floating point numbers are implemented.
For one, floating point arithmetic is not associative. You can verify it for yourself using something like this:</p>
<pre><code>print((0.7 + (0.2 + 0.1)) == 1) # True
print(((0.7 + 0.2) + 0.1) == 1) # False

</code></pre>
<p>Although arithmetically the result should be the same, due to the way floating point arithmetic works, the order of summation matters.</p>
<p>In general, the same operator will be implemented differently on different types of hardware in order to make best use of that hardware's features. So, the actual order of arithmetic operations performed to compute <code>conv2d</code> will not be the same.</p>
<p>Lack of associativity is only one of the ways in which FP arithmetic can perform unexpectedly and may not even be the root cause of the actual difference you observe. But this is enough to understand that there's no reason to expect the outputs to match exactly.</p>
<p>You can read more about the way floating point values work <a href=""https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html"" rel=""nofollow noreferrer"">here</a></p>
","2024-01-22 14:09:33","1","Answer"
"77859639","","Basic Conv2d has different results with CUDA and CPU","<p>I tested a basic <code>Conv2d</code> operation but it has different results with CUDA and CPU.</p>
<p>When running with the CPU, the values are exactly 2.5600, 3.8400, 3.8400, ..., 3.8400, 3.8400, 2.5600.
However, when running with CUDA,
the values are 2.5587, 3.8381, 3.8381, ..., 3.8381, 3.8381, 2.5587.</p>
<p>And when I converted the dtype to FP64, it is fixed.
However default data type for Torch is known to be FP32.
In that case, is this issue unavoidable?</p>
<p>The Code is below:</p>
<pre class=""lang-py prettyprint-override""><code># %%
conv_test = nn.Conv2d(64, 128, 3, padding=(1,1),bias=False,dtype=torch.float64)
conv_test.weight=torch.nn.Parameter (conv_test.weight*0+0.1)
conv_test.to(torch.device(&quot;cpu&quot;))
conv_test.to(torch.device(&quot;cuda:0&quot;))
# %%
dummy=torch.randn(10,64,8,8).to(torch.float64)
dummy=dummy*0+0.1
dummy=dummy.to(torch.device(&quot;cpu&quot;))
dummy=dummy.to(torch.device(&quot;cuda:0&quot;))

# %%
conv_test(dummy)
</code></pre>
","2024-01-22 11:48:00","1","Question"
"77858593","","Why does Pytorch DataLoader output a list?","<p>So as I was programming and skimming through the pytorch docs, I stumbled across DataLoader. I learnt a fair bit and continued researching, and then saw a comment on a youtube video covering it, stating that DataLoader actually outputs a list instead of a tensor.</p>
<p>I then used the type() on the iterable data and learnt that this was true, could anyone please help me understand why the DataLoader outputs a list instead of a tensor.</p>
","2024-01-22 08:44:43","0","Question"
"77858397","77858297","","<p>A roundabout way of converting .pkl file to a .pt or .pth file would be to flatten the .pkl file, and then initialize a new model with the same shapes. If you then reload the flatten weights and save that to .pth, it may work.</p>
","2024-01-22 08:06:39","0","Answer"
"77858297","","How to convert Spacy Model .pkl file into .pt/.pth pytorch supported format","<p>I have spacy model which I am using for inference in .pkl format. The datatype of .pkl file is &lt;class 'spacy.lang.en.English'&gt;. I want to make inference script run on GPU. I tried using different methods using spacy gpu, numba etc.</p>
<pre><code>import spacy  
spacy.prefer_gpu() # or spacy.require_gpu()
nlp = spacy.load(&quot;content/path&quot;)
</code></pre>
<p>It didn't work, I think by converting the .pkl into .pt will work by loading the pt file to 'cuda' device. Please suggest approach to handle this scenario.</p>
","2024-01-22 07:43:08","0","Question"
"77856550","77855368","","<p>If I understand correctly your problem, for each of the <code>40000000</code> rows, you want to be able to know the minimal distance over <code>3</code> vectors of dim <code>90</code>. Your output would be :</p>
<ul>
<li>a vector of length <code>40M</code> with all minimal distances</li>
<li>a <code>(40M, 2)</code> matrix with the corresponding indices (<code>(0, 1)</code>, <code>(0, 2)</code> or <code>(1, 2)</code>)</li>
</ul>
<p>Let us start by solving a more simple problem, find the minimal distance and indices for a <code>(3, 90)</code> tensor (could be seen as a sub-problem of the first one). The following script shows how to compute a <code>3x3</code> distance matrix using dummy axis (see the link in comment for more info) and how to extract the information you need using <code>torch.topk</code>:</p>
<pre><code>X = torch.randn(3, 90)

# https://sparrow.dev/adding-a-dimension-to-a-tensor-in-pytorch/
Xi = X[:, None, :]
Xj = X[None, :, :]
# Compute the 3x3 distance matrix
distances = ((Xi - Xj) ** 2).sum(dim=-1)

print(&quot;Distance matrix:&quot;)
print(distances)

# Fill diagonal with inf to avoid matching a point with itself
distances = distances.fill_diagonal_(float(&quot;inf&quot;))

# Compute the nearest neighbor of each point and associated distances
# See https://pytorch.org/docs/stable/generated/torch.topk.html
knn = distances.topk(k=1, largest=False)

values = knn.values.squeeze(-1) # Minimal distances for each point
indices = knn.indices.squeeze(-1) # Indices of the nearest points for each point
argmin = values.argmin() # Index of the point for which the minimal distance is minimal

minimal_distance = values[argmin] # The global minimum of distances
indices = torch.tensor([argmin, indices[argmin]]) # The indices of the points where the minimum is reached

print()
print(f&quot;Minimal distance: {minimal_distance}&quot;)
print(f&quot;Indices of the nearest points: {indices}&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Distance matrix:
tensor([[  0.0000, 208.3901, 204.1861],
        [208.3901,   0.0000, 195.4102],
        [204.1861, 195.4102,   0.0000]])

Minimal distance: 195.4102020263672
Indices of the nearest points: tensor([1, 2])
</code></pre>
<p>Now, you want to apply the same operation but for a <code>(n_rows, 3, 90)</code> tensor and outputs min_distances of size <code>(n_rows)</code> and corresponding indices <code>(n_rows, 2)</code>. The following function does the job. It is pretty similar to the script above except that we cannot apply <code>fill_diagonal_</code> to a <code>(n_rows, 3, 3)</code>, I followed the solution of <a href=""https://discuss.pytorch.org/t/set-diagonal-of-each-matrix-in-a-batch-to-0/113646"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/set-diagonal-of-each-matrix-in-a-batch-to-0/113646</a></p>
<pre><code>def compute_distances(X):

    n_rows, n_cols, dim = X.shape

    Xi = X[:, :, None, :]
    Xj = X[:, None, :, :]
    distances = ((Xi - Xj) ** 2).sum(dim=-1)
    # distances is a (n_rows, n_cols, n_cols) tensor

    # Check that the shape of distances is correct
    assert distances.shape == (n_rows, n_cols, n_cols)

    # We want to apply fill_diagonal_ to each matrix in the batch
    # https://discuss.pytorch.org/t/set-diagonal-of-each-matrix-in-a-batch-to-0/113646
    value = float('inf')
    mask = torch.eye(n_cols).repeat(n_rows, 1, 1).bool()
    distances[mask] = value

    # Now, we want to find the nearest neighbor for each point
    knn = distances.topk(1, largest=False)

    # Same as in the previous script: compute minimal distance from each point
    # then corresponding indices, then global minimum
    values = knn.values.squeeze(-1)
    indices = knn.indices.squeeze(-1)
    argmin = values.argmin(dim=1)

    min_distances = values[torch.arange(n_rows), argmin]
    min_indices_0 = indices[torch.arange(n_rows), argmin]
    min_indices_1 = argmin

    min_indices = torch.stack([min_indices_0, min_indices_1], dim=1)

    return min_distances, min_indices
</code></pre>
<p>Let's test it on an example:</p>
<pre><code>X = torch.randn(40000, 3, 90) # 40000 rows, 3 columns, 90 dimensions

# Apply the function
min_distances, min_indices = compute_distances(X)

# Manually compute the distances
Xi = X[:, :, None, :]
Xj = X[:, None, :, :]
distances =  ((Xi - Xj) ** 2).sum(dim=-1)

# Check that the minimal distances and indices are correct for a given row index
index = 1234

print(f&quot;Minimal distance: {min_distances[index]}&quot;)
print(f&quot;Indices of the nearest points: {min_indices[index]}&quot;)
print(f&quot;Distance matrix:&quot;)
print(distances[index])
</code></pre>
<p>Output:</p>
<pre><code>Minimal distance: 214.47735595703125
Indices of the nearest points: tensor([2, 1])
Distance matrix:
tensor([[  0.0000, 223.2144, 221.7425],
        [223.2144,   0.0000, 214.4774],
        [221.7425, 214.4774,   0.0000]])
</code></pre>
<p>Hope it will help you solve your problem.</p>
","2024-01-21 20:35:27","0","Answer"
"77856318","77855120","","<p>This turns out to be pytorch-speak for disk full. I guess in hindsight it isn't so unclear. I'm not sure if the reason for the failed file write can be bubbled up into the error which would have made this much less hairy, will see if I can pry into it.</p>
","2024-01-21 19:21:46","2","Answer"
"77855830","77855629","","<p>i have done it with Affine transform</p>
<pre><code>from PIL import Image
from pathlib import Path
import matplotlib.pyplot as plt

import torch
from torchvision.transforms import v2

plt.rcParams[&quot;savefig.bbox&quot;] = 'tight'


torch.manual_seed(0)

# you can download the assets and the
# helpers from https://github.com/pytorch/vision/tree/main/gallery/
from helpers import plot
orig_img = Image.open(Path('gallery/assets/astronaut.jpg'))

affine_transfomer = v2.RandomAffine(degrees=0,translate=(0.1, 0.3),scale=(0.5,0.5))
affine_imgs = [affine_transfomer(orig_img) for _ in range(4)]
plot([orig_img] + affine_imgs)
</code></pre>
<p><a href=""https://i.sstatic.net/LADTD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LADTD.png"" alt=""enter image description here"" /></a></p>
<p>On top of this you can also use 56x56 resize method<br />
<a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.RandomAffine.html#torchvision.transforms.RandomAffine"" rel=""nofollow noreferrer"">here</a> you can see more details, you can play with <code>translate</code> and <code>scale</code> params to shift the image from center</p>
<p>I hope this helps</p>
","2024-01-21 17:10:35","0","Answer"
"77855742","","How to convert safetensors model to onnx model?","<p>I want to convert a <code>model.safetensors</code> to ONNX, unfortunately I haven't found enough information about the procedure. The documentation of <code>safetensors</code> package isn't enough and actually is not clear even how to get the original (pytorch in my case) model, since when I try something as</p>
<pre><code>with st.safe_open(modelsafetensors, framework=&quot;pt&quot;) as mystf:
   ...
</code></pre>
<p>the <code>mystf</code> object has <code>get_tensor('sometensorname')</code> but it seems hasn't any <code>get_model()</code> method or something similar.</p>
","2024-01-21 16:47:45","3","Question"
"77855629","","Adding random positional variance to the MNIST dataset","<p>I agm trying to train an autoencoder on the MNIST set, where the digits are supposed to have a random translation applied to them.
Using the torch transforms, I can resize and translate, but this doens't have the desired effect (the digit gets translated out of frame). Does anyone here know of a transform or some other method that would allow me to get a smaller digit randomnly translated?</p>
<p>I have tried to do so manually using the following code:</p>
<pre><code>image = dataset[0][0][0]
background = np.zeros((56,56))
topLeft = (random.randint(0,27), random.randint(0,27))
background[topLeft[0]:topLeft[0]+28, topLeft[1]:topLeft[1]+28] = image[0][0]
</code></pre>
<p>but I am unable to do this transformation on the actual MNIST set.
Any help would be greatly appreciated.</p>
","2024-01-21 16:14:39","0","Question"
"77855486","77855368","","<p>To calculate the Euclidean (or 2-norm) you can use <code>torch.linalg.vector_norm()</code>. You provide the dimension over which the norm should be computed and the other dimensions are automatically treated as batch-dimensions.</p>
<p>But you want to compute the norm over distances between vectors, which you need to calculate first. You can write a function that does the calculation for each row and then vectorize it:</p>
<pre class=""lang-py prettyprint-override""><code>import torch as tor
from itertools import combinations

#calculate norm of distances for each row with 3 vectors
def vector_distances(x: tor.Tensor) -&gt; tor.Tensor:
    distances = tor.zeros_like(x)
    # calculate all the distance vectors (here: 3)
    for i, pairing_of_vectors in enumerate(combinations(range(3), r =2)):
        vec_1 = x[pairing_of_vectors[0]]
        vec_2 = x[pairing_of_vectors[1]]
        distances[i] = vec_1 - vec_2
    
    # calculate the norms of your vectors
    norm = tor.linalg.vector_norm(distances, dim = 1)
    return norm

# vectorize function so you can run it over all rows at the same time
vectorized_distances = tor.func.vmap(vector_distances)

# assuming your Tensor is called data
distances = vectorized_distances(data)
</code></pre>
<p>It might be possible to use <code>torch.nn.PairwiseDistance()</code>(which basically does the same as my function <code>vector_distances()</code>) but I did not manage to vectorize it.</p>
<p>Note also, that this code works well for 3 vectors (per row) because there are three different distances between 3 vectors so the tensor <code>distance</code> can have the same shape as your row data. For more vectors, one would need to to calculate the number of distance vectors (e.g. with <code>math.comb()</code> and create a distance tensor with that size.</p>
","2024-01-21 15:39:00","0","Answer"
"77855368","","Calculating the euclidean distance of each pair of vectors in a tensor","<p>I have a Tensor of size (4000000,3,90) which is , 4mil rows, 3 columns, and in each cell there is a 90 dim vector.</p>
<p>I would like to calculate the Euclidean distance of each 2 vectors in a row and save the distances in a distance matrix. The goal is to know which 2 vectors are the closest.</p>
<p>I am using <code>PyTorch</code> and I want to know what is the most efficient way/best practice.</p>
<h2>Edit:</h2>
<p>It doesn't have to be the euclidean norm it can also be dot product</p>
<p>My thoughts:
I thought about calculating to each pair the norm, and for the distance between V2 , V3 for example, put it in the 1st column , and V1,V3 put in 2nd column and so on.. and then take the <code>min()</code> of all entries for a row to know which vector is the farthest.I'm wondering if there is an efficient/built in pytroch way to do it or should i just do a for loop</p>
<p>Thanks in advance!</p>
","2024-01-21 15:05:28","1","Question"
"77855120","","How to fix Pytorch RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/17: file write failed","<p>I've hit this error <code>RuntimeError: [enforce fail at inline_container.cc:588] PytorchStreamWriter failed writing file data/17: file write failed</code> in a training sess that's been fine for several hours - full stacktrace is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/opt/conda/bin/stylegan2_pytorch&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.10/site-packages/stylegan2_pytorch/cli.py&quot;, line 190, in main
    fire.Fire(train_from_folder)
  File &quot;/opt/conda/lib/python3.10/site-packages/fire/core.py&quot;, line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File &quot;/opt/conda/lib/python3.10/site-packages/fire/core.py&quot;, line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File &quot;/opt/conda/lib/python3.10/site-packages/fire/core.py&quot;, line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/stylegan2_pytorch/cli.py&quot;, line 184, in train_from_folder
    mp.spawn(run_training,
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py&quot;, line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method=&quot;spawn&quot;)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py&quot;, line 202, in start_processes
    while not context.join():
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py&quot;, line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/serialization.py&quot;, line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/serialization.py&quot;, line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/17: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py&quot;, line 74, in _wrap
    fn(i, *args)
  File &quot;/opt/conda/lib/python3.10/site-packages/stylegan2_pytorch/cli.py&quot;, line 60, in run_training
    retry_call(model.train, tries=3, exceptions=NanException)
  File &quot;/opt/conda/lib/python3.10/site-packages/retry/api.py&quot;, line 101, in retry_call
    return __retry_internal(partial(f, *args, **kwargs), exceptions, tries, delay, max_delay, backoff, jitter, logger)
  File &quot;/opt/conda/lib/python3.10/site-packages/retry/api.py&quot;, line 33, in __retry_internal
    return f()
  File &quot;/opt/conda/lib/python3.10/site-packages/stylegan2_pytorch/stylegan2_pytorch.py&quot;, line 1147, in train
    self.save(self.checkpoint_num)
  File &quot;/opt/conda/lib/python3.10/site-packages/stylegan2_pytorch/stylegan2_pytorch.py&quot;, line 1368, in save
    torch.save(save_data, self.model_name(num))
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/serialization.py&quot;, line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/serialization.py&quot;, line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 79331008 vs 79330896

/opt/conda/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
</code></pre>
<p>The last saved file is of different size than the last, if I clear that and restart the error recurs.  Any help appreciated.</p>
","2024-01-21 13:59:09","2","Question"
"77854708","","Computing Fisher Information in pytorch models","<p>I want to implement a function that computes the fisher information of each parameter of a yolov5. So, I took a pre-trained model and computed the fisher info by iterating through a batch of data and calculating the required information. I loaded the model and created a new loss function.
This is the code:</p>
<pre><code>def compute_fisher_information_detection(model, dataloader, device):

    model.train()  # Set model to training mode 
    for param in model.parameters():
      param.requires_grad = True

    optimizer = optim.SGD(model.parameters(), lr=1e-2)
    total_gradients = {name: 0 for name, param in model.named_parameters() if param.requires_grad}
    total_fisher_information = {name: 0 for name, layer in model.named_children() if hasattr(layer, 'weight')}

    for batch in dataloader:
      inputs = batch['img'].to(device)

      targets = []
      for label_list in batch['label']:
        label = [float(value[0]) for value in label_list]
        targets.append(torch.tensor(label).to(device))

      optimizer.zero_grad()
      outputs = model(inputs, augment=False)[0]
      conf_thres = 0.2
      iou_thres = 0.6
      pred = non_max_suppression(outputs, conf_thres, iou_thres)
      if len(pred[0]) == 0:
        continue

      boxes = []
      scores = []
      for i, det in enumerate(pred):
        if det is not None and len(det):
          for *xyxy, conf, cls in det:
            boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])
            scores.append(cls)

      boxes = np.array(boxes)
      boxes[:, 2] -= boxes[:, 0]
      boxes[:, 3] -= boxes[:, 1]

      h_img, w_img = inputs.shape[2], inputs.shape[3]

      boxes_tensor = torch.tensor(boxes, dtype=torch.float32, requires_grad=True)

      coords = torch.zeros((len(boxes), 5), dtype=torch.float32)
      coords[:, 0] = torch.tensor(scores)
      coords[:, 1] = (boxes_tensor[:, 0] + boxes_tensor[:, 2]) / (2.0 * w_img)
      coords[:, 2] = (boxes_tensor[:, 1] + boxes_tensor[:, 3]) / (2.0 * h_img)
      coords[:, 3] = boxes_tensor[:, 2] / w_img
      coords[:, 4] = boxes_tensor[:, 3] / h_img
      coords = coords.to(device)
      targets = torch.tensor(targets[0], dtype=torch.float32, requires_grad=True)
      pred_conf, pred_boxes = coords[:, 0], coords[:, 1:]
      target_conf, target_boxes = targets[0], targets[1:]
      box_loss = F.smooth_l1_loss(pred_boxes, target_boxes)

      box_loss.backward() # Compute the loss and its gradients
      optimizer.step() # Adjust learning weights

    # Accumulate gradients for each model parameter
    for name, param in model.named_parameters():
      if param.requires_grad:
        if param.grad is None:
          print('Grad does not exist)
        total_gradients[name] += param.grad.data.clone()


    return 1

</code></pre>
<p>I get &quot;Grad does not exist&quot;, so the gradient is not being computed.</p>
<p>Basically, if the model is being trained and the parameters are being updated, the gradient is computed, how could I save it, I tried to save it but it is not working.</p>
","2024-01-21 12:04:45","0","Question"
"77853380","77853354","","<p>self.xt = tmp.detach().clone()</p>
<p>Did the job.</p>
","2024-01-21 02:00:04","0","Answer"
"77853354","","Trying to backward through the graph a second time","<p>I am trying to create custom CNN with using Linear layers.<br />
But getting this error:</p>
<p>Exception has occurred: RuntimeError
Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
File &quot;C:_\DS\RNN\main.py&quot;, line 52, in 
loss.backward()
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</p>
<p>Please help, have no idea how to implement similar network to this network:</p>
<p><a href=""https://i.sstatic.net/Q5eqr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Q5eqr.png"" alt=""enter image description here"" /></a></p>
<p>The code is:</p>
<pre class=""lang-python prettyprint-override""><code>class MyNet(nn.Module):
    def __init__(self, n=N):
        super(MyNet, self).__init__()
        self.lx = nn.Linear(n, n)
        self.l1 = nn.Linear(n, n)
        self.l1_t = nn.Linear(n, n)
        self.ly = nn.Linear(n, 1)

        self.xt = 0
        self.fn = nn.Tanh()

    def forward(self, x):
        lx = self.fn(self.lx(x))
        l1 = self.fn(self.l1(lx + self.xt))

        self.xt = self.fn(self.l1_t(self.xt + l1))

        x = self.fn(self.ly(l1))
        return x
</code></pre>

","2024-01-21 01:48:05","0","Question"
"77852693","77848436","","<p>Adding to the current answers with a more modern approach.</p>
<p>As others have pointed out, the RNN model takes as input a tensor of size <code>(bs, sl, n_features)</code> when <code>batch_first=True</code>. To accommodate this, you need to add an extra unit axis as you have a single feature.</p>
<p>For your prediction, you want to use a single directional RNN to predict the next value for every state. Doing next step prediction gives you extra training data for free, even if you intend to use it for multistep prediction. We can set up the data as follows:</p>
<pre class=""lang-py prettyprint-override""><code>X = np.array( [ 
                [0.40, 0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49], 
                [0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53], 
                [0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53, 3.37] 
            ], dtype=np.float32)

x = torch.from_numpy(X[:, :-1]).unsqueeze(-1) # all values except the last
y = torch.from_numpy(X[:, 1:]).unsqueeze(-1) # next step values shifted by one
</code></pre>
<p>Now the model. We add a <code>linear_projection</code> to the input just to give the model a bit more to work with. We also update the <code>forward</code> method to optionally take in an existing hidden state, which we will use for inference.</p>
<pre class=""lang-py prettyprint-override""><code>class RNNModel(nn.Module):
    def __init__(self, d_in, d_rnn, d_hidden, n_layers, d_output):
        super().__init__()
        
        self.linear_projection = nn.Linear(d_in, d_rnn)
        self.rnn = nn.RNN(d_rnn, d_hidden, num_layers=n_layers, batch_first=True)
        self.output_layer = nn.Linear(d_hidden, d_output)
        
        self.n_layers = n_layers
        self.d_hidden = d_hidden

    def forward(self, x, hidden=None):
        
        x = self.linear_projection(x)
        
        if hidden is None:
            hidden = self.get_hidden(x)
            
        x, hidden = self.rnn(x, hidden)
        
        x = self.output_layer(x)
        
        return x, hidden
        
    def get_hidden(self, x):
        hidden = torch.zeros(self.n_layers, x.shape[0], self.d_hidden, device=x.device)
        return hidden

</code></pre>
<p>Now we train</p>
<pre class=""lang-py prettyprint-override""><code>d_in = 1
d_rnn = 32
d_hidden = 128
n_layers = 2
d_output = 1

model = RNNModel(d_in, d_rnn, d_hidden, n_layers, d_output)

optimizer = optim.Adam(model.parameters())
loss_fn = nn.MSELoss()

loader = data.DataLoader(data.TensorDataset(x, y), shuffle=False, batch_size=3)

n_epoch = 10
for epoch in range(n_epoch):
    model.train()
    for X_batch, Y_batch in loader:
        Y_pred, hidden = model(X_batch)
        loss = loss_fn(Y_pred,Y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # omitting validation, etc for brevity 
</code></pre>
<p>And now we can do inference</p>
<pre class=""lang-py prettyprint-override""><code>init_value = torch.tensor([0.35, 0.55, 0.2])[:,None,None] # input of size (3, 1, 1)
hidden = None
prediction_steps = 5 # number of timesteps to predict 
preds = []

input_value = init_value
with torch.no_grad():
    for i in range(prediction_steps):
        input_value, hidden = model(init_value, hidden) # prediction + hidden are inputs for the next timestep
        preds.append(input_value)
        
preds = torch.cat(preds) # prediction size of (3, 5, 1)
</code></pre>
","2024-01-20 20:51:24","1","Answer"
"77852623","77852159","","<p>The total feature size output from the conv layers depends on the input size. It's not immediately clear from the model itself, but the code you posted works with an input size of <code>(bs, 3, 32, 32)</code>.</p>
<p>The model would not work with any other input size. This is why using <code>AdaptiveMaxPool2d</code> is generally preferred over fixed size pooling.</p>
","2024-01-20 20:28:09","0","Answer"
"77852435","77852333","","<p>Parallelism is pretty simple in python. The trickiness comes in how to divide up your job, and that sharing memory/state is hard and/or time consuming. Your ideal parallelisable function takes few/small inputs and returns few/small outputs. <code>sum(range(N, M))</code> is pretty ideal. It take two integers as input and returns one integer. Example:</p>
<pre class=""lang-py prettyprint-override""><code>import os
from concurrent.futures import ProcessPoolExecutor, as_completed

# job to parallelise computation of sum(range(N, M))
N = 0
M = 1_000_000_000
range_ = range(N, M)

with ProcessPoolExecutor(max_workers=os.cpu_count()) as pool:
    # compute batch size
    chunk_size, remainder = divmod(len(range_), os.cpu_count())
    if remainder:
        chunk_size += 1


    # split job into roughly equal size chunks
    futures = []
    for i in range(os.cpu_count()):
       fut = pool.submit(sum, range_[i*chunk_size:(i+1)*chunk_size])
       futures.append(fut)

    # process results as and when they become ready
    total = 0
    for future in as_completed(futures):
        total += future.result()

print(f'{total=}')
</code></pre>
<p><code>max_workers=os.cpu_count()</code> is not strictly needed, as this is the default behaviour of <code>ProcessPoolExecutor</code>.</p>
","2024-01-20 19:29:59","-1","Answer"
"77852333","","How to run multiple inferences in parallel on CPU?","<p>I've got some models implemented in PyTorch, where their performance is evaluated on a custom platform (wrapper around Pytorch, keeping overall interface).</p>
<p>This is really slow however: testing 10k CIFAR10 takes almost 30mins on a single CPU. My cloud farm has no GPU available, but is highly CPU-oriented with load of memory available. Thus I'm thinking about spawning multiple threads/processes to parallelize these inference tests.</p>
<p>I know this is not as trivial with Python due to GIL and Pytorch resource model; from some research I found <code>torch.multiprocessing.Pool</code>.</p>
<p>Is it the best way? How could I deploy say <code>N</code> inference tasks on <code>N</code> CPUs, and then collect the results into an array? I wonder whether some <code>torch.device</code> info must be handled or is done automatically.</p>
<p>Something like:</p>
<pre><code>for task in inference_tasks:
    p = spawn(process)
    accuracy = inference(model, p)
    ....
    #collect results
    results.append(accuracy)
</code></pre>
<p><strong>Edit:</strong> the inference predictions are all <strong>independent</strong> from each other. The <code>DataLoader</code> could be copied and fed to each process to do the inference, then collect all the results.</p>
","2024-01-20 19:00:19","0","Question"
"77852159","","How I Know the Output Image Shape before putting it into Linear Layer in PyTorch?","<pre><code>class Net(nn.Module):
    def __init__(self, l1=120, l2=84):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, l1) 
        self.fc2 = nn.Linear(l1, l2)
        self.fc3 = nn.Linear(l2, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
</code></pre>
<p>16 is the number of Activation Map After convolution .
I don't understand in  self.fc1 = nn.Linear(16 * 5 * 5, l1) How 5 came ?
Does any formula for calculate this?</p>
","2024-01-20 18:13:07","0","Question"
"77850649","77850282","","<p>Firstly, define x, y inside of the loop.</p>
<pre><code>for data in trainset:
    x, y = data[0][0], data[1][0]
    break
</code></pre>
<p>Then use .reshape() to change from (1, 28, 28) to (28, 28)</p>
<pre><code>x = x.reshape([28, 28])
</code></pre>
<p>Convert to a numpy array (better for matplotlib) and plot</p>
<pre><code>x = x.numpy()
plt.imshow(x, cmap=&quot;Greys_r&quot;)
plt.show()
</code></pre>
<p>Below is the output</p>
<p><a href=""https://i.sstatic.net/LZs91.png"" rel=""nofollow noreferrer"">Output</a></p>
","2024-01-20 10:34:44","1","Answer"
"77850282","","Training set not showing in graph","<p>For some reason my training set is not showing as a matplotlib graph, I am using the torchvision dataset MNIST.</p>
<p>The code is running without any errors however the graph of data won't be shown. I tried doing the .view() to reshape the data shape from a (1, 28, 28) to a (28, 28) that will be allowed, however the data won't be shown. Here is my code:</p>
<pre><code>import torch
import torchvision
import matplotlib.pyplot as plt
from torchvision import transforms, datasets

train = datasets.MNIST(&quot;&quot;, train=True, download=True,
                       transform= transforms.Compose([transforms.ToTensor()]))

test = datasets.MNIST(&quot;&quot;, train=False, download=True,
                       transform= transforms.Compose([transforms.ToTensor()]))

trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)
testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)


for data in trainset:
    print(data)
    break

x, y = data[0][0], data[1][0]


print(y)


plt.imshow(data[0][0].view(28,28))
plt.show()
</code></pre>
","2024-01-20 08:25:05","0","Question"
"77848617","77848436","","<p>The issue actually originates from this line:</p>
<pre><code>X = torch.tensor(X[:,:,np.newaxis]) 
</code></pre>
<p>where you are changing the shape of your input tensor <code>X</code> from <code>[3, 8]</code> to <code>[3, 8, 1]</code>. <a href=""https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"" rel=""nofollow noreferrer""><code>RNN()</code></a> expects batched input in the format of [N, L, Hin]:
<a href=""https://i.sstatic.net/Zwi6n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Zwi6n.png"" alt=""enter image description here"" /></a>
Now this <em>H<sub>in</sub></em> should match the <code>input_size</code> parameter of <code>nn.RNN()</code> that you defined here:</p>
<pre><code>self.rnn = nn.RNN(input_sz, self.hidden_dim, num_layers=n_layers, batch_first=True)
</code></pre>
<p>when you are initializing your model with <code>model = RNNModel(8,2)</code>, you're setting the <code>input_size</code> of <code>nn.RNN()</code> layer to be <code>8</code> which is mismatching with your input.
So just update the input with:</p>
<pre><code>X = torch.tensor(X[:,np.newaxis,:]) # torch.Size([3, 1, 8])
Y = torch.tensor(Y[:,np.newaxis,:]) # torch.Size([3, 1, 1])
</code></pre>
<p>and it should work fine.</p>
<p><strong>NOTE:</strong> You are returning both <code>out</code> and <code>hidden</code> from your <code>forward</code> function which would throw the below error:</p>
<pre><code>---&gt; 12         loss = loss_fn(Y_pred,Y_batch)
AttributeError: 'tuple' object has no attribute 'size'
</code></pre>
<p>You should either return only <code>out</code> or just replace this line in your train loop:</p>
<pre><code>Y_pred = model(X_batch)
</code></pre>
<p>with:</p>
<pre><code>Y_pred, _ = model(X_batch)
</code></pre>
","2024-01-19 20:18:44","1","Answer"
"77848610","77848436","","<p>Few Suggestion to start with</p>
<pre><code>X = X.reshape((3, 8, 1)) 
X = torch.tensor(X)
</code></pre>
<p>reshapes X to have a shape of (batch_size, seq_len, input_size)</p>
<pre><code>model = RNNModel(1,2)
</code></pre>
<p>Due to our above change now we are taking only one input</p>
<pre><code>out = self.linear(out[:, -1, :]) 
</code></pre>
<p>You should be concerned with the output of last timestep, currently this could hinder your final prediction as this can lead to inclusion of all timesteps outputs.</p>
<p>These are few things which I can suggest from just seeing the piece of segment once we run it we might face other issues too.</p>
","2024-01-19 20:17:19","1","Answer"
"77848436","","Simple RNN with more than one layer in Pytorch for squential prediction","<p>I got sequential time series data. At each time stamp, there is only variable to observe (if my understanding is correct this means number of features = 1). I want to train a simple RNN with more than one layer to predict the next observation.</p>
<p>I created training data using sliding window, with window size set to 8. To give a concrete idea, below is my original data, training data and target .</p>
<p><strong>Sample Data</strong></p>
<p><code>0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37 0.49</code></p>
<p><strong>Training Data</strong></p>
<pre><code>X = 
    0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49 
    0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53
    0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37

</code></pre>
<p>corresponding targets are</p>
<pre><code>Y = 
     0.53 
     3.37
     0.49
</code></pre>
<p>I set the batch size to 3. But it gives me an error saying</p>
<p><code>RuntimeError: input.size(-1) must be equal to input_size. Expected 8, got 1</code></p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import numpy as np

X = np.array( [ [0.40, 0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49], [0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53], [0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53, 3.37] ], dtype=np.float32)

Y = np.array([[0.53], [3.37], [0.49]], dtype=np.float32)

class RNNModel(nn.Module):
    def __init__(self, input_sz, n_layers):
        super(RNNModel, self).__init__()
        self.hidden_dim = 3*input_sz
        self.n_layers = n_layers
        output_sz = 1
        self.rnn = nn.RNN(input_sz, self.hidden_dim, num_layers=n_layers, batch_first=True)
        self.linear = nn.Linear(self.hidden_dim, output_sz)

    def forward(self,x):
        batch_sz = x.size(0)
        hidden = torch.zeros(self.n_layers, batch_sz, self.hidden_dim) #initialize n_layer*batch_sz number of hidden states of dimension hidden_dim)
        out, hidden = self.rnn(x, hidden)
        out = out.contiguous().view(-1, self.hidden_dim)
        return out,hidden

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = RNNModel(8,2)
X = torch.tensor(X[:,:,np.newaxis])
Y = torch.tensor(Y[:,:,np.newaxis])
X = X.to(device)
Y = Y.to(device)
model = model.to(device)
optimizer = optim.Adam(model.parameters())
loss_fn = nn.MSELoss()

loader = data.DataLoader(data.TensorDataset(X, Y), shuffle=False, batch_size=3)

n_epoch = 10
for epoch in range(n_epoch):
    model.train()
    for X_batch, Y_batch in loader:
        Y_pred = model(X_batch)
        loss = loss_fn(Y_pred,Y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    if epoch % 10 != 0:
        continue
        model.eval()
        with torch.no_grad():
            Y_pred = model(X)
            train_rmse = np.sqrt(loss_fn(Y_pred,Y))
        print(&quot;Epoch %d: train RMSE %.4f&quot; % (epoch, train_rmse))

</code></pre>
<p>What am I doing wrong? Can anyone help me?</p>
","2024-01-19 19:36:58","0","Question"
"77848097","77848096","","<p>The problem is the <code>DataParallel</code> wrapper, that didn't allow torchview to see through the network.</p>
<p>By &quot;unpacking&quot; it using <code>net.module</code>, everything works as expected:</p>
<pre class=""lang-py prettyprint-override""><code>from torchview import draw_graph
net = torch.nn.Sequential(torch.nn.Conv2d((3, 256, 256)))
net = torch.nn.DataParallel(net, gpu_ids)
draw_graph(model=net.module, input_size=(4, 3, 256, 256))  # note unpacking
</code></pre>
<p><a href=""https://i.sstatic.net/n3wb6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/n3wb6.png"" alt=""enter image description here"" /></a></p>
","2024-01-19 18:22:07","1","Answer"
"77848096","","torchview does only show input tensor node","<p>torchview does only show the input dimensions of my network</p>
<p><a href=""https://i.sstatic.net/A0kvs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A0kvs.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>from torchview import draw_graph
net = torch.nn.Sequential(torch.nn.Conv2d((3, 256, 256)))
net = torch.nn.DataParallel(net, gpu_ids)
draw_graph(model=net, input_size=(4, 3, 256, 256))
</code></pre>
<p>I expected to see the full graph.</p>
","2024-01-19 18:22:07","0","Question"
"77846363","77845845","","<p>With @Klops help, here's the complete solution:</p>
<pre class=""lang-py prettyprint-override""><code>import torch as T

batch_size = 8
height = 480
width = 640

mean = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 1, -1)).astype(&quot;float32&quot;)  # match input_batch dimension
std = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 1, -1)).astype(&quot;float32&quot;)

mean = T.from_numpy(mean).to(&quot;cuda&quot;)
std = T.from_numpy(std).to(&quot;cuda&quot;)

# N* H,W,C 
input_batch = [np.ones((height, width, channels))]*batch_size

# N,H,W,C
input_batch = T.stack([T.from_numpy(item).to(&quot;cuda&quot;).float()/255.0 for item in input_batch])
input_batch = (input_batch - mean)/std

# N,C,H,W
input_batch = T.permute(input_batch, (0,3,1,2)).contiguous()

</code></pre>
<p>When compared to torchvision.transforms.ToTensor(), there are slight variations in each item after the following line:
<code>input_batch = T.stack([T.from_numpy(item).to(&quot;cuda&quot;).float()/255.0 for item in input_batch])</code></p>
<p>This variation was found to be small when tested with <code>T.allclose()</code>.</p>
","2024-01-19 13:19:16","0","Answer"
"77846051","77845845","","<p>According to OPs clarification, this is a speedy way to peform the normalization on the gpu.</p>
<ul>
<li>Note: I reshape the <code>mean</code> and <code>std</code> variable so that I can multiply it with <code>input_batch</code> without stacking the same value multiple times (this is called broadcasting).</li>
</ul>
<pre><code>import torch as T
import numpy as np

batch_size = 8
height = 480
width = 640
channels = 3

# CPU
input_batch = np.ones((batch_size, height, width, channels))
mean = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 1, -1))  # match input_batch dimension
std = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 1, -1))  # match input_batch dimension

# GPU
input_batch = T.from_numpy(input_batch).to(&quot;cuda&quot;)
mean = T.from_numpy(mean).to(&quot;cuda&quot;)
std = T.from_numpy(std).to(&quot;cuda&quot;)

pre_items = (input_batch - mean)/std

</code></pre>
","2024-01-19 12:22:54","1","Answer"
"77845944","77845845","","<p>I think your approach is very good and performing pre-processing on CPU is something you want! While the GPU is busy doing heavy calculations, you can use multiple workers on your CPU to prepare the next batch(es) of data.</p>
<p>Of course you could pre-process your dataset at once and save it to disc. Looking at the types of pre-processing I would not recommend it. Many transformations are better as online-transformation (augmentations) or cannot be avoided (<code>ToTensor()</code>) at all.</p>
<p>If you are not happy with the speed (e.g. you GPU is not utilized by 100% because data preparation is a bottle neck), you should look into the parallization of CPU data preprocessing (<a href=""https://pytorch.org/vision/stable/datasets.html"" rel=""nofollow noreferrer"">docs</a>) by using <code>num_workers</code> parameter:</p>
<pre><code>torch.utils.data.DataLoader(
   imagenet_data,
   batch_size=4,
   shuffle=True,
   num_workers=8
)
</code></pre>
<p>You can also look into the <code>pin_memory=True</code> parameter, as this tells the dataloader to move the data directly to GPU (after loading and pre-processing) so that no time is wasted with transfer when the data is needed.</p>
","2024-01-19 12:05:58","0","Answer"
"77845845","","How to normalize a batch of tensors in pytorch in a speed optimized manner","<p>I have a input batch which is a list (size 8) of images (480,640,3), which I would like to convert to Pytorch tensors, normalize with mean and std, and pass to a model as (8,3,480,640). Presently I'm doing the following, which works.</p>
<pre class=""lang-py prettyprint-override""><code>import torch as T
from torchvision import transforms

batch_size=8
height = 480
width = 640
input_shape = (batch_size, 3, height, width)

transform = transforms.Compose([              
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                        [0.229, 0.224, 0.225])
])


input_batch = [...] # np.ones(480,640,3) * 8

pre_items = [transform(item) for item in input_batch]
pre_items = T.stack(pre_items).to(&quot;cuda&quot;)
</code></pre>
<p>This is obviously not optimal because the preprocessing happens on CPU before being moved to CUDA.</p>
<p>What's the correct way to perform this on GPU on the batch as a whole?</p>
<p>My attempt at a solution was:</p>
<pre class=""lang-py prettyprint-override""><code>import torch as T

batch_size = 8
height = 480
width = 640

mean = T.ones((batch_size, height, width, 3)).to(&quot;cuda&quot;) * T.tensor([0.485, 0.456, 0.406]).to(&quot;cuda&quot;)
std = T.ones((batch_size, height, width, 3)).to(&quot;cuda&quot;) * T.tensor([0.229, 0.224, 0.225]).to(&quot;cuda&quot;)

input_batch = T.stack([T.tensor(item).to(&quot;cuda&quot;).float() for item in input_batch])
pre_items = (input_batch - mean)/std
pre_items = T.permute(pre_items, (0,3,1,2))
</code></pre>
<p>The output of this script does not match the expected tensor from the bottlenecked solution.</p>
","2024-01-19 11:49:22","2","Question"
"77845770","77843454","","<p>In the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"" rel=""nofollow noreferrer"">docs</a> it states the following:</p>
<ul>
<li><p>src: <code>(S,E)</code> for unbatched input, <code>(S,N,E)</code> if <code>batch_first=False</code> or <code>(N, S, E)</code> if <code>batch_first=True</code>.</p>
</li>
<li><p>tgt: <code>(T,E)</code> for unbatched input, <code>(T,N,E)</code> if <code>batch_first=False</code> or <code>(N, T, E)</code> if <code>batch_first=True</code>.</p>
</li>
<li><p>Note: Default is (sequence, batch, feature)</p>
</li>
<li><p>Note: <code>S</code> is source sequence length, <code>T</code> is target sequence length, <code>N</code> is batch size and <code>E</code> is number of features.</p>
</li>
</ul>
<p>With this we can answer your question (assuming batched input and the default of <code>batch_first=False</code>):</p>
<pre><code>num_features = 512 # default value

&gt;&gt;&gt; transformer_model = nn.Transformer(d_model=num_features)
&gt;&gt;&gt; src = torch.rand((source_seq_len, batch_size, num_features))
&gt;&gt;&gt; tgt = torch.rand((target_seq_len, batch_size, num_features))
&gt;&gt;&gt; out = transformer_model(src, tgt)
</code></pre>
<p>You mix properties of the transformer with properties of a dataset. A dataset has a vocab size and is usually subject to these preprocessing steps:</p>
<ol>
<li>Remove words from vocabulary that are very infrequent</li>
<li>Learn a word embedding (use a pretrained word embedding)</li>
<li>Result from 2. is a vector representation for each word with an embedding dimension of, say 512 (which is <code>d_model=num_features</code>).</li>
</ol>
<p>I hope I could clarify the Transformer inputs and the misunderstanding that vocab size is a parameter that is related to the transformer when it is only related to the dataset.</p>
","2024-01-19 11:38:11","0","Answer"
"77843454","","PyTorch: what is d_model in Transfomer?","<p>Doc:
<a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html</a></p>
<pre><code>torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, ...)

Parameters
d_model (int) – the number of expected features in the encoder/decoder inputs (default=512).
</code></pre>
<p>When translating, what's the relationship between <code>sequence_length</code>, <code>vocabulary_size</code>, and <code>d_model</code>?</p>
<p>For example, when translating <strong>English(source)</strong> to <strong>Spanish(target)</strong>,</p>
<pre><code>English: src_sequence_length=400, src_vocabulary_size=40000
Spanish: tgt_sequence_length=300, tgt_vocabulary_size=30000
</code></pre>
<p>how to set the <code>d_model</code> value and other parameters in the code below? what are the shapes of <code>src</code> and <code>tgt</code>? what is the shape of <code>out</code>?</p>
<pre><code>&gt;&gt;&gt; transformer_model = nn.Transformer(d_model=???)
&gt;&gt;&gt; src = torch.rand((batch_size, ??, ??))
&gt;&gt;&gt; tgt = torch.rand((batch_size, ??, ??))
&gt;&gt;&gt; out = transformer_model(src, tgt)
</code></pre>
","2024-01-19 02:48:08","0","Question"
"77841089","77840815","","<p>The reason is that the output shape of the 2nd pooling layer is 64x100x100, <em>not</em> 64x200x200.</p>
<p>Generally speaking, you don't want to fiddle with the first dimension (the batch dim) otherwise you may have this kind of problem. So an alternate way to handle this is to use a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"" rel=""nofollow noreferrer""><code>nn.Flatten</code></a> layer, <a href=""https://stackoverflow.com/questions/65993494/difference-between-torch-flatten-and-nn-flatten"">see details here</a>:</p>
<pre><code>def forward(self, x):
    # Apply convolutional and pooling layers
    x = self.pool(nn.functional.relu(self.conv1(x)))
    x = self.pool(nn.functional.relu(self.conv2(x)))

    # Flatten dimensions starting at dim=1
    x = x.flatten(1)

    # Apply fully connected layers
    x = nn.functional.relu(self.fc1(x))
    x = self.fc2(x)
    
    return x
</code></pre>
","2024-01-18 16:49:41","0","Answer"
"77840815","","Batch Size in training loop of a CNN model is not matching","<p>Here is my CNN model, this is for a grayscale image that is 400x400:</p>
<pre><code>import torch
import torch.nn as nn

class MModel(nn.Module):
    def __init__(self):
        super(MModel, self).__init__()
        
        # Define convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        
        # Calculate the size of the flattened feature map before the fully connected layers
        self.fc_input_size = 64 * 200 * 200
        
        # Define fully connected layers
        self.fc1 = nn.Linear(self.fc_input_size, 128)
        self.fc2 = nn.Linear(128, 18)  # Adjust the output size based on your requirements
        
    def forward(self, x):
        # Apply convolutional and pooling layers
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        
        # Flatten the feature map
        x = x.view(-1, self.fc_input_size)
        
        # Apply fully connected layers
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        
        return x

# Create an instance of the CNN model
model = MModel()
</code></pre>
<p>and here is my training loop:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10

for epoch in range(num_epochs):
    running_loss = 0.0

    for i, data in enumerate(train_DL, 0):
        inputs, labels = data

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)

        # Calculate the loss
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Print statistics
        running_loss += loss.item()

        if i % 10 == 9:  # Print every 10 mini-batches
            print(f&quot;[{epoch + 1}, {i + 1}] Loss: {running_loss / 10:.3f}&quot;)
            running_loss = 0.0

print(&quot;Training finished&quot;)
</code></pre>
<p>When running this code, i get this error, my batch size is 32 for my DataLoader:</p>
<pre><code>ValueError: Expected input batch_size (8) to match target batch_size (32).
</code></pre>
<p>I tried changing the batch size to 8, which still gave me a value error. Also other solutions found on StackOverflow don't seem to be working.</p>
","2024-01-18 16:05:59","0","Question"
"77837531","",".pytorch optimizer can't work the loss backpropagation","<p>I set two free parameters as two deviations for distribution.
There is a true distribution with true deviations, and I want to optimize the free parameters.
the mean of distribution is fixed, of course.</p>
<p>However, the loss is well calculated. But there is no update of free parameters.</p>
<p>Could you please help me?</p>
<p>Here's the code</p>
<pre><code>import torch
import torch.optim as optim
import numpy as np
from torch.distributions.multivariate_normal import MultivariateNormal

# Set up optimized sigmas
sigma_x = torch.tensor(1.0, requires_grad=True)
sigma_y = torch.tensor(1.0, requires_grad=True)

# Generate grid
x, y = np.meshgrid(np.arange(1, 12), np.arange(1, 12))

# Set up a single subgoal
subgoal = (3, 5)  # Adjust as needed

# Convert subgoal to torch tensor
subgoal = torch.tensor(subgoal, dtype=torch.float32)

# True distribution parameters
true_sigma_x = 2.0
true_sigma_y = 1.5

# Target distribution (true distribution)
true_distribution = None

mean = subgoal
cov_matrix = np.array([[true_sigma_x**2, 0], [0, true_sigma_y**2]])
true_distribution = multivariate_normal.pdf(np.stack([x.flatten(), y.flatten()]).T, mean=mean, cov=cov_matrix)
true_distribution = true_distribution.reshape(x.shape) / np.max(true_distribution)
true_distribution = torch.tensor(true_distribution)

# Define MSE loss function
def mse_loss(estimated_distribution, true_distribution):
    return torch.mean((estimated_distribution - true_distribution)**2)

# Set up optimizer
optimizer = optim.Adam([sigma_x, sigma_y], lr=0.01)
x, y = torch.meshgrid(torch.arange(0, 11), torch.arange(0, 11))
# Training loop
num_epochs = 1000  # Adjust as needed

def loss(sigmas, true_distribution):
  # Calculate the final distribution with current sigmas
    mean = subgoal
    coords = torch.stack([x.flatten(), y.flatten()], dim=1)
    cov_matrix = torch.tensor([[sigmas[0]**2, 0], [0, sigmas[1]**2]], requires_grad=True)
    estimated_distribution = MultivariateNormal(mean, covariance_matrix=cov_matrix)
    estimated_dist = torch.exp(estimated_distribution.log_prob(coords).requires_grad_(True)).reshape(x.shape).requires_grad_(True)
    # Normalize the estimated distribution for MSE
    estimated_distribution = estimated_dist / torch.max(estimated_dist)
    mse = mse_loss(estimated_distribution, true_distribution)
    return mse

for epoch in range(num_epochs):
   
    # Compute loss
    total_loss = loss([sigma_x,sigma_y], true_distribution)

    # Optimization step
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    # Print loss every 100 epochs
    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {total_loss.item()}')

# Print optimized sigmas
print(f'Optimized Sigma_x: {sigma_x.item()}, Optimized Sigma_y: {sigma_y.item()}')



Epoch 0, Loss: 0.07415312548569078
Epoch 100, Loss: 0.07415312548569078
Epoch 200, Loss: 0.07415312548569078
Epoch 300, Loss: 0.07415312548569078
Epoch 400, Loss: 0.07415312548569078
Epoch 500, Loss: 0.07415312548569078
Epoch 600, Loss: 0.07415312548569078
Epoch 700, Loss: 0.07415312548569078
Epoch 800, Loss: 0.07415312548569078
Epoch 900, Loss: 0.07415312548569078
Optimized Sigma_x: 1.0, Optimized Sigma_y: 1.0
</code></pre>
<p>I want to figure out what interrupts the backpropagation.</p>
","2024-01-18 07:20:30","0","Question"
"77829919","","PyTorch3D file io throws error - AttributeError: _evt","<p>I finally got PyTorch3D to install on my conda environment with the following configuration -- <code>torch=1.13.0</code> and <code>torchvision=0.14.0</code> and <code>pytorch3d=0.7.5</code>.</p>
<p>I am trying to load a mesh from an .obj file using pytorch3d.io from <a href=""https://pytorch3d.readthedocs.io/en/v0.6.0/modules/io.html"" rel=""nofollow noreferrer"">https://pytorch3d.readthedocs.io/en/v0.6.0/modules/io.html</a>.</p>
<p>In the line <code>pytorch3d.io.load_obj(filename)</code>, I get the below error</p>
<pre><code>An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File &quot;/home/aditya/miniconda3/envs/py3d/lib/python3.10/site-packages/iopath/common/file_io.py&quot;, line 946, in __log_tmetry_keys
    handler.log_event()
  File &quot;/home/aditya/miniconda3/envs/py3d/lib/python3.10/site-packages/iopath/common/event_logger.py&quot;, line 97, in log_event
    del self._evt
AttributeError: _evt
</code></pre>
<p>I am on Nvidia RTX 4090 graphics card. Not sure what the issue is here.</p>
","2024-01-17 04:08:29","0","Question"
"77829566","77829488","","<p>Grads are computed for leaf tensors. In your example, <code>input</code> is a leaf tensor, while <code>interm</code> is not.</p>
<p>When you try to access <code>interm.grad</code>, you should get the following error message:</p>
<p><code>UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)</code></p>
<p>This is because grads are propagated back to the leaf tensor <code>input</code>, not to <code>interm</code>. You can add <code>interm.retain_grad()</code> if you want to get the grad for the <code>interm</code> variable.</p>
<p>However, even if you did this, there is nothing in your example that would cause the value of <code>interm</code> to change. Each optimizer step changes the <code>input</code> value, but this does not result in <code>interm</code> being recomputed. If you want <code>interm</code> to be updated, you need to recompute it each iteration with the new <code>input</code> value. ie:</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(5):
    optimizer.zero_grad()
    interm = sigmoid(input)
    interm.retain_grad()
    loss = torch.linalg.vector_norm(interm - torch.tensor([2.,2.]))
    print(epoch, loss, input, interm)

    loss.backward(retain_graph=True)
    optimizer.step()
    print(interm.grad)
</code></pre>
<p>There's also a fundamental problem with what you are trying to do. You say you want the <code>input</code> that results in <code>interm = [2., 2.]</code>. However, you are computing <code>interm = sigmoid(input)</code>. The sigmoid function is bounded between <code>(0, 1)</code>. There is no such value of <code>input</code> that would result in <code>interm = [2., 2.]</code>, because <code>2</code> is outside the range of the sigmoid function. If you ran your optimization loop indefinitely, you would get <code>input = [inf, inf]</code> and <code>interm = [1., 1.]</code>.</p>
","2024-01-17 01:39:29","0","Answer"
"77829488","","Why is my sigmoid layer blocking gradients?","<pre><code>import torch
import torch.optim as optim
import torch.nn as nn

input = torch.tensor([1.,2.], requires_grad=True)
sigmoid = nn.Sigmoid()

interm = sigmoid(input)

optimizer = optim.SGD([input], lr=1, momentum=0.9)

for epoch in range(5):
    optimizer.zero_grad()
    loss = torch.linalg.vector_norm(interm - torch.tensor([2.,2.]))
    print(epoch, loss, input, interm)

    loss.backward(retain_graph=True)
    optimizer.step()
    print(interm.grad)
</code></pre>
<p>So I created this simplified example with an <strong>input</strong> going into a sigmoid as an intermediate activation function.</p>
<p>I am trying to find the <strong>input</strong> that results in <strong>interm</strong> =  [2.,2.]</p>
<p>But the gradients are not passing through. Anyone know why?</p>
","2024-01-17 01:13:32","0","Question"
"77826669","77826406","","<p>try these</p>
<pre><code>!pip install -q transformers==4.11.0
!pip install -q peft==0.1.0
!pip install -q accelerate==0.5.0
</code></pre>
","2024-01-16 14:56:52","1","Answer"
"77826594","77826319","","<p><strong>Edit</strong> after you shared your dataset, you can use:</p>
<pre><code>import scipy.io
import torch
from torch.utils.data import TensorDataset, DataLoader

def extract_dataset(mnist, name):
    dataset = [(images, np.repeat(int(target[len(name):]), len(images)))
            for target, images in mnist.items()
            if target.startswith(name)]
    X, y = list(zip(*train))
    X = torch.tensor(np.concatenate(X), dtype=torch.uint8)
    y = torch.tensor(np.concatenate(y), dtype=torch.uint8)
    return TensorDataset(X, y)

mnist = scipy.io.loadmat('mnist_all.mat')

train_ds = extract_dataset(mnist, 'train')
test_ds = extract_dataset(mnist, 'test')

train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)
test_dl = DataLoader(test_ds, batch_size=32, shuffle=True)
</code></pre>
<p>Output:</p>
<pre><code># A batch of 32 images of 784 features (28x28) and 32 targets
&gt;&gt;&gt; next(iter(train_dl))
[tensor([[0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0],
         ...,
         [0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8),
 tensor([1, 8, 5, 8, 0, 6, 5, 1, 6, 9, 5, 2, 3, 0, 8, 3, 4, 9, 0, 1, 8, 1, 8, 9,
         2, 6, 3, 8, 5, 4, 2, 2], dtype=torch.uint8)]
</code></pre>
<p>Input data:</p>
<pre><code>&gt;&gt;&gt; mnist
{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNX86, Created on: Thu Nov 10 17:52:46 2005',
 '__version__': '1.0',
 '__globals__': [],
 'train0': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test0': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train1': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test1': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train2': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test2': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train3': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test3': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train4': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test4': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train5': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test5': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train6': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test6': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train7': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test7': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train8': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test8': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train9': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test9': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}
</code></pre>
<hr />
<p><strong>Old answer</strong></p>
<p>You can use <code>DataLoader</code> with your data:</p>
<pre><code>from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)
</code></pre>
<p>However is not sufficient. Where are your targets?</p>
<p>I used the <code>mat</code> file from <a href=""https://github.com/ZZUTK/An-Example-of-CNN-on-MNIST-dataset/tree/master/DeepLearnToolbox_trimmed/data"" rel=""nofollow noreferrer"">here</a>. Full example:</p>
<pre><code>import scipy.io
import torch
from torch.utils.data import TensorDataset, DataLoader

mnist = scipy.io.loadmat('mnist_uint8.mat')

# Extract data from mat file and convert numpy array as tensor
X_train = torch.Tensor(mnist['train_x'])
y_train = torch.Tensor(mnist['train_y'])
X_test = torch.Tensor(mnist['test_x'])
y_test = torch.Tensor(mnist['test_y'])

# Create tensor datasets with features (X) and target (y)
train_ds = TensorDataset(X_train, y_train)
test_ds = TensorDataset(X_test, y_test)

# Then make data loaders
train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)
test_dl = DataLoader(test_ds, batch_size=8, shuffle=True)
</code></pre>
<p>Input data:</p>
<pre><code>&gt;&gt;&gt; mnist
{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN, Created on: Wed Feb 22 20:38:11 2012',
 '__version__': '1.0',
 '__globals__': [],
 'train_x': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'train_y': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 1, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 1, 0],
        [0, 0, 1, ..., 0, 0, 0]], dtype=uint8),
 'test_x': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'test_y': array([[0, 0, 1, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}
</code></pre>
","2024-01-16 14:46:43","0","Answer"
"77826406","","want to install peft and accelerate compatible with torch 1.9.0+cu111","<p>i want to install peft and accelerate:
!pip install -q git+https://github.com/huggingface/peft.git
!pip install -q git+https://github.com/huggingface/accelerate.git</p>
<p>But as my torch version is 1.9.0+cu111, the latest accelerate doesn't support my torch version.</p>
<ul>
<li>The latest accelerate 0.27.0.dev0 requires torch&gt;=1.10.0, which is not compatible to my torch 1.9.0+cu111.</li>
<li>The latest peft 0.7.2.dev0 requires torch&gt;=1.13.0, is not compatible to my torch 1.9.0+cu111 which is incompatible.</li>
</ul>
<p>the command i am using is :</p>
<pre><code>!pip install -q git+https://github.com/huggingface/transformers.git
!pip install -q git+https://github.com/huggingface/peft.git
!pip install -q git+https://github.com/huggingface/accelerate.git
</code></pre>
<p>My torch and cuda are:</p>
<pre><code>import torch

print(&quot;torch.__version__&quot;, torch.__version__)
print(&quot;torch.version.cuda&quot;, torch.version.cuda)
print(&quot;torch.__config__&quot;, torch.__config__.show())
print(&quot;torch.cuda.device_count&quot;, torch.cuda.device_count())  # Print the number of CUDA devices

import torchvision
print(&quot;torchvision&quot;, torchvision.__version__)

torch.__version__ 1.9.0+cu111
torch.version.cuda 11.1
torch.__config__ PyTorch built with:
  - C++ Version: 199711
  - MSVC 192829337
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 2019
  - CPU capability usage: AVX2

  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=C:/w/b/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/w/b/windows/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, 

torch.cuda.device_count 1
torchvision 0.10.0+cu111
</code></pre>
<p>I appreciate your help.</p>
","2024-01-16 14:18:24","0","Question"
"77826319","","How to turn a numpy array of MNIST to pytorch dataset/data","<p>On my ML class my teacher give us the MNIST data and let us train that using CNN.</p>
<p>The data is in a matlab <code>.mat</code> file: <a href=""https://file.io/PS0CtiK2aom6"" rel=""nofollow noreferrer"">https://file.io/PS0CtiK2aom6</a></p>
<p>I managed to turn that into a numpy array of <code>(60000, 784)</code></p>
<p>(60000 train data and each of them is 28x28=784)</p>
<p>Also the label (numbers <code>0-9</code>) is stored in a <code>(60000, 1)</code> array</p>
<p>Now I need to load it into a <code>torch.utils.data.DataLoader</code></p>
<p>but all I find on the internet is the dataset from pytorch itself <code>torchvision.datasets.MNIST</code></p>
<p>and I am not sure if my data has the same shape and structure with pytorch's</p>
<p>Any Ideas? Thanks!</p>
","2024-01-16 14:03:12","0","Question"
"77825152","77825151","","<p>The answer is to do the following:</p>
<pre><code>output = t[b.argmax(0), torch.arange(t.shape[1]), :]
</code></pre>
","2024-01-16 10:43:32","0","Answer"
"77825151","","pytorch indexing larger tensor with argmax of smaller tensor","<p>I have a tensor of shape: [2, 1024]
e.g.,</p>
<pre><code>b = [[2.4, 3.2, 1.5, 6.9, ...],
     [5.9, 3.5, 1.2, 2.0, ...]]
</code></pre>
<p>and a tensor of shape [2, 1024, 10].
e.g.,</p>
<pre><code>t = ...
</code></pre>
<p>I would like to index <code>t</code> using the argmax of <code>b</code> across the 0th dimension and produce a tensor <code>output</code> of dimension [1024, 10].
e.g. I would like</p>
<pre><code>output[0,:] = t[1, 0, :]
output[1,:] = t[1, 1, :]
output[2,:] = t[0, 2, :]
output[3,:] = t[0, 3, :]
...
</code></pre>
<p>In other words, using <code>b</code> (its argmax) to choose whether to take the next row from sheet <code>[0]</code> or sheet <code>[1]</code> of <code>t</code>.</p>
<p>It is important that the order is unchanged with this indexing. If <code>b.max(0) = [a b c]</code> I cannot have the indices in the order <code>[b_ind, a_ind, c_ind]</code> or something like that.</p>
","2024-01-16 10:43:32","0","Question"
"77824716","77791735","","<p>L4CasADi supports PyTorch Models exceeding linear layers (such as convolutions). L4CasADi supports all PyTorch Models, which are jit traceable/scriptable.</p>
<p>L4CasADi Example with Convolution:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>import torch
import numpy as np
import l4casadi as l4c
import casadi as cs


# Create a model with convolutional layers
class ConvModel(torch.nn.Module):
    def __init__(self):
        super().__init__()

        self.conv1 = torch.nn.Conv2d(1, 32, 3, padding=1)
        self.conv2 = torch.nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = torch.nn.Conv2d(64, 64, 3, padding=1)
        self.fc1 = torch.nn.Linear(64 * 7 * 7, 128)
        self.fc2 = torch.nn.Linear(128, 1)

    def forward(self, x):
        x = x.reshape(-1, 1, 7, 7)
        x = torch.nn.functional.relu(self.conv1(x))
        x = torch.nn.functional.relu(self.conv2(x))
        x = torch.nn.functional.relu(self.conv3(x))
        x = x.view(-1, 64 * 7 * 7)
        x = torch.nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x


x = np.random.randn(49).astype(np.float32)
model = ConvModel()
y = model(torch.tensor(x)[None])
print(f'Torch output: {y}')

l4c_model = l4c.L4CasADi(model, model_expects_batch_dim=True)

x_sym = cs.MX.sym('x', 49)
y_sym = l4c_model(x_sym)

f = cs.Function('y', [x_sym], [y_sym])
y = f(x)

print(f'L4CasADi Output: {y}')</code></pre>
</div>
</div>
</p>
","2024-01-16 09:34:31","0","Answer"
"77824171","77819618","","<p>I am trying to answer my own question. My answer would be NO because</p>
<p>Every forward pass, PyTorch build a <a href=""https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/"" rel=""nofollow noreferrer"">computation graph</a> and save <a href=""https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/"" rel=""nofollow noreferrer"">intermediate activations (outputs between layers)</a>, which are needed for backward pass <code>.backward()</code>. In the second approach, I think the first intermediate activation will be overwritten by the second intermediate activations which created in the second forward pass. Then the model will only used the intermediate activation of the second pass to for the <code>.backward()</code></p>
","2024-01-16 07:50:26","0","Answer"
"77824012","","Pytorch LayerNorm’s mean and std div are not fixed while inferencing","<p>I’m working on recreating the input after torch.LayerNorm. As far as I know, the mean and standard deviation for LayerNorm are fixed during the inference phase. Therefore, I thought I could extract these factors and recreate the original input from the LayerNorm output.</p>
<p>I have successfully extracted the weight and bias, which are not necessarily identical to the mean and standard deviation because LayerNorm has its own weight and bias parameters. My weight and bias parameters are fused from various factors, but they successfully recreate the original input from the LayerNorm output.</p>
<p>However, when I applied these extracted weight and bias parameters to another input tensor and expected LayerNorm to work in the same way as with the previous input, I obtained a completely different output. I assumed that LayerNorm calculated new mean and standard deviation values for the second input, causing the difference. But I’m puzzled as to why LayerNorm computed the mean and standard deviation for the second input; they should have remained fixed during inference.
below is my code</p>
<pre><code>layer = layer().eval()
with torch.inference_mode():
    out = layer(input_data)

w = torch.zeros(len(out[0, :, 0]))
b = torch.zeros(len(out[0, :, 0]))

for i in range(len(out[0, :, 0])):
    w[i] = (input_data[0, i, 0] - input_data[0, i, 10]) / (out[0, i, 0] - out[0, i, 10])
    b[i] = (input_data[0, i, 0] * out[0, i, 10] - input_data[0, i, 10] * out[0, i, 0]) / (out[0, i, 10] - out[0, i, 0])

for i1 in range(len(input_remade[0, :, 0])):
    input_remade[0, i1, :] = out[0, i1, :] * w[i1] + b[i1]
print(torch.sum(input_remade - input_data))


input_data2 = torch.randn(1, 577, 768)
input_remade2 = torch.randn(1, 577, 768)
with torch.inference_mode():
    out2 = layer(input_data2)

for i1 in range(len(input_remade2[0, :, 0])):
    input_remade2[0, i1, :] = out2[0, i1, :] * w[i1] + b[i1]
print(torch.sum(input_remade2 - input_data2))

w1 = torch.zeros(len(out2[0, :, 0]))
b1 = torch.zeros(len(out2[0, :, 0]))

for i in range(len(out2[0, :, 0])):
    w1[i] = (input_data2[0, i, 0] - input_data2[0, i, 10]) / (out2[0, i, 0] - out2[0, i, 10])
    b1[i] = (input_data2[0, i, 0] * out2[0, i, 10] - input_data2[0, i, 10] * out2[0, i, 0]) / (out2[0, i, 10] - out2[0, i, 0])

for i1 in range(len(input_remade2[0, :, 0])):
    input_remade2[0, i1, :] = out2[0, i1, :] * w1[i1] + b1[i1]
print(torch.sum(input_remade2 - input_data2))
</code></pre>
<pre><code>tensor(-0.0061)
tensor(1280.9966)
tensor(0.0014)
</code></pre>
<p>Or is there Any way to extracte fixed mean and standard deviation from LayerNorm layer?</p>
","2024-01-16 07:15:28","0","Question"
"77819618","","Pytorch: Forward pass single time with full data and multiple times with subset of data, are they identical?","<p>I am optimizing my neural network (LSTM network) using PyTorch. For some reasons (during training), I cannot pass all data at once but only able to pass a subset of data (please see the code below). In both approaches, let assume that I ONLY update the network weights after all x data (x mini batch data) have passed. My question is that for optimization of the network weights, are the two approaches identical.</p>
<pre><code>import torch

# Create input
x = {}
x[&quot;one&quot;] = torch.rand(10,2)
x[&quot;two&quot;] = torch.rand(7,2)

y = {}
y[&quot;one&quot;] = torch.rand(10,1)
y[&quot;two&quot;] = torch.rand(7,1)

# LSTM model
model = torch.nn.LSTM(input_size=2, hidden_size=1)

# Train the model
optim = torch.optim.Adam(model.parameters(), lr=0.001)
loss = torch.nn.L1Loss()

#------------------------------------------------------------------------------
#   First approach
#------------------------------------------------------------------------------
y_true = torch.cat((y[&quot;one&quot;], y[&quot;two&quot;]), dim = 0)
for epoch in range(3):
    y_predict = {}
    for key in x.keys():
        y_predict[key], _ = model(x[key])
    
    # reset gradient to zero
    optim.zero_grad()
    
    # convert y predict to torch tensor
    y_predict = torch.cat((y_predict[&quot;one&quot;], y_predict[&quot;two&quot;]), dim = 0)
    
    # calculating loss and update weights
    L1loss = loss(y_true, y_predict)
    L1loss.backward()
    optim.step()
    
#------------------------------------------------------------------------------
# Second approach
#------------------------------------------------------------------------------

input = torch.cat((x[&quot;one&quot;], x[&quot;two&quot;]), dim = 0)
for epoch in range(3):
    y_predict, _ = model(input)

    # reset gradient to zero
    optim.zero_grad()

    # calculating loss and update weights
    L1loss = loss(y_true, y_predict)
    L1loss.backward()
    optim.step()
</code></pre>
","2024-01-15 11:59:09","0","Question"
"77819108","77818580","","<p>According to <a href=""https://dev-discuss.pytorch.org/t/pytorch-release-2-1-2-final-rc-is-available/1708"" rel=""nofollow noreferrer"">documentation</a>, have you tried modifying this lines :</p>
<pre><code>RUN pip3 install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -i https://pypi.tuna.tsinghua.edu.cn/simple \
&amp;&amp; pip3 install --no-cache-dir xformers==0.0.21 -i https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre>
<p>to :</p>
<pre><code>RUN pip3 install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/test/cu118
&amp;&amp; pip3 install --no-cache-dir xformers==0.0.21 -i https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre>
","2024-01-15 10:26:01","1","Answer"
"77818580","","Installing pytorch is not the version I want","<pre><code># Use Nvidia CUDA base image
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 as base

# Prevents prompts from packages asking for user input during installation
ENV DEBIAN_FRONTEND=noninteractive
# Prefer binary wheels over source distributions for faster pip installations
ENV PIP_PREFER_BINARY=1
# Ensures output from python is printed immediately to the terminal without buffering
ENV PYTHONUNBUFFERED=1 

# Install Python, git and other necessary tools
RUN apt-get update &amp;&amp; apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    wget \
    libgl-dev \
    python3-opencv \
    libglib2.0-0 \
    ffmpeg

# Clean up to reduce image size
RUN apt-get autoremove -y &amp;&amp; apt-get clean -y &amp;&amp; rm -rf /var/lib/apt/lists/*


# Install ComfyUI dependencies
RUN pip3 install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -i https://pypi.tuna.tsinghua.edu.cn/simple \
    &amp;&amp; pip3 install --no-cache-dir xformers==0.0.21 -i https://pypi.tuna.tsinghua.edu.cn/simple

# Install runpod
RUN pip3 install runpod requests -i https://pypi.tuna.tsinghua.edu.cn/simple

# Go back to the root
WORKDIR /

# Add the start and the handler
ADD src/start.sh src/rp_handler.py test_input.json ./
RUN chmod +x /start.sh

# Start the container
CMD /start.sh

</code></pre>
<p>During the installation, I get an error message, see below
<a href=""https://i.sstatic.net/S7rz3.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>After installing, I started the container and used <code>torch.__version__</code> to check the version number and got 2.0.1</p>
<p>I should get pytorch version number 2.1.2</p>
","2024-01-15 08:49:44","2","Question"
"77818460","77817401","","<p>I solved the problem by rebuilding the conda environment and Install torch separately and comment out the installation requirements related to cuda and torch in the requirements</p>
","2024-01-15 08:25:26","0","Answer"
"77817501","77817444","","<p>There are some known differences between stride conv2d result of PyTorch and Tensorflow. In short, these are potentially slightly different implementation of result of padding - boundary effects, weight initialization - precision, dilation and group convolutions, numerical precision and calculation related issues.</p>
<p>For example, stride 2 means 3px filter move which may result considerable output difference. Some more details previously discussed <a href=""https://stackoverflow.com/questions/52975843/comparing-conv2d-with-padding-between-tensorflow-and-pytorch"">here</a>:</p>
","2024-01-15 03:03:15","1","Answer"
"77817444","","Different outputs between torch and tensorflow conv2d according to stride parameter","<p>I made a simple convolution network on Pytorch and Tensorflow.</p>
<p>I copied weight to Tensorflow layer from pretrained layer on Pytorch, and there was significant different according to <code>stride</code> parameter</p>
<p>If the <code>stride</code> is set to 1, the l2 norm between the outputs is approximately same (0.8435), but when the <code>stride</code> is set to 2 or higher, the l2 norm is significantly high (156.6889).</p>
<p>Can anyone explain the why the computation of <code>stride</code> is different on Pytorch and Tensorflow?</p>
<pre><code># implementation on Pytorch
nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1, bias=False)

# implementation on Tensorflow
tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=&quot;same&quot;, use_bias=False)

-&gt; this results almost same output.

# implementation on Pytorch
nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=2, bias=False)

# implementation on Tensorflow
tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=2, padding=&quot;same&quot;, use_bias=False)

-&gt; this results different output.
</code></pre>
","2024-01-15 02:39:56","0","Question"
"77817401","","RuntimeError: CUDA error: no kernel image is available for execution on the device with 3090 cuda11.3 torch10.1","<p>Hi I am trying to run the evaluation code on an RTX 3090, but encountered the following issue:</p>
<pre><code>
[13.73019    90.45654    -0.80213207  0.57002985]
 [12.367015   90.64196     1.0528708   0.56709075]
 [12.335012   90.66197    -0.82074624  0.5729689 ]]
Error!

Traceback (most recent call last):
  File &quot;test.py&quot;, line 201, in &lt;module&gt;
    main()
  File &quot;test.py&quot;, line 197, in main
    eval_single_ckpt(model, test_loader, args, eval_output_dir, logger, epoch_id, dist_test=dist_test)
  File &quot;test.py&quot;, line 63, in eval_single_ckpt
    eval_utils.eval_one_epoch(
  File &quot;/workspace/Dual-Radar/tools/eval_utils/eval_utils.py&quot;, line 199, in eval_one_epoch
    pred_dicts, ret_dict = model(batch_dict)
  File &quot;/root/anaconda3/envs/DR/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/workspace/Dual-Radar/tools/../pcdet/models/detectors/pointpillar.py&quot;, line 21, in forward
    pred_dicts, recall_dicts = self.post_processing(batch_dict)
  File &quot;/workspace/Dual-Radar/tools/../pcdet/models/detectors/detector3d_template.py&quot;, line 273, in post_processing
    recall_dict = self.generate_recall_record(
  File &quot;/workspace/Dual-Radar/tools/../pcdet/models/detectors/detector3d_template.py&quot;, line 310, in generate_recall_record
    iou3d_rcnn = iou3d_nms_utils.boxes_iou3d_gpu(box_preds[:, 0:7], cur_gt[:, 0:7])
  File &quot;/workspace/Dual-Radar/tools/../pcdet/ops/iou3d_nms/iou3d_nms_utils.py&quot;, line 69, in boxes_iou3d_gpu
    max_of_min = torch.max(boxes_a_height_min, boxes_b_height_min)
RuntimeError: CUDA error: no kernel image is available for execution on the device
eval:   0%| 
</code></pre>
<p>I run this code:</p>
<pre><code>import torch
import sys
print('A', sys.version)
print('B', torch.__version__)
print('C', torch.cuda.is_available())
print('D', torch.backends.cudnn.enabled)
device = torch.device('cuda')
print('E', torch.cuda.get_device_properties(device))
print('F', torch.tensor([1.0, 2.0]).cuda())
</code></pre>
<p>and get:</p>
<pre><code>A 3.8.16 (default, Jun 12 2023, 18:09:05)
[GCC 11.2.0]
B 1.10.1+cu113
C True
D True
E _CudaDeviceProperties(name='NVIDIA GeForce RTX 3090', major=8, minor=6, total_memory=24259MB, multi_processor_count=82)
F tensor([1., 2.], device='cuda:0')

</code></pre>
<p>My CUDA version is 11.3, and PyTorch is 1.10.1+cu113.I have tried different versions of PyTorch, such as 1.9.0, 1.11.0, and 1.10.0, but none of them worked.From the output, it appears that my GPU and CUDA are running fine, but the evaluation process still encounters errors. Could it be that the matrices are too large? How can I solve it?</p>
","2024-01-15 02:20:34","0","Question"
"77817255","77817201","","<p>As far as I know, there is no special way to do this for non-singleton dimensions. So your solution seems like the proper way to me.</p>
<p>Alternatively, what would work here is:</p>
<pre class=""lang-py prettyprint-override""><code>#given a tensor points of some size
pshape = list(points.size())
pshape[-1] += 1
z = torch.ones(pshape)
z[..., 0:-1] = points
</code></pre>
<p>On my machine, your code is slightly faster for small tensors while my suggestion is slightly faster for larger tensors. The difference in both cases is marginal though.</p>
","2024-01-15 00:59:01","0","Answer"
"77817201","","Pytorch Increase dimension size","<p>Say I have a tensor of size (4,4,4,4), how can I make a tensor of size (4,4,4,5) such that the newly added entries are 1s?</p>
<p>i.e what's the proper way to do the following?</p>
<pre class=""lang-py prettyprint-override""><code>#given a tensor points of some size
pshape = list(points.size())
pshape[-1] = 1
z = torch.ones(pshape)
return torch.cat((points, z), -1)
</code></pre>
","2024-01-15 00:32:50","0","Question"
"77815416","77811886","","<p>This is not a problem of the summary, but a problem of your network. I think you got confused with the layer count due to <code>Flatten()</code> layer and its second argument.</p>
<p>I recommend you to assemble your network layer by layer and test it by inputing a random <code>x = torch.from_numpy(np.random.rand(batch_dim, channel_dim, spatial1, spatial2)</code> and see if it works well together.</p>
<p>Usually Flatten is used to flatten channels and spatial dimension but not batch dimension. You flatten channels and ONE spatial dimension, this is probably not what you want.</p>
<p>Also, check that your input channels fit to the previous output channels. I can debug your network if you provide an example that is copy-and-paste, not just the structure.</p>
<p>The Linear layer takes currently 6 values, but Flatten (previous layer) return <code>64 * first_spatial_dim</code>. This does not work well together.</p>
<p>You got the error only now, since when calling <code>summary()</code> its the very first time that you network is actually used. It faily not because of the function, but because of the ill-connected layers. When using fully connected layers and CNNs you need to have the input shape beforehand so you can build the network fitting your data, not guessing afterwards.</p>
<p>Good luck!</p>
","2024-01-14 14:26:52","1","Answer"
"77815370","77812279","","<p>Usually a full batch does not fit on your GPU, however, a minibatch does. Looking at the other extreme, a minibatch size of 1, it is obvious that the gradient will be very noise, since it depends on a single input. A noisy gradient will cause the optimizer to follow a very wiggly path through the search space, which is not efficient.</p>
<p>I cant follow your argument here:</p>
<blockquote>
<p>So if I use full batch (all data) and randomly remove n observations when calculating loss at each epoch. Is this equivalent with the idea of mini batch size.</p>
</blockquote>
<p>This is what I understood:</p>
<ol>
<li>You evaluate the network on a full batch</li>
<li>You calculate the loss on a subset</li>
<li>Therefore gradients will only be available for this subset</li>
<li>The optimizer applies the gradient step and will only consider the subset</li>
<li>You have the same result as using a minibatch size of <code>all-n</code> in the first place</li>
</ol>
<p>You should choose a batch size that is typical for your problem (maybe check in papers on your topic). Too big batch sizes will make the gradient smooth, but stochasticity has benefits, too, as it helps you to escape local minima and makes your network more robust in unseen cases (generalization).</p>
","2024-01-14 14:15:15","0","Answer"
"77815319","77815185","","<p>You are correct, the cosine similarity between any equal vectors should be one... except for a vector of zero length. In this case you have a division by zero and the result is undefined. The implementation you use seems to handle this case with a similarity of 0.</p>
<p>The image shows the formular and there you will see that the denominator is zero in your case.</p>
<p><a href=""https://i.sstatic.net/ZIFCU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZIFCU.png"" alt=""enter image description here"" /></a></p>
<p>If zero is a good choice to handle this &quot;forbidden&quot; case? I don't know. Since cosine similarity measures the angle between two vectors (and not if two vectors are equal), both -1 and +1 don't seem to be good either. Using 0 might just be the least informative compromise. Also, consider that the denominator is zero if ANY of the two vectors has zero length. Your case is a special case in this scenario.</p>
","2024-01-14 13:57:46","2","Answer"
"77815185","","Why does torch cosine similarity between exactly same vectors give similarity of zero instead of one?","<p>I have two tensors and I want to calculate the cosine similarity between them in Pytorch:</p>
<pre><code>a = torch.tensor([[0.,0.,0.,0.,0.]])
b = torch.tensor([[0.,0.,0.,0.,0.]])
</code></pre>
<p>I calculate the cosine similarity matrix using the following function:</p>
<pre><code>def calc_similarity_batch(a, b):
    representations = torch.cat([a, b], dim=0)
    return F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim = 2)
</code></pre>
<p>To my surprise the similarity matrix calculated by cosine_similarity function is:</p>
<pre><code>tensor([[0., 0.],
        [0., 0.]])
</code></pre>
<p>While it should have been:</p>
<pre><code>tensor([[1., 1.],
        [1., 1.]])
</code></pre>
<p>since the vectors are the same. Could someone explain what is wrong with my code?</p>
","2024-01-14 13:19:31","0","Question"
"77812469","77812375","","<p>Jupyter was using the old kernel (the dev one) even though I switched interpreters. Restarting Jupyter with the new anaconda environment (python 3.8 and the release version of PyTorch) works.</p>
","2024-01-13 17:44:22","1","Answer"
"77812375","","PyTorch error on MPS (Apple silicon metal)","<p>When I use PyTorch on the CPU, it works fine. When I try to use the mps device it fails. I'm using miniconda for osx-arm64, and I've tried both python 3.8 and 3.11 and both the stable and nightly PyTorch installs.</p>
<p>According to the website (<a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a>) mps acceleration is available now without nightly.</p>
<p>The code I've written is as follows:</p>
<pre><code>import torch
mps_device = torch.device(&quot;mps&quot;)

float_32_tensor1 = torch.tensor([3.0, 6.0, 9.0],
                               dtype=torch.float32,
                               device=mps_device,
                               requires_grad=False)

float_32_tensor2 = torch.tensor([3.0, 6.0, 9.0],
                               dtype=torch.float32,
                               device=mps_device,
                               requires_grad=False)

print(float_32_tensor1.mul(float_32_tensor2))
</code></pre>
<p>This results in the following (fairly long) error: <a href=""https://pastebin.com/svwZj8Ke"" rel=""nofollow noreferrer"">https://pastebin.com/svwZj8Ke</a></p>
<p>First line of error is:</p>
<pre><code>RuntimeError: Failed to create indexing library, error: Error Domain=MTLLibraryErrorDomain Code=3 &quot;program_source:168:1: error: type 'const constant ulong3 *' is not valid for attribute 'buffer'
</code></pre>
<p>How would I go about solving this?</p>
<p>edit: meta says pastebin shouldn't be used but the error is too long to include in the question</p>
<p>edit 2: Not that <code>torch.backends.mps.is_available()</code> returns true</p>
<p>edit 3: seems to work normally on the console but Jupyter has this error</p>
","2024-01-13 17:20:32","2","Question"
"77812279","","PyTorch LSTM: Should I use mini batch size or single batch but randomly remove n observation when calculating loss?","<p>As I understood, the idea of mini batch size is equivalent with fitting the model to only a portion of all training data at each step (one epoch consists of many steps, depending on the batch size) to avoid overfitting</p>
<p>So if I use full batch (all data) and randomly remove n observations when calculating loss at each epoch. Is this equivalent with the idea of mini batch size.</p>
<p>I am using LSTM neural network and train for time series data. Here, lets assume that I have unlimited storage and computational capacity</p>
<p>Thanks for any comments</p>
","2024-01-13 16:52:16","0","Question"
"77811886","","How to decide the 'input_size' parameter of torchsummary.summary(model=model.policy, input_size=(int, int, int))?","<p>This is my CNN network printed by 'print(model.policy)':</p>
<pre><code>CnnPolicy(
  (actor): Actor(
    (features_extractor): CustomCNN(
      (cnn): Sequential(
        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))
        (1): ReLU()
        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))
        (3): ReLU()
        (4): Flatten(start_dim=1, end_dim=-1)
      )
      (linear): Sequential(
        (0): Linear(in_features=6, out_features=128, bias=True)
        (1): ReLU()
      )
    )
    (mu): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=3, bias=True)
      (5): Tanh()
    )
  )
</code></pre>
<p>When I try to print the network architecture using torchsummary.summary(model=model.policy, input_size=(1, 32, 32)). I got the following error:
RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x50176 and 6x128)</p>
<p>I have tried lots of 'input_size' combinations, but all were wrong.</p>
<p>I want to know how to choose the 'input-size' parameter?</p>
","2024-01-13 15:00:25","1","Question"
"77811830","77811002","","<h2>Root cause</h2>
<p>Your local path needs to be registered on the paths models' registry.</p>
<h2>Solution</h2>
<p>I suggest you to use the <a href=""https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"" rel=""nofollow noreferrer"">save_pretrained</a> method. It registers the local directory for further use with .from_pretrained method.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import GPT2LMHeadModel, GPT2Config

your_model = torch.load('path/to/your/model.pth')
state_dict = your_model.state_dict()
config = GPT2Config.from_pretrained('gpt2')
model = GPT2LMHeadModel(config)
model.load_state_dict(state_dict)

# Save local directory
model.save_pretrained('path/to/save/transformers_model')

# Now, you can load the Transformers model using from_pretrained
loaded_model = GPT2LMHeadModel.from_pretrained('path/to/save/transformers_model')
</code></pre>
<h2>Extra Ball:</h2>
<p>Similar discussion on GitHub: <a href=""https://github.com/huggingface/transformers/issues/7849"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/7849</a></p>
","2024-01-13 14:45:17","1","Answer"
"77811002","","How to make a trained Torch model Transformeres-compatible?","<p>I have trained and saved a PyTorch model with <code>torch.save</code>. Now I want to load it as <code>GPT2LMHeadModel</code>. <code>from_pretrained</code> method looks for a directory, and then a HuggingFace registry, none of which exists. I simply have a serialized PyTorch model and an <code>nn.Module</code> class. How to I integrate them with the Transformers library? Training from scratch is not option (takes too long).</p>
","2024-01-13 09:59:33","1","Question"
"77807552","77785405","","<p>I would like to add my 2 cents. Some models achieve this by using a Postprocessing step.</p>
<p>Instead of penalising the model by modifying the loss, they keep it as it is. After the model has made the prediction mask, we remove <strong>all but the largest connected component from the mask.</strong> This ensures 1 mask per prediction and prevents disconnected masks. Do check if this approach is suitable for your use case.</p>
<p>There are libraries such as <a href=""https://pypi.org/project/connected-components-3d/"" rel=""nofollow noreferrer"">Connected-Components-3D</a> for doing this for 3D masks. OpenCV has support for this as well <a href=""https://stackoverflow.com/questions/47055771/how-to"">See this answer</a>.</p>
","2024-01-12 15:14:12","1","Answer"
"77803669","77792551","","<p>Installation of Darts library using Anaconda:</p>
<p>Step 1: Install Anaconda and launch the Anaconda prompt from the start menu.</p>
<p>Step 2: Enter the following code to create an environment and activate it:
conda create --name darts python=3.7
conda activate darts</p>
<p>Step 3: We install the various libraries required by darts in this environment; it may take some time; simply be patient.
conda install -c conda-forge -c pytorch pip fbprophet pytorch</p>
<p>Step 4: Install the second section:
conda install -c conda-forge -c pytorch pip fbprophet pytorch cpuonly</p>
<p>Step 5: Install darts
pip install darts</p>
<p>If the installation fails, enter：
conda install prophet
Then, once the installation is complete, enter:
pip install darts</p>
<p>step 6: Connect the environment to your PyCharm IDE and run your project
File&gt; Settings&gt; Project Interpreter&gt; Project Interpreter&gt; Show all&gt;+&gt;VirtualEnv Environment &gt; Existing Environment &gt; Navigate to the location of the darts environment on your PC (for example: D:\Anaconda3\envs\darts)&gt; python.exe</p>
","2024-01-12 00:00:37","0","Answer"
"77802901","77802119","","<p>You cannot mix Pytorch tensors and operations with operations from OpenCV. To build the computational graph and for operations to be differentiable you can only work with Pytorch operations. You can use for example a combination of <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html"" rel=""nofollow noreferrer"">grid_sample</a> and <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid"" rel=""nofollow noreferrer"">affine_grid</a> to apply a transformation matrix to an image in Pytorch.</p>
","2024-01-11 20:35:26","1","Answer"
"77802368","","ValueError: The provided lr scheduler ""<torch.optim.lr_scheduler.MultiStepLR object at 0x0000021897D84160>"" is invalid","<p>I encountered this error when I tried to NERF with the GPU, and this error occurred in train.fit() where the trainer is as follow:</p>
<pre><code>    from pytorch_lightning import LightningModule, Trainer
    trainer = Trainer(max_epochs=hparams.num_epochs,
                      callbacks=callbacks,
                      resume_from_checkpoint=hparams.ckpt_path,
                      logger=logger,
                      enable_model_summary=False,
                      accelerator='auto',
                      devices=hparams.num_gpus,
                      # gpus=-1,
                      num_sanity_val_steps=1,
                      benchmark=True,
                      profiler=&quot;simple&quot; if hparams.num_gpus==1 else None,
                      strategy=DDPPlugin(find_unused_parameters=False) if hparams.num_gpus&gt;1 else None)

    trainer.fit(system)
</code></pre>
<p>I don't know what to do with the problem, and i sincerely need your help, thanks very much!</p>
","2024-01-11 18:43:37","0","Question"
"77802119","","applying a transformation matrix to the image before proceeding with feature extraction","<p>In a section of the code, I am attempting to reimplement the commented code in a different manner. Specifically, I am working on applying a transformation matrix to the image before proceeding with feature extraction</p>
<pre><code> def compute_losses(self):
        

        assert self.net_recog.training == False
        trans_m = self.trans_m
        
        # if not self.opt.use_predef_M:
        #     trans_m = estimate_norm_torch(self.pred_lm, self.input_img.shape[-2])

       
       
        # pred_feat = self.net_recog(self.pred_face, trans_m)
        # gt_feat = self.net_recog(self.input_img, self.trans_m)
        # self.loss_feat = self.opt.w_feat * self.compute_feat_loss(pred_feat, gt_feat)
        if not self.opt.use_predef_M:
            trans_m = estimate_norm_torch(self.pred_lm, self.input_img.shape[-2])

       
        pred_feat1 = cv2.transform(self.pred_face, trans_m, None)
        pred_feat = self.net_recog(pred_feat1)
        gt_feat1 = cv2.transform(self.input_img, trans_m, None)
        gt_feat = self.net_recog(gt_feat1)
        self.loss_feat = self.opt.w_feat * self.compute_feat_loss(pred_feat, gt_feat)
</code></pre>
<p>I have used cv2.transform but it didn't work. Thank you in advance.</p>
<blockquote>
<p>cv2.error: OpenCV(4.8.1) :-1: error: (-5:Bad argument) in function 'transform'
Overload resolution failed:</p>
<ul>
<li>src is not a numpy array, neither a scalar</li>
<li>Expected Ptr&lt;cv::UMat&gt; for argument 'src'</li>
</ul>
</blockquote>
","2024-01-11 17:53:40","0","Question"
"77800561","77800331","","<p>The tokenizer is not responsible for the embeddings. It only generates the ids to be fed into the embedding layer.
Barts embeddings are learned, i.e. the embedding come from their own embedding layer.</p>
<p>You can retrieve both types of embeddings like this. Here <code>bart</code> is a <code>BartModel</code>. The encoding is (roughly) done like this:</p>
<pre class=""lang-py prettyprint-override""><code>embed_pos = bart.encoder.embed_positions(input_ids)
inputs_embeds = bart.encoder.embed_tokens(input_ids)
hidden_states = inputs_embeds + embed_pos
</code></pre>
<p>Full working code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BartForConditionalGeneration, BartTokenizer

bart = BartForConditionalGeneration.from_pretrained(&quot;facebook/bart-base&quot;, forced_bos_token_id=0)
tok = BartTokenizer.from_pretrained(&quot;facebook/bart-base&quot;)
example_english_phrase = &quot;UN Chief Says There Is No &lt;mask&gt; in Syria&quot;
input_ids = tok(example_english_phrase, return_tensors=&quot;pt&quot;).input_ids

embed_pos = bart.model.encoder.embed_positions(input_ids) * bart.model.encoder.embed_scale # by default the scale is 1.0
inputs_embeds = bart.model.encoder.embed_tokens(input_ids)
hidden_states = inputs_embeds + embed_pos
</code></pre>
<hr />
<p>Note that <code>embed_pos</code> is invariant to the actual token ids. Only their position matters. &quot;New&quot; embeddings are added if the input grows larger without changing the embeddings of the earlier positions:</p>
<p>These cases yield the same embeddings:
<code>embed_positions([0, 1]) == embed_positions([123, 241]) == embed_positions([444, 3453, 9344, 3453])[:2]</code></p>
","2024-01-11 13:51:58","0","Answer"
"77800331","","How to Find Positional embeddings from BARTTokenizer?","<p>The objective is to add token embeddings (customized- obtained using different model) and the positional Embeddings.</p>
<p>Is there a Way I can find out positonal embedding along with the token embeddings for an article(length 500-1000 words) using BART model.</p>
<pre><code>tokenized_sequence = tokenizer(sentence, padding='max_length', truncation=True, max_length=512, return_tensors=&quot;pt&quot;)
</code></pre>
<p>the output is <code>input_ids</code> and <code>attention_mask</code> but not parameter to return <code>position_ids</code> like in BERT model.</p>
<pre><code>bert.embeddings.position_embeddings('YOUR_POSITIONS_IDS')
</code></pre>
<p>Or the only way to obtain Positional Embedding is using sinusoidal positional encoding?</p>
","2024-01-11 13:13:15","1","Question"
"77797196","77783222","","<blockquote>
<p>Should I be using &quot;torch.set_grad_enabled&quot;?<br />
I read that it helps conserve memory</p>
</blockquote>
<p>I believe your question is about the usage of <code>torch.set_grad_enabled(False)</code>. This method invocation is not for optimization but rather to turn off gradient computations. For example, when running inference on a PyTorch model, gradient calculations may not be required at all. In such cases, you can disable Autograd with set_grad_enabled instead of removing <code>grad_fn</code> for each parameter. The memory conservation occurs because gradients are not stored in memory.</p>
<p>This <a href=""https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"" rel=""nofollow noreferrer"">tutorial</a> helps you understand Autograd.</p>
","2024-01-11 01:22:43","1","Answer"
"77795645","77795638","","<p>I'd missed <a href=""https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#cuda"" rel=""nofollow noreferrer"">this section</a> of their docs outlining that I needed to set <code>LD_INCLUDE_PATH</code> correctly in my container definition. I didn't realize I needed to set this on GKE Autopilot, but it turns out you do.</p>
<p>Adding this environment variable fixed things:</p>
<pre><code>LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/cuda-12.3/lib64
</code></pre>
","2024-01-10 18:30:07","2","Answer"
"77795638","","Getting CUDA Initialization Error on GKE Autopilot","<p>Getting this stack trace:</p>
<pre><code>terminate called after throwing an instance of 'c10::CUDAError'
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Exception raised from getDevice at ../c10/cuda/impl/CUDAGuardImpl.h:39 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x79b727243612 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: &lt;unknown function&gt; + 0x1a14b (0x79b72761a14b in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
frame #2: &lt;unknown function&gt; + 0x3637f3a (0x79b75b237f3a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: torch::autograd::Engine::thread_init(int, std::shared_ptr&lt;torch::autograd::ReadyQueue&gt; const&amp;, bool) + 0x2a (0x79b75b238eba in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr&lt;torch::autograd::ReadyQueue&gt; const&amp;, bool) + 0x5c (0x79b77112328c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #5: &lt;unknown function&gt; + 0xdc253 (0x79b7de6c6253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #6: &lt;unknown function&gt; + 0x94ac3 (0x79b7dfd78ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: clone + 0x44 (0x79b7dfe09814 in /lib/x86_64-linux-gnu/libc.so.6)
</code></pre>
<p>Trying to run on GKE Autopilot. What's going on?</p>
<ul>
<li>Bumping the k8s cluster to 1.28+ gave me a Nvidia driver 535.x.x which is sufficient for CUDA 12</li>
</ul>
","2024-01-10 18:28:52","-1","Question"
"77793004","77772327","","<p>I think the problem is that you shoul call transforms.PILToTensor before transforms.Normalize(mean, std) because this transformation doesn't support PIL format as input, see more <a href=""https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.Resize((256, 256), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),
    transforms.CenterCrop(size=[224,224]),
    transforms.PILToTensor(),
    transforms.Normalize(mean, std),

    
    
])

test_transform = transforms.Compose([
    transforms.Resize((256, 256), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),
    transforms.CenterCrop(size=[224,224]),
    transforms.PILToTensor(),
    transforms.Normalize(mean, std),
    
])
</code></pre>
","2024-01-10 11:27:12","1","Answer"
"77792833","77792551","","<p>Both packages <a href=""https://pypi.org/project/darts/#data"" rel=""nofollow noreferrer"">darts</a> and <a href=""https://pypi.org/project/torch/#data"" rel=""nofollow noreferrer"">pytorch</a> currently supports Python versions 3.8 - 3.11 but you're using Python 3.12.1 To resolve your issue, install a supported python version and use virtual environments to manage projects that requires specific python versions. For example, you can create a Python 3.10 virtual environment and <code>pip install darts</code> there.</p>
<p><strong>EDIT:</strong>
User @jkr pointed out that pytorch now supports Python 3.11 as can be seen in the <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">official docs</a>.</p>
","2024-01-10 10:57:20","6","Answer"
"77792560","77790472","","<p>The output of your residual blocks in <code>backBone</code> is a 4D tensor shaped <code>(N, 128, h, w)</code>. However, a linear layer in PyTorch works with inputs <code>(*, H_in)</code> and outputs <code>(*, H_out)</code>. So in <code>valueHead</code> and <code>policyHead</code> the input of the first linear layer (<code>nn.Linear(in_features=1, out_features=num_hidden)</code> for the value head and <code>nn.Linear(2, out_features=(action_size))</code> for the policy head) need to be permuted from <code>(B, 128, h, w)</code> to <code>(B, h, w, 128)</code>.</p>
","2024-01-10 10:18:13","0","Answer"
"77792551","","How to install PyTorch on python 3.12.1","<p>I am installing the <a href=""https://github.com/unit8co/darts/blob/master/INSTALL.md#enabling-optional-dependencies"" rel=""nofollow noreferrer"">DARTS TimeSeries library</a> but I run into an issue of dependencies installation. In the DARTS installation guide it said if we run into that issue we have to refer to the official installation guide for PyTorch, then try installing Darts again. Then, when I tried to install torch on python 3.12.1 I run into this error :</p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)</p>
<p>ERROR: No matching distribution found for torch.</p>
</blockquote>
<p>How to solve that?</p>
<p>I am using PyCharm as Python code editor.</p>
<p>I tried a <code>pip install darts</code> and it did not install all the packages and encounter this error  error: subprocess-exited-with-error</p>
<pre><code>  pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─&gt; [136 lines of output]
      Collecting setuptools&gt;=64.0
        Obtaining dependency information for setuptools&gt;=64.0 from https://files.pythonhosted.org/packages
</code></pre>
<p>Then, I tried to install torch with <code>pip install torch</code> and encountered this error:</p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)</p>
<p>ERROR: No matching distribution found for torch</p>
</blockquote>
","2024-01-10 10:16:06","1","Question"
"77791735","","Is there any solutions that help casadi MX variable to be used in pytorch nn.conv1d(convolution)?","<p>I now have a Sequential Neural Network which is used for predict robot states. But I have a problem when implementing the NN into Casadi to solve an MPC problem. The error keeps warning me that I can not use Casadi MX variable in a Sequential NN which requires convolution process.</p>
<p>I have seen the repo <a href=""https://github.com/Tim-Salzmann/l4casadi"" rel=""nofollow noreferrer"">l4casadi</a> but it seems only supporting nn.linear but not nn.conv1d. Hopes to find a solution here and thanks for answering.</p>
","2024-01-10 08:05:46","0","Question"
"77791508","77784936","","<p>Have you tried updating the CUDA_VISIBLE_DEVICES env variable?</p>
<p>Try executing the code by setting it to use both GPUs.</p>
<pre><code>CUDA_VISIBLE_DEVICES=0,1 python your_script.py
</code></pre>
","2024-01-10 07:17:08","0","Answer"
"77790472","","Pytorch CNN input dimensions not matching","<p>Im currently trying to implement a CNN for a Alpha Zero game player, using pytorch but im getting an error regarding matrices multiplication. My input consists of 3 channels with 10x10 matrices.</p>
<pre><code>model = Net(10, 10**2+1)
print(summary(model,(3,10,10)))
</code></pre>
<p>giving me the following error:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-32-0a101a882eb1&gt; in &lt;cell line: 3&gt;()
      1 model = Net(10, 10**2+1)
      2 
----&gt; 3 print(summary(model,(3,10,10)))

9 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)
    112 
    113     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 114         return F.linear(input, self.weight, self.bias)
    115 
    116     def extra_repr(self) -&gt; str:

RuntimeError: mat1 and mat2 shapes cannot be multiplied (40x10 and 2x101)
</code></pre>
<p>This is the current architecture:</p>
<pre><code>def conv3x3(in_planes, out_planes):
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, padding=1)

def conv1x1(in_planes, out_planes):
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, padding=0)


class Net(nn.Module):
    def __init__(self, board_size, action_size, num_resBlocks=20, num_hidden=128):
        super().__init__()
        self.device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

        # Initial convolution 
        self.startBlock = nn.Sequential(
            conv3x3(3, num_hidden),
            nn.BatchNorm2d(num_hidden),
            nn.ReLU()
        )
        
        # Loop of all 20 Residual Layers
        self.backBone = nn.ModuleList(
            [ResBlock(num_hidden) for i in range(num_resBlocks)]
        )
        
        
        # Outputs expected value of the state 
        self.valueHead = nn.Sequential(
            conv1x1(num_hidden, 1),
            nn.BatchNorm2d(1),
            nn.ReLU(),

            nn.Linear(in_features=1, out_features=num_hidden),
            nn.ReLU(),

            nn.Linear(in_features=num_hidden, out_features=1),
            nn.Tanh()
        )

    
        # Outputs the probabilities of each possible action 
        self.policyHead = nn.Sequential(
            conv1x1(num_hidden, 2),
            nn.BatchNorm2d(2),
            nn.ReLU(),
            nn.Linear(2, out_features=(action_size)),
            nn.Softmax(dim=1)
        )


        self.to(self.device)

    def forward(self, x):
        x = self.startBlock(x)

        for resBlock in self.backBone:
            x = resBlock(x)

        policy = self.policyHead(x)
        value = self.valueHead(x)

        return policy, value
    

class ResBlock(nn.Module):
    def __init__(self, num_hidden):
        super().__init__()
        self.conv1 = conv3x3(num_hidden, num_hidden)
        self.bn1 = nn.BatchNorm2d(num_hidden)
        self.conv2 = conv3x3(num_hidden, num_hidden)
        self.bn2 = nn.BatchNorm2d(num_hidden)
        self.relu = nn.ReLU()

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        # Skip connections
        out = self.relu(out + identity)

        return out
</code></pre>
<p>Thank you so much for your help, i really appreciate it!</p>
","2024-01-10 01:16:50","0","Question"
"77790117","77789645","","<h3>weird connections</h3>
<p>You see weird connections, because you restarted you training (or resumed) with the same name without setting the epoch (or step) to the correct resumed value. If your last datapoint is at <code>step=10</code> you should continue saving data at <code>step=11</code> not <code>step=0</code>. Otherwise the last value at <code>step=1o</code> will be conntect to a new value at <code>step=0</code> which causes the observed connection. On first training with clean log files, this will not happen. Either choose a new name, or resume with proper <code>step</code> or <code>epoch</code>.</p>
<h3>smoothing</h3>
<p>It's not a bug, it's a feature! Often training progress fluctuates a lot, making it hard to observe a general trend. Smoothing the time series allows you to quickly understand where the training is going.</p>
<p>Look at this GAN Generator training. On the first image, without smoothing, its hard to tell what is happening.</p>
<p><a href=""https://i.sstatic.net/AmEwy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AmEwy.png"" alt=""enter image description here"" /></a></p>
<p>Activating smoothing allows easier interpretation.</p>
<p><a href=""https://i.sstatic.net/4FofP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4FofP.png"" alt=""enter image description here"" /></a></p>
","2024-01-09 22:50:39","2","Answer"
"77790111","77785405","","<p>If we want to penalize the model for predicting disconnected components, we make sure the penalty should be less severe than not producing a mask(s) at all.</p>
<p>So I think a safe way to do that is by penalizing false negative pixels. In this direction you may explore <strong>Focal loss</strong> (<a href=""https://paperswithcode.com/method/focal-loss"" rel=""nofollow noreferrer"">https://paperswithcode.com/method/focal-loss</a>). This loss adds penalty to pixels that cause a disconnection.</p>
<p>Also you may explore other custom differentiable losses. Given you always only have one output mask, you may try out a loss that penalizes more contours. Since you apply the loss before thresholding, you can try out <strong>Sum/Mean of Gradients</strong> (use bigger shifts unlike used in edge detection).</p>
","2024-01-09 22:48:57","1","Answer"
"77790053","77789645","","<p>I think you are seeing both the actual values and the smoothed values. Drag the smoothing slider all the way down (it defaults to about 0.6).</p>
<p><a href=""https://i.sstatic.net/H9O4F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H9O4F.png"" alt=""enter image description here"" /></a></p>
","2024-01-09 22:28:13","2","Answer"
"77789645","","Why does my tensorboard plot have multiple lines?","<p>I'm only plotting a single sequence of 2d points. Can someone explain why my graph looks like this?<a href=""https://i.sstatic.net/5sP9U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5sP9U.png"" alt=""enter image description here"" /></a></p>
<p>I understand there might be actual + smooth, but why are there are mess of like 5 different lines here?</p>
","2024-01-09 20:43:13","2","Question"
"77789243","77777706","","<p>The error is here:</p>
<p><code>train_preds_probs = torch.softmax(train_logits,dim=1).argmax(dim=1).type(torch.float32)</code></p>
<p>When you <code>argmax</code>, you lose the gradient chain and can't backprop. For training, you pass the softmax output to your loss function without the argmax.</p>
","2024-01-09 19:21:23","0","Answer"
"77788560","77787313","","<p>In machine learning tuning parameters on the test set is always a bad idea. The only way to get a reasonable approximation on the generalisability of your model is by testing it on unseen data. As soon as you base any decision of the modelling process on the test set, you introduce bias, therefore degrading the approximation of the true generalisability.</p>
<p>I dont really follow why you need to asses performance on the test set to know where the model is heading. This can also be done on the validation set.</p>
<p>I have encountered instances before where I needed a second validation set, therefore dividing my dataset in 4 chunks (train, val1, val2, test). Its not standard, but maybe this can be a solution for your situation.</p>
","2024-01-09 17:12:34","0","Answer"
"77787662","77769473","","<p>You can achieve the desired output form [input_channels, out_channels] by transposing <code>Ht</code> before performing matrix multiplication with <code>dot_product_pseudo</code>. Here is the updated code:</p>
<pre class=""lang-py prettyprint-override""><code>dot_product_with_hidden_matrix = torch.mm(dot_product_pseudo, Ht.t())
</code></pre>
","2024-01-09 14:55:19","0","Answer"
"77787313","","How to split dataset in multiclass classification task of computer vision?","<p>I am generally talking about Zero-shot Learning.
I feel that the current data splitting method for multi-task classification is not very reasonable because the validation set and the test set contain completely different classes. This can easily lead to parameters tuned on the validation set performing poorly on the test set, making it challenging to select parameters that truly yield high performance.</p>
<p>As far as I can see, the only solution is to tune the parameters on the validation set and also assess performance on the test set simultaneously. Otherwise, it's hard to know where the model is heading on its own. However, this approach is not very standard and is almost equivalent to tuning parameters directly on the test set.
Of course, my current understanding is limited to the field of video classification. I'm not sure if other fields follow the same classification approach.</p>
","2024-01-09 13:59:25","0","Question"
"77786740","77786588","","<p>It seems that there's nothing wrong about your data loading code. Since your program is stuck, I guess the part most likely to cause problem is the way you set the device. Specifically, you have set the device as <code>cuda:2</code>, which may make the program to stuck when the <code>cuda:2</code> device is not available in your environment.</p>
<p>I suggest that you follow these steps to check whether the problem is caused by inappropriate GPU index:</p>
<ol>
<li>Replace the <code>cuda:2</code> which <code>cuda</code> in your code.</li>
<li>Check <code>nvidia-smi</code> to determine which GPU is available.</li>
<li>Run your program with prefix <code>CUDA_VISIBLE_DEVICES=2</code> to control the visible cuda devices. (Here I take <code>2</code> for example, and you may need to figure out the actual value by yourself.)</li>
</ol>
<p>Although I'm not sure whether this is what caused your problem, it's a good idea to decouple your code from physical devices anyway.</p>
","2024-01-09 12:24:24","0","Answer"
"77786588","","ImageFolder cannot load classify dataset in Remote Linux Server","<p>&quot;I am encountering an issue while using the classic image classification neural network ResNet-101 for an image classification task. Specifically, during the data loading phase, the program appears to be running but is stuck and not progressing. The interface looks as follows:</p>
<p><img src=""https://i.sstatic.net/zFJMU.png"" alt=""enter image description here"" /></p>
<p>&quot;I attempted to enter debugging mode and set breakpoints in the code segment responsible for loading the dataset. The feedback is as follows:</p>
<p><img src=""https://i.sstatic.net/w36Mq.png"" alt=""enter image description here"" /></p>
<p>Here is a portion of the code：</p>
<pre><code>def main():
    device = torch.device(&quot;cuda:2&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    print(&quot;using {} device.&quot;.format(device))

    data_transform = {
        &quot;train&quot;: transforms.Compose([transforms.RandomResizedCrop(224),
                                     transforms.RandomHorizontalFlip(),
                                     transforms.ToTensor(),
                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),
        &quot;val&quot;: transforms.Compose([transforms.Resize(256),
                                   transforms.CenterCrop(224),
                                   transforms.ToTensor(),
                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}

    image_path = r&quot;/home/sc/ClassicalNeuralNetwork/dataset/CGIARWheatGrowthStageChallenge&quot;  # flower data set path
    assert os.path.exists(image_path), &quot;{} path does not exist.&quot;.format(image_path)
    trains_dataset = datasets.ImageFolder(root=os.path.join(image_path, &quot;train&quot;),
                                         transform=data_transform[&quot;train&quot;],
                                          target_transform=None)
    # print(train_dataset)
    train_num = len(trains_dataset)

    # {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}
    flower_list = trains_dataset.class_to_idx
    cla_dict = dict((val, key) for key, val in flower_list.items())
    # write dict into json file
    json_str = json.dumps(cla_dict, indent=4)
    with open('class_indices.json', 'w') as json_file:
        json_file.write(json_str)

    batch_size = 4
    nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 4])  # number of workers
    print('Using {} dataloader workers every process'.format(nw))

    train_loader = torch.utils.data.DataLoader(trains_dataset,
                                               batch_size=batch_size, shuffle=True,
                                               num_workers=nw)

    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, &quot;val&quot;),
                                            transform=data_transform[&quot;val&quot;])
    val_num = len(validate_dataset)
    validate_loader = torch.utils.data.DataLoader(validate_dataset,
                                                  batch_size=batch_size, shuffle=False,
                                                  num_workers=nw)

    print(&quot;using {} images for training, {} images for validation.&quot;.format(train_num,
                                                                           val_num))

    net = resnet101(num_classes=7)
    net = net.cuda(device)`
</code></pre>
<p>And I also tried a convnet network, encountering the same error. Regardless of how I modify the name of the 'train_dataset', the same error persists during debugging.</p>
<p>but when I run it in my windows laptop,it works</p>
","2024-01-09 11:56:37","0","Question"
"77785405","","Loss function for semantic segmentation which do penalty for mask separation","<p>I have a semantic segmentation task, which I'm solving using PyTorch. I use (dice loss + BCE) as loss function. I know that each image has exactly one mask and I want to do additional penalty if mask was separated into several parts. Which loss function I can use for it?</p>
","2024-01-09 08:46:28","1","Question"
"77784936","","I have a two GPU NVIDIA Driver showing it's one GPU ID using python code on ubuntu","<p>I have a 2 A100 PCIE (80GB VRAM) GPU but NVIDIA is giving me on visible gpu is 1.</p>
<p>Such as a 0 index GPU and I am unable to get the 1 index GPU on ubuntu server.</p>
<p>Here is the code result
<a href=""https://i.sstatic.net/wEet9.png"" rel=""nofollow noreferrer"">Pytorch Image Here</a></p>
<p>what does this command show:</p>
<p><code>nvidia-smi</code></p>
<p>Here is the result</p>
<p><a href=""https://i.sstatic.net/PQWhj.png"" rel=""nofollow noreferrer"">Nvidia-SMI Image Here</a></p>
<p>So I want to run both of the GPU with their GPU ID using python in  ubntu server.</p>
","2024-01-09 07:20:23","-1","Question"
"77783222","","Should I be using ""torch.set_grad_enabled""?","<p>I've never seen any code examples using this line during training: torch.set_grad_enabled</p>
<p>I've also never used it before. I read that it helps conserve memory, but if that was the case, shouldn't most people be using this?</p>
","2024-01-08 21:22:33","1","Question"
"77783070","77782918","","<p>it's not the most efficient, but you could do something like:</p>
<pre><code>In [37]: mask = (A.repeat( 3,1  ).t() == before).t().sum(0).to(torch.bool)

In [38]: torch.where(~mask, A, after[mask.to(torch.int).cumsum(0)-1] )
Out[38]: tensor([ 1, 20,  3, 40, 50,  6])
</code></pre>
<p>in this case you're materializing every possible value to check and summing over the matches to see if there is one. It's not a loop, so maybe can take advantage of the GPU, but it's not inherently better complexity.</p>
<p>What you really want for efficiency is a set operation, but I can't think of any in pytorch that would give you the indexing you're looking for. You could try to find some alternative using <code>torch.unique(... return_counts=True)</code> or search for other questions about intersection of pytorch tensors to see if there's a way to construct the mask more efficiently.</p>
","2024-01-08 20:41:42","0","Answer"
"77782918","","Replace values in PyTorch tensor","<p>Given a <code>before</code> and <code>after</code> tensor, I want to replace all instance of <code>before</code> in another tensor <code>A</code> with <code>after</code> without using loops.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>before = torch.Tensor([2,4,5])
after  = torch.Tensor([20,40,50])
A = torch.Tensor([1,2,3,4,5,6])

result = replace(A, before, after)
</code></pre>
<p>The result should be <code>torch.Tensor([1,20,3,40,50,6])</code>.</p>
","2024-01-08 19:58:55","1","Question"
"77778279","77777706","","<p>Sometime this kind of error is thrown as part of your code is wrapped by a</p>
<pre><code>with torch.no_grad():
</code></pre>
<p>I suggest to check various function in your code checking for it.
Maybe you have already checked it but this is a good start!</p>
","2024-01-08 09:17:23","0","Answer"
"77777706","","Getting no grad set error in pytorch while traning","<p><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn </code></p>
<p>i'm getting this error with the following training loop, the grads must have been set by the sequential itself, but it's saying no grads.</p>
<pre><code>&quot;&quot;&quot;Training&quot;&quot;&quot;
Epochs = 100


for epoch in range(Epochs):
    model.train()

    train_logits = model(X_train)
    train_preds_probs = torch.softmax(train_logits,dim=1).argmax(dim=1).type(torch.float32)
    loss = loss_fn(train_preds_probs,y_train)
    train_accu = accuracy(y_train,train_preds_probs)
    print(train_preds_probs)
    optimiser.zero_grad()

    loss.backward()

    optimiser.step()

    #training
    model.eval()
    with torch.inference_mode():
        test_logits = model(X_test)
        test_preds = torch.softmax(test_logits.type(torch.float32),dim=1).argmax(dim=1)
        test_loss = loss_fn(test_preds,y_train)
        test_acc = accuracy(y_test,test_preds)

    
    if epoch%10 == 0:
        print(f'Epoch:{epoch} | Train loss: {loss} |Taining acc:{train_accu} | Test Loss: {test_loss} | Test accu: {test_acc}')





</code></pre>
<p>I tried surfing the internet for this, but didn't get a solution.</p>
<p>Any help is appreciated!</p>
","2024-01-08 07:51:44","0","Question"
"77774507","77768133","","<p>It is indeed filling the value with -100. You can find an example of that <a href=""https://github.com/pytorch/pytorch/blob/4b74bb6c3428c111bf0c823aa6db1fa58fcf82c2/aten/src/ATen/native/Loss.cpp#L292"" rel=""nofollow noreferrer"">here</a>.</p>
<p>This is most likely a hack to avoid an error caused by accidental rounding to zero.</p>
<p>Technically speaking, the input probabilities to <code>binary_cross_entropy</code> are supposed to be generated by a sigmoid function, which is bounded asymptotically between <code>(0, 1)</code>. This means the input should never actually be zero, but this may occur due to numerical precision issues for very small values.</p>
","2024-01-07 19:23:02","1","Answer"
"77772327","","TypeError when trying to display transformed images PyTorch","<p>I have some trouble defining the transforms for images using PyTorch.
Here you are the transforms I need:</p>
<pre><code>mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.Resize((256, 256), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),
    transforms.CenterCrop(size=[224,224]),
    transforms.Normalize(mean, std),
    transforms.PILToTensor()
    
    
])

test_transform = transforms.Compose([
    transforms.Resize((256, 256), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),
    transforms.CenterCrop(size=[224,224]),
    transforms.Normalize(mean, std),
    transforms.PILToTensor(),
])
</code></pre>
<p>Then, I create the loaders for the images of the dataset:</p>
<pre><code>BATCH_SIZE = 32

trainset = torchvision.datasets.ImageFolder(root='CVPR2023_project_2_and_3_data/train/', loader=open_image)

trainset_classes = trainset.classes.copy()

subset_size = int(0.15*len(trainset))

validset = torchvision.datasets.ImageFolder(root='CVPR2023_project_2_and_3_data/train/', loader=open_image)

indices = torch.randperm(len(trainset))

valid_indices = indices[:subset_size]
train_indices = indices[subset_size:]

trainset = Subset(trainset, train_indices)
validset = Subset(validset, valid_indices)

# Apply transformations only to the training set
trainset.dataset.transform = train_transform
# Apply transformations to the validation set
validset.dataset.transform = test_transform

trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True) # batch size of 1 because we have to crop in order to get all images to same size (64x64), also see pin_memory optin
validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)

testset = torchvision.datasets.ImageFolder(root='CVPR2023_project_2_and_3_data/test/', transform=test_transform, loader=Image.open)
testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)

print(f'entire train folder: {len(trainset)}, entire test folder: {len(testset)}, splitted trainset: {len(trainset)},  splitted validset: {len(validset)}')
</code></pre>
<p>Then I load a pre-trained network and freeze all the layers but the last one:</p>
<pre><code>model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)

model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=15, bias=True) #adapting to 15 classes

for param in model.parameters():
    param.requires_grad = False

for param in model.classifier[6].parameters():
    param.requires_grad = True

</code></pre>
<p>Then, I define a function for showing an image and I try to print one:</p>
<pre><code>def imshow(img):
    npimg = img.numpy()
    plt.axis(&quot;off&quot;)
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

images, labels = next(iter(trainloader)) # &lt;-- error

print(images[0])
</code></pre>
<p>This last piece of code does not work and the program crashes with the following error:</p>
<pre><code>img should be Tensor Image. Got &lt;class 'PIL.Image.Image'&gt;
</code></pre>
<p>I have already tried to change the transforms' order but I get the inverse error, i.e.</p>
<pre><code>img should be &lt;class 'PIL.Image.Image'&gt; Image. Got Tensor

</code></pre>
<p>Can anyone explain how I should solve this error?</p>
<p>Thank you in advance for your patience</p>
","2024-01-07 07:59:31","1","Question"
"77771376","77771262","","<p>Consider updating your Python version on Colab to an older one (3.8 for example),</p>
<p>I tried Python 3.8 and it worked fine .</p>
<p>See :</p>
<p><a href=""https://stackoverflow.com/a/68530310/18963373"">here</a> is the method that I followed to update the versio.</p>
","2024-01-06 23:15:44","0","Answer"
"77771262","","Can't install PyTorch 1.7.0","<p>I need to install these four packages (to run <a href=""https://github.com/kris927b/SkillSpan"" rel=""nofollow noreferrer"">this code</a>):</p>
<pre><code>!pip install allennlp==1.3
!pip install transformers==4.0.0
!pip install torch==1.7.0
!pip install networkx
</code></pre>
<p>These commands work in a Jupyter Notebook, but I need to use a GPU for this project. When I run it in Google Colab, Kaggle Kernel, and Deepnote, I get this error:</p>
<p>ERROR: Could not find a version that satisfies the requirement torch==1.7.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2) ERROR: No matching distribution found for torch==1.7.0</p>
<p>Running these (suggested by <a href=""https://stackoverflow.com/questions/66858277/could-not-find-a-version-that-satisfies-the-requirement-torch-1-7-0cpu"">other posts</a>) still produces an error:</p>
<pre><code>!pip install torch==1.7.0 -f https://download.pytorch.org/whl/torch_stable.html 
!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>I also tried the proxy fix from <a href=""https://stackoverflow.com/questions/52328655/pip-cant-install-any-package"">here</a>, but none of the 30 proxies I tried worked.</p>
<p>How can I install PyTorch 1.7.0 on a platform that provides free GPU access?</p>
","2024-01-06 22:28:47","0","Question"
"77770676","77769421","","<p>You are getting this error because your input tensor is in the cuda device and the weight tensor is in the CPU device.</p>
<p>To resolve this issue you need to move the data to the GPU by changing the lines to:</p>
<pre class=""lang-py prettyprint-override""><code>x, y = dls.train.one_batch().cuda()
activs = learn.model(x).cuda()
</code></pre>
<p>Hope this resolves your problem. Thanks!</p>
","2024-01-06 19:09:38","0","Answer"
"77769473","","Adjusting Output Size of Auxiliary Matrix in Python","<p>After hidden layer activation, I want to create an auxiliary matrix to better capture the temporal aspect of the data in the below code snippet. The current shape of the return variable is <code>[out_channels, out_channels]</code> but I want the returned shape to be <code>[input_channels, out_channels]</code>. Which part of the code should be modified to achieve the desired output while keeping the idea/logic as it is?</p>
<pre><code>def my_fun( self, H: torch.FloatTensor,) -&gt; torch.FloatTensor:
    
    self.input_channels = 4
    self.out_channels = 16
    self.forgettingFactor = 0.92
    self.lamb = 0.01
    self.M = torch.inverse(self.lamb*torch.eye(self.out_channels))
    HH = self.calculateHiddenLayerActivation(H) # [4,16]
    Ht = HH.t() # [16 , 4]
    ###### Computation of auxiliary matrix 
    
    initial_product = torch.mm((1 / self.forgettingFactor) * self.M, Ht) # [16, 4]
    intermediate_matrix = torch.mm(HH, initial_product )   # [4, 4]
    sum_inside_pseudoinverse = torch.eye(self.input_channels) + intermediate_matrix # [4, 4]

    pseudoinverse_sum = torch.pinverse(sum_inside_pseudoinverse) # [4, 4]
    product_inside_expression = torch.mm(HH, (1/self.forgettingFactor) * self.M) #  [4, 16]              
    dot_product_pseudo = torch.mm( pseudoinverse_sum  , product_inside_expression) # [4, 16]   
    dot_product_with_hidden_matrix = torch.mm(Ht, dot_product_pseudo ) # [16, 16]

    res = (1/self.forgettingFactor) * self.M - torch.mm((1/self.forgettingFactor) * self.M, dot_product_with_hidden_matrix  ) # [16,16]        

    return res
</code></pre>
","2024-01-06 12:55:49","1","Question"
"77769421","","How to solve RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same","<p>I am using Fastai for multi-categorical image classification.Here is the full code:</p>
<pre><code>from fastasi.vision.all import *
import numpy as np
import pandas as pd

path = untar_data(URLs.PASCAL_2007

df = pd.read_csv(path/'train.csv')

def get_x(r): return r['fname']

def get_y(r): return r['labels']

def splitter(df):
    train = df.index[~df['is_valid']].tolist()
    valid = df.index[df['is_valid']].tolist()
    return train, valid

dblock = DataBlock(blocks = (ImageBlock, MultiCategoryBlock),
                  splitter = splitter,
                  get_x = get_x,
                  get_y = get_y,
                  item_tfms = RandomResizedCrop(128, min_scale = 0.35))
dls = dblock.dataloaders(df)

learn = cnn_learner(dls, resnet18)
x,y = dls.train.one_batch()
activs = learn.model(x)
activs.shape
</code></pre>
<p>when I run this code, the above error occurs. By the way, I am using Kaggle notebook and GPU. The error occurs after
<code> activs = learn.model(x)</code>. Anyone to help</p>
","2024-01-06 12:40:38","1","Question"
"77768133","","Pytorch's `binary_cross_entropy` seems to implement ln(0) = -100. Why?","<p>I'm curious as to why Pytorch's <code>binary_cross_entropy</code> function seems to be implemented in such a way to calculate ln(0) = -100.</p>
<p>The binary cross entropy function from a math point of view calculates:</p>
<p>H = -[ p_0*log(q_0) + p_1*log(q_1) ]</p>
<p>In pytorch's <code>binary_cross_entropy</code> function, <code>q</code> is the first argument and <code>p</code> is the second.</p>
<p>Now suppose I do <code>p = [1,0]</code> and <code>q = [0.25, 0.75]</code>. In this case, <code>F.binary_cross_entropy(q,p)</code> returns, as expected: -ln(0.25) = 1.386.</p>
<p>If we reverse the function arguments and try <code>F.binary_cross_entropy(p,q)</code>, this should return an error, since we would try calculating -0.75*ln(0), and ln(0) is in the limit -infinity.</p>
<p>Nonetheless, if I do <code>F.binary_cross_entropy(p,q)</code> it gives me 75 as the answer (see below):</p>
<pre><code>&gt; import torch.nn.functional as F 
&gt; pT = torch.Tensor([1,0]) 
&gt; qT =torch.Tensor([0.25,0.75]) 
&gt; F.binary_cross_entropy(pT,qT)
</code></pre>
<blockquote>
<p>tensor(75.)</p>
</blockquote>
<p>Why it was implemented in this way?</p>
","2024-01-06 03:15:53","0","Question"
"77764141","77762084","","<p>The labels of the dataset should be annotated sequentially. The number of labels in the dataset should match the number specified in the code, ensuring a proper execution without assertion errors.</p>
","2024-01-05 11:06:32","0","Answer"
"77762546","","How to install pytorch=1.0 and why pytorch installing command no longer work","<p>In <a href=""https://pytorch.org/get-started/previous-versions/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/previous-versions/</a>, the way to install pytorch 1.0 is to run &quot;conda install pytorch==1.0.0 torchvision==0.2.1 cuda100 -c pytorch&quot;. It failed because pytorch==1.0.0 can no longer be found. How should I fix that?
My python version is 3.6.</p>
<p>BTW, does anyone have a wheel for pytorch=1.0 with '-D_GLIBCXX_USE_CXX11_ABI=0'? I ran &quot;conda install pytorch==1.0.1 -c pytorch&quot; successfully but the torch._C._GLIBCXX_USE_CXX11_ABI param is True. I have no idea how to fix it.</p>
","2024-01-05 04:37:23","0","Question"
"77762084","","PointNet++ model；I am encountering an assertion issue when modifying the categories for the segmentation model","<p>I previously had 13 categories for this model, and now I have changed it to 17 categories for segmentation. I have updated both the 'num_classes' and the weights to correspond to the 17 categories. I also modified 'pointnet_sem_seg' to accommodate 17 categories. However, I am still encountering an assertion error on my end.</p>
<pre><code>BN momentum updated to: 0.100000
  0%|                                          | 5/3121 [00:30&lt;05:34,  9.32it/s]/opt/conda/conda-bld/pytorch_1614378098133/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [10,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
/opt/conda/conda-bld/pytorch_1614378098133/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [26,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
  0%|                                        | 5/3121 [00:30&lt;5:19:21,  6.15s/it]
Traceback (most recent call last):
  File &quot;/home/dl/Pointnet_Pointnet2_pytorch/train_semseg.py&quot;, line 303, in &lt;module&gt;
    main(args)
  File &quot;/home/dl/Pointnet_Pointnet2_pytorch/train_semseg.py&quot;, line 202, in main
    loss = criterion(seg_pred, target, trans_feat, weights)
  File &quot;/home/dl/anaconda3/envs/python3.7/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/dl/Pointnet_Pointnet2_pytorch/models/pointnet_sem_seg.py&quot;, line 42, in forward
    mat_diff_loss = feature_transform_reguliarzer(trans_feat)
  File &quot;/home/dl/Pointnet_Pointnet2_pytorch/models/pointnet_utils.py&quot;, line 140, in feature_transform_reguliarzer
    I = I.cuda()
RuntimeError: CUDA error: device-side assert triggered
</code></pre>
","2024-01-05 01:38:33","0","Question"
"77760620","","SetFit training not finishing evaluation step","<p>I am trying to train a simple binary classification with SetFit, but I have a problem with the library. I use huggingface to manage my dataset. The dataset does consist of a text and a label column. If I print my dataset it looks like this:</p>
<pre><code>dataset = load_dataset(&quot;&lt;my-dataset&gt;&quot;)
print(dataset)
</code></pre>
<p>with output:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 20
    })
    eval: Dataset({
        features: ['text', 'label'],
        num_rows: 10
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 135
    })
})
</code></pre>
<p>Here is my code for the training:</p>
<pre><code># Initialize SetFit model with a pre-trained model and define label name
model = SetFitModel.from_pretrained(
    &quot;paraphrase-multilingual-mpnet-base-v2&quot;,
    labels=[&quot;negative&quot;, &quot;positive&quot;],
)

# Define the training arguments
args = TrainingArguments(
    batch_size=32,
    num_epochs=8,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True
)

# Initialize the trainer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset[&quot;train&quot;],
    eval_dataset=dataset[&quot;eval&quot;],
    metric=&quot;accuracy&quot;,
    column_mapping={&quot;text&quot;: &quot;text&quot;, &quot;label&quot;: &quot;label&quot;}  # Map dataset columns to text/label expected by trainer
)

# Train the model
trainer.train()
</code></pre>
<p>But the problem I now have is that the training behaves very weird. I do not get any Training or Validation losses, nor do the evaluation steps ever finish. I don't know what the problem is.</p>
<p><a href=""https://i.sstatic.net/GCvmK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GCvmK.png"" alt=""Overview of training"" /></a></p>
<p>Also please note, that I slightly changed the parameters to increase the training speed. It normally has more steps and so on. It still behaves weirdly with the normal parameters. I also use version 1.0.1 of SetFit. I haven't found any issues regarding this in the GitHub repository.</p>
","2024-01-04 19:02:49","1","Question"
"77760344","77755832","","<p><em>&quot;Why Pytorch does not provide any means to store list of references to tensors, instead of forcing everything to be copied?&quot;</em></p>
<p>I don't see a answer to this question in pytorch docs, but this question is irrelevant to this post. I say this because, when you comment out <code># abnormal_scores.append(score)</code> it removes the only reference to the score for the next iteration. <strong>This allows for garbage collecting to remove that score's computational graph, this is why you see the lower memory usage.</strong></p>
<p>This section of the model: <a href=""https://i.sstatic.net/MRt6J.png"" rel=""nofollow noreferrer"">enter image description here</a> where you are comparing different scores is what is causing this overhead not the way you are storing the tensor.</p>
<p>You have three possibilities:</p>
<p><strong>-- Gradient accumulation:</strong> broadly in models you could accumulate the gradients and divide by the sum rather than backpropagating across the aggregate, because <strong>a sum of gradients = a gradient of sums</strong>. Now this doesn't apply directly in this case because you are doing comparisons rather than summation, one option is you could sample from a distribution of the scores rather than maintaining a record of all of them  this would allow for lower overhead (storing fewer computational graphs).</p>
<p><strong>-- Smaller batches</strong>: somewhat self explanatory, you can use smaller batch sizes, or in this case fewer comparisons at a time</p>
<p><strong>-- CPU Memory / Storage</strong> You can move those tensors to CPU as you append them offloading the gpu memory strain to the RAM, but beware <a href=""https://stackoverflow.com/questions/77403056/pytorch-cpu-oom-kills-ssh-server-on-linux?noredirect=1#comment136456461_77403056"">Pytorch CPU OOM kills ssh server on linux</a> (this doesn't apply on colab, but any linux server)</p>
<p><strong>Regarding the pytorch references question, I will continue to look for a reference and edit this if I find something that isn't just a guess</strong></p>
","2024-01-04 18:06:42","0","Answer"
"77759006","77749015","","<p>A dataloader basically concatenates the items in a batch into one tensor. Lets say the items in your batch are of size N x M and you have batch size K, the input to the model becomes K x N x M. Therefore both N and M have to be same for all items.</p>
<p>Sometimes you have variable dimensions in your data, like with Recurrent Neural Networks. In that case you can use batches by doing one of the following:</p>
<ul>
<li>Sort your data and pick your batches based on their dimensionality. I.e. you enforce items in your batch to have the same size, but different batches can have different sizes.</li>
<li>Use padding to 'fill up' the items in your batches that are smaller than the maximum size within your data.</li>
</ul>
","2024-01-04 14:25:37","1","Answer"
"77756078","77756025","","<p>This can be done using the <code>mean</code> function and telling it which axes to perform the mean over and that the dimensions should be maintained. The code in pytorch is very similar to the code in numpy except <code>axis</code> is replaced with <code>dim</code> and <code>keepdims</code> is replaced with <code>keepdim</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import torch

a = torch.rand(7, 12, 12, 197, 197)
print(a.shape)  # torch.Size([7, 12, 12, 197, 197])
b = torch.mean(a, dim=(1,2,4), keepdim=True)
print(b.shape)  # torch.Size([7, 1, 1, 197, 1])
</code></pre>
<p>and in numpy</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

rng = np.random.default_rng(42)
a = rng.random((7, 12, 12, 197, 197))
print(a.shape)  # (7, 12, 12, 197, 197)
b = np.mean(a, axis=(1,2,4), keepdims=True)
print(b.shape)  # (7, 1, 1, 197, 1)
</code></pre>
","2024-01-04 05:29:18","0","Answer"
"77756066","77756025","","<p>Torch provides <code>mean</code> along with <code>dim</code> as argument. This <code>dim</code> can take tuple of integers.</p>
<pre><code>import torch
tensor = torch.randn(4, 5, 6, 7)

dims_to_reduce = (1,3)
means = torch.mean(tensor, dim=dims_to_reduce, keepdim=True)
print(means.shape)
&gt;&gt;torch.Size([4, 1, 6, 1])
</code></pre>
","2024-01-04 05:24:14","2","Answer"
"77756062","77756025","","<p>With <a href=""/questions/tagged/numpy"" class=""post-tag"" title=""show questions tagged &#39;numpy&#39;"" aria-label=""show questions tagged &#39;numpy&#39;"" rel=""tag"" aria-labelledby=""tag-numpy-tooltip-container"">numpy</a> you can use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.mean.html"" rel=""nofollow noreferrer""><code>mean</code></a> with a tuple of the dimensions/axes to use for the aggregation, and <code>keepdims=True</code> to retain the shape:</p>
<pre><code>out = arr.mean((1,2,4), keepdims=True)
</code></pre>
<p>Example:</p>
<pre><code>np.random.seed(0)
arr = np.random.random((7, 12, 12, 197, 197))
print(arr.shape)

out = arr.mean((1,2,4), keepdims=True)
print(out.shape)
</code></pre>
<p>Output:</p>
<pre><code>(7, 12, 12, 197, 197)
(7, 1, 1, 197, 1)
</code></pre>
","2024-01-04 05:23:37","1","Answer"
"77756042","77756025","","<p>Here is a simple method by using torch.mean , please check if it works.</p>
<pre><code>import torch

tensor = torch.randn(4, 5, 6, 7)  # Example tensor
nth_dim = 2  # Dimension to keep

means = torch.mean(tensor, dim=torch.arange(tensor.ndim) != nth_dim)
print(means.shape)  # Output: torch.Size([4, 6, 7])
</code></pre>
","2024-01-04 05:19:07","0","Answer"
"77756025","","Calculating mean across all dimensions of a tensor except nth dimension","<p>I have a torch tensor of dimension <code>[7, 12, 12, 197, 197]</code>. I would like to calculate mean such that resultant vector is of shape <code>[7,1,1,197,1]</code>. Is there any other way rather than using a <code>for loop</code>? Could you provide answer with benchmark on execution time?</p>
","2024-01-04 05:11:48","1","Question"
"77755832","","Storing neural network forwarded states in Pytorch","<p>I am implementing [1] model on Google Colaboratory with Pytorch in which I need to store multiple states of forward passes for later loss and back-propagation computations. To do this, I stored the resulting tensors into a Python list.</p>
<p>However, I later discovered that this approach would pose memory leaks since the entire computational graph is copied onto the list [2], leading to a huge amount of GPU RAM be used up. I can not do anything after a few tensors are passed through the network.</p>
<p>I did found a solution on Pytorch forum [3] that uses torch.utils.checkpoint, but I am not satisfied. Why Pytorch does not provide any means to store list of references to tensors, instead of forcing everything to be copied?</p>
<p>[1] <a href=""https://www.crcv.ucf.edu/projects/real-world/"" rel=""nofollow noreferrer"">https://www.crcv.ucf.edu/projects/real-world/</a> <br>
[2] <a href=""https://discuss.pytorch.org/t/correct-way-storing-states-inside-one-forwardpass/46868"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/correct-way-storing-states-inside-one-forwardpass/46868</a> <br>
[3] <a href=""https://discuss.pytorch.org/t/memory-when-storing-states-in-a-list/63012/2"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/memory-when-storing-states-in-a-list/63012/2</a></p>
<p>The part of my code for reference, notice I commented out the <code>append</code> method and observe a huge memory saved. I added <code>break</code> for debugging.</p>
<pre><code>    &quot;&quot;&quot; Run 01 epoch of training
    &quot;&quot;&quot;
    def fit_epoch(self):
        epoch_training_loss = []

        for batch_idx, (normal_bag, abnormal_bag) in enumerate(self._DATALOADER):
            self._OPTIM.zero_grad()
            self._MODEL.train()

            normal_scores = []
            for idx, segment in enumerate(normal_bag):
                segment = segment.type(torch.FloatTensor).to(device)
                score = self._MODEL.forward(segment)
                # normal_scores.append(score)

                del segment, score

                # Lookout memory leaks
                # https://discuss.pytorch.org/t/memory-leak-when-appending-tensors-to-a-list/25937/3

            abnormal_scores = []
            for idx, segment in enumerate(abnormal_bag):
                segment = segment.type(torch.FloatTensor).to(device)
                score = self._MODEL.forward(segment)
                # abnormal_scores.append(score)

                del segment, score

            break

            # Compute loss
            loss = self._LOSS_FN(normal_scores, abnormal_scores)

            # Back-propagation
            loss.backward()
            self._OPTIM.step()

            epoch_training_loss.append(loss.item())

        self._save_model()
        return epoch_training_loss
</code></pre>
<p><a href=""https://i.sstatic.net/ex9QF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ex9QF.png"" alt=""GPU piles up"" /></a></p>
","2024-01-04 04:06:43","0","Question"
"77754542","77752683","","<p>Some clarifying questions.</p>
<p>How many GPUs do you have access to?</p>
<p>Are the local models pre-trained or trained in-line? If they are pre-trained, is it necessary to backprop through them?</p>
<p>GPUs really do not like parallel processing within a single card. If you have access to multiple cards, you can look into parallelizing the local models across multiple cards. If you don't have enough GPUs for this, you're better off running the local models in series. Multi-model parallel processing with card sharing will be a huge pain and most likely be slower than running in series.</p>
<p>If the local models are pre-trained and you don't need to backprop through them, you can consider building a feature extraction pipeline that computes local model outputs prior to training.</p>
","2024-01-03 20:37:09","0","Answer"
"77753218","77752683","","<p>To run multiple modules in parallel, you often want multiple processes (and multiple GPUs). Python isn't very good at multithreading (due to the GIL).</p>
<p>Even without Python, GPU programming is highly optimized for parallelizing large individual operations performed over a tensor, not for performing multiple separate operations in parallel. To perform separate operations in parallel on a single GPU, you'd need to send them to different GPU executors (&quot;streams&quot;).</p>
<p>You didn't link to the paper, but it's also sometimes the case that splitting an operation into multiple parallel modules is done just to better utilize multiple GPUs.</p>
","2024-01-03 16:17:23","0","Answer"
"77752683","","How to run multiple modules within a module in parallel?","<p>I've been reading a paper which contained the following machine learning model which I would like to replicate in PyTorch</p>
<p><a href=""https://i.sstatic.net/Y8Frs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Y8Frs.png"" alt=""enter image description here"" /></a></p>
<p>Essentially the input gets split into n equally sized vectors and each of them gets passed to a separate local model. All outputs of local models are then concatenated and run through the next layer. (x is not relevant to my question here, so let's ignore it)</p>
<p>So far I've come up with this:</p>
<pre><code>class GlobalModel(torch.nn.Module):
    def __init__(self, n_local_models):
        super(GlobalModel, self).__init__()

        self.local_models = [LocalModel() for _ in range(n_local_models)]
        self.linear = torch.nn.Linear(100, 100)  
        self.activation = torch.nn.ReLU()
</code></pre>
<p>with LocalModel being some other torch.nn.Module. The linear Layer size is just a dummy, I will make it change dynamically with the local models later.</p>
<p>My question is, how can I best write a forward() function that runs all local models in parallel, before concatenating them and passing them on to the linear layer and activation function. Because the only way of implementing this that I can think of, is iterating through the list of local models and executing each of them in sequence. But this seems rather slow and I feel like there should be a more elegant solution.</p>
","2024-01-03 14:49:10","1","Question"
"77750951","77749015","","<p>In <code>collate_fn()</code> of dataloader, each element returned from <code>__getitem__()</code> function is simply stacked up themselves.</p>
<p>For example; tensors with a different shape cannot be stacked or concatenated with <code>torch.stack()</code> or <code>torch.cat()</code>.</p>
","2024-01-03 09:41:09","0","Answer"
"77749015","","Why does PyTorch dataloader require each element in the batch size to be the same?","<p>Otherwise you get this error: &quot;RuntimeError: each element in list of batch should be of equal size&quot;</p>
<p>Can someone explain why it's enforced?</p>
","2024-01-02 23:19:33","0","Question"
"77746860","77746352","","<p>You've mostly got the right idea. Small steps for implementing the second task:</p>
<ol>
<li>Load the pretrained VGG-19 model.</li>
<li>Freeze the weights of all layers except for the last two fully connected layers (FC1 and FC2). If you are defining a new Sequential, you won't have the pretrained weights since pretrained weights are not available, so the approach would involve training the layers from scratch rather than fine-tuning existing weights</li>
<li>Optionally, modify the last layer to fit the number of classes in your dataset.</li>
</ol>
<p>As you said above, the structure of the VGG-19's classifier section is:</p>
<pre><code>     (classifier): Sequential(
        (0): Linear(in_features=25088, out_features=4096, bias=True) # FC1
        (1): ReLU(inplace=True)
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=4096, out_features=4096, bias=True) # FC2
        (4): ReLU(inplace=True)
        (5): Dropout(p=0.5, inplace=False)
        (6): Linear(in_features=4096, out_features=1000, bias=True) # Original output layer
      )
</code></pre>
<p>The code for updating only FC1 and FC2 (and the final classifier layer if you choose to modify it) will be updated during training, while the rest of the network remains unchanged:</p>
<pre><code>    import torch.nn as nn
    from torchvision import models
    from torchvision.models import VGG19_Weights
    
    # Load the pretrained model
    vgg19_model = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1) # since 0.13, the argument 'pretrained' has been deprecated and may be deleted in the future
    
    # Freeze all layers' weights in the model
    for param in vgg19_model.parameters():  # get all the parameters of the model
        param.requires_grad = False  # these layers won't be update during training
    
    # Unfreeze weights of last two fully connected layers (FC1 and FC2)
    for param in vgg19_model.classifier[0].parameters():
        param.requires_grad = True  # will be updated during training
    for param in vgg19_model.classifier[3].parameters():
        param.requires_grad = True  # will be updated during training
    
    # (Recommended) Modify the last layer for your number of classes
    class_to_idx = TODO
    num_classes = len(class_to_idx)
    model.classifier[6] = nn.Linear(4096, num_classes)
</code></pre>
<p>The last fully connected layer (initially with 1000 output features for ImageNet) is replaced with the number of features equal to the number of classes in your dataset, and this is the recommended (if not essential) approach. If your task requires a different number of classes, you must adapt the model to output the correct number of possibilities. Another thing to consider is that, while your task may have the same number of classes as ImageNet, the classes themselves are likely to be different. Retraining the output layer helps the model to better discriminate between the classes specific to your task.</p>
","2024-01-02 15:01:22","0","Answer"
"77746479","77743228","","<p>This is not a problem specifically related to the value being <code>None</code>; you'd have the same issue if you were to use any other <code>nn.Module</code> (as the value of <code>additional</code> attribute) that is not a sequence (the <code>0</code> after <code>additional</code>) and does not have a parameter named <code>weight</code> in the first <code>nn.Module</code> in the sequential module (the <code>weight</code> after <code>additional.0</code>).</p>
<p>The issue is, in your <em>train</em> mode, when you initialized your model, you have passed <code>True</code> for <code>additional_layer</code> argument i.e.:</p>
<pre><code>model = YourModelClass(additional_layer=True)
</code></pre>
<p>hence <code>self.additional</code> is set to a <code>nn.Module</code> (<code>nn.Sequential</code> specifically). So, the <code>model</code> object's <code>state_dict</code> would have the parameters for the module referred to by the <code>self.additional</code> attribute.</p>
<p>Now, when you re-initialized the model for <em>inference</em>, you didn't have the additional layers as you initialized the model presumably by one of the following:</p>
<pre><code>model = YourModelClass(additional_layer=False)
model = YourModelClass()
</code></pre>
<p>This time there the <code>self.additional</code> i.e. <code>model.additional</code> attribute would be <code>None</code>. As a result, when you call <code>model.load_state_dict</code> and pass it the state dict that was saved earlier in <em>train</em> mode (when additinal layer was there), it gives you the exception that all the keys for the <code>additional</code> attribute are missing.</p>
<p>Assuming you have the correct conditional setup in the <code>forward</code> method when <code>self.additional</code> is <code>None</code>, you can ignore the exception and bypass the loading of missing keys/parameters by setting the <code>strict</code> argument to <code>False</code> while using <code>load_state_dict</code>:</p>
<pre><code>model.load_state_dict(best_model[&quot;my_model&quot;], strict=False)
</code></pre>
","2024-01-02 13:47:39","0","Answer"
"77746352","","How to fine tune FC1 and FC2 in VGG-19 PyTorch?","<p>I need to fine tune pretrained <a href=""https://i.sstatic.net/9SxXe.png"" rel=""nofollow noreferrer"">VGG-19</a> with pytorch. I have these specific tasks:</p>
<ol>
<li>Finetune the weights of all layers in the VGG-19 network.</li>
<li>Finetune the weights of only two last fully connected (FC1 and FC2) layers
in the VGG-19 network. And this is the only information given to me.</li>
</ol>
<p>VGG-19 structure is like below:</p>
<pre class=""lang-py prettyprint-override""><code>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): ReLU(inplace=True)
    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): ReLU(inplace=True)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
</code></pre>
<p>I tried to do first task like this and i think it is correct:</p>
<pre><code>model = models.vgg19(pretrained=True)

for param in model.parameters():
    param.requires_grad = True

model.classifier[6] = nn.Linear(4096, len(class_to_idx))
</code></pre>
<p>But i couldn't figure out second task, i tried this and i am not sure:</p>
<pre><code>model2 = models.vgg19(pretrained=True)

for param in model2.parameters():
    param.requires_grad = False

# Set requires_grad to True for FC1 and FC2
for param in model2.classifier[0].parameters():
    param.requires_grad = True

for param in model2.classifier[3].parameters():
    param.requires_grad = True

# Modify the last fully connected layers for the number of classes in your dataset
model2.classifier[6] = nn.Linear(4096, len(class_to_idx))
</code></pre>
<p>How to do second part? Should i keep the <code>model2.classifier[6]</code> or define a new Sequential structure?</p>
","2024-01-02 13:22:34","0","Question"
"77746203","","How to change the value of an attribute of a torch_geometric.data Data object element?","<p>I am trying to change the value of an attribute of a torch_geometric.data Data object element like this:</p>
<pre><code>    a = train_data[0]  # Data(edge_index=[2, 267], y=[1], x=[33, 401], num_nodes=33)
    print(train_data[0].num_nodes) # 33
    train_data[0].num_nodes = 777
    print(train_data[0].num_nodes) # 33
</code></pre>
<p>It didn't work. <em>num_nodes</em> didn't change. <em>train_data</em> is a customized graph datasets composed of lots of subgraphs and <em>train_data[0]</em> is one of them.</p>
<p>Any idea on how to solve this problem? Thanks.</p>
<p>And if I want change <em>data[i].edge_index</em> and <em>data[i].x</em>, what should I do?</p>
","2024-01-02 12:52:41","0","Question"
"77744471","77743228","","<p>Use <code>torch.nn.Identity()</code> rather than <code>None</code>.</p>
<p>Or you can pass the tensor in <code>forward()</code> function.</p>
","2024-01-02 06:34:09","0","Answer"
"77743228","","Handling models with optional members (can be none) properly?","<p>I have a subclass of torch.nn.Module, whose initialiser have the following form:
(in class A)</p>
<pre><code>def __init__(self, additional_layer=False):
    ...
    if additional_layer:
        self.additional = nn.Sequential(nn.Linear(8,3)).to(self.device)
    else:
        self.additional = None
    ...
    ...
</code></pre>
<p>I train with additional_layer=True and save the model with <code>torch.save</code>. The object I save is <code>model.state_dict()</code>. Then I load the model for inference. But then I get the following error:</p>
<pre><code>model.load_state_dict(best_model[&quot;my_model&quot;])

RuntimeError: Error(s) in loading state_dict for A:
        Unexpected key(s) in state_dict: &quot;additional.0.weight&quot;
</code></pre>
<p>Is using an optional field which can be None disallowed?? How to handle this properly? [Also posted <a href=""https://www.reddit.com/r/pytorch/comments/18w4ipz/handling_models_with_optional_members_can_be_none/"" rel=""nofollow noreferrer"">here</a>]</p>
","2024-01-01 20:21:44","1","Question"