Post Id,Parent Id,Title,Body,CreationDate,Score,PostType
"79640555","78452864","","<p>You need to install this package: <code>pip install google-genai</code></p>
<p>Then from the same package do
<code>client = genai.Client(api_key='YOUR-GOOGLE-API-KEY')</code>.<br />
This solved the problem for me.</p>
","2025-05-27 13:08:01","0","Answer"
"79638588","78507718","","<p>Additional steps required here. You have to create a variable (in Flowise) with name apiKey of type runtime and in the Agents Configuration, in Security, enable Override Configuration and then enable the apiKey variable override.</p>
","2025-05-26 08:32:50","0","Answer"
"79548737","78442780","","<p>I have had the same error but with a different case.<br />
The original code below lead me to this error</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import Dataset
data = {
    &quot;question&quot;: &quot;Where is the Eiffel Tower?&quot;,
    &quot;answer&quot;: &quot;In Paris.&quot;,
    &quot;contexts&quot;: &quot;Some longer string&quot;,
    &quot;ground_truths&quot;: &quot;Another string&quot;
}

# Convert to Huggingface Dataset
dataset = Dataset.from_dict(data)
</code></pre>
<p>The solution was simply to wrap all strings into lists</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import Dataset
data = {
    &quot;question&quot;: [&quot;Where is the Eiffel Tower?&quot;],
    &quot;answer&quot;: [&quot;In Paris.&quot;],
    &quot;contexts&quot;: [&quot;Some longer string&quot;],
    &quot;ground_truths&quot;: [&quot;Another string&quot;]
}

# Convert to Huggingface Dataset
dataset = Dataset.from_dict(data)
</code></pre>
","2025-04-01 13:11:08","0","Answer"
"79479317","78287134","","<p>The OpenAI Assistants API can process CSV files effectively when the Code Interpreter tool is enabled.</p>
<p>Unlike the File Search tool, which does not support CSV files natively, Code Interpreter allows the assistant to parse and analyze CSV data.</p>
<p>When creating or updating your assistant programmatically, include the code_interpreter tool in the configuration.</p>
<p>Below is an example JSON setup:</p>
<pre><code>{
  &quot;name&quot;: &quot;CSV Analyzer&quot;,
  &quot;instructions&quot;: &quot;You are a data analyst who interprets CSV files to answer questions.&quot;,
  &quot;model&quot;: &quot;gpt-4o&quot;,
  &quot;tools&quot;: [&quot;code_interpreter&quot;]
}
</code></pre>
<p>After enabling Code Interpreter, upload your prepared CSV file to the assistant via the API or platform. The assistant can then access and analyze it.</p>
<blockquote>
<p>Note: When retrieving the thread via the API (e.g.,
client.beta.threads.messages.list), the Code Interpreter logic is
hidden</p>
</blockquote>
<p><strong>References</strong></p>
<p><a href=""https://code.recuweb.com/2025/processing-csv-files-with-openai-assistant-manager/"" rel=""nofollow noreferrer"">https://code.recuweb.com/2025/processing-csv-files-with-openai-assistant-manager/</a></p>
","2025-03-02 15:23:25","0","Answer"
"79476080","79116496","","<p>if you are using to the current version of mistralai(1.5.0), you can use it while keeping up the conversation like a chatbot without even appending the message history, <br>
If you are using Mistral Agents, there is a method called stream / async_stream.
<br>
You can use that, for more details about how to use that method, just go to its definition. <br></p>
<p>Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
<br>
You will have an agent_id to maintain the continuity of conversation just like a chat thread.
<br>
Meanwhile i am also trying to know how to use it, I will update this response after learning how to use it.</p>
<h2>Older streaming approach:</h2>
<pre class=""lang-py prettyprint-override""><code>from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

api_key = os.environ[&quot;MISTRAL_API_KEY&quot;]
model = &quot;mistral-large-latest&quot;

client = MistralClient(api_key=api_key)

messages = [
    ChatMessage(role=&quot;user&quot;, content=&quot;What is the best French cheese?&quot;)
]

# With streaming
stream_response = client.chat_stream(model=model, messages=messages)

for chunk in stream_response:
    print(chunk.choices[0].delta.content)
</code></pre>
<h2>New Streaming Approach:</h2>
<pre class=""lang-py prettyprint-override""><code>import os

from mistralai import Mistral, UserMessage

api_key = os.environ[&quot;MISTRAL_API_KEY&quot;]
model = &quot;mistral-large-latest&quot;

client = Mistral(api_key=api_key)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;What is the best French cheese?&quot;,
    },
]
# Or using the new message classes
# messages = [
#     UserMessage(content=&quot;What is the best French cheese?&quot;),
# ]

stream_response = client.chat.stream(
    model=model,
    messages=messages,
)

for chunk in stream_response:
    print(chunk.data.choices[0].delta.content)
</code></pre>
","2025-02-28 16:01:59","0","Answer"
"79458662","79238208","","<p>I'm not an expert in this topic, but if you didn't add a DecisionRequester to the GameObject that holds your script, that could be why. As far as I know, a missing DecisionRequester doesn't cause an error, but it prevents the agent from making decisions. I tried this in one of my projects and got similar results to yours—the agent doesn't move, and the logs don't get printed. After a short moment, i even got the same error in the console, as in the last screenshot</p>
","2025-02-21 21:35:30","0","Answer"
"79457555","78734964","","<p>I just came across same problem. I ended up with running smaller models (fewer params).</p>
<p>Because what happens is that the <code>VRAM</code> is being fully occupied by this large model and thus parts of the model are forcibly shifted to run on the <code>RAM</code> and on the <code>CPU</code>.</p>
","2025-02-21 13:39:50","0","Answer"
"79452760","78174132","","<p>The problem can be solved if the constants are in lower case: alice, bob, james instead of Alice, Bob, James.</p>
","2025-02-19 20:57:42","0","Answer"
"79413755","78702863","","<p>Yes, your reasoning is correct to me.</p>
<p>For an infinite disconnected graph, the basic searching algorithms like BFS, DFS and so no will never terminate as the nodes will keep on expanding leading to never meeting failure condition.</p>
<p>In order to make these algorithms trigger the failure condition, you need to add another condition that detects the cycling of search or if the depth or cost is above the specified one.</p>
<p>This modification may help the infinite graphs to halt.</p>
","2025-02-05 05:40:59","0","Answer"
"79395356","78858483","","<p>Recently I had the same problem in a code, I managed to fix it by configuring <code>dotenv</code> in the same file that is calling the variable.
The problem was that the <code>dotenv</code> was configured in the server file and the variable was being called in another file that was not having access to it, the variable was returning <code>undefined</code>.</p>
","2025-01-28 22:44:16","0","Answer"
"79387269","79104305","","<p>He buddy, I'm try to fine-tune llama3.2:1b, 3b and llama3.1:8b and I have that same max_seq_length 2048 for all models but after fine-tuning the model context lenght will not be limited to 2048, I think!. I'm having some great problems in fine-tuning.</p>
<p>Sequence length:
This is the specific length of the input you provide to the model, which can be shorter than the model's overall context length.</p>
<p>Context length:
This is the maximum amount of text (in tokens) that the model can process and reference when generating an output, representing its ability to remember and understand the broader context of a conversation or document.</p>
","2025-01-25 18:23:27","0","Answer"
"79368567","78953075","","<pre class=""lang-none prettyprint-override""><code>from autogen import AssistantAgent, UserProxyAgent
from autogen.coding import DockerCommandLineCodeExecutor

config_list=[
    {
        'model':'gpt-4o',
        'api_key':'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'
    }
]


llm_config ={
    
    
    &quot;config_list&quot;:config_list,
    # &quot;temperature&quot;:0,

}
</code></pre>
<pre><code>

# new code
# create an AssistantAgent instance named &quot;assistant&quot; with the LLM configuration.
assistant = AssistantAgent(name=&quot;assistant&quot;, llm_config=llm_config)

# create a UserProxyAgent instance named &quot;user_proxy&quot; with code execution on docker.
code_executor = DockerCommandLineCodeExecutor()
user_proxy = UserProxyAgent(name=&quot;user_proxy&quot;, code_execution_config={&quot;executor&quot;: code_executor})


user_proxy.initiate_chat(
    assistant,
    message=&quot;&quot;&quot;What date is today? Which big tech stock has the largest year-to-date gain this year? How much is the gain?&quot;&quot;&quot;,
)
</code></pre>
<p>Update version if someone wants to run a similar code</p>
","2025-01-19 07:28:59","0","Answer"
"79366505","79203990","","<p>I've also had a same issue, where I tried to pass a message after keyword system and it was not a single line. So I have completely encoded the prompt inside triple quotes(&quot;&quot;&quot; complete prompt &quot;&quot;&quot;)
Also I have started my prompt in a same line next to keyword System.</p>
<p>You can try adding the message like below.</p>
<pre><code>from: llama3.2:latest
license: open
system: &quot;&quot;&quot; You are AdelBot, a friendly chatbot designed by Adel. Your responses are concise and include emojis for clarity and fun! 😊✨ &quot;&quot;&quot;
message:
  - role: user
    content: Who are you?
  - role: assistant
    content: I'm AdelBot, here to assist you with clear and concise answers. 🚀
</code></pre>
<p>I have enclosed text in triple quotes or you can try having it in a single line as prompt was small.</p>
<p>I have used java library and faced same issue. This quotes resolved my issue. Hope it helps!</p>
","2025-01-18 03:04:31","1","Answer"
"79349491","79287870","","<p>I found a hacky way to specify index name. First, import the pymilvus config.</p>
<pre class=""lang-py prettyprint-override""><code>from pymilvus.settings import Config
</code></pre>
<p>and before creating the instance of <code>MilvusVectorStore</code>, set the index name to the pymilvus config so in my case it becomes,</p>
<pre class=""lang-py prettyprint-override""><code># set index name to the config
Config.IndexName = &quot;embedding_index&quot;

# create MilvusVectorStore instance
self._store = MilvusVectorStore(
    uri=self.config.uri,
    collection_name=collection_name,
    dim=self.config.embedding_dim,
    similarity_metric=&quot;IP&quot;,
    embedding_field=&quot;embedding&quot;,
    index_name=&quot;embedding_index&quot;,
    search_config={
        &quot;metric_type&quot;: &quot;IP&quot;,
        &quot;params&quot;: {&quot;nprobe&quot;: self.config.nprobe},
        &quot;index_name&quot;: &quot;embedding_index&quot;,
    },
    index_config={
        &quot;field_name&quot;: &quot;embedding&quot;,
        &quot;index_type&quot;: &quot;IVF_FLAT&quot;,
        &quot;metric_type&quot;: &quot;IP&quot;,
        &quot;params&quot;: {&quot;nlist&quot;: self.config.nlist},
        &quot;index_name&quot;: &quot;embedding_index&quot;,
    },
)
</code></pre>
","2025-01-12 07:19:42","0","Answer"
"79340992","78907608","","<p>'messages' have to be a list of <a href=""https://python.langchain.com/api_reference/core/messages.html"" rel=""nofollow noreferrer"">message</a> of type HumanMessage/AIMessage/SystemMessage/ToolMessage. You can prepend the context (assuming it's a string) as a SystemMessage to your existing message list and then invoke.</p>
<pre><code>llm.invoke([SystemMessage(content=context)] + existing_messages)
</code></pre>
","2025-01-08 23:26:17","1","Answer"
"79301169","77932443","","<p>You can use mistralai 0.1.8 or 0.4.2 version</p>
","2024-12-22 14:28:45","0","Answer"
"79299491","79299405","","<p>Conditional effects are all of the following form…</p>
<pre class=""lang-none prettyprint-override""><code>(when &lt;cond&gt; &lt;eff&gt;)
</code></pre>
<p>…where <code>&lt;cond&gt;</code> can be a fluent or conjunction of fluents or something like that, and <code>&lt;eff&gt;</code> is similarly a fluent or a conjunction of ones. When you execute an action in state <code>s</code> and it takes you to state <code>s’</code>, the way to interpret a conditional effect is: “when <code>&lt;cond&gt;</code> holds in state <code>s</code>, then <code>&lt;eff&gt;</code> should hold in state <code>s’</code>”.</p>
<p>There are some extra details (disjunctions and existentials are ok in a condition but not an effect, if you have multiple conditional effects fire, then add effects take precedence, etc), but that’s the general process. Important thing to keep in mind is that <code>&lt;cond&gt;</code> only refers to what might hold in state <code>s</code>, while <code>&lt;eff&gt;</code> is describing what’s going to change.</p>
","2024-12-21 13:35:31","2","Answer"
"79299405","","PDDL check for conditionals","<p>I am struggling to understand how PDDL works. With this question I am interested in knowing if the problem is about syntax, edge-cases or just me understanding PDDL wrong. Especially the following: When I check for a conditional with the <strong>when</strong> clause, does it alter the predicate? In the following example, when I check for <em>(light-on ?room)</em> does it switch the <em>light-on</em> predicate to a True value? If so how to then encode such effect?</p>
<p>This is the domain of the problem:</p>
<pre><code>(define (domain vampire)
    (:requirements :conditional-effects)
    (:predicates
        (light-on ?r)
        (slayer-is-alive)
        (slayer-is-in ?r)
        (vampire-is-alive)
        (vampire-is-in ?r)
        (fighting)
        ;
        ; static predicates
        (NEXT-ROOM ?r ?rn)
        (CONTAINS-GARLIC ?r)
    )

    (:action toggle-light
        :parameters (?anti-clockwise-neighbor ?room ?clockwise-neighbor)
        :precondition (and
            (NEXT-ROOM ?anti-clockwise-neighbor ?room)
            (NEXT-ROOM ?room ?clockwise-neighbor)
            (not (fighting))
        )
        :effect (and
            (when
                (and (not(light-on ?room)) (vampire-is-in ?room) (not(light-on ?clockwise-neighbor)))
                (and (light-on ?room) (not(vampire-is-in ?room)) (vampire-is-in ?clockwise-neighbor))
            )
            (when
                (and (not(light-on ?room)) (vampire-is-in ?room) (not(light-on ?anti-clockwise-neighbor)))
                (and (light-on ?room) (not(vampire-is-in ?room)) (vampire-is-in ?anti-clockwise-neighbor))
            )
            (when
                (and (not(light-on ?room)) (vampire-is-in ?room) (not(light-on ?anti-clockwise-neighbor)) (not(light-on ?clockwise-neighbor)) )
                (and (light-on ?room) (not(vampire-is-in ?room)) (vampire-is-in ?anti-clockwise-neighbor))
            )
            (when
                (and (not(light-on ?room)) (vampire-is-in ?room) (light-on ?anti-clockwise-neighbor) (light-on ?clockwise-neighbor))
                (and (light-on ?room) (not(vampire-is-in ?room)) (vampire-is-in ?clockwise-neighbor))
            )
            ;;
            ;;
            (when
                (and (light-on ?room) (slayer-is-in ?room) (light-on ?clockwise-neighbor))
                (and (not (light-on ?room)) (not(slayer-is-in ?room)) (slayer-is-in ?clockwise-neighbor))
            )
            (when
                (and (light-on ?room) (slayer-is-in ?room) (not(light-on ?clockwise-neighbor)))
                (and (not (light-on ?room)) (not(slayer-is-in ?room)) (slayer-is-in ?anti-clockwise-neighbor))
            )
        )
    )

    (:action watch-fight
        :parameters (?room)
        :precondition (and
            (slayer-is-in ?room)
            (slayer-is-alive)
            (vampire-is-in ?room)
            (vampire-is-alive)
            (fighting)
        )
        :effect (and
             (when
                (CONTAINS-GARLIC ?room)
                (not(vampire-is-alive))
             )
             (when
                (light-on ?room)
                (not(vampire-is-alive))
             )
             (when
                 (not(light-on ?room))
                 (not(slayer-is-alive))
             )
        )
    )
)
</code></pre>
<p>and this is a possible problem:</p>
<pre><code>(define (problem p01)
(:domain vampire)
;
; Room1 Room2 Room3 Room4 Room5
; 1S    0     1     0G    0V
;
(:objects
   room1
   room2
   room3
   room4
   room5
)
(:init
   (light-on room1)
   (light-on room3)

   (slayer-is-alive)
   (slayer-is-in room1)

   (vampire-is-alive)
   (vampire-is-in room5)

   (CONTAINS-GARLIC room4)

   (NEXT-ROOM room1 room2)
   (NEXT-ROOM room2 room3)
   (NEXT-ROOM room3 room4)
   (NEXT-ROOM room4 room5)
   (NEXT-ROOM room5 room1)
)
(:goal (and
   (slayer-is-alive)
   (not (vampire-is-alive))
   (slayer-is-in room3)
))
)
</code></pre>
","2024-12-21 12:27:45","1","Question"
"79287870","","How do i specify index_name in llama-index MilvusVectorStore","<p>I'm working on a python rag project. where I'm using milvus as vector database and using llama-index for setting up the project.
I have done setting up milvus database, wrote code for embedding the documents, in milvus collection i have created a schema and added 3 indexes among those 2 are scalar indexes and one is a vector index.</p>
<pre class=""lang-py prettyprint-override""><code>def _create_indexes(self, collection: Collection):
    &quot;&quot;&quot;Create indexes on the collection&quot;&quot;&quot;
    try:
        collection.drop_index()  # Clear any existing indexes
    except Exception as e:
        logger.warning(f&quot;No existing indexes to drop: {e}&quot;)

    collection.create_index(field_name=&quot;account_id&quot;, index_name=&quot;account_id_index&quot;)
    collection.create_index(field_name=&quot;file_id&quot;, index_name=&quot;file_id_index&quot;)

    index_params = {
        &quot;index_type&quot;: &quot;IVF_FLAT&quot;,
        &quot;metric_type&quot;: &quot;IP&quot;,
        &quot;params&quot;: {&quot;nlist&quot;: self.config.nlist},
    }
    collection.create_index(
        field_name=&quot;embedding&quot;,
        index_params=index_params,
        index_name=&quot;embedding_index&quot;,
    )
    collection.load()
</code></pre>
<p>Milvus is getting populated, all 3 indexes are also getting created.
After this, for doing the search i was trying to create an instance of <code>MilvusVectorStore</code>
in the following way.</p>
<pre class=""lang-py prettyprint-override""><code>self._store = MilvusVectorStore(
    uri=self.config.uri,
    collection_name=collection_name,
    dim=self.config.embedding_dim,
    similarity_metric=&quot;IP&quot;,
    embedding_field=&quot;embedding&quot;,
    index_name=&quot;embedding_index&quot;,
    search_config={
        &quot;metric_type&quot;: &quot;IP&quot;,
        &quot;params&quot;: {&quot;nprobe&quot;: self.config.nprobe},
        &quot;index_name&quot;: &quot;embedding_index&quot;,
    },
    index_config={
        &quot;field_name&quot;: &quot;embedding&quot;,
        &quot;index_type&quot;: &quot;IVF_FLAT&quot;,
        &quot;metric_type&quot;: &quot;IP&quot;,
        &quot;params&quot;: {&quot;nlist&quot;: self.config.nlist},
        &quot;index_name&quot;: &quot;embedding_index&quot;,
    },
)
</code></pre>
<p>but, since I have multiple indexes, I'm getting the following error</p>
<pre class=""lang-bash prettyprint-override""><code> pymilvus.decorators - ERROR - RPC error: [describe_index], &lt;AmbiguousIndexName: (code=1, message=There are multiple indexes, please specify the index_name.)&gt;, &lt;Time:{'RPC start': '2024-12-17 17:24:17.551945', 'RPC error': '2024-12-17 17:24:17.584774'}&gt;
</code></pre>
<p>It looks like I'm making some mistake in initializing the MilvusVectorStore instance, specifically the params <code>index_config</code> and <code>search_config</code>.</p>
<p>I don't know how should i specify the <code>index_name</code> during this process(that's what error says). can anybody help ?</p>
","2024-12-17 12:29:06","0","Question"
"79285682","79279472","","<p>Your implementation of cosine similarity is wrong. You can see this by inspecting the values of the cosine similarity matrix. Run the following:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F

bs = 8
d_proj = 64
z_i = torch.randn(bs, d_proj)
z_j = torch.randn(bs, d_proj)

z_i_norm = F.normalize(z_i, dim=1)
z_j_norm = F.normalize(z_j, dim=1)

cosine_num = torch.matmul(z_i, z_j.T)
cosine_denom = torch.matmul(z_i_norm, z_j_norm.T)
cosine_similarity = cosine_num / cosine_denom

print(cosine_similarity)
</code></pre>
<p>You will see the values in <code>cosine_similarity</code> are quite large (when it should be bounded between -1 and 1).</p>
<p>Below are two correct ways of computing pairwise cosine similarity:</p>
<pre class=""lang-py prettyprint-override""><code># F.cosine_similarity is preferred for performance
cosine_similarity = F.cosine_similarity(z_i[:,None], z_j[None,:], dim=2)

# alt version to show how cosine similarity is computed 
cosine_similarity = (z_i[:,None] * z_j[None,:]).sum(-1) / (torch.norm(z_i, dim=-1)*torch.norm(z_j, dim=-1))
</code></pre>
<p>You also have errors in your cross entropy implementation. For example you shouldn't zero the diagonal values of the denominator, and <code>denominator = torch.exp(torch.sum(cosine_similarity, dim=1))</code> should instead be <code>denominator = torch.exp(cosine_similarity / temperature).sum(dim=1)</code> (include temperature scaling, sum after exp rather than before).</p>
<p>Overall, you should use <code>F.cross_entropy</code> instead of manually computing the log-exp values - this is much more numerically stable.</p>
<pre class=""lang-py prettyprint-override""><code>cosine_similarity = F.cosine_similarity(z_i[:,None], z_j[None,:], dim=2)
labels = torch.arange(cosine_similarity.shape[0], device=cosine_similarity.device)
loss = F.cross_entropy(cosine_similarity/temperature, labels)
</code></pre>
","2024-12-16 18:14:42","2","Answer"
"79285355","79284559","","<p>You need to run environment</p>
<p>Snippet:</p>
<pre><code>import gym
import ppaquette_gym_doom #&lt;== Add module
env = gym.make('ppaquette/DoomBasic-v0')
</code></pre>
","2024-12-16 16:19:21","0","Answer"
"79284559","","No module named 'ppaquette_gym_doom'","<p>i use spyder</p>
<p>i tried to install ppaquette_gym_doom but failed
i tried to install doom-py and failed</p>
<p>Failed building wheel for doom-py</p>
<p>this is error secntence but i can't understand what it means</p>
<p>i also tried below</p>
<pre><code>import gym
import gym_pull
gym_pull.pull('github.com/ppaquette/gym-doom')        # Only required once, envs will be loaded with import gym_pull afterwards
env = gym.make('ppaquette/DoomBasic-v0')
</code></pre>
<p>but alse error happen
cannot import name 'load_results'</p>
<p>installing ppaquette_gym_doom and doom-py
upgrade wheel and pip</p>
<p>i tried to install ppaquette_gym_doom but failed
i tried to install doom-py and failed</p>
<p>Failed building wheel for doom-py</p>
<p>this is error secntence but i can't understand what it means</p>
<p>i also tried below</p>
<pre><code>import gym
import gym_pull
gym_pull.pull('github.com/ppaquette/gym-doom')        # Only required once, envs will be loaded with import gym_pull afterwards
env = gym.make('ppaquette/DoomBasic-v0')
</code></pre>
<p>but alse error happen
cannot import name 'load_results'</p>
<p>installing ppaquette_gym_doom and doom-py
upgrade wheel and pip</p>
","2024-12-16 11:45:22","0","Question"
"79279472","","Contrastive Loss from Scratch","<p>I am trying to implement/learn how to implement contrastive loss. Currently my gradients are exploding into infinity and I think I must have misimplemented something. I was wondering if someone could take a look at my loss function and tell if they see an error</p>
<pre><code>class ContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.5):
        super(ContrastiveLoss, self).__init__()
        self.temperature = temperature

    def forward(self, projections_1, projections_2):
        z_i = projections_1
        z_j = projections_2
        z_i_norm = F.normalize(z_i, dim=1)
        z_j_norm = F.normalize(z_j, dim=1)
        cosine_num = torch.matmul(z_i, z_j.T)
        cosine_denom = torch.matmul(z_i_norm, z_j_norm.T)
        cosine_similarity = cosine_num / cosine_denom

        numerator = torch.exp(torch.diag(cosine_similarity) / self.temperature)

        denominator = cosine_similarity
        diagonal_indices = torch.arange(denominator.size(0))
        denominator[diagonal_indices, diagonal_indices] = 0
        denominator = torch.exp(torch.sum(cosine_similarity, dim=1))
        loss = -torch.log(numerator / denominator).sum()
        return loss
</code></pre>
","2024-12-13 20:16:24","1","Question"
"79278280","78356777","","<h2>Answer you accepted is completely wrong!</h2>
<p><em><strong>Importance Matrix</strong></em> has nothing to do with I-Quant.</p>
<br>
<hr />
<h2>Importance Matrix is not I-Quant</h2>
<p>Important Matrix can be used to <strong>BOTH</strong> K-Quants (Q4_k_m, etc) and I-Quants (IQ4_XS, etc). It's completely different thing.</p>
<p>That, is why you see imatrix quants with K-Quants like one here:</p>
<p><a href=""https://i.sstatic.net/o5mjdxA4.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/o5mjdxA4.png"" alt=""bartowski's Llama 3.3 GGUF model card in huggingface"" /></a></p>
<p>Or <a href=""https://huggingface.co/spaces/ggml-org/gguf-my-repo"" rel=""noreferrer"">gguf-my-repo</a> which does the quantizations for you, has no difference in quant methods regardless of imatrix option.</p>
<p><a href=""https://i.sstatic.net/j1jEjNFd.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/j1jEjNFd.png"" alt=""Image of gguf-my-repo's ggml quantization method list"" /></a></p>
<br>
<hr />
<h2>So what is I-Quant then</h2>
<p>Quant methods(<code>n</code> here means bpw, <code>#</code> here means suffix):</p>
<ul>
<li><p>I-Quant: <code>IQn_#</code> (IQ4_XS, IQ3-XXS, IQ4_NL, etc)</p>
<ul>
<li>Introduced in <a href=""https://github.com/ggerganov/llama.cpp/pull/4773"" rel=""noreferrer"">llama.cpp #4773</a> influenced by <a href=""https://github.com/Cornell-RelaxML/quip-sharp"" rel=""noreferrer"">QuIP#</a></li>
<li>Acts like compressed file, using lookup table to save extra space, basically trading off speed vs memory. Which means if you are memory bound, you can expect better yet a bit slower model from I-Quanted ggufs.</li>
<li>Use this for extra memory saving with less quality degrade over K-Quants, at cost of decoding speed due to lookup table accesses and etc (not generation speed - effect is minimal while generation) under &lt;= 4 bits.</li>
</ul>
</li>
<li><p>Legacy: <code>Qn_0</code> / <code>Qn_1</code> (Q4_0, Q4_1, etc)</p>
<ul>
<li>Old, basic quantization method. Fastest to decode but also inefficient in space. Not recommended nowdays over K-Quants.</li>
</ul>
</li>
<li><p>K-Quant</p>
<ul>
<li>Introduced in <a href=""https://github.com/ggerganov/llama.cpp/pull/1684"" rel=""noreferrer"">llama.cpp #1684</a>: <code>Qn_K</code> / <code>Qn_K_#</code> (Q4_k, Q4_k_m, etc)</li>
<li>Better overhall than legacy quants, currently most widely used quant types. Use this whenever possible for &gt;=4 bpw.</li>
<li>Mixes which layers to use which quantizations intelligiently. Suffixes like M, XS, etc refers those AFAIK.
To quote from bartowski - Q4_K_M uses Q4_K for the embedding, attention K, attention Q, feed forward network gate and up, and the attention output while using Q6_K for the attention V and feed forward network down matrices. For detail refer <a href=""https://www.reddit.com/r/LocalLLaMA/comments/1h6ojwr/comment/m0igzye/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button"" rel=""noreferrer"">comment</a>.</li>
</ul>
</li>
</ul>
<p>I highly recommend reading extremely well written description in <a href=""https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF#download-a-file-not-the-whole-branch-from-below"" rel=""noreferrer"">bartowski's gguf repo</a> for per-quant difference like which embeding it uses etc.</p>
<br>
<hr />
<h2>How these compares to each others?</h2>
<p>Refer this image made by ikawrakow for relative comparsion between each others.</p>
<p><a href=""https://i.sstatic.net/fzn9Vy26.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/fzn9Vy26.png"" alt=""GGUF quant method comparsion chart"" /></a></p>
<ul>
<li><strong>PPL</strong>
<ul>
<li><strong>Simple</strong>: How much model is 'damaged' during quantization. Lower the better.</li>
<li><strong>Actual</strong>: Represents how well/confidently model predicts for given sequence. But since this rely heavily on training data it's not good for comparsion between different models, but useful for quantization damage check via relative ppl diff between quants like the graph.</li>
</ul>
</li>
<li><strong>bpw</strong>
<ul>
<li>average bit per weight, so lower is better, as it saves VRAM and allow you to put more layers or context on gpu.</li>
</ul>
</li>
</ul>
<p>So it's a delicate balance between model degration vs size, just like how you encode your video.</p>
<p>You either:</p>
<ul>
<li>choose a better method (h264 -&gt; hevc, K-quant -&gt; I-quant)</li>
<li>choose quality over size (CRF 24 -&gt; 20, Q4 -&gt; Q5, IQ3 -&gt; IQ4)</li>
<li>choose size over quality (vise versa)</li>
</ul>
<br>
<hr />
<h2>There's too many! What quant should I use!?</h2>
<p>Generally many use Q4_K_M (IQ4_XS if low vram) and you'll do about 99% fine with it but here's how things generally works:</p>
<p>Imagine you're resizing a picture. Resize 4k img to 1/4 and it's still a good looking picture; Resize 480p to 1/4 and you basically can't see a thing.</p>
<p>Informations in smaller models' weights are extremely densely packed, that losing a bit lead to severe damage. On the other hand, Large models works decently under extreme quantization as weights are more sparsely distributed.</p>
<p>Hence, it's sometimes more viable to use smaller quant of large model than smaller model's higher quants. (i.e. IQ3_XS Qwen 2.5 32B over Q6_K_L Qwen 2.5 14B)</p>
<p>How viable this is differs heavily per model so you should try both and make decision yourself. This way, when you need to save extra memory to fit more context into vram for faster speed, you know how to find the model that fits your needs.</p>
<br>
<hr />
<h2>...So what was Imatrix?</h2>
<p>Imatrix &amp; IQ quants are commonly confused by so many people (so was I), that normal search of what those are will commonly put you into misconception like the answer you accepted.</p>
<p>Refer following <a href=""https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/"" rel=""noreferrer"">reddit post that did the research for us</a> for more details on each. To quote Imatrix part:</p>
<blockquote>
<p><strong>Importance matrix</strong></p>
<p>Somewhat confusingly introduced around the same as the i-quants, which made me think that they are related and the &quot;i&quot; refers to the &quot;imatrix&quot;. <em>But this is apparently not the case, and you can make both legacy and k-quants that use imatrix, and i-quants that do not</em>. All the imatrix does is telling the quantization method which weights are more important, so that it can pick the per-block constants in a way that prioritizes minimizing error of the important weights. The only reason why i-quants and imatrix appeared at the same time was likely that the first presented i-quant was a 2-bit one – without the importance matrix, such a low bpw quant would be simply unusable.</p>
<p>Note that this means you can't easily tell whether a model was quantized with the help of importance matrix just from the name.
...</p>
</blockquote>
","2024-12-13 12:11:41","12","Answer"
"79264170","79256501","","<p>Below is the code to send a request to a custom PromptFlow endpoint hosted in Azure, allowing interaction with an AI model deployment.</p>
<pre class=""lang-cs prettyprint-override""><code> var chatHistory = new[]
        {
            new { role = &quot;system&quot;, content = &quot;You are a helpful customer support chatbot.&quot; },
            new { role = &quot;user&quot;, content = &quot;Hi, I need help with my subscription.&quot; },
            new { role = &quot;assistant&quot;, content = &quot;Sure, can you provide more details about the issue?&quot; }
        };

        var prompt = &quot;What are the steps to cancel my subscription?&quot;;
        var requestBody = new
        {
            chat_history = chatHistory,
            question = prompt
        };

        string jsonRequestBody = JsonConvert.SerializeObject(requestBody);

        try
        {
            var response = await CallPromptFlowEndpoint(jsonRequestBody);
            Console.WriteLine(&quot;API Response:&quot;);
            Console.WriteLine(response);
        }
        catch (Exception ex)
        {
            Console.WriteLine($&quot;Error: {ex.Message}&quot;);
        }
    }

    private static async Task&lt;string&gt; CallPromptFlowEndpoint(string jsonRequestBody)
    {
        using (HttpClient client = new HttpClient())
        {
client.DefaultRequestHeaders.Add(&quot;Authorization&quot;, $&quot;Bearer {ApiKey}&quot;);
            client.DefaultRequestHeaders.Add(&quot;azureml-model-deployment&quot;, ModelDeploymentName);
            var response = await client.PostAsync(
                EndpointUrl,
                new StringContent(jsonRequestBody, Encoding.UTF8, &quot;application/json&quot;)
            );

            if (response.IsSuccessStatusCode)
            {
                return await response.Content.ReadAsStringAsync();
            }
            else
            {
                string errorMessage = await response.Content.ReadAsStringAsync();
                throw new Exception($&quot;Error: {response.StatusCode}, Details: {errorMessage}&quot;);
            }
        }
    }
}
</code></pre>
<p><img src=""https://i.imgur.com/OsHCSSu.png"" alt=""Output "" /></p>
<p>I referred to the Azure OpenAI Service REST API <a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/reference"" rel=""nofollow noreferrer"">documentation</a> for this implementation and the How-to Guide on <a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/reference"" rel=""nofollow noreferrer"">Creating</a> and Managing Compute Sessions in <a href=""https://learn.microsoft.com/en-us/azure/ai-studio/how-to/create-manage-compute-session"" rel=""nofollow noreferrer"">Azure</a>.</p>
<blockquote>
<p>Additionally, I referred to the How to Manage Compute Sessions for <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-manage-compute-session?view=azureml-api-2&amp;tabs=cli"" rel=""nofollow noreferrer"">PromptFlow</a> and How to Customize Session Base Image for <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-customize-session-base-image?view=azureml-api-2"" rel=""nofollow noreferrer"">PromptFlow</a>.</p>
</blockquote>
<p>Below is the code to create images given a prompt.</p>
<pre class=""lang-none prettyprint-override""><code>POST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?api-version=2024-10-21

{
 &quot;prompt&quot;: &quot;In the style of WordArt, Microsoft Clippy wearing a cowboy hat.&quot;,
 &quot;n&quot;: 1,
 &quot;style&quot;: &quot;natural&quot;,
 &quot;quality&quot;: &quot;standard&quot;
}

</code></pre>
<p><img src=""https://i.imgur.com/MhTyzZN.png"" alt=""Output of response"" /></p>
","2024-12-09 07:38:14","0","Answer"
"79256501","","Azure AI Foundry deployed promptflow providing different responses in compute session chat and endpoint API call","<p>I'm trying to make a customer support chatbot using Azure AI Studio/Foundry. I created a promptflow using the chatplayground and deployed it. When testing out the model in the chat function of the promptflow compute session, the response to a query is as expected, with the model responding in the expected format with relevant indexed data. However, when I call the endpoint of the deployed promptflow and send it the same query, the response is is not as expected and is in the correct format but does not provided relevant indexed data as part of its response and instead asks the user for more details regarding their inquiry.</p>
<p>I tested it multiple times with the same query as well as others and the responses are different to varying degrees every time.</p>
<p>Here's C# code I used to call the deployed endpoint of the promptflow:</p>
<pre><code>using (HttpClient client = new HttpClient())
{


    client.DefaultRequestHeaders.Add(&quot;Authorization&quot;, $&quot;Bearer {apiKey}&quot;);
    client.DefaultRequestHeaders.Add(&quot;azureml-model-deployment&quot;, modelDeploymentName);



    var requestBody = new
    {
        chat_history = chatHistory,
        question = prompt
    };

    string jsonRequestBody = Newtonsoft.Json.JsonConvert.SerializeObject(requestBody);

    var response = await client.PostAsync(
        endpointUrl,
        new StringContent(jsonRequestBody, Encoding.UTF8, &quot;application/json&quot;)
    );

    if (response.IsSuccessStatusCode)
    {

        return await response.Content.ReadAsStringAsync();
    }
    else
    {

        string errorMessage = await response.Content.ReadAsStringAsync();
        throw new Exception($&quot;Error: {response.StatusCode}, Details: {errorMessage}&quot;);
    }
}
</code></pre>
<p>I'm fairly sure that I'm calling the API correctly since it provides a response, just not the expected one. Would anyone have any idea what might be causing the disparity in the promptflows responses to the same query between calling its endpoint and chatting with it in the compute session directly?</p>
","2024-12-05 23:32:13","0","Question"
"79252426","79123038","","<p>Try running the code in kaggle</p>
","2024-12-04 18:57:48","0","Answer"
"79241022","78794834","","<p>I don't know, I'm still trying to figure out skills myself.
I don't know if you're using a container, but some of the things I've had to do are
1.) tell my agent to run/execute the code locally (or in docker I guess)
2.) you need to manage your return/summary message.  There's an options for last, LLM, or none (or something like that).  Give your agent specific instructions to summarize or return the value in the final response, and make sure you've picked last in the agent setup</p>
<p>try looking at the default setup, if you don't have that, or you have a lot of noise run autogenstudio ui --appdir = C:\temp\autogen or any other folder
3.) this doesn't look to be relevant since it's running the code, but tell your agent about the skill.  &quot;Use the add_two_numbers skill to solve the problems...&quot;</p>
<p>Your output shows that the system knows the answer is 6</p>
<p>My other suggestions would be to remove the print and input commands, I'm sure you added them for debugging purposes, but... well that's what I got.</p>
","2024-12-01 06:29:27","0","Answer"
"79240225","78378753","","<p>Alternatively, try <code>Python 3.12.1</code> and <code>tensorflow 2.18.0</code></p>
","2024-11-30 18:08:17","0","Answer"
"79239231","79237941","","<p>Your <code>prompt</code> variable in the if condition:</p>
<pre class=""lang-py prettyprint-override""><code>if prompt:=st.chat_input(placeholder=&quot;What is machine learning?&quot;):
</code></pre>
<p>is conflicting with the prompt you have pulled from the hub:</p>
<pre class=""lang-py prettyprint-override""><code>prompt = hub.pull(...)
</code></pre>
<p>Please fix that by changing the name of either of them.</p>
<p>The error clearly says that you are sending the prompt with the wrong datatype to <code>agent = create_react_agent(model, tools, prompt)</code>.</p>
<p>Try this:</p>
<pre class=""lang-py prettyprint-override""><code>.
.
.
prompt_from_hub = hub.pull(...)
.
.
.
agent = create_react_agent(model, tools, prompt_from_hub)
.
.
.
</code></pre>
<p>Also, before this please confirm once that the <code>prompt_from_hub</code> is not a string.</p>
","2024-11-30 09:37:39","1","Answer"
"79239144","79237584","","<p>Try to add n_ctx to 2048 in Llama constructor, so:</p>
<pre><code>Llama(n_ctx=2048, model_path=model_path)
</code></pre>
<p>This parameters tells model what is the maximum length of the prompt and response combined.</p>
","2024-11-30 08:53:36","0","Answer"
"79238208","","Unity mlAgents not connecting to Unity","<p>I followed a tutorial on how to set up mlagents in unity and that all mostly went smoothly. I got a simple scene set up like this:</p>
<p><a href=""https://i.sstatic.net/TpGx3WhJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TpGx3WhJ.png"" alt=""enter image description here"" /></a></p>
<p>With 3 Walls that reset the Episode and give a negative reward and one &quot;finishline&quot; that rewards the AI when hit. I have a script for the carmovement which takes a input vector2d for steering and driving/reversing. I have created a script for the agent that gives the ai the transform of the car and the finishline</p>
<pre><code> public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(transform.position);
        sensor.AddObservation(targetTransform.position);
    }
</code></pre>
<p>and takes its output and moves the car with it</p>
<pre><code> public override void OnActionReceived(ActionBuffers actions)
    {
        stepCounter++;
        Debug.Log(&quot;Step: &quot; + stepCounter);

        float moveX = actions.ContinuousActions[0];
        float moveY = actions.ContinuousActions[1];

        float distanceToGoal = Vector2.Distance(transform.position, targetTransform.position);
        Debug.Log(&quot;Distance to Goal: &quot; + distanceToGoal);

        SetReward(-0.01f);

        if (distanceToGoal &lt; goalThreshold)
        {
            SetReward(1.0f);
            EndEpisode();
            Debug.Log(&quot;Goal Reached!&quot;);
        }

        Debug.Log(&quot;Action Received: MoveX: &quot; + moveX + &quot;, MoveY: &quot; + moveY);
        carController.SetInputVector(new Vector2(moveX, moveY));
    }
</code></pre>
<p>I added logs all over the place.
When I run the <code>mlagents-learn</code> command in cmd it tells me to start the game in unity, when I do that nothing happens in unity and the console prints some info about the trainer confi etc. The unity console logs that 1 time a episode was started but noting else, the Ai didnt give any output etc. after 1-2 minutes in cmd it says <code>[WARNING] Restarting worker[0] after 'The Unity environment took too long to respond.</code> My behaviour looks like this:</p>
<p><a href=""https://i.sstatic.net/ecpWuUvI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ecpWuUvI.png"" alt=""enter image description here"" /></a></p>
<p>with 2 continuous actions for the steering and driving/reversing and a space size of 6 for the 2 vec3 transforms for the car and the goal.</p>
<p>I have these versions:</p>
<ul>
<li>mlagents (unity plugin) 2.0.1</li>
<li>mlagents (python) 0.30.0</li>
<li>python 3.9.0</li>
<li>pytorch 1.7.1</li>
</ul>
<p>At first I had some problems with numpy and soon but I could fix that by using compatible versions</p>
<p>As I said before it doesn't print anything in the console aside form starting the episode once, no ai inputs/outputs, no errors, nothing</p>
<p>I tried adding a maxstep number and that led to unity restarting the episode every 20ms but I don't think that information helps.
I run the mlagents command from a virtual environment and here is what the console say in detail.
<a href=""https://i.sstatic.net/oJMjct6A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oJMjct6A.png"" alt=""enter image description here"" /></a></p>
","2024-11-29 19:46:31","0","Question"
"79237941","","Why am I not able to create the react agent using create_react_agent()?","<p>What am I doing wrong here ?</p>
<pre><code>import streamlit as st 
from langchain_groq import ChatGroq 
from langchain_community.tools import DuckDuckGoSearchRun 
from langchain.agents import AgentExecutor, create_react_agent 
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler 
from langchain import hub 
import os  
from dotenv import load_dotenv 
load_dotenv() 

os.environ['GROQ_API_KEY'] =  os.getenv(&quot;GROQ_API_KEY&quot;) 
 
search=DuckDuckGoSearchRun(name=&quot;Search&quot;) 
prompt=hub.pull(&quot;hwchase17/react&quot;) 
 
 
st.title(&quot;🔎 LangChain - Chat with search&quot;) 

 
if &quot;messages&quot; not in st.session_state: 
    st.session_state[&quot;messages&quot;]=[ 
        {&quot;role&quot;:&quot;assisstant&quot;,&quot;content&quot;:&quot;Hi,I'm a chatbot who can search the web. How can I help you?&quot;} 
    ] 
 
for msg in st.session_state.messages: 
    st.chat_message(msg[&quot;role&quot;]).write(msg['content']) 
 
if prompt:=st.chat_input(placeholder=&quot;What is machine learning?&quot;): 
    st.session_state.messages.append({&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:prompt}) 
    st.chat_message(&quot;user&quot;).write(prompt) 
 
    model=ChatGroq(model_name=&quot;Llama3-8b-8192&quot;,streaming=True) 
    tools=[search] 
    agent = create_react_agent(model, tools, prompt) 
 
    search_agent = AgentExecutor(agent=agent, tools=tools) 
 
    with st.chat_message(&quot;assistant&quot;): 
        st_cb=StreamlitCallbackHandler(st.container(),expand_new_thoughts=False) 
        response=search_agent.invoke(st.session_state.messages,callbacks=[st_cb]) 
        st.session_state.messages.append({'role':'assistant',&quot;content&quot;:response}) 
        st.write(response) 
</code></pre>
<p>The error is</p>
<pre class=""lang-none prettyprint-override""><code>AttributeError: 'str' object has no attribute 'input_variables'
File &quot;/path/to/langchain/agents/react/agent.py&quot;, line 114, in create_react_agent

prompt.input_variables + list(prompt.partial_variables)
</code></pre>
<p>I was trying to create a simple Agent made through <code>create_react_agent</code></p>
","2024-11-29 17:39:54","2","Question"
"79237584","","Issue with Llama 2-7B Model Producing Output Limited to 511 Tokens","<p>I am facing an issue with the Llama 2-7B model where the output is consistently limited to only 511 tokens, even though the model should theoretically be capable of producing outputs up to a maximum of 4096 tokens.</p>
<p>I’ve tried setting the max_tokens parameter to higher values, such as 3000, and have calculated the available tokens by subtracting the prompt tokens from the model’s total token limit (4096 tokens). However, despite these adjustments, I continue to receive outputs capped at 511 tokens.</p>
<p>Here’s a snippet of the code I am using to interact with the model:</p>
<pre><code>import psutil
import os
import warnings
from llama_cpp import Llama

# Suppress warnings
warnings.filterwarnings(&quot;ignore&quot;)

# Path to the model
model_path = &quot;C:/Llama_project/models/llama-2-7b-chat.Q2_K.gguf&quot;

# Load the model
llm = Llama(model_path=model_path)

# System message to set the behavior of the assistant
system_message = &quot;You are a helpful assistant.&quot;

# Function to ask questions
def ask_question(question):
    # Use user input for the question prompt
    prompt = f&quot;Answer the following question: {question}&quot;

    # Calculate the remaining tokens for output based on the model's 4096 token limit
    prompt_tokens = len(prompt.split())  # Rough token count estimate
    max_output_tokens = 4096 - prompt_tokens  # Tokens left for output
    
    # Monitor memory usage before calling the model
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 ** 2  # Memory in MB

    # Get the output from the model with the calculated max tokens for output
    output = llm(prompt=prompt, max_tokens=max_output_tokens, temperature=0.7, top_p=1.0)

    # Monitor memory usage after calling the model
    mem_after = process.memory_info().rss / 1024 ** 2  # Memory in MB
    
    # Clean the output and return only the answer text
    return output[&quot;choices&quot;][0][&quot;text&quot;].strip()

# Main loop for user interaction
while True:
    user_input = input(&quot;Ask a question (or type 'exit' to quit): &quot;)
    
    if user_input.lower() == 'exit':
        print(&quot;Exiting the program.&quot;)
        break
    
    # Get the model's response
    answer = ask_question(user_input)
    
    # Print only the answer
    print(f&quot;Answer: {answer}&quot;)
</code></pre>
<p>Problem Details:</p>
<ul>
<li>Model: Llama 2-7B (Q2_K version)</li>
<li>Expected Output: I was expecting a response close to the maximum token limit (3000 or more tokens).</li>
<li>Actual Output: The output is capped at 511 tokens, regardless of the prompt length.</li>
</ul>
<p>Tried:</p>
<ul>
<li>Setting max_tokens to 3000 or higher.</li>
<li>Calculating the available tokens by subtracting the prompt length from the model’s total token limit.</li>
</ul>
<p>I would expect the model to generate responses that are close to the token limit (ideally closer to 3000 tokens or more, depending on the input), but it keeps producing output limited to 511 tokens.</p>
","2024-11-29 15:21:42","0","Question"
"79235430","78378753","","<p>I found it much easier to use the <code>.h5</code> format:
<code>model.save(path/to/model.h5)</code></p>
<p><code>tf.keras.models.load_model(path/to/model.h5)</code> works with no issues.</p>
<p>I don't know if you have the model in memory somewhere, but if you do, save it to <code>.h5</code> instead of <code>.keras</code></p>
","2024-11-28 22:26:41","0","Answer"
"79231830","78681837","","<p>Interesting question - I find GOAP is often either not discussed at all, or only at surface level.</p>
<h2>The Problem with OG-GOAP Statevars</h2>
<p>Jeff Orkin's original GOAP used <strong>binary state variables</strong>.</p>
<p>I can only guess why - it was mid-Noughties and developed for a real-time shooter AI, so performance was far more important than power, most likely. If you use binary vars, you can compress the whole state into a single bitflag integer and do bitwise ops super-fast.</p>
<p>In that setup, as you've noticed, 'stacking' Actions for increased effect is not straightforward (<em>you can sort of get around the limitation using horrible, horrible hacks like cloning Actions by intensity, but... it's not a good time</em>).</p>
<p>You're asking about regressive search, but it really doesn't matter - forward search has the same issues.</p>
<h2>Building a better GOAP</h2>
<p>However... <em><strong>it doesn't have to be that way</strong></em>! Nothing about GOAP <em>requires</em> an implementation to use bitflags.</p>
<p>What you do need is a <em>data structure</em> that provides <strong>key-value</strong> mappings and relatively cheap <strong>random-access reads/writes</strong>.</p>
<p>The lazy option here is obviously a HashMap, but if you know all your keys <em>upfront</em> you could just as well use structs or arrays (by mapping human-friendly keys to fixed array indices).</p>
<p>It's not going to be as fast as bitwise ops and we have to define a state arithmetic (although usually as simple as 'add values with the same key'), but now we're cooking with a whole <em>range</em> of numbers!</p>
<p>For instance, we can now set an AI goal to have <code>MONEY=250</code> and give it one action, <code>Work&lt;Pre:(), Eff:(MONEY=+50,)&gt;</code> and it will correctly figure out the following plan: <code>Work-&gt;Work-&gt;Work-&gt;Work-&gt;Work</code>.</p>
<h2>Regressing with style</h2>
<p>For regressive search, this is particularly nice - we can define our <code>SUCCESS</code> state as 'no negative-valued statevars'.</p>
<ol>
<li>We start from a <strong>blank state</strong>,</li>
<li>We <strong>add</strong> all initial statevars and <em><strong>subtract</strong></em> all Goal statevars from it,</li>
<li>We <strong>search</strong> for any Action whose Effects <strong>increase</strong> the negative-valued statevars,</li>
<li>We <strong>predict</strong> the new State (by adding Effects to the current state then subtracting their Preconditions)</li>
<li>We <strong>queue</strong> the current candidate Plan, its predicted State, and its cost in a PriorityQueue.</li>
<li>Once we reach a point where the State no longer has negatives (or has all zeros, if you want an exact solution), <strong>we found the best Plan that satisfies the Goal</strong>!</li>
</ol>
<h2>My assorted GOAP discoveries</h2>
<ul>
<li><p>I've resisted backwards-search supremacy for GOAP for some time for various semi-sensible reasons, but Jeff Orkin had it right the first time - the regressive search scheme in the previous section is about 100x-1000x faster on complex plans and still supports handling cycles.</p>
</li>
<li><p>The plan quality is also superior with regressive planning - a forward planner often winds up with entirely superfluous pointless steps if the actionspace is broad enough. Regressive search can filter out freeloaders that don't contribute to any unsettled statevar.</p>
</li>
<li><p>For... some reason, adding max queue size (i.e. turning plain AStar into a Beam Search) with a fairly low limit (depends on the actionspace, but 50 seemed like a sweet spot, but it scales with the complexity of the problem) works absolute <em>wonders</em> for regressive search. We're talking a 10x speedup when tuned well compared to wide/unlimited beam.</p>
</li>
</ul>
","2024-11-27 20:41:26","0","Answer"
"79228250","79221167","","<p>I had the same issue. You need to add a prompt in the processor:</p>
<pre><code>prompt = &quot; &quot;

inputs = processor(images=image, text=prompt, return_tensors=&quot;pt&quot;).to(device=&quot;cuda&quot;, dtype=torch.float16)
</code></pre>
<p>Hope it helps.</p>
","2024-11-26 20:56:16","1","Answer"
"79228229","79212687","","<p>If you use <code>shuffle=True</code> it should reshuffle the data at each epoch (<a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer"">from PyTorch documentation</a>). This means that at each epoch, the batches of data will be arranged differently with respect to the previous one.</p>
<p>During testing, since during testing the model doesn't change and you don't usually test for multiple epochs, it shouldn't make any difference.</p>
<p>However, setting <code>shuffle=False</code>, ensures consistency in the order of predictions, and it may make debugging easier.</p>
","2024-11-26 20:49:40","0","Answer"
"79226649","79145652","","<p>I had the same error and how it worked to me:</p>
<pre><code>with open('imagePath', &quot;rb&quot;) as f:
        image = f.read()
        image_base64 = base64.b64encode(image).decode('utf-8') 

    bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')

    kwargs = {
        &quot;modelId&quot;: &quot;anthropic.claude-3-5-sonnet-20240620-v1:0&quot;,
        &quot;contentType&quot;: &quot;application/json&quot;,
        &quot;accept&quot;: &quot;application/json&quot;,
        &quot;body&quot;: json.dumps({
            &quot;anthropic_version&quot;: &quot;bedrock-2023-05-31&quot;,
            &quot;max_tokens&quot;: 10000,
            &quot;messages&quot;: [
                {
                    &quot;role&quot;: &quot;user&quot;,
                    &quot;content&quot;: [
                        {
                            &quot;type&quot;: &quot;image&quot;,
                            &quot;source&quot;: {
                                &quot;type&quot;: &quot;base64&quot;,
                                &quot;media_type&quot;: &quot;image/jpeg&quot;,
                                &quot;data&quot;: image_base64
                            }
                        },
                        {
                            &quot;type&quot;: &quot;text&quot;,
                            &quot;text&quot;: &quot;What's in these images?&quot;
                        }
                    ]
                }
            ]
        })
    }

    response = bedrock_runtime.invoke_model(**kwargs)

    body = json.loads(response['body'].read())
</code></pre>
","2024-11-26 12:05:08","1","Answer"
"79222341","79221167","","<p>I'm pretty sure it's a model issue. I've been having issues with <a href=""https://huggingface.co/Salesforce/blip2-flan-t5-xl"" rel=""nofollow noreferrer"">https://huggingface.co/Salesforce/blip2-flan-t5-xl</a> since their update on Thursday, both on an older and the newest transformers version. The screenshot of a past model version (gets saved automatically, on <code>.cache/huggingface/hub</code> on a Mac) worked.</p>
","2024-11-25 09:06:57","2","Answer"
"79221167","","blip2 type mismatch exception","<p>I'm trying to create an image captioning model using hugging face blip2 model on colab. My code was working fine till last week (Nov 8) but it gives me an exception now.</p>
<p>To install packages I use the following command:</p>
<pre><code>!pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets
</code></pre>
<p>To load blip2 processor and model I use the following code:</p>
<pre><code>model_name = &quot;Salesforce/blip2-opt-2.7b&quot;
processor = AutoProcessor.from_pretrained(model_name)
model = Blip2ForConditionalGeneration.from_pretrained(model_name,device_map=&quot;auto&quot;,load_in_8bit=False)
</code></pre>
<p>I use the following code to generate captions:</p>
<pre><code>def generate_caption(processor, model, image_path):
  image = PILImage.open(image_path).convert(&quot;RGB&quot;)
  print(&quot;image shape:&quot; + image.size)

  device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

  # Preprocess the image
  inputs = processor(images=image, return_tensors=&quot;pt&quot;).to(device)

  print(&quot;Input shape:&quot;, inputs['pixel_values'].shape)

  print(&quot;Device:&quot;, device) # Additional debugging

  for key, value in inputs.items():
    print(f&quot;Key: {key}, Shape: {value.shape}&quot;)

  # Generate caption
  with torch.no_grad():
      generated_ids = model.generate(**inputs)
      caption = processor.decode(generated_ids[0], skip_special_tokens=True)

  return caption
</code></pre>
<p>here is the code that uses this method to generate captions:</p>
<pre><code>  image_path = &quot;my_image_path.jpg&quot;
  caption = generate_caption(processor, model, image_path)
  print(f&quot;{image_path}: {caption}&quot;
</code></pre>
<p>finally, this is the outputs and errors of running the code above:</p>
<pre><code>image shape:  (320, 240)
Input shape: torch.Size([1, 3, 224, 224])
Device: cuda
Key: pixel_values, Shape: torch.Size([1, 3, 224, 224])
---------------------------------------------------------------------------   
.
.
.
/usr/local/lib/python3.10/dist-packages/transformers/models/blip_2/modeling_blip_2.py in generate(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)
   2314         if getattr(self.config, &quot;image_token_index&quot;, None) is not None:
   2315             special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)
-&gt; 2316             inputs_embeds[special_image_mask] = language_model_inputs.flatten()
   2317         else:
   2318             logger.warning_once(

RuntimeError: shape mismatch: value tensor of shape [81920] cannot be broadcast to indexing result of shape [0]
</code></pre>
<p>I have searched the internet and used various AI models for help but to no avail. My guess is that this is a package update problem since my code had no problem last week. (I tried to restore my code to Nov 8 version but it throws an exception.) Moreover, I don't understand how 81920 is calculated in the error message.</p>
","2024-11-24 22:05:23","2","Question"
"79212687","","in testing dataset using dataloader , should we set shuffle=true or it doesn't matter?","<p>I have a custom dataset (images of pizza,sushi and steak).
I'm using torch DataLoader for it , now when writing the test dataloader custom should we set shuffle=true or it just doesn't matter??</p>
<p>I haven't seen difference yet , but just asking the  general.</p>
","2024-11-21 19:33:30","-3","Question"
"79208623","79165226","","<p>I was encountering this error since my ollama version did not support tool calling.</p>
<p>After upgrading ollama server version (to 0.4.2) now the role &quot;tool&quot; is available.</p>
","2024-11-20 19:06:44","0","Answer"
"79206666","78473568","","<p>AttributeError: type object 'Pinecone' has no attribute 'from_existing_index'.</p>
<p>This happenes when there are two Pinecones that you installed when importing  from langchain.vectorstore and also pinecone itself. so I the method so called 'from_existing_index' only exist in Pinecone from langchain. so when import from langchain ... use this:: 'from langchain.vectorstores import Pinecode as Pi'  and then use Pi when you are trying to access the attribute  'from_existing_index'</p>
<p>I hope you understood it.</p>
","2024-11-20 09:56:04","0","Answer"
"79206265","78316397","","<pre><code>Check if below code helps.. In my case, I tried extracting Vehicle Number and Date from a pdf file and create an excel output. I used Gemini. But logic should be same for other tools like ChatGPT etc too(But not sure!):
__________
__________
import google.generativeai as genai
from PyPDF2 import PdfReader
import json
import pandas as pd
import re

def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as pdf_file:
        pdf_reader = PdfReader(pdf_file)
        text = &quot;&quot;

        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num] 
            text += page.extract_text()

        return text 
    
def create_excel_file(vehicle_number, date, filename):
    if vehicle_number and date:
        data = {'Vehicle Number': [vehicle_number], 'Date': [date]}
        df = pd.DataFrame(data)
        df.to_excel(filename, index=False)
    else:
        print(&quot;No data to write to Excel.&quot;)

def get_gemini_response(prompt):
  model = genai.GenerativeModel(&quot;gemini-1.5-pro-latest&quot;, generation_config={&quot;response_mime_type&quot;: &quot;application/json&quot;})
  response = model.generate_content(prompt)
  try:
        response = model.generate_content(prompt)
        response_json = json.loads(response.text)
        if response_json:  # Check if response_json is not empty
            vehicle_number = response_json.get('vehicle_number', None)
            date = response_json.get('date', None)
            return vehicle_number, date
        else:
            print(&quot;Error: Empty response from Gemini&quot;)
            return None, None
  except Exception as e:
            print(f&quot;Error fetching Gemini response: {e}&quot;)
  return None, None

if __name__ == &quot;__main__&quot;:
  # Replace with your API key
  genai.configure(api_key=&quot;ACTUAL API KEY&quot;)

  pdf_path = 'D:\\XXX\\ExtractVHNumber.pdf'     #Provide path of the pdf file
  filename = 'D:\\XXX\\output.xlsx'     #Provide path of the excel file where output is require

  #text = &quot;The Vehicle that went has a Vehicle Number: AS01A1234 and it went on Date: 01/01/2024&quot; ...... Content of the test pdf file..

  text = extract_text_from_pdf(pdf_path)
  #query = &quot;What is the capital of France?&quot;
  query = &quot;Extract the Vehicle Number and Date from the following text:\n&quot; + text + &quot;\nPlease provide the output in JSON format with keys 'vehicle_number' and 'date'.&quot;
   
  vehicle_number, date = get_gemini_response(query)

  create_excel_file(vehicle_number, date, filename)
  print(&quot;Excel Creation done&quot;)
</code></pre>
","2024-11-20 07:51:28","0","Answer"
"79203990","","Error: ""command must be one of 'from', 'license', 'template', 'system', 'adapter', 'parameter', or 'message'"" when creating a custom model in Ollama","<p>I'm trying to create a custom model using Ollama, but when I run the following command:</p>
<pre class=""lang-bash prettyprint-override""><code>ollama create my-custom-model
</code></pre>
<p>I get the following error message:</p>
<pre><code>Error: (line 1): command must be one of &quot;from&quot;, &quot;license&quot;, &quot;template&quot;, &quot;system&quot;, &quot;adapter&quot;, &quot;parameter&quot;, or &quot;message&quot;
</code></pre>
<p>I’ve followed the instructions and created a <code>Modelfile</code> that looks like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>from: llama3.2:latest
license: open
system: |
  You are AdelBot, a friendly chatbot designed by Adel. Your responses are concise and include emojis for clarity and fun! 😊✨
message:
  - role: user
    content: Who are you?
  - role: assistant
    content: I'm AdelBot, here to assist you with clear and concise answers. 🚀
</code></pre>
<p>However, I’m still getting the same error.<br />
Can someone help me understand why the error is happening and how to fix it?</p>
<p><strong>Things I’ve Tried:</strong></p>
<ul>
<li>Double-checked the indentation in the <code>Modelfile</code>.</li>
<li>Ensured I’m using the correct commands (<code>from</code>, <code>license</code>, <code>system</code>, <code>message</code>).</li>
<li>Used the <code>llama3.2:latest</code> model, which is available on my system.</li>
</ul>
","2024-11-19 15:16:09","1","Question"
"79202411","79199510","","<p>I found the way to get the token count</p>
<pre><code>CompletionTokens = assistantClient.GetRuns(threadID).Select(f =&gt; f.Usage.CompletionTokens).SingleOrDefault();
PromptTokens = assistantClient.GetRuns(threadID).Select(f =&gt; f.Usage.PromptTokens).SingleOrDefault();
TotalTokens = assistantClient.GetRuns(threadID).Select(f =&gt; f.Usage.TotalTokens).SingleOrDefault();
</code></pre>
","2024-11-19 07:15:09","1","Answer"
"79199850","79192313","","<p>In agent.py, try to change the code where the <strong>KeyError</strong> occurs to include a check for 'key_name' in env_var before accessing it.</p>
<pre><code>if &quot;key_name&quot; in env_var and env_var[&quot;key_name&quot;] in unnacepted_attributes:
    continue
</code></pre>
<p>This makes it so that the code only tries to access 'key_name' if it is actually present in env_var. Here is a nice implementation using CrewAI [1].</p>
<p>[1] <a href=""https://devnavigator.com/home/setting-up-langchain-crewai-duckduckgo-a7d7b852-a3c7-4c8c-b6b2-c62e3ccae1d0"" rel=""nofollow noreferrer"">https://devnavigator.com/home/setting-up-langchain-crewai-duckduckgo-a7d7b852-a3c7-4c8c-b6b2-c62e3ccae1d0</a></p>
","2024-11-18 12:04:46","2","Answer"
"79199510","","How to count # of tokens consumed by OpenAI Assistant while streaming the message","<p>I am using openAI Assistant, I get the streamingUpdate using</p>
<pre><code>streamingUpdate = assistantClient.CreateRunStreaming(thread, assistant);
</code></pre>
<p>I could iterate through streamingUpdate and send the response to the client using SignalR api.  however I would like to know how to count the #of tokens consumed in the streaming mode.</p>
","2024-11-18 10:14:24","1","Question"
"79193604","79192313","","<p>Ok added a conditional check in agent.py to skip processing key_name unless it exists. This prevents the error and ensures compatibility for setups where key_name is not required.</p>
<p>Proposed Fix: A conditional check can be added in agent.py to skip processing key_name unless it's present and valid:</p>
<pre><code>if &quot;key_name&quot; in env_var and env_var[&quot;key_name&quot;] in unnacepted_attributes:
continue
</code></pre>
<p>This ensures the error doesn’t occur when key_name is absent but unnecessary for the task.</p>
<p>I created a Pull request <a href=""https://github.com/crewAIInc/crewAI/pull/1613"" rel=""nofollow noreferrer"">here</a></p>
","2024-11-15 18:42:59","1","Answer"
"79192313","","CrewAI - KeyError: 'key_name' When Running the Crew","<p>I’m following the CrewAI Getting Started Guide and running into a <code>KeyError: 'key_name'</code> when executing the <code>crewai run</code> command in the root of my project directory.</p>
<pre><code>(ai-crew) userk@mycelium:~/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development$ crewai run
Running the Crew
warning: `VIRTUAL_ENV=/home/userk/development/venv/ai-crew` does not match the project environment path `.venv` and will be ignored
Traceback (most recent call last):
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/.venv/bin/run_crew&quot;, line 8, in &lt;module&gt;
    sys.exit(run())
             ^^^^^
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/src/latest_ai_development/main.py&quot;, line 13, in run
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/.venv/lib/python3.12/site-packages/crewai/project/crew_base.py&quot;, line 35, in __init__
    self.map_all_task_variables()
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/.venv/lib/python3.12/site-packages/crewai/project/crew_base.py&quot;, line 145, in map_all_task_variables
    self._map_task_variables(
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/.venv/lib/python3.12/site-packages/crewai/project/crew_base.py&quot;, line 178, in _map_task_variables
    self.tasks_config[task_name][&quot;agent&quot;] = agents[agent_name]()
                                            ^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/.venv/lib/python3.12/site-packages/crewai/project/utils.py&quot;, line 7, in memoized_func
    cache[key] = func(*args, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/src/latest_ai_development/crew.py&quot;, line 12, in researcher
    return Agent(
           ^^^^^^
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/.venv/lib/python3.12/site-packages/pydantic/main.py&quot;, line 212, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/userk/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development/.venv/lib/python3.12/site-packages/crewai/agent.py&quot;, line 160, in post_init_setup
    if env_var[&quot;key_name&quot;] in unnacepted_attributes:
       ~~~~~~~^^^^^^^^^^^^
KeyError: 'key_name'
An error occurred while running the crew: Command '['uv', 'run', 'run_crew']' returned non-zero exit status 1.
</code></pre>
<p>Here is my setup:</p>
<pre><code>(ai-crew) userk@mycelium:~/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development$ python --version
Python 3.12.3
(ai-crew) userk@mycelium:~/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development$ pip freeze --local | grep crewai
crewai==0.80.0
crewai-tools==0.14.0
(ai-crew) userk@mycelium:~/development/git/vanto_ai_agents_cornerstone/crewai/latest_ai_development$ pip -V
pip 24.0 from /home/userk/development/venv/ai-crew/lib/python3.12/site-packages/pip (python 3.12)
</code></pre>
<p>I also tried using the virtual environment created by in the project directory with no luck.</p>
<p>How to resolve the error?</p>
","2024-11-15 12:07:49","0","Question"
"79187985","78339960","","<p>I faced the same issue,
Initially keras was a independent library, later on it was integrated with tensorflow.
It is highly recommended to import the classes from tensorflow.keras
not directly from keras.
Since tensorflow 2.x is tightly integrated with keras
but with keras alone, there is always a issue of different version , setup and all.</p>
<p>So import Tokenizer using this way -
from tensorflow.keras.preprocessing.text import Tokenizer</p>
","2024-11-14 09:02:11","0","Answer"
"79184879","78705977","","<p>Yes, it's possible.</p>
<h3>Fast and simple way</h3>
<ol>
<li>Download and launch desktop app <a href=""https://lmstudio.ai/"" rel=""nofollow noreferrer"">LM Studio</a> on the computer.</li>
<li>Enable LM Studio Server and Network discovery
<a href=""https://i.sstatic.net/26Hkx3NM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/26Hkx3NM.png"" alt=""enter image description here"" /></a></li>
<li>Load desired model <em>(like Llama3)</em> into LM Studio.</li>
<li>Connect <strong>CodeGPT</strong> to <strong>LM Studio</strong> via plugin's <em>&quot;Custom OpenAI&quot;</em> provider.</li>
</ol>
<p>I published detailed JetBrains IDE <strong>CodeGPT</strong> -&gt; <strong>LM Studio</strong> step-by-step guidence here: <a href=""https://github.com/carlrobertoh/CodeGPT/issues/384#issuecomment-2473137768"" rel=""nofollow noreferrer"">https://github.com/carlrobertoh/CodeGPT/issues/384#issuecomment-2473137768</a></p>
","2024-11-13 12:26:31","0","Answer"
"79173607","78437376","","<p>Overall, i really like the approach using xterm extension, as it provides useful logs during model run:</p>
<pre><code>!pip install colab-xterm
%load_ext colabxterm
Open a terminal
%xterm
</code></pre>
<p>but here's another approach, using subprocess.</p>
<p>The benefit of this approach is to stop/start ollama process running whenever we raise/restart the code cell.</p>
<pre class=""lang-py prettyprint-override""><code>import subprocess
with subprocess.Popen([&quot;ollama&quot;, &quot;serve&quot;]) as p:
    try:
      !ollama pull llama3.2
      #or !ollama list
      # or any code invoking ollama from python

    except Exception as e:
        p.terminate()
        raise e
</code></pre>
<p>decorator:</p>
<pre><code>def ollama_runner(func):
    def inner(*args,**kw):
        with subprocess.Popen([&quot;ollama&quot;, &quot;serve&quot;]) as p:
            try:
                return func(*args,**kw)
            except Exception as e:
                p.terminate()
                raise e

    return inner
</code></pre>
<p>usage:</p>
<pre><code>@ollama_runner
def main():
    pass
</code></pre>
<p>CAVEAT:
it causes decorated code to run forever because it awaits for subprocess.Popen forever.</p>
","2024-11-09 20:07:18","0","Answer"
"79165226","","""openai.BadRequestError: Invalid role"" error while using Ollama + LiteLLM + Autogen","<h3>Introduction</h3>
<p>I am running an ollama server with LiteLLM proxy. I aim to design a group chat using Autogen. The designated agents are:</p>
<ul>
<li><code>User_Proxy</code>: ask question</li>
<li><code>SQL_Generator</code>: generate SQL</li>
<li><code>SQL_Runner</code>: run SQL (tool calling)</li>
<li><code>Result_Validator</code>: validate result</li>
</ul>
<h3>Problems</h3>
<ol>
<li><p>The group chat works fine up until <code>SQL_Runner</code> returns the data from DB. But when the next speaker is expected to be the <code>Result_Validator</code>, the <code>SQL_Generator</code> is assigned as next speaker, although I explicitly state in the system message that <code>SQL_Runner</code> should pass the result to <code>Result_Validator</code>.</p>
</li>
<li><p>For some reason, <code>SQL_Generator</code> agent's role remains as <code>tool</code> following the activity of <code>SQL_Runner</code> instead of one of <code>[system, user, assistant]</code> and it leads to this error:</p>
</li>
</ol>
<pre class=""lang-none prettyprint-override""><code>***** Response from calling tool (call_6493f510-d318-4e85-b552-8963bece5fcb) *****
[
    // data from DB
]
**********************************************************************************
Next speaker: SQL_Generator

// error trace

openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid role: tool, role must be one of [system, user, assistant]', 'type': 'api_error', 'param': None, 'code': None}}
</code></pre>
<h3>Code</h3>
<h5>LLM Configs</h5>
<pre class=""lang-py prettyprint-override""><code>llama3_config_list = [
    {
        &quot;model&quot;: &quot;llama3:latest&quot;,
        &quot;base_url&quot;: f&quot;{OLLAMA_BASE_URL}:{OLLAMA_PORT}/v1&quot;,
        &quot;api_key&quot;: &quot;ollama&quot;,
    }
]

litellm_config_list = [
    {
        &quot;model&quot;: &quot;NotRequired&quot;,
        &quot;api_key&quot;: &quot;NotRequired&quot;,
        &quot;base_url&quot;: f&quot;{LITELLM_BASE_URL}:{LITELLM_PORT}&quot;,
        &quot;price&quot;: [0, 0],
    }
]

local_llm_config = {&quot;config_list&quot;: litellm_config_list, &quot;cache_seed&quot;: None}
</code></pre>
<h5>GroupChat</h5>
<pre class=""lang-py prettyprint-override""><code>user_proxy = autogen.UserProxyAgent(
    name=Role.USER_PROXY,
    system_message=Prompt.USER_PROXY,
    human_input_mode=&quot;NEVER&quot;,
    is_termination_msg=is_termination_msg,
    code_execution_config={&quot;use_docker&quot;: False, &quot;work_dir&quot;: &quot;test&quot;},
)

sql_generator = autogen.AssistantAgent(
    name=Role.SQL_GENERATOR,
    system_message=Prompt.SQL_GENERATOR,
    llm_config=get_custom_llm_config(config_list=llama3_config_list),
)

sql_runner = autogen.AssistantAgent(
    name=Role.SQL_RUNNER,
    system_message=Prompt.SQL_RUNNER,
    llm_config=local_llm_config,
)

@user_proxy.register_for_execution()
@sql_runner.register_for_llm(description=&quot;SQL query runner&quot;)
def run_sql(sql: Annotated[str, &quot;SQL query string to run&quot;]) -&gt; str:
    with PostgresManager() as db:
        db.get_rds_connection()
        return db.run_sql(sql)

result_validator = autogen.AssistantAgent(
    name=Role.RESULT_VALIDATOR,
    system_message=Prompt.RESULT_VALIDATOR,
    llm_config=get_custom_llm_config(config_list=llama3_config_list)
)

groupchat = autogen.GroupChat(
    agents=[
        user_proxy,
        sql_generator,
        sql_runner,
        result_validator,
    ],
    messages=[],
)

manager = autogen.GroupChatManager(groupchat=groupchat)

user_proxy.initiate_chat(manager, message=question)
</code></pre>
<p>For the problem number 1, I have already tried to set <code>autogen.GroupChat</code>'s <code>speaker_selection_method</code> to <code>&quot;round_robin&quot;</code> but the result did not change at all.</p>
<p>For the problem number 2, I am guessing that it might be overcome if I can somehow prevent agent role from being cached, but I cannot really figure out how.</p>
","2024-11-07 06:21:54","0","Question"
"79157886","79132695","","<p>To preserve the instance of an OpenAI <code>Assistant</code> using session variables, follow the steps below:</p>
<ul>
<li>Serialize Only Essential Data ,Restore Context with Chat History and Use Serializable Data Structures</li>
</ul>
<p>The code below defines a serializable class for storing chat history:</p>
<pre class=""lang-cs prettyprint-override""><code>public class SerializableChatContext
{
    public List&lt;string&gt; Messages { get; set; }
    public List&lt;DateTime&gt; Timestamps { get; set; }
    public string UserId { get; set; }
}

</code></pre>
<p>The following code stores the context in a session:</p>
<pre class=""lang-cs prettyprint-override""><code>
var chatContext = new SerializableChatContext
{
    Messages = new List&lt;string&gt; { &quot;Hello, how can I help you today?&quot; },
    Timestamps = new List&lt;DateTime&gt; { DateTime.UtcNow },
    UserId = &quot;user123&quot;
};
Session[&quot;ChatContext&quot;] = chatContext;




</code></pre>
<p>The code below lists messages from the thread. I used this <a href=""https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.openai.assistants-readme?view=azure-dotnet-preview#usage"" rel=""nofollow noreferrer"">link</a> for Azure OpenAI:</p>
<pre class=""lang-cs prettyprint-override""><code>Response&lt;PageableList&lt;ThreadMessage&gt;&gt; afterRunMessagesResponse
    = await client.GetMessagesAsync(thread.Id);
IReadOnlyList&lt;ThreadMessage&gt; messages = afterRunMessagesResponse.Value.Data;

foreach (ThreadMessage threadMessage in messages)
{
    Console.Write($&quot;{threadMessage.CreatedAt:yyyy-MM-dd HH:mm:ss} - {threadMessage.Role,10}: &quot;);
    foreach (MessageContent contentItem in threadMessage.ContentItems)
    {
        if (contentItem is MessageTextContent textItem)
        {
            Console.Write(textItem.Text);
        }
        else if (contentItem is MessageImageFileContent imageFileItem)
        {
            Console.Write($&quot;&lt;image from ID: {imageFileItem.FileId}&quot;);
        }
        Console.WriteLine();
    }
}

</code></pre>
<p>Output:
<img src=""https://i.imgur.com/9Jg3D8v.png"" alt=""enter image description here"" /></p>
<p>For more details, refer to this <a href=""https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI.Assistants"" rel=""nofollow noreferrer"">GitHub repository</a> for the OpenAI Assistants client library for .NET.</p>
","2024-11-05 06:23:52","0","Answer"
"79145652","","AWS Bedrock Claude Sonnet 3.5 with image and system prompt","<p>I need to invoke a Bedrock Claude Sonnet 3.5 model with:</p>
<ul>
<li>System prompt</li>
<li>An Image that I have in .JPEG locally in the disk</li>
<li>a context in text</li>
<li>a query</li>
</ul>
<p>Im getting an error in JSON payload with the image.</p>
<pre><code>A client error occured: messages.0.content.1.image.source: Field required
</code></pre>
<p>My code:</p>
<pre><code>def generate_conversation_bedrock(bedrock_client, model_id, input_text, input_image, max_tokens):

    response = client.invoke_model(
        modelId=model_id,
        body=json.dumps(
            {
                &quot;anthropic_version&quot;: &quot;bedrock-2023-05-31&quot;,
                &quot;max_tokens&quot;: max_tokens,
                &quot;system&quot;: sys_msg,
                &quot;messages&quot;: [
                    {
                        &quot;role&quot;: &quot;user&quot;,
                        &quot;content&quot;: [
                            {
                                &quot;type&quot;: &quot;text&quot;,
                                &quot;text&quot;: input_text
                            },
                            {
                                &quot;type&quot;: &quot;image&quot;,
                                &quot;image&quot;: {
                                    &quot;format&quot;: &quot;jpeg&quot;,
                                    &quot;source&quot;: {
                                        &quot;media_type&quot;: &quot;image/jpeg&quot;,
                                        &quot;data&quot;: input_image_base64  # Contiene la imagen en formato base64
                                    }
                                }
                            }
                        ]
                    }
                ],
            }
        ),
    )
    response_body = json.loads(response.get(&quot;body&quot;).read())
    return response_body

def main():
    model_id = &quot;anthropic.claude-3-5-sonnet-20240620-v1:0&quot;
    input_text = &quot;describe la  imagen&quot;
    input_image = &quot;./combined_image.jpg&quot;
    image = resize_and_compress_image(input_image)
    input_image_base64 = base64.b64encode(image).decode('utf-8')
   
    try:
        bedrock_client = boto3.client(service_name=&quot;bedrock-runtime&quot;, region_name=&quot;us-east-1&quot;)
        response = generate_conversation_bedrock(bedrock_client, model_id, input_text, input_image_base64, 1024)
        
        for output in response.get(&quot;content&quot;, []):
            print(output[&quot;text&quot;])
     

    
    except ClientError as err:
        message = err.response['Error']['Message']
        print(f&quot;A client error occured: {message}&quot;)

   

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>In <code>resize_and_compress_image</code> I turn the size of the image into the model input limits.</p>
","2024-10-31 16:57:26","0","Question"
"79142960","79108370","","<p>Had same issue. Here is alternate sample from the original authors.</p>
<p>The authors informed that the AddOpenAIChatCompletion() has a bug and we need to use AddOllamaChatCompletion().</p>
<p>This approach uses the Microsoft.SemanticKernel.Connectors.Ollama package for a better connection experience to Ollama, so you'll need to add that to your project as well.</p>
<p>The following link has the complete updated sample that was posted as a sample code on MSFT site.</p>
<p><a href=""https://github.com/dotnet/docs/pull/41317#issuecomment-2448142930"" rel=""nofollow noreferrer"">https://github.com/dotnet/docs/pull/41317#issuecomment-2448142930</a></p>
","2024-10-30 22:09:47","-2","Answer"
"79133725","79133223","","<p>Thanks Ryan
I did the same thing and it is now working.
Let me explain.
While loading data via the pipeline, in addition to putting data as a comma delimited value against the description as VectorValues nvarchar(max). I also added them as float in another table, against the Description Id (PK).
I then created a view on top to combine these two tables on PK = FK and while picking up, I converted those child records (vector values) into json array.</p>
<p>This seem to have resolved the issue. and data is loading in the index.</p>
<pre><code>SELECT 
     r.GIID
    ,r.ReportName
    ,r.[Description]
    ,JSON_QUERY('[' + STUFF((
                SELECT CONCAT(',', v.VectorValue,'')
                FROM dbo.ReportVectorData as v
                WHERE v.ReportGIID = r.GIID
                FOR XML PATH('')),1,1,'') + ']') AS VectorValue
FROM dbo.vwSearchReportsWithSecurity as r 
</code></pre>
","2024-10-28 14:00:49","0","Answer"
"79133604","79133223","","<p>Azure SQL and Cognitive Search have pretty limited transformation capabilities in the Indexer so you'd typically handle this outside the Indexer.</p>
<p>To convert VectorValues from a comma delimited string to a <code>Collection(Edm.Single)</code> in Azure Search you'll need to preprocess the data before it reaches the Indexer because the Indexer itself doesn't support direct conversion from a comma-separated string to a <code>Collection(Edm.Single)</code>.</p>
","2024-10-28 13:28:56","1","Answer"
"79133381","79130386","","<p>It seems like your laptop does not have a dedicated GPU and has a RAM of 16 GB based on the specs for the model name. <em>I might be wrong here, but I am going to assume this configuration and write the answer.</em></p>
<p>The model you are trying to use is <strong>~19 GB</strong> which does not fit on your RAM. Using quantized models might help in this case but since <strong>there is no GPU that supports CUDA on your laptop</strong>, <code>bitsandbytes</code> quantization is not supported (CPU and other backends are currently in experimental stage).</p>
<p><strong>In case you have one use you can use this code:</strong></p>
<pre class=""lang-py prettyprint-override""><code># pip install bitsandbytes accelerate
from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

app = Flask(__name__)
CORS(app)

model_id = &quot;MBZUAI-Paris/Atlas-Chat-9B&quot;
quantization_config = BitsAndBytesConfig(load_in_4bit=True)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    low_cpu_mem_usage=True, 
)


@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    user_input = data.get(&quot;input&quot;, &quot;&quot;)
    messages = [
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input},
    ]
    input_ids = tokenizer.apply_chat_template(messages, return_tensors=&quot;pt&quot;, return_dict=True, add_generation_prompt=True)
    outputs = model.generate(**input_ids, max_new_tokens=1000, temperature=0.0)
    response = tokenizer.decode(outputs[0]).split(&quot;&lt;start_of_turn&gt;model&quot;)[-1]
    return jsonify({&quot;output&quot;: response})


if __name__ == '__main__':
    app.run(port=5000)

</code></pre>
<p><strong>But if you don't there is still a way you can run this on your laptop's CPU using <code>llama.cpp</code>.</strong></p>
<p>First, install the llama.cpp Python bindings with:</p>
<pre class=""lang-bash prettyprint-override""><code>pip install llama-cpp-python
</code></pre>
<p>Then download the quantized model by typing this in your terminal:</p>
<pre class=""lang-bash prettyprint-override""><code>!huggingface-cli download mradermacher/Atlas-Chat-9B-GGUF Atlas-Chat-9B.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False
</code></pre>
<p><strong>Note:</strong> Change the <code>--local-dir</code> parameter as needed.</p>
<p>Then use this code:</p>
<pre class=""lang-py prettyprint-override""><code>from flask import Flask, request, jsonify
from flask_cors import CORS
from llama_cpp import Llama

app = Flask(__name__)
CORS(app)

model_path = &quot;./Atlas-Chat-9B.Q4_K_M.gguf&quot; # Download the model file first

# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.
llm = Llama(
  model_path=model_path,
  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources
  n_threads=2,  # The number of CPU threads to use, tailor to your system and the resulting performance
  n_gpu_layers=0  # The number of layers to offload to GPU, if you have GPU acceleration available
)


@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    user_input = data.get(&quot;input&quot;, &quot;&quot;)

    # Simple inference example
    output = llm(f&quot;&lt;start_of_turn&gt;user\n{user_input}&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n&quot;, # Prompt
        max_tokens=1000,  # Generate up to 1000 tokens
        stop=[&quot;&lt;end_of_turn&gt;&quot;],   # Example stop token
        echo=False        # Whether to echo the prompt
    )

    return jsonify({&quot;output&quot;: output[&quot;choices&quot;][0][&quot;text&quot;]})


if __name__ == '__main__':
    app.run(port=5000)

</code></pre>
<p>The input pattern is modified according to the format used in <code>Atlas-Chat</code> model.</p>
<p>I have used a 4-bit gguf quantized model here which effectively brings the model size down to <strong>5.76 GB</strong>. <em><strong>This model will load on your PC comfortably while sacrificing the quality of the generations to some extent.</strong></em></p>
","2024-10-28 12:24:24","0","Answer"
"79133223","","Azure AI Search, Pulling Vectors Embeddings from SQL table","<p>We have an Azure SQL table with a column called Description (<code>varchar(1000)</code>). While we loaded data into this table via ADF Pipeline, we used the REST API to create vector embeddings on Description. We used text-embedding-ada-002 model which creates 1536 vectors. These embeddings were then saved as a comma delimited values in another column called VectorValues (<code>varchar(max)</code>).</p>
<p>We are using Azure AI Search Indexer to pull data into an index.</p>
<pre><code>{&quot;name&quot;: &quot;Description&quot;,&quot;type&quot;: &quot;Edm.String&quot;, &quot;searchable&quot;: &quot;true&quot;, &quot;retrievable&quot;: &quot;true&quot;, &quot;sortable&quot;: &quot;false&quot;, &quot;filterable&quot;: &quot;false&quot;, &quot;facetable&quot;: &quot;false&quot;},
{
  &quot;name&quot;: &quot;VectorValues&quot;,
  &quot;type&quot;: &quot;Collection(Edm.Single)&quot;,
  &quot;dimensions&quot;: 1536,
  &quot;vectorSearchProfile&quot;: &quot;myHnswProfile&quot;,
  &quot;searchable&quot;: &quot;true&quot;,
  &quot;retrievable&quot;: &quot;true&quot;,
  &quot;filterable&quot;: &quot;false&quot;,
  &quot;sortable&quot;: &quot;false&quot;,
  &quot;facetable&quot;: &quot;false&quot;
}
</code></pre>
<p>How can we convert values of type string/varchar into Collection(Edm.Single), using the Indexer?</p>
<p>Apparently, while creating Indexer we can provide fieldMappings with Mapping Functions. However, we can only convert from string to string.</p>
<pre><code>&quot;fieldMappings&quot; : [
  {
    &quot;sourceFieldName&quot; : &quot;VectorValues&quot;, 
    &quot;targetFieldName&quot; : &quot;VectorValues&quot;,
    &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; }
  }]
</code></pre>
<p><a href=""https://i.sstatic.net/cWnVHPng.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cWnVHPng.png"" alt=""SQL table showing 2 columns"" /></a></p>
","2024-10-28 11:43:12","0","Question"
"79132695","","How to preserve the instance of Open AI assistance for the ongoing interaction with the model?","<p>I am trying to preserve the instance of the Assistant and AssistantClient class using session variable (asp.net application) and retrieve it for further conversation with the model.  I believe this will help the assistant to retain its chat history so that model response for the new question would be appropriate.</p>
<pre><code> public class AIHelper
 {


     AzureOpenAIClient? client;

     public OpenAI.Assistants.Assistant? openai_Assistant;
     public OpenAI.Assistants.AssistantClient? openai_AssistantClient;


  public void EstablishAISession()
    {

        client = new AzureOpenAIClient(apiEndPoint,azureKeyCredentials,azureOpenAIClientOptions)

openai_assistantClient = client.getAssistantClient();
                
openai_assistant = openai_assistantClient.CreateAssistant(modelName)

    }

}

//I do serailization and deserlization using below code
if( _httpContextAccessor.HttpContext.Session.GetString(&quot;AIHelper&quot;) == null){
AIHelper aiHelper = new AIHelper();
aiHelper.EstablishAISession();
                _httpContextAccessor.HttpContext.Session.SetString(&quot;AIHelper&quot;, JsonConvert.SerializeObject(aiHelper));
}
else
{

aiHelper = JsonConvert.DeserializeObject&lt;AIHelper&gt;(_httpContextAccessor.HttpContext.Session.GetString(&quot;AIHelper&quot;));

}
</code></pre>
<p>I always get aiHelper.openai_Assistant and openai_AssistantClient value as null after de-serialization.</p>
<p>I was expecting de-serialization helps to use the existing instance of Assistant and AssistantClient class.</p>
","2024-10-28 09:03:50","0","Question"
"79131824","78875648","","<p>Try to use <code>super().__init__()</code> to properly initializes the base class constructor. The warning says you should call the base class constructor using <code>super().__init__(**kwargs)</code>. Hope this works for you.</p>
","2024-10-28 01:08:29","-1","Answer"
"79130386","","Python transformers can't load","<p>I have this code:</p>
<pre><code>from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import pipeline

app = Flask(__name__)
CORS(app)
model = pipeline(&quot;text-generation&quot;, model=&quot;MBZUAI-Paris/Atlas-Chat-9B&quot;)

@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    user_input = data.get(&quot;input&quot;, &quot;&quot;)
    response = model(user_input, max_length=1000)
    return jsonify({&quot;output&quot;: response[0]['generated_text']})

if __name__ == '__main__':
    app.run(port=5000)
</code></pre>
<p>But program stops without output.</p>
<p>I have latest Transformers, Accelerate, SafeTensors and PyTorch. And I have Python 3.10.0.</p>
<p>My computer is Lenovo Yoga C-930 13IKB.</p>
<p>I tried this along with many other models, but only the smaller ones seem to work, like the Instruct models. Whenever I try a slightly larger model, it stops without producing any output.</p>
","2024-10-27 10:43:08","0","Question"
"79127185","79118798","","<p>Based on these <a href=""https://cloud.google.com/vertex-ai/docs/samples/aiplatform-create-batch-prediction-job-sample#code-sample"" rel=""nofollow noreferrer"">code sample</a> in creating a batch prediction job there are separated variable created:</p>
<pre><code>model_parameters_dict = {} model_parameters = json_format.ParseDict(model_parameters_dict, Value())
</code></pre>
<p>Try this format on setting the <a href=""https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters#api-examples"" rel=""nofollow noreferrer"">safety filter</a>:</p>
<pre><code>safety_settings = [
   SafetySetting(
       category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
       threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH
   ),
   SafetySetting(
       category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
       threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH
   ),
   SafetySetting(
       category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
       threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH
   ),
   SafetySetting(
       category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,
       threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH
   ),
]
</code></pre>
","2024-10-25 20:05:01","1","Answer"
"79125863","79125083","","<p>You will need to use the latest version of <code>Azure.AI.OpenAI</code> Nuget package (<code>2.1.0-beta.1</code> at the time of writing this answer).</p>
<p>You will then create a new instance of <code>AzureOpenAIClient</code>. Using this, you can then work with the latest version of Assistants API.</p>
<p>Here's a sample code to create an assistant:</p>
<pre><code>var client = new AzureOpenAIClient(new Uri(&quot;endpoint&quot;), new DefaultAzureCredential());
var assistant = (await client.GetAssistantClient().CreateAssistantAsync(_chatCompletionAIConnectionSettings.DeploymentId, new AssistantCreationOptions()
            {
                Name = assistantName,
                Tools = { ToolDefinition.CreateFileSearch(), ToolDefinition.CreateCodeInterpreter() },
                ToolResources = 
                { 
                    FileSearch = new FileSearchToolResources()
                    {
                        NewVectorStores = { new VectorStoreCreationHelper(&quot;list of file ids&quot;)}
                    } 
                },
                Instructions = &quot;You are a helpful assistant that answer user queries from the attached documents. &quot;
            })).Value;
</code></pre>
","2024-10-25 13:14:32","1","Answer"
"79125083","","Azure.AI.OpenAI.Assistants SDK - Report error while trying to use File Retrieval Tool","<p>It is about creating Assistant using chatGPT model. I use Azure.AI.OpenAI.Assistants SDK.</p>
<p>While trying to create an assistant using below code,  it reports OpenAI Retrieval V1 tool is not supported, in favor of the more advanced v2 file_search tool.</p>
<p>Code Snippet:</p>
<pre><code>AssistantCreationOptions aco = new AssistantCreationOptions(modelName);

aco.Tools.Add(new RetrievalToolDefinition());

aco.FileIds.AddRange(fileIds);   // This the list of files uploaded by the user i.e framework details file

aco.Instructions  = &quot;Summarize the framework details in 10 lines&quot;;

Assistant assistant = await client?.CreateAssistantAsync(aco);
</code></pre>
<p>Exact error is given below</p>
<pre><code>Azure OpenAI Retrieval v1 tool is not supported in favor of the more advanced v2 file_search tool. Please use \`file_search\` in the v2 api.
</code></pre>
<p>I need to know how to use file_search option</p>
","2024-10-25 09:30:33","0","Question"
"79123038","","Kernel restarting issue with ChromaDB code","<p>I'm working with <strong>ChromaDB</strong>, and my kernel keeps restarting when I try to add documents to a collection. Below is the code that is causing the problem:</p>
<pre><code>collection.add(
    documents=[
        &quot;This document is about New York&quot;,
        &quot;This document os about Tunisia&quot;
    ],
    ids = ['id1','id2']
)
</code></pre>
<p>I ran the above code to add documents to a ChromaDB collection. I expected the documents to be added without any issues. However, the kernel crashes and restarts each time.</p>
<p>Why the kernel might be crashing during this operation and how to fix this?</p>
","2024-10-24 17:17:21","0","Question"
"79122594","79108075","","<p>To ground your models to your source data, you need to have prepared and saved your data to Vertex AI Search. To do this, you need to create a <a href=""https://cloud.google.com/generative-ai-app-builder/docs/create-datastore-ingest#datastores-engines"" rel=""nofollow noreferrer"">data store in Vertex AI Agent Builder</a>.</p>
<p>If you are starting from scratch, you need to prepare your data for ingestion into Vertex AI Agent Builder. See <a href=""https://cloud.google.com/generative-ai-app-builder/docs/prepare-data"" rel=""nofollow noreferrer"">Prepare data for ingesting</a> to get started. Depending on the size of your data, ingestion can take several minutes to several hours. Only unstructured data stores are supported for grounding.</p>
<p>After you've prepared your data for ingestion, you can <a href=""https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es"" rel=""nofollow noreferrer"">Create a search data store</a>. After you've successfully created a data store, <a href=""https://cloud.google.com/generative-ai-app-builder/docs/create-engine-es"" rel=""nofollow noreferrer"">Create a search app</a> to link to it and <a href=""https://cloud.google.com/generative-ai-app-builder/docs/enterprise-edition"" rel=""nofollow noreferrer"">Turn Enterprise edition on</a>.</p>
<p>Here is an example on how to <a href=""https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#ground-gemini"" rel=""nofollow noreferrer"">ground the gemini 1.5 Flash model</a>. Use the following instructions to ground a model with your own data. But before proceeding, There are <a href=""https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#prerequisites"" rel=""nofollow noreferrer"">prerequisites</a> needed before you can ground model output to your data.</p>
","2024-10-24 15:10:06","0","Answer"
"79121340","79119450","","<p>No, it is not supported. You may need other tools to transform the data.</p>
","2024-10-24 09:46:26","0","Answer"
"79119450","","Milvus Backup and Restore: Schema Compatibility Error with Different Vector Types","<p>Does the Milvus backup and restore feature support transferring data between collections with different schemas? Specifically, I have two collections where most fields have the same name and type, except for the vector fields: collection v1 uses FloatVector(512) and collection v2 uses Float16Vector(512). Can I copy data from the v1 collection and import it into the v2 collection using Milvus version 2.4.4?</p>
<pre class=""lang-py prettyprint-override""><code>from pymilvus import Collection, connections, utility

connections.connect()

v1_collection = Collection(&quot;v1&quot;)
v2_collection = Collection(&quot;v2&quot;)

backup_files = utility.backup_collection(&quot;v1&quot;)

utility.restore_collection(backup_files, collection_name=&quot;v2&quot;)
</code></pre>
","2024-10-23 19:26:21","0","Question"
"79118798","","Safety setting for Vertex AI batch processing (Gemini Flash API)","<p>How do you set safety settings on Batch Prediction job for Vertex AI?</p>
<p>I can't seem to find anything on Google / docs about this.</p>
<blockquote>
<p>TypeError: BatchPredictionJob.submit() got an unexpected keyword argument 'model_parameters'</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code># File Format
# {&quot;request&quot;:{&quot;contents&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;parts&quot;: [{&quot;text&quot;: &quot;Translate this sentence to French: Hello, how are you today?&quot;}]}]}}

import os
import vertexai
from vertexai.preview.batch_prediction import BatchPredictionJob

PROJECT_ID = os.getenv(&quot;GOOGLE_CLOUD_PROJECT&quot;)


def batch_predict_gemini_createjob(
    input_uri: str, output_uri: str
) -&gt; BatchPredictionJob:
    &quot;&quot;&quot;Perform batch text prediction using a Gemini AI model.
    Args:
        input_uri (str): URI of the input file in BigQuery table or Google Cloud Storage.
            Example: &quot;gs://[BUCKET]/[DATASET].jsonl&quot; OR &quot;bq://[PROJECT].[DATASET].[TABLE]&quot;

        output_uri (str): URI of the output folder,  in BigQuery table or Google Cloud Storage.
            Example: &quot;gs://[BUCKET]/[OUTPUT].jsonl&quot; OR &quot;bq://[PROJECT].[DATASET].[TABLE]&quot;
    Returns:
        batch_prediction_job: The batch prediction job object containing details of the job.
    &quot;&quot;&quot;

    import time

    input_uri =&quot;gs://ge/in/vertex_batch_input.jsonl&quot;
    output_uri =&quot;gs://ge/out/&quot;

    # Initialize vertexai
    vertexai.init(project=PROJECT_ID, location=&quot;us-central1&quot;)

    # Submit a batch prediction job with Gemini model
    batch_prediction_job = BatchPredictionJob.submit(
        source_model=&quot;gemini-1.5-flash-002&quot;,
        input_dataset=input_uri,
        output_uri_prefix=output_uri,
        model_parameters={
            &quot;safety_settings&quot;: [
                {
                    &quot;category&quot;: &quot;HARM_CATEGORY_DANGEROUS_CONTENT&quot;,
                    &quot;method&quot;: &quot;SEVERITY&quot;,
                    &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;
                },
                {
                    &quot;category&quot;: &quot;HARM_CATEGORY_HATE_SPEECH&quot;,
                    &quot;method&quot;: &quot;SEVERITY&quot;,
                    &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;
                },
                {
                    &quot;category&quot;: &quot;HARM_CATEGORY_HARASSMENT&quot;,
                    &quot;method&quot;: &quot;SEVERITY&quot;,
                    &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;
                },
                {
                    &quot;category&quot;: &quot;HARM_CATEGORY_SEXUALLY_EXPLICIT&quot;,
                    &quot;method&quot;: &quot;SEVERITY&quot;,
                    &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;
                }
            ]
        }
    )
</code></pre>
","2024-10-23 16:16:57","1","Question"
"79118001","79117410","","<p>All credits to @Stefano Fiorucci - anakin87 for the solution to this problem.</p>
<p>The complete steps are:</p>
<ol>
<li><code>git clone https://github.com/deepset-ai/haystack.git</code></li>
<li><code>cd ./haystack</code></li>
<li><code>python -m pip install -e '.[all]'</code> or <code>'.[all-gpu]'</code> # Use the latest version to develop the project and checkout to the latest 1x release to use the annotation tool.</li>
<li><code>git checkout v1.26.x</code> # Do NOT execute python -m pip install again.</li>
<li><code>cd ./annotation_tool/</code></li>
<li><code>edit docker-compose.yml</code> # Configure the credentials and the database in <code>./haystack/annotation_tool/docker-compose.yml</code> (see OP link for the original version of the file). In this case I left everything as is, but uncommented lines 12 - 16.</li>
<li>Add a new user group to the annotation/ directory, in windows, so that Docker can have the privileges to create directories and files. Follow these steps <a href=""https://github.com/docker/for-win/issues/3385#issuecomment-493391443"" rel=""nofollow noreferrer"">here</a>.</li>
<li><code>docker-compose pull</code> # execute this command inside the <code>annotation_tool/</code> directory.</li>
<li><code>docker-compose up</code></li>
<li>Chrome browser: <code>http://localhost:7001</code></li>
</ol>
<p><strong>Results:</strong>
<a href=""https://i.sstatic.net/xFu9ck9i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFu9ck9i.png"" alt=""enter image description here"" /></a></p>
","2024-10-23 12:56:26","0","Answer"
"79117410","","Install local version of Haystack's Annotation Tool","<p>Although the instructions in <a href=""https://docs.haystack.deepset.ai/v2.1/docs/annotation"" rel=""nofollow noreferrer"">Haystack</a> describe how to install the annotation tool locally using Docker, all attempts have failed. Unfortunately, the website does not contain any more details.</p>
<p>Has anyone been able to get this to work? If so, what am I missing?</p>
<p><strong>Reproduction scenario:</strong></p>
<ul>
<li><code>git clone https://github.com/deepset-ai/haystack.git</code></li>
<li><code>cd haystack</code></li>
<li><code>python -m pip install -e '.[all-gpu]'</code> # I used the latest
version to develop a project and, now, I want to use the annotation
tool to create a custom dataset, thus the following checkout to an
older release.</li>
<li><code>git checkout v1.26.x</code></li>
<li><code>docker-compose pull</code></li>
<li>I left the docker-compose.yml as is:</li>
</ul>
<p>Contents of yml file (see <a href=""https://github.com/deepset-ai/haystack/blob/v1.26.x/annotation_tool/docker-compose.yml"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>version: &quot;3&quot;
services:
  backend:
    image: deepset/haystack-annotation:latest
    environment:
      NODE_ENV: &quot;production&quot;
      DB_HOSTNAME: &quot;db&quot;
      DB_NAME: &quot;databasename&quot;
      DB_USERNAME: &quot;somesafeuser&quot;
      DB_PASSWORD: &quot;somesafepassword&quot;
      # IMPORTANT: please configure credentials with secure strings.
      DEFAULT_ADMIN_EMAIL: &quot;example@example.com&quot;
      DEFAULT_ADMIN_PASSWORD: &quot;DEMO_PASSWORD&quot;
      COOKIE_KEYS: &quot;somesafecookiekeys&quot;
      JWT_SECRET: &quot;somesafesecret&quot;
      DOMAIN_WHITELIST: &quot;*&quot;
    ports:
      - &quot;7001:7001&quot;
    links:
      - &quot;db:database&quot;
    depends_on:
      - db
    networks:
      - app-network
    restart: unless-stopped

  db:
    image: &quot;postgres:12&quot;
    environment:
      POSTGRES_USER: &quot;somesafeuser&quot;
      POSTGRES_PASSWORD: &quot;somesafepassword&quot;
      POSTGRES_DB: &quot;databasename&quot;
    ports:
      - &quot;5432:5432&quot;
    volumes:
      - ./postgres-data:/var/lib/postgresql/data
    networks:
      - app-network
    healthcheck:
      test: &quot;pg_isready --username=somesafeuser --dbname=databasename &amp;&amp; psql --username=somesafeuser --list&quot;
      timeout: 3s
      retries: 5
    restart: unless-stopped

networks:
  app-network:
    driver: bridge
</code></pre>
<ul>
<li><code>docker-compose up</code></li>
<li>Open Chrome browser: <code>http://localhost:7001</code></li>
</ul>
<p><strong>Results (screenshots):</strong></p>
<p>Unable to open the page:
<a href=""https://i.sstatic.net/lSxfgt9F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lSxfgt9F.png"" alt=""Unable to open the page"" /></a>
Docker is running:
<a href=""https://i.sstatic.net/lGDZ8md9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lGDZ8md9.png"" alt=""Docker is running"" /></a></p>
","2024-10-23 09:57:12","0","Question"
"79116598","79116496","","<p>As mentioned <a href=""https://docs.mistral.ai/capabilities/completion/#tag/models/operation/jobs_api_routes_fine_tuning_unarchive_fine_tuned_model:%7E:text=The%20chat%20completion%20API%20accepts%20a%20list%20of%20chat%20messages%20as%20input%20and%20generates%20a%20response.%20This%20response%20is%20in%20the%20form%20of%20a%20new%20chat%20message%20with%20the%20role%20%22assistant%22%20as%20output."" rel=""nofollow noreferrer"">here</a>:</p>
<blockquote>
<p>The chat completion API accepts a list of chat messages as input and generates a response.</p>
</blockquote>
<p>So, to maintain context between each call when using the Mistral AI API, you can manage the conversation history yourself by keeping track of the previous messages and sending them along with each new message to the API.</p>
<p>Here's a bare-bone example of implementing this:</p>
<pre class=""lang-py prettyprint-override""><code>.
.
.
conversation_history = []

conversation_history.append({
    &quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: user_message,
})

chat_response = client.agents.complete(
    agent_id=&quot;my agent id&quot;,
    messages=conversation_history
)

response = chat_response.choices[0].message.content

conversation_history.append({
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response,
})
.
.
.
</code></pre>
","2024-10-23 06:18:03","0","Answer"
"79116566","79116496","","<p>Asking Mistrai Ai itself solved my issue !
I need to send all previous message with assistant role. here is a full working example that keep context:</p>
<pre><code>import os
from mistralai import Mistral

api_key = &quot;secret api key&quot;

client = Mistral(api_key=api_key)

# Initialiser l'historique des messages
message_history = []

def get_chat_response(user_message):
    # Ajouter le message de l'utilisateur à l'historique
    message_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message})

    # Appeler l'API avec l'historique des messages
    chat_response = client.agents.complete(
        agent_id=&quot;my agent id&quot;,
        messages=message_history
    )

    # Ajouter la réponse de l'agent à l'historique
    agent_message = chat_response.choices[0].message.content
    message_history.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: agent_message})

    return agent_message

while True :
    user_message = input(&quot;&gt;: &quot;)
    response = get_chat_response(user_message)
    print(response)
</code></pre>
","2024-10-23 06:02:52","0","Answer"
"79116496","","How to send context between multiple Mistral AI api call to keep conversation like a chat bot?","<p>I want to create a chat bot like using Mistral AI api. But Api respond like a new start between each call and it does not keep the context. Here is my python code (from <a href=""https://docs.mistral.ai/capabilities/completion/"" rel=""nofollow noreferrer"">documentation</a>):</p>
<pre><code>import os
from mistralai import Mistral

api_key = &quot;my api key&quot;

client = Mistral(api_key=api_key)

chat_response = client.agents.complete(
    agent_id=&quot;my agent id&quot;,
    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Hello&quot;,
        },
    ]
)
print(chat_response.choices[0].message.content)
</code></pre>
<p>How to send context between each call?</p>
","2024-10-23 05:32:04","0","Question"
"79113992","79068298","","<p>This is a bug in the _load_pretrained_model() function of
transformers/modeling_utils.py when loading sharded weight files. The
state_dict is applied to the empty model per shard. This is problematic as the
quantized weight and its meta data(*.quant_state.bitsandbytes__nf4) may be
stored in the different shards. The <a href=""https://github.com/schnell18/transformers/commit/57d3073d1830520891c00ead77a0ac188be8bf49"" rel=""nofollow noreferrer"">quick-and-dirty fix</a> is to merge tensors
from all shards into one state_dict. Similar issues have been reported on <a href=""https://github.com/unslothai/unsloth/issues/638"" rel=""nofollow noreferrer"">the unsloth github issue 638</a></p>
","2024-10-22 12:14:08","1","Answer"
"79113907","79111733","","<p>LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.</p>
<p>For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.</p>
<pre><code>You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:

- **Year**: The vintage year of the wine.
- **Color**: The color of the wine (e.g., Red, White, Rosé).
- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).
- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).
- **Brand**: The brand or producer of the wine.
- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).

**Instructions:**

- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.
- Be cautious of potential ambiguities. For example:
  - A brand name may include words like &quot;Red&quot; or &quot;White&quot; (e.g., &quot;Red Head Vineyard&quot;) which should not be confused with the wine color.
  - Large numbers may represent weight (e.g., &quot;1500 ml&quot;) rather than a year.
- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.

**Output Format:**

Provide the extracted information in XML format, using the following structure:

&lt;Wine&gt;
&lt;Year&gt;{{Year}}&lt;/Year&gt;
&lt;Color&gt;{{Color}}&lt;/Color&gt;
&lt;Weight&gt;{{Weight}}&lt;/Weight&gt;
&lt;WeightUnit&gt;{{WeightUnit}}&lt;/WeightUnit&gt;
&lt;Brand&gt;{{Brand}}&lt;/Brand&gt;
&lt;GrapeVariety&gt;{{GrapeVariety}}&lt;/GrapeVariety&gt;
&lt;/Wine&gt;

**Examples:**

  1. **Input:**

 `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`

 **Output:**



```xml
   &lt;Wine&gt;
     &lt;Year&gt;2017&lt;/Year&gt;
     &lt;Color&gt;Red&lt;/Color&gt;
     &lt;Weight&gt;750&lt;/Weight&gt;
     &lt;WeightUnit&gt;ml&lt;/WeightUnit&gt;
     &lt;Brand&gt;La Lecciaia&lt;/Brand&gt;
     &lt;GrapeVariety&gt;Cabernet Sauvignon&lt;/GrapeVariety&gt;
   &lt;/Wine&gt;
   ```

   
   `Red Head Vineyard Chardonnay 2020 1.5L`

   **Output:**

   &lt;Wine&gt;
     &lt;Year&gt;2020&lt;/Year&gt;
     &lt;Color&gt;&lt;/Color&gt;
     &lt;Weight&gt;1.5&lt;/Weight&gt;
     &lt;WeightUnit&gt;L&lt;/WeightUnit&gt;
     &lt;Brand&gt;Red Head Vineyard&lt;/Brand&gt;
     &lt;GrapeVariety&gt;Chardonnay&lt;/GrapeVariety&gt;
   &lt;/Wine&gt;

 

    **Task:**
    
    Given the following wine description, extract the components and provide the output in XML format as specified.
    
    {win_description}
</code></pre>
<p>Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.</p>
<p>The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs</p>
","2024-10-22 11:49:29","1","Answer"
"79113737","78649446","","<p>The error you're encountering is due to the fact that agent names can only contain alphanumeric characters, underscores (_), or hyphens (-). Spaces are not allowed in agent names, which is why the name is causing this issue. Remove the spaces from agent name and it will work.</p>
","2024-10-22 11:03:55","1","Answer"
"79111733","","How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)","<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>
<p>I have short product descriptions that I’d like to transform into structured attributes.</p>
<p>Example:</p>
<p>Input:</p>
<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”
</code></pre>
<p>Output:</p>
<pre><code>Year = 2017

Color = Red

Weight = 750

Weight Unit = ml
</code></pre>
<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>
<ol>
<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>
</li>
<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>
</li>
<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>
</li>
<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>
</li>
</ol>
<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>
<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>
<p>Appreciate any insight into approaches or suggested learning resources.</p>
<p></p>
<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>
","2024-10-21 20:54:56","0","Question"
"79108370","","Error 404 trying to execute local AI (oLlAMA-PHI3)","<p>I get a 404 error everytime i run this code
when it reaches <code>aiChatService.GetStreamingChatMessageContentsAsync(chatHistory)</code>.</p>
<p>I've tried to reinstall the service and the respective AI but nothing works. any ideas??</p>
<pre><code>using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

using var httpClient = new HttpClient()
{
    BaseAddress = new Uri(&quot;http://localhost:11434&quot;),
};

// Create a kernel with OpenAI chat completion
#pragma warning disable SKEXP0010
Kernel kernel = Kernel.CreateBuilder()
                    .AddOpenAIChatCompletion(
                        modelId: &quot;phi3:3.8b&quot;,
                        httpClient: httpClient,
                        apiKey: &quot;non required&quot;)
                    .Build();

var aiChatService = kernel.GetRequiredService&lt;IChatCompletionService&gt;();
var chatHistory = new ChatHistory([new ChatMessageContent(AuthorRole.System, &quot;Sos un asistente de programación de C#&quot;)]); ;

while (true)
{
    // Get user prompt and add to chat history
    Console.WriteLine(&quot;Your prompt:&quot;);
    var userPrompt = Console.ReadLine();
    chatHistory.Add(new ChatMessageContent(AuthorRole.User, userPrompt));

    // Stream the AI response and add to chat history
    Console.WriteLine(&quot;AI Response:&quot;);
    var response = &quot;&quot;;
    await foreach (var item in
        aiChatService.GetStreamingChatMessageContentsAsync(chatHistory))
    {
        Console.Write(item.Content);
        response += item.Content;
    }
    chatHistory.Add(new ChatMessageContent(AuthorRole.Assistant, response));
    Console.WriteLine();
}
</code></pre>
<p>I've tried to reinstall oLlama service and AI but nothing works.</p>
<pre><code>System.ClientModel.ClientResultException: 'Service request failed.
Status: 404 (Not Found)
</code></pre>
","2024-10-21 02:06:41","0","Question"
"79108075","","Getting a 400 error when calling Google Vertex API with ""retrieval"" function (datastore)","<p>I'm getting the below error whenever I include the retrieval tool in an API request. To confirm, the call works fine without adding in a retrieval tool.</p>
<p>I'm POSTing to this endpoint:</p>
<pre><code>https://us-central1-aiplatform.googleapis.com/v1/projects/%5C%5C\&lt;project_id\&gt;/locations/us-central1/publishers/google/models/gemini-1.0-pro-001:generateContent
</code></pre>
<p>And this payload:</p>
<pre><code>{
  &quot;contents&quot;: [{
    &quot;role&quot;: &quot;user&quot;,
    &quot;parts&quot;: [{
      &quot;text&quot;: &lt;prompt&gt;
    }]
  }],
  &quot;model&quot;: &quot;projects/&lt;project_id&gt;/locations/us/publishers/google/models/gemini-1.0-pro-001&quot;
}
</code></pre>
<p>Works fine, but adding in this to the payload:</p>
<pre><code>  &quot;tools&quot;: [{
    &quot;retrieval&quot;: {
      &quot;vertexAiSearch&quot;: {
        &quot;datastore&quot;: &quot;projects/&lt;project_id&gt;/locations/us/collections/default_collection/dataStores/&lt;datastore_id&gt;&quot;
      }
    }
  }]
</code></pre>
<p>Causes the 400 error:</p>
<pre><code>{
  &quot;error&quot;: {
    &quot;code&quot;: 400,
    &quot;message&quot;: &quot;Request contains an invalid argument.&quot;,
    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;
  }
}
</code></pre>
<p>Things suggested in other Stack Overflow threads that haven't helped:</p>
<ul>
<li><p>ensure the app and the bucket are the same location (they are)</p>
</li>
<li><p>ensure there's sufficient permissions (and wait for any permissions changes to take effect) (permI've read all the answers I can find about this issue but none of them helped me.</p>
</li>
<li><p>ensure the app and the bucket are the same location (they are)</p>
</li>
<li><p>ensure there's sufficient permissions (and wait for any permissions changes to take effect) (permissions are fine)</p>
</li>
<li><p>ensure Enterprise is activated (I ensured it was checked when creating the datastore)</p>
</li>
<li><p>ensure there's a search app created that links to the cloud storage (bucket) - I did this however a search app is completely separate to the agent app (I don't see how there's any link between the two) so I'm sure it doesn't help, but anyway.</p>
</li>
</ul>
<p>And just to confirm (as it may help others), the hierarchy of objects in Vertex is this:</p>
<ol>
<li>Endpoint - The location in the URL must be a specific location (eg; us-central1), or &quot;global&quot;, but can't be a larger region (eg; &quot;us&quot;) - I selected &quot;us-central1&quot;</li>
<li>Project (simply a container for everything else) - Projects don't have locations.</li>
<li>App (Can be agent, search, or chat) - You can select a specific region (eg; us-central1) or a larger region (us). I chose &quot;us&quot;.</li>
<li>Data stores (Kinda of links the App to data - Data can be website, cloud storage, API, etc) - I chose &quot;cloud storage&quot;.</li>
<li>Cloud Storage (the actual data) - You can only select a larger area (eg; us) on a bucket - So I selected &quot;us&quot;.</li>
</ol>
<p>Logs show that the bucket is indexed just fine, I can ask the model about the one test document I uploaded (using Workbench) and it works fine.</p>
<p>I've tried setting all different types of locations. No joy.</p>
<p>I know the Agent is connecting just fine to the Cloud Storage because if there's any permissions issue it doesn't let you save your selections.</p>
<p>I've stuck with gemini-1.0-pro-001 (not gemini-1.0-pro-002) across the board because the gemini-1.0-pro-002 is not an option in the Agent. I didn't want model clashes.</p>
<p>I can't think of what else to check.</p>
","2024-10-20 21:20:34","1","Question"
"79104305","","Context Length Limitation When Fine-Tuning Llama 3.1 in Colab","<p>I am fine-tuning the Llama 3.1 model in Google Colab Pro using an A100 GPU with a custom dataset (using LoRA techniques) via the Unsloth library. Below is the LoRA code I am using:</p>
<pre><code>max_seq_length = 2048
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128
    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                    &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],
    lora_alpha=16,
    lora_dropout=0,  # Supports any, but = 0 is optimized
    bias=&quot;none&quot;,     # Supports any, but = &quot;none&quot; is optimized
     
    use_gradient_checkpointing=&quot;unsloth&quot;,  # True or &quot;unsloth&quot; for very long context
    random_state=3407,
    use_rslora=False,  # We support rank stabilized LoRA
    loftq_config=None,  # And LoftQ
)
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,  # Can make training 5x faster for short sequences.
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps=60,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim=&quot;adamw_8bit&quot;,
        weight_decay=0.01,
        lr_scheduler_type=&quot;linear&quot;,
        seed=3407,
        output_dir=&quot;outputs&quot;,
    ),
)
</code></pre>
<p>When loading the model, we must specify a maximum sequence length, which restricts its context window. Llama 3.1 supports up to 128k context length, but I am setting it to 2048 in this example since it consumes more compute and VRAM. Additionally, the dtype parameter automatically detects if your GPU supports the BF16 format for more stability during training (this feature is restricted to Ampere and more recent GPUs).</p>
<p>My Questions:</p>
<ul>
<li>If I set max_seq_length to 2048 while training, what will be my model's context length after training, 128k or 2048?</li>
<li>After training the model, can we utilize a context length of 128k, or will it still be limited to 2048?</li>
</ul>
","2024-10-19 05:59:26","0","Question"
"79096660","79085126","","<p>I've solved this with the following command</p>
<pre class=""lang-bash prettyprint-override""><code>docker run --gpus all -p 8080:8080 -v &quot;$(pwd)/test_input.json:/test_input.json&quot; ${IMAGE_REPO}
</code></pre>
<p>This command will start the endpoint, run the test, and then terminate automatically.
Make sure to place the test_input.json file in your local directory.</p>
","2024-10-17 05:56:57","2","Answer"
"79096556","79089616","","<p>well it got fixed, I had to pass <code>mask_image=mask_pil</code> but I was passing <code>mask=mask_pil</code></p>
","2024-10-17 05:15:40","1","Answer"
"79096364","78457321","","<p>Install <code>tf_keras</code> before using it.</p>
<pre class=""lang-py prettyprint-override""><code>import tf_keras as k3

model = k3.models.load_model(YOUR_MODEL_PATH)
</code></pre>
","2024-10-17 03:04:23","0","Answer"
"79092343","79045085","","<p>maybe it's caused by streamlit-authenticator version 0.4.1
I tried downgrading to streamlit-authenticator==0.3.3 and it still works fine</p>
","2024-10-16 03:11:44","0","Answer"
"79089616","","ValueError: Input is in incorrect format","<p>I'm encountering a persistent issue when running the StableDiffusionInpaintPipeline for an inpainting task. Despite passing inputs in the expected formats (both the image and mask are in PIL.Image.Image format with correct sizes), I keep receiving the following error:</p>
<pre><code>ValueError: Input is in incorrect format. Currently, we only support &lt;class 'PIL.Image.Image'&gt;, &lt;class 'numpy.ndarray'&gt;, &lt;class 'torch.Tensor'&gt;
</code></pre>
<p>Here is the code:</p>
<pre><code>@torch.inference_mode()
def generate_image_bytes(base64_img, request, prompt) -&gt; dict:
    # Get a model from the pool
    model = model_pool.get_model()
    
    try:
        if base64_img is not None:
            img_data = base64.b64decode(base64_img)
            img = Image.open(io.BytesIO(img_data))

            print(&quot;Recieved image of size: &quot;, img.size)
        
        img = img.convert(&quot;RGB&quot;)
        image_np = np.array(img)
        
        roi_points = [
            request[&quot;box&quot;][&quot;x1&quot;],
            request[&quot;box&quot;][&quot;y1&quot;],
            request[&quot;box&quot;][&quot;x2&quot;],
            request[&quot;box&quot;][&quot;y2&quot;],
        ]

        results = model[&quot;seg_model&quot;](
            image_np, 
            device=&quot;cuda&quot;, 
            conf=0.6, 
            iou=0.9, 
            bboxes = roi_points,
            labels=[1],
            verbose=False    
        )
        black_mask = np.zeros_like(image_np[:, :, 0], dtype=np.uint8)
        for m in results[0].masks.data:
            mask = m.cpu().numpy().astype(np.uint8)

            orig_shape = results[0].masks.orig_shape
            resized_mask = cv2.resize(mask, (orig_shape[1], orig_shape[0]))

            black_mask[resized_mask == 1] = 255
        
        image_pil = Image.fromarray(image_np)
        mask_pil = Image.fromarray(black_mask).convert(&quot;L&quot;)

        print(f&quot;Image size: {image_pil.size}, mode: {image_pil.mode}&quot;)
        print(f&quot;Mask size: {mask_pil.size}, mode: {mask_pil.mode}&quot;)
        
        generator = torch.Generator(device=&quot;cuda&quot;).manual_seed(0)

        image = model[&quot;pipeline&quot;](
            prompt=prompt,
            negative_prompt=IMG_INPAINTING_NEG_PROMPT,
            image=image_pil,
            mask=mask_pil,
            guidance_scale=8.0,
            num_inference_steps=50,
            strength=0.05,
            generator=generator,
        ).images[0]

        # Convert image to bytes
        bytes = io.BytesIO()
        image.save(bytes, format=&quot;PNG&quot;)

        return {&quot;bytes&quot;: base64.b64encode(bytes.getvalue()).decode(&quot;utf-8&quot;)}

</code></pre>
<p>I tried viewing both images i.e image_pil and mask_pil and they have no problem. Their format and size is also same but I am still getting this error.</p>
","2024-10-15 10:59:04","1","Question"
"79085126","","RunPods Serverless - Testing Endpoint in Local with Docker and GPU","<p>I’m working on creating a custom container to run <strong>FLUX</strong> and <strong>Lora</strong> on <strong>Runpods</strong>, using this <a href=""https://github.com/runpod-workers/worker-stable_diffusion_v1"" rel=""nofollow noreferrer"">Stable Diffusion example</a> as a starting point. I successfully deployed my first pod on Runpods, and everything worked fine.</p>
<p>However, my issue arises when I make code changes and want to test my endpoints locally before redeploying. Constantly deploying to Runpods for every small test is quite time-consuming.</p>
<p>I found a guide for local testing in the Runpods documentation <a href=""https://docs.runpod.io/serverless/workers/development/local-testing"" rel=""nofollow noreferrer"">here</a>. Unfortunately, it only provides a simple example that suggests running the handler function directly, like this:</p>
<pre class=""lang-bash prettyprint-override""><code>python your_handler.py --test_input '{&quot;input&quot;: {&quot;prompt&quot;: &quot;The quick brown fox jumps&quot;}}'
</code></pre>
<p>This doe not work for me as it ignores the Docker setup entirely and just runs the function in my local Python environment. I want to go beyond this and test the Docker image end-to-end locally—on my GPU—with the exact dependencies and setup that will be used when deploying on Runpods.</p>
<p>Is there a specific documentation for testing Docker images locally for Runpods, or a recommended workflow for this kind of setup?</p>
<p>I tried following the guidelines for local testing here:
<a href=""https://docs.runpod.io/serverless/workers/development/local-testing"" rel=""nofollow noreferrer"">https://docs.runpod.io/serverless/workers/development/local-testing</a></p>
","2024-10-14 07:30:28","2","Question"
"79078845","79073591","","<p>I've had the same issue and just fixed it.</p>
<p>This is how:</p>
<ol>
<li><p>Set the API key based access control for the search service in the azure portal</p>
</li>
<li><p>Set the admin key for the same resource in Azure Ai Studio
in the project settings &gt; connected resources &gt; choose the Azure AI Search (Cognitive Search) type resource, then edit. Check also if the target is set correctly!</p>
</li>
</ol>
<p><img src=""https://i.sstatic.net/ARbFsW8J.png"" alt=""enter image description here"" /></p>
","2024-10-11 15:23:18","0","Answer"
"79073591","","azure ai studio - indexing problem (ai search)","<p>Through the web interface of ai.azure.com  I created all the necessary services such as azure ai studio with project, storage account, open AI, AI search, etc. And I connected all the services to a private endpoint.</p>
<p>The problem arose only when indexing files in Ai search if it is connected to a private endpoint I get error. If it is public, then everything works fine.</p>
<p>Has anyone encountered this problem?</p>
<p>[enter image description here](<a href=""https://i.sstatic.net/ieQvTJj8.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/ieQvTJj8.png</a>)    [enter image description here]</p>
<p><strong>step of ml pipeline - Creating Azure AI Search Index - Data ingestion failed</strong></p>
<pre><code>File &quot;/azureml-envs/rag-embeddings/lib/python3.9/site-packages/azureml/rag/tasks/update_acs.py&quot;, line 131, in create_search_index_sdk
    if acs_config[&quot;index_name&quot;] not in index_client.list_index_names():
  File &quot;/azureml-envs/rag-embeddings/lib/python3.9/site-packages/azure/core/paging.py&quot;, line 123, in __next__
    return next(self._page_iterator)
  File &quot;/azureml-envs/rag-embeddings/lib/python3.9/site-packages/azure/core/paging.py&quot;, line 75, in __next__
    self._response = self._get_next(self.continuation_token)
  File &quot;/azureml-envs/rag-embeddings/lib/python3.9/site-packages/azure/search/documents/indexes/_generated/operations/_indexes_operations.py&quot;, line 502, in get_next
    raise HttpResponseError(response=response, model=error)
azure.core.exceptions.HttpResponseError: Operation returned an invalid status 'Forbidden'
 (update_acs.py:467)
[2024-10-09 16:24:22] ERROR    azureml.rag.update_acs.update_acs - ActivityCompleted: Activity=update_acs, HowEnded=Failure, Duration=14317.24 [ms], Exception=HttpResponseError (activity.py:127)
Traceback (most recent call last):
  File &quot;/azureml-envs/rag-embeddings/lib/python3.9/runpy.py&quot;, line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/azureml-envs/rag-embeddings/lib/python3.9/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
</code></pre>
<pre><code>[2024-10-09 16:24:10] INFO     azureml.rag.connections - Using User Identity for authentication. (connections.py:265)
[2024-10-09 16:24:10] INFO     azureml.rag.update_acs - Using Index fields: {
  &quot;content&quot;: &quot;content&quot;,
  &quot;url&quot;: &quot;url&quot;,
  &quot;filename&quot;: &quot;filepath&quot;,
  &quot;title&quot;: &quot;title&quot;,
  &quot;metadata&quot;: &quot;meta_json_string&quot;
} (update_acs.py:309)
[2024-10-09 16:24:10] INFO     azureml.rag.update_acs - Ensuring search index shy-needle-jv648s7dcq99 exists (update_acs.py:127)
_AzureMLOnBehalfOfCredential.get_token succeeded
[2024-10-09 16:24:15] ERROR    azureml.rag.update_acs.update_acs - ActivityCompleted: Activity=update_acs, HowEnded=Failure, Duration=5516.44 [ms], Exception=HttpResponseError (activity.py:127)
[2024-10-09 16:24:16] ERROR    azureml.rag.update_acs - Failed to update ACS index (update_acs.py:429)
[2024-10-09 16:24:17] ERROR    azureml.rag.update_acs.update_acs - ServiceError: intepreted error = Rag system error, original error = Operation returned an invalid status 'Forbidden' (exceptions.py:124)
[2024-10-09 16:24:22] ERROR    azureml.rag.update_acs.update_acs - update_acs failed with exception: Traceback (most recent call last):
  File &quot;/azureml-envs/rag-embeddings/lib/python3.9/site-packages/azureml/rag/tasks/update_acs.py&quot;, line 465, in main_wrapper
    map_exceptions(main, activity_logger, args, logger, activity_logger)
  File &quot;/azureml-envs/rag-embeddings/lib/python3.9/site-packages/azureml/rag/utils/exceptions.py&quot;, line 126, in map_exceptions
    raise e
  File &quot;/azureml-envs/rag-embeddings/lib/python3.9/site-packages/azureml/rag/utils/exceptions.py&quot;, line 118, in map_exceptions
    return func(*func_args, **kwargs)
</code></pre>
<p>I tried adding different access rights (roles) but it didn't help</p>
","2024-10-10 09:08:03","0","Question"
"79072007","79070131","","<p>The problem is the missing embedding model property. You only have the chat model configured.</p>
<p>Example:</p>
<pre><code>ai:
ollama:
  embedding:
    model: llama3.2
  chat:
    options:
      model: llama3.2
      temperature: 0.5
vectorstore:
  pgvector:
    index-type: hnsw
    distance-type: cosine_distance
    dimensions: 1536
</code></pre>
","2024-10-09 20:42:09","3","Answer"
"79070131","","Mistral Model not found issue in Spring AI","<p>I am exploring Spring AI. I created a RAG application using PgVector store but when I hit the endpoint it's showing me the below error:</p>
<pre><code>Not Found - {&quot;error&quot;:&quot;model \&quot;mistral\&quot; not found, try pulling it first&quot;}
</code></pre>
<p>Even though I have mentioned in the <code>application.properties</code> file that I want to use llama3.2:1b model but still it's looking for mistral I don't know why.</p>
<p>Below is my controller:</p>
<pre><code>@GetMapping(&quot;/ai/generate&quot;)
    public Map&lt;String,String&gt; generate(@RequestParam(value = &quot;message&quot;, defaultValue = &quot;Tell me a joke&quot;) String message) {
        List&lt;Document&gt; similarDocuments = vectorStore.similaritySearch(SearchRequest.query(message).withTopK(2));
        List&lt;String&gt; contentList = similarDocuments.stream().map(Document::getContent).toList();
        PromptTemplate promptTemplate = new PromptTemplate(prompt);
        Map&lt;String, Object&gt; promptParameters = new HashMap&lt;&gt;();
        promptParameters.put(&quot;input&quot;, message);
        promptParameters.put(&quot;documents&quot;, String.join(&quot;\n&quot;, contentList));
        Prompt prompt =promptTemplate.create(promptParameters);
        return Map.of(&quot;generation&quot;, chatModel.call(prompt).getResult().getOutput().getContent());
    }
</code></pre>
<p>application.properties file:</p>
<pre><code>spring.application.name=codeAI

spring.main.allow-bean-definition-overriding=true

spring.datasource.url=jdbc:postgresql://localhost:5432/postgres
spring.datasource.username=postgres
spring.datasource.password=1

spring.ai.vectorstore.pgvector.index-type=HNSW
spring.ai.vectorstore.pgvector.distance-type=COSINE_DISTANCE
spring.ai.vectorstore.pgvector.dimensions=1536

spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama3.2:1b
spring.ai.ollama.chat.options.temperature=0.7
</code></pre>
<p>I searched regarding this on interent but didn't find any solution. I am not getting why it's fetching mistral model even I have mentioned that I want to use the llama3.2:1b model in the <code>application.properties</code> file.</p>
","2024-10-09 12:02:55","5","Question"
"79068298","","valueError: Supplied state dict for layers does not contain `bitsandbytes__*` and possibly other `quantized_stats`(when load saved quantized model)","<p>We are trying to deploy a quantized Llama 3.1 70B model(from Huggingface, using bitsandbytes), quantizing part works fine as we check the model memory which is correct and also test getting predictions for the model, which is also correct, the problem is: after saving the quantized model and then loading it we get</p>
<blockquote>
<p>valueError: Supplied state dict for layers.0.mlp.down_proj.weight does
not contain <code>bitsandbytes__*</code> and possibly other <code>quantized_stats</code>
components</p>
</blockquote>
<p>What we do is:</p>
<ul>
<li>Save the quantized model using the usual save_pretrained(save_dir)</li>
<li>Try to load the model using AutoModel.from_pretrained, passing the save_dir and the same quantization_config used when creating the model.</li>
</ul>
<p>Here is the code:</p>
<pre><code>model_id = &quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;


cache_dir = &quot;/home/ec2-user/SageMaker/huggingface_cache&quot;

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model_4bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=&quot;auto&quot;,
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config,
    low_cpu_mem_usage=True,
    offload_folder=&quot;offload&quot;,
    offload_state_dict=True,
    cache_dir=cache_dir
)

tokenizer = AutoTokenizer.from_pretrained(model_id,cache_dir=cache_dir)

pt_save_directory = &quot;test_directory&quot;
tokenizer.save_pretrained(pt_save_directory,)
model_4bit.save_pretrained(pt_save_directory)
## test load it

loaded_model = AutoModel.from_pretrained(pt_save_directory,
                                     quantization_config=quantization_config
                                     )
</code></pre>
","2024-10-09 02:25:28","1","Question"
"79067783","79064412","","<p>Yes you need a collection with that name first that has the matching schema that you want to load into.</p>
","2024-10-08 21:06:59","0","Answer"
"79067079","79052117","","<p>I faced this issue with my studio space on SageMaker, too.
<br>This is a compatibility issue when using <code>sentencepiece</code> <strong>v0.2.0</strong> with the latest <code>transformers</code> version.</p>
<p>I solved this problem by downgrading <code>sentencepiece</code> to <strong>0.1.99</strong>.</p>
","2024-10-08 17:04:51","3","Answer"
"79064412","","`do_bulk_insert` Requires Existing Collection - MilvusException: collection not found","<p>I'm attempting to use <code>do_bulk_insert</code> to create a new collection but I'm encountering an error indicating that the collection does not exist. The error message states:</p>
<pre><code>MilvusException: (code=100, message=collection not 
found[database=default][collection=a552b9ad4_adb1_47e0_9e98_33d913b52757])
</code></pre>
<p>Is it necessary to create an empty collection with a schema beforehand to use <code>do_bulk_insert</code>, or was it possible to create collections directly with this method in a previous version? I successfully used this feature a few weeks ago without this issue.</p>
","2024-10-08 05:31:13","2","Question"
"79062412","78999845","","<p>You can navigate into the PyTorch source code for <code>torch.nn.Transformer</code>, where the attention is implemented through the <code>MultiheadAttention</code> module.
<a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention</a></p>
<p>In the forward pass, the <code>MultiheadAttention</code> module calls the <code>multi_head_attention_forward</code> function. This function handles several operations, such as the QKVO linear projections and the attention mechanism itself.</p>
<p>The attention calculation can be found at the following line in the PyTorch codebase:
<a href=""https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py#L6238"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py#L6238</a></p>
<pre><code>B, Nt, E = q.shape
q_scaled = q * math.sqrt(1.0 / float(E))

assert not (
    is_causal and attn_mask is None
), &quot;FIXME: is_causal not implemented for need_weights&quot;

if attn_mask is not None:
    attn_output_weights = torch.baddbmm(
        attn_mask, q_scaled, k.transpose(-2, -1)
    )
else:
    attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
attn_output_weights = softmax(attn_output_weights, dim=-1)
if dropout_p &gt; 0.0:
    attn_output_weights = dropout(attn_output_weights, p=dropout_p)

attn_output = torch.bmm(attn_output_weights, v)

attn_output = (
    attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
)
attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))

# optionally average attention weights over heads
attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
if average_attn_weights:
    attn_output_weights = attn_output_weights.mean(dim=1)

if not is_batched:
    # squeeze the output if input was unbatched
    attn_output = attn_output.squeeze(1)
    attn_output_weights = attn_output_weights.squeeze(0)
return attn_output, attn_output_weights
</code></pre>
<p>This function returns both the O-projection (output) and the attention scores. Therefore, if you need to modify the function's return behavior, you can re-implement <code>multi_head_attention_forward</code> to customize the final return value, including the attention scores (<code>attn_output_weights</code>) as needed.</p>
","2024-10-07 14:47:54","1","Answer"
"79053638","79045085","","<p>I think this is a better practice and it works properply.</p>
<pre><code>import streamlit as st
import mysql.connector
from mysql.connector import Error
import streamlit_authenticator as stauth
from Streamlit_app import run_app
from Streamlit_signup import sign_up


def login():
    conn = create_connection()
    credentials = format_credentials(fetch_credentials(conn))

    if credentials:
        authenticator = stauth.Authenticate(
            credentials=credentials,
            cookie_name=&quot;auth&quot;,
            cookie_key=&quot;my_key&quot;,
            cookie_expiry_days=3,
        )

        name, auth_status, username = authenticator.login()

        if auth_status is None:
            st.warning(&quot;Please enter your credentials&quot;)
            return auth_status, authenticator

        if auth_status is False:
            st.error(&quot;Username or Password is incorrect&quot;)
            return auth_status, authenticator

        if auth_status:
            user_info = credentials['usernames'][username]
            first_name = user_info[&quot;name&quot;]
            last_name = user_info[&quot;last_name&quot;]
            date_of_birth = user_info[&quot;date_of_birth&quot;]

            st.sidebar.subheader(f'Welcome {first_name} {last_name}!')

            chat_dir = &quot;YOUT_CHAT_DIR&quot;
            run_app(chat_dir)

            return auth_status, authenticator
    else:
        st.error(&quot;No users found in the database&quot;)

    conn.close()


def logout(authenticator):
    authenticator.logout('Log Out', 'sidebar')
    st.session_state['auth_status'] = None


if 'page' not in st.session_state:
    st.session_state['page'] = 'login'

if 'auth_status' not in st.session_state:
    st.session_state['auth_status'] = None

if st.session_state['page'] == 'login':

    auth_status, authenticator = login()

    if auth_status:
        st.session_state['auth_status'] = auth_status
        logout(authenticator)
    else:
        if st.sidebar.button(&quot;Sign up&quot;):
            st.session_state['page'] = 'sign_up'
            st.rerun()

elif st.session_state['page'] == 'sign_up':
    sign_up()

    if st.button(&quot;Back to Login&quot;):
        st.session_state['page'] = 'login'
        st.rerun()
</code></pre>
","2024-10-04 09:32:40","0","Answer"
"79052117","","huggingface model inference: ERROR: Flag 'minloglevel' was defined more than once (in files 'src/error.cc' and ..)","<p>I'm trying to use llama 3.1 70b from huggingface(end goal is to quantize it and deploy it in amazon <code>sagemaker</code>), but I'm facing:</p>
<blockquote>
<p>ERROR: Flag 'minloglevel' was defined more than once (in files
'src/error.cc' and
'home/conda/feedstock_root/build_artifacts/abseil-split_1720857154496/work/absl/log/flags.cc').</p>
</blockquote>
<p>The code is super simple (quantization not applied yet), but I cannot make it work (originally I was using a notebook and the kernel died, after exporting the code to python file and running it in the command line I found the error mentioned previously).</p>
<pre><code>import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer
model_id = &quot;meta-llama/Llama-3.1-8B&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
model_id,
device_map=&quot;auto&quot;
)
input_text = &quot;&quot;&quot;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Translate the following English text to French:
'Hello, how are you?'&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;&quot;&quot;&quot;
input_ids = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)
print(input_ids)
output = model.generate(**input_ids, max_new_tokens=10)
</code></pre>
<p>I have tried both the 70b and 8b versions, but none worked (so I know it is not something specific to 70). I have a feeling that this is related to package conflicts or similar and not necessarily the models.</p>
","2024-10-03 20:46:31","0","Question"
"79051999","79051860","","<p>Make sure conan and all the pre-requisites are installed</p>
<p><a href=""https://github.com/milvus-io/milvus/discussions/35614"" rel=""nofollow noreferrer"">https://github.com/milvus-io/milvus/discussions/35614</a></p>
<p>Mac is sometimes picky on some of the build tools.</p>
","2024-10-03 19:52:35","1","Answer"
"79051860","","Building Milvus on Mac M3: Errors Encountered During Distribution Setup","<p>I am attempting to customize and build Milvus for distributed use on my Apple M3 system with 16GB of RAM. I want to create a custom Milvus image and use milvus-operator to start the system. However, I am uncertain if I need to modify milvus-operator due to its relationship with the official Docker repository.</p>
<p>While following the instructions in <code>milvus/DEVELOPMENT.md</code>, I've encountered issues during the build process. Here are my steps:</p>
<ol>
<li>Building Milvus with Docker:
<ul>
<li><code>./scripts/devcontainer.sh up</code></li>
<li><code>docker exec -ti milvus_builder_1 bash</code></li>
<li><code>make milvus</code></li>
</ul>
</li>
</ol>
<p>This resulted in the following error:</p>
<pre><code>CMake Error: Generator: execution of make failed. Make command was: /usr/bin/gmake -f Makefile
CMake Error at /usr/local/share/cmake-3.27/Modules/FetchContent.cmake:1662 (message):
  Build step for knowhere failed: 1
</code></pre>
<ol start=""2"">
<li>Building Milvus on a local OS/shell environment:
<ul>
<li><code>./scripts/install_deps.sh</code></li>
<li><code>make</code></li>
</ul>
</li>
</ol>
<p>This led to another error:</p>
<pre><code>error: unknown type name 'AuthorizationRef'
</code></pre>
<p>Could you please assist me in resolving these build issues?</p>
","2024-10-03 18:57:03","1","Question"
"79047152","79019184","","<p>I experienced this today as well. response status was 200, none of the reasons were 'filtered': True in the content_filter_response, but finish_reason was 'content_filter'. I got charged for 1768 completion_tokens but got nothing.</p>
<p>I am using gpt-4o 2024-08-06 with the newer response_format json_schema for structured json response.</p>
<p>When I submit the same query using gpt-4o 2024-05-13 and the older tools method of structuring the response, I get content?</p>
<p>When I submit the same query using gpt-4o 2024-08-06 and the older tools method of structuring the response I also get content.</p>
<p>This is very confusing, and I do not understand why I am being charged.</p>
","2024-10-02 14:12:32","0","Answer"
"79045085","","Issue with login widget in streamlit","<p>I am trying to create a login system to a stramlit app, but I have some issues with the logic. I have a python file where is the login logic and another one where is the sytem's logic. To be more specific I want to move from login screen to dashboard when click 'login' and from dashboard back to login when click 'logout'</p>
<p>I tried to seperate authedicator.logout() from the login() function of the first python file and call it to the other file inside st.session_state['page'] = 'dashboard' block. Also tried some other things like call run_app() inside st.session_state['page'] = 'dashbpard' block but I don't manage to achieve this. What I want is when I click on 'Log Out' to escape st.session_state['page'] = 'dashboard' block and redirect to st.session_state['page'] = 'login' block.</p>
<p>The first python file with login:</p>
<pre><code>import streamlit as st
import mysql.connector
from mysql.connector import Error
import streamlit_authenticator as stauth
from Streamlit_app import run_app

### def create_connetion() and def fetch_credentials() first ###

def login():
    conn = create_connection()
    credentials = format_credentials(fetch_credentials(conn))

    if credentials:
        authenticator = stauth.Authenticate(
            credentials=credentials,
            cookie_name=&quot;auth&quot;,
            cookie_key=&quot;my_key&quot;,
            cookie_expiry_days=3,
        )

        name, auth_status, username = authenticator.login()

        if auth_status is None:
            st.warning(&quot;Please enter your credentials&quot;)
            return auth_status

        if auth_status is False:
            st.error(&quot;Username or Password is incorrect&quot;)
            return auth_status

        if auth_status:
            user_info = credentials['usernames'][username]
            first_name = user_info[&quot;name&quot;]
            last_name = user_info[&quot;last_name&quot;]
            date_of_birth = user_info[&quot;date_of_birth&quot;]

            st.sidebar.subheader(f'Welcome {first_name} {last_name}!')

            chat_dir = &quot;your_chat_dir&quot;
            run_app(chat_dir)

            authenticator.logout('Log Out', 'sidebar')
            return auth_status
    else:
        st.error(&quot;No users found in the database&quot;)

    conn.close()


#st.title('Login System')

#login()
</code></pre>
<p>The second python file with the pages:</p>
<pre><code>import streamlit as st
from Streamlit_signup import sign_up
from Streamlit_login import login


if 'page' not in st.session_state:
    st.session_state['page'] = 'login'

if 'auth_status' not in st.session_state:
    st.session_state['auth_status'] = None

if st.session_state['page'] == 'login':
    st.title('AI Chatbot')

    col_1, col_2 = st.columns([14, 2])
    with col_2:
        if st.button(&quot;:green[Sign up]&quot;):
            st.session_state['page'] = 'sign_up'
            st.rerun()

    auth_status = login()
    st.write(&quot;auth status: &quot; + str(auth_status))

    if auth_status:
        st.write(&quot;auth status in dashboard: &quot; + str(auth_status))
        st.session_state['page'] = 'dashboard'
        st.session_state['auth_status'] = auth_status
        st.rerun()

elif st.session_state['page'] == 'sign_up':
    st.title(&quot;Sign Up Form&quot;)
    sign_up()

    if st.button(&quot;Back to Login&quot;):
        st.session_state['page'] = 'login'
        st.session_state['auth_status'] = False
        st.rerun()

elif st.session_state['page'] == 'dashboard':
    st.title(&quot;Dashboard&quot;)
    st.write(st.session_state['auth_status'])
    login()
    st.session_state['page'] = 'login'

</code></pre>
<p>Some pictures to help:</p>
<ol>
<li><p><a href=""https://i.sstatic.net/A2KMCpz8.png"" rel=""nofollow noreferrer"">Login page</a></p>
</li>
<li><p><a href=""https://i.sstatic.net/LhWiZ8fd.png"" rel=""nofollow noreferrer"">Dashboard</a></p>
</li>
<li><p><a href=""https://i.sstatic.net/QS4jMXCn.png"" rel=""nofollow noreferrer"">Remains on dashboard after logout</a></p>
</li>
</ol>
","2024-10-02 00:56:37","0","Question"
"79043906","78634762","","<p>As no imports or library versions were provided, I am assuming you used <a href=""https://api.python.langchain.com/en/latest/_modules/langchain_community/llms/bedrock.html#Bedrock"" rel=""nofollow noreferrer"">Bedrock from the langchain_community</a> given the assignment of <code>llm</code> to <code>Bedrock</code>. The <code>Bedrock</code> class does not contain the <code>bind_tools</code> method and thus is not being found.</p>
<p>You most likely are looking for <a href=""https://api.python.langchain.com/en/latest/_modules/langchain_aws/chat_models/bedrock.html#ChatBedrock"" rel=""nofollow noreferrer""><code>ChatBedrock</code></a> which contains the bind_tools method.</p>
","2024-10-01 15:59:57","0","Answer"
"79019184","","Azure Content Filter returning content=None but no filter was triggered","<p>I'm encountering an issue with the Azure OpenAI content filter (for responses from gpt-4o) where the content is returned as None, but all the content filters (hate, self-harm, sexual, violence) are evaluated as False. This is confusing because no content filter appears to be triggered, yet the finish_reason is 'content_filter'.</p>
<p>Here's the response I'm getting:</p>
<pre><code>
    finish_reason='content_filter', 
    index=0, 
    message=ChatCompletionMessage(
        content=None, 
        role='assistant', 
        function_call=None, 
        tool_calls=None
    ), 
    content_filter_results={
        'hate': {'filtered': False, 'severity': 'safe'}, 
        'self_harm': {'filtered': False, 'severity': 'safe'}, 
        'sexual': {'filtered': False, 'severity': 'safe'}, 
        'violence': {'filtered': False, 'severity': 'safe'}
    }

</code></pre>
<p>It seems like the content is being filtered, but all severity levels are marked as 'safe', and no individual filter is being triggered. Has anyone else encountered this issue? Any ideas on why the content would be None if no filters were actually triggered?</p>
<p>In my task I'm posting as input some email messages to the prompt and sometimes removing a link or other specific content helps, but I haven't found any consistent global fix for this issue. In some cases, small content changes can get the filter to allow the message, but this feels more like a workaround than a solution.</p>
","2024-09-24 14:51:04","2","Question"
"79019062","79018974","","<p>You need to use the ingestion endpoint: <a href=""https://api.reference.langfuse.com/#post-/api/public/ingestion"" rel=""nofollow noreferrer"">https://api.reference.langfuse.com/#post-/api/public/ingestion</a></p>
<p>You can also download the OpenAPI spec for types at the very top of the reference.</p>
","2024-09-24 14:24:01","2","Answer"
"79018974","","How to create Langfuse Trace via .Net code","<p>I am developing an AI solution which combine of .Net and Python app. The reason for Python app is mostly to communicate with OpenAI and Langfuse. I want to cut off the Python app to make solution simpler and easier to deploy, but couldn't find a way to write Langfuse trace via its <a href=""https://api.reference.langfuse.com/"" rel=""nofollow noreferrer"">API</a>. Does anyone know what is the endpoint and payload to do it?</p>
","2024-09-24 14:02:47","2","Question"
"79016520","79016339","","<p>What OS are you running?   Which CPU?  Any GPU?   How much RAM?   Do you have adequate disk space?</p>
<p>Following <a href=""https://milvus.io/docs/embed-with-bgm-m3.md"" rel=""nofollow noreferrer"">https://milvus.io/docs/embed-with-bgm-m3.md</a></p>
<p>This works for me on Mac M1 architecture with Python 3.10</p>
<pre><code>Embeddings: {'dense': [array([-0.02505938, -0.00142195,  0.04015466, ..., -0.02094925,
        0.02623658,  0.00324097], dtype=float32), array([ 0.0011846 ,  0.00649291, -0.00735767, ..., -0.01446293,
        0.04243685, -0.0179482 ], dtype=float32), array([ 0.0041529 , -0.01014917,  0.00098105, ..., -0.02559665,
        0.08084688,  0.00141646], dtype=float32)], 'sparse': &lt;3x250002 sparse array of type '&lt;class 'numpy.float32'&gt;'
    with 43 stored elements in Compressed Sparse Row format&gt;}
Dense document dim: 1024 (1024,)
Sparse document dim: 250002 (1, 250002)
</code></pre>
","2024-09-23 23:12:56","0","Answer"
"79016339","","Error Accessing Sparse Embedding in Milvus Hybrid Search","<p>I am following the hybrid search tutorial using BGE-M3 embeddings from Milvus, but I encounter an issue with this line of code: <code>sparse_results = sparse_search(col, query_embeddings['sparse'][0])</code>. It throws a <code>NotImplementedError</code> indicating that 1D sparse slices are not implemented.</p>
<p>I am using the following versions: milvus-lite==2.4.9, milvus-model==0.2.3, python=3.10.14. What could be causing this issue?</p>
","2024-09-23 21:25:05","0","Question"
"79014201","79011882","","<p>The issue you're facing comes from trying to use <code>os.environ</code> incorrectly. The <code>os.environ</code> dictionary is designed to access environment variables, which are stored as key-value pairs. However, it seems you're passing an actual URL and deployment name, treating them as if they were environment variable keys, which they are not.</p>
<p>If you're passing your Azure endpoint and deployment information directly, you should use them as plain strings rather than trying to fetch them from <code>os.environ</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from langchain_openai import AzureChatOpenAI


model = AzureChatOpenAI(
    azure_endpoint=&quot;https://testing123.cognitiveservices.azure.com/&quot;,  
    azure_deployment=&quot;testing123&quot;,  
    openai_api_version=&quot;2021-04-30&quot;  
)
</code></pre>
<p>If you prefer to store the values as environment variables, you need to set them first and then use them in your code.</p>
<p>First, set the environment variables:</p>
<pre class=""lang-py prettyprint-override""><code>import os

os.environ[&quot;AZURE_OPENAI_API_KEY&quot;] = &quot;your_api_key_here&quot;
os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;] = &quot;https://testing123.cognitiveservices.azure.com/&quot;
os.environ[&quot;AZURE_OPENAI_DEPLOYMENT&quot;] = &quot;testing123&quot;
os.environ[&quot;AZURE_OPENAI_API_VERSION&quot;] = &quot;2021-04-30&quot;
</code></pre>
<p>Then, access these variables in your code:</p>
<pre class=""lang-py prettyprint-override""><code>from langchain_openai import AzureChatOpenAI


model = AzureChatOpenAI(
    azure_endpoint=os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;],
    azure_deployment=os.environ[&quot;AZURE_OPENAI_DEPLOYMENT&quot;],
    openai_api_version=os.environ[&quot;AZURE_OPENAI_API_VERSION&quot;]
)
</code></pre>
<p>When using <code>os.environ</code>, make sure the keys refer to actual environment variable names that you’ve set, not the endpoint URL or deployment name directly unless you've saved them there. If you're not using environment variables, you can simply pass the values as strings when initializing the model.</p>
","2024-09-23 10:21:37","0","Answer"
"79012053","79011882","","<p>The errors says <code>KeyError</code> so let's fix the key. Remove those scape <code>\</code> and it should work.</p>
<p>Also looks like you are not accessing the environment vars below so remove the <code>os.environ</code> and pass the string directly.</p>
<pre class=""lang-py prettyprint-override""><code>import getpass
import os

os.environ[&quot;AZURE_OPENAI_API_KEY&quot;] = &quot;d957dadfasdfafdsfsafasfasfsafasf99f4b7&quot;

from langchain_openai import AzureChatOpenAI

model = AzureChatOpenAI(
  azure_endpoint=&quot;https://testing123.cognitiveservices.azure.com/&quot;,
  azure_deployment=&quot;testing123&quot;,
  openai_api_version=&quot;2021-04-30&quot;,
)
</code></pre>
","2024-09-22 16:02:43","0","Answer"
"79011882","","Encountered keyerror when run LangChain program with Azure API","<p>I encountered error when I run below codes (extracted from <a href=""https://python.langchain.com/docs/tutorials/llm_chain/#using-language-models"" rel=""nofollow noreferrer"">https://python.langchain.com/docs/tutorials/llm_chain/#using-language-models</a>) in Jupyter Notebook:</p>
<pre><code>import getpass
import os

os.environ\[&quot;AZURE_OPENAI_API_KEY&quot;\] = &quot;key&quot;

from langchain_openai import AzureChatOpenAI

model = AzureChatOpenAI(
azure_endpoint=os.environ\[&quot;https://testing123.cognitiveservices.azure.com/&quot;\],
azure_deployment=os.environ\[&quot;testing123&quot;\],
openai_api_version=os.environ\[&quot;2021-04-30&quot;\],
)
</code></pre>
<p>Error I got after run:</p>
<pre><code>KeyError                                  Traceback (most recent call last)
Cell In\[16\], line 10
5 os.environ\[&quot;AZURE_OPENAI_API_KEY&quot;\] = &quot;key&quot;
7 from langchain_openai import AzureChatOpenAI
9 model = AzureChatOpenAI(
\---\&gt; 10     azure_endpoint=os.environ\[&quot;https://testing123.cognitiveservices.azure.com/&quot;\],
11     azure_deployment=os.environ\[&quot;testing123&quot;\],
12     openai_api_version=os.environ\[&quot;2021-04-30&quot;\],
13 )

File \&lt;frozen os\&gt;:714, in __getitem__(self, key)

KeyError: 'https://testing123.cognitiveservices.azure.com/'
</code></pre>
<p>I have checked my Azure portal and end-point is exactly what I copied from. Don't understand why key error existed.</p>
","2024-09-22 14:36:02","0","Question"
"79009969","79000634","","<p>The calcDistance interface has been removed from the milvus server side. Most of the SDK have removed this interface except the Go SDK, we will remove it later.</p>
<p>You can use python numpy to calculate distance between vectors easily:
<a href=""https://www.geeksforgeeks.org/calculate-the-euclidean-distance-using-numpy/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/calculate-the-euclidean-distance-using-numpy/</a></p>
","2024-09-21 16:00:46","0","Answer"
"79002519","78999652","","<p><strong>ChatGPT</strong> suggested</p>
<blockquote>
<p>You can try using Python 3.10 or 3.11 to see if the issue is resolved</p>
</blockquote>
<p>Since my Python version was 3.12.5, I downgraded to 3.11.9 and re-ran <code>pip install</code>. This successfully resolved the problem.</p>
<p>I've noticed that <strong>Gemini 1.5 Flash</strong> only suggests me to update <strong>Rust and Cargo</strong>, while <strong>GPT-4o mini</strong> additionally mentions the issue of <strong>Python version</strong>. I have been using Gemini before, it seems I should compare these two models more in the future.</p>
","2024-09-19 12:27:51","0","Answer"
"79000634","","CalcDistance Deprecated: How to Calculate Vector Distance in Milvus SDK?","<p>I encountered an error when trying to calculate the distance between two vectors using the <code>CalcDistance()</code>:</p>
<pre><code>service unavailable: CalcDistance deprecated.
</code></pre>
<p>I am using milvus-sdk-go version v2.4.1 and standalone Milvus version v2.4.6.</p>
<p>Here is the code snippet:</p>
<pre><code>go
vector1 := entity.NewColumnFloatVector(&quot;vector&quot;, 512, v1)
vectortwo := entity.NewColumnFloatVector(&quot;vector&quot;, 512, v2)
distance, err := mc.CalcDistance(ctx, param.ColName, []string{}, entity.L2, vector1, vector2)
if err != nil {
    slog.Error(&quot;calc distance error&quot;, slog.Any(&quot;err&quot;, err))
    return 0, err
}
</code></pre>
<p>Since the <code>CalcDistance()</code> function is deprecated, what is the recommended way to calculate the distance between two vectors using milvus-sdk-go? Are there alternative methods available?</p>
","2024-09-19 02:00:22","0","Question"
"78999845","","Get the attention scores of a pretrained transformer in pytorch","<p>I've been trying to look at the attention scores of a pretrained transformer when I pass specific data in. It's specifically a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"" rel=""nofollow noreferrer"">Pytorch Transformer</a>. I've tried using <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html"" rel=""nofollow noreferrer"">forward hooks</a>, but I'm only able to get the final output of attention modules when what I want is NxN matrices of attention scores (softmax(QxK). I also would really prefer to do this via pytorch code and not use outside tools such as BertViz.</p>
<p>Does anyone know if there's a way to do this?</p>
","2024-09-18 19:06:33","3","Question"
"78999652","","Error during the compilation of the tokenizers package when trying to install transformers 4.27","<ul>
<li><p>The use of <code>chatglm-6b</code> requires the installation of <code>transformers==4.27.1</code>.</p>
</li>
<li><p>I'm trying to install <code>transformers==4.27.1</code>, but I'm encountering an error during the compilation of the <code>tokenizers</code> package, which prevents the successful building of the wheel file. The error message indicates that the <code>cargo rustc</code> command failed and returned code 101.
Here is the complete error message:</p>
</li>
</ul>
<pre class=""lang-bash prettyprint-override""><code>pip install -v transformers==4.27.1

....

error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib -- -C 'link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/tokenizers.cpython-312-darwin.so'` failed with code 101
  error: subprocess-exited-with-error
  
  × Building wheel for tokenizers (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: /Users/dragonfang/****/venv_agi/bin/python3.12 /Users/dragonfang/****/venv_agi/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /var/folders/ll/9dtz3vg150vfv8t75ppq_nr00000gn/T/tmpymy6ke0d
  cwd: /private/var/folders/ll/9dtz3vg150vfv8t75ppq_nr00000gn/T/pip-install-tz2dgt67/tokenizers_11ac58d2069c4ec1985eae0d4528f0ec
  Building wheel for tokenizers (pyproject.toml) ... error
  ERROR: Failed building wheel for tokenizers
Failed to build tokenizers
ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)
</code></pre>
<p>How to resolve this issue?</p>
<hr />
<p>Supplementary：</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/77265938/cargo-rustc-failed-with-code-101-could-not-build-wheels-for-tokenizers-which"">cargo rustc failed with code 101</a></li>
<li><a href=""https://stackoverflow.com/questions/78437953/how-can-i-solve-this-problem-in-chatglm-6b-attributeerror-chatglmtokenizer-o"">'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'</a></li>
</ul>
<p>I have tried some methods from the two links above, but none of them have solved my problem.</p>
<ul>
<li>The <code>rustc 1.72.1</code> version is too low, package clap_lex v0.7.2 cannot be built because it requires rustc 1.74 or newer.</li>
<li><code>transformers==4.34.0</code> no longer needs to install the Rust compiler, but the error occurs: <code>AttributeError: 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'</code>.</li>
<li><code>transformers==4.33.2</code> requires the installation of the Rust compiler, and the error is: <code>cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib -- -C 'link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/tokenizers.cpython-312-darwin.so' failed with code 101.</code></li>
</ul>
","2024-09-18 18:13:50","0","Question"
"78998355","78763327","","<p><a href=""https://medium.com/@manish.thota1999/an-experiment-to-unlock-ollamas-potential-video-question-answering-e2b4d1bfb5ba"" rel=""nofollow noreferrer"">https://medium.com/@manish.thota1999/an-experiment-to-unlock-ollamas-potential-video-question-answering-e2b4d1bfb5ba</a></p>
<pre><code>python3 /home/myles/llama.cpp/examples/llava/llava_surgery_v2.py -C -m /home/myles/llama.cpp/checkpoint-300



import argparse
import os
import json
import re

import torch
import numpy as np
from gguf import *

from transformers import SiglipModel, SiglipProcessor, SiglipVisionModel


TEXT = &quot;clip.text&quot;
VISION = &quot;clip.vision&quot;


def k(raw_key: str, arch: str) -&gt; str:
    return raw_key.format(arch=arch)


def should_skip_tensor(name: str, has_text: bool, has_vision: bool, has_llava: bool) -&gt; bool:
    if name in (
        &quot;logit_scale&quot;,
        &quot;text_model.embeddings.position_ids&quot;,
        &quot;vision_model.embeddings.position_ids&quot;,
    ):
        return True

    if has_llava and name in [&quot;visual_projection.weight&quot;, &quot;vision_model.post_layernorm.weight&quot;, &quot;vision_model.post_layernorm.bias&quot;]:
        return True

    if name.startswith(&quot;v&quot;) and not has_vision:
        return True

    if name.startswith(&quot;t&quot;) and not has_text:
        return True

    return False


def get_tensor_name(name: str) -&gt; str:
    if &quot;projection&quot; in name:
        return name
    if &quot;mm_projector&quot; in name:
        name = name.replace(&quot;model.mm_projector&quot;, &quot;mm&quot;)
        name = re.sub(r'mm\.mlp\.mlp', 'mm.model.mlp', name, count=1)
        name = re.sub(r'mm\.peg\.peg', 'mm.model.peg', name, count=1)
        return name

    return name.replace(&quot;text_model&quot;, &quot;t&quot;).replace(&quot;vision_model&quot;, &quot;v&quot;).replace(&quot;encoder.layers&quot;, &quot;blk&quot;).replace(&quot;embeddings.&quot;, &quot;&quot;).replace(&quot;_proj&quot;, &quot;&quot;).replace(&quot;self_attn.&quot;, &quot;attn_&quot;).replace(&quot;layer_norm&quot;, &quot;ln&quot;).replace(&quot;layernorm&quot;, &quot;ln&quot;).replace(&quot;mlp.fc1&quot;, &quot;ffn_down&quot;).replace(&quot;mlp.fc2&quot;, &quot;ffn_up&quot;).replace(&quot;embedding&quot;, &quot;embd&quot;).replace(&quot;final&quot;, &quot;post&quot;).replace(&quot;layrnorm&quot;, &quot;ln&quot;)


def bytes_to_unicode():
    &quot;&quot;&quot;
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a significant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    &quot;&quot;&quot;
    bs = (
        list(range(ord(&quot;!&quot;), ord(&quot;~&quot;) + 1))
        + list(range(ord(&quot;¡&quot;), ord(&quot;¬&quot;) + 1))
        + list(range(ord(&quot;®&quot;), ord(&quot;ÿ&quot;) + 1))
    )
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))


ap = argparse.ArgumentParser()
ap.add_argument(&quot;-m&quot;, &quot;--model-dir&quot;, help=&quot;Path to model directory cloned from HF Hub&quot;, required=True)
ap.add_argument(&quot;--use-f32&quot;, action=&quot;store_true&quot;, default=False, help=&quot;Use f32 instead of f16&quot;)
ap.add_argument(&quot;--text-only&quot;, action=&quot;store_true&quot;, required=False,
                help=&quot;Save a text-only model. It can't be used to encode images&quot;)
ap.add_argument(&quot;--vision-only&quot;, action=&quot;store_true&quot;, required=False,
                help=&quot;Save a vision-only model. It can't be used to encode texts&quot;)
ap.add_argument(&quot;--clip-model-is-vision&quot;, action=&quot;store_true&quot;, required=False,
                help=&quot;The clip model is a pure vision model (ShareGPT4V vision extract for example)&quot;)
ap.add_argument(&quot;--clip-model-is-openclip&quot;, action=&quot;store_true&quot;, required=False,
                help=&quot;The clip model is from openclip (for ViT-SO400M type))&quot;)
ap.add_argument(&quot;--llava-projector&quot;, help=&quot;Path to llava.projector file. If specified, save an image encoder for LLaVA models.&quot;)
ap.add_argument(&quot;--projector-type&quot;, help=&quot;Type of projector. Possible values: mlp, ldp, ldpv2&quot;, choices=[&quot;mlp&quot;, &quot;ldp&quot;, &quot;ldpv2&quot;], default=&quot;mlp&quot;)
ap.add_argument(&quot;-o&quot;, &quot;--output-dir&quot;, help=&quot;Directory to save GGUF files. Default is the original model directory&quot;, default=None)
# Example --image_mean 0.48145466 0.4578275 0.40821073 --image_std 0.26862954 0.26130258 0.27577711
# Example --image_mean 0.5 0.5 0.5 --image_std 0.5 0.5 0.5
default_image_mean = [0.48145466, 0.4578275, 0.40821073]
default_image_std = [0.26862954, 0.26130258, 0.27577711]
ap.add_argument('--image-mean', type=float, nargs='+', help='Mean of the images for normalization (overrides processor) ', default=None)
ap.add_argument('--image-std', type=float, nargs='+', help='Standard deviation of the images for normalization (overrides processor)', default=None)

# with proper
args = ap.parse_args()


if args.text_only and args.vision_only:
    print(&quot;--text-only and --image-only arguments cannot be specified at the same time.&quot;)
    exit(1)

if args.use_f32:
    print(&quot;WARNING: Weights for the convolution op is always saved in f16, as the convolution op in GGML does not support 32-bit kernel weights yet.&quot;)

# output in the same directory as the model if output_dir is None
dir_model = args.model_dir

if args.clip_model_is_vision or not os.path.exists(dir_model + &quot;/vocab.json&quot;) or args.clip_model_is_openclip:
    vocab = None
    tokens = None
else:
    with open(dir_model + &quot;/vocab.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        vocab = json.load(f)
        tokens = [key for key in vocab]

with open(dir_model + &quot;/config.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
    config = json.load(f)
    if args.clip_model_is_vision:
        v_hparams = config
        t_hparams = None
    else:
        v_hparams = config[&quot;vision_config&quot;]
        t_hparams = config[&quot;text_config&quot;]

# possible data types
#   ftype == 0 -&gt; float32
#   ftype == 1 -&gt; float16
#
# map from ftype to string
ftype_str = [&quot;f32&quot;, &quot;f16&quot;]

ftype = 1
if args.use_f32:
    ftype = 0

if args.clip_model_is_vision or args.clip_model_is_openclip:
    model = SiglipVisionModel.from_pretrained(dir_model)
    processor = None
else:
    model = SiglipModel.from_pretrained(dir_model)
    processor = SiglipProcessor.from_pretrained(dir_model)

fname_middle = None
has_text_encoder = True
has_vision_encoder = True
has_llava_projector = False
if args.text_only:
    fname_middle = &quot;text-&quot;
    has_vision_encoder = False
elif args.llava_projector is not None:
    fname_middle = &quot;mmproj-&quot;
    has_text_encoder = False
    has_llava_projector = True
elif args.vision_only:
    fname_middle = &quot;vision-&quot;
    has_text_encoder = False
else:
    fname_middle = &quot;&quot;

output_dir = args.output_dir if args.output_dir is not None else dir_model
os.makedirs(output_dir, exist_ok=True)
output_prefix = os.path.basename(output_dir).replace(&quot;ggml_&quot;, &quot;&quot;)
fname_out = os.path.join(output_dir, f&quot;{fname_middle}model-{ftype_str[ftype]}.gguf&quot;)
fout = GGUFWriter(path=fname_out, arch=&quot;clip&quot;)

fout.add_bool(&quot;clip.has_text_encoder&quot;, has_text_encoder)
fout.add_bool(&quot;clip.has_vision_encoder&quot;, has_vision_encoder)
fout.add_bool(&quot;clip.has_llava_projector&quot;, has_llava_projector)
fout.add_file_type(ftype)
model_name = config[&quot;_name_or_path&quot;] if &quot;_name_or_path&quot; in config else os.path.basename(dir_model)
fout.add_name(model_name)
if args.text_only:
    fout.add_description(&quot;text-only CLIP model&quot;)
elif args.vision_only and not has_llava_projector:
    fout.add_description(&quot;vision-only CLIP model&quot;)
elif has_llava_projector:
    fout.add_description(&quot;image encoder for LLaVA&quot;)
    # add projector type
    fout.add_string(&quot;clip.projector_type&quot;, args.projector_type)
else:
    fout.add_description(&quot;two-tower CLIP model&quot;)

if has_text_encoder:
    assert t_hparams is not None
    assert tokens is not None
    # text_model hparams
    fout.add_uint32(k(KEY_CONTEXT_LENGTH, TEXT), t_hparams[&quot;max_position_embeddings&quot;])
    fout.add_uint32(k(KEY_EMBEDDING_LENGTH, TEXT), t_hparams[&quot;hidden_size&quot;])
    fout.add_uint32(k(KEY_FEED_FORWARD_LENGTH, TEXT), t_hparams[&quot;intermediate_size&quot;])
    fout.add_uint32(&quot;clip.text.projection_dim&quot;, t_hparams.get(&quot;projection_dim&quot;, config[&quot;projection_dim&quot;]))
    fout.add_uint32(k(KEY_ATTENTION_HEAD_COUNT, TEXT), t_hparams[&quot;num_attention_heads&quot;])
    fout.add_float32(k(KEY_ATTENTION_LAYERNORM_EPS, TEXT), t_hparams[&quot;layer_norm_eps&quot;])
    fout.add_uint32(k(KEY_BLOCK_COUNT, TEXT), t_hparams[&quot;num_hidden_layers&quot;])
    fout.add_token_list(tokens)

if has_vision_encoder:
    # vision_model hparams
    fout.add_uint32(&quot;clip.vision.image_size&quot;, v_hparams[&quot;image_size&quot;])
    fout.add_uint32(&quot;clip.vision.patch_size&quot;, v_hparams[&quot;patch_size&quot;])
    fout.add_uint32(k(KEY_EMBEDDING_LENGTH, VISION), v_hparams[&quot;hidden_size&quot;])
    fout.add_uint32(k(KEY_FEED_FORWARD_LENGTH, VISION), v_hparams[&quot;intermediate_size&quot;])
    fout.add_uint32(&quot;clip.vision.projection_dim&quot;, v_hparams.get(&quot;projection_dim&quot;, config[&quot;projection_dim&quot;]))
    fout.add_uint32(k(KEY_ATTENTION_HEAD_COUNT, VISION), v_hparams[&quot;num_attention_heads&quot;])
    fout.add_float32(k(KEY_ATTENTION_LAYERNORM_EPS, VISION), v_hparams[&quot;layer_norm_eps&quot;])
    block_count = v_hparams[&quot;num_hidden_layers&quot;] - 1 if has_llava_projector else v_hparams[&quot;num_hidden_layers&quot;]
    fout.add_uint32(k(KEY_BLOCK_COUNT, VISION), block_count)
                            #     /**
                            #      &quot;image_grid_pinpoints&quot;: [
                            #         [
                            #         336,
                            #         672
                            #         ],
                            #         [
                            #         672,
                            #         336
                            #         ],
                            #         [
                            #         672,
                            #         672
                            #         ],
                            #         [
                            #         1008,
                            #         336
                            #         ],
                            #         [
                            #         336,
                            #         1008
                            #         ]
                            #     ],
                            #     Flattened:
                            #     [
                            #         336, 672,
                            #         672, 336,
                            #         672, 672,
                            #         1008, 336,
                            #         336, 1008
                            #     ]
                            #  *
                            #  */
    if &quot;image_grid_pinpoints&quot; in v_hparams:
        # flatten it
        image_grid_pinpoints = []
        for pinpoint in v_hparams[&quot;image_grid_pinpoints&quot;]:
            for p in pinpoint:
                image_grid_pinpoints.append(p)
        fout.add_array(&quot;clip.vision.image_grid_pinpoints&quot;, image_grid_pinpoints)
    if &quot;image_crop_resolution&quot; in v_hparams:
        fout.add_uint32(&quot;clip.vision.image_crop_resolution&quot;, v_hparams[&quot;image_crop_resolution&quot;])
    if &quot;image_aspect_ratio&quot; in v_hparams:
        fout.add_string(&quot;clip.vision.image_aspect_ratio&quot;, v_hparams[&quot;image_aspect_ratio&quot;])
    if &quot;image_split_resolution&quot; in v_hparams:
        fout.add_uint32(&quot;clip.vision.image_split_resolution&quot;, v_hparams[&quot;image_split_resolution&quot;])
    if &quot;mm_patch_merge_type&quot; in v_hparams:
        fout.add_string(&quot;clip.vision.mm_patch_merge_type&quot;, v_hparams[&quot;mm_patch_merge_type&quot;])
    if &quot;mm_projector_type&quot; in v_hparams:
        fout.add_string(&quot;clip.vision.mm_projector_type&quot;, v_hparams[&quot;mm_projector_type&quot;])


    if processor is not None:
        image_mean = processor.image_processor.image_mean if args.image_mean is None or args.image_mean == default_image_mean else args.image_mean  # pyright: ignore[reportAttributeAccessIssue]
        image_std = processor.image_processor.image_std if args.image_std is None or args.image_std == default_image_std else args.image_std  # pyright: ignore[reportAttributeAccessIssue]
    else:
        image_mean = args.image_mean if args.image_mean is not None else default_image_mean
        image_std = args.image_std if args.image_std is not None else default_image_std
    fout.add_array(&quot;clip.vision.image_mean&quot;, image_mean)
    fout.add_array(&quot;clip.vision.image_std&quot;, image_std)

use_gelu = v_hparams[&quot;hidden_act&quot;] == &quot;gelu&quot;
fout.add_bool(&quot;clip.use_gelu&quot;, use_gelu)


if has_llava_projector:
    model.vision_model.encoder.layers.pop(-1)  # pyright: ignore[reportAttributeAccessIssue]
    projector = torch.load(args.llava_projector)
    for name, data in projector.items():
        name = get_tensor_name(name)
        # pw and dw conv ndim==4
        if data.ndim == 2 or data.ndim == 4:
            data = data.squeeze().numpy().astype(np.float16)
        else:
            data = data.squeeze().numpy().astype(np.float32)

        fout.add_tensor(name, data)

    print(&quot;Projector tensors added\n&quot;)

state_dict = model.state_dict()  # pyright: ignore[reportAttributeAccessIssue]
for name, data in state_dict.items():
    if should_skip_tensor(name, has_text_encoder, has_vision_encoder, has_llava_projector):
        # we don't need this
        print(f&quot;skipping parameter: {name}&quot;)
        continue

    name = get_tensor_name(name)
    data = data.squeeze().numpy()

    n_dims = len(data.shape)

    # ftype == 0 -&gt; float32, ftype == 1 -&gt; float16
    ftype_cur = 0
    if n_dims == 4:
        print(f&quot;tensor {name} is always saved in f16&quot;)
        data = data.astype(np.float16)
        ftype_cur = 1
    elif ftype == 1:
        if name[-7:] == &quot;.weight&quot; and n_dims == 2:
            print(&quot;  Converting to float16&quot;)
            data = data.astype(np.float16)
            ftype_cur = 1
        else:
            print(&quot;  Converting to float32&quot;)
            data = data.astype(np.float32)
            ftype_cur = 0
    else:
        if data.dtype != np.float32:
            print(&quot;  Converting to float32&quot;)
            data = data.astype(np.float32)
            ftype_cur = 0

    print(f&quot;{name} - {ftype_str[ftype_cur]} - shape = {data.shape}&quot;)
    fout.add_tensor(name, data)


fout.write_header_to_file()
fout.write_kv_data_to_file()
fout.write_tensors_to_file()
fout.close()

print(&quot;Done. Output file: &quot; + fname_out)
</code></pre>
<pre><code> ./llama-llava-cli -m /home/myles/llama.cpp/Meta-Llama-3.1-8B-Instruct/meta-llama-3.1-8B-instruction_f32.gguf --mmproj /home/myles/llama.cpp/vit/mmproj-model-f16.gguf  --image /home/myles/Desktop/extreme_ironing.jpg -c 4096 --n-gpu-layers 33 -p &quot;how many cars are shown in the image&quot; 
</code></pre>
<p>crate a config file json like so</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;google/siglip-so400m-patch14-384&quot;,
  &quot;architectures&quot;: [
    &quot;siglip_vision_model&quot;
  ],
  &quot;attention_dropout&quot;: 0.0,
  &quot;dropout&quot;: 0.0,
  &quot;hidden_act&quot;: &quot;silu&quot;,
  &quot;hidden_size&quot;: 1152,
  &quot;image_size&quot;: 384,
  &quot;initializer_factor&quot;: 1.0,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 4304,
  &quot;layer_norm_eps&quot;: 1e-06,
  &quot;model_type&quot;: &quot;siglip_vision_model&quot;,
  &quot;num_attention_heads&quot;: 16,
  &quot;num_channels&quot;: 3,
  &quot;num_hidden_layers&quot;: 27,
  &quot;patch_size&quot;: 14,
  &quot;projection_dim&quot;: 768,
  &quot;torch_dtype&quot;: &quot;bfloat16&quot;,
  &quot;transformers_version&quot;: &quot;4.45.0.dev0&quot;,
  &quot;image_aspect_ratio&quot;: &quot;anyres_max_9&quot;,
  &quot;image_grid_pinpoints&quot;: [
    [
      384,
      384
    ],
    [
      384,
      768
    ],
    [
      384,
      1152
    ],
    [
      384,
      1536
    ],
    [
      384,
      1920
    ],
    [
      384,
      2304
    ],
    [
      768,
      384
    ],
    [
      768,
      768
    ],
    [
      768,
      1152
    ],
    [
      768,
      1536
    ],
    [
      768,
      1920
    ],
    [
      768,
      2304
    ],
    [
      1152,
      384
    ],
    [
      1152,
      768
    ],
    [
      1152,
      1152
    ],
    [
      1152,
      1536
    ],
    [
      1152,
      1920
    ],
    [
      1152,
      2304
    ],
    [
      1536,
      384
    ],
    [
      1536,
      768
    ],
    [
      1536,
      1152
    ],
    [
      1536,
      1536
    ],
    [
      1536,
      1920
    ],
    [
      1536,
      2304
    ],
    [
      1920,
      384
    ],
    [
      1920,
      768
    ],
    [
      1920,
      1152
    ],
    [
      1920,
      1536
    ],
    [
      1920,
      1920
    ],
    [
      1920,
      2304
    ],
    [
      2304,
      384
    ],
    [
      2304,
      768
    ],
    [
      2304,
      1152
    ],
    [
      2304,
      1536
    ],
    [
      2304,
      1920
    ],
    [
      2304,
      2304
    ]
  ],
  &quot;initializer_range&quot;: 0.02,
  &quot;max_position_embeddings&quot;: 131072,
  &quot;mlp_bias&quot;: false,
  &quot;mm_hidden_size&quot;: 1152,
  &quot;mm_newline_position&quot;: &quot;grid&quot;,
  &quot;mm_patch_merge_type&quot;: &quot;spatial_unpad&quot;,
  &quot;mm_projector_type&quot;: &quot;mlp2x_gelu&quot;,
  &quot;mm_spatial_pool_mode&quot;: &quot;bilinear&quot;,
  &quot;mm_tunable_parts&quot;: &quot;mm_vision_tower,mm_mlp_adapter,mm_language_model&quot;,
  &quot;mm_use_im_patch_token&quot;: false,
  &quot;mm_use_im_start_end&quot;: false,
  &quot;mm_vision_select_feature&quot;: &quot;patch&quot;,
  &quot;mm_vision_select_layer&quot;: -2,
  &quot;mm_vision_tower&quot;: &quot;google/siglip-so400m-patch14-384&quot;,
  &quot;mm_vision_tower_lr&quot;: 2e-06
}
</code></pre>
","2024-09-18 12:59:57","0","Answer"
"78997347","78996575","","<p>You wrote:</p>
<blockquote>
<p>If I run this code I get this output:</p>
<blockquote>
<p>Total number of times the for loop ran: 20</p>
</blockquote>
<p>That is O(edges=5)² then how is the time complexity O(V+E)? Shouldn't it be O(V + E²)?</p>
</blockquote>
<p>It is not clear how you conclude that it is O(edges=5)², as there are 10 edges, not 5. And 20 is not 5².</p>
<p>The code actually visits each edge in <em>both directions</em>, so for a graph with 10 edges, the <code>for</code> loop will make 20 iterations. Note how your <code>add_edge</code> code <em>doubles</em> the number of directed edges so to make the graph undirected, and every list member of the lists in <code>adj</code> corresponds to one iteration of the loop. If you would have a graph with 1000 edges, there would be 2000 iterations.</p>
<p>And O(2E) is the same as O(E) (See <a href=""https://en.wikipedia.org/wiki/Big_O_notation#Multiplication_by_a_constant"" rel=""nofollow noreferrer"">Wikipedia - Big O notation - Multiplication by a constant</a>), and so the overall time complexity is as was expected: O(V+E)</p>
<p>In general, be aware that you cannot derive the time complexity of a program by <em>running</em> it. It is by analysing the code and providing a proof of its complexity.</p>
","2024-09-18 08:55:20","5","Answer"
"78996575","","Depth First Search Time Complexity","<p>Below is Python code for a depth search algorithm.</p>
<pre class=""lang-py prettyprint-override""><code>def add_edge(adj, s, t):
    # Add edge from vertex s to t
    adj[s].append(t)
    # Due to undirected Graph
    adj[t].append(s)
    print('adj add edge', adj)


def dfs_rec(adj, visited, s, loop_count):
    # Mark the current vertex as visited
    visited[s] = True
    print(f'Visited vertex: {s}')

    # Recursively visit all adjacent vertices
    for i in adj[s]:
        loop_count[0] += 1  # Increment the loop counter
        if not visited[i]:
            dfs_rec(adj, visited, i, loop_count)


def dfs(adj, s):
    visited = [False] * len(adj)
    print('Initial visited list:', visited)

    # Create a list to hold loop count as a mutable object
    loop_count = [0]  # This will hold the number of times the loop runs

    # Start DFS recursion
    dfs_rec(adj, visited, s, loop_count)

    # Return the loop count
    return loop_count[0]


if __name__ == &quot;__main__&quot;:
    V = 5

    # Create an adjacency list for the graph
    adj = [[] for _ in range(V)]

    # Define the edges of the graph
    edges = [[1, 2], [1, 0], [2, 0], [2, 3], [2, 4], [1, 3], [1, 4], [3, 4], [0, 3], [0, 4]]

    # Populate the adjacency list with edges
    for e in edges:
        print('Adding edge:', e)
        add_edge(adj, e[0], e[1])

    source = 1
    print(f&quot;\nDFS traversal from source vertex: {source}&quot;)
    total_loops = dfs(adj, source)
    print(f&quot;Total number of times the for loop ran: {total_loops}&quot;)
</code></pre>
<p>The time complexity is given to be O(V+E). But if I run this code I get this output:</p>
<blockquote>
<p>Total number of times the for loop ran: 20</p>
</blockquote>
<p>That is O(edges=5)² then how is the time complexity O(V+E)? Shouldn't it be O(V + E²)?</p>
<p>By trying and actually executing the code, it shows the time complexity to be O(V + E²)... I tried running and finding the time complexity? What is my mistake?</p>
","2024-09-18 04:48:14","2","Question"
"78996069","78996028","","<p>Unless you're using Unsupervised Learning (or regression/forecasting where observations are available after the fact), traditionally you need lots of (expensive to acquire) labelled data to train a model. More recently &quot;few-shot&quot; or &quot;one-shot&quot; learning has emerged which (using a pre-trained model) can learn based on a handful of labelled examples - but you still need labelled data, just not nearly as much.</p>
<p>So your statement &quot;does it mean that it is supposed to be labeled manually&quot; .. &quot;what's the point of training and serving a model then?&quot; doesn't really make sense as you cannot train a model without labelled (at least some) data manually.</p>
<p>Secondly - there's the issue of &quot;data drift&quot;. A model trained on data which has characteristics that change over time (fraud detection in particular because bad actors are always looking for new methods) will degrade in performance, so you need to monitor and retrain using new (labelled) data. Using the fraud detection example - if you notice that the model is missing some new fraud technique you need to find examples and label them, and then retrain the model. Also note, it's highly unlikely that your original dataset will result in 100% accuracy anyway - the expert may get it wrong/miss some examples so the model will always have some uncertainty - especially with something that's rare and hard to define.</p>
","2024-09-17 23:04:10","1","Answer"
"78996028","","How to re-train ML model","<p>I was playing around with AWS Sagemaker, trained a model with some labeled data, deployed it to endpoint and set up Lambda for serving predictions.</p>
<p>All good, but I want to re-train my model regularly, using, say, 1-week historical data.</p>
<p>But my historical data is unlabeled which means it cannot be used for training. How do I label it?</p>
<p>I initially thought that I can just use my model's predictions to label new (unlabeled) data but I have read that this not a good idea because it would simply assure my model about its accuracy, even though it might be far from being accurate.</p>
<p>So where do I get the labels for my historical data?</p>
<p>And if historical data cannot be labeled by model, then does it mean that it is supposed to be labeled manually? In that case, what's the point of training and serving a model then?</p>
<p>As an example, let's take a fraudulent transaction detection. Ok, there is some initial data, labeled manually by someone who knows exactly if the transaction was fraudulent or not, and thus having 100% accuracy.
Is it then supposed to be periodically and manually updated with additional 100%-accurate events?</p>
","2024-09-17 22:44:46","-1","Question"
"78982580","78980639","","<p>I've solved this problem.</p>
<p>The key to the problem I did not show in the question I asked, because at that time I did not realize that <code>bitsandbytes</code> and <code>accelerate</code> library would automatically register <code>pre_forward_hook</code>.</p>
<p>It registered an <code>AlignDeviceHook</code> (maybe) at each forward method, which conflicts with the device control of pytorch lightning. When I removed <code>bitsandbytes</code>, everything worked fine.</p>
","2024-09-13 14:27:25","1","Answer"
"78980639","","Pytorch Lightning places model inputs and model to different devices","<p>I'm using Pytorch-lightning 2.4.0. In the following code snippet, <code>lmm</code> is a class inherited from <code>nn.Module</code> which is a wrapper class huggingface model and processor.</p>
<pre><code>class ICVModel(pl.LightningModule):
    def __init__(self, lmm, icv_encoder: torch.nn.Module) -&gt; None:
        super().__init__()
        self.lmm = lmm
        self.lmm.requires_grad_(False)
        self.icv_encoder = icv_encoder
        self.eos_token = self.lmm.processor.tokenizer.eos_token

    def forward(self, ice_texts, query_texts, answers, images):
        query_answer = [
            query + answer + self.eos_token
            for query, answer in zip(query_texts, answers)
        ]
        query_images = [img[-setting.num_image_in_query :] for img in images]
        query_inputs = self.lmm.process_input(query_answer, query_images)
        query_outputs = self.lmm.model(
            **query_inputs,
            labels=query_inputs[&quot;input_ids&quot;],
        )
</code></pre>
<p>However, a device mismatch error raised at</p>
<pre class=""lang-py prettyprint-override""><code>query_outputs = self.lmm.model(
        **query_inputs,
        labels=query_inputs[&quot;input_ids&quot;],
)
</code></pre>
<p>I printed device of <code>inputs.pixel_values.device</code>, <code>self.device</code>, <code>self.lmm.device</code> outside of <code>lmm.model.forward</code>, then I got</p>
<pre><code>rank[0]: cpu cuda:0 cuda:0
rank[1]: cpu cuda:1 cuda:1
</code></pre>
<p>In Idefics (<code>self.lmm.model</code>) forward process, when I printed <code>inputs.pixel_values.device</code> and <code>self.device</code>, I got</p>
<pre><code>rank[0]: cuda:0 cuda:0
rank[1]: cuda:0 cuda:1
</code></pre>
<p>Besides, I also tried to move <code>pixel_values</code> to correct device, but it still be moved to wrong device in later forward pass.</p>
<p>Error message:</p>
<pre><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)
</code></pre>
","2024-09-13 04:36:41","0","Question"
"78979382","78516192","","<p>Here ComfyUI\custom_nodes\ComfyUI-Frame-Interpolation\ckpts\rife</p>
","2024-09-12 17:45:21","0","Answer"
"78976871","78976058","","<p>When the dataset type is <code>MLTable</code>, you also need to give the <code>MLTable</code> file defining the files to be included and there transformations.</p>
<p>Next, <code>.xlsx</code> is not supported.</p>
<p>Refer <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-mltable?view=azureml-api-2&amp;tabs=cli#supported-file-types"" rel=""nofollow noreferrer"">here</a> for supported files in <code>MLTable</code>.</p>
<p>Below is the sample file you should be having.</p>
<pre><code>$schema: https://azuremlschemas.azureedge.net/latest/MLTable.schema.json 

paths:
    - pattern: ./*.csv

transformations:
  - read_delimited:
      delimiter: &quot;,&quot;
      header: all_files_same_headers
      encoding: utf8
</code></pre>
<p>Or</p>
<p>you can directly give the filename.</p>
<pre><code>paths: 
  - file: ./titanic.csv
transformations: 
  - read_delimited: 
      delimiter: ',' 
      encoding: 'ascii' 
      empty_as_string: false
      header: from_first_file
</code></pre>
<p>So, in your case the <code>MLTable</code> file is</p>
<pre><code>paths: 
  - file: ./CarPrice_Assignment.csv
transformations: 
  - read_delimited: 
      delimiter: ','
</code></pre>
<p>The folder should contain files like below.</p>
<p><img src=""https://i.imgur.com/7CVzp4F.png"" alt=""enter image description here"" /></p>
<p>And in mltable file you add the content according to your files.</p>
<p>Refer more about working with mltable files <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-mltable?view=azureml-api-2&amp;tabs=cli"" rel=""nofollow noreferrer"">here</a>.</p>
","2024-09-12 07:05:43","1","Answer"
"78976289","78973811","","<p>The following code works in MJML:</p>
<pre><code>&lt;mjml&gt;
  &lt;mj-body&gt;
    &lt;mj-section&gt;
      &lt;mj-column&gt;

        &lt;mj-image width=&quot;100px&quot; src=&quot;/assets/img/logo-small.png&quot;&gt;&lt;/mj-image&gt;

        &lt;mj-divider border-color=&quot;#F45E43&quot;&gt;&lt;/mj-divider&gt;

        
        &lt;mj-section
    background-position=&quot;bottom right&quot;
    padding-top=&quot;0px&quot;
    padding-right=&quot;25px&quot;
    padding-bottom=&quot;0px&quot;
    padding-left=&quot;25px&quot;
    background-color=&quot;#f7f7f7&quot;
    background-url=&quot;https://via.placeholder.com/600x300/ff0000&quot;
    background-repeat=&quot;no-repeat&quot;
&gt;
          &lt;mj-text font-size=&quot;20px&quot; color=&quot;#F45E43&quot; font-family=&quot;helvetica&quot;&gt;Hello World&lt;/mj-text&gt;
        &lt;/mj-section&gt;
      &lt;/mj-column&gt;
    &lt;/mj-section&gt;
  &lt;/mj-body&gt;
&lt;/mjml&gt;
</code></pre>
<p>So the problem is with the Python MJML program. That should be fixed.</p>
<p>You would be expecting this sort of output for Outlook Windows to work:</p>
<pre><code>&lt;!--[if mso | IE]&gt;
&lt;table align=&quot;center&quot; border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;&quot; style=&quot;width:600px;&quot; width=&quot;600&quot; bgcolor=&quot;#f7f7f7&quot; &gt;
&lt;tr&gt;
&lt;td style=&quot;line-height:0px;font-size:0px;mso-line-height-rule:exactly;&quot;&gt;
&lt;v:rect style=&quot;width:600px;&quot; xmlns:v=&quot;urn:schemas-microsoft-com:vml&quot; fill=&quot;true&quot; stroke=&quot;false&quot;&gt;
&lt;v:fill origin=&quot;0.5, 0&quot; position=&quot;0.5, 0&quot; src=&quot;https://via.placeholder.com/600x300/ff0000&quot; color=&quot;#f7f7f7&quot; type=&quot;tile&quot; /&gt;
&lt;v:textbox style=&quot;mso-fit-shape-to-text:true&quot; inset=&quot;0,0,0,0&quot;&gt;&lt;![endif]--&gt;
</code></pre>
","2024-09-12 02:43:40","0","Answer"
"78976058","","Creating a data asset in Azure but getting this Error","<p>I'm trying to create a ML data asset and I'm following this link: <a href=""https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/01-machine-learning.html"" rel=""nofollow noreferrer"">https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/01-machine-learning.html</a>
to get it done but I am constantly getting this error
<a href=""https://i.sstatic.net/I8InVrWk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/I8InVrWk.png"" alt=""enter image description here"" /></a></p>
<p>I know my files are not the exact ones in the examples but even when I use those files I'm getting the same error.So I'm not sure what the problem is since this my first time using ML in Azure.</p>
","2024-09-12 00:30:32","-1","Question"
"78973811","","I am using pthon mjml (https://pypi.org/project/mjml/) to convert mjml to html but converted html is not able to render background image in out look","<p>I am using pthon mjml (<a href=""https://pypi.org/project/mjml/"" rel=""nofollow noreferrer"">https://pypi.org/project/mjml/</a>) to convert mjml to html but converted html is not able to render background image in out look</p>
<ul>
<li>mjml code</li>
</ul>
<pre><code>&lt;mj-section
    background-position=&quot;bottom right&quot;
    padding-top=&quot;0px&quot;
    padding-right=&quot;25px&quot;
    padding-bottom=&quot;0px&quot;
    padding-left=&quot;25px&quot;
    background-color=&quot;#f7f7f7&quot;
    background-url=&quot;{{ static_url }}images/email_result_summry_bg.svg&quot;
    background-repeat=&quot;no-repeat&quot;
&gt;
</code></pre>
<ul>
<li>converted html</li>
</ul>
<pre><code>&lt;div style=\ &quot;background:url('https://purple.insightloupe.com/static/images/context-bg.jpg') center top / cover no-repeat;background-position:center top;background-repeat:no-repeat;background-size:cover;margin:0px auto;max-width:600px\&quot;&gt;
            &lt;div style=\ &quot;line-height:0;font-size:0\&quot;&gt;
                &lt;table align=\ &quot;center\&quot; background=\ &quot;https://purple.insightloupe.com/static/images/context-bg.jpg\&quot; border=\ &quot;0\&quot; cellpadding=\ &quot;0\&quot; cellspacing=\ &quot;0\&quot; role=\ &quot;presentation\&quot; style=\
                &quot;width:100%;background:url('https://purple.insightloupe.com/static/images/context-bg.jpg') center top / cover no-repeat;background-position:center top;background-repeat:no-repeat;background-size:cover\&quot;&gt;
</code></pre>
<ul>
<li>when i am removing background image it is working fine but i could not remove background image</li>
</ul>
","2024-09-11 12:32:48","0","Question"
"78971719","77823012","","<p>Pass in your batch argument to the <code>Explainer</code> object as a positional argument:</p>
<p><code>explanation = explainer(data.x, data.edge_index, batch = data.batch)</code></p>
","2024-09-11 00:53:07","0","Answer"
"78953465","78953075","","<p>Please change your code to the following in order to make it work:</p>
<pre><code># config_list = [
#     {
#         'model': 'gpt-4o',
#         'api_key': 'API_KEY_HERE'
#     }
# ]
llm_config={&quot;config_list&quot;: config_list}
</code></pre>
<p>After this it should throw the error: you do not have access to the model, after which you should make a minimum payment of 5$ to open ai to access it</p>
","2024-09-05 14:09:18","-1","Answer"
"78953075","","Completions.create() got an unexpected keyword argument 'request_timeout'","<p>I am using Autogen from microsoft with the below code:</p>
<pre><code>import autogen
from autogen import AssistantAgent, UserProxyAgent

config_list = [
    {
        'model': 'gpt-4',
        'api_key': 'API_KEY'
    }
]

llm_config={
    &quot;request_timeout&quot;: 600,
    &quot;seed&quot;: 42,
    &quot;config_list&quot;: config_list,
    &quot;temperature&quot;: 0
}

assistant = autogen.AssistantAgent(
    name=&quot;assistant&quot;,
    llm_config=llm_config,
    system_message=&quot;Chief technical officer of a tech company&quot;
)

user_proxy = autogen.UserProxyAgent(
    name=&quot;user_proxy&quot;,
    human_input_mode=&quot;ALWAYS&quot;,
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get(&quot;content&quot;, &quot;&quot;).rstrip().endswith(&quot;TERMINATE&quot;),
    code_execution_config={&quot;work_dir&quot;: &quot;web&quot;},
    llm_config=llm_config,
    system_message=&quot;&quot;&quot;Reply TERMINATE if the task has been solved at full satisfaction.
Otherwise, reply CONTINUE, or the reason why the task is not solved yet.&quot;&quot;&quot;
)

task = &quot;&quot;&quot;
Write python code to output numbers 1 to 100
&quot;&quot;&quot;

user_proxy.initiate_chat(
    assistant,
    message=task
)
</code></pre>
<p>when I try to run the python, it gives me this error:</p>
<p><strong>Completions.create() got an unexpected keyword argument 'request_timeout'</strong></p>
<pre><code>[autogen.oai.client: 09-05 14:32:12] {164} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.
[autogen.oai.client: 09-05 14:32:12] {164} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.
user_proxy (to assistant):


Write python code to output numbers 1 to 100


--------------------------------------------------------------------------------
Traceback (most recent call last):
  File &quot;c:\Users\HP\Desktop\prj\autogen-ve\Scripts\runningBots.py&quot;, line 42, in &lt;module&gt;
</code></pre>
<p>How to resolve this?</p>
","2024-09-05 12:37:45","-1","Question"
"78952571","78952522","","<p>There are several ways to go about it but you can start of with <a href=""https://ollama.com"" rel=""nofollow noreferrer""><code>Ollama</code></a>, you need to download a <code>model</code> such as <code>llama3</code>, if you have experience with python you are in luck.</p>
<p>You don't need to train a model, there are different models out there that does what you need or solves this kind of problem, all you need to do is provided the <code>llm</code> your documents(images, text, pdfs etc) and ask questions on them.</p>
<p>However in some cases, if your pdf contains <code>financial information such as annuity and the likes</code> , You might need to train it for it to understand how to do those kind of calculations or better still write a function which inherits from <a href=""https://python.langchain.com/v0.1/docs/modules/tools/"" rel=""nofollow noreferrer""><code>Langchain_tool</code></a> to instruct the <code>llm</code> on how to use it for those specific cases.</p>
<blockquote>
<p>It just feels like a black box that is liable to change without
notice, i.e. fragile. I assumed I'd need to train and deploy my own
model, is using chat gpt expensive overkill for what I want to do? If
I somehow ended up with a lot of users I'm thinking this would become
a problem.</p>
</blockquote>
<p>Here are the general steps on how to go go about it:</p>
<p>Step 1:
First download <code>Ollama</code> then you can pull the <code>llama</code> image which would serve as your <code>llm</code>, do a <code>docker pull</code> of llama, preferably <code>llama3</code>.</p>
<p>Step 2:
Find a library that converts images to text or pdf such as</p>
<p><code>Optical character recognition Library (OCR)</code></p>
<p>Step 3:
Find a vector_db(FAISS, chromadb and the likes) which converts text to vectors; this makes information extraction easy.</p>
<p>Step 4:
Feed your documents to the vector_db so it can convert it to vectors because numbers are good...</p>
","2024-09-05 10:32:22","1","Answer"
"78952556","78952522","","<p>You don't need to train a model to just extract text from documents/images, you can simply use python libraries like pytesseract etc.
By simply using 10 lines of code you can extract all the information.
You can integrate this model/code to your tool.
You can ask ChatGPT for a reference code.</p>
","2024-09-05 10:29:10","1","Answer"
"78952522","","How to extract information from a photo of a document","<p>I want to build a simple tool that will allow a user to take a photo of a document and extract information such as date/time and some other information</p>
<p>It is simple to do this through Chat GPT's UI, upload an image, ask it for some information from the document. But is just calling Chat GPT's API from my code really a viable solution?</p>
<ul>
<li>It just feels like a black box that is liable to change without notice, i.e. fragile.</li>
<li>I assumed I'd need to train and deploy my own model, is using chat gpt expensive overkill for what I want to do? If I somehow ended up with a lot of users I'm thinking this would become a problem.</li>
</ul>
<p>Note: I have no real AI experience or knowledge, but I do have lots of programming experience (I work full time as a developer).</p>
","2024-09-05 10:22:49","0","Question"
"78943332","77743390","","<p>The <code>path</code> in <code>mnist.load_data()</code> is relative to local cache of the dataset  (relative to <code>~/.keras/datasets</code>).</p>
<p>Just copy downloaded mnist.npz file to <code>~/.keras/datasets</code> and then</p>
<p><code>(X_train, y_train),(X_test, y_test) = mnist.load_data(path='mnist.npz')</code></p>
<p>will do the trick.</p>
","2024-09-03 08:22:57","0","Answer"
"78942627","78507089","","<p>The 0% GPU is definitely NOT normal during KSampler.</p>
<ol>
<li>Try profiling GPU sessions with <a href=""https://developer.nvidia.com/nvidia-visual-profiler"" rel=""nofollow noreferrer"">https://developer.nvidia.com/nvidia-visual-profiler</a></li>
<li>Try your workflow in one of those <a href=""https://www.runcomfy.com"" rel=""nofollow noreferrer"">GPU Cloud</a> tools and see if it is on par with your expectation. 4 secs of animatediff should be just within tens of minutes intead of 12 hours.</li>
</ol>
","2024-09-03 04:05:10","1","Answer"
"78937923","78926187","","<p>Reinstalling the library on new venv worked.</p>
<p>My approach:</p>
<ul>
<li>Trying the script on another laptop.(Worked)</li>
<li>Re-installing python on my machine. (I should have created venv instead)</li>
</ul>
","2024-09-01 17:10:55","0","Answer"
"78934276","78933726","","<p>Generally speaking, you are getting errors, because you are mixing and matching calls to <code>generate(...)</code>, which let's you prompt models in an adhoc way, and <code>dotprompt</code>, which has prompt management features, as well as a shortcut to &quot;run&quot; a prompt (i.e. <code>prompt.generate(...)</code>.</p>
<p>If you want to use chat history, we can stick to <code>generate</code> and modify your original code as follows:</p>
<pre class=""lang-js prettyprint-override""><code>import { generate } from '@genkit-ai/ai';

generate({
  model: 'googleai/gemini-1.5-flash-latest',
  prompt: 'You are a helpful AI assistant named Walt. Say hello.',
  history: [
    { role: 'user', content: [{ text: 'Hello.' }] },
    { role: 'model', content: [{ text: 'Hi there!' }] },
  ],
});
</code></pre>
<p>You can save the results from <code>generate(...)</code> to a variable, and call <code>toHistory()</code> on it, and then pass that history in to the next request. For example:</p>
<pre class=""lang-js prettyprint-override""><code>import { generate } from '@genkit-ai/ai';
import { gemini15Flash } from '@genkit-ai/googleai';

let response = await generate({
  model: gemini15Flash,
  prompt: &quot;How do you say 'dog' in French?&quot;,
});
let history = response.toHistory();

response = await generate({
  model: gemini15Flash,
  prompt: 'How about in Spanish?',
  history,
});
history = response.toHistory();
</code></pre>
<p>The Genkit docs describe in more detail: <a href=""https://firebase.google.com/docs/genkit/models#recording_message_history"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/genkit/models#recording_message_history</a></p>
","2024-08-31 03:54:16","0","Answer"
"78933726","","Firebase Genkit: how to set system / user /assistant chat history?","<p>I am trying to use Firebase Genkit.</p>
<p>I see <a href=""https://firebase.google.com/docs/genkit/prompts"" rel=""nofollow noreferrer"">here</a> that I can use:</p>
<pre><code>import { generate } from '@genkit-ai/ai';

generate({
  model: 'googleai/gemini-1.5-flash-latest',
  prompt: 'You are a helpful AI assistant named Walt. Say hello.',
});
</code></pre>
<p>Great, it works.</p>
<p>Now, I want to use a chat history.</p>
<p>As mentioned <a href=""https://firebase.google.com/docs/genkit/dotprompt.md"" rel=""nofollow noreferrer"">here</a>, I use:</p>
<pre><code>const result = await multiTurnPrompt.generate({
  model: llama3x70b, 
  history: [
    { role: 'user', content: [{ text: 'Hello.' }] },
    { role: 'model', content: [{ text: 'Hi there!' }] },
  ],
});
</code></pre>
<p>Here is my code:</p>
<pre><code>import {dotprompt, promptRef } from &quot;@genkit-ai/dotprompt&quot;;
....
const multiTurnPrompt = dotprompt();

const result = await multiTurnPrompt.generate({
  model: llama3x70b, 
  history: [
    { role: 'user', content: [{ text: 'Hello.' }] },
    { role: 'model', content: [{ text: 'Hi there!' }] },
  ],
});

console.log(await result.text());
</code></pre>
<p>I get error:</p>
<pre><code>TypeError: multiTurnPrompt.generate is not a function
</code></pre>
<p>How to solve this issue ?</p>
<p><strong>UPDATE</strong></p>
<p>Modified code following @Michael Doyle answer:</p>
<pre><code>const multiTurnPrompt = promptRef('nameOfPrompt');

const result = await multiTurnPrompt.generate({

 model: llama3x70b, 

 history: [
    { role: 'user', content: [{ text: 'Hello.' }] },
    { role: 'model', content: [{ text: 'Hi there!' }] },
  ],

});
</code></pre>
<p>I get the error:</p>
<pre><code>&quot;GenkitError: dotprompt: NOT_FOUND: Could not find 'nameOfPrompt.prompt' in the prompts folder.&quot; 
</code></pre>
<p>I don't want to use a chat described in a chat file, I want to define the chat in my JS code as mentioned in the doc. How to do that ? Thanks.</p>
","2024-08-30 21:02:04","1","Question"
"78933201","78625475","","<p>In VITS, the input to the HiFi-GAN module is not a traditional mel-spectrogram, but rather latent variables produced by the Encoder, which is conditioned by the text and an alignment model. In this way, one can say that VITS is an end-to-end model.</p>
<p>The HiFi-GAN in VITS acts as a decoder that takes these latent variables and generates the final audio waveform directly. Therefore, what you are trying to plot is not a mel-spectrogram, but rather these latent variations.</p>
<p>This approach allows VITS to maintain flexibility and high-quality synthesis without relying on traditional interactive representations such as mel-spectrograms.</p>
","2024-08-30 17:38:10","2","Answer"
"78927984","78926958","","<p>It looks like you are adding the vector ID to the message and not the content of the vector. Try adding the content of the most relevant vector instead of the ID.</p>
","2024-08-29 13:12:23","0","Answer"
"78926958","","Adding vectorized data to an AI chatbot in Python","<p>I'm trying to add vectorized data (file format .npz) to my Azure OpenAI chatbot in order to make it generate answers based on this data.
The thing is I tried watching various videos but none helped me in any way.</p>
<pre><code>import os
import numpy as np
import faiss
from dotenv import load_dotenv
import openai
import asyncio
from sentence_transformers import SentenceTransformer

load_dotenv()

AZURE_OPENAI_API_KEY = os.environ['AZURE_OPENAI_API_KEY']
AZURE_OPENAI_API_ENDPOINT = os.environ['AZURE_OPENAI_API_ENDPOINT']
AZURE_OPENAI_API_MODEL = os.environ['AZURE_OPENAI_API_MODEL']
AZURE_OPENAI_API_VERSION = os.environ['AZURE_OPENAI_API_VERSION']

openai.api_type = &quot;azure&quot;
openai.api_key = AZURE_OPENAI_API_KEY
openai.api_base = AZURE_OPENAI_API_ENDPOINT
openai.api_version = AZURE_OPENAI_API_VERSION

var_temperature = 0.7 
var_top_p = 0.95
var_max_tokens = 800
var_frequency_penalty = 0
var_presence_penalty = 0
var_past_messages_included = 10

system_message = &quot;Your name is Bot. Answer as concisely and politely as possible.&quot;
system_message_chunk = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_message}]

few_shot_chunk = []
chat_history_chunk = []

def load_vector_data(file_path=&quot;combined_vectors.npz&quot;):
    data = np.load(file_path)
    vectors = data['vectors']
    ids = data['ids']
    index = faiss.IndexFlatL2(vectors.shape[1])
    index.add(vectors)
    return index, ids

def find_relevant_data(user_input_vector, index, all_ids):
    D, I = index.search(np.array([user_input_vector]), k=1)
    closest_id = all_ids[I[0][0]]
    return closest_id

async def chat_with_bot(user_input, index, all_ids, model):
    global chat_history_chunk
    user_input_vector = model.encode([user_input])[0]
    relevant_id = find_relevant_data(user_input_vector, index, all_ids)
    relevant_content = f&quot;Contextual information related to ID: {relevant_id}&quot;
    chat_history_chunk.append({&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: relevant_content})
    chat_history_chunk.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input})

    if len(chat_history_chunk) &gt; var_past_messages_included:
        chat_history_chunk = chat_history_chunk[-var_past_messages_included:]

    messages = system_message_chunk + few_shot_chunk + chat_history_chunk

    response = await openai.ChatCompletion.acreate(
        deployment_id=AZURE_OPENAI_API_MODEL,
        messages=messages,
        temperature=var_temperature,
        max_tokens=var_max_tokens,
        top_p=var_top_p,
        frequency_penalty=var_frequency_penalty,
        presence_penalty=var_presence_penalty
    )

    chat_history_chunk.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response['choices'][0]['message']['content']})
    return response['choices'][0]['message']['content']

async def main():
    index, all_ids = load_vector_data()
    model = SentenceTransformer('all-MiniLM-L6-v2')
    while True:
        user_input = input(&quot;You: &quot;)
        if user_input.lower() == &quot;exit&quot;:
            break

        response = await chat_with_bot(user_input, index, all_ids, model)
        print(f&quot;Bot: {response}&quot;)

asyncio.run(main())
</code></pre>
<p>I tried with this code, but the bot doesn't react to this data at all. Perhaps it can't read it in vectorized format? It's my first time using vectorized data so perhaps I just don't understand something?</p>
","2024-08-29 09:02:16","0","Question"
"78926187","","Unable to import pipeline with crewai","<p>I am unable to import pipeline, I am following exactly as documented in docs.crewai.com I am using python 3.12.4 in virtual environment</p>
<pre><code>from crewai import Pipeline

Import error: cannot import name Pipeline from crewai
</code></pre>
<p>How to fix it?</p>
","2024-08-29 05:09:21","-1","Question"
"78924485","78909848","","<p>The LLMs are currently not support on TensorFlow Lite with NPU acceleration using the delegate. This is primarily because of 2 reasons:</p>
<ul>
<li>The delegates are &quot;runtime compiled&quot; which means the actual compilation of the .tflite model to the hardware is done when the model is loaded. LLM compilation can take several minutes and doing that at first load is not usually feasible. Most LLMs today are deployed using compilation ahead of time. That infra is currently being worked on for TensorFlow Lite.</li>
<li>Protobuf has a 2GB limit and many of these models are &gt; than that. Some infra is needed to make TensorFlow Lite compatible with that.</li>
</ul>
<p>To get an LLM working today, you have to use the QNN APIs (Qualcomm AI Engine Direct) or ONNX Runtime (which has an ahead of time compile path)</p>
","2024-08-28 17:00:21","0","Answer"
"78909848","","How to generate .tflite models form Qualcomm AI Hub Models","<p>I have a Qualcomm device and I want to deploy LLMs on it. I want to get .tflite model. I downloaded an Android app from Google AI which can deploy tensorflow lite models, while it only accepts .tflite file. And I cannot find a website to directly download .tflite model files<br />
I followed the instructions <a href=""https://huggingface.co/qualcomm/Llama-v2-7B-Chat"" rel=""nofollow noreferrer"">here</a>, and in the end it says the model can be .tflite exported. But I don't know how to get .tflite model files.</p>
<p><a href=""https://aihub.qualcomm.com/models?domain=Generative+AI"" rel=""nofollow noreferrer"">Qualcomm AI Hub</a> provides LLMs models that specially modified for specific devices. And I followed instructions <a href=""https://huggingface.co/qualcomm/Llama-v2-7B-Chat"" rel=""nofollow noreferrer"">here</a>.<br />
I ran command below to learn the usages, but no options like <code>--target-runting tflite</code>.</p>
<pre><code>python -m qai_hub_models.models.llama_v2_7b_chat_quantized.export -h
</code></pre>
<p>Then I ran the command below, and only .bin models are downloaded.</p>
<pre><code>python -m qai_hub_models.models.llama_v2_7b_chat_quantized.export --device &quot;XR2 Gen 2 (Proxy)&quot;
</code></pre>
<p>Can someone helps me to get LLM files of .tflite type?</p>
","2024-08-24 19:35:49","0","Question"
"78907713","78907434","","<p>Maybe you are not implementing the MSE cost function correctly, or you're initializing the hyperparameters (like learning rate) with other values, but look the MSE results on both: your answer and the result provided by google website, they're decreasing in each iteration, that doesn't mean that the algorithm is learning??</p>
","2024-08-23 22:23:29","0","Answer"
"78907679","78907434","","<p>So I made some minor adjustments to my script based on the feedback from the AI chat bot provided by google. See below:</p>
<pre><code>import pandas as pd
import numpy as np

def main():
    data = [3.5, 18], [3.69, 15], [3.44, 18], [3.43, 16], [4.34, 15], [4.42, 14], [2.37, 24]
    initial_data_df = pd.DataFrame(data,columns=['pounds','mpg'])

    number_of_iterations = 6
    weight = 0 # initialize weights
    bias = 0 # initialize weights
    weight_slope = 0
    bias_slope = 0
    final_results_df = pd.DataFrame()
    learning_rate = 0.01

    for i in range(number_of_iterations):
        loss = round(calculate_loss(initial_data_df,weight,bias),2)
        final_results_df = update_results(final_results_df,weight,bias,loss)
        weight_slope = find_weight_slope(initial_data_df,weight,bias)
        bias_slope = find_bias_slope(initial_data_df,weight,bias)
        weight = round(new_weight_update(weight,learning_rate,weight_slope),2)
        bias = round(new_bias_update(bias,learning_rate,bias_slope),2)
    print(final_results_df)

def calculate_loss(df,weight,bias):
    loss_summation = []
    for i in range(0,len(df)):
        loss_summation.append((df['mpg'][i]-((weight*df['pounds'][i])+bias))**2)
    return (sum(loss_summation)/len(df))

def update_results(df,weight,bias,loss):
    if df.empty:
        df = pd.DataFrame([[weight,bias,loss]],columns=['weight','bias','loss'])
    else:
        df = pd.concat([df,pd.DataFrame([[weight,bias,loss]],columns=df.columns)])
    return df

def find_weight_slope(df,weight,bias):
    slope = []
    for i in range(0,len(df)):
        predicted_mpg = (weight*df['pounds'][i])+bias   #   predicted_mpg = (weight * car_weight) + bias
        error = predicted_mpg-df['mpg'][i]
        slope.append(error*2*df['pounds'][i])
    return sum(slope)/len(df)

def find_bias_slope(df,weight,bias):
    slope = []
    for i in range(0,len(df)):
        predicted_mpg = (weight*df['pounds'][i])+bias   #   predicted_mpg = (weight * car_weight) + bias
        error = predicted_mpg-df['mpg'][i]
        slope.append(2*error)
    return sum(slope)/len(df)

def new_weight_update(old_weight,lr,slope):
    return old_weight-(lr*slope)

def new_bias_update(old_bias,lr,slope):
    return old_bias-(lr*slope)

if __name__=='__main__':
    main()
</code></pre>
<p>With these changes I get the following results:</p>
<pre><code>   weight  bias    loss
0    0.00  0.00  303.71
0    1.20  0.34  170.67
0    2.05  0.59  103.22
0    2.66  0.77   68.66
0    3.09  0.91   51.13
0    3.40  1.01   42.11
</code></pre>
<p>I provided the calculated weighted slope to get the next updated weight which should be the third iteration weight value.</p>
<p><a href=""https://i.sstatic.net/f7Mpbd6t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f7Mpbd6t.png"" alt=""enter image description here"" /></a></p>
<p>With these results and documentation I would conclude that google's example results are incorrect for gradient descent.</p>
","2024-08-23 22:04:41","0","Answer"
"78907608","","How to pass multiple inputs during invoke to a MessageGraph?","<p>We have a MessageGraph for an LLMCompiler implementation and as expected we pass a users's question when running invoke as a list of HumanMessage objects (which are mapped to a default &quot;messages&quot; key and passed to prompt templates), this works fine for simple use cases but now we need to pass an additional piece of information/context at invoke time (not at graph building time), we did something similar with a react agent and it was very easy passing a dictionary similar to <code>invoke({&quot;messages&quot;:input, &quot;context&quot;:context})</code>. But for MessageGraph this did not work, it looks as the list of messages passed when you run <code>invoke(messages)</code> is automatically mapped in the prompt to the &quot;messages&quot; key and no other inputs can be added, I tried passing a dicionary <code>invoke({&quot;messages&quot;:messages, &quot;context&quot;:context})</code> and it did not work, failed with error:</p>
<blockquote>
<p>Message dict must contain 'role' and 'content' keys, got
{&quot;messages&quot;:messages,&quot;context&quot;:context}</p>
</blockquote>
","2024-08-23 21:30:27","0","Question"
"78907434","","Need help verifying data provided by google/machine-learning course","<p>In the gradient descent page on the intro to <a href=""https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent"" rel=""nofollow noreferrer"">machine-learning course</a> provided by google provided are features and corresponding labels, MSE loss function, initial datasets, and results. I am having difficulty verifying the results that they have and I am wondering if anyone can assist in confirming whether or not I am making a mistake or they are.</p>
<p>I have the following:</p>
<pre><code>import pandas as pd
import numpy as np

data = [3.5, 18], [3.69, 15], [3.44, 18], [3.43, 16], [4.34, 15], [4.42, 14], [2.37, 24]
initial_data_df = pd.DataFrame(data,columns=['pounds','mpg'])

number_of_iterations = 6
weight = 0 # initialize weights
bias = 0 # initialize weights
weight_slope = 0
bias_slope = 0
final_results_df = pd.DataFrame()
learning_rate = 0.01

for i in range(number_of_iterations):
    loss = calculate_loss(initial_data_df,weight,bias)
    final_results_df = update_results(final_results_df,weight,bias,loss)
    weight_slope = find_weight_slope(initial_data_df,weight,bias)
    bias_slope = find_bias_slope(initial_data_df,weight,bias)
    weight = new_weight_update(weight,learning_rate,weight_slope)
    bias = new_bias_update(bias,learning_rate,bias_slope)
print(final_results_df)

def calculate_loss(df,weight,bias):
    loss_summation = []
    for i in range(0,len(df)):
        loss_summation.append((df['mpg'][i]-((weight*df['pounds'][i])+bias))**2)
    return (sum(loss_summation)//len(df))

def update_results(df,weight,bias,loss):
    if df.empty:
        df = pd.DataFrame([[weight,bias,loss]],columns=['weight','bias','loss'])
    else:
        df = pd.concat([df,pd.DataFrame([[weight,bias,loss]],columns=df.columns)])
    return df

def find_weight_slope(df,weight,bias):
    weight_update_summation = []
    for i in range(0,len(df)):
        wx_plus_b = (weight*df['pounds'][i])+bias
        wx_plus_b_minus_y = wx_plus_b-df['mpg'][i]
        weight_update_summation.append(2*(wx_plus_b_minus_y*df['pounds'][i]))
    return sum(weight_update_summation)//len(df)

def find_bias_slope(df,weight,bias):
    bias_update_summation = []
    for i in range(0,len(df)):
        wx_plus_b = (weight*df['pounds'][i])+bias
        wx_plus_b_minus_y = wx_plus_b-df['mpg'][i]
        bias_update_summation.append(2*wx_plus_b_minus_y)
    total_sum = sum(bias_update_summation)
    return total_sum//len(df)

def new_weight_update(old_weight,lr,slope):
    return old_weight-1*lr*slope

def new_bias_update(old_bias,lr,slope):
    return old_bias-1*lr*slope
</code></pre>
<p>Which yields:</p>
<pre><code>iter weight  bias   loss
0    0.00    0.00   303.0
0    1.20    0.35   170.0
0    2.06    0.60   102.0
0    2.67    0.79   67.0
0    3.10    0.93   50.0
0    3.41    1.04   41.0
</code></pre>
<p>This differs from the provided solution provided on the website:</p>
<pre><code>Iteration   Weight  Bias    Loss (MSE)
1           0       0       303.71
2           1.2     0.34    170.67
3           2.75    0.59    67.3
4           3.17    0.72    50.63
5           3.47    0.82    42.1
6           3.68    0.9     37.74
</code></pre>
","2024-08-23 20:09:14","-1","Question"
"78896115","78883811","","<p>It looks like the '<code>Import &quot;clarifai.rest&quot;</code>' part is showing you're trying to use an outdated Clarifai client that's no longer supported. I think ChatGPT might be giving you some old code.</p>
<p>I recommend switching to the new Python SDK, which you can find here: <a href=""https://github.com/Clarifai/clarifai-python"" rel=""nofollow noreferrer"">https://github.com/Clarifai/clarifai-python</a>. We also have other clients and a REST API you can check out: <a href=""https://docs.clarifai.com/api-guide/api-overview/api-clients/"" rel=""nofollow noreferrer"">https://docs.clarifai.com/api-guide/api-overview/api-clients/</a>.</p>
","2024-08-21 09:18:31","0","Answer"
"78895870","78895436","","<p>Do not give your python file the same name as the package. Your file is named <code>scrapegraphai.py</code>, hence <code>from scrapegraphai ...</code> tries to load from that file and not from the actual package. Just rename your file with your normal code.</p>
","2024-08-21 08:15:15","2","Answer"
"78895436","","Module not found error when using ScrapeGraphAI","<p>I am doing a simple example using ScrapeGraphAI, found on this link <a href=""https://github.com/ScrapeGraphAI/Scrapegraph-ai"" rel=""nofollow noreferrer"">ScrapeGraphAI Github</a>.</p>
<p>I have installed the packages in the virtual environment, as shown in the link:</p>
<pre><code>pip install scrapegraphai

playwright install
</code></pre>
<p>But when I run the code</p>
<pre><code>from scrapegraphai.graphs import SmartScraperGraph
</code></pre>
<p>I get the following error message:</p>
<pre><code>File &quot;C:\...\Python\ScrapeGraphAI\scrapegraphai.py&quot;, line 2, in &lt;module&gt;
    from scrapegraphai.graphs import SmartScraperGraph
ModuleNotFoundError: No module named 'scrapegraphai.graphs'; 'scrapegraphai' is not a package
</code></pre>
<p>What could be the problem if everything is installed properly?</p>
","2024-08-21 06:32:03","-1","Question"
"78883811","","command on virtual env. pip install clarifai.work on windows Import ""clarifai.rest""could not be resolvedPylancereportMissingImports (module) clarifai","<p>i am working in clarifai. i installed clarifai on my windows as pip install clarifai.</p>
<p>when i wrote <code>from clarifai.rest import ClarifaiApp</code> it is showing error. chatgpt even failed to solve the issues. i checked all the dependencies in my working environment. i could not understand what is the problem.</p>
<p>the error is:</p>
<pre><code>Import &quot;clarifai.rest&quot; could not be resolvedPylancereportMissingImports
(module) clarifai
</code></pre>
<p>How to resolve it?</p>
<p>notice here how chatgpt solves the question by giving a solution.</p>
<p><a href=""https://i.sstatic.net/itMIGuzj.png"" rel=""nofollow noreferrer"">image from chatgpt</a></p>
","2024-08-18 05:44:59","0","Question"
"78883690","78883470","","<p>Making API calls in a Gym environment's step function can slow down training. It's better to preprocess data, rebuild the simulation in Python for faster performance, and optimize API use by batching, caching, or using async programming to reduce delays.</p>
","2024-08-18 03:46:50","1","Answer"
"78883470","","Is it fine to make an API call inside a reinforcement learning program?","<p>I have made a game simulation with rest of the API available, and I would like to create a <strong>reinforcement learning AI</strong> in Python using gym from OpenAI.</p>
<p>So, is it fine to make API calls inside the <strong>step</strong> function in my custom gym environment? Won't it be too slow? Or should I rebuild the simulation in Python?</p>
","2024-08-17 23:42:00","0","Question"
"78881528","78091527","","<pre><code> string googleaikey = &quot;**6oY&quot;;
            RestClient client = new RestClient(&quot;https://generativelanguage.googleapis.com/v1beta/models/gemini-1.0-pro:generateContent?key=&quot;+googleaikey+ &quot;&quot;);
            RestRequest request = new RestRequest() { Method = Method.Post };
            ServicePointManager.SecurityProtocol = SecurityProtocolType.Tls12; // .NET 4.5
            ServicePointManager.SecurityProtocol = (SecurityProtocolType)3072; // .NET 4.0
            request.AddParameter(&quot;Content-Type&quot;, &quot;application/json&quot;, ParameterType.GetOrPost);
           // Create the request body with the message and other parameters
           
            string requestBody = @&quot;{{
                &quot;&quot;contents&quot;&quot;: [
                    {{
                        &quot;&quot;role&quot;&quot;: &quot;&quot;user&quot;&quot;,
                        &quot;&quot;parts&quot;&quot;: [
                            {{
                                &quot;&quot;text&quot;&quot;:&quot;+ userInput+ @&quot;
                            }}
                        ]
                    }}
                ],
                &quot;&quot;generationConfig&quot;&quot;: {{
                    &quot;&quot;temperature&quot;&quot;: 0.9,
                    &quot;&quot;topK&quot;&quot;: 50,
                    &quot;&quot;topP&quot;&quot;: 0.95,
                    &quot;&quot;maxOutputTokens&quot;&quot;: 4096,
                    &quot;&quot;stopSequences&quot;&quot;: []
                }},
                &quot;&quot;safetySettings&quot;&quot;: [

                ]
            }}&quot;;

            // Add the JSON body to the request
            request.AddJsonBody(JsonConvert.SerializeObject(requestBody));
            RestResponse response = client.Execute(request);
            var contents = response.Content;
</code></pre>
<p>Not working &quot;error&quot; Invalid Argument 400</p>
","2024-08-17 06:21:43","0","Answer"
"78881205","78507718","","<p>I'm hoping this will help you, or someone else who's stuck on this. I had the same issue and it took me quite a while to work it out. There are two ways of using a custom variable from an embedded chatbot:</p>
<p>in the client, use something like:</p>
<pre><code>import Chatbot from &quot;https://cdn.jsdelivr.net/npm/flowise-embed/dist/web.js&quot;
Chatbot.init({
    chatflowid: &quot;8a1a8b2e-101a-9500-a586-7dbd7f4d8e4c&quot;,
    apiHost: &quot;https://flowisehost.com&quot;,
    chatflowConfig: {
      vars: {
        apiKey: &quot;TEST_KEY&quot;
      }
    }
});
</code></pre>
<ol>
<li><p>in your flow, add a custom tool and get a handle on the variable, like so:</p>
<p>const myVar = $vars.apiKey;
return myVar;</p>
</li>
</ol>
<p>Add the custom tool to a flow and tell your tool agent to use it. This worked for me, hope it helps.</p>
<ol start=""2"">
<li>you can reference the variable in any node directly (e.g. a system prompt), using {{$vars.apiKey}}. In this case, you don't even need a custom tool.</li>
</ol>
","2024-08-17 01:16:05","1","Answer"
"78880596","78860941","","<p>remove line &quot;model_tokens&quot;: 2048. It worked after removing this for me. But not sure why removing this solved the issue.</p>
","2024-08-16 19:35:34","0","Answer"
"78878877","78391063","","<p>The imports you are using are depreciated.</p>
<p>Here are the updated versions:</p>
<p>First, install the embeddings:</p>
<pre><code>!pip install llama-index-embeddings-huggingface
!pip install llama-index-llms-llama-cpp
</code></pre>
<p>Next:</p>
<pre><code>Use llama_index.core instead of llama_index
Use llama_index.llms.llama_cpp.llama_utils instead of llama_index.llms.llama_utils 
</code></pre>
<p>Your entire code should be as follows:</p>
<pre><code>from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.llms.llama_cpp.llama_utils import (messages_to_prompt,completion_to_prompt,)
</code></pre>
","2024-08-16 11:09:47","0","Answer"
"78875648","","Transfer Learning Pretrained Model","<p>I'm fitting transfer learning models on Google Colab. However, I've encountered a warning message with the code</p>
<pre><code>Epoch 1/30
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: 
UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in 
its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. 
Do not pass these arguments to `fit()`, as they will be ignored.
  self._warn_if_super_not_called()
</code></pre>
<p>and after the first epoch, I'm receiving the following error:</p>
<pre><code>---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
&lt;ipython-input-23-962a870d4412&gt; in &lt;cell line: 16&gt;()
     14 # fit the model
     15 # Run the cell. It will take some time to execute
---&gt; 16 training_history = model_efficientnet.fit(
     17   training_set,
     18   validation_data=validate_set,
</code></pre>
<p>I've successfully fit six other transfer learning models without any issues, and their accuracies were satisfactory.</p>
<p>How to resolve this issue?</p>
<p>I would like to receive training accuracy and validation accuracy</p>
","2024-08-15 14:49:45","1","Question"
"78874358","78862219","","<p>On Microsoft docs I have found one interesting Note (<a href=""https://learn.microsoft.com/en-us/azure/bot-service/bot-builder-concept-authentication-types?view=azure-bot-service-4.0"" rel=""nofollow noreferrer"">original link</a>):</p>
<blockquote>
<p>Note: The token issued during Bot authentication isn't the same token
issued during User authentication. The first is used to establish
secure communication between a bot, channels and, ultimately, client
applications. The second is used to authorize the bot to access
secured resource on behalf of the user.</p>
</blockquote>
<p>Therefore after that comment I decided to</p>
<ul>
<li>register app as Multitenant (in App registry in Entra ID),</li>
<li>remove managed identities</li>
<li>recreate Azure Bot as multitenant but to add <strong>OAuth Connection Strings</strong></li>
</ul>
<p>And in result I could contact my bot with my token.</p>
","2024-08-15 09:00:52","0","Answer"
"78867197","78462357","","<p>On a macOS laptop or an Ubuntu Linux server, this runs correctly with the code:</p>
<pre><code>import spacy
import pytextrank

nlp = spacy.load(&quot;en_core_web_trf&quot;)
nlp.add_pipe(&quot;textrank&quot;)

doc = nlp(&quot;the quick brown fox jumped over the lazy dog.&quot;)

for tok in doc:
    print(tok)
</code></pre>
<p>Using:</p>
<ul>
<li><code>en_core_web_trf-3.7.3</code></li>
<li><code>spacy==3.7.5</code></li>
<li><code>pytextrank==3.3.0</code></li>
</ul>
<p>Results:</p>
<pre><code>the
quick
brown
fox
jumped
over
the
lazy
dog
.
</code></pre>
<p>You mentioned about about installing <code>nltk</code> though you probably meant <code>spaCy</code> ?</p>
<p>There is the difference of running with CUDA on Windows, although we haven't seen a &quot;Can't find factory...&quot; error before.  FWIW, I'm lead committer on the <code>pytextrank</code> project.</p>
<p>Would you mind opening a GitHub issue for this on <a href=""https://github.com/DerwenAI/pytextrank/issues"" rel=""nofollow noreferrer"">https://github.com/DerwenAI/pytextrank/issues</a> so we can track the issue and get more of our dev community involved?</p>
","2024-08-13 15:46:15","0","Answer"
"78864440","78864224","","<p>I just installed a standalone program on my computer and waited 30 minutes. While memory gradually increases to 120MB in the half-hour, CPU usage is only about 5% of total.
To print the performance report, use the pprof tool and the following two commands:</p>
<pre><code>curl http://127.0.0.1:9091/debug/pprof/heap &gt; heap_snapshot.heap

go tool pprof -png heap_snapshot.heap
</code></pre>
<p>A PNG file will be produced by the tool. When we open the PNG file, we can see how the CPU and memory resources are used. The standalone has a few task loops for sending time ticks, checking garbage collection, checking compaction, etc. Also, there are grpc connections between nodes that require upkeep. I don't think it is a problem.</p>
<p><a href=""https://i.sstatic.net/7cn9aOeK.png"" rel=""nofollow noreferrer"">result</a></p>
","2024-08-13 04:59:59","0","Answer"
"78864377","78860591","","<p>In Java v2.4.2 or v2.3.8, you can add customized properties into a collection with alterCollection() with the following code:</p>
<pre><code>    milvusClient.alterCollection(AlterCollectionParam.newBuilder()
            .withCollectionName(COLLECTION_NAME)
            .withProperty(&quot;my_prop&quot;, &quot;prop_value&quot;)
            .build());
    R&lt;DescribeCollectionResponse&gt; resp = milvusClient.describeCollection(DescribeCollectionParam.newBuilder()
            .withCollectionName(COLLECTION_NAME)
            .build());
    List&lt;KeyValuePair&gt; pairs = resp.getData().getPropertiesList();
    for (KeyValuePair pair : pairs) {
        System.out.println(pair);
    }
</code></pre>
","2024-08-13 04:23:30","0","Answer"
"78864224","","Why does ram memory increase in idle stage?","<p>I am deployed a milvus-standalond in docker.
Issue is i do nothing in milvus still milvus use cpu 5 to 6% continuously and ram is consumed continuously like reach 300 mb to 390 mb in duration of half hour.</p>
<p>Language is python
Milvus version 2.4.6
Code is write from (write very basic code from)
<a href=""https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/"" rel=""nofollow noreferrer"">https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/</a></p>
<p>Please help why milvus not release ram in idle stage.</p>
","2024-08-13 03:06:45","-1","Question"
"78863824","78862219","","<p>You cannot <em><strong>switch</strong></em> an Azure Bot from one type to another. If you create a new Azure Bot resource of type <code>Managed Identity</code>, then you can use your existing bot code and app service with that new Azure Bot. Please confirm whether you created a new resource or not.</p>
<p>If you want to migrate your existing bot code/App Service to Managed identity (after creating new Azure bot resource), you will need to do the following:</p>
<ul>
<li>Ensure the Managed Identity has been added to the App Services via the Identity blade.</li>
<li>Configure the following settings in the application: <code>MicrosoftAppId</code>, <code>MicrosoftAppTenantId</code> and <code>MicrosoftAppType</code> (value: <code>UserAssignedMSI</code>). Where the <code>Client ID</code> value of the Managed Identity is used for <code>MicrosoftAppId</code>.</li>
<li>Do not have <code>MicrosoftAppPassword</code> configured.</li>
</ul>
","2024-08-12 22:50:58","0","Answer"
"78862219","","Azure Bot: Failed to acquire token for client credentials","<p>I have hosted Azure Bot and AppService connected to it. With MicrosoftType = MultiTenant, everything worked fine. After that I decided to add MicrosoftType = UserAssignedMSI with Managed Identity. Unfortunately locally it's cannot be debugged properly. I received an error shown below.</p>
<p><a href=""https://i.sstatic.net/r7zKzrkZ.png"" rel=""nofollow noreferrer"">Error image</a>
The full error description is here:
Failed to acquire token for client credentials. ([Managed Identity] Error Message: No User Assigned or Delegated Managed Identity found for specified ClientId/ResourceId/PrincipalId. Managed Identity Correlation ID: 556f59d3-fb98-4c59-a358-0b6eeddd0db4 Use this Correlation ID for further investigation.) [Managed Identity] Error Message: No User Assigned or Delegated Managed Identity found for specified ClientId/ResourceId/PrincipalId. Managed Identity Correlation ID: 556f59d3-fb98-4c59-a358-0b6eeddd0db4 Use this Correlation ID for further investigation.</p>
<p>I went through <a href=""https://learn.microsoft.com/en-us/azure/bot-service/bot-builder-authentication?view=azure-bot-service-4.0&amp;tabs=userassigned%2Caadv2%2Ccsharp"" rel=""nofollow noreferrer"">Bot builder authentication</a>
The description here is well-defined. So using samples I wanted to sign in or at least to see the dialog window for authentication.</p>
<p>I expected to test the results using <strong>Azure Bot | Test in Web Chat</strong></p>
","2024-08-12 14:31:59","0","Question"
"78860941","","Model not found Scrapegraph-ai","<p>I'm trying to get scapegraph-ai working. I downloaded the repo from here <a href=""https://github.com/ScrapeGraphAI/Scrapegraph-ai"" rel=""nofollow noreferrer"">https://github.com/ScrapeGraphAI/Scrapegraph-ai</a> and followed the instructions in the readme. I downloaded Ollama from here <a href=""https://ollama.com/"" rel=""nofollow noreferrer"">https://ollama.com/</a> and downloaded the LLama3.1 model. When I launch Ollama from CMD everything seems to work correctly. I can use the model using the command ollama run llama3.1. However, I have problems in python. I'm trying to scrape using the scapegraph-ai library using the ollama models locally. I created a simple program in python that allows you to do scaping using scapegraph and taking as input from the prompt the URL and the prompt instructions:</p>
<p>Main.py:</p>
<pre><code>import sys
from scraper import Scraper

def main():
  if len(sys.argv) != 3:
    print(&quot;Command: python main.py &lt;URL&gt; &lt;PROMPT_INSTRUCTIONS&gt;&quot;)
    sys.exit(1)

  scr = Scraper(sys.argv[1], sys.argv[2])
  scr.scrape()

  if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>Scraper.py:</p>
<pre><code>from scrapegraphai.graphs import SmartScraperGraph
from scrapegraphai.utils import prettify_exec_info

class Scraper:
   def __init__(self, url, command):
       self.url = url
       self.command = command

   def scrape(self):
       try:
          graph_config = {
              &quot;llm&quot;: {
                  &quot;model&quot;: &quot;ollama/llama3.1&quot;,
                  &quot;temperature&quot;: 1,
                  &quot;format&quot;: &quot;json&quot;,
                  &quot;model_tokens&quot;: 2000,
                  &quot;base_url&quot;: &quot;http://localhost:11434&quot;
              },
              &quot;embeddings&quot;: {
                  &quot;model&quot;: &quot;ollama/nomic-embed-text&quot;,
                  &quot;temperature&quot;: 0,
                  &quot;base_url&quot;: &quot;http://localhost:11434&quot;
              }
          }

        smart_scraper_graph = SmartScraperGraph(
            prompt=self.command,
            source=self.url,
            config=graph_config
        )

        result = smart_scraper_graph.run()
        print(result)
        
    except Exception as e:
        print(f&quot;Error: {e}&quot;)
</code></pre>
<p>But when I launch my python code I get the error:</p>
<p><strong>Model not found, using default token size (8192)</strong></p>
<p>Does anyone know what I'm doing wrong?</p>
","2024-08-12 09:32:01","0","Question"
"78860591","","Having trouble storing collection metadata apart from using an external database","<p>It looks like there is not a practical way to store some metadata for a collection, which is something I would like to do.</p>
<p>System properties are stored in the 'properties' field; unfortunately, Milvus does not support the Collection.get_properties function. Custom properties can be stored in a different field. Although it's a somewhat improvised solution, I could store the metadata in a &quot;description&quot; field.</p>
<p>It appears that the only reliable method for storing collection metadata is to use an external database.</p>
<p>Can anyone provide some insights on this? That would be so helpful!</p>
","2024-08-12 08:05:43","0","Question"
"78860300","78343977","","<p>Just a note to someone who are facing this issue of Gemini API not giving response, it is been observed that setting the 'output tokens' to something less than 2048 is making Gemini behave unexpectedly and return None instead of a response. You may want to modify and set it to 2048 or high.</p>
<p>Reference - <a href=""https://github.com/google-gemini/generative-ai-python/issues/196"" rel=""nofollow noreferrer"">https://github.com/google-gemini/generative-ai-python/issues/196</a></p>
","2024-08-12 06:49:21","0","Answer"
"78858489","78858483","","<p>Use NEXT_PUBLIC prefix:
If you need to access this environment variable on the client side, you should prefix it with NEXT_PUBLIC_. Update your .env.local file:</p>
<pre><code>NEXT_PUBLIC_GEMINI_API_KEY=AIzaSyDyW49l7GXrk0QJmf425uNZgeHY2Sr-JEA
</code></pre>
<p>And in your code:</p>
<pre><code>const apiKey = process.env.NEXT_PUBLIC_GEMINI_API_KEY;
</code></pre>
<p>TypeScript configuration:
If you're using TypeScript, make sure your next-env.d.ts file includes the following:</p>
<pre><code>/// &lt;reference types=&quot;next&quot; /&gt;
/// &lt;reference types=&quot;next/types/global&quot; /&gt;

declare namespace NodeJS {
  interface ProcessEnv {
    NEXT_PUBLIC_GEMINI_API_KEY: string;
  }
}
</code></pre>
<p>it solve the problem for me now. it is releated to environment variable and typescript.</p>
","2024-08-11 13:51:19","1","Answer"
"78858483","","Error: [GoogleGenerativeAI Error]: Error fetching from ..: [400 ] API key not valid. Please pass a valid API key","<p>i am getiing this error:</p>
<pre><code>Error: [GoogleGenerativeAI Error]: Error fetching from https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent:
 [400 ] API key not valid. Please pass a valid API key. 
</code></pre>
<p>here is gemini-ai.ts code:</p>
<pre><code>import { GoogleGenerativeAI } from &quot;@google/generative-ai&quot;;   const apiKey = process.env.GEMINI_API_KEY as string ;  const genAI = new GoogleGenerativeAI(apiKey);  const model = genAI.getGenerativeModel({   model: &quot;gemini-1.5-flash&quot;, });  const generationConfig = {   temperature: 1,   topP: 0.95,   topK: 64,   maxOutputTokens: 8192,   responseMimeType: &quot;text/plain&quot;, };  export const chatSession = model.startChat({   generationConfig: generationConfig,   history: [], }); .
</code></pre>
<p>i think problem is related to environment variable.</p>
","2024-08-11 13:47:15","-1","Question"
"78844942","78833369","","<p>Adding index might not be that helpful and only improves less than 50% performance in your case (low cardinality field), and most time will be spent on HNSW. In fact, boolean filtering itself is super fast( 1ms&lt;) and doesn't really need any index.</p>
","2024-08-07 17:11:21","1","Answer"
"78844464","78841275","","<p>You can find those details at the <a href=""https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters"" rel=""nofollow noreferrer"">model parameters</a> documentation.</p>
<p>But in a short:</p>
<ul>
<li><code>max output tokens</code> limits the response max length. You literally limit how short (or long) you want your answer in tokens. Roughly speaking, just as a reference, 100 tokens is around 60-80 words.</li>
</ul>
<p>Gemini is a generative model which means that, in a high level explanation, it &quot;composes&quot; (or generates) an answer given its semantic knowledge in a given language (being a spoken language, a programming language, etc). So basically you can imagine a bag of possible &quot;next tokens&quot; when writing a sentence and top-k and top-p will customize the possible vocabulary to be considered.</p>
<ul>
<li><p>with <code>top-k</code> basically you limit the possible tokens universe. If the next tokens can be 200 possible different ones, you limit in the top first k ones. So <code>top-k = 30</code> means that the model you consider the first 30 tokens in the possible list. but the next tokens is not picked yet at this step.</p>
</li>
<li><p>with <code>top-p</code> you will work on a limit based on the cumulative probability. meaning: each token will have a probability related to how often the model saw the previous token followed by this token. So if you define <code>top-p = 20</code> each means that from the 30 token you limited with <code>top-k</code>, it will generate a new list with the tokens that sum a max probability of 20%. ie. if the first token has a 10% probability, the second has 5%, the third has 4% and the fifth has 2% - the list after the top-p analysis will contain the first, the second and the third (10% + 5% + 4%). Yet the next token is not picked in the step too.</p>
</li>
<li><p>finally comes the <code>temperature</code> parameter which defines how deterministic the next token will be picked. A temperature equals to 0 drives the more deterministic choice where the token with higher priority will be chosen; temperature at maximum will be the more random choice of next token, which means that even the less probable token may be chosen too.</p>
</li>
</ul>
<p>hope that helps.</p>
","2024-08-07 15:13:17","0","Answer"
"78841275","","What are Tokens, Top K and Top P?","<p>I'm learning to use Google AI Studio and when generating the snippet I came across these terms:</p>
<pre class=""lang-js prettyprint-override""><code>const generationConfig = {
  temperature: 1,
  topP: 0.95,
  topK: 64,
  maxOutputTokens: 8192,
  responseMimeType: &quot;text/plain&quot;,
};
</code></pre>
<p>I'm struggling to understand what those terms mean. What are <code>topP</code>, <code>topK</code>, and <code>maxOutputTokens</code>. I want to understand these in order to use them properly.</p>
","2024-08-06 22:55:28","0","Question"
"78836280","78836024","","<p>Unfortunately, you are going to have to resend all the context each time. Requests to openai conform to REST standards, meaning calls are stateless.</p>
","2024-08-05 20:09:48","2","Answer"
"78836024","","Efficiently Maintaining Chat Context with GenAI API (GPT, Claude) Without Resending All Prompts","<p>I'm using the GenAI API (GPT, Claude) to create a conversational AI that handles multi-turn dialogues. My goal is to maintain the context of the conversation without resending all previous prompts with each new request, as this approach quickly becomes expensive due to token usage.</p>
<h3>What I Have Tried</h3>
<p>Currently, I append each new user message and the assistant's responses to a list and send this entire list with each new API call:</p>
<pre><code>conversation_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message})
response = openai.ChatCompletion.create(model=&quot;gpt-3.5-turbo&quot;, messages=conversation_history)
conversation_history.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response['choices'][0]['message']['content']})
......
</code></pre>
<h3>My Question</h3>
<p>Is there a way to maintain the context of a conversation with the API without needing to resend all previous prompts and responses in each request? Ideally, I'm looking for a method to retain the conversational state on the server side or a more efficient way to manage the context.</p>
<ul>
<li>Using GPT or Claude API.</li>
<li>The primary concern is minimizing token usage to reduce costs.</li>
</ul>
","2024-08-05 18:54:04","3","Question"
"78833369","","Filtering results by gender: Adding a boolean field schema does not enhance search speed","<p>I have created a collection with the following specifications:</p>
<ul>
<li>Milvus Version: 2.4.4</li>
<li>CPU</li>
<li>Number of Entities: 20 million</li>
<li>Vector Field: One field of type float with a dimension of 512</li>
<li>Boolean Field: Represents gender, with a 50% probability for both male and female</li>
<li>Metric: COSINE</li>
<li>M: 64</li>
<li>efconstruction: 256</li>
<li>ef: 128</li>
<li>Index Type: HNSW</li>
<li>I did not configure values for partition, segment, or num_shards.
In my initial benchmark, I evaluated Milvus's performance against Numpy's dot product and was pleased with the results.</li>
</ul>
<p>Now, I want to add an additional field schema that also contains a boolean value indicating the gender of each embedding vector, allowing me to restrict queries based on gender. For instance, I aim to retrieve the 50 nearest neighbors that are male. To achieve this, I will generate gender data with an equal probability of 50%, resulting in half of the collection being male and the other half female.
I conducted benchmarks under this scenario, and the findings are outlined below. As illustrated in the plot, filtering results by gender did not confer any advantages; for example, in one case, the filtering was only 1.06 times faster than non-filtered queries.</p>
","2024-08-05 07:58:10","-1","Question"
"78830573","78767930","","<p>By default, in almost all cases, your interaction with model persists only through the one chat session, or to put it a bit differently, LLMs are stateless, so they don't retain any of the context input by a user.
Translation :</p>
<ul>
<li>only messages exchanged between you and the model are used for the following replies, and only during the single chat session. Every time you start a new chat, model starts with no knowledge of your previous exchanges.</li>
<li>there are also limits on how much messages are used for remembering, which is called 'context'. You will see 'context size' in model descriptions, that's what it means. Context can be used for other things too, but that's another topic.</li>
</ul>
<p>There are technique to achieve longer memory, which is also another topic, try searching for RAG (Retrieval Augmented Generation) and related 'embedding'.
Models can learn, yeah, another topic, check 'fine tuning'.</p>
","2024-08-04 08:19:00","1","Answer"
"78817467","78816015","","<p>way1: onnxruntime inference-session-impl.ts has following code</p>
<pre><code>  async release(): Promise&lt;void&gt; {
    return this.handler.dispose();
  }
</code></pre>
<p>so use <code>session.release()</code> release gpu mem</p>
<p>it will retain some gpu mem and release most gpu mem, when run infer with multi times, with <code>session.release()</code>, it will prevent gpu mem overflow</p>
<p>when run multi infers, run release after each infer, the gpu mem will</p>
<p>way2: following will relase all gpu mem but cannot use onnxruntime again(it need refresh to make onnxruntime work again)</p>
<p>I find the solution: <a href=""https://gpuweb.github.io/gpuweb/#dom-gpudevice-destroy"" rel=""nofollow noreferrer"">https://gpuweb.github.io/gpuweb/#dom-gpudevice-destroy</a>, in my case, use following ts code, it will free all gpu mem used by webgpu</p>
<pre><code>  const device = ort.env.webgpu.device;
  (device as any).destroy()
</code></pre>
","2024-07-31 16:34:46","0","Answer"
"78816015","","How to free webgpu gpu mem in onnxruntime web","<p>I use onnxruntime web with following code</p>
<pre><code>/**
 *
 * @param model don't pass session but pass model path and create session in infer inner. In this way, after infer finish, it will auto free gpu mem to prevent mem overflow
 * @param inputTensor
 */
export async function infer2(model: string, inputTensor: Tensor) {
  const session = await newSession(model)
  const feeds: any = {};
  const inputNames = session.inputNames;
  feeds[inputNames[0]] = inputTensor;
  const results = await session.run(feeds);
  const tensor = results[session.outputNames[0]]
  // await session.release() // free gpu mem
  await session.release() // free gpu mem
  return tensor;
}

/**
 * Load the ONNX model and perform inference
 * @param model don't pass session but pass model path and create session in infer inner. In this way, after infer finish, it will auto free gpu mem to prevent mem overflow
 * @param {onnxruntime.Tensor} inputTensor - Input tensor
 * @param {number[]} inputShape - Input tensor shape
 * @returns {Promise&lt;Float32Array&gt;} - Output tensor data
 */
export const infer = async (model: string, input: Ndarray) =&gt; {
  let inputTensor = ndarrayToTensor(input)
  const outTensor = await infer2(model, inputTensor);
  let na = new Ndarray(Array.from(outTensor.data as Float32Array) as number[], outTensor.dims as number[])
  inputTensor.dispose()
  outTensor.dispose()
  return na
  // const {data: out, dims: outShape} = results[session.outputNames[0]]
  // return {out: out as Float32Array, outShape: outShape as number[]}
};
</code></pre>
<p>and following is my test code</p>
<pre><code>  let input = await imgToNdarray(t);
  let out = await infer(model, input)
  let imgDataUrl = outToImgDataUrl(out)
  testReact(&lt;img src={imgDataUrl}/&gt;)
</code></pre>
<p>but fater infer, nvidia-smi show the gpu mem is still in use</p>
","2024-07-31 11:24:09","0","Question"
"78811718","78790401","","<p>As suggested by @Alex Alex, <a href=""https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html"" rel=""nofollow noreferrer"">Optical Flow</a> is certainly the way to go.</p>
<p>The idea of the code bellow is to compute Optical Flow between the two images, then to take a few points of the contours of each cell and average the flow of these point to obtain a &quot;cell flow&quot;. This flow is then displayed.</p>
<p>The result for your two images:
<a href=""https://i.sstatic.net/t80plAyf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t80plAyf.png"" alt=""enter image description here"" /></a></p>
<pre><code>def get_contour_center(contour):
    m = cv.moments(contour)
    if m['m00'] != 0: 
        cx = int(m['m10']/m['m00'])
        cy = int(m['m01']/m['m00'])

        return cx, cy
    return None

def draw_arrow(img, x, y, fx, fy, arrow_length_coef=2):
    lines = np.int32(np.array([x, y, x + fx*arrow_length_coef, y + fy*arrow_length_coef]) ).reshape(-1, 2, 2)

    cv.line(img, (int(x),int(y)),(int(x + fx*arrow_length_coef), int(y + fy*arrow_length_coef)), (0, 255, 0),1) 
    # cv.polylines(img, lines, 0, (0, 255, 0))
    cv.circle(img, (x, y), 1, (0, 255, 0), -1)
    return img

def draw_cell_movement(img1, img2, filtered_contour_list, result_image):

    img1_gray = cv.cvtColor(img1, cv.COLOR_BGR2GRAY) 
    img2_gray = cv.cvtColor(img2, cv.COLOR_BGR2GRAY) 

    # Calculates dense optical flow by Farneback method 
    flow = cv.calcOpticalFlowFarneback(img1_gray, img2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0) 

    for cnt in filtered_contour_list:
        
        # Take a few point of cell contour (here six at most)
        ids = np.random.randint(0,cnt.shape[0],min(cnt.shape[0],6))
        cell_flow = np.zeros(2)
        for id in ids:
            x, y = cnt[id,:,:][0]
            fx, fy = flow[y, x].T

            cell_flow += flow[y, x]
        cell_flow /= len(ids)

        # find cell center and draw arrow direction
        center = get_contour_center(cnt)
        if center:
            cx, cy = center
            result_image = draw_arrow(result_image, x, y, fx, fy)
    return result_image

img1 = cv.imread(&quot;/mnt/d/wsl/experiments/trasj/bac1.jpg&quot;)
img2 = cv.imread(&quot;/mnt/d/wsl/experiments/trasj/bac2.jpg&quot;)
result_image, filtered_contour_list = process_image(img1, size_threshold)
result_image = draw_cell_movement(img1, img2, filtered_contour_list, result_image)

plt.imshow(result_image)
plt.show()
</code></pre>
<p>Note that given that your contours are actually ellipses there is a smarter way to sample the contours points and to find the center.
Also I have modified your <code>process_image()</code> so that it also return the contours you used to draw your ellipses.</p>
","2024-07-30 12:27:47","0","Answer"
"78806714","78723306","","<p>It seems like issue between sh and bash invocations.<br />
One of many answers from SO: <a href=""https://stackoverflow.com/questions/3401183/bash-syntax-error-not-found"">here</a></p>
<p>Try :</p>
<pre><code>bash ./start.sh
</code></pre>
","2024-07-29 11:04:26","1","Answer"
"78797927","78738538","","<p>you can try making a more general request to the API and avoid giving a function call :</p>
<pre class=""lang-js prettyprint-override""><code>
import LlamaAI from &quot;llamaai&quot;;

const apiToken = process.env.LLAMA_SECRET_KEY;
const llamaAPI = new LlamaAI(apiToken);

const handleChat = async (req, res) =&gt; {

    try {
        const apiRequestJson = {
            model: &quot;llama-13b-chat&quot;,
            messages: [{ role: &quot;user&quot;, content: &quot;When's the next flight from Amsterdam to New York?&quot; }],
        };
        const response = await llamaAPI.run(apiRequestJson);

        if (response.choices[0].message.function_call) {
            const functionCall = response.choices[0].message.function_call;
            console.log(`Function to call: ${functionCall.name}`);
            console.log(`With arguments: ${JSON.stringify(functionCall.arguments)}`);
            // Esegui la logica appropriata, se possibile
        }

        res.json(response);
    } catch (err) {
        console.log(err);
        res.json(err);
    }
};

export default { handleChat };

</code></pre>
<p>you should get a similar response from the API:</p>
<pre><code>
{
    &quot;created&quot;: 1721993920,
    &quot;model&quot;: &quot;llama-13b-chat&quot;,
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 19,
        &quot;completion_tokens&quot;: 67,
        &quot;total_tokens&quot;: 86
    },
    &quot;choices&quot;: [
        {
            &quot;index&quot;: 0,
            &quot;message&quot;: {
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: &quot;According to flight schedules, the next flight from Amsterdam (AMS) to New York (JFK) is departing at 10:00 AM (CET) on [current date]. Please note that flight schedules are subject to change, and I recommend checking with the airline or a flight search engine for the most up-to-date information.&quot;,
                &quot;function_call&quot;: null
            },
            &quot;finish_reason&quot;: &quot;stop&quot;
        }
    ]
} 

</code></pre>
","2024-07-26 11:57:11","0","Answer"
"78797592","78797528","","<p>You have an implicit <code>shuffle=True</code> in your <code>tf.keras.preprocessing.image_dataset_from_directory()</code> call. Your validation dataset is shuffled each time you iterate over it.</p>
<p>In your first example, you run inference and you define your <code>y_true</code> in two separate steps, they get shuffled separately and don't match anymore.</p>
<p>In the second example, you define your predictions and ground truth at the same time, they are shuffled in the same way, and all is well.</p>
","2024-07-26 10:40:55","1","Answer"
"78797528","","Inconsistent Model Predictions When Using Entire Validation Dataset vs. Batch Sampling in TensorFlow","<p>I am training a deep learning model using TensorFlow and Keras on an image classification task. My model achieves high validation accuracy when evaluated using the validation_ds dataset. However, when I manually sample a batch from the validation dataset and make predictions, the results are significantly different and much worse.</p>
<p>Here is how I am creating and using the validation dataset:</p>
<pre><code>validation_ds = tf.keras.preprocessing.image_dataset_from_directory( 
  data_directory1, 
  validation_split=0.2,
  subset=&quot;validation&quot;,
  seed=123, 
  image_size=(img_height, img_width),
  batch_size=batch_size, )
</code></pre>
<p>When I use the entire validation dataset for predictions, I do the following:</p>
<pre><code># predict labels
y_pred = functional_model.predict(validation_ds)
y_pred_classes = np.argmax(y_pred, axis=1)

# true labels
y_true = np.concatenate([y.numpy() for x, y in validation_ds], axis=0)
y_true_classes = np.argmax(y_true, axis=1)

# Generate classification report
report = classification_report(y_true_classes, y_pred_classes, target_names=class_names)
print(&quot;Classification Report:&quot;)
print(report)
</code></pre>
<p>This approach shows much worse performance. However, when I manually sample a batch and make predictions like this:</p>
<pre><code>y_true = []
y_pred = []

for images, labels in validation_ds:
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    predictions = functional_model.predict(images)
    y_pred.extend(np.argmax(predictions, axis=1))

y_true = np.array(y_true)
y_pred = np.array(y_pred)

# Generate classification report
report = classification_report(y_true, y_pred, target_names=class_names)
print(&quot;Classification Report:&quot;)
</code></pre>
<p>The classification report shows reasonable results.</p>
<p>Why are the predictions inconsistent between using the entire validation dataset and a sampled batch?</p>
<p>How can I ensure consistent and accurate predictions for the validation dataset?</p>
","2024-07-26 10:29:46","1","Question"
"78794834","","My autogenstudio userproxy excute code is successful but without any output","<p>I have created an autonomous workflow with user proxy agent and my assistant agent.</p>
<p>For my assistant agent I provide a skill, and in the past it works. However recently my output from my user proxy agent is blank, even in the console the execution is successful. I don't get it.</p>
<pre class=""lang-none prettyprint-override""><code>exitcode: 0 (execution successful)
Code output:
</code></pre>
<p>In my skill (function) it returns the output I am looking for.
Besides, the final executed code did have <code>print()</code> at the end of the execution. However my user proxy shows Output which resulting my assistant agent saying that nothing is produced. I really don't understand why.</p>
<p>Below is my sample skill just for testing purpose:</p>
<pre><code>def add_two_numbers(number1, number2):
    print(number1 + number2)
    a=input() ## this is just to stop for a min in Python so I can view the result
    return number1 + number2
</code></pre>
<p>below is the console output:</p>
<pre class=""lang-none prettyprint-override""><code>userproxy1 (to test_sum_up_agent):

2024-07-25 13:50:25.319sum up 2,4
 |
--------------------------------------------------------------------------------
INFO     | autogenstudio.web.app:message_handler:38 - ** Processing Agent Message on Queue: Active Connections: [*] **
2024-07-25 13:50:25.334 | INFO     | autogenstudio.web.app:message_handler:45 - Sending message to connection_id: *. Connection ID: *

test_sum_up_agent (to userproxy1):

To sum up the numbers 2 and 4, you can use the `add_two_numbers` function from the `skills.py` file. Here is the Python code to do this:

```python
from skills import add_two_numbers

number1 = 2
number2 = 4

total = add_two_numbers(number1, number2)
print(total)
```

When you run this code, it will print the sum of the two numbers, which is 6.2024-07-25 13:50:31.096

--------------------------------------------------------------------------------
 |
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; EXECUTING CODE BLOCK (inferred language is python)...
INFO     | autogenstudio.web.app:message_handler:38 - ** Processing Agent Message on Queue: Active Connections: ['*'] **
2024-07-25 13:50:31.102 | INFO     | autogenstudio.web.app:message_handler:45 - Sending message to connection_id: *. Connection ID: *
userproxy1 (to test_sum_up_agent):

2024-07-25 13:50:48.344exitcode: 0 (execution succeeded)
**Code output:**
 |
INFO    --------------------------------------------------------------------------------
 | autogenstudio.web.app:message_handler:38 - ** Processing Agent Message on Queue: Active Connections: ['*'] **
2024-07-25 13:50:48.371 | INFO     | autogenstudio.web.app:message_handler:45 - Sending message to connection_id: *. Connection ID: *
test_sum_up_agent (to userproxy1):
2024-07-25 13:50:57.181
The task was to sum up the numbers 2 and 4. The Python code provided used the `add_two_numbers` function from the `skills.py` file to perform this task. The code was executed successfully and the sum of the numbers, which is 6, was printed. However, due to a technical issue, the output was not displayed. It was suggested to verify the result by running the code in a Python environment. The sum of the numbers 2 and 4 is confirmed to be 6.

TERMINATE |
</code></pre>
","2024-07-25 18:00:25","0","Question"
"78790401","","How do I track a contour from my previous image?","<p>So I have a video of bacterial cells moving around which I converted to frames. Now I want to find the instantaneous velocity of each bacterial cells. For this I am interested in finding by how much a bacterial cell moved but I dont know how to tell my programme to accurately identify that this specific bacteria moved. For example, lets say I have only two images. For each image, I have the COM coordinate of each bacteria. Now how do I relate this data. How do I ask my programme to accurately identify that this much was the change in COM for this specific bacteria. I have attached the two images for reference.</p>
<p><a href=""https://i.sstatic.net/Tj9oYiJj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tj9oYiJj.jpg"" alt=""sample image 1"" /></a>
<a href=""https://i.sstatic.net/MYNwsGpB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MYNwsGpB.jpg"" alt=""sample image 2"" /></a></p>
<p>A method I thought was to give each contour a unique id and associate the features of that contour with that unique id. For example its major axis and minor axis length. Then this way I can relate the contour's initial and final COM. But this idea assumes that all the bacterial cells are unique and that my code to accurately and precisely identifies contours of each bacterial cells which it does not. I have attached my code for finding contours of each bacterial cells too just in case you are interested. Can someone suggest some better ideas? Thankyou so much.</p>
<pre><code>import cv2 as cv
import numpy as np
from numpy.typing import NDArray
import math

def gaussian_filter_multiscale_retinex(image: NDArray, sigmas: list[float], weights: list[float]) -&gt; NDArray:
    img32 = image.astype('float32') / 255

    img32_log = cv.log(img32 + 1)

    msr = np.zeros(image.shape, np.float32)
    for sigma, weight in zip(sigmas, weights):

        blur = cv.GaussianBlur(img32, ksize=(0, 0), sigmaX=sigma)
        blur_log = cv.log(blur + 1)
        ssr = cv.subtract(img32_log, blur_log)
        ssr = cv.multiply(ssr, weight)

        msr = cv.add(msr, ssr)

    msr = cv.divide(msr, sum(weights))

    msr = cv.normalize(msr, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)
    return msr
def calculate_ellipse_area(ellipse):

    (cx, cy), (a, b), angle = ellipse
    semi_major_axis = a / 2
    semi_minor_axis = b / 2
    area = math.pi * semi_major_axis * semi_minor_axis
    return area,angle

def process_image(img, size_threshold):
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    rtnx = gaussian_filter_multiscale_retinex(gray, sigmas=[15, 55, 185], weights=[10, 5, 1])
    thresholded = cv.adaptiveThreshold(rtnx, 255, adaptiveMethod=cv.ADAPTIVE_THRESH_GAUSSIAN_C,
                                        thresholdType=cv.THRESH_BINARY, blockSize=7, C=-7)
    nb_components, output, stats, _ = cv.connectedComponentsWithStats(thresholded, connectivity=8)
    sizes = stats[1:, -1]
    new_img = np.zeros_like(thresholded)
    for i in range(0, nb_components - 1):
        if sizes[i] &gt;= size_threshold:
            new_img[output == i + 1] = 255
    connected_components = cv.connectedComponentsWithStats(new_img)
    (numLabels, labels, stats, centroids) = connected_components

    result_image = np.ones_like(img) * 255
    for i in range(1, numLabels):
        componentMask = (labels == i).astype('uint8')
        contours, _ = cv.findContours(componentMask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)
        if len(contours) &gt; 0:
            cnt = contours[0]
            if len(cnt) &gt;= 5:
                ellipse = cv.fitEllipse(cnt)
                area, angle = calculate_ellipse_area(ellipse)
                if area &lt; 250:
                    cv.ellipse(result_image, ellipse, (0, 0, 0), 1)  # Draw black contours on white background
    return result_image
img1path = &quot;/Users/yahya2/Desktop/1.png&quot;
img = cv.imread(img1path)
size_threshold = 16
result_image = process_image(img, size_threshold)

cv.imshow('Contours', result_image)
cv.waitKey(0)
cv.destroyAllWindows()
</code></pre>
","2024-07-24 20:14:25","1","Question"
"78778604","78712629","","<p>Same answer but to make it clear:
Change this:</p>
<p><code>agent = create_csv_agent( OpenAI(), csv_file, verbose=True)</code></p>
<p>to this:</p>
<p><code>agent = create_csv_agent( OpenAI(), csv_file, verbose=True, allow_dangerous_code=True)</code></p>
","2024-07-22 12:07:44","4","Answer"
"78777423","78713020","","<p>You can consider lowering the backup.parallelism.copydata parameter in the backup.yaml configuration file of the tool. You might want to set it to a lower number, such as 8 or 16.</p>
<p>Here's the link to the relevant section in the configuration file:</p>
<p><a href=""https://github.com/zilliztech/milvus-backup/blob/6f2556082165379bd40c782e630be812acd8d919/configs/backup.yaml#L52"" rel=""nofollow noreferrer"">https://github.com/zilliztech/milvus-backup/blob/6f2556082165379bd40c782e630be812acd8d919/configs/backup.yaml#L52</a></p>
","2024-07-22 07:25:46","0","Answer"
"78767930","","Does a model from LM Studio learn?","<p>I just started with local AI models, my question is if I download a model and ask for a task and correct it, does the model remember the correction for the next time, I meant, is it growing in knowledge?</p>
","2024-07-19 06:56:16","0","Question"
"78766865","78765368","","<p>Reserved IP Range name is used for VPC Peering.
The subnetwork allocation will use the range <em>name</em> if it's assigned.
Example: managed-notebooks-range-c</p>
<pre><code>PEERING_RANGE_NAME_3=managed-notebooks-range-c
gcloud compute addresses create $PEERING_RANGE_NAME_3 \
  --global \
  --prefix-length=24 \
  --description=&quot;Google Cloud Managed Notebooks Range 24 c&quot; \
  --network=$NETWORK \
  --addresses=192.168.0.0 \
  --purpose=VPC_PEERING
</code></pre>
<p>Example: <a href=""https://www.googlecloudcommunity.com/gc/Databases/Unable-to-create-PostGRE-SQL-db-with-Private-IP/m-p/655122/highlight/true"" rel=""nofollow noreferrer"">https://www.googlecloudcommunity.com/gc/Databases/Unable-to-create-PostGRE-SQL-db-with-Private-IP/m-p/655122/highlight/true</a></p>
","2024-07-18 22:15:27","0","Answer"
"78765368","","Reserved IP ranges for Vertex AI pipelines in GCP","<p>Can anyone explain what the &quot;reserved_ip_ranges&quot; parameter is in the GCP Vertex AI Pipeline job? From this doc: <a href=""https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob#google_cloud_aiplatform_PipelineJob_submit"" rel=""nofollow noreferrer"">https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob#google_cloud_aiplatform_PipelineJob_submit</a>
I have a shared vpc, peering with servicenetworking-googleapis-com (a project owned by gcp where my pipelines run), and several allocated IP ranges for this peering. But how to use specific IP address ranges in a Vertex AI Pipeline job?</p>
","2024-07-18 15:29:06","-1","Question"
"78764285","78762914","","<p>bm3d package you are using depends on a specific version of scipy causing the import error. To resolve this issue, you can try installing a compatible version of scipy along with the bm3d package.</p>
<p>Please use the following commands for this.</p>
<pre><code>!pip uninstall scipy -y 
!pip install scipy==1.8.0 
!pip install bm3d 

import bm3d 
</code></pre>
","2024-07-18 12:00:37","0","Answer"
"78763327","","Convert safetensors model format(LLaVA model) into gguf format","<p>I want to do LLaVA inference in ollama, so I need to convert it in gguf file format.
My model has the file format safetensors.(trained with lora)
It seems that ollama supports only llama, but not llava as shown here,
<a href=""https://github.com/ollama/ollama/blob/main/docs/import.md"" rel=""nofollow noreferrer"">https://github.com/ollama/ollama/blob/main/docs/import.md</a></p>
<p>I followed the instruction of llama.cpp, and used the code convert_lora_to_gguf.py here,
<a href=""https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py"" rel=""nofollow noreferrer"">https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py</a></p>
<p>But I get an error like,</p>
<pre><code>ERROR:lora-to-gguf:Model LlavaLlamaForCausalLM is not supported
</code></pre>
<p>If I write llama model in config.json of model file and run following code, then I got another error.</p>
<pre><code>model_instance.gguf_writer.add_string(gguf.Keys.General.TYPE, gguf.GGUFType.ADAPTER)
model_instance.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, &quot;lora&quot;)
model_instance.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, float(alpha))
model_instance.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
logger.info(&quot;Exporting model...&quot;)
model_instance.write()
logger.info(f&quot;Model successfully exported to {model_instance.fname_out}&quot;)
</code></pre>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\jjjy2\OneDrive\Desktop\VLM_FastAPI\ollama\convert_lora_to_gguf.py&quot;, line 373, in &lt;module&gt;
    model_instance.gguf_writer.add_string(gguf.Keys.General.FILE_TYPE, gguf.GGUFType.ADAPTER)
AttributeError: module 'gguf' has no attribute 'GGUFType'
</code></pre>
<p>It seems that all codes and gguf package don't support llava, but llama only. I have to convert my own trained model into gguf. I cannot use gguf llava model from hugging face for inference.</p>
<p>Is there a way to convert it?</p>
","2024-07-18 08:47:53","1","Question"
"78762914","","Unable to Import `bm3d` Package in Google Colab After Installation","<p>I am trying to use the <code>bm3d</code> package for image denoising in Google Colab. I was able to use it without any issues a few days ago. However, now I am facing an import error even though the package installs successfully.</p>
<ol>
<li><p>Install the <code>bm3d</code> package:
<code>!pip install bm3d</code></p>
</li>
<li><p>Try to import the <code>bm3d</code> package:
<code>import bm3d</code></p>
</li>
</ol>
<p>Observed Error:</p>
<pre><code>ImportError: cannot import name '_sub_module_deprecation' from 
'scipy._lib.deprecation' (/usr/local/lib/python3.10/dist-packages/scipy/_lib/deprecation.py)
</code></pre>
<p>How to resolve the issue and import the package in colab?</p>
","2024-07-18 07:16:33","0","Question"
"78762803","78751195","","<p>The default value for <code>indexNode.enableDisk</code> is true. This configuration determines whether disk indexing is supported. If set to true, disk indexing is enabled; if set to false, disk indexing is not supported. Disk indexing requires a hard disk. Sometimes, Milvus is deployed on hosts without a hard disk, which is why this configuration option is provided.</p>
<p>The type of index created depends on the <code>index_params</code> you specify when calling <code>create_index()</code>:</p>
<ul>
<li><code>FLAT</code>/<code>IVF_FLAT</code>/<code>HNSW</code>/<code>IVF_SQ8</code>/<code>IVF_PQ</code> — fully in memory</li>
<li><code>DISKINDEX</code> — partly in memory, partly on disk
It is not possible to store a <code>DISKINDEX</code> entirely in memory.</li>
</ul>
","2024-07-18 06:51:45","0","Answer"
"78762763","78754885","","<p>I solved the issue by incorporating twilio. We can also use AWS EC2 instance for it.</p>
<pre><code>const twilio = require(&quot;twilio&quot;); 

accountSid=&quot;xxxxxx&quot;
authToken=&quot;xxxxxx&quot;

const client = twilio(accountSid, authToken);

async function createToken() {
  const token = await client.tokens.create();

  console.log(token);
}
</code></pre>
","2024-07-18 06:41:58","0","Answer"
"78762646","78754885","","<p>As per this  <a href=""https://www.npmjs.com/package/@azure/communication-network-traversal"" rel=""nofollow noreferrer"">DOC</a> , the <code>@azure/communication-network-traversal</code> package is no longer maintained   and  the public preview of Azure Communication Services Network Traversal  is <a href=""https://azure.microsoft.com/en-us/updates/retirement-notice-azure-communication-services-network-traversal-turn-public-preview-is-retiring/"" rel=""nofollow noreferrer"">ending</a>.</p>
<p>We need to add extensions to the endpoint in order to reach the desired destination. For example, we need to utilize this endpoint in order to place a call using Azure communication services. Otherwise, we cannot simply access the endpoint as a rest API endpoint. <code>{endpoint}/callConnections/calling?api-version=2021-08-30-preview</code> Just so you know, this won't function until you give the access token.</p>
<p>The following are steps to CallMedia play:</p>
<ul>
<li>Create and configure a Postman collection</li>
<li>Add the pre-request script from this <a href=""https://learn.microsoft.com/en-us/azure/communication-services/tutorials/postman-tutorial#the-final-pre-request-script"" rel=""nofollow noreferrer"">link</a></li>
</ul>
<pre class=""lang-js prettyprint-override""><code>
const  dateStr = new  Date().toUTCString();

pm.request.headers.upsert({key:'Date', value: dateStr});

const  hashedBodyStr = CryptoJS.SHA256(pm.request.body.raw).toString(CryptoJS.enc.Base64)

  

pm.request.headers.upsert({

key:'x-ms-content-sha256',

value: hashedBodyStr

});

const  endpoint = pm.variables.get('endpoint')

const  hostStr = endpoint.replace('https://','');

const  url = pm.request.url.toString().replace('{{endpoint}}','');

  

const  stringToSign = pm.request.method + '\n' + url + '\n' + dateStr + ';' + hostStr + ';' + hashedBodyStr;

  

const  key = CryptoJS.enc.Base64.parse(pm.variables.get('key'));

const  signature = CryptoJS.HmacSHA256(stringToSign, key).toString(CryptoJS.enc.Base64);

pm.request.headers.upsert({

key:'Authorization',

value: &quot;HMAC-SHA256 SignedHeaders=date;host;x-ms-content-sha256&amp;Signature=&quot; + signature

});
</code></pre>
<ul>
<li>Connect the Cognitive Services to the Communication Service
<img src=""https://i.imgur.com/au36f8V.png"" alt=""Enter image description here"" />
Create two variables and add collection variables for Call Setup:</li>
<li>key - This variable is supposed to be one of your keys from the Azure portal's key page for your Azure Communication Services.</li>
<li>endpoint: The endpoint for your Azure Communication Services should be entered into this variable from the key page.
<strong>Ensure you remove the trailing slash</strong>. For example,  <code>https://contoso.communication.azure.com</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/RQ8mNlr.png"" alt=""Enter image description here"" /></p>
<p><strong>Call Setup with Cognitive Services:</strong></p>
<pre class=""lang-json prettyprint-override""><code>POST https://contoso.communications.azure.com/calling/callConnections?api-version=2021-08-30-preview
Authorization: Bearer {access_token}

{
  &quot;callbackUri&quot;: &quot;https://app.contoso.com/callback&quot;,
  &quot;targets&quot;: [
    {
      &quot;kind&quot;: &quot;phoneNumber&quot;,
      &quot;phoneNumber&quot;: {
        &quot;value&quot;: &quot;+919&quot;
      }
    }
  ],
  &quot;sourceCallerIdNumber&quot;: {
    &quot;value&quot;: &quot;+183&quot;
  },
  &quot;sourceDisplayName&quot;: &quot;Contoso Support&quot;,
  &quot;callIntelligenceOptions&quot;: {
    &quot;cognitiveServicesEndpoint&quot;: &quot;https://&lt;your-cognitive-service-endpoint&gt;.cognitiveservices.azure.com/&quot;
  }
}
</code></pre>
<p><img src=""https://i.imgur.com/BzVMntV.png"" alt=""Enter image description here"" /></p>
","2024-07-18 06:06:06","1","Answer"
"78761868","78761666","","<p>You can create that solution, using a combination of PHP+Python, there are several libraries that can operate images to get to a desired result, in case of employee ID verification, the python script should be capable of recognizing what is looking at, for this example let's there is a colleague in the company and a camera device or similar takes a picture of the ID:</p>
<p>Client side application will require constant read of the camera or a picture, here is the code for a camera and a local saved picture:</p>
<p><strong>Python Bar code reader: (Involving Numpy is a good idea to enhance the cv2 bar code recognition)</strong></p>
<pre><code>##import cv2
##img = cv2.imread('/home/jbsidis/Pictures/sofiaID.png')
##
##barcode_detector = cv2.barcode_BarcodeDetector()
##
### 'retval' is boolean mentioning whether barcode has been detected or not
##retval, points, _ = barcode_detector.detectAndDecode(img)
##
### copy of original image
##img2 = img.copy()
##
### proceed further only if at least one barcode is detected:
##if retval:
##    points = points.astype(np.int)
##    for i, point in enumerate(points):
##        img2 = cv2.drawContours(img2,[point],0,(0, 255, 0),2)
##
##
##print(retval,points,barcode_detector.detectAndDecode(img))

import sys
import cv2
import time
camera = cv2.VideoCapture(0)
if (camera.isOpened() == False):
    print(&quot;Can not open camera #0.&quot;)
    sys.exit(0)
print(&quot;Camera ready&quot;)
doAgain = True
while doAgain:
    ret, image = camera.read()
    if ret:
        qrCodeDetector = cv2.QRCodeDetector()
        text, points, _ = qrCodeDetector.detectAndDecode(image)
        if points is not None:
            print(text)
            cv2.imwrite(&quot;./result.jpg&quot;,image)
        else:
            print(&quot;QR code not detected&quot;)
        cv2.imshow(&quot;Image&quot;, image)
        key = cv2.waitKey(1) &amp; 0xFF
        if key == 27:
            cv2.destroyAllWindows()
            doAgain = False
camera.release()
</code></pre>
<p><strong>Server side application (PHP):</strong></p>
<pre><code>&lt;?php
//server side for the python process
$target_dir = &quot;uploads/&quot;; //add a directory where to save the file in the server
//this receives the data from the HTML form
if ($_SERVER[&quot;REQUEST_METHOD&quot;] == &quot;POST&quot;) {
    $file = $_FILES[&quot;picture&quot;];
    if ($file[&quot;type&quot;] != &quot;image/jpeg&quot; &amp;&amp; $file[&quot;type&quot;] != &quot;image/png&quot; &amp;&amp; $file[&quot;type&quot;] != &quot;image/gif&quot;) {
        echo &quot;This file is not allowed! IMages only please&quot;;
        exit();
    }
    if ($file[&quot;size&quot;] &gt; 1000000) {
        echo &quot;Error: File size is too large.&quot;;
        exit();
    }
    // here we are guessing that you have storage in your server or php server
    $filename = basename($file[&quot;name&quot;]);
    $target_path = $target_dir . $filename;
    move_uploaded_file($file[&quot;tmp_name&quot;], $target_path);

    echo &quot;THanks for uploading the image, Barcode is being recognized!&quot;;
} else {
    echo &quot;No file was uploaded. Try again please&quot;;
}
if (move_uploaded_file($file[&quot;tmp_name&quot;], $target_path)) {
    // this will run the script the python script we created, you have to modify to if this will read the png or the image file or a camera, in this case it must be the camera 
    $python_script = &quot;python /securepathtothescriptorsomeonecouldfindit/to/your/python/script.py &quot; . $target_path;
    $output = shell_exec($python_script);
    echo &quot;Barcode generated successfully!&quot;;
} else {
    echo &quot;There was an error processing your image, please contact support&quot;;
}
?&gt;
</code></pre>
<p>Client side (The user who is visiting your website or webapp):</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;User will upload Picture&lt;/title&gt;
    &lt;style&gt;
        .card {width: 900px;background-color: #f9f9f9;border: 1px solid #ddd;border-radius: 10px;box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);padding: 20px;margin: 40px auto;}
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class=&quot;card&quot;&gt;
        &lt;h2&gt;Upload Picture&lt;/h2&gt;
        &lt;form action=&quot;action.php&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;
            &lt;input type=&quot;file&quot; name=&quot;picture&quot; accept=&quot;image/*&quot;&gt;
            &lt;button type=&quot;submit&quot;&gt;Upload&lt;/button&gt;
        &lt;/form&gt;
        &lt;div id=&quot;image-preview&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;

    &lt;script&gt;
        
        const fileInput = document.querySelector('input[type=&quot;file&quot;]');

        
        fileInput.addEventListener('change', (event) =&gt; {
            const file = event.target.files[0];
            const reader = new FileReader();

            reader.onload = (event) =&gt; {
                const imageDataUrl = event.target.result;

                
                document.getElementById('image-preview').innerHTML = `&lt;img src=&quot;${imageDataUrl}&quot; alt=&quot;Uploaded Image width=200 height=550&quot;&gt;`;
            };

            reader.readAsDataURL(file);
        });
    &lt;/script&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Result:
<a href=""https://i.sstatic.net/Ja7Rc12C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ja7Rc12C.png"" alt=""enter image description here"" /></a></p>
<p>To test this, you can install XAMPP or a similar tool to locally run a PHP server and test it offline without cloud or you can use a public hosting compatible with PHP and test it.</p>
","2024-07-17 22:55:36","0","Answer"
"78761680","78761666","","<p>You could try by setting up a web server using Flask. By using it, you can next handle the file uploads from the HTML form, so, when an image is uploaded you can process it using a Python script to detect patterns like circles and squares.</p>
<p>Finally, you should send the result back to the user.</p>
<p>To resume:</p>
<ul>
<li>Set up Flask</li>
<li>Create the Flask application with python (there are a lot of good tutorials out there)</li>
<li>Create the HTML form</li>
<li>Run the Flask Application</li>
<li>Access the Web Application</li>
</ul>
<p>Hope this helps.</p>
","2024-07-17 21:29:24","1","Answer"
"78761666","","Operating on photo retrieved from php page using python","<p>I want to make pseudo-verification api in python that works with images(ID of a worker). It is supposed to be based on certain patterns in image like(circles squares etc. like in snapchat famous system). How do i run python script whenever image is sent from user using normal html input file form? I mean like user sends photo to server, it runs script in python and gives verdict? Thanks in advance.</p>
<p>I tried looking up problem on the internet. I didn't find any solutions. It seems like it's really expensive operation and probably suboptimatl, but neverthelles I want to do it that way.</p>
","2024-07-17 21:23:37","1","Question"
"78754885","","Azure Communication Services: Our services aren't available right now We're working to restore all services as soon as possible Please check back soon","<p>I am trying to use Azure Communication Services to create a user identity and then fetch relay configuration for network traversal.
Here is the code:</p>
<pre><code>const { CommunicationIdentityClient } = require(&quot;@azure/communication-identity&quot;);
const { CommunicationRelayClient } = require(&quot;@azure/communication-network-traversal&quot;);;

const main = async () =&gt; {
  console.log(&quot;Azure Communication Services - Relay Token Quickstart&quot;)

  const connectionString = &quot;CONNECTION STRING&quot;
 // Instantiate the identity client
  const identityClient = new CommunicationIdentityClient(connectionString);

  let identityResponse = await identityClient.createUser();
  console.log(`\nCreated an identity with ID: ${identityResponse.communicationUserId}`);

  const relayClient = new CommunicationRelayClient(connectionString);
  console.log(&quot;Getting relay configuration&quot;);

  const config = await relayClient.getRelayConfiguration(identityResponse);
  console.log(&quot;RelayConfig&quot;, config);

  console.log(&quot;Printing entire thing &quot;,config.iceServers);  
};

main().catch((error) =&gt; {
  console.log(&quot;Encountered and error&quot;);
  console.log(error);
})
</code></pre>
<p>I got error as</p>
<pre><code>&lt;h2&gt;Our services aren't available right now&lt;/h2&gt;&lt;p&gt;We're working to restore all services as soon as possible. Please check back soon.&lt;/p&gt;
</code></pre>
<p>I searched on community forum and tried to implement the solution but got the same error.</p>
<ul>
<li>The SOLUTION described was:</li>
</ul>
<p>**Solution:
&quot;we cannot directly access the endpoint as a rest API endpoint there are extensions to it that we need to add so that we can reach the correct destination like in order to place a call using Azure communication services we need to use this endpoint.</p>
<p>{endpoint}/calling/callConnections?api-version=2021-08-30-preview</p>
<p>FYI this will not work until you provide the access token&quot;**</p>
<ul>
<li>To which i modified the code as:</li>
</ul>
<pre><code>const { CommunicationIdentityClient } = require(&quot;@azure/communication-identity&quot;);
const axios = require('axios');

const main = async () =&gt; {
  console.log(&quot;Azure Communication Services - Relay Token Quickstart&quot;);

  const connectionString = &quot;CONNECTION STRING&quot;;

  // Extract endpoint and access key from the connection string
  const endpoint = connectionString.match(/endpoint=(.*?);/)[1];
  const accessKey = connectionString.match(/accesskey=(.*)/)[1];

  // Instantiate the identity client
  const identityClient = new CommunicationIdentityClient(connectionString);

  try {
    let identityResponse = await identityClient.createUser();
    console.log(`\nCreated an identity with ID: ${identityResponse.communicationUserId}`);

    // Issue an access token for the created user
    const tokenResponse = await identityClient.getToken(identityResponse, [&quot;voip&quot;]);
    const accessToken = tokenResponse.token;
    console.log(`\nIssued an access token: ${accessToken}`);

    // Construct the relay configuration request
    const relayConfigUrl = `${endpoint}/networkTraversal/:issueRelayConfiguration?api-version=2022-03-01-preview`;
    const response = await axios.post(relayConfigUrl, {
      id: identityResponse.communicationUserId
    }, {
      headers: {
        'Authorization': `Bearer ${accessToken}`,
        'Content-Type': 'application/json'
      }
    });

    console.log(&quot;RelayConfig&quot;, response.data);
    console.log(&quot;Printing entire thing &quot;, response.data.iceServers);
  } catch (error) {
    console.log(&quot;Encountered an error&quot;);
    console.log(error.response ? error.response.data : error.message);
  }
};

main().catch((error) =&gt; {
  console.log(&quot;Encountered an error&quot;);
  console.log(error);
});

</code></pre>
<p>I got SAME error.</p>
","2024-07-16 13:33:14","0","Question"
"78752226","78752135","","<p>I would like to suggest you to follow below considerations.</p>
<ul>
<li>Ensure that your Milvus collection is correctly indexed. Indexing plays a crucial role in how search results are retrieved and ordered. If the index configuration has changed or is not optimized, it might affect the retrieval quality.</li>
<li>In your screenshots, the consistency level is set to &quot;Bounded&quot;. Try experimenting with different consistency levels (e.g., &quot;Strong&quot; or &quot;Eventually&quot;) to see if it impacts the results. Consistency settings can influence the real-time availability of the indexed data.</li>
<li>Review the query parameters, especially the similarity_metric. Since you're using IP (Inner Product) as the similarity metric, ensure that your embedding vectors are normalized correctly. Inner Product search works best with normalized vectors.</li>
<li>Verify that the embedding vectors are of consistent quality and scale. If there were changes in the embedding model or preprocessing steps, it could lead to variations in the search results.</li>
<li>The inclusion of the embeddings_vector field in the output might affect the way Milvus scores and ranks the results. It's possible that returning the raw embeddings affects the internal ranking logic. Ensure that including this field does not inadvertently alter the search behavior.</li>
<li>Check the Milvus server logs and performance metrics to identify any anomalies or changes in the search behavior. This might provide insights into why the results differ when the embeddings_vector field is included.</li>
<li>Ensure that there are no version mismatches between the client (pymilvus) and the Milvus server. Sometimes, discrepancies between versions can cause unexpected behavior.</li>
<li>As a last resort, try modifying your code to exclude the embeddings_vector field programmatically during retrieval and compare the results. This can help isolate whether the issue is indeed caused by including the embeddings in the output.</li>
</ul>
<p>Please try out this code if it helps.</p>
<pre><code>vector_store = MilvusVectorStore(uri=MILVUS_URI,
                                 token=MILVUS_API_KEY,
                                 collection_name=collection_name,
                                 embedding_field='embeddings_vector',
                                 doc_id_field='chunk_id',
                                 similarity_metric='IP',
                                 text_key='chunk_text',
                                 output_fields=['chunk_id', 'chunk_text'])  # Exclude embeddings_vector

index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)
query_engine = index.as_query_engine(similarity_top_k=5, streaming=True)

rag_result = query_engine.query(prompt)
</code></pre>
","2024-07-15 23:27:33","0","Answer"
"78752135","","Inconsistent Query Results Based on Output Fields Selection in Milvus Dashboard","<p>I'm experiencing an issue with the Milvus dashboard where the search results change based on the selected output fields.</p>
<p>I'm working on a RAG project using text data converted into embeddings, stored in a Milvus collection with around 8000 elements. Last week, my retrieval results matched my expectations (&quot;good&quot; results), however, this week, the results have degraded (&quot;bad&quot; results).</p>
<p>I found that when I exclude the <code>embeddings_vector</code> field from the output fields in the Milvus dashboard, I get the &quot;good&quot; results; Including the <code>embeddings_vector</code> field in the output changes the results to &quot;bad&quot;.</p>
<p>I've attached two screenshots showing the difference in the results based on the selected output fields.</p>
<p>Any ideas on what's causing this or how to fix it?</p>
<p>Environment:</p>
<p>Python 3.11
pymilvus 2.3.2
llama_index 0.8.64</p>
<p>Thanks in advance!</p>
<pre><code>from llama_index.vector_stores import MilvusVectorStore
from llama_index import ServiceContext, VectorStoreIndex

# Some other lines..

# Setup for MilvusVectorStore and query execution
vector_store = MilvusVectorStore(uri=MILVUS_URI,
                                 token=MILVUS_API_KEY,
                                 collection_name=collection_name,
                                 embedding_field='embeddings_vector',
                                 doc_id_field='chunk_id',
                                 similarity_metric='IP',
                                 text_key='chunk_text')

embed_model = get_embeddings()
service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)
index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)
query_engine = index.as_query_engine(similarity_top_k=5, streaming=True)

rag_result = query_engine.query(prompt)
</code></pre>
<p>Here is the &quot;good&quot; result:
<a href=""https://i.sstatic.net/mj9uAODs.png"" rel=""nofollow noreferrer"">&quot;good&quot; result</a>
And here is the &quot;bad&quot; result:
<a href=""https://i.sstatic.net/Vk5S2Ith.png"" rel=""nofollow noreferrer"">&quot;bad&quot; result</a></p>
","2024-07-15 22:47:04","1","Question"
"78751195","","Why is DISKANN index type not supported on DataType.FLOAT_VECTOR with IP metric?","<p>I'm attempting to build an index using the DiskANN type, but I'm encountering an error stating that the index type is not supported. Here is my schema. I'm confused because it’s based on a float vector with the inner product (IP) metric, and according to the Milvus documentation <a href=""https://milvus.io/docs/disk_index.md"" rel=""nofollow noreferrer"">https://milvus.io/docs/disk_index.md</a>, this should be supported. Could someone explain why this might be happening?</p>
<pre><code>fields = [
    FieldSchema(
        name=&quot;id&quot;,
        dtype=DataType.INT64,
        is_primary=True,
        auto_id=False,
        max_length=100,
    ),
    FieldSchema(name=&quot;priority&quot;, dtype=DataType.FLOAT),
    FieldSchema(name=&quot;embeddings&quot;, dtype=DataType.FLOAT_VECTOR, dim=dim), ## dim = 100
]
schema = CollectionSchema(fields, &quot;points with id and priority&quot;)
points = Collection(&quot;points&quot;, schema, consistency_level=&quot;Strong&quot;)
points.insert(entities)
points.flush()

index_params: {
    &quot;index_type&quot;: &quot;DISKANN&quot;,
    &quot;metric_type&quot;: &quot;IP&quot;,
    &quot;params&quot;: {},
}


 points.create_index(&quot;embeddings&quot;, index_params)
</code></pre>
","2024-07-15 17:28:19","1","Question"
"78743675","78734964","","<p>I ran into the same issue, found the answer in <a href=""https://www.reddit.com/r/ollama/comments/1c8ddv8/comment/l3mj1ix/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button"" rel=""nofollow noreferrer"">this Reddit post</a>.</p>
<p>Set the <code>CUDA_VISIBLE_DEVICES</code> environment variable to <code>0,1</code> before running <code>ollama serve</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>:/# export CUDA_VISIBLE_DEVICES=0,1
:/# echo $CUDA_VISIBLE_DEVICES
0,1
:/# ollama serve
</code></pre>
<p>Tested on an A4000 pod.</p>
","2024-07-13 11:16:56","1","Answer"
"78738538","","Llama AI API not returning response","<p>I'm just trying to learn Llama AI API and I'm trying to get it to respond to the question &quot;What are common questions to ask on an NDA&quot;. but I'm unable to get an response. The content field in the response json is always NULL.</p>
<p>I tried the example given on their website but also was not able to get a response.</p>
<pre><code>const apiRequestJson = {
  &quot;messages&quot;: [
    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What are the common questions to ask to create an NDA for a food industry for an employee?&quot; }
  ],
  &quot;functions&quot;: [
    {
      &quot;name&quot;: &quot;get_nda_questions&quot;,
      &quot;description&quot;: &quot;Get the frequently asked questions to generate an NDA contract&quot;,
      &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
          &quot;Industry&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;description&quot;: &quot;The industry to generate this contract in&quot;
          },
          &quot;BusinessType&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;description&quot;: &quot;NDA for a business or individual&quot;
          }
        }
      },
      &quot;required&quot;: [&quot;Industry&quot;, &quot;BusinessType&quot;]
    }
  ],
  &quot;stream&quot;: false,
  &quot;function_call&quot;: &quot;get_nda_questions&quot;
};

app.get('/contract', async (req, res) =&gt; {
    try {
      const { default: LlamaAI } = await import('llamaai');
      const llamaAPI = new LlamaAI(apiToken);
  
      // Run the first API request
      const response = await llamaAPI.run(apiRequestJson);
      const output = response['choices'][0]['message'];
      

     // Parse the function call arguments
      const funcArguments = output['function_call']['arguments'];
  
      // Create flight_info object
      const flight_info = {
        &quot;Industry&quot;: funcArguments.Industry,
        &quot;BusinessType&quot;: funcArguments.BusinessType
      };
  
      // Convert flight_info to a JSON string
      const stringInfo = JSON.stringify(flight_info);      
  
      // Prepare the second API request JSON
      const second_api_request_json = {
        &quot;model&quot;: &quot;llama-70b-chat&quot;,
        &quot;messages&quot;: [
          {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What are the common questions to ask to create an NDA for a food industry for an employee?&quot;},
          {&quot;role&quot;: &quot;function&quot;, &quot;name&quot;: output['function_call']['name'], &quot;content&quot;: stringInfo}
        ],
        &quot;functions&quot;: [
            {
              &quot;name&quot;: &quot;get_nda_questions&quot;,
              &quot;description&quot;: &quot;Get the frequently asked questions to generate an NDA contract&quot;,
              &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                  &quot;Industry&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The industry to generate this contract in&quot;
                  },
                  &quot;BusinessType&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;NDA for a business or individual&quot;
                  }
                }
              },
              &quot;required&quot;: [&quot;Industry&quot;, &quot;BusinessType&quot;]
            }
          ]
      };
        
      const second_response = await llamaAPI.run(second_api_request_json);
      const second_output = second_response['choices'][0]['message']['content']; //This returns null for some reason
      
    res.send(second_response)      

    } catch (error) {
      res.status(500).json({ error: error.message });
    }
  });
</code></pre>
","2024-07-12 04:04:00","0","Question"
"78734964","","Ollama isnt using my gpu on a runpod.io pod","<p>I am testing different AI models on runpod.io. One of those models is dolphin-mixtral:8x22b. I followed Runpod's tutorial for setting up the pod with Ollama: <a href=""https://docs.runpod.io/tutorials/pods/run-ollama"" rel=""nofollow noreferrer"">https://docs.runpod.io/tutorials/pods/run-ollama</a>, and I used the H100SXM with 80GB VRAM and 16 vCPU 125 GB RAM.</p>
<p>However, when I start the model and ask it something like &quot;hey,&quot; it uses 100% of the CPU and 0% of the GPU, and the response takes 5-10 minutes.</p>
<p>How to make Ollama use my GPU?</p>
<p>I tried different server settings</p>
","2024-07-11 10:28:23","0","Question"
"78731219","78688976","","<p>Firstly, find vector embeddings of your facial database.</p>
<pre class=""lang-py prettyprint-override""><code>from deepface import DeepFace

students = [&quot;alice.jpg&quot;, &quot;bob.jpg&quot;, &quot;charlie.jpg&quot;]

for student in students:
   embedding = DeepFace.represent(img_path = student)[0][&quot;embedding&quot;]
   # store student and embedding in your database
</code></pre>
<p>Herein, the querying approach will be different according to the vector database you adopted. If you prefer to use postgres with pgvector extension.</p>
<pre class=""lang-py prettyprint-override""><code># real-time taken photo
target_img = &quot;target.jpg&quot;
target_embedding = DeepFace.represent(img_path = target_img)[0][&quot;embedding&quot;]

query = f&quot;&quot;&quot;
     SELECT *
     FROM (
         SELECT i.img_name, embedding &lt;-&gt; '{str(target_embedding)}' as distance
         FROM embeddings_datastore i
     ) a
     WHERE distance &lt; {threshold}
     ORDER BY distance asc
&quot;&quot;&quot;
</code></pre>
<p>the trick is to determine the correct threshold. the default facial recognition model in deepface is vgg-face. its pre-tuned threshold is 1.17 for euclidean distance as mentione <a href=""https://github.com/serengil/deepface/blob/master/deepface/modules/verification.py#L369"" rel=""nofollow noreferrer"">here</a>. so, the query will discard items with higher distance.</p>
<p>you can also adopt vector indexes such as annoy, voyager, faiss, nmslib or elasticsearch instead of a vector database. querying will be different according to your design but the idea will be same.</p>
<p>also, if the number of items in your database is not too big (~hundreds), then you may not need to use either vector index or vector database. postgres without pgvector extension, mongo, redis, cassandra or any relational database even sqlite would be fine.</p>
","2024-07-10 14:38:15","1","Answer"
"78728130","78656308","","<p>If you refer to the official document in snowflake, an error named “unknown model” occurs because it is not yet available for the region.
Please refer to the official document as following link.
<a href=""https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions</a></p>
","2024-07-09 23:58:46","0","Answer"
"78727757","78727719","","<p>If the data at <code>companies/$idCompany/quantities</code> is publicly readable, you may be able to point Gemini to the REST endpoint by appending <code>.json</code> to the URL:</p>
<pre><code>https://dbname-rtdb.firebaseio.com/companies/-idCompany/quantities.json
</code></pre>
<p>If the data is not publicly readable or Gemini can't read it, you'd have to read it in your own application code and pass the data into the Gemini prompt.</p>
<p>You might also want to look into <a href=""https://ai.google.dev/gemini-api/docs/function-calling"" rel=""nofollow noreferrer"">Function Calling with Gemini API</a>, which is a relatively new way to get external data like this injected into your Gemini prompts.</p>
","2024-07-09 21:13:55","1","Answer"
"78727719","","Can I use Gemini to analyze a JSON firebase RTDB?","<p>I have a DB at <a href=""https://dbname-rtdb.firebaseio.com/"" rel=""nofollow noreferrer"">https://dbname-rtdb.firebaseio.com/</a>. I would like to run a prompt in Gemini that says something like</p>
<blockquote>
<p>&quot;Do an analysis of the database on
<a href=""https://dbname-rtdb.firebaseio.com/companies/-idCompany/quantities/"" rel=""nofollow noreferrer"">https://dbname-rtdb.firebaseio.com/companies/-idCompany/quantities/</a>
and bring me the total quantity consumed for each type of product&quot;.</p>
</blockquote>
<p>When I try to use Gemini AI Studio within my Firebase console, Gemini returns a message that it cannot read external databases and should load the JSON directly.</p>
<p>Gemini really can't connect to databases securely to better enhance business data analysis?</p>
","2024-07-09 21:02:39","-1","Question"
"78724367","78724362","","<p>This is now supported.  Use the &quot;q&quot; param:</p>
<p><a href=""https://claude.ai/new?q=hello"" rel=""noreferrer"">https://claude.ai/new?q=hello</a></p>
<p>(Found by trial and error - this seems to be quite a recent addition.)</p>
","2024-07-09 07:56:25","5","Answer"
"78724362","","Can I use a query string variable to query Anthropic's claude.ai?","<p>I want to use a query string variable to send a question to Claude.  Is this possible?  There does not appear to be documentation for this.</p>
<p>This is highly useful for quickly sending queries, for example from Keyboard Maestro or Alfred on Mac.  It also allows quickly typing and sending a query, vs the current process of opening Claude.ai, waiting for it to load, then typing the query, which adds a fairly long and distracting pause in the middle of the thought process.</p>
<p>(OpenAI supports this via: <a href=""https://chatgpt.com?q=foo"" rel=""nofollow noreferrer"">https://chatgpt.com?q=foo</a> ... although there appears to be a bug where a lesser version is used even when the screen says 4o.)</p>
","2024-07-09 07:55:12","1","Question"
"78723306","","OpenWebUI + Pipelines (w/ langchain hopefully)","<p>I'm currently at the last step of <a href=""https://github.com/open-webui/pipelines"" rel=""nofollow noreferrer"">https://github.com/open-webui/pipelines</a>, and I tried to start the server, but it says the image below as my error. I'm not sure if the server is already running nor how to run a pipeline (like one of the examples) to test. Any advice would be greatly apprecieated!</p>
<p><a href=""https://i.sstatic.net/2fDpyZjM.png"" rel=""nofollow noreferrer"">error message</a></p>
<pre class=""lang-bash prettyprint-override""><code>lutzy@Lutzy:~/pipelines$ sh ./start.sh
RESET_PIPELINES_DIR is not set to true. No action taken.
./start.sh: 43: [[: not found
PIPELINES_REQUIREMENTS_PATH not specified. Skipping installation of requirements.
./start.sh: 115: Syntax error: redirection unexpected
</code></pre>
<p>I was expecting no error message, and more information on the docs on how to run one of the example pipelines.</p>
","2024-07-09 01:03:17","0","Question"
"78713314","78712629","","<p>The referenced security notice is in <a href=""https://api.python.langchain.com/en/latest/agents/langchain_experimental.agents.agent_toolkits.pandas.base.create_pandas_dataframe_agent.html"" rel=""nofollow noreferrer"">https://api.python.langchain.com/en/latest/agents/langchain_experimental.agents.agent_toolkits.pandas.base.create_pandas_dataframe_agent.html</a>.</p>
<p>Just do what the message tells you. Do a security analysis, create a sandbox environment for your thing to run in, and then add <code>allow_dangerous_code=True</code> to the arguments you pass to <code>create_csv_agent</code>, which just forwards the argument to <code>create_pandas_dataframe_agent</code> and run it in the sandbox.</p>
","2024-07-05 21:35:15","2","Answer"
"78713020","","Milvus backup tool is failing","<p>I am using the latest version of Milvus Backup. When attempting to back up the entire Milvus cluster, the process only backs up a few collections before getting stuck with the following error. Additionally, the meta directory is not created in the bucket.</p>
<pre><code>Error:
[2024/05/21 00:23:51.409 +00:00] [INFO] [core/backup_impl_create_backup.go:517] [&quot;Begin copy data&quot;] [dbName=misc_db] [collectionName=mld_rscls_USA] [segmentNum=637]
[2024/05/21 00:24:14.357 +00:00] [ERROR] [core/backup_impl_create_backup.go:525] [&quot;Fail to fill segment backup info&quot;] [collection_id=448161342919437791] [partition_id=448161342919437792] [segment_id=448161342941988531] [group_id=0] [error=&quot;Get empty input path, but segment should not be empty, milvus/s3-access/insert_log/448161342919437791/448161342919437792/448161342941988531/&quot;] [stack=&quot;github.com/zilliztech/milvus-backup/core.(BackupContext).backupCollectionExecute\n\t/app/core/backup_impl_create_backup.go:525\ngithub.com/zilliztech/milvus-backup/core.(BackupContext).executeCreateBackup.func2\n\t/app/core/backup_impl_create_backup.go:657\ngithub.com/zilliztech/milvus-backup/internal/common.(WorkerPool).work.func1\n\t/app/internal/common/workerpool.go:70\ngolang.org/x/sync/errgroup.(Group).Go.func1\n\t/go/pkg/mod/golang.org/x/sync@v0.3.0/errgroup/errgroup.go:75&quot;
</code></pre>
","2024-07-05 19:21:31","0","Question"
"78712629","","GPT LangChain experimental agent - allow dangerous code","<p>I'm creating a chatbot in VS Code where it will receive csv file through a prompt on  Streamlit interface.
However from the moment that file is loaded, it is showing a message with the following content:</p>
<blockquote>
<p>ValueError: This agent relies on access to a python repl tool which can execute arbitrary code. This can be dangerous and requires a specially sandboxed environment to be safely used. Please read the security notice in the doc-string of this function. You must opt-in to use this functionality by setting allow_dangerous_code=True.For general security guidelines, please see: <a href=""https://python.langchain.com/v0.2/docs/security/"" rel=""nofollow noreferrer"">https://python.langchain.com/v0.2/docs/security/</a></p>
</blockquote>
<p>Traceback</p>
<pre class=""lang-none prettyprint-override""><code>File &quot;c:\Users\  \langchain-ask-csv\.venv\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py&quot;, line 589, in _run_script
    exec(code, module.__dict__)
File &quot;C:\Users\ \langchain-ask-csv\main.py&quot;, line 46, in &lt;module&gt;
    main()
File &quot;C:\Users\  \langchain-ask-csv\main.py&quot;, line 35, in main
    agent = create_csv_agent( OpenAI(), csv_file, verbose=True)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;c:\Users\
\langchain-ask-csv\.venv\Lib\site-packages\langchain_experimental\agents\agent_toolkits\csv\base.py&quot;, line 66, in create_csv_agent
    return create_pandas_dataframe_agent(llm, df, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;c:\Users\ T\langchain-ask-csv\.venv\Lib\site-packages\langchain_experimental\agents\agent_toolkits\pandas\base.py&quot;, line 248, in create_pandas_dataframe_agent
    raise ValueError(
</code></pre>
<p>Here's is part of the code where I'm passing the file:</p>
<pre><code>def main():
    load_dotenv()
    
    # Load the OpenAI API key from the environment variable
    if os.getenv(&quot;OPENAI_API_KEY&quot;) is None or os.getenv(&quot;OPENAI_API_KEY&quot;) == &quot;&quot;:
        print(&quot;OPENAI_API_KEY is not set&quot;)
        exit(1)
    else:
        print(&quot;OPENAI_API_KEY is set&quot;)

    st.set_page_config(page_title=&quot;Ask your CSV&quot;)
    st.header(&quot;Ask your CSV 📈&quot;)

    csv_file = st.file_uploader(&quot;Upload a CSV file&quot;, type=&quot;csv&quot;)
    if csv_file is not None:

        agent = create_csv_agent( OpenAI(), csv_file, verbose=True)

        user_question = st.text_input(&quot;Ask a question about your CSV: &quot;)
        
        if user_question is not None and user_question != &quot;&quot;:
            with st.spinner(text=&quot;In progress...&quot;):
                st.write(agent.run(user_question))
      

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>I checked the link given as suggestion and also tried to search on similar reports but haven't had success.</p>
<p>What might be wrong and how to fix it?</p>
","2024-07-05 17:07:32","4","Question"
"78705977","","How to use LLama3 as copilot hosting on other device?","<p>I read about codegpt, it allows me to use every AI model in the same way as copilot is being used. I know I can use LLama3 there, but is there an option to place Llama3 on one device, and connect to that device with something like codegpt and ask questions? I want for multiple computers to use one model and I want to train that model</p>
","2024-07-04 08:41:47","0","Question"
"78702863","","AI search problem solving with infinite state spaces: when do algorithms halt?","<p>In classical artificial intelligence, there is an area of ​​problem solving by search. And there are several search algorithms, such as: Breadth-first search (BFS), Depth-first search (DFS), Iterative-deepening search (IDS), Uniform cost search (UCS), etc.</p>
<p>We consider that Depth-first search (DFS) is not complete, in infinite state spaces, even if the solution exists and can be found with a finite number of node expansions. That's the reason for introducing depth-first search (DFS) as a solution to this problem.</p>
<p>But, in this case, I was thinking the following, assuming that the graph starting from the initial node is infinite. And the objective node cannot be accessed from the initial state (we have a disconnected graph). In this case, the algorithm will not return failure, because it will always be able to expand the frontier. But soon after thinking about this issue, I realized that this problem occurs with all these search algorithms I mentioned. Because the classic criterion for returning failure is when the border is empty, and the solution has not yet been found, according to <a href=""https://aima.cs.berkeley.edu/cover.jpg"" rel=""nofollow noreferrer"">this book</a>:</p>
<p>Is my reasoning correct?</p>
","2024-07-03 15:03:22","0","Question"
"78699010","78688976","","<p>Yes, you've chosen the right approach.  My advice is to use Elasticsearch (built-in) capabilities to store huge data as well as to extremely fast compare long vectors.
I'm normally using <a href=""https://github.com/deepinsight/insightface"" rel=""nofollow noreferrer"">Insightface</a> for all face recognition tasks, so Elasticsearch docs have the following mapping:</p>
<pre><code>mapping = {
    &quot;properties&quot;: {
        &quot;face_id&quot;: {
            &quot;type&quot;: &quot;keyword&quot;
            },
        &quot;face_vector&quot;:{
            &quot;type&quot;: &quot;dense_vector&quot;,
            &quot;dims&quot;: 512,
            &quot;index&quot;: &quot;true&quot;,
            &quot;similarity&quot;: &quot;l2_norm&quot;
            },
        &quot;pic_file_path&quot;: {
            &quot;type&quot;: &quot;text&quot;,
            &quot;index&quot;: &quot;false&quot;
            },
        &quot;face_location&quot;: {
            &quot;type&quot;: &quot;integer&quot;,
            &quot;index&quot;: &quot;false&quot;
            },
        &quot;kps&quot;: {
            &quot;type&quot;: &quot;integer&quot;,
            &quot;index&quot;: &quot;false&quot;
            },
        &quot;gender&quot;: {
            &quot;type&quot;: &quot;byte&quot;
            },
        &quot;description&quot;: {
            &quot;type&quot;: &quot;text&quot;
            },
        &quot;descr_length&quot;: {
            &quot;type&quot;: &quot;integer&quot;
            }
        }
    }

}
</code></pre>
<p>The mapping will also allow to use extra fast KNN-search among millions of the students faces :)</p>
","2024-07-02 19:52:06","0","Answer"
"78688976","","Face Recognize From the Database using embedding technique","<p>I am working on a project aimed at recognizing whether a photo of any individual exists in the university's records. The proposed method involves storing the embeddings of each student's photo, along with their details, in a vector database. When a photo needs to be compared, the system will generate the embedding value for that photo and then compare this value against the database. If the value falls within a specific threshold, it will indicate that the individual exists in the record.</p>
<p>I am seeking expert advice on whether this approach is feasible. If there are any concerns with this method, I would appreciate recommendations for the best solution.</p>
","2024-06-30 15:09:55","-1","Question"
"78682998","78356777","","<p>IQ quantization uses an Importance Matrix (Imatrix) to determine the importance of different model activations during the quantization process.
This is an alternate quantization method to K quantization. The IQ quantization is generally a more advanced and higher-quality quantization technique than the legacy K-quant methods. Still, the optimal choice depends on the target hardware and performance requirements.</p>
<p>The &quot;M&quot;, &quot;S&quot;, &quot;XS&quot; and &quot;XXS&quot; suffixes in IQ quantization names refer to the model size, with &quot;M&quot; being the largest and &quot;XXS&quot; being the smallest. For example, the bitness is not exactly 3, as de M uses ~3.6 bits per parameter and XXS uses ~3.2 bits.</p>
","2024-06-28 14:19:43","-1","Answer"
"78681837","","Goal-Oriented Action Planning Regression Logic Confusion","<p>The image below shows the original example of GOAP regressive search by Dr. Jeff Orkin. It is explained that the planner searches regressively the space of actions for a sequence that will take the character from his starting state to his goal state. In each step of the regressive search, the planner tries to find an action that has an effect that will satisfy one of the unsatisfied goal conditions.</p>
<p>My question is as follows:<br />
Based on the example shown in the image, the regressive search result is goal -&gt; attack -&gt; load weapon -&gt; draw weapon, and when it comes to execution it goes in forward direction and becomes draw weapon -&gt; load weapon -&gt; attack goal. Each action from the action space only appears once. Assuming that the action space consists of [attack, load weapon, draw weapon, use potion, retreat], does the plan only represents the roadmap to achieve the goal instead of the full, complete list of actions? How does it deal with the situation for the regressive search where it requires multiple Attack actions to occur, such that the forward sequence is draw weapon -&gt; load weapon -&gt; attack -&gt; attack -&gt; attack? Or is this situation even valid in the first place?</p>
<p><a href=""https://i.sstatic.net/iVkFW99j.png"" rel=""nofollow noreferrer"">Dr. Jeff Orkin's example of regressive search</a></p>
<p>I tried looking up on the author's publication, went through ChatGPT, Gemini, and Google Scholars for any potential answers but it doesn't seem like I found any answer to my confusion. I would appreciate the help for helping me to clear up my confusions. Thank you in advance.</p>
","2024-06-28 10:03:26","1","Question"
"78681779","78649446","","<p>I had the same error and the problem was that I had named one of my agents using a name with a space. By removing the space, the error disappeared.</p>
","2024-06-28 09:52:26","9","Answer"
"78681392","78669401","","<p>Here, firstly you need to have a resource so, go to azure portal and search for translator resource.</p>
<p><img src=""https://i.imgur.com/kX9Yjcj.png"" alt=""enter image description here"" /></p>
<p>Click on create and provide the required details like subscription and resource group name and resource name.</p>
<ul>
<li>Enable managed identity for role assignments to get access.</li>
</ul>
<p><img src=""https://i.imgur.com/VPT69dp.png"" alt=""enter image description here"" /></p>
<p>Assign the below role that allows the user to give input and get the translated data.</p>
<p><img src=""https://i.imgur.com/6a47s0k.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/az4fzJF.png"" alt=""enter image description here"" /></p>
<p>We can also upload any text file, doc, or .mp4 file it will translate and provide as per the requirement.</p>
<ul>
<li>Go to ai.azure.com and check for the <code>AI services</code> and click on it.</li>
</ul>
<p><img src=""https://i.imgur.com/7cDUwp6.png"" alt=""enter image description here"" /></p>
<p>Click on document translator and create AI service resource. if you already having the resource, you can use it.</p>
<p><img src=""https://i.imgur.com/usmLeNh.png"" alt=""enter image description here"" /></p>
","2024-06-28 08:27:13","1","Answer"
"78673832","78672314","","<p>There are many different math functions that can be used to calculate similarity between two embedding vectors:</p>
<ul>
<li>Cosine distance,</li>
<li>Manhattan distance (L1 norm),</li>
<li>Euclidean distance (L2 norm),</li>
<li>Dot product,</li>
<li>etc.</li>
</ul>
<p>Each calculates similarity in a different way, where:</p>
<ul>
<li>The Cosine distance measures the cosine of the angle between two non-zero vectors. The Cosine distance is sensitive to the direction of the vectors and is less sensitive to the magnitude.</li>
</ul>
<p><a href=""https://i.sstatic.net/WxuYAMMwm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WxuYAMMwm.png"" alt=""Cosine distance"" /></a></p>
<ul>
<li>The Manhattan distance measures the absolute difference between the corresponding elements of two vectors. The Manhattan distance is sensitive to the magnitude of the vectors.</li>
</ul>
<p><a href=""https://i.sstatic.net/ykZPydn0m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ykZPydn0m.png"" alt=""Manhattan distance"" /></a></p>
<ul>
<li>The Euclidean distance measures the straight-line distance between two vectors.</li>
</ul>
<p><a href=""https://i.sstatic.net/cw5rfRgYm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cw5rfRgYm.png"" alt=""Euclidean distance"" /></a></p>
<ul>
<li>The Dot product measures the angle between two vectors multiplied by the product of their magnitudes.</li>
</ul>
<p><a href=""https://i.sstatic.net/yobdL90wm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yobdL90wm.png"" alt=""Dot product"" /></a></p>
<p><em><a href=""https://dzone.com/articles/beginners-guide-to-vector-similarity-search"" rel=""nofollow noreferrer"">Note: Image source for all four images</a></em></p>
<p>Consequently, the results of similarity calculations are different, where:</p>
<ul>
<li>The Cosine distance is always in the range [0, 2].</li>
<li>The Manhattan distance is always in the range [0, ∞).</li>
<li>The Euclidean distance is always in the range [0, ∞).</li>
<li>The Dot product is always in the range (-∞, ∞).</li>
</ul>
<p>See the table below.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Measure</th>
<th>Range</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cosine distance</td>
<td>[0, 2]</td>
<td>0 if vectors are the same, 2 if they are diametrically opposite.</td>
</tr>
<tr>
<td>Manhattan distance</td>
<td>[0, ∞)</td>
<td>0 if vectors are the same, increases with the sum of absolute differences.</td>
</tr>
<tr>
<td>Euclidean distance</td>
<td>[0, ∞)</td>
<td>0 if vectors are the same, increases with the sum of squared differences.</td>
</tr>
<tr>
<td>Dot product</td>
<td>(-∞, ∞)</td>
<td>Measures alignment, can be positive, negative, or zero based on vector direction.</td>
</tr>
</tbody>
</table></div>
","2024-06-26 17:19:33","2","Answer"
"78672314","","Why a difference in Score for Manhattan distance vs Cosine Distance despite same text chunk being returned?","<p>I am using the <a href=""https://python-client.qdrant.tech/qdrant_client.qdrant_client"" rel=""nofollow noreferrer"">qdrant DB and client</a> for embedding a document as part of a PoC that I am working on in building a RAG.</p>
<p>I see that when I use a <em>Manhattan distance</em> to build the vector collection I get a high score than when I use the <em>Cosine distance</em>. However, <strong>the text chunk returned is the same</strong>. I am not able to understand why and how? I am learning my ropes here at RAG still. Thanks in advance.</p>
<p><strong>USER QUERY</strong></p>
<pre><code>What is DoS?
</code></pre>
<p><strong>COSINE DISTANCE</strong></p>
<pre><code>response: [
ScoredPoint(id=0, 
version=10, 
score=0.17464592, 
payload={
'chunk': &quot;It also includes overhead bytes for operations, 
administration, and maintenance (OAM) purposes.\nOptical Network Unit 
(ONU)\nONU is a device used in Passive Optical Networks (PONs). It converts 
optical signals transmitted via fiber optic cables into electrical signals that 
can be used by end-user devices, such as computers and telephones. The ONU is 
located at the end user's premises and serves as the interface between the optical 
network and the user's local network.&quot;
}, 
vector=None, shard_key=None)
]
</code></pre>
<p><strong>MANHATTAN DISTANCE</strong></p>
<pre><code>response: [
ScoredPoint(id=0, 
version=10, 
score=103.86209, 
payload={
'chunk': &quot;It also includes overhead bytes for operations, administration, 
and maintenance (OAM) purposes.\nOptical Network Unit 
(ONU)\nONU is a device used in Passive Optical Networks (PONs). It converts 
optical signals transmitted via fiber optic cables into electrical signals that 
can be used by end-user devices, such as computers and telephones. The ONU is 
located at the end user's premises and serves as the interface between the optical 
network and the user's local network.&quot;
}, 
vector=None, shard_key=None)
]
</code></pre>
","2024-06-26 12:07:11","0","Question"
"78670750","78459632","","<p>You might try to increase the temperature(closer to 1 for more variation) to induce variability in the response. But generally speaking if all you want is it to randomly select books wont a random() function suffice?</p>
","2024-06-26 07:02:11","0","Answer"
"78669401","","Translating Documents in Azure AI Translator","<p>Can someone please point me to a good, easy-to-understand resource on how to get documents translated within Azure AI Translator? Ideally, a step-by-step instruction video. Thanks!</p>
<p>The Azure AI interface is incredibly cumbersome, hard to understand, and most online resources for navigating it are out-of-date and/or incorrect. I have not found any resources at Microsoft helpful.</p>
","2024-06-25 20:26:32","0","Question"
"78661589","78661505","","<p>As the message states. <strong>Gemini in Android Studio</strong> currently not available in your country.</p>
<p>Remember Gemini the web application is not the same as Gemini in Android Studio.   This is a different system.</p>
<p>You should check the <a href=""https://developer.android.com/studio/preview/gemini/availability"" rel=""nofollow noreferrer"">Gemini in Android Studio availability</a> page to ensure that your country is supported.</p>
","2024-06-24 09:30:39","-1","Answer"
"78661505","","Gemini AI is not available in my Android Studio","<p>It's well known that Gemini is now supported in Android Studio. But as we know that Gemini has a limitation for some country. Well, I can run Gemini with all of my Google Accounts at the <a href=""https://gemini.google.com"" rel=""nofollow noreferrer"">Gemini Webstie</a>:</p>
<p><a href=""https://i.sstatic.net/mLfpXOCD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mLfpXOCD.png"" alt=""My Gemini Web"" /></a></p>
<p>But for some reason that I don't know, I can't run Gemini in my Android Studio with the same Google Account. It appears like this:</p>
<p><a href=""https://i.sstatic.net/Wx304Bjw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wx304Bjw.png"" alt=""My Android Studio Gemini"" /></a></p>
<p>My Android Studio is the latest version (Koala) and I'm running on my Mac. Is there any configuration that I need to change or something?</p>
<p>Edit: I live in Indonesia, and Google stated that Gemini is available in Indonesia in <a href=""https://d.android.com/r/studio-ui/gemini/availability"" rel=""nofollow noreferrer"">this site</a>. My friends got Gemini in their Android Studio. But why don't I?</p>
","2024-06-24 09:13:10","1","Question"
"78658918","78325075","","<p>Finally I found the solution:</p>
<ul>
<li><p>for images, we can directly insert the base-64 encoded data(without data:image/....;base64,...) and/or we can use the file API</p>
</li>
<li><p>for large files such as audios/videos, its required to use the file API to insure the data(buffer) not crash the application</p>
</li>
</ul>
<p>The file API is a free service that can store up to 20GB of files(each file should not more than 2GB) and lasts for 48hrs from the time of upload</p>
<p>To use the file API, you can check the official gemini documentation</p>
<p>To include a file, you can use {fileData: {fileUri: &quot;&quot;, mimeType: &quot;&quot;}}</p>
<p><strong>A specific note about videos</strong>: all video files have a &quot;state&quot; Parameter which can be &quot;PROCESSING&quot;, &quot;FAILED&quot;, &quot;ACTIVE&quot;. Only videos with &quot;state&quot; of &quot;ACTIVE&quot; can be included in the model's requests</p>
","2024-06-23 14:16:32","0","Answer"
"78656308","","RAG framework not not working for many model listed ""snowparkSQLException 1304 -- unknown model \snowflake-arctic""","<p>I am referring below RAG example to do some experiment with some private data(some sample pdf).</p>
<p><a href=""https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/index.html#4"" rel=""nofollow noreferrer"">https://quickstarts.snowflake.com/guide/asking_questions_to_your_own_documents_with_snowflake_cortex/index.html#4</a></p>
<p>however the model works for the first - 'mixtral-8x7b' not for others and getting below error</p>
<pre><code>&quot;snowparkSQLException 1304 -- unknown model \snowflake-arctic&quot;
</code></pre>
<p>i used the exact code but with different pdf file(which i don't think making problem over here)</p>
<p>do i need to import other mode into the snowflake environment ?</p>
<p>Any solution</p>
","2024-06-22 14:16:47","0","Question"
"78650127","77968083","","<p>here is an example of setting the composite key: You will pass a dictionary in the <code>key</code> attribute:</p>
<pre><code>key={'pk': f&quot;history::{tenant_id}&quot;, 'sk': f&quot;1#{user_id}&quot;},
</code></pre>
<p>My table has partition key of &quot;pk&quot; and hashkey of &quot;sk&quot;, you do not need to pass the partition key in and if you do it will be ignored if you pass the key attribute as well.</p>
","2024-06-21 01:43:19","0","Answer"
"78650116","78607141","","<p><strong>A simple solution using regex:</strong></p>
<p>First, we must remove the accents from your text to make matching easier. Then, we can use the indices returned by the Regex to index into the input data to retrieve the original accented string. This approach is similar to extinctsion's approach, however, they differ in the normalization, stripping, and Regex use. Feel free to delete WriteLines() and or replace them with messageboxes.</p>
<p>I agree with t2solve in the fact that this problem does not need to be solved by AI. If your goal is to find <em>similar</em> text, not precise matches, then that needs to be specified. My answer finds precise matches, a decision guided by your expected outputs. There isn't usually a &quot;correct&quot; answer for AI. Similarity is also subjective, so clarification in what you want would be beneficial if you wanted answers specific to what you really wanted.</p>
<pre><code>using System.Globalization;
using System.Text;
using System.Text.RegularExpressions;

//Removes Diacritics from a string; credit to Blair Conrad and Marcel Popescu
string RemoveDiacritics(string text)
{
    var normalizedString = text.Normalize(NormalizationForm.FormD);
    var stringBuilder = new StringBuilder(capacity: normalizedString.Length);

    for (int i = 0; i &lt; normalizedString.Length; i++)
    {
        char c = normalizedString[i];
        var unicodeCategory = CharUnicodeInfo.GetUnicodeCategory(c);
        if (unicodeCategory != UnicodeCategory.NonSpacingMark)
        {
            stringBuilder.Append(c);
        }
    }

    return stringBuilder
        .ToString()
        .Normalize(NormalizationForm.FormC);
}

public class InputData
{
    public string Text { get; set; }
}

private void btnGo_Click(object sender, EventArgs e)
{

    //Original inputs and keywords
    var inputData = new List&lt;InputData&gt;
    {
        new() { Text = &quot;İşçilik Alacaklarında Kısmi Dava ve Belirsiz Alacak Davası Müesseselerinin İşletilmesi&quot; }
    };

    var keywords = new List&lt;string&gt; { &quot;kısmi dava&quot;, &quot;üçüncü kişi&quot;, &quot;işçi&quot;, &quot;işçilik alacakları&quot;, &quot;vazife maluliyeti&quot;, &quot;belirsiz alacak davası&quot;, &quot;yabancı banka&quot;, &quot;adli yargı&quot;, &quot;anonim şirket&quot;, &quot;ayni hak&quot;, &quot;ayni hak talebi&quot;, &quot;bohçacı sözleşmesi&quot; };

    //Remove diacritics from both OriginalInput and keywords
    string strippedInput = RemoveDiacritics(inputData);
    List&lt;string&gt; strippedKeywords = keywords.Select(RemoveDiacritics).ToList();

    Console.WriteLine(&quot;Stripped Input: &quot; + strippedInput);
    Console.WriteLine(&quot;Stripped Keywords: &quot; + string.Join(&quot;, &quot;, strippedKeywords));
    Console.WriteLine();

    //Convert stripped keywords to regexes. The @&quot;[^ ]*&quot; matches until a space character.
    Regex[] regexMatchForeachKeyword = strippedKeywords.Select(keyword =&gt; new Regex(keyword + &quot;[^ ]*&quot;, RegexOptions.IgnoreCase)).ToArray();
    List&lt;Match&gt; matches = new();

    //Find and record matches
    foreach (Regex regex in regexMatchForeachKeyword)
    {
        Match m = regex.Match(strippedInput);

        if (m.Success)
        {
            Console.WriteLine($&quot;{regex} matched: &quot; + m + $&quot; (Index: {m.Index}, Length: {m.Length})&quot;);
            matches.Add(m);
        }
    }

    //Return matches
    Console.WriteLine();
    Console.WriteLine(&quot;All matches: &quot; + string.Join(&quot;, &quot;, matches));
    string[]  matchesWithDiacritics = matches.Select(match =&gt; inputData[match.Index..(match.Index + match.Length)]).ToArray();
}
</code></pre>
<p><strong>Here's the output from the code I quickly ran:</strong></p>
<pre><code>Stripped Input: Iscilik Alacaklarında Kısmi Dava ve Belirsiz Alacak Davası Muesseselerinin Isletilmesi

Stripped Keywords: kısmi dava, ucuncu kisi, isci, iscilik alacakları, vazife maluliyeti, belirsiz alacak davası, yabancı banka, adli yargı, anonim sirket, ayni hak, ayni hak talebi, bohcacı sozlesmesi


kısmi dava[^ ]* matched: Kısmi Dava (Index: 22, Length: 10)
isci[^ ]* matched: Iscilik (Index: 0, Length: 7)
iscilik alacakları[^ ]* matched: Iscilik Alacaklarında (Index: 0, Length: 21)
belirsiz alacak davası[^ ]* matched: Belirsiz Alacak Davası (Index: 36, Length: 22)

All matches: Kısmi Dava, Iscilik, Iscilik Alacaklarında, Belirsiz Alacak Davası
</code></pre>
<p>If İşçilik should not be a keyword because it is already contained in İşçilik Alacaklarında, then you can do a quick check after the matches are complete to make sure there are no duplicates or check during the matching process itself, but that's a bit more complicated.</p>
<p>If you want more speed with the Regex, you can precompile the Regex using the built-in RegexGenerator attribute, which compiles the Regex to assembly for faster execution. Because your keywords are variable, and there may be many of them, creating compiled regex for each of your keywords may be difficult. You can read more about the RegexGenerator <a href=""https://learn.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-source-generators?pivots=dotnet-8-0"" rel=""nofollow noreferrer"">here</a>.</p>
<p>If you want the real string with diacritics returned, use the Regex's information to do so, which would look something like:</p>
<pre><code>string[] matchesWithDiacritics = matches.Select(match =&gt; OriginalInput[match.Index .. (match.Index + match.Length])).ToArray();
</code></pre>
<p>Just so you know, running this 100,000 times on the title given took around 276ms on my computer (Macbook Pro 2018), which is less than 0.003ms per title.</p>
<p>I hope this helps!
~Complex</p>
","2024-06-21 01:36:47","1","Answer"
"78649446","","AutoGen GroupChat error code (openai.BadRequestError: Error code: 400)","<p>I'm pretty new to using AutoGen so I don't know for sure if this is a simple problem to fix but I created two simple agents with the user_proxy to communicate with each other through the &quot;GroupChat&quot; function. However, after the first response from the first agent, it leads to an error code 400 from openai. The following below is the exact error code and I don't really know what the issue is.</p>
<p>openai.BadRequestError: Error code: 400 - {'error': {'message': &quot;Invalid 'messages[2].name': string does not match pattern. Expected a string that matches the pattern '^[a-zA-Z0-9_-]+$'.&quot;, 'type': 'invalid_request_error', 'param': 'messages[2].name', 'code': 'invalid_value'}}</p>
<p>I've been following the tutorials on the AutoGen Github repo and I don't think I've seen anyone really run into this problem.</p>
<p>At first I thought it was just an issue between using different LLMs so I decided to keep it to one LLM (GPT-4) and the issue is still recurring. Any insight?</p>
","2024-06-20 20:20:55","3","Question"
"78643566","78643476","","<p>If you need image (as <code>PIL</code>) only on server then you can use directly</p>
<pre><code>from PIL import Image

cloth_image = Image.open( request.files[&quot;cloth_image&quot;] )
</code></pre>
<p>Because <code>files[&quot;cloth_image&quot;]</code> has method <code>.read()</code> so it can be treated as file-like object, and <code>Image.open()</code> can use file-like object instead of filename.</p>
<p>And you can convert between <code>pillow</code> and <code>numpy</code>(and <code>opencv</code>)</p>
<pre><code># from pillow to numpy/opencv
arr = numpy.array(cloth_image)

# from numpy/opencv to pillow
cloth_image = Image.fromarray(arr)
</code></pre>
<p>(<code>opencv</code> may need to convert colors between <code>RGB</code> and <code>BGR</code>)</p>
<hr />
<p>Minimal working code:</p>
<pre><code>from flask import Flask, request
from PIL import Image

app = Flask(__name__)

@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST':
        if &quot;cloth_image&quot; in request.files:
            img = request.files[&quot;cloth_image&quot;]  # it has method .read() so it can be treated as file-like object
            cloth_image = Image.open(img)  # it can use file-like object instead of filename
            # ... process image ...

    return &quot;&quot;&quot;
    &lt;form method=&quot;POST&quot; enctype=&quot;multipart/form-data&quot;&gt;
    &lt;input type=&quot;file&quot; name=&quot;cloth_image&quot;&gt;
    &lt;button type=&quot;submit&quot;&gt;Send&lt;/button&gt;
    &lt;/form&gt;&quot;&quot;&quot;

if __name__ == '__main__':
    app.run()
</code></pre>
<p>But if you need to send it back then you may need to use <code>io.BytesIO</code> to save image in file-like object in memory, convert it to <code>base64</code>, and display it as</p>
<pre><code>html = '&lt;img src=&quot;data:image/png;base64,{}&quot;&gt;'.format(data)
</code></pre>
<p>More complex code which gets image, draw rectangle and send it back to browser.</p>
<pre><code>from flask import Flask, request
from PIL import Image, ImageDraw
import io
import base64

app = Flask(__name__)

@app.route('/', methods=['GET', 'POST'])
def index():
    img = 'empty'
    
    if request.method == 'POST':
        if &quot;image&quot; in request.files:
            print(request.files)

            # get image
            img = request.files[&quot;image&quot;]   # it has method .read()

            # read to pillow
            image = Image.open(img)

            # draw something
            draw = ImageDraw.Draw(image)                                      # https://pillow.readthedocs.io/en/stable/reference/ImageDraw.html
            draw.rectangle([(20,20), (280,280)], outline=(255,0,0), width=3)  # https://pillow.readthedocs.io/en/stable/reference/ImageDraw.html#PIL.ImageDraw.ImageDraw.rectangle
            draw.text((15,5), &quot;Hello World!&quot;)                                 # https://pillow.readthedocs.io/en/stable/reference/ImageDraw.html#PIL.ImageDraw.ImageDraw.text

            # convert to file-like data
            obj = io.BytesIO()             # file in memory to save image without using disk  #
            image.save(obj, format='png')  # save in file (BytesIO)                           # https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.save
            obj.seek(0)

            # convert to bases64
            data = obj.read()              # get data from file (BytesIO)
            data = base64.b64encode(data)  # convert to base64 as bytes
            data = data.decode()           # convert bytes to string

            # convert to &lt;img&gt; with embed image
            img = '&lt;img src=&quot;data:image/png;base64,{}&quot;&gt;'.format(data)

    return '&lt;form method=&quot;POST&quot; enctype=&quot;multipart/form-data&quot;&gt;&lt;input type=&quot;file&quot; name=&quot;image&quot;&gt;&lt;button type=&quot;submit&quot;&gt;Send&lt;/button&gt;&lt;/form&gt;&lt;br&gt;' + img

if __name__ == '__main__':
    app.run()
</code></pre>
<hr />
<p>And if you need it without reloading page then you may have to use JavaScript. But I skip this problem.</p>
","2024-06-19 16:04:26","1","Answer"
"78643476","","How can I convert Gradio app to flask api?","<p>I am trying to convert Gradio app to flask api.
In Gradio app, it is using components like Image, Text, and so on.
I have to change that component to request form data.
However, I am suffering with image processing.
I need any help.</p>
<p>This is my code.
<code>cloth_image = gr.Image(label=&quot;Your label...&quot;, type=&quot;pil&quot;) cloth_mask_image = gr.Image(label=&quot;Your label...&quot;, type=&quot;pil&quot;)</code></p>
<p>How can I get the same return value as gr.Image component by preprocessing of <code>cloth_image = request.files['cloth_image']</code></p>
","2024-06-19 15:41:17","0","Question"
"78636421","78607809","","<p>Go into Settings and click components. You should be able to see a Predictive Code Completion Model <a href=""https://i.sstatic.net/oThDeHiA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oThDeHiA.png"" alt=""Screenshot of component"" /></a>.</p>
","2024-06-18 09:03:52","2","Answer"
"78634762","","Tooling with Langchain Bedrock for RAG AI-Chat Generation","<p>I have a function that takes in a Langugaue Model, a vector store, question and tools; and returns a response, at the moment the tools argument is not being added because based on this <a href=""https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/"" rel=""nofollow noreferrer"">example</a> the function <code>.bind_tools</code> is not an attribute of the <code>llm</code> -&gt; llm is below</p>
<pre><code>## Bedrock Client
bedrock_client = boto3.client(service_name=&quot;bedrock-runtime&quot;, region_name=&quot;us-west-2&quot;)
bedrock_embeddings = BedrockEmbeddings(model_id=&quot;amazon.titan-embed-text-v1&quot;, client=bedrock_client)

llm=Bedrock(model_id=&quot;anthropic.claude-v2:1&quot;, client=bedrock_client,
                model_kwargs={'max_tokens_to_sample': 512})
</code></pre>
<p>without changing the LLM to <code>ChatOpenAPI</code> as in the example reference how do a bind a tool to langchain bedrock.</p>
<p>I have also tried <a href=""https://python.langchain.com/v0.1/docs/use_cases/tool_use/prompting/"" rel=""nofollow noreferrer"">tools rendering</a> but not working below is my main get response function</p>
<pre><code>def get_response(llm, vectorstore, question, tools ):
    
    ## create prompt / template this helps to guide the AI on what to look out for and how to answer
    prompt_template = &quot;&quot;&quot;

    System: You are a helpful ai bot, your name is Alex, you are to provide information to humans based on faq and user information, in the user information provided you are to extract the users' firstName and lastName from the json payload and recognize that as the persons name. use the currencyVerificationData to determine the number of currency accounts that the user has and if they are approved if the status is VALID, other statuses will indicate that the user is not yet approved and needs to provide more information for validation. use bankFilledData as the users beneficiaries, from that section of the payload you would be able to extract the beneficiaries bankName, bankAccountNumber; use accountDetails as information for bank account detail information;     
    
    Human: Please use the given context to provide concise answer to the question
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    If you need clarity, ask more questions, do not refer to the json payload when answering questions just use the values you retrieve from the payload to answer
    &lt;context&gt;
    {context}
    &lt;/context&gt;

    The way you use the information is to identify users name and use it in response
    
    Question: {question}

    Assistant:&quot;&quot;&quot;

     # llm.bind_tools(tools) // not working, python error attribute not found

    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=[&quot;context&quot;, &quot;question&quot;, &quot;user_information&quot;]
    )

    qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=&quot;stuff&quot;,
    retriever=vectorstore.as_retriever(
        search_type=&quot;similarity&quot;, search_kwargs={&quot;k&quot;: 5}
    ),
    return_source_documents=True,
    chain_type_kwargs={&quot;prompt&quot;: PROMPT}
)
    answer=qa({&quot;query&quot;:question})
    return answer['result']
</code></pre>
<p>Finally what I wish to achieve is just a way to call functions based on input from said user</p>
","2024-06-17 21:39:05","0","Question"
"78629751","78607141","","<h1>Introduction</h1>
<p>Here, I have tried to achieve the goal but using a different approach. While implementing searches, the main goal is to match the input letters/words with the letter/words we have in our database.</p>
<h2>What was wrong with your code?</h2>
<p>You were trying to map the transformed features back to the keywords. The transformed features generated by the TF-IDF transformation do not directly map to the indices of the keywords in your list. The TF-IDF transformation vectorizes the text based on the entire vocabulary it learns from the data, and this vocabulary is <strong>NOT</strong> directly related to the list of keywords you have.</p>
<h2>Simple approach:</h2>
<ol>
<li><strong>Input Normalization</strong>: We can simplify our input by ignoring the Upper case words or removing the punctuations from the input.</li>
<li><strong>Direct Matching of letters</strong>: We can use LINQ <code>Where</code> clause to direct match each letters.</li>
<li><strong>Displayed only matched keywords</strong>: Only matched keywords are displayed using <code>MessageBox.Show</code>.</li>
</ol>
<h2>Why this method?</h2>
<p>It is because this method effectively matches the predefined words directly within the article area, ensuring correct matches and removing unnecessary complexities.</p>
<h1>Alternate solution</h1>
<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Text.RegularExpressions;
using System.Windows.Forms;

public class InputData
{
    public string Text { get; set; }
}

private void btnGo_Click(object sender, EventArgs e)
{
    // Title of the article
    var inputData = new List&lt;InputData&gt;
    {
        new() { Text = &quot;İşçilik Alacaklarında Kısmi Dava ve Belirsiz Alacak Davası Müesseselerinin İşletilmesi&quot; }
    };

    // Given keywords in List&lt;string&gt;
    var keywords = new List&lt;string&gt; { &quot;kısmi dava&quot;, &quot;üçüncü kişi&quot;, &quot;işçi&quot;, &quot;işçilik alacakları&quot;, &quot;vazife maluliyeti&quot;, &quot;belirsiz alacak davası&quot;, &quot;yabancı banka&quot;, &quot;adli yargı&quot;, &quot;anonim şirket&quot;, &quot;ayni hak&quot;, &quot;ayni hak talebi&quot;, &quot;bohçacı sözleşmesi&quot; };

    // Normalize the title text
    var normalizedTitle = NormalizeText(inputData[0].Text);

    // Find matching keywords
    var matchingKeywords = keywords.Where(keyword =&gt; normalizedTitle.Contains(NormalizeText(keyword))).ToList();

    // Display matching keywords after matching the keywords
    foreach (var keyword in matchingKeywords)
    {
        MessageBox.Show(keyword);
    }
}

private string NormalizeText(string text)
{
    // Convert to lowercase, remove punctuation and extra whitespace from the input
    return Regex.Replace(text.ToLower(), @&quot;\p{P}+&quot;, &quot;&quot;).Replace(&quot;\n&quot;, &quot; &quot;).Replace(&quot;\r&quot;, &quot;&quot;).Trim();
}

</code></pre>
<p>This approach will be efficient as it is optimized using LINQ and hope this helps to find the solution. Happy coding!</p>
","2024-06-16 17:19:43","0","Answer"
"78625475","","Understanding usage of HiFi-GAN by Vits","<p>I'm (trying to) learn AI/ML for speech synthesis and trying to undestand how HiFi-GAN is used by Vits.</p>
<p>From my understanding, <a href=""https://github.com/jaywalnut310/vits"" rel=""nofollow noreferrer"">Vits</a> will convert text input into mel spectograms which is then converted to audio waves by <a href=""https://github.com/jik876/hifi-gan"" rel=""nofollow noreferrer"">HiFi-GAN</a>.</p>
<p>What confuses me is why the input sent from Vits to HiFi-GAN is not a mel spectogram.</p>
<p>For example, when I test <em><strong>other</strong></em> models and add the code below to the forward method from HiFi-GAN:</p>
<pre><code>class Generator(torch.nn.Module):
  ...
  def forward(self, x):
    plot_spectrogram(x[0].cpu().detach().numpy(), &quot;mel_spec.png&quot;)
    ...
  ...
</code></pre>
<p>it saves the correct image which looks like a mel spectogram image, however, when I do the same with vits, the saved image is a plain green image which of course is not a representation of a mel spectogram.</p>
<p>But the resulting audio file is of course a valida audio file.</p>
<p>So could anyone explain that to me?</p>
<p>I'm evaluating a few neural tts models and what I wanted to do is save the mel spectogram created by the models to compare them later and also run them through different vocoders to compare them as well.</p>
<p>I noticed that the HiFi-GAN code in the vits repo is slightly different from the original repo but I can't undertand why.</p>
<p>Is there any way I can convert the input param <code>x</code> to the mel spectogram representation without first converting it to audio and then convert the audio to mel?</p>
","2024-06-15 02:17:36","0","Question"
"78618072","78615842","","<p>First you should have data of such sentences. and get it annotated manually. By annotation i mean it to mark it as positive sentiment or negative sentiment.</p>
<p>you can replace positive or negative with anything depending upon your usecase. Once the data is ready then do some data cleaning post that create embedding(try count vectorizer) of the data. then split your data into 80:20 ratio and use 80% of the data to train the model (Try logistic regression). After the training evaluate your model using remaining 20% of the model.</p>
<p>And if you do not want to annotate the data then you can try out pretrained model from hugging face such as roberta etc.</p>
","2024-06-13 13:11:50","0","Answer"
"78615842","","how to know the behavior of a sentence using NLP","<p>I am trying to build the NLP model, using that nlp model i want to know the intention of text sentence.</p>
<p>Hi,
I am trying to build the NLP model, using that nlp model i want to know the intention of text sentence.
For Ex- If the sentence is like &quot;Create an incident for not able to fill the timesheet&quot;.</p>
<p>so in above example how we know the intention of the sentence using nlp model</p>
","2024-06-13 04:38:15","0","Question"
"78612253","78610947","","<p>If you simply wish to utilize a pre-trained PyTorch model, please follow the instructions provided below.</p>
<pre><code>import torch

# Load the model
model = torch.load('model.bin')
input_data = torch.tensor([1, 2, 3, 4, 5])  # Example input data

with torch.no_grad():
    output = model(input_data)
print(output)
</code></pre>
","2024-06-12 11:04:27","0","Answer"
"78612199","78559183","","<p>ConversableAgent:</p>
<ul>
<li>Primary Purpose: Base class for building customizable agents that can
talk and interact with other agents, people, and tools to solve
tasks.</li>
<li>Customizable: Highly customizable, allowing for the
integration of LL.M.s, people and tools.</li>
</ul>
<p>AssistantAgent:</p>
<ul>
<li>Primary Purpose: Act as an AI assistant to generate responses using LLM, such as writing Python code.</li>
<li>Human Input: Do not request human input (human_input_mode = &quot;NEVER&quot;).</li>
<li>Code Execution: Do not execute code (code_execution_config = False).</li>
</ul>
<p>UserProxyAgent:</p>
<ul>
<li>Primary Purpose: Act as a human agent, soliciting human input at
every interaction and executing code when needed.</li>
<li>Human Input: Always
request human input (human_input_mode=&quot;ALWAYS&quot;).</li>
<li>LLM Usage: LLM-based
responses are disabled by default, but can be enabled.</li>
</ul>
<p>References:</p>
<ul>
<li><a href=""https://microsoft.github.io/autogen/docs/tutorial/human-in-the-loop"" rel=""noreferrer"">https://microsoft.github.io/autogen/docs/tutorial/human-in-the-loop</a></li>
<li><a href=""https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat"" rel=""noreferrer"">https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat</a></li>
</ul>
","2024-06-12 10:54:19","6","Answer"
"78610947","","Input to machine learning model","<p>I trained a bert based model that I had been working on for quite sometime now. After the training, I got a few files in the model directory - pytorch_model.bin, training_args.bin, merges.txt, vocab.json. Now I want to test the model by providing an input to the model and examine it's output. But i'm unable to understand how am i supposed to do so.</p>
<p>I tried looking on the internet and was suggested to use Gradio.</p>
","2024-06-12 06:27:31","-1","Question"
"78607809","","Xcode 16.0 beta AI feature confirmation","<p>Ive installed Xcode 16 beta, and am wondering if there is a screen or setting I can use to confirm if the new AI feature pre-requisites are met and the service is active??</p>
<p>From talks it sounds like I have to be on MacOS 15 beta as well.</p>
<p>Thanks,
Chris</p>
","2024-06-11 13:22:50","2","Question"
"78607141","","My app returns all keywords instead of existing in the string list in ml.net and c#","<p>In an application for a law article, there is a list of strings with keywords. In the example it is assigned to the variable <code>keywords</code>.
In my example there is an article with the title <code>İşçilik Alacaklarında Kısmi Dava ve Belirsiz Alacak Davası Müesseselerinin İşletilmesi</code>. Using ML.Net, I want to derive keywords similar to those in the list assigned to the <code>keywords</code> variable from the article title. I wrote the following code for this:</p>
<pre><code>public class InputData
{
    public string Text { get; set; }
}

public class TransformedData
{
    public float[] Features { get; set; }
}

private void btnGo_Click(object sender, EventArgs e)
{
    var mlContext = new MLContext();

    // List with the given texts
    var inputData = new List&lt;InputData&gt;
    {
        new() { Text = &quot;İşçilik Alacaklarında Kısmi Dava ve Belirsiz Alacak Davası Müesseselerinin İşletilmesi&quot; }
    };

    // Given keywords
    var keywords = new List&lt;string&gt; { &quot;kısmi dava&quot;, &quot;üçüncü kişi&quot;, &quot;işçi&quot;, &quot;işçilik alacakları&quot;, &quot;vazife maluliyeti&quot;, &quot;belirsiz alacak davası&quot;, &quot;yabancı banka&quot;, &quot;adli yargı&quot;, &quot;anonim şirket&quot;, &quot;ayni hak&quot;, &quot;ayni hak talebi&quot;, &quot;bohçacı sözleşmesi&quot; };

    var data = mlContext.Data.LoadFromEnumerable(inputData);

    // TF-IDF transformation
    var textPipeline = mlContext.Transforms.Text.FeaturizeText(&quot;Features&quot;, new TextFeaturizingEstimator.Options
    {
        WordFeatureExtractor = new WordBagEstimator.Options { NgramLength = 1, UseAllLengths = false },
        Norm = TextFeaturizingEstimator.NormFunction.Infinity,
        KeepPunctuations = false,
        StopWordsRemoverOptions = new StopWordsRemovingEstimator.Options()
    }, &quot;Text&quot;);

    var textTransformer = textPipeline.Fit(data);
    var transformedData = textTransformer.Transform(data);

    // Define the schema of the transformed data
    var transformedDataPreview = mlContext.Data.CreateEnumerable&lt;TransformedData&gt;(transformedData, reuseRowObject: false).ToList();

    // Identify keywords
    foreach (var transformedRow in transformedDataPreview)
    {
        var features = transformedRow.Features;
        for (var i = 0; i &lt; features.Length; i++)
        {
            if (features[i] &gt; 0.0 &amp;&amp; i &lt; keywords.Count)
            {
                var word = keywords[i];
                MessageBox.Show(word);
            }
        }
    }
}
</code></pre>
<p>When I run the code, it returns all the keywords in the list assigned to the <code>keywords</code> variable. However, the keywords that should be returned correctly are: <code>işçilik alacakları</code>, <code>kısmi dava</code> and <code>belirsiz alacak davası</code></p>
<p>I couldn't find where I went wrong.</p>
","2024-06-11 11:18:07","1","Question"
"78602120","78600925","","<p>I would suggest you identify how many tasks are in each command first, then identify the keyword for each task. According to each keyword perform relevant action:</p>
<p>To identify how many tasks are in each command, parsing the command to each single char. Using some methods, for example, if the command contains keyword like &quot;and&quot;, &quot;then&quot;, it will likely involve multiple tasks. Then classifying each single char of each task into a group - like:</p>
<pre><code>tokens = command.split()

# create a list to store identified tasks
tasks = []

# (method)

# example
# command = &quot;Do task A and then task B&quot;
# Output: [['Do', 'task', 'A'], ['task', 'B']]

 
</code></pre>
<p>Then, label each group(task). You may use machine learning according to your data feature in the step. Finally, perform the relevant actions according to labels.</p>
","2024-06-10 11:51:37","0","Answer"
"78600925","","How to detect multiple tasks in a single command using LLM or any NLP model?","<p>I am building a bot that takes inputs from users and performs action. Now I want it to differentiate between commands that contain single task and multiple tasks.</p>
<p>How can I make this a reality? Should I utilize prompting but there are infinite types of commands and infinite ways they can be represented, I doubt if an LLM will be able to comprehend it across all commands.</p>
<p>Or should I train a model or fine-tune an LLM to comprehend it but yet again the dataset would be too big and there will be infinite ways a single command can be represented.</p>
<p>What do you guys suggest I do?</p>
<p>I tried prompting GPT-3.5 with the following:</p>
<pre><code>Carefully, analyze the request for multiple tasks and generate JSON output for all requests in below format:
            When there are multiple tasks:
            {{ 'task': the task, 'type': 'multiple_task' }}
            When there is a single task:
            {{'task': the task, 'type': 'single_task'}}

            ### Example Analysis:

            For the request: &quot;how many executions happen with success and fail so far&quot;

            This is considered a multiple task because it is asking for two distinct counts:
            1. The count of successful executions.
            2. The count of failed executions.

            Therefore, the JSON output for this request should indicate multiple tasks.

            Example JSON output for this request:
            {
                &quot;requests&quot;: {{
            'task': 'how many execution happen with success and fail so far',
            'type': 'multiple_task'
            }}
            }
            
            Now do the same for the following user request.
            User request: 
</code></pre>
<p>It gives some correct responses but when I send :
<code>When a New google calendar event is created, post a message to general channel in slack plus sync it to Salesforce leads </code>
It gives me:</p>
<pre><code>{
    &quot;requests&quot;: [
        {
            &quot;task&quot;: &quot;When a New google calendar event is created, post a message to general channel in slack&quot;,
            &quot;type&quot;: &quot;single_task&quot;
        },
        {
            &quot;task&quot;: &quot;sync it to Salesforce leads&quot;,
            &quot;type&quot;: &quot;single_task&quot;
        }
    ]
}

</code></pre>
","2024-06-10 07:11:28","1","Question"
"78600675","78597265","","<p>To enhance the relevance of the documents retrieved by the RAG-Token model and the quality of answers produced, consider fine-tuning the retriever on your custom knowledge base, adjusting retrieval parameters, and ensuring your knowledge base contains high-quality, relevant information. Additionally, refine your query wording and validate generated answers against retrieved documents for accuracy.</p>
","2024-06-10 06:01:05","0","Answer"
"78599806","78437376","","<p>If your service working different port</p>
<p>for example your <code>/etc/systemd/system/ollama.service</code> file is:</p>
<pre><code>[Service]
ExecStart=/usr/local/bin/ollama serve
Environment=&quot;OLLAMA_HOST=0.0.0.0:8000&quot;
Environment=&quot;OLLAMA_KEEP_ALIVE=-1&quot;
</code></pre>
<p><strong>you have to use command like this:</strong></p>
<pre class=""lang-bash prettyprint-override""><code>OLLAMA_HOST=0.0.0.0:8000 ollama list
</code></pre>
<p><strong>or</strong></p>
<pre class=""lang-bash prettyprint-override""><code>export OLLAMA_HOST=0.0.0.0:8000
ollama list
ollama run llama3
</code></pre>
","2024-06-09 21:42:39","1","Answer"
"78598983","78287134","","<p>It is not usual to pass large csv to LLMs. Alternatively you can generate proper code from LLM and then execute with exec() function. In this way it will execute the code on your PC and it will solve the token limit.</p>
","2024-06-09 15:35:17","0","Answer"
"78597265","","Retrieving relevant documents for specific queries","<p>I am trying to retrieve the top 5 relevant documents related to a user's query using the RAG-Token model. I'm using a custom knowledge base and I tried adjusting the retrieval parameters.</p>
<p>This is the code:</p>
<pre><code># Include a minimal, complete, and verifiable example of the code you are using
from transformers import RagTokenizer, RagTokenForGeneration, RagRetriever

tokenizer = RagTokenizer.from_pretrained(&quot;facebook/rag-token-nq&quot;)
model = RagTokenForGeneration.from_pretrained(&quot;facebook/rag-token-nq&quot;)
retriever = RagRetriever.from_pretrained(&quot;facebook/rag-token-nq&quot;, index_name=&quot;custom&quot;, passages_path=&quot;my_knowledge_dataset&quot;)

input_query = &quot;What is the impact of AI in healthcare?&quot;
input_ids = tokenizer(input_query, return_tensors=&quot;pt&quot;).input_ids

# Generate a response
output_ids = model.generate(input_ids)
print(&quot;Generated Response:&quot;, tokenizer.decode(output_ids[0], skip_special_tokens=True))
</code></pre>
<p>I expect the model to retrieve accurate and relevant documents that provide deep insights into the query. However, the retrieved documents are often irrelevant, and the generated answers often do not properly incorporate the retrieved information.</p>
<p>What might be going wrong here? How can I modify the code or change parameter values to get more relevant results?</p>
","2024-06-09 00:15:40","0","Question"
"78596193","78596157","","<p>You can use <code>StandardScaler()</code> and increase the number of epochs:</p>
<pre><code>import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler

test_length = 0.15
features = np.zeros((1000, 40, 40))
labels = np.random.rand(1000)

features[:, 0, 0] = labels.copy()

features_train = features[0:int(len(features) * (1 - test_length))]
labels_train = labels[0:int(len(labels) * (1 - test_length))]
features_test = features[int(len(features) * (1 - test_length)):]
labels_test = labels[int(len(labels) * (1 - test_length)):]

SS = StandardScaler()

TR = features_train.reshape(-1, features_train.shape[-1])
TS = features_test.reshape(-1, features_test.shape[-1])

TRN = SS.fit_transform(TR).reshape(features_train.shape)
TSN = SS.transform(TS).reshape(features_test.shape)

model = Sequential([
    LSTM(100, return_sequences=True, input_shape=(features_train.shape[1], features_train.shape[2])),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(50, activation='relu'),
    Dropout(0.2),
    Dense(30, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mean_squared_error')

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
history = model.fit(
    TRN, labels_train,
    epochs=100,
    batch_size=64,
    validation_data=(TSN, labels_test),
    callbacks=[early_stopping],
    verbose=1
)

</code></pre>
<h3>Note:</h3>
<p>You can also pass <code>EarlyStopping()</code> callbacks.</p>
","2024-06-08 15:44:44","0","Answer"
"78596192","78596157","","<p>Edit:</p>
<p>It was the Dropout (facepalm).</p>
<p>With dropout, the model can't rely on the one feature I am giving it that is useful.</p>
<p>Wow.</p>
","2024-06-08 15:44:17","0","Answer"
"78596157","","LSTM not learning with the label/answer literally in the features","<p>In my code, I put the labels as the first feature in the first timestep, and the LSTM is unable to learn that the answer is in the first timestemp, almost like it is blind to it.</p>
<p>I ran this test because in my real data, my LSTM thinks some variables in the last timestep have no importance in the prediction, yet a gradient tree on the same data finds this pattern.</p>
<p>What's up with my LSTMs? They seems so poor.</p>
<pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional

test_length = 0.15

test_length = 0.15

#features = np.random.rand(1000000, 20, 20)
features = np.zeros((100000, 40, 40))
labels = np.random.rand(100000)

features[:, 0, 0] = labels.copy()

features_train = features[0:int(len(features)*(1-test_length))]
labels_train = labels[0:int(len(labels)*(1-test_length))]
features_test = features[int(len(features)*(1-test_length)):]
labels_test = labels[int(len(labels)*(1-test_length)):]

model = Sequential([
    LSTM(100, return_sequences=False),
    Dropout(0.2),
    Dense(50, activation='relu'),
    Dropout(0.2),
    Dense(30, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(
    features_train, labels_train,
    epochs=1,
    batch_size=40,
    validation_data=(features_test, labels_test),
    verbose=1
)
</code></pre>
","2024-06-08 15:34:21","1","Question"
"78594990","78585615","","<p>The embeddings endpoint was added in <a href=""https://github.com/vllm-project/vllm/releases/tag/v0.4.3"" rel=""nofollow noreferrer"">version 0.4.3</a> released on June 1, 2024. Make sure you have this recent version installed.</p>
","2024-06-08 07:26:32","0","Answer"
"78589801","78576908","","<p>By passing <code>Body=jsonserializer.serialize(json.dumps({&quot;name&quot;: &quot;john mclain&quot;}))</code></p>
<p>I think you're double-serializing to</p>
<pre class=""lang-json prettyprint-override""><code>&quot;{\&quot;name\&quot;: \&quot;john mclain\&quot;}&quot;
</code></pre>
<p>...rather than if you just pass in <code>Body=json.dumps(...)</code>, for</p>
<pre class=""lang-json prettyprint-override""><code>{&quot;name&quot;: &quot;john mclain&quot;}
</code></pre>
<p>(For what it's worth, I'd probably rename your <code>jsonstr</code> variable since it's the request payload in dictionary form, not a string - so the name is a bit misleading).</p>
","2024-06-07 03:13:28","1","Answer"
"78585615","","Cannot access embeddings endpoint on vLLM hosting llama3-8b-instruct","<p>I'm using vllm to run llama3-8b-instruct on a machine, I can access the chat endpoint, but when I access the embedding endpoint using following code I get <strong>NotFoundError: Error code: 404 - {'detail': 'Not Found'}</strong></p>
<pre class=""lang-py prettyprint-override""><code>from openai import OpenAI

# Modify OpenAI's API key and API base to use vLLM's API server.
openai_api_key = &quot;EMPTY&quot;
openai_api_base = &quot;http://10.244.191.18:8000/v1&quot;

client = OpenAI(
    # defaults to os.environ.get(&quot;OPENAI_API_KEY&quot;)
    api_key=openai_api_key,
    base_url=openai_api_base,
)

models = client.models.list()
model = models.data[0].id

responses = client.embeddings.create(input=[
    &quot;Hello my name is&quot;,
    &quot;The best thing about vLLM is that it supports many different models&quot;
], model=model)

for data in responses.data:
    print(data.embedding)  # list of float of len 4096
</code></pre>
<p>vLLM error log:</p>
<p><code>INFO:     172.30.230.216:52870 - &quot;POST /v1/embeddings HTTP/1.1&quot; 404 Not Found</code></p>
<p>Code snippet is from vllm document: <a href=""https://docs.vllm.ai/en/latest/getting_started/examples/openai_embedding_client.html"" rel=""nofollow noreferrer"">https://docs.vllm.ai/en/latest/getting_started/examples/openai_embedding_client.html</a></p>
","2024-06-06 09:14:05","1","Question"
"78584644","78564560","","<p><code>I do know theres a script editor but i have no idea how to make it so the year month and date gets filled &quot;${dateEntryFieldID}=${YEAR}-${MONTH}-${DAY}&quot;</code></p>
<p>Google Forms does NOT have a feature to set a default date or time as the answer to a Google Form question.</p>
<p>As mentioned in the question, it IS possible to create a URL for the Google Form but, for a simple implementation, the Date and Time would not be dynamic; they would be the Date and Time on which the pre-fill was created.</p>
<p>For example:<br />
<strong>prefill created on 20 March 2024:</strong> <a href=""https://docs.google.com/forms/d/e/1FAIpQLSecqXJ2zEKK8siJ42wDqoR7_kYRtzB0K8gziFSOg4SaCUE9Kg/viewform?usp=pp_url&amp;entry.85836405=2004-03-20&amp;entry.431994197=13:44"" rel=""nofollow noreferrer"">https://docs.google.com/forms/d/e/1FAIpQLSecqXJ2zEKK8siJ42wDqoR7_kYRtzB0K8gziFSOg4SaCUE9Kg/viewform?usp=pp_url&amp;entry.85836405=2004-03-20&amp;entry.431994197=13:44</a><br />
<strong>prefill created on 5 June 2024:</strong> <a href=""https://docs.google.com/forms/d/e/1FAIpQLSecqXJ2zEKK8siJ42wDqoR7_kYRtzB0K8gziFSOg4SaCUE9Kg/viewform?usp=pp_url&amp;entry.85836405=2024-06-05&amp;entry.431994197=15:11"" rel=""nofollow noreferrer"">https://docs.google.com/forms/d/e/1FAIpQLSecqXJ2zEKK8siJ42wDqoR7_kYRtzB0K8gziFSOg4SaCUE9Kg/viewform?usp=pp_url&amp;entry.85836405=2024-06-05&amp;entry.431994197=15:11</a></p>
<p>A script can be used to create a dynamic Pre-filled URL (the second example was created by script) but it is unclear how this is to be communicated to a respondent.
In addition, though the pre-fill can include the Date and Time, there is nothing to stop a respondent from editing these fields.</p>
","2024-06-06 05:33:23","2","Answer"
"78584188","78542429","","<p>The sound inputs were too long, after resampling the audio into chunks, the problem was resolved.</p>
","2024-06-06 02:08:48","0","Answer"
"78584055","78464664","","<p>@misantroop I agree. These libraries do things very differently and a translator would potentially be a large project with many ways it could fail.  I've now taken about ten different plots that I originally created in matplotlib and recreated them with pyqtgraph. There's clearly many statements that have equivalents in both. Both libraries have titles, labels, axes, ranges, lines, points, text, fills, modifiers, etc.  A simple way to approach this is to do the mappings which can be established, leave the untranslated statements in as comments to be attended to and manage the expectation that the product is unfinished and likely to behave somewhat differently than the original.  I would expect that they handle user inconsistency differently.  What should they do if the user set the aspect ratio as equal and then supplied ranges that could not yield the desired aspect ratio. The answer would not be in a pure translation of the syntax.  Some decisions would be left to the user.  Like Ai bots, it would be more of a copilot rather than a pilot.</p>
","2024-06-06 00:54:47","0","Answer"
"78576908","","Why is Sagemaker Endpoint invocation giving an ""Failed to deserialize Error""","<p>I am trying to invoke model(Falcon 40B) via sagemaker endpoint, I get the below error when I make the call.</p>
<p>Error:-
<strong>Failed to deserialize the JSON body into the target type: invalid type: string &quot;{&quot;name&quot;: &quot;john mclain&quot;}&quot;, expected struct CompatGenerateRequest at line 1 column 29&quot;</strong></p>
<pre><code>sagemaker_runtime = boto3.client(&quot;sagemaker-runtime&quot;, region_name='ap-south-1')
endpoint_name = &quot;jumpstart-dft-hf-llm-falcon-40b&quot;
inference=&quot;huggingface-llm-falcon-40b-instruct&quot;
jsonstr={&quot;name&quot;: &quot;john mclain&quot;}
#Gets inference from the model hosted at the specified endpoint:
response = sagemaker_runtime.invoke_endpoint(
EndpointName=endpoint_name, 
ContentType=&quot;application/json&quot;,
Body=(jsonserializer.serialize(json.dumps(jsonstr))),
InferenceComponentName='huggingface-llm-falcon-40b'
) 
</code></pre>
","2024-06-04 17:29:35","0","Question"
"78574843","78573089","","<p>i did it just added a shut down command i hope this helped someone out there</p>
<pre><code>@bot.event
async def on_message(message):
    if message.author.bot:
        return  # Ignore messages from other bots

    # Check for custom responses first
    if message.content.lower() in custom_responses:
        await message.channel.send(custom_responses[message.content.lower()])
    elif message.content.lower() == &quot;quitquit&quot;:
        await message.channel.send(&quot;Goodnight! Shutting down...&quot;)
        await bot.close()  # Shut down the bot
    else:
        # If no custom response, check predefined intents
        for intent in intents:
            if any(pattern.lower() in message.content.lower() for pattern in intent['patterns']):
                response = random.choice(intent['responses'])
                await message.channel.send(response)
                break
        else:
            await message.channel.send(&quot;I'm not sure how to respond. Could you provide a custom response?&quot;)

    await bot.process_commands(message)
</code></pre>
","2024-06-04 10:36:30","0","Answer"
"78573089","","how to get my chat bot to remember custom responses in discord then put it in intents.json python","<p>hello am trying to get my discord bot to remeber what its learned after i shut it down but each time i boot it up again to test it forget everything its learned
heres the code</p>
<pre><code>import discord
from discord.ext import commands
import json
import asyncio
import random

# Load intents from intents.json
with open('intents.json', 'r') as intents_file:
    intents_data = json.load(intents_file)
    intents = intents_data.get('intents', [])

# Load custom responses from custom_responses.json
try:
    with open('custom_responses.json', 'r') as custom_responses_file:
        custom_responses = json.load(custom_responses_file)
except FileNotFoundError:
    custom_responses = {}  # Initialize an empty dictionary if the file doesn't exist

# Initialize the bot
bot = commands.Bot(command_prefix='!', intents=discord.Intents.all())


@bot.event
async def on_ready():
    print(f'Logged in as {bot.user.name} ({bot.user.id})')


@bot.event
async def on_message(message):
    if message.author.bot:
        return  # Ignore messages from other bots

    # Check for custom responses first
    if message.content.lower() in custom_responses:
        await message.channel.send(custom_responses[message.content.lower()])
    else:
        # If no custom response, check predefined intents
        for intent in intents:
            if any(pattern.lower() in message.content.lower() for pattern in intent['patterns']):
                response = random.choice(intent['responses'])
            `your text`    await message.channel.send(response)
                break
        else:
            await message.channel.send(&quot;I'm not sure how to respond. Could you provide a custom response?&quot;)

    await bot.process_commands(message)


@bot.command()
async def learn(ctx, *, response: str):
    &quot;&quot;&quot;
    Command to add a custom response.
    Usage: !learn &lt;user_input&gt; &lt;custom_response&gt;
    Example: !learn favorite_color Blue
    &quot;&quot;&quot;
    user_input, custom_response = response.split(maxsplit=1)
    custom_responses[user_input.lower()] = custom_response
    await ctx.send(f&quot;Learned: '{user_input}' -&gt; '{custom_response}'&quot;)


@bot.event
async def on_disconnect():
    # Save custom responses to custom_responses.json when the bot disconnects
    with open('custom_responses.json', 'w') as custom_responses_file:
        json.dump(custom_responses, custom_responses_file, indent=4)


print(&quot;Bot is starting...&quot;)
loop = asyncio.get_event_loop()
loop.run_until_complete(bot.start('TOKEN'))
</code></pre>
<p>then you got the custom_responses.json</p>
<p>{
&quot;user_input_1&quot;: &quot;custom_response_1&quot;,
&quot;user_input_2&quot;: &quot;custom_response_2&quot;
}</p>
<p>then the intents.json</p>
<p>{
&quot;intents&quot;: [</p>
<pre><code>    {
        &quot;tag&quot;: &quot;google&quot;,
        &quot;patterns&quot;: [
            &quot;google&quot;,
            &quot;search&quot;,
            &quot;internet&quot;
        ],
        &quot;responses&quot;: [
            &quot;https://www.youtube.com/watch?v=dQw4w9WgXcQ&amp;pp=ygUXbmV2ZXIgZ29ubmEgZ2l2ZSB5b3UgdXA%3D&quot;
        ]
    },
    {
        &quot;tag&quot;: &quot;greeting&quot;,
        &quot;patterns&quot;: [
            &quot;Hi there&quot;,
            &quot;How are you&quot;,
            &quot;Is anyone there?&quot;,
            &quot;Hey&quot;,
            &quot;Hola&quot;,
            &quot;Hello&quot;,
            &quot;Good day&quot;,
            &quot;Namaste&quot;,
            &quot;yo&quot;
        ],
        &quot;responses&quot;: [
            &quot;Hello&quot;,
            &quot;Good to see you again&quot;,
            &quot;Hi there, how can I help?&quot;
        ],
        &quot;context&quot;: [
            &quot;&quot;
        ]
    }
</code></pre>
<p>etc</p>
<p>any help would be helpful to me thanks you for you time as well</p>
<p>i want it to remember what you tell it like if you say !learn cat cat is good next time you boot it you say cat i want it to say cat is good</p>
","2024-06-04 02:00:55","0","Question"
"78570720","78557561","","<blockquote>
<p>BadRequest-This request is not authorized to perform this operation.This request is not authorized to perform this operation.This request is not authorized to perform this operation.This request is not authorized to perform this operation.This request is not authorized to perform this operation. Properties Stack trace Error: Request failed with status code 400 at cG</p>
</blockquote>
<p><em><strong>Use the Azure SDK or CLI to deploy the Flow from the template. Then you can be able to capture and handle any errors during the deployment process.</strong></em></p>
<p><strong>Code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>from azure.identity import DefaultAzureCredential
from azure.ai.ml import MLClient
from azure.ai.ml.entities import PipelineJob
from azure.ai.ml.dsl import pipeline

# Authenticate and create a client
def authenticate():
    credential = DefaultAzureCredential()
    subscription_id = &quot;&lt;YOUR_SUBSCRIPTION_ID&gt;&quot;
    resource_group = &quot;&lt;YOUR_RESOURCE_GROUP&gt;&quot;
    workspace_name = &quot;&lt;YOUR_WORKSPACE_NAME&gt;&quot;
    ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)
    return ml_client

# Verify template configuration
def verify_template_configuration(template):
    # Placeholder: Add actual template verification logic
    return True

# Validate resource access
def validate_resource_access(resources):
    # Placeholder: Add actual resource access validation logic
    return True

# Deploy Flow using Azure SDK
def deploy_flow(ml_client, template, parameters):
    try:
        job = PipelineJob(
            display_name=&quot;example-pipeline&quot;,
            description=&quot;A sample pipeline job&quot;,
            jobs=template  # This should be a pipeline definition or component
        )
        ml_client.jobs.create_or_update(job)
        return True
    except Exception as e:
        print(f&quot;Deployment failed: {e}&quot;)
        return False

# Check deployment logs
def check_deployment_logs(ml_client):
    # Placeholder: Retrieve and analyze deployment logs
    logs = ml_client.jobs.list()
    return logs

# Main function
def main():
    ml_client = authenticate()
    
    template = {
        # Example pipeline structure
        &quot;job1&quot;: {
            &quot;type&quot;: &quot;command&quot;,
            &quot;command&quot;: &quot;echo Hello World&quot;,
            &quot;environment&quot;: &quot;AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1&quot;
        }
    }
    parameters = {
        &quot;param1&quot;: &quot;value1&quot;,
        &quot;param2&quot;: &quot;value2&quot;
    }
    resources = [&quot;resource1&quot;, &quot;resource2&quot;]
    
    if verify_template_configuration(template):
        print(&quot;Template configuration is correct.&quot;)
    else:
        print(&quot;Template configuration issue detected.&quot;)
    
    if validate_resource_access(resources):
        print(&quot;Resource access is validated.&quot;)
    else:
        print(&quot;Resource access issue detected.&quot;)
    
    if deploy_flow(ml_client, template, parameters):
        print(&quot;Flow deployed successfully.&quot;)
    else:
        print(&quot;Flow deployment issue detected.&quot;)
    
    logs = check_deployment_logs(ml_client)
    print(f&quot;Deployment logs: {logs}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<ul>
<li>Use <code>PipelineJob</code> from the Azure SDK to define and deploy the Flow, making the process more specific and capturing detailed errors.</li>
</ul>
<p>The <code>authenticate</code> function uses <code>DefaultAzureCredential</code> and initializes <code>MLClient</code> with the subscription ID, resource group, and workspace name</p>
<p><strong>Reference:</strong></p>
<ul>
<li><p><a href=""https://microsoft.github.io/promptflow/tutorials/pipeline.html"" rel=""nofollow noreferrer"">Run flows in Azure ML pipeline)</a></p>
</li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/ai-studio/how-to/prompt-flow"" rel=""nofollow noreferrer"">Prompt flow in Azure AI Studio</a></p>
</li>
</ul>
","2024-06-03 13:53:53","0","Answer"
"78570690","77759685","","<p>Assuming that <code>setup_and_retrieval</code> outputs something like <code>{&quot;supporting_documents&quot; : ..., &quot;question&quot;: ...}</code> you can simply define your chain as</p>
<pre><code>chain = setup_and_retrieval.assign(answer = prompt | model | output_parser)
</code></pre>
<hr />
<p>Running the chain and getting the results :</p>
<pre><code>result = rag_chain.invoke(question)    
answer = result[&quot;answer&quot;]
supporting_documents = result[&quot;supporting_documents&quot;]
</code></pre>
","2024-06-03 13:50:19","2","Answer"
"78570339","78570282","","<p>You may have to gather enough image samples of what you expect to it to be. You can use existing models or techniques to preprocess your data. Once you get more image data, normalise the data</p>
","2024-06-03 12:44:46","0","Answer"
"78570282","","How to Fine-Tune Pre-Trained Stable Diffusion Models Using Custom Images","<p>I am utilizing Stable Diffusion XL Base 1.0 for image generation, but it does not accept my custom input image. I would like to generate a new image based on my input image and the specified prompt.</p>
<p>I need a solution to generate anthropomorphic pet portraits from user-uploaded pet photos. Specifically, I want the AI to use the pet's face from the uploaded image and create the rest of the body based on a given prompt, such as a king, doctor, or lawyer. The generated portraits should retain the pet's unique features, making it clear they are the same pets.</p>
<p>The problem I'm encountering is that when I input a custom pet image into the pre-trained Stable Diffusion model with an anthropomorphic prompt, the model generates an image based on its dataset instead of using my custom image. I want the AI to generate new images using the provided pet photos, incorporating the given prompt, rather than creating random images from its own dataset.</p>
<p>How can I fine-tune a pre-trained Stable Diffusion model, or any relevant model, with our custom images so that it uses these images to generate new portraits according to the given input and prompt?<a href=""https://i.sstatic.net/AJbG2zy8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJbG2zy8.jpg"" alt=""enter image description here"" /></a></p>
","2024-06-03 12:31:44","0","Question"
"78564560","","Google forms prefill url with current date and time or script or ai","<p>Essentially what im trying to do is make it so the entry field of my form for the date and time gets autofilled with the current date</p>
<p>I do know theres a script editor but i have no idea how to make it so the year month and date gets filled &quot;${dateEntryFieldID}=${YEAR}-${MONTH}-${DAY}&quot;</p>
<p>Or if someone knows how to make it so the URL can have dynamic number for the current date/time and not a static one that would also work (No idea how to do that outside of hosting a site that modifies the url and auto forwards)</p>
<p>I also thought what if theres a add on (ai or not) that can also do the job</p>
<p>(The main reason i want this is because i have a form for attendance and those 2 fields are annoying but required by people above me because stats, but i want to make it easier and faster on the user)</p>
<p>I tried to modify the url and i know it workd but idk how or were to get the data from so it auto paste into the url dynamic. and i did try o search for some ai addon but atleast the few that i tried didnt have autofill</p>
<p>I also did try the guides from other post but there a bit outdated and dont help with my solution</p>
","2024-06-01 18:00:32","0","Question"
"78562342","78452864","","<p>The class should exist since v0.3.0 (<a href=""https://github.com/google-gemini/generative-ai-python/commit/098854379496247617ef7fea882694022fca171d"" rel=""nofollow noreferrer"">https://github.com/google-gemini/generative-ai-python/commit/098854379496247617ef7fea882694022fca171d</a>)</p>
<p>You must be using an older version of the SDK.</p>
<p>There are two likely causes:</p>
<ol>
<li>You've installed the package to a different environment than the one you're actually using. If you use Jupyter, please use the <code>%pip install</code> magic to install packages.</li>
<li>You have updated the package, but have not restarted the Jupyter session.</li>
</ol>
","2024-05-31 22:33:51","4","Answer"
"78559183","","What is the difference of AssistantAgent, ConversableAgent and UserProxyAgent of autogen?","<p>I use the multiple agents from autogen  <code>https://microsoft.github.io/autogen/</code> .<br />
There are at least three agents: AssistantAgent, ConversableAgent and UserProxyAgent.
What is the difference of AssistantAgent, ConversableAgent and UserProxyAgent of autogen?</p>
","2024-05-31 09:21:00","5","Question"
"78557561","","failure error attempting to create a a new flow in Azure PromptFlow within Azure AI Studio","<p>Go to Azure AI Studio,<br />
open prompt flow,<br />
CREATE new flow...<br />
Choose from gallery/templates...<br />
Fails (multiple users)</p>
<p>Used to work a week ago, and still works in another AI Studio instance I have.   User Rights/Roles should be similar or exact in both cases.
My 3rd Azure Prompt Flow within another ML Workspace also works perfectly.   only under this particular AI Studio instance (and it's ML Workspace within the AI Hub)</p>
<p>Has anyone seen this error?   Looks like it can't access some ai.azure.com asset.</p>
<p>BadRequest</p>
<p>This request is not authorized to perform this operation.This request is not authorized to perform this operation.This request is not authorized to perform this operation.This request is not authorized to perform this operation.This request is not authorized to perform this operation.
Properties
Stack trace
Error: Request failed with status code 400
at cG (<a href=""https://ai.azure.com/assets/index-3968d242.js:307:15808"" rel=""nofollow noreferrer"">https://ai.azure.com/assets/index-3968d242.js:307:15808</a>)
at uG (<a href=""https://ai.azure.com/assets/index-3968d242.js:307:15996"" rel=""nofollow noreferrer"">https://ai.azure.com/assets/index-3968d242.js:307:15996</a>)
at XMLHttpRequest.y (<a href=""https://ai.azure.com/assets/index-3968d242.js:308:1672"" rel=""nofollow noreferrer"">https://ai.azure.com/assets/index-3968d242.js:308:1672</a>)</p>
<p>Made sure resources have same permissions as in working instance.
Expect it to deploy a Flow from the selected template</p>
","2024-05-30 23:22:55","0","Question"
"78549145","78547320","","<p>As it comes from the comments, you are using an old version of <strong>Ultralytics==8.0.0</strong>. It in fact returns the result as a list of <code>torch.Tensor</code> object instead of <code>ultralytics.engine.results.Results</code> object, and exactly the last one has such parameters like <em>boxes, masks, keypoints, probs, obb</em>. The documentation complies with the latest framework version, <strong>8.2.24</strong> for now, and 8.0.0 is from January 2023.</p>
<p>The easiest way to solve your problem is to <strong>upgrade the Ultralytics version</strong> to the latest one, so you will get all the results parameters described in the documentation.</p>
<p>If circumstances prevent you from this update, you will need some data postprocessing with the <strong>understanding of the returned results format</strong> from the 8.0.0 version.</p>
<p>OBJECT DETECTION task results, version==8.0.0</p>
<pre class=""lang-py prettyprint-override""><code># for 3 detected objects

[tensor([[2.89000e+02, 7.10000e+01, 1.44000e+03, 5.07000e+02, 8.91113e-01, 2.00000e+00],
         [1.26700e+03, 6.00000e+01, 1.68200e+03, 3.19000e+02, 8.31055e-01, 2.00000e+00],
         [6.96000e+02, 0.00000e+00, 1.32200e+03, 1.31000e+02, 2.56836e-01, 7.00000e+00]], device='cuda:0')]

# where every array stores 6 values, first 4 are in pixels:
[x_centre, y_centre, box_width, box_height, confidence, class_id]

# for easy manipulation you can run results[0].tolist() and will get the following format:
[[289.0, 71.0, 1440.0, 507.0, 0.89111328125, 2.0],
 [1267.0, 60.0, 1682.0, 319.0, 0.8310546875, 2.0],
 [696.0, 0.0, 1322.0, 131.0, 0.2568359375, 7.0]]
</code></pre>
<p>OBJECT SEGMENTATION task results, version==8.0.0. Will be the same as for detection, but adds the second torch.Tensor with the segmentation masks for every object.</p>
<pre class=""lang-py prettyprint-override""><code># for 3 detected objects

[[tensor([[1.23000e+02, 8.90000e+01, 4.21000e+02, 2.21000e+02, 2.55216e-01, 7.00000e+00],
          [1.26700e+03, 5.80000e+01, 1.68100e+03, 3.17000e+02, 8.04158e-01, 2.00000e+00],
          [2.70000e+02, 7.70000e+01, 1.46000e+03, 4.98000e+02, 8.19106e-01, 2.00000e+00]], device='cuda:0'),
  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]],
  
          [[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]],
  
          [[0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           ...,
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.],
           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]]

</code></pre>
<p>Knowing the data format you receive from this old Ultralytics version, you can easily reach them and translate them to the format you need. But upgrading Ultralytics to the latest version is still the best way to get the most effective usage of this framework.</p>
<p>Install Ultralytics: <a href=""https://docs.ultralytics.com/quickstart/"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/quickstart/</a></p>
","2024-05-29 11:15:12","1","Answer"
"78547320","","YoloV8 results have no 'box', 'max' properties in it","<p>I've trained a YOLOV8 model to identify objects in an intersection (ie cars, roads etc).
It is working OK and I can get the output as an image with the objects of interested segmented.</p>
<p>However, what I need to do is to capture the raw geometries (polygons) so I can save them on a txt file later on.</p>
<p>I tried what Ive found in the documentation (<a href=""https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode"" rel=""nofollow noreferrer"">https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode</a>) however the returning object is not the same as the documentation says.</p>
<p>In fact, the result is a list of tensorflow numbers:</p>
<p><a href=""https://i.sstatic.net/Fy4lI1sV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fy4lI1sV.png"" alt=""enter image description here"" /></a></p>
<p>Here's my code:</p>
<pre><code>import argparse
import cv2
import numpy as np
from pathlib import Path
from ultralytics.yolo.engine.model import YOLO    
    
# Parse command line arguments
parser = argparse.ArgumentParser()
parser.add_argument('--source', type=str, required=True, help='Source image directory or file')
parser.add_argument('--output', type=str, default='output', help='Output directory')
args = parser.parse_args()

# Create output directory if it doesn't exist
Path(args.output).mkdir(parents=True, exist_ok=True)

# Model path
model_path = r'C:\\_Projects\\best_100img.pt'

# Load your model directly
model = YOLO(model_path)
model.fuse()

# Load image(s)
if Path(args.source).is_dir():
    image_paths = list(Path(args.source).rglob('*.tiff'))
else:
    image_paths = [args.source]

# Process each image
for image_path in image_paths:
    img = cv2.imread(str(image_path))
    if img is None:
        continue

    # Perform inference
    predictions = model.predict(image_path, save=True, save_txt=True)
    
print(&quot;Processing complete.&quot;)
</code></pre>
<p>Here's the problem: the return object (predictions variable) has no <strong>boxes, masks, keypoints</strong> and etc.</p>
<p>I guess my questions are:</p>
<ul>
<li>Why the result is so different from the documentation?</li>
<li>Is there a conversion step?</li>
</ul>
","2024-05-29 04:32:01","0","Question"
"78542920","78541338","","<p>You can only have one &quot;messages&quot; key. And it is usual to have just one entry for the &quot;system&quot; role. So this would give you:</p>
<pre><code>&quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;Our car cleaning service offers ....
       We ensure the safety ...
       etc&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;What does your car cleaning service include?&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;Our car cleaning service includes ...&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;Are you insured ...?&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;Yes, we are ....&quot;
    },

etc
]
</code></pre>
","2024-05-28 08:43:24","0","Answer"
"78542429","","Running out of RAM when finetuning model","<p>I am currently trying to finetune <code>Wav2Vec2</code> model from: <a href=""https://huggingface.co/dima806/bird_sounds_classification"" rel=""nofollow noreferrer"">https://huggingface.co/dima806/bird_sounds_classification</a>. But my RAM utilisation is running over the free tier on Google Colab.</p>
<p>The following is my code:</p>
<pre><code>from transformers import TrainingArguments, Trainer

# Load model with ignore_mismatched_sizes=True
model = Wav2Vec2ForSequenceClassification.from_pretrained(
    &quot;dima806/bird_sounds_classification&quot;,
    num_labels=len(label2id),
    ignore_mismatched_sizes=True
)

# Set up training with gradient accumulation
batch_size = 1  # Reduce batch size to manage memory
accumulation_steps = 4  # Accumulate gradients over 4 steps

training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=accumulation_steps,  # Gradient accumulation
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=feature_extractor,
)

# Train the model
trainer.train()
</code></pre>
<p>What could be the reasons the RAM is going past 12.7GB? My dataset only contains 20 items. How can I address this issue?</p>
","2024-05-28 07:10:21","1","Question"
"78541338","","Training data for chatGPT wont work correctly","<p>I am trying to train a model in chatGPT (chatGPT-3.5-turbo-1106) for my cleaning business. based on the documentation I created and uploaded the training data successfully, but the answer I am getting is totally different that what I had in my training data. Here are the sample:</p>
<pre><code>{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Our car cleaning service offers different packages based on your needs.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What does your car cleaning service include?&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Our car cleaning service includes three options: 1) Exterior Wash. 2) Exterior Wash &amp; Wax. 3) Interior and Exterior Detail with Wash and Wax. You can choose the package that best suits your preferences and requirements.&quot;}]}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;We ensures the safety and reliability of our cleaning services.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Are you insured and bonded for your cleaning service?&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Yes, we are insured and bonded.&quot;}]}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Our staff are capable of handling basic handyman tasks.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Are you licensed to work on electrical or appliances at home?&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;No, we can only work on simple electrical issues.&quot;}]}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;We makes tipping easy and convenient for customers.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do you handle tips for the cleaning staff?&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;You can tip in person to the staff or on our website.&quot;}]}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Our cancellation policy ensures flexibility and convenience for our clients.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is your cancellation policy?&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;You can cancel 24 hours before the scheduled service.&quot;}]}
</code></pre>
<p>I tried different version of chatGPT, Also I revised the answers and system content number  of times with no luck</p>
","2024-05-27 22:49:05","-2","Question"
"78531825","78437686","","<p>After some experimenting, this is the approach that worked for me:</p>
<pre><code>public BestMove minmax(String[][] position, int depth, int alpha, int beta,
    boolean maximizingPlayer) {
    if (depth == 0 || noPiecesLeft(position)) {
        return new BestMove(AssessGame.assess(position, teamID, depth), position);
    }

    if (maximizingPlayer) {
        int maxEval = Integer.MIN_VALUE;
        String[][] bestPosition = null;
        for (String[][] child : generateChildren(position, true)) {
            BestMove move = minmax(child, depth - 1, alpha, beta, false);
            int eval = move.value;
            if (eval &gt; maxEval) {
                maxEval = eval;
                bestPosition = child;
            }
            alpha = Integer.max(alpha, eval);
            if (beta &lt;= alpha) {
                break;
            }
        }
        return new BestMove(maxEval, bestPosition);
    } else {
        int minEval = Integer.MAX_VALUE;
        String[][] bestPosition = null;
        for (String[][] child : generateChildren(position, false)) {
            BestMove move = minmax(child, depth - 1, alpha, beta, true);
            int eval = move.value;
            if (eval &lt; minEval) {
                minEval = eval;
                bestPosition = child;
            }
            beta = Integer.min(beta, eval);
            if (beta &lt;= alpha) {
                break;
            }
        }
        return new BestMove(minEval, bestPosition);
    }
}
</code></pre>
","2024-05-25 08:13:19","0","Answer"
"78529792","78476748","","<p>Instead of sum you can use sigmoid, relu or something different and brute force your way into it, kindly check the official documentation of NEAT</p>
","2024-05-24 16:46:28","0","Answer"
"78521252","77968544","","<p>Not sure if you've already fixed your blockers. I ran into something similar with MongoDb, and although it's not a new collection being created in the database when using the MongoDB implementation of the Embedded Store, it happens to be another issue with ObjectId's. My project was underway, until I realized I could use langchain4j to implement various AI actions. So, I already had various queries and aggregations to my MongoDB Collections in service classes, with data already stored as embeddings. What I did was reuse these queries and store them in a <code>InMemoryEmbeddingStore</code>. My query already received the most relevant embeddings from original prompt. Or you can store the information you want to embed as plain text in your DB and use a langchain4j EmbeddingModel to convert data into embeddings to push into that InMemoryEmbeddingStore. Here's the first iteration of my work-around.</p>
<pre><code>    // We will use initialize and use the ADA_002 to create embeddings on text field from database
    EmbeddingModel embeddingModel = new OpenAiEmbeddingModel.OpenAiEmbeddingModelBuilder()
            .modelName(OpenAiEmbeddingModelName.TEXT_EMBEDDING_ADA_002)
            .apiKey(openAiKey)
            .maxRetries(2)
            .build();

    InMemoryEmbeddingStore&lt;TextSegment&gt; inMemoryEmbeddingStore = new InMemoryEmbeddingStore&lt;&gt;();

    // we can use query service to get most relevant search results
    List&lt;QueryResults&gt; querySearchResult = queryService.getDataFromVectorSearch(prompt);

    querySearchResult.forEach(result -&gt; {
        TextSegment textSegment = TextSegment.from(result.getText());
        Embedding embedding = embeddingModel.embed(textSegment).content();
        inMemoryEmbeddingStore.add(embedding, textSegment);
    });

    // use embedding model and in memory store as retriever for optimal answer
    EmbeddingStoreContentRetriever retriever = EmbeddingStoreContentRetriever.builder()
            .embeddingModel(embeddingModel)
            .embeddingStore(inMemoryEmbeddingStore)
            .build();

    String question = prompt;

    // given the prompt with the added context and user question, we can now build our model
    ChatLanguageModel chatLanguageModel = OpenAiChatModel.builder()
            .apiKey(openAiKey)
            .modelName(GPT_4o)
            .timeout(Duration.ofSeconds(60))
            .build();

    // using our AI system interface build the prompt
    Bot bot = AiServices.builder(Bot.class)
            .chatLanguageModel(chatLanguageModel)
            .contentRetriever(retriever)
            .build();

    return bot.chat(1, question);
</code></pre>
<p>Something great about langchain4j's <code>InMemoryEmbeddingStore</code> is that you can save messages with an Id. There are few things still being updated frequently to langchain4j, so I'm sure you'll be able to find a suitable solution in no time. Hope this helps!</p>
","2024-05-23 06:17:15","0","Answer"
"78520001","78430061","","<p>If you're working from Anaconda, you'll need to install Git first. That is, first:</p>
<p><code>conda install git</code></p>
<p>And then, after installing Git:</p>
<p><code>pip install git+https://github.com/openai/whisper.git</code></p>
<p>A complete guide (for me) to the commands for installing whisper for Windows 11 in Anaconda as follows:</p>
<p><code>conda create --name whisper_env python=3.8</code></p>
<p><code>conda activate whisper_env</code></p>
<p><code>conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia</code></p>
<p><code>conda install git</code></p>
<p><code>pip install git+https://github.com/openai/whisper.git</code></p>
<p><code>conda install -c conda-forge ffmpeg</code></p>
","2024-05-22 21:08:13","3","Answer"
"78516192","","ComfyUI RIFE VFI error tried all GitHub base urls to download but no success?","<p>I'm working with <a href=""https://github.com/Fannovel16/ComfyUI-Frame-Interpolation"" rel=""nofollow noreferrer"">https://github.com/Fannovel16/ComfyUI-Frame-Interpolation</a> in ComfyUI and I get the following error when attempting to use the RIFE VFI node in this basic setup:</p>
<blockquote>
<p>Error occurred when executing RIFE VFI:</p>
<p>Tried all GitHub base urls to download rife49.pth but no suceess.
Below is the error log:</p>
<p>Error when downloading from:
<a href=""https://github.com/styler00dollar/VSGAN-tensorrt-docker/releases/download/models/rife49.pth"" rel=""nofollow noreferrer"">https://github.com/styler00dollar/VSGAN-tensorrt-docker/releases/download/models/rife49.pth</a></p>
</blockquote>
<p><a href=""https://i.sstatic.net/MCuiVPpB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MCuiVPpB.png"" alt=""enter image description here"" /></a></p>
<p>Worth noting I am on a Mac, but this doesn't appear to be a problem with Mac so much as unable to find any of the rife .pth files. I have manually downloaded rife49.pth but am unsure where in the ComfyUI-Frame-Interpolation folder structure I would put it:
<a href=""https://i.sstatic.net/UlshUsED.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UlshUsED.png"" alt=""enter image description here"" /></a></p>
<p>How can I fix this so that I can use the RIFE VFI node?</p>
","2024-05-22 08:26:15","0","Question"
"78515009","78300896","","<p>Sure, you can do that. But, if you won't use thread, just specify threaded to False.</p>
<pre><code>waitress.serve(threaded=False, ...)
</code></pre>
<p>Or you can specify the connection limit, by default <code>100</code>.</p>
<pre><code>waitress.serve(connection_limit=1)
</code></pre>
<p>For more information about waitress serve arguments, you can go with this <a href=""https://docs.pylonsproject.org/projects/waitress/en/stable/arguments.html"" rel=""nofollow noreferrer"">link</a>.</p>
","2024-05-22 01:55:17","0","Answer"
"78514087","78497575","","<p>Several things to note:</p>
<ol>
<li><p>I suspect you are using the old abandoned repo, just based on the paths you give in the example commands.  Note the AlexeyAB repo was abandoned in July 2021.  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#which_repos_to_use_and_avoid"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#which_repos_to_use_and_avoid</a></p>
</li>
<li><p>The new Darknet/YOLO repo people should be using is the one sponsored by Hank.ai.  All new development is being done there.  See here:  <a href=""https://github.com/hank-ai/darknet#table-of-contents"" rel=""nofollow noreferrer"">https://github.com/hank-ai/darknet#table-of-contents</a></p>
</li>
<li><p>Looks like your training images contains cropped images of fruit, is that right?  That first image of an apple that takes up almost 100% of the image...if you train with that, you're telling Darknet/YOLO that the objects you want to detect must take up ~100% of the image.  So if you then pass in images that have apples, it will fail to detect anything.  See here for details:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#crop_training_images"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#crop_training_images</a></p>
</li>
<li><p>Your training command is not the recommended one.  Please see the Darknet/YOLO FAQ where this is explained:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#training_command"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#training_command</a></p>
</li>
<li><p>You should also post a copy of your chart.png file.  Maybe have a peek at your other chart_*.png files as well to see if all 3 classes are training the same way.  But at the very least, please edit this question and add your chart.png file before trying to figure out why something cannot be detected.</p>
</li>
<li><p>I <strong>strongly</strong> recommend you download and run DarkMark to test your annotations <em>prior</em> to training.  I cannot begin to count the number of issues this has solved over the years.  It was written specifically to find and prevent problems.  (Disclaimer:  I'm the author.)  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#what_software_is_used"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#what_software_is_used</a></p>
</li>
<li><p>Join the Darknet/YOLO discord server.  Lots of folks there to help people get started.  <a href=""https://discord.gg/zSq8rtW"" rel=""nofollow noreferrer"">https://discord.gg/zSq8rtW</a></p>
</li>
</ol>
","2024-05-21 19:51:34","0","Answer"
"78511667","78511513","","<p>From the documentation:</p>
<p><strong>Create tensor</strong>:</p>
<blockquote>
<p>Creates a tensor with a user supplied buffer. Wraps
OrtApi::CreateTensorWithDataAsOrtValue.</p>
</blockquote>
<p><strong>CreateTensorWithDataAsOrtValue</strong>:</p>
<blockquote>
<p>Create a tensor backed by a user supplied buffer.</p>
<p><strong>Create a tensor with user's buffer.</strong> You can fill the buffer either before calling this function or after. <strong>p_data is owned by caller</strong>.
ReleaseValue won't release p_data.</p>
</blockquote>
<p>In your code:</p>
<pre><code>Ort::Value i2t(const std::string &amp;img_path) {
    Ndarray&lt;float&gt; input_img = i2t2(img_path);

    // Create the input tensor
    auto input_vector = input_img.data;
    auto input_shape = input_img.shape;
    return Ort::Value::CreateTensor&lt;float&gt;(
            mem_info,
            input_vector.data(),  // passing buffer owned by local variable
            input_vector.size(),
            input_shape.data(),   // passing buffer owned by local variable
            input_shape.size());

     // after the return statement local variables are deleted and passed buffers are now dangling pointers
}
</code></pre>
<p>In short commented out code works because local variables are still valid (as it's the same scope) and function doesn't as scope of those local variables ended after the return statement</p>
","2024-05-21 11:52:16","4","Answer"
"78511513","","In cpp, return cause std::vector value changed","<p>Following is my code</p>
<pre><code>#include &quot;gtest/gtest.h&quot;
#include &quot;onnx_helper.h&quot;
#include &quot;vector_helper.h&quot;
#include &quot;opencv_helper.h&quot;
#include &lt;iostream&gt;
#include &lt;opencv2/opencv.hpp&gt;

    class OnnxTest : public testing::Test {
    protected:
    //    static void SetUpTestCase() {
    //    }
    };
    
    Ndarray&lt;float&gt; i2t2(const std::string &amp;img_path) {// Load the image using OpenCV
        cv::Mat input_mat = cv::imread(img_path);
    
        // Convert to float
        cv::cvtColor(input_mat, input_mat, cv::COLOR_BGR2RGB);
        input_mat.convertTo(input_mat, CV_32F);
    
        // Normalize
        input_mat /= 255.0;
    
        // Convert the preprocessed image to a vector of floats
        Ndarray&lt;float&gt; input_img = mat_to_ndarray(input_mat);
    
        // Transpose and expand dimensions
        input_img.transpose({2, 0, 1});
        input_img.expand_dims(0);
        return input_img;
    }
    
    // Assuming you have already defined the necessary functions for image preprocessing
    Ort::Value i2t(const std::string &amp;img_path) {
        Ndarray&lt;float&gt; input_img = i2t2(img_path);
    
        // Create the input tensor
        auto input_vector = input_img.data;
        auto input_shape = input_img.shape;
        return Ort::Value::CreateTensor&lt;float&gt;(
                mem_info,
                input_vector.data(),
                input_vector.size(),
                input_shape.data(),
                input_shape.size());
    }
    
    
    TEST_F(OnnxTest, run_onnx) {
        // Define paths
        auto d = &quot;/home/roroco/Dropbox/cpp/cpp_lib/test/fix/onnx&quot;;
        std::string onnx_model_path = std::format(&quot;{}/RealESRGAN_x4plus_anime_6B.onnx&quot;, d);
        std::string input_img_path = std::format(&quot;{}/small.jpg&quot;, d);
        const std::string &amp;out_img_path = std::format(&quot;/tmp/t.jpg&quot;, d);
    
        // Run inference
        auto s = new_session(onnx_model_path);
    
        auto tensor = i2t(input_img_path);
    
    //    Ndarray&lt;float&gt; input_img = i2t2(input_img_path);
    //    // Create the input tensor
    ////    auto mem_info = Ort::MemoryInfo::CreateCpu(OrtAllocatorType::OrtArenaAllocator,
    ////                                               OrtMemType::OrtMemTypeDefault);
    //    auto input_vector = input_img.data;
    //    auto input_shape = input_img.shape;
    //    auto tensor = Ort::Value::CreateTensor&lt;float&gt;(
    //            mem_info,
    //            input_vector.data(),
    //            input_vector.size(),
    //            input_shape.data(),
    //            input_shape.size());
    
        Ort::Value out_tensor = infer(s, tensor);
        tensor_to_img(out_tensor, out_img_path);
        std::cout &lt;&lt; std::format(&quot;Output saved to {}&quot;, out_img_path) &lt;&lt; std::endl;
    }
</code></pre>
<p>When I use clion and set breakpoint in return CreateTensor, it's very strange return cause input_vector and input_shape change(see my following gif)</p>
<p><a href=""https://i.sstatic.net/gwOwxKJI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gwOwxKJI.png"" alt=""enter image description here"" /></a></p>
<p>If I don't use meth but use create tensor code in main, the input_vector and input_shape keep old value</p>
<p><a href=""https://i.sstatic.net/6tN5ljBM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6tN5ljBM.png"" alt=""enter image description here"" /></a></p>
<p>this err cause I get wrong tensor and get wrong result, how to fix it</p>
","2024-05-21 11:21:39","1","Question"
"78508729","78500810","","<p>thanks to the comment of @Christoph Rackwitz i was able to find a solution</p>
<p>what you need to do:</p>
<p><strong>1-</strong> <a href=""https://raw.githubusercontent.com/opencv/opencv/master/doc/js_tutorials/js_assets/utils.js"" rel=""nofollow noreferrer"">get the utils.js from OpenCV's GitHub repo</a>.</p>
<p><strong>2-</strong> in order for it to work properly, you need to add this line to it (use the relative path of opencv.js you are using)</p>
<pre><code>import * as cv from &quot;../opencv/opencv&quot;;
</code></pre>
<p><strong>3-</strong> import it into the component that you want to used the harrcascade in.</p>
<pre><code>import Utils from &quot;../assets/utils&quot;;
</code></pre>
<p>also, make sure to import opencv.js too.</p>
<p><strong>4-</strong> next, you'll need to put the haarcascade file of your choice in <code>/public</code>.</p>
<p><strong>5-</strong> assign it to a variable:</p>
<pre><code>const xmlURL = &quot;haarcascade_xxxx_license_plate.xml&quot;;
</code></pre>
<p><strong>6-</strong> finely, we'll use <code>createFileFromUrl()</code> from utils.js and load our haarcascade file:</p>
<pre><code>utils.createFileFromUrl(xmlURL, xmlURL, () =&gt; {
     licensePlateCascade.load(xmlURL);
</code></pre>
<p>here is the full function for your convenience:</p>
<pre><code>  const handleLicensePlateDetection = async (e) =&gt; {

try {
  console.log(&quot;Starting license plate detection...&quot;);

  // Clean up previous resources if necessary
  if (window.src) window.src.delete();
  if (window.gray) window.gray.delete();
  if (window.plates) window.plates.delete();
  if (window.licensePlateCascade) window.licensePlateCascade.delete();

  const file = e.target.files[0];
  const reader = new FileReader();
  reader.onload = async () =&gt; {
    console.log(&quot;Image loaded successfully.&quot;);
    const image = new Image();
    image.src = reader.result;
    image.onload = async () =&gt; {
      console.log(&quot;Converting image to grayscale...&quot;);
      const src = cv.imread(image);
      const gray = new cv.Mat();
      cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);

      console.log(&quot;Loading Haar Cascade XML file...&quot;);
      const licensePlateCascade = new cv.CascadeClassifier();
      const xmlURL = &quot;haarcascade_xxx_license_plate.xml&quot;; // Path to the XML file
      utils.createFileFromUrl(xmlURL, xmlURL, () =&gt; {
        try {
          licensePlateCascade.load(xmlURL);
          console.log(&quot;Haar Cascade XML file loaded successfully.&quot;);

          console.log(&quot;Detecting license plates...&quot;);
          const plates = new cv.RectVector();
          const msize = new cv.Size(0, 0);
          licensePlateCascade.detectMultiScale(
            gray,
            plates,
            1.1,
            3,
            0,
            msize,
            msize
          );

          console.log(&quot;Applying Gaussian blur to detected plates...&quot;);
          for (let i = 0; i &lt; plates.size(); ++i) {
            try {
              const plate = plates.get(i);
              const roi = src.roi(plate);
              cv.GaussianBlur(
                roi,
                roi,
                new cv.Size(23, 23),
                40,
                40,
                cv.BORDER_DEFAULT
              );
              roi.delete();
            } catch (error) {
              console.error(
                &quot;Error applying Gaussian blur to plate:&quot;,
                error
              );
            }
          }

          console.log(&quot;Displaying the result...&quot;);
          cv.imshow(&quot;canvas&quot;, src);

          // Clean up
          src.delete();
          gray.delete();
          plates.delete();
          licensePlateCascade.delete();
        } catch (error) {
          console.error(&quot;Error loading Haar Cascade XML file:&quot;, error);
        }
      });
    };
  };
  reader.readAsDataURL(file);
} catch (error) {
  console.error(&quot;Error initializing license plate detection:&quot;, error);
}
  };
</code></pre>
<p>note: <em>i'm new to this &quot;AI&quot; stuff and needed the assistance of an AI, thus i don't understand most of what i just pasted</em></p>
","2024-05-20 20:48:40","0","Answer"
"78507718","","Issue with Embedding Chatflow and Passing Parameters to Flowise","<p>I've encountered an issue while trying to embed the Flowise chat within a website and pass parameters via chatflowConfig to the flow, specifically to override the &quot;url&quot; for the API node. Unfortunately, none of the parameters I tested seem to work, and it appears the flow is completely ignoring them.</p>
<p>Here is the code snippet I'm using:</p>
<pre><code>&lt;script type=&quot;module&quot;&gt;
  import Chatbot from 'https://cdn.jsdelivr.net/npm/flowise-embed/dist/web.js';
  Chatbot.init({
    chatflowid: &quot;bc7495d5-aec7-4067-a413-b57471482ea7&quot;,
    apiHost: &quot;http://84.140.44.51:5777&quot;,
    chatflowConfig: {
      url: &quot;https://test.com/api.php?id=1&quot;
    },
    theme: {
      // theme configuration
    }
  });
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Despite following the documentation, the parameter does not seem to be recognized or utilized by the flow. I've tried several variations and approaches, but the issue persists. How to properly pass parameters like <code>&quot;url&quot;</code> to the chat flow? or let me know if there's a different method to achieve this?</p>
<p><strong>What I tried:</strong></p>
<p>Initial setup:</p>
<p>Embedded the Flowise chat using the provided script.
Passed the url parameter via chatflowConfig as shown in the code snippet.</p>
<p>Parameter variations:</p>
<p>Tried different variations of the parameter names and values.
Tested with other parameters in chatflowConfig.</p>
<p>Expected outcome:</p>
<p>I expected the flow to recognize and utilize the url parameter to direct the API node to the specified endpoint <code>https://test.com/api.php?id=1.</code></p>
<p>Actual result:</p>
<p>The flow completely ignored the url parameter and did not direct the API node to the specified endpoint. None of the variations I tried had any effect.</p>
","2024-05-20 16:22:45","0","Question"
"78507665","78507510","","<p>The error is telling you that <code>Ort::Value</code> does not have a usable copy constructor, but you are trying to make a copy in this statement:</p>
<pre><code>auto tensor = img_file_to_tensor(input_img_path);
</code></pre>
<p>As <code>auto</code> will be deduced as <code>Ort::Value</code> not <code>Ort::Value&amp;</code> as you are expecting. <code>auto</code> does not deduce references.  If you want a reference, you have to be explicit, ie:</p>
<pre><code>const auto &amp;tensor = img_file_to_tensor(input_img_path);
</code></pre>
<p>However, <code>CreateTensor()</code> returns a new <code>Ort::Value</code> by value. It is a <em>temporary</em> object. You are extending the lifetime of that temporary when you initialize your <code>tensor</code> reference variable using that temporary. The temporary remains alive only while that <code>tensor</code> reference remains in scope. Once you reach your <code>return</code> statement, the <code>tensor</code> reference goes out scope, thus the <code>Ort::Value</code> object goes out of scope and is destroyed, and you return a <em>dangling reference</em>, which is <strong>undefined behavior</strong>.  You simply can't return a reference to a temporary.</p>
<p>Fortunately, <code>Ort::Value</code> does support move semantics (ie, it has a move constructor and a move assignment operator), so there is no need to return the temporary by reference. You can return it by value instead, and let the compiler <em>move</em> it for you (no need to use <code>std::move</code> in this example):</p>
<pre><code>Ort::Value img_file_to_tensor(std::string input_img_path) {
    ...
    return Ort::Value::CreateTensor&lt;float&gt;(
            memoryInfo,
            input_vector.data(),
            input_vector.size(),
            input_shape.data(),
            input_shape.size());
}
</code></pre>
<pre><code>auto tensor = img_file_to_tensor(input_img_path);
</code></pre>
","2024-05-20 16:13:22","2","Answer"
"78507510","","In cpp, what is way to fix err ""use of deleted function ‘Ort::Value::Value(const Ort::Value&)""","<p>I use following code</p>
<pre><code>const Ort::Value &amp;img_file_to_tensor(std::string input_img_path) {
    // Load the image using OpenCV
    cv::Mat input_mat = cv::imread(input_img_path);

    // Convert to float
    cv::cvtColor(input_mat, input_mat, cv::COLOR_BGR2RGB);
    input_mat.convertTo(input_mat, CV_32F);
    // Normalize

    input_mat /= 255.0;
    // Convert the preprocessed image to a vector of floats
    Ndarray&lt;float&gt; input_img = mat_to_ndarray(input_mat);

//    std::string r = xarr_to_str(xa);
    input_img.transpose({2, 0, 1});
//    const std::string &amp;xa_str1 = xarr_to_str(xa);
//    std::cout &lt;&lt; xa_str1 &lt;&lt; std::endl;
    input_img.expand_dims(0);
//    const std::string &amp;xa_str2 = xarr_to_str(xa);
//    std::cout &lt;&lt; xa_str2 &lt;&lt; std::endl;

    auto input_vector = input_img.data;
    auto input_shape = input_img.shape;

    Ort::MemoryInfo memoryInfo = Ort::MemoryInfo::CreateCpu(OrtAllocatorType::OrtArenaAllocator,
                                                            OrtMemType::OrtMemTypeDefault);
    // Create the input tensor
    const Ort::Value &amp;tensor = Ort::Value::CreateTensor&lt;float&gt;(
            memoryInfo,
            input_vector.data(),
            input_vector.size(),
            input_shape.data(),
            input_shape.size());
    return tensor;
}
</code></pre>
<p>and call it</p>
<pre><code>auto tensor = img_file_to_tensor(input_img_path);
</code></pre>
<p>above call code raise err:</p>
<pre><code>error: use of deleted function ‘Ort::Value::Value(const Ort::Value&amp;)’
   23 |     auto tensor = img_file_to_tensor(input_img_path);
</code></pre>
<p>I think the reason is: when return, Ort::Value mem will be removed, so what is way to create tensor in meth to prevent this err</p>
","2024-05-20 15:38:02","0","Question"
"78507089","","ComfyUI KSampler step is too slow. Is it normal?","<p>I am using AnimateDiff in ComfyUI to output videos, but the speed feels very slow.
I am wondering if this is normal.</p>
<p>Below are the details of my work environment.</p>
<p>input img frames</p>
<ul>
<li>10~60
(KSampler speed 120~830s/it)</li>
</ul>
<p>checkpoint model</p>
<ul>
<li>wildcardTURBO_sdxl, anythingXL</li>
</ul>
<p>LoRA model</p>
<ul>
<li>gyblistyle, cartoon, EnvyOil
(also tried without Lora)</li>
</ul>
<p>controlnet model</p>
<ul>
<li>depth, canny(used both)</li>
</ul>
<p>KSampler(Advanced)
steps : 8
cfg : 2.5
sampler : dpm_2_ancestral, dpmpp_sde_gpu, dpmpp_2m, dpmpp_2(or3)m_sde_gpu
scheduler : karras</p>
<p>Python version: 3.10.11
VAE dtype: torch.bfloat16
Pytorch version: 2.1.2+cu121
Device: cuda:0 NVIDIA GeForce RTX 3090 : cudaMallocAsync
Platform: Windows10</p>
<p>and My GPU works only 0~4%(almost 1%) at KSampler</p>
<p>Things I have tried</p>
<p>• Executed using the run_gpu batch file.</p>
<p>• Tried adding –-lowvram, --normalvram, and –-gpu only to the run_gpu batch file.</p>
<p>•Received a PyTorch compilation error (video generation was successful).
Found that it was set to use the CPU.
Downgraded Python and PyTorch to enable GPU usage, and the PyTorch error disappeared.</p>
<p>•Observed the following message in the cmd window when running ComfyUI:
Total VRAM 24575 MB, total RAM 130990 MB
Set vram state to: NORMAL_VRAM
Device: cuda:0 NVIDIA GeForce RTX 3090 : cudaMallocAsync
VAE dtype: torch.bfloat16</p>
<p>•Adjusted the sampler and steps to fit the model.</p>
<p>•Changed the existing workflow and model optimized for SDXL to SD 1.5</p>
<p>Changing the conditions affected the video quality, but the progress speed at the KSampler step did not improve at all (the GPU also hardly works consistently).</p>
<p>When applying a LoRA model for a Pixar style to output a 3D video, it took about 12 hours to produce a 4-second video!!!</p>
<p>I am wondering if something is wrong or if it is supposed to take this long...</p>
","2024-05-20 14:11:07","1","Question"
"78505568","78490928","","<p>If you go to the <a href=""https://mvnrepository.com/artifact/org.springframework.ai/spring-ai-ollama/usages"" rel=""nofollow noreferrer"">maven repository</a> of the mentioned dependency, you will see that the dependency is relocated. To solve this problem, simply change the dependency location to the correct one:</p>
<pre><code>implementation 'io.springboot.ai:spring-ai-ollama-spring-boot-starter:1.0.3'
</code></pre>
<p>It is also worth considering to include <a href=""https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/getting-started.html#dependency-management"" rel=""nofollow noreferrer"">Spring AI Bill of Materials (BOM)</a> as noted by @M.Deinum's comment to avoids the need for you to specify and maintain the dependency versions yourself:</p>
<pre><code>    implementation platform(&quot;io.springboot.ai:spring-ai-bom:1.0.3&quot;)
    // Replace the following with the starter dependencies of specific modules you wish to use
    implementation 'io.springboot.ai:spring-ai-ollama-spring-boot-starter'
</code></pre>
","2024-05-20 08:46:03","1","Answer"
"78500810","","what is the correct way to load an xml file into reactjs?","<p>i'm working on a MERN Stack project for the university where users can upload pictures of cars, and i want to use opencv to blur out the license plates, thus i need to use a &quot;haarcascades&quot; file to identify the area.
here is the relevant code:</p>
<pre><code>  const handlePhotosChange = async (e) =&gt; {
    const { files } = e.target;
    const selectedPhotos = Array.from(files);

    for (const photo of selectedPhotos) {
      const img = new Image();
      img.src = URL.createObjectURL(photo);

      await new Promise((resolve) =&gt; {
        img.onload = () =&gt; {
          resolve();
        };
      });

      const mat = cv.imread(img);
      const gray = new cv.Mat();
      cv.cvtColor(mat, gray, cv.COLOR_RGBA2GRAY);

      const licensePlateCascade = new cv.CascadeClassifier();
      const cascadeLoaded = licensePlateCascade.load(
        &quot;haarcascade_license_plate_rus_16stages.xml&quot;
      );
      console.log(&quot;1&quot;);
      if (cascadeLoaded) {
        console.log(&quot;2&quot;);
        const plates = new cv.RectVector();
        const minSize = new cv.Size(30, 30);
        licensePlateCascade.detectMultiScale(gray, plates, 1.1, 3, 0, minSize);

        for (let i = 0; i &lt; plates.size(); i++) {
          const plate = plates.get(i);
          const plateMat = gray.roi(plate);
          cv.GaussianBlur(plateMat, plateMat, new cv.Size(5, 5), 0);
          plateMat.delete();
        }

        const blurredImg = new Image();
        blurredImg.src = URL.createObjectURL(
          new Blob([cv.imencode(&quot;.jpg&quot;, mat)], { type: &quot;image/jpeg&quot; })
        );
        const blurredBlob = await fetch(blurredImg.src).then((res) =&gt;
          res.blob()
        );

        selectedPhotos[selectedPhotos.indexOf(photo)] = blurredBlob;

        plates.delete();
      }

      mat.delete();
      gray.delete();
      licensePlateCascade.delete();
    }

    setPhotos(selectedPhotos);
  };

</code></pre>
<p>i'm having a problem with this line specifically :</p>
<pre><code>      const cascadeLoaded = licensePlateCascade.load(
        &quot;haarcascade_license_plate_rus_16stages.xml&quot;
      );
</code></pre>
<p>the moment the code executes, i get this error in console:</p>
<blockquote>
<p>[ERROR:0@5.090] global persistence.cpp:531 open Can't open file: 'haarcascade_license_plate_rus_16stages.xml' in read mode</p>
</blockquote>
<p>i tried putting the full path, using the <a href=""https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_license_plate_rus_16stages.xml"" rel=""nofollow noreferrer"">row github link</a> , putting the file in public, and putting the code and files in the backend.
nothing worked.</p>
<p>the expected behavior: the file loads with no errors.</p>
","2024-05-18 19:34:16","1","Question"
"78497575","","I train yolo model with my own data set but there is no test result","<p>I am training the model using the Yolov3 model with the data set I received from Kaggle. Model training is completed and I add the new weights to the backup folder. I run one of the fruits I trained with for testing, but object detection does not occur. The same image appears as Prediction.jpg. The training seems to be decent, but I don't understand why it can't detect objects. Please help me.</p>
<p>Train Terminal Code:</p>
<pre><code>./darknet detector train /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/Desktop/Projects/Bitirmeprojesi/yolov3.weights
</code></pre>
<p>Test Terminal Code:</p>
<pre><code>./darknet detector test /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/darknet/backup/yolov3_final.weights -thresh 0.25 -out predictions.jpg
</code></pre>
<p>I set up and edited obj.data and obj.names and yolov3.cfg files.</p>
<p>I have 3 classes: apple, banana and orange. I have properly set the values ​​such as filter and class values ​​in the cfg file according to the 3 classes.</p>
<pre><code>cfg file 
[net]
# Testing
batch=64
subdivisions=1
# Training
subdivisions=16
width= 608
height=608
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=0.3

learning_rate=0.001
burn_in=1000
max_batches = 6000 # classnum * 2000
policy=steps
steps=3600,4800 # max_batches num %80, %90 
scales=.1,.1
</code></pre>
<p>In addition to the .jpg images in the data set, there are .txt files with the same name in yolo format.</p>
<p>File image:
<a href=""https://i.sstatic.net/Ed1BBcZP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ed1BBcZP.png"" alt=""file image"" /></a></p>
<p>The train.txt and test.txt files containing the paths to all the images are also ready.</p>
<p>When I run the test command in the terminal, it works, but the picture looks the same, there are no bounding boxes that detect objects. I'm sure I have Opencv installed. I am using macOS. Why is it not detecting it? Someone please help. I cleaned the darknet many times by saying make clean and ran it by saying make opencv = 1, but the result does not change.</p>
<pre><code>[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00
Total BFLOPS 137.613 
avg_outputs = 1052318 
Loading weights from /Users/melisabagcivan/darknet/backup/yolov3_final.weights...
 seen 64, trained: 32013 K-images (500 Kilo-batches_64) 
Done! Loaded 107 layers from weights-file 
Enter Image Path: /Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg
 Detection layer: 82 - type = 28 
 Detection layer: 94 - type = 28 
 Detection layer: 106 - type = 28 
/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg: Predicted in 6738.129000 milli-seconds.
</code></pre>
<p>I tried it with many images and it doesn't draw the box in any of them. I don't understand whether it can't detect it or whether I'm making a mistake while testing it.</p>
","2024-05-17 19:21:32","1","Question"
"78497080","78452864","","<p>I'm having the same issue as you, a fix that you may or maynot be able to implement is downgrading your python version back to 3.10 in which gemini seems to be working without any issues.</p>
","2024-05-17 17:21:32","3","Answer"
"78495717","78494926","","<p>label.txt is a normal text file that contains the labels. You can write your own labels in the same order that you used to train the model.
You can use <a href=""https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;opi=89978449&amp;url=https://netron.app/&amp;ved=2ahUKEwjPwvPy2ZSGAxXo7TgGHWblD-8QFnoECAQQAQ&amp;usg=AOvVaw34UUIauzxbAmKRARkp44gz"" rel=""nofollow noreferrer"">Netron</a> to view your model input and outputs.</p>
","2024-05-17 12:38:47","0","Answer"
"78494926","","Converting a saved model to tflite and connecting it to flutter Application","<p>I am new to Flutter development and have been using Teachable Machine to create my models and connect them to my Flutter app using the flutter_tflite: ^1.0.1 dependency. When I export a model from Teachable Machine, I receive a .tflite file and a label.txt file. The conversion model is in floating-point format.</p>
<p>Now, I have created my own model in Jupyter Notebook. How can I achieve the same—converting my model to a .tflite file and obtaining a label.txt file where the conversion model is in floating-point format?<a href=""https://i.sstatic.net/oTr4XYtA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oTr4XYtA.png"" alt=""enter image description here"" /></a><a href=""https://i.sstatic.net/jk6J6rFd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jk6J6rFd.png"" alt=""enter image description here"" /></a></p>
<p>I've tried converting the model to TensorFlow Lite using this code, but I'm getting a .tflite model without the labels.txt file. Additionally, I'm not sure if the conversion model is in floating-point format.using this code</p>
<p>import tensorflow as tf
import tensorflow.lite as tflite
import os</p>
<h1>Path to the directory containing the saved model</h1>
<p>saved_model_dir = &quot;C:\Users\PC\Desktop\models\2&quot;</p>
<p>try:
# Load the model
converter = tflite.TFLiteConverter.from_saved_model(saved_model_dir)
print(&quot;Model loaded successfully.&quot;)</p>
<pre><code># Convert the model to TensorFlow Lite format
tflite_model = converter.convert()
print(&quot;Model converted successfully.&quot;)

# Specify the output directory and file name
output_dir = &quot;C:\\Users\\PC\\Desktop\\models\\converted&quot;
os.makedirs(output_dir, exist_ok=True)
output_file = os.path.join(output_dir, &quot;converted_model.tflite&quot;)

# Save the converted model to a file
with open(output_file, &quot;wb&quot;) as f:
    f.write(tflite_model)
print(f&quot;Model saved successfully as {output_file}.&quot;)
</code></pre>
<p>except Exception as e:
print(f&quot;An error occurred: {e}&quot;)</p>
","2024-05-17 10:04:53","0","Question"
"78490928","","Could not find org.springframework.ai","<p>I'm trying to integrate Llama3 in my spring application by using the following documentation:
<a href=""https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/ollama-chat.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/ollama-chat.html</a></p>
<p>When building the application with the added dependency:</p>
<pre><code> implementation 'org.springframework.ai:spring-ai-ollama-spring-boot-starter'
</code></pre>
<p>I got the following error:</p>
<pre><code>Execution failed for task ':compileJava'.
&gt; Could not resolve all files for configuration ':compileClasspath'.
   &gt; Could not find org.springframework.ai:spring-ai-ollama-spring-boot-starter:.
</code></pre>
<p>This github issue <a href=""https://github.com/spring-projects/spring-ai/issues/194"" rel=""nofollow noreferrer"">https://github.com/spring-projects/spring-ai/issues/194</a> also doesn't help. After changing the path based on a comment there to:</p>
<pre><code>    implementation 'org.springframework.experimental.ai:spring-ai-ollama-spring-boot-starter'
</code></pre>
<p>I got the following error:</p>
<pre><code>Execution failed for task ':compileJava'.
&gt; Could not resolve all files for configuration ':compileClasspath'.
   &gt; Could not find org.springframework.experimental.ai:spring-ai-ollama-spring-boot-starter:
</code></pre>
","2024-05-16 15:15:42","2","Question"
"78486667","78486660","","<p>That is because you need to add in more code to try and fix the problem</p>
","2024-05-15 21:47:44","-1","Answer"
"78486660","","AI SDK Retry Error from official vercel page","<p>I'm using code from the official <a href=""https://sdk.vercel.ai/docs/getting-started/nextjs-pages-router"" rel=""nofollow noreferrer"">AI SDK website</a> and this code does NOT run.</p>
<p>This is the error I'm getting: RetryError [AI_RetryError]: Failed after 3 attempts. Last error: Failed to process error response</p>
<p>I just want to run a basic AI program with nextJS.</p>
","2024-05-15 21:44:26","0","Question"
"78484966","78473568","","<p>I think your code was supposed to work for an earlier version of pinecone. I faced a similar issue and this code works for me:</p>
<pre><code>from pinecone import Pinecone
from pinecone import ServerlessSpec
from langchain_pinecone import PineconeVectorStore
import os

# initialize connection to pinecone (get API key at app.pinecone.io)
api_key = PINECONE_API_KEY
# configure client
pc = Pinecone(api_key=api_key)

os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY
index_name=&quot;chatbot&quot;
index = pc.Index(index_name)
PineconeVectorStore(index_name=index_name,embedding=embeddings)

vectorstore_from_docs = PineconeVectorStore.from_documents(
    text_chunks,
    index_name=index_name,
    embedding=embeddings
)

# index.describe_index_stats()
vectorstore = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)
qa=RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type=&quot;stuff&quot;, 
    retriever=vectorstore.as_retriever(search_kwargs={'k': 2}),
    return_source_documents=True, 
    chain_type_kwargs=chain_type_kwargs)
</code></pre>
","2024-05-15 15:24:45","0","Answer"
"78476748","","How do I solve the ""No such activation function: 'sum'"" error in Python NEAT?","<p>I was creating a simple game AI using NEAT. However, I discovered an issue where the game window would just open and close as it transitioned from generation 0 to generation 1, and in reality, it kept repeating generation 0, so I asked gpt to modify it. When I ran the modified code, I got the following message:</p>
<pre><code>eunwoocho@Eunwoos-MacBook-Pro python % /usr/local/bin/python3 &quot;/Users/eunwoocho/Documents/Python/import pygame.py&quot;
pygame 2.5.2 (SDL 2.28.3, Python 3.12.2)
Hello from the pygame community. https://www.pygame.org/contribute.html

****** Running generation 0 ******

Population's average fitness: 4.55500 stdev: 8.71705
Best fitness: 68.00000 - size: (10, 36) - species 69 - id 69
Average adjusted fitness: 0.067
Mean genetic distance 3.545, standard deviation 0.244
Population of 402 members in 200 species:
ID age size fitness adj fit stag
==== === ==== ======= ======= ====
1 0 2 1.0 0.015 0
.....omit....
200 0 2 0.0 0.000 0
Total extinctions: 0
Generation time: 2.484 sec

****** Running generation 1 ******

Traceback (most recent call last):
File &quot;/Users/eunwoocho/Documents/Python/import pygame.py&quot;, line 129, in &lt;module&gt;
run(config_path, 50)
File &quot;/Users/eunwoocho/Documents/Python/import pygame.py&quot;, line 121, in run
winner = p.run(lambda genomes, config: main(genomes, config, p.generation, target_generations), target_generations)
File &quot;/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/neat/population.py&quot;, line 89, in run
fitness_function(list(iteritems(self.population)), self.config)
File &quot;/Users/eunwoocho/Documents/Python/import pygame.py&quot;, line 121, in &lt;lambda&gt;
winner = p.run(lambda genomes, config: main(genomes, config, p.generation, target_generations), target_generations)
File &quot;/Users/eunwoocho/Documents/Python/import pygame.py&quot;, line 57, in main
net = neat.nn.FeedForwardNetwork.create(g, config)
File &quot;/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/neat/nn/feed_forward.py&quot;, line 51, in create
activation_function = config.genome_config.activation_defs.get(ng.activation)
File &quot;/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/neat/activations.py&quot;, line 128, in get
raise InvalidActivationFunction(&quot;No such activation function: {0!r}&quot;.format(name))
neat.activations.InvalidActivationFunction: No such activation function: 'sum'
</code></pre>
<p>The game did move from generation 0 to generation 1, but an error message</p>
<pre><code>&quot;No such activation function: 'sum'&quot;
</code></pre>
<p>occurred. However, I have never used 'sum' in config-feedfoword or in the game code, so I'm wondering why this error is occurring.</p>
<p>And this is the code modified by GPT and config-feedforward.txt file:</p>
<p><a href=""https://drive.google.com/drive/folders/1zgzum4HR72b0eOUtvjY7tlB3v8xk0_O5?usp=drive_link"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1zgzum4HR72b0eOUtvjY7tlB3v8xk0_O5?usp=drive_link</a></p>
<p>To solve the problem, I checked for any code containing sum, and following GPT's advice that the activation_default item was the issue, I changed it from activation_default = relu to sigmoid, but the 'sum' error still appears. The truly frustrating part is that I haven't used the word sum at all. Please help this suffering young developer.</p>
","2024-05-14 08:27:12","0","Question"
"78476582","78020093","","<p>You can consider <a href=""https://platform.openai.com/docs/assistants/tools/function-calling/quickstart"" rel=""nofollow noreferrer"">Function Calling</a>, the model is able to trigger of them.
You may need to run twice in this approach. Well, you can also consider using embeddings to define what user exactly need.</p>
<p>Create two lists for PDF and DB, put some example questions into them. Generate embeddings. Once you got user question compare embeddings which list is closer. Then run the model.</p>
","2024-05-14 07:57:38","0","Answer"
"78474243","78471070","","<p>If in your environment the seams may be different depending on the area of the chair it is recommended to classify the different seams (different bounding boxes for the same line).</p>
<p>To give an answer to the issue of instance segmentation.
You can use yolov5 without too many problems, you can learn to use <a href=""https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I don't know if you need particular applications, with yolov5 it is often necessary to work on the code to customize the problem</p>
","2024-05-13 18:49:28","0","Answer"
"78473568","","Pinecone serverless from existing index","<pre><code>from flask import Flask, render_template, jsonify, request
from src.helper import download_hugging_face_embeddings
# from langchain.vectorstores import Pinecone
import pinecone
from langchain.prompts import PromptTemplate
from langchain.llms import CTransformers
from langchain.chains import RetrievalQA
from dotenv import load_dotenv
from src.prompt import *
import os
from pinecone import Pinecone,ServerlessSpec
from langchain_pinecone import PineconeVectorStore

app = Flask(__name__)

load_dotenv()

PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')
PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV')


embeddings = download_hugging_face_embeddings()

#Initializing the Pinecone
# pinecone.init(api_key=PINECONE_API_KEY,
#               environment=PINECONE_API_ENV)
pc = Pinecone(
        api_key=os.environ.get(&quot;PINECONE_API_KEY&quot;)
    )
index_name=&quot;chatbot&quot;

#Loading the index
# docsearch=pc.from_existing_index(index_name, embeddings)
docsearch = Pinecone.from_existing_index(index_name, embeddings)

PROMPT=PromptTemplate(template=prompt_template, input_variables=[&quot;context&quot;, &quot;question&quot;])

chain_type_kwargs={&quot;prompt&quot;: PROMPT}

llm=CTransformers(model=&quot;model/llama-2-7b-chat.ggmlv3.q4_0.bin&quot;,
                  model_type=&quot;llama&quot;,
                  config={'max_new_tokens':512,
                          'temperature':0.8})


qa=RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type=&quot;stuff&quot;, 
    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),
    return_source_documents=True, 
    chain_type_kwargs=chain_type_kwargs)



@app.route(&quot;/&quot;)
def index():
    return render_template('chat.html')



@app.route(&quot;/get&quot;, methods=[&quot;GET&quot;, &quot;POST&quot;])
def chat():
    msg = request.form[&quot;msg&quot;]
    input = msg
    print(input)
    result=qa({&quot;query&quot;: input})
    print(&quot;Response : &quot;, result[&quot;result&quot;])
    return str(result[&quot;result&quot;])



if __name__ == '__main__':
    app.run(host=&quot;0.0.0.0&quot;, port= 8080, debug= True)

</code></pre>
<p>Why am I getting this error ?
Traceback (most recent call last):
File &quot;C:\final_year_chatbot\End-to-end-Medical-Chatbot-using-Llama2-main\app.py&quot;, line 34, in 
docsearch = Pinecone.from_existing_index(index_name, embeddings)
AttributeError: type object 'Pinecone' has no attribute 'from_existing_index'</p>
<p>I am trying to run this app.py but I am getting this error</p>
","2024-05-13 16:16:54","1","Question"
"78472683","78466048","","<p>Assuming you are referring to the <em>greedy</em> best-first search as described on <a href=""https://en.wikipedia.org/wiki/Best-first_search#Greedy_BeFS"" rel=""nofollow noreferrer"">Wikipedia</a>, you have expanded too many squares at the right side of the grid, while you have not expanded enough squares at the left side, which <em>would</em> be expanded by the best-first algorithm.</p>
<p>Let's do this step by step so I can show why that is. I omitted one black square on these images (by accident), but it plays no significant role so I decided not to redo the images:</p>
<p>After going for the promising squares at Manhattan distance 3 and 2, we cannot further expand that square with distance 2:</p>
<p><a href=""https://i.sstatic.net/FyDwiwaV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FyDwiwaV.png"" alt=""enter image description here"" /></a></p>
<p>The expandable square with the least distance has distance 3, and expands to the right. After that we have two squares at distance 4 which expand to their neighbors, which are all three at distance 5. Then these are expanded, which gives us this state:</p>
<p><a href=""https://i.sstatic.net/M6Mq3xgp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M6Mq3xgp.png"" alt=""enter image description here"" /></a></p>
<p>Here we have a square at distance 4, which gets priority over the other squares with distance 6. We thus expand that square to two neighbors which get distance 5. These two are now the ones with priority, and we expand those to three new squares:</p>
<p><a href=""https://i.sstatic.net/eAnSjMZv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eAnSjMZv.png"" alt=""enter image description here"" /></a></p>
<p>Again we get some squares with distance 6, but there is a new one with distance 4, so it gets priority, and is expanded to a square with distance 5. That latter square has now priority and expands to a square with distance 4, which takes the priority. At this point it becomes clear we will never expand any square with distance 6, since the distances of the new squares we will expand at the left side of the grid will all have less distance:</p>
<p><a href=""https://i.sstatic.net/F0Rj4fHV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F0Rj4fHV.png"" alt=""enter image description here"" /></a></p>
<p>The square with distance 4 at the left expands into a square with distance 3, getting the priority. It expands into two squares with distance 2, getting the priority:</p>
<p><a href=""https://i.sstatic.net/JtdlwQ2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JtdlwQ2C.png"" alt=""enter image description here"" /></a></p>
<p>These two squares will expand into two squares, including the left neighbor of &quot;g&quot;. It depends on the order in which these 2-squares will be expanded which path will be registered. I will assume here that the lower 2-square will be expanded first:</p>
<p><a href=""https://i.sstatic.net/8mZi4mTK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8mZi4mTK.png"" alt=""enter image description here"" /></a></p>
<p>The goal square has now been found and we can reconstruct the shortest path from the direction in which each involved square was expanded, following the arrows in opposite sense back to the source square. And so we get this path:</p>
<p><a href=""https://i.sstatic.net/3KeC6k2l.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3KeC6k2l.png"" alt=""enter image description here"" /></a></p>
<p>Again, the step from 3 to 2 could have gone the alternative way as well. It depends on information you have not given: the order in which squares are expanded when they have equal priority.</p>
<p>Also note that there was no further expansion of squares at the right and bottom sides of the grid, since their priority was never high enough.</p>
","2024-05-13 13:43:03","1","Answer"
"78471408","78469835","","<p>Your understanding is correct. You should follow these steps for setting up your neural network for binary classification:</p>
<p><em>Input Layer: 5 neurons (corresponding to your 5 inputs). First Hidden Layer: 32 neurons with ReLU activation. Second Hidden Layer: 32 neurons with ReLU activation. Output Layer: 1 neuron with Sigmoid activation (since this is a binary classification task).</em></p>
<p>The process of adjusting the weights of the neural network is generally handled through a method known as <a href=""https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd"" rel=""nofollow noreferrer"">backpropagation</a>, which is coupled with an optimization algorithm like Stochastic Gradient Descent (SGD), Adam, or others.</p>
<p>Below is an example of how you can implement this using TensorFlow:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Define the model
model = Sequential([
    Dense(32, input_shape=(5,), activation='relu'),
    Dense(32, activation='relu'),                    
    Dense(1, activation='sigmoid')             
])


model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Assuming X_train and y_train are your data matrices
# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
</code></pre>
","2024-05-13 09:50:52","1","Answer"
"78471329","78470683","","<p>Your labels have a shape of (16,), while your model's output has a shape of (None,3).</p>
<p>Probably the issue is that your labels are not <strong>one-hot encoded</strong>. They should have the same second dimension as your output layer:</p>
<pre><code>from tensorflow.keras.utils import to_categorical
num_classes = 3
labels = to_categorical(labels, num_classes=num_classes)
print(labels.shape)
</code></pre>
","2024-05-13 09:35:13","3","Answer"
"78471070","","Correct way to tag objects with YOLOv5","<p>I need to tag a series of images for sewing detection on fabric. I work with the YOLOv5 algorithm.</p>
<p>The problem I have is that it is not clear to me what should be the optimal way to label these sewings.</p>
<p>The image below shows a sewing in the fabric.</p>
<p><a href=""https://i.sstatic.net/6wJljjBM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6wJljjBM.jpg"" alt=""enter image description here"" /></a></p>
<p>As you can see in the image, the sewings are always going to occupy the whole width of the fabric. Initially I had thought of labelling several sections/portions of the sewing (the number of sewings detected is not really important, what really matters to me is that it detects that there is at least one sewing). The image below shows this idea:</p>
<p><a href=""https://i.sstatic.net/zOMsZTu5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zOMsZTu5.png"" alt=""enter image description here"" /></a></p>
<p>However it is not clear to me if this would be a correct method (optimal, rather) or it should be to create a single label that completely encloses the sewing.</p>
<p>On the other hand, according to the labelling tips given in the documentation, the label should exactly enclose the object to be detected, leaving as little space as possible between the object and the bounding box of the label.</p>
<blockquote>
<p>Label accuracy. Labels must closely enclose each object. No space
should exist between an object and it's bounding box. No objects
should be missing a label.</p>
</blockquote>
<p><a href=""https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/"" rel=""nofollow noreferrer"">Tips for Best Training Results</a></p>
<p>In this particular case, and given that the sewings will always appear in a very similar way (they will always have a horizontal orientation), these labels would be very thin (with very little height), so it is not clear to me that the algorithm will be able to detect them. In my humble opinion, I believe that slightly increasing the height of the labels would allow the algorithm to detect the sewings more efficiently, since the fabrics that are joined through these sewings may be of the same colour. (the second of the images shows the idea I am talking about).</p>
<p>I would be grateful if you could help me and tell me the best way to do this labelling.</p>
<p>Thank you very much in advance.</p>
","2024-05-13 08:48:35","0","Question"
"78470683","","ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 512), output.shape=(None, 3)","<p>I was trying to train a BERT model to solve a multi-classification problem.</p>
<p>I got this error while run the code below:</p>
<pre><code>Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 512), output.shape=(None, 3)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

epochs = 4

train_dataloader = train_dataset.shuffle(buffer_size=10000).batch(batch_size)
validation_dataloader = val_dataset.batch(batch_size)

# start training 
history = model.fit(
    train_dataloader,  # train_data
    validation_data=validation_dataloader,  # validation_data
    epochs=epochs,  
    verbose=1 
)
# save the model
model.save(&quot;bert_model.h5&quot;)
</code></pre>
<p>This is a test:</p>
<pre class=""lang-py prettyprint-override""><code>for batch in train_dataloader.take(1):
    input_ids, attention_masks, labels = batch
    print(&quot;Batch input_ids shape:&quot;, input_ids.shape) 
    print(&quot;Batch attention_masks shape:&quot;, attention_masks.shape) 
    print(&quot;Batch labels shape:&quot;, labels.shape)  

# I got this output
Batch input_ids shape: (16, 512)
Batch attention_masks shape: (16, 512)
Batch labels shape: (16,)
</code></pre>
<p>I already checked the tensor shape.</p>
","2024-05-13 07:32:02","2","Question"
"78469835","","How can i proceed further in this AI/ML project?","<p>I have 10 datasets (.csv) each with 100,000 rows, with each row containing 5 inputs ( -4.0f to +4.0f) and a output column (0/1). I want to train a Neural Network using this and predict the test dataset that I was given (which has 100,000 rows too, but without output column filled).</p>
<p>I thought of creating a 5--(reLU)--&gt; 32 --(reLU)--&gt; 32 --(sigmoid) --&gt; 1 Neural Network and train it with reward system like this [if (expec.op  ==0) reward=1- o/pfromNN; if (expec.op  ==1) reward= o/pfromNN].</p>
<p>How can I adjust weights of NN using this or How can I proceed further? I am a newbie to NN.</p>
<p>I thought of doing this like lunar lander module in gymnasium but since there are no states involved here I am confused</p>
","2024-05-13 02:25:15","-2","Question"
"78467885","78467829","","<p>The example of <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer"">sklearn.model_selection.train_test_split</a> states:</p>
<blockquote>
<p><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</code></p>
</blockquote>
<p>Since the code you provided is assigning the returning splittings in the wrong order, I am assuming you are providing the <code>.fit()</code> function of your model with the input test data instead of the desired output data of your train splitting. Try the following:</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=4)
</code></pre>
","2024-05-12 12:42:19","0","Answer"
"78467829","","ValueError: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 8 'y' sizes: 3","<p>whenever i try to run this code it shows this value error and i don't know why i check the lenght of the labels and images list and it's equal but x_train and y_train is different in length
note that i can't use tensorflow.keras for some reason it shows an error so i use only keras</p>
<pre><code>import numpy as np
import os
import keras
import tensorflow as tf
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
import cv2 as cv

people = ['H', 'J']
DIR = 'C:\AI'
images = []
labels = []
haar_cascade = cv.CascadeClassifier('haar_face.xml')

for person in people:
    path = os.path.join(DIR, person)
    label = people.index(person)
    for img in os.listdir(path):
        img_path = os.path.join(path, img)
        img_array = cv.imread(img_path)
        gray = cv.cvtColor(img_array, cv.COLOR_BGR2GRAY)
        face_rect = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=6)
        for (x, y, w, h) in face_rect:
            face_roi = img_array[y:y + h, x:x + w]
            face_roi = cv.resize(face_roi, (128, 128))
            images.append(face_roi)
            labels.append(label)



#images = np.array(images, dtype='float')/255.0
#labels = np.array(labels, dtype='float')/255.0

x_train, y_train, x_test, y_test = train_test_split(images, labels, test_size=0.2, random_state=4)

x_train = np.array(x_train, dtype='float')/255.0
y_train = np.array(y_train, dtype='float')/255.0
print(len(x_train), ' ', len(y_train))

model = keras.models.Sequential()
model.add(keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(128, 128, 3)))
model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))
model.add(keras.layers.BatchNormalization(axis=-1))
model.add(keras.layers.Dropout(0, 2))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(512, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
hist = model.fit(np.array(x_train), np.array(y_train), epochs=5, batch_size=64)
</code></pre>
","2024-05-12 12:19:55","0","Question"
"78466048","","Best First search to find the path from start to goal on a grid that has unassigned cost. Heuristic values","<p>I'm trying to answer this challenge:</p>
<blockquote>
<p>Consider the problem of finding a path in the grid shown in this figure from the position 𝑠 to the position 𝑔:</p>
<p><img src=""https://i.sstatic.net/iMGMYGj8.png"" alt=""Figure Ass-1"" /></p>
<p>A piece can move on the grid horizontally or vertically, one square at a time. No step may be made into a forbidden shaded area.</p>
<p>Using best-first search, nodes are expanding from 𝑠 to 𝑔. Manhattan distance should be used as the evaluation function. The Manhattan distance between two points is the distance in the 𝑥-direction plus the distance in the 𝑦-direction. It corresponds to the distance travelled along city streets arranged in a grid. Assume multiple-path pruning. What is the first path found?</p>
</blockquote>
<p>I tried the following approach, but I am not sure since there are no heuristic values given:</p>
<p><img src=""https://i.sstatic.net/MHvs44pB.png"" alt=""my attempt"" /></p>
<p>How can I determine what the first path found is?</p>
","2024-05-11 20:42:43","0","Question"
"78464664","","Matplotlib to pyqtgraph","<p>Does anyone have a converter that takes working plots created with matplotlib and produce a reasonable equivalent in pyqtgraph?  Seems like it should be possible.  It would save me a lot of time.</p>
<p>I'm new to pyqtgraph and find it's documentation less than helpful.  I'm amazed to see how long pyqtgraph has been available and yet the version number leads you to believe it's not ready to be released.  What am I missing? I also have tried using PyCharm's Ai Assistant and find it often struggles, creates non workable solutions, with pyqtgraph as if the documentation is not clear enough for it to work with AND it finds few helpful examples to study. Just guessing. I look for things I expect to find in pyqtgraph and find obtuse ways to create them like an arrow but not a simple way to create it.  Still looking for plots easily available in matploblib but not readily found in pyqtgraph such as boxplots, horizontal bar charts, confidence bands, clusters.  Examples like the short chapter in FitzGerald's book are less than helpful as they cover the most basic stuff, not even a histogram. I find recommendations like it's really fast not helpful - I'm concerned about speed of development/working with pyqtgraph infinitely more important than speed of code performance. Sounds more defensive than productive. I do want to use pyqtgraph as it fits well with the GUI I've built with Pyside6.  Just struggling to get my footing. Any helpful leads, recommendations would be appreciated.  I have yet to find any book on pyqtgraph.  What's up with that?</p>
","2024-05-11 12:47:27","-1","Question"
"78464435","78462606","","<p>If you only have the neighborhood relations and nothing else, then the only heuristic function available to you is h(X,Y) = 0. In this case, A* essentially becomes a Dijkstra search.</p>
<p>Note that in this setting your cost function also degrades to c(x,y) = 1, where x and y are neighbors. Consequently, you are better off running a breadth first search, because it doesn't need a priority queue like Dijkstra and A*, but can do away with a lifo queue, which is much faster.</p>
","2024-05-11 11:36:01","1","Answer"
"78464037","78451985","","<p>Based on keras <a href=""https://github.com/keras-team/keras/blob/v3.3.3/keras/src/trainers/data_adapters/data_adapter_utils.py#L117"" rel=""nofollow noreferrer"">implementation of this argument</a> :</p>
<pre><code>def class_weight_to_sample_weights(y, class_weight):
    sample_weight = np.ones(shape=(y.shape[0],), dtype=backend.floatx())
    if len(y.shape) &gt; 1:
        if y.shape[-1] != 1:
            y = np.argmax(y, axis=-1)
        else:
            y = np.squeeze(y, axis=-1)
    y = np.round(y).astype(&quot;int32&quot;)
    for i in range(y.shape[0]):
        sample_weight[i] = class_weight.get(int(y[i]), 1.0)
    return sample_weight
</code></pre>
<p>I think it is useful ONLY in case of <strong>classical classification</strong> problems. It expects <code>y</code> is going to be a list of classes (ohe-hot encoded or not) but not an image array (even it contains these classes).</p>
","2024-05-11 09:11:36","0","Answer"
"78462606","","Using a* algorithm without having distances","<p>I have a list of nodes and all of their neighboring nodes. I am trying to get from node X to node Y in the shortest path using A*.
What heuristic should I use?</p>
","2024-05-10 21:23:59","-1","Question"
"78462357","","How to use Spacy-PyTextRank in newer versions?","<p>I'm trying to do key phrase extraction with TextRank, I have installed 3.3.0 version, nltk and the en_core_web_trf are on 3.7.3, i dont know if this is the problem.
This is the error that I got</p>
<pre><code>import pytextrank
nlp=spacy.load('en_core_web_trf')
nlp.add_pipe(&quot;textrank&quot;)

  [E002] Can't find factory for 'textrank' for language English (en).
</code></pre>
<p>I'm using spacy for cuda on windows wsl ubuntu in a notebook</p>
<p>I tried to downgrade the versions, reinstalling all, calling directly the pipe(but says its deprecated) and using the efficiency core</p>
","2024-05-10 20:11:52","1","Question"
"78460752","78437376","","<p>You need to launch Ollama before you can pull or run models, in the case of a local attempt, you would simply run (after installing Ollama):</p>
<pre><code>ollama serve
</code></pre>
<p>And you would subsequently be able to run <code>llama3</code>:</p>
<pre><code>ollama run llama3
</code></pre>
<p>I believe the latter command will automatically pull the model <code>llama3:8b</code> for you and so running <code>ollama pull llama3</code> should not be mandatory.</p>
<p>However, my above suggestion is not going to work in Google Colab as the command <code>!ollama serve</code> is going to use the main thread and block the execution of your following commands and code.</p>
<p>Most tutorials using Google Colab seem to suggest to run ollama as an async process as in <a href=""https://stackoverflow.com/a/77828874/9634548"">this accepted answer</a>. I would suggest you use the code from the linked answer that runs Ollama in a separated thread based on the python modules <code>threading</code> and <code>asyncio</code>. As I just wanted to run a quick test code, I managed to make it run in Google Colab with a simple <code>subprocess</code> command:</p>
<pre><code>import subprocess
process = subprocess.Popen(&quot;ollama serve&quot;, shell=True)
!ollama run llama3
</code></pre>
","2024-05-10 14:23:13","9","Answer"
"78459632","","Ask LLM to pick random data from prompted dataset","<p>I try to generate suggestions of books. The problem is my LLM (here claude 3 api) give always the same well known books from one instance to an other...</p>
<p>So I have tried to give a [BOOKS DATASET] tag with a list of hundreds of books and I prompted the llm to take random books, but it keeps giving same well know books from the list again and again...</p>
<p>Any possibility to realy have random books ?</p>
","2024-05-10 10:52:18","0","Question"
"78458802","78457321","","<p>I solved the problem by saving the model as (.h5) file instead of (.hdf5) and make sure to call the absolute path</p>
","2024-05-10 08:07:45","0","Answer"
"78458236","78457370","","<p>First, this sound just like a classification problem, so in another word: yes, it does make sense.</p>
<p>Second, yes. Pytorch provides you everythings numpy offers (well, almost). For generating random uniform data, you can use the following:</p>
<pre><code>x = torch.rand(1000, 2).sub(.5).mul(200) # &lt;- torch.rand() return range [0,1]
y = (x.sum(1, keepdim = True) &gt; 10).float().sub(.5).mul(20) # &lt;- (x.sum(1) &gt; 10).float() return {0,1}
</code></pre>
","2024-05-10 05:48:04","0","Answer"
"78457370","","How to create a tensor based on another one - Studying PyTorch in practice?","<p>I'm studying IA using PyTorch and implementing some toy examples.
First, I created a one-dimensional tensor (X) and a second tensor (y), derived from the first one:</p>
<pre class=""lang-py prettyprint-override""><code>X = torch.arange(0, 100, 1.0).unsqueeze(dim=1)
y = X * 2
</code></pre>
<p>So I have something like</p>
<pre><code>X = tensor([[0.], [1.], [2.], [3.], [4.], [5.], ...
y = tensor([[ 0.], [ 2.], [ 4.], [ 6.], [ 8.], [10.], ...
</code></pre>
<p>Then, I trained a model to predict y and it was working fine.</p>
<p>Now, I would like something different. The X will be 2D and y 1D. y is calculated by an operation in the elements of X:
<code>If x[0] + x[1] &gt;0? y = 10: y -10 </code></p>
<pre><code>X = tensor([[ 55.5348, -97.7608],
            [ 29.0493, -52.1908],
            [ 47.1722, -43.1151],
            [ 11.1242, -62.8652],
            [ 44.8067,  80.8335],...
y = tensor([[-10.], [-10.], [ 10.], [-10.], [ 10.],...
</code></pre>
<p>First question, Is it make sense in terms of Machine Learning?</p>
<p>Second one...
I'm generating the tensors using numpy. Could I do it in a smarter way?</p>
<pre class=""lang-py prettyprint-override""><code># Criar X valores de entrada para testes
X_numpy = np.random.uniform(low=-100, high=100, size=(1000,2))
print(&quot;X&quot;, X_numpy)

#y_numpy = np.array([[ (n[0]+n[1]) &gt;= 0 ? 10:-10] for n in X_numpy])
y_numpy = np.empty(shape=[0, 1])
for n in X_numpy:
    if n[0] + n[1] &gt;= 0:
        y_numpy = np.append(y_numpy, [[10.]], axis=0)
    elif n[0] + n[1] &lt; 0:
        y_numpy = np.append(y_numpy, [[-10.]], axis=0)
</code></pre>
","2024-05-09 23:28:50","-2","Question"
"78457321","","Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). How to fix this bug?","<p>ValueError: File format not supported: filepath=C:\Users\fedmor\Desktop\AI\resnet50_coco_best_v2.0.1 (1). Keras 3 only supports V3 <code>.keras</code> files and legacy H5 format files (<code>.h5</code> extension). Note that the legacy SavedModel format is not supported by <code>load_model()</code> in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use <code>keras.layers.TFSMLayer(C:\Users\fedmor\Desktop\AI\resnet50_coco_best_v2.0.1 (1), call_endpoint='serving_default')</code> (note that your <code>call_endpoint</code> might have a different name).</p>
<p>This is the problem that I`ve got. Idk what to do with this, do you guys have anu ideas?</p>
<p>I did<code>t try to do anything. At this point I</code>m just lost</p>
","2024-05-09 23:05:25","1","Question"
"78457099","78455702","","<p>The main parameter controlling the upscaling of the input is <code>stride=</code>. Setting <code>stride=2</code> with <code>kernel_size=2</code> will exactly double the input size.</p>
<p>In your case, use <code>stride=2</code> with <code>kernel_size=3</code> to get a <em>doubling + 1</em> size transformation with each upconv layer. The first layer will produce an output sized <code>2 x 63 + 1 = 127</code>, and the second will yield <code>2 x 127 + 1 = 255</code>.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>x = torch.rand(1, 128, 63, 63) #the ouput from Conv2d-7 is shaped (63, 63)

x = nn.ConvTranspose2d(128, 32, kernel_size=3, stride=2)(x) #2h + 1 upconv
print(x.shape)
#out&gt; torch.Size([1, 32, 127, 127])

x = nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2)(x) #2h + 1 upconv
print(x.shape)
#out&gt; torch.Size([1, 3, 255, 255])
</code></pre>
","2024-05-09 21:43:12","2","Answer"
"78455702","","PyTorch convolutional autoencoder, output dimensions different from input","<p>I am new with working with PyTorch and wanted to make a simple autoencoder with 255x255 RGB images to play around with it, however the output shape isn't the same as the input shape.</p>
<p>Here's the model</p>
<pre class=""lang-py prettyprint-override""><code>class AutoEncoder(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )

        self.decoder = nn.Sequential(
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(in_channels=128, out_channels=32, kernel_size=3, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=3, output_padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x
</code></pre>
<p>And here are the shapes given by the torchsummary package</p>
<pre><code>----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 255, 255]             896
              ReLU-2         [-1, 32, 255, 255]               0
         MaxPool2d-3         [-1, 32, 127, 127]               0
            Conv2d-4        [-1, 128, 127, 127]          36,992
              ReLU-5        [-1, 128, 127, 127]               0
         MaxPool2d-6          [-1, 128, 63, 63]               0
            Conv2d-7          [-1, 128, 63, 63]         147,584
              ReLU-8          [-1, 128, 63, 63]               0
   ConvTranspose2d-9           [-1, 32, 66, 66]          36,896
             ReLU-10           [-1, 32, 66, 66]               0
  ConvTranspose2d-11            [-1, 3, 69, 69]             867
          Sigmoid-12            [-1, 3, 69, 69]               0
</code></pre>
<p>I have seen from another post that the <code>output_padding</code> option in the decoder part would help with the output shape but it hasn't worked for me.</p>
<p>I don't know what the problem might be, coming from Tensorflow I would've used an Upscale layer but from what I've seen this isn't the way to do it in PyTorch.</p>
<p>Could anyone explain to me why my shapes are broken with my current model? Thanks</p>
","2024-05-09 16:08:36","1","Question"
"78452864","","Gemini AI error - ""AttributeError: module 'google.generativeai' has no attribute 'GenerativeModel'""","<p>I am trying to create a script in <code>Jupyter</code> for testing <a href=""https://ai.google.dev/api/python/google/generativeai"" rel=""noreferrer"">Google Gemini AI model</a>.</p>
<pre class=""lang-py prettyprint-override""><code>import google.generativeai as genai
import os

# genai.configure(api_key=&quot;API Key&quot;)

model = genai.GenerativeModel('gemini-pro')
response = model.generate_content('Please summarise this document: ...')

print(response.text)
</code></pre>
<p>I'm getting this error:</p>
<pre><code>AttributeError
Traceback (most recent call last)
Input In [2], in &lt;cell line: 6&gt;()
      2 import os
      4 # genai.configure(api_key=os.environ['API_KEY'])
----&gt; 6 model = genai.GenerativeModel('gemini-pro')
      7 response = model.generate_content('Please summarise this document: ...')
      9 print(response.text)

AttributeError: module 'google.generativeai' has no attribute 'GenerativeModel'
</code></pre>
<p>I tried:</p>
<ul>
<li>Re-installing package - <code>python3.11 -m pip install google-generativeai</code></li>
<li>Checked whether the class named <code>GenerativeModel</code> existed in my installed package source code but it didn't help.</li>
</ul>
<p>Python and package versions:</p>
<ul>
<li>python: 3.11.7</li>
<li>google-generativeai=0.5.1</li>
</ul>
","2024-05-09 07:12:09","4","Question"
"78451985","","model. fit() doesn't work when I use class_weight with image segmentation","<p>When creating the dictionary with the weight values, model.fit() does not train, it shows the following error</p>
<p>I used the following code</p>
<pre><code>import pathlib
import tensorflow as tf
import imageio
import numpy as np
import keras
import glob
import random
import os
os.environ[&quot;SM_FRAMEWORK&quot;] = &quot;tf.keras&quot;
import segmentation_models as sm
#import cv2
from tqdm import tqdm
from matplotlib import pyplot as plt
from PIL import Image
from model_segmentation import simple_unet_model
from keras.metrics import MeanIoU
from sklearn.preprocessing import LabelEncoder


img = img[...,np.newaxis]
print(img.shape)
print(msk.shape)
#msk = msk[...,np.newaxis]
kernel_initializer =  'he_uniform' #Try others if you want

steps_per_epoch = len(train_img_list)//batch_size
#val_steps_per_epoch = len(val_img_list)//batch_size

model = simple_unet_model(IMG_HEIGHT=128, IMG_WIDTH=128, IMG_CHANNELS=1, num_classes=4)

total_loss = 'binary_crossentropy'
#metrics = ['accuracy', sm.metrics.IOUScore(threshold=0.5)]
metrics = ['accuracy']
optim = 'adam'

model.compile(optimizer = optim, loss=total_loss, metrics= metrics)
print(model.summary())

print(model.input_shape)
print(model.output_shape)

msk = msk.astype(np.uint8)
print(msk.shape)
msk_argmax=np.argmax(msk, axis=3)
masks_reshaped = msk_argmax.reshape(-1,1)
print(masks_reshaped.shape)
masks_reshaped_encoded = masks_reshaped.reshape(-1)
print(masks_reshaped_encoded.shape)

labelencoder = LabelEncoder()
masks_reshaped_encoded = labelencoder.fit_transform(masks_reshaped)
print(masks_reshaped_encoded.shape)

np.unique(masks_reshaped_encoded)

class_weights_manual= {0:1, 1:2, 2:4, 3:8}

history=model.fit(img, msk, epochs=50, verbose=1, validation_split=0.2, shuffle=False,
                  class_weight=class_weights_manual)
</code></pre>
<pre><code>
  File ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122 in error_handler
    raise e.with_traceback(filtered_tb) from None

  File ~\anaconda3\Lib\site-packages\keras\src\trainers\data_adapters\data_adapter_utils.py:126 in class_weight_to_sample_weights
    sample_weight[i] = class_weight.get(int(y[i]), 1.0)

TypeError: only size-1 arrays can be converted to Python scalars
</code></pre>
<p>Does the training data have to have a specific format?
or what do I need to change in the code?
I attach screenshots, thanks for your help</p>
","2024-05-09 02:41:14","0","Question"
"78446786","78442780","","<p>If anyone else has this issue, I managed to fix it.</p>
<p>Please see below for my working code.</p>
<pre><code>def process_row(batch):
    # Initialize lists to store the processed rows
    processed_texts = []
    processed_metadata = {k: [] for k, v in batch.items() if k != 'text'}

    # Process each row in the batch
    for i in range(len(batch['text'])):
        # Apply the chunk_text function to the text
        text = batch['text'][i]
        metadata = {k: v[i] for k, v in batch.items() if k != 'text'}
        chunks = chunk_text([text], metadata)['text_chunks']

        # For each chunk, add it to the processed_texts and replicate the metadata
        for chunk in chunks:
            processed_texts.append(chunk)
            for k in processed_metadata:
                processed_metadata[k].append(metadata[k])

    # Combine processed texts and metadata into a single dictionary
    results = {'text': processed_texts}
    for k, v in processed_metadata.items():
        results[k] = v

    return results
</code></pre>
","2024-05-08 07:33:45","0","Answer"
"78446049","78444988","","<p>The observation says it all. Use <code>python_repl_ast</code> instead of <code>Python REPL</code>. Following is the updated code:</p>
<pre><code>from langchain.agents import initialize_agent
from langchain.llms.fake import FakeListLLM
from langchain.agents import AgentType
from langchain_experimental.tools import PythonAstREPLTool

res = [&quot;Action: python_repl_ast\nAction Input: print(2.2 + 2.22)&quot;, &quot;Final Answer: 4.42&quot;]
llm = FakeListLLM(responses=res)
agent = initialize_agent(tools=[PythonAstREPLTool()], llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
agent.run(&quot;what is 2.2 + 2.22?&quot;)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/pe56U4fg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pe56U4fg.png"" alt=""enter image description here"" /></a></p>
","2024-05-08 04:08:03","1","Answer"
"78446042","78446023","","<p>I find following sh work</p>
<pre><code>!ollama serve &gt; server.log 2&gt;&amp;1 &amp;
!ollama run llama3
</code></pre>
","2024-05-08 04:05:43","0","Answer"
"78446023","","In colab, `ollama serve &` froze running","<p>I run following sh in colab</p>
<pre><code>!ollama serve &amp;
!ollama run llama3
</code></pre>
<p>it out</p>
<pre><code>2024/05/08 03:51:17 routes.go:989: INFO server config env=&quot;map[OLLAMA_DEBUG:false OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&quot;
time=2024-05-08T03:51:17.536Z level=INFO source=images.go:897 msg=&quot;total blobs: 0&quot;
time=2024-05-08T03:51:17.536Z level=INFO source=images.go:904 msg=&quot;total unused blobs removed: 0&quot;
time=2024-05-08T03:51:17.538Z level=INFO source=routes.go:1034 msg=&quot;Listening on 127.0.0.1:11434 (version 0.1.34)&quot;
time=2024-05-08T03:51:17.627Z level=INFO source=payload.go:30 msg=&quot;extracting embedded files&quot; dir=/tmp/ollama2853183168/runners
time=2024-05-08T03:51:28.627Z level=INFO source=payload.go:44 msg=&quot;Dynamic LLM libraries [rocm_v60002 cpu cpu_avx cpu_avx2 cuda_v11]&quot;
time=2024-05-08T03:51:28.629Z level=INFO source=gpu.go:122 msg=&quot;Detecting GPUs&quot;
time=2024-05-08T03:51:28.662Z level=INFO source=cpu_common.go:11 msg=&quot;CPU has AVX2&quot;
</code></pre>
<p>it is frozen in <code>!ollama serve &amp;</code>, I think &amp; has make running server in backend, why is it frozen?</p>
","2024-05-08 03:55:01","1","Question"
"78444988","","Observation: Python REPL is not a valid tool, try one of [python_repl_ast]. in Langchain","<pre><code>from langchain.agents import initialize_agent
from langchain.llms.fake import FakeListLLM
from langchain.agents import AgentType
from langchain_experimental.tools import PythonAstREPLTool
res = [&quot;Action: Python REPL\nAction Input: print(2.2 + 2.22)&quot;, &quot;Final Answer: 4.42&quot;]
llm = FakeListLLM(responses=res)
agent = initialize_agent(tools=[PythonAstREPLTool()],
llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
agent.run(&quot;what is 2.2 + 2.22?&quot;)
</code></pre>
<p>What's wrong in this code? I am getting the following error:</p>
<p><a href=""https://i.sstatic.net/zQWFMW5n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zQWFMW5n.png"" alt=""REPL is not a valid tool"" /></a></p>
<p>Can someone explain me the better way to implement a Fake LLM using <code>Langchain</code>?</p>
","2024-05-07 20:47:12","1","Question"
"78443356","78437376","","<p>first <em><strong>you have to pull</strong></em> the model(llama3. etc) then run this command:
<strong>ollama run llama3</strong></p>
","2024-05-07 14:49:32","0","Answer"
"78442780","","Getting a pyarrow.lib.ArrowInvalid: Column 1 named type expected length 44 but got length 21 when trying to create Hugging Face database","<p>I am getting the below error when trying to modify, chunk and resave a Huggingface Dataset.</p>
<p>I was wondering if anyone might be able to help?</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\conno\LegalAIDataset\LegalAIDataset\main.py&quot;, line 39, in &lt;module&gt;
    new_dataset = dataset.map(process_row, batched=True, batch_size=1, remove_columns=None)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\conno\LegalAIDataset\LegalAIDataset\.venv\Lib\site-packages\datasets\arrow_dataset.py&quot;, line 602, in wrapper
    out: Union[&quot;Dataset&quot;, &quot;DatasetDict&quot;] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\conno\LegalAIDataset\LegalAIDataset\.venv\Lib\site-packages\datasets\arrow_dataset.py&quot;, line 567, in wrapper
    out: Union[&quot;Dataset&quot;, &quot;DatasetDict&quot;] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\conno\LegalAIDataset\LegalAIDataset\.venv\Lib\site-packages\datasets\arrow_dataset.py&quot;, line 3156, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File &quot;C:\Users\conno\LegalAIDataset\LegalAIDataset\.venv\Lib\site-packages\datasets\arrow_dataset.py&quot;, line 3570, in _map_single
    writer.write_batch(batch)
  File &quot;C:\Users\conno\LegalAIDataset\LegalAIDataset\.venv\Lib\site-packages\datasets\arrow_writer.py&quot;, line 571, in write_batch
    pa_table = pa.Table.from_arrays(arrays, schema=schema)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;pyarrow\\table.pxi&quot;, line 4642, in pyarrow.lib.Table.from_arrays
  File &quot;pyarrow\\table.pxi&quot;, line 3922, in pyarrow.lib.Table.validate
  File &quot;pyarrow\\error.pxi&quot;, line 91, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Column 1 named type expected length 44 but got length 21
</code></pre>
<p>My minimun reproducable code is below:</p>
<pre><code>import datasets
from datasets import load_dataset, Dataset
from semantic_text_splitter import TextSplitter


# Step 1: Load the existing dataset
dataset = load_dataset('HF_Dataset')
# Slice the 'train' split of the dataset
sliced_data = dataset['train'][:100]

# Convert the sliced data back into a Dataset object
dataset = Dataset.from_dict(sliced_data)


def chunk_text(text_list, metadata):
    splitter = TextSplitter(1000)
    chunks = [chunk for text in text_list for chunk in splitter.chunks(text)]
    return {&quot;text_chunks&quot;: chunks, **metadata}
# Define a global executor
#executor = ThreadPoolExecutor(max_workers=1)

def process_row(batch):
    # Initialize a dictionary to store the results
    results = {k: [] for k in batch.keys()}
    results['text_chunks'] = []  # Add 'text_chunks' key to the results dictionary

    # Process each row in the batch
    for i in range(len(batch['text'])):
        # Apply the chunk_text function to the text
        chunks = chunk_text(batch['text'][i], {k: v[i] for k, v in batch.items() if k != 'text'})
        # Add the results to the dictionary
        for k, v in chunks.items():
            results[k].extend(v)

    # Return the results
    return results

# Apply the function to the dataset
new_dataset = dataset.map(process_row, batched=True, batch_size=1, remove_columns=None)

# Save and upload the new dataset
new_dataset.to_json('dataset.jsonl')
dataset_dict = datasets.DatasetDict({&quot;split&quot;: new_dataset})


# dataset_dict.save_to_disk(&quot;&quot;, format=&quot;json&quot;)
# dataset_dict.upload_to_hub(&quot;&quot;, &quot;This is a test dataset&quot;)

</code></pre>
<p>I was expecting the code to chunk the dataset, keep the metadata and save it as a .jsonl file.</p>
<p>Instead, I got the above error.</p>
","2024-05-07 13:14:28","1","Question"
"78441934","78420289","","<p>The error refers to the shape mismatch while calculating the cross-entropy loss.</p>
<p>The last layer defined by you have 5 output units, while looking at the error, it feels like you have 3 distinct classes. Verify using <code>len(set(Y))</code></p>
<pre class=""lang-bash prettyprint-override""><code>classifier.add(Dense(units = 5, activation = 'softmax'))
</code></pre>
<p>Change the last softmax layer to</p>
<pre class=""lang-bash prettyprint-override""><code>classifier.add(Dense(units = 3, activation = 'softmax'))
</code></pre>
<p>or maybe a little dynamic</p>
<pre class=""lang-bash prettyprint-override""><code>classifier.add(Dense(units = len(set(Y)), activation = 'softmax'))
</code></pre>
","2024-05-07 10:49:46","0","Answer"
"78441397","78435504","","<p>Your <code>fc_input_size</code> is wrong. First, you're putting in <code>data.shape[0]</code>, which is the number of data, and not the size of each data item. Second, you are forgetting about the 2 MaxPool operations you are doing, each of which reduce the spatial dimensions by 2x.</p>
<p>Calculate it like following:</p>
<pre><code>fc_input_size = (data.shape[1] // 4) * (data.shape[2] // 4)
</code></pre>
","2024-05-07 09:16:49","0","Answer"
"78437686","","How to receive position that lead to the highest evaluated branch, when using minmax","<p>Right now I am programming a simple game (Capture the Flag) in Java. For creating my AiBots I decided to use minmax with alpha beta pruning. What is the best way to access the position that lead to the branch with the best evaluation determined by running my function. That position would represent the move the Aibot chooses to do ( which is the branch that leads to my functions return when visualizing in a tree) Based ont his strucutre I try to achieve it:</p>
<pre><code>public int minmax(String[][] position,int depth, int alpha, int beta, boolean maximizingPlayer){
        //TODO recursion ends with game over
        if(depth == 0){
            return AssessGame.assess(position, maximizingPlayer);
        }
        if(maximizingPlayer){
            int maxEval = Integer.MIN_VALUE;
            for(String[][] child: generateChildren(position)){
                int eval = minmax(child, depth -1, alpha,beta,false);
                maxEval = Integer.max(maxEval,eval);
                alpha = Integer.max(alpha,eval);
                if( beta &lt;= alpha){
                    break;
                }
            }
            return maxEval;
        }
        else{
            int minEval = Integer.MAX_VALUE;
            for(String[][] child: generateChildren(position)){
                int eval = minmax(child, depth -1, alpha,beta,true);
                minEval = Integer.min(minEval,eval);
                beta = Integer.min(beta,eval);
                if( beta &lt;= alpha){
                    break;
                }
            }
            return minEval;
        }
    }

</code></pre>
<p>I tried creating a Evaluation Class which also saves the childs position but I cant seem to figure out where to return what to get the right position.</p>
<pre><code>public class BestMove {
    public int value;
    public String[][] position;

    public BestMove(int value, String[][] position) {
        this.value = value;
        this.position = position;
    }
}

public BestMove minmax(String[][] position, int depth, int alpha, int beta, boolean maximizingPlayer) {
    // TODO recursion ends with game over
    if (depth == 0) {
        return new BestMove(AssessGame.assess(position, maximizingPlayer), position);
    }

    if (maximizingPlayer) {
        int maxEval = Integer.MIN_VALUE;
        String[][] bestPosition = null;
        for (String[][] child : generateChildren(position)) {
            BestMove move = minmax(child, depth - 1, alpha, beta, false);
            int eval = move.value;
            if (eval &gt; maxEval) {
                maxEval = eval;
                bestPosition = child;
            }
            alpha = Integer.max(alpha, eval);
            if (beta &lt;= alpha) {
                break;
            }
        }
        return new BestMove(maxEval, bestPosition);
    } else {
        int minEval = Integer.MAX_VALUE;
        String[][] bestPosition = null;
        for (String[][] child : generateChildren(position)) {
            BestMove move = minmax(child, depth - 1, alpha, beta, true);
            int eval = move.value;
            if (eval &lt; minEval) {
                minEval = eval;
                bestPosition = child;
            }
            beta = Integer.min(beta, eval);
            if (beta &lt;= alpha) {
                break;
            }
        }
        return new BestMove(minEval, bestPosition);
    }
}
</code></pre>
","2024-05-06 15:37:33","1","Question"
"78437376","","run `!ollama run llama3` in colab raise err ""Error: could not connect to ollama app, is it running?""","<p>I use following code</p>
<pre><code>!apt install pciutils -y
!curl -fsSL https://ollama.com/install.sh | sh
!ollama run llama3
</code></pre>
<p>in <code>!ollama run llama3</code> code cell, it raise err &quot;Error: could not connect to ollama app, is it running?&quot;</p>
","2024-05-06 14:41:12","5","Question"
"78435504","","Problem in defining a ML model for my Npy dataset","<p>I need help in defining a torch model for my data. I have tried various methods but nothing seems to be working out. Error after error related to input size and shaping. How can I resolve these issues?</p>
<pre><code>import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as f

# Load data from .npy file
data = np.load(&quot;other py files/project_files/data/train/data.npy&quot;)
print(&quot;Data Shape: &quot;, data.shape)  # (401, 701, 255)

data_size = data.shape[0] * data.shape[1] * data.shape[2]
print(&quot;Data Size:&quot;, data_size)  # 71680755

# Load labeling data from .npy file
labels = np.load(&quot;other py files/project_files/data/train/label.npy&quot;)
print(&quot;Label Data Shape: &quot;, labels.shape)  # (401, 701, 255)

# Convert numpy arrays to PyTorch tensors
data_tensor = torch.Tensor(data)
labels_tensor = torch.Tensor(labels)


class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc_input_size = data_size
        self.fc = nn.Linear(self.fc_input_size, 2)

    def forward(self, x):
        x = self.pool(f.relu(self.conv1(x)))
        x = self.pool(f.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_size)
        x = self.fc(x)
        return x

model = MyModel()
print(model)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

dataset = TensorDataset(data_tensor, labels_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

num_epochs = 10
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in enumerate(dataloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs.unsqueeze(1))  # channel dimension
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:
            print(f&quot;[{epoch + 1}, {i + 1}] loss: {running_loss / 100}&quot;)
            running_loss = 0.0


with torch.no_grad():
    predictions = model(data_tensor.unsqueeze(1))  # channel dimension
</code></pre>
<p>Console Output:</p>
<pre><code>Connected to pydev debugger (build 223.8836.43)
Data Shape:  (401, 701, 255)
Data Size: 71680755
Label Data Shape:  (401, 701, 255)
MyModel(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (fc): Linear(in_features=71680755, out_features=2, bias=True)
)

File &quot;C:\Users\PC1\PycharmProjects\Project1\newmodel2.py&quot;, line 36, in forward
    x = x.view(-1, self.fc_input_size)
RuntimeError: shape '[-1, 71680755]' is invalid for input of size 22579200
python-BaseException
</code></pre>
","2024-05-06 08:45:53","0","Question"
"78434544","78434421","","<p>According to <a href=""https://stackoverflow.com/questions/40207422/binary-numbers-instead-of-one-hot-vectors"">this</a> answer, with the bit-based encoding, now the 2nd city and the 4th city share the feature x0, also the 3rd city and the 4th city share the feature x1, and it affects our predictions.</p>
","2024-05-06 04:29:10","2","Answer"
"78434421","","Regarding onehotencoder space cost","<p>Why doesn't one-hot encoding use bit-based encoding? Wouldn't it take much less memory? What I mean is when you encode for example four cities you can do it like what one-hot encoder does by expanding one column to 4 or like this in one column:</p>
<pre><code>    1st city = 0(base10) = 00000000
    2nd city = 1(base10) = 00000001
    3rd city = 2(base10) = 00000010
    4th city = 3(base10) = 00000011    
</code></pre>
<p>Isn't this just more efficient in terms of memory cost? Or does the encoding technique force it to take more space?</p>
","2024-05-06 03:24:38","1","Question"
"78430164","78430061","","<p>You need to install git for windows which is different from GitHub (a website that stores git repositories).</p>
<p>You can install git for windows here: <a href=""https://git-scm.com/download/win"" rel=""nofollow noreferrer"">https://git-scm.com/download/win</a></p>
<p>Alternatively, why don't you try doing <code>pip install -U openai-whisper</code> instead? Like so: <a href=""https://github.com/openai/whisper?tab=readme-ov-file#setup"" rel=""nofollow noreferrer"">https://github.com/openai/whisper?tab=readme-ov-file#setup</a></p>
","2024-05-04 19:29:23","1","Answer"
"78430061","","im trying to install pip install git+https://github.com/openai/whisper.git into the command prompt but it does not work from whisper AI and im stuck","<p>in whisper ai i am trying to put this code into command prompt pip install git+https://github.com/openai/whisper.git and it does not work it just tells me this  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version
ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH? but the first thing install from whisper ai did work but i do not know what to.i have github installed and i have looked videos trying to solve this but it do not show how to solve. if yall could tell me a step by step solution i would appericate it because i do not have no experience to coding.</p>
<p>okay my bad so i do have git installed from windows not github and i did install pip install openai-whisper from whisper Ai into command prompt, the thing is just that it when i put this command pip install git+https://github.com/openai/whisper.git it says that there is an ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH? and im confused on what to do, am i supposed to have a file to called grit in pycharm so it can be directed there?
because this how your suppose to do right by steps to install it?
1.pip install -U openai-whisper
2.pip install git+https://github.com/openai/whisper.git
3. pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git</p>
","2024-05-04 18:48:32","-3","Question"
"78426960","78419787","","<p>The series instance UID tag that is mentioned above (0020,000D) is <em>generally</em> the tag that can be used (is generally used) to differentiate images in a single <em>study</em> (ie have the same study UID tag) into separate <em>series</em>.</p>
<p>However, in particular for MR, this may sometimes not be sufficient.</p>
<p>In many MR studies, the images in a single series (ie have the same series UID) will contain images with different scan parameters.
Notably for MR, this often includes images with a different echo time (0018,0081) - in clinical terminology a single series will contain both T1 and T2 images.</p>
<p>From a clinical use (machine or user) perspective, in this case it is further necessary to split the images with the same series UID into subsets that have the same echo time.  Or other acquisition parameters that may be relevant.</p>
<p>Most PACS systems do this type of &quot;series splitting&quot; whenever they display an MR study, and have a list of tags that are used to split series (as defined by having the same series UID) into effectively different series from a clinical point of view.</p>
","2024-05-03 21:06:14","3","Answer"
"78420674","78420048","","<p>the <code>model</code> variable is not defined anywhere in the code and is certainly not in the <code>predict()</code> function where it is called.</p>
<p>in fact, there is a comment:</p>
<pre><code># Assume `model` is already defined and trained
</code></pre>
<p>presumably it is in the global scope (poor practice) but omitted in the question.</p>
<p>so it looks like this should either:</p>
<ol>
<li>be created inside the <code>predict()</code> function.
or</li>
<li>passed as an argument into the same function.</li>
</ol>
<p>As a side note, the code is also missing <code>cv</code> which one might assume comes from the <code>opencv</code> module. In any regard, the code will not work without this being defined too...</p>
","2024-05-02 17:01:03","0","Answer"
"78420289","","Getting ValueError while trying to train GAN model","<p>I am trying to train a GAN model to detect Diabetic retinopathic images but it's throwing error. Kindly help.
The images dataset is not empty I have tried looking into it
Error is:-</p>
<pre><code>Epoch 1/50
Traceback (most recent call last):
  File &quot;C:\Users\asus\OneDrive\Desktop\project\DR-GAN\TrainModel.py&quot;, line 65, in &lt;module&gt;
    classifier.fit(X, Y, batch_size=32, epochs=50)
  File &quot;C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\utils\traceback_utils.py&quot;, line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\backend\tensorflow\nn.py&quot;, line 553, in categorical_crossentropy
    raise ValueError(
ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 3), output.shape=(None, 5)
</code></pre>
<p>The code for the train model file is:</p>
<pre><code>import numpy as np
import imutils
import sys
import cv2
import os
from tensorflow.keras.utils import to_categorical
from keras.models import model_from_json
from keras.layers import MaxPooling2D
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D
from keras.models import Sequential 

images = []
image_labels  = []
directory = 'dataset'
list_of_files = os.listdir(directory)
index = 0
for file in list_of_files:
    subfiles = os.listdir(directory+'/'+file)
    for sub in subfiles:
        path = directory+'/'+file+'/'+sub
        img = cv2.imread(path)
        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        if img is None:
          print('Wrong path:', path)
        else:
         img = cv2.resize(img, (32,32))
         im2arr = np.array(img)
         im2arr = im2arr.reshape(32,32,3)
         images.append(im2arr)
         image_labels.append(file)
    print(file)    

X = np.asarray(images)
Y = np.asarray(image_labels)
Y = to_categorical(Y)
img = X[20].reshape(32,32,3)
cv2.imshow('ff',cv2.resize(img,(250,250)))
cv2.waitKey(0)
print(&quot;shape == &quot;+str(X.shape))
print(&quot;shape == &quot;+str(Y.shape))
print(Y)
X = X.astype('float32')
X = X/255

np.save(&quot;model/img_data.txt&quot;,X)
np.save(&quot;model/img_label.txt&quot;,Y)

X = np.load('model/img_data.txt.npy')
Y = np.load('model/img_label.txt.npy')
print(Y)
img = X[20].reshape(32,32,3)
cv2.imshow('ff',cv2.resize(img,(250,250)))
cv2.waitKey(0)

classifier = Sequential() #alexnet transfer learning code here
classifier.add(Convolution2D(32, 3, 3, input_shape = (32, 32, 3), activation = 'relu'))
classifier.add(MaxPooling2D((2, 2) , padding='same'))
classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))
classifier.add(MaxPooling2D((2, 2) , padding='same'))
classifier.add(Flatten())
classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 5, activation = 'softmax'))
classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
classifier.fit(X, Y, batch_size=32, epochs=50)
</code></pre>
<p>I have tried changing dimensions but it isn't working I am not able to understand is it a version error or code error so to address the same kindly give the solutions.</p>
","2024-05-02 15:39:54","0","Question"
"78420048","","prediction = model.predict(email_features_array)","<pre><code>  prediction = model.predict(email_features_array)
                 ^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'predict'

</code></pre>
<p>i have already built and trained ML Model i only wanna use it through flask, but whenever it's about enter the mode.predict it gives me this issue</p>
<p>this is my code:</p>
<p>these are my imports</p>
<pre><code>import pickle
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np  # Import NumPy
</code></pre>
<h1>Load the pre-trained model</h1>
<pre><code>
model = pickle.load(open('c:/Users/7rbe2/OneDrive/سطح المكتب/Main projects/Grad project/Phishward/PhishWarden/app/Python/logistic_regression_model.pkl', 'rb'))

# Initialize CountVectorizer
cv = CountVectorizer()
</code></pre>
<p>here are functions for cleaning</p>
<pre><code>def remove_special_characters(word):
    return word.translate(str.maketrans('', '', string.punctuation))

def remove_stop_words(words):
    stop_words = set(stopwords.words('english'))
    return [word for word in words if word not in stop_words]

def remove_hyperlink(word):
    return re.sub(r&quot;http\S+&quot;, &quot;&quot;, word)

def fit_count_vectorizer(text):
    # Clean and tokenize the text
    cleaned_text = remove_special_characters(text)
    cleaned_text = remove_hyperlink(cleaned_text)
    tokens = word_tokenize(cleaned_text)
    tokens = remove_stop_words(tokens)
    cleaned_text = ' '.join(tokens)
    return cleaned_text  # Return the cleaned text
</code></pre>
<p>here are the prediciton where th problem occurs</p>
<pre><code>def predict():
    email_text =&quot;hello world&quot;
    cleaned_text = fit_count_vectorizer(email_text)
    cv.fit([cleaned_text])  # Fit CountVectorizer on the cleaned text
    email_features_array = cv.transform([cleaned_text])  # Use transform instead of fit_transform
    # Assume `model` is already defined and trained
    prediction = model.predict(email_features_array)

    # Apply the pre-trained model to predict the probability of the email being phishing
    probability = model.predict_proba(email_features_array)
    if prediction[0] == 1:
        result = 'Phishing'
        probability_score = probability[0][1] * 100
        print(result, probability_score)
    else:
        result = 'Legitimate'
        probability_score = probability[0][0] * 100
        print(result, probability_score)


# Example usage
predict()
</code></pre>
<p>i checked the type of my ML Model and got this:</p>
<pre><code>&lt;class 'numpy.ndarray'&gt;
</code></pre>
<p>, idk what i could do to solve this i tried almost every way possible</p>
","2024-05-02 15:01:11","-1","Question"
"78419787","","Which DICOM Tag should we use to differentiate between the series?","<p>We've been working on DICOM datasets for a while and we have the following problem right now. We have been dealing with the Prostate-MRI case and we have the difficulty to distinguish between different DWI series of one study.</p>
<p>The following DICOM Tags are missing:</p>
<ul>
<li>Sequence Name (0018,0024),</li>
<li>Sequence Type (0018,0020),</li>
<li>Image Type (0008,0008),</li>
<li>Diffusion Gradient Direction (0018,9075)</li>
</ul>
<p>Do anyone have any idea through which DICOM Tag we can differentiate between the series besides the ones mentioned above? I'm also attaching the DICOM Tags below to be more understandable.</p>
<p><a href=""https://i.sstatic.net/gweWbf7I.jpg"" rel=""nofollow noreferrer"">Structure of the Dataset</a></p>
<p>We've searched some other DICOM tags and tried to find a way the distinguish between the series.</p>
","2024-05-02 14:17:39","2","Question"
"78419285","78409544","","<p>You can customize the prompts given to the LLM by passing a prompt argument to <code>create_sql_agent</code> function.</p>
<p>Looking at <code>create_sql_agent</code> source code at <a href=""https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/agent_toolkits/sql/base.py"" rel=""nofollow noreferrer"">https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/agent_toolkits/sql/base.py</a></p>
<p>you can see if prompt is None it will create a prompt using:</p>
<pre><code>messages = [
                SystemMessage(content=cast(str, prefix)),
                HumanMessagePromptTemplate.from_template(&quot;{input}&quot;),
                AIMessage(content=suffix or SQL_FUNCTIONS_SUFFIX),
                MessagesPlaceholder(variable_name=&quot;agent_scratchpad&quot;),
            ]
prompt = ChatPromptTemplate.from_messages(messages)
</code></pre>
<p>using prefix and suffix from there:</p>
<pre><code>from langchain_community.agent_toolkits.sql.prompt import (
    SQL_FUNCTIONS_SUFFIX,
    SQL_PREFIX,
)
</code></pre>
<hr />
<p>So you can do the same thing to create a prompt with more customized instructions.</p>
<p>And then pass it to the agent.</p>
<p>Here is an example with the prefix and suffix can explicitly be customized, to add a line about your issue.</p>
<pre><code>from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    MessagesPlaceholder,
)

prefix = &quot;&quot;&quot;
You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.
You can order the results by a relevant column to return the most interesting examples in the database.
Never query for all the columns from a specific table, only ask for the relevant columns given the question.
You have access to tools for interacting with the database.
Only use the below tools. Only use the information returned by the below tools to construct your final answer.
You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.

DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.

If the question does not seem related to the database, just return &quot;I don't know&quot; as the answer.
&quot;&quot;&quot;

suffix = &quot;&quot;&quot;I should look at the tables in the database to see what I can query.  Then I should query the schema of the most relevant tables.
&quot;&quot;&quot;

messages = [
                SystemMessagePromptTemplate.from_template(prefix),
                HumanMessagePromptTemplate.from_template(&quot;{input}&quot;),
                AIMessagePromptTemplate.from_template(suffix),
                MessagesPlaceholder(variable_name=&quot;agent_scratchpad&quot;),
            ]
prompt = ChatPromptTemplate.from_messages(messages)

agent_executor = create_sql_agent(llm,
                                   toolkit=toolkit,
                                   agent_type=&quot;openai-tools&quot;,
                                   prompt = prompt,
                                   verbose=False)
</code></pre>
","2024-05-02 12:50:32","0","Answer"
"78415682","78414172","","<p>Well, in my experience always the most viable programming language for this stuff is python. Almost all the community around machine learning and artificial intelligence is around python, so is where you may find more frameworks and the most help from the community.
Regarding the framework, I've been working on a RAG with <a href=""https://www.langchain.com/"" rel=""nofollow noreferrer"">LangChain</a> and I've really been liking it. It's quite easy to learn and is really well maintained and documented.</p>
<p>Regarding your problem in particular, it seems to be easily solved via <a href=""https://python.langchain.com/docs/modules/model_io/chat/function_calling/"" rel=""nofollow noreferrer"">function calling</a>. It's basically extracting the parameters of a function from a natural language query. You don't need to train anything (tho it will improve your results) just do a little of prompt engineering.</p>
","2024-05-01 19:50:35","0","Answer"
"78414172","","How to Build an AI Chatbot that can do CRUD Operations via API Requests","<p>I'm on a project to develop an AI chatbot capable of performing CRUD (Create, Read, Update, Delete) operations via API requests. I'm relatively new to this field and would greatly appreciate any advice or guidance you could offer.</p>
<p>My main objectives are to:</p>
<p>Develop an AI chatbot that can understand natural language queries.
Enable the chatbot to interact with an API to perform CRUD operations on a database.
Some specific questions I have include:</p>
<p>What programming languages and frameworks are best suited for this task?
Are there any existing libraries or tools that could streamline the development process?
How should I approach training the chatbot to understand and respond to user queries effectively?</p>
<p>Any insights, tips, or resources you could share would be immensely helpful. Thanks in advance for your assistance!</p>
","2024-05-01 14:12:56","0","Question"
"78409544","","How to tune agent _executor for better understanding of the database","<p>I have a database in which I have connected an agent too. However, I have noticed that it sometimes gets confused between whether or not it should return a column ID or persons first name when asked &quot;which person sold the most....?&quot; Is there a way to tune/adjust the create_sql_agent from langchain.agents at which I can tell the agent to not return column ID but return first and last name based on questions structured like that?</p>
<p>I think the question may be related to this post but I am unsure how to include that/and structure that properly: <a href=""https://github.com/langchain-ai/langchain/discussions/9591"" rel=""nofollow noreferrer"">https://github.com/langchain-ai/langchain/discussions/9591</a></p>
<p>System Info
langchain-openai==0.1.3
Python 3.11.7
Windows 11</p>
<h2>Basic Model</h2>
<pre><code>from langchain_openai import ChatOpenAI
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.agents import create_sql_agent
from langchain.sql_database import SQLDatabase


llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo-1106&quot;, temperature=0, openai_api_key=os.environ.get('OPENAI_API_KEY'))

toolkit = SQLDatabaseToolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=False,
    agent_type=&quot;openai-tools&quot;)


print(agent_executor.invoke(&quot;What is my data about&quot;))
</code></pre>
<p>Nothing, unsure how to progress as I can not find examples.</p>
","2024-04-30 15:20:49","0","Question"
"78408708","78405336","","<p>You have 2 options that could work (didnt try them, but should work according to docs):</p>
<ol>
<li>provide your self signed cert while creating client via <code>cert_path</code>:</li>
</ol>
<pre><code>client = kfp_tekton.TektonClient(
    host=route,
    existing_token=token,
    ssl_ca_cert=cert_path
 )
</code></pre>
<ol start=""2"">
<li>Disable SSL validation via <code>verify_ssl=false</code></li>
</ol>
<pre><code>client = kfp_tekton.TektonClient(
    host=route,
    existing_token=token,
    verify_ssl=false
 )
</code></pre>
","2024-04-30 13:02:56","0","Answer"
"78405336","","SSL Error when running Openshift AI Pipeline from code","<p>I made an Elyra Pipeline en Openshift AI and I have submited to pipeline server ok.</p>
<p>I tried to run the pipeline fromPython code:</p>
<pre><code>import os
import kfp_tekton
import os

token = &quot;[my_token]&quot;
route = &quot;[My_projectURL]&quot;
client = kfp_tekton.TektonClient(host=route, existing_token=token)

</code></pre>
<p>but I got this error:</p>
<p>MaxRetryError: HTTPSConnectionPool(host='console-openshift-console.apps.lab-ai.cloud.semperti.com', port=443): Max retries exceeded with url: /k8s/cluster/projects/rhoai-demo/apis/v1beta1/healthz (<strong>Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1129)')))</strong></p>
<p>I cant run pipeline from python code in Openshift AI, !!!</p>
","2024-04-29 21:53:39","0","Question"
"78404657","78404407","","<p>To tackle the problem, I experimented with different versions of dspy-ai alongside the instructor library to assess their compatibility, as there wasn't any documentation/resources available regarding their compatibility with pydantic.
After some trial and error, I managed to resolve the versioning issue by downgrading dspy-ai to version 2.3.0 while keeping the instructor library at its latest version, 1.2.2. This adjustment successfully addressed the compatibility concerns, and the application now functions smoothly.</p>
","2024-04-29 18:51:34","0","Answer"
"78404407","","dspy-ai==2.4.5 dependency issues with instructor library","<p>I am trying to install dspy-ai==2.4.5 which needs pydantic version 2.5.0 along with instructor library which needs minimum pydantic version 2.7.0. I am using poetry for dependency management.</p>
<p>Following is an issue I am facing:</p>
<pre><code>Because no versions of instructor match &gt;1.2.3,&lt;2.0.0
 and instructor (1.2.3) depends on pydantic (2.7.0), instructor (&gt;=1.2.3,&lt;2.0.0) requires pydantic (2.7.0).
And because dspy-ai (2.4.5) depends on pydantic (2.5.0)
 and no versions of dspy-ai match &gt;2.4.5,&lt;3.0.0, instructor (&gt;=1.2.3,&lt;2.0.0) is incompatible with dspy-ai (&gt;=2.4.5,&lt;3.0.0).
So, because personal-graph depends on both dspy-ai (^2.4.5) and instructor (^1.2.3), version solving failed.


</code></pre>
<p><strong>I need to install both instructor and dspy-ai ,but version solving failed.</strong>
Is their any way to solve this issue?</p>
","2024-04-29 17:56:03","0","Question"
"78403613","78343977","","<p>Just Increase the maxOutputTokens to 1000 or 5000.</p>
","2024-04-29 15:04:25","0","Answer"
"78397531","78396036","","<p>There are a few issues:</p>
<ul>
<li><p>When the condition in <code>if current_state.cost &gt; best_neighbor.cost:</code> is not true, the next iteration of the loop will have the exact same state as the preceding iteration, and so you'll have an infinite loop. This can happen, as it is not guaranteed that <em>any</em> of the neighbors will have a cost that is less than the cost of the current state.</p>
<p>In general, the current position might already be on a &quot;hill&quot;, where there is actually no direct improvement by making one move. In fact, it could even get <em>worse</em> by making any move. Consider this example, where the goal state is this:</p>
<pre><code>1 2 3 
4 5 6
7 8 0
</code></pre>
<p>... and the current state is this:</p>
<pre><code>0 2 3 
4 5 6
7 1 8
</code></pre>
<p>Note how the values 2, 3, 4, 5, 6 and 7 are all on their final place. We &quot;only&quot; need to move that 1 to the top-left corner, and the gap to the bottom-right corner. The current state is on a <em>huge</em> hill, and the valley around it is <em>large</em>! So the approach you took will in general not work.</p>
</li>
<li><p>The cost calculation is a bit rudimentary:</p>
<ul>
<li>Surely it is better if a misplaced value is <em>close</em> to its destination, than when it is far from it, but in both of these cases your cost will contribute 0 for this particular value. You could refine it by adding the (Manhattan) distance that each value has to its destination.</li>
<li>The distance that the value 0 (the gap) has to its destination, or even its correct positioning, is of little importance. As this gap needs to (necessarily) move around as you play moves, you should exclude it from the cost calculation.</li>
</ul>
</li>
<li><p>There is no protection from running in cycles. Implement a notion of visited states and avoid playing moves that lead to already visited positions. For implementing this, you need a <em>key</em> for each state. To achieve that, it will be handy to not represent the state as a 3x3 list, but as a 9-member tuple. Such a tuple can be used as a key.</p>
</li>
<li><p>Not a real issue, but it doesn't look right to hardcode the goal state inside the <code>State</code> constructor. Also, I would define the goal state as an instance of <code>State</code>. It should be defined only once -- not each time a <code>State</code> instance is created. I suggest making it a class member. (An alternative would be that you pass it as argument to <code>hill_climbing</code>)</p>
</li>
</ul>
<p>If we want to stick with the principle of <em>hillclimbing</em>, you'll need to foresee making several moves in a row in order to reach a state that improves on the cost of the current state. You could first look at the direct neighbors, but if none of them improve the cost, then look at the neighbors of the neighbors, and then the neighbors of those, ...etc, until you find a state that is an improvement (a &quot;climb&quot;).</p>
<p>Here is a modification of your code, with the above remarks implemented:</p>
<pre><code>class State:
    goal_state: 'State | None' = None
    
    def __init__(self, state, parent=None):
        self.parent = parent
        self.state = tuple(state)  # Use tuple representation to use it as key
        self.cost = self.calculate_cost()

    @staticmethod
    def distance(a, b):
        return abs(a // 3 - b // 3) + abs(a % 3 - b % 3)
    
    def calculate_cost(self):
        if not self.goal_state:
            return 0
        return sum(self.distance(self.state.index(val), i)
                   for i, val in enumerate(self.goal_state.state))

    # Create a nice representation 
    def __repr__(self):
        s = &quot; &quot;.join(map(str, self.state)).replace(&quot;0&quot;, &quot;.&quot;)
        return s[:5] + &quot;\n&quot; + s[6:11] + &quot;\n&quot; + s[12:]

    # Helper function to create a neighbor state
    def swap(self, a, b):
        ab = a + b
        return State(tuple(self.state[ab - i if i in (a, b) else i] 
                           for i in range(9)), self)

    # A generator: leave it for the caller to make this into a list
    def generate_neighbors(self):
        gap = self.state.index(0)
        if gap &lt; 6:     # move down
            yield self.swap(gap, gap + 3)
        if gap &gt; 2:     # move up
            yield self.swap(gap, gap - 3)
        if gap % 3 &lt; 2: # move right
            yield self.swap(gap, gap + 1)
        if gap % 3 &gt; 0: # move left
            yield self.swap(gap, gap - 1)

    # hash/__eq__ implementations to use the state field as key
    def __hash__(self):
        return hash(self.state)
    def __eq__(self, other):
        return self.state == other.state

def hill_climbing(current_state):
    path = [current_state]
    while current_state.cost &gt; 0:
        visited = set([current_state])  # Avoid running in cycles
        # Start a BFS
        queue = list(current_state.generate_neighbors())
        for neighbor in queue:
            if neighbor not in visited:
                visited.add(neighbor)
                if neighbor.cost &lt; current_state.cost:
                    break  # We have an improvement!
                queue.extend(neighbor.generate_neighbors())
        # Flush the path we found into a list
        subpath = []
        while neighbor != current_state:
            subpath.append(neighbor)
            neighbor = neighbor.parent
        # Extend the overall path with this subpath
        path += reversed(subpath)
        current_state = path[-1]

    return path
</code></pre>
<p>Here is how you would run it with the example configuration you gave in the question:</p>
<pre><code>State.goal_state = State((1, 2, 3, 8, 0, 4, 7, 6, 5))
start = State((2, 0, 3, 1, 8, 4, 7, 6, 5))
path = hill_climbing(start)
# Print the solution path
for state in path:
    print(&quot;-----&quot;)
    print(state)
</code></pre>
<p>This will output:</p>
<pre><code>-----
2 . 3
1 8 4
7 6 5
-----
. 2 3
1 8 4
7 6 5
-----
1 2 3
. 8 4
7 6 5
-----
1 2 3
8 . 4
7 6 5
</code></pre>
<p>On a final note: the A* algorithm is a better choice here than hill climbing.</p>
","2024-04-28 08:31:04","0","Answer"
"78396132","78396036","","<p>I am not sure, but maybe the problem is caused by the <code>if</code> statement in your <code>generate_neighbors</code> function.</p>
<p>Try to change:
<code>if self.state[i][j] == 0</code> to <code>if self.state[i][j] != 0</code></p>
","2024-04-27 19:21:53","0","Answer"
"78396036","","sliding-tile problem using hill climbing algorithm","<p>I am trying to solve the 8 puzzle or sliding tile problem using Hill-Climbing algorithm in python. I Wrote the code to print each intermediate step until goal node is reached but no output is shown by the interpreter. Please review my code and tell me about the errors in the code and solution to them.</p>
<p>Hi there, I am trying to solve the 8 puzzle or sliding tile problem using Hill-Climbing algorithm in python. I Wrote the code to print each intermediate step until goal node is reached but no output is shown by the interpreter. Please tell me about the errors in the code and solution to them.</p>
<pre><code>from copy import deepcopy

class State:
    def __init__(self, state, parent):
        goal_state = [[1, 2, 3], [8, 0, 4], [7, 6, 5]]
        self.parent = parent
        self.state = state
        self.cost = self.calculate_cost(goal_state)

    def calculate_cost(self, goal_state):

        cost = 0

        for i in range(3):
            for j in range(3):
                if self.state[i][j] != goal_state[i][j]:
                    cost += 1

        return cost


    def generate_neighbors(self):
        neighbors = []

        for i in range(3):
            for j in range(3):
                if self.state[i][j] == 0:

                    if i &gt;= 0 and i != 2:   #move down
                        new_state = deepcopy(self.state)
                        new_state[i][j], new_state[i+1][j] = new_state[i+1][j], new_state[i][j]
                        neighbors.append(State(new_state, self))

                    if i &lt;=2 and i != 0:    #move up
                        new_state = deepcopy(self.state)
                        new_state[i][j], new_state[i-1][j] = new_state[i-1][j], new_state[i][j]
                        neighbors.append(State(new_state, self))

                    if j &gt;= 0 and j != 2: # move right
                        new_state = deepcopy(self.state)
                        new_state[i][j], new_state[i][j+1] = new_state[i][j+1], new_state[i][j]
                        neighbors.append(State(new_state, self))

                    if j &lt;= 2 and j != 0:   # move left
                        new_state = deepcopy(self.state)
                        new_state[i][j], new_state[i][j-1] = new_state[i][j-1], new_state[i][j]
                        neighbors.append(State(new_state, self))


        return neighbors

def hill_climbing(initial_state):

    current_state = State(initial_state, None)

    while current_state.cost &gt; 0:

        neighbors = current_state.generate_neighbors()

        best_neighbor = min(neighbors, key = lambda state: state.cost)

        if current_state.cost &gt; best_neighbor.cost:
            current_state = best_neighbor

    path = []
    while current_state is not None:
        path.append(current_state.state)
        current_state = current_state.parent

    return path[::-1]






initial_state = [[2, 0, 3], [1, 8, 4], [7, 6, 5]]

path = hill_climbing(initial_state)

# Print the solution path
for state in path:
    print(state)
</code></pre>
","2024-04-27 18:47:20","-1","Question"
"78395051","78391063","","<p>it seems like your imports are depreciated
try using</p>
<pre><code>from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt
</code></pre>
<p>also install c dependencies for llama.cpp</p>
","2024-04-27 12:56:07","1","Answer"
"78393953","78393841","","<p>Try using this repository.
<a href=""https://github.com/renjithsasidharan/seven-segment-ocr"" rel=""nofollow noreferrer"">https://github.com/renjithsasidharan/seven-segment-ocr</a></p>
<p>It seems to work according to the results they have provided. Since this is implemented in Tensorflow lite, I think it would be ideal for faster inference.</p>
<p>If you have time to look into this <a href=""https://msie4.ait.ac.th/wp-content/uploads/sites/5/2020/05/MSIE-03-L-M3S2_L01.pdf"" rel=""nofollow noreferrer"">paper</a> as well</p>
","2024-04-27 05:54:12","-1","Answer"
"78393841","","How to read seven segment display and logos in images using Python?","<p>I have a bunch of photos of energy meters. In every photo there is info written on the meter and also a seven segment display which has reading in kWh. Also, there is logo of meter manufacturer. I want to extract information like meter serial number, meter manufacturer, meter reading in kWh(7 segmented display).</p>
<p>I have used easyocr module to detect text and using that I have achieved to extract meter serial numbers but for kWh reading easyocr fails to detect the digital digits.</p>
<p>Currently I have 2 requirements, one is to detect digits from 7 segment display and also detect manufacturer from logo symbol in meter. Only 2-3 different manufacturers are there so I need to train based on only 2-3 logos. But I don't know how to do it.</p>
<p>Any help or guidance with steps kr procedure will be really appreciated.</p>
<p>I tried googling, searched on YouTube but not getting any roadmap for the same. I have tried other ocr modules like tesseract but it also fails.</p>
","2024-04-27 04:51:39","-2","Question"
"78392257","78388600","","<p>Your code shows a bunch of word-vectors loaded – the 4GB circa-2012 GoogleNews vectors – but then the object <code>word_vectors</code> is never used for anything, so won't have any effect on your responses. If you meant to use it to help select some <em>portion</em> of your corpus, in a 'retrieval-augmented-generation' (RAG) style – that's not happening. And further: simple single-word vectors are a pretty crude &amp; weak basis for modeling of longer texts.</p>
<p>Your prompt <code>template</code> also looks a bit confused. You include the entire corpus – which might be many thousands or tens-of-thousands or hundreds-of-thousands of words? – at the top, then repeat the <code>query</code> twice.</p>
<p>For example, if your corpus is the sentence <code>&quot;Math is hard, let's go shopping.&quot;</code> and the query is <code>&quot;What is 2 + 2?&quot;</code>, what your template actually passes to the LLM is:</p>
<pre><code>&quot;&quot;&quot;
Math is hard, let's go shopping.
give an answer to the What is 2 + 2?:
What is 2 + 2?
&quot;&quot;&quot;
</code></pre>
<p>All that said:</p>
<p>LLMs are inherently prone to confabulating plausible texts ('hallucination') that don't precisely conform to their training material or the written instructions and material in their context window. This may be a larger problem with smaller models and higher temperatures.</p>
<p>There are no guarantees, but some tactics that may help:</p>
<ol>
<li>Ensuring the most relevant information isn't &quot;too far&quot; before the generation.</li>
</ol>
<p>IIUC – &amp; this may be more a vague approximation than the literal truth – a Mistral-7b tune like your Zephyr-7b-beta has a 'context-window' of about 4k tokens, where each token is something between a word &amp; meaningful subword. (So: this could be less than 4k words.)</p>
<p>That means generation can only definitively work off of the last ~4k words in your prompt (plus the LLM's inbuilt tendencies), plus some much vaguer retained sense of earlier context (that fades quickly with distance).</p>
<p>So any time your query needs info more than 4k words back in your corpus, the LLM will be more guessing/hallucinating than deducing from the corpus.</p>
<p>The gist of RAG is to ensure the best reference info is fed the LLM just before it needs it, so it's in the context-window. So if you upgraded your code to put the most relevant (by some semantic-similarity measure) excerpts of your corpus into the prompt, just before the generation, you'd likely get better results.</p>
<ol start=""2"">
<li><p>Using larger model, a model with a longer effective context, or adjusting the generation temperature. You'd have to tinker to find what works well for your specific domain, resource limits, and typical queries and desired responses.</p>
</li>
<li><p>Stacking techniques, such as using a 2nd LLM pass (itself perhaps enhanced with some corpus-excerpt-retrieval) to check that an answer does not contradict your canonical reference snippets, or choose which of sevral alternate generations is best.</p>
</li>
</ol>
<p>Ultimately, if you want your system to <em>only</em> say things that are definitively in the corpus, than LLMs may be both overkill, and unhelpfully prone to hallucinations, for your needs.</p>
<p>You might just want to break your corpus into chunks/ranges, then use some pre-LLM info-retrieval – keyword search, semantic similarity search, etc – to echo back only the best exact-unaltered text chunks that are canonical.</p>
<p>(Such a system can sometimes be refined with LLM techniques, even if the LLM isn't directly composing the answers. For example, a technique called 'HyDE' uses an LLM to generate candidate answers for a query that you don't ever show the user, or care if they're wrong - they're just used to find <em>similar</em> texts in your verified answers.)</p>
","2024-04-26 17:56:08","1","Answer"
"78391063","","I am facing ImportError: cannot import name 'LlamaCPP' from 'llama_index.llms' (unknown location) while implementing this","<p>I am facing ImportError: cannot import name 'LlamaCPP' from 'llama_index.llms' (unknown location) while implementing
and ModuleNotFoundError: No module named 'llama_index.llms.llama_utils' this
while implementing this:</p>
<pre><code>import torch

#from llama_index.llms import LlamaCPP
from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt
llm = LlamaCPP(
    
    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',
    # optionally, you can set the path to a pre-downloaded model instead of model_url
    model_path=None,
    temperature=0.1,
    max_new_tokens=256,
    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room
    context_window=3900,
    # kwargs to pass to __call__()
    generate_kwargs={},
    # kwargs to pass to __init__()
    # set to at least 1 to use GPU
    model_kwargs={&quot;n_gpu_layers&quot;: -1},
    # transform inputs into Llama2 format
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
    verbose=True,
)
</code></pre>
<p>PDF chat bot I am creating</p>
","2024-04-26 14:12:27","-1","Question"
"78391011","78271686","","<p>you can to use the</p>
<blockquote>
<p>chat_input</p>
</blockquote>
<p>(<a href=""https://docs.streamlit.io/develop/api-reference/chat/st.chat_input"" rel=""nofollow noreferrer"">doc</a>)
this component do exactly what you want to do</p>
","2024-04-26 14:04:13","0","Answer"
"78390228","78385859","","<p>Please check the policies of your Bedrock Agent's IAM role. If you want to be able to use all Claude models, try the one below. Make sure to change the region to your's.</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;AmazonBedrockAgentBedrockFoundationModelPolicyProd&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: &quot;bedrock:InvokeModel&quot;,
            &quot;Resource&quot;: [
                &quot;arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude*&quot;
            ]
        }
    ]
}
</code></pre>
","2024-04-26 11:50:54","2","Answer"
"78388600","","How to prevent an AI model from generating its own answers outside of a predefined corpus?","<p>so I'm developing a chatbot using Python. I integrated a vector database called Word2Vec, and I'm using the Zephyr model from Hugging Face. I have created a very simple text file corpus, and I want my model to go through this corpus when responding. This is actually working as intended. However, the issue arises when there are no details regarding the user input in the corpus.</p>
<p>For example, let's say there is a start date in my corpus. When I ask the model about the start date, it gives this information. But if there is no information in my corpus about a legal team, and I ask the model, it generates an answer that says, 'Yes, there is a legal department.'</p>
<p>What's your suggestion to stop this and restrict my model only to the corpus?</p>
<p>Appreciate any suggestions and thanks in advance.&quot;</p>
<p>I've tried giving is specific keywords. but this is not practical and scalable.</p>
<hr />
<pre><code>from langchain import HuggingFaceHub, PromptTemplate, LLMChain
import gensim.downloader as api

word_vectors = api.load(&quot;word2vec-google-news-300&quot;)

template = &quot;&quot;&quot;
{corpus_text}
give an answer to the {query}:
{query}
&quot;&quot;&quot;

prompt = PromptTemplate(template=template, input_variables=['corpus_text', 'query'])

model_id = &quot;HuggingFaceH4/zephyr-7b-beta&quot;

conv_model = HuggingFaceHub(
    huggingfacehub_api_token=&quot;12345&quot;,  
    repo_id=model_id,
    model_kwargs={&quot;temperature&quot;: 0.1, &quot;max_new_tokens&quot;: 1000}
)

conv_chain = LLMChain(llm=conv_model,
                      prompt=prompt,
                      verbose=True)

corpus_file = r/myfile.txt

with open(corpus_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:
    corpus_text = file.read()

while True:
    user_input = input(&quot;You: &quot;)
    
    
    generated_prompt = prompt.template.format(corpus_text=corpus_text, query=user_input)
    
    
    result = conv_chain.run({&quot;query&quot;: user_input, &quot;corpus_text&quot;: corpus_text})
    
    print(&quot;Model:&quot;, result)
</code></pre>
","2024-04-26 06:24:57","1","Question"
"78387790","78387721","","<p>You can use os.getcwd()
See here: <a href=""https://www.geeksforgeeks.org/python-os-getcwd-method/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/python-os-getcwd-method/</a></p>
","2024-04-26 01:09:22","0","Answer"
"78387721","","How to scope autogen tool to working dir?","<p>I am playing with <code>AutoGen</code> and I've added tools to read and write text files (mainly because don't want to waste resource)</p>
<p>My agent has working dir</p>
<pre class=""lang-py prettyprint-override""><code>executor = autogen.UserProxyAgent(
    name=&quot;executor&quot;,
    system_message=&quot;Executor. Execute the code written by the Engineer and report the result.&quot;,
    human_input_mode=&quot;NEVER&quot;,
    code_execution_config={
        &quot;last_n_messages&quot;: 3,
        &quot;work_dir&quot;: WORKING_DIR,
        &quot;use_docker&quot;: False,
    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.
)
</code></pre>
<p>And tools</p>
<pre class=""lang-py prettyprint-override""><code>def read_file(file_name: Annotated[str, &quot;File name has to be json, txt or html&quot;]) -&gt; int:
    if not file_name.endswith(&quot;.json&quot;) and not file_name.endswith(&quot;.txt&quot;) and not file_name.endswith(&quot;.html&quot;):
        return f&quot;I can read only .json, .txt or .html files you asked for {file_name}. Use python to read other files.&quot;
    if not os.path.exists(os.path.join(WORKING_DIR, file_name)):
        return f&quot;File {file_name} does not exist.&quot;
    with open(os.path.join(WORKING_DIR, file_name), &quot;r&quot;) as f:
        return f.read()


def write_file(file_name: Annotated[str, &quot;File name&quot;], content: Annotated[str, &quot;text or json content&quot;]) -&gt; int:
    # verify that nested folders exists
    if not os.path.exists(f&quot;{WORKING_DIR}/{os.path.dirname(file_name)}&quot;):
        os.makedirs(f&quot;{WORKING_DIR}/{os.path.dirname(file_name)}&quot;)

    with open(f&quot;{WORKING_DIR}/{file_name}&quot;, &quot;w&quot;) as f:
        return f.write(content)
</code></pre>
<p>How can I do it better? So I don't have to worry about working dir in my code.</p>
","2024-04-26 00:31:41","0","Question"
"78385859","","AWS Bedrock Agent Access Denied with Claude 3","<p>I'm using AWS Bedrock with Pinecone as my vector database, and with Claude 2.1, it works smoothly. Still, when I try to update it (or create a new one) using the recently enabled Claude 3.0 Haiku or Sonnet, it triggers the following error:</p>
<p><a href=""https://i.sstatic.net/WpBO9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WpBO9.png"" alt=""Error at AWS Bedrock with Claude 3.0"" /></a></p>
<p>There are no Advanced Prompts configurations or Action Groups; it's a simple use case of a PDF on an S3 bucket with Pinecone.</p>
<p>I'm running it using the &quot;playground&quot;, the agent test environment of Bedrock, and I also have the Model enabled/subscribed.</p>
<p><a href=""https://i.sstatic.net/vmq2H.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vmq2H.png"" alt=""AWS Bedrock - Enabled Models"" /></a></p>
<p>When I look at the step trace, I see that the first two steps (pre-processing and orchestration) were successfully executed. However, the traces lack any meaningful information about the error, and I can only see that it found the expected results.</p>
<p><a href=""https://i.sstatic.net/9SGaI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9SGaI.png"" alt=""AWS Bedrock Agent Test Print"" /></a></p>
<p>JIT: this is similar to this one <a href=""https://stackoverflow.com/questions/78157503/any-one-having-access-issues-with-claude-3-haiku-or-anthropic-claude-3-sonnet-fr"">Any one having access issues with claude-3-haiku or anthropic.claude-3-sonnet from boto client</a>, but called differently.</p>
<p>It is not similar to this one: <a href=""https://stackoverflow.com/questions/77674389/amazon-bedrock-agent-access-denied-when-calling-bedrock-check-your-request-pe"">Amazon Bedrock Agent - Access denied when calling Bedrock. Check your request permissions and retry the request</a> as The issue here was the lack of access to the model itself.</p>
<p>If I switch to Claude 2.1, it will work as expected.</p>
<p>I also tried to recreate the agent and the Knowledge Base unsuccessfully.</p>
<p><strong>Additional information</strong>: The Knowledge Base testing environment works smoothly. The problem is when I link it to an Agent.</p>
<p>Knowledge Base Models I tried:</p>
<ul>
<li>Titan Embeddings G1 - Textv1.2 - 1536 (with pinecone cosine 1536)</li>
<li>Embed Multilingualv3 - 1024 (with pinecone cosine 1024)</li>
</ul>
<p>But I don't think the problem is related to the knowledge base because, as I mentioned before, the trace shows correct search results.</p>
<p>Has anyone faced this issue before or knows how to address it correctly?</p>
","2024-04-25 15:59:18","2","Question"
"78378753","","Exception encountered: Unrecognized keyword arguments passed to Conv2D: {'batch_input_shape': [None, 135, 135, 3]}","<p>I have some models that were trained with an older version of Tensorflow, Keras and Python. Now I updated to Tensorflow 2.16.1, Keras 3.1.1 and Python 3.12.2. Now I cannot even load these models that I get a huge error message. The last message is:</p>
<p>Exception encountered: Unrecognized keyword arguments passed to Conv2D: {'batch_input_shape': [None, 135, 135, 3]}</p>
<p>Is there a way to access this model without downgrading or retraining the model?</p>
<p>I tried:</p>
<pre><code>    from tensorflow.keras.models import load_model
    models_folder = '/savedModels/models_stack/'
    model1 = load_model(f'{models_folder}model_best1.keras')
</code></pre>
","2024-04-24 13:17:53","0","Question"
"78378399","78378193","","<p>Check if there are some columns that all of it is <code>nan</code> values, this could be a reason why you get <code>inf</code> when you replace them with <code>mean</code> or <code>median</code> (if all the values are <code>nan</code> there is no meaning in <code>mean</code> or <code>meadian</code>)</p>
<p>Also if you can provide more information, it would be useful to check what are the potential problems in the task you're doing.</p>
","2024-04-24 12:19:42","0","Answer"
"78378193","","Why do I get ""inf"" values after replacing NaN values in my dataset with the mean or median?","<p>I'm using Python and I have a dataset containing NaN values. To clean up these data, I replaced the NaN values with the mean or median of each column using the fillna() function from pandas. However, after this operation, some values in my dataset became &quot;inf&quot;. I don't understand why this is happening and how I can fix this issue.</p>
","2024-04-24 11:45:41","0","Question"
"78375200","78199805","","<p><code>ToTensorV2</code> does not perform rescaling.</p>
<p><a href=""https://albumentations.ai/docs/api_reference/full_reference/?h=totensor#albumentations.pytorch.transforms.ToTensorV2"" rel=""nofollow noreferrer"">Documentation</a></p>
<p>Docstring:</p>
<pre><code>It converts images/masks to PyTorch Tensors, inheriting from BasicTransform. Supports images in numpy HWC format and converts them to PyTorch CHW format. If the image is in HW format, it will be converted to PyTorch HW.
</code></pre>
<p>Initially, there was <code>ToTensor,</code> which automatically rescaled inputs. But it was confusing, and <code>ToTensor</code> was deprecated.</p>
<p>Typically, people use <a href=""https://albumentations.ai/docs/api_reference/augmentations/transforms/?h=normalize#albumentations.augmentations.transforms.Normalize"" rel=""nofollow noreferrer"">Normalize</a> before <code>ToTensor</code></p>
<p>Say, for ImageNet normalization:</p>
<pre class=""lang-py prettyprint-override""><code>import albumentations as A

transform = A.Compose([A.HorizontalFlip(p=1), 
A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),
ToTensorV2()])
</code></pre>
<p>Or if you want images being normalized to [0, 1]</p>
<p>=&gt;</p>
<pre class=""lang-py prettyprint-override""><code>transform = A.Compose([A.HorizontalFlip(p=1),
A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0),
ToTensorV2()])
</code></pre>
<p>P.S. <code>p=1</code> in <code>A.HorizontalFlip</code> looks a bit strange, as it will flip all input images horizontally.</p>
","2024-04-23 22:16:18","3","Answer"
"78362646","78362630","","<p>Add 'accuracy' metric in compile:</p>
<pre><code>model.compile(optimizer=tf.optimizers.Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])
</code></pre>
<p>You will get only loss value without it.</p>
","2024-04-21 18:01:55","1","Answer"
"78362630","","How to fix a TypeError in this tensorflow code?","<p>The code:</p>
<pre><code>import tensorflow as tf
from tensorflow import keras


fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

model = keras.Sequential([
    keras.layers.Flatten(input_shape = (28,28)),
    keras.layers.Dense(128,activation = tf.nn.relu),
    keras.layers.Dense(10,activation = tf.nn.softmax)
])

model.compile(optimizer=tf.optimizers.Adam(),),
              loss = 'sparse_categorical_crossentropy')

model.fit(train_images,train_labels,epochs = 5)

test_loss, test_acc = model.evaluate (test_images, test_labels)
</code></pre>
<p>First, I had 'AttributeError: module 'tensorflow._api.v2.train' has no attribute 'AdamOptimizer''
and changed 'optimizer=tf.train.AdamOptimizer()' to 'tf.optimizers.Adam()' (worked) and then i had another error.(Found solution at [https://stackoverflow.com/questions/55318273/tensorflow-api-v2-train-has-no-attribute-adamoptimizer] )</p>
<p>The error:</p>
<p><code>line 25, in &lt;module&gt; test_loss, test_acc = model.evaluate(test_images, test_labels) ^^^^^^^^^^^^^^^^^^^ TypeError: cannot unpack non-iterable float object</code></p>
<p>Here is what ChatGPT advised me (also didn't work):</p>
<pre><code>import tensorflow as tf
from tensorflow import keras


fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Reshape the input data
train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer=tf.optimizers.Adam(),
              loss='sparse_categorical_crossentropy')

model.fit(train_images, train_labels, epochs=5)

# Reshape the test data
test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)

test_loss, test_acc = model.evaluate(test_images, test_labels)
</code></pre>
<p>also all the code I took from the Zero to Hero tensorflow YT videos</p>
","2024-04-21 17:56:48","0","Question"
"78356777","","What does ""I"" in the section ""_IQ"" and ""_M"" mean in this name ""Meta-Llama-3-8B-Instruct-IQ3_M.gguf""?","<p>Appreciate if someone could let me know what does &quot;I&quot; in the section &quot;_IQ&quot; and &quot;_M&quot; mean in this name &quot;Meta-Llama-3-8B-Instruct-IQ3_M.gguf&quot;???</p>
<p>I searched and found what does the &quot;Q&quot; mean(quantization), but I cannot find the meanings for &quot;I&quot; and &quot;M&quot;.</p>
","2024-04-20 02:01:44","4","Question"
"78355052","78300721","","<p>I just started with Flowise and I saw in a tutorial, I Think you are looking for &quot;Starter prompts&quot;, these will allow you to have the same initial buttons like the chat in shopify and have already a preconfiguration to return info.</p>
<p>I saw this thread in Github: <a href=""https://github.com/FlowiseAI/Flowise/discussions/1643"" rel=""nofollow noreferrer"">https://github.com/FlowiseAI/Flowise/discussions/1643</a></p>
","2024-04-19 16:22:08","0","Answer"
"78345746","78294529","","<p>You need to do <a href=""https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-azure-openai-embedding#sample-output"" rel=""nofollow noreferrer"">outputFieldMappings</a> in indexer.</p>
<p>Add below json element in indexer.</p>
<pre class=""lang-json prettyprint-override""><code>  &quot;outputFieldMappings&quot;: [
    {
      &quot;sourceFieldName&quot;: &quot;/document/vector/*&quot;,
      &quot;targetFieldName&quot;: &quot;Desc_vector&quot; #Target vector field in index.
    }
  ]
</code></pre>
<p><img src=""https://i.imgur.com/aeOZjq3.png"" alt=""enter image description here"" /></p>
<p>After saving the indexer <strong>Reset</strong> and <strong>Run</strong> it.</p>
<p><img src=""https://i.imgur.com/6RsZdA3.png"" alt=""enter image description here"" /></p>
<p>Make sure your index vector field dimensionality is same with the model output dimensionality.</p>
<p>To create the vector field with proper configuration refer <a href=""https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-create-index?tabs=config-2023-11-01%2Crest-2023-11-01%2Cpush%2Cportal-check-index"" rel=""nofollow noreferrer"">this</a></p>
","2024-04-18 07:52:19","0","Answer"
"78343977","","Gemini API not giving output when the input is a question and longer then 5 words and if it's not a question and longer then 7","<p>I am attempting to make a website to talk to Cyrus the Great. I have set up Gemini to respond to any input as if it's Cyrus the Great but when testing it I have encountered a unusual problem. I can get a response if the input is pretty short but the input &quot;Hi there ruler of the great Persian empire Cyrus&quot; gets a response but &quot;What did your people eat&quot; doesn't. I haven't found anything for this so I'm very confused. Here's the code segment for Gemini.</p>
<pre><code>import { GoogleGenerativeAI } from &quot;@google/generative-ai&quot;;

// Fetch your API_KEY
const API_KEY = 'API_KEY';

// Access your API key
const genAI = new GoogleGenerativeAI(API_KEY);

window.talkToGemini = async function(){
  //Set the variable to the output from gemini
  let userInput = document.getElementById('user_input').value + &quot; respond as if you're Cyrus the Great&quot;;

  // For text-only input, use the gemini-pro model
  const model = genAI.getGenerativeModel({ model: &quot;gemini-pro&quot;});

  const chat = model.startChat({
    history: [
      {
        role: &quot;user&quot;,
        parts: [{ text: userInput }],
      },
      {
       role: &quot;model&quot;,
       parts: [{ text: &quot;Great to meet you. What would you like to know?&quot; }],
     },
    ],
    generationConfig: {
      maxOutputTokens: 100,
    },
  });

  var result = await chat.sendMessage(userInput);
  var response = await result.response;
  var text = response.text();
  console.log(text);
</code></pre>
<p>I have looked around for anyone else who is having this problem but it doesn't seem to have happened before.</p>
","2024-04-17 22:13:44","-2","Question"
"78342209","78340533","","<p>In my case, my /usr/local have no cudnn libs, I should add cudnn lib dir to LD_LIBRARY_PATH</p>
<pre><code>LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/home/roroco/Downloads/cpp-lib/cudnn-linux-x86_64-8.9.0.131_cuda11-archive/lib cpp_cmd
</code></pre>
<p>way 1: set LD_LIBRARY_PATH forever:</p>
<p>in ~/.bashr, ad <code>export LD_LIBRARY_PATH=$LIBRARY_PATH:path/to/cudnn/lib/dir</code></p>
<p>way 2: set LD_LIBRARY_PATH tmply:</p>
<p>In clion debug conf, I can set env var</p>
<p><a href=""https://i.sstatic.net/dazW3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dazW3.png"" alt=""enter image description here"" /></a></p>
","2024-04-17 15:37:29","0","Answer"
"78341396","78339960","","<p>All old documentation (most of all documentation nowadays) says to import from keras.preprocessing.text import Tokenizer, but keras 3 integrated the tokenizer in the textvetorization.</p>
<p>So if you use the code example you will see that you import from keras.layers import TextVectorization, that is mostly what tokenizer does, in fact, tokenizer is a class IN TextVectorization.</p>
<p>So to import Tokenizer you need to import TextVectorization from keras.layers and import from TextVectorization Tokenizer, like this:</p>
<pre class=""lang-py prettyprint-override""><code>from keras.layers import TextVectorization; import tokenizer
</code></pre>
<p>that is for Keras 3.</p>
<p>If you want to use</p>
<pre class=""lang-py prettyprint-override""><code>from keras.preprocessing.text import Tokenizer
</code></pre>
<p>You can downgrade the version your using to Keras 2</p>
","2024-04-17 13:36:59","0","Answer"
"78341242","78340774","","<p>The response generated by the LLM model is based on the data it has been trained on. It is important to note that Google Gemini web and API may have some differences in the micro model in their models named Gemini, leading to different answers. Even if both models are identical, they may still provide different responses because data is generated in real-time.</p>
<p>To obtain accurate data consistently,  I recommend building an RAG pipeline and utilizing the LLM response from your data instead of the Gemini model data. However, it has its limitations and may not respond at all times. RAG still does not ensure 100% accuracy in the model.</p>
<p>In short, do whatever changes to get accurate information, but LLM will not guarantee to provide 100% accurate answers yet.</p>
","2024-04-17 13:12:16","0","Answer"
"78340967","78340774","","<blockquote>
<p>What do I need to do in order to get pretty much the same response I would get on the browser as an output from the API call?</p>
</blockquote>
<p>You can not. There is currently no way to get the exact same response from <a href=""https://aistudio.google.com/app/prompts/poem-writer"" rel=""nofollow noreferrer"">AI studio</a> in the API.</p>
<p>If you are trying to get the same response from the <a href=""https://gemini.google.com/app"" rel=""nofollow noreferrer"">Gemini web app</a> then the response will be completely different in most cases. The Gemini chat app at gemini.google.com includes many models and sets of information, some of which are proprietary. It does not have an API.</p>
<p>Remember this is an ai not a database the response is generated real time and may change. It picks the most likely answer it can come up with at the time it is run.</p>
","2024-04-17 12:26:43","1","Answer"
"78340774","","Google Gemini API giving back inaccurate information","<p>I am trying to use Google Gemini's API in order to generate responses for a list of buildings in NYC. I have structured the prompt to give a format of a list of bullet points about building information. When writing the same prompt in the browser in Gemini, I am getting accurate information, but when writing prompts to test in postman the responses are inaccurate. I provided and explained my example below:</p>
<p><a href=""https://i.sstatic.net/60GQ7.png"" rel=""nofollow noreferrer"">The address in the response was being constructed in 2001 not in 1929</a></p>
<p>This issue here is that the building was built in 2001 finished in 2003 not in 1929. Other attempts to change the prompt give other years that are also in the early 1900s. Some of the other information in the response of the image above is also inaccurate, but my assumption is that if I can get one to work it will have them all work.</p>
<p>I've played with the generationConfig to no avail. What do I need to do in order to get pretty much the same response I would get on the browser as an output from the API call or what do I need to change in order to get the accurate information?</p>
","2024-04-17 11:57:22","0","Question"
"78340533","","FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.8: cannot open shared object file: No such file or directory","<pre><code>terminate called after throwing an instance of 'Ort::Exception'
  what():  /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1193 onnxruntime::Provider&amp; onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.8: cannot open shared object file: No such file or directory

Signal: SIGABRT (Aborted)
</code></pre>
<p>in CMakeLists.txt, I have add this lib abs path and I make sure this lib path exist</p>
<pre><code>file(GLOB cuda_libs /usr/local/cuda-11.8/lib64/*.so*)
file(GLOB cudnn_libs /home/roroco/Downloads/cpp-lib/cudnn-linux-x86_64-8.9.0.131_cuda11-archive/lib/*.so.*)
file(GLOB onnx_libs /home/roroco/Downloads/cpp-lib/onnxruntime-linux-x64-gpu-1.16.3/lib/*.so.*)
file(GLOB cv_libs /home/roroco/Downloads/cpp-lib/opencv-4.9.0/dist/lib/*.so.*)
set(libs
        ${cuda_libs}
        ${cudnn_libs}
        ${onnx_libs}
        ${cv_libs}
        /home/roroco/Downloads/cpp-lib/cudnn-linux-x86_64-8.9.0.131_cuda11-archive/lib/libcudnn.so.8
)
message(&quot;libs:${libs}&quot;)
list(REMOVE_DUPLICATES libs)

# Link the libraries to your executable
target_link_libraries(inference ${libs})
</code></pre>
<p>How to fix it</p>
","2024-04-17 11:11:00","1","Question"
"78339960","","Keras tokenizer not appearing in import","<p>I'm trying to generate captions using a model I trained (.keras) and I'm following this instructions: <a href=""https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"" rel=""nofollow noreferrer"">Link</a>, I'm not following directly, I created and trained the model using Keras Image Captioning code example and saved using a function that GPT4 gave and it's working fine on that side.</p>
<p>Then on the generate captions, I tried GPT4 to give me some code examples but they don't work and I didn't understand so I did some research and found this <a href=""https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"" rel=""nofollow noreferrer"">Link</a>, and I jumped to generate Image Captioning and on the code it says I need to open a tokenizer:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = load(open('tokenizer.pkl', 'rb'))
</code></pre>
<p>after the debug it said to me (of course) that the file didn't exist, so I'm trying to create a tokenizer.pkl. And I can't create because I can't find the keras.preprocessing.text import Tokenizer</p>
<p>Trying to import the Tokenizer I realized that it can be on 2 directories, the <code>from keras.preprocessing.text import Tokenizer</code> or <code>from keras.legacy.preprocessing.text import Tokenizer</code>, and I have neither of them, my tensorflow version is: 2.17.0-dev20240410.</p>
<p>I've tried looking at the source code of Tokenizer and implement the Tokenizer function in my code but I doing that I can't found the API export:</p>
<pre class=""lang-py prettyprint-override""><code>from keras.api_export import keras_export
</code></pre>
<p>Thanks in advance.</p>
","2024-04-17 09:39:10","0","Question"
"78338619","78312867","","<p>It could be because the words in the lines may not be tab separated. I replaced it with 4 spaces and your code worked just fine for me otherwise it showed 'skipped' like you reported. Please ensure that they are tab separated indeed. To confirm, try:</p>
<p><code>city, latitude, longitude = line.strip().split('    ')</code> #4 spaces</p>
","2024-04-17 05:47:28","0","Answer"
"78338408","78228233","","<p>instead of using</p>
<pre><code>SimpleDirectoryReader(input_files=[&quot;./modals/llamaindex test/data/paul_graham_essay.txt&quot;])
</code></pre>
<p>you can use</p>
<pre><code>SimpleDirectoryReader(input_files=[&quot;./modals/llamaindex test/data)
</code></pre>
","2024-04-17 04:45:54","3","Answer"
"78336435","78025327","","<p>The problem lies with the Python version. The &quot;|&quot; operator in Python was introduced in Python 3.10 and is not available in Python 3.9.
Updating my Python interpreter to version 3.10, worked for me successfully.</p>
","2024-04-16 17:41:28","0","Answer"
"78332104","78331778","","<p>tlistc is created outside the for loop. Each iteration, you are mutating the same object.</p>
<p>You should instead created a new tlistc inside the for loop.</p>
","2024-04-16 04:24:26","0","Answer"
"78331778","","I am trying to create training data for a game in python. For that, i chose to create a two dimensional list. But list is not updating properly","<p>This is the part of the code i wrote:</p>
<pre><code>for e in range(episodes):
    qm=random.randint(0,5)
    env.game(action)
    fps.tick(1)
    screen.fill(bgcolor)
    s=env.get_state()
    rew=s[12]
    q1=rew+g*qm
    for k in range(12):
        tlistc[k]=s[k]
    tlistc[12]=action
    tlistc[13]=q1
    tlistm.append('a')
    tlistm[e]=tlistc
    if s[13]==1:
        env.game_reset()        
    action=random.randint(0,3)
    print(tlistm)
</code></pre>
<p><a href=""https://i.sstatic.net/oNUiA.jpg"" rel=""nofollow noreferrer"">This is what being returned to me</a></p>
<p>But after every iteration, all the items of the tlistm updates to tlistc, instead of only the last item.</p>
<p>Am i making some mistake or is there something i am missing?</p>
<p>I checked and rechecked my code, butcant seem to understand the problem..</p>
","2024-04-16 02:09:18","0","Question"
"78329900","78317989","","<p>It's <em>possible</em> (if you have the full word2vec model, which is more than just the word-vectors) to do additional training, Or, use existing vectors to initialize a model, that you then train further.</p>
<p>But, while this can be done, sometimes with beneficial results, there's no proven/robust recipe for the best way to do it.</p>
<p>Compared to training from scratch, when you're sure every word in the corpus had an equal chance to be co-trained into compatible positions alongside every other word, incremental fine-tuning requires a lot of extra choices about process &amp; relative weighting, &amp; careful attention to whether your specific choices are helping or hurting.</p>
<p>(In particular, if you have a large generic set of word-vectors, but then try to extend it with training over some new texts that <em>don't</em> cover all of its original vocabulary, the new words, and words that repeat, will continue to be pulled to new positions that are optimal <em>only for the new texts</em>, potentially losing their useful relative positioning with respect to other unmoved words not in the new texts. This might be survivable, if you don't do &quot;too much&quot; extra training! But it's a hard-to-quantify decay in the quality you want most – compatible word-vectors for all words of interest – directly trading off against learning your new words/texts.)</p>
<p>So rather than being as easy &quot;shortcut&quot; to an expanded set of word-vectors, as one might hope, fine-tuning can require extra complications &amp; work.</p>
<p>My impression is that two alternate approaches are better:</p>
<ul>
<li><p>Extend your corpus with other texts that should have compatible word-senses - perhaps making your triing corpus arbitrarily larger than your starting data, but ensuring that all words – from your starting texts, and the expanded corpus – get co-trained word-vectors covering all words of interest.</p>
</li>
<li><p>Train on your smaller corpus, then as a separate step <em>learn a projection</em> that moves your words into the coordinate space of a larger model whose full range of words you think good enough as a basis, using the 'anchoring points' of shared words.</p>
</li>
</ul>
<p>(This 2nd technique is briefly mentioned in the <a href=""https://arxiv.org/abs/1506.06726"" rel=""nofollow noreferrer"">Google &quot;skip-thoughts&quot; paper</a> (section 2.2 &quot;vocabulary expansion&quot;), and also used in <a href=""https://arxiv.org/abs/1412.6568"" rel=""nofollow noreferrer"">this paper on learning/improving interlanguage translations between word-vector sets</a>. There's a class inside the Gensim python project called <code>TranslationMatrix</code> that can learn such mappings, and also another project on PyPI called <code>transvec</code> that may work for such purposes.)</p>
","2024-04-15 16:46:15","0","Answer"
"78327028","78325075","","<p>To add files to gemini 1.5 for processing you must upload the file
To do this we use the <a href=""https://ai.google.dev/api/rest/v1beta/media/upload"" rel=""nofollow noreferrer"">media.upload</a> method.</p>
<p>To be clear videos are not supported.  What is supported is images.  What is recommend is that you take 1 frame a second and export it as an image then upload it to the file upload endpoint.  You can then use the files in your request to the AI.</p>
<p>Audio can be uploaded using the same method as uploading the images.</p>
<p>Then we can look at <a href=""https://ai.google.dev/tutorials/prompting_with_media"" rel=""nofollow noreferrer"">Prompting with media files</a> and here <a href=""https://ai.google.dev/docs/file_prompt_strategies"" rel=""nofollow noreferrer"">File prompting strategies</a></p>
<p>To send the actual url to the ai when you run your prompt is doine via the contents</p>
<p>you send the url that was returned when you uploaded the file.</p>
<pre><code>{
  &quot;contents&quot;:[
    {
      &quot;parts&quot;:[
        {&quot;text&quot;: &quot;What is this picture?&quot;},
        {
          &quot;inline_data&quot;: {
            &quot;mime_type&quot;:&quot;image/jpeg&quot;,
            &quot;file_data&quot;: {&quot;file_uri&quot;: file['uri'], &quot;mime_type&quot;: file['mimeType']}}
          }
        }
      ]
    }
  ]
}
</code></pre>
<p>Its not supported directly in the library yet but you can load it yourself manually.</p>
<p>See:</p>
<p><a href=""https://github.com/google-gemini/cookbook/blob/cf2dc7a3a851c5f553c2bf9ff884770719ca368d/quickstarts/file-api/sample.js"" rel=""nofollow noreferrer"">quickstarts/file-api/sample.js</a></p>
","2024-04-15 08:25:40","1","Answer"
"78325075","","I'm Unable to Upload Videos/audios in Google Gemini API 1.5Pro","<p>Can someone help me to use Google Gemini API1.5Pro model with Video/audio Processing?</p>
<p>I've already searched almost in all search engines, but can't find any resource to learn.
My requirement is to upload an audio file and ask the AI to summarize it.</p>
<p>I'm using Node.js with Google AI Studio(<code>@google/generative-ai</code>)
any suggestion will be highly valued!</p>
<p>I've tried the following:-</p>
<ul>
<li>I've installed the google generative package and explore with it
the Gemini AI API1.5Pro is working perfectly with text-only results and text-and/or-image results, but I couldn't find any way to explore with audio files and video files.</li>
</ul>
","2024-04-14 19:54:28","-2","Question"
"78325071","78323769","","<p>As the error indicates, the <code>Ollama</code> class does not have a <code>bind_functions()</code> method. In contrast, the <code>ChatOpenAI</code> class does have a <a href=""https://github.com/langchain-ai/langchain/blob/v0.1.16/libs/partners/openai/langchain_openai/chat_models/base.py#L730"" rel=""nofollow noreferrer""><code>bind_functions()</code></a> method.</p>
<p>Ollama doesn't natively support function calling, but an experimental <a href=""https://python.langchain.com/docs/integrations/chat/ollama_functions/"" rel=""nofollow noreferrer""><code>OllamaFunctions</code></a> wrapper class was created that gives it the same API as OpenAI Functions.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>from langchain_experimental.llms.ollama_functions import OllamaFunctions

model = OllamaFunctions(model=&quot;llama2&quot;)

model = model.bind(
    functions=[
        {
            &quot;name&quot;: &quot;route&quot;,
            &quot;description&quot;: &quot;Select the next role.&quot;,
            &quot;parameters&quot;: {
                &quot;title&quot;: &quot;routeSchema&quot;,
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {&quot;next&quot;: {&quot;title&quot;: &quot;Next&quot;, &quot;anyOf&quot;: [{&quot;enum&quot;: options}] }},
                &quot;required&quot;: [&quot;next&quot;],
            },
        }
    ],
    function_call={&quot;name&quot;: &quot;route&quot;},
)
</code></pre>
<p>This implementation may work for your function calling use case. But, as the documentation states, <em>&quot;more powerful and capable models will perform better with complex schema and/or multiple functions.&quot;</em></p>
<p>References:</p>
<ol>
<li><a href=""https://python.langchain.com/docs/integrations/chat/ollama_functions/"" rel=""nofollow noreferrer"">OllamaFunctions (LangChain)</a></li>
<li><a href=""https://github.com/langchain-ai/langchain/blob/v0.1.16/libs/community/langchain_community/llms/ollama.py"" rel=""nofollow noreferrer"">Ollama (GitHub)</a></li>
</ol>
","2024-04-14 19:52:16","3","Answer"
"78325018","78320486","","<p>Upon going through docs, first we need to run Ollama server by setting the host port and the allowed origins to communicate with it.</p>
<p>Run</p>
<pre><code>export OLLAMA_HOST=&quot;0.0.0.0:8888&quot; OLLAMA_ORIGINS=&quot;*&quot; ollama serve
</code></pre>
<p><code>*</code> for all if you want to use particular ip use <code>http://</code> or <code>https://</code> followed by the IP you want to allow.</p>
<p>Then start the Ollama server by</p>
<pre><code>ollama serve
</code></pre>
<p>Then run the API, such as</p>
<pre><code>curl http://&lt;pub-ip&gt;:8888/api/pull -d '{
  &quot;name&quot;: &quot;llama2&quot;
}'
</code></pre>
<p>To pull and image only once is enough (you can pull your desired model)</p>
<p>and</p>
<pre><code>curl http://&lt;pub-ip&gt;:8888/api/generate -d '{
  &quot;model&quot;: &quot;llama2&quot;,
  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;,
  &quot;response&quot;: &quot;The&quot;,
  &quot;done&quot;: false
}'
</code></pre>
","2024-04-14 19:32:17","2","Answer"
"78323769","","as i am running the code i am getting error that my ollama has no attribute bind_functions, the llm i am using is ollama model llama -2","<pre><code># Setup your supervisor, conditional logic, and integrate the nodes into the workflow
members = [&quot;Web_Searcher&quot;, &quot;Web_Searcher_Quality&quot;,&quot;Blog_Searcher&quot;, &quot;Blog_Searcher_Quality&quot;, &quot;Content_Writer&quot;]
system_prompt = (
    &quot;&quot;&quot;As a supervisor, your role is to oversee a dialogue between these
    workers: {members}. and excute all the agents one by one,
    determine which worker should take the next action. Each worker is responsible for
    executing a specific task and reporting back their findings and progress. Once all tasks are complete,
    indicate with 'FINISH'.&quot;&quot;&quot;
)

options = [&quot;FINISH&quot;] + members
function_def = {
    &quot;name&quot;: &quot;route&quot;,
    &quot;description&quot;: &quot;Select the next role.&quot;,
    &quot;parameters&quot;: {
        &quot;title&quot;: &quot;routeSchema&quot;,
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {&quot;next&quot;: {&quot;title&quot;: &quot;Next&quot;, &quot;anyOf&quot;: [{&quot;enum&quot;: options}] }},
        &quot;required&quot;: [&quot;next&quot;],
    },
}

prompt = ChatPromptTemplate.from_messages([
    (&quot;system&quot;, system_prompt),
    MessagesPlaceholder(variable_name=&quot;messages&quot;),
    (&quot;system&quot;, &quot;Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}&quot;),
]).partial(options=str(options), members=&quot;, &quot;.join(members))

manager_chain = (prompt | llm.bind_functions(functions=[function_def], function_call=&quot;route&quot;) | JsonOutputFunctionsParser())
</code></pre>
<p>i am expecting it to run, i dot know why i am getting stuck, this is the output i am getting.
flow
manager_chain = (prompt | llm.bind_functions(functions=[function_def], function_call=&quot;route&quot;) | JsonOutputFunctionsParser())
^^^^^^^^^^^^^^^^^^
AttributeError: 'Ollama' object has no attribute 'bind_functions'</p>
","2024-04-14 12:19:03","2","Question"
"78323755","78317989","","<p>As has been pointed out <a href=""https://stackoverflow.com/questions/76161758/fine-tune-a-custom-word2vec-model-with-gensim-4?rq=2"">before</a>, there is no &quot;go-to&quot; way for fine-tuning Word2Vec type models.</p>
<p>I would suggest training your own model from scratch, combining your data with other available data from a similar domain. Word2vec models are fairly quick to train and this would probably give you the best results. If you do not need static word-level embeddings, I would recommend considering contextualized embeddings, for example through the use of <a href=""https://sbert.net/"" rel=""nofollow noreferrer"">sentence-transformers</a> or similar frameworks, which has a wide selection of already pre-trained models you can choose from. You can fine-tune these types of models on your specific data rather easily, and there are tons of resources online on how to do that.</p>
<p>For your use case, you can embed all the documents into dense vector representations using the abovementioned library, and then construct a searchable index over this semantic space. In order to match queries, all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product, often referred to as a MIPS search. An example library to take a look at would be <a href=""https://github.com/facebookresearch/faiss"" rel=""nofollow noreferrer"">faiss</a>.</p>
","2024-04-14 12:15:53","1","Answer"
"78320486","","Trying to get API response from ollama setup in Azure virtual machine (ubuntu)","<p>I installed and configured the ollama on my Azure virtual machine running ubuntu and trying to make the API call from another machine, kind of like I'm trying to set up my own ollama server and facing issue with API connection.</p>
<p>I tried running the local host API such as:</p>
<pre><code>curl http://localhost:11434/api/generate -d '{
  &quot;model&quot;: &quot;llama2&quot;,
  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;,
  &quot;response&quot;: &quot;The&quot;,
  &quot;done&quot;: false
}'
</code></pre>
<p>This was successful,</p>
<p>I set up inbound rule to my VM port 11434, and tried to API call using the VM's public IP I got failed to connect: connection refused</p>
<p>Should I be using any password or authentication? Like what am I missing?</p>
<pre><code>curl http://&lt;public ip&gt;:11434/api/generate -d '{
  &quot;model&quot;: &quot;llama2&quot;,
  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;,
  &quot;response&quot;: &quot;The&quot;,
  &quot;done&quot;: false
}'
</code></pre>
","2024-04-13 11:50:14","2","Question"
"78319241","78316397","","<p>As the current another approach, how about using the PDF data by converting the images? Gemini 1.5 API can be used for analyzing the images. The flow is as follows.</p>
<ol>
<li>Convert PDF data to images (PNG and Jpeg).</li>
<li>Upload images to Gemini. <a href=""https://ai.google.dev/api/rest/v1beta/media"" rel=""nofollow noreferrer"">Ref</a></li>
<li>Generate content using the uploaded images. <a href=""https://ai.google.dev/api/rest/v1beta/models/generateContent"" rel=""nofollow noreferrer"">Ref</a></li>
</ol>
<p>In my case, I use this approach for parsing various invoices. <a href=""https://medium.com/google-cloud/parsing-invoices-using-gemini-1-5-api-with-google-apps-script-1f32af1678f2"" rel=""nofollow noreferrer"">Ref</a> I expect that PDF data will be able to be used in future updates.</p>
<h2>Updated on August 14, 2024</h2>
<p>In the current stage, the PDF data can be directly used with Gemini API. <a href=""https://stackoverflow.com/a/78781870/7108653"">Ref</a></p>
<p>The PDF data can be used as both <code>inlineData</code> as base64 and <code>file_data</code> as uri of the uploaded data to Gemini.</p>
","2024-04-13 00:33:30","3","Answer"
"78318637","78316397","","<p>the file api for gemini doesnt support pdf.</p>
<p>but what you can do is exactly what ai studio does and parse the pdf file as text and then just add it as part of your prompt</p>
<ol>
<li>read the file as text string</li>
<li>tell me about this text [dump string]</li>
</ol>
","2024-04-12 20:44:36","1","Answer"
"78317989","","Is it possible to fine-tune a pretrained word embedding model like vec2word?","<p>I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data.</p>
<p>However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model.</p>
<p>This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?</p>
","2024-04-12 17:57:56","0","Question"
"78316397","","Access PDF Files using the Gemini API","<p>I am trying to extract information from a pdf using the Gemini API (1.5-pro).
Right now it seems like the API can only access Text, Audio Files or Images. Is there any way for it to access PDF files (e.g. via Vertex AI or Google Drive uploads)?
The current documentations are quite intransparent.</p>
","2024-04-12 12:49:31","5","Question"
"78313047","78312158","","<p>Perhaps you could add an additional penalty to the loss that incentivises higher variance for feature 2.</p>
<p>One way of doing this is to directly incorporate <code>sigma_feature2</code> into the loss term, e.g. <code>loss_total = loss_original + lambda * 1 / sigma_feature2**2</code>, where <code>sigma_feature2</code> is the the variance parameter of the latent representation for feature 2.</p>
<p>Alternatively, you could add an activation loss term that looks at the output activations for feature 2, and adds <code>lambda * 1 / feature2_activations.var()</code> to the loss term.</p>
<p>Both methods above obtain a measure of the degree of variance related to feature 2, and calculate <code>1 / variance</code> before adding it to the loss. This means that when the variance is low, the loss increases and the model compensates by learning to increase the variance for that feature.</p>
<p><code>lambda</code> would need to be tuned to achieve a balance between accurate and normally-distributed reconstructions for feature 2 (small <code>lambda</code>), whilst still getting your desired level of variance for that feature (larger <code>lambda</code>).</p>
","2024-04-11 21:12:07","1","Answer"
"78312867","","Computing the Degree, Closeness, Eigenvector, Katz, PageRank, and Betweenness centralities and accepting input from a file","<p>i was trying to read from a text file in phyton but it's not doing it properly.</p>
<pre><code>def read_city_data(file_path):
    cities = {}
    with open(file_path, 'r') as file:
        next(file)  # Skip header line
        for line in file:
            try:
                city, latitude, longitude = line.strip().split('\t')
                if ' ' in city:
                    city = '&quot;{}&quot;'.format(city)
                cities[city] = (float(latitude), float(longitude))
            except ValueError:
                print(&quot;skipped&quot;)
                # Skip over lines with invalid format
                continue
    return cities
</code></pre>
<p>This was what how i was trying to read the text file, and this is the format of the text file:</p>
<pre><code>City    Latitude    Longitude
Oradea    47.0465005    21.9189438
Zerind    46.622511    21.517419
Arad    46.166667    21.316667
Timisoara    45.759722    21.23
Lugoj    45.68861    21.90306
Mehadia    44.904114    22.364516
Drobeta    44.636923    22.659734
Craiova    44.333333    23.816667000000052
Sibiu    45.792784    24.152068999999983
Rimnicu Vilcea    45.099675    24.369318
Fagaras    45.8416403    24.9730954
Pitesti    44.860556    24.867778000000044
Giurgiu    43.9037076    25.9699265
Bucharest    44.439663    26.096306
Urziceni    44.7165317    26.641121
Eforie    44.058422    28.633607
Hirsova    44.6833333    27.9333333
Vaslui    46.640692    27.727647
Iasi    47.156944    27.590278000000012
Neamt    47.2    26.3666667
</code></pre>
<p>But it's not working, it just keeps printing skipped indicating it's an exception.</p>
<p>I was also trying to write functions that compute the centralities; degree, closeness, Eigenvector, betweenness, Katz, and pagerank for a graph of cities, but I can't find their implementations anywhere, i can only access them from the imported networkx library. If anyone happens to know to do this please share.</p>
","2024-04-11 20:20:15","0","Question"
"78312158","","Give more weights to some input features for variational auto encoder","<p>I made a variation auto encoder to augment data.</p>
<p>It is working fine, but I would like some columns in the augmented data to have more variations  than the others.</p>
<p>For example, I actually have this data generated:</p>
<pre><code>100  16 2.6
105  16.6 2.7
110  16.7 2.8
</code></pre>
<p>You will tell me: it is normal, you don't have enough real input data in a big range for the second column around 105 value for the first column. That's true but my second column is important and here is what looks like my input data important :</p>
<pre><code>100 16 2.5
110 20 3.5
120 30 3.7
130 40 4 
200 80 7 
.....
</code></pre>
<p>I would like give more weight to the second column to vary in a full  range of the real input data from 1 to 100, and have a result like this:</p>
<pre><code>100 20  3
105 50  3.5
110 80  6
....
</code></pre>
<p>EDIT:
I tried it ... not really good :</p>
<pre><code>class customLoss(nn.Module):
    def __init__(self):
        super(customLoss, self).__init__()
        self.mse_loss = nn.MSELoss(reduction=&quot;sum&quot;)
    
    def forward(self, x_recon, x, mu, logvar):
        loss_MSE = self.mse_loss(x_recon, x)
        #   Kullback-Leibler (KL) divergence
        loss_KLD = -0.5 * torch.sum(1. + logvar - mu.pow(2) - logvar.exp())
        # Variance penalty for feature 2
        variance_penalty = torch.mean(logvar[:, 2])  # Average logvar for feature 2

        loss_original = loss_MSE + loss_KLD - variance_penalty * 2.
        return loss_original
</code></pre>
","2024-04-11 17:39:18","-4","Question"
"78311651","78311434","","<p>I think you're unpacking your train-test datasets wrongly.</p>
<ul>
<li>(Wrong) <code>x_train, x_test, y_test, y_train = train_test_split(x, y, test_size=0.2)</code></li>
<li>(Correct) <code>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)</code></li>
</ul>
","2024-04-11 15:58:21","0","Answer"
"78311434","","Data cardinality is ambiguous sklearn.train","<pre><code> model.fit(x_train, y_train, epochs=1000)
</code></pre>
<p>i'm trying to make a ai but mine code gives a error and i don't how to fix it?</p>
<p>this is the error</p>
<pre><code>ValueError: Data cardinality is ambiguous:
  x sizes: 455
  y sizes: 114
Make sure all arrays contain the same number of samples.

</code></pre>
<p>this is the whole code</p>
<pre><code>import pandas as pd
        dataset = pd.read_csv('cancer.csv')

x = dataset.drop(columns=[&quot;diagnosis(1=m, 0=b)&quot;])

y = dataset[&quot;diagnosis(1=m, 0=b)&quot;]

from sklearn.model_selection import train_test_split
        x_train, x_test, y_test, y_train = train_test_split(x, y, test_size=0.2)

import tensorflow as tf       
model = tf.keras.models.Sequential()

                                                                                                                      
model.add(tf.keras.layers.Dense(256, input_shape=x_train.shape, activation='sigmoid'))
               model.add(tf.keras.layers.Dense(256, activation='sigmoid'))
               model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentrop', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=1000)
</code></pre>
","2024-04-11 15:20:34","0","Question"
"78302644","78302444","","<p><strong>Yes</strong>, for two reasons:</p>
<ol>
<li><strong>Tokenization</strong>: &quot;I am&quot; vs. &quot;I'm&quot; lead to <strong>different tokenizations</strong>. BLEU scores are <strong>very sensitive</strong> to <strong>tokens</strong>. The different tokenization should lead to substantial impact.</li>
<li><strong>Short sentences</strong>: BLEU are notoriously <strong>unreliable</strong> for very <strong>short texts</strong>. The score will be disproportionately influenced by small differences if there isn't much text to begin with because there is a Brevity Penalty set in place to discourage the model from outputting fewer words and get a high score. Please take a look at the &quot;Brevity Penalty&quot; section in the following <a href=""https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b"" rel=""nofollow noreferrer"">article</a>.</li>
</ol>
<p>Hope you found the answer you are looking for 🤓.</p>
","2024-04-10 07:05:31","0","Answer"
"78302444","","Calculating BLEU score between candidate and reference sentences in python","<p>I am calculating BLEU score between 2 sentences which seem very similar to me but I am getting BLEU score as very low. Is it supposed to happen?</p>
<pre><code>prediction = &quot;I am ABC.&quot;
reference = &quot;I'm ABC.&quot;

from nltk.translate.bleu_score import sentence_bleu, corpus_bleu
from nltk.translate.bleu_score import SmoothingFunction
# Tokenize the sentences
prediction_tokens = prediction.split()
reference_tokens = reference.split()
   
# Calculate BLEU score
bleu_score = sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=SmoothingFunction().method4)

# Print the BLEU score
print(f&quot;BLEU score: {bleu_score:.4f}&quot;)

Output is 0.0725
</code></pre>
","2024-04-10 06:19:23","1","Question"
"78301934","78288542","","<p><code>Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [78,2], [batch]: [96,2]</code></p>
<p>The reason for this error is not related to your model architecture, but rather due to your data pipeline as the elements of your input data do not all have the <strong>exact same shape</strong> (this is required if you're batching your dataset - when you're using <code>.fit(train_dataset, epochs=200 , batch_size=128)</code>, or calling <code>train_dataset.batch(N)</code> explicitly).</p>
<p>Possible Solution</p>
<ol>
<li>Consider padding your data -
see <a href=""https://www.tensorflow.org/guide/data#batching_tensors_with_padding"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/data#batching_tensors_with_padding</a></li>
<li>Resize your data (if it make sense for your usecase - e.g: image data)</li>
<li>Use batch size of 1 (not recommended, because this would mean your SGD algorithm would have very high variance + training will be very slow as it doesn't fully utilise parallelization)</li>
</ol>
","2024-04-10 02:57:49","0","Answer"
"78300896","","Can I use waitress with 1 thread, and order the requests by a priority?","<p>I am currently using waittress with 1 thread because I am doing an AI inference API, however i would like to implement a priority queue to order the incomming requests.
Is there any way to make waitress resolve requests in a defined order, or is that not possible?</p>
<p>I've tried multiple thread and an internal queue, however I cant do this because 1 of the model seems to stop working when multiple threads are being executed at the same time
currently using</p>
<pre><code>waitress-serve --threads 1 --host 0.0.0.0 API:app
</code></pre>
","2024-04-09 20:12:42","0","Question"
"78300721","","Is it possible to add buttons that have an instant question for FlowiseAI's chatbot","<p>I love FlowiseAI but I'm not sure where I can ask this question.  Is it possible to add buttons that have an instant question?  Such as &quot;What are your business hours&quot;  or &quot;Do you have any specials&quot; to a FlowiseChatbot.  It would be great if such functionality could be added by reading an array or Google Sheet that could put that question into the prompt. Or if the instruction to dig into the Chatembed Github  <a href=""https://github.com/FlowiseAI/FlowiseChatEmbed"" rel=""nofollow noreferrer"">https://github.com/FlowiseAI/FlowiseChatEmbed</a>.</p>
<p>Like Shopify's online order chat at:</p>
<p><a href=""https://shop.wildrepublic.com/?syclid=ceac36fc-9300-44ee-9b6c-e8ec4d9ab3cb"" rel=""nofollow noreferrer"">https://shop.wildrepublic.com/?syclid=ceac36fc-9300-44ee-9b6c-e8ec4d9ab3cb</a></p>
<p>Shopify's documentation
<a href=""https://help.shopify.com/en/manual/inbox/chat-settings-and-appearance/instant-answers"" rel=""nofollow noreferrer"">https://help.shopify.com/en/manual/inbox/chat-settings-and-appearance/instant-answers</a></p>
","2024-04-09 19:32:01","0","Question"
"78299663","78299566","","<p>It does support conditional effects, yes, but it looks like you’re using a configuration that doesn’t. Probably the heuristic choice. You can see which support conditional effects over here: <a href=""https://www.fast-downward.org/Doc/Evaluator"" rel=""nofollow noreferrer"">https://www.fast-downward.org/Doc/Evaluator</a></p>
","2024-04-09 15:45:20","1","Answer"
"78299566","","Can Fast Downward planner deal with conditional effects like 'when'?","<p>I'm encountering an issue with a particular section of code.
This is the domain file:</p>
<pre><code>(define (domain gripper-strips)
   (:predicates (room ?r)
        (ball ?b)
        (gripper ?g)
        (at-robby ?r)
        (at ?b ?r)
        (free ?g)
        (carry ?o ?g)
        (world )
        (dfa )
        (q1)(q2)(q3)(q4)
        
)
        
(:action dfaact
       :parameters  ()
       :precondition (and (dfa ))
       :effect (and  (not(dfa )) (world ) 
         (when (and  (q1) (at ball1 roomb) (not(at ball4 roomb)) ) 
                       (and (q3) (not (q1)))
         ) 
         (when (and  (q1) (at ball1 roomb) (at ball4 roomb))
                (and (q4) (not(q1)))
         )
         (when (and  (q1) (at ball4 roomb) (not(at ball1 roomb)))
               (and  (q2) (not(q1)))
         )
         (when (and  (q3) (at ball1 roomb) (at ball4 roomb))
               (and  (q4) (not(q3)))
         )
         (when (and  (q3) (not(at ball1 roomb)))
               (and  (q2) (not(q3)))
         )

        )
)


   (:action move
       :parameters  (?from ?to)
       :precondition (and  (room ?from) (room ?to) (at-robby ?from) (world ))
       :effect (and  (at-robby ?to)
             (not (at-robby ?from))  (not(world )) (dfa ) ))


   (:action pick
       :parameters (?obj ?room ?gripper)
       :precondition  (and  (ball ?obj) (room ?room) (gripper ?gripper)
                (at ?obj ?room) (at-robby ?room) (free ?gripper)(world ))
       :effect (and (carry ?obj ?gripper)
            (not (at ?obj ?room)) 
            (not (free ?gripper))(not(world )) (dfa )))


   (:action drop
       :parameters  (?obj  ?room ?gripper)
       :precondition  (and  (ball ?obj) (room ?room) (gripper ?gripper)
                (carry ?obj ?gripper) (at-robby ?room)(world ))
       :effect (and (at ?obj ?room)
            (free ?gripper)
            (not (carry ?obj ?gripper))(not(world )) (dfa ))))
</code></pre>
<hr />
<p>And this is the problem file:</p>
<pre><code>(define (problem strips-gripper-x-1)
   (:domain gripper-strips)
   (:objects rooma roomb ball4 ball3 ball2 ball1 left right)
   (:init (q1)
      (dfa)
      (room rooma)
          (room roomb)
          (ball ball4)
          (ball ball3)
          (ball ball2)
          (ball ball1)
          (at-robby rooma)
          (free left)
          (free right)
          (at ball4 rooma)
          (at ball3 rooma)
          (at ball2 rooma)
          (at ball1 rooma)
          (gripper left)
          (gripper right))
   (:goal (and (q4)
        (at ball4 roomb)
               (at ball3 roomb)
               (at ball2 roomb)
               (at ball1 roomb))))
</code></pre>
<p>Then I use this command to run it:</p>
<pre><code>./fast-downward.py tests/gripper_ltlf/prob01.pddl --search &quot;astar(lmcut())&quot; 
</code></pre>
<p>And part of the error message is as follow:</p>
<pre><code>&quot;This configuration does not support conditional effects (operator dfaact)!  
Terminating.  
Tried to use unsupported feature.   
Peak memory: 23248 KB  
Remove intermediate file output.sas  
search exit code: 34&quot;
</code></pre>
<p>I try to solve a pddl problem using Fast Downward planner, and in the code I use the word &quot;when&quot;, but the information reminds that it does not support conditional effects (operator dfaact).
I doubt whether the search algorithm I used was wrong, but I just started to learn how to use it, so I still don't understand how to deal with it</p>
","2024-04-09 15:31:14","0","Question"
"78299496","78299375","","<p>As suggested, use a SmoothingFunction (take a look at <a href=""https://github.com/nltk/nltk/issues/1554"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/issues/1554</a>)</p>
<pre><code>prediction = &quot;I am ABC. I have completed my bachelor's degree in computer application at XYZ University and I am currently pursuing my master's degree in computer application through distance education.&quot;


reference = &quot;I'm ABC. I have finished my four-year certification in PC application at XYZ and I'm currently pursuing my graduate degree in PC application through distance training.&quot;

from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
# Tokenize the sentences
prediction_tokens = prediction.split()
reference_tokens = reference.split()
   
# Calculate BLEU score
bleu_score = sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=SmoothingFunction().method4)

# Print the BLEU score
print(f&quot;BLEU score: {bleu_score:.4f}&quot;)
</code></pre>
<p>BLEU score: 0.1379</p>
","2024-04-09 15:19:02","0","Answer"
"78299375","","Calculating BLEU score between two sumamries in python","<pre><code>prediction = &quot;I am ABC. I have completed my bachelor's degree in computer application at XYZ University and I am currently pursuing my master's degree in computer application through distance education.&quot;


reference = &quot;I'm ABC. I have finished my four-year certification in PC application at XYZ and I'm currently pursuing my graduate degree in PC application through distance training.&quot;

from nltk.translate.bleu_score import sentence_bleu

# Tokenize the sentences
prediction_tokens = prediction.split()
reference_tokens = reference.split()

# Calculate BLEU score
bleu_score = sentence_bleu([reference_tokens], prediction_tokens)

# Print the BLEU score
print(f&quot;BLEU score: {bleu_score:.4f}&quot;)
</code></pre>
<p>I am getting BLEU score as 0. I think I am making mistake somwhere. But not sure where.</p>
","2024-04-09 14:58:15","-1","Question"
"78294529","","Troubleshooting Azure Search: Embedding Data Not Populating in Index with Text Split and Azure Embedding Skill","<p>Thank you for your suggestion @JayashankarGS. I followed your advice and installed the beta version of @azure/search-documents using <code>npm i @azure/search-documents@12.1.0-beta.1</code>. After updating the library, I attempted to use the Text Split Skill and Azure Embedding Skill for our Azure Search Skillsets.</p>
<p>Unfortunately, we are still facing issues. The process runs without errors, but the embedding data is not being populated in the index tab, resulting in the vector field remaining empty. This issue is hindering the functionality of our search capabilities.</p>
<p><a href=""https://i.sstatic.net/AB6aC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AB6aC.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/6QWZf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6QWZf.jpg"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/LhFun.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LhFun.jpg"" alt=""enter image description here"" /></a></p>
<p>I have attached screenshots that illustrate the problem we are encountering. Could you please take a look and suggest what might be going wrong? We are keen on resolving this as soon as possible and would greatly appreciate any further assistance you can provide.</p>
","2024-04-08 18:38:09","0","Question"
"78293530","78027292","","<p>Please see the example actions available in the extension starter kit [1]. These could be extended by first adding a variable within Assistant that sets the common instructional prompt, then preface the call user message sent to the watsonx.ai model with the instructions from that variable.</p>
<p>[1] <a href=""https://github.com/watson-developer-cloud/assistant-toolkit/tree/master/integrations/extensions/starter-kits/language-model-watsonx"" rel=""nofollow noreferrer"">https://github.com/watson-developer-cloud/assistant-toolkit/tree/master/integrations/extensions/starter-kits/language-model-watsonx</a></p>
","2024-04-08 15:17:42","0","Answer"
"78292999","78288786","","<p>Answer from the support: &quot;The browsers need to stay on in order for notebooks to keep running. If you close the browser or Desktop application, then notebook will stop running&quot;</p>
","2024-04-08 13:49:43","1","Answer"
"78289518","78275988","","<p>I was indexing my data with the following code :</p>
<pre><code>es.index(
            index='{{index}}',
            document=json.dumps(page.__dict__),
            error_trace=True
        )
</code></pre>
<p>And with this new code it works :</p>
<pre><code> es.index(
            index='{{index}}',
            document={
                &quot;id&quot;: page.id,
                &quot;title&quot;: page.title,
                &quot;title_embeddings&quot;: page.title_embeddings,
                ...
            },
            error_trace=True
        )
</code></pre>
","2024-04-07 22:41:04","0","Answer"
"78288786","","How do I keep paperspace's gradients' notebooks running after I close my tabs","<p>I am running notebooks on paperspace's gradient. When I book a machine for 4 hours and start running a jupyter notebook in it, if I close my browser the execution stops. How can I change this behaviour? I did not have this issue on google collab pro</p>
<p>I tried free and paid GPU machines and I tried upgrading to Pro account</p>
","2024-04-07 17:58:26","0","Question"
"78288542","","How to create CNN-LSTM architecture?","<p>I try to create hybrid CNN and LSTM model. I had issue related to the shape of the architecture. This lead to the epoch could not run through the data 200 times.</p>
<p>My datasize is (96,2)</p>
<p>ERROR:</p>
<pre><code>Epoch 1/200
    178/Unknown 9s 34ms/step - loss: 1.2366 - mse: 5.4560
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[40], line 4
      2 is_train = True
      3 if is_train:
----&gt; 4    model_create.fit(train_dataset, epochs=200 , batch_size=128)

Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [78,2], [batch]: [96,2]
     [[{{node IteratorGetNext}}]] [Op:__inference_one_step_on_iterator_23678]
</code></pre>
<p>CNN-LSTM model :</p>
<pre><code>def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D(filters=64,
                               kernel_size=3,
                               activation='relu',
                               input_shape=input_data_shape),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=&quot;same&quot;),
        tf.keras.layers.Conv1D(filters=64,
                               kernel_size=3,
                               activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=&quot;same&quot;),    
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model_cnn

</code></pre>
<p>Compile model</p>
<pre><code>def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  metrics=[&quot;mse&quot;])
    return model_create

model_create = create_model()

model_create.summary()

model_create.fit(train_dataset, epochs=200 , batch_size=128)

</code></pre>
<p>I had try to add reshape before the flatten () function to change the shape. I also decrease the batch size and epoch size. None of this work. How could fit my model with train_data?</p>
","2024-04-07 16:34:57","0","Question"
"78287134","","Using OpenAI API for answering questions about csv file","<p>I'm starting with OpenAI API and experimenting with langchain. I have a .csv file with approximately 1000 rows and 85 columns with string values. I found some beginner article that I followed and have a colab notebook with the following code:</p>
<pre><code>txt_file_path = '/content/drive/My Drive/Colab Notebooks/preprocessed_data_10.csv'

with open(txt_file_path, 'r', encoding=&quot;utf-8&quot;) as file:
  data = file.read()

txt_file_path = '/content/drive/My Drive/Colab Notebooks/preprocessed_data_10.csv'
loader = TextLoader(file_path=txt_file_path, encoding=&quot;utf-8&quot;)
data = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
data = text_splitter.split_documents(data)

embeddings = OpenAIEmbeddings()

vectorstore = FAISS.from_documents(data, embedding=embeddings)


llm = ChatOpenAI(temperature=0.7, model_name=&quot;gpt-4&quot;)
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

conversation_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    chain_type=&quot;stuff&quot;,
    retriever=vectorstore.as_retriever(),
    memory=memory
)

query = &quot;question&quot;
result = conversation_chain({&quot;question&quot;: query})
answer = result[&quot;answer&quot;]
answer
</code></pre>
<p>The errors I got were:</p>
<pre><code>Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-xxx on tokens per min (TPM): Limit 10000, Requested 139816.
</code></pre>
<p>and</p>
<pre><code>BadRequestError: Error code: 400 - {'error': {'message': &quot;This model's maximum context length is 8192 tokens. However, your messages resulted in 32045 tokens.
</code></pre>
<p>I tried to figure out how big csv file I can feed it with and reduced the file for 10 rows and 53 columns.</p>
<p>What are possible workarounds so I can search on entire csv file?</p>
","2024-04-07 08:27:26","2","Question"
"78285033","78284882","","<p>It looks like you're using a previous version of the cv2 library, which still had the <code>findPosition</code> method (See <a href=""https://stackoverflow.com/a/69300598/5459839"">here</a>). But when looking at what you get as output (<code>([], [])</code>), and how that method is used <a href=""https://pythonrepo.com/repo/cvzone-cvzone"" rel=""nofollow noreferrer"">here in a basic example</a>:</p>
<pre><code>lmList, bbox = detector.findPosition(img)
</code></pre>
<p>...and also how it is used in <a href=""https://stackoverflow.com/q/69296432/5459839"">this Stack Overflow question</a>, it looks like you should not assign the returned value to <code>lmList</code>, but assign the first tuple member to <code>lmList</code>.</p>
<p>So change this line:</p>
<pre><code>lmList = detector.findPosition(img, draw=False)
</code></pre>
<p>to:</p>
<pre><code>lmList,_ = detector.findPosition(img, draw=False)
</code></pre>
<p>I guess the video was made with a still earlier version of the module, where the return value was just the list, as we can clearly see in the video the output of a list, not a tuple of two lists, like you are getting. The above change should overcome that difference.</p>
","2024-04-06 16:03:21","1","Answer"
"78284882","","Finger counting using hand tracking: tuple index out of range: what causes this?","<p>I'm following the video of Murtaza's Workshop - Robotics and AI (<a href=""https://www.youtube.com/watch?v=p5Z_GGRCI5s&amp;ab_channel=Murtaza%27sWorkshop-RoboticsandAI"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=p5Z_GGRCI5s&amp;ab_channel=Murtaza%27sWorkshop-RoboticsandAI</a>)
and I get an error:</p>
<pre class=""lang-none prettyprint-override""><code>INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
([], [])
Traceback (most recent call last):
  File &quot;C:\Users\thepr\PycharmProjects\pythonProject2\finger.py&quot;, line 22, in &lt;module&gt;
    if lmList[tipIds[id]][2] &lt; lmList[tipIds[id] - 2][2]:
       ~~~~~~^^^^^^^^^^^^
IndexError: tuple index out of range

Process finished with exit code 1
</code></pre>
<p>This is my code:</p>
<pre><code>import cv2
import time

from cvzone.HandTrackingModule import HandDetector
##import HandTrackingModule as htm

wcam, hcma = 640 ,480
cap = cv2.VideoCapture(0)
cap.set(3, wcam)
cap.set(4, hcma)
pTime = 0
detector = HandDetector(detectionCon=0.7, maxHands=5)
tipIds = [4,8,12,16,20]
while True:
    sccuess, img = cap.read()
    img = detector.findHands(img)
    lmList = detector.findPosition(img, draw=False)
    print(lmList)
    if len(lmList) != 0:
        fingers = []
        for id in range (0,5):
           if lmList[tipIds[id]][2] &lt; lmList[tipIds[id] - 2][2]:
               fingers.append(1)
           else:
               fingers.append(0)


    cTime = time.time()
    fps = 1 / (cTime - pTime)
    pTime = cTime
    cv2.putText(img,f'fps: {int(fps)}',(70,40), cv2.FONT_HERSHEY_PLAIN,3,(255,255,0),3)
    cv2.imshow(&quot;imgge&quot;,img)
    cv2.waitKey(1)
</code></pre>
<p>Why do I get this error, when I have followed the instructions from the video?</p>
<p>i tried something new from gpt and</p>
<pre><code>new error
[]
[]
Traceback (most recent call last):
  File &quot;C:\Users\thepr\PycharmProjects\pythonProject2\finger.py&quot;, line 21, in &lt;module&gt;
    if lmList[8][2] &lt; lmList[6][2]:
       ~~~~~~~~~^^^
IndexError: list index out of range
[[207, 319], [256, 314], [297, 291], [320, 262], [342, 238], [278, 215], [300, 174], [314, 150], [326, 129], [250, 201], [268, 155], [281, 128], [293, 107], [221, 200], [232, 153], [242, 126], [253, 104], [191, 209], [189, 169], [192, 144], [198, 121]]

Process finished with exit code 1
</code></pre>
<p>changed code</p>
<pre><code>    if len(lmList)!=0 :
        fingers = []
        for id in range(0, 5):
            if lmList[tipIds[id]][2] &lt; lmList[tipIds[id] - 2][2]:
                fingers.append(1)
            else:
                fingers.append(0)
    else:
        fingers = [0, 0, 0, 0, 0]  # Default value when lmList does not have enough elements```

</code></pre>
","2024-04-06 15:19:54","0","Question"
"78281906","78281880","","<p>The properties of the chat completion object (<a href=""https://platform.openai.com/docs/api-reference/chat/object"" rel=""nofollow noreferrer"">docs</a>) should be referenced as properties, not using dict syntax. Replace</p>
<pre><code>token_dict = {
  'prompt_tokens':response['usage']['prompt_tokens'],
  'completion_tokens':response['usage']['completion_tokens'],
  'total_tokens':response['usage']['total_tokens'],
}
</code></pre>
<p>with</p>
<pre><code>token_dict = {
  'prompt_tokens': response.usage.prompt_tokens,
  'completion_tokens': response.usage.completion_tokens,
  'total_tokens': response.usage.total_tokens,
}
</code></pre>
","2024-04-05 19:03:56","-1","Answer"
"78281880","","""TypeError: 'ChatCompletion' object is not subscriptable"" error","<p>Running next code and got an error. Help me please to fix! Thanks in advance</p>
<p>Here is my code:</p>
<pre><code>def get_completion_and_token_count(messages, 
                                   model=&quot;gpt-3.5-turbo&quot;, 
                                   temperature=0, 
                                   max_tokens=500):
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature, 
        max_tokens=max_tokens,
    )
    
    content = response.choices[0].message.content
    
    token_dict = {
'prompt_tokens':response['usage']['prompt_tokens'],
'completion_tokens':response['usage']['completion_tokens'],
'total_tokens':response['usage']['total_tokens'],
    }

    return content, token_dict
 
messages = [
{'role':'system', 
 'content':&quot;&quot;&quot;You are an assistant who responds\
 in the style of Dr Seuss.&quot;&quot;&quot;},    
{'role':'user',
 'content':&quot;&quot;&quot;write me a very short poem \ 
 about a happy carrot&quot;&quot;&quot;},  
] 
response, token_dict = get_completion_and_token_count(messages)
</code></pre>
<p>And this is the result I got after running the code above.</p>
<pre><code>TypeError: 'ChatCompletion' object is not subscriptable
</code></pre>
<p><a href=""https://i.sstatic.net/4akgK.png"" rel=""nofollow noreferrer"">error</a></p>
<p>I am expecting the issue is fixed. I tried to uninstall/install openai, changed model to 4, but it did not work unfortunately</p>
","2024-04-05 18:57:08","0","Question"
"78281128","78271686","","<p>It's seems, you must to do a function for clear it</p>
<p>Like that:</p>
<pre><code>import streamlit as st

def clear_text():
    st.session_state[&quot;text&quot;] = &quot;&quot;

input = st.text_input(&quot;text&quot;, key=&quot;text&quot;)    
st.button(&quot;clear text input&quot;, on_click=clear_text)
st.write(input)
</code></pre>
<p>As is explain here: <a href=""https://discuss.streamlit.io/t/clear-the-text-in-text-input/2225/12"" rel=""nofollow noreferrer"">streamlit issue</a></p>
","2024-04-05 16:01:47","0","Answer"
"78275988","","dense_vector becomes a field of type ""float""","<p>I try to implement a search engine using elasticsearch. For that i created an index with a mapping that contains dense_vector fields.
PUT {{elastic uri}}/{{index}}</p>
<pre><code>{
    &quot;mappings&quot;: {
        &quot;properties&quot;: {
            ...
            &quot;title_embeddings&quot;: {
                &quot;type&quot;: &quot;dense_vector&quot;,
                &quot;index&quot;: true,
                &quot;similarity&quot;: &quot;cosine&quot;,
                &quot;dims&quot;: 1024,
                &quot;index_options&quot;: {
                    &quot;type&quot;: &quot;hnsw&quot;,
                    &quot;ef_construction&quot;: 100,
                    &quot;m&quot;: 16
                }
            },
            &quot;title&quot;: {
                &quot;type&quot;: &quot;text&quot;
            },
            ...
        }
    }
}
</code></pre>
<p>The problem is that when i retrieve it using the API, the field is of type &quot;float&quot; instead of dense_vector.</p>
<p>GET {{elastic-uri}}/{{index}}/_search</p>
<pre><code>{
    &quot;{{index}}&quot;: {
        &quot;mappings&quot;: {
            &quot;properties&quot;: {
                ...
                &quot;title&quot;: {
                    &quot;type&quot;: &quot;text&quot;,
                    &quot;fields&quot;: {
                        &quot;keyword&quot;: {
                            &quot;type&quot;: &quot;keyword&quot;,
                            &quot;ignore_above&quot;: 256
                        }
                    }
                },
                &quot;title_embeddings&quot;: {
                    &quot;type&quot;: &quot;float&quot;
                }
                ...
            }
        }
    }
}
</code></pre>
<p>Also i can't use the KNN search on it since it's not the correct type :</p>
<p>GET {{elastic-uri}}/{{index}}/_search</p>
<pre><code>{
  &quot;knn&quot;: {
    &quot;field&quot;: &quot;title_embeddings&quot;,
    &quot;query_vector&quot;: [0.1, 3.2, 2.1],
    &quot;k&quot;: 2,
    &quot;num_candidates&quot;: 100
  }
}
</code></pre>
<p>Error : failed to create query: [knn] queries are only supported on [dense_vector] fields</p>
","2024-04-04 18:43:30","0","Question"
"78271686","","""Text Input in Streamlit Removes Entered Text Upon Hitting Enter Key: How to Fix?""","<pre><code>the input stored in user_question i need when press enter the txt in textbox getting clear and the output displing what can i did i tried js and streamlit function but when enter input and press send btn or press enter no output displaying 
</code></pre>
<p><a href=""https://i.sstatic.net/MheP5.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>here is an example of an image entered word pressed, and enter<br />
textbox didnot not get clear</p>
","2024-04-04 04:31:58","-2","Question"
"78270145","78269358","","<p>Note you are using an abandoned repo.  That fork of Darknet/YOLO hasn't been updated for several years now.  (See here:  <a href=""https://www.ccoderun.ca/programming/yolo_faq/#which_repos_to_use_and_avoid"" rel=""nofollow noreferrer"">https://www.ccoderun.ca/programming/yolo_faq/#which_repos_to_use_and_avoid</a>)</p>
<p>Instead, you should use the modern Hank.ai fork, and you should also update your training command.  See the instructions in this repo:  <a href=""https://github.com/hank-ai/darknet"" rel=""nofollow noreferrer"">https://github.com/hank-ai/darknet</a></p>
<p>The <a href=""https://discord.gg/zSq8rtW"" rel=""nofollow noreferrer"">Darknet/YOLO discord server</a> has a channel called #darknet-and-google-colab which has several notebooks with example code to build and use the new version of Darknet/YOLO on colab.  You can use those as templates to get started and train your network.</p>
","2024-04-03 19:44:32","0","Answer"
"78269358","","Yolov4 and Darknet Training with Google Colab Error","<p>I'm trying to train a model using YOLOv4 and Darknet. I'm using Google Colab for training during this process. However, everything was going smoothly until I encountered an error somewhere, and I can't figure it out.
.
There is no issue with my file path commands; I've checked them multiple times.
.
!./darknet detector train YOLO/objects.data custom_yolov4.cfg yolov4.conv.137 -dont_show
.
I'm running this command.
.
./darknet: error while loading shared libraries: libcuda.so.1: cannot open shared object file: No such file or directory
.
I'm getting this error.
<a href=""https://i.sstatic.net/JNFlM.png"" rel=""nofollow noreferrer"">enter image description here</a>
.
I'm expecting my issue to be resolved as soon as possible and for assistance to be provided to me.</p>
","2024-04-03 17:09:40","1","Question"
"78267429","78265967","","<p>The specific task you are looking to do here is called <em>forced alignment</em>. This link is a good collection of tools you can explore to do forced alignment. It includes some Python tools:</p>
<p><a href=""https://github.com/pettarin/forced-alignment-tools"" rel=""nofollow noreferrer"">https://github.com/pettarin/forced-alignment-tools</a></p>
","2024-04-03 11:50:11","1","Answer"
"78265967","","How to add timestamps to transcript file in accordance to audio file? (result is srt file)","<p>I use speech to text api to make srt files for audio\video (subtitles with timestamps) using python script. But its not 100% accurate. I have transcript for audio file which is accurate (it has some unnecessary lines). How to add timestamps to transcript in accordance to timestamps from audio so result will be srt file with lines from transcript and timestamps from audio?</p>
<p>I use api to make srt file and timestamps are very good accuracy, but text sometimes not.
So input is transcript (just text) with accurate lines and some unnecessary lines
Output is srt file where transcript lines in accordance to timestamps from audio (api making timestamps).</p>
<p>So basically i need some python code which adding lines from input transcript to generated using API timestamps. May be thats possible by comparing lines from transcript and lines from transcribed audio and then replacing it if it has high accuracy. Thanks.</p>
","2024-04-03 07:30:38","1","Question"
"78255048","78255027","","<p>LLMs in general are text-to-text models but there are &quot;multi-modal&quot; models (like chatGPT4, Gemini 1.5 Pro and others) which can accept more methods of input (like images, audio, video, etc.).
For your use case it seems like you can either use one of the above models with the audio directly or use a speech-to-text (like whisper) as a preprocessing step before passing the text to a text based model.</p>
","2024-04-01 10:54:05","0","Answer"
"78255027","","Do some LLMs understand the voice directly, or do they have to go through a text transcription stage?","<p>I want to interact with an LLM via voice.
In order to select the right model, I'd like to know if there are LLMs that understand voice directly.
If not, I'll have to transcribe the user's voice into text and the model's response into audio.</p>
<p>Thanks for your help.</p>
","2024-04-01 10:49:29","0","Question"
"78254663","78250851","","<p>I have same problem. I fixed but I dont know node.js nicely. :D</p>
<p>Changes I made:</p>
<pre><code>setChatHistory((oldChatHistory) =&gt; [
  ...oldChatHistory,
  {
    role: &quot;user&quot;,
    parts: [{ text: value }],
  }, {
    role: &quot;model&quot;,
    parts: [{ text: data }],
  },
]);
</code></pre>
<p>At the above code, I checked the Google Gemini document and I saw part field. The parts field is an array in the document and I changed my code like the document. And Then..</p>
<pre><code>{ chatItem.role } : { chatItem.parts[0].text }
</code></pre>
<p>In the above code, I got the answer and I wrote the answer on my project.</p>
<p>I hope, I have been help you.</p>
","2024-04-01 09:33:49","3","Answer"
"78250851","","Node.js Chatbot Error: GoogleGenerativeAIError - Content should have 'parts' property with an array of Parts","<p>I'm building a Node.js chatbot using the @google/generative-ai library. I'm encountering an error when handling the chat history between the user and the model.</p>
<p>Here's a breakdown of the problem:</p>
<ul>
<li>First Query Success: The initial user query works fine, likely because the history sent to the model is empty.</li>
<li>Second Query Error: Subsequent user queries result in the following error:
GoogleGenerativeAIError: [GoogleGenerativeAI Error]: Content should have 'parts' property with an array of Parts</li>
</ul>
<p>This suggests the model expects a specific format for the history object, which might not be being sent correctly.</p>
<p>Relevant Code Snippets:</p>
<p>App.js (Client-side):</p>
<pre><code>const getResponse = async () =&gt; {
    if (!value) {
      setError(&quot;Please enter something!&quot;);
      return;
    }
    try {
      const options = {
        method: &quot;POST&quot;,
        body: JSON.stringify({
          history: chatHistory,
          message: value,
        }),
        headers: {
          &quot;Content-Type&quot;: &quot;application/json&quot;,
        },
      };
      const response = await fetch(&quot;http://localhost:8000/gemini&quot;, options);
      const data = await response.text();
      console.log(data);
      setChatHistory((oldChatHistory) =&gt; [
        ...oldChatHistory,
        {
          role: &quot;user&quot;,
          parts: value,
        },
        {
          role: &quot;model&quot;,
          parts: data,
        },
      ]);
      setValue(&quot;&quot;);
    } catch (error) {
      console.log(error);
      setError(&quot;Something went wrong!&quot;);
    }
  };
</code></pre>
<p>server.js (Server-side):</p>
<pre><code>
app.post(&quot;/gemini&quot;, async (req, res) =&gt; {
  const model = genAI.getGenerativeModel({ model: &quot;gemini-pro&quot; });
  const chat = model.startChat({
    history: req.body.history,
  });
  const msg = req.body.message;
  const result = await chat.sendMessage(msg);
  const response = await result.response;
  const text = response.text();
  res.send(text);
});

</code></pre>
<p>Additional Information:</p>
<p>Node.js version: 18.17.0
@google/generative-ai version: 0.3.1</p>
<p>What I'm Asking:</p>
<ul>
<li>How can I ensure the history object sent to the model is formatted correctly with the parts property as an array of Parts?</li>
<li>Are there any specific considerations for handling empty history on the server-side?</li>
<li>Any other suggestions to resolve the &quot;Content should have 'parts' property with an array of Parts&quot; error.</li>
</ul>
","2024-03-31 09:28:53","0","Question"
"78249523","78224233","","<p>Tensorflow 2.16.1 has a bug. Please downgrade to 2.13.1:</p>
<pre><code>pip uninstall tensorflow
pip install tensorflow==2.13.1
</code></pre>
","2024-03-30 21:16:30","1","Answer"
"78244722","78244438","","<p>it seems like you made a mistake in your indentation starting at the for loop. Python indentation determines the scope where code runs, message cannot be found because it is out of scope (because it's missing a level of indentation)</p>
<p>i believe indenting 20 to 28 and unindenting the rest after should fix your issue</p>
<pre><code>async def on_message(self, message):
    print(message.content)
    if message.author == self.user:
        return
    command, user_message=None, None

    for text in ['/ai', '/bot', 'chatgpt']:**
        if message.content.startswith(text):
            command = message.content.split('')[0]
            user_message = message.content.replace(text, '')
            print(command, user_message)

        if command == '/ai' or command == '/bot' or command == '/chatgpt':
            bot_response = chatgpt_response(prompt=user_message)
            await message.channel.send(f&quot;Answer: {bot_response}&quot;)
</code></pre>
","2024-03-29 15:56:42","0","Answer"
"78244438","","Why does this error keep showing, what am i missing? await message.channel.send(f""Answer: {bot_response}"") IndentationError: unexpected indent","<p>So my code is basically copied from a walktrough for making a discord bot use chatgpt.</p>
<p>Im using Pycharm for the project.</p>
<p>This would be the main - code:</p>
<pre><code>from typing import Final
import os
import discord
from dotenv import load_dotenv
from chatgpt_ai.openai import chatgpt_response

load_dotenv()
TOKEN: Final[str] = os.getenv('DISCORD_TOKEN')

class MyClient(discord.Client):
    async def on_ready(self):
        print(&quot;Successfully logged in as: &quot;, self.user)

    async  def on_message(self, message):
        print(message.content)
        if message.author == self.user:
            return
        command, user_message=None, None

    **for text in ['/ai', '/bot', 'chatgpt']:**
        if message.content.startswith(text):
            command = message.content.split('')[0]
            user_message = message.content.replace(text, '')
            print(command, user_message)

        if command == '/ai' or command == '/bot' or command == '/chatgpt':
            bot_response = chatgpt_response(prompt=user_message)
            await message.channel.send(f&quot;Answer: {bot_response}&quot;)

    intents = discord.Intents.default()
    intents.message_content = True

    client = MyClient(intents=intents)




Terminal error code: 

File &quot;C:\Users\Flavi\PycharmProjects\discord_flavio_bot\discord_bot\main.py&quot;, line 28
    await message.channel.send(f&quot;Answer: {bot_response}&quot;)
IndentationError: unexpected indent
</code></pre>
<p>whats also weird - is that from line<code>**for text in ['/ai', '/bot', 'chatgpt']**:</code></p>
<p>everly line with &quot;message&quot; is marked as error with suggestions to import from &quot;email&quot;....</p>
<p>I tried various things, checked my code for errors, spellchecking and see if i missed something within the walktrough. But also its my very first attempt on a python project so i might just be to inexcpirienced to even know where to look for the error.<a href=""https://i.sstatic.net/EKMjY.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","2024-03-29 14:59:21","0","Question"
"78238614","78237430","","<p>There isn't a direct integration between OpenAI's GPT Models and Team Foundation Server, not that I am aware off. But to utilize GPT-4 for gaining insights from TFS data, you would generally need to build a custom integration.</p>
<p>You will use the TFS/Azure Rest API to programatically retrieve the data you are interested in. Then the data will be processed and cleaned so unnecessary data will not be sent to GPT. Once it is done you can send the data to GPT API using gpt-4-0125-preview which has context window of 128000 Tokens can generate output upto 4096 tokens</p>
","2024-03-28 13:41:10","0","Answer"
"78237430","","Integrating GPT-4 with Team Foundation Server for Data Insights","<p>I'm exploring options to integrate GPT-4, the latest version of OpenAI's powerful language model, with Team Foundation Server (TFS). My objective is to leverage GPT-4's capabilities to gain insights from the data stored within our TFS environment.</p>
<p>I'm wondering if there are any APIs or existing integrations available that facilitate this process. Specifically, I'm interested in extracting data from TFS and feeding it into GPT-4 for analysis and generating insights.</p>
<p>Could anyone provide guidance on whether such an API or integration exists? If not, are there any alternative approaches or workarounds that could achieve similar results?</p>
<p>Any advice, resources, or experiences related to integrating GPT-4 with TFS for data analysis and insights?</p>
<p>I tried exporting the data from TFS and then feeding the same data to Chat GPT, but exported data doe s not contain all the required details and also exported data is very huge, so it crosses the prompt limit. My expectation is to have an API, which can directly be integrated with TFS server. Like we can integrate 365 copilot with office and GitHub co - pilot with codebase.</p>
","2024-03-28 10:19:55","-1","Question"
"78233058","78196301","","<p>I was able to do that creating a custom brute force class modifying the <code>Bruteforce</code> original class:</p>
<pre><code>@tf.keras.saving.register_keras_serializable(package=&quot;MyLayers&quot;)
class BruteForce2(tf.keras.layers.Layer):
  &quot;&quot;&quot;Brute force retrieval.&quot;&quot;&quot;

  def __init__(self, model, k=5, **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.k = k
        self._k = k
        
  def _compute_score(self, queries: tf.Tensor,
                     candidates: tf.Tensor) -&gt; tf.Tensor:
    
        return tf.matmul(queries, candidates, transpose_b=True)

  
  # @tf.function
  def call(self, queries: tf.Tensor, candidates_raw: tf.data.Dataset, k):
    
    candidates = tf.data.Dataset.zip(candidates_raw.map(lambda x: 
x['id']), 
                                           
candidates_raw.map(final_model.candidate_model)
                                          )
    
    spec = candidates.element_spec

    if isinstance(spec, tuple):
      identifiers_and_candidates = list(candidates)
      candidates = tf.concat(
          [embeddings for _, embeddings in identifiers_and_candidates],
          axis=0
      )
      identifiers = tf.concat(
          [identifiers for identifiers, _ in identifiers_and_candidates],
          axis=0
      )
    else:
      candidates = tf.concat(list(candidates), axis=0)
      identifiers = None
    
    self._candidates = candidates

    if identifiers is None:
      identifiers = tf.range(candidates.shape[0])
    if tf.rank(candidates) != 2:
      raise ValueError(
          f&quot;The candidates tensor must be 2D (got {candidates.shape}).&quot;)
    if candidates.shape[0] != identifiers.shape[0]:
      raise ValueError(
          &quot;The candidates and identifiers tensors must have the same 
number of rows &quot;
          f&quot;(got {candidates.shape[0]} candidates rows and 
{identifiers.shape[0]} &quot;
          &quot;identifier rows). &quot;
      )
    # We need any value that has the correct dtype.
    identifiers_initial_value = tf.zeros((), dtype=identifiers.dtype)
    self._identifiers = self.add_weight(
        name=&quot;identifiers&quot;,
        dtype=identifiers.dtype,
        shape=identifiers.shape,
        initializer=tf.keras.initializers.Constant(
            value=identifiers_initial_value),
        trainable=False)
    self._candidates = self.add_weight(
        name=&quot;candidates&quot;,
        dtype=candidates.dtype,
        shape=candidates.shape,
        initializer=tf.keras.initializers.Zeros(),
        trainable=False)
    self._identifiers.assign(identifiers)
    self._candidates.assign(candidates)
    # self._reset_tf_function_cache()
    

    k = k if k is not None else self._k

    if self._candidates is None:
      raise ValueError(&quot;The `index` method must be called first to &quot;
                       &quot;create the retrieval index.&quot;)

    if self.model.query_model is not None:
      queries = self.model.query_model(queries)

    scores = self._compute_score(queries, self._candidates)

    values, indices = tf.math.top_k(scores, k=k)

    return values, tf.gather(self._identifiers, indices)
</code></pre>
","2024-03-27 16:04:15","0","Answer"
"78228233","","How can i import the document in Llamaindex","<p>I am trying to work with llama index and constantly i coming across the error of loading the document.
#Load and index the data with the simpleDirectoryReader of Llama Index
reader = SimpleDirectoryReader(input_files=[&quot;./modals/llamaindex test/data/paul_graham_essay.txt&quot;])
docs = readr.load_data()
print(f&quot;Loaded{len(docs)}docs&quot;) this is my code.
<a href=""https://i.sstatic.net/Ia9BI.png"" rel=""nofollow noreferrer"">Basically the error says document is not in the desired location and dont know the exact reason why is it so.</a></p>
<p>I am trying to provide the llama index with the document and then ask question regarding that. I am strictly following the website and completely new with this AI and coding.</p>
","2024-03-26 21:22:37","-1","Question"
"78227647","78169334","","<p>I encountered the error <code>ValueError: Unable to synchronously create dataset (name already exists)</code> when trying to save my model thus:</p>
<pre class=""lang-py prettyprint-override""><code>my_model.save('filename.h5')
</code></pre>
<p>...but when I used <code>keras</code> for a file extension, the save succeeded:</p>
<pre class=""lang-py prettyprint-override""><code>my_model.save('filename.keras')
</code></pre>
","2024-03-26 19:06:36","0","Answer"
"78224233","","Face Emotion Detection using Python model","<p>ValueError: Kernel shape must have the same length as input, but received kernel of shape (3, 3, 1, 32) and input of shape (None, None, 48, 48, 1).</p>
<pre><code>from keras.models import load_model
from tensorflow.keras.utils import img_to_array
from keras.preprocessing import image
import cv2
import numpy as np

face_classifier = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')
classifier = load_model(&quot;./Emotion_Detection.h5&quot;)

class_labels = ['Angry', 'Happy', 'Neutral', 'Sad', 'Surprise']
</code></pre>
<p>When I try to run the .py file, the value error appears for the load_model() function. how to resolve the issue?
I am not even load the model, so I can't go further</p>
","2024-03-26 09:31:15","-2","Question"
"78215424","78041567","","<p>Since you are using Faiss through the Langchain integration, it expects a wrapper Embeddings model class and not the model directly. What you can do is create a CustomEmbeddings model class and put your model in it.</p>
<p>If you are trying to use a hosted by hugginfaces model, like the models on setence_transformers for example you can use the class HuggingFaceEmbeddings.</p>
<pre><code>from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')

VectorStore = FAISS.from_texts(chunks, embeddings)
</code></pre>
","2024-03-24 17:16:00","0","Answer"
"78206438","78206345","","<p>We can convert all information to numbers. This is how computers work. For example. you can convert pixels into 3 number - Red, Green, Blue (RGB). Like if Red, Green and Blue were all equal to the maximum value which is most often 255, the output color would be white.</p>
","2024-03-22 13:04:57","2","Answer"
"78206345","","How do neural networks process images and data that isn't just numbers?","<p>From what I understand, neural networks use mathematical operations and calculations to process data. We need numerical values to calculate error so the model can correct itself, we need to calculate the gradient of the error function, etc. We need numerical values to process data, and images are not numerical data, so how does the neural network analyze and it and perform operations on it. The labels of the files are also strings so how does the model calculate an error? Futhermore, how is this implemented in python</p>
<p>I tried using AI and ChatGPT to answer my question, but unfortunately, I didn't get an answer.</p>
","2024-03-22 12:49:10","-1","Question"
"78199805","","About ToTensorV2() about albumentations","<p>I am using ToTensorV2() to transform the data, but I asked ChatGPT if this function can scale the data to 0 - 1, and it answered yes. However, when I use cv2.imread() to read an image and pass the image to the transform, the data doesn't scale. Do I have an error somewhere, or do I need to scale to 0 - 1 manually?</p>
<p>transform = A.Compose([A.HorizontalFlip(p=1), ToTensorV2()])</p>
","2024-03-21 12:01:59","2","Question"
"78196301","","Can I turn candidates dataset to input on retrieval topK tensorflow model?","<p>I have a retrieval tensorflow training model and i use a <code>tfrs.layers.factorized_top_k.BruteForce</code> to get a prediction of firsts k's nearby candidates as implemented below:</p>
<pre><code>index = tfrs.layers.factorized_top_k.BruteForce(final_model.query_model)

index.index_from_dataset(
    tf.data.Dataset.zip((parsed_topK.batch(128).map(lambda x: x['id']), parsed_topK.batch(128).map(final_model.candidate_model)))
)
</code></pre>
<p>and get the top 5 results:</p>
<pre><code>results = index(input_query, k=5)
</code></pre>
<p>I want to know if is possible to turn the search database, represented in this code by <code>parsed_topK</code>, in to an input to the model, something like:</p>
<pre><code>index(input_query, input_candidates, k=5)
</code></pre>
<p>where <code>input_candidates = parsed_topK</code> in this example</p>
<p>I have tryed to call <code>final_model.predict(input_query, input_candidates)</code>, but i need to implement a call() method and i dont know what this method need to do.</p>
","2024-03-20 21:22:27","0","Question"
"78180397","78179193","","<p>Unless you are creating a Pong version that includes special physics you could use simple maths instead of a complete simulation...</p>
<p>The movement of the ball is linear and constant, it will just bounce off the walls but bounces only create a change in the horizontal direction. This can be expressed using a simple Triangle Wave.</p>
<pre><code>y = abs( (freq*x + phase) % 2 - 1) * scale - offset
</code></pre>
<ul>
<li>freq is the frequency, the amount of time it bounces in one X.</li>
<li>phase moves the origin on the Y</li>
<li>scale is the amplitude of the wave</li>
<li>offset moves the origin on the X</li>
</ul>
<p>With this, we can substitute some variables already.</p>
<ul>
<li>We will make the line bounce off the walls, which means the scale should be the width of the board.</li>
<li>We will not use the offset, as this would be to get relative coordinates but we want absolute ones.</li>
</ul>
<p>Now we also need to calculate the frequency, at what rate will the ball bounce... For this we calculate how much Y will it have to traverse to travel a full board's width. We can also simplify by just using the ratio they produce and divide them by the scale.</p>
<pre><code>freq = ballVelocityX / ballVelocityY / scale
</code></pre>
<p>Last but not least we need to change the phase to get the origin perfectly aligned with the position of the ball. This will be between -1 and 1 depending on the direction and we'll convert to between 0 and 2.</p>
<pre><code>phase = ballX / BOARD_WIDTH
</code></pre>
<p>The final result, in one formula by inverting X and Y:</p>
<pre><code>x = abs( ((ballVelocityX / ballVelocityY / BOARD_WIDTH)* y + ballX / BOARD_WIDTH) % 2 - 1) * BOARD_WIDTH
</code></pre>
<p>Now to use it, we just need to use the Y distance between the ball and the paddle and we'll receive the X where the ball will end.</p>
<p>EDIT:</p>
<p>I was writing this answer while you changed your question. Since the ball's X will not change direction when a paddle is hit, it is very easy to figure out no matter the number of paddle hits it will have. Basically, you just sum the total distance and pass that to the formula. So if the board is 100 long and the balls is 10 away from the player's paddle, simple calculate for a distance of 110.</p>
<p>Here's the graph I played with to see it in action.
<a href=""https://www.desmos.com/calculator/zr3g9wumrn"" rel=""nofollow noreferrer"">https://www.desmos.com/calculator/zr3g9wumrn</a></p>
<pre><code>function getTargetX(){
    let distance = this.game.ball.velocity.y &lt;0 ? this.game.player1.position.y-this.game.ball.position.y + g.BOARD_HEIGHT : this.game.ball.position.y -this.game.player2.position.y;
    return Math.abs( ((this.game.ball.velocity.x / this.game.ball.velocity.y / BOARD_WIDTH)* distance + this.game.ball.position.x / g.BOARD_WIDTH) % 2 - 1) * g.BOARD_WIDTH
}
</code></pre>
<p>EDIT 2:</p>
<p>To be able to account for the change of direction when hitting a paddle, which in this answer was considered a simple Y inversion, you could calculate where the ball will be when hitting the paddle and calculate the min-max range of the paddles length that can possibly hit the ball when considering the remaining time the paddle has to move before hit.</p>
<p>Since we do not know if the paddle movement is a force applied or a constant vector I can't really tell the strategy to calculate it. The min-max will then allow you to calculate the min/max angle at which the ball will bounce the paddle.</p>
<p>The AI could then make sure to cover the most dangerous angle, which will be 90degrees since that would make the ball hit the paddle the fastest. That degree can then be used to create the possible new ball vector to use in the formula. This will provide the fastest collision point.</p>
<p>You could also use the min-max angles to calculate let's say 10 different degrees in that arc and average the collision points. That would provide with the most probable position of the collision. The AI would then adjust the next second but would already be somewhat close.</p>
<p>You can combine the two and use a weighted average to give better importance to a position according to a ratio between time for paddle to get there VS time for ball to get there. That would make a ball go almost horizontally have way less impact on the calculated average, because it will certainly take multiple seconds to reach de collision point, so adjustments will have time to happen.</p>
","2024-03-18 13:02:53","1","Answer"
"78179321","78179193","","<p>you can predict the position of the ball so your AI can react quickly even with checking the game state only once every second...
Im not sure about your games infrastructure and how everything is calculated but this should give you a vague idea of what im trying to do...
This will result in quicker reactions</p>
<p>funtion to predict the ball location:</p>
<pre><code>predictBallPositionAfterSecond() {
    // Clone the ball's position and velocity to not affect the actual game state
    let predictedPosition = {...this.game.ball.position};
    let predictedVelocity = {...this.game.ball.velocity};

    let timeRemaining = 1.0; // Predict for 1 second into the future
    const dt = 0.016; // Assuming a frame rate of 60 FPS, so each tick is approximately 1/60th of a second

    while (timeRemaining &gt; 0) {
        // Calculate time to next collision with top or bottom wall
        let timeToCollision;
        if (predictedVelocity.y &gt; 0) {
            // Moving downwards
            timeToCollision = (g.BOARD_HEIGHT - g.BOARD_WALL - predictedPosition.y - this.game.ball.size.y) / predictedVelocity.y;
        } else {
            // Moving upwards
            timeToCollision = (g.BOARD_WALL - predictedPosition.y) / predictedVelocity.y;
        }

        // If no collision within the next tick, move ball for the entire tick
        if (timeToCollision &gt; dt || timeToCollision &lt; 0) {
            predictedPosition.x += predictedVelocity.x * dt;
            predictedPosition.y += predictedVelocity.y * dt;
            timeRemaining -= dt;
        } else {
            // Move ball to the point of collision
            predictedPosition.x += predictedVelocity.x * timeToCollision;
            predictedPosition.y += predictedVelocity.y * timeToCollision;

            // Reflect the ball's velocity
            predictedVelocity.y *= -1;

            // Subtract the elapsed time to collision from the remaining time
            timeRemaining -= timeToCollision;
        }

        // Check for collision with left and right walls to reverse X direction if necessary
        // This part can be adjusted based on your game's rules about side wall bounces
        if (predictedPosition.x &lt;= g.BOARD_WALL || predictedPosition.x + this.game.ball.size.x &gt;= g.BOARD_WIDTH - g.BOARD_WALL) {
            predictedVelocity.x *= -1;
        }
    }

    return predictedPosition;
}
</code></pre>
<p>Use of the predict function in your code:</p>
<pre><code>if (elapsed &gt;= 1000) {
    this.reset();
    this.load_state(game);

    // Predict the ball's position
    const predictedPosition = this.predictBallPositionAfterSecond();

    // Now you can use predictedPosition to decide how the AI should move
}
</code></pre>
<p>Keep in mind that for precise predictions, you'll need to tweak the collision and bounce logic to match your game's rules. Basically, the better your prediction mimics your game's physics, the more accurate it will be.</p>
","2024-03-18 09:55:20","0","Answer"
"78179193","","How to improve Pong AI with limited game state checks?","<p>I'm currently developing a Pong game for a school project. <strong>One rule I have is that the AI can only check the game state once every second</strong>.</p>
<p>Currently, the AI operates on two simple rules:</p>
<ul>
<li><strong>(A)</strong> If the ball is moving away from the AI, it returns to the center of the board.</li>
<li><strong>(B)</strong> If the ball is coming toward the AI, it tries to catch it and then returns to the center.</li>
</ul>
<p>The physics of the game is pretty simple, if the ball hits a wall the velocity in x is reversed. If the ball hits a paddle, the ball is given a new velocity based on how far from the center of the paddle the collision occurred. The ball always travel at a constant speed.</p>
<pre><code>    /* Update the angle of the ball based on where it hits the paddle */
    update_ball_velocity(paddle, normal) {
        const expanded =
            new physics.Rectangle(
                paddle.position.x - this.ball.size.x / 2,
                paddle.position.y - this.ball.size.y / 2,
                paddle.size.x + this.ball.size.x,
                paddle.size.y + this.ball.size.y,
                0,
                0
            );
        const ball_center =
            new physics.Vector(
                this.ball.position.x + this.ball.size.x / 2,
                this.ball.position.y + this.ball.size.y / 2
            );
        const paddle_center =
            new physics.Vector(
                expanded.position.x + expanded.size.x / 2,
                expanded.position.y + expanded.size.y / 2
            );

        if (normal.x != 0) {
            let c = ((ball_center.y - paddle_center.y) / (expanded.size.y / 2)) * g.BALL_MAX_ANGLE;
            this.ball.velocity.x = normal.x * Math.cos(c) * g.BALL_SPEED_MAX;
            this.ball.velocity.y = Math.sin(c) * g.BALL_SPEED_MAX;
        } else if (normal.y != 0) {
            let c = ((ball_center.x - paddle_center.x) / (expanded.size.x / 2)) * g.BALL_MAX_ANGLE;
            this.ball.velocity.x = Math.sin(c) * g.BALL_SPEED_MAX;
            this.ball.velocity.y = normal.y * Math.cos(c) * g.BALL_SPEED_MAX;
        }
    }
</code></pre>
<p>I am looking for algorithms / strategies / techniques to make the AI more capable. I don't want to use machine learning. More specifically, I would like to improve rule A, so that the AI does not just go back to the center of the board but maybe try to predict where the player will send the ball and start going towards that area.</p>
","2024-03-18 09:36:50","-1","Question"
"78174132","","TweetyProject for first order logic","<p>I'm trying to learn how to use coding for Logic Knowledge Representation and ultimately ML/AI.</p>
<p>I'm trying to make a java Tweety code about first order logic run but
I always get the same error: <code>the formula is not closed.</code></p>
<p>I know the meaning of a formula not being the in FOL, I just can't understand where the problem is in my code.</p>
<p>Here is the code:</p>
<pre><code>public class SimpleFolExample {
    public static void main(String[] args) throws ParserException, IOException {
        // Create FOL signature
        FolSignature signature = new FolSignature(true);
     // Add sorts
     // Add sorts
        Sort sortPerson = new Sort(&quot;Person&quot;);
        signature.add(sortPerson);

        // Add constants
        Constant alice = new Constant(&quot;Alice&quot;, sortPerson);
        Constant bob = new Constant(&quot;Bob&quot;, sortPerson);
        Constant James = new Constant(&quot;James&quot;, sortPerson);
        signature.add(alice, bob, James);

        // Add predicates
     // Add predicates
        List&lt;Sort&gt; personPredicateSorts = new ArrayList&lt;&gt;();
        personPredicateSorts.add(sortPerson);
        Predicate person = new Predicate(&quot;Person&quot;, personPredicateSorts);
        signature.add(person); // Add predicates to the signature

        // Add the 'Knows' predicate (assuming it relates persons)
        List&lt;Sort&gt; knowsPredicateSorts = new ArrayList&lt;&gt;();
        knowsPredicateSorts.add(sortPerson);
        knowsPredicateSorts.add(sortPerson);
        Predicate knows = new Predicate(&quot;Knows&quot;, knowsPredicateSorts);
        signature.add(knows);
       

        // Display the signature
        System.out.println(&quot;Signature: &quot; + signature);

        // Parse formulas using the signature
        FolParser parser = new FolParser();
        parser.setSignature(signature); // Set the FOL signature for the parser

        FolBeliefSet beliefSet = new FolBeliefSet();
        FolFormula f1 = (FolFormula) parser.parseFormula(&quot;Knows(Alice, Bob)&quot;);
        FolFormula f2 = (FolFormula) parser.parseFormula(&quot;Knows(Bob, James)&quot;);
        beliefSet.add(f1, f2);
    

        System.out.println(&quot;\nParsed Belief Base: &quot; + beliefSet);

        // Use the reasoner to query the knowledge base
        FolReasoner.setDefaultReasoner(new SimpleFolReasoner());
        FolReasoner prover = FolReasoner.getDefaultReasoner();

        FolFormula query = (FolFormula) parser.parseFormula(&quot;exists X:(Knows(Alice, X))&quot;);


        System.out.println(&quot;Query: &quot; + query + &quot;\nResult: &quot; + prover.query(beliefSet, query));
    }
    }

</code></pre>
<p>here is the error:</p>
<pre><code>Signature: [_Any = {}, Person = {Bob, Alice, James}], [==(_Any,_Any), /==(_Any,_Any), Person(Person), Knows(Person,Person)], []

Parsed Belief Base: { Knows(Bob,James), Knows(Alice,Bob) }
Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: The given formula exists X: (Knows(Alice,X)) is not closed.
    at org.tweetyproject.logics.fol.reasoner.SimpleFolReasoner.query(SimpleFolReasoner.java:45)
    at org.tweetyproject.logics.fol.examples.SimpleFolExample.main(SimpleFolExample.java:82
</code></pre>
","2024-03-17 05:10:30","2","Question"
"78172763","78171698","","<p>The current structure of the chat history in Google document does not accurately reflect the expected format for multi-turn conversations. The model appears to be incorrectly handling the conversation turns, leading to errors in the conversation flow.
To fix this issue, I propose the following enhancement to the chat history model:</p>
<pre><code>   const chatHistory = [
 {
    role: &quot;user&quot;,
    parts: [{
      text: &quot;Hello, I have 2 dogs in my house.&quot; 
    }]
 },
 {
    role: &quot;model&quot;,
    parts: [{
      text: &quot;Great to meet you.&quot; 
    }]
 }
];
</code></pre>
","2024-03-16 17:41:07","1","Answer"
"78172520","78171698","","<p>I think there was an issue with the latest version in this issue: <a href=""https://github.com/google/generative-ai-js/pull/32"" rel=""nofollow noreferrer"">https://github.com/google/generative-ai-js/pull/32</a></p>
<p>try using an older version of gemini.
I've installed '@google/generative-ai@0.2.1' and it solved the problem for me</p>
","2024-03-16 16:31:09","0","Answer"
"78171698","","Issue encountered with the history parameter in the Gemini AI integration within a Node.js environment","<p>I copied and pasted the Gemini document directly into my Node.js server, and I encountered an error related to the <code>history</code> parameter.</p>
<pre><code>{&quot;error&quot;: &quot;Cannot use 'in' operator to search for 'text' in H&quot;}
</code></pre>
<p>However, the operation succeeded when I set <code>history</code> to <code>null</code>.</p>
<p>this is my code :</p>
<pre><code>   history: [
  {
    role: &quot;user&quot;,
    parts: : &quot;Hello, I have 2 dogs in my house.&quot;,
  },
  {
    role: &quot;model&quot;,
    parts: &quot;Great to meet you. What would you like to know?&quot;,
  },
],
generationConfig: {
  maxOutputTokens: 100,
},
</code></pre>
<p>});</p>
<p>and this is the error i got in the console :</p>
<pre><code>TypeError: Cannot use 'in' operator to search for 'text' in H
    at validateChatHistory (C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\@google\generative-ai\dist\index.js:760:25)
    at new ChatSession (C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\@google\generative-ai\dist\index.js:816:13)
    at GenerativeModel.startChat (C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\@google\generative-ai\dist\index.js:1035:16)
    at C:\myprojects\chat_app_generativeai_dartfrog\server\routes\chat.js:21:22
    at Layer.handle [as handle_request] (C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\express\lib\router\layer.js:95:5)
    at next (C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\express\lib\router\route.js:149:13)
    at Route.dispatch (C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\express\lib\router\route.js:119:3)
    at Layer.handle [as handle_request] (C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\express\lib\router\layer.js:95:5)
    at C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\express\lib\router\index.js:284:15
    at Function.process_params (C:\myprojects\chat_app_generativeai_dartfrog\server\node_modules\express\lib\router\index.js:346:12)
</code></pre>
","2024-03-16 12:13:17","-1","Question"
"78169334","","TensorFlow Save Model error ""Unable to synchronously create dataset (name already exists)""","<p>`I developed an AI model and successfully trained it However, I encountered an issue when attempting to save the model. An error message stated that the **h5 **file name already exists, but the situation seems inconsistent as the error persists regardless of whether the file exists or not'. Interestingly, when I tested similar code in other projects, I didn't encounter this problem.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Error: Unable to synchronously create dataset (name already exists)
Traceback (most recent call last):
  File ""C:\Users\Lenovo-Z\Documents\Text\Voice Line\main.py"", line 333, in main
    model.save('VoiceLine_Model.h5')
  File ""C:\Users\Lenovo-Z\.conda\envs\voiceline_myenv2\lib\site-packages\keras\src\utils\traceback_utils.py"", line 123, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\Lenovo-Z\AppData\Roaming\Python\Python310\site-packages\h5py\_hl\group.py"", line 183, in create_dataset
    dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)
  File ""C:\Users\Lenovo-Z\AppData\Roaming\Python\Python310\site-packages\h5py\_hl\dataset.py"", line 163, in make_new_dset
    dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)
  File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py\h5d.pyx"", line 137, in h5py.h5d.create
ValueError: Unable to synchronously create dataset (name already exists)

code:

def save_artifacts(tokenizer, encoder, model, embedding_matrix):
    # Save tokenizer as JSON
    tokenizer_data = {
        ""word_index"": tokenizer.word_index,
        ""index_word"": tokenizer.index_word,
        ""word_counts"": tokenizer.word_counts,
        ""document_count"": tokenizer.document_count
    }
    with open(""tokenizer.pkl"", ""wb"") as tokenizer_file:
        pickle.dump(tokenizer_data, tokenizer_file)

    # Save label encoder using pickle
    with open(""label_encoder.pkl"", ""wb"") as label_file:
        pickle.dump(encoder, label_file)

    # Save model architecture as JSON
    model_json = model.to_json()
    with open(""model_architecture.json"", ""w"") as json_file:
        json_file.write(model_json)

    # Save words using pickle
    with open(""words.pkl"", ""wb"") as words_file:
        pickle.dump(tokenizer.word_index, words_file)

    # Save classes using pickle
    with open(""classes.pkl"", ""wb"") as classes_file:
        pickle.dump(encoder.classes_, classes_file)

    # Save embedding matrix
    np.save(""embedding_matrix.npy"", embedding_matrix)

...

model = build_combined_model(embedding_dim=EMBEDDING_DIM, num_classes=len(encoder.classes_), vocab_size=len(tokenizer.word_index) + 1)

compile_model(model)

callbacks = get_callbacks()

model.fit(train_tokens, train_labels_one_hot, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(test_tokens, test_labels_one_hot), callbacks=callbacks)

print(""Model training completed."")
model.save('VoiceLine_Model.h5')</code></pre>
</div>
</div>
</p>
","2024-03-15 19:42:46","0","Question"
"78164500","78163861","","<p>You can try <a href=""https://pyodide.org/en/stable/"" rel=""nofollow noreferrer"">Pyodide</a> (a Python distribution for the browser and Node.js based on WebAssembly) to run python code in js environment.</p>
","2024-03-15 03:24:33","0","Answer"
"78163861","","How to run a Python model in React Native using TensorFlow.js?","<p>Trying to add a resNet50 model trained in Python to my React Native project using TensorFlow.js for image prediction to provide 5 similar images based on input:</p>
<pre><code>import streamlit as st
import os
from PIL import Image
import numpy as np
import pickle
import tensorflow
from tensorflow.keras.layers import GlobalMaxPooling2D
from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input
from sklearn.neighbors import NearestNeighbors
from numpy.linalg import norm
import cv2

feature_list = np.array(pickle.load(open('featurevector.pkl','rb')))
filenames = pickle.load(open('filenames.pkl','rb'))

model = ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))
model.trainable = False

model = tensorflow.keras.Sequential([
    model,
    GlobalMaxPooling2D()
])

st.title('Man &amp; Women Fashion Recommender System')

def save_uploaded_file(uploaded_file):
    try:
        with open(os.path.join('uploads',uploaded_file.name),'wb') as f:
            f.write(uploaded_file.getbuffer())
        return 1
    except:
        return 0

def extract_feature(img_path, model):
    img=cv2.imread(img_path)
    img=cv2.resize(img, (224,224))
    img=np.array(img)
    expand_img=np.expand_dims(img, axis=0)
    pre_img=preprocess_input(expand_img)
    result=model.predict(pre_img).flatten()
    normalized=result/norm(result)
    return normalized

def recommend(features,feature_list):
    neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='euclidean')
    neighbors.fit(feature_list)

    distances, indices = neighbors.kneighbors([features])

    return indices

# steps
# file upload -&gt; save
uploaded_file = st.file_uploader(&quot;Choose an image&quot;)
print(uploaded_file)
if uploaded_file is not None:
    if save_uploaded_file(uploaded_file):
        # display the file
        display_image = Image.open(uploaded_file)
        resized_img = display_image.resize((200, 200))
        st.image(resized_img)
        # feature extract
        features = extract_feature(os.path.join(&quot;uploads&quot;,uploaded_file.name),model)
        #st.text(features)
        # recommendention
        indices = recommend(features,feature_list)
        # show
        col1,col2,col3,col4,col5 = st.columns(5)

        with col1:
            st.image(filenames[indices[0][1]])
        with col2:
            st.image(filenames[indices[0][2]])
        with col3:
            st.image(filenames[indices[0][3]])
        with col4:
            st.image(filenames[indices[0][4]])
        with col5:
            st.image(filenames[indices[0][5]])
    else:
        st.header(&quot;Some error occured in file upload&quot;)

</code></pre>
","2024-03-14 22:55:45","0","Question"
"78144858","77894003","","<p>Try following steps:</p>
<ul>
<li><p>Agree to send data to Google so we can better understand how
effective Studio Bot is.</p>
</li>
<li><p>Give permission to share data either when you first install Android
Studio or later at</p>
<p><code>File &gt; Settings &gt; Appearance &amp; Behavior &gt; System Settings &gt; Data Sharing  (Android Studio &gt; Settings &gt; Appearance &amp; Behavior &gt; System Settings &gt; Data Sharing on macOS)</code></p>
</li>
<li><p>To launch Studio Bot, open or start an Android Studio project and
click</p>
<p><code>View &gt; Tool Windows &gt; Studio Bot</code></p>
</li>
<li><p>Sign in to your Google account when asked, if you aren't already
signed in.</p>
</li>
<li><p>The chat box appears and you can start using Studio Bot's
interactive, conversational interface.</p>
</li>
</ul>
","2024-03-12 05:42:19","1","Answer"
"78115038","78104913","","<p>It seems you are trying to build a semantic search engine from scratch. Elastic has features to make it easy to import LLMs from external sources (like HuggingFace) and use those insights into your search.</p>
<p>If you still want to build it bit by bit you can use a keyword extraction LLM and add that to your query; or you could use an embedding mode from the start so the queries search by <em>meaning</em> rather than word matching (which I would recommend in your example).</p>
<p>Take a look at the starting guide from the docs: <a href=""https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/search/00-quick-start.ipynb"" rel=""nofollow noreferrer"">https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/search/00-quick-start.ipynb</a></p>
<p>Or if you want to read more about semantic / lexical search there are some good articles on searhlabs: <a href=""https://www.elastic.co/search-labs/blog/articles/lexical-and-semantic-search-with-elasticsearch"" rel=""nofollow noreferrer"">https://www.elastic.co/search-labs/blog/articles/lexical-and-semantic-search-with-elasticsearch</a></p>
<p>The parts you would need for your use case:</p>
<ol>
<li><p>Importing a model through docker and setting up your index and mappings (including a dense vector field where the embeddings will go)</p>
</li>
<li><p>Running a pipeline or bulk process to generate embeddings for all the documents in your index.</p>
</li>
</ol>
<pre><code>operations = []
for book in books:
    operations.append({&quot;index&quot;: {&quot;_index&quot;: &quot;book_index&quot;}})
    # Transforming the title into an embedding using the model
    book[&quot;title_vector&quot;] = model.encode(book[&quot;title&quot;]).tolist()
    operations.append(book)
client.bulk(index=&quot;book_index&quot;, operations=operations, refresh=True)
</code></pre>
<ol start=""3"">
<li>When you search with a natural language query, like <code>query = &quot;what is your pricing?&quot;</code> you want to embed this query with the same model and send that as part of your knn search:</li>
</ol>
<pre><code>response = client.search(
    index=&quot;book_index&quot;,
    knn={
        &quot;field&quot;: &quot;title_vector&quot;,
        &quot;query_vector&quot;: model.encode(query),
        &quot;k&quot;: 10,
        &quot;num_candidates&quot;: 100,
    },
)
</code></pre>
<p>Hope this helps!</p>
","2024-03-06 14:07:44","1","Answer"
"78109756","78099801","","<p>I merged the fake and real data into a single matrix, and created a binary vector <code>y</code> where <code>1</code> means real and <code>0</code> means fake. The data had some invalid values (<code>np.inf</code>), which I replaced with the maximum allowable value for <code>float16</code>. I trained a classifier and got 87% classification accuracy on the validation set.</p>
<p>You'll probably also want to split a test set off. In the code I just worked with a training set and a validation set.</p>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>Accuracy: 0.87
Classification Report:
              precision    recall  f1-score   support

         0.0       0.84      0.89      0.87       141
         1.0       0.90      0.85      0.87       159

    accuracy                           0.87       300
   macro avg       0.87      0.87      0.87       300
weighted avg       0.87      0.87      0.87       300

Confusion Matrix:
[[126  15]
 [ 24 135]]
</code></pre>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

#
# Load the real and fake data, and stack into a single matrix
#
data_real = np.load('real.npz')['x']
data_fake = np.load('fake.npz')['x']
data = np.concatenate([data_real, data_fake], axis=0)

#Target vector: y=1 corresponds to real, and y=0 corresponds to fake
y = np.concatenate([np.ones(len(data_real)), np.zeros(len(data_fake))])

#Deal with inf by replacing with max value allowed
X = np.nan_to_num(data, posinf=np.finfo(data.dtype).max)
X = X.reshape(X.shape[0], -1) #flatten

#Spline into train and validation sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=0.8, shuffle=True, random_state=0
)

#Optionally reduce the dimensionality
# This signficantly speeds up the clf.fit() step (from 40s down to 3s)
# It also reduces memory consumption
# But accuracy goes down from about 87% to 77%
# Could be useful for speeding up exploratory analyses
reduce_dimensionality = False
if reduce_dimensionality:
    from sklearn.random_projection import SparseRandomProjection
    projector = SparseRandomProjection(n_components='auto', random_state=0).fit(X_train)    
    X_train, X_test = [projector.transform(x) for x in [X_train, X_test]]

#Fit classifier
np.random.seed(0)
clf = ExtraTreesClassifier(n_estimators=400)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(&quot;Accuracy:&quot;, accuracy)

print(&quot;Classification Report:&quot;)
print(classification_report(y_test, y_pred))

conf_matrix = confusion_matrix(y_test, y_pred)
print(&quot;Confusion Matrix:&quot;)
print(conf_matrix)
</code></pre>
","2024-03-05 18:15:01","0","Answer"
"78104913","","more accurate queries with elastic search","<p>I'm creating an elastic search based application where I essentially have a document in which I retrieve and split into chunks, then I have a query:</p>
<p><code>query = &quot;what is your pricing?&quot;</code></p>
<p>and then I feed the text, and the query into my function:</p>
<pre><code>def construct_enhanced_query(user_query):
    keywords = extract_keywords(user_query)
    should_clauses = [{&quot;match&quot;: {&quot;content&quot;: {&quot;query&quot;: keyword, &quot;boost&quot;: 2}}} for keyword in keywords]

    query = {
        &quot;query&quot;: {
            &quot;bool&quot;: {
                &quot;should&quot;: should_clauses,
                &quot;minimum_should_match&quot;: 1
            }
        }
    }
    return query
</code></pre>
<p>,  <code>extract_keywords()</code>, looks like so:</p>
<pre><code>def extract_keywords(text):
    doc = nlp(text)
    keywords = set([chunk.text for chunk in doc.noun_chunks] + [ent.text for ent in doc.ents])
    return keywords
</code></pre>
<p>Now, the thing is, in the chunks, I have things like this:</p>
<p><code>ABOUT PRICING CONTACT Learn more</code>
and also <code>PRICING PLANS Private Lessons $150 PER STUDENT 30 minute lessons 1 instructor and 1 student (1:1) Book Now Semi-Private $130 /PER STUDENT 30 minute lessons 1 instructor and 2 students (1:2)</code>. As you could imagine, I am looking to retrieve the $150 PER STUDENT part and its relevant information, and it should work because it's one chunk, which is good. the problem is, its only returning this:</p>
<pre><code>Document ID: nrIzDI4BhRAP3y-2FwQt Content: PRICING
Document ID: orIzDI4BhRAP3y-2FwQ0 Content: PRICING
Document ID: FLIwDI4BhRAP3y-2UwI2 Content: PRICING
Document ID: GLIwDI4BhRAP3y-2UwJD Content: PRICING
</code></pre>
<p>which as you could imagine is due to the query and keyword search plus the boosting. But I am trying to make it more dynamic, and be able to actually retrieve the relevant information, because the query wont always be &quot;what is your pricing&quot;, it could be anything, so I can't hardcode it. Any advice?</p>
","2024-03-05 01:41:16","0","Question"
"78103896","78084538","","<p><em>Note: The code below works with the <a href=""https://platform.openai.com/docs/api-reference/assistants-v1"" rel=""noreferrer"">OpenAI Assistants API <code>v1</code></a>. In April 2024, the <a href=""https://platform.openai.com/docs/api-reference/assistants"" rel=""noreferrer"">OpenAI Assistants API <code>v2</code></a> was released. See the <a href=""https://platform.openai.com/docs/assistants/migration/agents"" rel=""noreferrer"">migration guide</a>.</em></p>
<hr />
<p>I created a <a href=""https://github.com/rokbenko/ai-playground/tree/main/openai-tutorials/4-Build_customer_support_chatbot"" rel=""noreferrer"">customer support chatbot</a> and made a <a href=""https://youtu.be/xbgX8fu78DI"" rel=""noreferrer"">YouTube tutorial</a> about it.</p>
<p>The process is as follows:</p>
<p><strong>Step 1: Upload a File with an &quot;assistants&quot; purpose</strong></p>
<pre><code>my_file = client.files.create(
  file=open(&quot;knowledge.txt&quot;, &quot;rb&quot;),
  purpose='assistants'
)
</code></pre>
<p><strong>Step 2: Create an Assistant</strong></p>
<pre><code>my_assistant = client.beta.assistants.create(
    model=&quot;gpt-3.5-turbo-1106&quot;,
    instructions=&quot;You are a customer support chatbot. Use your knowledge base to best respond to customer queries.&quot;,
    name=&quot;Customer Support Chatbot&quot;,
    tools=[{&quot;type&quot;: &quot;retrieval&quot;}]
)
</code></pre>
<p><strong>Step 3: Create a Thread</strong></p>
<pre><code>my_thread = client.beta.threads.create()
</code></pre>
<p><strong>Step 4: Add a Message to a Thread</strong></p>
<pre><code>my_thread_message = client.beta.threads.messages.create(
  thread_id=my_thread.id,
  role=&quot;user&quot;,
  content=&quot;What can I buy in your online store?&quot;,
  file_ids=[my_file.id]
)
</code></pre>
<p><strong>Step 5: Run the Assistant</strong></p>
<pre><code>my_run = client.beta.threads.runs.create(
  thread_id=my_thread.id,
  assistant_id=my_assistant.id,
)
</code></pre>
<p><strong>Step 6: Periodically retrieve the Run to check on its status to see if it has moved to completed</strong></p>
<pre><code>keep_retrieving_run = client.beta.threads.runs.retrieve(
    thread_id=my_thread.id,
    run_id=my_run.id
)
</code></pre>
<p><strong>Step 7: Retrieve the Messages added by the Assistant to the Thread once the run status is &quot;completed&quot;</strong></p>
<pre><code>all_messages = client.beta.threads.messages.list(
    thread_id=my_thread.id
)

print(f&quot;User: {my_thread_message.content[0].text.value}&quot;)
print(f&quot;Assistant: {all_messages.data[0].content[0].text.value}&quot;)
</code></pre>
<p>See the <a href=""https://github.com/rokbenko/ai-playground/tree/main/openai-tutorials/4-Build_customer_support_chatbot"" rel=""noreferrer"">full code</a>.</p>
<h2>Important note</h2>
<p>The assistant might sometimes behave strangely. The Assistants API is still in beta, and it seems that OpenAI has trouble keeping it realiable, as discussed on the official <a href=""https://community.openai.com/t/assistant-not-able-to-access-uploaded-file/524495/1"" rel=""noreferrer"">OpenAI forum</a>.</p>
<p>The assistant might sometimes answer that it cannot access the files you uploaded. You might think you did something wrong, but if you run identical code later or the next day, the assistant will successfully access all files and give you an answer.</p>
<p>The weird responses I got were the following:</p>
<ul>
<li><em>Assistant: I currently do not have access to the file you uploaded.
Could you provide some details about what you're selling or any
specific questions you have in mind?</em></li>
<li><em>Assistant: I currently don't have the ability to directly access the
contents of the file you uploaded. However, if you can provide some
details or specific questions about the than happy to assist you in
finding the information you need.</em></li>
<li><em>Assistant: I currently don't have visibility into the specific
contents of the file you've uploaded. Could you provide more details
about the file or its contents so that I can assist you further?</em></li>
<li><em>Assistant: I see you've uploaded a file. How can I assist you with
it?</em></li>
</ul>
","2024-03-04 20:26:03","6","Answer"
"78099801","","Unable to use 3D X and 2D y for training","<p>I am training a model to differentiate between real and fake data.
I have a real dataset and a fake dataset, and since I didn't know how to train a single model on both, I am now training a model for real and a model for fake.</p>
<p>The problem is that training any of the models seems impossible, as I get this error:</p>
<pre><code>ValueError: Input X contains infinity or a value too large for dtype('float16')
</code></pre>
<p>For this code</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

scaler = RobustScaler()
X = np.load(&quot;fake_x.npy&quot;)
x_data_flat = X.reshape(X.shape[0], -1)
x_data_scaled = scaler.fit_transform(x_data_flat)
y = np.load(&quot;fake_y.npy&quot;)
x_data_flat, y = shuffle(x_data_flat, y)
X_train,X_test,y_train,y_test = train_test_split(x_data_scaled,y,test_size=0.1, train_size=0.1)

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(&quot;Accuracy:&quot;, accuracy)

print(&quot;Classification Report:&quot;)
print(classification_report(y_test, y_pred))


conf_matrix = confusion_matrix(y_test, y_pred)
print(&quot;Confusion Matrix:&quot;)
print(conf_matrix)
</code></pre>
<p>The data is certainly too large, x.shape = (750,1998,101) and y.shape = (750,496).
Data Link: <a href=""https://drive.google.com/drive/folders/1EYnOIOWP17ALs-903ESFM-02_6VszJEx"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1EYnOIOWP17ALs-903ESFM-02_6VszJEx</a>
For the classification itself, I wouldl use a similarity score on the input and whoever gets a higher similarity score the input will belong to its class.</p>
<p>Using a 2D spectrogram that was originally real.npz but extracted to real_x.npy and real_y.npy to make it easier, the model should recognize whether the input belongs to real or to the fake.npz which I extracted to fake_x.npy and fake_y.npy</p>
","2024-03-04 08:20:52","0","Question"
"78096961","78095622","","<p>the only reason i can see here: pathObjs.length equals 1. I suggest you to check it out by adding an additional log message:</p>
<pre><code> Transform[] pathObjs = pathGroup.GetComponentsInChildren&lt;Transform&gt;();
 Debug.Log($&quot;child count: {pathObjs.Length}&quot;);
</code></pre>
<p>try this and make sure it's true.</p>
<p>Maybe waypoint object are disabled? you can use this to get disabled objects too.</p>
<pre><code>pathGroup.GetComponentsInChildren&lt;Transform&gt;(true);
</code></pre>
","2024-03-03 15:43:52","2","Answer"
"78095622","","Path array is empty. Make sure to assign a valid path in the inspector. UnityEngine.Debug:LogError (object)","<p>Hello Unity community,</p>
<p>I'm currently facing an issue with my AICarScript in Unity, specifically with the <code>path</code> array being empty. I've implemented a waypoint-following system where the car should follow a path defined by child objects under a specified <code>pathGroup</code> in the inspector.</p>
<p>However, when I run the script, I encounter the following error message:</p>
<p>I've double-checked the following aspects:</p>
<ol>
<li><p><strong>Inspector Assignment:</strong> I have correctly assigned the <code>pathGroup</code> variable in the inspector to the GameObject containing the waypoints.</p>
</li>
<li><p><strong>Child Objects:</strong> The waypoints are indeed child objects of the GameObject assigned to <code>pathGroup</code>.</p>
</li>
<li><p><strong>Hierarchy:</strong> The waypoints are arranged in the correct hierarchy, with no additional unnecessary parent-child relationships.</p>
</li>
</ol>
<p>I've added debug logs to the <code>GetPath</code> method to print the length of the <code>path</code> array and the names of the child objects. Strangely, the array length is 0, even though I expect it to be greater.</p>
<pre class=""lang-cs prettyprint-override""><code>// GetPath method
void GetPath()
{
    Transform[] pathObjs = pathGroup.GetComponentsInChildren&lt;Transform&gt;();
    path = new Transform[pathObjs.Length - 1];

    for (int i = 1; i &lt; pathObjs.Length; i++)
    {
        if (pathObjs[i] != pathGroup)
        {
            path[i - 1] = pathObjs[i];
        }
    }

    Debug.Log(&quot;Path Length: &quot; + path.Length);

    for (int i = 0; i &lt; path.Length; i++)
    {
        Debug.Log(&quot;Waypoint &quot; + i + &quot;: &quot; + path[i].name);
    }

    if (path.Length == 0)
    {
        Debug.LogError(&quot;Path array is empty. Make sure to assign a valid path in the inspector.&quot;);
    }
}

I'm seeking guidance on what might be causing the path array to remain empty during runtime. Has anyone encountered a similar issue, or can someone offer insights into potential pitfalls related to this scenario?

Thank you for your time and assistance guys.
</code></pre>
","2024-03-03 08:33:04","1","Question"
"78095125","77894003","","<p>step1: make sure you check data sharing option
File &gt; Settings &gt; Appearance &amp; Behavior &gt; System Settings &gt; Data sharing</p>
<p>step2: toggle Studio Bot view section
View &gt; Tool Windows &gt; Studio Bot</p>
","2024-03-03 04:33:54","0","Answer"
"78094559","78094158","","<p>To be <a href=""https://en.wikipedia.org/wiki/Admissible_heuristic"" rel=""nofollow noreferrer"">admissible</a> for A* search, a heuristic function must not overestimate the distance left to go. That is, it's fine if it's an underestimate, but an overestimate is not allowed.</p>
<p>So with that in mind, you need to decide what you're trying to minimize with your search. Is it the number of squares traveled before the whole map is colored? In that case, the number of uncolored squares left would be an admissible heuristic, since you certainly need to move over every remaining uncolored square to complete the puzzle.</p>
<p>On the other hand, if you're measuring the number of straight line moves needed, that will require a fancier heuristic, since many squares can be colored in with a single move. I don't have a great idea for one, but perhaps the minimum (or sum?) of the unique X and unique Y coordinates of uncolored squares could work.</p>
<p>There are lots of possible admissible heuristics, so pick one and see if it's good enough for your use case. It might not be <em>the best</em> heuristic, but that just means the search will be a bit less efficient, not that it won't find the optimal route eventually. In the worst case, you wind up with <a href=""https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"" rel=""nofollow noreferrer"">Dijkstra's algorithm</a> (which is equivalent to A* with a heuristic function that always returns 0). Using a good heuristic just speeds things up, as you won't examine as many partial solutions that are likely to be slower than the optimal one.</p>
","2024-03-02 22:49:10","1","Answer"
"78094158","","Color Maze AI with A* Search- Heuristic function","<p>Solving Color Maze Puzzle using A* Search is the goal. This is the example of the game <a href=""https://www.mathplayground.com/logic_color_maze"" rel=""nofollow noreferrer"">https://www.mathplayground.com/logic_color_maze</a>. Basically you want to minimize the cost of movement, and the goal is to color all the maze going through cells. To implement it with A* Search we need heuristic function. I thought of looking at the number of the left colored cells, but it would be pointless. Shouldn't it be the estimated cost to the goal state(where every cell is colored)? Any ideas to work on that?</p>
","2024-03-02 20:04:33","-5","Question"
"78091527","","Code of CURL of Gemini AI API in C# .Net Framework","<p>I am developing a window app in c# .net framework about making ai using Google Gemini API. And I read the documentation of google gemini api over 10 times in <a href=""https://ai.google.dev/tutorials/rest_quickstart"" rel=""nofollow noreferrer"">CURL</a>. With some help of ChatGPT, I wrote some codes in C#. And here what is looks like:</p>
<pre><code>using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;

namespace KJAI
{
    public partial class Form1 : Form
    {
        private string apiKey = &quot;YOUR_API_KEY&quot;;

        public Form1()
        {
            InitializeComponent();
        }

        private async void button1_Click(object sender, EventArgs e)
        {
            string userInput = textBox1.Text;

            string output = await SendRequestAndGetResponse(userInput);

            output = output.Replace(&quot;\\n&quot;, Environment.NewLine)
                           .Replace(&quot;\n&quot;, &quot;&quot;)
                           .Replace(&quot;**&quot;, &quot;&quot;);

            richTextBox1.Text = output;
        }

        private async Task&lt;string&gt; SendRequestAndGetResponse(string userInput)
        {
            string jsonBody = $@&quot;{{
                &quot;&quot;contents&quot;&quot;: [
                    {{
                        &quot;&quot;role&quot;&quot;: &quot;&quot;&quot;&quot;,
                        &quot;&quot;parts&quot;&quot;: [
                            {{
                                &quot;&quot;text&quot;&quot;: &quot;&quot;{userInput}&quot;&quot;
                            }}
                        ]
                    }}
                ],
                &quot;&quot;generationConfig&quot;&quot;: {{
                    &quot;&quot;temperature&quot;&quot;: 0.9,
                    &quot;&quot;topK&quot;&quot;: 50,
                    &quot;&quot;topP&quot;&quot;: 0.95,
                    &quot;&quot;maxOutputTokens&quot;&quot;: 4096,
                    &quot;&quot;stopSequences&quot;&quot;: []
                }},
                &quot;&quot;safetySettings&quot;&quot;: [

                ]
            }}&quot;;

            using var client = new HttpClient();
            var request = new HttpRequestMessage(HttpMethod.Post, $&quot;https://generativelanguage.googleapis.com/v1beta/models/gemini-1.0-pro:generateContent?key={apiKey}&quot;);
            request.Content = new StringContent(jsonBody, Encoding.UTF8);
            request.Content.Headers.ContentType = new MediaTypeHeaderValue(&quot;application/json&quot;);

            var response = await client.SendAsync(request).ConfigureAwait(false);

            if (response.IsSuccessStatusCode)
            {
                string responseBody = await response.Content.ReadAsStringAsync();
                return responseBody.Substring(responseBody.IndexOf(&quot;\&quot;text\&quot;: \&quot;&quot;) + 9, responseBody.IndexOf(&quot;\&quot;&quot;, responseBody.IndexOf(&quot;\&quot;text\&quot;: \&quot;&quot;) + 10) - responseBody.IndexOf(&quot;\&quot;text\&quot;: \&quot;&quot;) - 9);
            }
            else
            {
                return $&quot;Error: {response.StatusCode} - {response.ReasonPhrase}&quot;;
            }
        }
    }
}
</code></pre>
<p>However, I want to make Gemini able to remember the previous conversation between an user and the ai, but I have no idea how to do this. Anyone who have ideas?</p>
<p>I tried my best to build chat history between user and ai and make it into dictionary form, but I failed. It is important to able ai to access and remember the conversation they had.</p>
","2024-03-02 05:06:13","1","Question"
"78087570","78077058","","<blockquote>
<p>Any ideas of what could be happening? I don't know why the method throws a 404. All the elements, urls and keys are on place.</p>
</blockquote>
<p>Here I have created an <a href=""https://i.imgur.com/VLRXn3c.png"" rel=""nofollow noreferrer"">application</a> to interact with the Azure Form Recognizer service.</p>
<p><strong>FormRecognizerService.cs:</strong></p>
<pre class=""lang-cs prettyprint-override""><code>using Azure;
using Azure.AI.FormRecognizer;
using Azure.AI.FormRecognizer.Models;
using System;
using System.Threading.Tasks;

namespace MyFormRecognizerApp
{
    public class FormRecognizerService
    {
        private readonly FormRecognizerClient _formRecognizerClient;

        public FormRecognizerService(string endpoint, string key)
        {
            var credential = new AzureKeyCredential(key);
            _formRecognizerClient = new FormRecognizerClient(new Uri(endpoint), credential);
        }

        public async Task AnalyzeDocumentAsync(Uri documentUri, string modelName)
        {
            try
            {
                // Start the custom form recognition operation
                RecognizeCustomFormsOperation operation = await _formRecognizerClient.StartRecognizeCustomFormsFromUriAsync(modelName, documentUri);

                // Wait for the operation to complete
                await operation.WaitForCompletionAsync();

                // Get the response
                Response&lt;RecognizedFormCollection&gt; response = await operation.WaitForCompletionAsync();

                // Check if the operation was successful
                if (response.GetRawResponse().Status == 200)
                {
                    RecognizedFormCollection recognizedForms = response.Value;
                    // Process the recognized forms
                    foreach (RecognizedForm form in recognizedForms)
                    {
                        Console.WriteLine($&quot;Form of type: {form.FormType}&quot;);
                        foreach (FormField field in form.Fields.Values)
                        {
                            Console.WriteLine($&quot;INVOICE: {form.Fields[&quot;INVOICE&quot;].Value}&quot;);
                            Console.WriteLine($&quot;DATE: {form.Fields[&quot;DATE&quot;].Value}&quot;);
                        }
                    }
                }
                else
                {
                    throw new RequestFailedException(&quot;The operation did not complete successfully.&quot;);
                }
            }
            catch (RequestFailedException ex)
            {
                Console.WriteLine($&quot;Error analyzing document: {ex.Message}&quot;);
            }
        }
    }
}
</code></pre>
<p><strong>Program.cs:</strong></p>
<pre><code>using System;
using System.Threading.Tasks;

namespace MyFormRecognizerApp
{
    class Program
    {
        static async Task Main(string[] args)
        {
            string endpoint = &quot;https://your-Document intelligence.cognitiveservices.azure.com/&quot;;
            string key = &quot;key&quot;;
            string modelName = &quot;modelID&quot;;

            var service = new FormRecognizerService(endpoint, key);

            Uri documentUri = new Uri(&quot;https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-invoice.pdf\r\n&quot;);

            await service.AnalyzeDocumentAsync(documentUri, modelName);
        }
    }
}
</code></pre>
<p>This is my Document intelligence resource.
<img src=""https://i.imgur.com/SIr0PP2.png"" alt=""enter image description here"" /></p>
<ul>
<li>Below is my model by taking a sample invoice to test and I have trained.</li>
</ul>
<p><img src=""https://i.imgur.com/ijeP13q.png"" alt=""enter image description here"" /></p>
<ul>
<li>Here my task is to analyse the document and get the below fields recognise and should be print in the console.</li>
</ul>
<p><img src=""https://i.imgur.com/jioC0Gq.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/Qty2qvx.png"" alt=""enter image description here"" /></p>
","2024-03-01 11:57:20","0","Answer"
"78084538","","OpenAI Assistants API: How do I upload a file and use it as a knowledge base?","<p>My goal is to create a chatbot that I can provide a file to that holds a bunch of text, and then use the OpenAI Assistants API to actually use the file when querying my chatbot. I will use the <code>gpt-3.5-turbo</code> model to answer the questions.</p>
<p>The code I have is the following:</p>
<pre><code>file_response = client.files.create(
   file=open(&quot;website_content.txt&quot;, &quot;rb&quot;),
   purpose=&quot;assistants&quot;
)

query_response = client.assistants.query(
   assistant_id=&quot;my_assistant_id&quot;, 
   input=&quot;Tell me about xxx?&quot;,
   files=[file_response['id']] 
)
</code></pre>
<p>However, this is not working, for what I think could be a few things. For one, I don't fully understand the way it is supposed to work, so I was looking for some guidance. I have already created an assistant via the dashboard, but now I want to just upload a file and then query it. Do I have to use something else, like &quot;threads&quot; via the API, or no?</p>
<p>How do I do this?</p>
","2024-02-29 22:06:45","2","Question"
"78077058","","AnalyzeDocumentAsync (Azure Document Intelligence) is throwing a 404 exception","<p>I have the following code:</p>
<pre><code>public async Task TestExtractionDataAsync()
{
    _documentAnalysisClient = new DocumentIntelligenceClient(new Uri(endpoint), new AzureKeyCredential(key));

    var documentUri = new Uri(&quot;https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-invoice.pdf&quot;);

    var analyzeDocumentContent = new AnalyzeDocumentContent()
{
    UrlSource = documentUri 
};

    var operation = await _documentAnalysisClient.AnalyzeDocumentAsync(WaitUntil.Completed, &quot;Test_6_Jornalera&quot;, analyzeDocumentContent);
}
</code></pre>
<p>And the method 'AnalyzeDocumentAsync' is throwing a '404 Not Found' exception. The file url is valid. The endpoint and the key are the sames from here:
<a href=""https://i.sstatic.net/fsm8d.png"" rel=""nofollow noreferrer"">Key and endpoint</a></p>
<p>And from the ModelId is extracted from here:
<a href=""https://i.sstatic.net/BNWmh.png"" rel=""nofollow noreferrer"">Models Ids</a></p>
<p>One thing I noticed is that in the Document Intelligence section in Azure AI my project doesn't appear:
<a href=""https://i.sstatic.net/XLbfI.png"" rel=""nofollow noreferrer"">DIS section</a></p>
<p>I created the Document Intelligence project from here:
<a href=""https://i.sstatic.net/W305H.png"" rel=""nofollow noreferrer"">Created in 1</a>
<a href=""https://i.sstatic.net/uuNwA.png"" rel=""nofollow noreferrer"">Created in 2</a></p>
<p>Any ideas of what could be happening? I don't know why the method throws a 404. All the elements, urls and keys are on place.</p>
<p>I tried the documentation step by steps but it seems to be obsolete (<a href=""https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/use-sdk-rest-api?view=doc-intel-4.0.0&amp;tabs=windows&amp;pivots=programming-language-csharp"" rel=""nofollow noreferrer"">Use Document Intelligence models</a>).</p>
","2024-02-28 19:15:35","1","Question"
"78074856","78074180","","<p>Install a version of <code>click</code> package later than <code>8.0.4</code>.</p>
<p>The <code>Group.resultcallback</code> method was renamed to <code>Group.result_callback</code> in versions of <code>click</code> package greater than <code>8.0.4</code>, .</p>
","2024-02-28 13:34:22","2","Answer"
"78074180","","I Get ""TypeError: 'NoneType' object is not callable"" Error on ""@main.result_callback()"" When executing run.sh for SimpleNet","<p>I am trying yo run SimpleNet network and when I enter the command <code>bash run.sh</code> on the terminal I see the following output:</p>
<pre><code>
Matplotlib created a temporary cache directory at /tmp/matplotlib-ovjj_rt5 because the default path (/home/username/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
Traceback (most recent call last):
  File &quot;main.py&quot;, line 43, in &lt;module&gt;
    @main.result_callback()
TypeError: 'NoneType' object is not callable
</code></pre>
<p><strong>Steps to Produce</strong></p>
<ol>
<li><p>Install WSL 20.04 on a Windows 11 computer.</p>
</li>
<li><p>Install python 2.7 and 3.8.10 in WSL</p>
</li>
<li><p>Build OpenCV 4.2.0 from sources with python support.</p>
</li>
<li><p>Clone this repository <a href=""https://github.com/DonaldRR/SimpleNet"" rel=""nofollow noreferrer"">here</a> into home directory.</p>
</li>
<li><p>Install required packages listed <a href=""https://github.com/DonaldRR/SimpleNet?tab=readme-ov-file#environment"" rel=""nofollow noreferrer"">here</a> with pip3.</p>
</li>
<li><p>Download the <a href=""https://www.mvtec.com/company/research/datasets/mvtec-ad/"" rel=""nofollow noreferrer"">MVTech dataset</a> into a newly created directory under your home directory.</p>
</li>
<li><p>Extract the tar archive with the command <code>tar -xf mvtec_anomaly_detection.tar.xz</code> inside the directory into which you have downloaded the dataset.</p>
</li>
<li><p>Edit first and tenth lines of run.sh to make it point to the dataset I have just downloaded.</p>
</li>
<li><p>Enter the command <code>bash run.sh</code></p>
</li>
<li><p>See the error on your terminal</p>
</li>
</ol>
<p><strong>Envirovment:</strong></p>
<ul>
<li><p>WSL 20.04</p>
</li>
<li><p>Python 3.8.10</p>
</li>
<li><p>OpenCV 4.2.0 (Built from source)</p>
</li>
<li><p>Torch 1.12.1</p>
</li>
<li><p>Torchvision 0.13.1</p>
</li>
<li><p>numpy 1.22.4</p>
</li>
</ul>
<p><strong>What Did I Try?</strong></p>
<ul>
<li>I have tried to debug main.py code.</li>
</ul>
<p>I have seen that up until <code>@main.result_callback()</code>, it was working as expected</p>
<ul>
<li>Searched If it have happened to someone else</li>
</ul>
<p>I have seen that someone have solved the issue <a href=""https://github.com/DonaldRR/SimpleNet/issues/40"" rel=""nofollow noreferrer"">here</a>. But, I couldn't understand what exactly they did to solve the issue.</p>
<p><strong>What I Have Expected</strong></p>
<p>I have expected that as I run the command <code>bash run.sh</code> I would see that the network begin it's training.</p>
","2024-02-28 11:54:37","1","Question"
"78062833","78062445","","<p><code>langchain.vectorstores</code> has been deprecated.</p>
<p>To update/intall the packages run the following command in terminal</p>
<pre><code>%pip install --upgrade --quiet  langchain-pinecone langchain-openai langchain
</code></pre>
<p>Update your code like this:</p>
<pre><code>import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;api-key&quot;
os.environ[&quot;PINECONE_API_KEY&quot;] = &quot;pinecone-key&quot;

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings

loader = PyPDFLoader(&quot;beyond-good-and-evil.pdf&quot;)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
# print(docs)

embeddings = OpenAIEmbeddings()

from langchain_pinecone import PineconeVectorStore

index_name = &quot;companion&quot;

docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)

query = &quot;What is the meaning life??&quot;
docs = docsearch.similarity_search(query)
print(docs[0].page_content)
</code></pre>
<p>Results :</p>
<p><a href=""https://i.sstatic.net/eV8LW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eV8LW.png"" alt=""enter image description here"" /></a></p>
","2024-02-26 17:31:39","0","Answer"
"78062445","","Index <pinecone.data.index.Index object at 0x000002655A3E1CD0> does not exist","<p>i kept trying and getting index not exisit even i hve the same index in my pinecone account but still facing same isssue</p>
<pre><code>
i kept trying and getting index not exisit even i hve the same index in my pinecone account but still facing same isssue 

from langchain import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.embeddings import huggingface,HuggingFaceEmbeddings
from langchain.vectorstores import Pinecone
import pinecone
from langchain.embeddings import OpenAIEmbeddings
from pinecone import Pinecone
from langchain.document_loaders import PyPDFLoader,DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain import text_splitter
from langchain.prompts import PromptTemplate
from langchain.llms import CTransformers```

</code></pre>
<p>#etract data from pdf
def load_pdf(data):
loader= DirectoryLoader(data,glob=&quot;*.pdf&quot;,loader_cls=PyPDFLoader)
documents=loader.load()
return documents</p>
<p>extracted_data=load_pdf(&quot;data/&quot;)</p>
<p>def text_split(extracted_data):
text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)
text_chunk=text_splitter.split_documents(extracted_data)
return text_chunk```</p>
<pre><code>text_chunks=text_split(extracted_data)
print(&quot;length of my chunk :&quot; ,len(text_chunks))

embeddings=download_hugging_face_embeddings()


query_result=embeddings.embed_query(&quot;hello world&quot;)
print(&quot;length&quot;,len(query_result))

#i kept trying and getting index not exisit even i hve the same #index in my pinecone account but still facing same isssue 

from pinecone import Pinecone
from langchain.vectorstores import Pinecone as PC
from langchain.vectorstores import Pinecone as PineconeVectorStore
# initialize connection to pinecone (get API key at app.pinecone.io)
api_key = API_KEY

pc = Pinecone(api_key=api_key)
index_name =pc.Index(&quot;medical-chatbot&quot;)

pc.list_indexes()
docsearch = PineconeVectorStore(
    index_name, embeddings, [t.page_content for t in text_chunks]
)

pc = Pinecone(api_key=api_key)
index_name =pc.Index(&quot;medical-chatbot&quot;)
docsearch=Pinecone.from_existing_index(index_name, embeddings)

query = &quot;What are Allergies&quot;

docs=docsearch.similarity_search(query, k=3)

print(&quot;Result&quot;, docs)
</code></pre>
","2024-02-26 16:25:38","0","Question"
"78051138","78050943","","<p><a href=""https://python.langchain.com/docs/use_cases/question_answering/sources"" rel=""nofollow noreferrer"">from docs</a></p>
<blockquote>
<h2>Returning sources</h2>
<p>Often in Q&amp;A applications it’s important to show users the sources
that were used to generate the answer. The simplest way to do this is
for the chain to return the Documents that were retrieved in each
generation.</p>
</blockquote>
<pre><code>from langchain_core.runnables import RunnableParallel

rag_chain_from_docs = (
    RunnablePassthrough.assign(context=(lambda x: format_docs(x[&quot;context&quot;])))
    | prompt
    | llm
    | StrOutputParser()
)

rag_chain_with_source = RunnableParallel(
    {&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()}
).assign(answer=rag_chain_from_docs)

rag_chain_with_source.invoke(&quot;What is Task Decomposition&quot;)
</code></pre>
","2024-02-24 03:33:59","0","Answer"
"78050943","","How to return Document that was used to answer a question using i am using RetrievalQA?","<p>I am using python and langchain RetrievalQA for RAG. Here is sample of code that i have written:</p>
<pre><code>loader = UnstructuredPDFLoader(filename, mode=&quot;elements&quot;)
data = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 0)
splits = text_splitter.split_documents(data)
splits = chromautils.filter_complex_metadata(splits)
vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_function, persist_directory=PERSIST_DIRECTORY)
llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;, temperature=0, openai_api_key=OPENAI_KEY)
qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={&quot;prompt&quot;: prompt},
        return_source_documents=True
    )
result = qa_chain({&quot;query&quot;: question})
</code></pre>
<p>It only returns the answer to question. How to return Document from <code>splits</code> that was used as context to answer the question?</p>
","2024-02-24 01:22:37","0","Question"
"78047008","78027292","","<p>watsonx.ai prompt lab doesn't save the prompts to call them. It saves them so you can work on them later.</p>
<p>If you click on the <code>&lt;/&gt;</code> it will give you the prompt details you need to fill out when sending your message from watsonx Assistant.</p>
<p>On the same tab there is an (i) information button that will link you to where you get your API key.</p>
","2024-02-23 11:17:37","0","Answer"
"78041567","","Cannot use Hugginfaceembedding in the process, need helps on FAISS","<pre class=""lang-py prettyprint-override""><code>import logging, os, pickle, torch, time
import streamlit as st
from streamlit_extras.add_vertical_space import add_vertical_space
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceInstructEmbeddings
from langchain_community.embeddings.openai import OpenAIEmbeddings
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel, AutoConfig, Pipeline
from dotenv import load_dotenv

# skip for streamlit process
        path = &quot;instructor_xl&quot;
        tokenizer = AutoTokenizer.from_pretrained(path)
        model = AutoModel.from_pretrained(path)

        token_texts = tokenizer(chunks, return_tensors=&quot;pt&quot;, padding=True, truncation=True)
        model = model.to(device)
        embeddings = model(**token_texts)
        duration = time.time() - start
        logging.info(f&quot;time to embed the books on {device}: {duration}&quot;)

        VectorStore = FAISS.from_texts(chunks, embeddings)
</code></pre>
<p>So the path is local path, with using Autotokenizer and AutoModel, I can run it in batches. However, the <code>FAISS.from_texts</code> cannot take the argument <code>embeddings</code>, because of the error <code>no attribute embed_document</code>.</p>
<p>Should I build the FAISS from scratch or any other libraries can help me?</p>
","2024-02-22 14:00:03","0","Question"
"78031235","78029160","","<p>Split the action into two actions, on's case and off's case?
It is probably <code>or</code> that is causing the error.</p>
","2024-02-21 02:16:25","0","Answer"
"78029160","","Resolving PDDL Parsing Errors in plasp while Successful in Fast Downward","<p>I am working on a PDDL project for a LightsOut game. While my PDDL code executes successfully in Fast Downward, I face parsing errors in the Visual Studio Code PDDL plugin, and plasp is unable to process it correctly.</p>
<p>Here's a part of my PDDL code causing issues:</p>
<pre><code>(define (domain lightsout)
    (:requirements :strips :disjunctive-preconditions :conditional-effects :typing)

    (:types
        light 
    )

    (:predicates
        (on ?l - light) 
        (off ?l - light) 
        (adjacent ?l1 ?l2 - light) 
    )

    (:action toggle
        :parameters (?l - light)
        :precondition (or (on ?l) (off ?l)) 
        :effect (and
            
            (when (on ?l) (and (not (on ?l)) (off ?l)))
            (when (off ?l) (and (not (off ?l)) (on ?l)))
            
            (forall (?adj - light) 
                (when (adjacent ?l ?adj)
                    (and
                        (when (on ?adj) (and (not (on ?adj)) (off ?adj)))
                        (when (off ?adj) (and (not (off ?adj)) (on ?adj)))
                    )
                )
            )
        )
    )
)

</code></pre>
<p>The errors reported by the VSC PDDL plugin at line 25 are:</p>
<p>&quot;Syntax error in (and ...)&quot;
&quot;Syntax error in action declaration.&quot;
&quot;Unreadable structure&quot;
In addition, plasp is unable to parse this PDDL code. I believe the issue is related to the nested when and forall structures in the action. How can I modify my PDDL code to resolve these parsing errors while maintaining the intended functionality for my game? Are there alternative structures or refactoring techniques I can use to make the code compatible with both the VSC PDDL plugin and plasp?</p>
<p>Any advice or insights to solve these parsing issues would be greatly appreciated.</p>
","2024-02-20 17:02:56","0","Question"
"78027292","","Call watsonx.ai prompt in watson assistant","<p>I created a prompt and satisified with the results. Now I want to connect that prompt i just created to my watson assistant. I googled and knew that it it ls possible by creating custom extensions.  <a href=""https://developer.ibm.com/tutorials/integrate-your-watson-assistant-chatbot-with-watsonxai-for-generative-ai/"" rel=""nofollow noreferrer"">https://developer.ibm.com/tutorials/integrate-your-watson-assistant-chatbot-with-watsonxai-for-generative-ai/</a>. But this example doesn't call the a specific prompt.</p>
<p>How can i specifically call the prompt I made through watson assistant?</p>
<p>Thanks in advance.</p>
","2024-02-20 12:23:45","0","Question"
"78025327","","getting error while importing agents, task, process and crew from crewai","<p>i am getting error , while importing agents, task, process and crew from crewai
error i am getting - &quot;TypeError: unsupported operand type(s) for |: 'type' and 'NoneType' &quot;
&quot;</p>
<p>code - from crewai import agents , process , crew, Task
error - TypeError: unsupported operand type(s) for |: 'type' and 'NoneType'</p>
<p>i tryed it on jupiter notbook as well as vr code</p>
","2024-02-20 07:03:54","1","Question"
"78020093","","Chatbot that dynamically selects between database or PDF based on user input","<p>I have developed two separate chatbots: one that searches data from my database and another that searches from PDF documents. Now, I want to create a unified chatbot that intelligently decides where to search based on the user's input.</p>
<p>Could someone please suggest approaches or algorithms for implementing this decision-making functionality in the chatbot?</p>
<p>As per example:</p>
<p><code>User Question --&gt; Do some analysis --&gt; (Search from PDF OR Search from DB) --&gt; Provide output.</code></p>
","2024-02-19 10:50:38","4","Question"
"77990527","77989391","","<p><em>Note: The code below works with the <a href=""https://platform.openai.com/docs/api-reference/assistants-v1"" rel=""nofollow noreferrer"">OpenAI Assistants API <code>v1</code></a>. In April 2024, the <a href=""https://platform.openai.com/docs/api-reference/assistants"" rel=""nofollow noreferrer"">OpenAI Assistants API <code>v2</code></a> was released. See the <a href=""https://platform.openai.com/docs/assistants/migration/agents"" rel=""nofollow noreferrer"">migration guide</a>.</em></p>
<hr />
<p>What you want to use is the <a href=""https://platform.openai.com/docs/assistants/overview/agents"" rel=""nofollow noreferrer"">Assistants API</a>.</p>
<p>As of today, there are 3 <a href=""https://platform.openai.com/docs/assistants/tools/tools-beta"" rel=""nofollow noreferrer"">tools</a> available:</p>
<ul>
<li><a href=""https://platform.openai.com/docs/assistants/tools/code-interpreter"" rel=""nofollow noreferrer"">Code Interpreter</a></li>
<li><a href=""https://platform.openai.com/docs/assistants/tools/knowledge-retrieval"" rel=""nofollow noreferrer"">Knowledge Retrieval</a></li>
<li><a href=""https://platform.openai.com/docs/assistants/tools/function-calling"" rel=""nofollow noreferrer"">Function calling</a></li>
</ul>
<p>You need to use the Knowledge Retrieval tool. As stated in the official <a href=""https://platform.openai.com/docs/assistants/tools/knowledge-retrieval"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<blockquote>
<p>Retrieval augments the Assistant with knowledge from outside its
model, such as proprietary product information or documents provided
by your users. Once a file is uploaded and passed to the Assistant,
OpenAI will automatically chunk your documents, index and store the
embeddings, and implement vector search to retrieve relevant content
to answer user queries.</p>
</blockquote>
<p>I've built a customer support chatbot in the past. Take this as an example. In your case, you want the assistant to use your PDF file (I used the <code>knowledge.txt</code> file). Take a look at my <a href=""https://github.com/rokbenko/ai-playground/tree/main/openai-tutorials/4-Build_customer_support_chatbot"" rel=""nofollow noreferrer"">GitHub</a> and <a href=""https://youtu.be/xbgX8fu78DI"" rel=""nofollow noreferrer"">YouTube</a>.</p>
<p><strong>customer_support_chatbot.py</strong></p>
<pre><code>import os
from openai import OpenAI
client = OpenAI()
OpenAI.api_key = os.getenv('OPENAI_API_KEY')

# Step 1: Upload a File with an &quot;assistants&quot; purpose
my_file = client.files.create(
  file=open(&quot;knowledge.txt&quot;, &quot;rb&quot;),
  purpose='assistants'
)
print(f&quot;This is the file object: {my_file} \n&quot;)

# Step 2: Create an Assistant
my_assistant = client.beta.assistants.create(
    model=&quot;gpt-3.5-turbo-1106&quot;,
    instructions=&quot;You are a customer support chatbot. Use your knowledge base to best respond to customer queries.&quot;,
    name=&quot;Customer Support Chatbot&quot;,
    tools=[{&quot;type&quot;: &quot;retrieval&quot;}]
)
print(f&quot;This is the assistant object: {my_assistant} \n&quot;)

# Step 3: Create a Thread
my_thread = client.beta.threads.create()
print(f&quot;This is the thread object: {my_thread} \n&quot;)

# Step 4: Add a Message to a Thread
my_thread_message = client.beta.threads.messages.create(
  thread_id=my_thread.id,
  role=&quot;user&quot;,
  content=&quot;What can I buy in your online store?&quot;,
  file_ids=[my_file.id]
)
print(f&quot;This is the message object: {my_thread_message} \n&quot;)

# Step 5: Run the Assistant
my_run = client.beta.threads.runs.create(
  thread_id=my_thread.id,
  assistant_id=my_assistant.id,
  instructions=&quot;Please address the user as Rok Benko.&quot;
)
print(f&quot;This is the run object: {my_run} \n&quot;)

# Step 6: Periodically retrieve the Run to check on its status to see if it has moved to completed
while my_run.status in [&quot;queued&quot;, &quot;in_progress&quot;]:
    keep_retrieving_run = client.beta.threads.runs.retrieve(
        thread_id=my_thread.id,
        run_id=my_run.id
    )
    print(f&quot;Run status: {keep_retrieving_run.status}&quot;)

    if keep_retrieving_run.status == &quot;completed&quot;:
        print(&quot;\n&quot;)

        # Step 7: Retrieve the Messages added by the Assistant to the Thread
        all_messages = client.beta.threads.messages.list(
            thread_id=my_thread.id
        )

        print(&quot;------------------------------------------------------------ \n&quot;)

        print(f&quot;User: {my_thread_message.content[0].text.value}&quot;)
        print(f&quot;Assistant: {all_messages.data[0].content[0].text.value}&quot;)

        break
    elif keep_retrieving_run.status == &quot;queued&quot; or keep_retrieving_run.status == &quot;in_progress&quot;:
        pass
    else:
        print(f&quot;Run status: {keep_retrieving_run.status}&quot;)
        break
</code></pre>
","2024-02-13 19:45:51","2","Answer"
"77989391","","How do I extract data from a document using the OpenAI API?","<p>I want to extract key terms from rental agreements.</p>
<p>To do this, I want to send the PDF of the contract to an AI service that must return some key terms in JSON format.</p>
<p>What are some of the different libraries and companies that can do this? So far, I've explored the OpenAI API, but it isn't as straightforward as I would have imagined.</p>
<p>When using the ChatGPT interface, it works very well, so I thought using the API should be equally simple.</p>
<p>It seems like I need to read the PDF text first and then send the text to OpenAI API.</p>
<p>Any other ideas to achieve this will be appreciated.</p>
","2024-02-13 16:13:32","3","Question"
"77987235","77986616","","<p>Replace the import:</p>
<pre><code>from llama_index.core.embeddings import resolve_embed_model
</code></pre>
<p>with this import:</p>
<pre><code>from llama_index.core.embeddings.utils import resolve_embed_model
</code></pre>
<p>They seem to be moving things around, and the tutorial is not well synchronised with the latest release.</p>
","2024-02-13 10:35:23","1","Answer"
"77986616","","Llama index core embbedings","<pre class=""lang-py prettyprint-override""><code>from llama_index.core.embeddings import resolve_embed_model
</code></pre>
<p>error:</p>
<pre><code>ImportError: cannot import name 'resolve_embed_model' from
 'llama_index.core.embeddings' (/usr/local/lib/python3.10/dist-packages/llama_index/core/embeddings/__init__.py)
</code></pre>
<p>After installing the Llama index , I am getting the error above.</p>
","2024-02-13 08:55:46","1","Question"
"77968544","","Langchain4j pgvector implementation as an EmbeddingStore?","<p>I'm building a RAG based AI service using Langchain4j. I have one microservice that is ingesting and saving my documents (pdfs, csv, words...) in my PostgreSQL DB (with vector extension) as embeddings.</p>
<p>From the other hand I'm building another microservice to hold the AI conversation logic.</p>
<p>To do this I'm creating the next beans</p>
<pre><code>    @Bean
public EmbeddingStore&lt;TextSegment&gt; embeddingStore() {
    return new InMemoryEmbeddingStore&lt;&gt;();
}

    @Bean
public ContentRetriever contentRetriever() {
    return EmbeddingStoreContentRetriever.builder()
            .embeddingStore(embeddingStore())
            .embeddingModel(bedrockTitanEmbeddingModel())
            .maxResults(10) // on each interaction we will retrieve the 5 most relevant segments
            .minScore(0.2) // we want to retrieve segments very similar to the user query
            .build();
}
    @Bean
public RetrievalAugmentor retrievalAugmentor() {
    return DefaultRetrievalAugmentor.builder()
            .queryTransformer(queryTransformer())
            .contentRetriever(contentRetriever())
            .build();
}

    @Bean
public AiAgent aiAgent() {
    return AiServices.builder(ErekyAiAgent.class)
            .retrievalAugmentor(retrievalAugmentor())
            .chatLanguageModel(bedrockAnthropicChatModel())
            .contentRetriever(contentRetriever())
            .build();
}
</code></pre>
<p>The <code>ContentRetriever</code> is asking me as a mandatory parameter the embeddingStore. Now for testing I'm using the memory one but I saw that Langchain4j has an implementation with pgvector.</p>
<p>In the flow what I'm doing is:</p>
<ol>
<li>Doing the query to my PostgreSQL database with the user asked question</li>
<li>Returning the document text list found</li>
<li>Transforming the List of Strings containing the document text I got to a list of <code>List&lt;TextSegment&gt;</code> that is a type of langchain4j library.</li>
<li>Then I need to transform the <code>List&lt;TextSegment&gt;</code> to embeddings again and add them along with the <code>List&lt;TextSegment&gt;</code> without embedding them to the embedding store I'm using.</li>
</ol>
<p>The logic is</p>
<pre><code>List&lt;String&gt; documentTexts = getDocumentTextsFromUserQuestion(promptDto);
        List&lt;TextSegment&gt; textSegments = getTextSegments(documentTexts);
        embeddingStore.addAll(embedComponent.getEmbeddingsFromTextSegments(textSegments), textSegments);
        return new PromptDTO(aiAgent.answer(documentTexts, promptDto.getText()));
</code></pre>
<p>I saw that for some reason the logic always need for me to add that data to the embedding store to be able to give a correct answer based on my data. When I used the pgvector implementation of Langchain4j and did the same thing I saw that the implementation is creating a table in my DB with the data I already had saved before inserted in this new table to give the answer. And the data is being duplicated, there is a way to make this work without that?</p>
<p>And since I already have the data saved in the DB, I can't directly do the call to the AI with the data found from the user question + the user question?</p>
<p>I did it like this in Python calling chain.run being documents the data found in the DB and the question being the user question and it works and I don't need this intermmediate embedding store.</p>
<pre><code>chain = load_qa_chain(llm, chain_type=&quot;stuff&quot;)
    # Call to the model
    # response = st.session_state.conversation({'question': user_question})
    response = chain.run(input_documents=document, question=user_question)
</code></pre>
","2024-02-09 13:41:41","0","Question"
"77968199","77968083","","<p>Its very well explained in the documentation:</p>
<h3>Composite Keys</h3>
<p>When using an existing DynamoDB table, you may need to modify the key structure from the default of to something including a Sort Key. To do this you may use the key parameter.</p>
<p>Passing a value for key will override the primary_key parameter, and the resulting key structure will be the passed value.</p>
<pre><code>from langchain_community.chat_message_histories import DynamoDBChatMessageHistory

composite_table = dynamodb.create_table(
    TableName=&quot;CompositeTable&quot;,
    KeySchema=[
        {&quot;AttributeName&quot;: &quot;studentId&quot;, &quot;KeyType&quot;: &quot;HASH&quot;},
        {&quot;AttributeName&quot;: &quot;sessionId&quot;, &quot;KeyType&quot;: &quot;RANGE&quot;},
    ],
    AttributeDefinitions=[
        {&quot;AttributeName&quot;: &quot;PK&quot;, &quot;AttributeType&quot;: &quot;S&quot;},
        {&quot;AttributeName&quot;: &quot;SK&quot;, &quot;AttributeType&quot;: &quot;S&quot;},
    ],
    BillingMode=&quot;PAY_PER_REQUEST&quot;,
)

# Wait until the table exists.
composite_table.meta.client.get_waiter(&quot;table_exists&quot;).wait(TableName=&quot;CompositeTable&quot;)

# Print out some data about the table.
print(composite_table.item_count)
</code></pre>
<p><a href=""https://python.langchain.com/docs/integrations/memory/aws_dynamodb"" rel=""nofollow noreferrer"">src</a></p>
<p>You cannot alter keys on an existing table, you must always create a new table.</p>
","2024-02-09 12:40:48","-1","Answer"
"77968083","","Langchain DynamoDBChatMessageHistory add sortkey and customer attributes","<p>A conversational chain with DynamoDb for memory is setup.
Everything works fine, regarding retrieving and inserting messages into the memory. However, I want to add a sortkey and add custom attributes on top of the base key provided by the <code>DynamoDBChatMessageHistory</code> class.</p>
<pre><code>    const { DynamoDBChatMessageHistory } = require(&quot;@langchain/community/stores/message/dynamodb&quot;);

    const MESSAGE_HISTORY = new DynamoDBChatMessageHistory(
      {
        tableName: &quot;Conversation&quot;,
        partitionKey: &quot;studentId&quot;, // documentation requires this to be unique, 
                                   // however i want to partition using the studentId
        studentId: studentId.toString(),
        sortKey: &quot;sessionId&quot;, // i want to add this sortkey
        sessionId: `chat_${sessionId}`, // this is not applied
        dateTimeStamp: new Date(), // i want to add this custom attribute
        config: {
          region: &quot;us-east-1&quot;,
          credentials: {
            accessKeyId: process.env.ACCESS_KEY_ID,
            secretAccessKey: process.env.SECRET_ACCESS_KEY,
          },
        },
      }
    )
</code></pre>
<p>DynamoDb:</p>
<p><a href=""https://i.sstatic.net/MhXrz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MhXrz.png"" alt=""Dynamo"" /></a></p>
<p>I realize this is incorrect in terms of dynamo conventions, I need support on how to extend the base <code>DynamoDBChatMessageHistory</code> to suit my requirements.</p>
","2024-02-09 12:16:42","1","Question"
"77963646","77959410","","<p>Like you guessed, the issue is with the computational graph that gets created when you do backpropagation.</p>
<p>Let me explain the above point:</p>
<p>When you initialize a tensor in pytorch, it usually signals that the operations you perform on them should be tracked. When you do a forward pass, the functions for backward prop are set up and the graph is set.</p>
<p>In case 2, you are deleting the tensor and hence the entire process is reset -- the computation graph is reset. In case 3, you are clearly resetting the parameters.</p>
<p>The output tensor and the model parameters are connected to the graph.</p>
<p>If you want to clearly visualize where the TBackward0 function is, use torchviz to visualize the computational graph.</p>
","2024-02-08 17:22:04","-1","Answer"
"77959410","","Pruning nn.Linear weights inplace causes unexpected error, requires slightly weird workarounds. Need explanation","<h2>This fails</h2>
<pre class=""lang-py prettyprint-override""><code>import torch

def test1():  
  layer = nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test1()
</code></pre>
<p>with error</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-3-bb36a010bd86&gt; in &lt;cell line: 10&gt;()
      8     x = 5 - torch.sum(layer(torch.ones(90)))
      9     x.backward()
---&gt; 10 test1()
     11 # and this works as well
     12 

2 frames
/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    249     # some Python versions print out the first line of a multi-line function
    250     # calls in the traceback and some print out the last line
--&gt; 251     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    252         tensors,
    253         grad_tensors_,

RuntimeError: Function TBackward0 returned an invalid gradient at index 0 - got [10, 90] but expected shape compatible with [10, 100]
</code></pre>
<h2>This works</h2>
<pre class=""lang-py prettyprint-override""><code>import torch

def test2():  
  layer = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  del x    #main change
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test2()
</code></pre>
<h2>and this works as well</h2>
<pre class=""lang-py prettyprint-override""><code>import torch
def test3():  
  layer = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  layer.weight.data = layer.weight.data[:, :90]
  layer.weight.grad.data = layer.weight.grad.data[:, :90]
  layer.weight = torch.nn.Parameter(layer.weight)   #main change
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
test3()
</code></pre>
<p>I encountered this when trying to implement a paper on model pruning (Temporal Neuron Variance Pruning). I believe this has something to do with the autograd graph, but I have am not sure what exactly is going on. I've already seen the link on pruning and got my code working using the 3rd snippet. I am now trying to figure out why 1 and 2 did not work. Is there some explanation for why these almost identical code snippets work or fail?</p>
<h2>Major points I'd like to figure out -</h2>
<ol>
<li>what is <code>TBackward0</code></li>
<li>where is it defined</li>
<li>where is the runtime error raised</li>
<li>why is the compatibility with the old shape expected - especially when the grad has been modified correctly (I am assuming I have edited the tensors correctly because cases 2, 3 work)</li>
<li>can I change something else (other than the 2 working cases) to make this work ?</li>
</ol>
","2024-02-08 05:17:32","4","Question"
"77953178","77902326","","<p>In AutoGen Studio, your work (skills, agents, workflows) is saved in a <code>database.sqlite</code> file. This file is crucial for backing up and ensuring you don't lose your work (See FAQS: <a href=""https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/"" rel=""nofollow noreferrer"">https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio/</a>).</p>
<p>For managing and resetting your development environment, such as performing a factory reset, deleting the <code>database.sqlite file</code> effectively clears all your configurations and data, allowing you to start over.</p>
<p>This process is demonstrated in a video tutorial at <a href=""https://youtu.be/X6pP9zyfkyY?t=353"" rel=""nofollow noreferrer"">https://youtu.be/X6pP9zyfkyY?t=353</a>, where it's used to reset the entire conversation history with an agent.</p>
","2024-02-07 08:19:06","3","Answer"
"77942208","77941625","","<p>I would use following heuristic, first use <a href=""https://docs.python.org/3/library/stdtypes.html#str.casefold"" rel=""nofollow noreferrer""><code>.casefold</code></a> method of <code>str</code>, then take longest word, that is</p>
<pre><code>data = [
    'EXFORGE 5 MG/160 B/28 COMP',
    'EXFORGE 5MG /160 Bte 28',
    'doliprane 500MG',
    'dolipran 5.0 MG',
    'EXFORGE COMP 5 MG _160 B / 28',
    'Exforge 5 MG 160 B/28 COMP.',
    'Doliprane 500MG Comp',
    'EXFORGE CP.PEL 5MG/ 160 MG 28',
    'Ciprofloxacine 500 MG/5 ML IV 100 ML Solution for Infusion',
    'CIPROFLOXACINE 500MG/5ML IV 100 ML Sol for Inf',
    'Ciprofloxacine Sol for Infusion 500 MG/5 ML 100 ML',
    'CIPROFLOXACINE 500 MG/5 ML IV 100 ML Sol for Infusion',
    'Ciprofloxacine 500 MG IV 100 ML Sol for Inf',
    'Paracetamol 500 MG Tab 30s',
    'PARACETAMOL 500MG Tab 30s',
    'Paracetamol Tab 500 MG 30s',
    'PARACETAMOL Tab 500 MG 30s',
    'Paracetamol 500 MG 30s Tab',
    'Lisinopril 10 MG Tab 28s',
    'LISINOPRIL 10MG Tab 28s',
    'Lisinopril Tab 10 MG 28s',
    'LISINOPRIL Tab 10 MG 28s',
    'Lisinopril 10 MG 28s Tab',
    'Simvastatin 20 MG Tab 100s',
    'SIMVASTATIN 20MG Tab 100s',
    'Simvastatin Tab 20 MG 100s',
    'SIMVASTATIN Tab 20 MG 100s',
    'Simvastatin 20 MG 100s Tab',
    'Omeprazole 20 MG Caps 28s',
    'OMEPRAZOLE 20MG Caps 28s',
    'Omeprazole Caps 20 MG 28s',
    'OMEPRAZOLE Caps 20 MG 28s',
    'Omeprazole 20 MG 28s Caps'
]
def get_name(full_name):
    return max(full_name.casefold().split(), key=len)
names = [get_name(i) for i in data]
print(names)
</code></pre>
<p>gives output</p>
<pre><code>['exforge', 'exforge', 'doliprane', 'dolipran', 'exforge', 'exforge', 'doliprane', 'exforge', 'ciprofloxacine', 'ciprofloxacine', 'ciprofloxacine', 'ciprofloxacine', 'ciprofloxacine', 'paracetamol', 'paracetamol', 'paracetamol', 'paracetamol', 'paracetamol', 'lisinopril', 'lisinopril', 'lisinopril', 'lisinopril', 'lisinopril', 'simvastatin', 'simvastatin', 'simvastatin', 'simvastatin', 'simvastatin', 'omeprazole', 'omeprazole', 'omeprazole', 'omeprazole', 'omeprazole']
</code></pre>
<p>Observe that whilst it counteract various cases (lower, upper, title) but does not counter spelling errors. Keep in mind it might fail for very short names.</p>
","2024-02-05 15:50:14","0","Answer"
"77941990","77940890","","<p>Can you try upgrading your Google Auth it appears that it has some issues with the some of the versions:</p>
<pre><code>pip install --upgrade google-auth
</code></pre>
<p>Reference Issue: <a href=""https://github.com/googleapis/google-cloud-python/issues/12254"" rel=""nofollow noreferrer"">https://github.com/googleapis/google-cloud-python/issues/12254</a></p>
","2024-02-05 15:14:55","1","Answer"
"77941625","","Group similar names together using a python algorithm","<p>I need help on a project I have where I have a list of names referring to multiple products but some names refer to the same product just written in a different way (ex below) what's the best way to classify the most similar names hat are most likely referring to the same product in groups</p>
<p>Example
just one product could be named like this:</p>
<p>-EXFORGE 5 MG/160 B/28 COMP</p>
<p>-EXFORGE 5MG /160 Bte 28</p>
<p>-EXFORGE COMP 5 MG _160 B / 28</p>
<p>-Exforge 5 MG 160 B/28 COMP.</p>
<p>Tried FuzzyWuzzy library but it didn't give me a good result
this was what I did</p>
<pre><code>from fuzzywuzzy import process

# Assuming 'data' is your column of product names in tabular data
data = [
    'EXFORGE 5 MG/160 B/28 COMP',
    'EXFORGE 5MG /160 Bte 28',
    'doliprane 500MG',
    'dolipran 5.0 MG',
    'EXFORGE COMP 5 MG _160 B / 28',
    'Exforge 5 MG 160 B/28 COMP.',
    'Doliprane 500MG Comp',
    'EXFORGE CP.PEL 5MG/ 160 MG 28',
    'Ciprofloxacine 500 MG/5 ML IV 100 ML Solution for Infusion',
    'CIPROFLOXACINE 500MG/5ML IV 100 ML Sol for Inf',
    'Ciprofloxacine Sol for Infusion 500 MG/5 ML 100 ML',
    'CIPROFLOXACINE 500 MG/5 ML IV 100 ML Sol for Infusion',
    'Ciprofloxacine 500 MG IV 100 ML Sol for Inf',
    'Paracetamol 500 MG Tab 30s',
    'PARACETAMOL 500MG Tab 30s',
    'Paracetamol Tab 500 MG 30s',
    'PARACETAMOL Tab 500 MG 30s',
    'Paracetamol 500 MG 30s Tab',
    'Lisinopril 10 MG Tab 28s',
    'LISINOPRIL 10MG Tab 28s',
    'Lisinopril Tab 10 MG 28s',
    'LISINOPRIL Tab 10 MG 28s',
    'Lisinopril 10 MG 28s Tab',
    'Simvastatin 20 MG Tab 100s',
    'SIMVASTATIN 20MG Tab 100s',
    'Simvastatin Tab 20 MG 100s',
    'SIMVASTATIN Tab 20 MG 100s',
    'Simvastatin 20 MG 100s Tab',
    'Omeprazole 20 MG Caps 28s',
    'OMEPRAZOLE 20MG Caps 28s',
    'Omeprazole Caps 20 MG 28s',
    'OMEPRAZOLE Caps 20 MG 28s',
    'Omeprazole 20 MG 28s Caps'
]

# Define a threshold for similarity
threshold = 87

# Initialize groups list
groups = []

# Function to add similar names to a group
def add_to_group(group, name):
    for existing_name in group:
        if process.extractOne(existing_name, [name])[1] &gt;= threshold:
            return True
    return False

# Iterate through data to form groups
for name in data:
    added = False
    for group in groups:
        if add_to_group(group, name):
            group.add(name)
            added = True
            break
    if not added:
        groups.append({name})

# Print the groups of most similar names
for idx, group in enumerate(groups, 1):
    print(f&quot;Group {idx}: {group}&quot;)
</code></pre>
<p>and got just 3 groups instead of 7 and when I adjusted the threshold to just 88 instead of 87 it went 10 groups</p>
","2024-02-05 14:22:59","2","Question"
"77940890","","Google semantic retriever example error 'Credentials' object has no attribute 'universe_domain'","<p>Here is the code example and steps to create service account and enable API:</p>
<p><a href=""https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/docs/semantic_retriever.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/docs/semantic_retriever.ipynb</a></p>
<p>Error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-20-ee7d7add68db&gt; in &lt;cell line: 8&gt;()
      6 
      7 # Make the request
----&gt; 8 create_corpus_response = retriever_service_client.create_corpus(create_corpus_request)
      9 
     10 # Set the `corpus_resource_name` for subsequent sections.

2 frames
/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/retriever_service/client.py in _compare_universes(client_universe, credentials)
    514         &quot;&quot;&quot;
    515         if credentials:
--&gt; 516             credentials_universe = credentials.universe_domain
    517             if client_universe != credentials_universe:
    518                 default_universe = RetrieverServiceClient._DEFAULT_UNIVERSE

AttributeError: 'Credentials' object has no attribute 'universe_domain'
</code></pre>
","2024-02-05 12:20:31","-1","Question"
"77933348","77933173","","<p>You're doing a lot of weird stuff here that breaks the gradient chain.</p>
<p>When you call <code>.item()</code> on a tensor, you convert it to a scalar python value, which removes any gradient tracking.</p>
<p>Adding <code>tensor.requires_grad_(True)</code> only tracks gradients from that point onward. Since you computed that tensor by calling <code>.item()</code> at various points, there's no way for the gradient to propagate back.</p>
<p>Also the gradient isn't computed until you call <code>.backward()</code> on your loss, so no matter what your statement <code>print(&quot;Gradient of tensor:&quot;, tensor.grad)</code> in the forward pass will always return <code>None</code>.</p>
<p>Overall, it looks like your model is doing feature engineering within the model itself (ie the <code>salary</code> and <code>debt</code> calculations) in addition to the actual modeling part. You want to separate these.</p>
<p>An additional minor point, it looks like your model is creating a fixed length vector (<code>tensor = torch.tensor([x[6], x[7], x[8], x[10], x[11], x[12], self.mathematical_modeling(x).item()])</code>). If that is the case, there is no need to use a RNN, you can just use a MLP.</p>
","2024-02-03 19:00:32","0","Answer"
"77933173","","Why isn't all the weight of the model updated?","<p>I'm making a model that predicts grades. It is a model that calculates with RNN using the value calculated through mathematical modeling. However, from the weight used for mathematical modeling to the weight of RNN, everything has not been updated. As a result of checking, the gradients of each weight are all being output as None.</p>
<p>I wonder why the weight isn't being updated.</p>
<p>This code is modified.</p>
<pre><code>class Expector(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers):
    super(Expector, self).__init__()

    self.weight_0 = nn.Parameter(torch.tensor([0.2]))
    self.weight_1 = nn.Parameter(torch.tensor([0.8, 0.7, 0.6, 0.5])) #homeownership weight
    self.weight_2 = nn.Parameter(torch.tensor([0.5 for i in range(0, 13)])) #loan_purpose weight

    #interesst_rate_expector
    self.num_features = 13
    self.linear_1 = torch.nn.Linear(self.num_features, self.num_features*2)
    self.linear_2 = torch.nn.Linear(self.num_features*2, self.num_features*4)
    self.linear_3 = torch.nn.Linear(self.num_features*4, self.num_features*8)
    self.linear_out = torch.nn.Linear(self.num_features*8, 1)

    #loan_rating_expector
    self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
    self.fc = nn.Linear(hidden_size, input_size)

  def interest_rate_expector(self, x):
    x = [x[i] for i in range(len(x)-1)]
    for i in [0,4,9,10,11]:
      if x[i] &gt; 0:
        x[i] = math.log(x[i])
    input = torch.tensor(x).to(DEVICE)
    out1 = self.linear_1(input)
    out1 = torch.nn.functional.softplus(out1)
    out2 = self.linear_2(out1)
    out2 = torch.nn.functional.softplus(out2)
    out3 = self.linear_3(out2)
    out3 = torch.nn.functional.softplus(out3)
    logits = self.linear_out(out3)
    interest_rate = torch.sigmoid(logits)

    return interest_rate

  def mathematical_modeling(self, x):
    salary = x[4] / 12 if x[4] != 0 else 300000
    debt = x[4] * (1 / x[5]) if x[5] != 0 else 0
  
    interest_rate = float(self.interest_rate_expector(x))

    if x[2] &gt; 4:
        repayment = (x[0] * interest_rate * ((1 + interest_rate) ** x[1])) / ((1 + interest_rate) ** x[1]) - 1
        result = (debt * self.weight_0[0] + repayment) / (salary * x[3])
    else:
        result = (debt * self.weight_0[0] + x[0] * x[1]) / (salary * x[3])

    return torch.tensor(result)

  def forward(self, x):
    x[3] = torch.matmul(one_hot_encoding1(x[3]), self.weight_1.view(-1, 1))
    x[7] = torch.matmul(one_hot_encoding2(x[7]), self.weight_2.view(-1, 1))

    tensor = torch.tensor([x[6], x[7], x[8], x[10], x[11], x[12], self.mathematical_modeling(x)])

    tensor = tensor.unsqueeze(0)
    output, _ = self.rnn(tensor.to(DEVICE))
    final_output = self.fc(output)
    output_probabilities = F.softmax(final_output, dim=1)
    
    return output_probabilities
</code></pre>
<ol>
<li>I have explicitly set requests_grad=True.</li>
<li>The mathematical_modeling function attempted to return the tensor.</li>
<li>I've adjusted the learning rate.</li>
</ol>
<p>Despite these efforts, the gradient is not updated, and when I check, I only see the phrase Gradient of sensor: None. Please help me.</p>
","2024-02-03 18:02:18","0","Question"
"77932783","77915597","","<p>To disable Docker in AutoGen version 0.2.8 and later, set code_execution_config to false in your configuration, as Docker is now enabled by default.</p>
<p>For a detailed guide on how to make this change, you can watch this tutorial: <a href=""https://youtu.be/aWml0ncqPnU?t=195"" rel=""nofollow noreferrer"">https://youtu.be/aWml0ncqPnU?t=195</a></p>
","2024-02-03 16:01:22","1","Answer"
"77932581","77932443","","<p>Please see if you are using <code>ChatMessage</code> anywhere else in your notebook.
I would suggest to create a new virtual environment and then try the code.</p>
<p>I can see that <code>'model_dump'</code> is available in <code>ChatMessage</code>.</p>
<p>I have <code>mistralai-0.0.12</code> and <code>poetry-1.7.1</code> on my google colab.</p>
<pre><code>print('model_dump' in dir(ChatMessage))
#True
</code></pre>
<p>All the methods in <code>ChatMessage</code> are:</p>
<pre><code>print(dir(ChatMessage))
#['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'json', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'update_forward_refs', 'validate']
</code></pre>
","2024-02-03 15:01:10","0","Answer"
"77932443","","mistralai - AttributeError: 'ChatMessage' object has no attribute 'model_dump'","<p>I am trying to run a Mistral AI's <a href=""https://docs.mistral.ai/platform/client/#installation"" rel=""nofollow noreferrer"">python client code example</a> shown below.</p>
<pre class=""lang-py prettyprint-override""><code>from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

model = &quot;mistral-tiny&quot;

client = MistralClient(api_key=userdata.get('MISTRAL_API_KEY'))

messages = [
    ChatMessage(role=&quot;user&quot;, content=&quot;What is the best French cheese?&quot;)
]

# No streaming
chat_response = client.chat(
    model=model,
    messages=messages,
)
</code></pre>
<p>I keep getting <code>AttributeError: 'ChatMessage' object has no attribute 'model_dump'</code> but there is nothing about this anywhere.</p>
<p>Can someone help please?</p>
","2024-02-03 14:23:01","2","Question"
"77929633","77929619","","<p>The type error is because you are using <code>in</code> on a class that does not store values. <code>in</code> only works on sets, lists, and other data structures that contain values.</p>
<p>If you want to see whether something has the <code>chat_session</code> attribute, you can try <code>hasattr(st.session_state, &quot;chat_session&quot;)</code>.</p>
<p>See also:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/610883/how-to-check-if-an-object-has-an-attribute"">How to check if an object has an attribute?</a></li>
<li><a href=""https://www.w3schools.com/python/ref_keyword_in.asp"" rel=""nofollow noreferrer"">Python's <code>in</code> keyword</a></li>
</ul>
","2024-02-02 20:12:22","0","Answer"
"77929619","","Type Error when trying to make a LLM chatbot using Gemini-Pro","<p>Here is the code</p>
<pre><code>import os 

import streamlit as st 
from dotenv import load_dotenv 
import google.generativeai as gen_ai  

load_dotenv() 

st.set_page_config(
    page_title=&quot;Chat with Gemini Pro&quot;,
    page_icon=&quot;:brain:&quot;,
    layout=&quot;centered&quot; 
)

GOOGLE_API_KEY = os.getenv(&quot;GOOGLE_API_KEY&quot;) 
gen_ai.configure(api_key=GOOGLE_API_KEY) 
model = gen_ai.GenerativeModel(&quot;gemini-pro&quot;) 

def translate_role_for_streamlit(user_role): 
  if user_role == &quot;model&quot;: 
    return &quot;assistant&quot; 
  else: 
    return user_role 

if &quot;chat_session&quot; not in st.session_state:
  st.session_state.chat_session = model.start_chat(history=[])
st.title(&quot;CAIE Bot&quot;) 

for message in st.session_state.chat_session.history:
  with st.chat_message(translate_role_for_streamlit(message.role)): 
    st.markdown(message.parts[0].text) 

user_prompt = st.chat_input(&quot;Ask CAIE bot...&quot;) 
if user_prompt: 
  st.chat_message(&quot;user&quot;).markdown(user_prompt) 
  gemini_response = st.session_state.chat_session.send_message(user_prompt) 
  with st.chat_message(&quot;assistant&quot;): 
    st.markdown(gemini_response.text)  

!streamlit run main.py
</code></pre>
<p>I'm getting error in line</p>
<pre><code>if &quot;chat_session&quot; not in st.session_state:
</code></pre>
<p>I wanted to check if user had an active chat with the bot and if it did, then the bot would save it previous conversation history for the next conversation. And after a session is over, history would then reset</p>
","2024-02-02 20:09:24","0","Question"
"77928953","77928887","","<p>From your logs, it looks like you are using <code>python 3.12</code></p>
<p>Document says that chatterbot only support till <code>python 3.8</code></p>
<p><a href=""https://i.sstatic.net/sFS1P.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sFS1P.png"" alt=""enter image description here"" /></a></p>
<p>You need to install <code>python 3.8</code> separately to work on chatterbot.</p>
<p><a href=""https://pypi.org/project/ChatterBot/"" rel=""nofollow noreferrer"">https://pypi.org/project/ChatterBot/</a></p>
","2024-02-02 17:45:03","1","Answer"
"77928887","","ChatterBot throws error when creating bot","<p>I am working on a ChatterBot project in Python, and I am encountering an error during training. The error message is as follows:</p>
<pre><code>[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data] C:\Users\Adam\AppData\Roaming\nltk_data...
[nltk_data] Package averaged_perceptron_tagger is already up-to-
[nltk_data] date!
[nltk_data] Downloading package punkt to
[nltk_data] C:\Users\Adam\AppData\Roaming\nltk_data...
[nltk_data] Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data] C:\Users\Adam\AppData\Roaming\nltk_data...
[nltk_data] Package stopwords is already up-to-date!
Traceback (most recent call last):
File &quot;C:\Users\Adam\Downloads\bot].py&quot;, line 12, in &lt;module&gt;
trainer.train(&quot;chatterbot.corpus.english.greetings&quot;,
File &quot;C:\Users\Adam\AppData\Local\Programs\Python\Python312\Lib\site-packages\chatterbot\trainers.py&quot;, line 138, in train
for corpus, categories, file_path in load_corpus(*data_file_paths):
File &quot;C:\Users\Adam\AppData\Local\Programs\Python\Python312\Lib\site-packages\chatterbot\corpus.py&quot;, line 63, in load_corpus
corpus_data = read_corpus(file_path)
File &quot;C:\Users\Adam\AppData\Local\Programs\Python\Python312\Lib\site-packages\chatterbot\corpus.py&quot;, line 38, in read_corpus
return yaml.load(data_file)
TypeError: load() missing 1 required positional argument: 'Loader'
</code></pre>
<p>I am using the ChatterBot library, and the code I am running is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer
import yaml
import time

chatbot = ChatBot('phil')
trainer = ChatterBotCorpusTrainer(chatbot)

trainer.train(&quot;chatterbot.corpus.english.greetings&quot;,
              &quot;chatterbot.corpus.english.conversations&quot;)

while True:
    try:
        user_input = input(&quot;You: &quot;)
        bot_response = chatbot.get_response(user_input)
        print(f&quot;MyChatBot: {bot_response}&quot;)
    except (KeyboardInterrupt, EOFError, SystemExit):
        break
````      note: This error originates from a subprocess, and is likely not a problem with pip.
        ERROR: Failed building wheel for srsly
      Failed to build preshed thinc blis srsly
      ERROR: Could not build wheels for preshed, thinc, blis, srsly, which is required to install pyproject.toml-based projects
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─&gt; See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
Chatterbot is installed as 1.2.0 as it threw errors when i tried to upgrade: 
</code></pre>
<p>I would like my code to be a chatbot console, and the chatbot could access a database from a public training database. Please could you help me</p>
","2024-02-02 17:32:21","0","Question"
"77915597","","Need help in turn off docker in 'Autogen AutoBuilder""","<p>I am new and struggling to turn off Docker. I like to either set 'use_docker': false or set up environment variable. Either option will work. Thanks for your help. The screenshot has been enclosed.
<a href=""https://i.sstatic.net/5Vhqt.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5Vhqt.gif"" alt=""enter image description here"" /></a></p>
","2024-01-31 18:14:23","0","Question"
"77902326","","AutoGen 2.0 Where are my workflows saved?","<p>I'm working in windows, with Microconda and Microsoft AutoGen 2.0.  I got AutoGen up and running, and have started creating skills.</p>
<p>I want to understand where my work (skills, agents, workflows) is stored, so that I can ensure it's backed up, etc. so I don't lose it.</p>
<p>And along the same lines, can I control where I want my work stored, so that I can have it together with all my other various dev projects.  I'm just not seeing any options in the tool or the documentation.</p>
","2024-01-29 20:02:56","0","Question"
"77899104","77897242","","<p>You don't show exactly how you're doing &quot;document extraction&quot;, but you suggest that you're giving it a URL.</p>
<p>Gemini is <em>just</em> a model and does not have a component that will access the URL in question and return information from it. All the text generated was based on token generation with the most context provided what it is able to get from the URL itself.</p>
<p>If you want to do document extraction for most of the Gemini models, you'll need to provide the content of the page yourself.</p>
<p>While Bard is built on Gemini - it includes other components that are tuned to work with web resources specifically.</p>
","2024-01-29 10:41:03","0","Answer"
"77897242","","Gemini PRO returns more details than on the suggested Page/URL","<p>I was playing around with Gemini Pro on Document extraction and for trial i gave a sample URL , say this Uber API reference page and requested an extract of all Rest points in the page. I see that the model works however it is returning much more data than what is on the page. How can one limit the scope of the model to the suggested documents alone ? I tried playing around with temperature and other modifiers with no luck. Any advice on how to solve this will be very helpful.</p>
<p><a href=""https://developer.uber.com/docs/riders/references/api"" rel=""nofollow noreferrer"">https://developer.uber.com/docs/riders/references/api</a></p>
<p>I see that Bard is more accurate than Gemini Pro for the same task.</p>
","2024-01-29 03:03:42","0","Question"
"77895429","77891961","","<p>The condition <code>if hasattr(results, 'xyxy')</code> here is always negative. The <a href=""https://docs.ultralytics.com/modes/predict/#working-with-results"" rel=""nofollow noreferrer"">available attributes</a> of the <code>results</code> are:</p>
<pre><code>orig_img, orig_shape, boxes, masks, probs, keypoints, obb, speed, names, path
</code></pre>
<p>To get the <strong>xyxy</strong> box coordinates, <strong>score</strong>, and <strong>class_id</strong> please refer to the <code>results.boxes</code>. The <a href=""https://docs.ultralytics.com/modes/predict/#boxes"" rel=""nofollow noreferrer"">available properties</a> of the <code>boxes</code> are:</p>
<pre><code>xyxy, conf, cls, id, xywh, xyxyn, xywhn
</code></pre>
<p>All of them are returning in the form of a <code>torch.Tensor</code>. To get the row values you can do the following:</p>
<pre><code># Iterate through the list of results
for results in results_list:
    # Check if the current result has the necessary attributes
    if hasattr(results, 'boxes'):
        for box in results.boxes:
            x1, y1, x2, y2 = box.xyxy.tolist()[0]
            x1, x2, y1, y2 = int(x1), int(x2), int(y1), int(y2)
            score = box.conf.item()
            class_id = int(box.cls.item())
</code></pre>
","2024-01-28 16:03:10","0","Answer"
"77894003","","Studio Bot Missing in latest andriod studio bot","<p>I have just installed latest version of andriod studio lguana. But it is missing the studio Bot an AI code companion. I have searched a lot but unable to find it. <a href=""https://i.sstatic.net/yCYK0.png"" rel=""nofollow noreferrer"">Preview of lguana more tools expanded</a></p>
<p>I want to know why studio bot is missing?</p>
","2024-01-28 07:49:11","0","Question"
"77893590","77890548","","<p>I found the cause of this problem.
It is the difference between running in py or python.</p>
<pre><code># Success example:
python main.py

# Failure example:
py main.py.
```
</code></pre>
","2024-01-28 03:51:44","1","Answer"
"77891961","","I have my custom trained model (best.pt), it detects two things person and headlight. Now I want the output according to these conditions","<p>can you please help me.........
I have my custom trained model (best.pt), it detects two things person and headlight. Now I want the output according to these conditions: 1. If model detect only headlight return 0, 2. If model detect only person return 1, 3. If model detect headlight and person both return 0.</p>
<pre><code>import cv2
from ultralytics import YOLO

video_path = 'data/video1.mp4'
video_out_path = 'out.mp4'

cap = cv2.VideoCapture(video_path)

# Check if the video file is opened successfully
if not cap.isOpened():
    print(&quot;Error: Could not open the video file.&quot;)
    exit()

ret, frame = cap.read()

# Check if the first frame is read successfully
if not ret:
    print(&quot;Error: Could not read the first frame from the video.&quot;)
    exit()

cap_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS),
                          (int(cap.get(3)), int(cap.get(4))))  # Use cap.get(3) and cap.get(4) for width and height

model = YOLO(&quot;bestall5.pt&quot;)

detection_threshold = 0.5
while ret:
    results_list = model(frame)

    headlight_detected = False
    person_detected = False

    # Iterate through the list of results
    for results in results_list:
        # Check if the current result has the necessary attributes
        if hasattr(results, 'xyxy'):
            for result in results.xyxy:
                x1, y1, x2, y2, score, class_id = result.tolist()
                x1, x2, y1, y2 = int(x1), int(x2), int(y1), int(y2)

                # Assuming class_id is the index of the class in the model's class list
                class_name = model.names[class_id]

                if class_name == &quot;headlight&quot; and score &gt; detection_threshold:
                    headlight_detected = True
                elif class_name == &quot;person&quot; and score &gt; detection_threshold:
                    person_detected = True

    # Output based on the specified conditions
    if headlight_detected and person_detected:
        output = 0
    elif headlight_detected:
        output = 0
    elif person_detected:
        output = 1
    else:
        output = -1  # No person or headlight detected

    print(&quot;Output:&quot;, output)

    cap_out.write(frame)

    cv2.imshow('Object Detection', frame)
    
    # Break the loop if 'q' key is pressed
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

    ret, frame = cap.read()

cap.release()
cap_out.release()
cv2.destroyAllWindows()
</code></pre>
<p>I tried this but getting only -1 as output but my video has both headlight and person</p>
","2024-01-27 16:40:44","-1","Question"
"77890548","","ValueError: unet/hotshot_xl.py does not exist in hotshotco/Hotshot-XL","<p>I am thinking of creating an AI video generation app that incorporates Hotshot.
However, the inference.py cloned directly from github works, but when I import the modified inference.py in the app I am building, I get the following error.
<code>ValueError: unet/hotshot_xl.py as defined in </code>model_index.json<code> does not exist in hotshotco/Hotshot-XL and is not a module in 'diffusers/pipelines'.</code></p>
<p>Modified inference.py</p>
<pre><code>def main(pretrained_path='hotshotco/Hotshot-XL',
         xformers=False,
         spatial_unet_base=None,
         lora=None,
         output='output_scuba.gif',
         steps=30,
         prompt='Sasquatch scuba diving, anime style',
         negative_prompt='blurry',
         seed=455,
         width=672,
         height=384,
         target_width=512,
         target_height=512,
         og_width=1920,
         og_height=1080,
         video_length=8,
         video_duration=1000,
         low_vram_mode=False,
         scheduler='EulerAncestralDiscreteScheduler',
         control_type=None, 
         controlnet_conditioning_scale=0.7,
         control_guidance_start=0.0,
         control_guidance_end=1.0,
         gif=None,
         precision='f16',
         autocast=None):
    
    args = argparse.Namespace(
        pretrained_path=pretrained_path,
        xformers=xformers,
        spatial_unet_base=spatial_unet_base,
        lora=lora,
        output=output,
        steps=steps,
        prompt=prompt,
        negative_prompt=negative_prompt,
        seed=seed,
        width=width,
        height=height,
        target_width=target_width,
        target_height=target_height,
        og_width=og_width,
        og_height=og_height,
        video_length=video_length,
        video_duration=video_duration,
        low_vram_mode=low_vram_mode,
        scheduler=scheduler,
        control_type=control_type,
        controlnet_conditioning_scale=controlnet_conditioning_scale,
        control_guidance_start=control_guidance_start,
        control_guidance_end=control_guidance_end,
        gif=gif,
        precision=precision,
        autocast=autocast
    )
# The following is the same as inference.py
</code></pre>
<p>Some of the Python code for my app</p>
<pre><code>if generate_value == 4:
            main(pretrained_path='hotshotco/Hotshot-XL',
                 xformers=False,
                 spatial_unet_base=None,
                 lora=None,
                 output=filename_vid,
                 steps=steps,
                 prompt=prompt,
                 negative_prompt=negative_prompt,
                 seed=455,
                 width=width,
                 height=height,
                 target_width=width,
                 target_height=height,
                 og_width=width,
                 og_height=height,
                 video_length=video_length,
                 video_duration=1000,
                 low_vram_mode=False,
                 scheduler='EulerAncestralDiscreteScheduler',
                 control_type=None,
                 controlnet_conditioning_scale=0.7,
                 control_guidance_start=0.0,
                 control_guidance_end=1.0,
                 gif=None,
                 precision=precision,
                 autocast=None
                )
</code></pre>
<p>(These were all translated from Japanese by DeepL)</p>
<p>I was hoping to incorporate the main function with prompt, negative prompt, length of video to generate, precision, width and height as arguments in my app and then create a gif based on it, but I got an error.</p>
","2024-01-27 08:37:26","1","Question"
"77883499","77883320","","<p>A string literal (regardless of the quotes used) is not the same thing as a <code>str</code> value. The <em>literal</em> contains a <code>\n</code> digraph; the <code>str</code> value it creates contains U+000a.</p>
<pre><code>&gt;&gt;&gt; x = &quot;&quot;&quot;foo\nbar
...baz&quot;&quot;&quot;
&gt;&gt;&gt; len(x)
11  # not 12
&gt;&gt;&gt; x
'foo\nbar\nbaz'  # String representation of a string uses a digraph
&gt;&gt;&gt; print(x)  # U+000a is rendered as a line break and a carriage return
foo
bar
baz
</code></pre>
","2024-01-25 23:33:50","0","Answer"
"77883320","","Text has \n and response has both \n and ""real new lines"". print() output both as line breaks","<p>I have some Python function like this:</p>
<pre><code>def translate(text):
    while True:
        response = openai.ChatCompletion.create(
            model=&quot;gpt-4&quot;,
            messages=[
                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a translator, you will receive English language sentences from a videogame needing localization. You must respect the punctuation and capitalization. You must maintain \n as is&quot;},
                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;The target language is Spanish&quot;},
                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: text}
            ],
            temperature=0
        )
        return response.choices[0].message['content']
</code></pre>
<p>and for the <code>text</code> input parameter I have a string like this (but really longer):</p>
<pre><code>text_to_translate = &quot;&quot;&quot;
Blocks melee and\n ranged units' line of sight.
Sink the unit. Sunken\n units are more vulnerable to enemy attacks.
Teleport the unit through\n this portal to an exit portal.
&quot;&quot;&quot;
</code></pre>
<p>For the response I'm doing:</p>
<pre><code>translated_text = translate(text_to_translate)
print(translated_text, flush=True)
</code></pre>
<p>The problem is, print() is taking \n and (correctly) inserting line breaks on the output like this:</p>
<pre><code>Bloquea la línea de visión de
unidades de combate cuerpo a cuerpo y a distancia.
Hunde la unidad. Las unidades
hundidas son más vulnerables a los ataques enemigos.
Teletransporta la unidad a través
de este portal a un portal de salida.
</code></pre>
<p>How can I make it so \n stay the same but the actual line breaks from the triple-quoted strings are respected?</p>
","2024-01-25 22:36:39","0","Question"
"77879917","77872605","","<p>You need to ensure that gradients are enabled only for the <strong>parameters</strong> of the last layer. Replace <code>model_vgg.classifier[-1].requires_grad = True</code> with the below code snippet</p>
<pre><code>for param in model_vgg.classifier[-1].parameters():
    param.requires_grad = True
</code></pre>
","2024-01-25 12:17:01","1","Answer"
"77872605","","Training the VGG16 Model for Image Classification using PyTorch","<p>I am using PyTorch for image classification.</p>
<p>I coded the following train function that worked with a simple linear model:</p>
<pre><code>criterion = nn.CrossEntropyLoss()
def train(model, dataloader, epoch):
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
running_loss, running_acc = 0., 0.
loss_history = []
accuracy_history = [](data_train):.2f}%&quot;)

for i in range(1, epoch + 1):
  model.train()
  for inputs, targets in dataloader:
      inputs, targets = inputs.to(device), targets.to(device)
      outputs = model(inputs)
      loss = criterion(outputs, targets)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      preds = torch.argmax(outputs, 1)
      running_loss += loss.item()
      running_acc += torch.sum(preds == targets).item()

  print(f&quot;[TRAIN epoch {i}] Loss: {running_loss/len(data_train):.2f} Acc: {100 * running_acc/len
 
</code></pre>
<p>I have the pre-trained VGG16 model and I want to change the weights of its last layer:</p>
<pre><code>model_vgg = models.vgg16(weights='DEFAULT')
model_vgg.classifier[6] = nn.Linear(4096, 2)

for param in model_vgg.parameters():
    param.requires_grad = False
model_vgg.classifier[-1].requires_grad = True

train(model_vgg, train_loader, 2)
</code></pre>
<p>However, when training it I get the following error:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)

&lt;timed eval&gt; in &lt;module&gt;

&lt;ipython-input-27-1f64686a5cfd&gt; in train(model, dataloader, epoch)
     39           loss = criterion(outputs, targets)
     40           optimizer.zero_grad()
---&gt; 41           loss.backward()
     42           optimizer.step()
     43           preds = torch.argmax(outputs, 1)

/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
--&gt; 251     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    252         tensors,
    253         grad_tensors_,

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>How do I fix this?</p>
","2024-01-24 11:21:50","0","Question"
"77870947","77860389","","<p>If you are using conda, Check the environment whether you set the environment to  your project or not. once the environment is set, check whether tensorflow is installed in that environment using</p>
","2024-01-24 06:28:10","0","Answer"
"77868554","77868483","","<p>Update :
I was able to resolve the issue using the</p>
<pre><code>client = OpenAI

completion = client.chat.completions.create(
  model=&quot;gpt-3.5-turbo&quot;,
  messages=[
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Compose a poem that explains the concept of recursion in programming.&quot;}
  ]
)

print(completion.choices[0].message)
</code></pre>
","2024-01-23 18:34:02","0","Answer"
"77868483","","TypeError: 'module' object is not callable for OpenAI","<pre><code>import openai as OpenAI 
client = OpenAI()

completion = client.chat.completions.create(
  model=&quot;gpt-3.5-turbo&quot;,
  messages=[
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Compose a poem that explains the concept of recursion in programming.&quot;}
  ]
)

print(completion.choices[0].message)
</code></pre>
<p>The response I am getting is</p>
<pre><code>
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-12-6de2ef8831ea&gt; in &lt;cell line: 2&gt;()`your text`
      1 import openai as OpenAI
----&gt; 2 client = OpenAI()
      3 
      4 completion = client.chat.completions.create(
      5   model=&quot;gpt-3.5-turbo&quot;,

TypeError: 'module' object is not callable`
</code></pre>
<p>I tried to solve the issue with Google Bard and chat gpt but could not</p>
","2024-01-23 18:19:59","0","Question"
"77865947","77847415","","<p>Since you don't seem to be using any grid-specific functions anyway, you could swap it for a 2d array - that will throw a proper error on out-of-bounds access. Initialization would have to be done like so:</p>
<pre><code>QTable = array_create(QTableWidth);
for (var i = 0; i &lt; QTableWidth; i++) QTable[i] = array_create(QTableHeight, 0);
</code></pre>
<p>You can run the game in debug mode to inspect the local/instance variables at the time of error.</p>
","2024-01-23 11:28:53","0","Answer"
"77860389","","I have python (path set) and Tensorflow installed and updated, yet Tensorflow doesn't import","<p>Although I have installed and updated python and tensorflow in vs code, I am still getting this error:</p>
<pre><code>&quot;message&quot;: &quot;Import \&quot;tensorflow\&quot; could not be resolved&quot;,
</code></pre>
<p>this is happening from the line:</p>
<pre><code>import tensorflow as tf 
</code></pre>
<p>I have tried fixing the interpreter, I installed it from home brew and set it as my path:</p>
<pre><code>% which python3 
/opt/homebrew/bin/python3
</code></pre>
<pre><code>py version: 3.11.7
tf version: 2.15.0
</code></pre>
","2024-01-22 13:57:58","0","Question"
"77859197","77859016","","<p>You can use PyMuPDF for writing text to PDF pages ... in multiple ways.</p>
<p><strong>Note:</strong> I am a maintainer and the original creator of <a href=""https://pypi.org/project/PyMuPDF/"" rel=""nofollow noreferrer"">PyMuPDF</a>.</p>
<p>You need to locate the image position on the page first. Then decide about a rectangle (like above or below the image boundary box) to receive the caption text.</p>
<p>For example, assume the image boundary box is called <code>bbox</code>, then define <code>rect = (bbox.x0, bbox.y1, bbox.x1, bbox.y1 + 20)</code>. This is a rectangle below the image with the same width as bbox and a height of 20.</p>
<p>Then do <code>page.insert_htmlbox(rect, caption)</code> using the caption text.</p>
<p>That method also allows you to align (e.g. center) the caption text via HTML styling instructions, like <code>page.insert_htmlbox(rect, caption, css=&quot;* {text-align: center;}&quot;)</code>.</p>
","2024-01-22 10:35:21","0","Answer"
"77859016","","Is it possible to put the captions generated by AI models back into the pdf file?","<p>I have a pdf that contains multiple pages where each page consists of texts and/or images. I have found ways to extract images from a pdf file and I have found ways to use AI models to generate captions for images. But is it possible to put back the captions generated by the AI model to the corresponding image in the pdf file? If it is possible, then what library should I use? Or does anyone know how to code it?</p>
","2024-01-22 10:06:28","-1","Question"
"77854849","77846133","","<p>You need to use Python 3.10,Install it and modify the run.sh:</p>
<pre><code>virtualenv -p python3.10 .
</code></pre>
<p>In the requirements.txt they said This file is autogenerated by pip-compile with Python 3.10.</p>
<p>However the alphageometry doesn't work well in my machine.The Error Info is</p>
<pre><code>ERROR: In --require-hashes mode,all requirements must...etils[epath] from https://.../etils-1.6.0-py3-none-any-whl (from array-record...)
</code></pre>
<p>and you can try modify the run.sh:</p>
<pre><code>pip install -r requirements.in
</code></pre>
","2024-01-21 12:46:46","0","Answer"
"77854465","77807710","","<p>I found a way that uses CNN network to find differences between two images
code:</p>
<pre><code># Importing necessary libraries
import tensorflow as tf
import matplotlib.pyplot as plt

# Specify the file paths for the two images
image_path1 = '1.jpg'
image_path2 = '2    .jpg'

# Read and decode images, then normalize pixel values to the range [0, 1]
img1 = tf.io.read_file(image_path1)
img1 = tf.image.decode_image(img1, channels=1)
img1 = tf.cast(img1, tf.float32) / 255.0

img2 = tf.io.read_file(image_path2)
img2 = tf.image.decode_image(img2, channels=1)
img2 = tf.cast(img2, tf.float32) / 255.0

# Add a batch dimension to the images
img1 = tf.expand_dims(img1, axis=0)
img2 = tf.expand_dims(img2, axis=0)

# Create a Conv2D layer with specified parameters
conv2d_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', padding='same')

# Apply the Conv2D layer to both images
output1 = conv2d_layer(img1)
output2 = conv2d_layer(img2)

# Calculate the absolute difference between the Conv2D outputs
diff = tf.abs(output1 - output2)

# Plotting the images and Conv2D outputs for visualization
plt.figure(figsize=(10, 5))

plt.subplot(1, 4, 1)
plt.imshow(tf.squeeze(img1), cmap='gray')
plt.title('Image 1')
plt.axis('off')

plt.subplot(1, 4, 2)
plt.imshow(tf.squeeze(img2), cmap='gray')
plt.title('Image 2')
plt.axis('off')

plt.subplot(1, 4, 3)
plt.imshow(tf.squeeze(output1), cmap='gray')
plt.title('Conv2D Image 1')
plt.axis('off')

plt.subplot(1, 4, 4)
plt.imshow(tf.squeeze(diff), cmap='gray')
plt.title('Absolute Difference')
plt.axis('off')

# Display the plot
plt.show()

</code></pre>
<p>this code use CNN network to Calculate the distance between two image array</p>
","2024-01-21 10:44:02","0","Answer"
"77847415","","Qtable index out of bounds","<p>Im trying to make a simple Q-learning AI in gms2, but im horrible messing with grinds and aways get the same problem when i try to update the qTable:</p>
<pre><code>index out of bounds
</code></pre>
<p>project is simple, the AI can go to all directions and need to touch the &quot;obj_goal&quot; while avoid the &quot;obj_obstacles&quot;</p>
<p>Create event:</p>
<pre><code>QTableWidth = 100; // Ajuste o tamanho conforme necessário
QTableHeight = 100; // Ajuste o tamanho conforme necessário
QTable = ds_grid_create(QTableWidth, QTableHeight);


learningRate = 0.1;
discountFactor = 0.9;
explorationRate = 0.7;

reward = 0;


currentX = x;
currentY = y;


iniX = x;
iniY = y;




spd = 2;
</code></pre>
<p>step event:</p>
<pre><code>function getCurrentState() {
    return string(currentX) + string(&quot;_&quot;) + string(currentY);
}

id_action = choose(0, 1, 2, 3);


currentX = x;
currentY = y;
currentState = getCurrentState();

#region randomizar ou andar com base em aprendizado
if random(1) &lt; explorationRate {
    // Exploração (ação aleatória)
    action = choose(&quot;up&quot;, &quot;down&quot;, &quot;left&quot;, &quot;right&quot;);
} else {
    var bestActionIndex = 0;
    var bestActionValue = QTable[# currentState, 0];
    for (var i = 1; i &lt; QTableHeight; i++) {
        var value = QTable[# currentState, i];
        if (value &gt; bestActionValue) {
            bestActionValue = value;
            bestActionIndex = i;
        }
    }
    action = bestActionIndex;
}
#endregion

#region açoes
switch (action) {
    case &quot;up&quot;:
        y -= spd;
        break;
    case &quot;down&quot;:
        y += spd;
        break;
    case &quot;left&quot;:
        x -= spd;
        break;
    case &quot;right&quot;:
        x += spd;
        break;
}
#endregion

if place_meeting(x, y, obj_objetivo) {
    reward += 10;
    
    x = iniX;
    y = iniY;
} else if place_meeting(x, y, obj_Obstaculo) {
    reward -= 10;
    
    x = iniX;
    y = iniY;
}

newState = getCurrentState();

var currentStateIndex = floor(currentState);
var idActionIndex = floor(id_action);



if currentState &gt;= 0 &amp;&amp; currentState &lt; QTableWidth &amp;&amp; id_action &gt;= 0 &amp;&amp; id_action &lt; QTableHeight {


    QTable[# currentState, id_action] += learningRate * (reward + discountFactor * QTable[# newState, id_action] - QTable[# currentState, id_action]);
    
} else {
    show_debug_message(&quot;index out of bounds.&quot;);
}

currentX = x;
currentY = y;
</code></pre>
<p>it was supossed to update the Qtable to make the AI learn with the rewards that was given.</p>
","2024-01-19 16:10:55","1","Question"
"77846133","","why is run.sh in alpha geometry not working?","<p>i want to install alpha geometry on my mac using the discord link <a href=""https://github.com/google-deepmind/alphageometry"" rel=""nofollow noreferrer"">https://github.com/google-deepmind/alphageometry</a> and i am getting the following error:</p>
<pre><code>ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python &gt;=3.7,&lt;3.11; 1.21.3 Requires-Python &gt;=3.7,&lt;3.11; 1.21.4 Requires-Python &gt;=3.7,&lt;3.11; 1.21.5 Requires-Python &gt;=3.7,&lt;3.11; 1.21.6 Requires-Python &gt;=3.7,&lt;3.11
ERROR: Could not find a version that satisfies the requirement tensorflow-text==2.13.0 (from versions: none)
ERROR: No matching distribution found for tensorflow-text==2.13.0
</code></pre>
<p>please explain it in easy to understand terms, because i am not that good in it</p>
","2024-01-19 12:38:03","0","Question"
"77833212","77833137","","<p>If you just cut the audio into 6 second pieces at random positions, you will end up with partial words in the blocks. The recognizer won't be able to handle that.</p>
<p>The Python <code>speech_recognition</code> library <a href=""https://github.com/Uberi/speech_recognition/blob/master/examples/microphone_recognition.py"" rel=""nofollow noreferrer"">microphone example</a> uses <code>recognizer.listen()</code>, which detects gaps in speech by monitoring the audio volume. For continuous recording, you would use <code>recognizer.listen_in_background()</code> which calls a function every time there is a gap in the speech. The gap thresholds can be adjusted as fields of the class instance (see <a href=""https://github.com/Uberi/speech_recognition/blob/master/speech_recognition/__init__.py#L331-L339"" rel=""nofollow noreferrer"">default values</a>).</p>
<p>This will give near realtime response, which performs the recognition as soon as you stop speaking for a moment.</p>
<p>If you need to update more often than that, the best approach would be to perform recognition on the whole audio block captured since previous gap. You can then update the recognized text as new information becomes available. The next word will often affect the probability estimate for prior words.</p>
","2024-01-17 14:32:36","2","Answer"
"77833137","","Real time speech recognition from PC audio","<p>I'm trying to make real time speech recognition software that will show text on screen (with tkinter). I already found a way to get an audio from PC output using Virtual Cable Audio, pyaudio and speech_recognition library. However i feel like im doing something wrong, of course AI is not perfect but sending voice in 6 seconds batches leads to AI wrong understanding of voice. Do you see any better way to do so? I could also use different API for speech recognition.</p>
<pre class=""lang-py prettyprint-override""><code>import time
import tkinter as tk
import speech_recognition as sr

from app.ui.window import Window
import wave

import pyaudiowpatch as pyaudio

if __name__ == &quot;__main__&quot;:
    recognizer = sr.Recognizer()

    p = pyaudio.PyAudio()
    device = p.get_default_input_device_info()

    sample_rate = int(device[&quot;defaultSampleRate&quot;])

    duration_seconds = 6  # Duration to capture
    total_frames = sample_rate * duration_seconds

    buffer_size = 1024  # Size of each read
    frames = []
    overlap_frames = int(sample_rate * 0.5)

    while True:
        try:
            with p.open(
                channels=device[&quot;maxInputChannels&quot;],
                format=pyaudio.paInt16,
                rate=sample_rate,
                input=True,
                frames_per_buffer=buffer_size,
                input_device_index=device[&quot;index&quot;],
            ) as stream:
                print(&quot;Capturing audio...&quot;)
                frames = [stream.read(buffer_size) for _ in range(total_frames // buffer_size)]
                combined_audio = b''.join(frames[-overlap_frames:] + frames)

                try:
                    recognized_text = recognizer.recognize_wit(
                        sr.AudioData(combined_audio, int(device[&quot;defaultSampleRate&quot;]), 2),
                        key=&quot;API_KEY&quot;
                    )
                except Exception as e:
                    print(e)
                    continue

                frames = []

                print(f&quot;Recognized: {recognized_text}&quot;)
        except Exception as e:
            print(f&quot;Error: {e}&quot;)
            p.terminate()

    # root = tk.Tk()
    # app = Window(root)
    # root.mainloop()

</code></pre>
","2024-01-17 14:19:18","1","Question"
"77823012","","How to use GNNexplainer for graph classification in the latest version of torch-geometric (2.4.0)?","<p>I applied a GCN for graph binary classification, and I want to use GNNexplainer to explain the prediction. But I don't know how to apply GNNexplainer on my model.
Here is my GCN model code:</p>
<pre><code>class GCNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCNModel, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.linear1 = nn.Linear(hidden_dim, output_dim)


    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        
        x = F.relu(self.conv1(x, edge_index, edge_weight))
        x = self.conv2(x, edge_index, edge_weight)
        x = global_mean_pool(x, batch)
        x= `self.linear1(x)
        x = torch.sigmoid(x)
        return x
</code></pre>
<p>Here is the GNNexplainer code:</p>
<pre><code>for batch in test_loader:
    graphs_batch, labels_batch = batch
    # Choose the first graph from the batch for explanation
    graph_to_explain = graphs_batch[0].to(device)
    labels_batch = labels_batch.to(device)
    
    model.eval()
    with torch.no_grad():
        prediction = model(graph_to_explain).squeeze()

        data = Data(x=graph_to_explain, edge_index=graph_to_explain.edge_index)

        explainer = Explainer(
            model=model,
            algorithm=GNNExplainer(epochs=200),
            explanation_type='model',
            node_mask_type='attributes',
            edge_mask_type='object',
            model_config=dict(
                mode='binary_classification',
                task_level='graph',
                return_type='probs',
            ),
        )
        explanation = explainer(data.x, data.edge_index, data.batch)

        print(&quot;Node Importance Scores:&quot;, explanation.node_importance)
        print(&quot;Edge Importance Mask:&quot;, explanation.edge_mask)

    break
</code></pre>
<p>Then the error is like this :</p>
<pre><code>
&quot;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[27], line 31
     18 # Initialize the GNNExplainer with your model and algorithm settings
     19 explainer = Explainer(
     20     model=model,
     21     algorithm=GNNExplainer(epochs=200),
   (...)
     29     ),
     30 )
---&gt; 31 explanation = explainer(data.x, data.edge_index, data.batch)
     33 # explanation = explainer(
     34 #     x=data.x,
     35 #     edge_index=data.edge_index,
   (...)
     40 # You can use the explanation for visualization or analysis
     41 # Access the importance scores using explanation.node_importance and explanation.edge_mask
     42 print(&quot;Node Importance Scores:&quot;, explanation.node_importance)

TypeError: Explainer.__call__() takes 3 positional arguments but 4 were given&quot;

</code></pre>
<p>How to use GNNexplainer for graph classification in the latest version of torch_geometric (2.4.0)?</p>
","2024-01-16 01:06:48","1","Question"
"77807710","","Spot the Difference between two images using AI","<p>I'm looking for a method to Spot the difference between two images using AI.</p>
<p>It's my university project, that my professor asked me to create a program to detect and spot the differences in two pairs of images using AI.</p>
<p>I deployed it using the Siamese Network, to calculate the difference, and if the difference was greater than the threshold then, I used the following code to show the differences :</p>
<pre><code>input_images = np.array([[img1, img2]])
difference_image = np.abs(input_images[0, 0] - input_images[0, 1])
plt.imshow(difference_image)
</code></pre>
<p>But my prof didn't accept it
he gave me a hint to split images into smaller shapes using <strong>Conv2D</strong> and then compare those shapes and if there is a difference, highlight that using the bounding box.</p>
<p>Can anyone help to deploy this code?</p>
<p>My previous code is :</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers

img1 = plt.imread('1-1.jpg')
img2 = plt.imread('1-2.jpg')

input_shape = img1.shape  # Assuming images are of the same shape


# Function to create    
# def create_siamese_model(input_shape):
input_image_1 = layers.Input(shape=input_shape, name='input_image_1')
input_image_2 = layers.Input(shape=input_shape, name='input_image_2')

# Base network
base_network = keras.Sequential([
    layers.Conv2D(40, (3, 3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(256, activation='relu')
])
# Encoded representations of input images
encoded_image_1 = base_network(input_image_1)
encoded_image_2 = base_network(input_image_2)

# L1 distance layer
l1_distance = layers.Lambda(lambda tensors: keras.backend.abs(tensors[0] - tensors[1]))([encoded_image_1, encoded_image_2])

# Output layer
output_layer = layers.Dense(15, activation='sigmoid')(l1_distance)

model = keras.Model(inputs=[input_image_1, input_image_2], outputs=output_layer)

input_images = np.array([[img1, img2]])
predictions = model.predict([input_images[:, 0], input_images[:, 1]])


threshold=0.5

if predictions[0, 0] &gt; threshold:
    # Highlight differences if the prediction is above the threshold
    difference_image = np.abs(input_images[0, 0] - input_images[0, 1])
    difference_image
    plt.imshow(difference_image)
    plt.show()

</code></pre>
","2024-01-12 15:42:06","2","Question"
"77785827","77758436","","<p>You need to pass your callback function via <code>register_reply</code> function of the Agent class to get or print the response on your web UI. Below is the concept of the code:</p>
<pre><code>def print_messages(recipient, messages, sender, config):
    # each time when the agent receive the message
    # do your own logic here
    messages.append((messages[-1]['name'], messages[-1]['content']))
    return False, None

assistant.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages, 
    config={&quot;callback&quot;: None},
)
</code></pre>
<p>You may want to check out the <a href=""https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#register_reply"" rel=""nofollow noreferrer"">offical document</a></p>
","2024-01-09 09:56:55","1","Answer"
"77780432","77779488","","<p>It is stated on <a href=""https://pypi.org/project/openllm/"" rel=""nofollow noreferrer"">PyPi</a> that it's highly recommended to use a <a href=""https://docs.python.org/3/library/venv.html"" rel=""nofollow noreferrer"">Virtual Environment</a> to prevent package conflicts.</p>
<p>You don't appear to be using one yet.</p>
","2024-01-08 13:07:48","0","Answer"
"77779488","","ValueError: bad marshal data (unknown type code) - OpenLLM","<p>After pip install openllm , when trying to access openllm, getting the below error:</p>
<p><a href=""https://i.sstatic.net/EPMnm.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>My python version is : 3.11.x</p>
","2024-01-08 11:26:25","-1","Question"
"77759686","77759685","","<p>This works well for me:</p>
<pre><code>rag_chain = (
    RunnablePassthrough.assign(source_documents=condense_question | retriever)
    | RunnablePassthrough.assign(context=lambda inputs: format_docs(inputs[&quot;source_documents&quot;]) if inputs[&quot;source_documents&quot;] else &quot;&quot;)
    | RunnablePassthrough.assign(prompt=qa_prompt)
    | RunnablePassthrough.assign(response=lambda inputs: llm(inputs[&quot;prompt&quot;].messages))
)
</code></pre>
<p>It's called like this:</p>
<pre><code>response_dict = rag_chain.invoke({&quot;question&quot;: question, &quot;chat_history&quot;: chat_history})
ai_msg = response_dict[&quot;response&quot;]
source_documents = response_dict[&quot;source_documents&quot;]
</code></pre>
<p>The way that helped me understand how to do it was this:</p>
<ol>
<li>You initially pass a dictionary into the chain (in my case with the keys <code>question</code> and <code>chat_history</code>).</li>
<li>Every time you use <code>RunnablePassthrough.assign</code>, you can ADD stuff to that dictionary and then pass that on to the next step.</li>
<li><code>RunnablePassthrough.assign</code> always RETURNS a dictionary.</li>
</ol>
<p>This is what happens in my code example:</p>
<ol>
<li>We use <code>RunnablePassthrough.assign</code> to add a new <code>source_documents</code> key to the dictionary. Its value is the result of calling the <code>condense_question</code> function (defined elsewhere) that builds and returns a condenser chain. Its condensed result is passed into our retriever (also defined elsewhere).</li>
<li>We use <code>RunnablePassthrough.assign</code> to add a new <code>context</code> key to the dictionary. Its value is the result of calling a <code>format_docs</code> method (defined elsewhere) that combines the source_documents into a single context string.</li>
<li>We use <code>RunnablePassthrough.assign</code> to add a new <code>prompt</code> key to the dictionary. Its value is the result of calling <code>qa_prompt</code>, which is defined as <code>qa_prompt = ChatPromptTemplate.from_messages(...)</code>.</li>
<li>We use <code>RunnablePassthrough.assign</code> one more time to add a new <code>response</code> key to the dictionary. Its value is the result of actually calling the llm with the messages from our prompt.</li>
<li>The chain returns a dictionary with all the keys we've added along the way. The <code>response</code> key contains the LLM's response as an <code>AIMessage</code>, and the <code>source_documents</code> key contains the source documents.</li>
</ol>
<p>I'm sure this can be done in a more concise way, but this worked for me and I can understand it :)</p>
","2024-01-04 16:12:44","3","Answer"
"77759685","","How to return source documents when using LangChain Expression Language (LCEL)?","<p>Most samples of using <a href=""https://python.langchain.com/docs/expression_language/"" rel=""nofollow noreferrer"">LangChain's Expression Language</a> (LCEL) look like this:</p>
<pre><code>chain = setup_and_retrieval | prompt | model | output_parser
</code></pre>
<p>How can I access the <code>source_documents</code> in a RAG application when using this expression language?</p>
","2024-01-04 16:12:44","3","Question"
"77758436","","Autogen response in a variable","<pre><code>import autogen
from nicegui import ui, context
from uuid import uuid4

# AutoGen Configuration
config_list = [
    {
        'model': 'gpt-4',
        'api_key': '' 
    }
]
llm_config = {
    'seed': 42,
    'config_list': config_list,
    'temperature': 0.2
}

# Initialize AutoGen Agents
assistant = autogen.AssistantAgent(name='Albert', llm_config=llm_config)
user_proxy = autogen.UserProxyAgent(name='user_proxy', human_input_mode=&quot;NEVER&quot;, max_consecutive_auto_reply=1, is_termination_msg=lambda x: x.get(&quot;content&quot;, &quot;&quot;).rstrip().endswith(&quot;TERMINATE&quot;), code_execution_config={&quot;work_dir&quot;: &quot;web&quot;}, llm_config=llm_config)

@ui.page('/')
def main():
    messages = []
    user_id = str(uuid4())  # Unique ID for each user session

    @ui.refreshable
    def chat_messages():
        for name, text in messages:
            ui.chat_message(text=text, name=name, sent=name == 'You')
        if context.get_client().has_socket_connection:
            ui.run_javascript('setTimeout(() =&gt; window.scrollTo(0, document.body.scrollHeight), 0)')


    async def send():
        user_message = task_input.value
        messages.append(('You', user_message))  # Append user's message to the messages list
        chat_messages.refresh()  # Refresh chat messages to display the latest message
        task_input.value = ''  # Clear the input field after sending the message

        try:
            response = await user_proxy.initiate_chat(assistant, message=user_message)
            if response and 'content' in response[0]:
                assistant_response = response[0]['content']
                messages.append(('Albert', assistant_response))  # Append assistant's response to messages
            else:
                messages.append(('Albert', &quot;Assistant did not provide a response.&quot;))
        except Exception as e:
            messages.append(('Albert', f&quot;Error: {e}&quot;))
        finally:
            chat_messages.refresh()


    with ui.scroll_area().classes('w-full h-60 p-3 bg-white overflow-auto'):
        chat_messages()

    with ui.footer().style('position: fixed; left: 0; bottom: 0; width: 100%; background: white; padding: 10px; box-shadow: 0 -2px 5px rgba(0,0,0,0.1);'):
        task_input = ui.input().style('width: calc(100% - 100px);')
        ui.button('Send', on_click=send).style('width: 90px;')

ui.run(title='Chat with Albert')
</code></pre>
<p>trying to use this <a href=""https://nicegui.io/"" rel=""nofollow noreferrer"">GUI</a> over Autogen. However, I cannot figure out where the response is coming from? The response variable doesn't seem to have it. When there is an exception, it is printed in the UI, when it works well, Autogen prints the answer in the terminal but not the UI.</p>
","2024-01-04 12:55:32","1","Question"
"77746911","77743390","","<p>I managed to solve the problem by doing this:</p>
<ol>
<li>I downloaded the dataset and placed it in the same directory with the script.</li>
<li>I tried lots of ways to load the data from the dataset, but most of them either kept downloading the data from internet, or didn't want to load the data from the local dataset for some reasons. The method that worked looks like this:</li>
</ol>
<pre><code>path = 'C:/Users/.../mnist.npz'  #full path to the dataset
with np.load(path, allow_pickle=True) as f:
    x_train, y_train = f['x_train'], f['y_train']
    x_test, y_test = f['x_test'], f['y_test']
</code></pre>
<p>At the end I have the <code>x_train, y_train</code> and <code>x_test, y_test</code> variables. Just what I needed. Hope someone will find this helpful.</p>
","2024-01-02 15:12:46","0","Answer"
"77743390","","How to load a dataset(.npz) locally in tensorflow?","<p>I'm working on a handwritten didgits recognition project. I need to load the data from mnist dataset. My problem is that I can't use the <code>load_data()</code> function because when it starts downloading the dataset, it returns an error:</p>
<p><code>URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: No connection could be made because the target machine actively refused it</code></p>
<p>Here is the piece of code that I use:</p>
<pre><code>mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
</code></pre>
<p>I downloaded the <code>mnist.npz</code> file using device that has access to the directory. How can I use the file locally instead of loading it?</p>
<p>This is what I tried:
<code>(x_train, y_train), (x_test, y_test) = tf.keras.utils.get_file(os.path.abspath('mnist.npz'), origin='', extract=True)</code></p>
<p>It returned an error: <code>too many values to unpack (expected 2)</code></p>
","2024-01-01 21:18:38","1","Question"
"77742623","77740903","","<p>The reason you are getting this error is because <code>-Type</code> parameter indicates the type of Cognitive Service you are trying to create and <code>StorageV2</code> is not a valid type (it is valid for a storage account type of resource). From the documentation <a href=""https://learn.microsoft.com/en-us/powershell/module/az.cognitiveservices/new-azcognitiveservicesaccount?view=azps-11.1.0#-type"" rel=""nofollow noreferrer""><code>here</code></a>:</p>
<p><a href=""https://i.sstatic.net/WseZQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WseZQ.png"" alt=""enter image description here"" /></a></p>
<p>You need to provide the correct type. You can find the valid values for the type of Cognitive Service by using <a href=""https://learn.microsoft.com/en-us/powershell/module/az.cognitiveservices/get-azcognitiveservicesaccounttype?view=azps-11.1.0"" rel=""nofollow noreferrer""><code>Get-AzCognitiveServicesAccountType</code></a> Cmdlet.</p>
","2024-01-01 16:54:42","0","Answer"
"77740903","","New-AzCognitiveServicesAccount","<p>I've created a subdomain in Azure Cloud Shell using:</p>
<p><code>Set-AzContext -SubscriptionName &quot;Azure subscription 1&quot;</code></p>
<p>When I run the following command:</p>
<pre><code>$account = New-AzCognitiveServicesAccount -ResourceGroupName cloud-shell-storage-southcentralus -name cs710032000dd678293 -Type StorageV2 -SkuName S0 -Location southcentralus -CustomSubdomainName 'Progression' 
</code></pre>
<p>I receive this error:</p>
<p>New-AzCognitiveServicesAccount: Operation returned an invalid status code 'BadRequest'
The account type 'StorageV2' is either invalid or unavailable in given region.</p>
<p>I'm following the Microsoft training module to prepare for a certification exam in a couple weeks, any help would be appreciated.</p>
<p>I've tried changing the account type multiple times and modify the command to resemble examples found on here and the Microsoft learn pages. As previously mentioned, I'm studying for a certification, (Microsoft Certified: Azure AI Engineer Associate) and this error has turned into a brick wall, please help.</p>
","2024-01-01 03:41:45","1","Question"