Post Id,Parent Id,Body,Score,PostType
"79321937","","<p>The image is showing confusion matrix in which text which are numbers and color of image is same in result confusion matrix not showing the number in first row of confusion matrix</p>
<p><a href=""https://i.sstatic.net/GsVNbJRQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GsVNbJRQ.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code># calculate accuracy
from sklearn import metrics

result_N = metrics.confusion_matrix(y_test, y_pred_N)
print(&quot;Confusion Matrix:&quot;)
print(result_N)

def plt1():
    import seaborn as sns; sns.set()
    plt.figure(figsize=(4,4))
    c_mtrx_N = pd.crosstab(y_test, y_pred_N, rownames=['Actual'], colnames=['Predicted'])
    sns.heatmap(c_mtrx_N, annot=True, fmt = '.3g')

plt1()
</code></pre>
<p>How to show numbers with the color in confusion matrix? I tried to change color of confusion matrix too but the same problem persists.</p>
","0","Question"
"79325633","","<p>I'm working on a machine learning project in Python. Using pandas <code>pd.get_dummies</code> I'm trying to create dummy variables for a categorical column in my data but the variables are being converted to booleans rather than integers and it is making it impossible for statsmodels to fit an OLS model. I've tried to convert the boolean into an integer but I keep getting errors. How can I solve this??</p>
<p>I used the <code>.astype(int)</code> method, I also tried using numpy to convert to an integer.</p>
<pre class=""lang-py prettyprint-override""><code>ocean_proximity_dummies = pd.get_dummies(data['ocean_proximity'], prefix= 'ocean_proximity')

data = pd.concat([data.drop(&quot;ocean_proximity&quot;, axis =1), ocean_proximity_dummies], axis=1)

ocean_proximity_dummies = ocean_proximity_dummies.astype(int)
ocean_proximity_dummies
</code></pre>
","-2","Question"
"79327647","","<p>I have a logistic regression model to predict a binary output (0 or 1). I'd like to understand the P/R of the 0-class, and produce the respective curve. I use this code:</p>
<pre><code>clf = linear_model.LogisticRegression().fit(ohe_X_train, y_train)
# Predict labels on the one hot encoded test set
clf_predictions = clf.predict(ohe_X_test)
y_scores = clf.predict_proba(ohe_X_test)

class_of_interest = 0

# P/R based on precision_recall_fscore_support
precision, recall, fscore, support = precision_recall_fscore_support(y_test, clf_predictions, labels=[class_of_interest])

# P/R curve using precision_recall_curve
y_scores = clf.predict_proba(ohe_X_test)[:, class_of_interest]
precision_curve, recall_curve, thresholds = precision_recall_curve(y_test, y_scores)

plt.plot(recall_curve, precision_curve, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Class 0')
plt.grid(True)
plt.show()
</code></pre>
<p>In this case the PR curve increases precision as it increases recall. What am I doing wrong? It works perfectly when class_of_interest = 1.</p>
","0","Question"
"79328556","","<p>I have seen in the Mathworks official website for the <code>pixelClassificationLayer()</code> <a href=""https://www.mathworks.com/help/vision/ref/nnet.cnn.layer.pixelclassificationlayer.html"" rel=""nofollow noreferrer"">function</a> that I should update it to a custom loss function using the following code:</p>
<pre><code>function loss = modelLoss(Y,T) 
  mask = ~isnan(T);
  targets(isnan(T)) = 0;
  loss = crossentropy(Y,T,Mask=mask,NormalizationFactor=&quot;mask-included&quot;); 
end

netTrained = trainnet(images,net,@modelLoss,options); 
</code></pre>
<p>However, I can't see any mention of the inputs 'Classes' or 'ClassWeights', which I'm currently using to define the custom pixelClassificationLayer:
<code>pixelClassificationLayer('Classes',classNames,'ClassWeights',classWeights)</code>, where classNames is a vector containing the names of each class as a string and classWeights is a vector containing the weights of each class to balance classes when there are underrepresented classes in the training data.</p>
<p>How can I include these parameters in my custom loss function?</p>
","2","Question"
"79329352","","<p>I used this to import</p>
<pre><code>from llama_index.text_splitter import SentenceSplitter
</code></pre>
<p>and my python version - 3.11.5 and llama_index version - 0.12.8 but give this error</p>
<pre><code>ModuleNotFoundError: No module named 'llama_index.text_splitter
</code></pre>
<p>How to solve this?</p>
<p>I ask to chatgpt, google other AI but can't find the solution.</p>
","0","Question"
"79330782","","<p>I am trying to implement RoboFlow model API in my Flutter app to analyze images. I would like to take an image from the user and analyze it using Roboflow model API.
Url, API key, Model API, version everything is correct but something is wrong with input data.</p>
<p>Here you go with my function:</p>
<pre><code>Future&lt;void&gt; testRoboflowAPI(File imagePath) async {
    try {
      List&lt;int&gt; imageBytes = imagePath.readAsBytesSync();
      String base64Image = base64Encode(imageBytes);

      final url =
          'https://classify.roboflow.com/**********/1?api_key=*********';
      final headers = {'Content-Type': 'application/x-www-form-urlencoded'};

      final body = {
        &quot;data&quot;: base64Image,
      };

      final encodedBody = Uri(queryParameters: body).query;
      final response = await http.post(
        Uri.parse(url),
        headers: headers,
        body: encodedBody,
      );

      if (response.statusCode == 200) {
        print('Success: ${response.body}');
      } else {
        print(
            'Error: Status Code ${response.statusCode}, Response: ${response.body}');
      }
    } catch (e) {
      print('Exception: ${e.toString()}');
    }
  }
</code></pre>
<p>I am getting the given error:</p>
<pre><code>Error: Status Code 400, Response: {&quot;message&quot;:&quot;Could not load input image. Cause: Malformed base64 input image.&quot;}
</code></pre>
<p>Could you please help me to get proper response and solve the error? Thanks!</p>
","1","Question"
"79331871","","<p>I am running a code repository and I got an error message while recreate the model.</p>
<blockquote>
<p>ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='resnet50_input'), name='resnet50_input', description=&quot;created by layer 'resnet50_input'&quot;) at layer &quot;resnet50&quot;. The following previous layers were accessed without issue:......</p>
</blockquote>
<p>I am using Tensorflow 2.14.0 and Python 3.10</p>
<p>This is the test code,which I found is the code giving the error from the repo:</p>
<pre><code>num_classes = 10
input_shape = (32, 32, 3)
model = tf.keras.Sequential()
base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
for layer in base_model.layers:
    layer.trainable = False
model.add(base_model)
model.add(tf.keras.layers.GlobalAveragePooling2D())
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

extractor = tf.keras.Model(inputs=model.layers[0].input,
                                outputs=[layer.output for layer in model.layers])
</code></pre>
<p>I searched through Google and tried every methods. Some people said I should first create an input but still got the error with the code below. Can anyone give me a solution. I appreciate it.</p>
<pre><code>num_classes = 10
input_shape = (32, 32, 3)
input_tensor = tf.keras.Input(shape=input_shape)
model = tf.keras.Sequential()
base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape, input_tensor=input_tensor)
for layer in base_model.layers:
    layer.trainable = False
model.add(base_model)
model.add(tf.keras.layers.GlobalAveragePooling2D())
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

extractor = tf.keras.Model(inputs=model.layers[0].input,
                                outputs=[layer.output for layer in model.layers])
</code></pre>
","1","Question"
"79333072","","<p>I am new to Rust and I'm looking into using <a href=""https://burn.dev/burn-book/"" rel=""nofollow noreferrer"">Burn</a> to port some Python/Torch code that I have for a new statistical parametric method.</p>
<p>Baby step 1: I want to generate a (10, 1) tensor with random values generated from a Cauchy distribution with known parameters. The distributions in Burn are very limited, so I'm using <a href=""https://docs.rs/statrs/latest/statrs/"" rel=""nofollow noreferrer"">statrs</a>. By using <code>statrs</code>, I can get a <code>Vec&lt;f64&gt;</code> and then I can wrap that into a <code>TensorData</code> in Burn and thus generate a <code>Tensor</code>.</p>
<p>I added some type signatures, but Burn has <code>Float</code> rather than the specific <code>f64</code> and I'm a bit confused by this. In fact, just for debugging purposes, I want to extract the data from the Burn tensor as a <code>Vec&lt;f64&gt;</code> to see it (I should see the same values from <code>vec: Vec&lt;f64&gt;</code>) but I am getting a runtime type incompatibility.</p>
<pre class=""lang-rust prettyprint-override""><code>use rand::prelude::Distribution;
use statrs::distribution::Cauchy;
use rand_chacha::ChaCha8Rng;
use rand_core::SeedableRng;
use burn::tensor::{Tensor, TensorData, Float};
use burn::backend::Wgpu;

type Backend = Wgpu;

fn main() {
    // some global refs
    let device = Default::default();
    let mut rng: ChaCha8Rng = ChaCha8Rng::seed_from_u64(2);

    // create random vec using statrs, store in a Vec&lt;f64&gt;
    let dist: Cauchy = Cauchy::new(5.0, 2.0).unwrap();
    let vec: Vec&lt;f64&gt; = dist.sample_iter(&amp;mut rng).take(10).collect();

    // wrap this into a Burn tensor
    let td: TensorData = TensorData::new(vec, [10, 1]);
    let tensor: Tensor&lt;Backend, 2, Float&gt; = Tensor::&lt;Backend, 2, Float&gt;::from_data(td, &amp;device);

    print!(&quot;{:?}\n&quot;, tensor.to_data().to_vec::&lt;f64&gt;().unwrap());
}

</code></pre>
<p>When running above, I get</p>
<pre><code>thread 'main' panicked at src/main.rs:23:55:
called `Result::unwrap()` on an `Err` value: TypeMismatch(&quot;Invalid target element type 
(expected F32, got F64)&quot;)
</code></pre>
<p>Using <code>to_vec::&lt;f32&gt;</code> works, but I would like the Burn tensor to have f64 values (torch has this) as the error seems to imply that I lost precision at some point - no great.</p>
<p>Is storing <code>f64</code> in a Burn tensor possible?</p>
","1","Question"
"79337075","","<p>I'm using <code>AdaBoostClassifier</code> with a weak learner (<code>DecisionTreeClassifier</code>) to classify a dataset. The dataset has 7857 samples:</p>
<pre class=""lang-py prettyprint-override""><code>X.shape
# Output: (7857, 5)

y.shape
# Output: (7857,)
</code></pre>
<p>Here’s the code for splitting the dataset and training the model:</p>
<pre class=""lang-py prettyprint-override""><code>X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=28
)

weak_learner = DecisionTreeClassifier(max_depth=1)

adb = AdaBoostClassifier(estimator=weak_learner, n_estimators=50, random_state=42)
adb_model = adb.fit(X_train, y_train)

y_pred = adb_model.predict(X_test)
print(classification_report(y_test, y_pred))
</code></pre>
<p>When I run this code with <code>test_size=0.25</code>, the output for the classification metrics is 100% for all categories:</p>
<pre class=""lang-none prettyprint-override""><code>              precision    recall  f1-score   support

       Cheap       1.00      1.00      1.00       496
   Expensive       1.00      1.00      1.00       506
  Reasonable       1.00      1.00      1.00       963

    accuracy                           1.00      1965
   macro avg       1.00      1.00      1.00      1965
weighted avg       1.00      1.00      1.00      1965
</code></pre>
<p>This cannot be true, as my data points are not perfectly separable. (I checked with a graph)</p>
<p>However, when I change the <code>test_size</code> to any other value (e.g., <code>0.3</code>, <code>0.2</code>), I get the following error:</p>
<pre class=""lang-none prettyprint-override""><code>ValueError: Found input variables with inconsistent numbers of samples
</code></pre>
<hr />
<p><strong>What I've Checked:</strong></p>
<ol>
<li>Ensured that <code>X</code> and <code>y</code> have the same number of samples.</li>
<li>Confirmed there are no missing values in <code>X</code> or <code>y</code>.</li>
</ol>
<hr />
<p><strong>Questions:</strong></p>
<ol>
<li>Why does <code>test_size=0.25</code> produce perfect metrics, but other <code>test_size</code> values result in an error?</li>
<li>How can I fix this issue to use different <code>test_size</code> values?</li>
</ol>
","0","Question"
"79337434","","<p>I am training a sklearn classifier, and inserted in a pipeline a feature selection step. Via grid search, I would like to determine what's the number of features that allows me to maximize performance. Still, I'd like to explore in the grid search the possibility that <em>no feature selection</em>, just a &quot;passthrough&quot; step, is the optimal choice to maximize performance.</p>
<p>Here's a reproducible example:</p>
<pre class=""lang-py prettyprint-override""><code>import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

# Load the Titanic dataset
titanic = sns.load_dataset('titanic')

# Select features and target
features = ['age', 'fare', 'sex']
X = titanic[features]
y = titanic['survived']

# Preprocessing pipelines for numeric and categorical features
numeric_features = ['age', 'fare']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant')),
    ('scaler', StandardScaler())
])

categorical_features = ['sex']
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant')),
    ('onehot', OneHotEncoder(drop='first'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# Initialize classifier and feature selector
clf = LogisticRegression(max_iter=1000, solver='liblinear')
sfs = SequentialFeatureSelector(clf, direction='forward')

# Create a pipeline that includes preprocessing, feature selection, and classification
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('feature_selection', sfs),
    ('classifier', clf)
])

# Define the parameter grid to search over
param_grid = {
    'feature_selection__n_features_to_select': [2],
    'classifier__C': [0.1, 1.0, 10.0],  # Regularization strength
}

# Create and run the grid search
grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X, y)

# Output the best parameters and score
print(&quot;Best parameters found:&quot;, grid_search.best_params_)
print(&quot;Best cross-validation score:&quot;, grid_search.best_score_)
</code></pre>
<p><code>X</code> here has three features (even after the <code>preprocessor</code> step), but the grid search code above doesn't allow to explore models in which all 3 features are used, as setting</p>
<pre><code> feature_selection__n_features_to_select: [2,3]
</code></pre>
<p>will give a <code>ValueError: n_features_to_select must be &lt; n_features</code>.</p>
<p>The obstacle here is that <code>SequentialFeatureSelector</code> doesn't consider the selection of all features (aka a passthrough selector) as a valid feature selection.</p>
<p>In other words, I would like to run a grid search that considers also the setting of</p>
<pre><code>('feature_selection', 'passthrough')
</code></pre>
<p>in the space of possible pipeline configurations. Is there an idiomatic/nice way to do that?</p>
","1","Question"
"79338394","","<p>During a cross-validation, <code>fit_resamples</code> return the average of the metric from the validation set.</p>
<pre><code>lr_model &lt;-
  linear_reg() |&gt;
  set_engine('lm')

lr_wf &lt;-
  workflow() |&gt;
  add_recipe(basic_recipe) |&gt;
  add_model(lr_model)

lr_cv &lt;-
  lr_wf |&gt;
  fit_resamples(
    folds,
    metrics = metric_set(rmse),
    control = control
  )
  
# let' extract result from CV. that will help us to compare it with other models
lr_cv |&gt;
  collect_metrics()
# That's the RMSE validation error
# .metric .estimator  mean     n  std_err .config             
# &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               
# rmse    standard   0.161    10 0.000370 Preprocessor1_Model1
</code></pre>
<p>The issue I have is how to get the training error.</p>
<p>The same issue occurs after the tuning of hyperparameters.</p>
<p>For example, when tuning the KNN to find the best number of neighbors, the <code>collect_metrics</code> and <code>show_best</code> return the average of the metrics of the validation set from cross-validation, whereas we all know that the best number of neighbors is when while the training errors decreased the validation errors start increasing.
Unfortunately, the <code>autoplot</code> function does not show us the training errors only the validation errors.</p>
<p>In this case, for example</p>
<pre><code>tree_grid &lt;-
  grid_regular(
    cost_complexity(),
    tree_depth(),
    min_n(),
    levels = c(3, 5, 10)
  )

tree_wf &lt;-
  workflow() %&gt;%
  add_model(tree_model) %&gt;%
  add_recipe(basic_recipe)

tree_res &lt;- 
  tree_wf %&gt;%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(rmse),
    control = control
  )
</code></pre>
<p>How to extract the training errors of each couple hyperparameters/folds?</p>
","1","Question"
"79341200","","<p>I’m facing an issue while trying to create an MLTable YAML file for a dataset in Azure ML.</p>
<p>I have a default datastore in my workspace containing two folders (OK and NOK) with images. My goal is to read all images and use the folder name as the label for each image.</p>
<p>Here’s what I’ve tried so far:</p>
<pre><code>mltable_yaml = &quot;&quot;&quot;
type: mltable
paths:
  - file: ./OK  
  - file: ./NOK 
transformations:
  - read_from_directory:
      image_column: image_url  
      folder_column: label  
      recursive: true         
&quot;&quot;&quot;

# Create directory and save MLTable
mltable_dir = &quot;image_data&quot;
os.makedirs(mltable_dir, exist_ok=True)
with open(os.path.join(mltable_dir, &quot;MLTable&quot;), &quot;w&quot;) as f:
    f.write(mltable_yaml)

training_data = Input(
    type=&quot;mltable&quot;,
    path=mltable_dir
)
</code></pre>
<p>However, when I run the experiment, I encounter the following error:</p>
<pre><code>MLTable input is invalid. UserErrorException:
    Message: Encountered user error while fetching data from Dataset. Error: UserErrorException:
    Message: MLTable yaml schema is invalid: 
Error Code: ScriptExecution.Validation
Validation Error Code: Invalid
Validation Target: Script
Native error: Dataflow script error: InvalidScriptElement(&quot;read_from_directory&quot;)
    ScriptError(InvalidScriptElement(&quot;read_from_directory&quot;))
=&gt; Invalid script element &quot;read_from_directory&quot;
    InvalidScriptElement(&quot;read_from_directory&quot;)
Error Message: Yaml script is invalid: InvalidScriptElement(&quot;read_from_directory&quot;).| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b
    InnerException None
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;MLTable yaml schema is invalid: \nError Code: ScriptExecution.Validation\nValidation Error Code: Invalid\nValidation Target: Script\nNative error: Dataflow script error: InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt; Invalid script element \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\nError Message: Yaml script is invalid: InvalidScriptElement(\&quot;read_from_directory\&quot;).| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b&quot;
    }
}
    InnerException UserErrorException:
    Message: MLTable yaml schema is invalid: 
Error Code: ScriptExecution.Validation
Validation Error Code: Invalid
Validation Target: Script
Native error: Dataflow script error: InvalidScriptElement(&quot;read_from_directory&quot;)
    ScriptError(InvalidScriptElement(&quot;read_from_directory&quot;))
=&gt; Invalid script element &quot;read_from_directory&quot;
    InvalidScriptElement(&quot;read_from_directory&quot;)
Error Message: Yaml script is invalid: InvalidScriptElement(&quot;read_from_directory&quot;).| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b
    InnerException None
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;MLTable yaml schema is invalid: \nError Code: ScriptExecution.Validation\nValidation Error Code: Invalid\nValidation Target: Script\nNative error: Dataflow script error: InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt; Invalid script element \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\nError Message: Yaml script is invalid: InvalidScriptElement(\&quot;read_from_directory\&quot;).| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b&quot;
    }
}
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;Encountered user error while fetching data from Dataset. Error: UserErrorException:\n\tMessage: MLTable yaml schema is invalid: \nError Code: ScriptExecution.Validation\nValidation Error Code: Invalid\nValidation Target: Script\nNative error: Dataflow script error: InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt; Invalid script element \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\nError M
</code></pre>
<p>From the error details, it seems like the read_from_directory element is not recognized, but I’m unsure how to structure the YAML to correctly map the folder name to the label.</p>
<p>How to resolve this?</p>
","0","Question"
"79343645","","<p>I’m working on an OCR system for Oman license plates and struggling to improve the accuracy of alphabet recognition. The plates often include small, bold characters, and my current preprocessing pipeline isn’t yielding satisfactory results.</p>
<p>so far  what i have done is :</p>
<p>PaddleOCR and Tesseract struggled with alphabet recognition, despite preprocessing and configurations (--oem 3, --psm 6).
Preprocessing Steps:
tried with Sauvola and Wolf-Jolion binarization, scaled images (1.5x), and applied dilation to enhance text.
Issue:</p>
<p>Alphabets remain challenging to recognize.</p>
<p>How can I improve preprocessing for better OCR recognition of small, bold alphabets?
Are there any OCR models or custom training approaches better suited for license plates with intricate designs like Oman’s?</p>
<p>sample license plate:</p>
<p><a href=""https://i.sstatic.net/yr7nQUS0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yr7nQUS0.png"" alt=""1"" /></a></p>
","-1","Question"
"79344545","","<p>I am migrating from XGBoost to LightGBM (since I need it's exact handling of interaction constraints) and I am struggling to understand the result of LightGBM CV. In the example below, the minimum log-loss is achieved on iteration 125, but <code>model['cvbooster'].best_iteration</code> returns -1. I would have expected it to return 125 as well - or am I misunderstanding something here? Is there a better way to get the best iteration, or does one just need to manually check?</p>
<p>I have seen <a href=""https://github.com/microsoft/LightGBM/issues/4777"" rel=""nofollow noreferrer"">this discussion</a> but even when I check the <code>boosters</code> in <code>cvbooster</code> (e.g., <code>model['cvbooster'].boosters[0].best_iteration</code>), they all return -1 as well...</p>
<pre><code>import lightgbm as lgb
import numpy as np
from sklearn import datasets

X, y = datasets.make_classification(n_samples=10_000, n_features=5, n_informative=3, random_state=9)

data_train_lgb = lgb.Dataset(X, label=y)

param = {'objective':   'binary',
         'metric':      ['binary_logloss'],
         'device_type': 'cuda'}

model = lgb.cv(param,
               data_train_lgb,
               num_boost_round=1_000,
               return_cvbooster=True)

opt_1 = np.argmin(model['valid binary_logloss-mean'])
print(f&quot;index argmin: {opt_1}&quot;)
print(f&quot;logloss argmin: {model['valid binary_logloss-mean'][opt_1]}&quot;)

opt_2 = model['cvbooster'].best_iteration
print(f&quot;index best_iteration: {opt_2}&quot;)
print(f&quot;logloss best_iteration: {model['valid binary_logloss-mean'][opt_2]}&quot;)

---

&gt;&gt;&gt; index argmin: 125
&gt;&gt;&gt; logloss argmin: 0.13245999867688793

&gt;&gt;&gt; index best_iteration: -1
&gt;&gt;&gt; logloss best_iteration: 0.2661896445658779
</code></pre>
","2","Question"
"79344565","","<p>While loading the tokenizer, I received this error:</p>
<pre><code>ImportError: Using bitsandbytes 4-bit quantization requires the latest version of bitsandbytes: 
pip install -U bitsandbytes.
</code></pre>
<p>I am using Jupyter notebook on Macbook M2 pro.</p>
<p>Below is the source code:</p>
<pre><code>quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type=&quot;nf4&quot;

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=quant_config,
    device_map=&quot;auto&quot;,
 )

base_model.generation_config.pad_token_id = tokenizer.pad_token_id
</code></pre>
<p>Can someone help on this?</p>
<p>I updated bitsandbytes as instructed, but the error persists.</p>
","1","Question"
"79345260","","<p>I'm trying to use torchrl's SyncDataCollector with a DQN I implemented myself in torch. As the DQN uses Conv2d and Linear Layer I have to calculate the correct size for the input of the first Linear Layer, the <code>size</code> param in the following net</p>
<pre><code>class PixelDQN(nn.Module):
    def __init__(self, input_shape, n_actions) -&gt; None:
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
        )
        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]
        self.fc_adv = nn.Sequential(
            NoisyLinear(size, 256),
            nn.ReLU(),
            NoisyLinear(256, n_actions),
        )
        self.fc_val = nn.Sequential(
            NoisyLinear(size, 256),
            nn.ReLU(),
            NoisyLinear(256, 1)
        )

    def forward(self, x: torch.Tensor):
        print(x.shape)
        conv = self.conv(x)
        print(conv.shape)
        adv = self.fc_adv(conv)
        val = self.fc_val(conv)
        outp = val + (adv - adv.mean(dim=1, keepdim=True))
        return outp
</code></pre>
<p>is responsible for that. As you can see I expect batched inputs as I will use a replay buffer and sample a batch from that.</p>
<p>I wrap that DQN in the following way and then use the SyncDataCollector:</p>
<pre><code>n_obs = [4,84,84]
n_act = 6

agent = QValueActor(
  module=PixelDQN(n_obs, n_act), in_keys=[&quot;pixels&quot;], spec=env.action_spec
)
policy_explore = EGreedyModule(
  env.action_spec, eps_end=EPS_END, annealing_num_steps=ANNEALING_STEPS
)
agent_explore = TensorDictSequential(
  agent, policy_explore
)

collector = SyncDataCollector(
  env,
  agent_explore,
  frames_per_batch=FRAMES_PER_BATCH,
  init_random_frames=INIT_RND_STEPS,
  postproc=MultiStep(gamma=GAMMA, n_steps=N_STEPS)
)
</code></pre>
<p>This however fails as the SyncDataCollector doesn't batch the obs from the env before giving them to the DQN so <code>size</code> calc gets wrong and the Linear layer get a wrong input dimension.
RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x49 and 3136x256)</p>
<p>I already tried to set <code>buffer=True</code> in SyncDataCollector. I also tried to use</p>
<pre><code>agent_explore = TensorDictSequential(
  UnsqueezeTransform(0, allow_positive_dim=True), agent, policy_explore
)
</code></pre>
<p>as this was kinda suggested by ChatGPT, however it didn't seem to have any effect.</p>
<p>I also tried the <code>UnsqueezeTransform</code> in my env creation, but that didn't work either, my env looks like this:</p>
<pre><code>def make_env(env_name: str):
    return TransformedEnv(
        GymEnv(env_name, from_pixels=True),
        Compose(
            RewardSum(),
            EndOfLifeTransform(),
            NoopResetEnv(noops=30),
            ToTensorImage(),
            Resize(84, 84),
            GrayScale(),
            FrameSkipTransform(frame_skip=4),
            CatFrames(N=4, dim=-3),
        )
    )
</code></pre>
<p>I could pull the <code>size</code> calc into the forward pass of my PixelDQN and check the size of the input tensor to adapt the calc, but this seems like a weird thing to do, since it would mean I'd need to run the size calc at each single forward pass.</p>
","0","Question"
"79350213","","<p>I’m working with a large dataset on Kaggle and want to speed up the imputation process by using GPU acceleration for KNN imputation. My current approach uses the CPU-based KNNImputer from sklearn, but it’s too slow for my needs.</p>
<p>I’ve heard that RAPIDS cuML offers GPU-accelerated KNN imputation. Here’s the code I tried so far</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import cudf
from cuml.experimental.preprocessing import KNNImputer

# Convert Pandas DataFrame to cuDF DataFrame
df_bad_cleaned_gpu = cudf.DataFrame.from_pandas(df_bad_cleaned)

# Initialize KNN imputer with neighbors
knn_imputer_gpu = KNNImputer(n_neighbors=36)

# Fit and transform
df_bad_knn_filled_gpu = knn_imputer_gpu.fit_transform(df_bad_cleaned_gpu)

# Convert back to Pandas DataFrame (if needed)
df_bad_knn_filled = df_bad_knn_filled_gpu.to_pandas()
</code></pre>
<p>Is this the correct way to implement KNN imputation on the GPU using RAPIDS?</p>
","1","Question"
"79350403","","<p>I have equations:</p>
<pre class=""lang-latex prettyprint-override""><code>$e_{ij} = \frac{X_i W^Q (X_j W^K + A^K_{ij}) }{\sqrt{D_z}}$
$\alpha_{ij} = softmax(e_{ij})$
$z_{i} = \sum_j \alpha_{ij} (X_j W^V + A^V_{ij})$
</code></pre>
<p>where sizes:</p>
<pre><code>X: [B, S, H,D]
each W: [H,D,D]
each A: [S, S, H,D]
</code></pre>
<p>how i can calculate it via matrix operations?</p>
<p>i have a partial solution</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F

B, S, H, D = X.shape
d_z = D  # Assuming d_z is equal to D for simplicity

W_Q = torch.randn(H, D, D)
W_K = torch.randn(H, D, D)
W_V = torch.randn(H, D, D)

a_K = torch.randn(S, S, H, D)
a_V = torch.randn(S, S, H, D)
}
XW_Q = torch.einsum('bshd,hde-&gt;bshe', X, W_Q)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]
XW_K = torch.einsum('bshd,hde-&gt;bshe', X, W_K)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]

e_ij_numerator = XW_Q.unsqueeze(2) @ (XW_K.unsqueeze(1) + a_K).transpose(-1, -2)  # [B, S, 1, H, D] @ [B, 1, S, H, D] -&gt; [B, S, S, H, D]
e_ij = e_ij_numerator / torch.sqrt(torch.tensor(d_z, dtype=torch.float32))  # [B, S, S, H, D]

XW_V = torch.einsum('bshd,hde-&gt;bshe', X, W_V)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]
alpha = F.softmax(e_ij, dim=2)  # [B, S, S, H, D]

z_i = torch.einsum('bshij,bshjd-&gt;bshid', alpha, XW_V.unsqueeze(1) + a_V)  # [B, S, S, H, D] @ [B, 1, S, H, D] -&gt; [B, S, S, H, D]
</code></pre>
<p>but z should be [B, S, H,D]</p>
","1","Question"
"79353843","","<p>I was working with stable_baselines3 library, when I found something that i did not expect.</p>
<p>Here a simple code to reproduce the issue:</p>
<pre><code>import gymnasium as gym

from stable_baselines3 import DQN

env = gym.make(&quot;CartPole-v1&quot;)

model = DQN(&quot;MlpPolicy&quot;, env, verbose=0, stats_window_size=100_000)
model.learn(total_timesteps=100_000)
</code></pre>
<p>Taking a look at the last episode reward:</p>
<pre><code>print(model.ep_info_buffer[-1])
</code></pre>
<blockquote>
<p>{'r': 409.0, 'l': 409, 't': 54.87983}</p>
</blockquote>
<p>But if I evaluate the model, with the following code:</p>
<pre><code>obs, info = env.reset()
total_reward = 0
while True:
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    total_reward = total_reward + reward
    if terminated or truncated:
        obs, info = env.reset()
        break

print(&quot;total_reward {}&quot;.format(total_reward))
</code></pre>
<blockquote>
<p>total_reward 196.0</p>
</blockquote>
<p>I get a different reward, what I did not expected.</p>
<p>I expected get the same 409 than in the model.ep_info_buffer[-1].</p>
<p>Why that difference? Is that .ep_info_buffer a different thing than the reward per episode?</p>
","2","Question"
"79359767","","<p>This <a href=""https://www.sciencedirect.com/science/article/pii/S0010482524006759"" rel=""nofollow noreferrer"">paper</a> proposes a medical image segmentation hybrid CNN - Transformer model for segmenting organs and lesions in medical images simultaneously. Their model has two output branches, one to output organ mask, and the other to output lesion mask. Now they describe the testing process as follows:</p>
<blockquote>
<p>In order to compare the performance of our approach with the state-
of-the-art approaches, the following evaluation metrics have been used: F1-score (F1-S), Dice score (D-S), Intersection Over Union (IoU), and
HD95, which are defined as follows:</p>
<p><a href=""https://i.sstatic.net/lnUW339F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lnUW339F.png"" alt=""enter image description here"" /></a></p>
</blockquote>
<blockquote>
<p><a href=""https://i.sstatic.net/53lIhglH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/53lIhglH.png"" alt=""enter image description here"" /></a></p>
<p>where T P is True Positives, T N is True Negatives, F P is False
Positives,and F N is False Negatives, all associated with the
segmentation classes of the test images. The Dice score is a macro
metric, which is calculated for N testing images as follow:</p>
<p><a href=""https://i.sstatic.net/AJvkURd8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJvkURd8.png"" alt=""enter image description here"" /></a>
where TPi, FPi and FNi are True Positives, True Negatives, False. Positives and False
Negative for the ith image, respectively.</p>
</blockquote>
<p>I am confused regarding how to implement those metrics (excluding HD95) like in this paper, what I understand is that to compute TP, FP, and FN for f1-score and IoU, I need to aggregate those 3 quantities (TP, FP, and FN) across all the samples in the test set for the two outputs (lesion and organ), and the aggregation is a sum operation. So for example to calculate the TP, I need to calculate it for every output of every sample and sum this TP. Then repeat this for calculating the TP  for every sample in a similar manner and then add all those TPs to get the overall TP. Then I do the same for FP and FN and then plug them in the formulas.</p>
<p>I am not sure if my understanding is correct or not. For Dice score, I need to calculate it for every output separately and then average them? I am not sure about that, so I accessed the <a href=""https://github.com/faresbougourzi/D-TrAttUnet"" rel=""nofollow noreferrer"">GitHub</a> for this paper. The model is defined <a href=""https://github.com/faresbougourzi/D-TrAttUnet/blob/main2/Architecture.py"" rel=""nofollow noreferrer"">here</a>, and the coding for the testing procedure is defined <a href=""https://github.com/faresbougourzi/D-TrAttUnet/blob/main2/detailed%20train%20and%20test/train_test_DTrAttUnet_BinarySegmentation.py"" rel=""nofollow noreferrer"">here</a>. The used framework is PyTorch. I don't have any knowledge regarding PyTorch, so still I can't understand how these metrics have been implemented, and hence, I cant confirm if my understanding is correct or not. So please can somebody explain the logic used to implement these metrics.</p>
<p>Edit 1 : I went through the code for calculating TP,FP, and FN in <code>train_test_DTrAttUnet_BinarySegmentation.py</code>:</p>
<pre><code>TP += np.sum(((preds == 1).astype(int) +
                             (yy == 1).astype(int)) == 2)
                TN += np.sum(((preds == 0).astype(int) +
                             (yy == 0).astype(int)) == 2)
                FP += np.sum(((preds == 1).astype(int) +
                             (yy == 0).astype(int)) == 2)
                FN += np.sum(((preds == 0).astype(int) +
                             (yy == 1).astype(int)) == 2)
</code></pre>
<p>It seems like they were doing the forward pass using a for loop and then accumulating the these quantities, and after this loop they calculate the metrics:</p>
<pre><code>    F1score = TP / (TP + ((1/2)*(FP+FN)) + 1e-8)
    IoU = TP / (TP+FP+FN)
</code></pre>
<p>So this means that they are accumulating the TP,FP and FN through all the images for both outputs and then they calculate the metrics, Is that correct ?
For Dice Score it seems tricky for me, they still inside the loop calculate some quantities :</p>
<pre><code>for idice in range(preds.shape[0]):
                    dice_scores += (2 * (preds[idice] * yy[idice]).sum()) / (
                        (preds[idice] + yy[idice]).sum() + 1e-8
                    )
    
                predss = np.logical_not(preds).astype(int)
                yyy = np.logical_not(yy).astype(int)
                for idice in range(preds.shape[0]):
                    dice_sc1 = (2 * (preds[idice] * yy[idice]).sum()) / (
                        (preds[idice] + yy[idice]).sum() + 1e-8
                    )
                    dice_sc2 = (2 * (predss[idice] * yyy[idice]).sum()) / (
                        (predss[idice] + yyy[idice]).sum() + 1e-8
                    )
                    dice_scores2 += (dice_sc1 + dice_sc2) / 2
</code></pre>
<p>Then at the end of the loop :</p>
<pre><code> epoch_dise = dice_scores/len(dataloader.dataset)
 epoch_dise2 = dice_scores2/len(dataloader.dataset)
</code></pre>
<p>Still, I cant understand what is going on for Dice Score.</p>
","3","Question"
"79360229","","<p>I want to inherit the <code>torch.utils.data.Dataset</code> class to load my custom image dataset, let's say for a classification task. here is the example of official pytorch website in this <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"" rel=""nofollow noreferrer"">link</a>:</p>
<pre><code>import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
</code></pre>
<p>I have noticed that:</p>
<ol>
<li>in the <code>__getitem__</code> we are reading an image from disk to memory. It means if we train our model for several epochs, we are re-reading the same image into memory several times. To my knowledge it is a costly action</li>
<li>a transform is applied each time an image is read from disk and that seems to me a nearly redundant action.</li>
</ol>
<p>I undrestand in very big datasets, we cannot fit the data fully into the memory and thus we have no choice but to read it this way (as we must iterate over all data in an epoch) and I was wondering, in the case that all my data can be fit into memory, isn't reading it all from the disk in the <code>__init__</code> function a better approach?</p>
<p>Through my little experience in computer vision I have noticed that croping images into fixed size images is very recurring in the <code>transform</code>. Then why shouldn't we crop the images once and store it on the disk somewhere else and throughout training only read the cropped images? This seems a more efficient approach to me.</p>
<p>I undrestand some transforms such as those used for augmentation rather than normalization would be better to be applied in the <code>__getitem__</code> to have a randomly generated data rather than a fixed one.</p>
<p>Can you clarify the subject for me?
If it is a common knowledge that I'm missing, please guide me to codebases with the proper approach.</p>
","0","Question"
"79361226","","<p>I am trying to reproduce by myself the LGBMRegressor predictions so when I succeed I will switch mean with median. But for now it seems that I am not able to.
Here is a simple script that I created for the proposes of checking if I can reproduce the results.</p>
<p>I need reg_y_hat to be the same as self_y_hat.
what am I missing? if I know what samples in train fall to each leaf I can aggregate the prediction myself...</p>
<pre><code> import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# Generate some random regression data
np.random.seed(42)
X = np.random.rand(100, 5)
y = 4 * X[:, 0] - 2 * X[:, 1] + np.random.rand(100) * 0.1

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the LGBMRegressor
model = lgb.LGBMRegressor(objective='regression', n_estimators=2, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)

# Regular predict:
reg_y_hat = model.predict(X_test)

# Get the initial prediction (mean of y_train)
init_pred = np.mean(y_train)

# Get the train leaf values
train_leaf_indices = model.predict(X_train, pred_leaf=True)
leaf_samples = {(i, leaf_id): [] for i in range(model.n_estimators) for leaf_id in np.unique(train_leaf_indices[:, i])}

# Store corresponding target values for each leaf
for i, row in enumerate(train_leaf_indices):
    for j, leaf_id in enumerate(row):
        leaf_samples[(j, leaf_id)].append(y_train[i])

# Compute avg for each leaf:
leaf_agg = {}
for key, values in leaf_samples.items():
    leaf_agg[key] = np.mean(values)

# Predict by aggregating the mean values and adding the initial prediction:
preds = []
test_leaf_indices = model.predict(X_test, pred_leaf=True)
for row_indices in test_leaf_indices:
    row_pred = init_pred
    for i, leaf_index in enumerate(row_indices):
        row_pred += model.learning_rate * (leaf_agg[(i, leaf_index)] - init_pred) # only the residual contribution of the leaf after initial prediction
    preds.append(row_pred)
self_y_hat = np.array(preds)

# Verify the results
print('Difference between reg_y_hat and self_y_hat:', np.abs(reg_y_hat - self_y_hat).sum())
</code></pre>
","1","Question"
"79361940","","<pre><code>import torch
import torch.nn as nn

class PINN(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_layers, neurons_per_layer):
        super(PINN, self).__init__()
        layers = []
        layers.append(nn.Linear(input_dim, neurons_per_layer))
        for _ in range(hidden_layers):
            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))
        layers.append(nn.Linear(neurons_per_layer, output_dim))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Example: generating random input data
inputs = torch.rand((1000, 3))  # 3D input coordinates


model = PINN(input_dim=3, output_dim=3, hidden_layers=4, neurons_per_layer=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10000
for epoch in range(epochs):
    optimizer.zero_grad()  
    nn_output = model(inputs) # Compute the NN prediction
    # Compute e.g gradient of nn_output
    loss.backward()  
    optimizer.step() 
</code></pre>
<p>I want to implement a physics-informed NN where the inputs are <code>N</code> 3d points (x,y,z) and the NN output is a vector-valued quantitiy at this point, that is, both input dimension and output dimension are the same.</p>
<p>To calculate the loss at every epoch, I need to have the value of the quantity at all points. Example: For <code>N=1000</code>points, I need all <code>1000</code> NN-predictions before I can proceed with the loss calculation.
In my code, I am basically giving a <code>1000x3</code> object to the input layer assuming that pytorch passes each row (<code>1x3</code>) separately to the network and at the end organizes it again as an <code>1000x3</code>object.</p>
<p>Does pytorch work like that or do I have to rethink this approach?</p>
","0","Question"
"79362113","","<p>I'm currently training my simple prediction AI but my GPU is training at 40S per epochs while my CPU is training at 9S per epochs</p>
<p>my CPU is i7-4720HQ and my GPU is Nvidia 950m</p>
<p>this is my code</p>
<pre><code>`import tensorflow as tf
 import pandas as pd
 from sklearn.preprocessing import LabelEncoder
 from sklearn.model_selection import train_test_split
 import matplotlib.pyplot as plt

 df = pd.read_csv('Updated_Train.csv')
 df = df.drop(columns='date')
 df = df.drop(columns='id')
 label_encoder = LabelEncoder()

 df['Country'] = label_encoder.fit_transform(df['Country'])
 df['Store'] = label_encoder.fit_transform(df['Store'])
 df['Product'] = label_encoder.fit_transform(df['Product'])

 x = df.iloc[:, :-1]
 y = df.iloc[:, -1]


 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = (
    train_dataset
    .shuffle(buffer_size=1000)  # Shuffle the data
    .batch(32)  # Batch size of 32
    .prefetch(tf.data.AUTOTUNE)  # Prefetch to improve pipeline performance
)

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
with tf.device('/CPU:0'):
    Input = tf.keras.layers.Input(shape=(3,))
    m = tf.keras.layers.Dense(32, activation='relu')(Input)
    m = tf.keras.layers.Dense(16,  activation='relu')(m)
    m = tf.keras.layers.Dense(8, activation='relu')(m)
    m = tf.keras.layers.Dense(8, activation='relu')(m)
    output = tf.keras.layers.Dense(1)(m)

    model = tf.keras.models.Model(inputs=Input, outputs=output)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  loss='mse')

    history = model.fit(train_dataset,
                        epochs=100,
                        validation_data=test_dataset)

model.save('Sticker.keras')

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Over Time')
plt.legend()
plt.show()

`
</code></pre>
<p>my data has 3 input column and 1 output column with 230129 rows</p>
<p>I ask gpt and was told to improve my data pipeline which i did but it still took 40s per epochs for my GPU</p>
","0","Question"
"79363695","","<p>I am trying to deploy a TensorFlow model to a Sagemaker endpoint. I have the model artifact at generic_graph.pb and the model's labels at labels.txt.</p>
<p>I started by creating a tar file with the following contents:</p>
<pre><code># #model directory structure 
# #model.tar.gz
# └── &lt;model_name&gt;
#     └── &lt;version_number&gt;
#         ├── saved_model.pb
#         └── variables
#             ├── labels.txt
</code></pre>
<p>I uploaded the file to a bucket in S3. Then, I tried to deploy the model with the following code:</p>
<pre><code>sagemaker_session = sagemaker.Session()
role = 'my-role'

model = TensorFlowModel(model_data='s3://my-bucket/model.tar.gz',
                    role=role,
                    framework_version='2.3.0')


predictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')
</code></pre>
<p>I keep getting the following error in my cloudwatch logs:</p>
<pre><code>ValueError: no SavedModel bundles found!
</code></pre>
<p>Not sure what else to try.</p>
","0","Question"
"79365374","","<p>I have access to a large CPU cluster that does not have GPUs. Is it possible to speed up YOLO training by parallelizing between multiple CPU nodes?<br />
The docs say that <code>device</code> parameter specifies the computational device(s) for training: a single GPU (device=0), multiple GPUs (device=0,1), CPU (device=cpu), or MPS for Apple silicon (device=mps).
What about multiple CPUs?</p>
","0","Question"
"79365624","","<p>I experienced problems when installing TFX on Google Colab, while other components such as</p>
<ul>
<li>tensorflow_model_analysis</li>
<li>tensorflow_data_validation</li>
<li>tensorflow_transform</li>
<li>tensorflow_transform.beam</li>
</ul>
<p>can now be installed</p>
<p>Just install TFX, which doesn't work yet</p>
<pre><code>from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner, Evaluator, Pusher
from tfx.proto import example_gen_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing
</code></pre>
<p>When installing TFX an error appears as follows
<a href=""https://i.sstatic.net/lGHLrF09.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lGHLrF09.png"" alt=""Erro Install TFX"" /></a></p>
<p>Please help, what errors should I fix?</p>
","-2","Question"
"79367182","","<p>I’m trying to import <code>Tensor</code> from PyTorch:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import Tensor
</code></pre>
<p>but I keep getting this error:</p>
<pre><code>ImportError: cannot import name 'Tensor' from 'torch' (unknown location)
</code></pre>
<h3>What I’ve Tried:</h3>
<ol>
<li>Checked that PyTorch is installed (<code>pip show torch</code>), and I’m using version <code>2.5.1</code>.</li>
<li>Reinstalled PyTorch:
<pre><code>pip uninstall torch
pip install torch
</code></pre>
</li>
<li>Tested the import in a Python shell, but the error persists.</li>
</ol>
<h3>Environment:</h3>
<ul>
<li>Python version: 3.10</li>
<li>PyTorch version: 2.5.1</li>
<li>OS: Windows 10</li>
<li>Virtual environment: Yes</li>
</ul>
<p>How can I fix this issue?</p>
","9","Question"
"79367409","","<p>I have mixed type multiple output (one regression and one classification) Keras model. I am trying to pass the same sample weights for both outputs as below.</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras

# Generate some sample data
np.random.seed(42)
X = np.random.rand(1000, 10)  # 1000 samples, 10 features
y_regression = X.sum(axis=1) + np.random.normal(0, 0.1, 1000) # Regression target
y_classification = (X.sum(axis=1) &gt; 5).astype(int) # Classification target (binary)

# Create sample weights 
sample_weights = np.random.rand(1000)

# Define the model with mixed outputs
def create_model():
    input_layer = keras.layers.Input(shape=(10,))
    dense1 = keras.layers.Dense(64, activation='relu')(input_layer)
    dense2 = keras.layers.Dense(32, activation='relu')(dense1)

    # Regression output
    regression_output = keras.layers.Dense(1, name='regression_output')(dense2)

    # Classification output
    classification_output = keras.layers.Dense(1, activation='sigmoid', name='classification_output')(dense2)

    model = keras.Model(inputs=input_layer, outputs=[regression_output, classification_output])

    return model


model = create_model()

# Compile the model with appropriate losses and metrics for each output
model.compile(
    optimizer='adam',
    loss={'regression_output': 'mse', 'classification_output': 'binary_crossentropy'},
    metrics={'regression_output': 'mae', 'classification_output': 'accuracy'},
)

# Train the model with sample weights
history = model.fit(
    X,
    {'regression_output': y_regression, 'classification_output': y_classification},
    epochs=10,
    batch_size=32,
    sample_weight=sample_weights,
)
</code></pre>
<p>Have also tried specifying weights for each output</p>
<pre><code>history = model.fit(
    X,
    {'regression_output': y_regression, 'classification_output': y_classification},
    epochs=10,
    batch_size=32,
    sample_weight={'regression_output': sample_weights, 'classification_output': sample_weights},  
)
</code></pre>
<p>However in both cases I am getting the below error which normally indicates a mismatch between the shape of <code>sample_weight</code> array and the shape of input data although that's not the case here. What's the correct way to pass <code>sample_weight</code> in multiple output model?</p>
<pre><code>KeyError                                  Traceback (most recent call last)
Cell In[18], line 58
     49 model.compile(
     50     optimizer='adam',
     51     loss={'regression_output': 'mse', 'classification_output': 'binary_crossentropy'},
     52     metrics={'regression_output': 'mae', 'classification_output': 'accuracy'},
     53 )
     57 # Train the model with sample weights
---&gt; 58 history = model.fit(
     59     X,
     60     {'regression_output': y_regression, 'classification_output': y_classification},
     61     epochs=20,
     62     batch_size=32,
     63     sample_weight=sample_weights,
     64 )

File /opt/jupyter/notebooks-generic/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--&gt; 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /opt/jupyter/notebooks-generic/.venv/lib/python3.9/site-packages/keras/src/trainers/compile_utils.py:785, in CompileLoss.call.&lt;locals&gt;.resolve_path(path, object)
    783 def resolve_path(path, object):
    784     for _path in path:
--&gt; 785         object = object[_path]
    786     return object

KeyError: 0
</code></pre>
","0","Question"
"79374019","","<p>getting attribute error while passing values to keras classifier</p>
<pre><code>from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from sklearn.model_selection import cross_val_score
from scikeras.wrappers import KerasClassifier

def create_model():
    model = Sequential([
        Dense(32,input_dim=16,kernel_initializer='normal',activation='relu'),
        Dense(16,kernel_initializer='normal',activation='relu'), 
        Dense(1,kernel_initializer='normal',activation='sigmoid')
    ]) 
    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
    return model 


estimator = KerasClassifier(model=create_model,epochs=100,verbose=0)

cv_scores = cross_val_score(estimator, all_features, all_classes, cv=10)
print(&quot;Mean cross-validation accuracy:&quot;, cv_scores.mean())
</code></pre>
<p>passing values to cross_val_score(estimator, all_features, all_classes, cv=10). Not sure if any there any change in input fields</p>
<p>I am getting Attribute error here : cv_scores = cross_val_score(estimator, all_features, all_classes, cv=10)</p>
<p>Complete error message :</p>
<pre><code>--------------------------------------------------------------------------- 
AttributeError                            Traceback (most recent call last) Cell In[38], line 18
     13     return model 
     16 estimator = KerasClassifier(model=create_model,epochs=100,verbose=0)
---&gt; 18 cv_scores = cross_val_score(estimator, all_features, all_classes, cv=10)
     19 print(&quot;Mean cross-validation accuracy:&quot;, cv_scores.mean())

File ~\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py:216, in validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args,
**kwargs)
    210 try:
    211     with config_context(
    212         skip_parameter_validation=(
    213             prefer_skip_nested_validation or global_skip_validation
    214         )
    215     ):
--&gt; 216         return func(*args, **kwargs)
    217 except InvalidParameterError as e:
    218     # When the function is just a wrapper around an estimator, we allow
    219     # the function to delegate validation to the estimator, but we replace
    220     # the name of the estimator by the name of the function in the error
    221     # message to avoid confusion.
    222     msg = re.sub(
    223         r&quot;parameter of \w+ must be&quot;,
    224         f&quot;parameter of {func.__qualname__} must be&quot;,
    225         str(e),
    226     )

File ~\anaconda3\Lib\site-packages\sklearn\model_selection\_validation.py:684, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)
    681 # To ensure multimetric format is not supported
    682 scorer = check_scoring(estimator, scoring=scoring)
--&gt; 684 cv_results = cross_validate(
    685     estimator=estimator,
    686     X=X,
    687     y=y,
    688     groups=groups,
    689     scoring={&quot;score&quot;: scorer},
    690     cv=cv,
    691     n_jobs=n_jobs,
    692     verbose=verbose,
    693     params=params,
    694     pre_dispatch=pre_dispatch,
    695     error_score=error_score,
    696 )
    697 return cv_results[&quot;test_score&quot;]

File ~\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py:216, in validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args,
**kwargs)
    210 try:
    211     with config_context(
    212         skip_parameter_validation=(
    213             prefer_skip_nested_validation or global_skip_validation
    214         )
    215     ):
--&gt; 216         return func(*args, **kwargs)
    217 except InvalidParameterError as e:
    218     # When the function is just a wrapper around an estimator, we allow
    219     # the function to delegate validation to the estimator, but we replace
    220     # the name of the estimator by the name of the function in the error
    221     # message to avoid confusion.
    222     msg = re.sub(
    223         r&quot;parameter of \w+ must be&quot;,
    224         f&quot;parameter of {func.__qualname__} must be&quot;,
    225         str(e),
    226     )

File ~\anaconda3\Lib\site-packages\sklearn\model_selection\_validation.py:347, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)
    345 X, y = indexable(X, y)
    346 params = {} if params is None else params
--&gt; 347 cv = check_cv(cv, y, classifier=is_classifier(estimator))
    349 scorers = check_scoring(
    350     estimator, scoring=scoring, raise_exc=(error_score == &quot;raise&quot;)
    351 )
    353 if _routing_enabled():
    354     # For estimators, a MetadataRouter is created in get_metadata_routing
    355     # methods. For these router methods, we create the router to use
    356     # `process_routing` on it.

File ~\anaconda3\Lib\site-packages\sklearn\base.py:1237, in is_classifier(estimator)    1230     warnings.warn(    1231         f&quot;passing a class to {print(inspect.stack()[0][3])} is deprecated and &quot;    1232         &quot;will be removed in 1.8. Use an instance of the class instead.&quot;,    1233         FutureWarning,    1234     )    1235  return getattr(estimator, &quot;_estimator_type&quot;, None) == &quot;classifier&quot;
-&gt; 1237 return get_tags(estimator).estimator_type == &quot;classifier&quot;

File ~\anaconda3\Lib\site-packages\sklearn\utils\_tags.py:430, in get_tags(estimator)
    428 for klass in reversed(type(estimator).mro()):
    429     if &quot;__sklearn_tags__&quot; in vars(klass):
--&gt; 430         sklearn_tags_provider[klass] = klass.__sklearn_tags__(estimator)  # type: ignore[attr-defined]
    431         class_order.append(klass)
    432     elif &quot;_more_tags&quot; in vars(klass):

File ~\anaconda3\Lib\site-packages\sklearn\base.py:540, in ClassifierMixin.__sklearn_tags__(self)
    539 def __sklearn_tags__(self):
--&gt; 540     tags = super().__sklearn_tags__()
    541     tags.estimator_type = &quot;classifier&quot;
    542     tags.classifier_tags = ClassifierTags()
</code></pre>
","0","Question"
"79375287","","<p>I am fine-tuning a <a href=""https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2"" rel=""nofollow noreferrer"">Donut Cord-v2</a> model with my invoice data which is around 360 GB in size when preprocessed and saved on disk as a dataset. I am following <a href=""https://github.com/philschmid/document-ai-transformers/blob/main/training/donut_sroie.ipynb"" rel=""nofollow noreferrer"">this</a> notebook almost exactly, except I have 6 training epochs instead of 3.</p>
<p>I am training on single Nvidia H100 SXM GPU / Intel Xeon® Gold 6448Y / 128 GB RAM.</p>
<p>Whenever I start training, and inspect CPU and GPU utilization using <code>htop</code> and <code>nvidia-smi</code>, I see that CPU is at 10-12% utilization, used by python, GPU memory is almost 90% filled constantly, but GPU Utilization is almost always 0. If I keep refreshing the output of <code>nvidia-smi</code>, once every 10-12 seconds the utilization will jump to 100% and then go back to 0 immediately. I cant help but feel ther eis a bottleneck between my CPU and GPU, where CPU attempts to constantly process data and send it to GPU, GPU processes it very fast, and just idles, awaiting for the next batch from cpu. I load already pre-processed dataset from disk like so:</p>
<pre><code>from datasets import load_from_disk
processed_dataset = load_from_disk(r&quot;/dataset/dataset_final&quot;)
</code></pre>
<p>My processor config is as follows:</p>
<pre><code>from transformers import DonutProcessor

new_special_tokens = [] # new tokens which will be added to the tokenizer
task_start_token = &quot;&lt;s&gt;&quot;  # start of task token
eos_token = &quot;&lt;/s&gt;&quot; # eos token of tokenizer

processor = DonutProcessor.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# add new special tokens to tokenizer
processor.tokenizer.add_special_tokens({&quot;additional_special_tokens&quot;: new_special_tokens + [task_start_token] + [eos_token]})

# we update some settings which differ from pretraining; namely the size of the images + no rotation required
processor.feature_extractor.size = [1200,1553] # should be (width, height)
processor.feature_extractor.do_align_long_axis = False
</code></pre>
<p>My model config is:</p>
<pre><code>import torch
from transformers import VisionEncoderDecoderModel, VisionEncoderDecoderConfig

#print(torch.cuda.is_available())

# Load model from huggingface.co
model = VisionEncoderDecoderModel.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# Resize embedding layer to match vocabulary size
new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))
print(f&quot;New embedding size: {new_emb}&quot;)
# Adjust our image size and output sequence lengths
model.config.encoder.image_size = processor.feature_extractor.size[::-1] # (height, width)
model.config.decoder.max_length = len(max(processed_dataset[&quot;train&quot;][&quot;labels&quot;], key=len))

# Add task token for decoder to start
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['&lt;s&gt;'])[0]
</code></pre>
<p>And my training code is:</p>
<pre><code>import gc
gc.collect()

torch.cuda.empty_cache()


from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

import logging
logging.basicConfig(level=logging.INFO)

# Arguments for training
training_args = Seq2SeqTrainingArguments(
    output_dir=r&quot;/trained&quot;,  # Specify a local directory to save the model
    num_train_epochs=6,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    weight_decay=0.01,
    fp16=True,
    logging_steps=50,
    save_total_limit=2,
    evaluation_strategy=&quot;no&quot;,
    save_strategy=&quot;epoch&quot;,
    predict_with_generate=True,
    report_to=&quot;none&quot;,
    # Disable push to hub
    push_to_hub=False
   
)

# Create Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset[&quot;train&quot;],
)


# Start training
trainer.train()
</code></pre>
<p>The estimated time to complete the training with 6 epochs, with 360 GB dataset, is 54 hours. When I run the same exact code on my PC that has Intel i9 11900KF / RTX 3050, I see GPU utilization constantly at 100%. Is there a bottleneck in my code? Why does CPU keep processing so much on already preprocessed dataset? Cuda 12.6</p>
<p>Edit:</p>
<p>Does it make sense to change the <a href=""https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.dataloader_num_workers"" rel=""nofollow noreferrer"">dataloader_num_workers</a> parameter of <code>Seq2SeqTrainer</code> to &gt;0 value, since my RAM and CPU core count allows it? (and since CPU utilization is at 10-12% max.)</p>
","1","Question"
"79377971","","<p>I'm trying to train a TensorFlow experimental model from the official TensorFlow Object Detection API, specifically a RetinaNet model. However, I encounter the following error during the training process:</p>
<pre><code>restoring or initializing model...
train | step:      0 | training until step 100...

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

&lt;ipython-input-19-d18d13cd1da2&gt; in &lt;cell line: 0&gt;()
----&gt; 1 model, eval_logs = tfm.core.train_lib.run_experiment(
      2     distribution_strategy=distribution_strategy,
      3     task=task,
      4     mode='train_and_eval',
      5     params=exp_config,

7 frames

/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57       e.message += &quot; name: &quot; + name
     58     raise core._status_to_exception(e) from None
---&gt; 59   except TypeError as e:
     60     keras_symbolic_tensors = [x for x in inputs if _is_keras_symbolic_tensor(x)]
     61     if keras_symbolic_tensors:

InvalidArgumentError: Graph execution error:

Detected at node retina_net_model/res_net/conv2d/Conv2D defined at (most recent call last):
  File &quot;/usr/lib/python3.11/threading.py&quot;, line 1002, in _bootstrap

  File &quot;/usr/lib/python3.11/threading.py&quot;, line 1045, in _bootstrap_inner

  File &quot;/tmp/__autograph_generated_file_fu7t7gj.py&quot;, line 30, in step_fn

  File &quot;/usr/local/lib/python3.11/dist-packages/official/vision/tasks/retinanet.py&quot;, line 327, in train_step

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py&quot;, line 588, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py&quot;, line 1142, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 96, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/official/vision/modeling/retinanet_model.py&quot;, line 129, in call

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py&quot;, line 588, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py&quot;, line 1142, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 96, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/functional.py&quot;, line 514, in call

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/functional.py&quot;, line 671, in _run_internal_graph

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py&quot;, line 1142, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 96, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/layers/convolutional/base_conv.py&quot;, line 289, in call

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/layers/convolutional/base_conv.py&quot;, line 261, in convolution_op

Detected at node retina_net_model/res_net/conv2d/Conv2D defined at (most recent call last):
  File &quot;/usr/lib/python3.11/threading.py&quot;, line 1002, in _bootstrap

  File &quot;/usr/lib/python3.11/threading.py&quot;, line 1045, in _bootstrap_inner

  File &quot;/tmp/__autograph_generated_file_fu7t7gj.py&quot;, line 30, in step_fn

  File &quot;/usr/local/lib/python3.11/dist-packages/official/vision/tasks/retinanet.py&quot;, line 327, in train_step

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py&quot;, line 588, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py&quot;, line 1142, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 96, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/official/vision/modeling/retinanet_model.py&quot;, line 129, in call

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py&quot;, line 588, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py&quot;, line 1142, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 96, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/functional.py&quot;, line 514, in call

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/functional.py&quot;, line 671, in _run_internal_graph

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 65, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py&quot;, line 1142, in __call__

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py&quot;, line 96, in error_handler

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/layers/convolutional/base_conv.py&quot;, line 289, in call

  File &quot;/usr/local/lib/python3.11/dist-packages/tf_keras/src/layers/convolutional/base_conv.py&quot;, line 261, in convolution_op

2 root error(s) found.
  (0) INVALID_ARGUMENT:  No DNN in stream executor.
     [[{{node retina_net_model/res_net/conv2d/Conv2D}}]]
     [[while/body/_1/while/NoOp/_31]]
  (1) INVALID_ARGUMENT:  No DNN in stream executor.
     [[{{node retina_net_model/res_net/conv2d/Conv2D}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_loop_fn_32055]
</code></pre>
<p>I am using the exact notebook for training [https://www.tensorflow.org/tfmodels/vision/object_detection]. I didnt change anything.</p>
<p>What I've tried:</p>
<ol>
<li>TensorFlow version compatibility - ensuring I'm using the correct version of TensorFlow for the GPU.</li>
<li>Checking GPU availability - confirmed that the GPU is being used in Colab.</li>
<li>Reducing batch size - to avoid memory issues, but still facing the same error.</li>
</ol>
","0","Question"
"79381185","","<p>I'm not referring to <a href=""https://arxiv.org/abs/1904.09675"" rel=""nofollow noreferrer"">BERTScore</a>. BERTScore uses token-level word embeddings, you compute pairwise cosine similarity of word embeddings and obtain scores using greedy matching.</p>
<p>I'm referring to <a href=""https://arxiv.org/abs/1908.10084"" rel=""nofollow noreferrer"">Sentence BERT</a>. I.e., pure cosine similarity to compare semantic similarity of sentences, not precision, recall or f1-measure. <strong>The question is, if we do this on document level, i.e., several sentences, do we then just compute the mean cosine similarity or is this metric not suitable as a machine translation evaluation metric (alternative to BLEU)?</strong></p>
<p>Because for individual sentences, it does make sense as it capture semantic similarity. Sentences that mean the same thing but are phrased differently get penalized by BLEU but get rather high values with Sentence BERT, which is exactly what I want. However, I could not find the use of Sentence BERT in recent WMT Shared Metric Task papers, so I assume there is a catch I am missing which explains why people do not use this approach.</p>
","0","Question"
"79387529","","<p>I am trying to run a simple Random Forest Classification Model using the iris dataset and integrate it into Gemini AI</p>
<p>Here is my code:</p>
<pre><code>import google.generativeai as genai
from vertexai.preview.generative_models import (
   

 FunctionDeclaration,
    GenerativeModel,
    Part,
    Tool,
)


genai.configure(api_key=&quot;API KEY&quot;)

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Load and train the model
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

def predict_iris_species(sepal_length, sepal_width, petal_length, petal_width):
    &quot;&quot;&quot;
    Predicts the iris species based on sepal and petal measurements.

    Args:
        sepal_length (float): Length of the sepal in cm.
        sepal_width (float): Width of the sepal in cm.
        petal_length (float): Length of the petal in cm.
        petal_width (float): Width of the petal in cm.

    Returns:
            str: The predicted iris species.
    &quot;&quot;&quot;
    input_data = [[sepal_length, sepal_width, petal_length, petal_width]]
    prediction = model.predict(input_data)
    return str(iris.target_names[prediction[0]])

tools = Tool(
    function_declarations=[
        FunctionDeclaration(
            name=&quot;predict_iris_species&quot;,
            description=&quot;predicts the iris species based on sepal and petal measurements&quot;,
            parameters={
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;sepal_length&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;Length of the sepal in cm.&quot;},
                    &quot;sepal_width&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;Width of the sepal in cm.&quot;},
                    &quot;petal_length&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;Length of the petal in cm.&quot;},
                    &quot;petal_width&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;Width of the petal in cm.&quot;}
                },
                &quot;required&quot;: [&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;petal_length&quot;, &quot;petal_width&quot;]
            }
        )
    ]
)

llm = genai.GenerativeModel(model_name='gemini-1.5-flash',
                              tools=[tools])

chat = llm.start_chat()
response = chat.send_message(&quot;what is the  species of the iris flower with sepal length 5.1, sepal width 3.5, petal length 1.4, and petal width 0.2?&quot;)
response.text
</code></pre>
<p>I get an error saying:</p>
<pre><code>TypeError: Invalid input type. Expected an instance of `genai.FunctionDeclarationType`.
However, received an object of type: &lt;class 'vertexai.generative_models._generative_models.Tool'&gt;.
Object Value: function_declarations {
...
    property_ordering: &quot;petal_length&quot;
    property_ordering: &quot;petal_width&quot;
  }
}
</code></pre>
<p>What does it mean? I thought that's how you format the JSON data? Could it be that my function from <code>predict_iris_species</code> need to return something else instead of a <code>string?</code></p>
<p>Would it be the fact that it needs to output a JSON dictionary?</p>
","0","Question"
"79387801","","<p>This is a question about grain, the python library for data ingestion. <a href=""https://google-grain.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">https://google-grain.readthedocs.io/en/latest/index.html</a></p>
<p>In the context of reinforcement learning I have a data pre-processing step where I have match replays as individual files, pre-process into JAX arrays, and then process.</p>
<p>I would be nice to have a library that understand the logic of transforming lists of JAX arrays and bundle them together into sequences for training recurrent networks, and into batches for gradient descent.</p>
<p>I think I have almost all components. I've implemented a <code>ReplayDataSource</code> which subclasses <code>grain.RandomAccessDataSource</code> which opens and pre-processes the match file into memory. I've implemented a Sampler which returns contiguous indices to build sequences. With one data source I can have all I need with (schematically)</p>
<pre><code>source = ReplayDataSource( ... )
transformations = [grain.Batch(batch_size)]
sampler = MySampler( ... )

data_loader = grain.DataLoader(
    data_source = source,
    sampler = sampler,
    operations = transformations,
    shard_options=grain.NoSharding(),
)
</code></pre>
<p>This will add 2 outer dimensions to my dataset, one for sequences and another for batches. Great!</p>
<p>There's one element missing. If I have say 1000 replay files and I can't load all to memory, how do I tell the DataLoader to instantiate load ReplayDataSources into memory and get rid of them as needed, as I iterate on the elements of the DataLoader?</p>
<p>I've tried <code>source = [source1, source2]</code> on the DataLoader args but that didn't work. I've try making a <code>MapDataset</code> out of the data sources, but I don't know how to compose those either. Same with the <code>IterDataset</code>.</p>
<p>The only grain functionality I really care about is sequences and batches. How do I create a dataset compose out of make replay files and only open those as needed?</p>
","0","Question"
"79388942","","<p>I came up against a problem when trying to use <code>tf.keras.models.load_model(Path_to_pb_model)</code> to load a Savedformat model.</p>
<p>I'm using Tensorflow 2.17.1 with Keras 3.5.</p>
<p>Apparently Keras 3 only accepts <code>.keras</code> or <code>.h5</code> formats.</p>
<p>First off, I'm a bit confused because according to this documentation:
<a href=""https://www.tensorflow.org/tutorials/keras/save_and_load"" rel=""nofollow noreferrer"">save and load models </a>, you should be able to save &amp; load .pb models in Keras 3.</p>
<p><a href=""https://i.sstatic.net/WxclpAYw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WxclpAYw.png"" alt=""enter image description here"" /></a></p>
<p>Is the documentation wrong? Because when I try to run this code in Colab, I get an error saying only <code>.keras</code> &amp; <code>.h5</code> file formats are supported.</p>
<p>Alternatively, I also checked <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">this guide</a> on how to load &amp; save the SavedModel format using the tf.saved_model APIs but that doesn't return a Keras Object and there's no <code>.evaluate()</code> or <code>.predict()</code> methods.</p>
<p>So is there any way to load SavedModel format in Keras3? Then evaluate that model on my test data?</p>
","0","Question"
"79393871","","<p>The boxes returned by the predict function doesn't seem to be of normalized form, even after multiplying with the image width and height i can't get the coordinates of the bounding boxes.</p>
<pre><code>import torch
from groundingdino.util.inference import load_model, load_image, predict, annotate
import cv2

# Load the model
model = load_model(&quot;../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py&quot;,
                   &quot;../GroundingDINO/weights/groundingdino_swint_ogc.pth&quot;)
IMAGE_PATH = &quot;.asset/cat_dog.jpeg&quot;
TEXT_PROMPT = &quot;person . animal . bird . object&quot;
BOX_THRESHOLD = 0.35
TEXT_THRESHOLD = 0.25

# Load the image
image_source, image = load_image(IMAGE_PATH)

# Perform prediction
boxes, logits, phrases = predict(
    model=model,
    image=image,
    caption=TEXT_PROMPT,
    box_threshold=BOX_THRESHOLD,
    text_threshold=TEXT_THRESHOLD
)

# Get image dimensions
ht, wd = image_source.shape[:2]
print(ht, wd, image_source.shape[:2])

# Convert bounding boxes to absolute coordinates
abs_box = boxes * torch.tensor([wd, ht, wd, ht])
abs_box = [abs_bo.numpy().astype(&quot;int&quot;) for abs_bo in abs_box]

annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)

for abs_bo in abs_box:
    cv2.rectangle(annotated_frame, (abs_bo[0], abs_bo[1]),[![enter image description here][1]][1] (abs_bo[2], abs_bo[3]), (255, 0, 0), 2)

cv2.imwrite(&quot;annotated_image.jpg&quot;, annotated_frame)

</code></pre>
<p><a href=""https://i.sstatic.net/eACzpJ0v.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eACzpJ0v.png"" alt=""enter image description here"" /></a></p>
<p>The blue boxes are made from the &quot;absolute&quot; coordinates, any insight on how to manipulate the data returned to obtain absolute coordinates would be super helpful, thank you.</p>
","0","Question"
"79395423","","<p>I'm currently trying to implement my own version of 2D Pooling (with channels included) in Python using only NumPy, but I've run into a bit of a roadblock with vectorizing the backpropagation process. The operation I want to do is just to add each of the windows from an <code>as_strided</code> view back into the original location from where it was taken.</p>
<p>The windowed view has 5 dimensions, with the shape being (channels, height, width, kernel height, kernel width). The problem that I am facing is that these windows can overlap depending on the stride value, so other solutions that I have seen can cause incorrect values in the finished gradients.</p>
<p>Here is the class that I made that includes both the forward and backward passes:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

def maxPool(partition: np.ndarray, backwards: bool = False):
        max_val = np.max(partition)
        if backwards:
            mask = np.zeros(partition.shape, dtype=partition.dtype)
            mask[partition.argmax()] = 1
            return mask
        return max_val

class Pooling2D():
    def __init__(self, pool_funct: callable, kernel_size: tuple[int, int], 
                 input_size: tuple[int, int, int], strides: tuple[int, int] = (1, 1), padding: int = 0) -&gt; None:
        self.funct = pool_funct
        self.in_size = input_size
        self.out_size = (input_size[0],
                        (input_size[1] - kernel_size[0] + 2*padding) // strides[0] + 1, 
                        (input_size[2] - kernel_size[1] + 2*padding) // strides[1] + 1)
        self.kernel_height, self.kernel_width = kernel_size
        self.window_shape = (*self.out_size, 
                             self.kernel_height, 
                             self.kernel_width)
        self.stride_h, self.stride_w = strides
        self.padding = padding

    def forward(self, x: np.ndarray) -&gt; np.ndarray:
        x_padded = np.pad(x, pad_width=((0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')
        strides = (x_padded.strides[0], 
                   self.stride_h * x_padded.strides[1], 
                   self.stride_w * x_padded.strides[2], 
                   x_padded.strides[1], 
                   x_padded.strides[2])

        #Split into windows, and apply the pooling function to each window.
        A_s = np.lib.stride_tricks.as_strided(x_padded, shape=self.window_shape, strides=strides)
        pool_val =  np.apply_along_axis(self.funct, -1, A_s.reshape(*self.out_size, -1))

        self.last_in = x
        self.last_out = pool_val
        return pool_val
    
    def backward(self, dx: np.ndarray) -&gt; np.ndarray:
        x_padded = np.pad(self.last_in, ((0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode=&quot;constant&quot;)
        strides = (x_padded.strides[0], 
                   self.stride_h * x_padded.strides[1], 
                   self.stride_w * x_padded.strides[2], 
                   x_padded.strides[1], 
                   x_padded.strides[2])
        
        #Window frames for previous input / Mask creation
        main_windows = np.lib.stride_tricks.as_strided(x_padded, self.window_shape, strides, writeable=False)
        mask = np.apply_along_axis(self.funct, -1, main_windows.reshape(*self.out_size, -1), backwards=True).reshape(main_windows.shape)

        #Use mask to distribute the gradient into the mask, reshaped into (channels, kernel height, kernel width, num of windows)
        pre_grad = np.einsum(&quot;ghw,ghwxy-&gt;ghwxy&quot;, dx, mask).transpose(0, 3, 4, 1, 2)
        pre_grad = pre_grad.reshape(*pre_grad.shape[:3], -1) 

        # Zero array of original size (channels, in height, in width)
        final_grad = np.zeros_like(x_padded)

        # _____________________________________________
        # - - - - - THE PART TO BE VECTORIZED - - - - -
        # _____________________________________________
        right = 0
        down = 0
        for i in range(pre_grad.shape[3]):
            if right+self.kernel_width &gt; x_padded.shape[2]:
                right = 0
                down += self.stride_h
            final_grad[:, down:down+self.kernel_height, 
                          right:right+self.kernel_width] += pre_grad[:, :, :, i]
            right += self.stride_w
        
        return final_grad[:, self.padding:-self.padding, self.padding:-self.padding] if self.padding &gt; 0 else final_grad
</code></pre>
<p>My main question is, can I do the last part of the backward pass without any for loops? I wanted to reach a fully vectorized solution but wasn't able to find one that worked well with varying strides, padding, and channels yet.</p>
<p>An example solution for this problem would be:</p>
<pre class=""lang-py prettyprint-override""><code>pool = Pooling2D(function=maxPool, kernel_size=(3, 3), input_size=(2, 6, 6), strides=(1, 1), padding=0)

A = np.random.randint(3, 10, pool.in_size)
B = np.random.randint(3, 10, pool.out_size)

# Forward pass, should result in a (2, 4, 4) array
result = pool.forward(A)
print(result.shape)

# Backward pass, should result in a (2, 6, 6) array
grad = pool.backward(result - B)
print(grad)

# Where the output should look something like this:
# (2, 4, 4)
# [[[ 0  0  0  0 10  0]
#   [ 0  0  0 14  0  0]
#   [ 1  0  0  0  0  3]
#   [ 0  2  0  4  0  0]
#   [ 0  0  0  0  0  0]
#   [ 0  4  0  0  1  0]]
#
#  [[ 0  0  0  0  0  0]
#   [ 0  0  0 14  0  0]
#   [ 0  8  0  0  0  0]
#   [ 0  0  7  0  0  0]
#   [ 0  0  0  9  0  0]
#   [ 0  0  0  0  0  0]]]

</code></pre>
<p>I also attempted to do a secondary <code>as_strided</code> operation using the shape of <code>x</code> and a reconstructed strides array using <code>x</code>'s shape and item-size, but no matter what I tried it would sometimes place the values of the gradient in the incorrect spots and didn't allow for any sort of overlap.</p>
<p>Here's an example of what I tried before that didn't work properly:</p>
<pre class=""lang-py prettyprint-override""><code># Get the strides from the original array
_, Hs, Ws = x.strides
new_strides = (Hs*Ws*pre_grad.itemsize, Ws*pre_grad.itemsize, pre_grad.itemsize)

# Do a secondary as_strided
final_grad = np.lib.stride_tricks.as_strided(pre_grad, x.shape, new_strides)
</code></pre>
<p>But with this solution, if I have something like a (1, 1) stride and a (3, 3) kernel size, then it doesn't add any overlapped values together and it also does not put them in the correct locations.</p>
","0","Question"
"79395477","","<p>How can I save a <code>tf.data.Dataset</code> in multiple shards using <code>tf.data.Dataset.save()</code>?  I am reading in my dataset from CSV using <code>tf.data.experimental.make_csv_dataset</code>.</p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#save"" rel=""nofollow noreferrer"">TF docs here</a> are not very helpful. There is a <code>shard_func</code> argument, but the examples given aren't helpful and its not clear how to map to an <code>int</code> in a deterministic way. Using random <code>int</code>s doesn't seem to work either.</p>
<p>The solution in a similar question <a href=""https://stackoverflow.com/questions/77067982/how-to-use-the-shard-func-in-tensorflows-tf-data-dataset-save"">here</a> generates an error for me</p>
<blockquote>
<p>TypeError: unsupported operand type(s) for %: 'collections.OrderedDict' and 'int'</p>
</blockquote>
<p><strong>Single Shard</strong> (works)</p>
<p>This code successfully saves to a single shard:</p>
<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf

# gen data
n=10000
pd.DataFrame(
    {'label': np.random.randint(low=0, high=2, size=n),
     'f1': np.random.random(n),
     'f2': np.random.random(n),
     'f3': np.random.random(n),
     'c1': np.random.randint(n),
     'c2': np.random.randint(n)}
).to_csv('tmp.csv')
# load data into a tf.data.Dataset
data_ts = tf.data.experimental.make_csv_dataset(
        'tmp.csv', 1, label_name='label', num_epochs=1)
data_ts.save('tmp.data')  # single shard, works!
</code></pre>
<p><strong>Multiple shards using <code>randint</code></strong> (saves single shard)</p>
<p>Trying to save to multiple shard using a random number, still only saves to a single shard, albeit with a random int in the file name.</p>
<pre><code># Try sharding, using random numbers.
def random_shard_function(features, label):
    return np.int64(np.random.randint(10))
data_ts.save('tmp2.data', shard_func=random_shard_function)

</code></pre>
<p><a href=""https://i.sstatic.net/9QlzVVHKm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9QlzVVHKm.png"" alt=""image of filesystem"" /></a></p>
<p><strong>Modulo shard</strong> (error)</p>
<p>Trying the solution from <a href=""https://stackoverflow.com/questions/77067982/how-to-use-the-shard-func-in-tensorflows-tf-data-dataset-save"">this question</a>.</p>
<pre><code>def modulo_shard_function(features, label):
    return x % 10
data_ts.save('tmp2.data', shard_func=modulo_shard_function)
</code></pre>
<blockquote>
<p>TypeError: unsupported operand type(s) for &amp;: 'collections.OrderedDict' and 'int'</p>
</blockquote>
<p><strong>Debugging</strong> - no idea how <code>shard_fun</code> works</p>
<p>If I print out the inputs, it seems that the shard func is only run once, and the tensors are <code>SymbolicTensors</code></p>
<pre><code>def debug_shard_function(features, label):
    for val in features.items():
        print(f'{val=}')
    print(f'{label=}')
    print(f'{type(val[1])}')
    return np.int64(10)
data_ts.save('tmp2.data', shard_func=debug_shard_function)
</code></pre>
<p>Output - still saves to a single shard:</p>
<pre><code>val=('', &lt;tf.Tensor 'args_0:0' shape=(None,) dtype=int32&gt;)
val=('f1', &lt;tf.Tensor 'args_3:0' shape=(None,) dtype=float32&gt;)
val=('f2', &lt;tf.Tensor 'args_4:0' shape=(None,) dtype=float32&gt;)
val=('f3', &lt;tf.Tensor 'args_5:0' shape=(None,) dtype=float32&gt;)
val=('c1', &lt;tf.Tensor 'args_1:0' shape=(None,) dtype=int32&gt;)
val=('c2', &lt;tf.Tensor 'args_2:0' shape=(None,) dtype=int32&gt;)
label=&lt;tf.Tensor 'args_6:0' shape=(None,) dtype=int32&gt;
&lt;class 'tensorflow.python.framework.ops.SymbolicTensor'&gt;
</code></pre>
","2","Question"
"79396860","","<p>I am trying to train a CNN model with an image dataset for medical image segmentation:</p>
<pre><code>  history = model.fit(x = train_dataset,
                        validation_data = val_dataset,
                        epochs= epochs,
                        steps_per_epoch= steps_per_epoch ,
                        callbacks = [iou_monitor,es,mc]
                        )
</code></pre>
<p>Such that the epochs =1 and batch_size = 4. The <code>train_dataset</code> consists of x, (y1,y2) where x is the input CT image ,and y1 and y2 are the corresponding segmentation masks ( multi-task model). Now, when I check  <code>train_dataset</code> from the file system manually I found that the total number of samples are 948. Also, when I run the following code:</p>
<pre><code># Initialize sample counter
total_samples = 0

# Iterate over dataset to count samples
for batch in train_dataset:
    x_batch, (y1_batch, y2_batch) = batch  # Unpack batch
    batch_size = tf.shape(x_batch)[0].numpy()  # Get batch size from X
    total_samples += batch_size  # Accumulate count
print(f&quot;Total samples in dataset: {total_samples}&quot;)
</code></pre>
<p>I get:</p>
<pre><code>Total samples in dataset: 948 
</code></pre>
<p>Which confirms my finding. This means that the steps_per_epoch is <code>num_of_samples_in_train_dataset / batch_size = 948/4 = 237</code>. Now, when I trian the model I get:</p>
<pre><code>236/237 ━━━━━━━━━━━━━━━━━━━━ 0s 388ms/step - loss: 0.6321 - mask_1_output_loss: 0.1476 - mask_2_output_loss: 0.4845
/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.
  self.gen.throw(typ, value, traceback)
</code></pre>
<p>My data pipeline is defined as follows:</p>
<pre><code># Function to read the input image
def read_image(image_path):
    try:
        image_path = image_path.decode()
        x = plt.imread(image_path)
        x = x.astype(np.float32)
        # Normalize x 
        x = x / 255.0
        if x.shape != (512, 512, 3):
            raise ValueError(f&quot;Invalid image dimensions for x: {x.shape}, expected (512, 512, 3)&quot;)
        #resize the image to 256,256,3
        x = cv2.resize(x, None, fx = img_size / original_size, fy = img_size / original_size, interpolation=cv2.INTER_AREA) .astype('float32')
        # Add channel dimension as the first dimension
        #x = np.transpose(x, (2, 0, 1))
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        return [x]
    except Exception as e:
        print(f&quot;Error reading image: {image_path}, Error: {e}&quot;)
        return None

# Function to read the mask image
def read_mask(mask_path):
    try:
        # The mask is in the npy format
        mask_path= mask_path.decode()
        y = np.load(mask_path)
        y = y.astype(np.float32)
        if y.shape != (512, 512, 2):
            raise ValueError(f&quot;Invalid original image dimensions: {y.shape}, expected (512, 512, 2)&quot;)
                # Check if the mask is binary

        # resize the image to 256,256,2
        y = cv2.resize(y, None, fx = img_size / original_size, fy = img_size / original_size, interpolation=cv2.INTER_NEAREST).astype('float32')
        #print(&quot;Shape after resize : &quot;,y.shape)
        # Add channel dimension as the first dimension
        y = np.transpose(y, (2, 0, 1))
      
       
        
        y1,y2 = y[0,:,:] , y[1,:,:]
        

        y1 = np.expand_dims(y1, axis=-1)  # Converts (img_size, img_size) to ( img_size, img_size,1)
        y2 = np.expand_dims(y2, axis=-1)

        y1 = tf.convert_to_tensor(y1, dtype=tf.float32)
        y2 = tf.convert_to_tensor(y2, dtype=tf.float32)    
        return [y1,y2]
    except Exception as e:
        print(f&quot;Error reading mask: {mask_path}, Error: {e}&quot;)
        return None

# Data Augmentation 
def augment_image(image,mask1,mask2):
        
    # Flip and rotate the image and mask 50% of the time
    if tf.random.uniform(()) &gt; 0.5:
        image = tf.image.flip_left_right(image)
        mask1 = tf.image.flip_left_right(mask1)
        mask2 = tf.image.flip_left_right(mask2)
        #image_channels_last_shape_2 = image_channels_last.shape
        #mask_channels_last_shape_2 = mask_channels_last.shape

    if tf.random.uniform(()) &gt; 0.5:
    
        image = tf.image.flip_up_down(image)
        mask1 = tf.image.flip_up_down(mask1)
        mask2 = tf.image.flip_up_down(mask2)
        #image_channels_last_shape_3 = image_channels_last.shape
        #mask_channels_last_shape_3 = mask_channels_last.shape

    if tf.random.uniform(()) &gt; 0.5:
    
        # Randomly select how many 90-degree rotations to apply (0, 1, 2, or 3)
        rotation_count = tf.random.uniform((), minval=0, maxval=4, dtype=tf.int32)

        # Apply the same rotation to the image and masks
        image = tf.image.rot90(image, k=rotation_count)
        mask1 = tf.image.rot90(mask1, k=rotation_count)
        mask2 = tf.image.rot90(mask2, k=rotation_count)

    
 
    return image, mask1, mask2
    
# Preprocessing function
def preprocess(x, y ,data_augmentation = False):
   
        #x = x.decode()
        #y = y.decode()
        output = tf.numpy_function(read_image, [x], [ tf.float32]) 
        x= output [0]
        output = tf.numpy_function(read_mask, [y], [ tf.float32, tf.float32]) 
        y1, y2  = output
        # convert the input image to grayscale
        #x=tf.image.rgb_to_grayscale(x)
        # Standardize each channel of the input image
        #x= standardize_image(x)
        
        # Apply data augmentation
        if data_augmentation:
 
            x,y1,y2 = augment_image(x,y1,y2)

        # Convert  to channels_first format 
        x = tf.transpose(x, perm=[2, 0, 1])  # [H, W, C] -&gt; [C, H, W]
        y1 = tf.transpose(y1, perm=[2, 0, 1])  # [H, W, C] -&gt; [C, H, W]
        y2 = tf.transpose(y2, perm=[2, 0, 1])  # [H, W, C] -&gt; [C, H, W]
        
        
        x.set_shape((3,img_size, img_size))
        y1.set_shape((1,img_size, img_size))
        y2.set_shape((1,img_size, img_size))
       

        return x, (y1,y2)



# Create the TensorFlow dataset
def tf_dataset(x, y, batch_size=32, epochs=30, data_augmentation = False):
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.shuffle(buffer_size=100)
    dataset = dataset.map(lambda a, b: preprocess(a, b, data_augmentation), num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.repeat(epochs).batch(batch_size)
    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
    return dataset
</code></pre>
<p>Also, I tried to turn off the data augmentation, but still the I get the same warning. I don't know what to do now.</p>
","0","Question"
"79396894","","<p>Currently working on a classifier using PyWavelets, here is my calculation block:</p>
<pre><code>class WaveletLayer(nn.Module):
    def __init__(self):
        super(WaveletLayer, self).__init__()

    def forward(self, x):
        def wavelet_transform(img):
            coeffs = pywt.dwt2(img.cpu().numpy(), &quot;haar&quot;)
            LL, (LH, HL, HH) = coeffs
            return (
                torch.from_numpy(LL).to(img.device),
                torch.from_numpy(LH).to(img.device),
                torch.from_numpy(HL).to(img.device),
                torch.from_numpy(HH).to(img.device),
            )

        # Apply wavelet transform to each channel separately
        LL, LH, HL, HH = zip(
            *[wavelet_transform(x[:, i : i + 1]) for i in range(x.shape[1])]
        )

        # Concatenate the results
        LL = torch.cat(LL, dim=1)
        LH = torch.cat(LH, dim=1)
        HL = torch.cat(HL, dim=1)
        HH = torch.cat(HH, dim=1)

        return torch.cat([LL, LH, HL, HH], dim=1)

</code></pre>
<p>The output from this module goes to a resnet block for learning, while doing this I find my CPU clogged and thus slowing down my training process</p>
<p>I am trying to use the GPUs for these calculations.</p>
","2","Question"
"79399452","","<p>I tried to deploy Whisper on Azure ML.
I am using the Whipser-openAI-v3 model for deployment.
The endpoint creation successes but deployment failed with the error <code>ResouceOperationFailed</code>
and so the creation of deployment failed.</p>
<p>I think the problem is outdated packages required by the model to be deployed but I don't know how to degrade them from Azure ML.</p>
<p>This is the log:</p>
<pre><code>Liveness Probe: GET   127.0.0.1:31311/
Score:          POST  127.0.0.1:31311/score

2025-01-30 00:02:33,483 W [379] azmlinfsrv - Found extra keys in the config file that are not supported by the server.
Extra keys = ['AZUREML_ENTRY_SCRIPT', 'AZUREML_MODEL_DIR', 'HOSTNAME']
2025-01-30 00:02:33,755 W [379] azmlinfsrv - AML_FLASK_ONE_COMPATIBILITY is set. However, compatibility patch for Flask 1 has failed. This is only a problem if you use @rawhttp and relies on deprecated methods such as has_key().
Traceback (most recent call last):
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/azureml_inference_server_http/server/create_app.py&quot;, line 58, in &lt;module&gt;
    patch_flask()
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/azureml_inference_server_http/server/create_app.py&quot;, line 33, in patch_flask
    patch_werkzeug = LooseVersion(werkzeug.__version__) &gt;= LooseVersion(&quot;2.1&quot;)
AttributeError: module 'werkzeug' has no attribute '__version__'

Initializing logger
2025-01-30 00:02:33,757 I [379] azmlinfsrv - Starting up app insights client
WARNING:entry_module:No signature information provided for model. If no sample information was provided with the model the deployment's swagger will not include input and output schema and typing information.For more information, please see: https://aka.ms/aml-mlflow-deploy.
2025/01/30 00:02:35 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:
 - scikit-learn (current: 1.2.2, required: scikit-learn&lt;=1.1.3)
To fix the mismatches, call mlflow.pyfunc.get_model_dependencies(model_uri) to fetch the model's environment and install dependencies using the resulting environment file.
Traceback (most recent call last):
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/azureml/evaluate/mlflow/__init__.py&quot;, line 67, in &lt;module&gt;
    import mlflow.gluon as gluon
ModuleNotFoundError: No module named 'mlflow.gluon'
2025-01-30 00:02:38,617 E [379] azmlinfsrv - Traceback (most recent call last):
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/azureml_inference_server_http/server/user_script.py&quot;, line 77, in load_script
    main_module_spec.loader.exec_module(user_module)
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 883, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed
  File &quot;/var/mlflow_resources/mlflow_score_script.py&quot;, line 378, in &lt;module&gt;
    model = load_model(model_path)
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/mlflow/tracing/provider.py&quot;, line 383, in wrapper
    is_func_called, result = True, f(*args, **kwargs)
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py&quot;, line 1120, in load_model
    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1050, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 883, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/azureml/evaluate/mlflow/hftransformers/__init__.py&quot;, line 30, in &lt;module&gt;
    from azureml.evaluate.mlflow import pyfunc, aml
ImportError: cannot import name 'pyfunc' from 'azureml.evaluate.mlflow' (/opt/miniconda/envs/userenv/lib/python3.10/site-packages/azureml/evaluate/mlflow/__init__.py)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/azureml_inference_server_http/server/aml_blueprint.py&quot;, line 91, in setup
    self.user_script.load_script(config.app_root)
  File &quot;/opt/miniconda/envs/userenv/lib/python3.10/site-packages/azureml_inference_server_http/server/user_script.py&quot;, line 79, in load_script
    raise UserScriptImportException(ex) from ex
azureml_inference_server_http.server.user_script.UserScriptImportException: Failed to import user script because it raised an unhandled exception

2025-01-30 00:02:38,617 I [379] gunicorn.error - Worker exiting (pid: 379)
2025-01-30 00:02:39,347 E [9] gunicorn.error - Worker (pid:379) exited with code 3
2025-01-30 00:02:39,348 E [9] gunicorn.error - Shutting down: Master
2025-01-30 00:02:39,348 E [9] gunicorn.error - Reason: Worker failed to boot.
Azure ML Inferencing HTTP server v1.3.4

Server Settings
---------------
Entry Script Name: /var/mlflow_resources/mlflow_score_script.py
Model Directory: /var/azureml-app/azureml-models/openai-whisper-large-v3/5
Config File: None
Worker Count: 1
Worker Timeout (seconds): 300
Server Port: 31311
Health Port: 31311
Application Insights Enabled: false
Application Insights Key: None
Inferencing HTTP server version: azmlinfsrv/1.3.4
CORS for the specified origins: None
Create dedicated endpoint for health: None
</code></pre>
","0","Question"
"79400379","","<p>I am looking for image processing tools in python to get the coordinates from the Blade Tips of a Wind Turbine, in this case a small model of one. The blades already get segmented by a yoloV8 Segmantation Model, and now I want to use that image to get the xy coordinates of the tips. Example image:
<a href=""https://i.sstatic.net/trBo33Jy.png"" rel=""nofollow noreferrer"">masked wings of a wind energy turbine</a>.
Can someone recommend some ideas how I could go about this? The rotor can be rotated, so the three tips could be anywhere on an ellipse.</p>
<p>I already tried training a yolo-pose model for keypoint detection, but it didn't give precise enough results. I will use these coordinates to calculate the excentricity of the rotor disk, so the points need to be somewhat precise.</p>
","-2","Question"
"79406524","","<p>I build a confusion matrix according to the code below:</p>
<pre><code>conf_matrix = confusion_matrix(y_test, y_test_predictions)
 
print(conf_matrix)

[[122  27]
 [ 40  42]]
</code></pre>
<p>Observe that the values ​​for it are all armed in an array. I want to plot these values ​​right now with this information using matplotlib and the seaborn library. For this reason, use or follow the code trecho.</p>
<pre><code>plt.figure(figsize=(3, 3), dpi=300)
# Scale up the size of all text
sns.set(font_scale = 1.1)
 
ax = sns.heatmap(conf_matrix, annot=True, fmt='d', )
 
# set x-axis label and ticks. 
ax.set_xlabel(&quot;Predicted Diagnosis&quot;, fontsize=14, labelpad=20)
ax.xaxis.set_ticklabels(['Negative', 'Positive'])
 
# set y-axis label and ticks
ax.set_ylabel(&quot;Actual Diagnosis&quot;, fontsize=14, labelpad=20)
ax.yaxis.set_ticklabels(['Negative', 'Positive'])
 
# set plot title
ax.set_title(&quot;Confusion Matrix for the Diabetes Detection Model&quot;, fontsize=14, pad=20)
 
plt.show()
</code></pre>
<p>To use this code, which should plot the confusing matrix values ​​in each cell, numbers 40 and 42 will not appear (second matrix line). Have you ever passed by someone else?</p>
<p>I'm used the jupyter notebook 7.0.8, python 3.11.7, matplotlib 3.8.0 and seaborn 0.12.2.</p>
","1","Question"
"79406743","","<p>I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns &quot;UNK&quot;. Here is my code:</p>
<pre><code>from quickumls import QuickUMLS

quickumls_fp = &quot;med7_en/lib/python3.10/site-packages/quickumls&quot;
matcher = QuickUMLS(quickumls_fp)

def extract_umls_cuis(text):
    &quot;&quot;&quot;Extract UMLS CUIs using QuickUMLS.&quot;&quot;&quot;
    if isinstance(text, str):
        matches = matcher.match(text)
        if matches:
            return [match['cui'] for match in matches[0]]
        else:
            return &quot;UNK&quot;

sample_text = &quot;diclofenac.&quot;
print(extract_umls_cuis(sample_text))
</code></pre>
<p>What I Have Checked:</p>
<ul>
<li>QuickUMLS Installation: I have installed QuickUMLS correctly.</li>
<li>UMLS Data Availability: I have set the correct path to QuickUMLS.</li>
<li>Different Input Words: I tried various medical terms, but all return &quot;UNK&quot;.</li>
</ul>
","0","Question"
"79407078","","<p>I am trying to understand the GaussianProcessRegressor object in scikit-learn alas, unsuccessfully.</p>
<p>Considering the example in the documentation <a href=""https://i.sstatic.net/UmisFKaE.png"" rel=""nofollow noreferrer"">Example with noisy targets</a>, which I copy below for convenience (with the minor change of using a <code>ConstantKernel</code> instead of multiplication by a constant in the kernel definition)</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)
y = np.squeeze(X * np.sin(X))

#############
#############
noise_std = 0.75


import matplotlib.pyplot as plt

plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;True generative process&quot;)
rng = np.random.RandomState(1)

training_indices = rng.choice(np.arange(y.size), size=6, replace=False)
X_train, y_train = X[training_indices], y[training_indices]


noise_std = 0.75
y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel,ConstantKernel

# kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel()
kernel =  ConstantKernel(constant_value=1)*RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) 
gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9)

gaussian_process.fit(X_train, y_train_noisy)
gaussian_process.kernel_
mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)
plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.errorbar(
    X_train,
    y_train_noisy,
    noise_std,
    linestyle=&quot;None&quot;,
    color=&quot;tab:blue&quot;,
    marker=&quot;.&quot;,
    markersize=10,
    label=&quot;Observations&quot;,
)
plt.plot(X, mean_prediction, label=&quot;Mean prediction&quot;)
plt.fill_between(
    X.ravel(),
    mean_prediction - 1.96 * std_prediction,
    mean_prediction + 1.96 * std_prediction,
    color=&quot;tab:orange&quot;,
    alpha=0.5,
    label=r&quot;95% confidence interval&quot;,
)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;Gaussian process regression on a noisy dataset&quot;)
</code></pre>
<p>I get these results</p>
<p><a href=""https://i.sstatic.net/UmisFKaE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UmisFKaE.png"" alt=""enter image description here"" /></a></p>
<p>Now, I would like to get the same results by using a  kernel with 'fixed'parameters (for some other purpose which is irrelevant for the question).
So, I get the optimised hyperparameters of the kernel above,</p>
<p><code>gaussian_process.kernel_.get_params()</code></p>
<p>outputting</p>
<pre><code>{'k1': 4.28**2,
 'k2': RBF(length_scale=1.1),
 'k1__constant_value': 18.30421069841903,
 'k1__constant_value_bounds': (1e-05, 100000.0),
 'k2__length_scale': 1.1043558649730463,
 'k2__length_scale_bounds': (0.01, 100.0)}
</code></pre>
<p>So, I modify the previous kernel &amp; gaussian process definition</p>
<pre class=""lang-py prettyprint-override""><code>kernel_fixed = ConstantKernel(constant_value=18.30, constant_value_bounds='fixed') *RBF(length_scale=1.1043, length_scale_bounds='fixed') 
gaussian_process_fixed = GaussianProcessRegressor(kernel=kernel_fixed, alpha=noise_std**2, n_restarts_optimizer=9)
gaussian_process_fixed.fit(X,y)
mean_prediction, std_prediction = gaussian_process_fixed.predict(X, return_std=True)

plt.plot(X, y, label=r&quot;$f(x) = x \sin(x)$&quot;, linestyle=&quot;dotted&quot;)
plt.errorbar(
    X_train,
    y_train_noisy,
    noise_std,
    linestyle=&quot;None&quot;,
    color=&quot;tab:blue&quot;,
    marker=&quot;.&quot;,
    markersize=10,
    label=&quot;Observations&quot;,
)
plt.plot(X, mean_prediction, label=&quot;Mean prediction&quot;)
plt.fill_between(
    X.ravel(),
    mean_prediction - 1.96 * std_prediction,
    mean_prediction + 1.96 * std_prediction,
    color=&quot;tab:orange&quot;,
    alpha=0.5,
    label=r&quot;95% confidence interval&quot;,
)
plt.legend()
plt.xlabel(&quot;$x$&quot;)
plt.ylabel(&quot;$f(x)$&quot;)
_ = plt.title(&quot;Gaussian process regression on a noisy dataset&quot;)

</code></pre>
<p>but the results are very different, and yet kernel's parameters are as I fixed them, very close to the optimised ones</p>
<pre><code>{'k1': 4.28**2,
 'k2': RBF(length_scale=1.1),
 'k1__constant_value': 18.3,
 'k1__constant_value_bounds': 'fixed',
 'k2__length_scale': 1.1043,
 'k2__length_scale_bounds': 'fixed'}
</code></pre>
<p>What is that I am missing??
<a href=""https://i.sstatic.net/DafvvAA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DafvvAA4.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79409149","","<p>I was doing the PyTorch Deep Learning course from FreeCodeCamp and the doubt is:</p>
<pre><code>weight = 0.7
bias = 0.3
start = 0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y=weight*X + bias
X[:10], y[:10]
train_split=int(0.8*len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test=X[train_split:], y[train_split:]
</code></pre>
<p>Why the <strong>unsqueeze function</strong> is used to <strong>make the tensor of size [50, 1] and not [50]</strong>? The mentor was telling that it will cause error but I don't know why the error is happening?</p>
<p>Can you answer this question using maths and as well as basic fundamentals without maths as well ?</p>
<p>After trying to train the model I am getting this error:</p>
<pre><code>class LinearRegressionModelv2(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear_layer = nn.Linear(in_features=1, out_features=1)

  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    return self.linear_layer(x)

torch.manual_seed(42)
model_v2 = LinearRegressionModelv2()
</code></pre>
<p><code>y_prediction = model_v2(X_train)</code></p>
<p>IndexError: Dimension out of range (expected to be in range of [-1, 0], but got -2)</p>
","0","Question"
"79409259","","<p>In the configuration management library <a href=""https://hydra.cc/"" rel=""nofollow noreferrer"">Hydra</a>, it is possible to only partially instantiate classes defined in configuration using the <a href=""https://hydra.cc/docs/1.1/advanced/instantiate_objects/overview/#partial-instantiation-for-hydra-version--112"" rel=""nofollow noreferrer""><code>_partial_</code> keyword</a>. The library explains that this results in a <a href=""https://docs.python.org/3/library/functools.html#functools.partial"" rel=""nofollow noreferrer""><code>functools.partial</code></a>. I wonder how this interacts with seeding. E.g. with</p>
<ul>
<li><a href=""https://pytorch.org/docs/stable/notes/randomness.html"" rel=""nofollow noreferrer"">pytorch <code>torch.manual_seed()</code></a></li>
<li><a href=""https://pytorch-lightning.readthedocs.io/en/1.7.7/api/pytorch_lightning.utilities.seed.html#pytorch_lightning.utilities.seed.seed_everything"" rel=""nofollow noreferrer"">lightnings <code>seed_everything</code></a></li>
<li>etc.</li>
</ul>
<p>My reasoning is, that if I use the <code>_partial_</code> keyword while specifying <em>all</em> parameters for <code>__init__</code>, then I would essentially obtain a factory which could be called after specifying the seed to do multiple runs. But this assumes that <code>_partial_</code> does not bake the seed in already. To my understanding that should not be the case. Is that correct?</p>
","2","Question"
"79410458","","<p>I am using sklearn to run a random forest. I am setting the seed for the random forest, as well as splitting the data for cross validation. When I re-run the code consecutive times, it gives me the same result. However, re-running the same code after a month, I got slightly different feature importances. In some other similar analyses, the accuracy metrics are different too. The data has not been changed. I am running on Google Colab.</p>
<p>Here is my code:</p>
<pre><code># Configuration
file_path = '/content/drive/My Drive/dataset.csv'
columns_to_keep = [
    'target_column', 'feature_a', 'feature_b', 'feature_c', 'feature_d', 'feature_e',
    'feature_f', 'feature_g', 'feature_h', 'feature_i', 'feature_j', 'feature_k', 'feature_l',
    'feature_m', 'feature_n', 'feature_o', 'feature_p', 'feature_q', 'feature_r', 'feature_s',
    'feature_t', 'feature_u', 'feature_v', 'feature_w', 'feature_x', 'feature_y', 'feature_z',
    'feature_aa', 'feature_ab', 'feature_ac', 'feature_ad', 'feature_ae', 'feature_af', 'feature_ag',
    'feature_ah', 'feature_ai', 'feature_aj', 'feature_ak', 'feature_al', 'feature_am', 'feature_an'
]

df = pd.read_csv(file_path, usecols=columns_to_keep)

categorical_columns = ['feature_ak', 'feature_al', 'feature_am', 'feature_an', 'feature_ao']
one_hot_columns = ['feature_al', 'feature_ak']

df = df.dropna()

# One-hot encode the specified column
le = LabelEncoder()
for col in one_hot_columns:
    df[col] = le.fit_transform(df[col])

# Convert specified columns to categorical
for col in categorical_columns:
  df[col] = df[col].astype('category')

# Split into features and target
X = df.drop(columns=['target_column'])
y = df['target_column']

# Initialize the RandomForestClassifier model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Initialize k-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Store results
feature_importances_list = []
all_y_true = []
all_y_pred = []

# Perform k-fold cross-validation
for fold_num, (train_index, test_index) in enumerate(kf.split(X), start=1):
    # Split the data
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Train the Random Forest model
    rf_model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = rf_model.predict(X_test)

    # Collect all true and predicted labels
    all_y_true.extend(y_test)
    all_y_pred.extend(y_pred)

    # Get feature importances for this fold
    feature_importances_list.append(rf_model.feature_importances_)

    # Calculate and print the accuracy for this fold
    accuracy_fold = accuracy_score(y_test, y_pred)
    print(f&quot;Fold {fold_num} Accuracy: {accuracy_fold:.4f}&quot;)

# Calculate accuracy across all predictions
accuracy_cv = accuracy_score(all_y_true, all_y_pred)

# Generate a classification report
final_report = classification_report(all_y_true, all_y_pred, digits=3)

# Average feature importances across folds
average_importance = sum(feature_importances_list) / len(feature_importances_list)

# Create a DataFrame with feature names and their corresponding average importance
feature_names = X.columns
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': average_importance
}).sort_values(by='Importance', ascending=False)

# Print Results
print(f&quot;Overall Accuracy of Random Forest model with k-fold CV: {accuracy_cv:.4f}&quot;)

print(&quot;\nFinal Classification Report:&quot;)
print(final_report)

print(&quot;\nRandom Forest Feature Importances (averaged across folds):&quot;)
print(importance_df.head(20))
</code></pre>
","-1","Question"
"79411192","","<p>I have been trying to get an image segmentation model from huggingface (<a href=""https://huggingface.co/briaai/RMBG-2.0"" rel=""nofollow noreferrer"">RMBG-2.0</a>) to work for inference using ML.NET. After a lot of trial and error, I finally got the code to compile and produce an output but it is wildly different from the result i get from using the demo on huggingface.</p>
<p>The code:</p>
<pre><code>public static void RemoveGreenBackgroundAI2(string imagePath, string outputfile)
{
    string modelPath = Path.Combine( Application.StartupPath,&quot;ONNX&quot;,&quot;model.onnx&quot;); 
    MLContext mlContext = new MLContext();

    var imageData = new ImageInputData
    {
        Image = MLImage.CreateFromFile (imagePath)
    };


    var imageDataView = mlContext.Data.LoadFromEnumerable(new[] { imageData });
   
   var pipeline = mlContext.Transforms.ResizeImages(
                        outputColumnName: &quot;input&quot;,
                        imageWidth: 1024,
                        imageHeight: 1024,
                        inputColumnName: nameof(ImageInputData.Image))
                  .Append(mlContext.Transforms.ExtractPixels(
                        outputColumnName: &quot;out1&quot;,
                        inputColumnName: &quot;input&quot;,
                        interleavePixelColors: true,
                        scaleImage: 1f / 255f,
                        offsetImage: 0,
                        outputAsFloatArray: true))
                    .Append(mlContext.Transforms.CustomMapping&lt;CustomMappingInput, CustomMappingOutput&gt;( 
                       mapAction: (input, output) =&gt;
                        {
                            output.pixel_values = new float[input.out1.Length];
                            for (int i = 0; i &lt; input.out1.Length; i += 3)
                            {
                                // R
                                output.pixel_values[i] = (input.out1[i] - 0.485f) / 0.229f;

                                //G
                                output.pixel_values[i + 1] = (input.out1[i + 1] - 0.456f) / 0.224f;

                                //B
                                output.pixel_values[i + 2] = (input.out1[i + 2] - 0.406f) / 0.225f;
                            }
                        }, contractName: null))
                  .Append(mlContext.Transforms.ApplyOnnxModel(
                        modelFile: modelPath,
                        outputColumnNames: new[] { &quot;alphas&quot; },
                        inputColumnNames: new[] { &quot;pixel_values&quot; },
                        shapeDictionary: new Dictionary&lt;string, int[]&gt;
                        {
                            { &quot;pixel_values&quot;, new[] { 1, 3, 1024, 1024 } }

                        },
                        fallbackToCpu:true,
                        gpuDeviceId:null
                        ));

    
    var model = pipeline.Fit(imageDataView);
    var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ImageInputData, ModelOutput&gt;(model);
    var prediction = predictionEngine.Predict(imageData);
    ApplyMaskAndSaveImage(imagePath, prediction, outputfile);

}

public static void ApplyMaskAndSaveImage(string originalImagepath, ModelOutput prediction, string outputPath)
{
    int width = 1024;
    int height = 1024;
    float[] outputData = prediction.Output;

    Bitmap originalImage = (Bitmap)Bitmap.FromFile(originalImagepath);
    int originalWidth = originalImage.Width;
    int originalHeight = originalImage.Height;

    Bitmap resizedImage = new Bitmap(originalImage, new System.Drawing.Size(width, height));
    Bitmap outputImage = new Bitmap(width, height, PixelFormat.Format32bppArgb);

    for (int y = 0; y &lt; height; y++)
    {
        for (int x = 0; x &lt; width; x++)
        {
            float maskValue = outputData[y * width + x];
            float threshold = 0.5f;
            byte alpha = maskValue &gt;= threshold ? (byte)255 : (byte)0;
            Color pixelColor = resizedImage.GetPixel(x, y);
            Color newColor = Color.FromArgb(alpha, pixelColor.R, pixelColor.G, pixelColor.B);
            outputImage.SetPixel(x, y, newColor);
        }
    }      
    outputImage.Save(outputPath, ImageFormat.Png);
}

public class ModelOutput
{
    [ColumnName(&quot;alphas&quot;)]
    [VectorType(1, 1, 1024, 1024)]
    public float[] Output { get; set; }
}
public class ImageInputData
{
    [ColumnName(&quot;Image&quot;)]
    [ImageType(1024, 1024)]
    public MLImage Image { get; set; }
}
public class CustomMappingInput
{
    [VectorType(3, 1024, 1024)]
    public float[] out1 { get; set; }
}
public class CustomMappingOutput
{
    [VectorType(3, 1024, 1024)]
    public float[] pixel_values { get; set; } 
}
</code></pre>
<p>I know the code is far from optimal (<code>GetPixel()</code>and <code>SetPixel()</code>have to be replaced amongst other things), and that the aspect ratio of my result is wrong because I have not scaled the image back to the original dimensions. First I would like to get the background removal working correctly.</p>
<p>Any advice or idea of what I might be doind incorrectly?</p>
<p>BTW, the onnx file is available in the RMBG-2.0 link at the beginning. There is also a code snippet in python for using the model and that is why I am applying thosee transformations to the image in the pipeline.</p>
<p><a href=""https://i.sstatic.net/oTJSO60A.jpg"" rel=""nofollow noreferrer"">Input Image</a></p>
<p><a href=""https://i.sstatic.net/ZLouZcpm.png"" rel=""nofollow noreferrer"">Expected result</a></p>
<p><a href=""https://i.sstatic.net/pB1FKT6f.png"" rel=""nofollow noreferrer"">Result I am getting</a></p>
","0","Question"
"79411501","","<p>I am trying to use ResNet3D from <code>tensorflow-models</code> library but I am getting this weird error when trying to run the block</p>
<pre><code>!pip install tf-models-official==2.17.0
</code></pre>
<p>Tensorflow version is <code>2.18</code> on the Kaggle notebook.</p>
<p>After installing <code>tf-models-official</code></p>
<pre><code>from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling3D, Input
from tensorflow.keras.optimizers import AdamW
import tensorflow_models as tfm

def create_model():
    base_model = tfm.vision.backbones.ResNet3D(model_id = 50,
        temporal_strides= [3,3,3,3],
        temporal_kernel_sizes = [(5,5,5),(5,5,5,5),(5,5,5,5,5,5),(5,5,5)],
        input_specs=tf.keras.layers.InputSpec(shape=(None, None, IMG_SIZE, IMG_SIZE, 3))
    )
    
    # Unfreeze the base model layers
    base_model.trainable = True
    
    # Create the model
    inputs = Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
    x = base_model(inputs) # B,1,7,7,2048
    x = GlobalAveragePooling3D(data_format=&quot;channels_last&quot;, keepdims=False)(x)
    x = Dense(1024, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.3)(x)  # Add dropout to prevent overfitting
    outputs = Dense(NUM_CLASSES, activation='softmax')(x)
    
    model = Model(inputs, outputs)
    
    # Compile the model with class weights
    optimizer = AdamW(learning_rate=1e-4, weight_decay=1e-5)
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC()]
    )
    
    return model

# Create and display model
model = create_model()
model.summary()
</code></pre>
<p>When I run this, I get the error below:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-56-363271b4dda8&gt; in &lt;cell line: 39&gt;()
     37 
     38 # Create and display model
---&gt; 39 model = create_model()
     40 model.summary()

&lt;ipython-input-56-363271b4dda8&gt; in create_model()
     18     # Create the model
     19     inputs = Input(shape=(None, None, IMG_SIZE, IMG_SIZE, 3))
---&gt; 20     x = base_model(inputs) # B,1,7,7,2048

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py in __call__(self, *args, **kwargs)
    586             layout_map_lib._map_subclass_model_variable(self, self._layout_map)
    587 
--&gt; 588         return super().__call__(*args, **kwargs)

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/base_layer.py in __call__(self, *args, **kwargs)
   1101             training=training_mode,
   1102         ):
-&gt; 1103             input_spec.assert_input_compatibility(
   1104                 self.input_spec, inputs, self.name
   1105             )

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)
    300                             &quot;incompatible with the layer: &quot;
    301                             f&quot;expected shape={spec.shape}, &quot;
--&gt; 302                             f&quot;found shape={display_shape(x.shape)}&quot;
    303                         )
    304 

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py in display_shape(shape)
    305 
    306 def display_shape(shape):
--&gt; 307     return str(tuple(shape.as_list()))
    308 
    309 

AttributeError: 'tuple' object has no attribute 'as_list'
</code></pre>
<p>I have tried passing the input to the <code>shape</code> argument as a list, but still getting the same error.</p>
<p>The error is occurring with this</p>
<pre><code>!pip install tf-models-official==2.17.0

import tensorflow as tf

inputs = tf.keras.Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
print(inputs.shape.as_list())
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-39-6e88680ff7df&gt; in &lt;cell line: 2&gt;()
      1 inputs = tf.keras.Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
----&gt; 2 print(inputs.shape.as_list())

AttributeError: 'tuple' object has no attribute 'as_list'
</code></pre>
","1","Question"
"79413251","","<p>I am learning through PyTorch's seq2seq tutorial: <a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html</a></p>
<p>I have a question about the decoder</p>
<pre><code>class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)
        self.out = nn.Linear(hidden_size, output_size)

    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):
        batch_size = encoder_outputs.size(0)
        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)
        decoder_hidden = encoder_hidden
        decoder_outputs = []

        for i in range(MAX_LENGTH):
            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)
            decoder_outputs.append(decoder_output)

            if target_tensor is not None:
                # Teacher forcing: Feed the target as the next input
                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing
            else:
                # Without teacher forcing: use its own predictions as the next input
                _, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze(-1).detach()  # detach from history as input

        decoder_outputs = torch.cat(decoder_outputs, dim=1)
        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)
        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop

</code></pre>
<p>Why is it that <code>if target_tensor is not None</code>:</p>
<pre><code>decoder_input = target_tensor[:, i].unsqueeze(1)
</code></pre>
<p>but <code>if target_tensor is None</code>:</p>
<pre><code>_, topi = decoder_output.topk(1)
decoder_input = topi.squeeze(-1).detach()
</code></pre>
<p>specifically, isn't the shape of decoder_input different in both cases?</p>
<p>I feel like in the first case, the shape of decoder_input is a 2D tensor but 1D in the second case.</p>
","0","Question"
"79416292","","<p>I'm currently using MinMaxScaler() on my dataset. However, because my dataset is large I'm doing a first iteration pass in batches to compute the Min and Max Values for my Scaler. i'm using partial_fit() to help with this.</p>
<p>Anyway, for some of my features I do know their min and max values. Is there anyway I can explicity inform the scaler about these min and max values?</p>
","0","Question"
"79419018","","<p>I trained a YOLOv8 detection model with 3 classes, but the raw forward pass still shows a final detect output of (1, 7, 8400) instead of (1, 8, 8400).</p>
<p>What I’ve Done:
Checked my data.yaml:</p>
<pre class=""lang-yaml prettyprint-override""><code>train: path/to/train/images
val: path/to/val/images
nc: 3
names: ['glioma', 'meningioma', 'pituitary']
</code></pre>
<p>Confirmed nc: 3 is correct.
Trained from scratch with the command:</p>
<pre class=""lang-bash prettyprint-override""><code>yolo detect train \
    data=path/to/data.yaml \
    model=yolov8x \
    epochs=1000 \
    imgsz=640 \
    device=1 \
    patience=100
</code></pre>
<p>The training runs without error and completes successfully.
Installed the latest Ultralytics version (v8.3.72) to ensure no version issues:</p>
<pre class=""lang-bash prettyprint-override""><code>pip uninstall ultralytics
pip install ultralytics
</code></pre>
<p>Loaded the new best.pt directly:</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO
import torch

model = YOLO(r&quot;best.pt&quot;).model
model.eval()

dummy_input = torch.randn(1, 3, 640, 640)
with torch.no_grad():
    outputs = model(dummy_input)

for out in outputs:
    # Some outputs are lists; checking each element carefully
    if isinstance(out, torch.Tensor):
        print(out.shape)
    else:
        print(&quot;List output:&quot;, [o.shape for o in out if hasattr(o, 'shape')])
</code></pre>
<p>The console shows (1, 7, 8400) for the detection output.
Verified model metadata says nc=3 and model.names has 3 classes. However, the raw detect layer output is still 7 channels.</p>
<p>Observations:
If a YOLO detect layer is genuinely for 3 classes, it should output (5 + 3)=8 channels per anchor, not 7.
The mismatch (1, 7, 8400) typically indicates it’s still set for 2 classes despite nc=3.</p>
<p>Question / Request for Help:
Why is the raw detect head still (1, 7, 8400) even though I trained from scratch for 3 classes?
How can I ensure the detect layer is fully re-initialized to (5 + 3)=8 for 3-class detection?
I’ve tried deleting old .pt files, re-checking my data.yaml, reinstalling ultralytics, and confirming model.model.nc == 3. But the final detect layer continues to yield 7 channels instead of 8.</p>
<p>Any ideas on what might cause this persistent mismatch?</p>
","1","Question"
"79420818","","<p>I'm running into an issue when implementing a training loop that uses a <code>tf.data.Dataset</code> as input to a Keras model. My dataset has an element spec of the following format:</p>
<pre class=""lang-py prettyprint-override""><code>({'data': TensorSpec(shape=(15000, 1), dtype=tf.float32), 'index': TensorSpec(shape=(2,), dtype=tf.int64)}, TensorSpec(shape=(1,), dtype=tf.int32))
</code></pre>
<p>So, basically, each sample is structured as tuple <code>(x, y)</code>, in which <code>x</code> has the structure of a dict containing two tensors, one of data with shape <code>(15000, 1)</code>, and the other an index of shape <code>(2,)</code> (the index is not used during training), and <code>y</code> is a single label.</p>
<p>The <code>tf.data.Dataset</code> is created using <code>dataset = tf.data.Dataset.from_tensor_slices((X, y))</code>, where <code>X</code> is a dict of two keys:</p>
<ul>
<li><code>data</code>: an np array of shape <code>(200k, 1500, 1)</code>, <code>index</code> with</li>
<li><code>index</code>: an np array of shape <code>(200k, 2)</code></li>
</ul>
<p>and <code>y</code> is a single array of shape <code>(200k, 1)</code></p>
<p>My dataset has about 200k training samples (after running undersampling) and 200k validation samples.</p>
<p>Right after calling <code>tf.data.Dataset.from_tensor_slices</code> I noticed a spike in GPU memory usage, with about 16GB being occupied after creating the training <code>tf.Dataset</code>, and 16GB more after creating the validation <code>tf.Dataset</code>.</p>
<p>After creating of the <code>tf.Dataset</code>, I run a few operations (e.g. shuffle, batching, and prefetching), and call <code>model.fit</code>. My model has about 500k trainable parameters.</p>
<p>The issue I'm running into is <em>after</em> fitting the model. I need to run inference on some additional data, so I create a new <code>tf.Dataset</code> with this data, again using <code>tf.Dataset.from_tensor_slices</code>. However, I noticed the training and validation <code>tf.Dataset</code> still reside in GPU memory, which causes my script to break with an out of memory problem for the new <code>tf.Dataset</code> I want to run inference on.</p>
<p>I tried calling <code>del</code> on the two <code>tf.Dataset</code>, and subsequently calling <code>gc.collect()</code>, but I believe that will only clear RAM, not GPU memory. Also, I tried disabling some operations I apply, such as <code>prefetch</code>, and also playing with the batch size, but none of that worked. I also tried calling <code>keras.backend.clear_session()</code>, but it also did not work to clear GPU memory. I also tried importing <code>cuda</code> from <code>numba</code>, but due to my install I cannot use it to clear memory. Is there any way for me to clear the <code>tf.data.Dataset</code> from GPU memory?</p>
<p>Minimum reproducible example below</p>
<p>Setup</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf

from itertools import product

# Setting tensorflow memory growth for GPU
gpus = tf.config.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
</code></pre>
<p>Create dummy data with similar size as my actual data (types are the same as the actual data):</p>
<pre><code>train_index = np.array(list(product(np.arange(1000), np.arange(200)))).astype(np.int32)
train_data = np.random.rand(200000, 15000).astype(np.float32)
train_y = np.random.randint(0, 2, size=(200000, 1)).astype(np.int32)

val_index = np.array(list(product(np.arange(1000), np.arange(200)))).astype(np.int32)
val_data = np.random.rand(200000, 15000).astype(np.float32)
val_y = np.random.randint(0, 2, size=(200000, 1)).astype(np.int32)
</code></pre>
<p>This is the nvidia-smi output at this point:
<a href=""https://i.sstatic.net/pVKioJfg.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/pVKioJfg.png"" alt=""nvidia-smi before calling the first tf.data.Dataset"" /></a></p>
<p>Creating the training <code>tf.data.Dataset</code>, with as batch size of 256</p>
<pre class=""lang-py prettyprint-override""><code>train_X = {'data': train_data, 'index':train_index}
train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))
train_dataset = train_dataset.batch(256)
</code></pre>
<p>This is the nvidia-smi output after the <code>tf.data.Dataset</code> creation:
<a href=""https://i.sstatic.net/IYzxOcFW.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/IYzxOcFW.png"" alt=""nvidia-smi after calling the first tf.data.Dataset"" /></a></p>
<p>Creating the validation <code>tf.data.Dataset</code>, with as batch size of 256</p>
<pre class=""lang-py prettyprint-override""><code>val_X = {'data': val_data, 'index':val_index}
val_dataset = tf.data.Dataset.from_tensor_slices((val_X, val_y))
val_dataset = val_dataset.batch(256)
</code></pre>
<p>This is the nvidia-smi output after the second <code>tf.data.Dataset</code> creation:
<a href=""https://i.sstatic.net/19oNOJx3.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/19oNOJx3.png"" alt=""nvidia-smi after calling the second tf.data.Dataset"" /></a></p>
<p>So GPU usage grows when creating each <code>tf.data.Dataset</code>. Since after running <code>model.fit</code> I need to create a new <code>tf.data.Dataset</code> of similar size, I end up running out of memory. Is there any way to clear this data from GPU memory?</p>
","9","Question"
"79424312","","<p>I am using this code from huggingface:</p>
<p>This code is directly pasted from the <a href=""https://huggingface.co/deepseek-ai/DeepSeek-R1"" rel=""nofollow noreferrer"">HuggingFace website's page on deepseek</a> and is supposed to be plug-and-play code:</p>
<blockquote>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

messages = [
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},
]
pipe = pipeline(&quot;text-generation&quot;, model=&quot;deepseek-ai/DeepSeek-R1&quot;, &gt;trust_remote_code=True)
pipe(messages)
</code></pre>
</blockquote>
<p>But I'm unable to load the model. When I do, I get this issue:</p>
<pre><code>File &quot;&lt;...&gt;/site-packages/transformers/quantizers/auto.py&quot;, line 97, in from_dict

raise ValueError(

ValueError: Unknown quantization type, got fp8 - supported types are: 
['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 
'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet']
</code></pre>
<p>I tried different code:</p>
<pre><code>import torch
generate_text = pipeline(model=&quot;deepseek-ai/DeepSeek-R1&quot;,torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=&quot;auto&quot;)
generate_text(messages)
</code></pre>
<p>This gives the following error:</p>
<blockquote>
<p>raise ValueError( ValueError: Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq']</p>
</blockquote>
<p>What can I do?</p>
","6","Question"
"79430117","","<p><a href=""https://i.sstatic.net/WxCaIuew.jpg"" rel=""nofollow noreferrer"">1]Original page of Doc1 contain 4 tables</a></p>
<p><a href=""https://i.sstatic.net/A2OHOQf8.jpg"" rel=""nofollow noreferrer"">1]Output .html page of Doc1 not detecting table properly,sometimes extracting text from table as plain text</a></p>
<p><a href=""https://i.sstatic.net/QoEKcInZ.jpg"" rel=""nofollow noreferrer"">2]Original page of Doc2 contain images in table cell</a>  <a href=""https://i.sstatic.net/FH1yFHVo.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FH1yFHVo.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/9nYpzo0K.jpg"" rel=""nofollow noreferrer"">2]Output .html page of Doc2,Complex tables with embedded images not properly handle &amp; sometimes for simple table also structure is not getting proper in .html file</a> <a href=""https://i.sstatic.net/FeQdv0Vo.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FeQdv0Vo.jpg"" alt=""enter image description here"" /></a></p>
<p>I am extracting content from PDF files and converting it into HTML format while maintaining the original structure and formatting. I am using the <strong>Docling library</strong> for this purpose.</p>
<p>I am getting the output with the same flow of content as in the original PDF file in the <code>.html</code> file. However, I am facing issues in preserving table structures in the output HTML file.</p>
<h4><strong>What I Expect to Happen:</strong></h4>
<ul>
<li><p>Extract tables from the PDF with correct row and column structure.</p>
</li>
<li><p>Retain the table layout in <code>&lt;table&gt;</code>, <code>&lt;tr&gt;</code>, and <code>&lt;td&gt;</code> HTML tags.</p>
</li>
<li><p>Maintain the original formatting, alignment, and cell content as seen in the PDF.</p>
</li>
</ul>
<h4><strong>What Actually Happens:</strong></h4>
<ul>
<li><p>Tables are not detected correctly—table data appears inside <code>&lt;p&gt;</code> tags instead of proper <code>&lt;table&gt;</code> structure.</p>
</li>
<li><p>Misaligned tables—cell content is split incorrectly, and <strong>images inside table cells appear outside</strong> the table.</p>
</li>
<li><p>Complex tables with embedded images are not preserved properly.</p>
</li>
</ul>
<h4><strong>Code Used:</strong></h4>
<pre class=""lang-py prettyprint-override""><code>from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.base_models import InputFormat
from docling_core.types.doc import ImageRefMode
from pathlib import Path
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)  # Corrected: _name_ -&gt; __name__

# Configure image settings
IMAGE_RESOLUTION_SCALE = 2.0

# Path to your PDF file
source = Path(r&quot;C:\Users\Downloads\Journal.pdf&quot;)
output_path = Path(r&quot;C:\Users\Desktop\output20.html&quot;)

# Configure pipeline options for image handling
pipeline_options = PdfPipelineOptions()
pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE
pipeline_options.generate_page_images = True
pipeline_options.generate_picture_images = True

# Create converter instance with image options
converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)

# Convert PDF to document
result = converter.convert(source)

# Save HTML with embedded images
result.document.save_as_html(output_path, image_mode=ImageRefMode.EMBEDDED)

log.info(f&quot;HTML file with embedded images created at: {output_path}&quot;)
</code></pre>
<h4><strong>What I Have Tried So Far:</strong></h4>
<ol>
<li><p>Checked the extracted HTML output—tables are missing or displayed incorrectly.</p>
</li>
<li><p>Tried different pipeline options in <code>PdfPipelineOptions()</code> to see if they affect table extraction.</p>
</li>
<li><p>Compared output with Document Intelligence library—it extracts headers/footers better but still struggles with complex tables.</p>
</li>
</ol>
<h4><strong>Key Challenges:</strong></h4>
<ul>
<li><p>Tables are not preserved in <code>&lt;table&gt;</code> tags.</p>
</li>
<li><p>Cell splitting issues—data is misaligned or broken into multiple parts.</p>
</li>
<li><p>Images inside tables are misplaced (appearing above/below instead of inside table cells).</p>
</li>
</ul>
<h4><strong>Additional Observation:</strong></h4>
<ul>
<li>The flow of content in the extracted HTML file matches the original PDF file**, but the **table structures are not correctly formatted.</li>
</ul>
<h4><strong>Question:</strong></h4>
<p>How can I properly extract tables from a PDF and convert them into structured HTML (<code>&lt;table&gt;</code>, <code>&lt;tr&gt;</code>, <code>&lt;td&gt;</code>) while maintaining the original layout and formatting using the Docling library?</p>
","0","Question"
"79433458","","<p>Im trying to find a way to train a lightgbm model forcing to have some features to be in the splits, i.e.: &quot;to be in the feature importance&quot;, then the predictions are afected by these variables.</p>
<p>Here is an example of a the modeling code with an usless variable as it is constant, but the idea is that there could be an important variable from business perspective that is not in the feature</p>
<pre><code>from lightgbm import LGBMRegressor
import pandas as pd
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generar un dataset de regresión aleatorio
X, y = make_regression(n_samples=1000, n_features=10, noise=0.9, random_state=42)
feature_names = [f&quot;feature_{i}&quot; for i in range(X.shape[1])]

# Convertir a DataFrame para mayor legibilidad
X = pd.DataFrame(X, columns=feature_names)

# Agregar características inútiles
X[&quot;useless_feature_1&quot;] = 1

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Definir el modelo LGBMRegressor
model = LGBMRegressor(
    objective=&quot;regression&quot;,
    metric=&quot;rmse&quot;,
    random_state=1,
    n_estimators=100
)

# Entrenar el modelo
model.fit(X_train, y_train, eval_set=[(X_test, y_test)])

# Predicciones y evaluación
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f&quot;Test RMSE: {rmse:.4f}&quot;)

# Importancia de características
importance = pd.DataFrame({
    &quot;feature&quot;: X.columns,
    &quot;importance&quot;: model.feature_importances_
}).sort_values(by=&quot;importance&quot;, ascending=False)

print(&quot;\nFeature Importance:&quot;)
print(importance)
</code></pre>
<p>Expected solution: There should be some workarround, but the most interesting one would be the one that is using some param in the fit or the regressor method.</p>
","3","Question"
"79434756","","<p>I have the following code where I try to predict price of tools for which I use poisson regression.</p>
<pre><code># --- Load and Prepare Data ---
y = train['PriceToday']
X = train.drop(columns=['PriceToday'])

# Define non-standard types
non_standard_types = [&quot;Nar&quot;, &quot;Orch&quot;, &quot;Fru&quot;,&quot;Comp&quot;]

# Create a flag feature for non-standard
X[&quot;Non_Standard_Flag&quot;] = X[&quot;Type_LS&quot;].isin(non_standard_types).astype(int)

# Identify numerical and categorical columns
num_features = [&quot;AGE&quot;, &quot;POWER&quot;, &quot;Hours&quot;, &quot;Non_Standard_Flag&quot;]
cat_features = [&quot;BRAND&quot;, &quot;Country&quot;, &quot;Final_Trans&quot;]

# Define preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), num_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)
    ], remainder=&quot;drop&quot;
)

# --- Train/Test Split ---
# Create a weight column
train[&quot;sample_weight&quot;] = train[&quot;Type_LS&quot;].apply(lambda x: 1 if x == &quot;Standard&quot; else 5)

train[&quot;stratify_group&quot;] = train[&quot;BRAND&quot;].astype(str)
X_train, X_val, y_train, y_val, train_weights, val_weights = train_test_split(
    X, y, train[&quot;sample_weight&quot;], test_size=0.2, random_state=42, stratify=train[&quot;stratify_group&quot;]
)
# Fit the preprocessor once on training data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_val_preprocessed = preprocessor.transform(X_val)

# Define models
models = {
    &quot;Poisson&quot;: PoissonRegressor(alpha=0.01)
}

# Train and evaluate models
model_results = {}

for model_name, model in models.items():
    model.fit(X_train_preprocessed, y_train, sample_weight=train_weights)

    # Predictions
    predictions = model.predict(X_val_preprocessed)
    # Calculate Metrics
    r2 = r2_score(y_val, predictions)

    model_results[model_name] = {
        &quot;model&quot;: model,
        &quot;R2&quot;: r2
    }
</code></pre>
<p>I have a test data for which I want to compare the prices of them with the predicted price from the model.
My test data is something like this:</p>
<pre><code># Ensure new data has the correct format
new_data = pd.DataFrame({
    &quot;AGE&quot;: [12, 24, 36, 48, 60, 72, 84, 12, 24, 36, 48, 60, 72, 84],
    &quot;Hours&quot;: [500, 1000, 1500, 2000, 2500, 3000, 3500, 500, 1000, 1500, 2000, 2500, 3000, 3500],
    &quot;BRAND&quot;: [&quot;NH&quot;] * 14,
    &quot;POWER&quot;: [150] * 7 + [80] * 7,
    &quot;Final_Trans&quot;: [&quot;Cv&quot;] * 14,
    &quot;Country&quot;: [&quot;DEU&quot;] * 14,
    &quot;Type_LS&quot;: [Nar, Nar, Nar, ST, ST, ST, ST, ST,ST, ST, ST, ST, ST, ST] 
    &quot;Current_Pred&quot;: [105614, 96681, 88504, 81018, 74165, 67892, 62150, 42608, 39728, 37043, 34540, 32206, 30029, 28000]
})
</code></pre>
<p>My code is:</p>
<pre><code>new_df = pd.DataFrame(new_data)
# Create the 'Non_Standard_Flag'
new_df[&quot;Non_Standard_Flag&quot;] = new_df[&quot;Type_LS&quot;].isin(non_standard_types).astype(int)

# Select the columns required by the preprocessor
X_new = new_df[['AGE', 'POWER', 'Hours', 'Non_Standard_Flag', 'BRAND', 'Country', 'Final_Trans']]

X_new_preprocessed = preprocessor.transform(X_new) 

# Get the column names after one-hot encoding from the training data
ohe = preprocessor.named_transformers_['cat']
encoded_cat_columns = ohe.get_feature_names_out(cat_features)

# Create column names for numeric features
num_columns = num_features

# Combine the column names
all_columns = num_columns + list(encoded_cat_columns)

# Create a DataFrame from the preprocessed data
X_new_preprocessed_df = pd.DataFrame(X_new_preprocessed, columns=all_columns)

# --- Predict with the Poisson Model ---
poisson_model = model_results[&quot;Poisson&quot;][&quot;model&quot;]  # Access the trained Poisson model
predicted_prices = poisson_model.predict(X_new_preprocessed_df)

# Compare and Store Results ---
new_df['Predicted_Price'] = predicted_prices

# Calculate the difference between predicted and current prices
new_df['Price_Difference'] = new_df['Predicted_Price'] - new_df['Current_Pred']
</code></pre>
<p>But after doing this I get an error:</p>
<pre><code>X has 7 features, but ColumnTransformer expects 13 features
</code></pre>
<p>I have the same number of columns so I do no understand why I have this error.</p>
","0","Question"
"79436775","","<p>I am working on a pill detection project using YOLOv8 and applying Albumentations for data augmentation. However, some augmented images turn out with too much noise or distortion (example attached).</p>
<p>What I Want to Achieve:
Apply realistic augmentations (rotation, brightness, noise, etc.)
Ensure the pills remain clear and recognizable</p>
<p>Problem:
Some images become too noisy or unrecognizable.
The issue seems related to GaussNoise or color space handling.</p>
<p>Some images generated:
<a href=""https://i.sstatic.net/pBc7hxpf.jpg"" rel=""nofollow noreferrer"">Image 1</a> &amp;
<a href=""https://i.sstatic.net/0kqdqUYC.jpg"" rel=""nofollow noreferrer"">Image 2</a>
I think they are overly zoomed.</p>
<p>My Code:</p>
<pre><code>import cv2
import os
import albumentations as A

# Set input and output directories
INPUT_DIR = r&quot;C:\Users\...\pills\01_01&quot;
OUTPUT_DIR = INPUT_DIR
TARGET_COUNT = 350  

# Define Augmentations
transform = A.Compose([
    A.RandomBrightnessContrast(p=0.3),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.2),
    A.Rotate(limit=25, p=0.5),
    A.Blur(blur_limit=2, p=0.2),
    A.GaussNoise(var_limit=(5.0, 10.0), p=0.2),  # Might be too strong?
    A.RandomCrop(height=256, width=256, p=0.1),
    A.HueSaturationValue(p=0.3)
])

# Process images
image_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(('.jpg', '.png', '.jpeg'))]

for i in range(1, TARGET_COUNT + 1):
    img_name = image_files[i % len(image_files)]
    img_path = os.path.join(INPUT_DIR, img_name)

    image = cv2.imread(img_path)
    if image is None:
        continue
    
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB
    augmented = transform(image=image)['image']
    augmented = cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR)  # Convert back before saving

    new_filename = f&quot;01_{i}.jpg&quot;
    cv2.imwrite(os.path.join(OUTPUT_DIR, new_filename), augmented)

print(f&quot;Saved augmented images in {OUTPUT_DIR}&quot;)
</code></pre>
<p>Troubleshooting Steps I've Tried:
✔ Reduced var_limit in GaussNoise → Still some noise issues.
✔ Checked if cv2 was handling color incorrectly.
✔ Reduced Blur intensity.</p>
<p>Question:
How can I fix the excessive noise issue while keeping useful augmentations?
Are there better augmentation settings for pill detection?
Should I use a different approach for ensuring annotation correctness in YOLOv8?</p>
","0","Question"
"79437180","","<p>I'm trying to do a very primitive reinforced learning model on tensorflow. Although it's relatively small, a single iteration takes ~6-7 seconds.</p>
<pre><code>def build_model():
    model = keras.Sequential([
        layers.Input(shape=(400,)),
        layers.Dense(128, activation=&quot;relu&quot;),
        layers.Dense(128, activation=&quot;relu&quot;),
        layers.Dense(3)
    ])
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=&quot;huber&quot;)
    return model

class DQNAgent:
    def __init__(self):
        self.model = build_model()
        self.target_model = build_model()
        self.target_model.set_weights(self.model.get_weights())

        self.memory = deque(maxlen=1000)
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.95
        self.batch_size = 32

    def choose_action(self, state):
        if np.random.rand() &lt; self.epsilon:
            return random.choice([0, 1, 2])
        q_values = self.model.predict(np.array([state]), verbose=0)
        return np.argmax(q_values[0])

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def train(self):
        if len(self.memory) &lt; self.batch_size:
            return
        batch = random.sample(self.memory, self.batch_size)
        states, targets = [], []

        for state, action, reward, next_state, done in batch:
            target = reward
            if not done:
                target += self.gamma * np.max(self.target_model.predict(np.array([next_state]), verbose=0))

            q_values = self.model.predict(np.array([state]), verbose=0)
            q_values[0][action] = target

            states.append(state)
            targets.append(q_values[0])

        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)

        if self.epsilon &gt; self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())
</code></pre>
<p>After profiling all the given code, I saw that the <code>model.predict()</code> takes that much time to complete:
<a href=""https://i.sstatic.net/JnRjwn2C.png"" rel=""nofollow noreferrer"">Profiler results</a></p>
<p>Initially I thought I just need to compute on GPU, but after two days wasted trying to do so, nothing really changed.
Does it really take that much time, or I messed smth up in the code?</p>
<pre><code>GPU:Geforce 2060,
CPU:Intel Core i7, 
Windows 11,
Python:3.10
Tensorflow: 2.10
</code></pre>
","-1","Question"
"79439462","","<p>I have a ML model made with &quot;knn&quot; in scikit-learn and noticed that the more i have data, more precise my model is getting with it's predictions. The problem is, i have lot's of DataFrames that show different situations for the same system i want to predict. Is it possible to train the model in those different Dataframes? Because if i call .fit() it'll reset it's previous trainning.</p>
<pre><code>X_cleaned = X.dropna()
y_cleaned = y[X_cleaned.index]
X_training, X_test, y_training, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.15, random_state=0)
</code></pre>
","-1","Question"
"79440021","","<p>i made a gradient descent code but it doesnt seem to work well</p>
<pre><code>import numpy as np
from random import randint,random
import matplotlib . pyplot as plt


def calculh(theta, X):
    h = 0
    h+=theta[0]*X # w*X
    h+= theta[-1] # +b
    return h


def calculY(sigma, h) :
    return sigma(h) # sigma peut-etre tanh, signoide etc.


def erreurJ(theta, sigma):
    somme = 0
    somme = 1/4*(sigma(theta[1])**2+sigma(theta[0]+theta[1])**2)
    return somme


def gradient(X, Y, Ysol, sigmaprime, h):
    return ((Y-Ysol)*sigmaprime(h)*X ,(Y-Ysol)*sigmaprime(h)*1)
def grad(theta):
    w,b = theta[0],theta[1]
    #print(theta)
    return [2*b**3+3*b**2*w+3*b*w**2-2*b+w**3-w,b**3+3*b**2*w+3*b*w**2-b+w**3-w]
# *X correspond a 0 ou 1  : nos 2 entrées ; *1 correspond a derivee de b

def pasfixe(theta, eta, epsilon, X, Y, Ysol, sigma, sigmaprime, h):
    n=0
    while np.linalg.norm(gradient(X, Y, Ysol, sigmaprime, h)) &gt; epsilon and n&lt;10000 :
        for i in range(len(theta)) :
            theta[i] = theta[i] - eta*gradient(X, Y, Ysol, sigmaprime, h)[i]
            h = calculh(theta, X)
            Y = calculY(sigma, h)
            n+=1
            if theta[i]&gt;100 : ### cas de divergence
                return [100,100],Y
    return theta,Y

sigma = lambda z : z**2-1
sigmaprime = lambda z : 2*z
eta = 0.1

X = 1
Ysol = 0
listeY = []
listetheta = []
lst = [[3*random()*(-1)**randint(0,1),3*random()*(-1)**randint(0,1)] for i in range(5000)]
nb = 0
for i in lst:
        nb+=1
        if nb%50 == 0:
            print(nb)
        theta = i[:]
        h = calculh(theta, X)
        Y = calculY(sigma, h)
        CalculTheta = pasfixe( theta, eta, 10**-4, X,Y, Ysol, sigma, sigmaprime, h)
        listetheta.append(CalculTheta[0])
        listeY.append(CalculTheta[1])


for i in range (len(listeY)):
          listeY[i] = round(listeY[i],2)
print (listeY)

for i in range (len(listetheta)):
      for j in range(2):
          listetheta[i][j] = round(listetheta[i][j],2)
print (listetheta)

for i in range(len(lst)):
    if [int(listetheta[i][0]),int(listetheta[i][1])] in [[-2,1]]:
        plt.plot(lst[i][0],lst[i][1],&quot;bo&quot;)
    elif [int(listetheta[i][0]),int(listetheta[i][1])] in [[2,-1]]:
        plt.plot(lst[i][0],lst[i][1],&quot;co&quot;)
    elif  [int(listetheta[i][0]),int(listetheta[i][1])] in [[0,-1]]:
        plt.plot(lst[i][0],lst[i][1],&quot;go&quot;)
    elif  [int(listetheta[i][0]),int(listetheta[i][1])] in [[0,1]]:
        plt.plot(lst[i][0],lst[i][1],&quot;mo&quot;)
    elif  int(listetheta[i][0])**2 +int(listetheta[i][1])**2 &gt;= 10:
        plt.plot(lst[i][0],lst[i][1],&quot;ro&quot;)

plt.show()
</code></pre>
<p>in the end i make a graph with the bias and weight values ans each point is colored in function of what the theta (weight,bias) value given at the beginning of the loop is converging to.
<a href=""https://i.sstatic.net/W5ivHcwX.png"" rel=""nofollow noreferrer"">the graph i am supposed to have</a></p>
<p>i tried to calculate the gradient myself but it didnt work as well. i am supposed to get a graph like this one</p>
","0","Question"
"79442381","","<p>I'm trying to implement a perceptron in python using numpy, when using the notation z=XW+b everything is fine. While studying ML I do see though that z=WX+b is also common, especially when talking about neural networks. The problem is that the dimensions of the matrices don't add up, I tried following some answers on the web but the output doesn't have the right dimensions. I tried also asking chatgpt but it only implemented the code following the z=XW+b notation.
This is the code I used for z=XW+b:</p>
<pre><code>import numpy as np

n_inpts = 10
in_feats = 5
n_hidden = 8
out_feats = 1

X = np.random.randn(n_inpts,in_feats)

W_x = np.random.randn(in_feats, n_hidden)

bias_h  = np.random.randn(1, n_hidden)
H = np.dot(X,W_x) + bias_h
#H is nxh

relu = lambda x: max(0, x)
v_relu = np.vectorize(relu)

H = v_relu(H)

W_h = np.random.randn(n_hidden, out_feats)

bias_o = np.random.randn(1, out_feats)

output = np.dot(H, W_h) + bias_o
</code></pre>
<p>Can anybody give me an implementation that gives the same result while using z=WX+b?
Every single implementation I found follows the z=XW+b notation. I guess it comes down to how you specify the X and W matrices but as of now I have had no luck finding a solution to my question</p>
","0","Question"
"79448585","","<p>I have footage of labels coming out of a printer.</p>
<p>I want to extract the individual labels from the frame (i.e., detect the boundaries of each individual label), and then find the distance between the printed rectangle and the label's top edge in mm.</p>
<p><a href=""https://i.sstatic.net/mL7CFv2D.jpg"" rel=""nofollow noreferrer"">This is a frame from the footage</a></p>
<p>I initially tried contour detection alongside some morphological operations, but the printed content inside the labels (rectangle and number) is interfering with edge detection, making it difficult to isolate just the label borders.</p>
<p>Has anyone tackled a similar problem? What preprocessing techniques or alternative methods would work best for reliably segmenting only the label edges?</p>
","0","Question"
"79448915","","<p>I was trying to import QuantumKernals using the following:</p>
<pre><code>from qiskit_machine_learning.kernels import QuantumKernel
</code></pre>
<p>but i am getting this error:</p>
<pre><code> from qiskit_machine_learning.kernels import QuantumKernel
ImportError: cannot import name 'QuantumKernel' from 'qiskit_machine_learning.kernels' 
(C:\Users\pshre\AppData\Local\Programs\Python\Python310\lib\site-packages\qiskit_machine_learning\kernels\__init__.py)
</code></pre>
<p>Qiskit version: 0.8.2</p>
<p>i have updated the module:</p>
<pre><code>pip install --upgrade qiskit-machine-learning
</code></pre>
","-2","Question"
"79449572","","<p>I'm training a LSTM for time series prediction, where data comes from sensors at irregular intervals. I'm using the last 5 min data to predict the next value, but some sequences are larger than others.</p>
<p>My input array's shape is (611,1200,15) where (sample, timesteps, features). The second dimension is not completed for every sample, so i padded the missing data with np.nan values. For instance, sample (1,:,:) has 1000 timesteps and 200 np.nan.</p>
<p>While training, loss equals nan.</p>
<p>What am i doing wrong? How can I train it?</p>
<p>Here's my attempt to train the LSTM:</p>
<pre class=""lang-py prettyprint-override""><code>def lstmFit(y, X, n_hidden=1, n_neurons=30, learning_rate=1e-2):   
    lstm = Sequential()
    lstm.add(Masking(mask_value=np.nan, input_shape=(None, X.shape[2])))
        
    for layer in range(n_hidden):
        lstm.add(LSTM(n_neurons, 
                      activation=&quot;tanh&quot;,
                      recurrent_activation = &quot;sigmoid&quot;,
                      return_sequences=True))
        
    lstm.add(Dense(1))
    
    lstm.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;)
    
    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1, restore_best_weights=True)
  
    
    lstm.fit(X, y.reshape(-1), epochs=100, callbacks=[early_stopping])
    
    y_train_fit = lstm.predict(X)
    
    return lstm, y_train_fit
</code></pre>
<p>The model's summary:</p>
<pre><code>lstm.summary()
Model: &quot;sequential_9&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 masking_7 (Masking)         (None, None, 15)          0         
                                                                 
 lstm_6 (LSTM)               (None, None, 30)          5520      
                                                                 
 dense_10 (Dense)            (None, None, 1)           31        
                                                                 
=================================================================
Total params: 5551 (21.68 KB)
Trainable params: 5551 (21.68 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
<p>And the first epochs of training:</p>
<pre><code>Epoch 1/100
18/18 [==============================] - 20s 335ms/step - loss: nan
Epoch 2/100
18/18 [==============================] - 6s 335ms/step - loss: nan
Epoch 3/100
18/18 [==============================] - 7s 365ms/step - loss: nan
</code></pre>
","0","Question"
"79455291","","<p>I am trying to predict LSDC from MRI images. For each <code>study_id</code> there are multiple images. Each <code>study_id</code> represents each patient. I want to predict 3 level of severity for 5 conditions on 5 levels.</p>
<p>I am trying to create dataset using custom data generator using <code>Sequence</code> class from tensorflow.</p>
<p>Here is my <code>Datagenerator</code> class:</p>
<pre><code> class CustomDataGenerator(Sequence):
    def __init__(self, image_dict, num_img, labels_dict=None, batch_size=8, image_size=(224, 224), shuffle=True):
       self.image_dict = image_dict
       self.labels_dict = labels_dict
       self.batch_size = batch_size
       self.image_size = image_size
       self.shuffle = shuffle
       self.ids = list(image_dict.keys())
       self.num_img = num_img
       self.on_epoch_end()

    def __len__(self):
       return int(np.floor(len(self.ids) / self.batch_size))

    def __getitem__(self, index):
       start = index * self.batch_size
       end = min((index + 1) * self.batch_size, len(self.ids))
       batch_ids = self.ids[start:end]
       batch_images = []
       batch_labels = []

       for id_ in batch_ids:
           images = []

           for image_path in self.image_dict.get(id_, []):
               dicomdata = pydicom.dcmread(image_path)
               image = dicomdata.pixel_array
               image = cv2.resize(image, self.image_size)
               image = np.expand_dims(image, axis=-1)
               image = image.astype('float32') / np.max(image)
               image = np.repeat(image, 3, axis=-1)
               images.append(image)

           images = np.array(images)

           if len(images) &lt; self.num_img:
               pad_amount = self.num_img - len(images)
               padding = [(0, pad_amount)] + [(0, 0)] * (len(images.shape) - 1)
               images = np.pad(images, padding, mode='constant')

           batch_images.append(images)

           if self.labels_dict:
               label = np.array(self.labels_dict.get(id_), dtype=np.float32)
               batch_labels.append(label)

       batch_images = np.stack(batch_images)
       if self.labels_dict:
           batch_labels = np.array(batch_labels, dtype=np.float32)
           return batch_images, batch_labels

       return batch_images

    def on_epoch_end(self):
       if self.shuffle:
           np.random.shuffle(self.ids)
</code></pre>
<p>My labels dictionary is as follows:</p>
<pre><code>    for i, sid in enumerate(train_df['study_id']):
        labels_dict[str(sid)] = []
        for con in conditions:
            if train_df.loc[i, con] == 'normal_mild':
                labels_dict[str(sid)].append([1, 0, 0])
            elif train_df.loc[i, con] == 'severe':
                labels_dict[str(sid)].append([0, 0, 1])
            else:
                labels_dict[str(sid)].append([0, 1, 0])

       labels_dict[str(sid)] = np.array(labels_dict[str(sid)], dtype=np.float32)
</code></pre>
<p>I have tried various ways to convert <code>labels_dict</code> to numpy array. But either at the time of training it shows shape mismatch error. Or when trying to see the data it showed error.</p>
<p>This is the error it shows:</p>
<pre><code>----&gt; 1 model.fit(train_generator, epochs=2) #, steps_per_epoch=len(train_generator)//8)

/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
    120             # To get the full stack trace, call:
    121             # `keras.config.disable_traceback_filtering()`
--&gt; 122             raise e.with_traceback(filtered_tb) from None
    123         finally:
    124             del filtered_tb

&lt;ipython-input-12-cf42609bddda&gt; in __getitem__(self, index)
     47         batch_images = np.stack(batch_images)
     48         if self.labels_dict:
---&gt; 49             batch_labels = np.array(batch_labels, dtype=np.float32)
     50             return batch_images, batch_labels
     51 

ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part.
</code></pre>
<p>I tried using <code>np.stack</code> or <code>batch_labels = batch_labels.reshape((batch_labels.shape[0], len(conditions), 3))</code>, but it shows different error. My data does not have any <code>nan</code> and all <code>labels_dict</code> is of shape <strong>(batch_size, num_of_condition, severity_class)</strong>. Even when I tried printing the data from generator. Generator data shape from <code>data_x, data_y = next(iter(train_generator))</code> show desired shape of data for model input and output. I cannot figure out the problem.</p>
","0","Question"
"79456368","","<p>I want to forecast hourly temperature values using the fbprophet model. So far I have trained the model on just ds and y variable and it is giving me good results. But now I want to add extra regressors and then perform the forecast.</p>
<p>After fitting the model with extra regressors, I test it on test dataset which is giving me good accuracy. But the main issue is how to forecast future with it (data beyond my test set)</p>
<p>Here's what I've done so far.</p>
<pre><code># Data preparation and feature engineering

temp = df[[&quot;Temperature&quot;]].apply(kelvinToDegC).copy()
temp[&quot;hourlyLag&quot;] = temp[&quot;Temperature&quot;].shift(1).bfill()
temp[&quot;dailyLag&quot;] = temp[&quot;Temperature&quot;].shift(24).bfill()
temp[&quot;weeklyLag&quot;] = temp[&quot;Temperature&quot;].shift(24*7).bfill()
temp[&quot;movMean&quot;] = temp[&quot;Temperature&quot;].rolling(window=&quot;24h&quot;).mean().bfill()
temp[&quot;mStd&quot;] = temp[&quot;Temperature&quot;].rolling(window=&quot;24h&quot;).std().bfill()
temp[&quot;ub&quot;] = temp[&quot;movMean&quot;] + (1.6 * temp[&quot;mStd&quot;])
temp[&quot;lb&quot;] = temp[&quot;movMean&quot;] - (1.6 * temp[&quot;mStd&quot;])
temp[&quot;devFromMean&quot;] = temp[&quot;movMean&quot;] - temp[&quot;Temperature&quot;]
temp[&quot;devFromUB&quot;] = temp[&quot;ub&quot;] - temp[&quot;Temperature&quot;]
temp[&quot;devFromLB&quot;] = temp[&quot;lb&quot;] - temp[&quot;Temperature&quot;]
temp[&quot;hour&quot;] = temp.index.hour
temp[&quot;dayOfYear&quot;] = temp.index.day
temp[&quot;month&quot;] = temp.index.month
temp = temp.reset_index()
temp.rename(columns={&quot;Date&quot;: &quot;ds&quot;, &quot;Temperature&quot;: &quot;y&quot;}, inplace=True)

model = Prophet()
model.add_regressor(&quot;hourlyLag&quot;)
model.add_regressor(&quot;dailyLag&quot;)
model.add_regressor(&quot;weeklyLag&quot;)
model.add_regressor(&quot;movMean&quot;)
model.add_regressor(&quot;mStd&quot;)
model.add_regressor(&quot;ub&quot;)
model.add_regressor(&quot;lb&quot;)
model.add_regressor(&quot;devFromMean&quot;)
model.add_regressor(&quot;devFromUB&quot;)
model.add_regressor(&quot;devFromLB&quot;)
model.add_regressor(&quot;hour&quot;)
model.add_regressor(&quot;dayOfYear&quot;)
model.add_regressor(&quot;month&quot;)

model.fit(train)

These are the mae and mape scores
MAE:  0.00
MAPE:  0.17 %

now future = model.make_future_dataframe(periods=24 * 365 * 3, freq=&quot;h&quot;)

I am getting this error

ValueError                                Traceback (most recent call last)
Cell In[68], line 2
      1 future = model.make_future_dataframe(periods=8760, freq=&quot;h&quot;)
----&gt; 2 predictions = model.predict(future)

File c:\Users\5923imtiaz\AppData\Local\anaconda3\envs\ai\Lib\site-packages\prophet\forecaster.py:1270, in Prophet.predict(self, df, vectorized)
   1268     if df.shape[0] == 0:
   1269         raise ValueError('Dataframe has no rows.')
-&gt; 1270     df = self.setup_dataframe(df.copy())
   1272 df['trend'] = self.predict_trend(df)
   1273 seasonal_components = self.predict_seasonal_components(df)

File c:\Users\5923imtiaz\AppData\Local\anaconda3\envs\ai\Lib\site-packages\prophet\forecaster.py:297, in Prophet.setup_dataframe(self, df, initialize_scales)
    295 for name in self.extra_regressors:
    296     if name not in df:
--&gt; 297         raise ValueError(
    298             'Regressor {name!r} missing from dataframe'
    299             .format(name=name)
    300         )
    301     df[name] = pd.to_numeric(df[name])
    302     if df[name].isnull().any():

ValueError: Regressor 'hourlyLag' missing from dataframe
</code></pre>
","1","Question"
"79457237","","<p>I have implemented this function to fit the model</p>
<pre><code>def fit_model(model, X_train_sequence_tensor,Y_train_sequence_tensor, epochs, val_set, time_windows, scaler):
    
    X_column_list = [item for item in val_set.columns.to_list() if item not in ['date', 'user', 'rank','rank_group', 'counts', 'target']]
    X_val_set = val_set[X_column_list].round(2)
                    
    X_val_set[X_val_set.columns] = scaler.transform(X_val_set[X_val_set.columns] )
    X_val_sequence = get_feature_array(X_val_set , X_column_list, time_windows)
    X_val_sequence_tensor = tf.convert_to_tensor(X_val_sequence, dtype=tf.float32)
    
    Y_column_list = ['target']                
    Y_val_set = val_set[Y_column_list].round(2)
    Y_val_sequence = get_feature_array(Y_val_set , Y_column_list, time_windows)
    Y_val_sequence_tensor = tf.convert_to_tensor(Y_val_sequence, dtype=tf.float32)

                    
    history = model.fit(X_train_sequence_tensor,Y_train_sequence_tensor, epochs, 
                        validation_data=(X_val_sequence_tensor, Y_val_sequence_tensor))
    return model, history

</code></pre>
<p>but when I call it as</p>
<pre><code>fitted_model, history = fit_model(model, X_train_sequence_tensor,Y_train_sequence_tensor, 
                    epochs=100, val_set=val_set, time_windows=90, scaler=scaler)
</code></pre>
<p>it stops after the first epoch. It does not run for all the 100 as required.</p>
<p>I tried to call it outside of the function call and it worked.</p>
<pre><code>`# Step 3.2 : Fit the model + We pass some validation for
                                                # monitoring validation loss and metrics
                                                # at the end of each epoch
                    X_val_set = val_set[X_column_list].round(2)
                    
                    #X_val_set.values = scaler.transform(X_val_set.values)
                    
                    X_val_set[X_val_set.columns] = scaler.transform(X_val_set[X_val_set.columns] )
                    X_val_sequence = get_feature_array(X_val_set , X_column_list, 90)
                    X_val_sequence_tensor = tf.convert_to_tensor(X_val_sequence, dtype=tf.float32)
                    
                    Y_val_set = val_set[Y_column_list].round(2)
                    Y_val_sequence = get_feature_array(Y_val_set , Y_column_list, 90)
                    Y_val_sequence_tensor = tf.convert_to_tensor(Y_val_sequence, dtype=tf.float32)

                    
                    training_history = cnn1d_bilstm_model.fit(X_train_sequence_tensor,Y_train_sequence_tensor, epochs=200, 
                                                            # We pass some validation for
                                                            # monitoring validation loss and metrics
                                                            # at the end of each epoch
                                                            validation_data=(X_val_sequence_tensor, Y_val_sequence_tensor))
</code></pre>
<p>What am I doing wrong?</p>
","1","Question"
"79457437","","<p>I want to save and load a CNN model to further training. I have developed a model and saved it as <code>.h5</code> file. There is no problem when creating, training, and saving at first run.</p>
<p>The problem exists when loading an existing <code>.h5</code> model and attempting to train it. The following code describes the implementation and issue.</p>
<pre class=""lang-py prettyprint-override""><code>import os.path
import tensorflow as tf

# Enable eager execution
tf.compat.v1.enable_eager_execution()

... # removed for readibility

def train_model(model_name:str, model_handler: TensorModel, visualiser: Visualiser, logger: Logger, x_train, y_train, x_test, y_test, batch_size) -&gt; tuple:
    logger.info(f&quot;Eager enabled: {tf.executing_eagerly()}&quot;)

    # Check to see if the model has already been trained
    if USE_EXISTING_MODELS and os.path.exists(f&quot;models/{model_name}.h5&quot;):
        model = model = tf.keras.models.load_model(f&quot;models/{model_name}.h5&quot;)
        (x_train, y_train), (x_test, y_test) = model_handler.load_data()
    else:
        if model_name == &quot;base_model&quot;:
            model = model_handler.create_cnn()
        else:
            model = model_handler.create_cnn(batch_normalisation=True)

    history = model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=batch_size, validation_data=(x_test, y_test))
    test_loss, test_acc = model.evaluate(x_test, y_test)
    model.save(f&quot;models/{model_name}.h5&quot;)
    
    logger.info(f&quot;Model accuracy: {test_acc * 100:.2f}%&quot;)
    visualiser.plot_training_history(history, model_name)
    return (model, model_name), (test_acc)
</code></pre>
<p>The following is the error produced.</p>
<pre class=""lang-bash prettyprint-override""><code>File &quot;/home/arief/development/Machine_Learning/Machine-Learning-Real-Time-Object-Classification/Train.py&quot;, line 40, in train_model
    history = model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=batch_size, validation_data=(x_test, y_test))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/arief/development/Machine_Learning/Machine-Learning-Real-Time-Object-Classification/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py&quot;, line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;/home/arief/development/Machine_Learning/Machine-Learning-Real-Time-Object-Classification/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py&quot;, line 155, in convert_to_numpy
    return np.array(x)
           ^^^^^^^^^^^
NotImplementedError: numpy() is only available when eager execution is enabled.
</code></pre>
<p>The logging shows that <code>eager execution</code> is enabled with the following:</p>
<pre class=""lang-bash prettyprint-override""><code>2025-02-21 13:46:42,505 - INFO - Eager enabled: True
</code></pre>
<p>What am I missing?</p>
","1","Question"
"79457834","","<p>I am trying to tune some hyperparameters for my neural network for an image segmentational problem. I set up the tuner as simple as it can be, but when I run my code i get the following error:</p>
<pre><code>2025-02-21 15:19:59,571 INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265
2025-02-21 15:20:00,697 INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.
Traceback (most recent call last):
  File &quot;Q:\Msc\Diplomadolgozat\Unet\Pytorch-UNet\train.py&quot;, line 232, in &lt;module&gt;
    analysis = tune.run(train_unet_model, config=config)
  File &quot;C:\Users\sajte\AppData\Local\Programs\Python\Python310\lib\site-packages\ray\tune\tune.py&quot;, line 758, in run
    experiments[i] = Experiment(
  File &quot;C:\Users\sajte\AppData\Local\Programs\Python\Python310\lib\site-packages\ray\tune\experiment\experiment.py&quot;, line 149, in __init__
    self._run_identifier = Experiment.register_if_needed(run)
  File &quot;C:\Users\sajte\AppData\Local\Programs\Python\Python310\lib\site-packages\ray\tune\experiment\experiment.py&quot;, line 360, in register_if_needed
    raise type(e)(str(e) + &quot; &quot; + extra_msg) from None
TypeError: ray.cloudpickle.dumps(&lt;class 'ray.tune.trainable.function_trainable.wrap_function.&lt;locals&gt;.ImplicitFunc'&gt;) failed.
To check which non-serializable variables are captured in scope, re-run the ray script with 'RAY_PICKLE_VERBOSE_DEBUG=1'. Other options:
-Try reproducing the issue by calling `pickle.dumps(trainable)`.
-If the error is typing-related, try removing the type annotations and try again.
</code></pre>
<p>I found nothing usable about how to resolve it.
Here is my code:</p>
<p>Main:</p>
<pre><code>if __name__ == '__main__':
    config={
        &quot;BATCH_SIZE&quot;: tune.grid_search([1, 4, 8]),
        &quot;LEARNING_RATE&quot;: tune.grid_search([1e-4, 1e-5]),
        &quot;BASE_CHANNELS&quot;: tune.grid_search([32, 64, 128]),
        &quot;GRADIENT_CLIPPING&quot;: tune.grid_search([0.25, 1.0, 5.0]),
        &quot;WEIGHT_DECAY&quot;: tune.grid_search([1e-2, 1e-4, 1e-8]),
        &quot;SCHEDULER_PATIENCE&quot;: tune.grid_search([0, 1, 4])
        }
    
    analysis = tune.run(train_unet_model, config=config)
    
    print(&quot;Best config: &quot;, analysis.get_best_config(metric=&quot;dice_loss&quot;))
</code></pre>
<p>train function:</p>
<pre><code>def train_unet_model(config):
    dataset = BasicDataset(dir_img, dir_mask, IMG_SCALE, '_mask')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f'Using device {device}')

    model = UNet(n_channels=3, n_classes=CLASSES, bilinear=BILINEAR, base_channels=config[&quot;BASE_CHANNELS&quot;])
    model = model.to(memory_format=torch.channels_last)
    model.to(device=device)

    # 2. Split into train / validation partitions
    n_val = int(len(dataset) * VAL_PERCENT)
    n_train = len(dataset) - n_val
    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))

    # 3. Create data loaders
    loader_args = dict(batch_size=config[&quot;BATCH_SIZE&quot;], num_workers=os.cpu_count(), pin_memory=True)
    train_loader = DataLoader(train_set, shuffle=True, **loader_args)
    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)

    # optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=config[&quot;MOMENTUM&quot;], foreach=True)
    optimizer = optim.AdamW(model.parameters(), lr=config[&quot;LEARNING_RATE&quot;], weight_decay=config[&quot;WEIGHT_DECAY&quot;], foreach=True)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=config[&quot;SCHEDULER_PATIENCE&quot;]) 
    grad_scaler = torch.GradScaler(device, enabled=AMP)
    criterion = nn.CrossEntropyLoss() if model.n_classes &gt; 1 else nn.BCEWithLogitsLoss()
    global_step = 0

    # 5. Begin training
    for epoch in range(1, EPOCHS + 1):
        model.train()
        with tqdm(total=n_train, desc=f'Epoch {epoch}/{EPOCHS}', unit='img') as pbar:
            epoch_loss = 0.0
            for batch in train_loader:
                images, true_masks = batch['image'], batch['mask']

                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)
                true_masks = true_masks.to(device=device, dtype=torch.long)

                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=AMP):
                    masks_pred = model(images)
                    loss = criterion(masks_pred, true_masks)
                    loss += dice_loss(
                        F.softmax(masks_pred, dim=1).float(),
                        F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float(),
                        multiclass=True
                    )

                optimizer.zero_grad(set_to_none=True)
                grad_scaler.scale(loss).backward()
                grad_scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config[&quot;GRADIENT_CLIPPING&quot;])
                grad_scaler.step(optimizer)
                grad_scaler.update()

                global_step += 1
                epoch_loss += loss.item()

                pbar.update(images.shape[0])
                pbar.set_postfix(**{'loss (batch)': loss.item(), 'loss (epoch)': epoch_loss})
                
                # Evaluation round
                division_step = (n_train // (1 * config[&quot;BATCH_SIZE&quot;]))
                if division_step &gt; 0:
                    if global_step % division_step == 0:
                        val_score = evaluate(model, val_loader, device, AMP)
                        scheduler.step(val_score)
                        tune.track.log(dice_loss=val_score)

                        logging.info('Validation Dice score: {}'.format(val_score))                                

                        Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)
                        state_dict = model.state_dict()
                        state_dict['mask_values'] = dataset.mask_values
                        torch.save(state_dict, str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch)))
                        logging.info(f'Checkpoint {epoch} saved!')
</code></pre>
<p>My imports and constants:</p>
<pre><code>import logging
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from torch import optim
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
from ray import tune

from evaluate import evaluate
from unet import UNet
from utils.data_loading import BasicDataset
from utils.dice_score import dice_loss

dir_img = Path('./data/wildfire/train/')
dir_mask = Path('./data/wildfire/train_masks/')
dir_checkpoint = Path('./checkpoints/')

EPOCHS = 5
VAL_PERCENT = 0.1
IMG_SCALE = 1.0
AMP = True
CLASSES = 2
BILINEAR = False
</code></pre>
<p>I found more materials about how to tune with Ray, but each seems more unnecessarily complicated than the other. If someone can direct me to some clean and straightforward material that would be good. But i have no idea what's the issue with the above code.</p>
","-2","Question"
"79467839","","<p>I'm doing a single layer perceptron where I need to get my model to predict whether the user makes a T or an L using some buttons on a website. rn i'm trying to get the weights and bias so I can implement them into my websites pyscript code since I can't have the website fitting and predicting. However, whenever I try to print the weights they come back rounded to sci notation. I've tried doing repr, but that doesn't work for some reason. Could anyone show me where I'm going wrong?</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import datasets
import matplotlib.pyplot as plt

def activation_function(z):
    return np.where(z &gt;= 0, 1, 0)

#importing the dataset as a pandas dataframe
df = pd.read_csv(&quot;TrainingData.csv&quot;)
df.head()

#split training data into X and Y
X = df.iloc[:, 1:]
print(X)

y = df.iloc[:, 0]
print(y)


#Convert Y labels into integers
y = y.map({'L': 0, 'T': 1})

#Convert X and Y to numpy arrays for later processing
X = X.to_numpy(dtype = np.float64)
y = y.to_numpy(dtype = np.int32)

print(X)
print(y)

print(X.dtype)
print(y.dtype)

#split training data and check shapes. Doing a 70/30 split due to the dataset not being very large (58 samples, 16 features)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#Fitting the model to retreive the weights and biases for the pyscript portion of the code
weights = np.zeros(16)*1
bias = 0

learning_rate = 0.01
    
for _ in range(1000):
    for idx, x_i in enumerate(X_train):
        
        linear_product = np.dot(x_i, weights) + bias 
        
        y_pred = activation_function(linear_product)
        
        loss =  y_pred - y_train[idx]
                
        weights -= learning_rate * loss * x_i
        
        bias -= learning_rate * loss

repr((weights))
repr((bias))
</code></pre>
<p>I'm expecting to get very large decimals assuming my conversions and equations are accurate. I've tried asking friends, prof, and chatGPT and using the repr among other methods to get it to print the correct way.</p>
","0","Question"
"79468013","","<p>I am trying to run DeepSeek locally according to their instructions but it does not work with some silly error (I will show it later).
This is what I am doing.</p>
<ol>
<li>Download the smallest model (3.5GB) from here <a href=""https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"" rel=""nofollow noreferrer"">https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</a></li>
<li>Follow the steps from here: <a href=""https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file#6-how-to-run-locally"" rel=""nofollow noreferrer"">https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file#6-how-to-run-locally</a></li>
</ol>
<p>2.1 Get this project
<a href=""https://github.com/deepseek-ai/DeepSeek-V3.git"" rel=""nofollow noreferrer"">https://github.com/deepseek-ai/DeepSeek-V3.git</a></p>
<p>2.2 Run docker container like this with pre-created volume to put the model</p>
<pre><code>docker run --gpus all -it --name deepseek01 --rm --mount source=deepseekv3,target=/root/deepseekv3 python:3.10-slim bash
</code></pre>
<p>I am using python:3.10-slim because here (<a href=""https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file#6-how-to-run-locally"" rel=""nofollow noreferrer"">https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file#6-how-to-run-locally</a>) it is written
&quot;<em>Linux with Python 3.10 only. Mac and Windows are not supported</em>.&quot;</p>
<p>2.3 Install latest updates
apt-get update</p>
<p>2.4 get this file <a href=""https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/requirements.txt"" rel=""nofollow noreferrer"">https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/requirements.txt</a> and install the requirements</p>
<pre><code>pip install -r requirements.txt
</code></pre>
<p>2.5 Copy the model to the volume mounted to the docker container. these 5 files from here <a href=""https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"" rel=""nofollow noreferrer"">https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</a></p>
<pre><code>config.json
generation_config.json
model.safetensors
tokenizer.json
tokenizer_config.json
</code></pre>
<p>2.6 Convert the model as it is written here <a href=""https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file#model-weights-conversion"" rel=""nofollow noreferrer"">https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file#model-weights-conversion</a> by this command</p>
<pre><code>python convert.py --hf-ckpt-path /root/deepseekv3/source_model --save-path /root/deepseekv3/converted_model --n-experts 256 --model-parallel 16
</code></pre>
<p>In this step (converting the model) I got this error</p>
<pre><code>Traceback (most recent call last):
  File &quot;/root/deepseekv3/inference/convert.py&quot;, line 96, in &lt;module&gt;
    main(args.hf_ckpt_path, args.save_path, args.n_experts, args.model_parallel)
  File &quot;/root/deepseekv3/inference/convert.py&quot;, line 63, in main
    assert key in mapping
AssertionError
</code></pre>
<p>so, basically, the next steps do not make sense, as this is the essential step.</p>
<p>My questions:</p>
<ol>
<li>What I am doing wrong?</li>
<li>There are some videos on YouTube where deepseek was installed with ollama. Is it really required? Should I be able to run it without it like they described here <a href=""https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file#6-how-to-run-locally"" rel=""nofollow noreferrer"">https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file#6-how-to-run-locally</a>?</li>
</ol>
<p>UPDATE 1</p>
<p>In order to debug a bit I added these 2 lines.</p>
<pre><code>print(&quot;Missing key:&quot;, key)
print(&quot;Available keys:&quot;, list(mapping.keys()))
</code></pre>
<p>Missing keys were identified as these:</p>
<pre><code>embed_tokens
input_layernorm
down_proj
gate_proj
up_proj
post_attention_layernorm
k_proj
</code></pre>
<p>Although all of them do exist inside model.safetensors file.</p>
<p>Also, it was mentioned by @Hans Kilian in the comment, that I might put some file, which is not needed into source_model folder.
I checked line 11 in the convert.py and some of the keys there do not exist inside model.safetensors, but logging reports different keys.</p>
","1","Question"
"79469004","","<p>If I understand the process correctly, when scaling test data in machine learning, you should use the scaling parameters (like mean and standard deviation) learned from the training data to transform the test data, not the test data itself.</p>
<p>So correct steps are:</p>
<ol>
<li>Split the data: Divide your dataset into training and test sets.</li>
<li>Scale the training data: Calculate and apply scaling parameters (e.g., mean, standard deviation) to the training data.</li>
<li>Apply the same parameters to the test data</li>
</ol>
<p>To achieve the steps above, I use:</p>
<ol>
<li><code>fit_transform</code> to scale &quot;Training Data&quot;,</li>
<li><code>transform</code> to carry &quot;Training Data&quot; parameters over to scale &quot;Testing Data&quot;</li>
</ol>
<p>However, how do I achieve this in Palantir Foundry Model when I evaluate &quot;Test data&quot;, I don't see an option in Evaluation Configuration. Does anyone know if Palantir pre-build the function in Evaluation Configuration? Do carry over parameters process happen automatically? If not, what should I do to achieve it?</p>
<p><code>fit_transform</code> then <code>transform</code> equivalent in Palantir Foundry Model Training</p>
","1","Question"
"79471261","","<p>I’m using BigQuery ML to train an ARIMA_PLUS model for forecasting CPU usage. The model trains successfully, but when I run ML.EVALUATE, all result values are NULL.</p>
<p><strong>Model Training Query</strong></p>
<pre><code>CREATE OR REPLACE MODEL `project.dataset.arima_model`
OPTIONS(
  MODEL_TYPE = 'ARIMA_PLUS',
  TIME_SERIES_TIMESTAMP_COL = 'timestamp_column',
  TIME_SERIES_ID_COL = ['id_column_1', 'id_column_2'],
  TIME_SERIES_DATA_COL = 'data_column',
  FORECAST_LIMIT_LOWER_BOUND = 0,
  FORECAST_LIMIT_UPPER_BOUND = 100
) AS
SELECT data_column, id_column_1, id_column_2, timestamp_column
FROM `project.dataset.source_table`
WHERE DATE(timestamp_column) BETWEEN '2025-02-5' AND '2025-02-12';
</code></pre>
<p><strong>Evaluation Query</strong></p>
<pre><code>SELECT * 
FROM ML.EVALUATE(
  MODEL `project.dataset.arima_model`,
  (
    SELECT data_column, id_column_1, id_column_2, timestamp_column
    FROM `project.dataset.source_table`
    WHERE DATE(timestamp_column) BETWEEN '2025-02-13' AND '2025-02-20'
  ),
  STRUCT(
    TRUE AS perform_aggregation, 
    10 AS horizon, 
    0.9 AS confidence_level
  )
);
</code></pre>
<p>ML.evaluate run successfully but it return null for all id
<a href=""https://i.sstatic.net/HUMx9iOy.png"" rel=""nofollow noreferrer"">query result</a></p>
","1","Question"
"79471646","","<p>I've done my research about but I'm not satisfied with the answer I looked up both on the documentation and gemini. <code>use_encoded_value</code> what does it mean? Do I have to pass an argument to act as an encoded values? If so could you give an example on its usage?</p>
","0","Question"
"79473125","","<p>I have a combined loss funcation like this:</p>
<pre><code>loss = alpha * loss0 + beta* loss1 + gamma * loss2 + delta* loss3
</code></pre>
<p>I would like to make alpha, beta, gamma, and delta learnable parameters. Notice that alpha, beta, gamma, and delta are outside the nn.Module. How can I do that?</p>
","0","Question"
"79474361","","<p>I wanted to test VotingClassifier from sklearn and comparae performance with different parameters. I used param grid and then I notice something unintelligible.</p>
<p>I prepared three classifiers</p>
<pre class=""lang-py prettyprint-override""><code>gnb = GaussianNB() # Accuracy 0.795
lr = LogisticRegression() # Accuracy 0.7925
rfc = RandomForestClassifier() # Accuracy 0.94

</code></pre>
<p>then I made two VoitingClassifiers. Both have voiting set to &quot;hard&quot; but different weights. The decision is made by majority vote but their accuracy is different, how it is possible?</p>
<pre class=""lang-py prettyprint-override""><code>vc_hard_equals = VotingClassifier(estimators=[
        (&quot;NaiveBayes&quot;, gnb), 
        (&quot;LogisticRegression&quot;, lr), 
        (&quot;RandomForest&quot;, rfc)
    ], 
    voting=&quot;hard&quot;, 
    weights=(1, 1, 1), # Equals weights
    )
vc_hard_forest_priority = VotingClassifier(estimators=[
        (&quot;NaiveBayes&quot;, gnb), 
        (&quot;LogisticRegression&quot;, lr), 
        (&quot;RandomForest&quot;, rfc)], 
    voting=&quot;hard&quot;, 
    weights=(1, 1, 3), # Bigger weight of RandomForest (best model in this case)
    )

vc_hard_equals.fit(X_train, y_train)
vc_hard_forest_priority.fit(X_train, y_train)

print(vc_hard_equals.score(X_test, y_test)) # 0.832
print(vc_hard_forest_priority.score(X_test, y_test)) # 0.915
</code></pre>
","1","Question"
"79474503","","<p>I'm training a simple Keras model with one Dense layer, but when I call model.summary(), it shows 4 parameters instead of 2.</p>
<pre><code>import tensorflow as tf
import numpy as np

# Generate dummy data
X_train = np.random.rand(40)  # Shape (40,)
y_train = np.random.rand(40)

# Define a simple model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])

# Compile the model
model.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=[&quot;mae&quot;])

# Train the model
model.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100)

# Check model summary
model.summary()
</code></pre>
<p>Since Dense(1) should have 1 weight + 1 bias, I expected 2 parameters, not 4.</p>
<p>Why does passing a 1D input (shape=(40,)) cause Keras to use 4 parameters instead of 2?
Is Keras automatically handling the input shape in a way that duplicates the parameters?</p>
<p>this is the result:</p>
<p><a href=""https://i.sstatic.net/5YVb6dHO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5YVb6dHO.png"" alt=""enter image description here"" /></a></p>
","1","Question"
"79477586","","<p>I created a pipeline in Azure ML that trains a model using <strong>Boosted Decision Tree Regression</strong>. From my understanding, the model is saved as <strong><code>data.ilearner</code></strong>.</p>
<p>However, I am unable to convert this model into a <strong><code>model.pkl</code></strong> format that can be loaded using <code>joblib</code>.</p>
<h3><strong>Questions:</strong></h3>
<ol>
<li>How can I create a <code>model.pkl</code> file in <strong>Azure ML</strong> for a Boosted Decision Tree Regression model?</li>
<li>How can I convert <code>data.ilearner</code> to <code>model.pkl</code>?</li>
</ol>
<p><a href=""https://i.sstatic.net/KnoJYUyG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KnoJYUyG.png"" alt=""enter image description here"" /></a></p>
<p>I attempted to use the following Python script to load and convert the model:</p>
<pre class=""lang-py prettyprint-override""><code>import lightgbm as lgb
import joblib

# Load the LightGBM model
model = lgb.Booster(model_file=&quot;data.ilearner&quot;)

# Save as a Pickle file
joblib.dump(model, &quot;model.pkl&quot;) 
</code></pre>
<p>But when running the script, I get the following error:</p>
<pre><code>% python3 convert_to_model_pkl.py 
[LightGBM] [Fatal] Unknown model format or submodel type in model file data.ilearner
Traceback (most recent call last):
  File &quot;/Users/tomasz.olchawa/ng/ml/convert_to_model_pkl.py&quot;, line 5, in &lt;module&gt;
    model = lgb.Booster(model_file=&quot;data.ilearner&quot;)
  File &quot;/Users/tomasz.olchawa/ng/ml/myenv/lib/python3.13/site-packages/lightgbm/basic.py&quot;, line 3697, in __init__
    _safe_call(
    ~~~~~~~~~~^
        _LIB.LGBM_BoosterCreateFromModelfile(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...&lt;3 lines&gt;...
        )
        ^
    )
    ^
  File &quot;/Users/../ng/ml/myenv/lib/python3.13/site-packages/lightgbm/basic.py&quot;, line 313, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode(&quot;utf-8&quot;))
lightgbm.basic.LightGBMError: Unknown model format or submodel type in model file data.ilearner
</code></pre>
","0","Question"
"79478755","","<p>In matlab, the function <code>fitcsvm</code> trains a support vector machine.
In output, there is a component <code>BoxConstraints</code>. I have read the post <a href=""https://stackoverflow.com/questions/31161075/svm-in-matlab-meaning-of-parameter-box-constraint-in-function-fitcsvm"">this post</a>
and understood the meaning of box constraint.</p>
<p>But in the answer of the post, the box constraint C is a scalar, while in the output of Matlab it is a vector with same length of number of samples, and it is all 1, where I have used default box constraint 1 in the input.</p>
<p>I do not understand what is the output.</p>
<p>(I have read the sections of SVM on <em>the Element of Statistical Learning</em>, and did not find this term. So I think perhaps it is different choice of term. If you would like to answer, you could skip explaining basic concepts of SVM.)</p>
","1","Question"
"79485691","","<p>Why this quite simple example of XGBoost ML produces all-nulls even on input, that's equivalent to training data? This looks like a trivial case of input which should not require any fine tuning of ML, but even if I tweak hyperparams for ML (max_depth, eta and so on) nothing changes.</p>
<pre><code>import pandas as pd
import xgboost as xgb

X = pd.DataFrame(([[0], [1], [2], [3], [4], [5]]), columns=['x'])
y = pd.DataFrame([0, 1, 0, 1, 0, 1], columns=['y'])

model = xgb.XGBClassifier()
model.fit(X, y)
print(model.predict([[0], [1], [2], [3], [4], [5]]))

[0 0 0 0 0 0]
</code></pre>
","0","Question"
"79486105","","<p>I've been trying to train a language model (text classification), our lab has two GPUs, a 4090 and a 3090. However, I encountered a perplexing phenomenon during training: the model's performance varies slightly when using different GPUs (or different GPU combinations). If the difference were negligible, I might not be concerned. But I observed that the performance gap between using a single 4090 and a single 3090 reaches around 5%, which makes me feel that this is an issue worth addressing. Why does this happen and how to resolve it? Below is the terminal output for reference:</p>
<p><strong>Using single RTX 3090:</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=1 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 153.9599, 'train_samples_per_second': 14.134, 'train_steps_per_second': 1.767, 'train_loss': 0.38275421366972084, 'epoch': 8.0}

Evaluation loss: 0.4403

Model Performance:
accuracy: 0.7606
f1_score: 0.7619
precision: 0.7634
recall: 0.7606
</code></pre>
<p><strong>Using single RTX 4090:</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=0 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 143.1252, 'train_samples_per_second': 15.203, 'train_steps_per_second': 1.9, 'train_loss': 0.3407774532542509, 'epoch': 8.0}

Evaluation loss: 0.3961

Model Performance:
accuracy: 0.8169
f1_score: 0.8103
precision: 0.8132
recall: 0.8169
</code></pre>
<p><strong>Using RTX 4090 + RTX 3090:</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=0,1 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 195.1275, 'train_samples_per_second': 11.152, 'train_steps_per_second': 0.697, 'train_loss': 0.43211137547212486, 'epoch': 8.0}

Evaluation loss: 0.3961

Model Performance:
accuracy: 0.8028
f1_score: 0.8064
precision: 0.8152
recall: 0.8028

</code></pre>
<p><strong>GPU info:</strong></p>
<pre><code>$ nvidia-smi
Wed Mar  5 17:33:02 2025
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.56.06    Driver Version: 520.56.06    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  Off |
|  0%   44C    P8    19W / 450W |     59MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:04:00.0 Off |                  N/A |
|  0%   46C    P8    28W / 350W |      5MiB / 24576MiB |      1%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
</code></pre>
<p><strong>What I have tried:</strong></p>
<p>✅ Ensured that random_seed is set to a fixed value. When using the same GPU, the model's performance remains consistent across runs, so I believe this is not a random_seed issue (e.g., <code>torch.backends.cudnn.deterministic = True</code>).</p>
<p>✅ Tried different servers. I tested on two different servers (one of them has eight 2080 Ti GPUs), but even with GPUs of the same model (but different CUDA indices), I observed similar issues.</p>
<p><strong>Using single RTX 2080ti(cuda:1):</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=1 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 157.7439, 'train_samples_per_second': 13.795, 'train_steps_per_second': 1.724, 'train_loss': 0.3536217072430779, 'epoch': 8.0}

Evaluation loss: 0.3865

Model Performance:
accuracy: 0.7887
f1_score: 0.7951
precision: 0.8265
recall: 0.7887
</code></pre>
<p><strong>Using single RTX 2080ti(cuda:0):</strong></p>
<pre><code>$ CUDA_VISIBLE_DEVICES=0 python scripts/train.py --config configs/model_config.yaml
All random seeds have been set to: 323

Dataset Statistics:
total_samples: 340
avg_text_length: 1445.3764705882354

Loading emilyalsentzer/Bio_ClinicalBERT...
Starting training on cuda...
{'train_runtime': 158.6497, 'train_samples_per_second': 13.716, 'train_steps_per_second': 1.714, 'train_loss': 0.36603568581973805, 'epoch': 8.0}

Evaluation loss: 0.4049

Model Performance:
accuracy: 0.8028
f1_score: 0.8028
precision: 0.8028
recall: 0.8028
</code></pre>
<pre><code>$ nvidia-smi
Wed Mar  5 18:02:33 2025
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti      Off| 00000000:1D:00.0 Off |                  N/A |
| 34%   51C    P2               72W / 250W|   4821MiB / 11264MiB |      4%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 2080 Ti      Off| 00000000:1E:00.0 Off |                  N/A |
| 28%   46C    P2               90W / 250W|   3795MiB / 11264MiB |      5%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+--------------------
...(ignore)
</code></pre>
","1","Question"
"79490841","","<p>I am training an <strong>XGBClassifier model using Snowflake ML</strong> and attempting to register it using <strong>Snowflake’s model registry</strong>. The training and evaluation steps complete successfully, but when I try to log the model using <code>log_model()</code>, I get the following error:</p>
<pre><code>Training Accuracy: 0.9610
Evaluation Accuracy: 0.8730
Traceback (most recent call last):
  File &quot;C:\Mas\rla_projects\Claims-AI--lodgement\code\python\src\explain\train_model.py&quot;, line 56, in &lt;module&gt;
    model_version = native_registry.log_model(
  File &quot;C:\Mas\py39_sf_ml\lib\site-packages\snowflake\ml\_internal\telemetry.py&quot;, line 542, in wrap
    return ctx.run(execute_func_with_statement_params)
  ...
  File &quot;C:\Mas\py39_sf_ml\lib\site-packages\snowflake\ml\model\_packager\model_task\model_task_utils.py&quot;, line 149, in _get_model_task    
    raise ValueError(f&quot;Model type {type(model)} is not supported&quot;)
ValueError: Model type &lt;class 'NoneType'&gt; is not supported
</code></pre>
<p>Code Snippet:</p>
<pre class=""lang-py prettyprint-override""><code>from snowflake.snowpark import Session
from snowflake.ml.registry import registry
from snowflake.ml.modeling.preprocessing import StandardScaler
from snowflake.ml.modeling.impute import SimpleImputer
from snowflake.ml.modeling.pipeline import Pipeline
from snowflake.ml.modeling.xgboost import XGBClassifier

# Snowflake connection parameters
conn_params = {
    &quot;user&quot;: &quot;&lt;...&gt;&quot;,
    &quot;account&quot;: &quot;&lt;...&gt;&quot;,
    &quot;warehouse&quot;: &quot;&lt;...&gt;&quot;,
    &quot;database&quot;: &quot;&lt;...&gt;&quot;,
    &quot;schema&quot;: &quot;&lt;...&gt;&quot;,
    &quot;role&quot;: &quot;&lt;...&gt;&quot;,
    &quot;authenticator&quot;: &quot;externalbrowser&quot;,
}

# Create session
session = Session.builder.configs(conn_params).create()

# Load and prepare data
all_data = session.sql(&quot;SELECT *, IFF(CLASS = 'g', 1.0, 0.0) AS LABEL FROM Gamma_Telescope_Data&quot;).drop(&quot;CLASS&quot;)
train_data, test_data = all_data.random_split(weights=[0.9, 0.1], seed=0)

# Define feature and label columns
FEATURE_COLS = [c for c in train_data.columns if c != &quot;LABEL&quot;]
LABEL_COLS = [&quot;LABEL&quot;]

# Construct pipeline
pipeline = Pipeline(steps=[
    (&quot;impute&quot;, SimpleImputer(input_cols=FEATURE_COLS, output_cols=FEATURE_COLS)),
    (&quot;scaler&quot;, StandardScaler(input_cols=FEATURE_COLS, output_cols=FEATURE_COLS)),
    (&quot;model&quot;, XGBClassifier(input_cols=FEATURE_COLS, label_cols=LABEL_COLS))
])

# Train the pipeline
pipeline.fit(train_data)

# Register model
native_registry = registry.Registry(
    session=session, 
    database_name=session.get_current_database(), 
    schema_name=session.get_current_schema()
)

model_name = &quot;Gamma_test&quot;
version = &quot;V8&quot;

model_version = native_registry.log_model(
    model=pipeline,  # &lt;-- This line triggers the error
    model_name=model_name,
    version_name=version,
    sample_input_data=test_data,
    comment=&quot;Gamma test&quot;,
    conda_dependencies=[&quot;snowflake-ml-python==1.7.4&quot;, &quot;snowflake-snowpark-python==1.28.0&quot;],
    options={&quot;enable_explainability&quot;: True}
)
</code></pre>
<h3><strong>Observations &amp; Debugging Attempts:</strong></h3>
<ol>
<li><strong>Pipeline Training Works</strong> – <code>pipeline.fit(train_data)</code> runs without errors.</li>
<li><strong>Pipeline Predictions Work</strong> – Predictions on training and test data succeed.</li>
<li><strong>Model Explanation Works Without Pipeline</strong> – If I train an <code>XGBClassifier</code> <strong>without a pipeline</strong>, I can successfully generate predictions and explanations.</li>
<li><strong>Session is Active</strong> – <code>session.get_current_database()</code> and <code>session.get_current_schema()</code> return valid values.</li>
<li><strong>Feature &amp; Label Columns Look Correct</strong> – <code>FEATURE_COLS</code> and <code>LABEL_COLS</code> contain expected values.</li>
</ol>
<h3><strong>Additional Context:</strong></h3>
<ul>
<li>Environment:
<ul>
<li>Win 10</li>
<li>Python 3.9</li>
<li>snowflake-connector-python 3.14.0</li>
<li>snowflake-ml-python        1.7.4</li>
<li>snowflake-snowpark-python  1.28.0</li>
</ul>
</li>
<li>This example is based on <strong>Snowflake’s official documentation</strong>:<br />
<strong><a href=""https://docs.snowflake.com/en/developer-guide/snowflake-ml/modeling#feature-preprocessing-and-training-on-non-synthetic-data"" rel=""nofollow noreferrer"">Feature Preprocessing and Training on Non-Synthetic Data</a></strong></li>
<li>The documentation suggests using a <code>Pipeline</code>, but it does not provide an example of registering a <code>Pipeline</code>-based model that has explainability.</li>
<li>The error message suggests that the model is somehow being treated as <code>NoneType</code> when passed to <code>log_model()</code>.</li>
<li>As mentioned the whole pipeline can be registered and retrieved when I remove this <code>options={&quot;enable_explainability&quot;: True}</code> from <code>.log_model</code>.</li>
<li>This <code>options={&quot;enable_explainability&quot;: True}</code> works if I don't have a pipeline: <a href=""https://stackoverflow.com/questions/79488423/snowflake-ml-there-is-no-method-with-name-explain-available-in-the-model"">Snowflake ML: `There is no method with name explain available in the model`</a></li>
</ul>
<h3><strong>Question:</strong></h3>
<p>Is it possible to register a pipeline with <code>options={&quot;enable_explainability&quot;: True}</code>?</p>
","0","Question"
"79492869","","<p>I'm trying to submit a training job to AWS SageMaker AI API via GitHub using the HuggingFace estimator. However, every time I try to create the job, the estimator creates an object of type <strong>None</strong> and the script hangs up indefinitely. It never finishes executing and a training job is never even submitted to AWS SageMaker, as both the UI and the CLI show no training jobs when I check them.</p>
<p>Here is the code I'm using, with sensitive information removed:</p>
<pre><code># Import modules
import sagemaker
from sagemaker.huggingface import HuggingFace
import logging

# Troubleshoot errors
logging.getLogger(&quot;sagemaker&quot;).setLevel(logging.DEBUG)

# Create session
session = sagemaker.Session()

# Config AWS role
role = [AWS ARN]

# Config various login info
git_config = {
    [REPOSITORY LOGIN INFO]
}
hf_token = [HUGGINGFACE LOGIN INFO]

# Configure training container
print(&quot;Creating estimator object...&quot;)
estimator = HuggingFace(
    role=role,
    session=session,
    instance_count=1,
    source_dir=&quot;jobs&quot;,  # Subdirectory in repo
    py_version='py310',
    git_config=git_config,
    entry_point=&quot;train.py&quot;,  # Name of training script in GitHub
    requirements_file=&quot;requirements.txt&quot;,  # Additional packages to install
    output_path='s3://llama-data/model-output/',  # Output of model checkpoints
    instance_type=&quot;ml.p3.2xlarge&quot;,
    environment={
        &quot;HF_TOKEN&quot;: hf_token
    },
    enable_sagemaker_metrics=True,  # Log in CloudWatch
    image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:2.1.0-transformers4.36.0-gpu-py310-cu121-ubuntu20.04',  # Image to pull
    hyperparameters=None,  # All hyperparams declared in train.py
    training_repository_access_mode='Platform'  # Docker image is in Amazon ECR
)
print(&quot;Estimator object created successfully&quot;)

# Configure training/validation data
train_input = sagemaker.inputs.TrainingInput(
    s3_data=&quot;s3://llama-data/train/&quot;,
    content_type=&quot;text/csv&quot;
)

validation_input = sagemaker.inputs.TrainingInput(
    s3_data=&quot;s3://llama-data/validate/&quot;,
    content_type=&quot;text/csv&quot;
)

# Debugging: Print the SageMaker training request before submission
# request_dict = estimator.latest_training_job.describe_request()
print(&quot;SageMaker Training Job Request:&quot;, estimator.latest_training_job)  # **THIS ALWAYS PRINTS 'NONE', NO REQUEST IS SUBMITTED**

print(&quot;Calling .fit()...&quot;)
estimator.fit({  # **SCRIPT STOPS WORKING AROUND THIS POINT**
    &quot;train&quot;: train_input,
    &quot;validate&quot;: validation_input
}, wait=True, job_name=&quot;train-llama-small&quot;, logs=&quot;All&quot;)
print(&quot;Training job created successfully. Exiting script.&quot;)
</code></pre>
<p>After the estimator is created I try to list the most recent training job, and this is printed:</p>
<pre><code>Estimator object created successfully
SageMaker Training Job Request: None
</code></pre>
<p>When the script hangs up, this is the message I see in my terminal:</p>
<pre><code>Your branch is up to date with 'origin/main'.
[03/07/25 09:31:21] DEBUG    Using provided s3_resource      fw_utils.py:474
[03/07/25 09:31:22] DEBUG    Train args after processing   estimator.py:2513
                             defaults:
</code></pre>
<p>And then it lists all of the training arguments for the session. I've tried pulling a different Docker image to see if that would change anything, with no effect.</p>
<p>What am I doing wrong that the training job is never actually created?</p>
","0","Question"
"79494100","","<p>I have one set of weights, one tokenizer, the same prompt, and identical generation parameters. Yet somehow, when I load the model using AutoModelForCausalLM, I get one output, and when I construct it manually with LlamaForCausalLM plus the same config and state_dict, I get another output entirely.</p>
<p>This code can show the difference on both a6000 and a100.</p>
<pre><code>import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LlamaForCausalLM,
    LlamaConfig
)

# 1) Adjust these as needed
model_name = &quot;meta-llama/Llama-3.1-8B&quot;
prompt = &quot;Hello from Llama 3.1! Tell me something interesting.&quot;
dtype = torch.float16  # or torch.float32 if needed

# 2) Get the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)

# Prepare input
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)

############################################
# A) Load with AutoModelForCausalLM
############################################

print(&quot;=== Loading with AutoModelForCausalLM ===&quot;)

model_auto = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,  # matches your usage
    torch_dtype=dtype
).cuda()
model_auto.eval()  # turn off dropout
config = model_auto.config
with torch.no_grad():
    out_auto = model_auto(**inputs)
logits_auto = out_auto.logits  # shape: [batch_size, seq_len, vocab_size]

del model_auto
torch.cuda.empty_cache()

############################################
# B) Load with LlamaForCausalLM + config
############################################

print(&quot;=== Loading with LlamaForCausalLM + config ===&quot;)

# Get config from the same checkpoint
# Build Llama model directly
model_llama = LlamaForCausalLM(config).cuda()
model_llama.eval()

# Load the same weights that AutoModelForCausalLM used
model_auto_temp = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype)
model_llama.load_state_dict(model_auto_temp.state_dict())
del model_auto_temp
torch.cuda.empty_cache()

with torch.no_grad():
    out_llama = model_llama(**inputs)
logits_llama = out_llama.logits

############################################
# C) Compare the Logits
############################################

# Compute maximum absolute difference
max_diff = (logits_auto - logits_llama).abs().max()
print(f&quot;\nMax absolute difference between logits: {max_diff.item()}&quot;)

if max_diff &lt; 1e-7:
    print(&quot;→ The logits are effectively identical (within floating-point precision).&quot;)
else:
    print(&quot;→ There is a non-trivial difference in logits!&quot;)
</code></pre>
","1","Question"
"79494728","","<p>I am trying to deconstruct the SD3.5 (specifically 3.5 medium) pipeline in order to have a controlled process over the denoising steps. I can't do callbacks because I need to modify the latent according to other pipelines.</p>
<p>I am trying to perform the steps on the following huggingface guide:
<a href=""https://huggingface.co/docs/diffusers/en/using-diffusers/write_own_pipeline#deconstruct-the-stable-diffusion-pipeline"" rel=""nofollow noreferrer"">https://huggingface.co/docs/diffusers/en/using-diffusers/write_own_pipeline#deconstruct-the-stable-diffusion-pipeline</a></p>
<p>I modified my text encoding to fit SD3.5, I also tried to load the entire pipeline and just run the encode_prompt function on it to get the text embedding and pooled embeddings for both the prompt and negative prompt. When running the function and putting its outputs as input to the regular pipeline instead of the prompt and negative prompt it works properly so it seems like this is not what's causing the problem.</p>
<p>I also changed the unet from the article to use the pre-trained transformer of the model. After that I adjusted the decoding to match the same decoding on the pipeline's source code on diffusers.</p>
<p>the output images don't look same as they are looking when running the pipeline through diffusers. I'm not sure where I can find a similar implementation to deconstruction of the SD3 pipeline or what am I missing.</p>
<p><a href=""https://i.sstatic.net/xyW0hOiI.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","-1","Question"
"79495211","","<p>I installed PyTorch using <code>pip install torch</code>, but when I try to import it, I get the following error:</p>
<pre><code>OSError                                   Traceback (most recent call last)  
Cell In[1], line 1  
----&gt; 1 import torch  
      2 print(torch.__version__)  
      3 print(torch.version.cuda) 

File &quot;\.venv\Lib\site-packages\torch\__init__.py&quot;, line 274  
    270                     raise err  
    272         kernel32.SetErrorMode(prev_error_mode)  
--&gt; 274     _load_dll_libraries()  
    275     del _load_dll_libraries


File &quot;\.venv\Lib\site-packages\torch\__init__.py&quot;, line 257, in _load_dll_libraries  
    253     err = ctypes.WinError(last_error)  
    254     err.strerror += (  
    255         f' Error loading &quot;{dll}&quot; or one of its dependencies.'  
    256     )  
--&gt; 257     raise err  
    258 elif res is not None:  
    259     is_loaded = True 

OSError: [WinError 127] The specified procedure could not be found. Error loading &quot;\.venv\Lib\site-packages\torch\lib\torch_python.dll&quot; or one of its dependencies.
</code></pre>
<p><strong>My System Configuration:</strong> <br>
OS: Windows  11 <br>
Python Version: 3.11 (pyenv) <br>
PyTorch Version: 2.6.0 <br>
CUDA Toolkit Installed: 11.8 <br>
Installation Command Used: pip install torch <br>
Virtual Environment: Yes (venv)</p>
<p><strong>Tried Different PyTorch Installation</strong></p>
<p>Installed using the recommended command from the official PyTorch
website:</p>
<pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
</code></pre>
<p><a href=""https://stackoverflow.com/questions/69958526/oserror-winerror-127-the-specified-procedure-could-not-be-found"">OSError: [WinError 127] The specified procedure could not be found</a> suggests to create a conda environment, but is there a way to get it working without installing conda.</p>
","0","Question"
"79495542","","<p>I've trained a ResNet50 on v2-8 TPU accelerator using google colab, I've fed it with 5000 images of shape (224, 224, 3). I've normalized them, no nans, no inf, no class imbalance and everything is alright:</p>
<pre><code>INPUT_SHAPE = (224, 224, 3)

with strategy.scope():
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)
    base_model.trainable = False
    model = tf.keras.models.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(1024, activation='relu'),
        tf.keras.layers.Dense(6, activation='sigmoid') 
    ])
    model.compile(optimizer='adam', 
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    
model.fit(
    X_train, 
    y_train, 
    epochs=10, 
    validation_data=(X_val, y_val),
    batch_size=32
  )
</code></pre>
<p>When I train on TPU, accuracy and loss become NaN during training. When I switch to CPU, everything works fine.</p>
<p>Why is this happening and how to fix it?</p>
<p>I tried training the model on both TPU and CPU in Google Colab. I expected the model to train without any issues, including NaN values for loss or accuracy, especially since the training works fine on the CPU. However, when using the TPU, I encountered NaN values for both accuracy and loss. I also verified that the data is clean, with no NaN, infinity, or imbalance issues, and ensured that the model compilation and training setup were correct.</p>
","-1","Question"
"79498849","","<p>I have build an XGBoost multiclass classification model using mlr and i want to visualize the partial dependence for some features. However, if i try to do so using <code>generatePartialDependenceData()</code> i get the following error:</p>
<blockquote>
<p>Error in melt.data.table(as.data.table(out), measure.vars = target, variable.name = if (td$type ==  : One or more values in 'measure.vars' is invalid.</p>
</blockquote>
<p>I have checked for discrepancies between the <code>task.desc</code> in the <code>Task</code> object and the <code>factor.levels</code> in the <code>WrappedModel</code> object, but everything seems fine. Additionally, i have no trouble generating the data for a regression XGBoost with a different target variable using the same function.
Is there a problem on my end, or is this a bug?</p>
<p>Here is an example using the <code>palmerpenguins</code> dataset:</p>
<pre><code># library
library(tidyverse)
library(caret)
library(mlr)

peng &lt;- palmerpenguins::penguins

# data partition
set.seed(1234)
inTrain &lt;- createDataPartition(
  y = peng$species,
  p = 0.7,
  list = F
)

# build task
train_class &lt;- peng[inTrain,] %&gt;% select(-sex, -year) %&gt;% 
  createDummyFeatures(target = &quot;species&quot;, cols = &quot;island&quot;) %&gt;% 
  makeClassifTask(data = ., target = &quot;species&quot;)

# build learners
xgb_class_learner &lt;- makeLearner(
  &quot;classif.xgboost&quot;,
  predict.type = &quot;response&quot;
)

# build model
xgb_class &lt;- train(xgb_class_learner, train_class)

# generate partial dependence
generatePartialDependenceData(xgb_class, train_class)
</code></pre>
","-1","Question"
"79500227","","<p>I am trying to build a conversational chatbot using VS Code. The code is following:</p>
<pre><code>## RAG Q&amp;A Conversation With PDF Including Chat History
import streamlit as st
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_groq import ChatGroq
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
import os

from dotenv import load_dotenv
load_dotenv()

os.environ['HF_TOKEN']=os.getenv(&quot;HF_TOKEN&quot;)
embeddings=HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)


## set up Streamlit 
st.title(&quot;Conversational RAG With PDF uplaods and chat history&quot;)
st.write(&quot;Upload Pdf's and chat with their content&quot;)

## Input the Groq API Key
api_key=st.text_input(&quot;Enter your Groq API key:&quot;,type=&quot;password&quot;)

## Check if groq api key is provided
if api_key:
    llm=ChatGroq(groq_api_key=api_key,model_name=&quot;Gemma2-9b-It&quot;)

    ## chat interface

    session_id=st.text_input(&quot;Session ID&quot;,value=&quot;default_session&quot;)
    ## statefully manage chat history

    if 'store' not in st.session_state:
        st.session_state.store={}

    uploaded_files=st.file_uploader(&quot;Choose A PDf file&quot;,type=&quot;pdf&quot;,accept_multiple_files=True)
    ## Process uploaded  PDF's
    if uploaded_files:
        documents=[]
        for uploaded_file in uploaded_files:
            temppdf=f&quot;./temp.pdf&quot;
            with open(temppdf,&quot;wb&quot;) as file:
                file.write(uploaded_file.getvalue())
                file_name=uploaded_file.name

            loader=PyPDFLoader(temppdf)
            docs=loader.load()
            documents.extend(docs)

    # Split and create embeddings for the documents
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
        splits = text_splitter.split_documents(documents)
        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        retriever = vectorstore.as_retriever()    

        contextualize_q_system_prompt=(
            &quot;Given a chat history and the latest user question&quot;
            &quot;which might reference context in the chat history, &quot;
            &quot;formulate a standalone question which can be understood &quot;
            &quot;without the chat history. Do NOT answer the question, &quot;
            &quot;just reformulate it if needed and otherwise return it as is.&quot;
        )
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
                [
                    (&quot;system&quot;, contextualize_q_system_prompt),
                    MessagesPlaceholder(&quot;chat_history&quot;),
                    (&quot;human&quot;, &quot;{input}&quot;),
                ]
            )
        
        history_aware_retriever=create_history_aware_retriever(llm,retriever,contextualize_q_prompt)

        ## Answer question

        # Answer question
        system_prompt = (
                &quot;You are an assistant for question-answering tasks. &quot;
                &quot;Use the following pieces of retrieved context to answer &quot;
                &quot;the question. If you don't know the answer, say that you &quot;
                &quot;don't know. Use three sentences maximum and keep the &quot;
                &quot;answer concise.&quot;
                &quot;\n\n&quot;
                &quot;{context}&quot;
            )
        qa_prompt = ChatPromptTemplate.from_messages(
                [
                    (&quot;system&quot;, system_prompt),
                    MessagesPlaceholder(&quot;chat_history&quot;),
                    (&quot;human&quot;, &quot;{input}&quot;),
                ]
            )
        
        question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)
        rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)

        def get_session_history(session:str)-&gt;BaseChatMessageHistory:
            if session_id not in st.session_state.store:
                st.session_state.store[session_id]=ChatMessageHistory()
            return st.session_state.store[session_id]
        
        conversational_rag_chain=RunnableWithMessageHistory(
            rag_chain,get_session_history,
            input_messages_key=&quot;input&quot;,
            history_messages_key=&quot;chat_history&quot;,
            output_messages_key=&quot;answer&quot;
        )

        user_input = st.text_input(&quot;Your question:&quot;)
        if user_input:
            session_history=get_session_history(session_id)
            response = conversational_rag_chain.invoke(
                {&quot;input&quot;: user_input},
                config={
                    &quot;configurable&quot;: {&quot;session_id&quot;:session_id}
                },  # constructs a key &quot;abc123&quot; in `store`.
            )
            st.write(st.session_state.store)
            st.write(&quot;Assistant:&quot;, response['answer'])
            st.write(&quot;Chat History:&quot;, session_history.messages)
else:
    st.warning(&quot;Please enter the GRoq API Key&quot;)
</code></pre>
<p>Following is the error I am getting on my vs code terminal:</p>
<pre><code> Examining the path of torch.classes raised:
Traceback (most recent call last):
  File &quot;D:\projects\PDF_chatbot\venv\lib\site-packages\streamlit\web\bootstrap.py&quot;, line 345, in run
    if asyncio.get_running_loop().is_running():
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;D:\projects\PDF_chatbot\venv\lib\site-packages\streamlit\watcher\local_sources_watcher.py&quot;, line 217, in get_module_paths
    potential_paths = extract_paths(module)
  File &quot;D:\projects\PDF_chatbot\venv\lib\site-packages\streamlit\watcher\local_sources_watcher.py&quot;, line 210, in &lt;lambda&gt;
    lambda m: list(m.__path__._path),
  File &quot;D:\projects\PDF_chatbot\venv\lib\site-packages\torch\_classes.py&quot;, line 13, in __getattr__
    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)
RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_
</code></pre>
<p>I tried reinstalling the requirements but its not working.
Following is the image of error I am getting on streamlit UI:
<a href=""https://i.sstatic.net/Tp1AlLbJ.png"" rel=""nofollow noreferrer"">Error on the streamlit webapp after uploading pdf and asking any question</a></p>
","-1","Question"
"79500324","","<p>I'm training a custom model using a script in Amazon SageMaker and launching the job with the Python SDK. I want to pass some environment variables (like API keys or config flags) to the training job so they’re accessible inside the script via os.environ.</p>
<p>Here’s a simplified version of my code:</p>
<pre><code>from sagemaker.estimator import Estimator

estimator = Estimator(
    image_uri='123456789012.dkr.ecr.us-west-2.amazonaws.com/my-custom-image:latest',
    role=role,
    instance_count=1,
    instance_type='ml.g5.xlarge',
    entry_point='train.py',
    source_dir='src',
    environment={
        'MY_API_KEY': 'abcdef123456',
        'DEBUG_MODE': 'true'
    }
)
</code></pre>
<p>In my training script, I try to read the variable:</p>
<pre><code>import os

api_key = os.environ.get('MY_API_KEY')
print(&quot;API Key:&quot;, api_key)
</code></pre>
<p>Is this the correct way to pass environment variables to a SageMaker training job using the Python SDK? Are there any limitations or best practices I should be aware of, especially for sensitive information like API keys?</p>
","2","Question"
"79502752","","<p>I am using Ubuntu 24.04.1 on an AWS EC2 instance g5.8xlarge.</p>
<p>I am receiving the following error message:</p>
<pre><code>OutOfMemoryError: Allocation on device 
</code></pre>
<p><strong>Code:</strong></p>
<pre><code>import os
os.environ[&quot;PYTORCH_CUDA_ALLOC_CONF&quot;] = &quot;backend:cudaMallocAsync&quot;
import torch
torch.cuda.empty_cache()
import transformers
    
if torch.cuda.is_available():
    torch.set_default_device(&quot;cuda&quot;)
    
device = torch.device(&quot;cuda&quot;)
    
model = transformers.AutoModelForCausalLM.from_pretrained(&quot;microsoft/Orca-2-13b&quot;, device_map=device)
</code></pre>
<p><strong>Full error:</strong></p>
<pre><code>/home/ubuntu/anaconda3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML
  warnings.warn(&quot;Can't initialize NVML&quot;)

Loading checkpoint shards:  33%
 2/6 [00:04&lt;00:06,  1.72s/it]

/home/ubuntu/anaconda3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML
  warnings.warn(&quot;Can't initialize NVML&quot;)

---------------------------------------------------------------------------
OutOfMemoryError                          Traceback (most recent call last)
Cell In[5], line 6
      2     torch.set_default_device(&quot;cuda&quot;)
      4 device = torch.device(&quot;cuda&quot;)
----&gt; 6 model = transformers.AutoModelForCausalLM.from_pretrained(&quot;microsoft/Orca-2-13b&quot;, device_map=device)
      8 # https://github.com/huggingface/transformers/issues/27132
      9 # please use the slow tokenizer since fast and slow tokenizer produces different tokens
     10 tokenizer = transformers.AutoTokenizer.from_pretrained(
     11         &quot;microsoft/Orca-2-13b&quot;,
     12         use_fast=True,
     13     )

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    562 elif type(config) in cls._model_mapping.keys():
    563     model_class = _get_model_class(config, cls._model_mapping)
--&gt; 564     return model_class.from_pretrained(
    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    566     )
    567 raise ValueError(
    568     f&quot;Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n&quot;
    569     f&quot;Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.&quot;
    570 )

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:262, in restore_default_torch_dtype.&lt;locals&gt;._wrapper(*args, **kwargs)
    260 old_dtype = torch.get_default_dtype()
    261 try:
--&gt; 262     return func(*args, **kwargs)
    263 finally:
    264     torch.set_default_dtype(old_dtype)

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4319, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)
   4309     if dtype_orig is not None:
   4310         torch.set_default_dtype(dtype_orig)
   4312     (
   4313         model,
   4314         missing_keys,
   4315         unexpected_keys,
   4316         mismatched_keys,
   4317         offload_index,
   4318         error_msgs,
-&gt; 4319     ) = cls._load_pretrained_model(
   4320         model,
   4321         state_dict,
   4322         loaded_state_dict_keys,  # XXX: rename?
   4323         resolved_archive_file,
   4324         pretrained_model_name_or_path,
   4325         ignore_mismatched_sizes=ignore_mismatched_sizes,
   4326         sharded_metadata=sharded_metadata,
   4327         _fast_init=_fast_init,
   4328         low_cpu_mem_usage=low_cpu_mem_usage,
   4329         device_map=device_map,
   4330         offload_folder=offload_folder,
   4331         offload_state_dict=offload_state_dict,
   4332         dtype=torch_dtype,
   4333         hf_quantizer=hf_quantizer,
   4334         keep_in_fp32_modules=keep_in_fp32_modules,
   4335         gguf_path=gguf_path,
   4336         weights_only=weights_only,
   4337     )
   4339 # make sure token embedding weights are still tied if needed
   4340 model.tie_weights()

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4897, in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)
   4895     else:
   4896         fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)
-&gt; 4897         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
   4898             model_to_load,
   4899             fixed_state_dict,
   4900             start_prefix,
   4901             expected_keys,
   4902             device_map=device_map,
   4903             offload_folder=offload_folder,
   4904             offload_index=offload_index,
   4905             state_dict_folder=state_dict_folder,
   4906             state_dict_index=state_dict_index,
   4907             dtype=dtype,
   4908             hf_quantizer=hf_quantizer,
   4909             is_safetensors=is_safetensors,
   4910             keep_in_fp32_modules=keep_in_fp32_modules,
   4911             unexpected_keys=unexpected_keys,
   4912         )
   4913         error_msgs += new_error_msgs
   4914 else:
   4915     # Sharded checkpoint or whole but low_cpu_mem_usage==True

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:896, in _load_state_dict_into_meta_model(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)
    893         param_device = &quot;cpu&quot; if is_local_dist_rank_0() else &quot;meta&quot;
    895     # For backward compatibility with older versions of `accelerate` and for non-quantized params
--&gt; 896     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
    897 else:
    898     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/accelerate/utils/modeling.py:330, in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)
    328             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)
    329 elif isinstance(value, torch.Tensor):
--&gt; 330     new_value = value.to(device)
    331 else:
    332     new_value = torch.tensor(value, device=device)

File ~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:104, in DeviceContext.__torch_function__(self, func, types, args, kwargs)
    102 if func in _device_constructors() and kwargs.get('device') is None:
    103     kwargs['device'] = self.device
--&gt; 104 return func(*args, **kwargs)

OutOfMemoryError: Allocation on device 
</code></pre>
","0","Question"
"79504547","","<p>I'm building a Variational auto encoder (VAE) with tfjs.
For now I'm only exploring with the fashionMNIST dataset and a simple model as follows:</p>
<pre><code>input layer (28*28*1)
flatten
intermediate_1 (dense 50 units - relu)
mean (dense 10 units - relu) // logVar (dense 10 units relu)
SampleLayer (10 units)
intermediate_2 (dense 50 units relu)
reconstructed (dense 784 units - sigmoid)
reshape (28*28*1) =&gt; loss=MSE
</code></pre>
<p>I created a custom sampling layer extending tf.layers.Layer as below. It's different from other examples that I could find online because I'm using td.addLoss() to add the KL loss function in the layer itself.</p>
<pre><code>class wb_sampling extends tf.layers.Layer {
    constructor(config) {
        super(config);
        this.KL_weight = config.KL_weight; // weights the KL_loss compared to reconstruction loss. If KL_weiht==0, reconstruction loss is the only loss used
        if (this.KL_weight === undefined) {
            this.KL_weight=0.0001; // default
        }
        this.last_mu;
        this.last_logVar;
        
        // Adds KL loss 
        this.addLoss(() =&gt; {
            const retour = tf.tidy(() =&gt; {
                let kl_loss;
                let z_log_var=this.last_logVar;
                let z_mean=this.last_mu;
                kl_loss = tf.scalar(1).add(z_log_var).sub(z_mean.square()).sub(z_log_var.exp());
                kl_loss = tf.sum(kl_loss, -1);
                kl_loss = kl_loss.mul(tf.scalar(-0.5 * this.KL_weight));
                return (tf.mean(kl_loss));
            }); 
            return (retour);

            
        }); // end of addLoss
    } // end of constructor

    computeOutputShape(inputShape) {
        return inputShape[0]; // same shape as mu
    }

    call(inputs, training) {
        return tf.tidy(() =&gt; {
            const [mu, logVar] = inputs;
            
            // store mu and logVar values to be used by the KL loss function
            this.last_mu=mu; // zMean
            this.last_logVar=logVar; // zLogVar
            
            const z = tf.tidy(() =&gt; {
                const batch = mu.shape[0];
                const dim = mu.shape[1];
                const epsilon = tf.randomNormal([batch, dim]);
                const half = tf.scalar(0.5);
                const temp = logVar.mul(half).exp().mul(epsilon);
                const sample = mu.add(temp);
                return sample;
            });
            return z;
        });
    } // end of call()

    static get className() {
        return 'wb_sampling';
    }
} // end of wb_sampling layer
</code></pre>
<p>The model works well, and the reconstruction is correct, but something seems strange to me : the output tensor of the logVar layer contains only zeros.</p>
<p>I tried with more or less units in the mean / logVar / sample layers (from 2 to 20).
I tried to change the KL_weight parameter(that weights the KL loss against the reconstruction loss). I even tried with a KL_weight of 0 (which means that the KL loss is totally disregarded and the model only changes due to the reconstruction loss MSE).
Whatever, the output of the logVar layer includes only zeros (note : in the beginning of the training, there are different values, but after a few steps of training, only zeros remain in the output).</p>
<p>The mean layer however outputs varied values, and I noticed that they are smaller when  KL_weight &gt; 0 than when KL_weight==0 so the KL loss function seems to be working.</p>
<p>Could this be normal? maybe outputs higher than zero in the logVar layer wouldn't improve the reconstruction in a task as simple as this one?</p>
<p>Have you experienced such all-zeros outputs in your logVar outputs? If not, what are the usual values you have there? And do you have any idea of what may cause the problem?</p>
","1","Question"
"79508535","","<p>What is the format of node features that <code>graph2vec</code> and <code>GL2vec</code> in <code>karateclub</code> require? It does mention that there should be no string features but with or without it I am running into an error with the following code:</p>
<pre><code>import networkx as nx
import karateclub as kc
import matplotlib.pyplot as plt

G = nx.DiGraph()

G.add_node(0, label = &quot;A&quot;, feature=[0.5])
G.add_node(1, label = &quot;B&quot;, feature=[1.2])
G.add_node(2, label = &quot;C&quot;, feature=[0.8])

G.add_edge(0, 1)
G.add_edge(0, 2)
G.add_edge(1, 2)

nx.draw_networkx(G, with_labels=True)
plt.show()

graphs = [G]

model = kc.graph_embedding.Graph2Vec()
model.fit(graphs)
embeddings = model.get_embedding()
print(embeddings)
</code></pre>
<p>Error: <code>RuntimeError: you must first build vocabulary before training the model</code></p>
<p>I saw an option to build_vocab in <code>word2vec</code>, but how do I do it for <code>graph2vec</code>?</p>
<p>Are there any alternate packages, preferably with simpler implementations like <code>karateclub</code>, that I can use to generate embeddings for a list of directed node/edge attributed <code>networkx</code> graphs without the need to define training/test sets?</p>
","1","Question"
"79512816","","<p>I want to train a simple feed forward neural net that I've built in <a href=""https://penzai.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">penzai</a>, but I want to use different learning rates for each parameter group. I store the learning rate scale factor in each parameter's <code>metadata</code>, e.g. like this:</p>
<pre class=""lang-py prettyprint-override""><code>Parameter(
    label='mlp/Affine_0/Linear.weights',
    value=&lt;NamedArray float32(| features:784, features_out:128) (wrapping jax.Array)&gt;,
    metadata={'learning_rate': 0.0012755102040816326},
)
</code></pre>
<p>I use Penzai's <code>StatefulTrainer</code> for training and declare the optimizer like this:</p>
<pre class=""lang-py prettyprint-override""><code>optax.chain(
    optax.scale_by_adam(),
    scale_by_metadata_value(&quot;learning_rate&quot;),
    optax.scale_by_learning_rate(0.01),
)
</code></pre>
<p>where I define <code>scale_by_metadata_value</code> like this:</p>
<pre class=""lang-py prettyprint-override""><code>def scale_by_metadata_value(metadata_field_name: str):
    def init_fn(params):
        learning_rates = jax.tree.map(
            lambda param: param.metadata[metadata_field_name],
            params,
            is_leaf=(lambda node: isinstance(node, pz.ParameterValue))
        )
        return {&quot;learning_rates&quot;: learning_rates}

    def update_fn(updates, state, params):
        del params
        updates = jax.tree.map(
            # This is where the TypeError is thrown:
            lambda lr, g: lr * g, state[&quot;learning_rates&quot;], updates
        )
        return updates, state

    return optax.GradientTransformation(init_fn, update_fn)
</code></pre>
<p>However, when I run a training step, I get</p>
<pre><code>TypeError: unsupported operand type(s) for *: 'jaxlib.xla_extension.ArrayImpl' and 'ParameterValue'
</code></pre>
<p>I am especially puzzled because everything works when I remove the <code>scale_by_metadata_value(&quot;learning_rate&quot;)</code> line, even though optax.scale_by_learning_rate(0.01) does essentially the same thing as what I do with <code>scale_by_metadata_value</code>.</p>
<p><strong>What is the correct / best way to implement <code>scale_by_metadata_value</code>?</strong></p>
<p>Here is a minimal failing example (penzai version 0.2.4):</p>
<pre class=""lang-py prettyprint-override""><code>import penzai.toolshed.basic_training
import penzai
import penzai.pz as pz
import jax
import jax.numpy as jnp
import optax
import numpy as np


model = pz.nn.Linear(
    weights=pz.Parameter(
        value=pz.nx.wrap(np.ones((8, 4)), &quot;features&quot;, &quot;features_out&quot;),
        label=&quot;linear&quot;,
        metadata={&quot;learning_rate&quot;: 0.5},
    ),
    in_axis_names=(&quot;features&quot;,),
    out_axis_names=(&quot;features_out&quot;,),
)


def softmax_cross_entropy_loss(
    model, rng, state, current_input, current_target: pz.nx.NamedArray
):
    del rng, state
    logits: pz.nx.NamedArray = model(current_input)
    loss = jnp.sum(
        optax.losses.softmax_cross_entropy(
            logits.unwrap(&quot;features_out&quot;),
            current_target.unwrap(&quot;features_out&quot;),
        )
    )
    return (loss, None, {&quot;softmax_cross_entropy_loss&quot;: loss})


def scale_by_metadata_value(metadata_field_name: str):
    def init_fn(params):
        learning_rates = jax.tree.map(
            lambda param: param.metadata[metadata_field_name],
            params,
            is_leaf=(lambda node: isinstance(node, pz.ParameterValue)),
        )
        return {&quot;learning_rates&quot;: learning_rates}

    def update_fn(updates, state, params):
        del params
        updates = jax.tree.map(lambda lr, g: lr * g, state[&quot;learning_rates&quot;], updates)
        return updates, state

    return optax.GradientTransformation(init_fn, update_fn)


trainer = penzai.toolshed.basic_training.StatefulTrainer.build(
    root_rng=jax.random.key(2025),
    model=model,
    optimizer_def=optax.chain(
        optax.scale_by_adam(),
        scale_by_metadata_value(&quot;learning_rate&quot;),
        optax.scale_by_learning_rate(0.01),
    ),
    loss_fn=softmax_cross_entropy_loss,
    jit=False,
)

trainer.step(
    current_input=pz.nx.wrap(np.zeros(8), &quot;features&quot;),
    current_target=pz.nx.wrap(np.ones(4), &quot;features_out&quot;),
)
# TypeError: unsupported operand type(s) for *: 'jaxlib.xla_extension.ArrayImpl' and 'ParameterValue'
</code></pre>
","0","Question"
"79512981","","<p>I am getting below error when running the code snippet in AWS sagemaker JupyterLab notebook:</p>
<blockquote>
<p>ModuleNotFoundError: No module named 'ragas.metrics.critique'</p>
</blockquote>
<pre><code>import warnings
warnings.filterwarnings('ignore')   # ignore warnings related to pydantic v1 to v2 migration

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    context_entity_recall,
    answer_similarity,
    answer_correctness
)

from ragas.metrics.critique import (
harmfulness, 
maliciousness, 
coherence, 
correctness, 
conciseness
)

#specify the metrics here
metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        context_entity_recall,
        answer_similarity,
        answer_correctness,
        harmfulness, 
        maliciousness, 
        coherence, 
        correctness, 
        conciseness
    ]

result = evaluate(
    dataset = dataset, 
    metrics=metrics,
    llm=llm_for_evaluation,
    embeddings=bedrock_embeddings,
)

df = result.to_pandas()
</code></pre>
<p>I tried to reinstall ragas by using the command &quot;pip install ragas&quot; and still facing same issue.</p>
<p>When I checked ragas, it seems to have properly installed.</p>
<p>pip show ragas</p>
<pre><code>Name: ragas
Version: 0.2.14
Summary: 
Home-page: 
Author: 
Author-email: 
License: 
Location: /opt/conda/lib/python3.11/site-packages
Requires: appdirs, datasets, diskcache, langchain, langchain-community, langchain-core, langchain_openai, nest-asyncio, numpy, openai, pydantic, tiktoken
Required-by: 
Note: you may need to restart the kernel to use updated packages.
</code></pre>
<p>How to resolve this?</p>
<p><strong>Update ----</strong></p>
<p>I was able to resolve the mentioned issue in above case by following the below steps of installing ragas of version 0.1.16 , but I am getting a new issue (mentioned below) when I run the above code section.</p>
<p>%pip install ragas==0.1.16</p>
<hr />
<pre><code>ImportError Traceback (most recent call last)
Cell In[27], line 17
4 from ragas import evaluate
5 from ragas.metrics import (
6 faithfulness,
7 answer_relevancy,
(...)
12 answer_correctness
13 )
---&gt; 17 from ragas.metrics.critique import (
18 harmfulness,
19 maliciousness,
20 coherence,
21 correctness,
22 conciseness
23 )
25 #specify the metrics here
26 metrics = [
27 faithfulness,
28 answer_relevancy,
(...)
38 conciseness
39 ]

File /opt/conda/lib/python3.11/site-packages/ragas/metrics/critique.py:13
11 from ragas.llms.output_parser import RagasoutputParser, get_json_format_instructions
12 from ragas.llms.prompt import Prompt
---&gt; 13 from ragas.metrics.base import EvaluationMode, MetricWithLLM
15 if t.TYPE_CHECKING:
16 from langchain_core.callbacks.base import Callbacks

ImportError: cannot import name 'EvaluationMode' from 'ragas.metrics.base' (/opt/conda/lib/python3.11/site-packages/ragas/metrics/base.py)
</code></pre>
","0","Question"
"79515278","","<p>I followed the published Keras CV computer vision tutorial (<a href=""https://keras.io/examples/vision/yolov8/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/yolov8/</a>) using the YOLO V8 detector with a custom dataset and have ran into a couple of issues I was hoping to get some help with.</p>
<p>The issue is that the mAP scores are very low (1-10%) using the Yolo v8 detector in Keras CV.  During training and inference, I see that Non-Max Suppression doesn't appear to be working correctly.  I see that there are several overlapping bounding boxes for the same object with the same confidence score.  Changing the NMS settings doesn't improve this.</p>
<p>To validate my dataset, I have tried using Roboflow and the Ultralytics library to fine-tune a model using my dataset and was able to get around 71%-91.4% mAP. The dataset size is 586 with 4 classes, 1 instance of each class per image (on purpose to avoid class imbalance issues).</p>
<p>This is an example of how I am running inference after training and tweaking NMS:</p>
<pre class=""lang-py prettyprint-override""><code>model.prediction_decoder = keras_cv.layers.NonMaxSuppression(
    bounding_box_format=BOUNDING_BOX_FORMAT,
    from_logits=True, # or False
    iou_threshold=0.5, # tried adjusting everything here 
    confidence_threshold=0.5,
    max_detections=10
)
</code></pre>
<p>I am using the PyCOCOCallback during training (as well as others):</p>
<pre class=""lang-py prettyprint-override""><code>    callbacks=[
        keras_cv.callbacks.PyCOCOCallback(  # COCO metrics (AP/AR @ IOU)
            val_ds,
            bounding_box_format=BOUNDING_BOX_FORMAT,
        ),
        keras.callbacks.TensorBoard(  # training progress visualization
            log_dir=&quot;training_logs&quot;
        ),
        keras.callbacks.ModelCheckpoint(
            filepath=&quot;./model/best_model.keras&quot;,
            monitor=&quot;val_AP&quot;,
            mode=&quot;max&quot;,  # Save when mAP is maximized
            save_best_only=True,  # Only keep the best model
            verbose=1,
        ),
        keras.callbacks.EarlyStopping(
            patience=20,  # Stop after 25 epochs of no improvement
            monitor=&quot;val_AP&quot;,
            mode=&quot;max&quot;,  # Monitor for maximum mAP
            verbose=1,
            restore_best_weights=True,  # Restore weights of the best epoch
        ),
        # VisualizeDetections(),  # visual validation after each epoch
    ],
</code></pre>
<p>Example of how I am training the model:</p>
<pre class=""lang-py prettyprint-override""><code>backbone = keras_cv.models.YOLOV8Backbone.from_preset(
    &quot;yolo_v8_xs_backbone_coco&quot;, load_weights=True
)

prediction_decoder = keras_cv.layers.NonMaxSuppression(
    bounding_box_format=BOUNDING_BOX_FORMAT,
    from_logits=False,
    iou_threshold=0.5,
    confidence_threshold=0.6,
    max_detections=200
)

model = keras_cv.models.YOLOV8Detector(
    num_classes=len(CLASS_IDS),
    bounding_box_format=BOUNDING_BOX_FORMAT,
    backbone=backbone,
    fpn_depth=2,
    prediction_decoder=prediction_decoder,
)

initial_learning_rate = 0.001
lr_schedule = keras.optimizers.schedules.CosineDecay(
    initial_learning_rate,
    decay_steps=1000,
    alpha=0.0
)
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule, global_clipnorm = 10.0)

model.compile(
    classification_loss=keras.losses.CategoricalCrossentropy(),
    box_loss=keras_cv.losses.CIoULoss(bounding_box_format=BOUNDING_BOX_FORMAT),
    optimizer=optimizer,
)
</code></pre>
<p>I have tried:</p>
<ul>
<li>Checking to ensure bounding boxes and labels are correct.</li>
<li>Using different optimizers, class/loss functions etc.</li>
<li>With and without augmentation.</li>
<li>Using augmentation via Roboflow, Keras/Tensorflow or both.</li>
<li>Different backbones with and without pre-trained weights.</li>
<li>Different NMS settings (Confidence, from_logits and IOU).</li>
</ul>
<p>I am expecting the performance of YoloV8 Detector in Keras CV to perform somewhat close to the Ultralytics implementation.  It seems practical that it may perform slightly under since my augmentation pipeline is likely not as robust as Ultralytics, but this is a huge gap.</p>
<p>I cannot figure out what I seem be overlooking! How to close this huge gap?</p>
","1","Question"
"79515542","","<p>Im running a random forest model and to get some feature importance and Im trying to run a SHAP analysis. The problem is that every time I try to plot the shap values, I keep getting this error:</p>
<pre><code>DimensionError: Length of features is not equal to the length of shap_values. 
</code></pre>
<p>I don't know whats going on. When I run my XGBoost model, everything seems to go fine, i can see the SHAP plot for the data set. Its the exact same data set but it just wont run with random forest. Its for a binary classification.</p>
<p>Here is my python code:</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Remove the primary key column 'id' from the features

features = result.drop(columns=['PQ2', 'id'])  # Drop target and ID columns
target = result['PQ2']  # Target variable

# Split data into training and testing sets with 80-20 ratio
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)
 
# Initialize Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model on the training data
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

import shap

# Create a Tree SHAP explainer for the Random Forest model
explainer = shap.TreeExplainer(rf_model)

# Calculate SHAP values for the test set
shap_values = explainer.shap_values(X_test)

# Plot a SHAP summary plot
shap.summary_plot(shap_values, X_test, feature_names=features_names)

# Plot a SHAP bar plot for global feature importance

shap.summary_plot(shap_values, X_test, feature_names=features_names, plot_type=&quot;bar&quot;)
</code></pre>
<p>The shape of test set is (829,22), yet the SHAP values consistently return (22,2) for random forest and I dont know how to fix it. The data set has been preprocessed, columns are either 0-1s or numerical columns.</p>
","0","Question"
"79521051","","<p>I have exported my VertexAI model to TFJS as &quot;edge&quot;, which results in:</p>
<ul>
<li>dict.txt</li>
<li>group1_shard1of2.bin</li>
<li>group1_shard2of2.bin</li>
<li>model.json</li>
</ul>
<p>Now, I send an image from my client to the Node/Express endpoint which I am really having a tough time figuring out - because I find the TFJS docs to be terrible to understand what I need to do.  But here is what I have:</p>
<pre><code>&quot;@tensorflow/tfjs-node&quot;: &quot;^4.22.0&quot;,
&quot;@types/multer&quot;: &quot;^1.4.12&quot;,
&quot;multer&quot;: &quot;^1.4.5-lts.1&quot;,
</code></pre>
<p>and then in my endpoint handler for image &amp; model:</p>
<pre class=""lang-js prettyprint-override""><code>
const upload = multer({
  storage: memoryStorage(),
  limits: {
    fileSize: 10 * 1024 * 1024, // 10MB limit
  },
}).single('image');

// Load the dictionary file
const loadDictionary = () =&gt; {
  const dictPath = path.join(__dirname, 'model', 'dict_03192025.txt');
  const content = fs.readFileSync(dictPath, 'utf-8');
  return content.split('\n').filter(line =&gt; line.trim() !== '');
};

const getTopPredictions = (
  predictions: number[],
  labels: string[],
  topK = 5
) =&gt; {
  // Get indices sorted by probability
  const indices = predictions
    .map((_, i) =&gt; i)
    .sort((a, b) =&gt; predictions[b] - predictions[a]);

  // Get top K predictions with their probabilities
  return indices.slice(0, topK).map(index =&gt; ({
    label: labels[index],
    probability: predictions[index],
  }));
};

export const scan = async (req: Request, res: Response) =&gt; {
  upload(req as any, res as any, async err =&gt; {
    if (err) {
      return res.status(400).send({ message: err.message });
    }

    const file = (req as any).file as Express.Multer.File;

    if (!file || !file.buffer) {
      return res.status(400).send({ message: 'No image file provided' });
    }

    try {
      // Load the dictionary
      const labels = loadDictionary();

      // Load the model from JSON format
      const model = await tf.loadGraphModel(
        'file://' + __dirname + '/model/model_03192025.json'
      );

      // Process the image
      const image = tf.node.decodeImage(file.buffer, 3, 'int32');
      const resized = tf.image.resizeBilinear(image, [512, 512]);
      const normalizedImage = resized.div(255.0);
      const batchedImage = normalizedImage.expandDims(0);
      const predictions = await model.executeAsync(batchedImage);

      // Extract prediction data and get top matches
      const predictionArray = Array.isArray(predictions)
        ? await (predictions[0] as tf.Tensor).array()
        : await (predictions as tf.Tensor).array();

      const flatPredictions = (predictionArray as number[][]).flat();
      const topPredictions = getTopPredictions(flatPredictions, labels);

      // Clean up tensors
      image.dispose();
      resized.dispose();
      normalizedImage.dispose();
      batchedImage.dispose();
      if (Array.isArray(predictions)) {
        predictions.forEach(p =&gt; (p as tf.Tensor).dispose());
      } else {
        (predictions as tf.Tensor).dispose();
      }

      return res.status(200).send({
        message: 'Image processed successfully',
        size: file.size,
        type: file.mimetype,
        predictions: topPredictions,
      });
    } catch (error) {
      console.error('Error processing image:', error);
      return res.status(500).send({ message: 'Error processing image' });
    }
  });
};

// Wrapper function to handle type casting
export const scanHandler = [
  upload,
  (req: Request, res: Response) =&gt; scan(req, res),
] as const;
</code></pre>
<p>Here is what I am concerned about:</p>
<ol>
<li>am I loading the model correctly as <code>graphModel</code>? I tried others and this is the only which worked.</li>
<li>I am resizing to 512x512 ok?</li>
<li>How can I better handle results?  If I want the highest &quot;rated&quot; image, what's the best way to do this?</li>
</ol>
","-1","Question"
"79521352","","<p>I am trying to pass individual np.arrays of sample_weights for two outputs of my Keras model, one of which is a confidence measure of a binary value and one of which is a continuous output. However, I receive a <code>KeyError: 0</code> as tf.keras tries to read it using <code>object = object[_path]</code> according to the traceback. Imports are provided in the code snippet for clarity.</p>
<pre><code>def train_model(model, X_ts_train, X_item_train, y_train_conf, y_train_pct, epochs=50, batch_size=32):
    import numpy as np
    from sklearn.utils.class_weight import compute_class_weight

    # ---- Ensure y_train_conf is integer (0 or 1) ---- #
    y_train_conf = np.asarray(y_train_conf).astype(int)

    # ---- Compute per-class weights for binary classification ---- #
    unique_classes = np.unique(y_train_conf.ravel())
    class_weight_dict = {0: 1.0, 1: 1.0}  # Default weights
    if len(unique_classes) == 2:  # Ensure both 0 and 1 exist
        class_weights = compute_class_weight(class_weight='balanced', classes=unique_classes, y=y_train_conf.ravel())
        class_weight_dict = {int(unique_classes[i]): class_weights[i] for i in range(len(unique_classes))}

    # ---- Convert class weights into per-sample weights (matching y_train_conf shape) ---- #
   sample_weights_conf = np.array([class_weight_dict[label] for label in y_train_conf.ravel()])
   sample_weights_conf = sample_weights_conf.reshape(y_train_conf.shape)  # Now shape is (84, 5)

   # ---- Compute per-sample weights for continuous spike percentage ---- #
   y_train_pct = np.asarray(y_train_pct)
   sample_weights_pct = np.ones_like(y_train_pct)  # Default weight = 1

   nonzero_mask = y_train_pct &gt; 0
   if np.any(nonzero_mask):
       scaling_factor = np.sum(nonzero_mask) / y_train_pct.size
       sample_weights_pct[nonzero_mask] = 1 / max(scaling_factor, 1e-6)
   print(sample_weights_conf.shape, sample_weights_pct.shape, y_train_conf.shape, y_train_pct.shape, flush=True)

   # sample_weights_binary = np.mean(sample_weights_conf, axis=1)
   # sample_weights_continuous = np.mean(sample_weights_pct, axis=1)

   # print(sample_weights_continuous.shape, sample_weights_binary.shape, flush=True)
   # ---- Train Model ---- #
   history = model.fit(
       {&quot;ts_input&quot;: X_ts_train, &quot;item_input&quot;: X_item_train},
       {&quot;output_binary&quot;: y_train_conf, &quot;output_continuous&quot;: y_train_pct},
       epochs=epochs,
       batch_size=batch_size,
       validation_split=0.1,
       verbose=2,
       sample_weight={'output_binary': sample_weights_conf, 'output_continuous': sample_weights_pct}  # Pass separately
   )

   return history
</code></pre>
<p>I expected the model to take the sample weights without issue, and also tried to pass them as a list, pass them as one array that is a mean of both, and tried to give y_train, x_train, and sample_weights all as arrays, all of which gave me a myriad of errors and still failed to give a positive result. The outputs of my model are defined as following:</p>
<pre><code>output_binary = Dense(num_binary_targets, activation='sigmoid', name=&quot;output_binary&quot;)(dense_out)     
output_continuous = Dense(num_continuous_targets, activation='linear', name=&quot;output_continuous&quot;)(dense_out)
</code></pre>
","0","Question"
"79521737","","<p>I'm using version 3.0.1 of the Microsoft.ML library in my C# WinForms object detection program.</p>
<p>When I add a Machine Learning Model to my Visual Studio project, training it on the MLModel1.mbconfig page shows a useful training log in the VS output window.</p>
<p>However, when I create the training pipeline and call the Fit() function in my program, I want to be able to see the training log in a window since training can be quite time consuming and I want to be able to see the results as it progresses. How can I access this output log while training is taking place?</p>
","1","Question"
"79523261","","<p>I'm using CodeBERT to compare how similar two pieces of code are. For example:</p>
<pre><code># Code 1
def calculate_area(radius):
return 3.14 * radius * radius
</code></pre>
<pre><code># Code 2
def compute_circle_area(r):
return 3.14159 * r * r
</code></pre>
<p>CodeBERT creates &quot;embeddings,&quot; which are like detailed descriptions of the code as numbers. I then compare these numerical descriptions to see how similar the codes are. This works well for telling me how much the codes are alike.</p>
<p>However, I can't tell which parts of the code CodeBERT thinks are similar. Because the &quot;embeddings&quot; are complex, I can't easily see what CodeBERT is focusing on. Comparing the code word-by-word doesn't work here.</p>
<p>My question is: How can I figure out which specific parts of two code snippets CodeBERT considers similar, beyond just getting a general similarity score?</p>
<p>I tried simple diff methods but that defeats the purpose of purely using CodeBERT.
I want to know if it's possible using CodeBERT alone.</p>
","2","Question"
"79524578","","<p>I'm training a YOLO model in Google Colab, but the training stops automatically as if I manually pressed Ctrl+C. The last output in the terminal shows ^C, and the process terminates without any error messages.</p>
<p><a href=""https://i.sstatic.net/fykpT46t.png"" rel=""nofollow noreferrer"">No error, just ^C appearing as if I interrupted manually</a></p>
","-1","Question"
"79525759","","<p>I'm loading a model checkpoint using</p>
<pre><code>model.load_state_dict(state_dict, strict=False)
</code></pre>
<p>because the model architecture does not fully match the weights. As expected, this results in a warning message like this:</p>
<pre><code>The model and loaded state dict do not match exactly
unexpected key in source state_dict: backbone.blocks.0.mlp.experts.0.weight, ...
</code></pre>
<p>The warning is very long because there are many unexpected keys. I understand this is just a warning and not an actual error, and I want to suppress or hide this message entirely.</p>
<p>Is there a clean way to do this in PyTorch?</p>
","0","Question"
"79528929","","<p>I have a training dataset with six features and I am using <code>SequentialFeatureSelector</code> to find an &quot;optimal&quot; subset of the features for a linear regression model. The following code returns three features, which I will call <code>X1, X2, X3</code>.</p>
<pre><code>sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select='auto', 
                                tol=0.05, direction='forward', 
                                scoring='neg_root_mean_squared_error', cv=8)
sfs.fit_transform(X_train, y_train)
</code></pre>
<p>To check the results, I decided to run the same code using the subset of features <code>X1, X2, X3</code> instead of <code>X_train</code>. I was expecting to see the features <code>X1, X2, X3</code> returned again, but instead it was only the features <code>X1, X2</code>. Similarly, using these two features again in the same code returned only <code>X1</code>. It seems that the behavior of <code>sfs</code> is always to return a proper subset of the input features with at most <code>n_features_in_ - 1</code> columns, but I cannot seem to find this information in the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html"" rel=""nofollow noreferrer"">scikit-learn docs</a>. Is this correct, and if so, what is the reasoning for not allowing
<code>sfs</code> to return the full set of features?</p>
<p>I also checked to see if using backward selection would return a full feature set.</p>
<pre><code>sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select='auto', 
                                tol=1000, direction='backward', 
                                scoring='neg_root_mean_squared_error', cv=8)
sfs.fit_transform(X_train, y_train)
</code></pre>
<p>I set the threshold <code>tol</code> to be a large value in the hope that there would be no satisfactory improvement from the full set of features of <code>X_train</code>. But, instead of returning the six original features, it only returned five. The docs simply state</p>
<blockquote>
<p>If the score is not incremented by at least tol between two consecutive feature additions or removals, stop adding or removing.</p>
</blockquote>
<p>So it seems that the full feature set is not being considered during cross-validation, and the behavior of <code>sfs</code> is different at the very end of a forward selection or at the very beginning of a backwards selection. If the full set of features outperforms any proper subset of the features, then don't we want <code>sfs</code> to return that possibility? Is there a standard method to compare a selected proper subset of the features and the full set of features using cross-validation?</p>
","2","Question"
"79528937","","<p>Consider a regression task where the parameters of the model differ significantly in magnitude, say:</p>
<pre class=""lang-py prettyprint-override""><code>def func(x, p):
    p1, p2, p3 = p
    return np.sin(p1*x) * np.exp(p2*x) * p3

# True Parameters:
p1, p2, p3 = np.pi/0.01, -1.25, 1.2356  # 314.1592, -1.25, 1.2356
</code></pre>
<p>The system requires a feasible initial guess to converge to the correct solution, for example:</p>
<pre class=""lang-py prettyprint-override""><code>p0 = [np.pi/0.01-80, -1.25-1, 1.2356-1]
</code></pre>
<p>However, if <strong>the initial guess is too far from the true solution</strong>, the regression may fail to converge. For example, the following initial guess might not work:</p>
<pre class=""lang-py prettyprint-override""><code>p0 = [np.pi/0.02-10, -1.25-1, 1.2356-1]
</code></pre>
<p>The reproducible code is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.optimize import least_squares
import matplotlib.pyplot as plt

def func(x, p):
    p1, p2, p3 = p
    return np.sin(p1*x) * np.exp(p2*x) * p3

def residuals(p, y, x):
    return y - func(x, p)

x = np.linspace(0, 0.05, 50)
p1, p2, p3 = np.pi/0.01, -1.25, 1.2356
y0 = func(x, [p1, p2, p3])
y = y0 + np.random.randn(len(x)) * 0.05

p0 = [np.pi/0.02-10, -1.25-1, 1.2356-1]

result = least_squares(residuals, p0, args=(y, x))

print(&quot;True parameters:&quot;, [p1, p2, p3])
print(&quot;Approximated parameters:&quot;, result.x)

x_test = np.linspace(0, 0.05, 200)
y_test = func(x_test, result.x)
y_real = func(x_test, [p1, p2, p3])
plt.plot(x_test, y_test, label=&quot;predict&quot;)
plt.plot(x, y, '.r', label=&quot;real&quot;)
</code></pre>
<p>Or here is a PyTorch version:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
from torch import nn
import matplotlib.pyplot as plt

class guessEq(nn.Module):
    def __init__(self):
        super(guessEq, self).__init__()
        self.params = nn.Parameter(torch.tensor([np.pi/0.01-10, -1.25+1, 1.2356-1]))
    
    def forward(self, x):
        out = torch.sin(self.params[0]*x) * \
            torch.exp(-self.params[1]*x) * \
            self.params[2]
        return out

x = np.linspace(0, 0.05, 100)
y = np.sin(np.pi/0.01*x) * np.exp(-1.25*x) * 1.2356 + np.random.rand(x.shape[0]) * 0.05

x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)
x = x.reshape((-1, 1))
y = y.reshape((-1, 1))

model = guessEq()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
mse = nn.MSELoss()
for i in range(2000):
    optimizer.zero_grad()
    y_pred = model(x)
    loss = torch.mean(torch.square(y_pred - y))
    loss.backward()
    if i % 100 == 0:
        print(loss)
    optimizer.step()

x_test = torch.linspace(0, 0.05, 200).reshape((-1, 1))
y_test = model(x_test)
print(&quot;True parameters: [{} {} {}]&quot;.format(np.pi/0.01, -1.25, 1.2356))
print(&quot;Approximated parameters: {}&quot;.format(model.params.detach().numpy()))
plt.plot(x_test.detach().cpu().numpy().flatten(), y_test.detach().cpu().numpy().flatten(), c=&quot;blue&quot;, linewidth=2, label=&quot;predict&quot;)
plt.plot(x.detach().cpu().numpy().flatten(), y.detach().cpu().numpy().flatten(), '.r', label=&quot;train&quot;)
plt.legend()
plt.show()
</code></pre>
<p>How to address the issue when the initial guess is far from the true solution, or when there is no prior of the initial guess?</p>
","0","Question"
"79531266","","<p>I've been trying to concatenate TF-IDF data with categorical data. However, when concatenating, the categorical data is automatically converted to float by default. Since CatBoost doesn't support float for categorical features, this causes an error for sparse data because it's no longer recognized as categorical data.</p>
<p>Is there a solution to this issue? Please find my code below for reference:</p>
<pre><code>import numpy as np
import pandas as pd
from catboost import CatBoostClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import hstack, csr_matrix

text_data = [
    &quot;I love machine learning and data science&quot;,
    &quot;Deep learning is a subset of machine learning&quot;,
    &quot;Natural language processing is amazing&quot;,
    &quot;AI is transforming the world&quot;,
    &quot;Big data and AI are revolutionizing industries&quot;
]

categorical_data = {
    &quot;Category&quot;: [&quot;Tech&quot;, &quot;Tech&quot;, &quot;NLP&quot;, &quot;AI&quot;, &quot;Big Data&quot;],
    &quot;Region&quot;: [&quot;US&quot;, &quot;Europe&quot;, &quot;Asia&quot;, &quot;US&quot;, &quot;Europe&quot;]
}

y = np.array([0, 1, 0, 1, 1])

df_cat = pd.DataFrame(categorical_data)

vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(text_data)

df_cat_encoded = df_cat.apply(LabelEncoder().fit_transform)

X_categorical = csr_matrix(df_cat_encoded.values)

X_combined = hstack([X_tfidf, X_categorical])

model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=5, verbose=0)

model.fit(X_combined, y, cat_features=[X_tfidf.shape[1], X_tfidf.shape[1] + 1])

predictions = model.predict(X_combined)

print(predictions)
</code></pre>
<p>Error:</p>
<pre><code>CatBoostError: 'data' is scipy.sparse.spmatrix of floating point numerical type, 
it means no categorical features, but 'cat_features' parameter specifies nonzero 
number of categorical features
</code></pre>
","1","Question"
"79535519","","<p>I am working on a deep learning assignment that requires implementing a <strong>feedforward neural network (FNN)</strong> from scratch using <strong>only NumPy</strong> (without TensorFlow, PyTorch, or other auto-differentiation tools). The network has <strong>three layers</strong> (2048, 512, 10 neurons), uses <strong>ReLU and Softmax activation</strong>, and is optimized with <strong>mini-batch stochastic gradient descent (SGD)</strong>.</p>
<p>The assignment requires implementing <strong>forward propagation, backpropagation, and weight updates manually</strong>, ensuring that I correctly compute gradients for each layer using the <strong>backpropagation algorithm</strong>.</p>
<p>However, I am struggling with correctly computing the <strong>gradients for each layer</strong>, especially when handling <strong>ReLU activation's derivative</strong> and <strong>cross-entropy loss's gradient with Softmax</strong>. I want to confirm whether my <strong>gradient computations</strong> and <strong>weight updates</strong> are correct.</p>
<pre><code>import numpy as np

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x &gt; 0).astype(float)  # ReLU derivative (1 if x &gt; 0, else 0)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def cross_entropy_loss(y_pred, y_true):
    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-9), axis=1))  # Prevent log(0)

def softmax_cross_entropy_grad(y_pred, y_true):
    return y_pred - y_true  # Softmax + cross-entropy derivative

def gradient_check():
    np.random.seed(42)  # Ensure reproducibility
    
    # Fake input (batch_size=3, input_dim=5)
    X = np.random.randn(3, 5)
    W1 = np.random.randn(5, 4)
    b1 = np.zeros((1, 4))

    # Fake one-hot encoded labels (batch_size=3, num_classes=4)
    Y = np.array([[0, 1, 0, 0], 
                  [1, 0, 0, 0], 
                  [0, 0, 1, 0]])

    Z1 = np.dot(X, W1) + b1
    A1 = relu(Z1)
    Y_pred = softmax(A1)
    loss = cross_entropy_loss(Y_pred, Y)

    dL_dA1 = softmax_cross_entropy_grad(Y_pred, Y)  # Gradient w.r.t. softmax output
    dL_dZ1 = dL_dA1 * relu_derivative(Z1)  # Chain rule with ReLU

    print(&quot;Y_pred (Softmax Output):\n&quot;, Y_pred)
    print(&quot;Loss:&quot;, loss)
    print(&quot;Gradient w.r.t. Softmax Output (dL/dA1):\n&quot;, dL_dA1)
    print(&quot;Gradient after ReLU Derivative (dL/dZ1):\n&quot;, dL_dZ1)

gradient_check()
</code></pre>
<p>I implemented a <strong>feedforward neural network (FNN)</strong> using <strong>NumPy only</strong> and manually computed backpropagation. I expected the <strong>loss to decrease</strong> and <strong>accuracy to improve</strong>, but instead, the <strong>loss fluctuates</strong>, accuracy stays <strong>low</strong>, and sometimes gradients <strong>explode or become NaN</strong>. I suspect issues with <strong>ReLU’s derivative</strong> and <strong>Softmax + cross-entropy gradient</strong> calculations and need to verify if my backpropagation is correct.</p>
","2","Question"
"79536891","","<p>Is using two different <code>nn.ModuleList()</code> zipped lists correct to build the computational graph for training a neural net in <code>PyTorch</code>? <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html"" rel=""nofollow noreferrer"">nn.ModuleList</a> is a wrapper around Python's list with a registration of a module for training.</p>
<p>I'm building a network which consists of 2x interchanging types of blocks in <code>__init__</code>:</p>
<pre class=""lang-py prettyprint-override""><code>    def __init__(self, in_channels):
        super().__init__()

        self.encoder_conv_blocks = nn.ModuleList()
        self.downsample_blocks = nn.ModuleList()

        for out_channels in _FILTERS:
            conv_block = _ConvBlock(in_channels, _CONV_BLOCK_LEN, _CONV_BLOCK_GROWTH_RATE)
            downsample_block = _DownsampleBlock(conv_block.out_channels, out_channels)

            self.encoder_conv_blocks.append(conv_block)
            self.downsample_blocks.append(downsample_block)

            in_channels = out_channels
</code></pre>
<p>later in <code>forward</code>, I'm zipping the layers, as I need the outputs of the first type of block later in skip connections:</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(self, x):
        skip_connections = []
        
        for conv_block, downsample_block in zip(self.encoder_conv_blocks,
                                                self.downsample_blocks):
            x = conv_block(x)
            skip_connections.append(x)
            x = downsample_block(x)
</code></pre>
<p>However when pritting the summary <a href=""https://github.com/TylerYep/torchinfo"" rel=""nofollow noreferrer"">torchinfo</a>, we can see that summary of the registered methods using 2x zipped <code>nn.ModuleList</code> looks different compared to the summary where one single <code>nn.ModuleList</code> was used. I suspect that this can cause issues for training and inference in the future.</p>
<p><code>zip(nn.ModuleList(), nn.ModuleList())</code>:</p>
<pre><code>========================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #
========================================================================================================================
MyNet                                     [16, 4, 128, 256]         [16, 3, 128, 256]         --
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-1                        [16, 4, 128, 256]         [16, 84, 128, 256]        26,360
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-2                  [16, 84, 128, 256]        [16, 64, 64, 128]         48,448
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-3                        [16, 64, 64, 128]         [16, 144, 64, 128]        70,160
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-4                  [16, 144, 64, 128]        [16, 128, 32, 64]         166,016
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-5                        [16, 128, 32, 64]         [16, 208, 32, 64]         116,880
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-6                  [16, 208, 32, 64]         [16, 128, 16, 32]         239,744
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-7                        [16, 128, 16, 32]         [16, 208, 16, 32]         116,880
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-8                  [16, 208, 16, 32]         [16, 128, 8, 16]          239,744
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-9                        [16, 128, 8, 16]          [16, 208, 8, 16]          116,880
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-10                 [16, 208, 8, 16]          [16, 256, 4, 8]           479,488
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-11                       [16, 256, 4, 8]           [16, 336, 4, 8]           210,320
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-12                 [16, 336, 4, 8]           [16, 256, 2, 4]           774,400
├─ModuleList: 1-13                            --                        --                        (recursive)
│    └─_ConvBlock: 2-13                       [16, 256, 2, 4]           [16, 336, 2, 4]           210,320
├─ModuleList: 1-14                            --                        --                        (recursive)
│    └─_DownsampleBlock: 2-14                 [16, 336, 2, 4]           [16, 512, 1, 2]           1,548,800
</code></pre>
<p>single <code>nn.ModuleList()</code>:</p>
<pre><code>MyNet                                     [16, 4, 128, 256]         [16, 3, 128, 256]         --
├─ModuleList: 1-1                             --                        --                        --
│    └─_ConvBlock: 2-1                        [16, 4, 128, 256]         [16, 84, 128, 256]        26,360
│    └─_DownsampleBlock: 2-2                  [16, 84, 128, 256]        [16, 64, 64, 128]         48,448
│    └─_ConvBlock: 2-3                        [16, 64, 64, 128]         [16, 144, 64, 128]        70,160
│    └─_DownsampleBlock: 2-4                  [16, 144, 64, 128]        [16, 128, 32, 64]         166,016
│    └─_ConvBlock: 2-5                        [16, 128, 32, 64]         [16, 208, 32, 64]         116,880
│    └─_DownsampleBlock: 2-6                  [16, 208, 32, 64]         [16, 128, 16, 32]         239,744
│    └─_ConvBlock: 2-7                        [16, 128, 16, 32]         [16, 208, 16, 32]         116,880
│    └─_DownsampleBlock: 2-8                  [16, 208, 16, 32]         [16, 128, 8, 16]          239,744
│    └─_ConvBlock: 2-9                        [16, 128, 8, 16]          [16, 208, 8, 16]          116,880
│    └─_DownsampleBlock: 2-10                 [16, 208, 8, 16]          [16, 256, 4, 8]           479,488
│    └─_ConvBlock: 2-11                       [16, 256, 4, 8]           [16, 336, 4, 8]           210,320
│    └─_DownsampleBlock: 2-12                 [16, 336, 4, 8]           [16, 256, 2, 4]           774,400
│    └─_ConvBlock: 2-13                       [16, 256, 2, 4]           [16, 336, 2, 4]           210,320
│    └─_DownsampleBlock: 2-14                 [16, 336, 2, 4]           [16, 512, 1, 2]           1,548,800
</code></pre>
","0","Question"
"79546578","","<p>I'm creating an AI model to generate density plots of crowds. When splitting the dataset into two, one for training and one for validation, I create the two data sets and try to load the datasets using <code>torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)</code>. After that, to test the data I iterate through and use the next function to get the next element of the dataset and then I get the TypeError.
The dataset I'm using his the ShanghaiTech Crowd Counting data set from Kaggle: <a href=""https://www.kaggle.com/datasets/tthien/shanghaitech"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/tthien/shanghaitech</a></p>
<p>Here is the full code:</p>
<pre><code>batch_size = 8 
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

train_root_dir = &quot;data/part_A/train_data/&quot;
init_training_set = DataLoader(train_root_dir, shuffle=True)

# split part of the training set as validation set
train_size = int(0.9 * len(init_training_set))
val_size = len(init_training_set) - train_size

train_indices = list(range(train_size))
val_indices = list(range(train_size, len(init_training_set)))
train_dataset = torch.utils.data.dataset.Subset(init_training_set, train_indices)
val_dataset = torch.utils.data.dataset.Subset(init_training_set, val_indices)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

test_root_dir = &quot;data/part_A/test_data/&quot;
test_set = DataLoader(test_root_dir, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)

dataiter = iter(train_loader)
ex_images, ex_dmaps, ex_n_people = next(dataiter)


# Show images and density map
plot_corresponding_pairs(ex_images, ex_dmaps)
</code></pre>
<p>The specific error is:</p>
<pre><code>Traceback (most recent call last):
 line 61, in &lt;module&gt;
    for ex_images, ex_dmaps, ex_n_people in train_loader
TypeError: 'DataLoader' object is not subscriptable
</code></pre>
","0","Question"
"79547006","","<p>I'm building a time series forecasting model in Python to predict hourly kWh loads for different customer types at a utility company. The dataset contains ~81 million rows, with hourly load data for ~2,300 customers over 2-4 years. Customer type is represented by binary columns: EV, HP, Solar, and TOU. The dataset has the following variables:</p>
<pre><code>  - read_date: datetime64[us]
  - meter: string
  - kwh: float64
  - city: string
  - temperature: float64
  - ev: int64
  - solar: int64
  - hp: int64
  - tou: int64
  - hour: int32
  - day: int32
  - month: int32
  - year: Int64
  - day_of_week: int32
  - season: string
  - customer_type: string
  - hour_sin: float64
  - hour_cos: float64
  - month_sin: float64
  - month_cos: float64
  - day_of_week_sin: float64
  - day_of_week_cos: float64
  - day_sin: float64
  - day_cos: float64
  - is_holiday: int64
  - city_reading: int64
  - city_lynnfield: int64
  - city_northreading: int64
  - city_wilmington: int64
  - season_winter: int64
  - season_spring: int64
  - season_summer: int64
  - season_fall: int64
</code></pre>
<p>After cleaning the data, I dropped the following features from both the training and test datasets: <code>meter</code>, <code>customer_type</code>, <code>season</code>, <code>read_date</code>, <code>city</code>, <code>day</code>, <code>month</code>, <code>hour</code>, <code>day_of_week</code>. My target variable is hourly kWh load.</p>
<p>I attempted to build an XGBoost model using Dask for distribution, but it keeps crashing with errors like:</p>
<pre><code>AssertionError: error
2025-03-31 14:12:26,995 - distributed.nanny - WARNING - Restarting worker
</code></pre>
<p>I’m working on a local machine with 128GB of RAM and an Intel i7-14700K 3.40 GHz processor. I'm looking for guidance on how to handle time series forecasting with this large dataset and how to avoid crashes while using Dask for distribution. Here is my sample code:</p>
<pre><code># Import necessary libraries
import numpy as np
import dask.dataframe as dd
import dask.array as da
import xgboost as xgb
from dask.distributed import Client
from dask.diagnostics import ProgressBar 
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import matplotlib.pyplot as plt
from tqdm import tqdm

# Load the data using Dask (efficient for large Parquet files)
some_feats_dd = dd.read_parquet(&quot;pre_ml_some_features.parquet&quot;)

# Rename DataFrame
df_processed = some_feats_dd

# Filter the data based on the read_date for training and testing
df_train = df_processed[df_processed[&quot;year&quot;] &lt; 2025]  # Keep rows before 2025
df_test = df_processed[df_processed[&quot;year&quot;] == 2025]  # Keep rows from 2025 onwards

# Exclude columns and prepare features and target variables for training
exclude_cols = [&quot;kwh&quot;, &quot;meter&quot;, &quot;customer_type&quot;, &quot;season&quot;, &quot;read_date&quot;, &quot;city&quot;, 
                &quot;day&quot;, &quot;month&quot;, &quot;hour&quot;, &quot;day_of_week&quot;]

# Prepare training features (X) and target variable (y)
X_train = df_train.drop(columns=exclude_cols)
y_train = df_train[&quot;kwh&quot;]

# Compute total lengths and ensure exact 3 chunks
train_size = len(y_train.compute())
test_size = len(df_test)  # No need to compute, Dask can infer

# Convert y_train and y_test to Dask arrays with forced 3 chunks
y_train = da.from_array(y_train.compute(), chunks=(train_size // 3,))
y_test = da.from_array(df_test[&quot;kwh&quot;].compute(), chunks=(test_size // 2,))

# Ensure partitions match for X_train and X_test
X_train = X_train.repartition(npartitions=3)
X_test = X_test.repartition(npartitions=3)

# Start Dask client for parallel processing
client = Client()

# Print the Dask dashboard URL
print(f&quot;Dask dashboard is available at: {client.dashboard_link}&quot;)

# Use DaskDMatrix from xgboost.dask
dask_train_data = xgb.dask.DaskDMatrix(client, X_train, y_train)

# Set up parameters for XGBoost
params = {
    'objective': 'reg:squarederror',  # Regression task
    'eval_metric': 'rmse',
    'tree_method': 'hist',  # Use histogram-based method for faster training
    'verbosity': 1,  # Enables basic logging
}

# Initialize Dask-XGBoost model
dask_gbr = xgb.dask.DaskXGBRegressor(**params)

# Train the model using Dask (this will automatically parallelize)
with ProgressBar():  # Shows progress during training
    dask_gbr.fit(dask_train_data)
</code></pre>
","0","Question"
"79552254","","<p>I am learning to use the TensorFlow Recommenders library and the subclassing API and have been working through the documentation. I would like to know how to run and review the output of the individual sub-models that feed into the final model.</p>
<p>I have recreated an excerpt of the first sub model (I have compiled and fit the full model and it runs okay).</p>
<p>I would like to run this UserFeaturesModel in isolation, to be able to inspect the concatenated data that it outputs so that I can determine if it is doing what it should, what its shape is,  and to understand more about how it feeds into the next sub model.</p>
<p>But I am not sure how to compile and fit this model, as it requires a loss function when I try to compile, but is this model not just simply running embeddings and therefore has no loss function?</p>
<p>Apologies if this is a stupid question but I have spent days trying to figure this out and have tried looking through previous questions on Stack and watched many tutorials - please could someone show me to run this model and generate an output that I can review?</p>
<pre class=""lang-py prettyprint-override""><code># imports
import tensorflow as tf
import tensorflow_recommenders as tfrs
import tensorflow_datasets as tfds
import numpy as np


# load datasets from tf
ratings = tfds.load(&quot;movielens/100k-ratings&quot;, split=&quot;train&quot;)
movies = tfds.load(&quot;movielens/100k-movies&quot;, split=&quot;train&quot;)


# keep required feature columns for each dataset
ratings = ratings.map(lambda x: {
    &quot;movie_title&quot;: x[&quot;movie_title&quot;],
    &quot;user_id&quot;: x[&quot;user_id&quot;],
    &quot;user_rating&quot;: x[&quot;user_rating&quot;],
    &quot;timestamp&quot;: x[&quot;timestamp&quot;],
    &quot;raw_user_age&quot;: x[&quot;raw_user_age&quot;],
})

movies = movies.map(lambda x: x[&quot;movie_title&quot;])


# create datasets of user_ids and movie_titles
user_ids = ratings.batch(1000).map(lambda x: x['user_id'])
movie_titles = movies.batch(1000)


# find unique vocabs from above datasets
unique_user_ids = np.unique(np.concatenate(list(user_ids)))
unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))


# create timestamp array for bucketization and normalization
timestamps = np.concatenate(list(ratings.map(lambda x: x[&quot;timestamp&quot;]).batch(100)))
max_timestamp = timestamps.max()
min_timestamp = timestamps.min()
timestamp_buckets = np.linspace(min_timestamp, max_timestamp, num = 2000)


# create user_age array for normalization
user_age = np.concatenate(list(ratings.map(lambda x: x[&quot;raw_user_age&quot;]).batch(100)))


# vocab sizes for embedding
user_vocab_size = len(unique_user_ids) + 1
movie_vocab_size = len(unique_movie_titles) + 1
timestamp_buckets_size = len(timestamp_buckets) + 1


# set random seed
tf.random.set_seed(42)

# shuffle and split data 80:20
shuffled_data = ratings.shuffle(100000, seed=42, reshuffle_each_iteration=False)
train = shuffled_data.take(80000)
test = shuffled_data.skip(80000).take(20000)

# batch and cache the train and test datasets
cached_train = train.shuffle(80000).batch(512).cache()
cached_test = test.batch(512).cache()


# define model
class UserFeaturesModel(tf.keras.Model):
  # model construction and attributes
  def __init__(self):
    super().__init__()

    embedding_dim = 32

    # user_id feature embeddings
    self.user_id_embeddings = tf.keras.Sequential([
       tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),
       tf.keras.layers.Embedding(user_vocab_size, embedding_dim)
    ])

    # timestamp bucket featrure embeddings
    self.timestamp_bucket_embeddings = tf.keras.Sequential([
        tf.keras.layers.Discretization(timestamp_buckets.tolist()),
        tf.keras.layers.Embedding(timestamp_buckets_size, embedding_dim)
    ])

    # timestamp normalization
    self.normalized_timestamp = tf.keras.layers.Normalization(axis=None)
    self.normalized_timestamp.adapt(timestamps)

    # age normalization
    self.normalized_user_age = tf.keras.layers.Normalization(axis=None)
    self.normalized_user_age.adapt(user_age)


  # model call and forward pass
  def call(self, inputs):
    return tf.concat([
        self.user_id_embeddings(inputs['user_id']),
        self.timestamp_bucket_embeddings(inputs['timestamp']),
        tf.reshape(self.normalized_timestamp(inputs['timestamp']),(-1,1)),
        tf.reshape(self.normalized_user_age(inputs['raw_user_age']),(-1,1))
    ], axis=1)


# instantiate model
model = UserFeaturesModel()

# run model - not sure how to compile, fit and run model and inspect output?
</code></pre>
<p>run model - not sure how to compile, fit and run model and inspect output?</p>
","2","Question"
"79552491","","<p>I'm working on implementing image segmentation using my own custom TFLite model, following the code example from MediaPipe. Here's my code:</p>
<pre><code>options = vision.ImageSegmenterOptions(
    base_options=base_options,
    running_mode=mp.tasks.vision.RunningMode.IMAGE,
    output_confidence_masks=True,
    output_category_mask=False
)

mp_image = mp.Image.create_from_file(image_path)
with vision.ImageSegmenter.create_from_options(options) as segmenter:
    segmentation_result = segmenter.segment(mp_image)
    output_mask = segmentation_result.confidence_masks[0]
</code></pre>
<p>I've encountered two issues with the above code:</p>
<ol>
<li><p>The model has two outputs:</p>
<p>Output 0: Name = Identity0, Shape = [1, 1], Type = numpy.float32</p>
<p>Output 1: Name = Identity1, Shape = [1, x, y, z], Type = numpy.float32 (where x * y * z == image_width * image_height * image_channel=1)</p>
<p><strong>How can I retrieve both outputs instead of just one</strong>?</p>
</li>
<li><p>The confidence_masks values are almost identical (min/max = 0.0701157/0.070115715), which seems unusual. The original image contains a person, and the output is correct when using my custom TFLite model with tf.lite.Interpreter.get_tensor().</p>
</li>
</ol>
<p>I know that many frameworks support models with multiple inputs and outputs, so I'm confused about what I might be missing. Here are my specific questions:</p>
<ol>
<li>Do I need to add special metadata to the TFLite model file?</li>
<li>How should I modify the original MediaPipe code to handle multiple outputs?</li>
</ol>
","0","Question"
"79553429","","<p>I am trying to train a yolov11 model for a set of images but I am running into an issue. The GPU the model is training on is a GeForce RTX 4070 super.</p>
<p>I am using yolov11 with torch with GPU.</p>
<pre><code>print(torch.__version__)
2.5.1+cu121
</code></pre>
<p>I did train the model a few times before without running into issues with this command:</p>
<pre><code>yolo task=detect mode=train device=0 epochs=2000 batch=32 data=&quot;C:\Project\Yolo\data_custom.yaml&quot; model=&quot;C:\Project\Yolo\yolov11m.pt&quot; imgsz=640
</code></pre>
<p>But this did not give me results that are good enough. So if I understand proprely, by increasing the value of &quot;imgsz&quot; to 1440 (my images are 2560 by 1440), the model should train with images of grater quality. So I tried to run this command:</p>
<pre><code>yolo task=detect mode=train device=0 epochs=2000 batch=32 data=&quot;C:\Project\Yolo\data_custom.yaml&quot; model=&quot;C:\Project\Yolo\yolov11m.pt&quot; imgsz=640
</code></pre>
<p>Running this command gives me an error that I don't know how to solve. The error is the following:</p>
<pre><code>torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.48 GiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 19.12 GiB is allocated by PyTorch, and 406.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
</code></pre>
<p>Does anyone how to solve this issue ?</p>
","1","Question"
"79559377","","<p>I am working on a regression problem where, given an input X of fixed size, the output Y can be a sequence of values of variable length.
Both input and output are normalised float values. Thus, we are talking about a regression task.</p>
<p>The problem lays in the variable size of the Y array samples (max length 46).</p>
<p>On average, only the first 30-35 values (out of 46) are valid.
So, to train the network, the solution that I am trying is to:</p>
<ul>
<li>Pad the non-valid values of each Y-array sample with zeroes (0.0)</li>
<li>train and predict</li>
<li>&quot;Unpad&quot; the output arrays by removing all the trailing zeroes (or very small values).</li>
</ul>
<p>The problems are that:</p>
<ul>
<li>Small values around 0.0 can in fact be also a valid values, thus generating ambiguity. Maybe better to use a ludicrous weird number like -100?</li>
<li>It seems that the network is never really jumping from a valid value (e.g.: 2.3) to 0.0, but it is generating smooth transitions towards 0.0, thus generating very bad outputs.</li>
</ul>
<p>What would be a good solution for this problem?
If array values were integers, I could use a special int as  token. But having floats makes everything more tricky.</p>
","1","Question"
"79560879","","<p>Basically I am trying to create a model that will detect angle on which a specific image is rotated. Also I have a dataset of 1500 documents resulting with images rotated on</p>
<pre><code>random.sample([0, 90, -90, 180], 2)
</code></pre>
<p>and each of this angles has variation of</p>
<pre><code>random.uniform(-10, 10)
</code></pre>
<p>resulting in ~4k rotated images.</p>
<p>So I've come up with current model to predict sin and cos of the desired angle:</p>
<pre><code>class CnnRotateRegression(nn.Module):
    def __init__(self):
        super(CnnRotateRegression, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3)
        self.conv5 = nn.Conv2d(512, 512, kernel_size=3)
        
        self.bn1 = nn.BatchNorm2d(64)
        self.bn2 = nn.BatchNorm2d(128)
        self.bn3 = nn.BatchNorm2d(256)
        self.bn4 = nn.BatchNorm2d(512)
        self.bn5 = nn.BatchNorm2d(512)
        
        self.activation = nn.ReLU()
        self.pool = nn.AvgPool2d(kernel_size=2)
        self.pool2 = nn.AdaptiveAvgPool2d((8,8))

        self.linear_l1 = nn.Linear(512*8*8, 512)
        self.linear_l2 = nn.Linear(512, 256)
        self.linear_l3 = nn.Linear(256, 2) # sin + cos

    def forward(self, x):
        x = self.activation(self.pool(self.bn1(self.conv1(x))))
        x = self.activation(self.pool(self.bn2(self.conv2(x))))
        x = self.activation(self.pool(self.bn3(self.conv3(x))))
        x = self.activation(self.pool(self.bn4(self.conv4(x))))
        x = self.activation(self.pool(self.bn5(self.conv5(x))))
        
        x = self.pool2(x)
        x = x.view(x.size(0), -1)
        
        x = self.activation(self.linear_l1(x))
        x = self.activation(self.linear_l2(x))
        x = self.linear_l3(x)

        x = F.normalize(x, p=2, dim=1)

        return x
</code></pre>
<p>training part:</p>
<pre><code>model = CnnRotateRegression()
model = model.to(device)

loss_function = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_of_epochs = 11


for epoch in range(num_of_epochs):
    model.train()
    running_loss = 0.0
    for images, labels in tqdm(train_Loader, desc=&quot;training loop&quot;):

        images, labels = images.to(device), labels.to(device).float()

        angles = angle_to_sin_cos(labels)
        norm_angles = F.normalize(angles, p=2, dim=1)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_function(outputs, norm_angles)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    train_loss = running_loss / len(train_Loader)
</code></pre>
<p>functions to convert sin and cos to angle and vice versa:</p>
<pre><code>def angle_to_sin_cos(angle):
    tensor_angle = angle.clone().detach()
    radian = tensor_angle * torch.pi / 180.0
    return torch.stack([torch.cos(radian), torch.sin(radian)], dim=1)

def sin_cos_to_angle(outputs):
    cos_val, sin_val = outputs[:, 0], outputs[:, 1]
    angle_rad = torch.atan2(sin_val, cos_val)
    angle_deg = angle_rad * (180 / torch.pi)
    return angle_deg
</code></pre>
<p>My model performs poorly in determining small angles in range +-10 degrees. What would you suggest to improve/enhance to achieve better &quot;small-degree&quot; prediction?</p>
","0","Question"
"79561884","","<p>I am training on CIFAR10 the following simple CNN</p>
<pre class=""lang-py prettyprint-override""><code>class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()

        self.fc1 = nn.Linear(64 * 8 * 8, 128, bias=False)
        self.fc2 = nn.Linear(128, 128, bias=False)
        self.fc3 = nn.Linear(128, 10, bias=False)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x
</code></pre>
<p>I would like to perform global structured pruning on the last part of the network, the MLP part (so <code>fc1</code>, <code>fc2</code>, <code>fc3</code>) with a percentage cutoff. Basically I would like to cut off <code>x</code> percent of the neurons (and all the relative connections) on the basis of the total connectivity. In PyTorch there is a function that performs a similar job: <code>ln_structured</code></p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn.utils.prune as prune
prune.ln_structured(model.fc1, name='weight', amount=fraction_of_neurons_to_prune)
prune.ln_structured(model.fc2, name='weight', amount=fraction_of_neurons_to_prune)
prune.ln_structured(model.fc3, name='weight', amount=fraction_of_neurons_to_prune)
</code></pre>
<p><strong>But the main problem is</strong> that this function will apply the fraction to prune layer by layer, and not globally; I want something to cutoff let's say 80 percent of the neurons in the MLP portion of the network, and not 80 percent layer by layer.</p>
<p>Is there a function that performs what I want? How could I write it myself? The absence of a function in PyTorch that performs this very common operation seems very strange to me, but I can't find anything.</p>
","1","Question"
"79567319","","<p>I have a dataset that I used to make a random forest (it is split into testing and training data). I have already made the random forest and generated predictions (code below), but I don't know how to take those predictions and use them to generate a full data table that includes the gap-filled values.</p>
<pre><code>#data table head
TIMESTAMP &lt;- c(&quot;2019-05-31 17:00:00&quot;, &quot;2019-05-31 17:30:00&quot;, &quot;2019-05-31 18:00:00&quot;, &quot;2019-05-31 18:30:00&quot;, &quot;2019-05-31 19:00:00&quot;, &quot;2019-05-31 19:30:00&quot;, &quot;2019-05-31 20:00:00&quot;, &quot;2019-05-31 20:30:00&quot;, &quot;2019-05-31 21:00:00&quot;, &quot;2019-05-31 21:30:00&quot;)
RH&lt;-c(38, 40, 41, 42, 44, 49, 65, 72, 74, 77)
Fch4 &lt;- c(0.045, -0.002, 0.001, 0.004, 0, -0.013, 0.004,-0.003, -0.001,-0.002)
distance &lt;- c(1000,1000,180,125.35,1000,180,1000,5.50,180,1000)
Ta &lt;-c(29.52, 29.01, 29.04, 28.39, 27.87, 26.68, 23.28, 21.16, 19.95, 19.01)
Fe&lt;- c(95.16, 68.95, 68.62, 39.24, 35.04, 27.26, -2.60, 5.09,7.28, 2.08)

dd&lt;- data.frame(TIMESTAMP, RH, Fch4, distance, Ta, Fe)

#Making RF and Prediction
set.seed(1)
inTraining &lt;- createDataPartition(dd$Fch4, p = 0.65, list=FALSE)
training &lt;- dd[inTraining,]
testing &lt;- dd[-inTraining,]

set.seed(1)
pfpfit &lt;- randomForest(Fch4 ~ ., training, ntree=500, type=&quot;regression&quot;)
predicted &lt;- predict(pfpfit, newdata = testing)
</code></pre>
<p>So, with the above code, I have the prediction model, but I don't know how to apply it to a dataset I already have (example below) that has gaps. I also don't know if it is a problem to have gaps in variables that <em>aren't</em> the variable I want to gap fill (I want to gap fill Fch4, but I also have gaps in Fe and Ta).
An example of the dataset I want to gap fill is below:</p>
<pre><code>#data table head
    TIMESTAMP &lt;- c(&quot;2019-05-31 17:00:00&quot;, &quot;2019-05-31 17:30:00&quot;, &quot;2019-05-31 18:00:00&quot;, &quot;2019-05-31 18:30:00&quot;, &quot;2019-05-31 19:00:00&quot;, &quot;2019-05-31 19:30:00&quot;, &quot;2019-05-31 20:00:00&quot;, &quot;2019-05-31 20:30:00&quot;, &quot;2019-05-31 21:00:00&quot;, &quot;2019-05-31 21:30:00&quot;)
    RH&lt;-c(38, 40, 41, 42, 44, 49, 65, 72, 74, 77)
    Fch4 &lt;- c(NA, -0.002, 0.001, 0.004, NA, -0.013, 0.004,NA, -0.001,-0.002)
    distance &lt;- c(1000,1000,180,125.35,1000,180,1000,5.50,180,1000)
    Ta &lt;-c(29.52, 29.01, NA, 28.39, 27.87, 26.68, 23.28, NA, 19.95, 19.01)
    Fe&lt;- c(NA, NA, 68.62, 39.24, 35.04, 27.26, -2.60, NA,7.28, 2.08)
dd&lt;- data.frame(TIMESTAMP, RH, Fch4, distance, Ta, Fe)
</code></pre>
<p>I would want the gap filled dataset to look like this:</p>
<pre><code>#data table head
    TIMESTAMP &lt;- c(&quot;2019-05-31 17:00:00&quot;, &quot;2019-05-31 17:30:00&quot;, &quot;2019-05-31 18:00:00&quot;, &quot;2019-05-31 18:30:00&quot;, &quot;2019-05-31 19:00:00&quot;, &quot;2019-05-31 19:30:00&quot;, &quot;2019-05-31 20:00:00&quot;, &quot;2019-05-31 20:30:00&quot;, &quot;2019-05-31 21:00:00&quot;, &quot;2019-05-31 21:30:00&quot;)
    RH&lt;-c(38, 40, 41, 42, 44, 49, 65, 72, 74, 77)
    Fch4 &lt;- c(0.045, -0.002, 0.001, 0.004, 0, -0.013, 0.004,-0.003, -0.001,-0.002)
    distance &lt;- c(1000,1000,180,125.35,1000,180,1000,5.50,180,1000)
    Ta &lt;-c(29.52, 29.01, 29.04, 28.39, 27.87, 26.68, 23.28, 21.16, 19.95, 19.01)
    Fe&lt;- c(95.16, 68.95, 68.62, 39.24, 35.04, 27.26, -2.60, 5.09,7.28, 2.08)
dd&lt;- data.frame(TIMESTAMP, RH, Fch4, distance, Ta, Fe)
</code></pre>
<p>(I realize the three datasets are mostly the same and that you shouldn't test on the same data as your training data. This is just for the purposes of having something that runs. I can refine it on my own.)</p>
","3","Question"
"79571067","","<p>I'm trying to conduct the residual analysis for simple linear regression. I need to prove that the residuals follow an approximate Normal Distribution.</p>
<p>The csv file I'm using has values for Percentage of marks in Grade 10 and the Salary the student makes.<br>
Once I run the below code, my plot looks like this:</p>
<p><img src=""https://i.sstatic.net/kmjATAb8.png"" alt=""Plot of residuals from code"" /></p>
<p>The plot in the book looks like this:</p>
<p><img src=""https://i.sstatic.net/8M2OC2fT.png"" alt=""Plot of residuals from the book"" /></p>
<p>I was expecting my plot to show up like the book as the data is the same. I have double-checked to make sure I'm not missing any data etc. I have split the data set into training and test as per the book as well.</p>
<pre><code>Data is as follows:

Percentage Salary
62  270000
76.33   200000
72  240000
60  250000
61  180000
55  300000
70  260000
68  235000
82.8    425000
59  240000
58  250000
60  180000
66  428000
83  450000
68  300000
37.33   240000
79  252000
68.4    280000
70  231000
59  224000
63  120000
50  260000
69  300000
52  120000
49  120000
64.6    250000
50  180000
74  218000
58  360000
67  150000
75  250000
60  200000
55  300000
78  330000
50.08   265000
56  340000
68  177600
52  236000
54  265000
52  200000
76  393000
64.8    360000
74.4    300000
74.5    250000
73.5    360000
57.58   180000
68  180000
69  270000
66  240000
60.8    300000
</code></pre>
<p>The code is below:</p>
<pre><code># Importing all required libraries for building the regression model
import pandas as pd import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load the dataset into dataframe
mba_salary_df = pd.read_csv( 'MBA Salary.csv' )

# Add constant term of 1 to the dataset
X = sm.add_constant( mba_salary_df[‘Percentage in Grade 10’] )
Y = mba_salary_df['Salary']

# Split dataset into train and test set into 80:20 respectively
train_X, test_X, train_y, test_y = train_test_split( X, Y, train_size = 0.8,random_state = 100 )

# Fit the regression model
mba_salary_lm = sm.OLS( train_y, train_X ).fit()

mba_salary_resid = mba_salary_lm.resid 

probplot = sm.ProbPlot(mba_salary_resid) 

plt.figure( figsize = (8, 6) ) 

probplot.ppplot(line='45') 

plt.title(&quot;Normal P-P Plot of Regression Standardized Residuals&quot;) 

plt.show()
</code></pre>
","5","Question"
"79573345","","<p>I'm trying to make a LSTM model to predict sign language sentence with this format of json file. This json file is containing the coordinates information of 21 hand landmark joints for each frames and each hands.(left hand, right hand)
Here's the sneak peak of my json time series data.</p>
<pre><code>{

        &quot;frame&quot;: 123,
        &quot;hands&quot;: [
            {
                &quot;hand&quot;: &quot;Left&quot;,
                &quot;landmarks&quot;: [
                    {
                        &quot;x&quot;: 0.4636201858520508,
                        &quot;y&quot;: 0.3758980929851532,
                        &quot;z&quot;: 7.529240519943414e-08,
                        &quot;body_part&quot;: &quot;wrist&quot;
                    },

   ...

                    {
                        &quot;x&quot;: 0.4639311134815216,
                        &quot;y&quot;: 0.2574789524078369,
                        &quot;z&quot;: -0.013109659776091576,
                        &quot;body_part&quot;: &quot;pinky_tip&quot;
                    }
                ]
            },

            {
                &quot;hand&quot;: &quot;Right&quot;,
                &quot;landmarks&quot;: [
                    {
                        &quot;x&quot;: 0.5393109321594238,
                        &quot;y&quot;: 0.6190552711486816,
                        &quot;z&quot;: 1.0587137921902467e-07,
                        &quot;body_part&quot;: &quot;wrist&quot;
                    },
...

                    {
                        &quot;x&quot;: 0.4721616506576538,
                        &quot;y&quot;: 0.5990280508995056,
                        &quot;z&quot;: -0.006831812672317028,
                        &quot;body_part&quot;: &quot;pinky_tip&quot;
                    }
                ]
            }
        ]
    },
</code></pre>
<p>The coordinate info with different location is being repeated for each frames.
I'm still in progress of making correction of these json time series data.
So, I haven't started making code for LSTM. However, I'm worrying whether I can use this time series json data for LSTM.</p>
","0","Question"
"79575941","","<p>I am training a random forest classifier in python <code>sklearn</code>, see code below-</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)
rf.fit(X = df.drop(&quot;AP&quot;, axis =1), y = df[&quot;AP&quot;].astype(int))
</code></pre>
<p>When I predict the values using this classifier on another dataset that has <code>NaN</code> values, the model provides some output. Not even that, I tried predicting output on a row with all variables as <code>NaNs</code>, it predicted the outputs.</p>
<pre><code>#making a row with all NaN values 
row = pd.DataFrame([np.nan] * len(rf.feature_names_in_), index=rf_corn.feature_names_in_).T
rf.predict(row)
</code></pre>
<p>It predicts-
<code>array([1])</code></p>
<p>I know that RandomForestClassifier in scikit-learn does not natively support missing values. So I expected a ValueError, not a prediction.</p>
<p>I can ignore the NaN rows and only predict on non-nan rows but I am concerned if there is something wrong with this classifier.
Any insight will be appreciated.</p>
","1","Question"
"79576028","","<p>I am looking to better understand sklearn IsolationForest <code>decision_function.</code> My understanding from this previous stack overflow post,  <a href=""https://stackoverflow.com/questions/68061530/what-is-the-difference-between-decision-function-and-score-samples-in-isolation"">What is the difference between decision function and score_samples in isolation_forest in SKLearn</a>, if the metric is closer to -1 then the model is more confident if it is an outlier. If it is closer to 0, then it is an inliner.</p>
<p>Please confirm or add to my understanding--thank you.</p>
<p>EDIT: Assuming we are using the default value for <code>contamination</code></p>
","0","Question"
"79577043","","<p>I have a dataset of approximately 750 lines containing quite short texts (less than 150 words each). These are all event descriptions related to a single broad topic (which I cannot specify for anonymity reasons).</p>
<p>I want to apply unsupervised topic modeling methods to get more precise and non-exclusive categories/tags for each text. My goal is to implement a multi-label approach where each text can belong to multiple categories simultaneously. Ideally, I'd like to identify around 10-20 relevant topics.</p>
<p>So far, I've tried:</p>
<ul>
<li>BERTopic: Since it relies on clustering, I end up with each text belonging to a single cluster, which is not what I want.</li>
<li>KeyBERT + BERTopic: I first used KeyBERT to extract keywords from each text and then applied BERTopic to get non-exclusive categories. This gave me the best results so far, but I'm still not completely convinced.</li>
</ul>
<p>I haven't tried LDA as I consider it a bit outdated and expected disappointing results for such short texts.</p>
","-1","Question"
"79586550","","<p>My Streamlit app reads and visualizes data from a large CSV file (~100MB). The app takes several seconds to load.
What are the best practices for improving performance—such as using <code>@st.cache_data</code>, lazy loading, or preprocessing?</p>
","0","Question"
"79591703","","<p>When I run the following code i get error <code>'module' object is not callable</code></p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == &quot;__main__&quot;:
    env_name = &quot;4x4grid&quot;
    register_env(
        env_name,
        lambda _: ParallelPettingZooEnv(
           sumo_rl.parallel_env(
                net_file=&quot;./4x4-Lucas/4x4.net.xml&quot;,
                route_file=&quot;./4x4-Lucas/4x4c1c2c1c2.rou.xml&quot;,
                out_csv_name=&quot;outputs/4x4grid/ppo&quot;,
                use_gui=False,
                num_seconds=80000,
            )
        ),
    )

   config = (
        PPOConfig()
        .environment(env=env_name, disable_env_checking=True)
    )

    config.env_runners(num_env_runners=4, rollout_fragment_length=128)
    config.training(
        train_batch_size=512,
        use_gae=True,         
        grad_clip=None,  
        num_epochs=10)
    config.debugging(log_level=&quot;ERROR&quot;)
    config.framework(framework=&quot;torch&quot;)
    config.resources(num_gpus=int(os.environ.get(&quot;RLLIB_NUM_GPUS&quot;, &quot;0&quot;)))

    tune.run(
        &quot;PPO&quot;,
        name=&quot;PPO&quot;,
        stop={&quot;timesteps_total&quot;: 100000},
        checkpoint_freq=10,
        storage_path=&quot;~/ray_results/&quot; + env_name,
        config=config.to_dict(),
    )
</code></pre>
<pre class=""lang-py prettyprint-override""><code>(SingleAgentEnvRunner pid=11388) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::SingleAgentEnvRunner.__init__() (pid=11388, ip=127.0.0.1, actor_id=a44169f772470169f85ced2001000000, repr=&lt;ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x000002620CC666F0&gt;)
(SingleAgentEnvRunner pid=11388)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(SingleAgentEnvRunner pid=11388)   File &quot;C:\Users\thaigiang\AppData\Local\Temp\ipykernel_12400\3020126705.py&quot;, line 17, in &lt;lambda&gt;
(SingleAgentEnvRunner pid=11388)   File &quot;C:\Users\thaigiang\anaconda3\Lib\site-packages\pettingzoo\utils\conversions.py&quot;, line 14, in par_fn
(SingleAgentEnvRunner pid=11388)     env = env_fn(**kwargs)
(SingleAgentEnvRunner pid=11388)           ^^^^^^^^^^^^^^^^
(SingleAgentEnvRunner pid=11388)   File &quot;C:\Users\thaigiang\anaconda3\Lib\site-packages\sumo_rl\environment\env.py&quot;, line 32, in env
(SingleAgentEnvRunner pid=11388)     env = SumoEnvironmentPZ(**kwargs)
(SingleAgentEnvRunner pid=11388)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(SingleAgentEnvRunner pid=11388)   File &quot;C:\Users\thaigiang\anaconda3\Lib\site-packages\sumo_rl\environment\env.py&quot;, line 523, in __init__
(SingleAgentEnvRunner pid=11388)     self._agent_selector = agent_selector(self.agents)
(SingleAgentEnvRunner pid=11388)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(SingleAgentEnvRunner pid=11388) TypeError: 'module' object is not callable
</code></pre>
<p>Environment:<br />
ray[rllib]==2.44.1<br />
python==3.12.7<br />
torch==2.7.0</p>
<p>I have researched and found that the error may arise during the env register process, <code>parallel_env</code> may be a module and not a function. but I don't know how to solve the error.</p>
","0","Question"
"79592576","","<p>I am currently developing a robot that uses YOLOv8 for real-time object detection based on the video captured by the robot’s camera. However, when the robot rotates, the input image becomes blurred, which causes a significant drop in detection accuracy. As a result, the robot has to either slow down or pause every 10 degrees during rotation, leading to poor efficiency.</p>
<p>To address this issue, I attempted to apply Sobel edge detection. Since the robot’s change in direction involves only rotation, which corresponds to horizontal movement (along the x-axis) in the camera view, I used a Sobel filter that emphasizes vertical edges by filtering x-axis signals. I then fed the processed images into a YOLOv8 model that had been trained on a dataset pre-processed with the same Sobel filtering.</p>
<p>However, despite this approach, the robot still fails to accurately detect objects while rotating.</p>
<pre class=""lang-py prettyprint-override""><code>while(True):

    ret, frame = cap.read()
    if not ret:
        print(&quot;Can't receive frame (stream end?). Exiting ...&quot;)
        break


    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    

    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = 3)

    #sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = 3)
    #sobel = cv2.magnitude(sobel_x, sobel_y)
    #sobel = np.uint8(np.clip(sobel, 0, 255))

    absX = cv.convertScaleAbs(sobel_x) 
    absX_rgb = cv2.cvtColor(absX, cv2.COLOR_GRAY2BGR)
    
    cv2.imshow(&quot;Sobel Edge3&quot;, absX)
    

    results = trt_model(absX_rgb)

   
    for result in results:
        boxes = result.boxes  # Boxes object for bounding box outputs
        center = boxes.xyxy
        shuttlecock_prob = boxes.conf
    
    ##########other process
   
    if cv2.waitKey(1) == ord('q'):
        break
    

</code></pre>
<p>This is the result when the robot is static
<a href=""https://i.sstatic.net/He8z4KOy.png"" rel=""nofollow noreferrer"">Detection result when the robot is static</a></p>
<p>The result with sobel filter(I displayed the results in RGB colors for easier observation.) when the robot is turning
<a href=""https://i.sstatic.net/X9vsoRcg.png"" rel=""nofollow noreferrer"">Sobel filter result</a></p>
<p>The result with blurred dataset when the robot is turning
<a href=""https://i.sstatic.net/EdOs0UZP.png"" rel=""nofollow noreferrer"">Blurred dataset result</a></p>
<p>Another attempt with this approach was to annotate the blurry images captured by the robot and include them in the training dataset. Although this helped improve object recognition under blurry conditions, it resulted in a decrease in overall accuracy.</p>
","-1","Question"
"79594764","","<p>Recently I trained two MLP model and saved weights for future work. I develop one module for loading model and use these model in another module.</p>
<p>Load model module contain this code to load models:</p>
<pre><code>def creat_model_extractor(model_path, feature_count):
    &quot;&quot;&quot;
    This function create model and set weights
    :param model_path: address of weights files
    :param feature_count: Number of nodes in input layer
    &quot;&quot;&quot;
    try:
        tf.keras.backend.clear_session()
        node_list = [1024, 512, 256, 128, 64, 32]

        model = Sequential()
        model.add(Input(shape=(feature_count,)))

        for node in node_list:
            model.add(Dense(node, activation='relu'))
            model.add(Dropout(0.2))
            model.add(LayerNormalization())

        model.add(Dense(16, activation='relu'))
        model.add(LayerNormalization())
        model.add(Dense(1, activation='sigmoid'))

        @tf.function
        def inference_step(inputs):
            return tf.stop_gradient(model(inputs, training=False))
 
        model.inference_step = inference_step

        model.load_weights(model_path)
        model.trainable = False
        for layer in model.layers:
            layer.trainable = False
    except Exception as error:
        logger.warning(error, exc_info=True)
        return None

    return model
</code></pre>
<p>And this is predict function</p>
<pre><code>SMALL_MODEL = creat_model_extractor(MODEL_PATH_SMALL, small_blocks_count)

(SMALL_MODEL.inference_step(small_blocks_normal) &gt; 0.5).numpy().astype(int)

</code></pre>
<p>Problem:
After predict label 'SMALL_MODEL' size change. It become bigger and And after a while, the RAM fills up.
What should I do to prevent RAM from filling up?</p>
<p>This problem happen even in one module</p>
","1","Question"
"79595516","","<p>I am working on a market risk assessment involving a hybrid of LSTM and Random Forest. This post might seem dumb , but I am really struggling with the model right now , here are my struggles in the model :</p>
<p>LSTM requires huge historical dataset unlike Random Forest , so do I use multiple datasets or single? because I am using RF for intra/daily trade option and LSTM for long term investments</p>
<p>I try to extract real time data using Alpha Vantage for now , but it has limited amount to how many requests I can ask.</p>
<p>At this point any input from you guys will just be super helpful to me, I am really having trouble with this project right now. Also any suggestions regarding online source materials or youtube videos that can help me with this project?</p>
","-1","Question"
"79597119","","<p>I'm trying to run Stable Diffusion using HuggingFace's <code>diffusers</code> library, but I keep getting CUDA out of memory errors on my RTX 3060 (12GB VRAM). I'm using the standard <code>StableDiffusionPipeline</code> from the <code>&quot;CompVis/stable-diffusion-v1-4&quot;</code> checkpoint.</p>
<p>Here's my code:</p>
<pre><code>from diffusers import StableDiffusionPipeline
import torch

# Load the pre-trained Stable Diffusion model
pipe = StableDiffusionPipeline.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;)
pipe = pipe.to(&quot;cuda&quot;)

# Define the prompt
prompt = &quot;a futuristic cityscape, high resolution, ultra detailed&quot;

# Generate the image
image = pipe(prompt).images[0]

# Show the image
image.show()
</code></pre>
<p>I tried running the code as shown above, expecting it to generate a high-quality image from the text prompt without memory issues. I also tried adding <code>torch_dtype=torch.float16</code> when loading the pipeline to reduce memory usage, like this:</p>
<pre><code>pipe = StableDiffusionPipeline.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;, torch_dtype=torch.float16)
pipe = pipe.to(&quot;cuda&quot;)
</code></pre>
<p>However, I still encounter CUDA out of memory errors, especially with larger prompts or higher resolutions. I expected the float16 setting to help, but it didn't fully solve the issue.</p>
<p>I also expected to control the resolution by adjusting parameters, but I'm not sure how to properly do that with this pipeline.</p>
","1","Question"
"79597666","","<p>I made a cnn model like this and trained the batch dataset.
My data preprocessing code was this</p>
<pre><code>train_dir = r'C:\Users\roobe\BACH\norm\train\patches\train'  # 훈련 데이터 폴더
val_dir = r'C:\Users\roobe\BACH\norm\train\patches\val'  # 검증 데이터 폴더

# 데이터 전처리 정의 (크기 조정, 텐서로 변환, 정규화)
transform = transforms.Compose([
    transforms.Resize((224, 224)),       # 크기 조정
    transforms.ToTensor(),               # 이미지 -&gt; 텐서
    transforms.Normalize([0.5]*3, [0.5]*3)  # 정규화
])


# ImageFolder로 데이터셋 불러오기
train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)
val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)

# DataLoader 생성 (훈련 데이터는 shuffle=True로 설정)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # 훈련 데이터는 섞기
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)  # 검증 데이터는 섞지 않음

# 클래스 목록 출력 (라벨 확인)
print(&quot;클래스 목록:&quot;, train_dataset.classes)  # ['0', '1', '2', '3'] 등의 출력

</code></pre>
<p>and I made CNN model like this</p>
<pre><code>class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # L1 ImgIn shape=(15161, 224, 224, 3)
        self.layer1 = torch.nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        # L2 ImgIn shape=(?, 32, 56, 56)
        self.layer2 = torch.nn.Sequential(
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2))
        
        self.dropout = nn.Dropout2d(p=0.5)  # Dropout 레이어 추가
        self.fc = nn.Linear(64*14*14, 4)
        nn.init.kaiming_normal_(self.fc.weight)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = self.dropout(out)  # Dropout 적용
        out = out.view(out.size(0), -1)   # Flatten
        out = self.fc(out)
        return out
</code></pre>
<p>It worked well in test and train code, but when I tried to make heatmap with gradcam, they kept give me message like<br />
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x16384 and 12544x4).
I've checked the image after resizing and the size was like this<br />
torch.Size([32, 3, 224, 224]). The code I used for gradcam wa this:</p>
<pre><code>import os
from PIL import Image
import numpy as np
import torch
import matplotlib.pyplot as plt
from torchvision import transforms, models
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image
from torchvision.transforms import Compose, ToTensor, Normalize
import math

def preprocess_image(img: np.ndarray, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) -&gt; torch.Tensor:
    preprocessing = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
    return preprocessing(img.copy()).unsqueeze(0)

# 이미지 폴더 경로 설정
img_folder = 'C:/Users/roobe/BACH/norm/train/patches/val'
img_files = [os.path.join(root, file) for root, dirs, files in os.walk(img_folder) 
             for file in files if file.endswith(('.png', '.jpg', '.jpeg'))]

# 최대 20개 이미지만 시각화
img_files = img_files[:20]

# 모델 불러오기
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNN().to(device)
model.load_state_dict(torch.load('cnnnew.pth'))
model.eval()

# Grad-CAM 인스턴스 설정
cam = GradCAM(model=model, target_layers=[model.layer2[0]])

# 시각화
num_images = len(img_files)
cols = 5
rows = math.ceil(num_images / cols)
plt.figure(figsize=(20, 4 * rows))

for idx, img_path in enumerate(img_files):
    img_pil = Image.open(img_path).convert('RGB')
    img_np = np.array(img_pil).astype(np.float32) / 255.
    input_tensor = preprocess_image(img_np).to(device)

    # 예측 및 Grad-CAM 계산
    output = model(input_tensor)
    pred_class = torch.argmax(output, dim=1).item()
    targets = [ClassifierOutputTarget(pred_class)]
    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]
    visualization = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)

    # 서브플롯에 시각화 결과 표시
    plt.subplot(rows, cols, idx + 1)
    plt.imshow(visualization)
    plt.axis('off')
    plt.title(f&quot;Class {pred_class}\n{os.path.basename(img_path)}&quot;, fontsize=9)

plt.tight_layout()
plt.show() 
</code></pre>
<p>any advice would be thankful!</p>
","0","Question"
"79598098","","<pre><code>from transformers import pipeline

summarizer = pipeline(&quot;summarization&quot;)
result = summarizer(
    &quot;&quot;&quot;
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.
&quot;&quot;&quot;
)
print(result)
</code></pre>
<p>The code in question uses the transformers library, which is designed for state-of-the-art natural language processing tasks. The specific error being invoked when use new pipelines, like summarization model, question-answering model, name entity recognition model and etc.</p>
<p>with first execution result shows as following?</p>
<pre><code>Error displaying widget
Error displaying widget
Error displaying widget
Error displaying widget
Error displaying widget
</code></pre>
<p>Second execution shows the right output</p>
<pre><code>[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in America has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . Engineering curricula now concentrate on  the study of engineering science and technology . While the latter is important, it should not be at the expense of more traditional engineering .'}]
</code></pre>
<p>Why isn't the result showing within the first of execution?</p>
","0","Question"
"79600456","","<p>Which one of the following 2 methods is preferred?</p>
<ol>
<li>To scale the entire dataset to make it into numpy array</li>
<li>To scale the dataset row by row in such a way that you get dataframe back but this time each row in the dataframe has scaled data.</li>
</ol>
<p>I have tried both of them but I am confused which one to choose.</p>
","-1","Question"
"79601928","","<p>I'm building a classification model for sleep disorders using Voting Ensemble and I have three base models: Logistic Regression, Random Forest and SVM.</p>
<p>Now I want to combine these models using a Voting Ensemble with soft voting.I don't really have anyone to ask so I came here to ask about the correct approach: I know LR and SVM needs to be scaled but because they all need to be in the same format (as in scaled or raw), I need to scale the rf model too right? I know rf doesn't need scaling but others do, what is the best approach here?</p>
<p>I tried both approaches: training Random Forest on scaled data to match other models, and also manually combining predictions where each model receives its appropriate data format. I expected the second approach to perform better since Random Forest shouldn't need scaling, but wanted to verify this is the correct practice when using VotingClassifier.</p>
","-1","Question"
"79604724","","<p>I am working on a data science project and trying to find the optimal parameters for my project</p>
<p>this is what I want to test but it takes forever and I could not see the output since its been 1 hour.</p>
<pre><code>scores =[]
for k in range(1, 200):
    rfc = RandomForestClassifier(n_estimators=k)
    rfc.fit(X_train_scaled, y_train)
    y_pred3 = rfc.predict(X_test_scaled)
    scores.append(accuracy_score(y_test, y_pred3))

import matplotlib.pyplot as plt
%matplotlib inline

# plot the relationship between K and testing accuracy
# plt.plot(x_axis, y_axis)
plt.plot(range(1, 200), scores)
plt.xlabel('Value of n_estimators for Random Forest Classifier')
plt.ylabel('Testing Accuracy')
</code></pre>
<p>Is there a solution for speeding up this process. I am using Macbook Pro 14 inc and working on a M1 Pro chip</p>
","1","Question"
"79604798","","<p>I have just seen some code which has sparked my interest.</p>
<pre class=""lang-py prettyprint-override""><code>with tf.GradientTape() as g:
    y = f(x)

dy_dx = g.gradient(y, x)
</code></pre>
<p>(Code loosely taken from this <a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">reference</a>.)</p>
<p>There are two things which I think are interesting about this:</p>
<ul>
<li>The use of a context manager</li>
<li>The fact that when the context manager has closed the gradient value(s) are available</li>
</ul>
<p>What interests me is understanding the software behind what might look like &quot;magic code&quot;. You might ask questions such as</p>
<ul>
<li>why is there a <code>with</code> keyword involved here?</li>
<li>if I evaluated <code>f(x)</code>, then how is it that a value for a gradient (which is a different function to <code>f</code>) become available?</li>
</ul>
<p>I think the context manager is fairly easy to figure out. It causes the magic dunder functions <code>__enter__</code> and <code>__exit__</code> to be triggered, and so I am fairly confident this is a convenient way of marking where the beginning and end of gradient calculation. I would <em>guess</em> this is to make the overall code more efficient - there is no point calculating gradients if they are not needed, and therefore we need a way to mark where we should start and finish gradient calculations. The API could just as well have exposed a <code>.start()</code> and <code>.stop()</code> function.</p>
<p>The second point is less easy to understand. My initial assumption was that perhaps the gradients were being approximated numerically. However, after doing some research I understand this is not the case, and that something called <em>Automatic Differentiation</em> is used instead. Numerical methods suffer from numerical errors due to finite precision, so it is not particularly surprising that a potentially numerically unstable algorithm is not used here.</p>
<p>What I don't understand is how to connect the dots. A context manager is used to mark the beginning and end of where some gradient calculations must be performed, and those calculations are calculated using Automatic Differentiation.</p>
<p>I don't think I have a particularly intuitive understanding of how Automatic Differentiation works, either. I understand that the chain rule is used, but that it is <strong>not</strong> symbolic differentiation. So what does Tensorflow do here? (Having read some references on Auto Diff, I can't see how it is in any way different to chain rule application of symbolic differentiation. So possibly I am missing something here.)</p>
<ul>
<li>Does it effectively have a big list of <code>if-elseif</code> statements to calculate the right function? For example, if Tensorflow sees an expression like <code>matmul(x, y)</code> does it just contain logical rules which say that <code>d_matmul_d_arg1 = arg2</code>, and <code>d_matmul_d_arg2 = arg1</code>?</li>
<li>And then to actually calculate the <em>value</em> of the gradient, does it just run the calculation for <code>d_matmul_d_arg1(arg2)</code> in the same way that it would calculate the value of <code>matmul(x, y)</code> with <code>x=...</code>, <code>y=...</code> using the usual numerical algorithm for matrix multiplication?</li>
</ul>
<p>Basically what is it doing, and how does it work at a low level?</p>
","0","Question"
"79608270","","<p>i am trying to validate a tensorflow code for simple timeseries prediction in where:</p>
<pre><code>X = np.arange(0, 2000, 0.5)
y = 2 * np.sin(X) + 0.8 * np.random.rand(X.shape[0])
</code></pre>
<p>and the following parameters for the timeseries:</p>
<pre><code>LOOK_BACK = 100

FORECAST_HORIZON = 100
</code></pre>
<p>any model I use or try gets very bad performance, LSTM, MLP, CNN...
loss is not improved in any epoch.</p>
<pre><code> def create_dataset(X_data, y_data, shuffle=False, repeat=False):
    # Ensure input and output are tensors
    X_data = tf.convert_to_tensor(X_data, dtype=tf.float32)
    y_data = tf.convert_to_tensor(y_data, dtype=tf.float32)

    # Create windowed dataset
    dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
        data=tf.concat([X_data, y_data], axis=-1),  # combine to keep alignment
        targets=None,
        sequence_length=look_back + forecast,
        sequence_stride=1,
        shuffle=shuffle,
        batch_size=1,
    )

    def split_input_target(sequence):
        input_seq = sequence[:, :look_back, : X_data.shape[-1]]
        target_seq = sequence[:, look_back:, X_data.shape[-1] :]
        return input_seq, target_seq

    dataset = dataset.map(split_input_target)

    return dataset

train_dataset = create_dataset(X_train, y_train, shuffle=True, repeat=True)
val_dataset = create_dataset(X_val, y_val)
test_dataset = create_dataset(X_test, y_test)

return train_dataset, val_dataset, test_dataset
</code></pre>
<p>Model is as follows:</p>
<pre><code>def build_and_compile_model(num_features, out_steps, targets):
    model = tf.keras.Sequential(
        [
            tf.keras.layers.Input(shape=(LOOK_BACK, num_features)),
            tf.keras.layers.LSTM(128),
            tf.keras.layers.Dense(out_steps * targets),
            tf.keras.layers.Reshape((out_steps, targets)),
        ]
    )

    model.compile(
        loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.RMSprop(0.001), metrics=[&quot;mse&quot;]
    )
    return model
</code></pre>
<p>The following code is what i am currently trying:</p>
<pre><code>import matplotlib.pyplot as plt
from keras.callbacks import CSVLogger
import tensorflow as tf
import numpy as np


BATCH_SIZE = 16
LOOK_BACK = 100
FORECAST_HORIZON = 100

seed = 101
tf.keras.utils.set_random_seed(seed)


X = (np.arange(0, 2000, 0.5)).reshape(-1, 1)
y = 20 * np.sin(X)


#### DEF FUNCTIONS ####


def manual_train_test_split(X, y, train_size=0.7, val_size=0.1):
    total_len = len(X)

    train_end = int(total_len * train_size)
    val_end = train_end + int(total_len * val_size)

    X_train, y_train = X[:train_end], y[:train_end]
    X_val, y_val = X[train_end:val_end], y[train_end:val_end]
    X_test, y_test = X[val_end:], y[val_end:]

    return X_train, X_val, X_test, y_train, y_val, y_test


def make_time_series_datasets(
    X_train, y_train, X_val, y_val, X_test, y_test, look_back, forecast, batch_size
):
    def create_dataset(X_data, y_data, shuffle=False, repeat=False):
        # Ensure input and output are tensors
        X_data = tf.convert_to_tensor(X_data, dtype=tf.float32)
        y_data = tf.convert_to_tensor(y_data, dtype=tf.float32)

        # Create windowed dataset
        dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
            data=tf.concat([X_data, y_data], axis=-1),  # combine to keep alignment
            targets=None,
            sequence_length=look_back + forecast,
            sequence_stride=1,
            shuffle=shuffle,
            batch_size=1,
        )

        def split_input_target(sequence):
            input_seq = sequence[:, :look_back, : X_data.shape[-1]]
            target_seq = sequence[:, look_back:, X_data.shape[-1] :]
            return input_seq, target_seq

        dataset = dataset.map(split_input_target)

        return dataset

    train_dataset = create_dataset(X_train, y_train, shuffle=True, repeat=True)
    val_dataset = create_dataset(X_val, y_val)
    test_dataset = create_dataset(X_test, y_test)

    return train_dataset, val_dataset, test_dataset


def build_and_compile_model(num_features, out_steps, targets):
    model = tf.keras.Sequential(
        [
            tf.keras.layers.Input(shape=(LOOK_BACK, num_features)),
            tf.keras.layers.LSTM(128),
            tf.keras.layers.Dense(out_steps * targets),
            tf.keras.layers.Reshape((out_steps, targets)),
        ]
    )

    model.compile(
        loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.RMSprop(0.001), metrics=[&quot;mse&quot;]
    )
    return model


def build_and_compile_mlp_model(input_dim, output_dim):
    model = tf.keras.Sequential(
        [
            tf.keras.layers.Input(shape=(input_dim,)),
            tf.keras.layers.Dense(128),
            tf.keras.layers.Activation(&quot;relu&quot;),
            tf.keras.layers.Dense(
                64,
            ),
            tf.keras.layers.Activation(&quot;relu&quot;),
            tf.keras.layers.Dense(output_dim),
        ]
    )

    model.compile(optimizer=&quot;adam&quot;, loss=&quot;mae&quot;, metrics=[&quot;mape&quot;])
    model.summary()
    return model


#### START ####


X_train, X_val, X_test, y_train, y_val, y_test = manual_train_test_split(X, y)


train_dataset, val_dataset, test_dataset = make_time_series_datasets(
    X_train,
    y_train,
    X_val,
    y_val,
    X_test,
    y_test,
    LOOK_BACK,
    FORECAST_HORIZON,
    BATCH_SIZE,
)


csv_logger = CSVLogger(&quot;training.log&quot;, separator=&quot;,&quot;, append=False)

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor=&quot;val_loss&quot;, patience=100000, restore_best_weights=True
)
model = build_and_compile_model(
    num_features=X_train.shape[-1], out_steps=FORECAST_HORIZON, targets=1
)

history = model.fit(
    train_dataset,
    epochs=20,
    callbacks=[csv_logger, early_stopping],
    validation_data=val_dataset,
).history


# Helper function to plot input sequence, true future, and predicted future
def multi_step_output_plot(true_future, prediction):
    plt.figure(figsize=(18, 6))
    print(np.shape(true_future))

    plt.plot(true_future, label=&quot;True Future&quot;)
    plt.plot(prediction, label=&quot;Predicted Future&quot;)

    plt.legend(loc=&quot;upper left&quot;)
    plt.xlabel(&quot;Time Step&quot;)
    plt.ylabel(&quot;Value&quot;)
    plt.title(&quot;Multi-Step Time Series Forecasting&quot;)
    plt.grid(True)
    plt.show()


# Take one batch from validation dataset
for x_batch, y_batch in val_dataset.take(1):
    input_seq = x_batch  # First sequence in batch
    true_future = y_batch[0]  # Corresponding true future values
    prediction = model.predict(input_seq)[0]  # Shape: (output steps,)
    print(y_batch[0])
    print(prediction)

    multi_step_output_plot(np.squeeze(true_future), np.squeeze(prediction))
</code></pre>
","1","Question"
"79609185","","<p>Im trying to implement a simple linear regression algo, and for that i've written two functions:</p>
<ul>
<li>Cost function</li>
<li>Gradient descent</li>
</ul>
<p>Cost function appears to work normally, as in it's not giving me unusually large or unusually small numbers.<br />
Whereas the gradient descent function is making the parameters suddenly jump to riddiculous numbers.</p>
<h3>Cost function code:</h3>
<pre><code>m = len(x) # Training examples
def cost(w, b, x, y):
    j_wb = 0.
    for i in range(m):
        f_wb = w * x[i] + b
        err = f_wb - y[i]
        j_wb += err**2
    j_wb = j_wb / (m * 2)
return j_wb
</code></pre>
<p>Here's a test of it:</p>
<pre><code>at itteration 0: COST = 0.0
at itteration 1: COST = 11.906281776347848
at itteration 2: COST = 24.406303301163156
at itteration 3: COST = 25.89142657618535    
at itteration 4: COST = 31.71690104577324
at itteration 5: COST = 32.222444954452776
at itteration 6: COST = 52.79887560513525
at itteration 7: COST = 57.723294484239304
at itteration 8: COST = 59.252477506721256
at itteration 9: COST = 61.178601048944415
Final cost: 4.5292025547813575
</code></pre>
<h3>Gradient decent function code:</h3>
<pre><code>def gradient(w, b, x, y, itterations, alphar):
# Initialize
dj_dw = 0
dj_db = 0

# Gradient descent
for i in range(itterations):
    j_wb = cost(w, b, x, y)
    dj_dw = j_wb * x[i]
    dj_db = j_wb
    w = w - alphar * dj_dw
    b = b - alphar * dj_db
return dj_dw, dj_db
</code></pre>
<p>And now here is where <strong>it returns the main problem:</strong></p>
<pre><code>&gt; Itteration 0 || Cost = 4.5292025547813575 || w = -0.08700861314752584
&gt; Itteration 1 || Cost = 1919.5314293706836 || w = -959.8527232984893
&gt; Itteration 2 || Cost = 1540639463.935084 || w = -231096879.44298592
&gt; Itteration 3 || Cost = 8.924767986122691e+19 || w = -3.3914118347497325e+19
&gt; Itteration 4 || Cost = 1.9197504655865147e+42 || w = -1.6701829050602676e+42
&gt; Itteration 5 || Cost = 4.653919574489622e+87 || w = -1.675411046816264e+87
&gt; Itteration 6 || Cost = 4.685387293902403e+177 || w = -5.622464752682884e+176
&gt; Itteration 7 || Cost = inf || w = -inf
&gt; Itteration 8 || Cost = nan || w = nan
&gt; Itteration 9 || Cost = nan || w = nan
&gt; Itteration 10 || Cost = nan || w = nan
</code></pre>
<p>Here are the first 10 itterations. On itteration 2 the cost suddenly shoots up to ~2000 and at itteration 3 the parameter w is at a very low number.</p>
<p>Then it climaxes at itteration 7 where they're both infinity.</p>
<p>Can someone please guide me to where i'm going wrong? Any help or advice will be greatly appreciated</p>
","0","Question"
"79609919","","<p>Imagine a simple problem: make a function that gets a month index as input (zero-based: 0=Jan, 1=Feb, etc) and outputs number of days in this month (leap year ignored).</p>
<p>Of course, using NN for that task is an overkill, but I wondered, can NN actually be trained for that. Education purposes only.</p>
<p>In fact, it is possible to hand-tailor the accurate solution. I.e.</p>
<pre class=""lang-none prettyprint-override""><code>model = Sequential(
    Linear(1, 10),
    ReLU(),
    Linear(10, 5), 
    ReLU(),
    Linear(5, 1),    
)

state_dict = {
    '0.weight': [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]],
    '0.bias':   [ 0, -1, -2, -3, -4, -5, -7, -8, -9, -10],
    '2.weight': [
        [1, -2,  0,  0,  0,  0,  0,  0,  0,  0],
        [0,  0,  1, -2,  0,  0,  0,  0,  0,  0],
        [0,  0,  0,  0,  1, -2,  0,  0,  0,  0],
        [0,  0,  0,  0,  0,  0,  1, -2,  0,  0],
        [0,  0,  0,  0,  0,  0,  0,  0,  1, -2],
    ],
    '2.bias':   [0, 0, 0, 0, 0],
    '4.weight': [[-3, -1, -1, -1, -1]],
    '4.bias' :  [31]
}
model.load_state_dict({k:torch.tensor(v, dtype=torch.float32) for k,v in state_dict.items()})

inputs = torch.tensor([[0],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11]], dtype=torch.float32)
with torch.no_grad():
    pred = model(inputs)
print(pred)
</code></pre>
<p>Output:</p>
<pre><code>tensor([[31.],[28.],[31.],[30.],[31.],[30.],[31.],[31.],[30.],[31.],[30.],[31.]])
</code></pre>
<p>Probably more compact and elegant solution is possible, but the only thing I care about is that optimal solution actually exists.</p>
<p>Though it turns out that it's totally impossible to train NN. Adding more weights and layers, normalizing input and output and adjusting loss function doesn't help at all: it stucks on a loss around 0.25, and output is something like &quot;every month has 30.5 days&quot;.</p>
<p>Is there any way to make training process smarter?</p>
","0","Question"
"79615828","","<p>I have a machine learning model and I calculated SHAP on it using following code:</p>
<pre><code>import shap

background = shap.kmeans(X_dev, k=100)
explainer = shap.TreeExplainer(model, feature_perturbation=&quot;interventional&quot;, model_output='probability', data=background.data)

shap_values = explainer.shap_values(X_val)
</code></pre>
<p>I observed that when I do <code>explainer.model.predict()</code> and <code>model.predict()</code> both predictions don't match.</p>
<pre><code>model.predict_proba(X_val)[:10]
&gt;&gt;
array([[0.90563095, 0.09436908],
       [0.675441  , 0.324559  ],
       [0.7728198 , 0.22718018],
       [0.00906086, 0.99093914],
       [0.5687084 , 0.4312916 ],
       [0.146478  , 0.853522  ],
       [0.21917653, 0.78082347],
       [0.871528  , 0.12847197],
       [0.34144473, 0.65855527],
       [0.25084436, 0.74915564]], dtype=float32)

explainer.model.predict(X_val)[:10]
&gt;&gt;&gt;
array([0.09436912, 0.32455891, 0.22718007, 0.99093911, 0.43129163,
       0.85352203, 0.7808235 , 0.12847196, 0.65855535, 0.74915555])
</code></pre>
<p><strong>Question:</strong> Why is there a difference in predictions after a few decimal points?? Why aren't they exactly similar?</p>
<p><strong>What I observed:</strong> I have also observed that this difference increases if you train on bigger dataset. Like if you use larger dataset to train a model, like with 1 millon rows and 100s of columns, this differece in prediction increases. The example which I showed above is created on Titanic dataset (which is very small) but if you try it on larger dataset, the difference will be massive.</p>
<p><strong>What I tried:</strong></p>
<ul>
<li>Tried both TPD and Interventional SHAP.</li>
<li>Tried comparing raw scores.</li>
<li>Tried using xgb.DMatrix to predict on the model.</li>
<li>Tried making prediction using XGBClassifier API on pd.DataFrame data (not DMatrix).</li>
</ul>
<p><strong>Minimal reproducible code:</strong></p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb
import shap

# Load Titanic dataset
data = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')

# Preprocessing
data = data.drop(['Name', 'Ticket', 'Cabin'], axis=1)
data = pd.get_dummies(data, columns=['Sex', 'Embarked'], drop_first=True)
data = data.fillna(data.median())

X = data.drop('Survived', axis=1)
y = data['Survived']

#----------
# If you want to use large data to see big difference
#n_samples = 1000000
#n_features = 150
#X_large = pd.DataFrame(np.random.rand(n_samples, n_features), columns=[f'feature_{i}' for i in range(n_features)])
#y_large = pd.Series(np.random.randint(0, 2, size=n_samples), name='target')
#X_dev_large, X_val_large, y_dev_large, y_val_large = train_test_split(X_large, y_large, test_size=0.2, random_state=42)
#----------

X_dev, X_val, y_dev, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
dtrain = xgb.DMatrix(X_dev, label=y_dev)
dval = xgb.DMatrix(X_val, label=y_val)

params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'max_depth': 4,
    'eta': 0.1
}
model = xgb.train(params, dtrain, num_boost_round=100)
model.save_model('xgboost_model.json')

from xgboost import XGBClassifier

loaded_model = XGBClassifier()
loaded_model.load_model('xgboost_model.json')

background = shap.kmeans(X_dev, k=100)
explainer = shap.TreeExplainer(loaded_model, feature_perturbation=&quot;interventional&quot;, model_output='probability', data=background.data)
shap_values = explainer.shap_values(X_val)

print(loaded_model.predict_proba(X_val)[:10])
print(explainer.model.predict(X_val)[:10])
</code></pre>
<p><strong>Other details:</strong></p>
<ul>
<li><p>Shap version: <code>0.47.2</code> (I also tried 0.37 something)</p>
</li>
<li><p>Python version: <code>3.9.21</code></p>
</li>
<li><p>XGBoost version: <code>2.1.1</code> (tried 1.7 as well)</p>
</li>
</ul>
","1","Question"
"79618769","","<p>I am currently working on a convolutional neural network in python, and I am currently implementing back propagation. As of now I am just implementing it for the output layer, and I just wrote the code to calculate the derivatives of the bias values.</p>
<p>Here is the code:</p>
<pre><code> def back_prop(self, learning_rate, d_values = None, desired_outputs = None):
        self.d_biases = np.zeros((desired_outputs.size, self.biases.size))
        self.d_weights = np.zeros((desired_outputs.size, *self.weights.shape)) 
        
        #checks if the layer is an output to determine the method for finding the derivatives
        if self.is_output_layer:
            
            #finds the derivatives of all of the biases in the output layer given respects to the loss 
            self.d_biases = np.array([[float(softmax(self.output)[i][x] - 1) if desired_outputs[i] == x else float(softmax(self.output)[i][x]) for x in range(self.biases.size)] for i in range(desired_outputs.size)])
            self.d_biases *= learning_rate
            self.d_biases = np.sum(self.d_biases, axis = 0, keepdims = True) 

        #updates parameters
        self.biases += self.d_biases
        
</code></pre>
<p>desired_outputs is an array of the class indices of a training batch and self.output is a 2d array organized by batches x classes. To calculate loss, I use cross entropy and here is the code:</p>
<pre><code>def check_loss(self, target_indices):
        target_percents = self.output[range(len(self.output)),target_indices]
        target_percents = np.clip(target_percents, 1e-7, 1-1e-7)
        self.losses = -np.log(target_percents)
        return np.mean(self.losses)
</code></pre>
<p>target_indices will be the same variable as desired_outputs.</p>
<p>Also, check_loss() is in a different class than back_prop() but self.output is still the same value for both.</p>
<p>I am using a simple spiral data pattern from the internet with and x and a y input and 3 classes with no hidden layers to test the code but every time I attempt to propagate a batch through the model the loss increases.</p>
<p>I believe it maybe has something to do with how I am calculating the derivatives, but I checked and to my understanding it works.</p>
<p>According to my knowledge, the derivative for a bias within the output layer is softmax activation of that neuron minus the predicted value for that activation but I could be wrong.</p>
<p>Any clue what is wrong?</p>
<p>Edit:</p>
<p>I changed the code to subtract the biases and changed how I calculated the gradients and now the loss decreases as it is supposed to.</p>
<p>Here is the edited code:</p>
<pre><code>def back_prop(self, learning_rate, d_values = None, desired_outputs = None):
    
    #checks if the layer is an output to determine the method for fidning the derivatives
    if self.is_output_layer:
        
        #finds the derivatives of all of the biases in the output layer given respects to the loss 
        self.d_biases = softmax(self.output)
        self.d_biases[range(desired_outputs.size), desired_outputs] -= 1
        self.d_biases *= learning_rate
        self.d_biases = np.sum(self.d_biases, axis = 0, keepdims = True) 

        #finds the derivatives of all of the weights in the output layer given respects to the loss

    

    #updates parameters
    self.biases -= self.d_biases
    
  
    return self.biases
</code></pre>
<p>I am open to any other suggestions for improving it but otherwise it is working as it is supposed to.</p>
","-1","Question"
"79619113","","<p>I'm using fine-tuned T5 model for performing spell checks in my dataset of consisting of reviews. However, I'm facing an issue where the model when performing spell checks does not give entire string as an output or sometimes repeats the phrases of the given review. It is not in large amounts but there do exists some reviews. During the fine-tuning of the model, the <strong>training and validation losses were 0.0003 and 0.0002 respectively</strong>. I have attached my code and 2 reviews as well for your reference.</p>
<pre><code>class ReviewDataset(Dataset):
    def __init__(self, texts):
        self.inputs = [&quot;fix: &quot; + text for text in texts]

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        return self.inputs[idx]

def collate_fn(batch):
    encodings = tokenizer(
        batch,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors=&quot;pt&quot;
    )
    return encodings
</code></pre>
<p>The for loop which performs the task is:</p>
<pre><code>all_predictions = []

with torch.no_grad():
    for batch in dataloader:
        input_ids = batch[&quot;input_ids&quot;].to(device)
        attention_mask = batch[&quot;attention_mask&quot;].to(device)

        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=64)
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        all_predictions.extend(decoded)```
</code></pre>
<p>The reviews are below:
1)As per the global review I purchased this product. Also I used this product for 5 times from my 1st purchase of bottle.. Suddenly the product color changes into golden and shimmery.. While I was spray the product on my face it was exhausted in a shimmering liquid form. It was a real shock to me.. It's a big problem.....</p>
<p><em>As per the global review I purchased this product. Also I used this product for 5 times from my first purchase of bottle..Suddenly the product color changes into golden and shimmery.. While I was spraying the product on my face it was exhausted in a shimmering liquid form. It was a</em></p>
<p>2)I am literally not so happy with this product I am disappointed with this product</p>
<p><em>I am literally not so happy with this product I am disappointed with this product I am disappointed with this product.</em></p>
","0","Question"
"79621002","","<p>I have trained a classification model using yolo11n , now i have to write its inference script , but the problem is for development and deployment I have python version 3.6, but yolo and ultralytics supports python version 3.8 and above only , I have tried to convert the classification model i.e .pt file into torchscript to remove this dependency issue , but converting into torchscript is lowering the accuracy of the model , is there any way , or any source I can refer to resolve this issue? My requirement is to use .pt file directly  to infer my model on python version 3.6 without loading model using ultralytics.</p>
<p>I tried different approches</p>
<ol>
<li>Converting the classification .pt file into torchscript but is is lowering the accuracy.</li>
<li>I tries rebuilding the architecture for yolo11 and then extarcting weights</li>
</ol>
","1","Question"
"79621225","","<p>I am trying to make a project using Intel Realsense d415 that will allow me to determine the coordinate, conventionally the tip of my index finger in coordinates based on ArUco tags, for brush capture I would like to use MediaPipe.</p>
<p>Could you please tell me what direction to go in, or how I can determine the distance from the camera to the marks and from the camera to the finger, and then knowing the distance translate everything into an ArUco based coordinate system....</p>
<p>Sorry, no code yet, I'll attach a rough schematic below...
I am using Windows 10, Python 3.9..</p>
<p><img src=""https://i.sstatic.net/kEE6qaAb.jpg"" alt=""enter image description here"" /></p>
","-2","Question"
"79622840","","<p>For Design of experiment I am trying to do some Data-processing in python using sklearn. Therefore I need to make a multivariate polynomial regression. The code is based on <a href=""https://saturncloud.io/blog/multivariate-polynomial-regression-with-python/"" rel=""nofollow noreferrer"">https://saturncloud.io/blog/multivariate-polynomial-regression-with-python/</a>.</p>
<p>So <strong>for my specific task I need to &quot;kill&quot; some coefficients. Is it possible to do so?</strong>
For example let's say I have a degree 2 polynomial:</p>
<pre><code>y = a0 + a1 * x1 + a2 * x2 + a3 * x1 * x2 + a4 * x1^2 + a5 * x2^2
</code></pre>
<p>I want to kill x1 * x2 and x2^2 which leads to a3 and a5 being 0. My final function should look like:</p>
<pre><code>y = a0 + a1 * x1 + a2 * x2 + a4 * x1^2
</code></pre>
<p>I want to do so before model.fit() so the terms would be ignored while fit. Would be nice if someone had an idea of doing so.
I know there is the possibility to use Lasso instead of Linear Regression to reduce coefficients but not in the way I would like to do.</p>
<p>Here is the code:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.datasets import fetch_california_housing

# Load the California housing dataset
california = fetch_california_housing()
df = pd.DataFrame(california.data, columns=california.feature_names)
df['PRICE'] = california.target

# Select appropriate features from the dataset
# Assuming you want to use features like 'MedInc' (median income), 'HouseAge', and 'AveRooms' (average rooms)
X = df[['MedInc', 'HouseAge', 'AveRooms']].values
y = df['PRICE'].values

# Polynomial features
poly = PolynomialFeatures(degree=5)
X_poly = poly.fit_transform(X)

# Linear Regression model
model = LinearRegression()
model.fit(X_poly, y)

# New data with median income, house age, and average number of rooms
new_data = np.array([[3, 20, 5]])  # Example values for median income, house age, and average rooms
new_data_poly = poly.transform(new_data)

# Predicting the price
predicted_price = model.predict(new_data_poly)
print(predicted_price)
</code></pre>
","0","Question"
"79623456","","<p>I am a little bit lost in tidymodels. I have a some data from topicmodeling:</p>
<ul>
<li>prevalent_topic: factor variable with most prevalent topic, ranging from &quot;Topic_1&quot; to &quot;Topic_5&quot;</li>
<li>value1 and value2: two numeric variables used as predictor</li>
</ul>
<p>I want to predict/classify the prevalent_topic based on value1 and value2:</p>
<pre><code>prevalent_topic ~ value1 + value2
</code></pre>
<p>I started with multiclass classification using glmnet and nnet with tidymodels. Now I want to try &quot;one-vs-rest&quot; binary classification and created a recipe to begin with:</p>
<pre><code>dfFT_rec &lt;- recipe( ~ value1 + value2, data = dfFT_train) %&gt;%
  step_dummy(prevalent_topic, one_hot = TRUE) %&gt;%
  step_normalize(c(value1, value2)) 
</code></pre>
<p>The second step creates dummy variables that I would like to use as outcome, e.g. &quot;prevalent_topic_Topic_1&quot;, &quot;&quot;prevalent_topic_Topic_2&quot;, ...</p>
<p>I tried to update the recipe's formula to use &quot;prevalent_topic_Topic_1 ~ value1 + value2&quot; but that did not work. I also tried to fit a workflow to my data without specifying the outcome but only got an error: &quot;<code>logistic_reg()</code> was unable to find an outcome.&quot;</p>
<p>Is this possible at all? Or is there a different way to turn an outcome factor variable into dummy-coded outcome variables?</p>
","0","Question"
"79628713","","<p>I got an error in jupyternotebook said that
<code>RuntimeError: Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.</code></p>
<p>This is the result I got in jupyter notebook</p>
<pre class=""lang-py prettyprint-override""><code>torch.cuda.is_available() # True
torch.cuda.device_count() # 0
sys.executable # '/home/quiet98k/anaconda3/envs/lol/bin/python'
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] # -1
</code></pre>
<p>This is what I got in python terminal</p>
<pre class=""lang-py prettyprint-override""><code>Python 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import sys
&gt;&gt;&gt; torch.cuda.is_available()
True
&gt;&gt;&gt; torch.cuda.device_count()
1
&gt;&gt;&gt; torch.cuda.get_device_name(0)
'NVIDIA GeForce RTX 3070'
&gt;&gt;&gt; sys.executable
'/home/quiet98k/anaconda3/envs/lol/bin/python'
</code></pre>
<p>I'm sure that cuda is available in my wsl</p>
<pre><code>└─[$] &lt;git:(main*)&gt; nvidia-smi                             
Mon May 19 05:06:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 572.83         CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3070        On  |   00000000:01:00.0  On |                  N/A |
| 53%   33C    P5             31W /  280W |    2371MiB /   8192MiB |     34%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      5838      C   /python3.10                                 N/A      |
+-----------------------------------------------------------------------------------------+
</code></pre>
<p>I tried to reinstall pytorch and even the entire python environment, but the problem remains. What is wrong here?</p>
","2","Question"
"79631112","","<p>I'm trying to use LeNet to classify univariate time series data with 300 time steps.</p>
<pre><code>num_channels = 1;
num_classes = 3;
filterSize = 5;
numFilters = 32;
num_of_features = size(X_train(1, :), 2);
assert(num_of_features==300);

net = dlnetwork;

LeNeT = [
    % featureInputLayer(num_of_features, &quot;Name&quot;, &quot;input&quot;)
    sequenceInputLayer(num_of_features, &quot;Name&quot;, &quot;input&quot;, &quot;MinLength&quot;, num_of_features)
    convolution1dLayer( 5, 6,&quot;Name&quot;,&quot;conv1&quot;)
    tanhLayer(&quot;Name&quot;,&quot;tanh1&quot;)
    averagePooling1dLayer( 2, &quot;Name&quot;,&quot;pool1&quot;,&quot;Stride&quot;, 2)
    tanhLayer(&quot;Name&quot;,&quot;tanh2&quot;)
    convolution1dLayer( 5, 16,&quot;Name&quot;,&quot;conv2&quot;)
    tanhLayer(&quot;Name&quot;,&quot;tanh3&quot;)
    averagePooling1dLayer( 2,&quot;Name&quot;,&quot;pool2&quot;,&quot;Stride&quot;, 2)
    tanhLayer(&quot;Name&quot;,&quot;tanh4&quot;)
    convolution1dLayer( 5, 120,&quot;Name&quot;,&quot;conv3&quot;)
    tanhLayer(&quot;Name&quot;,&quot;tanh5&quot;)
    fullyConnectedLayer( 84, &quot;Name&quot;,&quot;fc1&quot;)
    tanhLayer(&quot;Name&quot;,&quot;tanh6&quot;)
    fullyConnectedLayer( num_classes, &quot;Name&quot;,&quot;new_fc&quot;)
    softmaxLayer(&quot;Name&quot;,&quot;prob&quot;)];

net = addLayers(net,LeNeT);
net = initialize(net);

train_opts = trainingOptions(&quot;adam&quot;, ...
    MaxEpochs=20, ...
    MiniBatchSize=128, ... 
    InitialLearnRate=0.01, ...
    Shuffle=&quot;once&quot;, ...
    ValidationData={X_val,Y_val}, ...
    Plots=&quot;none&quot;, ...
    Metrics=&quot;accuracy&quot;, ...
    Verbose=true);

%% Train the network.

net = trainnet(X_train, Y_train, net, &quot;crossentropy&quot;, train_opts);

%% Test the network.

accuracy = testnet(net, X_test, Y_test, &quot;accuracy&quot;);
scores = minibatchpredict(net, X_test);
predicted_labels = scores2label(scores, categories(Y));
figure
confusionchart(Y_test, predicted_labels)

</code></pre>
<p>The dimension of the predictors (X) and targets (Y):</p>
<ul>
<li>X_train = 11788 x 300 double (array: the rows represent the samples and the columns represent the time steps)</li>
<li>Y_train = 11788 x 1 categorical (categorical labels: the rows are the samples and include three types of categories)</li>
<li>X_val = 5894 x 300 double</li>
<li>Y_val = 5894 x 1 categorical</li>
<li>X_test = 5895 x 300 double</li>
<li>Y_test = 5895 x 1 categorical</li>
</ul>
<p>Running the code above results in this error message:</p>
<pre><code>Training stopped: Error occurred
Error using trainnet (line 54)
Error evaluating loss function.

Error in main (line 158)
net = trainnet(X_train, Y_train, net, &quot;crossentropy&quot;, train_opts);
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Caused by:
    Error using validateTrueValues (line 54)
    Size of predictions and targets must match.
    Size of predictions:
        3(C) × 1(B) × 2940(T)
    Size of targets:
        3(C) × 1(B) × 11788(T)
 
&gt;&gt; 
</code></pre>
<p>The problem is most likely the way I formatted the data, but I just can't figure out exactly what the issue is. To be honest, I'm not sure about the difference between a 'channel' and a 'feature,' so maybe the source of the problem lies in me setting the <code>inputSize</code> parameter of the <code>sequenceInputLayer</code> to <code>num_of_features</code>. However, according to the MATLAB documentation, 'For vector sequence input, inputSize is a scalar corresponding to the number of features.'</p>
<p>Originally, I formatted the data into a cell array (with dimension 11788 x 1 cell), where each predictor cell (each sample) held a 300 step long vector (300 x 1 double). The target formatting was the same. For this approach, I followed the example from this <a href=""https://www.mathworks.com/help/deeplearning/ug/classify-sequence-data-using-lstm-networks.html"" rel=""nofollow noreferrer"">link</a>, so my data was formatted very similarly to the data used in this example, but I encountered a similar error message described above.</p>
","1","Question"
"79641499","","<p>I have a YOLO v11 model that is predicting a single class from images. In some cases it is misinterpreting the image and predicting multiple of the class in one image while there is only one.  I have noticed that the correct one has the highest confidence value.
Here is my code for predicting:</p>
<pre><code>results = model([imagepath+imgs[997]])  # return a list of Results objects
for result in results:
    boxes = result.boxes

    result.show()

    print(boxes.conf.tolist())
    print(boxes.data.tolist())

    print(boxes)
</code></pre>
<p>Here is the output:</p>
<pre><code>0: 640x608 4 numberplates, 235.7ms
Speed: 9.1ms preprocess, 235.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 608)
</code></pre>
<p><a href=""https://i.sstatic.net/Q2WTOznZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Q2WTOznZ.png"" alt=""Output image"" /></a></p>
<pre><code>cls: tensor([0.,0.,0.,0.])
conf: tensor([0.8853,0.4671,0.4509,0.3296])
data: tensor([[ 61.6508,256.6027,232.8067,290.9552,0.8853,0.0000],[144.6730, 170.1858,296.8442, 229.6742,0.4671,0.0000],
[ 13.3585,179.2076,177.5936,238.9743,0.4509,0.0000],[127.6127, 176.1886, 282.9232,233.7913,0.3296,0.0000]])
</code></pre>
<p>The box with 0.8853 confidence shows up as the first entry in the conf tensor but shows up last at the data tensor. How do I choose the box with the highest confidence and find its data?</p>
","-2","Question"
"79645357","","<p>I'm using <code>open-images-v7</code> dataset (accessing via <code>fiftyone</code> lib) and <code>keras_cv</code> lib to fine tune <code>DeepLabV3Plus</code> with <code>mobilenet_v3_small</code> backbone, but the accuracy doesn't improve with epochs at all, and I'm getting shape warning.</p>
<p><strong>Dataset Preparation Code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

def preprocess_sample(sample):
    img = tf.io.read_file(sample[&quot;filepath&quot;])
    img = tf.image.decode_jpeg(img, channels=3)
    img.set_shape([None, None, 3])
    img = tf.image.resize(img, (512, 512))
    # img = img / 255.0  # Normalize

    for detection in sample.ground_truth.detections:
        if detection.label == 'Vehicle registration plate':
            mask = detection.mask.astype(np.float32)
            break
    mask = tf.expand_dims(mask, axis=-1)
    mask.set_shape([None, None, 1])
    mask = tf.image.resize(mask, (512, 512), method=&quot;nearest&quot;)

    return img, mask

# Convert FiftyOne samples to TF Dataset
tf_train_dataset = tf.data.Dataset.from_generator(
    lambda: (preprocess_sample(s) for s in train_dataset),
    output_signature=(
        tf.TensorSpec(shape=(512, 512, 3), dtype=tf.float32),  # Image
        tf.TensorSpec(shape=(512, 512, 1), dtype=tf.float32),  # Mask
    )
)
tf_val_dataset = tf.data.Dataset.from_generator(
    lambda: (preprocess_sample(s) for s in val_dataset),
    output_signature=(
        tf.TensorSpec(shape=(512, 512, 3), dtype=tf.float32),  # Image
        tf.TensorSpec(shape=(512, 512, 1), dtype=tf.float32),  # Mask
    )
)

# Batch and shuffle
tf_train_dataset = tf_train_dataset.batch(8).prefetch(tf.data.AUTOTUNE)
tf_val_dataset = tf_val_dataset.batch(8).prefetch(tf.data.AUTOTUNE)
</code></pre>
<p><strong>Fine Tuning Code</strong></p>
<pre class=""lang-py prettyprint-override""><code>model = keras_cv.models.DeepLabV3Plus.from_preset(
    &quot;mobilenet_v3_small&quot;,
    input_shape=(512, 512, 3),
    num_classes=1,
    # activation=None  # Remove final activation
)
# outputs = tf.keras.layers.Activation(&quot;sigmoid&quot;)(model.output)
# model = tf.keras.Model(inputs=model.input, outputs=outputs)

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-4), # 'adam'
    loss=&quot;binary_crossentropy&quot;,
    metrics=[&quot;binary_accuracy&quot;] # accuracy
)

# Train
model.fit(
    tf_train_dataset,
    validation_data=tf_val_dataset,
    epochs=5,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
    ],
)
</code></pre>
<p><strong>Error/Output</strong></p>
<pre><code>Epoch 1/5

/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.
Expected: ['keras_tensor']
Received: inputs=Tensor(shape=(None, 512, 512, 3))
  warnings.warn(msg)
/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 512, 512, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?
  warnings.warn(

     13/Unknown 200s 12s/step - binary_accuracy: 0.7506 - loss: 0.6739

/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.
  self._interrupted_warning()

13/13 ━━━━━━━━━━━━━━━━━━━━ 211s 13s/step - binary_accuracy: 0.7506 - loss: 0.6725 - val_binary_accuracy: 0.6972 - val_loss: 0.6890
Epoch 2/5
13/13 ━━━━━━━━━━━━━━━━━━━━ 202s 13s/step - binary_accuracy: 0.7506 - loss: 0.5131 - val_binary_accuracy: 0.6972 - val_loss: 0.6835
Epoch 3/5
13/13 ━━━━━━━━━━━━━━━━━━━━ 182s 14s/step - binary_accuracy: 0.7506 - loss: 0.4288 - val_binary_accuracy: 0.6972 - val_loss: 0.6792
Epoch 4/5
13/13 ━━━━━━━━━━━━━━━━━━━━ 166s 13s/step - binary_accuracy: 0.7506 - loss: 0.3606 - val_binary_accuracy: 0.6972 - val_loss: 0.6756
Epoch 5/5
13/13 ━━━━━━━━━━━━━━━━━━━━ 166s 13s/step - binary_accuracy: 0.7506 - loss: 0.3141 - val_binary_accuracy: 0.6972 - val_loss: 0.6723

</code></pre>
","0","Question"
"79647350","","<p>I have a set of images in (c, h, w) jax arrays. These arrays have been converted to (patch_index, patch_dim) arrays where patch_dim == c * h * w.</p>
<p>I am trying to reconstruct the original images from the patches. Here is vanilla python code that works:</p>
<pre class=""lang-py prettyprint-override""><code>kernel = jnp.ones((PATCH_DIM, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH), dtype=jnp.float32)

def fwd(x):
    xcv = lax.conv_general_dilated_patches(x, (PATCH_HEIGHT, PATCH_WIDTH), (PATCH_HEIGHT, PATCH_WIDTH), padding='VALID')

    # return channels last
    return jnp.transpose(xcv, [0,2,3,1])

patches = fwd(bfrc)

patch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))

# V_PATCHES == IMG_HEIGHT // PATCH_HEIGHT
# H_PATCHES == IMG_WIDTH // PATCH_WIDTH

reconstructed = np.zeros(EXPECTED_IMG_SHAPE)

for vpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[0]):
    for hpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[1]):
        for ch in range(0, patch_reshaped_ph_pw_c_h_w.shape[2]):
            for prow in range(0, patch_reshaped_ph_pw_c_h_w.shape[3]):
                for pcol in range(0, patch_reshaped_ph_pw_c_h_w.shape[4]):
                    row = vpatch * PATCH_HEIGHT + prow
                    col = hpatch * PATCH_WIDTH + pcol
                    reconstructed[0, ch, row , col] = patch_reshaped_ph_pw_c_h_w[vpatch, hpatch, ch, prow, pcol]

# This assert passes
assert jnp.max(jnp.abs(reconstructed - bfrc[0])) == 0
</code></pre>
<p>Of course this vanilla python code is very inefficient. How can I convert the for loops into efficient JAX code?</p>
","1","Question"
"79648131","","<p>I am running a training job using MLflow and an MLflow recipe. In the <code>recipe.train</code> step, the training starts an experiment and runs for 350 epochs. After the 350 epochs are completed and I try to log the artifacts, the process gets stuck for a long time and I keep getting this error multiple times:</p>
<pre class=""lang-none prettyprint-override""><code>ValueError: filedescriptor out of range in select()

Sun Jun  1 02:38:07 2025 Connection to spark from PID  4942

Sun Jun  1 02:38:08 2025 Initialized gateway on port 44549

ERROR:py4j.java_gateway:Error while waiting for a connection.

Traceback (most recent call last):

  File &quot;/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py&quot;, line 2316, in run

    readable, writable, errored = select.select(

                                  ^^^^^^^^^^^^^^

ValueError: filedescriptor out of range in select()

Sun Jun  1 02:38:08 2025 Connection to spark from PID  4942

Sun Jun  1 02:38:08 2025 Initialized gateway on port 35473

ERROR:py4j.java_gateway:Error while waiting for a connection.

Traceback (most recent call last):

  File &quot;/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py&quot;, line 2316, in run

    readable, writable, errored = select.select(

                                  ^^^^^^^^^^^^^^

ValueError: filedescriptor out of range in select()
</code></pre>
<p>During this time, the CPU usage reaches almost 100% (98% of which is from system usage), and after an hour or so the <code>recipe.train()</code> step fails with
<code>Fatal error: The Python kernel is unresponsive.</code></p>
<p>While throughout the training step the CPU and GPU usage are below 40% mostly. I am also using the databricks recipe to log the regular artifacts as part of the experiment.</p>
<p>Has anyone faced the above issue? Please let me know if any logs would help in identifying the real problem.</p>
","0","Question"
"79650016","","<p>I'm working on my final studies project, which is detecting anomalies in an irrigation system.
We simulated the irrigation network on a limited environment as you can see in the photo below.<a href=""https://i.sstatic.net/bZph1giU.jpg"" rel=""nofollow noreferrer"">the model we created</a></p>
<p>There is a PCB where an ESP32 microcontroller reads the values from the pressure sensor.
In our model, 4 valves give us 2^4 = 16 combinations. My idea is to make it simple and create a simple neural network model using Pytorch that gets the state of valves as input (0 is off and 1 is on ex:0110) and one output, which is the pressure value. So that means our system will be predicting pressure from the valve combinations. The data set is created by getting certain valve combinations(using fractional factorial design), but I added other combinations like 1000, 0100, 0010, 00001, 1110, and 0111, and then it is augmented to get a large data set.
here is the model :</p>
<pre><code>import torch
import torch.nn as nn   

class PressureNeuralNet(nn.Module):
def __init__(self, n_valves, hidden_size=32, dropout_rate=0.1):
    super().__init__()
    self.network = nn.Sequential(
        nn.Linear(n_valves, hidden_size),
        nn.ReLU(),
        nn.Dropout(dropout_rate),

        nn.Linear(hidden_size, hidden_size),
        nn.ReLU(),
        nn.Linear(hidden_size, 1)
    )

def forward(self, x): 
    return self.network(x)
</code></pre>
<p>learning process:</p>
<pre><code>    nn_model = PressureNeuralNet(n_valves)

    # Define loss and optimizers
    criterion = nn.MSELoss()
    nn_optimizer = optim.Adam(nn_model.parameters(), lr=0.01, weight_decay=1e-5)
    nn_scheduler = optim.lr_scheduler.ReduceLROnPlateau(nn_optimizer, mode='min', patience=5, factor=0.5, verbose=True)

   

    print(&quot;\nTraining Neural Network...&quot;)
    nn_train_losses, nn_test_losses = train_model(
        nn_model, train_loader, test_loader, criterion, nn_optimizer, nn_scheduler
    )
</code></pre>
<p>train model function :</p>
<pre><code>def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler=None, epochs=1000):
train_losses = []
test_losses = []
best_test_loss = float('inf')

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for valve_states, pressures in train_loader:
        optimizer.zero_grad()
        outputs = model(valve_states).squeeze()
        loss = criterion(outputs, pressures)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    avg_train_loss = running_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    
    model.eval()
    test_loss = 0.0
    with torch.no_grad():
        for valve_states, pressures in test_loader:
            outputs = model(valve_states).squeeze()
            loss = criterion(outputs, pressures)
            test_loss += loss.item()
    
    avg_test_loss = test_loss / len(test_loader)
    test_losses.append(avg_test_loss)

    if avg_test_loss &lt; best_test_loss:
        best_test_loss = avg_test_loss
        torch.save(model.state_dict(), 'best_model.pth')
        print(f&quot;Saved best model with Test Loss: {best_test_loss:.4f}&quot;)

    if scheduler:
        scheduler.step(avg_test_loss)
    
    if (epoch + 1) % 10 == 0:
        print(f&quot;Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}&quot;)
        if scheduler:
            print(f&quot;Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}&quot;)

return train_losses, test_losses
</code></pre>
<p>The MSE value is: 0.0004
That meas the model predicts very well. The problem now is that when predicting a combination that it didn't learn from the data set  (1101 read value: 0.53 predicted: 0.10, which is a false alert), due to the sampling method that we used (fractional factorial design).
(If you ask why we used this method is to get certain samples that have influence on the output and to prevent water and energy consumption.)
But the other combinations are well predicted.
there might be a problem because of the values of the single valves combinations they all heve the same value: 0.5 but the combination 0100 has a pressure value : 0.45 .</p>
<p>What I'm asking is whether my method is wrong or if there is a problem in my model.</p>
<p>The motivation of this project is the use it on larger irrigation systems with more valves.</p>
","-1","Question"
"79651091","","<pre><code>!python -V
#%%
import pandas as pd
import pickle
from sklearn.metrics import root_mean_squared_error
from sklearn.feature_extraction import DictVectorizer
import mlflow

def read_dataframe1(filename):
    df = pd.read_parquet(filename)

    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime
    df.duration = df.duration.apply(lambda td: td.total_seconds()/60)

    df = df[df.duration &gt;= 1] &amp; (df.duration &lt;= 60)

    categorical = ['PULocationID', 'DOLocationID']
    df[categorical] = df[categorical].astype(str)

    return df

#%%
train_df = read_dataframe1(&quot;https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet&quot;) 
</code></pre>
<p>When I run the last line for loading the parquet file with the function read_dataframe1, it does not load even after waiting over 30 minutes and the cell keeps running</p>
<p>What can be the reason?</p>
","-3","Question"
"79655017","","<p>I have been trying to tackle a regression problem by training a neural network to predict a continuous variable using r-torch. My question pertains to the syntax used to achieve this.</p>
<p>When initializing the dataset for torch, I have to declare what 'x' and 'y' are. I understand that 'y' is the target variable that the model will aim to predict and that 'x' should contain the predictor variables whose data would be used to make the predictions. What I am unsure about is whether 'y' should be included in the 'x' matrix. Here is what my code looks like so far:</p>
<pre><code>torch_ds &lt;- dataset(
  name = &quot;torch_dataset()&quot;,
  
  initialize = function(df) {
    vars &lt;- df %&gt;% 
      select_if(is.factor) %&gt;% 
      colnames() # get names of categorical variables (all have been turned to factors)
    
    df[vars] &lt;- lapply(df[vars], as.numeric)
    
    self$x &lt;- as.matrix(df) %&gt;% # This x matrix contains the target variable, y
      torch_tensor()
    self$y &lt;- torch_tensor(as.matrix(df$target)) # Defining the target variable
    },
  
  .getitem = function(i) {
    list(x = self$x[i, ], y = self$y[i])
  },
  
  .length = function() {
    dim(self$x)[1]
  }
  
)

# Convert the train and test data frames to torch-compatible tensors:
train_tensor &lt;- torch_ds(train_df)
test_tensor &lt;- torch_ds(test_df)

# Turn those tensors into dataloader objects:
train_dl &lt;- dataloader(train_tensor, batch_size = 100, shuffle = FALSE)
test_dl &lt;- dataloader(test_tensor, batch_size = 100, shuffle = FALSE)


# Save the dimensions entering/exiting the layers of the neural network:
d_in &lt;- 44 # Number of features (columns in dataset)

# dimensionality of hidden layer
d_hidden &lt;- 500

# output dimensionality (number of predicted features)
d_out &lt;- 1

# Structure of the network:
net &lt;- nn_module(
  initialize = function(d_in, d_hidden, d_out) {
    self$net &lt;- nn_sequential(
      nn_linear(d_in, d_hidden),
      nn_relu(),
      nn_linear(d_hidden, d_out)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)


# Fit the network model to the train dataloader object: 

fitted &lt;- net %&gt;% 
  setup(loss = nn_mse_loss(), optimizer = optim_adam, metrics = list(luz_metric_mae(), luz_metric_rmse(), luz_metric_mse())) %&gt;%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %&gt;%
  fit(train_dl, epochs = 1000) 

</code></pre>
","1","Question"
"79657416","","<p>I'm testing a Self-supervise pre-training model, independently of the model I'm getting the same Error. My environment is Jupiter Labs for AWS Sagemaker, the problem comes when, using args and trying to load the data (either from an S3 bucket or a local file on the instance) I keep getting this error:</p>
<pre><code>usage: VICReg training script [-h] [--data_path DATA_PATH] [--exp-dir EXP_DIR]
                              [--log-freq-time LOG_FREQ_TIME] [--arch ARCH]
                              [--mlp MLP] [--epochs EPOCHS]
                              [--batch-size BATCH_SIZE] [--base-lr BASE_LR]
                              [--wd WD] [--sim-coeff SIM_COEFF]
                              [--std-coeff STD_COEFF] [--cov-coeff COV_COEFF]
                              [--num-workers NUM_WORKERS] [--device DEVICE]
                              [--world-size WORLD_SIZE]
                              [--local_rank LOCAL_RANK] [--dist-url DIST_URL]
                              [--rank RANK]
VICReg training script: error: unrecognized arguments: -f /home/sagemaker-user/.local/share/jupyter/runtime/kernel-299646e9-32c7-47f8-869a-72e80aa9271b.json
</code></pre>
<p>On more information about how I'm setting my code, this is where the Error most probably is:</p>
<pre><code>def get_arguments():
    parser = argparse.ArgumentParser(description=&quot;Pretrain a resnet model with VICReg&quot;, add_help=False)

    # Data
    parser.add_argument(&quot;--data_path&quot;,
                        default='AnnualCrop/', type=Path,
                        help='dataset path')

    # Checkpoints
    parser.add_argument(&quot;--exp-dir&quot;, type=Path, default=&quot;./exp&quot;,
                        help='Path to the experiment folder, where all logs/checkpoints will be stored')
    parser.add_argument(&quot;--log-freq-time&quot;, type=int, default=60,
                        help='Print logs to the stats.txt file every [log-freq-time] seconds')
</code></pre>
<p>NOTE: I've tried with other models and codes as well but the same args.pars structure and keep getting the very same error.</p>
","0","Question"
"79657687","","<p>I am working on a ML model using LightGBM classifier with a task to maximise a value( PnL) for a quantitative strategy.
I am entering a trade every Monday and can hold till Friday close.
I want to create labels to maximise optimum exit within the week</p>
<p>I am using the target variable as difference of current rolling pnl vs projected pnl till Friday
like (current_close/monday_open)-1 = current pnl
And than (current_close/friday_close ) = projected pnl
so in a nutshell i am using future leakage for the target variable.
is this approach justified?</p>
<p>Basically if current_pnl&lt;future_pnl i am marking the label as '1'
(means more juice left in the trade, else 0).</p>
<p>is this a good way to use ML?</p>
<p>I have a bunch of 50 features and than this forward looking Target Variable
What should be a good, simple target variable?
I am worried as my target feature is forward looking (future-leakage).</p>
","-1","Question"
"79658224","","<p>I've recently been attempting to fine-tune the pretrained neural network, &quot;cardiffnlp/twitter-roberta-base-offensive.&quot; However, when I attempted to run a Python program I created, &quot;evaluation_strategy&quot; was considered to be an &quot;unexpected argument&quot; regardless that I have version 4.52.6 installed. Could anyone help me with fixing this error? (There also might be another one with the classifiers since I'm attempting to have it transition to a 6-label classifier rather than a binary classifier since the Jigsaw Dataset uses 6 labels, so if that seems incorrect in my code, please fix that as well).</p>
<p>Here's the source code:</p>
<pre><code>
from datasets import load_dataset
from transformers import *
import torch
import numpy as np
from sklearn.metrics import f1_score, accuracy_score
from sklearn.preprocessing import MultiLabelBinarizer

# Load dataset (adjust path if needed)
dataset = load_dataset('csv', data_files={
    &quot;train&quot;: &quot;data/train_split.csv&quot;,
    &quot;validation&quot;: &quot;data/validation_split.csv&quot;
})

# Define label columns used in Jigsaw multi-label setup
LABEL_COLUMNS = [&quot;toxic&quot;, &quot;severe_toxic&quot;, &quot;obscene&quot;, &quot;threat&quot;, &quot;insult&quot;, &quot;identity_hate&quot;]

# Tokenizer and model setup
MODEL_NAME = &quot;cardiffnlp/twitter-roberta-base-offensive&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Define label count and problem type
config = AutoConfig.from_pretrained(
    MODEL_NAME,
    num_labels=len(LABEL_COLUMNS),
    problem_type=&quot;multi_label_classification&quot;
)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    config=config,
    ignore_mismatched_sizes=True
)

# Preprocessing function
def preprocess(example):
    encoding = tokenizer(
        example[&quot;comment_text&quot;],
        truncation=True,
        padding=&quot;max_length&quot;,
        max_length=128
    )
    labels = [example[col] for col in LABEL_COLUMNS]
    encoding[&quot;labels&quot;] = torch.tensor(labels, dtype=torch.float)
    return encoding

# Apply preprocessing
encoded_dataset = dataset.map(preprocess)

# Training configuration
training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    save_strategy=&quot;epoch&quot;,
    evaluation_strategy=&quot;epoch&quot;,   # &lt;-- Add this line
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;f1&quot;
)


# Metric function
def compute_metrics(pred):
    preds = torch.sigmoid(torch.tensor(pred.predictions)).numpy()
    labels = pred.label_ids
    preds = (preds &gt; 0.5).astype(int)
    f1 = f1_score(labels, preds, average=&quot;macro&quot;)
    acc = accuracy_score(labels, preds)
    return {&quot;f1&quot;: f1, &quot;accuracy&quot;: acc}

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=encoded_dataset[&quot;validation&quot;],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()
</code></pre>
<p>Could anyone fix it?</p>
","1","Question"
"79659212","","<p>While working on a SSL model, before start training the first epochs I got the following Error:</p>
<pre><code>        samples = samples.to(device, non_blocking=True)
              ^^^^^^^^^^
AttributeError: 'list' object has no attribute 'to'
</code></pre>
<p>However, after starting debugging I realized that the object is a list of Tensors as depicted in the following image:</p>
<p><a href=""https://i.sstatic.net/WxUTYcJw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WxUTYcJw.jpg"" alt=""enter image description here"" /></a></p>
<p>As far as I know usually this problem raises when the object is not a Tensor, but at least the list I'm trying to train is a tensor.</p>
<p>EDIT:</p>
<p>Python indicates that my samples is a list of Tensors, not just a list.
<a href=""https://i.sstatic.net/AJGEkvr8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJGEkvr8.jpg"" alt=""enter image description here"" /></a></p>
","-1","Question"
"79660425","","<p>I need to generate artificial subsets of features that are complementary to each other. Given a measure of complementarity between features, do you know any algorithms or methods that could help me construct such subsets? As a starting point, I'm particularly interested in splitting a set of features into two complementary subsets.</p>
","-3","Question"
"79661708","","<p>I have a 1440x1080 video with SAR 4:3 and DAR 16:9 recorded with a Sony α6000.
Having DAR 16:9 when viewing the video it appears correctly shown as if it were 1920x1080.
However, when extracting the frames (with opencv) with the native resolution and opening them they appear squashed.
If these video frames are to be used for training or for inference, should a resize of the images to 1920x1080 or should they be used with their native resolution 1440x1080 even if when you open it it appears deformed?</p>
","-5","Question"
"79662773","","<p>I want to use ImageFolder from Torchvision with an AWS S3 bucket where I have my dataset saved, however, it requires a os.Path like file and with boto3 I can just get a directory or a list which is not allowed by ImageFolder. Do you know how can I get a S3 directory as os.Path in Python with boto3 or a similar strategy?</p>
<p>I'm loading my dataset as follows:</p>
<pre><code># Initialize S3 client
s3 = boto3.client('s3')
response_test = s3.list_objects_v2(Bucket=BUCKET_NAME_test, Prefix=PREFIX_test)
jpg_files_test = [item['Key'] for item in response_test.get('Contents', []) if  item['Key'].lower().endswith('.jpeg')]
</code></pre>
<p>And using this code to load the data with Torchvision:</p>
<pre><code>transform_train = transforms.Compose([
        transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.10231429, 0.18161748, 0.26542443], std=[0.06512465, 0.10374635, 0.16232868])])
dataset_train = datasets.ImageFolder(os.path.join(args.data_path, 'train'), transform=transform_train)   # 'train'
print(dataset_train)
</code></pre>
<p>But as it's expected I get this error:</p>
<pre><code>TypeError: expected str, bytes or os.PathLike object, not list
</code></pre>
","-1","Question"
"79662912","","<p>I'm training a Transformer-based text classification model from scratch using PyTorch. The goal is to classify news articles (headline + short description) into 15 classes. I'm achieving about 72–75% validation accuracy, but I want to improve it further without switching to BERT.</p>
<p>I implemented a TransformerEncoder model using PyTorch with positional encoding and a linear classifier on top of the <code>[CLS]</code> token. I used 2 encoder layers, 4 attention heads, and <code>d_model = 256</code>. For tokenization, I used spaCy (<code>blank(&quot;en&quot;)</code>) to split and lowercase tokens, removing punctuation and spaces.</p>
<p>Here is the main model definition:</p>
<pre><code>class TransformerClassifier(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=2, num_classes=15):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.classifier = nn.Linear(d_model, num_classes)

    def forward(self, src):
        x = self.embedding(src)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        return self.classifier(x[:, 0])
</code></pre>
<p>I expected the validation accuracy to go above 80% after tuning the learning rate, dropout, and using AMP (automatic mixed precision). However, the performance remains stuck around 73–75%. I’ve also tried StepLR scheduling and early stopping but with limited effect.</p>
","2","Question"
"79663911","","<p>I've a time series data points but the data points have weights in addition to their value, mostly of 1, but it can differ.
How can I make a (regplot/lmplot) plot linear regression model fit that takes the weight of the data points into account?</p>
<p>I could duplicate the data point times its weight but that feels very unelegant, and I was wondering if there is a proper method.</p>
","2","Question"
"79663977","","<p>I want to perform parallel computing on my ShapeletTransform. According to the documentation,  I need to set the backend:parallel flag to 'loky' to activate the parallel method of the class. However, the set_config method accepts &quot;**config_dict&quot; as parameter.</p>
<p>Here's what I tried:</p>
<pre><code>y_transformed = y
    
y_transformed = pd.DataFrame(y.apply(lambda row: pd.Series(row), axis=1))
    
transformer = ShapeletTransform(verbose=1)
dict_config = {
    &quot;backend:parallel&quot;: &quot;default=loky&quot;
}
transformer.set_config(dict_config)
    
transformer.get_config()
    
transformer.fit(X_3d, y_transformed)
</code></pre>
<p>Here's the error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
\&lt;ipython-input-96-2327096163\&gt; in \&lt;cell line: 0\&gt;()
12     &quot;backend:parallel&quot;: &quot;default=loky&quot;
13 }
\---\&gt; 14 transformer.set_config(dict_config)
15 # Get the current configuration
16 transformer.get_config()


TypeError: BaseObject.set_config() takes 1 positional argument but 2 were given
</code></pre>
","0","Question"
"79322329","79321937","<p>Is there something that's preventing you from just using seaborn directly?</p>
<pre><code>def plt1():
    # Added this example confusion matrix.
    c_mtrx_N = pd.DataFrame([[1, 2], [3, 4]], index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])
    plt.figure(figsize=(4,4))
    # c_mtrx_N = pd.crosstab(y_test, y_pred_N, rownames=['Actual'], colnames=['Predicted'])
    sns.heatmap(c_mtrx_N, annot=True, fmt = '.3g')
    plt.show()

plt1()
</code></pre>
<p>Also, sns.set() is deprecated. Use <a href=""https://seaborn.pydata.org/generated/seaborn.set_theme.html#seaborn.set_theme"" rel=""nofollow noreferrer"">sns.set_theme()</a> instead.</p>
","0","Answer"
"79326454","79325633","<p>The statsmodels documentation has a dedicated example about using statsmodels OLS with dummy variables. You can check it <a href=""https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html#OLS-estimation"" rel=""nofollow noreferrer"">here</a>.</p>
<p>They suggest to input not a dataframe, but a numpy array, and they even have a code example, explaining how to produce input. This code is from their example:</p>
<pre><code>nsample = 50
groups = np.zeros(nsample, int)
groups[20:40] = 1
groups[40:] = 2

dummy = pd.get_dummies(groups).values
x = np.linspace(0, 20, nsample)
# drop reference category
X = np.column_stack((x, dummy[:, 1:]))
X = sm.add_constant(X, prepend=False)

res2 = sm.OLS(y, X).fit()
</code></pre>
","1","Answer"
"79327848","79327647","<p>Instead of</p>
<pre><code>precision_curve, recall_curve, thresholds = precision_recall_curve(y_test, y_scores)
</code></pre>
<p>it should be</p>
<pre><code>precision_curve, recall_curve, thresholds = precision_recall_curve(1-y_test, y_scores)
</code></pre>
","0","Answer"
"79329355","79329352","<p>The <code>SentenceSplitter</code> class is defined in the <code>llama_index.text_splitter</code> module. So, your error might be due to one of the following reasons:</p>
<ul>
<li>The <code>llama_index</code> package is not installed or not properly installed in your Python environment. You can check this by running <code>pip show llama_index</code> in your terminal. If the package is not installed, you can install it in a terminal session by running: <code>pip install llama_index</code>.</li>
<li>The <code>llama_index</code> package is installed, but Python cannot find it. This can happen if the package is installed in a location that is not on Python's path. You can check Python's path in your script using:</li>
</ul>
<pre><code>import sys
print(sys.path)
</code></pre>
<ul>
<li>The <code>llama_index.text_splitter</code> module is not correctly imported in the <code>__init__.py</code> file of the <code>llama_index</code> package. This would prevent you from importing <code>SentenceSplitter</code> directly from <code>llama_index.text_splitter</code>.</li>
</ul>
<p>See this closed similar <a href=""https://github.com/run-llama/llama_index/issues/7156"" rel=""nofollow noreferrer"">issue on GitHub</a>.</p>
","0","Answer"
"79332298","79330782","<p>Instead of</p>
<pre><code>List&lt;int&gt; imageBytes = imagePath.readAsBytesSync();
String base64Image = base64Encode(imageBytes);
</code></pre>
<blockquote>
<p>You need to first convert the image to Uint8List then convert it to base64.</p>
</blockquote>
<pre><code>final bytes = imagePath.readAsBytesSync();
final imageList = bytes.buffer.asUint8List();
</code></pre>
<p>Now check if the imageList is empty or not. Then proceed with your normal code.</p>
<pre><code>if(imageList != null) {
   String base64Image = base64Encode(imageList);
} else {
   print(&quot;no image list found&quot;
}
</code></pre>
<blockquote>
<p>Use Image.memory() to show the Uint8List if needed.</p>
</blockquote>
","1","Answer"
"79333091","79328556","<p>You need to explicitly account for these parameters within your custom loss function.</p>
<p>Below an example, but adjust accordingly:</p>
<pre><code>function loss = modelLoss(Y, T, classNames, classWeights)

    % normalized to 1
    classWeights = classWeights / sum(classWeights);

    mask = ~isnan(T);
    T(isnan(T)) = 0;

    numClasses = numel(classNames);
    T_onehot = zeros([size(T, 1), size(T, 2), numClasses, size(T, 4)], 'like', Y);
    for i = 1:numClasses
        T_onehot(:, :, i, :) = (T == i);
    end

    % class-wise weighted cross-entropy
    weightedLoss = 0;
    for c = 1:numClasses
        classMask = mask &amp; (T == c);
        weightedLoss = weightedLoss + classWeights(c) * crossentropy(Y(:, :, c, :), T_onehot(:, :, c, :), Mask=classMask);
    end

    % Normalize by # of valid pixels
    numValidPixels = sum(mask(:));
    loss = weightedLoss / max(numValidPixels, 1);
end


% Define weights
classNames = [...];
classWeights = [...]; % Example weights

customLoss = @(Y, T) modelLoss(Y, T, classNames, classWeights);

netTrained = trainnet(images, net, customLoss, options);
</code></pre>
","1","Answer"
"79332514","79331871","<p>This is a know limitation of <a href=""https://keras.io/api/models/sequential/"" rel=""nofollow noreferrer"">sequential</a> modelling with keras, see this <a href=""https://github.com/keras-team/keras/issues/16355"" rel=""nofollow noreferrer"">ticket</a>. In order to get intermediate feature vector, you can adopt <a href=""https://keras.io/api/models/model/"" rel=""nofollow noreferrer"">functional</a> modelling approach. Here's how to structure it:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

inputs = tf.keras.Input(shape=input_shape)
pretrained_model = tf.keras.applications.ResNet50(
        weights='imagenet', 
        include_top=False, 
        input_tensor=inputs,
    )
pretrained_model.trainable = False

x = tf.keras.layers.GlobalAveragePooling2D()(
    pretrained_model.output
)
outputs = tf.keras.layers.Dense(10, activation='softmax')(x)
model = tf.keras.Model(inputs, outputs)
feature_extractor = tf.keras.Model(
    model.inputs, [layer.output for layer in model.layers]
)
</code></pre>
<p>Let's see some test result.</p>
<pre class=""lang-py prettyprint-override""><code># test
sample = np.ones((1, 32, 32, 3))

sample_output = model(sample)
sample_output.shape
TensorShape([1, 10])

feat_output = feature_extractor(sample)
print(len(feat_output))
for fo in feat_output:
    print(fo.shape)

177
(1, 32, 32, 3)
(1, 38, 38, 3)
(1, 16, 16, 64)
...
(1, 1, 1, 2048)
(1, 2048)
(1, 10)
</code></pre>
","0","Answer"
"79333175","79333072","<p>The type used for floating-point values is a function of the backend. At the time of writing, all backends default to <code>f32</code>, but you can use e.g. <code>type Backend = Wgpu&lt;f64&gt;</code> if you want to use <code>f64</code> instead.</p>
","2","Answer"
"79337506","79337075","<p>The <code>test_size = 0.25</code> doesn’t really have an impact on the metrics, I don’t think. But your model is too good, probably because the function that the label follows is very simple. So your Adaboost doesn’t need more than a <code>DecisionTree(depth=1)</code> to learn that function.</p>
<p>But, you’re probably using the same name <code>y_pred</code> in all the code and thus, <code>y_pred</code> should be actualized when you make any changes on the model or on the test dataset. The <code>test_size</code> actually modifies the length of the test dataset and the previous length was 25% of the total size of the dataset. If you modify the <code>test_size</code>, you modify that test dataset and you need to actualize <code>y_pred</code> with <code>adb.predict(X_test)</code>, both for having new prediction matching with new datapoints, and not have a mismatch error.</p>
<p><em><strong>To fix the issue,</strong></em> you just need to to add the following before using any function that needs <code>y_test</code> and <code>y_pred</code>:</p>
<pre><code>y_pred = adb.predict(X_test)
</code></pre>
<p><a href=""https://i.sstatic.net/5gBMT8HO.png"" rel=""nofollow noreferrer"">Screenshot of code</a></p>
","-1","Answer"
"79337575","79337434","<p>The parameter <code>n_features_to_select</code> can be an integer (number of features) or a float (proportion of features).
So instead of [1, 2, 3], the pipeline can run with [1/3, 2/3, 1.0].</p>
<p>To get the scores for each combination of parameters in the grid search, you can run</p>
<pre><code>display(pd.DataFrame(grid_search.cv_results_))
</code></pre>
<p>The results for <code>n_features = 1.0</code> and those for a pipeline without the SequentialFeatureSelector (e.g. setting that to 'passthrough') should be the same.</p>
","1","Answer"
"79339302","79338394","<p>UPDATE: based on more specifics, see the code at the end:</p>
<p>tidymodels doesn’t have a method to automatically generate them because, for many models, they are highly problematic (e.g., unrealistically optimistic). That can be frustrating but, overall, more harm is done than good if those results were included.</p>
<p>You can do a little work to get them using the <code>extract</code> options for the control functions.</p>
<p>Here is a reproducible example:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidymodels)
tidymodels_prefer()

set.seed(1)
reg_dat &lt;- sim_regression(200)
reg_rs &lt;- vfold_cv(reg_dat)

tree_grid &lt;-
  grid_space_filling(
    cost_complexity(),
    tree_depth(),
    min_n(),
    size = 3)

tree_spec &lt;-
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune()
  ) %&gt;%
  set_mode(&quot;regression&quot;)

tree_wf &lt;- workflow(outcome ~ ., tree_spec)

# Write a function to make predictions: 
get_train_rmse &lt;- function(x) {
  require(tidymodels) # &lt;- sometimes needed for parallel proc
  # 'x' is the fitted workflow
  augment(x, reg_dat) %&gt;% 
    rmse(outcome, .pred)
}

# pass it in:
ctrl &lt;- control_grid(extract = get_train_rmse)  
  
tree_res &lt;- 
  tree_wf %&gt;%
  tune_grid(
    resamples = reg_rs,
    grid = tree_grid,
    metrics = metric_set(rmse),
    control = ctrl
  )

names(tree_res)
#&gt; [1] &quot;splits&quot;    &quot;id&quot;        &quot;.metrics&quot;  &quot;.notes&quot;    &quot;.extracts&quot;

collect_extracts(tree_res) %&gt;% 
  unnest(.extracts) %&gt;% 
  relocate(training_error = .estimate)
#&gt; # A tibble: 30 × 8
#&gt;    training_error id     cost_complexity tree_depth min_n .metric .estimator
#&gt;             &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     
#&gt;  1           12.9 Fold01    0.0000000001          8    21 rmse    standard  
#&gt;  2           18.0 Fold01    0.00000316            1     2 rmse    standard  
#&gt;  3           16.2 Fold01    0.1                  15    40 rmse    standard  
#&gt;  4           12.3 Fold02    0.0000000001          8    21 rmse    standard  
#&gt;  5           18.1 Fold02    0.00000316            1     2 rmse    standard  
#&gt;  6           16.1 Fold02    0.1                  15    40 rmse    standard  
#&gt;  7           12.5 Fold03    0.0000000001          8    21 rmse    standard  
#&gt;  8           18.0 Fold03    0.00000316            1     2 rmse    standard  
#&gt;  9           16.4 Fold03    0.1                  15    40 rmse    standard  
#&gt; 10           12.4 Fold04    0.0000000001          8    21 rmse    standard  
#&gt; # ℹ 20 more rows
#&gt; # ℹ 1 more variable: .config &lt;chr&gt;
</code></pre>
<p><sup>Created on 2025-01-08 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.1.0</a></sup></p>
<p>Note that this <em>may not automatically work</em> for some parallel processing technologies since <code>reg_dat</code> is not in the worker processes. However, the parallel package has functions that can pass the data (or other objects) into each worker to make it available for computations.</p>
<p>UPDATE: based on this feedback:</p>
<blockquote>
<p>Thank you, @topepo, for your reply, but I have a problem
with that. You are reusing the whole training dataset
instead of using the training part of the data from
the current fold. How do I access the training part
of the data of the current fold when running
<code>fit_resamples</code>?</p>
</blockquote>
<p>No problem. This is exactly why we don't use &quot;training set&quot; to describe the data used for modeling within resamples; we use the term <a href=""https://aml4td.org/chapters/resampling.html#fig-resampling-scheme"" rel=""nofollow noreferrer""><em>analysis set</em></a> to be more specific.</p>
<p>Here's another reprex:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidymodels)
tidymodels_prefer()

set.seed(1)
reg_dat &lt;- sim_regression(200)
reg_rs &lt;- vfold_cv(reg_dat)

tree_grid &lt;-
  grid_space_filling(
    cost_complexity(),
    tree_depth(),
    min_n(),
    size = 3)

tree_spec &lt;-
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune()
  ) %&gt;%
  set_mode(&quot;regression&quot;)

tree_wf &lt;- workflow(outcome ~ ., tree_spec)

# Write a function to return the fitted workflow: 
get_fit &lt;- function(x) {
  x
}

# pass it in:
ctrl &lt;- control_grid(extract = get_fit)  
rmse_set &lt;- metric_set(rmse)

tree_res &lt;- 
  tree_wf %&gt;%
  tune_grid(
    resamples = reg_rs,
    grid = tree_grid,
    metrics = rmse_set,
    control = ctrl
  ) 

tree_res %&gt;% 
  select(splits, starts_with(&quot;id&quot;), .extracts) %&gt;% 
  # Expand the models fit within each of the resamples
  unnest(.extracts) %&gt;% 
  mutate(
    # Predict each splits analysis set
    tr_pred = map2(splits, .extracts, ~ augment(.y, analysis(.x))),
    # Compute metrics
    tr_metrics = map(tr_pred, ~ rmse_set(.x, outcome, .pred))
  ) %&gt;% 
  select(starts_with(&quot;id&quot;), cost_complexity, tree_depth, min_n, tr_metrics) %&gt;% 
  unnest(tr_metrics)
#&gt; # A tibble: 30 × 7
#&gt;    id     cost_complexity tree_depth min_n .metric .estimator .estimate
#&gt;    &lt;chr&gt;            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
#&gt;  1 Fold01    0.0000000001          8    21 rmse    standard        12.2
#&gt;  2 Fold01    0.00000316            1     2 rmse    standard        17.1
#&gt;  3 Fold01    0.1                  15    40 rmse    standard        15.6
#&gt;  4 Fold02    0.0000000001          8    21 rmse    standard        11.3
#&gt;  5 Fold02    0.00000316            1     2 rmse    standard        17.7
#&gt;  6 Fold02    0.1                  15    40 rmse    standard        15.7
#&gt;  7 Fold03    0.0000000001          8    21 rmse    standard        11.7
#&gt;  8 Fold03    0.00000316            1     2 rmse    standard        17.5
#&gt;  9 Fold03    0.1                  15    40 rmse    standard        16.1
#&gt; 10 Fold04    0.0000000001          8    21 rmse    standard        12.3
#&gt; # ℹ 20 more rows
</code></pre>
<p><sup>Created on 2025-01-10 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.1.1</a></sup></p>
","1","Answer"
"79341861","79341200","<p>There is no <code>read_from_directory</code>  tranformantion schema in MLTable, check this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-mltable?view=azureml-api-2#transformations"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>For AutoML image classification you need data in <code>.jsonl</code> file with below fields, check this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/reference-automl-images-schema?view=azureml-api-2#data-schema-for-training"" rel=""nofollow noreferrer"">documentation</a>/</p>
<pre><code>{
   &quot;image_url&quot;:&quot;azureml://subscriptions/&lt;my-subscription-id&gt;/resourcegroups/&lt;my-resource-group&gt;/workspaces/&lt;my-workspace&gt;/datastores/&lt;my-datastore&gt;/paths/&lt;path_to_image&gt;&quot;,
   &quot;image_details&quot;:{
      &quot;format&quot;:&quot;image_format&quot;,
      &quot;width&quot;:&quot;image_width&quot;,
      &quot;height&quot;:&quot;image_height&quot;
   },
   &quot;label&quot;:&quot;class_name&quot;,
}
</code></pre>
<p><code>image_url</code> and <code>label</code> are required fields, also you need to give image url as complete datastore path.</p>
<p>Follow below steps to create <code>jsonl</code> file.</p>
<p>First, you need datastore path to each image so you create new data asset and take the path.</p>
<pre><code>from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes, InputOutputModes
from azure.ai.ml import Input

from azure.identity import DefaultAzureCredential
from azure.ai.ml import MLClient

credential = DefaultAzureCredential()

ml_client = MLClient.from_config(credential)
my_data = Data(
    path=&quot;./images&quot;,
    type=AssetTypes.URI_FOLDER,
    description=&quot;Fridge-items images&quot;,
    name=&quot;items-images&quot;,
)

uri_folder_data_asset = ml_client.data.create_or_update(my_data)
</code></pre>
<p>Here, i am having <code>OK</code> and <code>NOK</code> folders inside <code>images</code>.</p>
<p><img src=""https://i.imgur.com/HcxpLyP.png"" alt=""enter image description here"" /></p>
<p>You will get path in <code>uri_folder_data_asset.path</code>.</p>
<p>Next create <code>jsonl</code> file using below code.</p>
<pre><code>import os
import json

folders = {
    &quot;OK&quot;: &quot;./images/OK&quot;,
    &quot;NOK&quot;: &quot;./images/NOK&quot;
}

mltable_dir = &quot;image_data_mltable&quot;
os.makedirs(mltable_dir, exist_ok=True)

output_file = &quot;./image_data_mltable/image_data.jsonl&quot;

with open(output_file, &quot;w&quot;) as jsonl_file:
    for label, folder_path in folders.items():
        for file_name in os.listdir(folder_path):
            if file_name.lower().endswith((&quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.png&quot;, &quot;.bmp&quot;, &quot;.gif&quot;)):
                record = {
                    &quot;image_url&quot;: os.path.join(folder_path.replace('./images/',uri_folder_data_asset.path), file_name).replace(&quot;\\&quot;, &quot;/&quot;),
                    &quot;label&quot;: label
                }
                jsonl_file.write(json.dumps(record) + &quot;\n&quot;)

print(f&quot;JSONL file created: {output_file}&quot;)
</code></pre>
<p>and create mltable file.</p>
<pre><code>mltable_yaml = &quot;&quot;&quot;
paths:
  - file: ./image_data.jsonl
transformations:
  - read_json_lines:
        encoding: utf8
        invalid_lines: error
        include_path_column: false
  - convert_column_types:
      - columns: image_url
        column_type: stream_info       
&quot;&quot;&quot;

with open(os.path.join(mltable_dir, &quot;MLTable&quot;), &quot;w&quot;) as f:
    f.write(mltable_yaml)
</code></pre>
<p>Use <code>read_json_lines</code> in transformation, check <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-prepare-datasets-for-automl-images?view=azureml-api-2&amp;tabs=python#create-mltable"" rel=""nofollow noreferrer"">this</a> on how to prepare image data.</p>
<p>Output:</p>
<p><img src=""https://i.imgur.com/RipDExZ.png"" alt=""enter image description here"" /></p>
<p>Now use it as input.</p>
<pre><code>import mltable

training_data  = Input(type=AssetTypes.MLTABLE, path=&quot;./image_data_mltable&quot;)

tbl = mltable.load(uri=&quot;./image_data_mltable&quot;)
tbl.to_pandas_dataframe()
</code></pre>
<p>You refer this sample <a href=""https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/automl-standalone-jobs"" rel=""nofollow noreferrer"">github documentation</a> for AutoML classification to know more about it.</p>
","1","Answer"
"79345029","79343645","<p>I have worked on many OCR architectures. I have even trained ANPR for 24 different European languages and ANPR for India. Basically there is no pre-trained recognizer for OCR of license plates. You need to train a model for that. You can choose multiple OCR architectures and see which one is the best for your application. For training the OCR model, you need to first prepare a dataset of license plate images. You can scrape online images, crop them from CCTV cameras on roads, label them manually or from OCR service like amazon or google and create a good quality dataset.</p>
","0","Answer"
"79347513","79344545","<p>In <code>lightgbm</code> (the Python package for LightGBM), <code>best_iteration</code> isn't the iteration where the model achieved the best performance on evaluation metrics... it's the last iteration (1-based) where performance on evaluation metrics improved, <strong>if early stopping is used</strong>.</p>
<p>See this example (using <code>lightgbm==4.5.0</code>, <code>scikit-learn==1.6.0</code>, and Python 3.11).</p>
<pre class=""lang-py prettyprint-override""><code>import lightgbm as lgb
import numpy as np
from sklearn import datasets

X, y = datasets.make_classification(
    n_samples=10_000,
    n_features=5,
    n_informative=3,
    random_state=9
)

params = {
    &quot;deterministic&quot;: True,
    &quot;objective&quot;: &quot;binary&quot;,
    &quot;metric&quot;: &quot;binary_logloss&quot;,
    &quot;seed&quot;: 708
}

# train without early stopping
model = lgb.cv(
    params=params,
    train_set=lgb.Dataset(X, label=y),
    num_boost_round=200,
    return_cvbooster=True
)

model['cvbooster'].best_iteration
# -1

opt_1 = np.argmin(model['valid binary_logloss-mean'])
print(f&quot;index argmin: {opt_1}&quot;)
# index argmin: 114
print(f&quot;logloss argmin: {model['valid binary_logloss-mean'][opt_1]:.6f}&quot;)
logloss argmin: 0.132579

# train WITH early stopping
model = lgb.cv(
    params={**params, &quot;early_stopping_rounds&quot;: 5},
    train_set=lgb.Dataset(X, label=y),
    num_boost_round=200,
    return_cvbooster=True
)

model['cvbooster'].best_iteration
# 115

opt_1 = np.argmin(model['valid binary_logloss-mean'])
print(f&quot;index argmin: {opt_1}&quot;)
# index argmin: 114
print(f&quot;logloss argmin: {model['valid binary_logloss-mean'][opt_1]:.6f}&quot;)
# logloss argmin: 0.132579
</code></pre>
<p>Notes on that:</p>
<ul>
<li>adding <code>&quot;deterministic&quot;: True</code> and setting <code>&quot;seed&quot;</code> to a positive value helps make training deterministic</li>
<li>early stopping in <code>cv()</code> can be enabled by passing a positive value for <code>&quot;early_stopping_rounds&quot;</code> through <code>params</code></li>
</ul>
","1","Answer"
"79347581","79345260","<p>I found the solution, I changed to <code>UnsqueezeTransform(-4, in_keys=[&quot;pixels&quot;])</code> within agent_explore and now I have the wanted behaviour ... (:</p>
","0","Answer"
"79350659","79344565","<p><a href=""https://pypi.org/project/bitsandbytes/#description"" rel=""nofollow noreferrer"">bitsandbytes package</a> is only compatible with Windows and Linux as can be seen in the available wheels <a href=""https://pypi.org/project/bitsandbytes/#files"" rel=""nofollow noreferrer"">here</a>.</p>
<p>This issue was raised on <a href=""https://github.com/bitsandbytes-foundation/bitsandbytes/issues/1378"" rel=""nofollow noreferrer"">GitHub</a> and a comment by a member of bitsandbytes
foundation says:</p>
<blockquote>
<p>None of the releases are supported on macOS yet. Releases up through 0.42.0 were incorrectly tagged as supporting all platforms, but this was an oversight. Those older wheels only contain x86-64 binaries for Linux.</p>
</blockquote>
<p>However they seem to be looking for contributions to get it supported on macOS. See <a href=""https://github.com/bitsandbytes-foundation/bitsandbytes/discussions/1340"" rel=""nofollow noreferrer"">https://github.com/bitsandbytes-foundation/bitsandbytes/discussions/1340</a>.</p>
","1","Answer"
"79352146","79350403","<p>So, If I understood your question correctly, you're implementing an attention mechanism between the <code>i</code>th and <code>j</code>th sequences in a batch. First you linearly project your data (X) to get the queries: XW_Q, then you linearly project your data to get the keys: XW_K. You then add bias a_K and finally you want compute the dot product (similarity) between XW_Q @ (XW_K + a_K).</p>
<p>In this case, each D-dimensional embedding from the queries is multiplied (in the dot product sense) with every D-dimensional embedding from the keys. The output of a dot product of two vectors is a scalar, to the shape of e_ij should be [B, S, S, H], rather than [B, S, H, D].</p>
<p>Then, after normalization, you apply softmax such that every ith row sums to 1 to get the scaling matrix alpha which is also [B, S, S, H]</p>
<p>Now, you project your input the get the values: X@W_V. This should result in a [B, S, H, D] Tensor.</p>
<p>Finally, you get the new ith sequence (z_i) by scaling every jth sequence column of XW_V by the jth scaling factor in alpha_i and sum, resulting in a [B, S, H, D] tensor as you expected.
See the modified code below.</p>
<p>Hopefully, this is a clear enough explanation. I hope I got your intention right and I that I didn't mix up any indices.</p>
<pre><code>import torch
import torch.nn.functional as F
X = torch.randn((10, 20, 30, 40))
B, S, H, D = X.shape
d_z = D  # Assuming d_z is equal to D for simplicity

W_Q = torch.randn(H, D, D)
W_K = torch.randn(H, D, D)
W_V = torch.randn(H, D, D)

a_K = torch.randn(S, S, H, D)
a_V = torch.randn(S, S, H, D)

XW_Q = torch.einsum('bshd,hde-&gt;bshe', X, W_Q)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]
XW_K = torch.einsum('bshd,hde-&gt;bshe', X, W_K)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]

e_ij_numerator = (XW_Q.unsqueeze(2) * (XW_K.unsqueeze(1) + a_K)).sum(dim=-1)  # [B, S, S, H]
e_ij = e_ij_numerator / torch.sqrt(torch.tensor(d_z, dtype=torch.float32))  # [B, S, S, H]
alpha = F.softmax(e_ij, dim=2)  # [B, S, S, H]
XW_V = torch.einsum('bshd,hde-&gt;bshe', X, W_V)  # [B, S, H, D] @ [H, D, D] -&gt; [B, S, H, D]


z_i = torch.einsum('bijh,bijhd -&gt; bihd', alpha, (XW_V.unsqueeze(1) + a_V))  # [B, S, S, H] * [B, S, S, H, D] -&gt; [B, S, H, D]
print(z_i.shape) # [B, S, H, D]. 
</code></pre>
","0","Answer"
"79353218","79350213","<p>The issue for this seems to still be open at time I am making this post: <a href=""https://github.com/rapidsai/cuml/issues/4694"" rel=""nofollow noreferrer"">https://github.com/rapidsai/cuml/issues/4694</a>.</p>
<p>There is someone trying an implementation towards the end of the thread, so you can reply to them with your desire to know more.  It is <a href=""https://docs.rapids.ai/api/cuml/nightly/api/#experimental"" rel=""nofollow noreferrer"">also not documented as a feature</a>, even in our current nightlies, thus, your code will not work and should not be expected to work.  Please do check our stable and nightly docs for the latest available algos.</p>
<p>Also, as we're talking best practices, if using pandas, one thing you can use is <a href=""https://docs.rapids.ai/api/cudf/stable/cudf_pandas/"" rel=""nofollow noreferrer"">cudf.pandas</a> instead of both cudf and pandas. Then you don't have to do the conversion steps between the two(it will be done for you)</p>
","0","Answer"
"79361202","79359767","<p>Disclaimers:</p>
<ul>
<li>My answer is a mix of code reading and &quot;educated guessing&quot;. I did not run the actual code, but a run with the help of a debugger should help you verify/falsify my assumptions.</li>
<li>The code shared below is a condensed version of the <a href=""https://github.com/faresbougourzi/D-TrAttUnet/blob/51079c65a3669ef91186315eea837d7b7830c6cb/detailed%20train%20and%20test/train_test_DTrAttUnet_BinarySegmentation.py#L220-L311"" rel=""nofollow noreferrer"">relevant section</a> of the score/metrics calculations, to help focus on the essentials. It is not runnable and should be understood as pseudocode.</li>
</ul>
<p>Anyway, let's break down their code (maybe put the code sample side by side with the explanations below it):</p>
<pre class=""lang-py prettyprint-override""><code>dice_scores, dice_scores2, TP, TN, FP, FN = 0, 0, 0, 0, 0, 0

for batch in tqdm(dataloader):

    x, y, _, _ = batch
    outputs, _ = model(x)
    preds = segm(outputs) &gt; 0.5
    yy = y &gt; 0.5

    TP += np.sum(((preds == 1) + (yy == 1)) == 2)
    TN += np.sum(((preds == 0) + (yy == 0)) == 2)
    FP += np.sum(((preds == 1) + (yy == 0)) == 2)
    FN += np.sum(((preds == 0) + (yy == 1)) == 2)

    for idice in range(preds.shape[0]):
        dice_scores += ((2 * (preds[idice] * yy[idice]).sum()) /
                        ((preds[idice] + yy[idice]).sum() + 1e-8))

    predss = np.logical_not(preds)
    yyy = np.logical_not(yy)

    for idice in range(preds.shape[0]):
        dice_sc1 = ((2 * (preds[idice] * yy[idice]).sum()) /
                    ((preds[idice] + yy[idice]).sum() + 1e-8))
        dice_sc2 = ((2 * (predss[idice] * yyy[idice]).sum()) /
                    ((predss[idice] + yyy[idice]).sum() + 1e-8))
        dice_scores2 += (dice_sc1 + dice_sc2) / 2

epoch_dise = dice_scores/len(dataloader.dataset)
epoch_dise2 = dice_scores2/len(dataloader.dataset)
F1score = TP / (TP + ((1/2)*(FP+FN)) + 1e-8)
IoU = TP / (TP+FP+FN)
</code></pre>
<ul>
<li>The first line initializes all accumulated values to <code>0</code>.</li>
<li>With <code>for batch in tqdm(dataloader)</code>, the code iterates over all samples in the data set (or rather, over all samples accessible to the used <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noreferrer""><code>DataLoader</code></a>, which might be a subset or an otherwise preprocessed version of the underlying data). This implies that the accumulated values represent the &quot;global&quot; results, i.e the results for the complete data set.</li>
<li>After applying the <code>model</code> to the sample data in the batch, <code>x</code>, via <code>model(x)</code>, the resulting predictions, <code>outputs</code>, are thresholded to a binary representation, <code>preds</code>, via <code>segm(outputs) &gt; 0.5</code>. The <code>segm</code> function, in this case, is simply a sigmoid (see line 190 in the original code), which maps all values to the range <code>[.0, .1]</code>. A similar step is performed for the &quot;ground truth&quot; (i.e. the true/known segmentation), <code>y</code>, to produce its binary representation, <code>yy</code>. <strong>[Update]</strong> The <code>outputs</code> variable, in this context, holds the outputs of one of the two branches of the model only <a href=""https://github.com/faresbougourzi/D-TrAttUnet/blob/51079c65a3669ef91186315eea837d7b7830c6cb/Architecture.py#L252-L292"" rel=""nofollow noreferrer"">(compare relevant model code)</a>, thus either lesion or organ segmentation. Also compare Figure 2 in the paper: the corresponding branches are the ones with blue and green background. While suppressed in my pseudocode above (<code>outputs, _ = model(x)</code>), the actual code also uses the output of the other branch, though only for <em>loss</em> calculation and not for the calculation of the <em>scores/metrics</em>. <strong>[/Update]</strong></li>
<li>So, to summarize, what we have at the current point:
<ul>
<li>A binary mask <code>preds</code> that holds the predicted segmentations for all samples in the current batch.</li>
<li>A binary mask <code>yy</code> that holds the true/known segmentations for all samples in the current batch.</li>
</ul>
</li>
<li>The <code>sum()</code> calculations in the following lines are counting (albeit written in a bit of an unconventional way maybe) the number of voxels matching between predictions and true/known segmentations, over all samples in the current batch. For example, this means summing the number of matching voxels of predicted foreground and true/known background for the false positives, which is then added on top of their global count, <code>FP</code>.</li>
<li>The Dice scores are handled a bit differently: values are calculated for each index along dimension 0 separately, before adding them to the global value, <code>dice_scores</code>. Dimension 0 usually indexes individual samples in the batch, so this would mean calculating a separate Dice score for each sample. The values are later normalized by dividing through the number of samples, <code>len(dataloader.dataset)</code>, to gain <code>epoch_dise</code>. These two steps are in accordance with the equation <em>Dice score = …</em> shared in the question, which calculates the Dice score separately for each sample, adds all corresponding results, then divides by the number of samples (called <em>testing images</em> there), <code>N</code>.</li>
<li>A second Dice score is then calculated by not only measuring foreground overlaps, but also measuring background overlaps:
<ul>
<li>The masks <code>predss</code> and <code>yyy</code> are the negations of <code>preds</code> and <code>yy</code>, respectively, i.e. <code>True</code> for background voxels, <code>False</code> for foreground voxels.</li>
<li>For each sample, the Dice score for the foreground voxels is recalculated as <code>dice_sc1</code>; but then also the Dice score for the background voxels is calculated as <code>dice_sc2</code>. Then their average is taken.</li>
<li>This sample average is accumulated in the global value <code>dice_scores2</code>, which is later normalized to <code>epoch_dise2</code>, just as <code>dice_scores</code> and <code>epoch_dise</code> above.</li>
</ul>
</li>
<li>At that point what is missing is the calculation of the F1 score and IoU. This is done with <code>F1score = …</code> and <code>IoU = …</code> over the global values of the true/false positives/negatives, in accordance with the corresponding equations cited in the question.</li>
</ul>
<p>So, to summarize once more:</p>
<ul>
<li><strong>[Update]</strong> While the model produces two outputs (lesion and organ segmentation) for each sample via two branches, only one output is used for calculating the <em>scores/metrics</em>, while both outputs are used for calculating the <em>losses</em>. I am not sure which branch is used for the score calculation, but assuming the authors use the same order in their code as they use in their paper, it would be the lesion branch. <strong>[/Update]</strong></li>
<li>Indeed, as assumed in the question, F1 score and IoU are calculated over all samples.</li>
<li>Dice scores are calculated for each sample separately, then averaged over all samples. This is done in two versions:
<ul>
<li>Version 1 (<code>dice_scores</code>, <code>epoch_dise</code>) calculates what I would call the &quot;standard&quot; Dice coefficient, i.e. the score for overlapping foreground voxels.</li>
<li>Version 2 (<code>dice_scores2</code>, <code>epoch_dise2</code>) calculates what I would call a &quot;weighted&quot; Dice coefficient: for each sample, it calculates both the score for overlapping foreground voxels and the score for overlapping background voxels, then averages them as the sample's score, and only then accumulates and averages again to get the global score.</li>
</ul>
</li>
</ul>
","1","Answer"
"79362023","79360229","<blockquote>
<ol>
<li>in the <strong>getitem</strong> we are reading an image from disk to memory. It means if we train our model for several epochs, we are re-reading the same image into memory several times. To my knowledge it is a costly action</li>
</ol>
</blockquote>
<p>An alternative would be to cache the sample when it is first read, such that by the end of the first epoch all of the sample will be cached in memory for faster subsequent access. However, this would require enough RAM to hold the entire dataset. This is probably the limitation that the example is trying to circumvent - they read each sample each time it is needed because of memory limitations (i.e. trading off repeated reads vs available memory).</p>
<blockquote>
<ol start=""2"">
<li>a transform is applied each time an image is read from disk and that seems to me a nearly redundant action.</li>
</ol>
</blockquote>
<p>Transforms usually have randomness built-in as it helps prevent the net from overfitting. That's why we transform each time a sample is requested - we need a different random transformation each time. If the transformations were only applied a single time and re-used thereafter, it would defeat the purpose of random augmentation as the net could just learn the single static transformation.</p>
<blockquote>
<p>[...] in the case that all my data can be fit into memory, isn't reading it all from the disk in the <strong>init</strong> function a better approach?</p>
</blockquote>
<p>I think that would be worth trying if disk-to-RAM speed is a bottleneck in your pipeline. Pre-loading all the data once would result in a speed improvement in that case. Bear in mind that even though your data might fit in RAM, by the time it's going through the model, the RAM requirements will be higher due to the model's size and training gradients (there will be different RAM requirements at training vs inference, and CPU vs GPU).</p>
<blockquote>
<p>[...] why shouldn't we crop the images once and store it on the disk somewhere else and throughout training only read the cropped images?</p>
</blockquote>
<p>Random cropping is usually used to prevent the net from simply memorising the exact features of an image. The net is forced to generalise beyond the randomness to more robust and general properties. Cropping once and re-using the same image would mean the net could learn the single crop and not generalise as well to unseen data.</p>
","0","Answer"
"79362135","79361940","<p>Yes, PyTorch is set up to work with batches of samples exactly as you suggest. You can pass all 1000 samples in and you'll get 1000 samples out of your network, where, internally, each one of those 1000 samples will get passed through the network. Try it!</p>
<p>Take this simple network as an example:</p>
<pre class=""lang-py prettyprint-override""><code># linear network with 3 input parameters and 3 output parameters
network = torch.nn.Linear(3, 3, bias=False)

# input with 2 samples
inputs = torch.randn(2, 3)

outputs = network(inputs)
print(outputs)
tensor([[0.1277, 0.5881, 0.1048],
        [0.3140, 0.2438, 0.1175]], grad_fn=&lt;MmBackward0&gt;)

# which is equivalent to matrix multiplying the network weights
# by each input sample 
for sample in inputs:
    print(torch.matmul(sample, network.weight.T))
tensor([0.1277, 0.5881, 0.1048], grad_fn=&lt;SqueezeBackward4&gt;)
tensor([0.3140, 0.2438, 0.1175], grad_fn=&lt;SqueezeBackward4&gt;)
</code></pre>
<p>In general, in many cases of training a neural network training, you will have a very large number of training samples (hundreds of thousands, millions, etc!), and during each epoch you will want to pass all those samples through the network before calculating the overall loss. Often, that's not practical, and during each epoch you will have to pass through the samples in smaller batches (&quot;minibatches&quot;), calculate the loss on the output of each minibatch as an approximation of the overall loss, and optimize on that. If you only have 1000 samples, which are each only 3 parameters in size, then there's no issue in just passing them all through the network in one go during a training epoch.</p>
","1","Answer"
"79364094","79362113","<p>The idea that GPU's are faster than CPU's is true for modern hardware, in combination with modern software. Both this CPU and this GPU are ancient. They're both slow; the GPU just happens to be even slower.</p>
","0","Answer"
"79366129","79365624","<p>The stable release of tfx supports Python versions &gt;=3.9,&lt;3.11. It is not compatible with Python 3.11. You can install this package using any of the supported Python versions (3.9 - 3.10). See <a href=""https://github.com/tensorflow/tfx?tab=readme-ov-file#compatible-versions"" rel=""nofollow noreferrer"">Compatible versions</a>.</p>
","0","Answer"
"79366151","79361226","<p>In a gradient boosted tree model, the value at a leaf <strong>is not</strong> the average target value among samples reaching that leaf, nor does the final prediction require subtracting the base prediction from each tree's value. Working out leaf values from scratch would be quite tricky (it involves the gradient of the loss function <em>of the model up to that point</em>), but fortunately there's a method for that: your <code>model</code> has an attibute <code>booster_</code> which has a method <code>get_leaf_value</code>.  So, we can omit a good chunk of previous calculations, and the last code block just becomes:</p>
<pre class=""lang-py prettyprint-override""><code>preds = []
test_leaf_indices = model.predict(X_test, pred_leaf=True)
for observation_leaves in test_leaf_indices:
    row_pred = 0  # not init_pred, b/c init_pred is included in the first tree (!?)
    for tree_index, leaf_index in enumerate(observation_leaves):
        row_pred += model.booster_.get_leaf_output(tree_index, leaf_index)
    preds.append(row_pred)
</code></pre>
","2","Answer"
"79369467","79365374","<p>You can use <code>torch.set_num_threads(int)</code> (<a href=""https://pytorch.org/docs/stable/generated/torch.set_num_threads.html"" rel=""nofollow noreferrer"">docs</a>) to control how many CPU processes pytorch uses to execute operations.</p>
","0","Answer"
"79372581","79353843","<p>You are comparing the reward from the last episode of the training process with the reward from one episode of using the trained model. The resulting reward could be different because:</p>
<ol>
<li>During training DQN does some exploration based on the evolving state-action value function that it is learning (represented by an MLP in your example)</li>
</ol>
<p><a href=""https://i.sstatic.net/tCeoENdy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tCeoENdy.jpg"" alt=""enter image description here"" /></a></p>
<p>(picture taken from &quot;Reinforcement Learning: An Introduction by Barto and Sutton&quot;)</p>
<p>But after training, you are using the trained model to generate episodes for which it no exploration would be done, as you are using <code>deterministic=True</code>. Even if you didn't use <code>deterministic=True</code> it wouldn't necessarily select actions the same way it did during training (for example it could have used ε-greedy during training and might use ε-soft during prediction).</p>
<ol start=""2"">
<li><p>The state to which environment gets reset at the end of each episode during training might not match that you get when you reset it later with <code>env.reset()</code>. The stable baselines API doesn't seem to guarantee anything about this, so you shouldn't assume anything. In the case of this environment, there seems to be some randomness coming from <a href=""https://github.com/openai/gym/blob/dcd185843a62953e27c2d54dc8c2d647d604b635/gym/envs/classic_control/cartpole.py#L202"" rel=""nofollow noreferrer"">here</a>.</p>
</li>
<li><p>In some cases, the environment itself is stochastic. Meaning that when you apply an action, the resulting state can itself be nondeterministic. This doesn't seem to be the case with the CarPole-v1 environment, but more generally you can't assume that all environments are similar.</p>
</li>
</ol>
","1","Answer"
"79541971","79367182","<p>Reinstalling torch using the following command (from <a href=""https://stackoverflow.com/a/27254355/2023370)"">here</a>) worked for me:</p>
<pre><code>(venv) $ pip install --upgrade --no-deps --force-reinstall torch
</code></pre>
<p>I issued the above command from within my venv, activated via <code>source /path/to/venv/bin/activate</code>. Then, things work again:</p>
<pre><code>(venv) $ cat hello.py 
from torch import Tensor

print(&quot;Hello, World!&quot;)
(venv) $ python hello.py 
Hello, World
</code></pre>
","0","Answer"
"79546320","79363695","<p>Sagemaker does not support models before tensorflow 2 and since this model was from before tensorflow 2, I was unable to deploy the model.</p>
","0","Answer"
"79602009","79344565","<p>Acc. to <a href=""https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face"" rel=""nofollow noreferrer"">https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face</a></p>
<p><code>bitsandbytes had to be bumped to 0.45.2 to avoid errors in Colab env</code></p>
<p>this is what has been working for me</p>
<pre><code>!pip install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 -q
</code></pre>
","0","Answer"
"79654105","79367182","<p>You meet this problem probably because there is a folder named 'torch' (but is not your package) somewhere else on your device.</p>
<p>Steps you may follow:</p>
<ol>
<li><p>open your command board and type:</p>
<p>pip show torch</p>
</li>
<li><p>the location of this package will be displayed. follow the location and ensure there is Torch.py in that folder.</p>
</li>
<li><p>search on your device if there is any other folder named 'Torch'. If yes, change its name.</p>
</li>
</ol>
","0","Answer"
"79367710","79367409","<p>Changing y from dictionary to a list within model.fit resolved the error.</p>
<pre><code>history = model.fit(
    X,
    [y_regression, y_classification],
    epochs=10,
    batch_size=32,
    sample_weight=sample_weights,
)
</code></pre>
","0","Answer"
"79378532","79375287","<p>You seem to have an IO bottleneck. It means the data cannot be transfered fast enough and your GPU ends up waiting for the data most of the time.
You can verify that claim by checking the status of the Python workers in <code>htop</code>.</p>
<p>You do not seem to have a CPU bottleneck because your CPU isn't fully used.</p>
<p>This often happens on VMs when the data is being transfered using old protocols like NFS.
If the VM you're using has a local disk, you can try copying the data there before the training, and point your huggingface dataset to that local path.
This could also be due to a suboptimal configuration of the data loading process. You might want to give <a href=""https://huggingface.co/docs/datasets/v3.2.0/en/cache#improve-performance"" rel=""nofollow noreferrer"">this</a> a read.</p>
<p>You might not be seeing this issue on your PC because:</p>
<ol>
<li>Your GPU is slower than an H100 hence takes more time to process a single batch. As a result, your system has more time to load the next batch.</li>
<li>Your data is stored in your local disk and therefore the time to load the data is much smaller.</li>
</ol>
<p>And yes, please increase your number of workers, it can drastically improve the performance.</p>
","1","Answer"
"79387933","79387529","<p>Okay, I figured it out.</p>
<p>It seems that since I'm only testing one function, I can skip the whole <code>function_declaration</code> thing. (You can do it but not necessary).</p>
<p>The important thing is to include <code>enable_automatic_function_calling</code> on the <code>llm.start_chat()</code> so basically:</p>
<p><code>llm_model.start_chat(enable_automatic_function_calling=True)</code></p>
<p>So in the end, the whole code comes to this:</p>
<pre><code>def predict_iris_species(sepal_length:float, sepal_width:float, petal_length:float, petal_width:float):

     #nothing changes...

llm_model = genai.GenerativeModel(model_name='gemini-1.5-flash', tools=[predict_iris_species])

chat = llm_model.start_chat(enable_automatic_function_calling=True)

prompt= &quot;&quot;&quot;

insert prompt here

&quot;&quot;&quot;
chat.send_message(prompt).text
</code></pre>
<p>This will show the output. Hopes this helps to the people experiencing the same issue</p>
","0","Answer"
"79390027","79377971","<p>For my case, the error &quot;InvalidArgumentError: No DNN in stream executor&quot; for training <a href=""https://www.tensorflow.org/tfmodels/vision/instance_segmentation"" rel=""nofollow noreferrer"">MaskR-CNN</a> on Colab disappeared after I downgraded Tensorflow and tf-models-official to the versions 2.17.1 and 2.17.0 (the old version is 2.18.0 for both), respectively. Hopefully, it will work for you.</p>
","2","Answer"
"79395715","79395477","<p><code>shard_func</code> must return a scalar <code>Tensor</code> of type <code>tf.int64</code> (not Python or NumPy integer). So you cannot just return <code>np.int64(...)</code> or do a Python‐level % on dictionary.You need to pick (or compute) tensor inside dataset element and return <code>tf.cast(..., tf.int64)</code>. For example if your CSV has a column &quot;c1&quot; you could do:</p>
<pre class=""lang-py prettyprint-override""><code>def shard_func(features, label):
    return tf.cast(features['c1'][0] % 10, tf.int64)

data_ts.save(&quot;my_data&quot;,shard_func=shard_func)
</code></pre>
<p>This will produce up to 10 different shards (files) named <code>my_data_0</code>, <code>my_data_1</code>, etc</p>
","1","Answer"
"79395798","79393871","<p>I checked grounding dino's source code and found that the normalized coordinates are of the form, <code>[center_x, center_y, width, height]</code>, so all i had to do was apply this code to obtain perfect bounding boxes.</p>
<pre><code>abs_box = boxes * torch.tensor([wid, hgt, wid, hgt])
abs_box = abs_box.numpy().astype(&quot;int&quot;)

cx, cy, wd, ht = abs_box 
cv2.rectangle(annotated_frame, (cx - (wd // 2), cy - (ht // 2)), (cx + (wd // 2), cy + (ht // 2)), (255, 0, 0), 2)
</code></pre>
<p>Hope it helps.</p>
","1","Answer"
"79395928","79374019","<p>The issue can be resolved by installing a previous version of scikit-learn
using the command <code>!pip install scikit-learn==1.3.1</code>. I created a dummy dataset
and trained the model on it, and the model works well for me. Please refer to
this <a href=""https://colab.sandbox.google.com/gist/malla456/e517d50255be5583c0cf91638ade3f1c/79374019.ipynb#scrollTo=LxeQkZS_u9TA"" rel=""nofollow noreferrer"">gist</a>.</p>
","1","Answer"
"79396902","79396860","<p>I think your dataset is running out cause you're batching before repeating.
Try replacing dataset = dataset.repeat(epochs).batch(batch_size), it should not be the other way around.
Try this, and do you have a Git repo for this</p>
","0","Answer"
"79397282","79396894","<p>Since you only seem to be interested in the <a href=""https://en.wikipedia.org/wiki/Haar_wavelet"" rel=""nofollow noreferrer"">Haar wavelet</a>, you can pretty much implement it yourself:</p>
<ul>
<li>The high-frequency component of the Haar wavelet along each dimension can be written as a pairwise difference.</li>
<li>The low-frequency component of the Haar wavelet along each dimension can be written as a pairwise sum.</li>
</ul>
<p>The following code achieves this in pure PyTorch:</p>
<pre class=""lang-py prettyprint-override""><code>class HaarWaveletLayer(nn.Module):
    
    def l_0(self, t):  # sum (&quot;low&quot;) along cols
        t = torch.cat([t, t[..., -1:, :]], dim=-2) if t.shape[-2] % 2 else t
        return (t[..., ::2, :] + t[..., 1::2, :])
    def l_1(self, t):  # sum (&quot;low&quot;) along rows
        t = torch.cat([t, t[..., :, -1:]], dim=-1) if t.shape[-1] % 2 else t
        return (t[..., :, ::2] + t[..., :, 1::2])
    def h_0(self, t):  # diff (&quot;hi&quot;) along cols
        t = torch.cat([t, t[..., -1:, :]], dim=-2) if t.shape[-2] % 2 else t
        return (t[..., ::2, :] - t[..., 1::2, :])
    def h_1(self, t):  # diff (&quot;hi&quot;) along rows
        t = torch.cat([t, t[..., :, -1:]], dim=-1) if t.shape[-1] % 2 else t
        return (t[..., :, ::2] - t[..., :, 1::2])
    
    def forward(self, x):
        
        x = .5 * x
        l_1 = self.l_1(x)
        h_1 = self.h_1(x)
        ll = self.l_0(l_1)
        lh = self.h_0(l_1)
        hl = self.l_0(h_1)
        hh = self.h_0(h_1)
        
        return torch.cat([ll, lh, hl, hh], dim=1)
</code></pre>
<p>In combination with your given code, you can convince yourself of the equivalence as follows:</p>
<pre class=""lang-py prettyprint-override""><code>t = torch.rand((7, 3, 127, 128)).to(&quot;cuda:0&quot;)
result_given = WaveletLayer()(t)
result_proposed = HaarWaveletLayer()(t)

# Same result?
assert (result_given - result_proposed).abs().max() &lt; 1e-5

# Time comparison
from timeit import Timer
num_timings = 100
print(&quot;time given:   &quot;, Timer(lambda: WaveletLayer()(t)).timeit(num_timings))
print(&quot;time proposed:&quot;, Timer(lambda: HaarWaveletLayer()(t)).timeit(num_timings))
</code></pre>
<p>The timing shows a speedup of more than a factor of 10 on my machine.</p>
<h2>Notes</h2>
<ul>
<li>The <code>t = torch.cat...</code> parts are only necessary if you want to be able to handle odd-shaped images: In that case, we pad by replicating the last row and column, respectively, mimicking the default padding of PyWavelets.</li>
<li>Multiplying <code>x</code> with .5 is done for normalization. Compare <a href=""https://dsp.stackexchange.com/questions/1739/"">this discussion</a> on the Signal Processing Stack Exchange for more details.</li>
</ul>
","4","Answer"
"79400582","79400379","<p>Due to some bad segmentations you will detect the background also but here is how to make it :</p>
<p>here you can find a colab notebook with this code -&gt;<a href=""https://colab.research.google.com/github/onuralpArsln/InfoBits/blob/main/bladeDetect.ipynb"" rel=""nofollow noreferrer"">colab</a> but you need to upload your image.</p>
<p>In colab it uses a diffrent opencv patch. Thats why colab code and the one i write down is diffrent.</p>
<p>You might need to play with binarization to optimize your solution.</p>
<p>It basically binarizes the image then, then using countour detection it finds biggest contour. Since image has some false positive segmentations. The it find extremites in given contour.</p>
<p>We used to use this logic for detecting cell ends this is pretty similar so it solves this too.</p>
<pre><code>import cv2
import numpy as np

# Load the image i have saved yours as img.png
image = cv2.imread(&quot;img.png&quot;, cv2.IMREAD_GRAYSCALE)

   # Binarization turns your detect pixels into big white blob so it is easiet to find contours.
_, binary = cv2.threshold(image, 10, 255, cv2.THRESH_BINARY)


# Find contours in the binary image we assume all the detected pixel are for windmill
contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Check if contours were found
if len(contours) == 0:
    print(&quot;No contours detected. Check the input image or thresholding step.&quot;)
else:
    # Find the largest contour (most likely the windmill blades as white blob)
    largest_contour = max(contours, key=cv2.contourArea)

    # Find the extreme points (leftmost, rightmost, topmost, bottommost)
    leftmost = tuple(largest_contour[largest_contour[:, :, 0].argmin()][0])   # Smallest X
    rightmost = tuple(largest_contour[largest_contour[:, :, 0].argmax()][0])  # Largest X
    topmost = tuple(largest_contour[largest_contour[:, :, 1].argmin()][0])    # Smallest Y
    bottommost = tuple(largest_contour[largest_contour[:, :, 1].argmax()][0]) # Largest Y

    # Print the coordinates of the extreme points
    print(f&quot;Leftmost: {leftmost}&quot;)
    print(f&quot;Rightmost: {rightmost}&quot;)
    print(f&quot;Topmost: {topmost}&quot;)
    print(f&quot;Bottommost: {bottommost}&quot;)

    # Draw the results on the image
    output = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)
    
    # Mark the extreme points on the image with red circles
    for point in [leftmost, rightmost, topmost]:
        cv2.circle(output, point, 10, (0, 0, 255), -1)  # Red color for points

    # Draw the contour in blue and convex hull in green
    cv2.drawContours(output, [largest_contour], -1, (255, 255, 0), 2)
    cv2.drawContours(output, [cv2.convexHull(largest_contour)], -1, (0, 255, 0), 2)

    
    cv2.imshow(&quot;result&quot;,output)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
</code></pre>
","0","Answer"
"79400820","79400379","<p>You'll need several processing to robustly find the tips coordinates of the turbine, no matter its rotation. Basically you want to find the edges of the turbine, then compute the center of the shape and cluster each points of the contour in 3 groups (cause there is 3 blades). You can do everything using <a href=""https://pypi.org/project/opencv-python/"" rel=""nofollow noreferrer"">opencv</a> and <a href=""https://pypi.org/project/numpy/"" rel=""nofollow noreferrer"">numpy</a></p>
<h2>Loading the image</h2>
<p>We use opencv to load the image and rasterize the colors to pure black or white. This will later help us find the contour:</p>
<pre class=""lang-py prettyprint-override""><code>def load_image(path):

    # Load the image
    image = cv2.imread(path)
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Threshold the image to get a binary image
    _, binary = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)

    return image, binary
</code></pre>
<p><a href=""https://i.sstatic.net/vpTw0qo7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vpTw0qo7.png"" alt=""binary image"" /></a>
It's a good start but as you can see the mask you got from yolo isn't 100% perfect. We'll easily fix that while computing the contours.</p>
<h2>Compute the turbine contour</h2>
<p>We can use <code>cv2.findContours</code> function to compute the different contours of the binary image. This will returns the contour of the turbine as well as the mask artefacts. So we will only keep the contour with the largest area, assuming that artifacts are smaller than the actual turbine:</p>
<pre class=""lang-py prettyprint-override""><code>def find_contour(binary):

    # Find the contours
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Since the masking is not perfect, we have several contours
    # Assuming the largest contour area corresponds to the turbine
    if len(contours) &gt; 0:
        largest_contour = max(contours, key=cv2.contourArea)
        return largest_contour
    
    # If no contours, return False
    return False
</code></pre>
<p><a href=""https://i.sstatic.net/WxNCCxAw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WxNCCxAw.png"" alt=""Turbine contours"" /></a></p>
<h2>Find the tips coordinates</h2>
<p>In order to find the tips we'll cluster the contour points on 3 groups, for the 3 blades. So let's first find the center of the turbine:</p>
<pre class=""lang-py prettyprint-override""><code>def find_tips(contour):

    # Calculate the centroid of the contour
    M = cv2.moments(contour)
    cx = int(M['m10'] / M['m00'])
    cy = int(M['m01'] / M['m00'])
    centroid = (cx, cy)

    [...]
</code></pre>
<p>Then we need to compute the angles and distances from the center for each points. I also sort the <code>angles_distances</code> list by angle:</p>
<pre class=""lang-py prettyprint-override""><code>def find_tips(contour):

    [...]

    # Calculate angles and distances from centroid
    angles_distances = []
    for point in contour:
        x, y = point[0]
        angle = angle_from_centroid(centroid, (x, y))
        distance = np.sqrt((x - cx)**2 + (y - cy)**2)
        angles_distances.append((angle, distance, (x, y)))

        # Sort by angle
        angles_distances.sort()

    [...]
</code></pre>
<p>Here is my <code>angle_from_centroid</code> function to calculate the angle (in radians):</p>
<pre class=""lang-py prettyprint-override""><code>def angle_from_centroid(centroid, point):
    # Compute the angle of the vector from centroid to the point
    angle = np.arctan2(point[1] - centroid[1], point[0] - centroid[0])
    return angle
</code></pre>
<p>We can finally split our points into 3 cluster and save the farthest point of each cluster, these are our tips coordinates:</p>
<pre class=""lang-py prettyprint-override""><code>    # Divide into three clusters and find farthest point in each
    num_points = len(angles_distances)
    tips = []
    for i in range(3):
        cluster = angles_distances[i * num_points // 3: (i + 1) * num_points // 3]
        farthest_point = max(cluster, key=lambda x: x[1])[2]
        tips.append(farthest_point)
            
    return tips
</code></pre>
<p>This last part is a bit complexe, I'm not getting to deep into it. But we can now draw the tips of the blades.</p>
<h2>Final result</h2>
<p>We can now run the 3 functions described above and draw the tips coordinates on the image:</p>
<pre class=""lang-py prettyprint-override""><code>image, binary = load_image('resources/trBo33Jy.png')
contour = find_contour(binary)
coordinates = find_tips(contour)

# Draw the tips coordinates on the image
for point in coordinates:
    cv2.circle(image, point, 20, (0, 255, 0), -1)
    print(point)

# Display the image with marked points
cv2.imshow('Image with Tips', image)
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>
<p><a href=""https://i.sstatic.net/UmYBH0IE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UmYBH0IE.png"" alt=""Final result, turbine with tip coordinates drawn"" /></a></p>
<p>If you're looking for the most precise result possible, you could try to calculate the average angle of each cluster to get the vector of the blades</p>
","-1","Answer"
"79406713","79406524","<p>I ran your code on Google Colab with Python version 3.11.11 and it worked perfectly.</p>
<p>I made dummy <code>y_test</code> and <code>y_predicted</code> arrays.</p>
<pre><code>y_test = [0]*122 + [0]*27 + [1]*40 + [1]*42
y_predicted = [0]*122 + [1]*27 + [0]*40 + [1]*42
</code></pre>
<p>Then, running your provided code plotted the confusion matrix using matplotlib.</p>
<pre><code>conf_matrix = confusion_matrix(y_test, y_predicted)
print(conf_matrix)

plt.figure(figsize=(3, 3), dpi=300)
# Scale up the size of all text
sns.set(font_scale = 1.1)
 
ax = sns.heatmap(conf_matrix, annot=True, fmt='d',)
 
# set x-axis label and ticks. 
ax.set_xlabel(&quot;Predicted Diagnosis&quot;, fontsize=14, labelpad=20)
ax.xaxis.set_ticklabels(['Negative', 'Positive'])
 
# set y-axis label and ticks
ax.set_ylabel(&quot;Actual Diagnosis&quot;, fontsize=14, labelpad=20)
ax.yaxis.set_ticklabels(['Negative', 'Positive'])
 
# set plot title
ax.set_title(&quot;Confusion Matrix for the Diabetes Detection Model&quot;, fontsize=14, pad=20)
 
plt.show()
</code></pre>
<p>Here's how it looked:
<a href=""https://i.sstatic.net/jtYpwKYF.png"" rel=""nofollow noreferrer"">Confusion Matrix</a></p>
","0","Answer"
"79408315","79406743","<p>QuickUMLS setup isn't finding the UMLS data correctly, leading to the &quot;UNK&quot; response.</p>
<p>You can try to test Different Inputs. Try common medical terms like <code>&quot;aspirin&quot;</code> or <code>&quot;heart attack&quot;</code> to see if they return valid CUIs, confirming the data is correctly linked.</p>
<p>Additionally, ensure that your quickumls_fp points to the actual data directory, not just the Python package location. The data directory should have subfolders like <code>CDB</code>, <code>json_db</code>, or <code>sqlite_db</code>.</p>
","1","Answer"
"79409575","79409259","<p>Generally speaking Hydra is independent of PyTorch and does not directly interact with (except via plugins).
<code>_partial_</code> has nothing at all to do with PyTorch or seeding.</p>
<p>At a glance what you are suggesting should work, but it's best if you just verify it.</p>
","1","Answer"
"79410558","79410458","<p>Even with the random state set, there can be change's with really small changes to any part of the process. It usually is a result of Changes in the Software Environment. There is nothing per say to do, the best thing you can do is ensure everything from the dataset to the package's and everything is locked down ensuring that there are no changes in between the run's.</p>
<p>There were some discussions on how this can be prevented I am linkthing them here so you can explore other options as well:</p>
<ul>
<li><a href=""https://github.com/scikit-learn/scikit-learn/discussions/25411"" rel=""nofollow noreferrer"">https://github.com/scikit-learn/scikit-learn/discussions/25411</a></li>
<li><a href=""https://github.com/scikit-learn/scikit-learn/issues/12188"" rel=""nofollow noreferrer"">https://github.com/scikit-learn/scikit-learn/issues/12188</a></li>
<li><a href=""https://github.com/scikit-learn/scikit-learn/issues/12259"" rel=""nofollow noreferrer"">https://github.com/scikit-learn/scikit-learn/issues/12259</a></li>
</ul>
","0","Answer"
"79410732","79409259","<p>Your understanding is correct.Using <em>partial</em> in Hydra simply returns <code>functools.partial</code> object and doesnt immediately execute class constructor or otherwise &quot;bake in&quot; seed.As result seed is not fixed at the time of creating that partial.You can safely call <code>torch.manual_seed(...)</code> or any other seed-setting functions just before you invoke the partial object multiple times for reproducible runs.</p>
<p>A common pattern is something like:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from hydra import compose, initialize

# For example your Hydra config defines partial for your model
with initialize(config_path=&quot;conf&quot;,version_base=None):
    cfg =compose(config_name=&quot;config&quot;)

model_partial=cfg.model  # A functools.partial(MyModel, ...)

# Then in your experiment loop :
for seed in [123, 456, 789]:
    torch.manual_seed(seed)
    model = model_partial()  # Actually instantiate model with given seed
    # train or evaluate your model
</code></pre>
","0","Answer"
"79411035","79399452","<p>You can find the artifacts in model page like shown below.</p>
<p><img src=""https://i.imgur.com/SOKKMbv.png"" alt=""enter image description here"" /></p>
<p>Download all of them and update the dependencies in
<code>conda.yaml</code> or  <code>requirement.txt</code>.</p>
<p>Then use below code to register it again with updated files.</p>
<pre><code>from azure.ai.ml.entities import Model
from azure.ai.ml.constants import AssetTypes

mlflow_model = Model(
    path=&quot;mlflow-model&quot;,#provide the download folder path and make sure you updated required dependencies.
    type=AssetTypes.MLFLOW_MODEL,
    name=&quot;local-mlflow-example&quot;,
    description=&quot;MLflow model created from local path&quot;,
)
ml_client.create_or_update(mlflow_model)
</code></pre>
<p>This will register new dataset or creates new version of the current one, further you deploy it.</p>
<p>Next, you also getting error from scoring script make sure you have compatible mlflow version.</p>
<blockquote>
<p>ModuleNotFoundError: No module named 'mlflow.gluon'</p>
</blockquote>
<p>Below is the recommended dependencies you need to add in yaml or <code>requirement.txt</code> file.</p>
<pre><code>      - werkzeug&lt;2.1.0
      - scikit-learn&lt;=1.1.3
      - mlflow==2.3.1
      - azureml-inference-server-http==1.3.4
      - azureml-evaluate-mlflow==0.1.0
</code></pre>
","0","Answer"
"79411171","79409259","<p>Before using <code>hydra.utils.instantiate</code> no third party code is not run by hydra. So you can set your seeds before each use of instantiate; or if a <code>partial</code> before each call to the partial.</p>
<p>Here a complete toy example, based on Hydra's <a href=""https://hydra.cc/docs/advanced/instantiate_objects/overview/"" rel=""nofollow noreferrer"">doc overview</a>, which creates a partial to instantiate an optimizer or a model, that takes a callable <code>optim_partial</code> as an argument.</p>
<pre class=""lang-yaml prettyprint-override""><code># config.yaml
model:
  _target_: &quot;__main.__.MyModel&quot;
  optim_partial:
    _partial_: true
    _target_: __main__.MyOptimizer
    algo: SGD
  lr: 0.01
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from functools import partial
from typing import Callable
import random
from pprint import pprint

import hydra
from omegaconf import DictConfig, OmegaConf


class MyModel:
    def __init__(self, lr, optim_partial: Callable[..., &quot;MyOptimizer&quot;]):
        self.optim_partial = optim_partial
        self.optim1 = self.optim_partial()
        self.optim2 = self.optim_partial()


class MyOptimizer:
    def __init__(self, algo):
        print(algo, random.randint(0, 10000))


@hydra.main(config_name=&quot;config&quot;, config_path=&quot;./&quot;, version_base=None)
def main(cfg: DictConfig):
    # Check out the config
    pprint(OmegaConf.to_container(cfg, resolve=False))
    print(type(cfg.model.optim_partial))
    
    # Create the functools.partial
    optim_partial: partial[MyOptimizer] = hydra.utils.instantiate(cfg.model.optim_partial)
    # Set the seed before you call the a partial
    random.seed(42)
    optimizer1: MyOptimizer = optim_partial()
    optimizer2: MyOptimizer = optim_partial()
    random.seed(42)
    optimizer1b: MyOptimizer = optim_partial()
    optimizer2b: MyOptimizer = optim_partial()

    # model is not a partial; use seed before creation
    random.seed(42)
    model: MyModel = hydra.utils.instantiate(cfg.model)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Output
{'model': {'_target_': '__main__.MyModel',
           'lr': 0.01,
           'optim_partial': {'_partial_': True,
                             '_target_': '__main__.MyOptimizer',
                             'algo': 'SGD'}}}
type of cfg.model.optim_partial &lt;class 'omegaconf.dictconfig.DictConfig'&gt;
SGD 1824
SGD 409
SGD 1824
SGD 409
SGD 1824
SGD 409
</code></pre>
","1","Answer"
"79411996","79411192","<p>I finally solved the problem going at it from another angle. Using the Ml.OnnxRuntime and ImageSharp greatly simplified the task.</p>
<p>Here is the working code:</p>
<pre><code>public class ImageSegmentationService : IDisposable
{
    private readonly InferenceSession _session;
    private const int ImageSize = 1024;

    public ImageSegmentationService(string modelPath)
    {
        _session = new InferenceSession(modelPath);
    }

    public float[] ProcessImage(string imagePath)
    {
        using var image = Image.Load&lt;Rgb24&gt;(imagePath);
        image.Mutate(x =&gt; x.Resize(ImageSize, ImageSize));
    
        // Prepare input tensor (normalize to [0,1] and convert to NCHW)
        var inputTensor = new DenseTensor&lt;float&gt;(new[] { 1, 3, ImageSize, ImageSize });
    
        for (int y = 0; y &lt; ImageSize; y++)
        {
            for (int x = 0; x &lt; ImageSize; x++)
            {
                var pixel = image[x, y];
                inputTensor[0, 0, y, x] = pixel.R / 255f;
                inputTensor[0, 1, y, x] = pixel.G / 255f;
                inputTensor[0, 2, y, x] = pixel.B / 255f;
            }
        }

        // Run inference
        var inputs = new List&lt;NamedOnnxValue&gt; 
        { 
            NamedOnnxValue.CreateFromTensor(&quot;pixel_values&quot;, inputTensor) 
        };

        using var outputs = _session.Run(inputs);
        var alphas = outputs.First().AsTensor&lt;float&gt;();
        
        return alphas.ToArray();
    }

    public void Dispose()
    {
        _session?.Dispose();
    }

    public class RmbgInput
    {
        [VectorType(1, 3, 1024, 1024)]
        public float[] pixel_values { get; set; }
    }

    public class RmbgOutput
    {
        [VectorType(1, 1024, 1024)]
        public float[] alphas { get; set; }
    }
}
</code></pre>
<p>The result of <code>ProcessImage(string imagePath)</code> is the alpha mask that should be applied to the original image (in 1024*1024 dimensions) to remove the background.</p>
","0","Answer"
"79413303","79413251","<p>It is about &quot;Teacher Forcing&quot; which is a trainign technique for seq2seq.</p>
<p>While teacher forcing it learns and when no theacher forcing it interferes( makes guesses.) So what is the realaiton with the shape?</p>
<p>Since decoder generates one token at time for each token in batch it needs <code>(batch_size, 1)</code> with 2D shape.
During teacher force it picks from batch (<code>target_tensor[:, i]</code>) and with unsqueze it generates the required shape.</p>
<p>In second case no teacher is not forcing so model makes a guess and it generate in shape of <code>(batch_size, 1)</code>.  But for tokens (words in nlp) represented with number making you guess token 1D is healtiher for general conssitency so <code>squeeze(-1)</code> to make it 1D. It will be reshaped down to GRU pipline but sending inside 1D makes it auto handle reshapes inside.</p>
","0","Answer"
"79414400","79411501","<p>This is indeed a bit tricky as several things here are mixed on the tf and keras level.</p>
<p>At best you use the model factories and <a href=""https://www.tensorflow.org/api_docs/python/tfm/vision/factory_3d/build_model"" rel=""nofollow noreferrer"">setups via configs</a></p>
<p>The main problem here is that you should not pass a layer here but an Input tensor created from <code>tf_keras.Input</code>.
Below I pointed you out with two ways to set up your model:</p>
<pre class=""lang-py prettyprint-override""><code>import tf_keras

def create_model():

    input_specs = tf.keras.layers.InputSpec(shape=(None, None, IMG_SIZE, IMG_SIZE, 3))
    # Setup Backbone
    backbone = tfm.vision.backbones.ResNet3D(
        model_id=50,
        temporal_strides=[3, 3, 3, 3],
        temporal_kernel_sizes=[(5, 5, 5), (5, 5, 5, 5), (5, 5, 5, 5, 5, 5), (5, 5, 5)],
        input_specs=input_specs,
    )

    # Variant 1 use functional API yourself
    inputs = tf_keras.Input(shape=input_specs.shape[1:], name=input_specs.name)
    endpoints = backbone(inputs)
    x = endpoints[max(endpoints.keys())]  # &lt;- Use your own function API from here

    ...  # set up your head and model manually

    # -- OR ---

    # Variant 2 Use a Classification Model with your backbone
    base_model = tfm.vision.classification_model.ClassificationModel(
        backbone=backbone,
        num_classes=NUM_CLASSES,
        input_specs=input_specs,
        dropout_rate=0.3,
        skip_logits_layer=False, # set to true to skip Droputout and final Dense Layer
        add_head_batch_norm=True, # adds a batchNorm by default
    )
</code></pre>
","0","Answer"
"79419484","79416292","<p>You could simply create your own function to transform your data:</p>
<pre><code>def myMinMaxScaler(X, Xmin, Xmax):
    return (X - Xmin) / (Xmax - Xmin)
</code></pre>
<p>Another option could be to add rows (with the samples containing the min and max) at the end of your batches and after the transformation remove the added rows.</p>
","1","Answer"
"79421112","79388942","<p>Posting the answer that worked for me in case anyone else comes across this issue.</p>
<p>Loading a SavedModel format in Keras 3 can be done using the tf.saved_model.load() function.</p>
<p>To use the model to predict on test data, you retrieve the model's prediction function by calling the &quot;serving_default&quot; signature attribute.</p>
<p>Here's the code i sued to predict on a batch of 32 (224,224,3) images, with (101,)  labels. I used the 101 Food classes dataset(<a href=""https://www.kaggle.com/datasets/dansbecker/food-101"" rel=""nofollow noreferrer"">101_Food_Dataset</a>)</p>
<pre><code>def predict_on_dataset(model, dataset):
  &quot;&quot;&quot;Predicts on a tf.data.Dataset and returns predictions and labels.&quot;&quot;&quot;
  predictions = []
  labels = []
  for i,images in enumerate(dataset.take(1).as_numpy_iterator().next()[0]):
    print(f&quot;Checking image {i}==========================================================&quot;)
    batch_predictions = model.signatures['serving_default'](tf.constant(images))['dense_8']
    rounded_pred= tf.round(batch_predictions.numpy())
    #print(f&quot;Rounded predition are {rounded_pred}&quot;)

    argument_pred = rounded_pred.numpy().argmax()
    #print(f&quot;Prediction corresponds to index {argument_pred}&quot;)

    text_prediction = test_data_10_percent.class_names[argument_pred]
    print(f&quot;Model predcits: {text_prediction}&quot;)

    correct_label = test_data_10_percent.take(1).as_numpy_iterator().next()[1][i].argmax()
    print(f&quot;Actual label is: {test_data_10_percent.class_names[correct_label]}&quot;)

    predictions.append(text_prediction)
    labels.append(test_data_10_percent.class_names[correct_label])
    i+=1
  return predictions, labels
</code></pre>
<p>Here's a detailed explanation on what happens in the code above:
<a href=""https://def%20predict_on_dataset(model,%20dataset):%20%20%20%22%22%22Predicts%20on%20a%20tf.data.Dataset%20and%20returns%20predictions%20and%20labels.%22%22%22%20%20%20predictions%20=%20%5B%5D%20%20%20labels%20=%20%5B%5D%20%20%20for%20i,images%20in%20enumerate(dataset.take(1).as_numpy_iterator().next()%5B0%5D):%20%20%20%20%20print(f%22Checking%20image%20%7Bi%7D==========================================================%22)%20%20%20%20%20batch_predictions%20=%20model.signatures%5B%27serving_default%27%5D(tf.constant(images))%5B%27dense_8%27%5D%20%20%20%20%20rounded_pred=%20tf.round(batch_predictions.numpy())%20%20%20%20%20#print(f%22Rounded%20predition%20are%20%7Brounded_pred%7D%22)%20%20%20%20%20%20argument_pred%20=%20rounded_pred.numpy().argmax()%20%20%20%20%20#print(f%22Prediction%20corresponds%20to%20index%20%7Bargument_pred%7D%22)%20%20%20%20%20%20text_prediction%20=%20test_data_10_percent.class_names%5Bargument_pred%5D%20%20%20%20%20print(f%22Model%20predcits:%20%7Btext_prediction%7D%22)%20%20%20%20%20%20correct_label%20=%20test_data_10_percent.take(1).as_numpy_iterator().next()%5B1%5D%5Bi%5D.argmax()%20%20%20%20%20print(f%22Actual%20label%20is:%20%7Btest_data_10_percent.class_names%5Bcorrect_label%5D%7D%22)%20%20%20%20%20%20predictions.append(text_prediction)%20%20%20%20%20labels.append(test_data_10_percent.class_names%5Bcorrect_label%5D)%20%20%20%20%20i+=1%20%20%20return%20predictions,%20labels"" rel=""nofollow noreferrer"">Working with SavedModel Format in Keras3</a></p>
","0","Answer"
"79430020","79424312","<p>Starting with <strong>transformers v4.51.0</strong> <a href=""https://huggingface.co/deepseek-ai/DeepSeek-R1"" rel=""nofollow noreferrer"">deepseek-ai/DeepSeek-R1</a> can be loaded directly with the library. To install the latest version of transformers run:</p>
<pre class=""lang-bash prettyprint-override""><code>pip install --upgrade transformers
</code></pre>
<p>To check the loaded transformers version run:</p>
<pre class=""lang-py prettyprint-override""><code>import transformers
print(transformers.__version__)
</code></pre>
<p>After that the code in the original post will run as long as the required hardware is available:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

messages = [
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},
]
pipe = pipeline(&quot;text-generation&quot;, model=&quot;deepseek-ai/DeepSeek-R1&quot;, trust_remote_code=True)
pipe(messages)
</code></pre>
<p><strong>transformers &lt;4.51.0</strong>:</p>
<p>The code you posted is auto generated and is not correct. The <a href=""https://huggingface.co/deepseek-ai/DeepSeek-R1"" rel=""nofollow noreferrer"">model card</a> states:</p>
<blockquote>
<p>NOTE: Hugging Face's Transformers has not been directly supported yet.</p>
</blockquote>
<p>The transformer library doesn't support the quantization method DeepSeek used for their model. Huggingface is working on a <a href=""https://github.com/huggingface/transformers/pull/35926"" rel=""nofollow noreferrer"">PR</a> to officially support it, but it will take some more time.</p>
<p>You can still deploy the model on a GPUs that support fp8 via, for example, <a href=""https://github.com/vllm-project/vllm"" rel=""nofollow noreferrer"">vllm</a>:</p>
<pre class=""lang-bash prettyprint-override""><code># Install vLLM from pip:
pip install vllm
# Load and run the model:
vllm serve &quot;deepseek-ai/DeepSeek-R1&quot;
# Call the server using curl:
curl -X POST &quot;http://localhost:8000/v1/chat/completions&quot; \
    -H &quot;Content-Type: application/json&quot; \
    --data '{
        &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1&quot;,
        &quot;messages&quot;: [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;What is the capital of France?&quot;
            }
        ]
    }'
</code></pre>
<p><a href=""https://github.com/huggingface/transformers/issues/35471#issuecomment-2624801096"" rel=""nofollow noreferrer"">Some people</a> have simply removed the quantization part from the config to load it with transformers. I haven't tested the performance impact this might have, so use it with caution:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM, AutoConfig

config = AutoConfig.from_pretrained(&quot;deepseek-ai/DeepSeek-R1&quot;, trust_remote_code=True)
del config.quantization_config

model = AutoModelForCausalLM.from_pretrained(&quot;deepseek-ai/DeepSeek-R1&quot;, config=config, trust_remote_code=True)
</code></pre>
","4","Answer"
"79430422","79409149","<p>Mathematically, the formula for Linear regression is given by</p>
<p>(y = X.W<sup>T</sup> + b )</p>
<p>here,</p>
<ul>
<li>Input tensor, <code>X</code>, has shape (batch_size, in_features)</li>
<li>Weight matrix, <code>W</code> has shape (out_features, in_features)</li>
<li>Bias, <code>B</code> is a vector of shape (out_features).</li>
</ul>
<p>When you make <code>X</code> in 1D (e.g., [50] in your case), matrix multiplication is undefined because the dimensions don't align. Therefore, to ensure correct computation input data (<code>X</code>) should be 2 dimensional, where:</p>
<ul>
<li>each row represents a sample</li>
<li>and each column represents a feature.</li>
</ul>
","0","Answer"
"79442212","79420818","<p>The problem is due to cache that does not get cleared when needed, this is an open issue.</p>
<p>the only way I found is to make a large data set to replace the old one in cache</p>
<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.Dataset.range(num_epochs // 8) #drop the cache every 8 epochs
dataset = dataset.flat_map(lambda newcache1: create_dataset().repeat(8))
Model.fit(newcache1=dataset, ...)
</code></pre>
","2","Answer"
"79457719","79424312","<p>The code you should use is:</p>
<pre><code>from huggingface_hub import InferenceClient

client = InferenceClient(
    provider=&quot;nebius&quot;,
    api_key=&quot;hf_xxxxxxxxxxxxxxxxxxxxxxxx&quot;
)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;What is the capital of France?&quot;
    }
]

completion = client.chat.completions.create(
    model=&quot;deepseek-ai/DeepSeek-V3&quot;, 
    messages=messages, 
    max_tokens=500,
)

print(completion.choices[0].message)
</code></pre>
","0","Answer"
"79516038","79395423","<p>I had overlooked the fact that <code>as_strided</code> creates a <em>view</em> of the original array rather than a new copy, so the solution in this case was to just add the grad windows into an empty windowed array and then return the source of the previous empty array.</p>
<p>With help from @user2357112, I found that the best replacement solution is:</p>
<pre><code>#Window frames for previous input / Mask creation
main_windows = np.lib.stride_tricks.as_strided(x_padded, self.window_shape, strides, writeable=False)
mask = np.apply_along_axis(self.funct, -1, main_windows.reshape(*self.out_size, -1), backwards=True).reshape(main_windows.shape)

#Use mask to distribute the gradient into the mask, reshaped into (channels, kernel height, kernel width, num of windows)
pre_grad = np.einsum(&quot;ghw,ghwxy-&gt;ghwxy&quot;, dx, mask)

# Zero array of original size (channels, in height, in width)
final_grad = np.zeros_like(x_padded)
final_windows = np.lib.stride_tricks.as_strided(final_grad, self.window_shape, strides, writeable=True)
np.add.at(final_windows, (slice(None)), pre_grad)
</code></pre>
<p>And this should properly add all of the windows back into the appropriate spots including overlap.</p>
","0","Answer"
"79580295","79387801","<p>Sorry, may be a bit late, but you can use <a href=""https://google-grain.readthedocs.io/en/latest/_autosummary/grain.experimental.InterleaveIterDataset.html#grain.experimental.InterleaveIterDataset"" rel=""nofollow noreferrer"">this API</a> with <code>cycle_length=1</code> :</p>
<pre><code>import grain


# You'd have ReplayDataSource instead of this class.
class List(list):

  def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    print(f&quot;{id(self)} got created&quot;)

  def __del__(self):
    print(f&quot;{id(self)} got deleted&quot;)


class MakeSourceMapDataset(grain.MapDataset):

  def __len__(self):
    return 10

  def __getitem__(self, index):
    return grain.MapDataset.source(List(range((index - 1) * 10, index * 10)))


ds = grain.experimental.InterleaveIterDataset(
    MakeSourceMapDataset(), cycle_length=1
).batch(5)

print(list(ds))
</code></pre>
<p>You can see that the sources are created and deleted one at a time (deletion in time is not guaranteed though, due to how Python garbage collection works):</p>
<pre><code>123792112779344 got created
123792112779344 got deleted
123792112780384 got created
123792112780384 got deleted
123792112783104 got created
123792112783104 got deleted
123792112784464 got created
123792112784464 got deleted
123792112787344 got created
123792112787344 got deleted
123792112781904 got created
123792112781904 got deleted
123792112788864 got created
123792112788864 got deleted
123792112787744 got created
123792112787744 got deleted
123792112789184 got created
123792112789184 got deleted
123792112785824 got created
123792112785824 got deleted
[array([-10,  -9,  -8,  -7,  -6]), array([-5, -4, -3, -2, -1]), array([0, 1, 2, 3, 4]), array([5, 6, 7, 8, 9]), array([10, 11, 12, 13, 14]), array([15, 16, 17, 18, 19]), array([20, 21, 22, 23, 24]), array([25, 26, 27, 28, 29]), array([30, 31, 32, 33, 34]), array([35, 36, 37, 38, 39]), array([40, 41, 42, 43, 44]), array([45, 46, 47, 48, 49]), array([50, 51, 52, 53, 54]), array([55, 56, 57, 58, 59]), array([60, 61, 62, 63, 64]), array([65, 66, 67, 68, 69]), array([70, 71, 72, 73, 74]), array([75, 76, 77, 78, 79]), array([80, 81, 82, 83, 84]), array([85, 86, 87, 88, 89])]
</code></pre>
","0","Answer"
"79581158","79407078","<p>Since the number of training data samples is completely different in your two example scripts, <strong>the first example uses only 6 training samples, while the second one uses 1,000 samples</strong>.</p>
<p>For Gaussian process regression, having more data generally results in a smaller <em><strong>confidence interval</strong></em> because the <em><strong>uncertainty</strong></em> decreases.</p>
<p>If you modify the second example to use the same training data as the first one, the resulting plot will look almost identical to the first example.</p>
<p>Specifically, if you change the following line:</p>
<p>note: the X &amp; y sample number are <strong>1,000.</strong></p>
<pre><code>gaussian_process_fixed.fit(X, y)
</code></pre>
<p>to:</p>
<p>note: the X_train &amp; y_train_noisy sample number are <strong>6.</strong></p>
<pre><code>gaussian_process_fixed.fit(X_train, y_train_noisy)
</code></pre>
<p>then the resulting graph will be very similar to the first one.</p>
","0","Answer"
"79592294","79419018","<p>Regarding the output shape of your YOLOv8 detection model being <strong>(1, 7, 8400)</strong> for 3 classes, instead of perhaps what you might have expected, this is actually the correct and <strong>expected raw output format for YOLOv8 before post-processing.</strong></p>
<p>Let's break down the meaning of this shape:</p>
<ul>
<li><p><strong>1</strong>: Represents the <strong>batch size</strong>. It's typically `1` for single-image inference.</p>
</li>
<li><p><strong>7</strong>: This dimension contains all the relevant information for each prediction location. For a detection task with <em>3</em> classes, this <em>7</em> is the sum of the prediction scores for the <strong>3 classes</strong> plus the <strong>&quot;4 parameters for each bounding box&quot; <em>(x, y, width, height)</em></strong>. Thus, `7 = 3 (number of classes) + 4 (bounding box parameters)`. Each of the `8400` locations outputs these `7` values.</p>
</li>
<li><p><strong>8400</strong>: Represents the total number of **potential detection locations** considered across all output levels (different scales) by the model. YOLOv8 makes predictions on feature maps of different sizes, and `8400` is the flattened total count of these prediction locations.</p>
</li>
</ul>
<p>Contrast this with the standard <strong>YOLOv</strong>8 detection model (trained on 80 COCO classes), whose raw detection output shape is typically <strong>(1, 84, 8400)</strong>. Here, `<strong>84</strong>` also follows the same pattern: `<strong>80 (number of classes) + 4 (bounding box parameters) = 84</strong>`. This further confirms that the output dimension structure is &quot;<strong>number of classes + 4</strong>&quot;.</p>
<p>This <em>(1, 7, 8400)</em> tensor is the <strong>raw prediction result</strong> generated by the YOLOv8 model after the network layers. It still needs to go through **post-processing steps**, such as confidence thresholding and <strong>Non-Maximum Suppression (NMS)</strong>, to obtain the final list of detected bounding boxes (e.g., each detection including location, confidence, class ID, etc.). The final detection results you typically work with are the output after these post-processing steps, not this raw <em>(1, 7, 8400)</em> tensor itself.</p>
<p>Please note that within the YOLOv8 model family, the output shapes for different tasks (such as detection vs. segmentation) are different. For example, the output of a <strong>YOLOv8 segmentation model (like YOLOv8n-seg)</strong> might include a tensor with a shape like <em>(1, 116, 8400)</em> (combining classes, box parameters, and mask coefficients) and another output for prototype masks. This also illustrates that the output shape structure is determined by the specific task and configuration of the model.</p>
","1","Answer"
"79655515","79381185","<p>Although the <a href=""https://arxiv.org/abs/1908.10084"" rel=""nofollow noreferrer"">Sentence BERT</a> improve the ability to evaluate of semantic similarity to BLEU, it lacks sufficient sensitivity to surface-level error such as spelling mistake, word order issue etc. According to this paper ( <a href=""https://arxiv.org/abs/2109.14250"" rel=""nofollow noreferrer"">Evaluation of Metrics Performance</a> ) research, I think the best evaluation is BLEU + <a href=""https://arxiv.org/abs/1908.10084"" rel=""nofollow noreferrer"">Sentence BERT</a>.</p>
","0","Answer"
"79430483","79430117","<p>It is not simple to convert PDF into HTML (as they are totally different constructions) and most of a Documents source styling and format structure is destroyed, since not needed for a &quot;printout format&quot; such as PDF for screen or paper (not web usage).</p>
<p>From the question see page 7 of <a href=""https://web.archive.org/web/20231015055310/https://awsdocs.s3.amazonaws.com/gettingstarted/latest/awsgsg-intro.pdf"" rel=""nofollow noreferrer"">https://web.archive.org/web/20231015055310/https://awsdocs.s3.amazonaws.com/gettingstarted/latest/awsgsg-intro.pdf</a></p>
<p>Here we have a PDF and one HTML reproduction, Where not all of the source data can be emulated.<br />
<code>&gt;mutool draw -o intro.html intro.pdf 7</code></p>
<p>The <strong>Python equivalence</strong> will be using PyMuPDF.</p>
<p>There is much collateral damage. So in this case the hyperlinks Table of Contents and line work is missing. Thus each PDF extractor will not be perfect, and nearly all require manual enhancements.</p>
<p><a href=""https://i.sstatic.net/IygexsWk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IygexsWk.png"" alt=""enter image description here"" /></a>
This one sees the lines of text as &quot;<code>&lt;p&gt;</code>aragraphs&quot;.
<a href=""https://i.sstatic.net/bm7wrBxU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bm7wrBxU.png"" alt=""enter image description here"" /></a></p>
<p>This one is older and thus better at conversion of linework and links.<br />
<code>Xpdf&gt;tools\bin32\pdftohtml.exe -f 7 -l 7 intro.pdf intro</code></p>
<p><a href=""https://i.sstatic.net/itTNY1rj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/itTNY1rj.png"" alt=""enter image description here"" /></a></p>
<p>However the table just like the PDF divisions cannot be a table just a lot of <code>&lt;div&gt;</code>s.</p>
<p><a href=""https://i.sstatic.net/eAKt5aQv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eAKt5aQv.png"" alt=""enter image description here"" /></a></p>
<p>Free Online services will often use a poppler <strong>variation</strong> of xpdf such as <a href=""https://github.com/pdf2htmlEX/pdf2htmlEX"" rel=""nofollow noreferrer"">https://github.com/pdf2htmlEX/pdf2htmlEX</a></p>
","1","Answer"
"79435055","79433458","<p>As of this writing, LightGBM does not have functionality like &quot;force at least 1 split on a given feature, but let LightGBM choose the threshold&quot;.</p>
<p>However, it is possible to force LightGBM to split on specific features with specific thresholds.</p>
<p>Here's an example (I tested it with <code>lightgbm</code> 4.5.0):</p>
<pre class=""lang-py prettyprint-override""><code>import json
import lightgbm as lgb
import numpy as np
from sklearn.datasets import make_regression

X, y = make_regression(
    n_samples=10_000,
    n_features=5,
    n_informative=5,
    random_state=42
)

# add a noise feature
noise_feature = np.random.random(size=(X.shape[0], 1))
X = np.concatenate((X, noise_feature), axis=1)

# train a small model
model1 = lgb.LGBMRegressor(
    random_state=708,
    n_estimators=10,
)
model1.fit(X, y)

# notice: that noise feature (the 6th one) was never chosen for a split
model1.feature_importances_
# array([  0,  97, 110,   0,  93,   0], dtype=int32)

# force the use of that noise feature in every tree
forced_split = {
    &quot;feature&quot;: 5,
    &quot;threshold&quot;: np.mean(noise_feature),
}
with open(&quot;forced_splits.json&quot;, &quot;w&quot;) as f:
    f.write(json.dumps(forced_split))

# train another model, forcing it to use those splits
model2 = lgb.LGBMRegressor(
    random_state=708,
    n_estimators=10,
    forcedsplits_filename=&quot;forced_splits.json&quot;,
)
model2.fit(X, y)

# noise feature was used once in every tree
model2.feature_importances_
# array([  0, 104, 131,   0,  55,  10], dtype=int32)
</code></pre>
<p>That JSON file defining the splits can be extended with arbitrarily deep nesting. (<a href=""https://lightgbm.readthedocs.io/en/latest/Parameters.html#forcedsplits_filename"" rel=""nofollow noreferrer"">LightGBM docs</a>)</p>
<p>For example, here's how to force it to use the 6th, 1st, and 4th features (in that order), split on their means, all down the left side of each tree.</p>
<pre class=""lang-py prettyprint-override""><code>forced_split = {
    &quot;feature&quot;: 5,
    &quot;threshold&quot;: np.mean(noise_feature),
    &quot;left&quot;: {
        &quot;feature&quot;: 0,
        &quot;threshold&quot;: np.mean(X[:,0]),
        &quot;left&quot;: {
            &quot;feature&quot;: 3,
            &quot;threshold&quot;: np.mean(X[:,2]),
        }
    }
}
with open(&quot;forced_splits.json&quot;, &quot;w&quot;) as f:
    f.write(json.dumps(forced_split))

model3 = lgb.LGBMRegressor(
    random_state=708,
    n_estimators=10,
    forcedsplits_filename=&quot;forced_splits.json&quot;,
).fit(X,y)

model3.feature_importances_
# array([ 10, 114, 133,  10,  23,  10], dtype=int32)
</code></pre>
<p>If you don't want the same structure for every tree, you could look into using &quot;training continuation&quot;, changing this parameter for each batch of training rounds. See <a href=""https://stackoverflow.com/questions/73664093/lightgbm-train-vs-update-vs-refit/73669068#73669068"">LightGBM: train() vs update() vs refit()</a>.</p>
","1","Answer"
"79437505","79437180","<p>Pretty silly of me, but I think the problem is that the net was learning every step, instead of accumulating it's decisions and then learn them. I overlooked that.</p>
","-1","Answer"
"79440695","79439462","<p>Without additional knowledge on what the different situations of the same system mean, I believe you should train a different KNN for each dataframe by <strong>first defining the KNN for each dataframe</strong> separately then fitting them.</p>
<pre><code>from sklearn.neighbors import KNeighborsClassifier
        
# assuming your training data for 2 example dataframes are X_training_1 and X_training_2, etc.
        
knn_1 = KNeighborsClassifier(n_neighbors=3)
knn_1.fit(X_training_1,y_training_1)
    
knn_2 = KNeighborsClassifier(n_neighbors=3)
knn_2.fit(X_training_2,y_training_2)
</code></pre>
","1","Answer"
"79442701","79442381","<p>Just transpose everything!</p>
<p>In the example I transpose each W and X to get the same distribution of random numbers, so you can compare the result. But you would normally define each variable already transposed as in the commented lines.</p>
<pre><code>import numpy as np

np.random.seed(42) # for reproducible results
n_inpts = 10
in_feats = 5
n_hidden = 8
out_feats = 1

X = np.random.randn(n_inpts, in_feats).T
# X = np.random.randn(in_feats, n_inpts)

W_x = np.random.randn(in_feats, n_hidden).T
# W_x = np.random.randn(n_hidden, in_feats)

bias_h  = np.random.randn(n_hidden, 1) # column vector
H = np.dot(W_x, X) + bias_h
#H is nxh

relu = lambda x: max(0, x)
v_relu = np.vectorize(relu)

H = v_relu(H)

W_h = np.random.randn(n_hidden, out_feats).T
# W_h = np.random.randn(out_feats, n_hidden)

bias_o = np.random.randn(out_feats, 1)

output = np.dot(W_h, H) + bias_o
print(output)
</code></pre>
<p>Some people like to think the &quot;default&quot; vector as a column. So if you start thinking your perceptron/network from the point of view of one single input example, <em>X</em> is going to have shape (features, 1), the same as the bias. And later observations are added as more columns. It can be pedagogical, but the resulting <em>X</em> is a transposed version of the classic &quot;spreadsheet&quot; representation, with one observation per row and one feature per column.</p>
","0","Answer"
"79443272","79440021","<p>The changes are covered that gradient descent, correcting previous errors in the gradient calculation and how the parameters were updated.  It now uses the correct analytical gradient, grad(theta), derived directly from the cost function (<strong>erreurJ</strong>), replacing the previous incorrect gradient() function, and <strong>removes an unnecessary inner loop</strong>, ensuring a single, <strong>vectorized parameter update</strong> in each iteration. These key changes—using the correct gradient and a streamlined update step—ensure the algorithm properly converges towards the minima of the cost function.</p>
<pre><code>import numpy as np
from random import randint, random
import matplotlib.pyplot as plt

# Activation function (sigma) and its derivative (sigmaprime)
sigma = lambda z: z**2 - 1
sigmaprime = lambda z: 2 * z

# Cost function (erreurJ) -  !here some updates!
def erreurJ(theta, sigma):
    somme = 0
    somme = 1/4*(sigma(theta[1])**2+sigma(theta[0]+theta[1])**2)
    return somme

def grad(theta):
    w, b = theta[0], theta[1]
    return np.array([2*b**3+3*b**2*w+3*b*w**2-2*b+w**3- 
                     w,b**3+3*b**2*w+3*b*w**2-b+w**3-w])

# Gradient Descent (pasfixe)
def pasfixe(theta, eta, epsilon, sigma, sigmaprime): 
    n = 0
    theta = np.array(theta, dtype=np.float64)  # transform appropriate np.array
    while np.linalg.norm(grad(theta)) &gt; epsilon and n &lt; 10000:
        gradient = grad(theta)  
        theta = theta - eta * gradient 
        n += 1

        #Check divergence
        if np.any(np.abs(theta) &gt; 100):
            return [100, 100]

    return theta


eta = 0.01  # Learning rate
epsilon = 1e-4 # Tolerance

#initial values
lst = [[3 * random() * (-1)**randint(0, 1), 3 * random() * (-1)**randint(0, 1)] for i in range(5000)]
listetheta = []
listeY = []

# Gradient Descent for each starting point
for i in lst:
    theta = i[:]  #Copy initial values
    CalculTheta = pasfixe(theta, eta, epsilon, sigma, sigmaprime) 
#gradient descent
    listetheta.append(CalculTheta)  #the new parameters
    listeY.append(erreurJ(CalculTheta, sigma))  #final cost function value


# Rounding
for i in range(len(listeY)):
    listeY[i] = round(listeY[i], 2)

for i in range(len(listetheta)):
    for j in range(2):
        listetheta[i][j] = round(listetheta[i][j], 2)

# Visualization
for i in range(len(lst)):
    if [int(listetheta[i][0]), int(listetheta[i][1])] in [[-2, 1]]:
        plt.plot(lst[i][0], lst[i][1], &quot;bo&quot;)
    elif [int(listetheta[i][0]), int(listetheta[i][1])] in [[2, -1]]:
        plt.plot(lst[i][0], lst[i][1], &quot;co&quot;)
    elif [int(listetheta[i][0]), int(listetheta[i][1])] in [[0, -1]]:
        plt.plot(lst[i][0], lst[i][1], &quot;go&quot;)
    elif [int(listetheta[i][0]), int(listetheta[i][1])] in [[0, 1]]:
        plt.plot(lst[i][0], lst[i][1], &quot;mo&quot;)
    elif int(listetheta[i][0])**2 + int(listetheta[i][1])**2 &gt;= 10: #divergence zone
        plt.plot(lst[i][0], lst[i][1], &quot;ro&quot;)

plt.xlabel(&quot;Initial w (Weight)&quot;)
plt.ylabel(&quot;Initial b (Bias)&quot;)
plt.title(&quot;Gradient Descent Convergence&quot;)
plt.show()
</code></pre>
","1","Answer"
"79446543","79439462","<p>You have to either combine all your dataframes into one, or get benefit of the potential for model ensemble methods (i.e. combining predictions from multiple models trained on different datasets can sometimes yield better performance than a single model.)
You can train separate KNN models on each DataFrame and then combine them using a voting classifier as follows:</p>
<pre><code>from sklearn.ensemble import VotingClassifier

estimators = []
for i, df in enumerate(dataframes):
    X = df.drop('target', axis=1).dropna()
    y = df['target'].loc[X.index]
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X, y)
    estimators.append((f'knn_{i}', knn))

ensemble = VotingClassifier(estimators=estimators, voting='hard')
ensemble.fit(X_train, y_train)
</code></pre>
","1","Answer"
"79449274","79448585","<p>This is to be approached physically. No &quot;processing&quot;.</p>
<p>You should backlight the substrate. Substrate will be brighter than substrate + label.</p>
<p>Make sure to fix the camera's exposure. If it auto-adjusts to a bright substrate, it might underexpose the label itself, which would be bad. You still need to find the rectangle on each label, and that works best when the label is still lit decently, not too dark.</p>
<hr />
<p>Typically, the printer recognizes the &quot;top of form&quot; (label) using <em>some</em> sensor, which might be optical or capacitive. Those sensors usually work very reliably. If your setup has no such sensor, you should talk to whoever made the printer.</p>
<p>A feedback loop through computer vision is much larger than a feedback loop through a sensor reacting directly to the paper's position. You always want feedback loops to be as tight as possible.</p>
","1","Answer"
"79449715","79448915","<p>There is no class named <em>QuantumKernel</em>, hence the error.</p>
<p>See the API ref <a href=""https://qiskit-community.github.io/qiskit-machine-learning/apidocs/qiskit_machine_learning.kernels.html#module-qiskit_machine_learning.kernels"" rel=""nofollow noreferrer"">https://qiskit-community.github.io/qiskit-machine-learning/apidocs/qiskit_machine_learning.kernels.html#module-qiskit_machine_learning.kernels</a> for the classes that are there.</p>
<p>Also see the tutorials such as this one for quantum kernels <a href=""https://qiskit-community.github.io/qiskit-machine-learning/tutorials/03_quantum_kernel.html"" rel=""nofollow noreferrer"">https://qiskit-community.github.io/qiskit-machine-learning/tutorials/03_quantum_kernel.html</a></p>
","0","Answer"
"79450559","79434756","<p>Your <code>X_new</code> has 7 columns but in the training set <code>X</code> has more columns (it seems that there are 13). From your code at least column <code>&quot;Type_LS&quot;</code> is in <code>X</code> but not in <code>X_new</code>.</p>
<p>Even though you passed parameter <code>remainder=&quot;drop&quot;</code> to <code>ColumnTransformer</code> it expects the same number of columns so it knows <em>what to drop</em> and <em>what to transform</em>.</p>
","0","Answer"
"79451470","79449572","<p>Assuming nans are at the end of the sequences, you can try:</p>
<ul>
<li><p>Replacing all nan values in the input with 0. Or other values if they make more sense.</p>
</li>
<li><p>Cutting the sequences so length of the sequences is equal to minimal sequence length.</p>
</li>
<li><p>Duplicating last or first data point in sequences so all sequences are of same length(max sequence length).</p>
</li>
</ul>
<p>Pick whichever makes more sense in your case. If you don't know which is better, try them all and compare the results.</p>
","0","Answer"
"79456650","79448915","<p>You could try this version:
qiskit==0.44.0 qiskit-machine-learning==0.6.1</p>
","0","Answer"
"79457331","79457237","<p>Perhaps the issue is that inside the <code>fit_model</code> function, the <code>epochs</code> parameter in <code>model.fit</code> is not passed as a named argument, and the model interprets <code>epochs</code> as something else. Try changing it to:</p>
<pre><code>history = model.fit(X_train_sequence_tensor, Y_train_sequence_tensor, epochs=epochs, 
                    validation_data=(X_val_sequence_tensor, Y_val_sequence_tensor))
</code></pre>
<p>Here, it is explicitly stated that the <code>epochs</code> parameter of the model is equal to the <code>epochs</code> parameter you passed to the function.</p>
","1","Answer"
"79457380","79457237","<p>If <code>epochs</code> is not explicitly passed, Python may use a default value, which could be <code>None</code> or another unintended value. Explicitly passing <code>epochs=epochs</code> ensures that the function uses the value intended by the caller.</p>
<p>Here is updated code:</p>
<pre><code>def fit_model(model, X_train_sequence_tensor,Y_train_sequence_tensor, epochs, val_set, time_windows, scaler):
    
    X_column_list = [item for item in val_set.columns.to_list() if item not in ['date', 'user', 'rank','rank_group', 'counts', 'target']]
    X_val_set = val_set[X_column_list].round(2)
                    
    X_val_set[X_val_set.columns] = scaler.transform(X_val_set[X_val_set.columns] )
    X_val_sequence = get_feature_array(X_val_set , X_column_list, time_windows)
    X_val_sequence_tensor = tf.convert_to_tensor(X_val_sequence, dtype=tf.float32)
    
    Y_column_list = ['target']                
    Y_val_set = val_set[Y_column_list].round(2)
    Y_val_sequence = get_feature_array(Y_val_set , Y_column_list, time_windows)
    Y_val_sequence_tensor = tf.convert_to_tensor(Y_val_sequence, dtype=tf.float32)
    try:
        history = model.fit(X_train_sequence_tensor,
                            Y_train_sequence_tensor,
                            epochs=epochs, 
                            validation_data=(X_val_sequence_tensor, Y_val_sequence_tensor))
    except Exception as e:
        print(f&quot;Training stopped due to an error: {e}&quot;)
                    
    
    return model, history


fitted_model, history = fit_model(model, X_train_sequence_tensor,Y_train_sequence_tensor,  epochs=100, val_set=val_set, time_windows=90, scaler=scaler) 
# Print Training History
print(&quot;Training Completed Successfully!&quot;)
</code></pre>
","1","Answer"
"79457623","79457437","<p>After some rubber-ducking, I realised that the model is loaded but not compiled or built. After implementing the compilation and building, the model can successfully be trained again.</p>
<p>The following are the necessary modifications.</p>
<pre class=""lang-py prettyprint-override""><code># Check to see if the model has already been trained
if USE_EXISTING_MODELS and os.path.exists(f&quot;models/{model_name}.h5&quot;):
        model = model = tf.keras.models.load_model(f&quot;models/{model_name}.h5&quot;)
        model.compile(optimizer=&quot;adam&quot;, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
        model.build()
        model.summary()
</code></pre>
","0","Answer"
"79458692","79455291","<p>A list arrays of differing sizes will produce your error message:</p>
<pre><code>In [8]: alist = [np.array([1,2]), np.array([3,4]), np.array([5,6,7])]

In [9]: alist
Out[9]: [array([1, 2]), array([3, 4]), array([5, 6, 7])]

In [10]: np.array(alist)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[10], line 1
----&gt; 1 np.array(alist)

ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.
</code></pre>
<p><code>stack</code> applied to the same list produces a different message:</p>
<pre><code>In [11]: np.stack(alist)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[11], line 1
----&gt; 1 np.stack(alist)

File ~\miniconda3\envs\mypy\Lib\site-packages\numpy\_core\shape_base.py:460, in stack(arrays, axis, out, dtype, casting)
    458 shapes = {arr.shape for arr in arrays}
    459 if len(shapes) != 1:
--&gt; 460     raise ValueError('all input arrays must have the same shape')
    462 result_ndim = arrays[0].ndim + 1
    463 axis = normalize_axis_index(axis, result_ndim)

ValueError: all input arrays must have the same shape
</code></pre>
<p>Applied to a correct list:</p>
<pre><code>In [12]: alist = [np.array([1,2]), np.array([3,4]), np.array([5,6])]

In [13]: np.array(alist)
Out[13]: 
array([[1, 2],
       [3, 4],
       [5, 6]])

In [14]: np.stack(alist)
Out[14]: 
array([[1, 2],
       [3, 4],
       [5, 6]])
</code></pre>
<p>If the number of <code>conditions</code> is variable, your <code>batch_labels</code> list could be something like</p>
<pre><code>In [16]: alist = [np.array([]), np.array([[1,2]]), np.array([[3,4],[5,6]])]

In [17]: alist
Out[17]: 
[array([], dtype=float64),   # 0 conditions
 array([[1, 2]]),            # 1
 array([[3, 4],              # 2 conditions
        [5, 6]])]

In [18]: [i.shape for i in alist]
Out[18]: [(0,), (1, 2), (2, 2)]

In [19]: np.array(alist)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[19], line 1
----&gt; 1 np.array(alist)

ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.
</code></pre>
","0","Answer"
"79462927","79430117","<p>Try to use microsoft document intelligence studio , make sure you have to give output_content_format=&quot;markdown&quot;</p>
","-2","Answer"
"79464697","79456368","<p>Here you are fitting 'train' instead of 'temp' which contains the extra regressors. Also note that you need future values for any extra regressor you pass to prophet.</p>
<p>When you create your future dataframe,</p>
<pre><code>future = model.make_future_dataframe(periods=24 * 365 * 3, freq=&quot;h&quot;)
</code></pre>
<p>This expects to produce a 3 year forecast at hourly frequency resulting in &gt; 20,000 rows. However the 'hourlyLag' regressor is only shifted down 1 position, assuming your 'Temperature' data ends at the start of your forecast horizon, this is not sufficient data to predict your forecast horizon length.</p>
<p>You can review the <a href=""https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#additional-regressors"" rel=""nofollow noreferrer"">Prophet</a> documentation for a more detailed explaination.</p>
","0","Answer"
"79467871","79467839","<p>Try not to use repr. repr() shows the representation numpy uses internally, which can still use scientific notation. To avoid that use <code>np.set_printoptions(suppress=True)</code> which forces numpy to avoid scientific notation</p>
<p>Add print configuration right after imports</p>
<pre><code>import numpy as np
np.set_printoptions(suppress=True, precision=20)
</code></pre>
<p>After training loop:</p>
<pre><code>print(&quot;Final weights:&quot;)
print(np.array2string(weights, separator=', '))
print(&quot;\nFinal bias:&quot;)
print(f&quot;{bias:.20f}&quot;)
</code></pre>
","-1","Answer"
"79468786","79457834","<p>When i was making this post i cleaned up some logging and argument parsing from the code, which was initially causing the serialization issue. The problematic part in my code was a line where i called the argument parser from the train function as follows:</p>
<pre><code>def train_unet_model(config):
    ...
    args = get_args()
    model = UNet(n_channels=3,
                 n_classes=args.classes,
                 bilinear=args.bilinear,
                 base_channels=config[&quot;BASE_CHANNELS&quot;],
                 kernel_size=config[&quot;SAMPLING_KERNEL_SIZE&quot;],
                 use_bias=config[&quot;USE_BIAS&quot;],
                 base_mid_channels=config[&quot;BASE_MID_CHANNELS&quot;])
    ...
</code></pre>
<p>and the argument getter function looking like:</p>
<pre><code>def get_args():
    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')
    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=5, help='Number of epochs')
    ...
    return parser.parse_args()
</code></pre>
<p>If you want to keep the parser, then use it outside of the trainable and pass the extracted values down via simple variables.</p>
","-1","Answer"
"79471910","79469004","<p>The key for this to work with scaling is to design the model and its adapter right, such that there is no need to pass any scaling parameters or what function to use in the evaluation configuration.</p>
<p>As part of Evaluation in Modeling Objectives, Foundry calls your model's predict method on the input dataset to produce inference results, which are then used for evaluation. Foundry actually needs to call your model's predict method in many cases - for any type of inference, whether <a href=""https://www.palantir.com/docs/foundry/manage-models/create-a-model-deployment"" rel=""nofollow noreferrer"">live</a> or <a href=""https://www.palantir.com/docs/foundry/model-integration/tutorial-train-code-repositories/#2b4-how-to-test-inference-logic-in-code-repositories"" rel=""nofollow noreferrer"">batch</a>. This is why the concept of Model adapters exists: so that Foundry can call the model safely.</p>
<p>The <a href=""https://www.palantir.com/docs/foundry/model-integration/tutorial-train-jupyter-notebook/#model-training"" rel=""nofollow noreferrer"">Linear Regression example</a> in the Foundry documentation has an example of how to design the model training code and adapter code to scale features based on the training data's distribution:</p>
<pre><code>from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

numeric_features = ['median_income', 'housing_median_age', 'total_rooms']
numeric_transformer = Pipeline(
    steps=[
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)),
        (&quot;scaler&quot;, StandardScaler())
    ]
)

model = Pipeline(
    steps=[
        (&quot;preprocessor&quot;, numeric_transformer),
        (&quot;classifier&quot;, LinearRegression())
    ]
)
X_train = train_df[numeric_features]
y_train = train_df['median_house_value']
model.fit(X_train, y_train)
</code></pre>
<p>The &quot;model&quot; here is a scikit-learn pipeline which includes the Scaler, and is fit using the training data. This entire pipeline model - including the Scaler - is passed to the adapter to be saved to Foundry. This implicitly saves any scaling parameters computed on training data alongside the model for later use during inference.</p>
<pre><code>from linear_regression_model_adapter import LinearRegressionModelAdapter

# Wrap the trained model in a model adapter for Foundry
linear_regression_model_adapter = LinearRegressionModelAdapter(model=model)
</code></pre>
<p>During inference, prior to evaluation, Foundry calls your model adapter's predict method which in turn calls predict on the pipeline model, applying the saved scaler's transform method <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.predict"" rel=""nofollow noreferrer"">in the process</a>:</p>
<pre><code>import palantir_models as pm


class LinearRegressionModelAdapter(pm.ModelAdapter):

    @pm.auto_serialize
    def __init__(self, model):
        self.model = model
    ...

    def predict(self, df_in):
        # This calls .transform from the inputer and scaler,
        # and finally .predict from the LinearRegression model
        df_in['prediction'] = self.model.predict(
            df_in[['median_income', 'housing_median_age', 'total_rooms']]
        )
        return df_in
</code></pre>
","1","Answer"
"79473713","79473125","<p>Simply create them via one of the creation methods (e.g. <code>torch.tensor</code> or <code>torch.randn</code>) but with <code>requires_grad=True</code>:</p>
<pre class=""lang-py prettyprint-override""><code>alpha = torch.tensor(1., requires_grad=True)
gamma = torch.randn(1, requires_grad=True)
</code></pre>
","1","Answer"
"79474049","79473125","<p>Loss scaling factors are hyperparameters. They need to be set outside the learning loop - they cannot be learned.</p>
<p>The reason for this is the model can trivially achieve zero loss by ignoring the actual loss term and instead setting the scaling term to 0 or a large negative number.</p>
<p>The coefficients <code>alpha</code>, <code>beta</code>, etc cannot be part of the loss optimization itself. Look into hyperparameter tuning for methods on selecting and evaluating different hyperparameters outside the loss optimization loop.</p>
","3","Answer"
"79474388","79474361","<p>Weights can still be used with hard voting. From <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"" rel=""nofollow noreferrer"">the documentation</a> for the <code>weights</code> parameter:</p>
<blockquote>
<p>Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) [...]</p>
</blockquote>
<p>So in your example, the random forest gets 3 votes while the other models each get 1 (and so the ensemble is really just the random forest).</p>
","2","Answer"
"79474472","79473125","<p>Theoretically you can do it as follows:</p>
<pre class=""lang-py prettyprint-override""><code>
alpha = torch.tensor([1.], requires_grad=True)
beta = torch.tensor([1.], requires_grad=True)
gamma = torch.tensor([1.], requires_grad=True)
delta = torch.tensor([1.], requires_grad=True)

your_other_model = YourOtherModelClass()

optimizer = torch.optim.Adam([
    {'params': your_other_model.parameters(), 'lr': 1e-4},
    {'params': [alpha, beta, gamma, delta], 'lr': 1e-4}    # put trainable tensors in an iterable such as a list 
])

# training loop
for i in range(num_iters):
    loss0, ..., loss3 = ... # model forward and loss computation
    loss = alpha * loss0 + beta * loss1 + gamma * loss2 + delta* loss3
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>
<p>But, as other answers pointed out, making hyperparameters optimizable may not make practical sense.</p>
","1","Answer"
"79475074","79474503","<p>You have the two trainable parameters from the network, weight and bias, and there are two non-trainable params from the optimizer. This <a href=""https://faroit.com/keras-docs/2.0.9/optimizers/#:%7E:text=The%20parameters%20clipnorm%20and%20clipvalue%20can%20be%20used,of%201.%20sgd%20%3D%20optimizers.SGD%28lr%3D%200.01%2C%20clipnorm%3D%201.%29"" rel=""nofollow noreferrer"">link</a> explains it pretty well.</p>
","1","Answer"
"79475280","79474503","<p>When you compile your model with keras, adding two more parameters. This happens due to automatic parameter tracking and serialization mechanisms. Try to use <code>model.optimizer.get_config()</code> for see additional parameters.</p>
","0","Answer"
"79479722","79471646","<p>You just need to add the parameter <code>unknown_value</code> and give it a constant value like -1 or 9999 or whatever you like. This value will be given to all the categories that were not in the data used to fit the encoder. For example:</p>
<pre class=""lang-py prettyprint-override""><code>    enc = OrdinalEncoder(handle_unknown=&quot;use_encoded_value&quot;, unknown_value=-1)
</code></pre>
","0","Answer"
"79481427","79471261","<p>There are multiple underlying issues why we encountered NULL values after using the ML.EVALUATE function. You may review this <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-evaluate#mlevaluate_with_no_input_data_specified"" rel=""nofollow noreferrer"">ML.EVALUATE public documentation</a>; it describes the ML.EVALUATE function, which lets you evaluate model metrics. You may also try this <a href=""https://cloud.google.com/bigquery/docs/arima-speed-up-tutorial#console"" rel=""nofollow noreferrer"">guide on executing ML.EVALUATE</a>.</p>
<p>Since you already successfully trained your model you may try simplifying your evaluation query.</p>
<pre><code>SELECT *
FROM ML.EVALUATE(
  MODEL `project.dataset.arima_model`,
  (
    SELECT data_column, id_column_1, id_column_2, timestamp_column
    FROM `project.dataset.source_table`
    WHERE DATE(timestamp_column) BETWEEN '2025-02-13' AND '2025-02-20'
  )
);
</code></pre>
","0","Answer"
"79485986","79478755","<p>As described in <a href=""https://stackoverflow.com/a/31171332"">this answer</a> (to the question you already linked in your question), the scalar box constraint <code>C</code> defines a penalty on the sum of slack variables. In the dual optimization problem, this corresponds to an upper bound to the Lagrange multipliers, hence the name &quot;Box Constraint&quot;. As you mention, this is a <em>scalar</em>, not a vector.</p>
<p>The <code>BoxConstraints</code> property is, however, a <em>n</em>-by-<em>1</em> vector, where <em>n</em> is the number of training observations. Note that when calling <code>fitcsvm</code> you can only set <code>C</code> to a scalar and not a vector, so the <code>BoxConstraints</code> property is internally calculated.</p>
<p>In the <a href=""https://ch.mathworks.com/help/stats/fitcsvm.html#bt7oo83-5"" rel=""nofollow noreferrer"">Algorithms section of the <code>fitcsvm</code> function documentation</a>, MATLAB specifies, what actually happens under the hood (highlighting by me):</p>
<blockquote>
<p>For two-class learning, <code>fitcsvm</code> <strong>assigns a box constraint to each observation</strong> in the training data.
The formula for the box constraint of observation <em>j</em> is</p>
<p><em>C<sub>j</sub> = nC<sub>0</sub>w<sub>j</sub><sup></em></sup>*</p>
<p>where <em>C<sub>0</sub></em> is the initial box constraint,
and <em>w<sub>j</sub><sup></em></sup>* is the observation weight adjusted by Cost and Prior for observation <em>j</em>.</p>
</blockquote>
<p>The concept of Cost, Prior and Weight is further documented <a href=""https://ch.mathworks.com/help/stats/supervised-learning-machine-learning-workflow-and-algorithms.html#mw_4cd1857b-b486-4247-b328-5fd810649696"" rel=""nofollow noreferrer"">here</a> .When calling <code>fitcsvm</code>, you can specify a <code>Cost</code>, <code>Prior</code> and/or <code>Weights</code>:</p>
<ul>
<li>With a <em>Cost</em> matrix <code>C</code> (caution, also letter <code>C</code> but now used as a cost and not as the box constraint!), you can specify the cost of misclassifying an observation For <em>K</em> classes, it is a <em>K</em>-by-<em>K</em> matrix where <em>c<sub>ij</sub></em> is the cost of classifying an observation of class <em>i</em> into class <em>j</em></li>
<li>The prior <em>p</em> is a vector containing the prior probabilities for all classes.</li>
<li>The weight <em>w</em> allows you to specify an arbitrary weight for that observation.</li>
</ul>
<p>From these values, MATLAB calculates an adjusted weight <em>w<sub>j</sub><sup></em></sup>*, which is then used by <code>fitcsvm</code> as shown above to scale the Box Constraint individually for each observation. The result of this calculation is saved in the <code>BoxConstraints</code> property of the <code>ClassificationSVM</code> class. Hence, the <code>BoxConstraints</code> contains a weighted version for every observation <em>j</em> of the standard box constraint that you specified.</p>
<p><strong>tl;dr</strong>: <code>fitcsvm</code> adjusts the scalar box constraint <code>C</code> to account for prior probabilities, misclassification costs and/or observation weights that you specify. If you use default values for these, the <code>BoxConstraints</code> will just be <code>C</code> for all values - which is exactly what you see.</p>
","2","Answer"
"79486390","79486105","<p>Possible sources of non-determinism (including the ones you have tried, but to double-check) might be (roughly in the order of what I would check):</p>
<ol>
<li><strong>Multiple seeds</strong> (such as <code>python.random</code>, <code>numpy.random</code>, <code>pytorch.random</code>);<a href=""https://pytorch-lightning.readthedocs.io/en/1.7.7/api/pytorch_lightning.utilities.seed.html#pytorch_lightning.utilities.seed.seed_everything"" rel=""nofollow noreferrer""><code>pytorch_lightning.utilities.seed.seed_everything</code></a> is an example of more comprehensive seeding</li>
<li><strong>Batch size</strong> - given different GPUs and their maximum batch size, each weights update will be a little bit different, which, if the dataset's examples vary largely, will push the weights in different directions. Same with multi-GPU training and averaging (taking the mean) across them. See <a href=""https://stackoverflow.com/a/26094278/10886420"">mean of all means is not the same as mean of the whole dataset</a> as an example. <strong>This one affects the results for sure</strong></li>
<li>Non-deterministic per-gpu operations - see <a href=""https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms"" rel=""nofollow noreferrer"">PyTorch reproducibility</a> for more information. <strong>NOTE:</strong> Not all algorithms have fully deterministic variants as far as I remember, especially in bleeding-edge library like <code>huggingface</code></li>
<li><strong>Mixed precision training</strong> - might increase differences as the <code>float</code> precision is not enough to handle the updates. Support for different modes also differs per graphic card</li>
<li><strong>Different test/validation sets</strong> - if you take a random subset it might be different given some seeding problems (with, say, <code>python</code> seeding)</li>
<li><strong>Order of samples</strong> - Different sample order due to indeterminism or batch size</li>
<li><strong>Random weights initialization</strong> - might make a difference, but unlikely as you start from the same checkpoint</li>
</ol>
","1","Answer"
"79486806","79485691","<p>There is nothing wrong with the code or python package.</p>
<p>Based on the comment below this post, indeed, XGBoost’s default regularization is more aggressive. In a small, non-monotonic dataset like yours, this regularization can prevent the model from making any splits, resulting in a constant prediction (all 0s). Tuning or reducing these regularization parameters gives XGBoost more flexibility to overfit—allowing it to capture the alternating pattern.</p>
<p>I just tried <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"" rel=""nofollow noreferrer"">GradientBoostingClassifier</a> and the classifier works as intended.</p>
","1","Answer"
"79493002","79492869","<p>You have set</p>
<pre><code>wait=True
</code></pre>
<p>which means the code will wait until the training job is complete. You should check your account to see the history of the training jobs.</p>
","0","Answer"
"79493183","79492869","<p>The <code>sagemaker</code> module only orchestrates sending <code>boto3</code> requests to create and run the Training job when you actually call <code>fit()</code>, not before.</p>
<p>From the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase.fit"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>The API calls the Amazon SageMaker CreateTrainingJob API to start model training. The API uses configuration you provided to create the estimator and the specified input training data to send the CreatingTrainingJob request to Amazon SageMaker.</p>
</blockquote>
<p>From looking at your arguments, it's hard to tell why your fit is hanging. I recommend trying to debug this by manually invoking the CreateTrainingJob yourself in boto3 (via <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_training_job.html"" rel=""nofollow noreferrer"">create_training_job</a>).</p>
","0","Answer"
"79501820","79500227","<pre><code>deactivate  # Exit venv  
venv\Scripts\activate  # Reactivate (Windows)  
source venv/bin/activate  # Reactivate (Mac/Linux)  
streamlit run your_script.py
</code></pre>
<p>Run this in terminal and run VSCode as administrator</p>
","0","Answer"
"79503831","79498849","<p>As mentioned by KacZdr, setting the <code>predict.type</code> argument to <code>&quot;prob&quot;</code> works fine.</p>
<pre><code># build learners
xgb_class_learner &lt;- makeLearner(
  &quot;classif.xgboost&quot;,
  predict.type = &quot;prob&quot;
)
</code></pre>
<p>However, since Lars kotthoff mentioned that the <code>mlr</code> package is deprecated, here is an alternative code using <code>mlr3</code> . There seems to be an issue with ggplot in the <code>$plot()</code> function for <code>FeatureEffects</code> objects, when i try using <code>effect$plot()</code> i get:</p>
<blockquote>
<p><strong>Error in `geom_rug()`:</strong></p>
<p>! problem while computing position.</p>
<p>i Error occured in the 2nd layer.</p>
<p><strong>Caused by error in `if (params$width &gt; 0) ...`:</strong></p>
<p>! Missing value, where TRUE/FALSE is required</p>
</blockquote>
<p>So i just generate the data and plot it myself.</p>
<pre><code># library
library(tidyverse)
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
library(iml)

peng &lt;- palmerpenguins::penguins

# buil task
tsk_peng &lt;- peng %&gt;% select(-sex, -year) %&gt;% 
  as_task_classif(target = &quot;species&quot;)

# data partition
splits &lt;- partition(tsk_peng)

# build learner
lrn_classif &lt;- as_learner(po(&quot;encode&quot;, method = &quot;one-hot&quot;) %&gt;&gt;% lrn(&quot;classif.xgboost&quot;))

# train model
lrn_classif$train(tsk_peng, row_ids = splits$train)

# partail dependence
predictor &lt;- Predictor$new(
  lrn_classif, 
  data = tsk_peng$data(rows = splits$test, cols = tsk_peng$feature_names),
  y = tsk_peng$data(rows = splits$test, cols = tsk_peng$target_names)
  )

effect &lt;- FeatureEffects$new(predictor, method = &quot;pdp&quot;)

# plot
## continuous
effect$results %&gt;% 
  keep(names(.) %in% effect$features[1:4]) %&gt;% 
  bind_rows() %&gt;% 
  ggplot(aes(x = .borders, y = .value, col = .class))+
  geom_line()+
  facet_grid(~.feature, scale = &quot;free&quot;)

## factor
effect$results$island %&gt;% 
  ggplot(aes(x = .borders, y = .value, fill = .class))+
  geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;)
</code></pre>
","-1","Answer"
"79504029","79500324","<p>Yes, your approach is correct. Using the environment parameter in the Estimator and accessing variables with <code>os.environ.get()</code> in your script is the standard way to pass environment variables in SageMaker. As @furas pointed out in their <a href=""https://stackoverflow.com/questions/79500324/how-can-i-pass-environment-variables-to-a-custom-training-script-in-amazon-sagem#comment140205530_79500324"">comment</a>, <code>os.environ.get()</code> is the common approach in Python.</p>
<p>That said, for handling secrets like API keys, it's better to avoid hardcoding them in your code or environment. A more secure approach is to store them in AWS Secrets Manager and fetch them inside your training script at runtime. You can pass the secret's name as an environment variable and retrieve the value securely using boto3:</p>
<pre><code>import boto3  
import os  

secret_name = os.environ.get('API_KEY_SECRET_NAME')  
region = os.environ.get('AWS_REGION', 'us-west-2')  

client = boto3.client('secretsmanager', region_name=region)  
secret_value = client.get_secret_value(SecretId=secret_name)  
api_key = secret_value['SecretString']

print(&quot;API Key:&quot;, api_key)
</code></pre>
<p>This keeps the actual secret out of your environment config and allows for better access control via IAM.</p>
","2","Answer"
"79504165","79502752","<p>You can check out information on the specific model <a href=""https://llm.extractum.io/model/microsoft%2FOrca-2-13b,61363fW16x75Cre5MxWM0A"" rel=""nofollow noreferrer"">here</a>. But you can see it requires <code>52.1 GB</code> of VRAM (GPU memory).</p>
<p>Based on <a href=""https://instances.vantage.sh/aws/ec2/g5.8xlarge"" rel=""nofollow noreferrer"">this table</a> we see that you have <code>24GB</code> of GPU memory. So it won't be able to fit. If you aren't able to get more GPU memory, you can look into quantized models.</p>
<p>You can check out the models on <a href=""https://huggingface.co/TheBloke/Orca-2-13B-GGUF"" rel=""nofollow noreferrer"">huggingface</a> that have quantized versions, the GPU memory required, and the best use case.</p>
","2","Answer"
"79505737","79504547","<p>This is due to the <code>relu</code> activations in the encoder output. To understand why this is an issue -- the KL-divergence loss pulls the variance output to 1. At the same time, the reconstruction loss generally pulls it to 0, since less variance makes the codes more reliable, leading to better reconstructions.</p>
<p>Thus, variances can be expected to be between 0 and 1. That corresponds to log variances of 0 (equal to variance of 1) or below (variance smaller than 1 -&gt; negative log variance). But with a <code>relu</code> output, you cannot get values less than 0 for the log variance, and a value of exactly 0 is the best the model can do.</p>
<p>Similarly, <code>relu</code> for the means also doesn't really make sense, as there is no reason why the mean values shouldn't be negative.</p>
","0","Answer"
"79512436","79495542","<p>You can refer to the following URL</p>
<p><a href=""https://github.com/tensorflow/tensorflow/issues/86953#event-16275455512"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/86953#event-16275455512</a></p>
<p>This seems to be a problem with keras</p>
<p>I used this method to solve it before</p>
<p>You can try it</p>
<p>But in recent days, colab TPU seems to have problems and I can't connect to TPU</p>
","0","Answer"
"79512903","79512816","<p>The issue is that <code>jax.tree.map</code> gets trees with different depths: for the learning rates the leafs are the float learning rates, but for the updates, the leaves are the <em>values</em> of the <code>ParameterValue</code> object(s). To resolve this issue, I define</p>
<pre><code>def pruned_tree_map(fn, parameter_value_tree, *trees):
    structure = jax.tree.structure(parameter_value_tree)
    return jax.tree.map(
        fn,
        parameter_value_tree,
        *[jax.tree.unflatten(structure, jax.tree.leaves(tree)) for tree in trees],
    )
</code></pre>
<p>Now I can use this in place of <code>jax.tree.map</code> in <code>update_fn</code>:</p>
<pre><code>def update_fn(updates, state, params):
    del params
    updates = pruned_tree_map(
        lambda u, lr: lr * u,
        updates,
        state[&quot;learning_rates&quot;],
    )
    return updates, state
</code></pre>
","0","Answer"
"79513322","79490841","<p>it seems that in the new version (1.7.5) they have added support for</p>
<ul>
<li>Explainability: Support native and snowml sklearn pipeline</li>
</ul>
<p>issue I raises:<br />
<a href=""https://github.com/snowflakedb/snowflake-ml-python/issues/145"" rel=""nofollow noreferrer"">https://github.com/snowflakedb/snowflake-ml-python/issues/145</a></p>
","1","Answer"
"79516017","79515278","<p>Yolov8 has been a little broken since they started moving everything from keras_cv to keras_hub. I think they have been working on it. I have not been able to get sensible results in my own recent work with YOLOv8 keras.  I am also having a problem with surplus boxes showing up when I validate, low map, and I think there may be some weird  behavior with the data augmentation pipeline.</p>
<p>I think it would be awesome if the team published an updated example soon that works seamlessy with the new hub</p>
","0","Answer"
"79516651","79494728","<p>It looks like the issue might be with the noise scheduler or latent processing. Make sure your scheduler settings match the default in diffusers, and check if the UNet’s predicted noise aligns with the official pipeline. If the results are still off, compare the shape and scale of your latents.</p>
","-1","Answer"
"79521388","79521352","<p>If you intend to use a dictionary for sample weights, make sure your model’s output layers are defined with the matching names. For example:</p>
<pre><code>binary_output = Dense(1, activation='sigmoid', name='output_binary')(previous_layer)
continuous_output = Dense(1, activation='linear', name='output_continuous')(another_layer)
model = Model(inputs=[ts_input, item_input], outputs=[binary_output, continuous_output])
</code></pre>
<p>With these names, the keys in your sample weight dictionary will correctly map to the outputs.</p>
<p>However, if your model outputs are unnamed or you prefer using indices, you can pass the sample weights as a list that matches the order of the outputs. This way, the first element of the list will be applied to the first output and the second to the second output.:</p>
<pre><code>sample_weight = [sample_weights_conf, sample_weights_pct]
</code></pre>
","0","Answer"
"79521598","79494100","<p>In your example specifically, you set <code>attn_implementation=&quot;eager&quot;</code> in 1st AutoModelForCausalLM (the config you save), but not to the 2nd AutoModelForCausalLM (from which you actually load the weights).</p>
<pre><code>model_auto = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;, #&lt;---- You set this here
    torch_dtype=dtype
).cuda()
config = model_auto.config

# Later...
model_auto_temp = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=dtype
)

model_llama.load_state_dict(model_auto_temp.state_dict())
</code></pre>
<p>One of those “Auto” calls picks up a different default than the other. That can lead to differences in logits.</p>
<p><strong>Pass identical arguments to every load</strong></p>
<p>If you rely on AutoModelForCausalLM for everything:</p>
<pre><code>model_auto_1 = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,
    torch_dtype=dtype
).cuda()

config = model_auto_1.config

# Ensure 2nd time also uses the same attn_implementation etc.
model_auto_2 = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,
    torch_dtype=dtype
)

model_llama = LlamaForCausalLM(config).cuda()
model_llama.eval()

model_llama.load_state_dict(model_auto_2.state_dict())
</code></pre>
<p>Now both auto calls have same arguments.</p>
<p><strong>Skip the intermed. model</strong></p>
<p>Let LlamaForCausalLM do the work, LlamaForCausalLM supports from_pretrained:</p>
<pre><code>#  A) &quot;Auto&quot; way
model_auto = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,
    torch_dtype=dtype
).cuda()

#  B) Direct Llama way
model_llama = LlamaForCausalLM.from_pretrained(
    model_name,
    attn_implementation=&quot;eager&quot;,
    torch_dtype=dtype
).cuda()
</code></pre>
<p>Now both read the same config plus the same checkpoint weights without you having to do any manual .state_dict() copy.</p>
<p>If you compare their outputs:</p>
<pre><code>out_auto = model_auto(**inputs).logits
out_llama = model_llama(**inputs).logits
diff = (out_auto - out_llama).abs().max()
print(diff.item())
</code></pre>
<p>…you should see almost no differences</p>
","0","Answer"
"79521791","79521737","<p>Without a code example, it's hard to see exactly what you've attempted, which parameters you've given, etc.</p>
<p>However, a quick google search gave me <a href=""https://learn.microsoft.com/en-gb/azure/machine-learning/tutorial-pipeline-python-sdk?view=azureml-api-1"" rel=""nofollow noreferrer"">this tutorial</a>, which gave verbose=1 as one of the parameters for the model.fit() function. Perhaps this is what you are looking for?</p>
","1","Answer"
"79521933","79521737","<p>The solution was to add a log handler to the MLContext object as follows:</p>
<pre><code>public void handler(object sender, LoggingEventArgs args)
{
    this.Invoke(() =&gt;
    {
        textBoxLog.AppendText(args.Message.ToString()+&quot;\r\n&quot;);
    });
}

...

var mlContext = new MLContext();
textBoxLog.Text = &quot;&quot;;
mlContext.Log += new EventHandler&lt;Microsoft.ML.LoggingEventArgs&gt;(handler);

...

var model = pipeline.Fit(data);
</code></pre>
","1","Answer"
"79523933","79521051","<p>Technically based on the public documentation of TensorFlowJS, <a href=""https://js.tensorflow.org/api/latest/#loadGraphModel"" rel=""nofollow noreferrer"">tf.loadGraphModel</a> loads a graph model given a URL to the model definition. To further ensure the model is loaded correctly, you can add a simple check after loading:</p>
<pre><code>if (!model) { 
   return res.status(500).send({ message: 'Failed to load model' }); 
}
</code></pre>
<p>Resizing to 512x512 is likely correct, but it depends on the input size your model was trained with. Check the model's documentation or metadata to confirm the expected input dimensions. If you are unsure, you can add a console log to print out the shape of the input tensor after resizing.</p>
<pre><code>console.log(&quot;Resized image shape:&quot;, resized.shape);
</code></pre>
<p>Lastly, for handling object detection results models typically output multiple tensors, including:</p>
<ul>
<li><p>Bounding box coordinates.</p>
</li>
<li><p>Class labels (indices).</p>
</li>
<li><p>Confidence scores (probabilities).</p>
</li>
</ul>
<p>You can use <a href=""https://js.tensorflow.org/api/latest/#tf.LayersModel.summary"" rel=""nofollow noreferrer"">model.summary()</a> in a TensorFlow.js environment to see the output shapes and types.</p>
","0","Answer"
"79525082","79477586","<ul>
<li>In Azure ML, after training, save the trained model using the <strong>Export Model</strong> module. If this option is not available, you may need to use <strong>Convert Model to ONNX</strong> and then convert it to a format suitable for scikit-learn.</li>
</ul>
<p>Azure ML’s Boosted Decision Tree Regression model is typically in a proprietary format, so you'll need to extract it using Azure ML SDK</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace, Model
import pickle

# Load the workspace
ws = Workspace.from_config()

# Get the registered model
model = Model(ws, name=&quot;your_model_name&quot;)

# Download the model (if needed)
model.download(target_dir=&quot;./&quot;, exist_ok=True)

# Load the model using joblib
with open(&quot;data.ilearner&quot;, &quot;rb&quot;) as f:
    trained_model = pickle.load(f)

# Save as a Pickle file
with open(&quot;model.pkl&quot;, &quot;wb&quot;) as f:
    pickle.dump(trained_model, f)
</code></pre>
<p>If Azure ML does not allow direct export to a pickle-compatible format, convert the model to ONNX in Azure ML. Convert ONNX to a scikit-learn model using <code>onnxmltools</code> or <code>skl2onnx</code></p>
<pre class=""lang-py prettyprint-override""><code>import onnx
from skl2onnx.helpers import load_model
import pickle

# Load the ONNX model
onnx_model = onnx.load(&quot;model.onnx&quot;)

# Convert ONNX model to scikit-learn model
sklearn_model = load_model(onnx_model)

# Save as a Pickle file
with open(&quot;model.pkl&quot;, &quot;wb&quot;) as f:
    pickle.dump(sklearn_model, f)
</code></pre>
<p>Azure ML’s <code>data.ilearner</code> is <strong>not</strong> a LightGBM format. Once in scikit-learn format, you can use <code>joblib.dump()</code> to save the model.</p>
","0","Answer"
"79525809","79525759","<p>Do you mean to simply hide or solve this warning? If it is hidden,</p>
<pre><code>import warnings

with warnings.catch_warnings():
    warnings.simplefilter(&quot;ignore&quot;, UserWarning)
    model.load_state_dict(state_dict, strict=False)
</code></pre>
","1","Answer"
"79525931","79524578","<p>It may be that the process was killed by Colab due to OOM. Have you tried to reduce the batch_size?</p>
","0","Answer"
"79527134","79523261","<p>Using vanilla <strong>BERT</strong>, we can use <code>bertviz</code>'s neuron view that visualizes the intermediate representations  that are used to compute <strong>attention</strong>.</p>
<pre><code>from bertviz.transformers_neuron_view import BertModel, BertTokenizer
from bertviz.neuron_view import show

model_type = 'bert'
model_version = 'bert-base-uncased'
model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)
show(model, model_type, tokenizer, \
            &quot;def calculate_area(radius): return 3.14 * radius * radius&quot;, \
            &quot;def compute_circle_area(r):return 3.14159 * r * r&quot;, \
            layer=4, head=3)
</code></pre>
<p><a href=""https://i.sstatic.net/WR7PtBwX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WR7PtBwX.png"" alt=""enter image description here"" /></a></p>
<p>It outputs above interactive visualizer, where you can choose to the layer / <strong>attention head</strong> to view. The width of lines are proportional to <strong>attention weights</strong>.</p>
<p>You can try to make it work with <code>codebert.</code></p>
","1","Answer"
"79528950","79528937","<p>This happens because your model parameters have very different scales, and optimisers (like least_squares or Adam) can struggle without a good initial guess.</p>
<p>Tips to fix it:</p>
<ul>
<li>Normalise parameters – Scale large values like <code>p1</code> down during fitting.</li>
<li>Use bounds – Guide the optimiser with realistic parameter ranges.</li>
<li>Try global optimisers – Use <code>scipy.optimize.differential_evolution</code> if you don’t have a good initial guess.</li>
<li>Reshape the problem – Sometimes rewriting the function can make it easier to fit.</li>
</ul>
","0","Answer"
"79529307","79528937","<p>Since it most likely gets stuck in the <strong>local minimum</strong>, and the optimization algorithms are very sensitive to the <strong>initial guess</strong>, we can start with <strong>random initialization</strong> <strong>multiple times</strong> and then choose the solution corresponding to minimum cost in all the runs. It increases the chance of obtaining the global minimum, as shown below (global minimum could be found in 20 runs):</p>
<pre><code>def residuals(p, y, x):
    return (y - func(x, p))**2 # squared error 

np.random.seed(10) # for reproducible result

num_runs = 20 
best_init = None
best_cost = np.inf
best_result = None
p0 = np.zeros(3)

for i in range(num_runs):
    p0 = np.random.normal(size=len(p0)) # random init, can use any other distribution
    result = least_squares(residuals, p0, args=(y, x), method='lm', verbose=2) 
    # print(result.cost)
    if best_cost &gt; result.cost:
        best_init = p0
        best_cost = result.cost
        best_result = result

print(best_init, best_cost)
# [ 0.91745894 -0.11227247 -0.36218045] 0.00041268250199614506
result = best_result
</code></pre>
<p><a href=""https://i.sstatic.net/jyBfV2yF.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jyBfV2yF.gif"" alt=""enter image description here"" /></a></p>
<p>We can try different methods (<em><code>trf</code>, <code>dogbox</code>, <code>lm</code></em>) for least squares and also explicitly provide the jacobian too and see if it gets better results.</p>
","0","Answer"
"79530131","79512981","<p>I think you might have to force re-install since you're reverting to older versions. I tried version <code>0.1.21</code> which is the last <code>0.1.x</code> version which seems to got the <code>ragas.metrics.critique</code> and <code>ragas.metrics.base.EvaluationMode</code> modules</p>
<pre><code>pip install --force-reinstall ragas==0.1.21
</code></pre>
","0","Answer"
"79530498","79528929","<p>Check the source code, <a href=""https://github.com/scikit-learn/scikit-learn/blob/98ed9dc73/sklearn/feature_selection/_sequential.py#L28"" rel=""nofollow noreferrer"">lines 240-46</a> inside the method <code>fit()</code>:</p>
<pre><code>if self.n_features_to_select == &quot;auto&quot;:
    if self.tol is not None:
        # With auto feature selection, `n_features_to_select_` will be updated
        # to `support_.sum()` after features are selected.
        self.n_features_to_select_ = n_features - 1
    else:
        self.n_features_to_select_ = n_features // 2
</code></pre>
<p>As can be seen, even with <code>auto</code> selection mode and a given <code>tol</code>,  maximum numbers of features that can be added is bounded by <code>n_features - 1</code> for some reason (may be we can report this issue in github).</p>
<p>We can override the implementation in the following way, by defining a function <code>get_best_new_feature_score()</code> (similar to the method <code>_get_best_new_feature_score()</code> from the source code), as shown below:</p>
<pre><code>from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import cross_val_score

def get_best_new_feature_score(estimator, X, y, cv, current_mask, direction, scoring):
    candidate_feature_indices = np.flatnonzero(~current_mask)
    scores = {}
    for feature_idx in candidate_feature_indices:
        candidate_mask = current_mask.copy()
        candidate_mask[feature_idx] = True
        if direction == &quot;backward&quot;:
            candidate_mask = ~candidate_mask
        X_new = X[:, candidate_mask]
        scores[feature_idx] = cross_val_score(
            estimator,
            X_new,
            y,
            cv=cv,
            scoring=scoring
        ).mean()
    new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])
    return new_feature_idx, scores[new_feature_idx]
</code></pre>
<p>Now, let's implement the <code>auto</code> (forward) selection, using a regression dataset with 5 features, let' add all the features one-by-one, reporting the improvement in score and stopping by comparing with provided <code>tol</code>:</p>
<pre><code>from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression

X, y = make_regression(n_features=5) # data to be used
X.shape 
# (100, 5)
lm = LinearRegression() # model to be used

# now implement 'auto' feature selection (forward selection)   
cur_mask = np.zeros(X.shape[1]).astype(bool) # no feature selected initially
cv, direction, scoring = 8, 'forward', 'neg_root_mean_squared_error'
tol = 1 # if score improvement &gt; tol, feature will be added in forward selection
old_score = -np.inf
ids, scores = [], []
for i in range(X.shape[1]):
    idx, new_score = get_best_new_feature_score(lm, X, y, current_mask=cur_mask, cv=cv, direction=direction, scoring=scoring)
    print(new_score - old_score, tol, score - old_score &gt; tol)
    if (new_score - old_score) &gt; tol:
        cur_mask[idx] = True
        ids.append(idx)
        scores.append(new_score)
        old_score = new_score
        print(f'feature {idx} added, CV score {score}, mask {cur_mask}')

# feature 3 added, CV score -90.66899644023539, mask [False False False  True False]
# feature 1 added, CV score -59.21188041830155, mask [False  True False  True False]
# feature 2 added, CV score -16.709218665372905, mask [False  True  True  True False]
# feature 4 added, CV score -3.1862116620446166, mask [False  True  True  True  True]
# feature 0 added, CV score -1.4011801838814216e-13, mask [ True  True  True  True  True]
</code></pre>
<p><a href=""https://i.sstatic.net/eGlOcXvI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eGlOcXvI.png"" alt=""enter image description here"" /></a></p>
<p>If <code>tol=10</code>, set to 10 instead, then only 4 features will be added in forward-selection. Similarly, if <code>tol=20</code>, then only 3 features will be added in forward-selection, as expected.</p>
","3","Answer"
"79534839","79508535","<p>As explained in their <a href=""https://arxiv.org/pdf/1707.05005v1"" rel=""nofollow noreferrer"">paper on graph2vec</a>:</p>
<blockquote>
<p>With the background on word and document embeddings presented in the previous section, an important intuition we extend in graph2vec is to view an entire graph as a document and the rooted subgraphs (that encompass a neighborhood of certain degree) around every node in the graph as words that compose the document. In other words, different subgraphs compose graphs in a similar way that different words compose sentences/documents when used together.</p>
<p>Similar to the document convention, the only required input is a corpus of graphs for graph2vec to learn their representations. Given a dataset of graphs, graph2vec considers the set of all rooted subgraphs (i.e., neighbourhoods) around every node (up to a certain degree) as its vocabulary. Subsequently, following the doc2vec skipgram training process, we learn the representations of each graph in the dataset.</p>
</blockquote>
<p>in order to train the embeddings a graph corpus is needed, from which rooted subgraphs are extracted and the embeddings are learnt, using the Weisfeiler-Lehman (WL) hashing algorithm) so that graphs with similar structures will have similar feature representations in the embedding space.</p>
<p>From the examples <a href=""https://github.com/benedekrozemberczki/karateclub/blob/master/examples/whole_graph_embedding/graph2vec_example.py"" rel=""nofollow noreferrer"">here</a>, they have used the following graph corpus:</p>
<pre><code>graphs = [nx.newman_watts_strogatz_graph(50, 5, 0.3) for _ in range(1000)] # 1000 graphs in the corpus
model = kc.graph_embedding.Graph2Vec()
model.fit(graphs)
embeddings = model.get_embedding()
print(embeddings.shape) 
# (1000, 128) # for each graph learnt a 128-dim embedding vector
# now infer
model.infer([G])
# array([[ 0.00371773,  0.00212566, -0.00158476,  0.00075081,  0.00121916,
#        -0.00078127, -0.00187484,  0.00024809,  0.00116782, -0.00179677,
#        ...     ...    ...
</code></pre>
<p>Once the training is over, we can ask the model to compute the embedding for a new graph <code>G</code>, using the method <code>infer()</code> , as shown above.</p>
<p>Providing repeated samples of the same graph <code>G</code> as training corpus also works, e.g., the following code will also learn embeddings for the graphs, but I think the quality of the embedding will be poor and it will capture the similarity in the rooted-subgraph structure in the embedding vector space more, as we provide richer and more diverse graph corpus as training dataset.</p>
<pre><code>graphs = [G for _ in range(100)] # 100 graphs in the corpus
model = kc.graph_embedding.Graph2Vec()
model.fit(graphs)
embeddings = model.get_embedding()
print(embeddings.shape) 
# (100, 128)
model.infer([G])
# array([[ 0.0037295 ,  0.00211123, -0.00169718,  0.00071603,  0.00125103,
#        -0.00072378, -0.00181226,  0.00028596,  0.00113032, -0.00179663,
#        ...         ...           ...  
</code></pre>
","1","Answer"
"79537580","79531266","<p>Scipy sparse arrays have one common type, hence the behavior when you stack. Since pandas dataframes can have separate types per column, that's one solution:</p>
<pre class=""lang-py prettyprint-override""><code>df_tfidf = pd.DataFrame.sparse.from_spmatrix(
    X_tfidf,
    columns=vectorizer.get_feature_names_out(),
)

X_combined = pd.concat([df_tfidf, df_cat_encoded], axis=1)
</code></pre>
","1","Answer"
"79577574","79436775","<p>First of all, ultralytics yolov8 has in-built augmentation (with albumentations backend). You should just set parameter augment=True  in model.train() method.</p>
<p>I think that these &quot;super-noisy&quot; images appear when you mix too much augmentations. For example if you  apply HSV, blur and GaussianNoise on a single image, the image will become kinda messy.</p>
<p>If you want to reduce amount of noisy image you can use A.OneOf([A.Blur(), A.GaussianNoise()]) to select one of methods that makes images dirty. Or try to tune probability for each method.</p>
<p>Also, I've noticed, that you use Crops and Flips, but you should keep in mind that after those transforms bounding boxes will change :) if your goal is classification, then it's fine</p>
","1","Answer"
"79583922","79515542","<p>I suspect that the issue is due to the fact that the <code>shap_values</code> array has slight differences in its output format depending on the model used (e.g., XGBoost vs. RandomForestClassifier).</p>
<p>You can successfully generate SHAP analysis plots simply by adjusting the dimensions of the <code>shap_values</code> array.</p>
<p>Since I don't have your data, I generated a sample dataset as an example for your reference:</p>
<pre><code>import numpy as np
import pandas as pd
import shap
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Generate sample data
np.random.seed(42)
features = pd.DataFrame({
    &quot;feature_1&quot;: np.random.randint(18, 70, size=100),
    &quot;feature_2&quot;: np.random.randint(30000, 100000, size=100),
    &quot;feature_3&quot;: np.random.randint(1, 4, size=100), 
    &quot;feature_4&quot;: np.random.randint(300, 850, size=100),
    &quot;feature_5&quot;: np.random.randint(1000, 50000, size=100)
})
target = np.random.randint(0, 2, size=100)
features_names = features.columns.tolist()

# The following code is just like your example.
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Adjust the dimensions of the shap_values object.
shap.summary_plot(shap_values[:,:,0], X_test, feature_names=features_names)
shap.summary_plot(shap_values[:,:,0], X_test, feature_names=features_names, plot_type=&quot;bar&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/mPnSyHDs.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.sstatic.net/nubp6lRP.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>With the above, you can successfully run the SHAP analysis by simply adjusting <code>shap_values</code> to <code>shap_values[:,:,0]</code>.<br />
As for what the third dimension of <code>shap_values</code> represents when using <code>RandomForestClassifier</code>, you can explore it further on your own.</p>
","0","Answer"
"79600867","79495211","<p>I don't think Pytorch 2.6.0 is out officially and that might cause problems. For your system configuration you can try the following install instead:</p>
<pre><code>pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cu118
</code></pre>
","0","Answer"
"79604065","79468013","<p>You can run DeepSeek models locally and easily with <a href=""https://ollama.com/"" rel=""nofollow noreferrer"">ollama</a>.</p>
<p>Available models are:</p>
<ul>
<li>deepseek-r1</li>
<li>deepseek-v2</li>
<li>deepseek-v2.5</li>
<li>deepseek-v3</li>
<li>deepseek-llm</li>
<li>deepseek-coder-v2</li>
<li>deepseek-coder-v2</li>
</ul>
<p>Pull the model you are interested in:</p>
<pre><code>ollama pull deepseek-r1
</code></pre>
<p>and run it:</p>
<pre><code>ollama run deepseek-r1
</code></pre>
<p>You can also define a <a href=""https://ollama.readthedocs.io/en/modelfile"" rel=""nofollow noreferrer"">Model file</a> (the equivalent of Dockerfile for models) to set parameters, prompt templates, system instructions, ...</p>
","1","Answer"
"79653512","79430117","<p>This is a bug in the Docling <a href=""https://github.com/docling-project/docling"" rel=""nofollow noreferrer"">https://github.com/docling-project/docling</a> code as of 4-Jun-2025.</p>
<p>I also face the same issue.</p>
<p>The pull request #285, <a href=""https://github.com/docling-project/docling-core/pull/285"" rel=""nofollow noreferrer"">https://github.com/docling-project/docling-core/pull/285</a>, will add a feature which is expected to solve the problem of cells in tables containing images.</p>
","0","Answer"
"79536000","79535519","<p>As explained in the <strong>back propagation</strong> section in this <a href=""https://sandipanweb.wordpress.com/2018/05/31/8626/"" rel=""nofollow noreferrer"">blog post</a>, we need to use chain rule to implement backpropagation. If the last layer has softmax activation, computing the gradients are a little tricky, using the derivation from <a href=""https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function"">here</a>, and the schematic diagram of a single-layer neural net model shown in the next figure (note that if it is a k-class softmax, the output layer will have k neurons), we have</p>
<p><a href=""https://i.sstatic.net/GP4hCOSQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GP4hCOSQ.png"" alt=""enter image description here"" /></a></p>
<p>Let's now implement training a simple neural net model with <strong>back-propagation</strong> from scratch and test it using simple <strong>gradient-descent</strong> optimization, on the <code>digits</code> dataset from <code>sklearn.datasets</code>, we can see how the loss is decreasing with  epochs and the classifier achieves a decent accuracy:</p>
<pre><code>from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

X, y = load_digits(n_class=3, return_X_y=True) # use only digits 0, 1, 2
y = pd.get_dummies(y).values # one-hot-encoding

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)

np.random.seed(1) # for the sake of reproducibility

lr = 1e-2 # learning rate for GD
X, y = X_train, y_train
W1 = np.random.randn(64, 3)
b1 = np.zeros((1, 3))
# train the model for 150 epochs
losses = []
for epoch in range(150):
    # forward-prop
    Z1 = np.dot(X, W1) + b1
    Y_pred = A1 = softmax(Z1)
    loss = cross_entropy_loss(Y_pred, y)
    # back-prop
    dL_dZ1 = A1 - y        # ∂L/∂Z1
    dZ1_dW1 =  X.T         # ∂Z1/∂W1
    dZ1_db1 =  np.ones(len(X))[None,:] # ∂Z1/∂b1
    dL_dW1 = dZ1_dW1 @ dL_dZ1 / len(X) # chain rule: ∂L/∂W1 = ∂L/∂Z1.∂Z1/∂W1
    dL_db1 =  dZ1_db1 @ dL_dZ1 / len(X) # chain rule: ∂L/∂b1 = ∂L/∂Z1.∂Z1/∂b1
    # gradient descent
    W1 -= lr*dL_dW1 # GD update of weights
    b1 -= lr*dL_db1 # GD update of bias
    losses.append(loss)

plt.plot(losses)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/FD0FZ3Vo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FD0FZ3Vo.png"" alt=""enter image description here"" /></a></p>
<pre><code># now test the model
y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(softmax(np.dot(X_test, W1) + b1), axis=1)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_true, y_pred))
# [[54  0  1]
# [ 0 50  2]
# [ 3  1 67]]
</code></pre>
<p>Now, show the predicted labels of a few test images with the model:</p>
<pre><code>indices = np.random.choice(len(y_test), 64)
plt.gray()
for i in range(64):
    plt.subplot(8,8,i+1), plt.imshow(np.reshape(X_test[indices[i]], (8,8))), plt.axis('off'), plt.title(f'{y_pred[indices[i]]}')
plt.suptitle('predicted labels', size=15);
</code></pre>
<p><a href=""https://i.sstatic.net/6HW0hPJB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6HW0hPJB.png"" alt=""enter image description here"" /></a></p>
<p>For gradient checking we can <a href=""https://stackoverflow.com/questions/26192592/gradient-checking-in-backpropagation"">compute numerical gradients</a> and compare with the backprop gradients, to check if they are correct:</p>
<pre><code>def gradient_check(X, y, W1, b1, eps=1e-9): # compute numerical gradients
    dL_dW1 = np.zeros_like(W1)
    dL_db1 = np.zeros_like(b1)
    
    for i in range(W1.shape[0]):
        for j in range(W1.shape[1]):
            W1_minus = W1.copy() # Copy all the weights
            W1_minus[i,j] = W1_minus[i,j] - eps # Change only this parameter
            W1_plus = W1.copy(); # Copy all the weights
            W1_plus[i,j] = W1_plus[i,j] + eps; # Change only this parameter
            cost_minus = cross_entropy_loss(softmax(np.dot(X, W1_minus) + b1), y)
            cost_plus = cross_entropy_loss(softmax(np.dot(X, W1_plus) + b1), y)
            dL_dW1[i,j] = (cost_plus - cost_minus)/(2*eps)

    for i in range(len(b1)):
        b1_minus = b1.copy() # Copy all the weights
        b1_minus[i] = b1_minus[i] - eps # Change only this parameter
        b1_plus = b1.copy(); # Copy all the weights
        b1_plus[i] = b1_plus[i] + eps; # Change only this parameter
        cost_minus = cross_entropy_loss(softmax(np.dot(X, W1) + b1_minus), y)
        cost_plus = cross_entropy_loss(softmax(np.dot(X, W1) + b1_plus), y)
        dL_db1[i] = (cost_plus - cost_minus)/(2*eps)

    return (dL_dW1, dL_db1) 
</code></pre>
","1","Answer"
"79539796","79536891","<p>Both methods are equivalent - change in print-out is just an artifact of how <code>torchinfo</code> crawls the model.</p>
<p><code>torchinfo</code> tracks the model's forward pass, looking at every module involved. If the same module appears more than once, it is labeled <code>recursive</code>. For <code>nn.ModuleList</code> objects, using an item in the same <code>ModuleList</code> at different points of the <code>forward</code> gets flagged as recursive simply because the <code>ModuleList</code> container is showing up more than once in different places. Here's a simple example:</p>
<p>Example 1:</p>
<pre><code>class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.ModuleList([nn.Linear(8, 8) for i in range(2)])
        self.l2 = nn.Linear(8,8)
        
    def forward(self, x):
        x = self.l1[0](x)
        x = self.l1[1](x)
        x = self.l2(x)
        return x

m = MyModel()
summary(m, (1, 8), depth=5)

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MyModel                                  [1, 8]                    --
├─ModuleList: 1-1                        --                        --
│    └─Linear: 2-1                       [1, 8]                    72
│    └─Linear: 2-2                       [1, 8]                    72
├─Linear: 1-2                            [1, 8]                    72
==========================================================================================
Total params: 216
Trainable params: 216
Non-trainable params: 0
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
==========================================================================================
</code></pre>
<p>Example 2:</p>
<pre><code>class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.ModuleList([nn.Linear(8, 8) for i in range(2)])
        self.l2 = nn.Linear(8,8)
        
    def forward(self, x):
        x = self.l1[0](x)
        x = self.l2(x)
        x = self.l1[1](x)
        return x

m = MyModel()
summary(m, (1, 8), depth=5)

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MyModel                                  [1, 8]                    --
├─ModuleList: 1-3                        --                        (recursive)
│    └─Linear: 2-1                       [1, 8]                    72
├─Linear: 1-2                            [1, 8]                    72
├─ModuleList: 1-3                        --                        (recursive)
│    └─Linear: 2-2                       [1, 8]                    72
==========================================================================================
Total params: 216
Trainable params: 216
Non-trainable params: 0
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
==========================================================================================
</code></pre>
<p>In the first example, we use all layers in the <code>ModuleList</code> in order, and get no <code>recursive</code> flag. In the second, we use the layers in the <code>ModuleList</code> at different times, and get the <code>recursive</code> flag on the <code>ModuleList</code> object itself. This is just an artifact of how <code>torchinfo</code> crawls the model.</p>
<p>As a purely style-based note, there's nothing wrong with zipping modulelists, but if you know each <code>_ConvBlock</code> will be paired 1-1 with a <code>_DownsampleBlock</code>, you might consider putting them into a combined module</p>
<pre><code>class CombinedBlock(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.conv_block = _ConvBlock(...)
        self.down_block = _DownsampleBlock(...)
    def forward(self, x):
        x = self.conv_block(x)
        skip = x
        x = self.down_block(x)
        return x, skip
</code></pre>
","1","Answer"
"79548870","79546578","<p>Alternative solution adapted from PyTorch <a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network"" rel=""nofollow noreferrer"">docs</a>.:</p>
<pre><code>for i, data in enumerate(train_loader, 0):
    ex_images, ex_dmaps, ex_n_people = data
</code></pre>
","0","Answer"
"79552530","79552491","<p>Why do you have <code>output_category_mask=False</code> and are expecting 2 outputs ? You are specifically asking the model to only return 1 output.</p>
<p>Please check the <a href=""https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/ImageSegmenterOptions#:%7E:text=Image%20segmenter%20task%20has%20three,data%2C%20such%20as%20from%20camera."" rel=""nofollow noreferrer"">documentation</a> and <a href=""https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/python/vision/image_segmenter.py"" rel=""nofollow noreferrer"">source code</a>.</p>
<p><code>output_confidence_masks:</code> Whether to output confidence.</p>
<p><code>output_category_mask:</code> Whether to output category mask.</p>
","0","Answer"
"79553533","79552254","<p>I have solved the issue and include the answer here to help others with the same issue.</p>
<p>I have created a dictionary and used tf.constant() to create a Tensor for an example value, and specified the data type and I now get the expected output.</p>
<pre><code>inputs = {
    &quot;user_id&quot;: tf.constant([&quot;138&quot;], dtype=tf.string),
    &quot;timestamp&quot;: tf.constant([879024327], dtype=tf.int64),
    &quot;raw_user_age&quot;: tf.constant([46.0], dtype=tf.float32)
}


model = UserFeaturesModel()

output = model(inputs)
print(output)
</code></pre>
","0","Answer"
"79555749","79547006","<p>Didn't tried your code obviously with no access to the dataset, but here are some advice and a proposition of code update:</p>
<ul>
<li><p>you should first read only the columns you need, even if with dask-expr it might be optimized by Dask.</p>
</li>
<li><p>I'm not sure why you want excalty 3 chunks, but with no reason this is a bad idea, that will cause very big sized chunks probably. Maybe why your Worker are failing (stack trace is not detailed)</p>
</li>
<li><p>You don't need to compute to now the len, actually you usually never want to compute.</p>
</li>
<li><p>y_train = da.from_array(y_train.compute(), chunks=(train_size // 3,)) --&gt; This is a really bad operation, loading all y_train data into memory of the Client, and then resending it to the Workers.</p>
</li>
</ul>
<p>So you might want to test a code that looks like this:</p>
<pre><code># Import necessary libraries
import numpy as np
import dask.dataframe as dd
import dask.array as da
import xgboost as xgb
from dask.distributed import Client
from dask.diagnostics import ProgressBar 
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import matplotlib.pyplot as plt
from tqdm import tqdm

# Include columns 
inlude_cols = [&quot;...&quot;, &quot;...&quot;]

# Load the data using Dask (efficient for large Parquet files)
df_processed = dd.read_parquet(&quot;pre_ml_some_features.parquet&quot;, columns=inlude_cols)


# Filter the data based on the read_date for training and testing
df_train = df_processed[df_processed[&quot;year&quot;] &lt; 2025]  # Keep rows before 2025
df_test = df_processed[df_processed[&quot;year&quot;] == 2025]  # Keep rows from 2025 onwards


# Prepare training features (X) and target variable (y)
X_train = df_train
y_train = df_train[&quot;kwh&quot;]

#X_test was not defined in your snippet
X_test = df_test 
y_test = df_test [&quot;kwh&quot;]

# If you need arrays, you might want to use df.to_dask_arrays()

# Start Dask client for parallel processing
client = Client()

# Print the Dask dashboard URL
print(f&quot;Dask dashboard is available at: {client.dashboard_link}&quot;)

# Use DaskDMatrix from xgboost.dask
dask_train_data = xgb.dask.DaskDMatrix(client, X_train, y_train)

# Set up parameters for XGBoost
params = {
    'objective': 'reg:squarederror',  # Regression task
    'eval_metric': 'rmse',
    'tree_method': 'hist',  # Use histogram-based method for faster training
    'verbosity': 1,  # Enables basic logging
}

# Initialize Dask-XGBoost model
dask_gbr = xgb.dask.DaskXGBRegressor(**params)

# Train the model using Dask (this will automatically parallelize)
with ProgressBar():  # Shows progress during training
    dask_gbr.fit(dask_train_data)
</code></pre>
","0","Answer"
"79556864","79553429","<p>Even with your GPU, larger image sizes mean:</p>
<ol>
<li><p>Larger tensors;</p>
</li>
<li><p>More memory-hungry backpropagation;</p>
</li>
<li><p>Heavier activation maps.</p>
</li>
</ol>
<p><strong>TLDR:</strong> To solve this issue you should reduce <code>batch</code> size and use smaller <code>imgsz</code>.</p>
<p>Training with <code>imgsz=1440</code> and <code>batch=32</code> is not possible with 12 GB VRAM. While <code>imgsz=1440</code> matches your image size (2560x1440), you might not need full resolution. Smaller image sizes reduce GPU load while still improving model precision over 640px.</p>
<p>In your case I suggest using this parameters:</p>
<pre><code>yolo task=detect mode=train device=0 epochs=2000 batch=8 imgsz=1024 data=&quot;C:\Project\Yolo\data_custom.yaml&quot; model=&quot;C:\Project\Yolo\yolov11m.pt&quot;
</code></pre>
<p>You can benchmark how much memory your model uses with different settings with <code>nvidia-smi</code>. Open terminal and run <code>watch -n 0.5 nvidia-smi</code> to see real-time GPU memory usage during training. Or you can try different combinations of <code>batch</code> size to see where the top limit is.</p>
","1","Answer"
"79560937","79560879","<p>It is very hard to say what exactly is happening here without knowing the exact images, but when it comes to smaller angle rotations there is going to be some data lost. Square images will not have any data loss on multiples of 90 degree rotations, but anything off of 90 degrees will necessarily lose information around two opposing corners. I do not know how you are filling in this data, or if you are simply resizing the existing images till they are square again by some sort of zooming or filling function.</p>
<p>Having trouble with small angles might either mean that the whole image is being memorized and when a part of it is not present that it makes angle calculations harder OR however the missing data is being filled in might be throwing off any ability to recognize angle changes. If I am understanding the torch code correctly there are just a bunch of conv layers and some linear activations. This should mean that every part of every training image is getting the same amount of attention, which may be the thing that is throwing off small angle detection since corners are getting over-weighted if that same corner data is not present in the test data set.</p>
<p>Potential Solution:<br />
If you consider for a moment a square image being freely rotated from 0 to 360 degrees, you will see that a perfect circle of that square is always in frame. The radius of that circle is a half length of the square. The area that is outside of that circle will still be in frame somewhere between 1% of the time (furthest corners that sit right on the edge) to 99% of the time (spot closest to the circle, furthest from the edges). In this case you can see that the circle itself will always be in frame and certain areas outside the circle will be in frame to varying degrees. To improve your approach I would suggest to retrain the model with 100% attention on that inner circle and have progressively less attention towards the edges, with a weighting based on how much a certain pixel location is in-frame for a 0 - 360 degree rotation. When the corners are out of frame, they are not available for the NN to evaluate, thus, giving them progressively lower attention will help train the model on the area that is always available, the inner circle. If the NN can focus all of its learning on that inner circle I would expect it to become better with evaluating smaller angle changes!</p>
","1","Answer"
"79562979","79561884","<p>As far as I'm aware, there is no built-in function to do what you want, but a function like this should work, I believe:</p>
<pre><code>import torch
import torch.nn.utils.prune as prune
import torch.nn as nn
import numpy as np

def global_structured_prune(model, layers, amount=0.8, norm=2):
    # Step 1: Collect all neuron norms
    all_norms = []
    layer_neuron_norms = {}

    for layer in layers:
        weight = getattr(model, layer).weight.detach().cpu()
        if isinstance(getattr(model, layer), nn.Linear):
            norms = torch.norm(weight, p=norm, dim=1)  # Norm across incoming connections
            all_norms.append(norms)
            layer_neuron_norms[layer] = norms

    all_norms_flat = torch.cat(all_norms)
    
    # Step 2: Compute global threshold
    k = int(amount * all_norms_flat.numel())
    if k == 0:
        return  # Nothing to prune
    threshold = torch.kthvalue(all_norms_flat, k).values.item()

    # Step 3: Apply structured pruning to neurons below threshold
    for layer in layers:
        norms = layer_neuron_norms[layer]
        to_prune = (norms &lt;= threshold).nonzero(as_tuple=True)[0].tolist()
        prune.ln_structured(getattr(model, layer), name=&quot;weight&quot;, amount=to_prune, n=norm, dim=0)

# Usage
model = SimpleCNN()
global_structured_prune(model, layers=['fc1', 'fc2', 'fc3'], amount=0.8, norm=2)
</code></pre>
","2","Answer"
"79567394","79567319","<p>If the <code>TIMESTAMP</code> column is required by your model, I have to convert it to a <code>factor</code> .</p>
<pre><code>dd &lt;- data.frame(TIMESTAMP, RH, Fch4, distance, Ta, Fe, stringsAsFactors = TRUE)
</code></pre>
<p>Assuming you have already built your random forest model <code>pfpfit</code> on a training set, this is how you can fill the gaps in <code>Fch4</code> column:</p>
<pre><code>dd_imputed &lt;- na.roughfix(dd[,-3]) # Impute missing values by median/mode

predicted_Fch4 &lt;- predict(pfpfit, newdata = dd_imputed)

dd$Fch4 &lt;- round(ifelse(is.na(dd$Fch4), predicted_Fch4, dd$Fch4),3)
</code></pre>
<p>Output:</p>
<pre><code>&gt; dd
             TIMESTAMP RH   Fch4 distance    Ta    Fe
1  2019-05-31 17:00:00 38  0.021  1000.00 29.52    NA
2  2019-05-31 17:30:00 40 -0.002  1000.00 29.01    NA
3  2019-05-31 18:00:00 41  0.001   180.00    NA 68.62
4  2019-05-31 18:30:00 42  0.004   125.35 28.39 39.24
5  2019-05-31 19:00:00 44  0.001  1000.00 27.87 35.04
6  2019-05-31 19:30:00 49 -0.013   180.00 26.68 27.26
7  2019-05-31 20:00:00 65  0.004  1000.00 23.28 -2.60
8  2019-05-31 20:30:00 72 -0.002     5.50    NA    NA
9  2019-05-31 21:00:00 74 -0.001   180.00 19.95  7.28
10 2019-05-31 21:30:00 77 -0.002  1000.00 19.01  2.08
</code></pre>
<p>Furthermore, if you want to fill in the columns with missing values using their imputed values, use this:</p>
<pre><code>dd$Ta &lt;- ifelse(is.na(dd$Ta), dd_imputed$Ta, dd$Ta)
dd$Fe &lt;- ifelse(is.na(dd$Fe), dd_imputed$Fe, dd$Fe)

&gt; dd
             TIMESTAMP RH   Fch4 distance     Ta    Fe
1  2019-05-31 17:00:00 38  0.021  1000.00 29.520 27.26
2  2019-05-31 17:30:00 40 -0.002  1000.00 29.010 27.26
3  2019-05-31 18:00:00 41  0.001   180.00 27.275 68.62
4  2019-05-31 18:30:00 42  0.004   125.35 28.390 39.24
5  2019-05-31 19:00:00 44  0.001  1000.00 27.870 35.04
6  2019-05-31 19:30:00 49 -0.013   180.00 26.680 27.26
7  2019-05-31 20:00:00 65  0.004  1000.00 23.280 -2.60
8  2019-05-31 20:30:00 72 -0.002     5.50 27.275 27.26
9  2019-05-31 21:00:00 74 -0.001   180.00 19.950  7.28
10 2019-05-31 21:30:00 77 -0.002  1000.00 19.010  2.08
</code></pre>
<p><code>randomForest</code> package actually does <strong>handle missing values</strong> with <code>na.action = na.roughfix</code> parameter, you can read more here: <a href=""https://stackoverflow.com/a/56936983/12382064"">https://stackoverflow.com/a/56936983/12382064</a>.</p>
","3","Answer"
"79571203","79571067","<p>It's hard to reproduce exactly what you have due to many things to fix in your code. However, just looking at your plot and what &quot;the book&quot; offers, I think your problem is the use of <strong><code>ppplot</code> instead of <code>qqplot</code> .  So change</strong></p>
<pre><code>probplot.ppplot(line='45')
</code></pre>
<p>to</p>
<pre><code>probplot.qqplot(line='45')
</code></pre>
","1","Answer"
"79571269","79571067","<p>So, if I understand correctly, you are trying to get the residual part of a linear regression (so the error) on your training dataset, and check if the distribution of that residual part follows a normal law.</p>
<p>But <code>ppplot</code> or <code>qqplot</code> need to know which law you want to compare your dataset against.</p>
<p>As you probably understand, what it does is, for each sample data <code>s</code>, plot a point whose <code>x</code> coordinate is the theoretical CDF of a distribution, and <code>y</code> coordinate is the experimental CDF (so which proportion of <code>s</code> in your dataset are lower that this <code>s</code>).</p>
<p>If you don't specify a distribution function, then, a centered and reduced normal law (μ=0, σ=1) is used by default. But your residual data have a scale way bigger than a standard deviation of 1. So, in practice, all your residuals are very very negative, or very very positive (from a standpoint of a N(0,1) law). I mean by that that either s is so negative than ℙ(X&lt;s) is practically 0, or s is so positive that ℙ(X&lt;s) is practically 1. (for X~N(0,1) that happens for any s lower than -3, or greater than +3, roughly. As you know ℙ(X&lt;2.58)=99%... And 2.58 or 3 is very small compared to your values).</p>
<p>So, in short, you need to say against which law you want to test your residual. If you don't, the default is a N(0,1) law that is obviously not similar at all to the distribution of your residuals (in other words: it works! and the pp-plot being bad indicates exactly what it is supposed to indicate: that, no, your residual does not follow at all a N(0,1) law).</p>
<p>If you have no idea of that law (well, you already said you wanted to test against a normal law), maybe you want to fit one. Either by centering/reducing your data before (to that they are indeed supposed to follow approx a N(0,1)).
Or by computing mean and stdev of your residual data and pass them as loc and scale argument to <code>ProbPlot</code>.
Or create a normal law yourself (<code>sta.norm(mean, stdev)</code>) and pass that law to <code>ProbPlot</code></p>
<p>Or, even simpler, ask <code>ProbPlot</code> to fit the parameters for you (in which case, it fits a normal law. You can't choose another kind of distribution, like a weibull or Cauchy or... ; but I understand you don't want to anyway)</p>
<p>So, long story short, if I understand correctly what you want to do</p>
<pre><code>probplot = sm.ProbPlot(mba_salary_resid, fit=True) 
</code></pre>
<p>is probably what your want
<a href=""https://i.sstatic.net/Fmdok8Vo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fmdok8Vo.png"" alt=""enter image description here"" /></a></p>
","3","Answer"
"79573973","79573345","<p>Not sure what you mean by</p>
<blockquote>
<p>I'm still in progress of making correction of these json time series data.</p>
</blockquote>
<p>does that mean that you need to &quot;clean&quot; your dataset like for example, remove outliers etc. or does that mean this is a rough structure and there could be additional elements to it.</p>
<p>In any case, I think this question relates more to how you can train the model with a JSON like structure rather than the data itself.</p>
<p>What you need to first do is think about how this data or any data can be represented as a <a href=""https://en.wikipedia.org/wiki/Tensor"" rel=""nofollow noreferrer"">tensor</a>. Then you need to figure out what dimensions would the tensor likely have, given your information you data would convert to a tensor of shape</p>
<p><code>[number_samples, number_frames, number_features]</code></p>
<p>i.e</p>
<ol>
<li><p>number_samples : well your dataset size</p>
</li>
<li><p>number_frames : total frames over which the data was collected in each example</p>
</li>
<li><p>number_features: 21 landmarks * 3 co-ordinates  * 2 hands</p>
</li>
</ol>
<p>once you have those tensors you can train your LSTM model. You will need to divide those features appropriately across your two hands for that representation to work.</p>
","0","Answer"
"79575966","79575941","<p>In the most recent version of scikit-learn (v1.4) they added support for missing values to RandomForestClassifier when the criterion is gini (default).</p>
<p>Source: <a href=""https://scikit-learn.org/dev/whats_new/v1.4.html#id7"" rel=""nofollow noreferrer"">https://scikit-learn.org/dev/whats_new/v1.4.html#id7</a></p>
","2","Answer"
"79576712","79576028","<p>Your understanding is generally correct, but I would like to add something:</p>
<p>Strictly speaking, <code>decision_function</code> is not necessarily closer to -1  be an outlier, or closer to 0 be an inliner.</p>
<p>This is related to the <code>offset_</code> calculated by the model.</p>
<pre><code>decision_function = score_samples - offset_
</code></pre>
<p>The  <code>offset_</code> is also affected by the hyper-parameter <code>contamination</code> in <code>IsolationForest</code> .</p>
<p>But generally, the smaller <code>decision_function</code> value  is closed to the outlier, and the larger <code>decision_function</code>  value is closed to the inliner.</p>
<p>Please refer to the following examples:</p>
<pre><code>from sklearn.ensemble import IsolationForest
import numpy as np
import matplotlib.pyplot as plt

## Generate a set of simulation data
rng = np.random.RandomState(42)
X = rng.randn(100, 2)

## Add some outliers
X_outliers = rng.uniform(low=-10, high=10, size=(5, 2))
X_full = np.r_[X, X_outliers]

## create IsolationForest model 
## the hyper-parameter contamination use default value:'auto'
clf = IsolationForest(contamination = 'auto')
clf.fit(X_full)
scores = clf.decision_function(X_full)

## plot graph to display the results
plt.figure(figsize=(8, 6))
plt.title(&quot;IsolationForest anomaly detection with decision_function&quot;)
plt.scatter(X_full[:, 0], X_full[:, 1], c=scores, cmap='coolwarm', edgecolor='k')
plt.colorbar(label='Anomaly Score')
plt.xlabel(&quot;Feature 1&quot;)
plt.ylabel(&quot;Feature 2&quot;)
plt.grid(True)
plt.show()

'''
Observe the value of decision_function, the last five strokes are outliers.
When contamination is auto, offset is -0.5. 
The data close to 0.5 is the inliner, and the data close to -0.5 is the outlier.
As in the following example
'''

print('offset:',clf.offset_)
plt.figure(figsize=(4, 3))
plt.plot(scores,'.')
plt.title(&quot;decision_function value&quot;)
plt.show()

'''
If contamination is set to 0.0001, the offset calculated by the model is approximately -0.78
At this time, the outlier is the one whose decision_function is close to 0. 
As shown in the following example
'''

clf = IsolationForest(contamination = 0.0001)
clf.fit(X_full)
scores = clf.decision_function(X_full)
print('offset:',clf.offset_)
plt.figure(figsize=(4, 3))
plt.plot(scores,'.')
plt.title(&quot;decision_function value&quot;)
plt.show()
</code></pre>
","0","Answer"
"79577807","79559377","<p>You should describe your field more precisely. It's hard to suggest something, when you need to predict &quot;array with variable length&quot;. For example, time-series prediction uses it's specific technics. So, pls give some info about your data.</p>
<p>Also, for regression you'd keep data normalized. If you use -100 in output, while other values would be very small like 0.1 it may lead to exploding gradient problem. Big values will affect on loss fn very much. So, if your goal is predicting vector try to use zero-padding.</p>
","1","Answer"
"79578551","79577043","<p>Maybe the following approach will help. Try to use usual LLM (like llama, qwen, deeepseek etc..). Specify all possible categories in prompt and ask model to pick categories that fit to some text :)</p>
<p>Also you can get embeddings from all texts and feed them into multiclustering algorithm. Here's an example of such algorithm - <a href=""https://www.researchgate.net/figure/Proposed-K-multicluster-algorithm_fig2_346086809"" rel=""nofollow noreferrer"">https://www.researchgate.net/figure/Proposed-K-multicluster-algorithm_fig2_346086809</a>.</p>
","0","Answer"
"79588642","79586550","<p>Given lack of code I'm assuming you are just loading the CSV file directly into your Streamlit dashboard. Wrap your data loader in <code>@st.cache_data</code> so repeated reads return instantly and convert your big CSV to Parquet offline since Parquet files load faster and take less space (doesn't have to be Parquet just a suggestion there are other options). I personally like lazy loading idea especially in this scenario, try chunking by reading only the columns you need with something like <code>usecols</code> and loading in chunks like <code>pd.read_csv(chunksize=…)</code> then stop as soon as you have enough rows, caching each filtered slice in its own cached function to avoid parsing the entire dataset on every rerun. About preprocessing I recommend pre computing heavy calculating or operations ahead of time and save them to a small JSON or CSV, and offload any long loops to background threads, or pre load them.</p>
<p>Quick tip since I burned myself on it. Logos and pictures should be a static import rather than regular import since large photos can take long time to load in Streamlit. Check your code with <code>timeit</code> or I'm pretty sure Streamlit has something build in I just can't remember right now</p>
","0","Answer"
"79592865","79592576","<p>That is motion blur.</p>
<p>Do not try to &quot;filter it away&quot;. That <strong>will not work</strong>.</p>
<p>Motion blur is reduced using <strong>more light</strong> and <strong>shorter exposure time</strong>. Or strobe light synchronized to image exposures, which you cannot do with an arbitrary webcam.</p>
","0","Answer"
"79594797","79594764","<p>Every time you call <code>model.inference_step()</code>, TensorFlow creates a new computation graph, because your <code>@tf.function</code> is dynamically bound <strong>inside your model object</strong>.</p>
<p>TensorFlow is trying to trace and cache the <code>@tf.function</code>, but it can't re-use the existing trace properly, because <code>model.inference_step</code> is reattached dynamically and behaves non-standardly.</p>
<p>You should not dynamically attach <code>inference_step</code> inside the function. Instead, move the <strong><code>@tf.function</code> outside the model</strong> and use the <code>model</code> directly for prediction.</p>
<pre><code>@tf.function
def inference(model, inputs):
    return tf.stop_gradient(model(inputs, training=False))
</code></pre>
<p>and call:</p>
<pre><code>SMALL_MODEL = creat_model_extractor(MODEL_PATH_SMALL, small_blocks_count)

predictions = inference(SMALL_MODEL, small_blocks_normal)
labels = (predictions &gt; 0.5).numpy().astype(int)
</code></pre>
","0","Answer"
"79595541","79595516","<p>For historical data, you can use metaTrader5's python integration. For exp, it has a method <code>copy_rates_from</code> that does the following: &quot;Get bars from the MetaTrader 5 terminal starting from the specified date&quot;. you can find docs <a href=""https://www.mql5.com/en/docs/python_metatrader5/mt5copyratesfrom_py"" rel=""nofollow noreferrer"">here</a>.</p>
<p>ps: historical data starts from 1970.01.01 as stated in the docs. (data is of OHLC type).</p>
","0","Answer"
"79597306","79597119","<p>You could add</p>
<pre><code>pipe.enable_attention_slicing()
</code></pre>
<p>before sending it to cuda, however this will reduce the speed of the image generation.</p>
<p>Resolution can be changed by adding height and width arguments to the image definition.</p>
<pre><code>image = pipe(prompt, height=512, width=512).images[0]
</code></pre>
","2","Answer"
"79598176","79598098","<p>This issue is common in Jupyter/Colab when widgets like progress bars fail to render during the first run, usually due to the frontend not being fully initialized. It’s not a code problem—just rerunning the cell typically fixes it. This often happens with libraries like <code>transformers</code> or <code>torch</code>. To avoid it entirely, you can run the code as a Python file in VS Code or another script-based environment.</p>
","2","Answer"
"79602050","79601928","<p>You don’t have to scale your Random Forest – it will happily work on raw features</p>
<p>You do need all three models to live in the same VotingClassifier pipeline. The trick is to give each estimator its own preprocessing. Something like this:</p>
<pre class=""lang-py prettyprint-override""><code>pipe_lr = Pipeline([('scale', StandardScaler()),
                    ('lr',    LogisticRegression())])

pipe_svc = Pipeline([('scale', StandardScaler()),
                     ('svc',   SVC(probability=True))])

pipe_rf = Pipeline([('rf', RandomForestClassifier())])

voting = VotingClassifier(
    estimators=[('lr',  pipe_lr),
                ('svc', pipe_svc),
                ('rf',  pipe_rf)],
    voting='soft'
)

voting.fit(X_train, y_train)
</code></pre>
<p>Note that the RF pipeline has no scaler.</p>
<p>So, by wrapping each base learner in its own pipeline - applying <code>StandardScaler</code> only to Logistic Regression and SVM and leaving Random Forest on the original features - you ensure that the VotingClassifier receives consistent inputs while each model trains under its ideal conditions, which keeps your code clean and your ensemble’s performance optimal.</p>
","1","Answer"
"79604760","79604724","<p>I can suggest two simple ways to speed up tuning the parameters for a <code>RandomForestClassifier</code>. First, try enabling multi-threading. The algorithm supports parallel processing, and with the right settings, it can use all available CPU cores.</p>
<p>The second way is to reduce the number of iterations in your loop. Going through values from 1 to 199 can be overkill and take a long time. Instead, you can increase the step size, like using every 5th value, or focus on a more likely range, say, from 10 to 100.</p>
<p>Both options work well together and can significantly cut down the computation time.</p>
","3","Answer"
"79605692","79604798","<h4>Note on autodiff: Autodiff is effectively the same as symbolic differentiation.</h4>
<p>I was a bit confused by this.</p>
<p>The &quot;difference&quot; between the two is that autodiff stores intermediate values. It's a bit tricky to explain, but you effectively build a computational graph to calculate the derivative.</p>
<p>However, for each node in this computation graph, you still use symbolic differentiation to obtain the result.</p>
<p>Whether you consider this to be &quot;different&quot; to symbolic differentiation is somewhat a matter of personal opinion, since you might do pretty much the same thing with a pen and paper to calculate the symbolic derivative.</p>
<p>Simple example:</p>
<pre><code>f(x, y) = x * x * y

# symbolic partial
# (which I calculated without a pen and paper etc because this is a relatively
# simple expression. For more complex expressions, most people, including
# myself, would probably have to start writing things down on paper
# which might include using variable substitutions to simplify the calculation.
# If intermediate variables are introduced, you effectively are doing a version
# of the autodiff algorithm.)
df_dx(x) = 2 * x * y

# autodiff partial
u(x) = x * x
f(u, y) = u(x) * y
df_dx = df_du * du_dx
df_du = y
du_dx = 2 * x
df_du = y * 2 * x
</code></pre>
<p>I think it makes more sense to view autodiff as an <em>algorithm</em> for calculating a derivative, rather than a totally different method. Typically to implement the algorithm you would</p>
<ul>
<li>use a tuple containing a variable value and the value of its derivate</li>
<li>in combination with operator overloading</li>
</ul>
<p>both of which together allows for numerical evaluation of the derivative in combination with the original function.</p>
<p>Usually we think of operators being applied to floating point values to evaluate a function. Just think of this as those same primative operators applied to a tuple of floats <code>(value, gradient)</code>.</p>
<hr />
<p>I think this in combination with an educated guess about what the context manager is being used for pretty much answers most of the question, although perhaps without as much depth as I would like.</p>
<ul>
<li>The context manager triggers the beginning and end of an autodifferentiation algorithm, so presumably it does something to switch between evaluating a function (which is an expression containing operators) using a single variable to obtain the function <em>value</em> to evaluating a function using a pair of variables to obtain the <em>value and gradient</em>.</li>
</ul>
","0","Answer"
"79607514","79598098","<p>You're likely facing a runtime environment issue. The first execution triggers model downloads and widget rendering. If you're using Jupyter/Colab, the frontend may not be fully ready, causing display errors. Once cached, the second run works smoothly.</p>
<p>This might help:</p>
<ul>
<li><p>Isolate model loading in a separate cell or script.</p>
</li>
<li><p>Add simple print logs to track download/init time.</p>
</li>
<li><p>Run in a standard Python script (e.g., VS Code terminal) to avoid frontend rendering issues.</p>
</li>
</ul>
<p>This is a classic case of execution environment quirks,, similar to what you'd encounter when debugging backend issues in server-side apps. If you're into that sort of thing, <a href=""https://www.cloudways.com/blog/php-debug/"" rel=""nofollow noreferrer"">this quick read</a> offers a good debugging mindset.</p>
","1","Answer"
"79607818","79600456","<p>The effectiveness of different scaling methods can vary depending on the characteristics of the dataset.</p>
<p>Generally , you can apply the sklearn.preprocessing process.</p>
<p>Commonly used ones are <code>StandardScaler()</code>,  <code>MinMaxScaler()</code>,  <code>RobustScaler().</code></p>
<p>But it also depends on the needs of your problem. Reference to <a href=""https://scikit-learn.org/stable/api/sklearn.preprocessing.html"" rel=""nofollow noreferrer"">sklearn.preprocessing.html</a>.</p>
<p>A simple and practical way to choose the most suitable method is to use <code>cross-validation</code> to compare their predictive accuracy and select the one that performs best.</p>
<p>Reference to <a href=""https://scikit-learn.org/stable/modules/cross_validation.html"" rel=""nofollow noreferrer"">cross_validation.html</a>.</p>
","0","Answer"
"79609193","79609185","<p>I think the gradient function seems better:</p>
<pre><code>def gradient_descent(w, b, x, y, iterations, alpha):
    m = len(x)
    
    for i in range(iterations):
        dj_dw = 0
        dj_db = 0
        
        for j in range(m):
            f_wb = w * x[j] + b
            error = f_wb - y[j]
            dj_dw += error * x[j]
            dj_db += error
        
        dj_dw /= m
        dj_db /= m

        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        cost_value = cost(w, b, x, y)
        print(f&quot;Iteration {i} || Cost = {cost_value} || w = {w} || b = {b}&quot;)

    return w, b
</code></pre>
","1","Answer"
"79609613","79608270","<p>The original approach attempts to predict a sine wave by feeding the model a sequence of time values (<code>x</code>), which is a linearly increasing array. The model is then asked to predict the corresponding future values of <code>sin(x)</code>. Notwhithstanding, Neural Networks do not perform well with linear and non-bounded inputs. In the case of series forecasting, you want to use previous values of the predicting variable (y) to forecast the next output. Additionally, you may need to normalize the amplitude value, as the output performs well with values between -1 and 1 (not 20).</p>
<p>Let <code>sin_generator</code> be a sin-wave data generator for a given configuration, in your case, you can use the following config:</p>
<pre><code>def sin_generator(look_back: int,
                  forecast_horizon: int,
                  num_samples: int = 1000,
                  x_initial_value: float = 0.0,
                  x_end_value: float = 2000.0,
                  y_amplitude: float = 20.0,
                  dtype: str = 'float32'):
    &quot;&quot;&quot;
    Create a generator for the time series data.

            y = y_amplitude * np.sin(x)

    :param look_back: The number of previous time steps to use as input.
    :param forecast_horizon: The number of time steps to predict.
    :param num_samples: The number of samples to generate.
    :param x_initial_value: The initial value of x.
    :param x_end_value: The end value of x.
    :param y_amplitude: The amplitude of the sine wave.
    :param dtype: The data type of the output.
    :return: A generator that yields (x, y) pairs.
    &quot;&quot;&quot;
    for _ in range(num_samples):
        # Random loopback:
        initial_pos = np.random.randint(low=x_initial_value, high=x_end_value - look_back)
        # Create x values:
        x = np.arange(initial_pos, initial_pos + look_back + forecast_horizon, dtype=dtype)
        # Create y values:
        y = (y_amplitude * np.sin(x)).astype(dtype)
        # Create the generator:
        # yield x[:look_back], y[look_back:] -&gt; wrong
        yield y[:look_back], y[look_back:]
</code></pre>
<p>You can build a <code>tf_dataset</code>  as follows:</p>
<pre><code>def sin_tf_dataset(generator, look_back: int, forecast_horizon: int, batch_size: int):
    &quot;&quot;&quot;
    Create a TensorFlow dataset from the generator.

    :param generator: The generator to use.
    :param look_back: The number of previous time steps to use as input.
    :param forecast_horizon: The number of time steps to predict.
    :param batch_size: The batch size for the dataset.
    :return: A TensorFlow dataset.
    &quot;&quot;&quot;
    # Create the dataset:
    dataset = tf.data.Dataset.from_generator(
        generator,
        output_signature=(
            tf.TensorSpec(shape=(look_back,), dtype=tf.float32),
            tf.TensorSpec(shape=(forecast_horizon,), dtype=tf.float32)
        )
    )
    # Batch the dataset:
    dataset = dataset.batch(batch_size)
    return dataset
</code></pre>
<p>And, finally let <code>plot_dataset</code> be an utility function to show the generated data:</p>
<pre><code>def plot_dataset(tf_dataset, n_examples=3):
    &quot;&quot;&quot;
    Plot a few examples from a time series dataset.

    :param tf_dataset: The tf.data.Dataset object (should yield (input, target)).
    :param n_examples: How many examples to plot.
    &quot;&quot;&quot;
    for i, (x, y) in enumerate(tf_dataset.unbatch().take(n_examples)):
        plt.figure(figsize=(10, 4))
        look_back = x.shape[0]
        forecast_horizon = y.shape[0]

        # Plot input (past)
        plt.plot(range(look_back), x.numpy(), label=&quot;Input (Past)&quot;, color='blue')

        # Plot target (future)
        plt.plot(range(look_back, look_back + forecast_horizon), y.numpy(), label=&quot;Target (Future)&quot;, color='orange')

        # Vertical line to mark the split
        plt.axvline(x=look_back - 1, color=&quot;gray&quot;, linestyle=&quot;--&quot;)

        plt.title(f&quot;Sample {i+1}: Forecasting {forecast_horizon} steps from {look_back} past values&quot;)
        plt.xlabel(&quot;Timestep&quot;)
        plt.ylabel(&quot;Value&quot;)
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plt.show()
</code></pre>
<p>Then, a simple LSTM model can predict the time series with the following snipet:</p>
<pre><code>def create_model(input_shape, output_shape):
    &quot;&quot;&quot;
    Create a simple LSTM model for time series forecasting.

    :param input_shape: The shape of the input data.
    :param output_shape: The shape of the output data.
    :return: A compiled Keras model.
    &quot;&quot;&quot;
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(128, activation='tanh', input_shape=input_shape),
        tf.keras.layers.Dense(output_shape)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model


if __name__ == &quot;__main__&quot;:
    # Set parameters:
    batch_size = 32
    config = {
        'look_back': 100,
        'forecast_horizon': 100,
        'num_samples': 1000,
        'x_initial_value': 0.0,
        'x_end_value': 2000.0,
        'y_amplitude': 1.0,
        'dtype': 'float32',
    }

    # Create generator:
    dataset = sin_tf_dataset(
        lambda: sin_generator(**config),
        look_back=config['look_back'],
        forecast_horizon=config['forecast_horizon'],
        batch_size=batch_size
    )

    # Plot the generator:
    plot_dataset(dataset, n_examples=3)

    # Create model:
    input_shape = (config['look_back'], 1)
    output_shape = config['forecast_horizon']
    model = create_model(input_shape, output_shape)
    model.summary()

    # Train model:
    model.fit(dataset, epochs=10)
</code></pre>
<p>In this case, you need to normalize the y_amplitude, you can lately multiply by the value you need as part of pos-processing. This model reports the following log:</p>
<pre><code>Model: &quot;sequential&quot;
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ (None, 128)            │        66,560 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 100)            │        12,900 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 79,460 (310.39 KB)
 Trainable params: 79,460 (310.39 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 3s 45ms/step - loss: 0.4757
Epoch 2/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 1s 44ms/step - loss: 0.1000
Epoch 3/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 1s 43ms/step - loss: 0.0011
Epoch 4/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 1s 43ms/step - loss: 1.5433e-04
Epoch 5/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - loss: 8.2073e-05
Epoch 6/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 2s 47ms/step - loss: 6.9301e-05
Epoch 7/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 2s 47ms/step - loss: 6.1493e-05
Epoch 8/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - loss: 5.0608e-05
Epoch 9/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 2s 51ms/step - loss: 4.3731e-05
Epoch 10/10
32/32 ━━━━━━━━━━━━━━━━━━━━ 2s 48ms/step - loss: 3.9795e-05
</code></pre>
<p>Here, you have transformed the task from a regression over raw time to a standard sequence-to-sequence forecasting problem, which is what recurrent models like LSTM are designed for. This structure makes it easier for the model to capture periodicity, trends, and normalized local dynamics in the signal.</p>
","1","Answer"
"79610781","79609919","<p>Generally, when you have a set of inputs where the numbers are <em>labels</em> rather than a meaningful ordering, you want to encode them as a one-hot vector:
So April would be represented as [ 0 0 0 1 0 0 0 0 0 0 0 0 ].</p>
<p>What this does is allow the network to immediately map each possible input to a unique row  in the first weight matrix (take a look at how matrix-vector multiplication works if you don't see why that is), which makes the rest of the job a lot simpler, because it can <em>almost</em> treat them as independent problems at that point. (It <em>could</em> learn how to do that by itself, but you usually need a larger network and a lot more training time.)</p>
<p>You might also do a similar trick on the output, by treating the output as three categories rather than a number, and doing a softmax output and categorical cross-entropy as your loss function, but that's less essential than it is on the input.</p>
<p>Playing with the layer sizes is another thing to try: sometimes wider is better than deeper. And 10 and 5 are very small layers, although for this problem, they might suffice. I'd probably go with two layers of size 16-32, though, and then tweak it from there by seeing how changes affect the training speed. Keep in mind that sometimes smaller is better. A smaller network has less capacity, but also has fewer weights to learn. It's not uncommon to find a network train faster as you decrease the layer sizes, until you decrease them a bit too far and it stops being able to fit the data.</p>
<p>Finally, you might just need to train it longer. A lot of examples out there only train for a small number of epochs, but they can get away with that because each epoch has a bunch of samples in it. If you only have 12 samples, give it at least a few hundred epochs. If it still isn't learning after 1000, something else is wrong, though.</p>
","1","Answer"
"79612704","79591703","<p>TL;DR use <code>pettingzoo&lt;1.25.0</code> or use the GitHub version with the unreleased patch that supports 1.25+</p>
<hr />
<p>The problem is that <code>pettingzoo</code> changed <code>pettingzoo.utils.agent_selector</code>. Prior to <code>1.25.0</code> from <code>pettingzoo.utils import agent_selector</code> imported the class <code>pettingzoo.utils.agent_selector.agent_selector</code>. However, with the change of <code>agent_selector -&gt; AgentSelector</code> this line now exports the module.</p>
<p>You could monkeypatch/locally modify the 3rd party library you are using using to use either the new <code>AgentSelector</code> or modify the import to <code>pettingzoo.utils.agent_selector import agent_selector</code> to import the deprecated class.</p>
<hr />
<p>I've created a PR for sumo_rl <a href=""https://github.com/LucasAlegre/sumo-rl/pull/231"" rel=""nofollow noreferrer"">https://github.com/LucasAlegre/sumo-rl/pull/231</a> that has been merged now, you could switch the the latest GitHub version instead.</p>
","0","Answer"
"79616866","79615828","<p>The <code>TreeExplainer</code> <a href=""https://github.com/shap/shap/blob/f21a8a7873d58861df34407c222870009cb9578b/shap/explainers/_tree.py#L278"" rel=""nofollow noreferrer"">attempts to copy the model into a common interface</a>, the <code>TreeEnsemble</code>. I suppose it's not surprising that something might slip through the cracks (various packages might fundamentally store different precision numbers and with different languages), but serious changes to predictions should be reported as an issue on shap's github.</p>
","2","Answer"
"79618985","79618769","<p>The issue lies in your incorrect computation of the gradient for the output layer during backpropagation. When using softmax activation followed by cross-entropy loss, the gradient simplifies to the predicted probabilities (<code>self.output</code>) minus the one-hot encoded ground truth labels. Your current implementation manually iterates over each class and sample, reapplying softmax and calculating differences, which is both inefficient and prone to numerical instability. Instead, you should directly subtract 1 from the softmax outputs at the target class indices (<code>self.output[range(batch_size), desired_outputs] -= 1</code>) and normalize over the batch size. This gives the correct gradient for backpropagation. Additionally, ensure that weights and biases are updated using this gradient, scaled by the learning rate. Correcting this will allow the model to learn properly and reduce the loss during training.</p>
","0","Answer"
"79621251","79621225","<p>You need the following libs and tools.</p>
<ul>
<li><p>Intel RealSense SDK (<code>pyrealsense2</code>)</p>
</li>
<li><p><a href=""https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe/blob/main/README_EN.md"" rel=""nofollow noreferrer"">MediaPipe</a> (for hand and fingertip tracking)</p>
</li>
<li><p><a href=""https://opencv.org/"" rel=""nofollow noreferrer"">OpenCV</a> (for ArUco detection and transformation)</p>
</li>
<li><p><a href=""https://www.theengineeringprojects.com/2023/06/installation-and-functions-of-numpy-in-python.html"" rel=""nofollow noreferrer"">NumPy</a> (for coordinate math)</p>
</li>
<li><p>Python 3.9</p>
</li>
</ul>
<p>You have to determine fingertip 3D coordinates in the ArUco tag coordinate system, not just the camera frame. Use OpenCV’s <code>cv2.aruco</code> to detect markers. Use <code>cv2.aruco.estimatePoseSingleMarkers()</code> to get pose (rotation and translation) of each tag in camera coordinates. This gives you the rotation vector (<code>rvec</code>) and the translation vector (<code>tvec</code>). Use MediaPipe Hands to get landmark 8 (index finger tip) in image coordinates. Use RealSense depth frame to get Z (depth in mm) at the fingertip pixel. Use the RealSense intrinsics to convert (x, y, depth) to 3D camera space coordinates. Use the rotation (<code>rvec</code>) and translation (<code>tvec</code>) from ArUco detection to transform the 3D point into the marker’s coordinate system.</p>
","1","Answer"
"79623013","79622840","<p>You can use <code>np.delete</code> to remove the indices directly prior to fitting the model.</p>
<pre class=""lang-py prettyprint-override""><code># Polynomial features
poly   = PolynomialFeatures(degree=5)
X_poly = poly.fit_transform(X)

# Remove unwanted terms (e.g., x1 * x2 and x2^2)
exclude_indices = [3, 5]
X_poly_reduced  = np.delete(X_poly, exclude_indices, axis=1)

# Linear Regression model
model = LinearRegression()
model.fit(X_poly_reduced, y)
</code></pre>
","1","Answer"
"79623580","79623456","<p>As long as the values in prevalent_topic are mutually exclusive (and are in the normal factor class), you can use <code>multinom_reg()</code> to get a model. Instead of fitting a set of logistic regressions, you can simultaneously model all of your categories.</p>
<p>If there are not mutually exclusive (like a multiple choice question), you would probably need to make separate factors and model each separately. That &quot;multilabel&quot; structure isn't currently supported in tidymodels.  You might look at the recipe step <code>step_dummy_multi_choice()</code> (<a href=""https://recipes.tidymodels.org/reference/step_dummy_multi_choice.html"" rel=""nofollow noreferrer"">https://recipes.tidymodels.org/reference/step_dummy_multi_choice.html</a>) (followed by <code>step_bin2factor()</code> (<a href=""https://recipes.tidymodels.org/reference/step_bin2factor.html"" rel=""nofollow noreferrer"">https://recipes.tidymodels.org/reference/step_bin2factor.html</a>)) to make the different outcome columns.</p>
","1","Answer"
"79624373","79621002","<h4>1. Extract Raw PyTorch Model and Save Independently</h4>
<p>The best approach is to <strong>extract the core PyTorch model (backbone + head)</strong> and save it <strong>without using ultralytics</strong>.</p>
<p>Here’s how you can do this:</p>
<pre class=""lang-py prettyprint-override""><code>from ultralytics import YOLO
import torch

# Load YOLOv8 classification model (e.g., yolo11n-cls.pt)
model = YOLO('yolo11n-cls.pt')

# Get the actual nn.Module
core_model = model.model  # This is a standard torch.nn.Module

# Save the raw PyTorch model
torch.save(core_model.state_dict(), 'yolo11n_raw_weights.pth')
</code></pre>
<p>Then, on your Python 3.6 environment:</p>
<ul>
<li><p>Rebuild the <strong>exact same model architecture manually</strong> using PyTorch 1.8 (compatible with Python 3.6)</p>
</li>
<li><p>Load the state dict:</p>
</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>model = YourReimplementedYOLOClsModel()  # Define manually
model.load_state_dict(torch.load('yolo11n_raw_weights.pth'))
model.eval()
</code></pre>
<p>You avoid <code>ultralytics</code> completely, and maintain accuracy since you're using original weights and logic.</p>
<h4>2. Export to ONNX (Optional)</h4>
<p>Another option is exporting to <strong>ONNX</strong>, which can run with runtime libraries that support Python 3.6:</p>
<pre class=""lang-py prettyprint-override""><code># Still in a compatible env (Python 3.8+)
model = YOLO('yolo11n-cls.pt')
core_model = model.model

dummy_input = torch.randn(1, 3, 224, 224)  # Adjust to your input size
torch.onnx.export(core_model, dummy_input, &quot;yolo11n.onnx&quot;, opset_version=11)
</code></pre>
<p>Then use ONNX Runtime in your Python 3.6 environment.</p>
<hr />
","0","Answer"
"79630936","79597666","<p>You hardcoded the input size to the <code>nn.Linear</code> layer like this:</p>
<p>self.fc = nn.Linear(<code>64*14*14, 4) </code></p>
<p>But after your convolutions and pooling, the real shape is not necessarily <code>64×14×14</code>, especially if input image size or the conv/pool settings change.</p>
<p>You need to dynamically compute the feature size instead of hardcoding it.</p>
","0","Answer"
"79631273","79631112","<p>With a convolutional network, &quot;features&quot; are the output of its filters, and &quot;channels&quot; are the inputs you give it. For example, a 2D conv network might either have 3 colour channels, or 1 greyscale one. The layer can output the results of perhaps 10, 100 or 1000 different features from &quot;convoluting&quot; a &quot;filter&quot; over the data - it's up to us how many filters we want to use in each layer. It is common practice to make the size of the &quot;filter&quot; (eg 7x7, or in your case, just 7) decrease and number of features increase per each layer deep into the network. <a href=""https://ieeexplore.ieee.org/abstract/document/9142089"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/abstract/document/9142089</a></p>
<p>I think the issue you have is that your sequence input layer might be reshaping the input. Perhaps this layer is squashing together the samples into an RNN format, and then when it goes through your network it gives the wrong dimensions at the output. If you are learning on a bunch of 300-length inputs, you might be better using an input layer <a href=""https://uk.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.inputlayer.html"" rel=""nofollow noreferrer"">https://uk.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.inputlayer.html</a></p>
<p>I recommend printing out the dimension of each layer when debugging to help catch these sorts of bugs - you should persist with the same samples that you start with as what you end with.</p>
","1","Answer"
"79631646","79628713","<p>The issue is that <code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] == -1</code> which masks all devices.</p>
<p>A quick workaround is to use:</p>
<pre class=""lang-py prettyprint-override""><code>import os  # very first import
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;  # or different indices as appropriate

# all other, especially ML libraries after
import torch
</code></pre>
<p>possibly also before starting the notebook:</p>
<pre class=""lang-bash prettyprint-override""><code>export CUDA_VISIBLE_DEVICES=0
jupyter notebook
</code></pre>
<hr />
<p>This should work but does not fix underlying problem which is tied to the launch of jupyter or the kernel.</p>
<p>One possibility is that <code>CUDA_VISIBLE_DEVICES</code> is loaded from an <code>.env</code> file according to this <a href=""https://discourse.jupyter.org/t/jupyter-notebook-not-detecting-gpu/2272"" rel=""nofollow noreferrer"">comment</a>.</p>
<p>Another location where the problem could occur and can be fixed is to set the environment variables in the <code>kernel.json</code> file. For a detailed step-by-step setup see this answer <a href=""https://stackoverflow.com/a/53595397/12439683"">answer</a></p>
<p>Essentially check or define <code>CUDA_VISIBLE_DEVICES</code> in the <code>kernel.json</code> in the appropriate environment folder</p>
<pre class=""lang-json prettyprint-override""><code>{
 &quot;display_name&quot;: &quot;Your Env Display name&quot;,
 &quot;language&quot;: &quot;python&quot;,
 &quot;argv&quot;: ...,
 &quot;env&quot;: {&quot;CUDA_VISIBLE_DEVICES&quot;:&quot;0,1,2,3&quot;}
}
</code></pre>
","0","Answer"
"79635080","79619113","<p>According to your two problems:</p>
<ol>
<li><p><strong>performing spell checks does not give entire string as an output</strong></p>
</li>
<li><p><strong>sometimes repeats the phrases of the given review</strong></p>
</li>
</ol>
<p>I think you can adjust two arguments ( max_length, no_repeat_ngram_size ) for model.generate() to improve two problems:</p>
<ol>
<li><p><strong>Enlarge max_length size to solve problem-1.</strong></p>
</li>
<li><p><strong>Add no_repeat_ngram_size argument to reduce problem-2 error.</strong></p>
</li>
</ol>
<p>reference:</p>
<ol>
<li><p><a href=""https://huggingface.co/docs/transformers/main/main_classes/text_generation#transformers.GenerationConfig.max_length"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main/main_classes/text_generation#transformers.GenerationConfig.max_length</a></p>
</li>
<li><p><a href=""https://huggingface.co/docs/transformers/main/main_classes/text_generation#transformers.GenerationConfig.no_repeat_ngram_size"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main/main_classes/text_generation#transformers.GenerationConfig.no_repeat_ngram_size</a></p>
</li>
</ol>
","0","Answer"
"79641738","79641499","<p>Usually it is as simple as this:</p>
<pre class=""lang-py prettyprint-override""><code># Find the index of the highest confidence
max_conf_index = torch.argmax(conf)

# Retrieve the corresponding box data
best_box = data[max_conf_index]
</code></pre>
<p>Please double check that really your <code>data</code> and <code>conf</code> mismatch.</p>
<p>You can also get the coordinates directly from the boxes:</p>
<pre class=""lang-py prettyprint-override""><code>for box in boxes:
    b = box.xyxy[0]  # Get box coordinates
</code></pre>
<p>For your example, just to prove <em>again</em>:</p>
<pre class=""lang-py prettyprint-override""><code>import cv2
import numpy as np

conf = [0.8853, 0.4671, 0.4509, 0.3296]
data = [
    [61.6508, 256.6027, 232.8067, 290.9552, 0.8853, 0.0000],
    [144.6730, 170.1858, 296.8442, 229.6742, 0.4671, 0.0000],
    [13.3585, 179.2076, 177.5936, 238.9743, 0.4509, 0.0000],
    [127.6127, 176.1886, 282.9232, 233.7913, 0.3296, 0.0000]
]

# Paint a given box onto an image
def paint_box(image, box, color=(0, 255, 0), thickness=2):
    x1, y1, x2, y2, conf, _ = box
    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)
    cv2.putText(image, f&quot;confidence: {conf:.2f}&quot;, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

# Load the image
image_path = &quot;Q2WTOznZ.png&quot;  # from https://i.sstatic.net/Q2WTOznZ.png
image      = cv2.imread(image_path)

# Find the index of the highest confidence
max_conf_index = np.argmax(conf)

# Retrieve the corresponding box data
best_box = data[max_conf_index]

# Paint the box with the highest confidence
paint_box(image, best_box, color=(0, 255, 0), thickness=2)

# Display the image
cv2.imshow(&quot;Image with Box&quot;, image)
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>
<p>yields</p>
<p><a href=""https://i.sstatic.net/f5FlvC36.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f5FlvC36.png"" alt=""box with highest confidence"" /></a></p>
","1","Answer"
"79645952","79645357","<p>The loss function, as you have defined it, expects values in the form of probability values for a given class. If you do not apply an activation function to the model output, raw values (<strong>logits</strong>) are fed into the loss function.</p>
<p>You can perform training without using an activation function, but when defining the loss function, you must specify that the values provided were NOT converted to probability values:</p>
<pre><code>loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-4), # 'adam'
    loss=loss,
    metrics=[&quot;binary_accuracy&quot;] # accuracy
)
</code></pre>
","0","Answer"
"79647605","79647350","<p>I'm not sure what happened here:</p>
<pre><code>patch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))
</code></pre>
<p>but I assume it's some kind of mistake.</p>
<p>Assuming bfrc has shape of <code>(batch, channels, height, width)</code>, and</p>
<pre><code>V_PATCHES = IMG_HEIGHT // PATCH_HEIGHT
H_PATCHES = IMG_WIDTH // PATCH_WIDTH
</code></pre>
<p>then <code>patch_reshaped_pn_c_h_w</code> will have the shape of <code>(V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH)</code>.</p>
<p>Keeping this in mind, you can simply reconstruct the image via simply transposing and reshaping, which is quite cheaper than these nested loops.</p>
<pre><code>V, H, C, PH, PW = patch_reshaped_ph_pw_c_h_w.shape

H_total = V * PH
W_total = H * PW

patches = jnp.transpose(patch_reshaped_ph_pw_c_h_w, (0, 1, 3, 4, 2))  # (V, H, PH, PW, C)

reconstructed = patches.reshape(V, H, PH, PW, C)
reconstructed = reconstructed.transpose(0, 2, 1, 3, 4)
reconstructed = reconstructed.reshape(H_total, W_total, C)
reconstructed = jnp.transpose(reconstructed, (2, 0, 1))[jnp.newaxis, ...] # (1, C, H, W)
</code></pre>
<p>You can additionally wrap it into <code>@jax.jit</code>, which should be slightly faster.</p>
","1","Answer"
"79651151","79650016","<blockquote>
<p><strong>The MSE value is: 0.0004. That means the model predicts very well.</strong></p>
</blockquote>
<p>This isn't necessarily true. Typically, you divide the dataset into training and testing subsets, and evaluate the model's accuracy on the test set to get a more reliable measure of performance.</p>
<blockquote>
<p><strong>The problem now is that when predicting a combination that it didn't learn from the data set</strong></p>
</blockquote>
<p>Statistical models like neural networks aren't designed to predict on data points that differ from the training data distribution .</p>
<p>To make this system work, you'd need to transform the inputs into meaningful features. I would recommend you read more about how machine learning models work before proceeding.</p>
","0","Answer"
"79651408","79651091","<p>I suspect that your issue is here:</p>
<pre><code>df = df[df.duration &gt;= 1] &amp; (df.duration &lt;= 60)
</code></pre>
<p>This is creating a result that is <code>n * m</code> rows and columns. Where <code>n</code> is the number of rows matching <code>df[df.duration &gt;= 1]</code> and <code>m</code> is the number of rows in <code>df</code>. I strongly suspect what you want to do is filter <code>df</code> for rows matching that combined criteria (duration between 1 and 60), so you likely wanted to do:</p>
<pre><code>df = df[(df.duration &gt;= 1) &amp; (df.duration &lt;= 60)]
</code></pre>
<p>The following will likely do the same in the event you find it simpler:</p>
<pre><code>df = df[df.duration.between(1, 60)]
</code></pre>
<p>For Reference:</p>
<pre><code>import pandas

df = pandas.DataFrame([
    {&quot;foo&quot;: -1}, {&quot;foo&quot;: -2}, {&quot;foo&quot;: -3},
    {&quot;foo&quot;: 1}, {&quot;foo&quot;: 2}, {&quot;foo&quot;: 3},
    {&quot;foo&quot;: 11}, {&quot;foo&quot;: 12}, {&quot;foo&quot;: 13},
])

print(&quot;-------------------&quot;)
print(&quot;Rows matching df.foo &lt;= 10&quot;)
print(df.foo &lt;= 10)
print(&quot;-------------------\n&quot;)

print(&quot;-------------------&quot;)
print(&quot;Rows matching df[df.foo &gt;= 1]&quot;)
print(df[df.foo &gt;= 1])
print(&quot;-------------------\n&quot;)

print(&quot;-------------------&quot;)
print(&quot;Your current result&quot;)
print(&quot;Rows matching df[df.foo &gt;= 1] &amp; (df.foo &lt;= 10)&quot;)
print(df[df.foo &gt;= 1] &amp; (df.foo &lt;= 10))
print(&quot;-------------------\n&quot;)

print(&quot;-------------------&quot;)
print(&quot;Likely your objective&quot;)
print(&quot;Rows matching df[(df.foo &gt;= 1) &amp; (df.foo &lt;= 10)]&quot;)
print(df[(df.foo &gt;= 1) &amp; (df.foo &lt;= 10)])
print(&quot;-------------------\n&quot;)
</code></pre>
<p>That will give you:</p>
<pre><code>-------------------
Rows matching df.foo &lt;= 10
0     True
1     True
2     True
3     True
4     True
5     True
6    False
7    False
8    False
Name: foo, dtype: bool
-------------------

-------------------
Rows matching df[df.foo &gt;= 1]
   foo
3    1
4    2
5    3
6   11
7   12
8   13
-------------------

-------------------
Your current result
Rows matching df[df.foo &gt;= 1] &amp; (df.foo &lt;= 10)
     foo      0      1      2      3      4      5      6      7      8
3  False  False  False  False  False  False  False  False  False  False
4  False  False  False  False  False  False  False  False  False  False
5  False  False  False  False  False  False  False  False  False  False
6  False  False  False  False  False  False  False  False  False  False
7  False  False  False  False  False  False  False  False  False  False
8  False  False  False  False  False  False  False  False  False  False
-------------------

-------------------
Likely your objective
Rows matching df[(df.foo &gt;= 1) &amp; (df.foo &lt;= 10)]
   foo
3    1
4    2
5    3
-------------------
</code></pre>
","0","Answer"
"79655271","79655017","<p>No, don't include the response variable, just include the vector of predictors.</p>
<p>There are some models where you would pass the entire matrix and then specify which is the target, but in general unless you are sure about that you don't want the response included as a variable when you train the model. In this case, since you are passing a vector you can remove that column from the matrix.</p>
<p>If you haven't come across it you could check out Deep Learning and Scientific Computing with R torch by Sigrid Keydana. It's free if you google it. Chapter 13.2 contains an example dataloader using Palmer Penguins.</p>
","0","Answer"
"79658158","79657416","<p>When you run a Python script from a Jupyter notebook, Jupyter automatically adds a <code>-f</code> argument (pointing to the kernel connection file).</p>
<p>Your VICReg training script's argument parser doesn't recognize this argument and treats it as an invalid training parameter. The best solution is to run the script from a terminal instead.</p>
<p>If you need to run it from the notebook, use the <code>subprocess</code> module to execute it as a separate process.</p>
","1","Answer"
"79659173","79657416","<p>I found the solution on a website indicating to add an empty space on the following lines of my code:</p>
<pre><code>if __name__ == '__main__':
    args = get_args_parser()
    args = args.parse_args(&quot;&quot;) # here the empty space is added as &quot;&quot;
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main(args)
</code></pre>
<p>That actually solved my problem. You can check the following link for detailed information: <a href=""https://medium.com/@data.scientist/ipython-trick-how-to-use-argparse-in-ipython-notebooks-a07423ab31fc"" rel=""nofollow noreferrer"">https://medium.com/@data.scientist/ipython-trick-how-to-use-argparse-in-ipython-notebooks-a07423ab31fc</a></p>
","0","Answer"
"79659417","79659212","<p>When you call <code>to</code>, you are referencing a specific function implemented for the <code>torch.Tensor</code> type. The python <code>list</code> type does not have the <code>to</code> function implemented, so you get the error.</p>
<p>You can get around this in one of two ways:</p>
<ol>
<li><p>Call <code>to</code> on each individual tensor, ie <code>samples = [i.to(device, non_blocking=True) for i in samples]</code></p>
</li>
<li><p>Convert the list of tensors into a tensor, then call <code>to</code>, ie <code>samples = torch.stack(samples, 0).to(device, non_blocking=True)</code>. Note that this requires all tensors in <code>samples</code> to be the same shape</p>
</li>
</ol>
","1","Answer"
"79661370","79660425","<p>Okay, if you want to split a set of N features (F) into two complementary subsets (S1, S2), and you have a complementarity score C(f_i, f_j) between any two features f_i and f_j:</p>
<p><strong>Goal is to</strong> Maximize the total complementarity between S1 and S2.<br />
say, Total_Complementarity(S1, S2) = sum(C(f_i, f_j) for f_i in S1 for f_j in S2)</p>
<p><strong>Greedy Algorithm:</strong></p>
<ul>
<li><p>Initialize S1 and S2 (e.g., S1 with one arbitrary feature, S2 with the rest).</p>
</li>
<li><p>Iteratively move a feature from one subset to the other if that move increases Total_Complementarity(S1, S2).</p>
</li>
<li><p>Or, start with S1 empty, S2 = F. Iteratively move the feature from S2 to S1 that results in the largest <em>increase</em> in Total_Complementarity. Stop when S1 has N/2 features, or when no move improves the score.</p>
</li>
</ul>
","-1","Answer"
"79661611","79648131","<p>It looks like you've logged too many artifacts, which caused the system to exceed the file descriptor limit used by <code>select()</code> calls.</p>
<p>Here are two ways you can try to resolve the issue:</p>
<ol>
<li>Reduce the frequency of artifact logging. For example, log only every 10 epochs:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>if epoch % 10 == 0:
    mlflow.log_artifact(&quot;your_artifact_file.xxx&quot;)
</code></pre>
<ol start=""2"">
<li>Upgrade to a newer version of the Py4J library (and the Spark runtime, if applicable), where the underlying system call used to wait for socket events has been changed from <code>select()</code> to <code>poll()</code>. This helps avoid the file descriptor limit that the system call <code>select()</code> imposes.</li>
</ol>
<p>Also, consider calling <code>mlflow.end_run()</code>
after training to ensure all resources are properly released.</p>
","0","Answer"
"79661861","79661708","<p>OpenCV does not pay attention to any metadata such as SAR/DAR. It gives you the data <em>as it is</em>, not as it should be <em>displayed</em>.</p>
<p>It's on you to resize the frames as needed.</p>
<p>1440 by 1080 with a 16:9 DAR is <strong>anamorphic</strong> (non-square pixels). That's no good for processing (ML training). You should resize these frames.</p>
<p>OpenCV allows you to read the SAR using the Cap props <code>CAP_PROP_SAR_NUM</code> and <code>CAP_PROP_SAR_DEN</code> (<a href=""https://docs.opencv.org/4.x/d4/d15/group__videoio__flags__base.html#ggaeb8dd9c89c10a5c63c139bf7c4f5704da4fc3ef1222d9a8be3d7b6d044494a523"" rel=""nofollow noreferrer"">docs</a>).</p>
<p>This code maintains the number of lines in the frame and applies a horizontal stretch:</p>
<pre class=""lang-py prettyprint-override""><code># vid = cv.VideoCapture(&quot;some video file&quot;)

SAR_num = vid.get(cv.CAP_PROP_SAR_NUM)
SAR_den = vid.get(cv.CAP_PROP_SAR_DEN)
assert SAR_num != SAR_den, &quot;square pixels, operation is pointless&quot;
SAR = SAR_num / SAR_den

frame_for_display = cv.resize(
    frame,
    dsize=None, fx=SAR, fy=1, interpolation=cv.INTER_LINEAR)
</code></pre>
<p>If you need to display that in any other resolution, calculate the fx/fy or dsize accordingly. Do not resize twice.</p>
","2","Answer"
"79662997","79658224","<p>&quot;evaluation_strategy&quot; has been deprecated since version 4.46 of the Hugging Face Transformers library.
<a href=""https://github.com/huggingface/transformers/pull/30190"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/pull/30190</a></p>
<p>Changing <code>evaluation_strategy=&quot;&quot;</code> to <code>eval_strategy=&quot;&quot;</code> should fix the unexpected argument issue.</p>
<p>Your configuration for the 6-label classifier looks correct (num_labels=6, problem_type=&quot;multi_label_classification&quot;). If you run into any errors, please share the traceback for further assistance.</p>
","0","Answer"
"79663055","79662773","<p><code>ImageFolder</code> expects files to be stored locally, not on remote cloud storage like S3. So you'll need to first download the files from S3 to your local system and then load them using <code>ImageFolder</code>.</p>
<p>Here's an example of how you can do this:</p>
<pre class=""lang-py prettyprint-override""><code>import os
import boto3

def download_s3_folder(bucket_name, prefix, local_dir, exts=('.jpeg', '.jpg', '.png')):
    # Download jpeg, jpg, png files by default
    s3 = boto3.client('s3')
    os.makedirs(local_dir, exist_ok=True)
    paginator = s3.get_paginator('list_objects_v2')

    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
        for item in page.get('Contents', []):
            key = item['Key']
            if key.endswith('/') or not key.lower().endswith(exts):
                continue
            local_path = os.path.join(local_dir, os.path.relpath(key, prefix))
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            print(f&quot;Downloading: s3://{bucket_name}/{key} -&gt; {local_path}&quot;)
            s3.download_file(bucket_name, key, local_path)

# Only download .jpeg files
download_s3_folder(BUCKET_NAME_test, PREFIX_test, 'local_data/train', exts=('.jpeg',))

dataset_train = datasets.ImageFolder('local_data/train', transform=transform_train)
</code></pre>
<p>If downloading files to local storage is not feasible, you might consider moving the training process to AWS and running it on SageMaker.</p>
<p>In your train.py, you can use the S3 bucket as a data input channel, which SageMaker will automatically mount at the specified directory (e.g., /opt/ml/data/train). Then, <code>ImageFolder</code> should be able to load the dataset directly from that mounted directory:</p>
<pre class=""lang-py prettyprint-override""><code>data_dir = '/opt/ml/data/train'  # S3 mount point provided by SageMaker
dataset = datasets.ImageFolder(data_dir, transform=transform_train)
</code></pre>
","0","Answer"
"79663181","79662912","<p>Try replacing the sinusoidal positional encoding with a learnable version using <code>nn.Embedding</code>. The original Transformer uses fixed (sinusoidal) positional encoding, which isn't learnable and might not adapt well to your specific dataset.</p>
<p>A learnable positional encoding can allow the model to better capture task-specific position patterns, potentially leading to improved performance.</p>
<p>Here’s a modified version of your model with learnable positional encoding:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

# Learnable Positional Encoding
class LearnablePositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=512):
        super().__init__()
        self.pos_embedding = nn.Embedding(max_len, d_model)

    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)
        pos_embed = self.pos_embedding(positions)
        return x + pos_embed

# Transformer-based Classifier with learnable position encoding
class TransformerClassifier(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=2, num_classes=15, max_len=512):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = LearnablePositionalEncoding(d_model, max_len=max_len)

        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.classifier = nn.Linear(d_model, num_classes)

    def forward(self, src):
        x = self.embedding(src)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        return self.classifier(x[:, 0])

</code></pre>
","1","Answer"
"79663990","79663977","<p>In Python, when you see <code>**config_dict</code> (or, way more often, <code>**kwargs</code>) inside a function's definition, it means the function can accept any number of <code>keyword arguments</code>. The actual name <code>kwargs</code> is just a convention, the important part is the <code>**</code> operator.<br></p>
<p>So the <code>set_config</code> method in <code>sktime</code> expects <code>keyword arguments</code> directly, not a dictionary as a single positional argument.</p>
<p>Try this</p>
<pre><code>transformer.set_config(backend__parallel=&quot;loky&quot;) 
</code></pre>
<p>Use the double underscore for nested parameters like <code>backend:parallel</code>.</p>
","0","Answer"
"79665678","79657687","<p>You shouldn't use a variable that comes from the future. You can only look at it for inspiration when you are designing the model.</p>
","0","Answer"
"79666151","79663911","<p>Let us demonstrate with a sample time series data (with linear trend) generated randomly.</p>
<p>Let us fit</p>
<ul>
<li><p>a linear regression model without weights</p>
</li>
<li><p>another one with sample weights and</p>
</li>
<li><p>the third one with the weights simulated by repeating datapoints (assuming that weights are integers).</p>
</li>
</ul>
<p>Note that the intercept and slope estimated by the linear regression model with explicitly provided sample weights is same as the one with weights simulated by repeating datapoints, as expected, but they are different from the ones obtained using the unweighted regression, since the datapoints with high weight values turn out to be influential points often.</p>
<p>The <code>matplotlib</code> scatterplot uses the size and the color of the points to visualize the weight values.</p>
<pre><code>from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pylab as plt

np.random.seed(11) # reproducibility
n = 1000           # generate n data points
t = np.array(range(n)) # generate time series data with linear trend
y = 0.5*t + 5 + 50*np.random.randn(n)
w = np.ones(n) # weights, mostly ones, with a few higher weight values
w[np.random.choice(range(n), 25)] = np.random.choice([2**i for i in range(4,9)], 25)
plt.scatter(t, y, s=w, c=w, cmap='seismic')
plt.colorbar()

# linear regression without weights
regr = LinearRegression()
regr.fit(t.reshape(-1,1), y)

# Linear regression with repeated data points (proportional to corresponding weights)
tr = np.repeat(t, w.astype(int))
yr = np.repeat(y, w.astype(int))
regrr = LinearRegression()
regrr.fit(tr.reshape(-1,1), yr)
# same as solving the following normal equation:
X = np.concatenate((np.ones(len(tr)).reshape(-1,1),tr.reshape(-1,1)), axis=1)
np.linalg.solve(X.T@X, X.T@yr)
# array([26.70301261,  0.48131083])

# Linear regression with sample weights
regrw = LinearRegression()
regrw.fit(t.reshape(-1,1), y, sample_weight=w)

print(regr.intercept_, regr.coef_[0])
# 4.649131751216686 0.5000052026805413
print(regrw.intercept_, regrw.coef_[0])
# 26.703012605026487 0.4813108312042462
print(regrr.intercept_, regrr.coef_[0])
# 26.70301260502677 0.48131083120424567

# plot
plt.plot(t, regr.predict(t.reshape(-1,1)), color='green', linewidth=3, label='unweighted')
plt.plot(tr, regrr.predict(tr.reshape(-1,1)), color='black', linewidth=3, label='repeated')
plt.plot(t, regrw.predict(t.reshape(-1,1)), color='red', linewidth=3, label='weighted')
plt.legend()
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/gYrP41JI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gYrP41JI.png"" alt=""enter image description here"" /></a></p>
<p>Compare the time taken to run linear regression with repeated samples vs. using sample weights:</p>
<pre><code>import timeit
# with repeated samples
print(timeit.timeit(&quot;tr, yr = np.repeat(t, w.astype(int)), np.repeat(y, w.astype(int)); LinearRegression().fit(tr.reshape(-1,1), yr)&quot;, 
                    &quot;import numpy as np; from sklearn.linear_model import LinearRegression;n=1000;np.random.seed(11);t = np.array(range(n));y = 0.5*t + 5 + 50*np.random.randn(n);w = np.ones(n);w[np.random.choice(range(n), 25)] = np.random.choice([2**i for i in range(4,9)], 25)&quot;,
                   number=1000))
# 1.0433578000229318

# with sample weights
print(timeit.timeit(&quot;LinearRegression().fit(t.reshape(-1,1), y, sample_weight=w)&quot;,
                   &quot;import numpy as np; from sklearn.linear_model import LinearRegression;n=1000;np.random.seed(11);t = np.array(range(n));y = 0.5*t + 5 + 50*np.random.randn(n);w = np.ones(n);w[np.random.choice(range(n), 25)] = np.random.choice([2**i for i in range(4,9)], 25)&quot;,
                   number=1000))
# 0.8825260000012349
</code></pre>
","0","Answer"
"79666186","79663911","<p>Below I use the ordinate of each point as the weight, so I expect to see a &quot;better&quot; fit in the zone with high ordinates:</p>
<p><a href=""https://i.sstatic.net/QsRmKZ7n.png"" rel=""nofollow noreferrer"" title=""Two different linear fits.""><img src=""https://i.sstatic.net/QsRmKZ7n.png"" alt=""two regressions"" title=""Two different linear fits."" /></a></p>
<p>and this is shown in the figure above.</p>
<p>Here it's my code, I have used the <code>numpy.polynomial.Polynomial</code> class (introduced in 2009 but still not very much loved…).</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt

np.random.seed(20250614)
x = np.sort(10 + np.random.rand(20) * 10)  # inside 10 t0 20
y = np.sin((x - 9) / 7)
plt.plot(x, y, &quot;o&quot;, label=&quot;data&quot;)

lin0 = np.polynomial.Polynomial.fit(x, y, deg=1)
x0, y0 = lin0.linspace(20)
plt.plot(x0, y0, marker=&quot;.&quot;, label=&quot;unweighted&quot;)

linw = np.polynomial.Polynomial.fit(x, y, deg=1, w=y)
xw, yw = linw.linspace(20)
plt.plot(xw, yw, marker=&quot;.&quot;, label=&quot;weighted&quot;)

plt.legend()

plt.show()
</code></pre>
","0","Answer"