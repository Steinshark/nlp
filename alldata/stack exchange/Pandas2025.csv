Post Id,Parent Id,Body,Score,PostType
"79321942","","<pre><code># Setup two dataframes
population_2024 = [
    [&quot;Jamaica&quot;, 2.826],
    [&quot;Japan&quot;, 124.5],
]

population_2024 = pd.DataFrame(
    columns=[&quot;Country&quot;, &quot;Population (M)&quot;], data=population_2024
)

new_murders = [
    [&quot;Jamaica&quot;, 2023, 1393],
    [&quot;Jamaica&quot;, 2024, 1138],
    [&quot;Japan&quot;, 2023, 912],
    [&quot;Japan&quot;, 2024, 912],
]

new_murders_df = pd.DataFrame(
    columns=[&quot;Country&quot;, &quot;Year&quot;, &quot;Number of murders&quot;], data=new_murders
)

# this line fails because of: population_2024.query(&quot;Country==@row['Country']&quot;)

new_murders_df[&quot;Homicide rate per 100,000&quot;] = new_murders_df.apply(
    lambda row: (row[&quot;Number of murders&quot;] / population_2024.query(&quot;Country==@row['Country']&quot;)[&quot;Population (M)&quot;]) * 100000,
    axis=1
)
</code></pre>
<p>Is there a way to refer to an element of a variable within a pandas query string? The <code>@</code> symbol allows me to refer to the variable row, but I am unable to access an element of row within the query.</p>
<p>I am expecting <code>population_2024.query(&quot;Country==@row['Country']&quot;)</code> to return the relevant Country row i.e. the first return in this loop should be Jamaica.</p>
","1","Question"
"79322295","","<p>I need help for my Dissertation Project. I am working on a Python project as part of my Masters Degree at my university in England, UK. The dataset I have gotten through the Kaggle platform which contains over one million movies in terms of their titles, budget, box-office revenue, genres, popularity, reviews, keywords etc. Here is the weblink for clarification (I got the latest update):</p>
<p><a href=""https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies</a></p>
<p><strong>UPDATE</strong>: The cleaning of the data is completed but I need help in collaboration with data visualizations for my Exploratory Data Analysis (EDA) because as the dataset is larger for memory at 504mb it has created ridiculous visualizations from the data. I am very familiar with <code>matplotlib</code> and <code>seaborn</code> functions for Python.</p>
<p>The simple codes I used for count plots for part of this data was:</p>
<pre><code>plt.figure(figsize=(20,16))
sns.countplot(x = 'Genres', data=df2)  
plt.xlabel('Genres', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/0kMfUrEC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0kMfUrEC.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone know how I can create clear and concise data visualizations for specific parts of the data in terms of Bivariate, Univariate and Multivariate analysis?</p>
<p>For example:</p>
<p><em>Most popular movie genres/movie production companies in terms of frequency</em></p>
<p><em>Top movies with biggest box office revenue/budgets, popularity etc.</em></p>
<p><em>Countries with biggest number of production/distribution movies made.</em></p>
<p>And much more. Anything would help. Thank you very much in advance.</p>
","1","Question"
"79323746","","<p>Here is a sample data, even there is no negative or np.nan, it still show error message:</p>
<p>Data:</p>
<pre><code>   gvkey  sale  ebit
4   1000  44.8  16.8
5   1000  53.2  11.5
6   1000  42.9   6.2
7   1000  42.4   0.9
8   1000  44.2   5.3
9   1000  51.9   9.7
</code></pre>
<p>Function:</p>
<pre><code>def calculate_ln_values(df):
    conditions_ebit = [
        df['ebit'] &gt;= 0.0,
        df['ebit'] &lt;  0.0
    ]
    choices_ebit = [
        np.log(1 + df['ebit']),
        np.log(1 - df['ebit']) * -1
    ]
    df['lnebit'] = np.select(conditions_ebit, choices_ebit, default=np.nan)
    
    conditions_sale = [
        df['sale'] &gt;= 0.0,
        df['sale'] &lt;  0.0
    ]
    choices_sale = [
        np.log(1 + df['sale']),
        np.log(1 - df['sale']) * -1
    ]
    df['lnsale'] = np.select(conditions_sale, choices_sale, default=np.nan)
    return df
</code></pre>
<p>Run</p>
<pre><code>calculate_ln_values(data)
</code></pre>
<p>Error Warning:</p>
<pre><code>C:\Users\quoc\anaconda3\envs\uhart\Lib\site-packages\pandas\core\arraylike.py:399: RuntimeWarning: invalid value encountered in log
  result = getattr(ufunc, method)(*inputs, **kwargs)
C:\Users\quoc\anaconda3\envs\uhart\Lib\site-packages\pandas\core\arraylike.py:399: RuntimeWarning: invalid value encountered in log
  result = getattr(ufunc, method)(*inputs, **kwargs)
</code></pre>
<p>I would very appreciate if someone could help me this issue</p>
<p>---- Edit: reply to Answer of @Emi OB and @Quang Hoang: ---------------</p>
<p>The formula as in the paper is:</p>
<p><a href=""https://i.sstatic.net/UDHZnHaE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UDHZnHaE.png"" alt=""enter image description here"" /></a></p>
<p>ln(1+EBIT) if EBIT ≥ 0</p>
<p>-ln(1-EBIT) if EBIT &lt; 0</p>
<p>so my code:</p>
<pre><code>np.log(1 + df['ebit']),
np.log(1 - df['ebit']) * -1
</code></pre>
<p>follows the paper.</p>
<p>The part <code>np.log(1 - df['ebit'])</code> is impossible to be negative since it fall under the condition of <code>ebit &lt; 0</code>.</p>
","0","Question"
"79325079","","<p>I have a large text file, with a header of 18 lines.</p>
<p>If I try to display the entire dataframe:</p>
<pre><code>df = pd.read_csv('my_log')
print(df)
</code></pre>
<p>I get:
<code>pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 19, saw 3</code></p>
<p>If I try to use exclude the header:</p>
<pre><code>df = pd.read_csv('my_log', header=18)
</code></pre>
<p>I get the first row (line 19), then the second row (showing indexed at 0)
No matter which index number I use in: <code>print(df.loc[[0]])</code>, I always get that first row displayed (no index number) before the row that I want.</p>
<p>I've checked out the text file, and every row ends in a CR/LF. I've also completely removed line 19; but, the same behavior occurs.</p>
<p>Also, if I completely remove the header and print the entire dataframe, I still get the same behavior. The first row prints (without an index number) and the row count is 1 less than the true row count.</p>
<p>Any suggestions greatly appreciated!</p>
","-1","Question"
"79325094","","<p>I try to create a visualization about the 'Number Of Cafes Within A 200 M Radius Of The <a href=""https://en.wikipedia.org/wiki/TransJakarta"" rel=""nofollow noreferrer"">Transjakarta</a> Route' using OSMNX. I do my work <a href=""https://github.com/lokalhangatt/stackoverlow/blob/main/Tugas.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>
<p><a href=""https://i.sstatic.net/6vCeGgBM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6vCeGgBM.png"" alt=""enter image description here"" /></a></p>
<p>But I have some questions here:</p>
<ol>
<li>Yes, I plot the Transjakarta route, plot the cafe, inside the map of Jakarta. But I want to remove all the cafe's plots that are far away from Transjakarta's line. I just want to show the cafe that intersects with Transjakarta's line because the radius range is only 200m from Transjakarta's line. How can I do that?</li>
<li>How can I show the legend title?</li>
</ol>
<p>Thank you.</p>
","0","Question"
"79325219","","<p>I am trying to scrape event links and contact information from the RaceRoster website (<a href=""https://raceroster.com/search?q=5k&amp;t=upcoming"" rel=""nofollow noreferrer"">https://raceroster.com/search?q=5k&amp;t=upcoming</a>) using Python, requests, Pandas, and BeautifulSoup. The goal is to extract the Event Name, Event URL, Contact Name, and Email Address for each event and save the data into an Excel file so we can reach out to these events for business development purposes.</p>
<p>However, the script consistently reports that no event links are found on the search results page, despite the links being visible when inspecting the HTML in the browser. Here’s the relevant HTML for the event links from the search results page:</p>
<pre><code>&lt;a href=&quot;https://raceroster.com/events/2025/98542/13th-annual-delaware-tech-chocolate-run-5k&quot; 
   target=&quot;_blank&quot; 
   rel=&quot;noopener noreferrer&quot; 
   class=&quot;search-results__card-event-name&quot;&gt;
    13th Annual Delaware Tech Chocolate Run 5k
&lt;/a&gt;
</code></pre>
<p>Steps Taken:</p>
<ol>
<li>Verified the correct selector for event links:</li>
</ol>
<pre><code>soup.select(&quot;a.search-results__card-event-name&quot;)
</code></pre>
<ol start=""2"">
<li><p>Checked the response content from the requests.get() call using soup.prettify(). The HTML appears to lack the event links that are visible in the browser, suggesting the content may be loaded dynamically via JavaScript.</p>
</li>
<li><p>Attempted to scrape the data using BeautifulSoup but consistently get:</p>
</li>
</ol>
<pre><code>Found 0 events on the page.
Scraped 0 events.
No contacts were scraped.
</code></pre>
<p>What I Need Help With:</p>
<ul>
<li>How can I handle this JavaScript-loaded content? Is there a way to scrape it directly, or do I need to use a tool like Selenium?</li>
<li>If Selenium is required, how do I properly integrate it with BeautifulSoup for parsing the rendered HTML?</li>
</ul>
<p>Current Script:</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_event_contacts(base_url, search_url):
    headers = {
        &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&quot;
    }
    event_contacts = []

    # Fetch the main search page
    print(f&quot;Scraping page: {search_url}&quot;)
    response = requests.get(search_url, headers=headers)

    if response.status_code != 200:
        print(f&quot;Failed to fetch page: {search_url}, status code: {response.status_code}&quot;)
        return event_contacts

    soup = BeautifulSoup(response.content, &quot;html.parser&quot;)
    # Select event links
    event_links = soup.select(&quot;a.search-results__card-event-name&quot;)


    print(f&quot;Found {len(event_links)} events on the page.&quot;)

    for link in event_links:
        event_url = link['href']
        event_name = link.text.strip()  # Extract Event Name

        try:
            print(f&quot;Scraping event: {event_url}&quot;)
            event_response = requests.get(event_url, headers=headers)
            if event_response.status_code != 200:
                print(f&quot;Failed to fetch event page: {event_url}, status code: {event_response.status_code}&quot;)
                continue

            event_soup = BeautifulSoup(event_response.content, &quot;html.parser&quot;)

            # Extract contact name and email
            contact_name = event_soup.find(&quot;dd&quot;, class_=&quot;event-details__contact-list-definition&quot;)
            email = event_soup.find(&quot;a&quot;, href=lambda href: href and &quot;mailto:&quot; in href)

            contact_name_text = contact_name.text.strip() if contact_name else &quot;N/A&quot;
            email_address = email['href'].split(&quot;mailto:&quot;)[1].split(&quot;?&quot;)[0] if email else &quot;N/A&quot;

            if contact_name or email:
                print(f&quot;Found contact: {contact_name_text}, email: {email_address}&quot;)
                event_contacts.append({
                    &quot;Event Name&quot;: event_name,
                    &quot;Event URL&quot;: event_url,
                    &quot;Event Contact&quot;: contact_name_text,
                    &quot;Email&quot;: email_address
                })
            else:
                print(f&quot;No contact information found for {event_url}&quot;)
        except Exception as e:
            print(f&quot;Error scraping event {event_url}: {e}&quot;)

    print(f&quot;Scraped {len(event_contacts)} events.&quot;)
    return event_contacts

def save_to_spreadsheet(data, output_file):
    if not data:
        print(&quot;No data to save.&quot;)
        return
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)
    print(f&quot;Data saved to {output_file}&quot;)

if __name__ == &quot;__main__&quot;:
    base_url = &quot;https://raceroster.com&quot;
    search_url = &quot;https://raceroster.com/search?q=5k&amp;t=upcoming&quot;
    output_file = &quot;/Users/my_name/Documents/event_contacts.xlsx&quot;

    contact_data = scrape_event_contacts(base_url, search_url)
    if contact_data:
        save_to_spreadsheet(contact_data, output_file)
    else:
        print(&quot;No contacts were scraped.&quot;)
</code></pre>
<p>Expected Outcome:</p>
<ul>
<li>Extract all event links from the search results page.</li>
<li>Navigate to each event’s detail page.</li>
<li>Scrape the contact name () and email () from the detail page.</li>
<li>Save the results to an Excel file.</li>
</ul>
","3","Question"
"79325561","","<p>I'm trying to take the value from the last row of a df col and replace it with the first value. I'm returning a value error.</p>
<pre><code>import pandas as pd
df = pd.DataFrame({'name': ['tom','jon','sam','jane','bob'],
       'age': [24,25,18,26,17],
       'Notification': [np.nan,'2025-01-03 14:19:35','2025-01-03 14:19:39','2025-01-03 14:19:41','2025-01-03 14:19:54'],
       'sex':['male','male','male','female','male']})

df_test = df.copy()

df_test['Notification'] = pd.to_datetime(df_test['Notification'])

df_test['Notification'].iloc[0] = df_test['Notification'].tail(1)
</code></pre>
<p>Error:</p>
<pre><code>ValueError: Could not convert object to NumPy datetime
</code></pre>
","2","Question"
"79325633","","<p>I'm working on a machine learning project in Python. Using pandas <code>pd.get_dummies</code> I'm trying to create dummy variables for a categorical column in my data but the variables are being converted to booleans rather than integers and it is making it impossible for statsmodels to fit an OLS model. I've tried to convert the boolean into an integer but I keep getting errors. How can I solve this??</p>
<p>I used the <code>.astype(int)</code> method, I also tried using numpy to convert to an integer.</p>
<pre class=""lang-py prettyprint-override""><code>ocean_proximity_dummies = pd.get_dummies(data['ocean_proximity'], prefix= 'ocean_proximity')

data = pd.concat([data.drop(&quot;ocean_proximity&quot;, axis =1), ocean_proximity_dummies], axis=1)

ocean_proximity_dummies = ocean_proximity_dummies.astype(int)
ocean_proximity_dummies
</code></pre>
","-2","Question"
"79326029","","<p>I am storing stock prices for different entities as separate feather files in S3 bucket. On high level, the content of any feather file looks like below.</p>
<pre><code>month | value | observation |
-----------------------------
2024-01 | 12 | High 

2024-01 | 5 | Low
</code></pre>
<p>A lambda function written in python uses pandas to update this data - insert new rows, update existing rows, delete rows etc.</p>
<p>Each day when new prices are received for a given entity, the existing code reads the feather file for this entity into memory (using pandas) and concatenates the incoming new data and then writes back the updated feather file from memory back to S3. This is working fine for now but as the size of these feather file grows, we are seeing &quot;out of memory&quot; exceptions in some cases when lambda tries to load a large feather file into memory during merge operations. This is the case when I have assigned 10 GB(max) memory to the lambda.</p>
<p>All the supported operations - merge, update, delete are done in memory once the files are loaded fully into memory.</p>
<p>Is there a better way or another library which can help me to do these merges/ other operations without loading everything in memory ? I check duckDB and looks like it supports predicate pushdowns to storage level but it doesn't support feather files natively.</p>
<p>Looking for any other ideas to approach this problem.</p>
<p>Thanks</p>
<h2>Update</h2>
<p>We are doing date partition by year on feather files. That makes the merge operation slow as i have to touch multiple partitions in case the incoming data (manual load in this case) has datapoints from different years.</p>
<p>Also, when the user may ask for data which ranges multiple years...for eg the query might say -&gt; give me all data with &quot;High&quot; observation, i still need to visit multiple partitions which may slow down things.</p>
","1","Question"
"79326157","","<p>I want to reorder columns name if the columns have the same part name. Sample as below:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    'Branch': ['Hanoi'],
    '20241201_Candy': [3], '20241202_Candy': [4], '20241203_Candy': [5],
    '20241201_Candle': [3], '20241202_Candle': [4], '20241203_Candle': [5],
    '20241201_Biscuit': [3], '20241202_Biscuit': [4], '20241203_Biscuit': [5]})
</code></pre>
<p>Below is my Expected Ouput:</p>
<pre><code>df2 = pd.DataFrame({
    'Branch': ['Hanoi'],
    '20241201_Biscuit': [3], '20241201_Candle': [3], '20241201_Candy': [3],
    '20241202_Biscuit': [4], '20241202_Candle': [4], '20241202_Candy': [4],
    '20241203_Biscuit': [5], '20241203_Candle': [5], '20241203_Candy': [5]})
</code></pre>
<p>So I want to auto reorder dataframe if it has same date.</p>
","0","Question"
"79327355","","<p>I have a pandas Series containing strictly non-negative integers like so:</p>
<pre><code>1
2
3
4
5
</code></pre>
<p>I want to convert them into n-bits binary representation <strong>based on the largest value</strong>. For example, the largest value here is 5, so we would have 3 bits/3 columns, and the resulting series would be something like this</p>
<pre><code>0 0 1
0 1 0
0 1 1
1 0 0
1 0 1
</code></pre>
<p>Thanks a lot in advance!</p>
","2","Question"
"79327563","","<p>When having multiple df's to which similar changes need to be made I usually tend to use a for loop as a quick solution to avoid repetitive work.</p>
<p>I understand that the df change happening in the loop is being applied to a copy. So for some methods I can overcome that with the 'inplace' parameter where available. Unfortunately for things like filtering I don't seem to have such an option. Below are examples of a code where I try to filter on rows where the date is higher or equal then the variable 'now'. Regarding the same filtering outside of the loop it will work correctly so it's not the matter of incorrect dtype or lack of data to be filtered out. Below are my code examples and would like to learn if the incorrect result is due to my poor knowledge or there are specific limitations and if so how do more experience users tackle this:</p>
<p>original code:</p>
<pre><code>now = pd.to_datetime(date.today(), format='%Y%m%d')

for df in df_list:
    df.dropna(how='all', axis=1, inplace=True)
    df['Valid From Converted'] = pd.to_datetime(df['Valid From'], format='%Y%m%d')
    df['to Converted'] = pd.to_datetime(df['to'], format='%Y%m%d')
    df = df[df['to Converted'] &gt;= now]
</code></pre>
<p>enumerate approach:</p>
<pre><code>now = pd.to_datetime(date.today(), format='%Y%m%d')

for i, df in enumerate(df_list):
    df.dropna(how='all', axis=1, inplace=True)
    df['Valid From Converted'] = pd.to_datetime(df['Valid From'], format='%Y%m%d')
    df['to Converted'] = pd.to_datetime(df['to'], format='%Y%m%d')
    df = df[df['to Converted'] &gt;= now]
    df_list[i] = df
</code></pre>
<p>range approach:</p>
<pre><code>for i in range(len(df_list)):
    df_list[i].dropna(how='all', axis=1, inplace=True)
    df_list[i]['Valid From Converted'] = pd.to_datetime(df_list[i]['Valid From'], format='%Y%m%d')
    df_list[i]['to Converted'] = pd.to_datetime(df_list[i]['to'], format='%Y%m%d')
    df_list[i] = df_list[i][df_list[i]['to Converted'] &gt;= now]
</code></pre>
","0","Question"
"79327761","","<p>I am trying to go through a large number of files in my folder and read them into a dataframe ready to do some NLP on them. Some are .pdf, .xlsx, .txt, etc. However, I am having the biggest issue with some documents which are .msg. I have been able to use the <code>extract-msg</code> package with moderate success, however it is really struggling to process emails which have attachments, specifically if those attachments happen to be emails themselves.</p>
<p>I have built this function, and it works for when an attachment is anything but an email, however it does not process the email that has an email attachment:</p>
<pre><code>import os
import extract_msg
import pandas as pd
import pdfplumber
from docx import Document
import re
import glob

# Function to process emails
def process_msg(file_path, input_folder):
    try:
        msg = extract_msg.openMsg(file_path)
        email_content = {
            &quot;sender&quot;: msg.sender,
            &quot;subject&quot;: msg.subject,
            &quot;received_time&quot;: msg.receivedTime,
            &quot;body&quot;: msg.body or msg.htmlBody or &quot;No content available&quot;
        }
        
        attachments_data = [] 
        attachments_folder = os.path.join(input_folder, &quot;attachments&quot;)
        os.makedirs(attachments_folder, exist_ok=True)
        
        for att in msg.attachments:
            att_name = get_unique_filename(sanitize_filename(att.name), input_folder)
            att_ext = re.search(r&quot;\.(\w+)$&quot;, att_name)
            att_ext = att_ext.group(1).lower() if att_ext else None
            
            saved_path = os.path.join(attachments_folder, att_name)
            att.save(customPath=attachments_folder, customFilename=att_name)
            
            if att_ext == &quot;msg&quot;:
                attachment_content  = process_msg(saved_path, input_folder)
                attachments_data.append({
                    &quot;type&quot;: &quot;email&quot;,
                    &quot;name&quot;: att_name,
                    &quot;content&quot;: attachment_content
                })

            # This specifically removes .png and .jpg files from the attachments folder.
            else:
                print(f&quot;Unsupported attachment type: {att_name}&quot;)
                os.remove(saved_path)
                attachments_data.append({
                    &quot;type&quot;: &quot;unsupported&quot;,
                    &quot;name&quot;: att_name,
                    &quot;content&quot;: f&quot;Unsupported file type: {att_ext}&quot;
                })

        return {
            &quot;email_content&quot;: email_content,
            &quot;attachments&quot;: attachments_data
        }    
    
    except Exception as e:
        print(f&quot;Error processing email {file_path}: {e}&quot;)
        return {
            &quot;email_content&quot;: &quot;Error processing email&quot;,
            &quot;attachments&quot;: [],
            &quot;error&quot;: str(e)
        }
</code></pre>
<p>Ultimately, I would like a table that resembles this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Doc ID</th>
<th>Data</th>
<th>File path</th>
<th>Attachment</th>
<th>ID of Attachment</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Email 1</td>
<td>Filepath 1</td>
<td>N</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Word1</td>
<td>Filepath 2</td>
<td>Y</td>
<td>Email 1</td>
</tr>
<tr>
<td>3</td>
<td>PDF1</td>
<td>Filepath 3</td>
<td>N</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>Email 2</td>
<td>Filepath 4</td>
<td>Y</td>
<td>Email 1</td>
</tr>
<tr>
<td>5</td>
<td>Email 3</td>
<td>Filepath 5</td>
<td>Y</td>
<td>Email 2</td>
</tr>
</tbody>
</table></div>
<p>Where Email 1 had 2 attachments, a Word document (word 1) and an email attachment (email 2). Also email 2 had an attachment (Email 3).</p>
","0","Question"
"79327904","","<p>I am trying to use Pandas to extract data from an Excel file.  Sample file:</p>
<p><a href=""https://i.sstatic.net/H3ckhU0O.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H3ckhU0O.png"" alt=""enter image description here"" /></a><br />
I want to be able to read an entire spreadsheet with a separate tab for each month (done) and then populate a tkinter window with a table that shows (for example) heating, electricity, and water in separate columns by month.</p>
<p>I imported the entire spreadsheet and then ran a filter to break it down to only the utilities that contain the data that I want to summarize, but I can not figure out how to extract the &quot;expense&quot; and place it in the proper column on the page.</p>
<p>My vision is that the resulting table would look similar to this:</p>
<pre class=""lang-none prettyprint-override""><code>            Electricity  Heating  Water
January     54.00        100.00   32.00
February    60.00        105.00   25.00
March       57.00         97.00   30.00 
</code></pre>
<p>I am using &quot;Alpha&quot; as a numerical representation of the month for sorting purposes, but not knowing which line of any given month contains the electricity, or heating, in my example, I can't figure out how to extract the data.  I will be running this report throughout the year, so the starting df size (length) will vary, plus my water is only billed every other month.</p>
<p>I could provide pages of failed code attempts but didn't think that would be helpful.</p>
<p>Thank you in advance for any help.</p>
","0","Question"
"79329759","","<p>So, i'm a beginner at the Pandas Python and noticed the interpolate function is pretty interesting, but i have one problem when using the line:</p>
<pre><code>result = df.interpolate(method='linear')
</code></pre>
<p>I found out that even though it did filled a lot of the NaN's in 'df', the four first NaN's of Ambient_temp and the first NaN on the Intake_temp column are not filled the way i wanted. Any hints on how to get this working? The interpolation worked very well with every other column besides those two.</p>
<p><a href=""https://i.sstatic.net/ngLgo9PN.png"" rel=""nofollow noreferrer"">Image of said dataframe</a></p>
<p>Example:</p>
<pre><code>amb_temp = [np.nan, np.nan, 32, 32]
in_temp = [ 29, 27, 23, 22]
volts = [np.nan, 13, 11, 11]

dict = {'ambient_temperature': amb_temp, 'temperature_inside': in_temp, 'volts': volts} 

df = pd.DataFrame(dict)
</code></pre>
<p>(it's not exactly the same dataframe, but encapsulates the same problem. I got this one based off and example on 'geeksforgeeks' and used numpy.nan to simulate the absence of data.)</p>
","1","Question"
"79329930","","<p>I am trying to plot a time series with <strong>plt.plot()</strong> but I am constantly observing some strange output. A sample of the dataset I am providing below (the whole dataset has roughly 150,000 entries). One of the columns of this dataset consists of time values and serves as an index. Depending on whether I convert the index from a string to a datetime object or not, I am getting two different outputs when plotting against the target variable.</p>
<p><strong>Scenario 1:</strong> string index</p>
<pre><code>import matplotlib.pyplot as plt
import pandas as pd

sample_df = pd.DataFrame()
sample_df[&quot;Date&quot;] = [ &quot;2002-12-31 01:00:00&quot;, &quot;2002-06-06 10:00:00&quot;, &quot;2003-11-10 19:00:00&quot;,  
                      &quot;2003-04-15 04:00:00&quot;, &quot;2004-09-19 14:00:00&quot;, &quot;2004-02-24 23:00:00&quot;,  
                      &quot;2005-07-30 08:00:00&quot;, &quot;2005-01-03 17:00:00&quot;, &quot;2006-06-08 02:00:00&quot;,  
                      &quot;2007-11-12 11:00:00&quot;, &quot;2007-04-18 20:00:00&quot;, &quot;2008-09-21 06:00:00&quot;,  
                      &quot;2008-02-26 15:00:00&quot;, &quot;2009-08-03 00:00:00&quot;, &quot;2009-01-05 09:00:00&quot;,  
                      &quot;2010-06-11 19:00:00&quot;, &quot;2011-11-14 04:00:00&quot;, &quot;2011-04-20 13:00:00&quot;,  
                      &quot;2012-09-24 23:00:00&quot;, &quot;2012-02-28 08:00:00&quot;, &quot;2013-08-04 17:00:00&quot;,  
                      &quot;2013-01-07 02:00:00&quot;, &quot;2014-06-13 09:00:00&quot;, &quot;2015-11-17 18:00:00&quot;,
                      &quot;2015-04-22 01:00:00&quot;, &quot;2016-09-26 09:00:00&quot;, &quot;2016-03-02 18:00:00&quot;,
                      &quot;2017-08-06 01:00:00&quot;, &quot;2017-01-10 10:00:00&quot;, &quot;2018-01-16 19:00:00&quot; ]
sample_df[&quot;Energy_MW&quot;] = [ 26498.0, 39167.0, 36614.0, 21837.0, 26644.0,
                           33574.0, 30255.0, 33781.0, 24344.0, 34708.0,
                           33996.0, 21127.0, 36255.0, 31982.0, 35448.0,
                           37066.0, 22116.0, 31326.0, 26569.0, 33565.0,
                           34649.0, 25709.0, 33516.0, 33032.0, 22333.0,
                           28064.0, 33905.0, 25304.0, 41505.0, 39543.0 ]
sample_df = sample_df.set_index(&quot;Date&quot;)

# Basic plot.
fig = plt.figure( figsize = (10,5) )
plt.plot(sample_df.index, sample_df[&quot;Energy_MW&quot;], 'b')
plt.grid(True)
plt.show()
</code></pre>
<p>This is the plot corresponding to a string index:</p>
<p><img src=""https://i.sstatic.net/yNAsYK0w.png"" alt=""Plot corresponding to a string index"" /></p>
<p><strong>Scenario 2:</strong> datetime index</p>
<pre><code>import matplotlib.pyplot as plt
import pandas as pd

sample_df = pd.DataFrame()
sample_df[&quot;Date&quot;] = [ &quot;2002-12-31 01:00:00&quot;, &quot;2002-06-06 10:00:00&quot;, &quot;2003-11-10 19:00:00&quot;,  
                      &quot;2003-04-15 04:00:00&quot;, &quot;2004-09-19 14:00:00&quot;, &quot;2004-02-24 23:00:00&quot;,  
                      &quot;2005-07-30 08:00:00&quot;, &quot;2005-01-03 17:00:00&quot;, &quot;2006-06-08 02:00:00&quot;,  
                      &quot;2007-11-12 11:00:00&quot;, &quot;2007-04-18 20:00:00&quot;, &quot;2008-09-21 06:00:00&quot;,  
                      &quot;2008-02-26 15:00:00&quot;, &quot;2009-08-03 00:00:00&quot;, &quot;2009-01-05 09:00:00&quot;,  
                      &quot;2010-06-11 19:00:00&quot;, &quot;2011-11-14 04:00:00&quot;, &quot;2011-04-20 13:00:00&quot;,  
                      &quot;2012-09-24 23:00:00&quot;, &quot;2012-02-28 08:00:00&quot;, &quot;2013-08-04 17:00:00&quot;,  
                      &quot;2013-01-07 02:00:00&quot;, &quot;2014-06-13 09:00:00&quot;, &quot;2015-11-17 18:00:00&quot;,
                      &quot;2015-04-22 01:00:00&quot;, &quot;2016-09-26 09:00:00&quot;, &quot;2016-03-02 18:00:00&quot;,
                      &quot;2017-08-06 01:00:00&quot;, &quot;2017-01-10 10:00:00&quot;, &quot;2018-01-16 19:00:00&quot; ]
sample_df[&quot;Energy_MW&quot;] = [ 26498.0, 39167.0, 36614.0, 21837.0, 26644.0,
                           33574.0, 30255.0, 33781.0, 24344.0, 34708.0,
                           33996.0, 21127.0, 36255.0, 31982.0, 35448.0,
                           37066.0, 22116.0, 31326.0, 26569.0, 33565.0,
                           34649.0, 25709.0, 33516.0, 33032.0, 22333.0,
                           28064.0, 33905.0, 25304.0, 41505.0, 39543.0 ]
sample_df = sample_df.set_index(&quot;Date&quot;)
sample_df.index = pd.to_datetime(sample_df.index)

# Basic plot.
fig = plt.figure( figsize = (10,5) )
plt.plot(sample_df.index, sample_df[&quot;Energy_MW&quot;], 'b')
plt.grid(True)
plt.show()
</code></pre>
<p>And this is the plot corresponding to a datetime index:</p>
<p><img src=""https://i.sstatic.net/poyLjsfg.png"" alt=""Plot corresponding to a datetime index"" /></p>
<p>Why does the conversion to a datetime object drastically make the results worse? How to resolve the issue? Please explain in simple words. I tried to obtain graphical results in both of the scenarios in order to locate the source of the problem. I would wish to see some ideas on how to get the correct plot when the index as well has the correct data type (datetime). I also came across a YouTube video covering the same dataset and the same time series. In the video I did not see any wrong plot despite the fact the index of the dataframe was converted to the correct datatype.</p>
","1","Question"
"79329991","","<p>I have a set of data and want to get its value_counts result:</p>
<pre><code>df = pd.DataFrame(
    [5.01, 5.01, 5.08, 6.1, 5.54, 6.3, 5.56, 5.55, 6.7],
    columns=['val'])
</code></pre>
<pre><code>&gt;&gt;&gt; df
    val
0  5.01
1  5.01
2  5.08
3  6.10
4  5.54
5  6.30
6  5.56
7  5.55
8  6.70
&gt;&gt;&gt; df.val.value_counts()
5.01    2
5.08    1
6.10    1
5.54    1
6.30    1
5.56    1
5.55    1
6.70    1
Name: val, dtype: int64
</code></pre>
<p>Is there a way to allow a certain tolerance when using value_counts, such as plus or minus 0.01, so that 5.54, 5.55, and 5.56 in the series are calculated as a group? The result I hope is:</p>
<pre><code>[5.54,5.56,5.55] 3
[5.01] 2
[5.08] 1
[6.10] 1
...
</code></pre>
","0","Question"
"79330288","","<p>I have a Series which is hourly sampled:</p>
<pre><code>Datetime    
2023-01-01 11:00:00     6
2023-01-01 12:00:00     60
2023-01-01 13:00:00     53
2023-01-01 14:00:00     14
2023-01-01 17:00:00     4
2023-01-01 18:00:00     66
2023-01-01 19:00:00     38
2023-01-01 20:00:00     28
2023-01-01 21:00:00     0
2023-01-02 11:00:00     9
2023-01-02 12:00:00     32
2023-01-02 13:00:00     44
2023-01-02 14:00:00     12
2023-01-02 18:00:00     42
2023-01-02 19:00:00     43
2023-01-02 20:00:00     34
2023-01-02 21:00:00     9
2023-01-03 11:00:00     19
...
</code></pre>
<p>(We can assume missing hours as 0s).</p>
<p>I want to compute a 4-week moving average of the series, but using the past 4 week values <strong>on same weekday and hour</strong>.</p>
<p>For example, to compute the average on <code>2023-01-31 01:00:00</code> (Tuesday), take the average of the hours on
<code>2023-01-03 01:00:00</code>, <code>2023-01-10 01:00:00</code>, <code>2023-01-17 01:00:00</code>, and <code>2023-01-24 01:00:00</code>.</p>
<p>The series is quite long. What is an efficient way to do this?</p>
","0","Question"
"79331430","","<pre><code>import pandas as pd
import pyarrow as pa
import numpy as np
from datetime import datetime, timedelta

df = pd.DataFrame({
   &quot;product_id&quot;: pd.Series(
       [&quot;PROD_&quot; + str(np.random.randint(1000, 9999)) for _ in range(100)], 
       dtype=pd.StringDtype(storage=&quot;pyarrow&quot;)
   ),
   &quot;transaction_timestamp&quot;: pd.date_range(
       start=datetime.now() - timedelta(days=30), 
       periods=100, 
       freq='1H'
   ),
   &quot;sales_amount&quot;: pd.Series(
       np.round(np.random.normal(500, 150, 100), 2),
       dtype=pd.Float64Dtype()
   ),
   &quot;customer_segment&quot;: pd.Series(
       np.random.choice(['Premium', 'Standard', 'Basic'], 100),
       dtype=pd.StringDtype(storage=&quot;pyarrow&quot;)
   ),
   &quot;is_repeat_customer&quot;: pd.Series(
       np.random.choice([True, False], 100, p=[0.3, 0.7])
   )
})

def types_mapper(pa_type):
  if pa_type == pa.string():
      return pd.StringDtype(&quot;pyarrow&quot;)

df = df.convert_dtypes(dtype_backend=&quot;pyarrow&quot;)
df_pa = pa.Table.from_pandas(df).to_pandas(types_mapper=types_mapper)
pd.testing.assert_frame_equal(df, df_pa)
</code></pre>
<p>The dtypes are seemingly the same but I get the following error.</p>
<pre><code>AssertionError: Attributes of DataFrame.iloc[:, 0] (column name=&quot;product_id&quot;) are different

Attribute &quot;dtype&quot; are different
[left]: string[pyarrow]
[right]: string[pyarrow]
</code></pre>
","1","Question"
"79331481","","<p>I have a DataFrame that looks like this:
<code>{&quot;1578286800000&quot;:71,&quot;1578373200000&quot;:72,&quot;1578459600000&quot;:72,&quot;1578546000000&quot;:74,&quot;1578632400000&quot;:7,&quot;1578891600000&quot;:7,&quot;1578978000000&quot;:6,&quot;1579064400000&quot;:7,&quot;1579150800000&quot;:6}</code></p>
<p>The format is:
<code>Datetime:int</code></p>
<p>I want to create sub graph out of the data like, graph one would be for the first 5 data pairs and graph two would be for the rest.</p>
<p>I've tried to graph the entire dataframe but keeps getting this error:
<code>ValueError: If using all scalar values, you must pass an index</code></p>
<p>As you can see the dataframe doesn't have an index, and I don't know how to specify <code>Datetime</code> as the x axis and <code>int</code> as the y axis.</p>
<p>Edit 1 (with code):</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_json(&quot;somedata.json&quot;)
df.plot.line()
plt.show()
</code></pre>
<p><code>somedata.json</code> contains the same data as mentioned at the beginning of the question.</p>
<p>Edit 2:</p>
<pre><code>with open('temp.json', 'r') as json_file:
data_pairs = json.load(json_file)

dataframe = pd.DataFrame.from_dict(data_pairs, orient='index')

fig, axes = plt.subplots(2, 1)
dataframe[0:5].plot(ax=axes[0], legend=False)
_ = plt.xticks(rotation=45)
dataframe[5:].plot(ax=axes[1], legend=False)
_ = plt.xticks(rotation=45)
</code></pre>
","0","Question"
"79331776","","<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""></th>
<th style=""text-align: center;"">A</th>
<th style=""text-align: center;"">B</th>
<th style=""text-align: center;"">C</th>
<th style=""text-align: center;"">D</th>
<th style=""text-align: center;"">E</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">Key 1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">-1</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Key 2</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">-1</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Key 3</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">-1</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Key 4</td>
<td style=""text-align: center;"">-1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Key 5</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">-1</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Key 6</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">-1</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Key 7</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">-1</td>
</tr>
<tr>
<td style=""text-align: center;"">Key 8</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">-2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;""></td>
</tr>
</tbody>
</table></div>
<p>Final Result</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">A</th>
<th style=""text-align: center;"">B</th>
<th style=""text-align: center;"">C</th>
<th style=""text-align: center;"">D</th>
<th style=""text-align: center;"">E</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">-1</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;""></td>
</tr>
</tbody>
</table></div>
<p>suppose we have the above dataframe where each key is an option used to create a combination to get to the desired Final Result. Suppose that you can also specify max number of combinations it can use to achieve the final result below, how would one iterate through the dataframe and when a set of combinations equals the final result, it prints all the keys that make up the combo as well as number of combos it took?</p>
<p>For example, let's say the maximum number of combinations is 3-key combo. Then the following combinations will satisfy both the Final Result and stay under or equal to the number of key combos allowed to achieve it</p>
<p>Key 2 (itself), combos 1</p>
<p>Key 4 + Key 5, combos 2</p>
<p>Key 3 + Key 8, combos 2</p>
","1","Question"
"79333079","","<p>I'm looking for a way to drop columns that contain a specific word but without using loops, even if my present Dataframe doesn't have a lot of columns, I know that there is a lot of methods that avoid using loop and realize the exact same thing but incredibly faster. (as the vectorization to create new_column from existing ones)</p>
<p>I want to learn doing things properly.</p>
<pre><code>for col in df_web.columns :
    if 'post' in col and col != 'post_title':
        df_web.drop(columns=col, inplace = True)
</code></pre>
<p>I also could use list comprehension but that still use for loop :</p>
<pre><code>my_col = [col for col in df_web.columns if not col.startswith(&quot;post&quot;) or col == 'post_title']
df_web = df_web.loc[:, my_col]
</code></pre>
<p>Here is the original list of columns of my dataframe :</p>
<p>['sku', 'total_sales', 'post_author', 'post_date', 'post_date_gmt',
'product_type', 'post_title', 'post_excerpt', 'post_name',
'post_modified', 'post_modified_gmt', 'guid', 'post_type']</p>
","0","Question"
"79334351","","<p>I am reading from a json data file and loading it into a dictionary. It as key:value like below.</p>
<pre><code>&quot;1707195600000&quot;:1,&quot;1707282000000&quot;:18,&quot;1707368400000&quot;:1,&quot;1707454800000&quot;:13,&quot;1707714000000&quot;:18,&quot;1707800400000&quot;:12,&quot;1707886800000&quot;:155,&quot;1707973200000&quot;:1&quot;
</code></pre>
<p>Code Snippet:</p>
<pre><code>with open('data.json', 'r') as json_file:
    data_pairs = json.load(json_file)

dataframe = pd.DataFrame.from_dict(data_pairs, orient='index')
</code></pre>
<p>Can it be done with <code>Pandas.DataFrame.from_dict</code>? Or I should convert it all the keys in the dictionary before using <code>from_dict</code>?</p>
","0","Question"
"79334864","","<p>I have a data frame that contains four columns that do not have a label (so they are df[0],df[1], df[2], and df[3] based on position).</p>
<p>I need to interpret what the column names are based on the data. For example purposes, let's say the columns needed are 'First Name', 'Last Name', 'Age', and 'Gender'. Currently I assume that this is the order of the data, but I need to be able to accept the data in any order (based on varied user input). The data is not labeled in a way that would be useful for me to say column 3 is age or column 1 is gender, because there is no header.</p>
<p>I know, based on an allowed set of values, what each column can contain. Age can be from 0-99. Gender can be Male, Female, or Other. The first and last names are from known lists. Is there a way I can set the name of the columns based on this information? I know I can count the number of values in each column that are within the expected value for age, gender, name, and determine that column 1 in the data frame has 100% values expected for age, but I cannot figure out how to rename the column based on this information.</p>
<p>Here is an example of the data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">0</th>
<th style=""text-align: center;"">1</th>
<th style=""text-align: center;"">2</th>
<th style=""text-align: center;"">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">Tom</td>
<td style=""text-align: center;"">Male</td>
<td style=""text-align: center;"">25</td>
<td style=""text-align: center;"">Smith</td>
</tr>
<tr>
<td style=""text-align: center;"">John</td>
<td style=""text-align: center;"">Male</td>
<td style=""text-align: center;"">18</td>
<td style=""text-align: center;"">Doe</td>
</tr>
<tr>
<td style=""text-align: center;"">Lucy</td>
<td style=""text-align: center;"">Female</td>
<td style=""text-align: center;"">7</td>
<td style=""text-align: center;"">Black</td>
</tr>
<tr>
<td style=""text-align: center;"">Jane</td>
<td style=""text-align: center;"">Female</td>
<td style=""text-align: center;"">48</td>
<td style=""text-align: center;"">Doe</td>
</tr>
</tbody>
</table></div>
","0","Question"
"79335580","","<p>I am working with a Timeseries data wherein I am trying to perform outlier detection using IQR method.</p>
<p>Sample Data:</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'datecol' : pd.date_range('2024-1-1', '2024-12-31'),
                   'val' : np.random.random.randin(low = 100, high = 5000, size = 8366})
</code></pre>
<p>my function:</p>
<pre><code>def is_outlier(x):
    iqr = x.quantile(.75) - x.quantile(.25)
    outlier = (x &lt;= x.quantile(.25) - 1.5*iqr) | (x &gt;= x.quantile(.75) + 1.5*iqr)
    return np.select([outlier], [1], 0)

df.groupby(df['datecol'].dt.weekday)['val'].apply(is_outlier)
</code></pre>
<p>to which the output is something like below:</p>
<pre><code>0    [1,1,0,0,....
1    [1,0,0,0,....
2    [1,1,0,0,....
3    [1,0,1,0,....
4    [1,1,0,0,....
5    [1,1,0,0,....
6    [1,0,0,1,....
</code></pre>
<p>I am expecting a single series as output which I can add back to the original <code>dataframe</code> as a flag column.</p>
<p>Can someone please help me with this</p>
","1","Question"
"79335633","","<p>I am using the function <code>.to_excel()</code> in pandas,</p>
<pre><code>import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]])
with pd.ExcelWriter('output.xlsx') as writer:
    df.to_excel(writer, sheet_name=&quot;MySheet&quot;)
</code></pre>
<p>This results in the error <code>OSError: [Errno 9] Bad file descriptor</code></p>
<p>The excel file already exists, but is not open elsewhere.</p>
<p>I am using python 3.10.11, pandas 2.2.3, openpyxl 3.1.5</p>
<p><strong>Traceback:</strong></p>
<pre><code>    with pd.ExcelWriter('output.xlsx') as writer:

  File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\io\excel\_base.py:1353 in __exit__
    self.close()

  File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\io\excel\_base.py:1357 in close
    self._save()

  File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\io\excel\_openpyxl.py:110 in _save
    self.book.save(self._handles.handle)

  File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\openpyxl\workbook\workbook.py:386 in save
    save_workbook(self, filename)

  File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\openpyxl\writer\excel.py:294 in save_workbook
    writer.save()

  File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\openpyxl\writer\excel.py:275 in save
    self.write_data()

  File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\openpyxl\writer\excel.py:60 in write_data
    archive.writestr(ARC_APP, tostring(props.to_tree()))

  File ~\AppData\Local\Programs\Python\Python310\lib\zipfile.py:1816 in writestr
    with self.open(zinfo, mode='w') as dest:

  File ~\AppData\Local\Programs\Python\Python310\lib\zipfile.py:1180 in close
    self._fileobj.seek(self._zinfo.header_offset)

OSError: [Errno 9] Bad file descriptor
</code></pre>
","0","Question"
"79336210","","<p>I hope to identify the peaks in a segment of data (selecting the top 7 points with the highest prominences), which are clearly visible to the naked eye. However, I am unable to successfully obtain the results using the find_peaks function.</p>
<p>The data is accessible <a href=""https://gist.github.com/yuewander/aa520375eee904d48320d24e6a5b29fd"" rel=""nofollow noreferrer"">in this gist</a>.</p>
<p>Error Result: If I directly use <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html"" rel=""nofollow noreferrer""><code>find_peaks</code></a>:</p>
<pre><code>find_peaks(series, prominence=np.max(series) * 0.1, distance=48)
</code></pre>
<p>and then select the top 7 points with the highest prominences, I end up with some undesired points.</p>
<p>Clumsy Method: I can first smooth the data:</p>
<pre><code>percentile_80 = series.rolling(
    window=61, center=True, min_periods=1
    ).apply(lambda x: np.percentile(x, 80))
smoothed_series = series - percentile_80
</code></pre>
<p>Then, use <code>find_peaks(smoothed_series, prominence=np.max(smoothed_series) * 0.1, distance=48)</code>, and select the top 7 points with the highest prominences, which yields the expected results.</p>
<p>However, this approach is much slower.</p>
<p><img src=""https://i.sstatic.net/AJjNBO48.png"" alt=""wrong detect result"" /></p>
<p>Edit on 2025.1.9:
Thanks mozway, this is a good method.
And I found another way to speed up: first find all peaks, then compare peaks with neighboring peaks,find the prominence peaks with neighbor. is this a good method?</p>
<pre><code>    def find_significant_peaks(x, prominence_diff_ratio=0.1, initial_distance=3):
        # Step 1: Get all candidate peaks and their properties
        peaks, properties = find_peaks(
            x, distance=initial_distance, prominence=np.max(x) * 0.01
        )
    
        if len(peaks) == 0:
            return peaks, properties
    
        # Get prominences of all peaks
        prominences = properties[&quot;prominences&quot;]
    
        # Calculate prominence differences using vectorized operations
        diffs = np.abs(np.subtract.outer(prominences, prominences))
        threshold_values = prominences * prominence_diff_ratio
    
        valid_peaks_mask = np.ones(len(peaks), dtype=bool)
    
        # For each peak, check prominence difference with neighbors and local maximality
        compare_num = 10
        for i in range(len(peaks)):
            # Get local window range
            start_idx = max(0, i - compare_num)
            end_idx = min(len(peaks), i + 1 + compare_num)
    
            # Get prominence values within local window
            local_prominences = prominences[start_idx:end_idx]
            current_prominence = prominences[i]
    
            # Condition 1: Check if it's a local maximum
            if current_prominence &lt; np.max(local_prominences):
                valid_peaks_mask[i] = False
                continue
    
            # Condition 2: Get prominence differences with neighbors
            neighbor_diffs = diffs[i, start_idx:end_idx]
            neighbor_diffs = neighbor_diffs[neighbor_diffs != 0]  # Remove self-difference
    
            # Check if all neighboring differences are greater than threshold
            if np.any(neighbor_diffs &lt;= threshold_values[i]):
                valid_peaks_mask[i] = False
    
        # Filter valid peaks and properties
        valid_peaks = peaks[valid_peaks_mask]
    
        # Update all properties in the properties dictionary
        valid_properties = {}
        for key in properties:
            valid_properties[key] = properties[key][valid_peaks_mask]
    
        return valid_peaks, valid_properties
</code></pre>
","4","Question"
"79336443","","<p>I am trying to scrape Yield tables for several countries and several maturities from a website.
So far I only get empty tables:</p>
<p><a href=""https://i.sstatic.net/md6bhS2D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/md6bhS2D.png"" alt=""enter image description here"" /></a></p>
<p>while it should rather look like:</p>
<p><a href=""https://i.sstatic.net/Tp2yhYJj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tp2yhYJj.png"" alt=""enter image description here"" /></a></p>
<p>So far I have been doing the following:</p>
<pre><code>import time 
import datetime as dt
import pandas as pd
from bs4 import BeautifulSoup
from dateutil.relativedelta import relativedelta
import requests
import re 
import os

path = os.getcwd()

def ZCCWord(Date,country): 

    # Site URL
    url=&quot;http://www.worldgovernmentbonds.com/country/&quot;+country

    html_content = requests.get(url).text
    soup = BeautifulSoup(html_content, &quot;lxml&quot;)
    #gdp = soup.find_all(&quot;table&quot;, attrs={&quot;class&quot;: &quot;w3-table w3-white table-padding-custom w3 small font-family-arial table-valign-middle&quot;})
    gdp = soup.find_all(&quot;table&quot;) # , attrs={&quot;class&quot;: &quot;w3-table money pd44 -f15&quot;})
    table1 = gdp[0]
    body = table1.find_all(&quot;tr&quot;)
    body_rows = body[1:] 
    all_rows = [] # will be a list for list for all rows
    for row_num in range(len(body_rows)): # A row at a time
        row = [] # this will old entries for one row
        for row_item in body_rows[row_num].find_all(&quot;td&quot;): #loop through all row entries
            aa = re.sub(&quot;(\xa0)|(\n)|,&quot;,&quot;&quot;,row_item.text)
            #append aa to row - note one row entry is being appended
            row.append(aa)
        # append one row to all_rows
        all_rows.append(row)

    AAA           = pd.DataFrame(all_rows)
    ZCC           = pd.DataFrame()
    ZCC           = AAA[1].str.extract('([^a-zA-Z]+)([a-zA-Z]+)', expand=True).dropna().reset_index(drop=True)
    ZCC.columns   = ['TENOR', 'PERIOD'] 
    ZCC['TENOR'] = ZCC['TENOR'].str.strip().str.isdigit()  # Remove leading/trailing spaces
    #ZCC = ZCC[ZCC['TENOR'].str.isdigit()] 
    ZCC['TENOR']  = ZCC['TENOR'].astype(int)
    ZCC['RATES']  = AAA[2].str.extract(r'([0-9.]+)', expand=True).dropna().reset_index(drop=True).astype(float)
    ZCC['RATES']  = ZCC['RATES']/100

    row2      = []
    for i in range(len(ZCC)): 
        if ZCC['PERIOD'][i]=='month' or  ZCC['PERIOD'][i]=='months':
            b  = ZCC['TENOR'][i]
            bb = Date + relativedelta(months = b)
            row2.append(bb)
        else: 
            b  = ZCC['TENOR'][i]
            bb = Date + relativedelta(years = b)
            row2.append(bb)

    ZCC['DATES'] = pd.DataFrame(row2)
    ZCC = ZCC.reindex(['TENOR','PERIOD','DATES','RATES'], axis=1)
    return ZCC



LitsCountries   =  ['spain','portugal','latvia','ireland','united-kingdom',
                'germany', 'france','italy','sweden','finland','greece',
                'poland','romania','hungary','netherlands']

todays_date     = path+'\\WorldYields' +str(dt.datetime.now().strftime(&quot;%Y-%m-%d-%H-%M&quot;) )+ '.xlsx'   
writer          = pd.ExcelWriter(todays_date, engine='xlsxwriter',engine_kwargs={'options':{'strings_to_urls': False}})
dictYield       = {}

for i in range(len(LitsCountries)): 
        country         = LitsCountries[i]
        Date            = pd.to_datetime('today').date()
        country         = LitsCountries[i] 
        ZCC             = ZCCWord(Date,country)  
        dictYield[i]    = ZCC
        ZCC.to_excel(writer, sheet_name=country)     
       
writer.close()    
time.sleep(60) # wait one minute
</code></pre>
<p>I would be fine also with other websites, solutions or methods which provide similar outputs.
Any idea?</p>
<p>thanks in advance!</p>
","0","Question"
"79337606","","<p>Our HR database lists individual assignments for each employee. Employees can have multiple assignments (ie. work in multiple departments or locations), or multiple roles (concurrent temporary contract and casual contract), and the db is updated daily. At any given time, there can be several entries for an employee; some of these will be duplicates, and some will have minor differences.</p>
<p>I'm already discarding duplicate entries, where the only thing that's different is the EntryID and ChangeDate, so I'm left with something like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>EmployeeEntryID</th>
<th>ChangeDate</th>
<th>Employee#</th>
<th>Firstname</th>
<th>Lastname</th>
<th>Type</th>
<th>Title</th>
<th>Department</th>
<th>Starting</th>
<th>Ending</th>
<th>FTE</th>
</tr>
</thead>
<tbody>
<tr>
<td>1420</td>
<td>20241122</td>
<td>1234</td>
<td>Tom</td>
<td>Jones</td>
<td>CONP</td>
<td>TEACHER</td>
<td>School 2</td>
<td>20240826</td>
<td>99999999</td>
<td>0.5330</td>
</tr>
<tr>
<td>1420</td>
<td>20241122</td>
<td>1234</td>
<td>Tom</td>
<td>Jones</td>
<td>TEPT</td>
<td>TEACHER</td>
<td>School 3</td>
<td>20240826</td>
<td>20250630</td>
<td>0.1000</td>
</tr>
<tr>
<td>5540</td>
<td>20241202</td>
<td>1234</td>
<td>Tom</td>
<td>Jones</td>
<td>TEPT</td>
<td>TEACHER</td>
<td>School 1</td>
<td>20240826</td>
<td>20250630</td>
<td>0.1801</td>
</tr>
</tbody>
</table></div>
<p>What I'd like to do for a nightly update merge those three rows into one, where the employee is assigned to the school with the highest FTE, but all schools are listed under Description (in order of FTE):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Employee#</th>
<th>Firstname</th>
<th>Lastname</th>
<th>Type</th>
<th>Title</th>
<th>Department</th>
<th>Description</th>
<th>Starting</th>
<th>Ending</th>
<th>FTE</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>Tom</td>
<td>Jones</td>
<td>CONP</td>
<td>TEACHER</td>
<td>School 2</td>
<td>School 2, School 1, School 3</td>
<td>20250106</td>
<td>99999999</td>
<td>0.5330</td>
</tr>
</tbody>
</table></div>
<p>Unfortunately, everything I've found regarding merging rows with Pandas seems to assume you're dealing with numbers, not strings, so I'm not sure how to get started. I've tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>df.sort_values(['Employee#','FTE'],ascending=True).groupby('Employee#')['Department'].agg(', '.join).reset_index()
</code></pre>
<p>which will list the schools in order of FTE (which is fine for the Description field), but I can't figure out how to isolate the highest FTE school for the assignment field.</p>
<p>Edit: I've modified the second 'Department' line from @samhita's answer, so I don't get any repetitions:
'Department': lambda x: ', '.join(list(set(x)))</p>
<p>Any idea how I'd include the FTE for each school in the description (ie. School 2(0.5330 FTE), School 1(0.1801 FTE), School 3 (0.1000 FTE)&quot;</p>
","1","Question"
"79338219","","<p>I am trying to iterate multiple column rows and multiply nth row to n+1 row after that add columns.</p>
<p>I tried below code and it's working fine.</p>
<p>Is there any other simply way to achieve the subtraction and multiplication part together?</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'C': [&quot;Spark&quot;,&quot;PySpark&quot;,&quot;Python&quot;,&quot;pandas&quot;,&quot;Java&quot;],
                    'F' : [2,4,3,5,4],
                    'D':[3,4,6,5,5]})

df1 = pd.DataFrame({'C': [&quot;Spark&quot;,&quot;PySpark&quot;,&quot;Python&quot;,&quot;pandas&quot;,&quot;Java&quot;],
                    'F': [1,2,1,2,1],
                    'D':[1,2,2,2,1]})

df = pd.merge(df, df1, on=&quot;C&quot;)

df['F_x-F_y'] = df['F_x'] - df['F_y']
df['D_x-D_y'] = df['D_x'] - df['D_y']

for index, row in df.iterrows():
    df['F_mul'] = df['F_x-F_y'].mul(df['F_x-F_y'].shift())
    df['D_mul'] = df['D_x-D_y'].mul(df['D_x-D_y'].shift())

df['F+D'] = df['F_mul'] + df['D_mul']
</code></pre>
<p>Output -</p>
<pre><code>         C  F_x  D_x  F_y  D_y  F_x-F_y  D_x-D_y  F_mul  D_mul   F+D
0    Spark    2    3    1    1        1        2    NaN    NaN   NaN
1  PySpark    4    4    2    2        2        2    2.0    4.0   6.0
2   Python    3    6    1    2        2        4    4.0    8.0  12.0
3   pandas    5    5    2    2        3        3    6.0   12.0  18.0
4     Java    4    5    1    1        3        4    9.0   12.0  21.0
</code></pre>
","2","Question"
"79339719","","<p>I have an excel sheet contain the following data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>counts</th>
</tr>
</thead>
<tbody>
<tr>
<td>1446/05/25</td>
<td>12</td>
</tr>
<tr>
<td>1446/05/26</td>
<td>2</td>
</tr>
<tr>
<td>1446/05/26</td>
<td>6</td>
</tr>
<tr>
<td>1446/05/26</td>
<td>1</td>
</tr>
<tr>
<td>1446/05/26</td>
<td>1</td>
</tr>
<tr>
<td>1446/05/26</td>
<td>6</td>
</tr>
<tr>
<td>1446/05/27</td>
<td>6</td>
</tr>
<tr>
<td>1446/05/27</td>
<td>6</td>
</tr>
<tr>
<td>1446/05/28</td>
<td>4</td>
</tr>
<tr>
<td>1446/05/28</td>
<td>6</td>
</tr>
<tr>
<td>1446/05/29</td>
<td>9</td>
</tr>
</tbody>
</table></div>
<p>where I want to plot them together. But my problem is I have a duplicated date in x shown in plot. How to make them unique by summing up the counts and show unique date?
<a href=""https://i.sstatic.net/Ix7fUeWk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ix7fUeWk.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79339822","","<p>I want to plot precip type from the start of the GFS run (hour 0) through hour 240 at 6-hour intervals. (in this code I only try to go to hour 108) Also, at the end of the code when saving the the plots, how do I save them each as separate <code>.png</code> images?</p>
<p>Here is the code:</p>
<pre><code>from datetime import datetime
from datetime import timedelta
import pandas as pd
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import xarray as xr
import numpy as np
import metpy.calc as mpcalc
from metpy.plots import USCOUNTIES
import netCDF4
from netCDF4 import Dataset
from netCDF4 import num2date
from metpy.units import units
from scipy.ndimage import gaussian_filter
import scipy.ndimage as ndimage
from siphon.catalog import TDSCatalog

start_time = datetime(2025, 1, 7, 12, 0, 0)
time_deltas = [timedelta(hours=6), timedelta(hours=12), timedelta(hours=18), timedelta(hours=24), timedelta(hours=30), timedelta(hours=36),
                timedelta(hours=42), timedelta(hours=48), timedelta(hours=54), timedelta(hours=60), timedelta(hours=66), timedelta(hours=72),
               timedelta(hours=78), timedelta(hours=84), timedelta(hours=90), timedelta(hours=96), timedelta(hours=102), timedelta(hours=108)]
for time_delta in time_deltas:
        dt = start_time + time_delta
#dt = datetime(2025,1,4,12)
best_gfs = TDSCatalog('https://thredds.ucar.edu/thredds/catalog/grib/NCEP/GFS/Global_0p25deg/catalog.xml?dataset=grib/NCEP/GFS/Global_0p25deg/Best')
best_ds = best_gfs.datasets[0]
ncss = best_ds.subset()
query = ncss.query()
query.accept('netcdf')
query.lonlat_box(north=75, south=15, east=320, west=185)
query.time(dt)
query.variables('Geopotential_height_isobaric', 'Pressure_reduced_to_MSL_msl', 'Precipitation_rate_surface', 'Snow_depth_surface', 'Categorical_Snow_surface','Categorical_Freezing_Rain_surface', 'Categorical_Ice_Pellets_surface')

data = ncss.get_data(query)
print(list(data.variables))

plev = list(data.variables['isobaric'][:])

lat = data.variables['latitude'][:].squeeze()
lon = data.variables['longitude'][:].squeeze()
time1 = data['time']
vtime = num2date(time1[:].squeeze(), units=time1.units)
emsl_var = data.variables['Pressure_reduced_to_MSL_msl']
preciprate = data.variables['Precipitation_rate_surface'][:].squeeze()
snowdepth = data.variables['Snow_depth_surface'][:].squeeze()
catsnow = data.variables['Categorical_Snow_surface'][:].squeeze()
catice = data.variables['Categorical_Freezing_Rain_surface'][:].squeeze()
catsleet = data.variables['Categorical_Ice_Pellets_surface'][:].squeeze()
EMSL = units.Quantity(emsl_var[:], emsl_var.units).to('hPa')
mslp = gaussian_filter(EMSL[0], sigma=3.0)
hght_1000 = data.variables['Geopotential_height_isobaric'][0, plev.index(100000)]
hght_500 = data.variables['Geopotential_height_isobaric'][0, plev.index(50000)]
thickness_1000_500 = gaussian_filter((hght_500 - hght_1000)/10, sigma=3.0)
lon_2d, lat_2d = np.meshgrid(lon, lat)

precip_inch_hour = preciprate * 141.73228346457
precip2 = mpcalc.smooth_n_point(precip_inch_hour, 5, 1)

precip_colors = [
   &quot;#bde9bf&quot;,  # 0.01 - 0.02 inches 1
   &quot;#adddb0&quot;,  # 0.02 - 0.03 inches 2
   &quot;#9ed0a0&quot;,  # 0.03 - 0.04 inches 3
   &quot;#8ec491&quot;,  # 0.04 - 0.05 inches 4
   &quot;#7fb882&quot;,  # 0.05 - 0.06 inches 5
   &quot;#70ac74&quot;,  # 0.06 - 0.07 inches 6
   &quot;#60a065&quot;,  # 0.07 - 0.08 inches 7
   &quot;#519457&quot;,  # 0.08 - 0.09 inches 8
   &quot;#418849&quot;,  # 0.09 - 0.10 inches 9
   &quot;#307c3c&quot;,  # 0.10 - 0.12 inches 10
   &quot;#1c712e&quot;,  # 0.12 - 0.14 inches 11
   &quot;#f7f370&quot;,  # 0.14 - 0.16 inches 12
   &quot;#fbdf65&quot;,  # 0.16 - 0.18 inches 13
   &quot;#fecb5a&quot;,  # 0.18 - 0.2 inches 14
   &quot;#ffb650&quot;,  # 0.2 - 0.3 inches 15
   &quot;#ffa146&quot;,  # 0.3 - 0.4 inches 16
   &quot;#ff8b3c&quot;,   # 0.4 - 0.5 inches 17
   &quot;#f94609&quot;,   # 0.5 - 0.6 inches 18
]

precip_colormap = mcolors.ListedColormap(precip_colors)

clev_precip =  np.concatenate((np.arange(0.01, 0.1, .01), np.arange(.1, .2, .02), np.arange(.2, .61, .1)))
norm = mcolors.BoundaryNorm(clev_precip, 18)

datacrs = ccrs.PlateCarree()
plotcrs = ccrs.LambertConformal(central_latitude=35, central_longitude=-100,standard_parallels=(30, 60))
bounds = ([-105, -90, 30, 40])
fig = plt.figure(figsize=(14,12))
ax = fig.add_subplot(1,1,1, projection=plotcrs)
ax.set_extent(bounds, crs=ccrs.PlateCarree())
ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth = 0.75)
ax.add_feature(cfeature.STATES, linewidth = 1)
ax.add_feature(USCOUNTIES, edgecolor='grey', linewidth = .5)
clevs = (np.arange(0, 540, 6),
         np.array([540]),
         np.arange(546, 700, 6))
colors = ('tab:blue', 'b', 'tab:red')
kw_clabels = {'fontsize': 11, 'inline': True, 'inline_spacing': 5, 'fmt': '%i',
              'rightside_up': True, 'use_clabeltext': True}

# Plot MSLP
clevmslp = np.arange(800., 1120., 2)
cs2 = ax.contour(lon_2d, lat_2d, mslp, clevmslp, colors='k', linewidths=1.25,
                 linestyles='solid', transform=ccrs.PlateCarree())

cf = ax.contourf(lon_2d, lat_2d, precip2, clev_precip, cmap=precip_colormap, norm=norm, extend='max', transform=ccrs.PlateCarree())

ax.set_title('GFS Precip Type, Rate(in/hr), MSLP (hPa), &amp; 1000-500mb Thickness (dam)', loc='left', fontsize=10, weight = 'bold')
ax.set_title('Valid Time: {}z'.format(vtime), loc = 'right', fontsize=8)
</code></pre>
<p>I tried using this: <code>dt = start_time + time_delta</code> but this only plots the last <code>timedelta</code> which is hour 108 and not all the other <code>timedelta</code> hours.</p>
<p>Plot: <a href=""https://i.sstatic.net/MBODb8rp.png"" rel=""nofollow noreferrer"">Plot</a></p>
","0","Question"
"79341984","","<p>I'm having difficulty with Pandas when trying to convert this column to a date. The table doesn't include a year, so I think that's making the conversion difficult.</p>
<pre><code>    28 JUL  Unnamed: 0        Alura *Alura - 7/12     68,00
0   28 JUL         NaN  Passei Direto S/A. - 3/12     19,90
1   31 JUL         NaN          Drogarias Pacheco     25,99
2   31 JUL         NaN     Mundo Verde - Rj - Sho      5,90
3   31 JUL         NaN              Paypal *99app      4,25
4   04 AGO         NaN            Saldo em atraso  1.091,17
5   04 AGO         NaN          Crédito de atraso  1.091,17
6   06 AGO         NaN             Apple.Com/Bill     34,90
7   07 AGO         NaN        Pagamento em 07 AGO  1.091,17
8   07 AGO         NaN            Juros de atraso     16,86
9   07 AGO         NaN              IOF de atraso      4,43
10  07 AGO         NaN            Multa de atraso     21,91
11  08 AGO         NaN             Apple.Com/Bill     21,90
12  09 AGO         NaN      Google Youtubepremium     20,90
13  10 AGO         NaN              Amazon.Com.Br     41,32
14  12 AGO         NaN           Uber *Uber *Trip     17,91
15  12 AGO         NaN           Uber *Uber *Trip     16,94
16  12 AGO         NaN                Mia Cookies     47,50
17  13 AGO         NaN           Uber *Uber *Trip     16,96
18  13 AGO         NaN           Uber *Uber *Trip     19,98
19  16 AGO         NaN           Uber *Uber *Trip     11,93
20  16 AGO         NaN           Uber *Uber *Trip      9,97
21  18 AGO         NaN           Uber *Uber *Trip      9,91
22  22 AGO         NaN           Uber *Uber *Trip      9,96
23  23 AGO         NaN              Amazonprimebr     14,90
24  27 AGO         NaN        Paypal *Sacocheiotv     15,00
25  27 AGO         NaN        Pag*Easymarketpleno      6,50
</code></pre>
<p>I tried to transform it using this code, but it's not working:</p>
<pre><code>df[&quot;Data&quot;] = pd.to_datetime(df[&quot;Data&quot;], format=&quot;%d %b&quot;, errors=&quot;coerce&quot;)
</code></pre>
<p>Incorrect output:</p>
<pre><code>Data                      Local  Valor
0  1900-07-28        Alura *Alura - 7/12  68,00
1  1900-07-28  Passei Direto S/A. - 3/12  19,90
2  1900-07-31          Drogarias Pacheco  25,99
3  1900-07-31     Mundo Verde - Rj - Sho   5,90
4  1900-07-31              Paypal *99app   4,25
7         NaT             Apple.Com/Bill  34,90
9         NaT            Juros de atraso  16,86
10        NaT              IOF de atraso   4,43
11        NaT            Multa de atraso  21,91
12        NaT             Apple.Com/Bill  21,90
13        NaT      Google Youtubepremium  20,90
14        NaT              Amazon.Com.Br  41,32
15        NaT           Uber *Uber *Trip  17,91
16        NaT           Uber *Uber *Trip  16,94
17        NaT                Mia Cookies  47,50
18        NaT           Uber *Uber *Trip  16,96
19        NaT           Uber *Uber *Trip  19,98
20        NaT           Uber *Uber *Trip  11,93
21        NaT           Uber *Uber *Trip   9,97
22        NaT           Uber *Uber *Trip   9,91
23        NaT           Uber *Uber *Trip   9,96
24        NaT              Amazonprimebr  14,90
25        NaT        Paypal *Sacocheiotv  15,00
26        NaT        Pag*Easymarketpleno   6,50
</code></pre>
<p>Could someone help me with this?</p>
","1","Question"
"79343315","","<p>I have a file with some family relationship data and I would like to create a family id column based on <code>id</code> and <code>sibling_id</code>. My data looks like the following:</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 4, 5, 6, 7],
    'field_a': list('AABBCCDEFG'),
    'sibling_id': [2, 3, 1, 3, 1, 2, np.nan, np.nan, 7, 6],
    'sibling_field_a': ['B', 'C', 'A', 'C' , 'A', 'B', np.nan, np.nan, 'G', 'F']
})

df['sibling_id'] = df['sibling_id'].astype('Int64')

   id field_a  sibling_id sibling_field_a
0   1       A           2               B
1   1       A           3               C
2   2       B           1               A
3   2       B           3               C
4   3       C           1               A
5   3       C           2               B
6   4       D        &lt;NA&gt;             NaN
7   5       E        &lt;NA&gt;             NaN
8   6       F           7               G
9   7       G           6               F
</code></pre>
<p>My expected output is</p>
<pre><code>   id field_a  sibling_id sibling_field_a  family_id
0   1       A           2               B          0
1   1       A           3               C          0
2   2       B           1               A          0
3   2       B           3               C          0
4   3       C           1               A          0
5   3       C           2               B          0
6   4       D        &lt;NA&gt;             NaN          1
7   5       E        &lt;NA&gt;             NaN          2
8   6       F           7               G          3
9   7       G           6               F          3
</code></pre>
<p>Thank you for the help in advance.</p>
","1","Question"
"79343427","","<p>I have two dataframes of city data with many rows and a few columns. I am trying to find a way to see if dfA values is in dfB and then print the value in dfA with its index for those that is in dfB in a list, then also for those in dfA that is NOT, in another list. The order of the information per row does not have to be in exact order in the two dfs, but in total, it must have the info as a whole. So for example, dfA Index 1 New York would be a match with dfB Index 3, and since in dfA, there is no row for Atlanta but in dfB it does, it will be printed in the second list.</p>
<p>For example below:</p>
<p>dfA</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Index</th>
<th style=""text-align: center;"">Column 1</th>
<th style=""text-align: center;"">Column 2</th>
<th style=""text-align: center;"">Column 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">Albuquerque</td>
<td style=""text-align: center;"">NM</td>
<td style=""text-align: center;"">87101</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">New York</td>
<td style=""text-align: center;"">NY</td>
<td style=""text-align: center;"">10009</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">Miami</td>
<td style=""text-align: center;"">FL</td>
<td style=""text-align: center;"">33101</td>
</tr>
</tbody>
</table></div>
<p>dfB</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Index</th>
<th style=""text-align: center;"">Column 1</th>
<th style=""text-align: center;"">Column 2</th>
<th style=""text-align: center;"">Column 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">NM</td>
<td style=""text-align: center;"">Albuquerque</td>
<td style=""text-align: center;"">87101</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">Atlanta</td>
<td style=""text-align: center;"">GA</td>
<td style=""text-align: center;"">30033</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">San Francisco</td>
<td style=""text-align: center;"">CA</td>
<td style=""text-align: center;"">94016</td>
</tr>
<tr>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">10009</td>
<td style=""text-align: center;"">NY</td>
<td style=""text-align: center;"">New York</td>
</tr>
</tbody>
</table></div>
","0","Question"
"79343929","","<p>I'm taking an online class, and there appears to be a glitch in the coursework, which seems to derive from different versions of Pandas. The code the course is providing does not run. However, the course provides a type of patch or update to jog in a repair - but - that repair/updated code patch doesn't appear to be necessary in one instance, and it malfunctions in another part.</p>
<p>We want to group by planet type and then subdivide further on the basis of whether the planet has a magnetic ring. So six groups in theory - but only four actual existing groups arise from the data. And once we have these groups, we want to perform some sum and agg operations.</p>
<p>The code patch was delivered immediately prior, where we are messaged that to avoid an error, we need to tweak a parameter for the sum() function, where we need to insert: (numeric_only=True). Although my current IDE doesn't throw an error without this tweak, instead it just concats the non-numerics.</p>
<p>But the real problem is where we are asked to run the code at the bottom of the block, with the agg() function. I think the problem derives from the fact that I'm seeking to perform mathematical operations on non-numeric data  - specifically, the column for &quot;rings&quot; is a bool type. But while I have been able to adjust the parameters for the mean and max functions individually (so that they are only assessing numeric columns) but I can't make this adjustment on the agg() function because it does not have this parameter. And without being able to make this adjustment for numeric only on the agg(), the coursework itself produces and error.</p>
<p>And if I pursue my own fix as outlined above, and seperate the mean() and max() operations and perform them individually - I can apparently tweak this parameter for &quot;numeric_only=True&quot; for each:</p>
<pre><code>print(planets.groupby(['type', 'magnetic_field']).max(numeric_only=True))
print(planets.groupby(['type', 'magnetic_field']).mean(numeric_only=True))
</code></pre>
<p>This does produce all the correct data, albeit less efficiently - Bnd shouldn't these two functions have the same parameters as agg() since they are here part of pandas aggregate functions?</p>
<p>And aside from that question, there is the issue of reproducing the coursework results and just getting it right - I want all the data on the same output dataframe. Ultimately, if I separate these functions and adjust the parameters individually, then I can collect the data correctly - but much less efficiently. And the course work wants all the output in the same printout. Any ideas what I'm missing on this syntax to get do in one execution? THANKS!!</p>
<pre><code>import numpy as np
import pandas as pd
data = {'planet': ['Mercury', 'Venus', 'Earth', 'Mars',
                   'Jupiter', 'Saturn', 'Uranus', 'Neptune'],
        'radius_km': [2440, 6052, 6371, 3390, 69911, 58232,
                     25362, 24622],
        'moons': [0, 0, 1, 2, 80, 83, 27, 14],
        'type': ['terrestrial', 'terrestrial', 'terrestrial', 'terrestrial',
                 'gas giant', 'gas giant', 'ice giant', 'ice giant'],
        'rings': ['no', 'no', 'no', 'no', 'yes', 'yes', 'yes','yes'],
        'mean_temp_c': [167, 464, 15, -65, -110, -140, -195, -200],
        'magnetic_field': ['yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes'] }

planets = pd.DataFrame(data)
P = planets.groupby(['type', 'magnetic_field']).agg(['mean', 'max'])
print(P)
</code></pre>
","2","Question"
"79344594","","<p>This should be simple using pandas merge_asof function, but unfortunately it's not working because the function complains: <code>ValueError: left keys must be sorted</code>.</p>
<p>I want to assign the correct values of seniority of individuals to their achievements over time. I have a data frame with the achievements of 3,500 people over time. In total, there are &gt; 75,000 achievements over a time period of 1970 to this year. Now, the individuals progress in seniority over time.</p>
<p>I want to match the achievements to their seniority.</p>
<p>Below, under the heading ACHIEVEMENTDATA is an example of data for two people. The relevant identifiers are <code>identifier</code> and <code>achievement_year</code>. <code>achieve_count</code> is the number of achievements by <code>achievement_year</code> for a person.</p>
<p>Now, I want a column (<code>seniority</code>) added to <code>dfa</code> based on achievement data over time from <code>df</code> (see PERSONALNDATA below), such that the rows in <code>dfa</code> reflect the proper seniority for each row in <code>dfa</code>. I manually did that below.</p>
<p>Note that some rows in <code>df</code> have duplicates identifier per year (A in 2015 and 2019) in these cases, rely on the row with the highest value of seniority.</p>
<pre><code>PERSONALNDATA (df)
identifier  seniority   year
A   2   2009
A   3   2015
A   3   2015
A   4   2019
A   4   2019
A   4   2023
B   2   2012
B   4   2024


ACHIEVEMENTDATA (dfa):
identifier  achievement_year    achieve_count   seniority
A   2003    2   
A   2004    3   
A   2005    1   
A   2006    3   
A   2007    1   
A   2008    1   
A   2010    2   2
A   2011    2   2
A   2012    2   2
A   2013    4   2
A   2014    8   2
A   2015    4   3
A   2016    4   3
A   2017    4   3
A   2018    7   3
A   2019    4   4
A   2020    12  4
A   2021    8   4
A   2022    5   4
A   2023    7   4
A   2024    5   4
B   2007    1   
B   2009    1   
B   2010    2   
B   2011    1   
B   2012    2   2
B   2013    1   2
B   2014    1   2
B   2017    3   2
B   2019    1   2
B   2020    2   2
B   2021    1   2
B   2023    2   2
B   2024    2   4
</code></pre>
","0","Question"
"79346256","","<p>I am currently working on a project where I have to first merge two datasets:</p>
<p>The first dataset contains weather data in 30 minute intervals. The second dataset contains minute-level data with PV voltage and cloud images but unlike the first, the second lacks time consistency, where several hours of a day might be missing. note that both have a time column</p>
<p>The goal is to do a multi-modal analysis (time series+image) to predict the PV voltage.</p>
<p>my problem is that I expanded the weather data to match the minute level intervals by forward filling the data within each 30 minute interval(the weather data(temp, wind speed,solar irradiance etc) is not linearly interpolated, rather for each 30 minute block with the corresponding data, I expanded the the time (0-29 mins) and gave that the same data of the 30 minute block that it contains), but after merging the combined dataset has fewer rows. What are the optimal ways to merge two datasets on the <code>time</code> column without losing thousands of rows. For reference, the PV and image dataset spans between a few months less than 3 years but only has close to 400k minutes logged. so that's a lot of days with no data.</p>
<p>the PV/image dataset, has time image and PV voltage data and the issue is that it would be consistent, lets say for example from 8 am to 10 pm each minute with corresponding time, image pv voltage data but then cuts off to the next day at 3am and continues like that, the content of each row is consistent there are no &quot;NaN&quot; values basically it's just that some hours of the day are missing and that happens a lot. as I said only 400k minutes (rows) is logged out of 3 years.</p>
<p>and for the goal its not clear yet but I think it would be multi-modal analysis on the two datasets where in the end the outputs of the heads of each model( one that accepts image inputs, the other accepts the weather data) would be used to make a prediction? I'm not really sure and haven't done this before.</p>
<p>Also, since this would be introduced to a CNN model in time series, is the lack of consistent time spacing going to be a problem or is there a way around that? I have never dealt with time-series model and wondering if I should bother with this at all anyway.</p>
<pre class=""lang-none prettyprint-override""><code>from PIL import Image
import io

def decode_image(binary_data):
    # Convert binary data to an image
    image = Image.open(io.BytesIO(binary_data))
    return np.array(image)  # Convert to NumPy array for processing

# Apply to all rows
df_PV['decoded_image'] = df_PV['image'].apply(lambda x: decode_image(x['bytes']))


# Insert the decoded_image column in the same position as the image column
image_col_position = df_PV.columns.get_loc('image')  # Get the position of the image column
df_PV.insert(image_col_position, 'decoded_image', df_PV.pop('decoded_image'))

# Drop the old image column
df_PV = df_PV.drop(columns=['image'])


print(df_PV.head())


# Remove timezone from the column
expanded_weather_df['time'] = pd.to_datetime(expanded_weather_df['time']).dt.tz_localize(None)

# also remove timezone
df_PV['time'] = pd.to_datetime(df_PV['time']).dt.tz_localize(None)

# merge
combined_df = expanded_weather_df.merge(df_PV, on='time', how='inner')```
</code></pre>
","0","Question"
"79346496","","<p>I’m trying to perform Named Entity Recognition (NER) using NLTK, SpaCy, and a dataset in PyCharm. However, I’m encountering an error related to a missing resource (punkt_tab) when tokenizing text. Here's the full error message:</p>
<p><a href=""https://i.sstatic.net/KnyQO12G.jpg"" rel=""nofollow noreferrer"">Output</a></p>
<p>I have already downloaded the necessary NLTK resources in my script:</p>
<p><a href=""https://i.sstatic.net/5ndFliHO.jpg"" rel=""nofollow noreferrer"">Necessary Nltk</a></p>
<p>Here’s the relevant code for my use case:</p>
<p><a href=""https://i.sstatic.net/poFJDSfg.jpg"" rel=""nofollow noreferrer"">Code</a></p>
<p>What I Tried</p>
<ul>
<li>Verified that punkt is downloaded successfully (nltk.download('punkt')).</li>
<li>Checked the folders listed in the error message to ensure punkt is present.</li>
<li>Searched online for any mention of punkt_tab, but couldn't find documentation for this specific resource.</li>
</ul>
<p>My Questions</p>
<ul>
<li>Is punkt_tab a separate resource from punkt? If so, how can I download it?</li>
<li>Could this error be caused by an issue in my NLTK or Python environment?</li>
<li>What steps should I take to fix this error and proceed with tokenization in PyCharm?</li>
</ul>
","-1","Question"
"79346684","","<p>So I've been working with a LOT of data, but for simplicity we can use the sample data below. However, what I'm trying to do is plot a line chart that will let me see the extremes, compress the Y axis where no values are present, and expand the Y axis where data is present. The problem I have is the picture below. We can see that there is no data between 3,500 and 500, yet there is a huge gap, and then an almost solid line at the bottom.
<a href=""https://i.sstatic.net/XIvKrIcg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XIvKrIcg.png"" alt=""line chart with all data plotted"" /></a></p>
<p>What I'd like to have is the line chart be displayed where we also include the extreme (totals), but not have the huge gap between the sales data, and still be able to see the sales data like this, but also include the line at the top for the Totals:
<a href=""https://i.sstatic.net/eL2cl3vI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eL2cl3vI.png"" alt=""line chart with 'normal' employee data"" /></a></p>
<p>Here's the code I have that does the charts so far, but I need to be able to apply this to a much larger set of data. The data set I would use for this would contain hundreds of &quot;employees&quot; across multiple stores. So the extreme values would be something like &quot;Store_A&quot;:9560, &quot;Store_B&quot;:6470, but the 'normal' values for the employee sales would only range between 0 and 300 (but 300 is variable, some weeks we've got guys that do more than 300).</p>
<pre><code>import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Sample data
data = {
    'Date': ['2025-01-10', '2025-01-17', '2025-01-24', '2025-01-31'],
    'Bob': [156, 60, 58, 62],
    'Joe': [37, 40, 139, 42],
    'Sally': [62, 265, 63, 67],
    'Total': [3698, 3750, 3720, 3800]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Melt the DataFrame to long format
df_melted = df.melt(id_vars='Date', var_name='Employee', value_name='Sales')

# Convert Date column to datetime
df_melted['Date'] = pd.to_datetime(df_melted['Date'])

# Create the line plot
plt.figure(figsize=(12, 8))
sns.lineplot(x='Date', y='Sales', hue='Employee', data=df_melted, marker='o')

# Set y-axis to display whole numbers only
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))

# Add labels and title
plt.xlabel('Date')
plt.ylabel('Number of Sales')
plt.title('Employee Sales Data Over Time')

# Set custom y-axis ticks with greater spacing for lower numbers and compressed ranges without data
custom_ticks = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100] #+ list(range(200, 4001, 1000))
plt.gca().set_yticks(custom_ticks)

# Show the plot
plt.show()

</code></pre>
","0","Question"
"79346829","","<p>I have a list of phone numbers and another column with a 1 or 0, some numbers are duplicates. I can find the list of duplicates using a df to read the csv</p>
<pre><code>df = pd.read_csv(&quot;Example_File&quot;)
duplicates = df[df.duplicated()]
</code></pre>
<p>I want to take that list of duplcates, and see if column 2 with the 0 or 1 match for the duplicated number.</p>
<p>Example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Col 1</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td>555-555-5555</td>
<td>1</td>
</tr>
<tr>
<td>555-555-5555</td>
<td>1</td>
</tr>
<tr>
<td>555-123-4567</td>
<td>0</td>
</tr>
<tr>
<td>555-123-4567</td>
<td>1</td>
</tr>
<tr>
<td>777-555-5555</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>I would like to get a list of the items where col 1 has a duplicate and column 2 does not match. Example: 555-123-4567 has 0 and 1 in column 2.</p>
<p>I have tried comparing the 2 lists as a df and got as far as exporting the duplicates in column 1 to a new df along with the corresponding column 2 but cant resolve to to find the final list where I have duplicated phone numbers ob column 2 does not match for each phone number.</p>
","0","Question"
"79346838","","<p>I am using Python <strong>openpyxl</strong> to update an Excel sheet that contains a table and conditional formatting. The table has a predefined style, and the conditional formatting uses a 3-color scale applied to a specific range. I am trying to:</p>
<ul>
<li>Extend the table range to include a new column.</li>
<li>Apply the same table formatting to the extended range.</li>
<li>Update the conditional formatting to cover the extended range as well.</li>
</ul>
<p>Image below is the initial Excel sheet I want to update,</p>
<p><a href=""https://i.sstatic.net/Z4jo8APm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Z4jo8APm.png"" alt=""table before"" /></a></p>
<p>and below is the code I am using</p>
<pre><code>import openpyxl
from openpyxl.formatting.formatting import ConditionalFormattingList
from openpyxl.formatting.rule import ColorScaleRule
     
path = &quot;test_colors.xlsx&quot;
wb = openpyxl.load_workbook(path)    
sheet = wb[&quot;tab1&quot;]
  
sheet.tables[&quot;tab1_table&quot;].ref = &quot;A1:D4&quot;

new_rule_range = &quot;A2:D4&quot;
rule = ColorScaleRule(start_type='min', start_color='00FF00',    
                      mid_type='percentile', mid_value=50, mid_color='FFFF00',    
                      end_type='max', end_color='AA4A44')         

sheet.conditional_formatting=ConditionalFormattingList()    
sheet.conditional_formatting.add(new_rule_range, rule)
    
wb.save(path)    
wb.close()
</code></pre>
<p>but once I open the Excel file I get the following warning pop-up</p>
<p><a href=""https://i.sstatic.net/82aZukmT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82aZukmT.png"" alt=""warning message"" /></a></p>
<p>if I click yes, I get the table below with the rules applied without any table formatting.</p>
<p><a href=""https://i.sstatic.net/6HSbO6rB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6HSbO6rB.png"" alt=""table after"" /></a></p>
<p>can someone help figure out the problem, and how can I resolve it?</p>
","1","Question"
"79347389","","<p>I would like to import a csv file with headers to pandas. Somehow, pandas appends a &quot;.7&quot; to the last headers name</p>
<p>The last header in the csv contains a &quot;<code>?</code>&quot; as the last character (on purpose, the question mark is part of the last column's header)</p>
<pre><code>EXAMPLE XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX;XXXXXXXXXXXXXXXXXXXXXX;XXXXXXXXXXXXXXXXXXXXXX;XXXXXXXX?
;;;
</code></pre>
<p>(in reality there are about 400 columns…)</p>
<pre><code>import pandas as pd

file_path = 'rawdata.csv'

df = pd.read_csv(file_path, sep=';')
print(df.head(1))
</code></pre>
<p>Output</p>
<pre><code>  EXAMPLE XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  XXXXXXXXXXXXXXXXXXXXXX  \
0                                           NaN                     NaN   

  XXXXXXXXXXXXXXXXXXXXXX XXXXXXXX?.7  
0                    NaN         NaN  
</code></pre>
<p>As you can see, the last header ends with a &quot;?&quot;, but pandas added .7.</p>
<p>I have tried other ways of outputting the header names and it shows with .7 as well.</p>
<p>During testing I once saw a .4, but I can't reproduce it with the .4…</p>
<p>Any other special symbols instead of <code>?</code> behave as expected.
<code>?</code> at the end of any other line behaves as expected as well.</p>
<p>I have saved the original csv with UTF-8 (no BOM) and \n (LF) line terminator via VS Code. I have checked the file for any invisible characters visually (&quot;Render Control Characters&quot; setting turned on in VS Code)</p>
","0","Question"
"79347619","","<p>Whenever I add more than one column to my customdata, it saves the data at the first index (customdata[0]) and separates them by commas, instead of spreading them out across different indexes. Any reference to the other indexes shows the literal text (ex. calling %{customdata[1]} will just show %{customdata[1]} itself.)</p>
<p>Also, whenever I try to format the data (ex. $%{customdata[0]:,.2f}) when customdata has multiple inputs, it turns the data into NaN.</p>
<p>How can I make sure the data gets spread to the other indexes?
Is this even the correct approach/syntax to passing my data to the hover text?</p>
<p>Context:
I'm making a donut chart with plotly express, and using Pandas to pass it data from my spreadsheet. I want to display additional information when I hover over each part of the graph by using hover text. I'm doing this by passing the spreadsheet's columns as customdata under the update_traces function, and referencing them in the hovertemplate.</p>
<pre><code>    exceldf = pd.read_excel('spreadsheetname.xlsx', sheet_name='Transaction', usecols='B:H', skiprows=4)
    exceldf = exceldf.dropna(how='any')

    donutchart = px.pie(
        exceldf, 
        names='Ticker', 
        values='Weight', 
        hole=0.5,
    )
    
    donutchart.update_traces(
    customdata=exceldf[['Live Price', 'Purchase Price', 'Quantity']].to_numpy(),
    hovertemplate='Customdata[0]: %{customdata[0]}&lt;br&gt;Customdata[1]: %{customdata[1]}&lt;br&gt;Customdata[2]: %{customdata[2]}&lt;extra&gt;&lt;/extra&gt;'
    )
</code></pre>
<p>I've tried to correct this by:
-Changing customdata to a numpy array (no change)
-Checking whether the columns' data types were correct (these 3 are floats)
-Using hover_data instead of hovertemplate (gives me an ambiguous input error)</p>
","1","Question"
"79348067","","<p>I'm a beginner in Python and JupyeterLab</p>
<p>I have <em>frame</em> extracted from an excel</p>
<p>I'm trying to convert the same as png.<br>
The following code was working fine in my earlier installation where I used mostly pip.<br>
Now I have moved to anaconda and fresh installing all the packages there but the earlier code now is throwing some error.</p>
<pre><code>import numpy as np
import dataframe_image as dfi

#df = pd.DataFrame(frame)
df_styled = frame.style.background_gradient()
dfi.export(df_styled,&quot;mytable.png&quot;)
</code></pre>
<p>Error</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[28], line 30
     28 #df = pd.DataFrame(frame)
     29 df_styled = frame.style.background_gradient()
---&gt; 30 dfi.export(df_styled,&quot;mytable.png&quot;)
     37 #import ipywidgets as widgets
     38 #from IPython.display import display
     39 #w = widgets.IntSlider()
     40 #display(w)

File ~\AppData\Local\anaconda3\Lib\site-packages\dataframe_image\_pandas_accessor.py:24, in export(obj, filename, fontsize, max_rows, max_cols, table_conversion, chrome_path)
     22 def export(obj, filename, fontsize=14, max_rows=None, max_cols=None, 
     23                table_conversion='chrome', chrome_path=None):
---&gt; 24         return _export(obj, filename, fontsize, max_rows, max_cols, table_conversion, chrome_path)

File ~\AppData\Local\anaconda3\Lib\site-packages\dataframe_image\_pandas_accessor.py:69, in _export(obj, filename, fontsize, max_rows, max_cols, table_conversion, chrome_path)
     66     max_cols = None
     68 if is_styler:
---&gt; 69     html = '&lt;div&gt;' + obj.render() + '&lt;/div&gt;'
     70 else:
     71     html = obj.to_html(max_rows=max_rows, max_cols=max_cols, notebook=True)

AttributeError: 'Styler' object has no attribute 'render'
</code></pre>
<p>websearch shows some incompatibility error<br />
The error message &quot;AttributeError: 'Styler' object has no attribute 'render'&quot; indicates that the dfi.export function you're using might be incompatible with the version of pandas.Styler you have installed.</p>
<p>But not sure how to correct the same.</p>
<p>when in try with conda shell command for update the dataframe_image package it is showing the 0.1.1 is the latest one. Im not that familar with conda and pip. I thought i will stick with conda as using pip and conda together i saw from other posts may cause issues. But it seems the conda repository doesnt have the latest dataframe_image package version. So why is like that and is there any work around.</p>
<p>Do anyone know if i use pip now whether it will update the 0.1.1 version or if it will do a fresh install. Is there any chance of messing up or installing in different path something like that</p>
<p>Currently I have following installed:</p>
<pre><code># Name                    Version                   Build  Channel
pandas                    2.2.3           py311hcf9f919_1    conda-forge
dataframe_image           0.1.1                      py_0    conda-forge
</code></pre>
","0","Question"
"79348168","","<p>I'm computing technical indicators on a rolling basis to avoid any look-ahead bias, for example, for model training and back-testing. To that end I would like to compute the indicator <a href=""https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html?highlight=forceindexindicator#ta.volume.ForceIndexIndicator"" rel=""nofollow noreferrer""><code>ForceIndexIndicator</code></a> using the TA Python project. However this needs two inputs instead of one: close and volume, and I can't get hold of both on my rolling - apply pipeline:</p>
<pre><code>import pandas as pd
import ta

...
df.columns = ['close', 'volume']
df['force_index_close'] = (
    df.rolling(window=window)
    .apply(
        lambda x: ta.volume.ForceIndexIndicator(
            close=x['close'],
            volume=x['volume'],
            window=13,
            fillna=True)
        .force_index().iloc[-1]))
</code></pre>
<p>I get the error <code>KeyError: 'close'</code> because apply gets one column at a time and not both simultaneously as needed.</p>
","0","Question"
"79348389","","<p>I im trying to extract a title from the famous &quot;Titanic&quot; dataset, where the format is like this:</p>
<p>[Name]
[1]: <a href=""https://i.sstatic.net/HlkH8zHO.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/HlkH8zHO.png</a></p>
<p>I'm trying to avoid an iterative solution, so i've tried something like this:</p>
<p><code>df['Title'] = df['Name'].str[df['Name'].str.find(' ') + 1 : df['Name'].str.find('.')]</code></p>
<p>This doesn't work since i'm using series as indexes instead of an unique value.
¿What would be the correct way to do this?</p>
<p>This works, but seems too complex:</p>
<pre><code>space_pos=data.Name.str.find(&quot; &quot;)
dot_pos=data.Name.str.find(&quot;.&quot;)
data[&quot;Title&quot;]=[data.Name[i][space_pos[i]+1:dot_pos[i]] for i in range(len(data.Name))]


</code></pre>
","0","Question"
"79348640","","<p>I built a model based on LSTM and trained it to predict stock price changes during the day, where the unit of time is one second. The test data gives a result comparable to the real one, but I need to get a forecast for the future outside the existing range. What am I doing wrong?</p>
<pre><code>
        close = df['close']
        values = close.values
        values = values.reshape(-1, 1)

        training_scaler = MinMaxScaler(feature_range=(0, 1))

        testing_input = values
        testing_input = training_scaler.fit_transform(testing_input)
        testing = []
        for i in range(50, len(testing_input) + 50):
            testing.append(testing_input[i - 50:i][0])

        testing = np.array(testing)
        testing = np.reshape(testing, (testing.shape[0], testing.shape[1], 1))
        predict = model.predict(testing)
        predict = training_scaler.inverse_transform(predict)

        plt.plot(values, color='blue', label='Stock Price')
        plt.plot(predict, color='red', label='Predicted Stock Price')
        plt.title('Changes')
        plt.xlabel('Timeline')
        plt.ylabel('Stock Price')
        plt.legend()
        plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/xmQ72EiI.png"" rel=""nofollow noreferrer"">My results</a></p>
<p>It turns out that the model predicts data that I already know. How can I predict future data?</p>
","-1","Question"
"79348687","","<p>My current problem is with loading large amount of data from a table with about 5.000.000 rows from a SQL Server database.</p>
<p>The setup (which I can't influence) is:</p>
<ul>
<li>0 GPU</li>
<li>4000 CPU</li>
<li>15.0 Gi memory</li>
</ul>
<p>My SQL code is stored as a <code>.sql</code> file in the project folder.</p>
<p>I started on chunks of 500.000 rows, but that caused a kernel's crash. Tried 250.000 with the same result. Now on 100.000 and still crashing.</p>
<p>Based on the company rules, I have to make the initial connection to the database as shown below, which is working:</p>
<pre><code># Connection to SQL Server with Kerberos + pyodbc

def mssql_conn_kerberos(server, driver, trusted_connection, trust_server_certificate, kerberos_cmd):
   # Run Kerberos for authentifications
   os.system(kerberos_cmd)
    
   try:
   # First connection attempt
     c_conn = pyodbc.connect(
     f'DRIVER={driver};'
     f'SERVER={server};'
     f'Trusted_Connection={trusted_connection};'
     f'TrustServerCertificate={trust_server_certificate}'
     )
   except:
     # Re-run Kerberos and try authentification
     os.system(kerberos_cmd)
     c_conn = pyodbc.connect(
     f&quot;DRIVER={driver};&quot;
     f&quot;SERVER={server};&quot;
     f&quot;Trusted_Connection={trusted_connection};&quot;
     f&quot;TrustServerCertificate={trust_server_certificate}&quot;
     )
    
    c_cursor = c_conn.cursor()

    print(&quot;Pyodbc connection ready.&quot;)

    return c_conn # Connection to the database
</code></pre>
<p>Then I have a function to read and process my SQL query (which is in a <code>.sql</code> file saved in the project folder):</p>
<pre><code>def call_my_query(path_to_query, query_name, chunk, connection):
    
    file_path = os.path.join(path_to_query, query_name)
    with open(file_path, &quot;r&quot;) as file:
        query = file.read()

    # SQL processing in chunks + time
    chunks = []
    start_time = time.time()

    for x in pd.read_sql_query(query, connection, chunksize=chunk):
        chunks.append(x)

    # Concating the chunks - joining all the chunks together
    df = pd.concat(chunks, ignore_index=True)

    # Process end-time
    end_time = time.time()

    print(&quot;Data loaded successfully!&quot;)
    print(f'Processed {len(df)} rows in {end_time - start_time:.2f} seconds')

    return df
</code></pre>
<p>Which leads to Kernel crash:</p>
<blockquote>
<p>The Kernel crashed while executing code in the current cell or a previous cell.<br />
Please review the code in the cell(s) to identify a possible cause of the failure.<br />
Click here for more info.<br />
View Jupyter log for further details.</p>
</blockquote>
<p>I also tried to run this task via Dask, with altering the <code>call_my_query</code> function, but for some reason Dask causes troubles with pyodbc.</p>
<p>Alteration of the <code>call_my_query</code> for Dask:</p>
<pre><code>def call_my_query_dask(query_name, chunk, connection, index_col):
    
    # Load query from file
    file_path = os.path.join(path_to_query, query_name)
    with open(file_path, &quot;r&quot;) as file:
        query_original = file.read()

    # Convert the SQL string/text
    query = sqlalchemy.select(query_original)

    # Start timing the process
    start_time = time.time()

    # Use Dask to read the SQL query in chunks
    print(&quot;Executing query and loading data with Dask...&quot;)
    df_dask = dd.read_sql_query(
        sql=query,
        con=connection_url,
        npartitions=10,
        index_col = index_col
    )

    # Process end-time
    end_time = time.time()
    print(&quot;Data loaded successfully!&quot;)
    print(f&quot;Processed approximately {df_dask.shape[0].compute()} rows in {end_time - start_time:.2f} seconds&quot;)

    return df_dask
</code></pre>
<p>Which is causing this error:</p>
<blockquote>
<p>Textual column expression 'SELECT\n\t[COL1]\n\t, [COL...' should be explicitly declared with text('SELECT\n\t[COL1]\n\t, [COL...'), or use literal_column('SELECT\n\t[COL1]\n\t, [COL...') for more specificity</p>
</blockquote>
<p>Thank you all for any help.</p>
","0","Question"
"79349015","","<p>I'm testing various algorithms on solving various tasks.
Tasks has some characteristics, naturally dividing them into classes.
The classes are uneven and has different number of tasks.</p>
<p>Algorithms has some performance on solving tasks, but may also fail to solve a task completely. Failed tasks are not recorded.</p>
<p>I want to show success rate of each algo in each task class in a histogram-like chart,
but with values normalized in each bin to total number of tasks in each bin.</p>
<p>Tasks are in a datafame like:</p>
<pre><code>Index ['task']: 360 entries, t0001 to t0360
Data columns (total 7 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   V       360 non-null    float64
 1   nk      360 non-null    int64  
 2   nt      360 non-null    int64  
 3   nm      360 non-null    int64  
 4   Vtot    360 non-null    float64
...
 dtypes: float64(4), int64(3)
</code></pre>
<p>Results are in a dataframe like:</p>
<pre><code>MultiIndex ['task', 'algo']: 862 entries, ('t0001', 'H0') to ('t0080', 'TDC-87-06-03')
Data columns (total 19 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   time    862 non-null    float64
 1   qmem    862 non-null    int64
...
dtypes: float64(17), int64(2)
</code></pre>
<p>In SQL (not sure if it's proper and valid though) that would be something like:</p>
<pre class=""lang-none prettyprint-override""><code>WITH Bins(bin, floor, ceil, total) AS (
  SELECT
     &quot;something&quot; AS bin,
     foo - 0.5 AS floor,
     foo + 0.5 AS ceil,
     count(*) AS total 
  FROM Tasks
  GROUPBY bin
)
SELECT Results.algo, Bins.bin, count(*) / Bins.total as rate
FROM Results, Tasks, Bins ON Results.task = Tasks.task AND Tasks.foo BETWEEN(Bins.floor, Bins.ceil) 
GROUPBY algo, Bins.bin
</code></pre>
<p>And then plot it like <code>px.bars(df, color='algo', x='bin', y='rate')</code></p>
<p>It's almost the same as simply <code>px.histogram(df, color='algo', x='foo', nbins=whatever)</code> on a merged tasks+results dataframe -- but that plots bars in absolute counts.</p>
<p>The desired feature is that <code>count(*) / Bins.total</code> to show bars relative to total size of bin.</p>
","0","Question"
"79349123","","<p>Objective: Calculate the expanding mean of the 'valuation' column for each 'slug' group, excluding the current row's value (and respecting ordering i.e. 'week').</p>
<p>Example dataset (output is the desired result).</p>
<pre><code>idx week    slug    valuation   output
0   2   slouk   -4  12.00
1   3   slouk   7   4.00
2   4   slouk   8   5.00
3   3   kenun   10  14.00
4   1   kenun   11  
5   1   slouk   12  
6   2   kenun   17  11.00
7   4   kenun   21  12.67
</code></pre>
<p>I have tried (and failed) :</p>
<pre><code># chaining
td[&quot;output&quot;] = (
    td.sort_values(by=&quot;week&quot;)
    .groupby(&quot;slug&quot;)[&quot;valuation&quot;]
    .shift()
    .expanding()
    .mean()
    .reset_index(drop=True)
)

# apply
td[&quot;output&quot;] = (
    td.sort_values(by=&quot;week&quot;)
    .groupby(&quot;slug&quot;)[&quot;valuation&quot;]
    .apply(lambda x: x.shift().expanding().mean())
    .reset_index(drop=True)
)
</code></pre>
<p>Then I stumbled across this <a href=""https://stackoverflow.com/questions/76065644/pandas-expanding-average-per-group-without-changing-the-order-of-the-rows"">related topic</a> and the <code>.sort_index(level=1)</code> did the trick.</p>
<p>So now, this is works :</p>
<pre><code>td[&quot;output&quot;] = (
    td.sort_values(by=&quot;week&quot;)
    .groupby(&quot;slug&quot;)[&quot;valuation&quot;]
    .apply(lambda x: x.shift().expanding().mean())
    .sort_index(level=1)
    .reset_index(drop=True)
)
</code></pre>
<p>The &quot;apply&quot; version works, though is rather slow on a large dataset. When I try the <code>.sort_index(level=1)</code> with the &quot;chaining&quot; version, this still does not work.</p>
<p>Although it works, I still have several points I'd like to understand better :</p>
<ul>
<li><p>Chaining Methods Issue: When I use method chaining like <code>td.groupby('slug')['valuation'].shift().expanding().mean()</code>, it seems to lose track of the grouping after a certain point. I'm puzzled about why this happens and how it differs from using apply.</p>
</li>
<li><p>Indexing and Assignment with apply: Using apply with a lambda function performs the operation correctly, but the resulting Series has a different order than my original DataFrame. When I try to assign it back using <code>.reset_index(drop=True)</code>, things don't align properly. I'm seeking clarity on how this reassignment process works.</p>
</li>
<li><p>Is there any other, more efficient way to do what i am after?</p>
</li>
</ul>
<p>My goal is to build a better understanding of these methods.</p>
","0","Question"
"79349155","","<p>I have tried to do this in order to create a new column, with each row being an array containing the values of column b multiplied by column a.</p>
<pre><code>data = {'a': [3, 2], 'b': [[4], [7, 2]]}
df = pd.DataFrame(data)
df['c'] = df.apply(lambda row: [row['a'] * x for x in row['b']])
</code></pre>
<p>The final result should look like this</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">a</th>
<th style=""text-align: center;"">b</th>
<th style=""text-align: right;"">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: center;"">[4]</td>
<td style=""text-align: right;"">[12]</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">[7, 2]</td>
<td style=""text-align: right;"">[14, 4]</td>
</tr>
</tbody>
</table></div>
","2","Question"
"79349347","","<p>I have a DataFrame with tuples in cells:</p>
<pre><code>import numpy as np
import pandas as pd

data = np.empty((3, 4), dtype=[('cost', np.int32), ('count', np.int32)])

data['count'] = 0
data['cost']  = [[10, 2, 20, 11],
                 [12,  7,  9, 20],
                 [ 2, 14, 16, 18]]

sup            = [1, 2, 3]

df = pd.DataFrame(data=data.tolist(), columns=[f'{i+1}' for i in range(len(data[0]))])
df = df.join(pd.DataFrame(sup, columns=['Supplies']))

print(df)
</code></pre>
<p>Which gives the following output:</p>
<pre><code>         1        2        3        4  Supplies
0  (10, 0)   (2, 0)  (20, 0)  (11, 0)         1
1  (12, 0)   (7, 0)   (9, 0)  (20, 0)         2
2   (2, 0)  (14, 0)  (16, 0)  (18, 0)         3
</code></pre>
<p>Next, I'd like to update a cell:</p>
<pre><code>df.loc[1, '1'] = (1,2)
</code></pre>
<p>but I'm getting <code>ValueError: Must have equal len keys and value when setting with an iterable</code></p>
<p>The same issue happens with a list. Assignment of any not iterable data types like string or integer works well.</p>
<p>Next, if I remove this join:</p>
<pre><code>df = df.join(pd.DataFrame(sup, columns=['Supplies']))
</code></pre>
<p>I can assign cells with tuples without any problem.</p>
<p>I guess it happens because it tries to update not a single cell (but what?)</p>
<p>Meanwhile, the assignment with <code>df.at</code> works well, but why not <code>df.loc</code>?</p>
<pre><code>df.at[1, '1'] = (1,2)
</code></pre>
","2","Question"
"79350196","","<p>Not able to use explode from pandas df.</p>
<p>I want to &quot;explode&quot; a named column with named sub columns in a data frame on Jupyter Notebook.</p>
<p>Here is the data frame :</p>
<pre class=""lang-none prettyprint-override""><code>   State or territory  Census population[8][9][a]               
   State or territory         July 1, 2024 (est.)  April 1, 2020
0          California                  39431263.0       39538223
1               Texas                  31290831.0       29145505
2             Florida                  23372215.0       21538187
3            New York                  19867248.0       20201249
4        Pennsylvania                  13078751.0       13002700
</code></pre>
<p>I want to explode Census population and then delete April 1 2020 leaving &quot;State or territory&quot; and &quot;July 1, 2024 (est.)&quot;</p>
<pre><code>import pandas as pd

tables1 = pd.read_html(&quot;https://en.wikipedia.org/wiki/Fortune_500&quot;)
tables2 = pd.read_html(
    &quot;https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population&quot;)
</code></pre>
<pre><code>df1 = tables[1]
df2 = tables2[0]
</code></pre>
<pre><code>df1copy = df1.drop([&quot;Rank&quot;], axis=1)
df2copy = df2.drop(
    [&quot;Change, 2010–2020[9][a]&quot;,
     &quot;House seats[b]&quot;,
     &quot;Pop.  per elec. vote (2020)[c]&quot;,
     &quot;Pop. per seat (2020)[a]&quot;,
     &quot;% US (2020)&quot;,
     &quot;% EC (2020)&quot;],
    axis=1)
print(df1copy.head())
print(df2copy.head())
df2.drop([&quot;July 1, 2024 (est.)&quot;], axis=1)
print(df2.head())
</code></pre>
<p>Here is the result:</p>
<pre><code>KeyError  Traceback (most recent call last)
  File ~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)
    3804 try:
 -&gt; 3805     return self._engine.get_loc(casted_key)
    3806 except KeyError as err:
KeyError: 'July 1, 2024 (est.)'
</code></pre>
","0","Question"
"79350477","","<p>In short, the code I was running two days ago worked perfectly. However, after attempting to mess with python (rookie mistake) I began receiving the following error.</p>
<pre><code>isseyyohannes@Isseys-MBP ~ % /usr/local/bin/python3 /Users/isseyyohannes/Desktop/afkasjjsf
[*********************100%***********************]  1 of 1 completed
Traceback (most recent call last):
  File &quot;/Users/isseyyohannes/Desktop/afkasjjsf&quot;, line 47, in &lt;module&gt;
    data['above_200_SMA'] = data['Close'] &gt; data['SMA200']
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/ops/common.py&quot;, line 76, in new_method
    return method(self, other)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/arraylike.py&quot;, line 56, in __gt__
    return self._cmp_method(other, operator.gt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py&quot;, line 7897, in _cmp_method
    self, other = self._align_for_op(other, axis, flex=False, level=None)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py&quot;, line 8205, in _align_for_op
    raise ValueError(
ValueError: Operands are not aligned. Do `left, right = left.align(right, axis=1, copy=False)` before operating.
</code></pre>
<p>The minimum reproducible code which shows the error on my terminal is as follows</p>
<pre><code>#!/usr/bin/env python3

import pandas as pd
import yfinance as yf

################### Grabs Data/Shows Historical Returns (A) ######################


#Companies (US Stocks)
# Function to fetch data for historical backtesting with user-defined tickers
def fetch_data_with_buffer():
    # Ask the user for ticker(s) for historical backtesting
    tickers =  'ABBV'
    
    # Fetch 1 year of daily data for the tickers provided by the user
    data = yf.download(tickers, period='2y', interval='1d')
    
    # Return the fetched data
    return data



# Define a function to calculate moving averages
def calculate_moving_averages(df):
    df['SMA10'] = df['Close'].rolling(window=10).mean()
    df['SMA20'] = df['Close'].rolling(window=20).mean()
    df['SMA50'] = df['Close'].rolling(window=50).mean()
    df['SMA200'] = df['Close'].rolling(window=200).mean()


    df['Slope10'] = df['SMA10'].diff()  # Calculate the difference between consecutive SMA10 values
    df['Slope20'] = df['SMA20'].diff()  # Slope of SMA20
    df['Slope50'] = df['SMA50'].diff()  # Slope of SMA50
    df['Slope200'] = df['SMA200'].diff()  # Slope of SMA200

    # Check for negative slope condition for SMA200
    df['isNegativeSlope'] = df['SMA200'] &lt; df['SMA200'].shift(1)
    
    return df


# If you want to fetch data for multiple tickers:
data = fetch_data_with_buffer()  
data = calculate_moving_averages(data)

# Logic for coloring candles based on moving averages
data['above_200_SMA'] = data['Close'] &gt; data['SMA200']
</code></pre>
<p>I am not looking to alter my code (complex) as much as I am trying to make it operate as it once was. If any insight can be provided as to which version of pandas would have enabled this code to operate without trouble would be appreciated.</p>
<p><strong>SMA's naturally have NaN values for first few periods.</strong></p>
<p><strong>For example, SMA200 has NaN values for the first 200 rows of data frame because it has to compile 200 days/rows of data. This is normal, I would simply want the code to ignore NaN values as it once was and not plot SMA200 (or anything if necessary) for those rows.</strong></p>
<p>I have exhausted my solutions (changing python/pandas versions and checking permissions to pandas directories listed in the error). Thank you.</p>
","3","Question"
"79352565","","<p>Is there a way to vectorize the following three-nested loop that calcuate the daily mean of hourly data? The function below loops first over the year, then months, and finally over days. It also check for the last month and day to ensure that the loop does not go beyond the last month or day of the data.</p>
<pre><code>def hourly2daily(my_var,my_periods):
 
    import pandas as pd 
    import numpy as np
    import sys
    
    print('######### Daily2monthly function ##################')
    Frs_year   =my_periods[0].year
    Frs_month  =my_periods[0].month
    Frs_day    =my_periods[0].day
    Frs_hour   =my_periods[0].hour
    
    Last_year  =my_periods[-1].year
    Last_month =my_periods[-1].month
    Last_day   =my_periods[-1].day
    Last_hour  =my_periods[-1].hour
    

    print('First year   is '+str(Frs_year) +'\n'+\
          'First months is '+str(Frs_month)+'\n'+\
          'First day    is '+str(Frs_day)+'\n'+\
          'First hour   is '+str(Frs_hour))
    print('        ')
    
    print('Last  year   is '+str(Last_year)+'\n'+\
          'Last  months is '+str(Last_month)+'\n'+\
          'Last  day    is '+str(Last_day)+'\n'+\
          'Last  hour   is '+str(Last_hour))
    
    
    Frs = str(Frs_year)+'/'+str(Frs_month)+'/'+str(Frs_day)+' '+str(Frs_hour)+&quot;:00&quot;
    
    Lst = str(Last_year)+'/'+str(Last_month)+'/'+str(Last_day)+' '+str(Last_hour)+&quot;:00&quot;
    
    my_daily_time=pd.date_range(Frs,Lst,freq='D')
    
    ## END of the data_range tricks ###########

    nt_days=len(my_daily_time)
    nd=np.ndim(my_var)

        
    if (nd == 1): # only time series
        var_mean=np.full((nt_days),np.nan)
    
    if (nd == 2): # e.g., time, lat or lon or lev
        n1=np.shape(my_var)[1]
        var_mean=np.full((nt_days,n1),np.nan)
    
    if (nd == 3): #  e.g., time, lat, lon 
        n1=np.shape(my_var)[1]
        n2=np.shape(my_var)[2]
        var_mean=np.full((nt_days,n1,n2),np.nan)
    
    if (nd == 4): # e.g., time, lat , lon, lev 
        n1=np.shape(my_var)[1]
        n2=np.shape(my_var)[2]
        n3=np.shape(my_var)[3]
        var_mean=np.full((nt_days,n1,n2,n3),np.nan)
    
    end_mm=12
    k=0
    ####### loop over years ################
    for yy in np.arange(Frs_year,Last_year+1):
        print('working on the '+str(yy))
        # in case the last month is NOT 12
        if (yy == Last_year):
            end_mm=Last_month 
            print('The last month is '+str(end_mm))
        ## Loop over months ################
        for mm in np.arange(1,end_mm+1):
            end_day=pd.Period(str(yy)+'-'+str(mm)).days_in_month
            # in case the last day is not at the end of the month.
            if ((yy == Last_year) &amp; (mm == Last_month)):
                end_day=Last_day 
            #### loop over days ###############
            for dd in np.arange(1,end_day+1):
                print(str(yy)+'-'+str(mm)+'-'+str(dd))
                #list all days of the month and year.
                I=np.where((my_periods.year ==  yy) &amp;\
                           (my_periods.month == mm) &amp;\
                           (my_periods.day == dd  ))[0]
                
                print(I)
                # if there is a discontinuity in time.
                # I will be empty and then you have to quit. 
                # you have first to reindex the data. 
                if len(I) == 0 :
                    print('Warning time shift here &gt;&gt;')
                    print('Check the continuity of your time sequence')
                    sys.exit()
                
                var_mean[k,...]=np.nanmean(my_var[I,...],0)
                k=k+1
            
        
    return var_mean,my_daily_time
</code></pre>
<p>Here is, perhaps, easy and quick way to call this function.
Note that you may be asked to install Pooch</p>
<pre><code>import numpy as np
import xarray as xr

x = xr.tutorial.load_dataset(&quot;air_temperature&quot;)
time      = x['time']      # reading the time
period=time.to_index().to_period('h')
bb0,bb1=hourly2daily(x['air'],period)
</code></pre>
<p>I am aware that there is another way to implement this; for example, I can do the previous calculation in one single loop as shown below, but it won’t help for data with discontinues in time.</p>
<pre><code>daily_tem2m = np.full((int(len_time/24),len_lat,len_lon),np.nan,float)

counter=0
timemm=[]
for i in np.arange(0,len_time,24):
    print(period[i])
    timemm.append(period[i])
    daily_tem2m[counter,:,:]=np.nanmean(cleaned_tem2m_celsius.data[i:i+24,:,:],0)
    counter=counter+1
</code></pre>
","0","Question"
"79353068","","<p>I'm doing comparisons (equality) of some series which have some NaN elements and numeric elements. I'd like every comparison involving a NaN to return NaN instead of False - what's the best Numpy function to do this?</p>
<pre><code>df = pd.DataFrame({'a': [np.NaN, np.NaN, 1], 'b': [np.NaN, 1, 1]})

df['a'] == df['b']
</code></pre>
<p>gives</p>
<pre><code>0    False
1    False
2     True
dtype: bool
</code></pre>
<p>when I'd like it to return</p>
<pre><code>0    NaN
1    NaN
2    1
dtype: float
</code></pre>
<p>or</p>
<pre><code>0    NaN
1    NaN
2    True
dtype: bool
</code></pre>
","1","Question"
"79353192","","<p>I am interested in seeing the source code of the <code>pandas.Series.reindex</code>,  so I jumped to the source code using the link in the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.reindex.html"" rel=""nofollow noreferrer"">documentation page</a>, yet I found a return to other function <a href=""https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/series.py#L5136-L5161"" rel=""nofollow noreferrer"">super().reindex</a>, which I could not yet find out where this function was introduced.
How can I get into the definition of this function  <code>pandas.Series.reindex</code>?</p>
","0","Question"
"79354118","","<p>I am trying to scrape the pitching stats on this url and then save the dataframe to a csv file.</p>
<p><a href=""https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml"" rel=""nofollow noreferrer"">https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml</a></p>
<p>My  current code is below (Python 3.9.7)</p>
<pre><code>_URL = &quot;https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml&quot;
data = pd.read_html(_URL,attrs={'id': 'ArizonaDiamondbackspitching'},header=1)[0]
data.to_csv('boxscore.csv', index='False')
return data
</code></pre>
<p>When I run this code I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;d:\BaseballAlgo\Baseball_WhoWins.py&quot;, line 205, in &lt;module&gt;
    getBoxScore('ARI','2022-04-07')
  File &quot;d:\BaseballAlgo\Baseball_WhoWins.py&quot;, line 99, in getBoxScore
    data = pd.read_html(_URL,attrs={'id': 'ArizonaDiamondbackspitching'},header=1)[0]
  File &quot;D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py&quot;, line 1240, in   read_html
    return _parse(
  File &quot;D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py&quot;, line 1003, in _parse
    raise retained
  File &quot;D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py&quot;, line 983, in   _parse
    tables = p.parse_tables()
  File &quot;D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py&quot;, line 249, in parse_tables
    tables = self._parse_tables(self._build_doc(), self.match, self.attrs)
  File &quot;D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py&quot;, line 598, in   _parse_tables
    raise ValueError(&quot;No tables found&quot;)
ValueError: No tables found
</code></pre>
<p>Past iterations of code:</p>
<pre><code>session = BRefSession()
_URL = &quot;https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml&quot;
content =session.get(_URL).content
soup = BeautifulSoup(content, &quot;html.parser&quot;)
table = soup.find_all('table', id=&quot;ArizonaDiamondbackspitching&quot;)
print (table)
data = pd.read_html(StringIO(str(table)))[0]
</code></pre>
<p>This code runs and when it prints the table the output is &quot;[]&quot;. The same traceback above is also outputted as a result of the last line.</p>
<p>I understand what the error is saying but I simply do not understand how that possible. It seems as if the soup.findall function is not able to find the specific table I need but I am not sure why. How can I fix this issue?</p>
","1","Question"
"79354633","","<p>I have some data that is saved in a dictionary of dataframes. The real data is much bigger with index up to 3000 and more columns.</p>
<p>In the end I want to make a violinplot of two of the columns in the dataframes but for multiple dictionary entries. The dictionary has a tuple as a key and I want to gather all entries which first number is the same.</p>
<pre><code>
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data_dict = {
    (5, 1): pd.DataFrame({&quot;Data_1&quot;: [0.235954, 0.739301, 0.443639],
                          &quot;Data_2&quot;: [0.069884, 0.236283, 0.458250],
                          &quot;Data_3&quot;: [0.170902, 0.496346, 0.399278],
                          &quot;Data_4&quot;: [0.888658, 0.591893, 0.381895]}),
    (5, 2): pd.DataFrame({&quot;Data_1&quot;: [0.806812, 0.224321, 0.504660],
                          &quot;Data_2&quot;: [0.070355, 0.943047, 0.579285],
                          &quot;Data_3&quot;: [0.526866, 0.251339, 0.600688],
                          &quot;Data_4&quot;: [0.283107, 0.409486, 0.307315]}),
    (7, 3): pd.DataFrame({&quot;Data_1&quot;: [0.415159, 0.834547, 0.170972],
                          &quot;Data_2&quot;: [0.125926, 0.401789, 0.759203],
                          &quot;Data_3&quot;: [0.398494, 0.587857, 0.130558],
                          &quot;Data_4&quot;: [0.202393, 0.395692, 0.035602]}),
    (7, 4): pd.DataFrame({&quot;Data_1&quot;: [0.923432, 0.622174, 0.185039],
                          &quot;Data_2&quot;: [0.759154, 0.126699, 0.783596],
                          &quot;Data_3&quot;: [0.075643, 0.287721, 0.939428],
                          &quot;Data_4&quot;: [0.983739, 0.738550, 0.108639]})
}
</code></pre>
<p>My idea was that I could re-arrange it into a different dictionary and then plot the violinplot. Say that 'Data_1' and 'Data_4' are of interest. So then I loop over the keys in <code>dict</code> as below.</p>
<pre><code>new_dict = {}
for col in ['Data_1','Data_4']:
    df = pd.DataFrame()
    for i in [5,7]:
        temp = []   
        for key, value in dict.items():
            if key[0]==i:
                temp.extend(value[col])
        df[i] = temp
    new_dict[col] = df
</code></pre>
<p>This then make the following dict.</p>
<pre><code>new_dict = 
{'Data_1':           5         7
 0  0.235954  0.415159
 1  0.739301  0.834547
 2  0.443639  0.170972
 3  0.806812  0.923432
 4  0.224321  0.622174
 5  0.504660  0.185039,
 'Data_4':           5         7
 0  0.888658  0.202393
 1  0.591893  0.395692
 2  0.381895  0.035602
 3  0.283107  0.983739
 4  0.409486  0.738550
 5  0.307315  0.108639}
</code></pre>
<p>Which I then loop over to make the violin plots for <code>Data_1</code>and <code>Data_4</code>.</p>
<pre><code>for key, value in new_dict.items():
    fig, ax = plt.subplots()
    ax.violinplot(value, showmeans= True)
    ax.set(title = key, xlabel = 'Section', ylabel = 'Value')
    ax.set_xticks(np.arange(1,3), labels=['5','7'])
</code></pre>
<p>While I get the desired result it's very cumbersome to re-arrange the dictionary. Could this be done in a faster way? Since it's the same column I want for each dictionary entry I feel that it should.</p>
","2","Question"
"79355334","","<p>I have a Pandas dataframe that looks like:</p>
<pre><code>Group_ID   feature1   feature2   label
1          3          2          0
1          5          7          0
1          2          4          1
1          9          9          1
1          2          0          1
2          4          1          1
2          8          8          0
2          5          5          0
3          0          9          1
3          4          7          1
3          2          3          0
3          7          2          0
</code></pre>
<p>and for each feature i, I would like to create a new feature called <code>featurei_rel</code> for i=1,2 using the following logic: it is given by feature i for that row divided by the mean of the smallest two feature i in the same group.</p>
<p>So for example, for feature1 row 1, the smallest two values in Group 1 are 2 and 2 (row 3 and 5 respectively), hence feature1_rel for row 1 is given by 3/((2+2)/2) = 3/2 = 1.5 and the desired result looks like</p>
<pre><code>Group_ID   feature1   feature2   feature1_rel   feature2_rel   label
1          3          2          3/((2+2)/2)    2/((0+2)/2)    0
1          5          7          5/((2+2)/2)    7/((0+2)/2)    0
1          2          4          2//((2+2)/2)   4/((0+2)/2)    1
1          9          9          9//((2+2)/2)   9/((0+2)/2)    1
1          2          0          2/((2+2)/2)    0/((0+2)/2)    1
2          4          1          4/((4+5)/2)    1/((1+5)/2)    1
2          8          8          8/((4+5)/2)    8/((1+5)/2)    0
2          5          5          5/((4+5)/2)    5/((1+5)/2)    0
3          0          9          0/((0+2)/2)    9/((2+3)/2)    1
3          4          7          4/((0+2)/2)    7/((2+3)/2)    1
3          2          3          2/((0+2)/2)    3/((2+3)/2)    0
3          7          2          7/((0+2)/2)    2/((2+3)/2)    0
</code></pre>
<p>So here is what I have tried:</p>
<pre><code># create columns to find the smallest element in a group
df['feature1min'] = df.groupby('Group_ID')['feature1'].transform('min')
# create columns to find the second smallest element in a group
df['feature1min2'] = df.groupby('Group_ID')['feature1'].nsmallest(2)

df['feature1_rel'] = df['feature1']/((df['feature1min'] + df['feature1min2']) / 2)
</code></pre>
<p>However, in my actual dataset, I have hundreds of features and millions of rows, so I was wondering is there any fast way to do it, thank you so much in advance.</p>
","1","Question"
"79356088","","<p>def request_pickup(connections, request_id):</p>
<pre><code>env_path = connections.loc[0, 'm_drive']

hip_directory = connections.loc[0, 'hip_directory']

sys.path.append(env_path + '/global_info')

from py_variables import sqlalchemy_cnx_string, ssl_args

engine = sqlalchemy.create_engine(sqlalchemy_cnx_string,

                                  connect_args = ssl_args)



pickup_info_query = (f'''

    SELECT

        fld40 as record_id,

        fld3 as client_name

    FROM rmf_tb.rfsocatalogrh

    WHERE fld40 = {request_id}''')

pickup_info = pd.read_sql(pickup_info_query, engine)



if len(pickup_info) != 1:

    sys.exit(&quot;Pickup Request Error&quot;)
</code></pre>
<p>The line that it is failing is on the pd.read_sql</p>
<p>Our goal is to be able to run python script without any error as we did before.</p>
<p>We are using a python tool no configuration was necessary for this tool.</p>
<p>We able to run workflow on local designer with out any issue.</p>
<p>But workflow fails on the server with error attached</p>
<p>SQLAlchemy Version: 1.3.18
Pandas Version: 2.0.3</p>
","0","Question"
"79356240","","<p>I have a unstructured dataframe like below:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>khan</td>
<td>Salary</td>
<td>5000</td>
</tr>
<tr>
<td>Age</td>
<td>42</td>
<td>phone</td>
<td>01783232575</td>
</tr>
</tbody>
</table></div>
<p>I want to map the value based on my keyword. For example, map value from right index where index with name value is found. My final output data should be like below dataframe:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Age</th>
<th>Salary</th>
<th>Phone</th>
</tr>
</thead>
<tbody>
<tr>
<td>khan</td>
<td>42</td>
<td>5000</td>
<td>01783232575</td>
</tr>
</tbody>
</table></div>
","0","Question"
"79356690","","<pre><code>#Column X contains the suffix of one of V* columns. Need to put set column V{X} to 9 if X &gt; 1.  
#But my code created a new column 'VX' instead of updating one of the V* columns
    
import pandas as pd

df = pd.DataFrame({'EMPLID': [12, 13, 14, 15, 16, 17, 18],
    'V1': [2,3,4,50,6,7,8],
    'V2': [3,3,3,3,3,3,3],
    'V3': [7,15,8,9,10,11,12],
    'X': [2,3,1,3,3,1,2]
})

# Expected output:
     
#    EMPLID  V1  V2  V3  X  
#    12       2   9   7  2  
#    13       3   3   9  3 
#    14       4   3   8  1
#    15      50   3   9  3 
#    16       6   3   9  3
#    17       7   3  11  1  
#    18       8   9  12  2 
</code></pre>
<p>My code created a new column 'VX' instead of updating one of the V* columns:</p>
<pre><code>df.loc[(df['X'] &gt; 1), f&quot;V{'X'}&quot;] = 9
</code></pre>
<p>Any suggestion is appreciated. Thank you.</p>
","1","Question"
"79357226","","<p>I'm trying to avoid a for loop with DateTimeIndex. I have a function <code>get_latest</code> that looks up the most recent wage index value. When I step through the dates of pay days, the lookup works fine. When I attempt to vectorize the operation, I'm advised <code>TypeError: 'numpy.ndarray' object is not callable</code>. I've tried all manners of dt, date, to_pydatetime, etc. to no avail.</p>
<pre><code># payroll.py
# %%
import pandas as pd
import datetime

# %%
def get_latest(date, series):
    return series.loc[max([x for x in series.index if x &lt;= date])]

# %%
start_rate = 1000.0
start = '2025-08-01'
end = '2026-02-01'

raise_series = pd.Series ({
    datetime.date(2024, 10, 1) : 0.06,
    datetime.date(2025, 10, 1) : 0.05,
    datetime.date(2026, 1, 1) : 1.055,
    datetime.date(2026, 10, 1) : 0.04,
    datetime.date(2027, 10, 1) : 0.04,
    datetime.date(2028, 10, 1) : 0.04,
    datetime.date(2029, 10, 1) : 0.04
})

# %%
initial_index = pd.Series ({
    datetime.date(1, 1, 1): 0.00
})

index_series = pd.concat([initial_index, raise_series], axis=0)
index_series += 1
index_series = index_series.cumprod()

# %%
pay_days = pd.date_range(start=start, end=end, freq='2W')

# %%
gross = []
for row in pay_days:
    gross.append(get_latest(row.date(), index_series) * start_rate) 
pay_days_gross1 = pd.Series(gross)
# %%
pay_days_gross2 = get_latest(pay_days.date(), index_series) * start_rate
# %%
</code></pre>
","0","Question"
"79357992","","<p>matplotlib crashes when a Pandas Serie contains specific special chars in a string, in my case <code>$$</code>.</p>
<pre><code>import matplotlib.pyplot as plt
import random
import pandas as pd
list_= 'abcdefghijklmnopqrstuvwxyz'

l = pd.Series()
for i in range(0,100):
    l[i] = random.choice(list_)
l[50] = '$$'
l.value_counts(normalize=False).plot(kind='bar')
plt.show()
</code></pre>
<p>This code will crash due to the <code>l[50] = '$$'</code> line.</p>
<p>Question: am I expected to clean such strings beforehand, or is it a bug in matplotlib?</p>
<p>I'm fairly new to using python for data science, so bear with my naive approach.<br />
Thanks</p>
<p>EDIT: thanks to @mozway and @chrslg for their answers, <code>$$</code> is indeed interpreted as Latex by matplotlib and it raises an error because there's nothing between the two <code>$</code> signs.</p>
<p>Having no control over the data I'm plotting, I've opted to deactivate the parsing of Latex by matplotlib like so:<br />
<code>plt.rcParams['text.parse_math'] = False</code></p>
","0","Question"
"79360275","","<p>I have a table named &quot;products&quot; on SQL Server. I would like to read the table into a DataFrame in Python using SQLAlchemy.</p>
<p>The pandas.read_sql function has a &quot;sql&quot; parameter that accepts two types of SQLAlchemy &quot;selectable&quot; objects:</p>
<ul>
<li>select object</li>
<li>text object</li>
</ul>
<p>I succeeded in using the text object but failed in using the select object. What corrections are needed?</p>
<p>successful code using text object:</p>
<pre><code>import pandas as pd

from sqlalchemy import text
from sqlalchemy.engine import URL
from sqlalchemy import create_engine


url_object = URL.create(
    &quot;mssql+pyodbc&quot;,
    host=&quot;abgsql.xx.xx.ac.uk&quot;,
    database=&quot;ABG&quot;,
    query={
        &quot;driver&quot;: &quot;ODBC Driver 18 for SQL Server&quot;,
        &quot;TrustServerCertificate&quot;: &quot;yes&quot;,
    },
)
engine = create_engine(url_object)
stmt = text(&quot;SELECT * FROM products&quot;)
df = pd.read_sql(sql=stmt, con=engine)
</code></pre>
<p>not successful code using select object:</p>
<pre><code>import pandas as pd

from sqlalchemy import select
from sqlalchemy.engine import URL
from sqlalchemy import create_engine

from sqlalchemy import Table, MetaData


url_object = URL.create(
    &quot;mssql+pyodbc&quot;,
    host=&quot;abgsql.xx.xx.ac.uk&quot;,
    database=&quot;ABG&quot;,
    query={
        &quot;driver&quot;: &quot;ODBC Driver 18 for SQL Server&quot;,
        &quot;TrustServerCertificate&quot;: &quot;yes&quot;,
    },
)
engine = create_engine(url_object)
products = Table(&quot;products&quot;, MetaData())
stmt = select(products)
df = pd.read_sql(sql=stmt, con=engine)
</code></pre>
<pre><code>sqlalchemy.exc.ProgrammingError: (pyodbc.ProgrammingError) ('42000', &quot;[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Incorrect syntax near the keyword 'FROM'. (156) (SQLExecDirectW)&quot;)
[SQL: SELECT
FROM products]
(Background on this error at: https://sqlalche.me/e/20/f405)
</code></pre>
","0","Question"
"79361211","","<p>I read in an excel file into a pandas dataframe. The first row in the excel file is a row of headers. I read the row of headers into a numpy array. I then use the replace function to modify the text in the headers in the array. For some reason, this also changes the actual dataframe it seems like.</p>
<p>I actually wrote the initial code in Python 3.7, where it works just fine. But the exact same code does not work in Python 3.13.1 or Python 3.12.4.</p>
<p>Below is an example of what I do (I tested the script and it runs in all three python versions but it only produces the expected result in 3.7):</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

df = pd.read_excel(&quot;path to excelfile&quot;)

# Based on header, I read in columns from the dataframe and do stuff with them.
header = df.keys()[1:] # Yes, I want to skip the first cell in the header.

# I use header_legend to make a nice legend in my plots.
header_legend = np.array(df.keys()[1:])

for i in range(0, len(header_legend)):
    header_legend[i] = header_legend[i].replace(&quot;word1_word2&quot;, &quot;word1 word2&quot;)

print(header)
print(header_legend)
</code></pre>
<p>Example CSV file (I guess just convert it to an excel file using excel, I use xlsx-format):</p>
<pre class=""lang-none prettyprint-override""><code>word_skip,word1_word2,word3_word4,word5_word6
1,10,100,1000
2,20,200,2000
3,30,300,3000
4,40,400,4000
5,50,500,5000
</code></pre>
<p>Now, in Python 3.7, <code>header</code> remained unmodified and <code>df.keys()[1:]</code> remained unmodified. Only <code>header_legend</code> is modified by the replace function. But in Python 3.12 and 3.13, <code>header</code> and <code>df.keys()[1:]</code> are also modified, although I only apply the replace function to <code>header_legend</code>.</p>
<p>Why does applying replace to <code>header_legend</code> also change <code>header</code> and <code>df.keys()[1:]</code> in Python 3.12 and 3.13?</p>
","2","Question"
"79361494","","<p>I am working with multiple Pandas DataFrames with a similar structure and would like to create reusable filters that I can define once and then apply or combine as needed.</p>
<p>The only working solution I came up with so far feels clunky to me and makes it hard to combine filters with OR:</p>
<pre><code>import pandas as pd
df = pd.DataFrame({&quot;A&quot;:[1,1,2],&quot;B&quot;:[1,2,3]})

def filter_A(df):
    return df.loc[df[&quot;A&quot;]==1]

def filter_B(df):
    return df.loc[df[&quot;B&quot;]==2]

print(filter_A(filter_B(df)).head())
</code></pre>
<p>I am hoping for something along the lines of</p>
<pre><code>filter_A = (df[&quot;A&quot;]==1)
filter_B = (df[&quot;B&quot;]==2)

print(df.loc[(filter_A) &amp; (filter_B)])
</code></pre>
<p>but reusable after changing the df and also applicable to other DataFrames with the same columns. Is there any cleaner or more readable way to do this?</p>
","3","Question"
"79361495","","<p>I'm trying to write a script that can collect information about phones and add it to a dataframe. I have such a dataset with customer ID. At the same time, the phone numbers are stored inside the web page in the form of a link.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>ID</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>20240514 May, 14 22:00</td>
<td>R_111</td>
<td>Le client ne répond pas</td>
</tr>
</tbody>
</table></div>
<p>I think you can take a list of ID customers from Dataframe and use the library to notify the phone number by ID. Example example.com/client/?id=111</p>
<p>The order (ID) page looks like this:</p>
<pre><code>%%html
&lt;!doctype html&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;id 111&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
    &lt;div&gt;
            &lt;div id=&quot;contactButton&quot; class=&quot;bg-primary-subtle py-2 px-3 rounded-3 text-primary fw-medium&quot; style=&quot;cursor: pointer&quot;&gt;
                Contact
            &lt;/div&gt;
            &lt;div class=&quot;d-flex flex-column position-relative mt-2 d-none&quot; id=&quot;contactBlock&quot;&gt;
                &lt;div id=&quot;phone&quot; class=&quot;position-absolute end-0 text-nowrap&quot;&gt;
                    &lt;a href=&quot;tel:+77777777777&quot; class=&quot;btn btn-lg btn-outline-primary fw-medium&quot;&gt;
                        
                    &lt;button class=&quot;btn btn-lg btn-outline-secondary fw-medium&quot; data-bs-toggle=&quot;modal&quot; data-bs-target=&quot;#exampleModal&quot;&gt;
                     
                    &lt;/button&gt;
                &lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>I want to get such a dataframe:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Phone</th>
</tr>
</thead>
<tbody>
<tr>
<td>R_111</td>
<td>777777777</td>
</tr>
</tbody>
</table></div>
<p>I wrote the following code:</p>
<pre><code>import requests
from bs4 import BeautifulSoup

def get_client_phone(client_id):
    # url client 
    _url = f&quot;https://example.com/client/?id={client_id}&quot;

    response = requests.get(_url, data=cfg.payload, headers=headers)
    
    # Status
    if response.status_code != 200:
        print(f&quot;Eror: {response.status_code}&quot;)
        return None

    # Parse page
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Find phone
    phone_element = soup.find(id='phone')
    
    if phone_element:
        # Extract phone
        phone_link = phone_element.find('a', href=True)
        if phone_link:
            phone_number = phone_link['href'].replace('tel:', '')  # Remove 'tel:'
            return phone_number
    else:
        print(&quot;The phone was not found&quot;)
        return None


client_id = 'R_111' 
phone_number = get_client_phone(client_id)

if phone_number:
    print(f&quot;Phone {client_id}: {phone_number}&quot;)
else:
    print(&quot;Error&quot;)
</code></pre>
","0","Question"
"79362404","","<p>I have a pandas dataframe which looks something like this.</p>
<pre><code>orig |  dest |  type | class |  BKT   | BKT_order | value   | fc_Cap | sc_Cap
-----+-------+-------+-------+--------+-----------+---------+--------+---------
 AMD |  TRY  |   SA  | fc    |  MA    |   1       |   12.04 |   20   |   50
 AMD |  TRY  |   SA  | fc    |  TY    |   2       |   11.5  |   20   |   50
 AMD |  TRY  |   SA  | fc    |  NY    |   3       |   17.7  |   20   |   50
 AMD |  TRY  |   SA  | fc    |  MU    |   4       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | fc    |  RE    |   1       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | sc    |  EW    |   5       |   07.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  PO    |   2       |   08.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  TU    |   3       |   12.5  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  MA    |   1       |   16.7  |   20   |   50
</code></pre>
<p>Also i have an override Dataframe which may look something like this:</p>
<pre><code>orig |  dest |  type |  max_BKT 
-----+-------+-------+-----------
 AMD |  TRY  |   SA  |  TY
 NCL |  MNK  |   PE  |  PO
 NCL |  AGZ  |   PE  |  PO
</code></pre>
<p>what i want to do is modify the original dataframe such that after comparison of  <code>orig</code> <code>dest</code> <code>type</code> &amp; <code>BKT</code> ( with <code>max_BKT</code>)  values, the <code>value</code> column for any rows which have the <code>BKT_order</code> higher than or equal to the <code>max_BKT</code> in override DF is set to either <code>fc_Cap</code> or <code>sc_Cap</code> depending on the <code>class</code> value.</p>
<p>For Example in above scenario,</p>
<p>Since the Override DF sets <code>max_BKT</code> as <code>TY</code> for <code>AMD |  TRY  |   SA</code> and the bucket order for <code>TY</code> is <code>2</code> in original Df, i need to set the <code>value</code> column equal to <code>fc_Cap</code> or <code>sc_Cap</code>
depending on the value of <code>class</code> for all rows where <code>BKT_order</code> &gt;= <code>2</code></p>
<p>So basically:</p>
<ul>
<li>filter the rows for <code>orig</code> <code>dest</code> <code>type</code> combination</li>
<li>Get the <code>BKT_order</code> of <code>max_BKT</code> from the Original DF</li>
<li>for each row that matches the above criteria
<ul>
<li>if <code>class == fc</code> update value column with fc_Cap</li>
<li>if <code>class == sc</code> update value column with sc_Cap</li>
</ul>
</li>
</ul>
<p>So our original DF looks something like this:</p>
<pre><code>orig |  dest |  type | class |  BKT   | BKT_order | value   | fc_Cap | sc_Cap
-----+-------+-------+-------+--------+-----------+---------+--------+---------
 AMD |  TRY  |   SA  | fc    |  MA    |   1       |   12.04 |   20   |   50
 AMD |  TRY  |   SA  | fc    |  TY    |   2       |   20    |   20   |   50
 AMD |  TRY  |   SA  | fc    |  NY    |   3       |   20    |   20   |   50
 AMD |  TRY  |   SA  | fc    |  MU    |   4       |   20    |   20   |   50
 AMD |  TRY  |   PE  | fc    |  RE    |   1       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | sc    |  EW    |   5       |   07.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  PO    |   2       |   50    |   20   |   50
 NCL |  MNK  |   PE  | sc    |  TU    |   3       |   50    |   20   |   50
 NCL |  MNK  |   PE  | sc    |  MA    |   1       |   16.7  |   20   |   50
</code></pre>
<p>I have tried an approach to iterate over the override df and try to handle 1 row at a time but, i get stuck when i need to do a reverse lookup to get the <code>BKT_order</code>  of the <code>max_BKT</code> from original df.</p>
<p>Hope that makes sense... i am fairly new to pandas.</p>
","2","Question"
"79362783","","<p>Snowflake’s modin.pandas.dataframe df has 7 columns
Snowflake Table “Db.S.table” has 10 columns - 1 auto increment and most that are nullable</p>
<p>I’m not able to do <code>session.write_pandas(df,database=“Db”,schema=“schema”,table_name=“table”,overwrite=False)</code></p>
<p>Since it’s giving a column mismatch error. Why is this not an issue with native pandas</p>
","0","Question"
"79363433","","<pre><code>import pandas as pd

columns = pd.MultiIndex.from_tuples(
    [('A', 'one'), ('A', 'two'), ('B', 'one'), ('B', 'two'), ('C', '')],
    names=[None, 'number'])

df = pd.DataFrame([[1, 2, 3, 4, 'X'], [5, 6, 7, 8, 'Y']], columns=columns)

         A       B      C
number one two one two   
0        1   2   3   4  X
1        5   6   7   8  Y
</code></pre>
<p>I'd like to remove the multi-index by making <code>number</code> a column:</p>
<pre><code>A   B   C   number
1   3   X   one
5   7   Y   one
2   4   X   two
6   8   Y   two
</code></pre>
<p>I tried extracting the values with <code>df[[('number', ('A','one')]]</code> so that I can assign them to individual columns, but it doesn't work.</p>
","0","Question"
"79364078","","<p>I was analysing some scRNA-seq data from GSE214966 when I encountered a problem in executing <code>cnv_score()</code> function from <code>infercnvpy</code> package. It consistenly raises a AttributeError: 'Series' object has no attribute 'nonzero'.</p>
<p>I also tried reproducing the example from their website tutorial (<a href=""https://infercnvpy.readthedocs.io/en/latest/notebooks/tutorial_3k.html"" rel=""nofollow noreferrer"">https://infercnvpy.readthedocs.io/en/latest/notebooks/tutorial_3k.html</a>) but it raises the same error. That probably mean that the problem it's on my end but I can't figure out what's wrong.</p>
<p>I tried reinstalling all packages and python as well but nothing worked out. The error suggests that the problem is that <code>Pandas.Series</code> doesn't have a attribute such as <code>nonzero</code>, which it doesn't. But then I don't know what's wrong or how to solve this problem since it is a function from a package.</p>
<p>Can anybody help me with this?</p>
<h2>Error description</h2>
<pre><code>AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_9248\2255080222.py in ?()
      1 import infercnvpy as cnv
----&gt; 2 cnv.tl.cnv_score(adata, groupby=&quot;leiden&quot;)

d:\Datos de usuario\Desktop\Single Cell Analysis\Glioblastoma (GSE214966)\Glioblastoma-GSE214966\.venv\Lib\site-packages\infercnvpy\tl\_scores.py in ?(adata, groupby, use_rep, key_added, inplace, obs_key)
     61         groupby = obs_key
     62 
     63     if groupby not in adata.obs.columns and groupby == &quot;cnv_leiden&quot;:
     64         raise ValueError(&quot;`cnv_leiden` not found in `adata.obs`. Did you run `tl.leiden`?&quot;)
---&gt; 65     cluster_score = {
     66         cluster: np.mean(np.abs(adata.obsm[f&quot;X_{use_rep}&quot;][adata.obs[groupby] == cluster, :]))
     67         for cluster in adata.obs[groupby].unique()
     68     }

d:\Datos de usuario\Desktop\Single Cell Analysis\Glioblastoma (GSE214966)\Glioblastoma-GSE214966\.venv\Lib\site-packages\scipy\sparse\_index.py in ?(self, key)
     29     def __getitem__(self, key):
---&gt; 30         index, new_shape = self._validate_indices(key)
     31 
     32         # 1D array
     33         if len(index) == 1:

d:\Datos de usuario\Desktop\Single Cell Analysis\Glioblastoma (GSE214966)\Glioblastoma-GSE214966\.venv\Lib\site-packages\scipy\sparse\_index.py in ?(self, key)
    265                 if ix.shape != mid_shape:
    266                     raise IndexError(
    267                         f&quot;bool index {i} has shape {mid_shape} instead of {ix.shape}&quot;
    268                     )
--&gt; 269                 index.extend(ix.nonzero())
    270                 array_indices.extend(range(index_ndim, tmp_ndim))
    271                 index_ndim = tmp_ndim
    272             else:  # dense array

d:\Datos de usuario\Desktop\Single Cell Analysis\Glioblastoma (GSE214966)\Glioblastoma-GSE214966\.venv\Lib\site-packages\pandas\core\generic.py in ?(self, name)
   6295             and name not in self._accessors
   6296             and self._info_axis._can_hold_identifiers_and_holds_name(name)
   6297         ):
   6298             return self[name]
-&gt; 6299         return object.__getattribute__(self, name)

AttributeError: 'Series' object has no attribute 'nonzero'
</code></pre>
<p>Session_info</p>
<pre><code>-----
anndata             0.11.3
infercnvpy          0.5.0
matplotlib          3.10.0
numpy               1.26.4
openpyxl            3.1.5
pandas              2.2.3
scanpy              1.10.4
scipy               1.15.1
session_info        1.0.0
sklearn             1.6.1
-----
PIL                 11.1.0
asttokens           NA
attr                24.3.0
attrs               24.3.0
cairo               1.27.0
cattr               NA
cattrs              NA
certifi             2024.12.14
charset_normalizer  3.4.1
colorama            0.4.6
comm                0.2.2
cycler              0.12.1
...
Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]
Windows-10-10.0.19045-SP0
-----
Session information updated at 2025-01-16 16:27
</code></pre>
","1","Question"
"79364338","","<p>In a series or df column, I want to count the number of values that fit within predefined bins (easy) and meaningfully label the bin values (problem).</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = [{'A': 1, 'B': &quot;Jim&quot;}, {'A': 5, 'B': &quot;Jim&quot;}, {'A': 2, 'B': &quot;Bob&quot;}, {'A': 3, 'B': &quot;Bob&quot;}]
df = pd.DataFrame(data)

mBins = [-1, 2, 4, 6]
mLabels = [&quot;0-2&quot;, &quot;3-4&quot;, &quot;5-6&quot;]

simple_VC = df[&quot;A&quot;].value_counts(bins=mBins)
</code></pre>
<pre class=""lang-none prettyprint-override""><code>Out[25]:   # ugly bin values
(-1.001, 2.0]    2
(2.0, 4.0]       1
(4.0, 6.0]       1

# Wanted more meaningful bin values:
0-2    2
3-4    1
5-6    1
</code></pre>
<p>I've tried using pd.cut, which allows me to label the bins, but I'm not sure how to use this in a value count. I've also tried to rename, but I don't know how to specify values like (4.0, 6.0], which are neither text or non-text.</p>
<p>How do I label the binned value counts - if possible during the value count, and how to rename bin ranges?</p>
","1","Question"
"79364551","","<p>I have the following <code>pd.DataFrame</code></p>
<pre><code>match_id    player_id   round   points  A   B   C   D   E
5890    3750    1   10  0   0   0   3   1
5890    3750    2   10  0   0   0   1   0
5890    3750    3   10  0   8   0   0   1
5890    2366    1   9   0   0   0   5   0
5890    2366    2   9   0   0   0   5   0
5890    2366    3   9   0   0   0   2   0
</code></pre>
<p>I want to subtract the values of A, B, C, D and E of the two players and create two new columns that represent the number of points of the two players.</p>
<p>My desired output looks as follows:</p>
<pre><code>match_id    round   points_home points_away A   B   C   D   E
5890    1   10  9   0   0   0   -2  1
5890    2   10  9   0   0   0   -4  0
5890    3   10  9   0   8   0   -2  1
</code></pre>
<p>Please advice</p>
","2","Question"
"79365680","","<p>In some sources, I found that pandas works faster than numpy with 500k rows or more. Can someone explain this to me?</p>
<blockquote>
<p>Pandas have a better performance when the number of rows is <strong>500K or more.</strong></p>
<p>— <a href=""https://www.geeksforgeeks.org/difference-between-pandas-vs-numpy/"" rel=""nofollow noreferrer"">Difference between Pandas VS NumPy</a> - GeeksforGeeks</p>
</blockquote>
<blockquote>
<p>If the number of rows of the dataset is more than five hundred thousand (500K), then the performance of Pandas is better than NumPy.</p>
<p>— <a href=""https://www.interviewbit.com/blog/pandas-vs-numpy/"" rel=""nofollow noreferrer"">Pandas Vs NumPy: What’s The Difference? [2023]</a> - InterviewBit</p>
</blockquote>
<blockquote>
<p>[...] Pandas generally performs better than numpy for 500K rows or more [...]</p>
<p>— <a href=""https://github.com/firmai/pandapy"" rel=""nofollow noreferrer"">PandaPy</a> - firmai on GitHub</p>
</blockquote>
<p>I tried to find where this fact came from. I couldn't figure it out and couldn't see any information from the documentation.</p>
","2","Question"
"79366709","","<p>I have successfully connected to MS Access, and my code accurately displays the data from the MS Access database table in a DataFrame. However, when I attempt to insert the data from the Pandas DataFrame into the MS Access table, I encounter an error stating, “Type Error: The first argument to execute must be a string or Unicode query.” I would appreciate it if you could review my code, correct it, and provide me with the appropriate code to insert the DataFrame data into the MS Access table. Thank you in advance for your guidance.</p>
<pre><code>
import streamlit as st
import pyodbc
import pandas as pd
conn_str = (r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'
            r'DBQ=C:\iqra\mfa.accdb;')
conn = pyodbc.connect(conn_str)
cursor = conn.cursor()
query = &quot;SELECT * FROM Payments&quot;
dataf = pd.read_sql(query, conn)
rows = [tuple(x) for x in dataf.values]
conn.execute(&quot;INSERT INTO Payments VALUES (:0:1,:2,:3)&quot;,rows)  # insert df into msaccess give error TypeError: The first argument to execute must be a string or unicode query.
conn.commit()
conn



</code></pre>
<pre><code>conn.execute(&quot;INSERT INTO Payments VALUES (:0:1,:2,:3)&quot;,rows)  # insert df into msaccess 
</code></pre>
","0","Question"
"79366813","","<p>I am running a virtual environment for my project in VS code, and installed pandas and PyMuPDF but I cannot fix the flagged problems below. Please note that I am using a WSL command terminal with a Windows OS.</p>
<ul>
<li><code>&quot;Import &quot;pandas&quot; could not be resolved from source Pylance(reportMissingImports)&quot;</code></li>
<li><code>&quot;Import &quot;pymupdf&quot; could not be resolved Pylance(reportMissingImports)&quot;</code></li>
</ul>
<p>I've looked at prior stackoverflow threads, but the suggestions have not been able to fix this issue. I made sure that my packages are installed properly (see screenshot). My installed panadas version is 2.2.3 and my python version is 3.12.3 in the virtual environment.</p>
<p><img src=""https://i.sstatic.net/2fD4UUOM.png"" alt=""enter image description here"" /></p>
<p>This leads me to believe that I do not have the correct Python interpreter selected, but when I browse my file system to select a different interpreter in the venv/bin folder, it's empty so I cannot select a python interpreter. However, when I run &quot;ls venv/vin&quot;, it shows that there are several Python interpreters installed (see screenshot).</p>
<p><img src=""https://i.sstatic.net/TYHN8pJj.png"" alt=""enter image description here"" /></p>
<p>I tried manually adding a path to my settings.json file to the Python interpreter, but that did not work either.</p>
<p>I do not know what to do from this point. If someone could please help, that would be much appreciated!</p>
","0","Question"
"79366943","","<p>Suppose I have a dataframe <code>Old</code> with columns <code>A</code>, <code>B</code>, and <code>C</code>. I want a new dataframe <code>New</code> where two columns <code>D</code> and <code>E</code>. For each cell in <code>Old</code>, I want a corresponding row in the <code>D</code> column in <code>New</code> where the value in <code>E</code> is the name of the column the cell was in.</p>
<p>I know that straight up iterating over a dataframe is bad, but that's how I did it. Here, I only cared about some column names in the <code>Old</code> dataframe, so if the cell wasn't under a column I cared about, I just assigned it the value <code>other</code>. But the principle is the same.</p>
<pre><code>for column in df.columns:
    for entry in df[column]:
        entries.append(entry)
        labels.append(column_labels.get(column, &quot;other&quot;))  # Assign label based on column
</code></pre>
<p>My question is what are some better ways to do this? Running this will become exceedingly slow as the dataset grows.</p>
","0","Question"
"79367366","","<p>There is a dataframe like this:</p>
<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame({'x':np.arange(1,29),'y':[5.69, 6.03, 6.03, 6.03, 6.03, 6.03, 6.03, 5.38, 5.21, 5.4 , 5.24,
       5.4 , 5.36, 5.47, 5.58, 5.5 , 5.61, 5.53, 5.4 , 5.51, 5.47, 5.44,5.39, 5.27, 5.38, 5.35, 5.32, 5.09],
          'valley':[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
          'peak':[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,0, 0, 0, 0, 0, 0]})

&gt;&gt;&gt; df
     x     y  valley  peak
0    1  5.69       1     0
1    2  6.03       0     0
2    3  6.03       0     1
3    4  6.03       0     0
4    5  6.03       0     0
5    6  6.03       0     0
6    7  6.03       0     0
7    8  5.38       0     0
8    9  5.21       1     0
9   10  5.40       0     0
10  11  5.24       0     0
11  12  5.40       0     0
12  13  5.36       0     0
13  14  5.47       0     0
14  15  5.58       0     0
15  16  5.50       0     0
16  17  5.61       0     1
17  18  5.53       0     0
18  19  5.40       0     0
19  20  5.51       0     0
20  21  5.47       0     0
21  22  5.44       0     0
22  23  5.39       0     0
23  24  5.27       0     0
24  25  5.38       0     0
25  26  5.35       0     0
26  27  5.32       0     0
27  28  5.09       1     0
</code></pre>
<p>I hope to add a new column 'grp' to this dataframe, with the requirement that for each row that starts with &quot;1&quot; in the valley column and ends with &quot;1&quot; in the peak column, the value in the added column is &quot;A&quot;, and conversely, for each row that starts with &quot;1&quot; in the peak column and ends with &quot;1&quot; in the valley column, the value in the added column is 'B'.</p>
<p>The desire result is:</p>
<pre><code>&gt;&gt;&gt; out
     x     y  valley  peak  grp
0    1  5.69       1     0  A
1    2  6.03       0     0  A
2    3  6.03       0     1  B
3    4  6.03       0     0  B
4    5  6.03       0     0  B
5    6  6.03       0     0  B
6    7  6.03       0     0  B
7    8  5.38       0     0  B
8    9  5.21       1     0  A
9   10  5.40       0     0  A
10  11  5.24       0     0  A
11  12  5.40       0     0  A
12  13  5.36       0     0  A
13  14  5.47       0     0  A
14  15  5.58       0     0  A
15  16  5.50       0     0  A
16  17  5.61       0     1  B
17  18  5.53       0     0  B
18  19  5.40       0     0  B
19  20  5.51       0     0  B
20  21  5.47       0     0  B
21  22  5.44       0     0  B
22  23  5.39       0     0  B
23  24  5.27       0     0  B
24  25  5.38       0     0  B
25  26  5.35       0     0  B
26  27  5.32       0     0  B
27  28  5.09       1     0  A
</code></pre>
<p>If we don't use apply with a function and for-loops, is there a native way to achieve by use pandas?</p>
","1","Question"
"79367389","","<p>The code is attached below. It works fine until it gets to <code>ai: df_ai</code> in the <code>database</code> dict.</p>
<pre><code>data = pd.read_csv('survey_results_public.csv')

df_demographics = data[['ResponseId', 'MainBranch', 'Age', 'Employment', 'EdLevel', 'YearsCode', 'Country']]

df_learn_code = data[['ResponseId', 'LearnCode']]

df_language = data[['ResponseId', 'LanguageAdmired']]

df_ai = data[['ResponseId', 'AISelect', 'AISent', 'AIAcc', 'AIComplex', 'AIThreat', 'AIBen', 'AIToolCurrently Using']]

database = {'demographics': df_demographics, 'learn_code': df_learn_code, 'language': df_language, 'ai': df_ai}

def find_semicolons(dataframe):
    result = []

    firstFifty = dataframe.head(50)

    for column in firstFifty.columns:
        if firstFifty[column].apply(lambda x: ';' in str(x)).any():
            result.append(column)

    return result


def transform_dataframe(dataframe):
    result = find_semicolons(dataframe)

    for column in result:
        values = [str(x).split(';') for x in dataframe[column].unique().tolist()]
        flat_values = []
        for x in values:
            flat_values.extend(x)
        flat_values = set(flat_values)
        for x in flat_values:
            dataframe[x] = dataframe[column].str.contains(x, na=False).astype(int)



for x in database:
    transform_dataframe(database.get(x))
    database.get(x).to_csv(x + '.csv')

</code></pre>
<p>Here's the traceback</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/shalim/PycharmProjects/work/stackoverflow.py&quot;, line 45, in &lt;module&gt;
    transform_dataframe(database.get(x))
  File &quot;/Users/shalim/PycharmProjects/work/stackoverflow.py&quot;, line 40, in transform_dataframe
    dataframe[x] = dataframe[column].str.contains(x, na=False).astype(int)
  File &quot;/Users/shalim/PycharmProjects/work/venv/lib/python3.9/site-packages/pandas/core/strings/accessor.py&quot;, line 137, in wrapper
    return func(self, *args, **kwargs)
  File &quot;/Users/shalim/PycharmProjects/work/venv/lib/python3.9/site-packages/pandas/core/strings/accessor.py&quot;, line 1327, in contains
    if regex and re.compile(pat).groups:
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py&quot;, line 252, in compile
    return _compile(pattern, flags)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py&quot;, line 304, in _compile
    p = sre_compile.compile(pattern, flags)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_compile.py&quot;, line 764, in compile
    p = sre_parse.parse(p, flags)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py&quot;, line 948, in parse
    p = _parse_sub(source, state, flags &amp; SRE_FLAG_VERBOSE, 0)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py&quot;, line 443, in _parse_sub
    itemsappend(_parse(source, state, verbose, nested + 1,
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py&quot;, line 671, in _parse
    raise source.error(&quot;multiple repeat&quot;,
re.error: multiple repeat at position 2
</code></pre>
","0","Question"
"79367707","","<p>I am trying to get the indices of the missing date by comparing it to a list of un-missed dates, as the following:</p>
<pre><code>a = pd.DatetimeIndex([&quot;2000&quot;, &quot;2001&quot;, &quot;2002&quot;, &quot;2003&quot;,
                      &quot;2004&quot;, &quot;2005&quot;, &quot;2009&quot;, &quot;2010&quot;])
b = pd.DatetimeIndex([&quot;2000&quot;, &quot;2001&quot;, &quot;2002&quot;, &quot;2003&quot;,
                      &quot;2004&quot;, &quot;2005&quot;, &quot;2006&quot;, &quot;2007&quot;,
                      &quot;2008&quot;, &quot;2009&quot;, &quot;2010&quot;])
a.reindex(b)
</code></pre>
<p>I got the following</p>
<pre><code>(DatetimeIndex(['2000-01-01', '2001-01-01', '2002-01-01', '2003-01-01',
                '2004-01-01', '2005-01-01', '2006-01-01', '2007-01-01',
                '2008-01-01', '2009-01-01', '2010-01-01'],
               dtype='datetime64[ns]', freq=None),
 array([ 0,  1,  2,  3,  4,  5, -1, -1, -1,  6,  7]))
</code></pre>
<p>I tried to replace all missing value which is -1 to Nan, by using <code>a.reindex(b, fill_value=np.NAN)</code> but I got the following error <code>TypeError: Index.reindex() got an unexpected keyword argument ‘fill_value’</code></p>
<p>According the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer"">pandas documentation</a> fill_vaue is among the parameters of reindex.
Any ideas?</p>
","-1","Question"
"79368140","","<p>I couldn't figure out where and what the issue is. I have ensured that I'm only returning 2 values (caption and title), but I am unsure where the other 2 values are coming from.</p>
<p>here is my function for the image model:</p>
<pre><code>def generate_caption_and_title(image_path, device): 
# Load the BLIP model and processor
processor = BlipProcessor.from_pretrained(&quot;Salesforce/blip-image-captioning-base&quot;)
model = BlipForConditionalGeneration.from_pretrained(&quot;Salesforce/blip-image-captioning-base&quot;)
# Move the model to the selected device (CPU or GPU)
model.to(device)
try:
    # Open the image
    image = Image.open(image_path).convert(&quot;RGB&quot;)

    # Generate the caption (Description)
    description_prompt = &quot;Describe this image in detail.&quot;
    inputs_description = processor(image, text=description_prompt, return_tensors=&quot;pt&quot;).to(device)
    description_ids = model.generate(
        inputs_description['input_ids'],
        max_length=50,
        num_beams=5,
        early_stopping=True
    )
    caption = processor.decode(description_ids[0], skip_special_tokens=True)

    # Generate a title (can be the first few words or a summary of the caption)
    title_prompt = &quot;Provide a one-word title for this image.&quot;
    inputs_title = processor(image, text=title_prompt, return_tensors=&quot;pt&quot;).to(device)
    title_ids = model.generate(
        inputs_title['input_ids'],
        max_length=15,
        num_beams=5,
        early_stopping=True
    )
    title = processor.decode(title_ids[0], skip_special_tokens=True)

    return caption, title  # Ensure the order matches the unpacking
except Exception as e:
    print(f&quot;Error in generate_caption_and_title for {image_path}: {e}&quot;)
    return &quot;Error Title&quot;, &quot;Error Description&quot;  # Default return for failed cases
</code></pre>
<p>here is where I'm calling out the statement for generate_caption_and_title:</p>
<pre><code>def process_images(image_dir, output_csv, device):
for root, dirs, files in os.walk(image_dir):
    for filename in files:
        file_path = os.path.join(root, filename)

        # Process only image files
        if os.path.isfile(file_path) and filename.lower().endswith((&quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.png&quot;, &quot;.tiff&quot;)):
            image_files.append(file_path)
# Process only image files
for file_path in tqdm(image_files, desc=&quot;Processing Images&quot;, unit=&quot;image&quot;):
    try:
        # Generate description and title using AI
        title, caption = generate_caption_and_title(file_path, device)

        # Open the image
        with Image.open(file_path) as img:
            # Extract EXIF data
            exif_data = img._getexif()
            metadata = {
                &quot;Filename&quot;: filename, 
                &quot;File Path&quot;: file_path, 
                &quot;Resolution&quot;: f&quot;{img.width}x{img.height}&quot;, 
                &quot;Description&quot;: caption, 
                &quot;Title&quot;: title}

        # Extract specific EXIF tags
        if exif_data:
            for tag_id, value in exif_data.items():
                tag_name = TAGS.get(tag_id, tag_id)
                metadata[tag_name] = value

        # Append metadata to the list
        metadata_list.append(metadata)
    except Exception as e:
        print(f&quot;Error processing {filename}: {e}&quot;)

# Create a DataFrame and save it to a CSV file
df = pd.DataFrame(metadata_list)
df.to_csv(output_csv, index=False)

print(f&quot;Metadata has been saved to {output_csv}&quot;)
</code></pre>
<p>Also here is the full error message:</p>
<pre><code>Error in generate_caption_and_title for D:\Pictures\Jacob Brockwell Graphic Arts and Pictures\Photo Gallaries\Photo Gallary\JPEG Photos\RAW Exports\Other\Other #2\Food\DSC_0006.jpg: not enough values to unpack (expected 4, got 2)
Full Traceback:
Traceback (most recent call last):
  File &quot;d:\other-files\school\database_dev\Personal Projects\Image Database\imageScan.py&quot;, line 32, in generate_caption_and_title
    description_ids = model.generate(
                      ^^^^^^^^^^^^^^^
  File &quot;C:\Users\Jacob Brockwell\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\torch\utils\_contextlib.py&quot;, line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Jacob Brockwell\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\models\blip\modeling_blip.py&quot;, line 1187, in generate
    vision_outputs = self.vision_model(
                     ^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Jacob Brockwell\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\torch\nn\modules\module.py&quot;, line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Jacob Brockwell\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\torch\nn\modules\module.py&quot;, line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Jacob Brockwell\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\models\blip\modeling_blip.py&quot;, line 726, in forward
    hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Jacob Brockwell\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\torch\nn\modules\module.py&quot;, line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Jacob Brockwell\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\torch\nn\modules\module.py&quot;, line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Jacob Brockwell\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\models\blip\modeling_blip.py&quot;, line 277, in forward
    batch_size, _, height, width = pixel_values.shape
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: not enough values to unpack (expected 4, got 2)
</code></pre>
","1","Question"
"79369011","","<p>In below code of pipeline. Even though i have encoded the sex column, i am getting string to float error.</p>
<pre><code>from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Step 1: Imputation
trf1 = ColumnTransformer([
    ('impute_age', SimpleImputer(), [2]),  # Impute Age
    ('impute_embarked', SimpleImputer(strategy='most_frequent'), [6])  # Impute Embarked
], remainder='passthrough')

# Step 2: One-Hot Encoding
trf2 = ColumnTransformer([
    ('onehot_sex_embarked', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [1, 6])  # Encode Sex and Embarked
], remainder='passthrough')

# Step 3: Scaling
trf3 = ColumnTransformer([
    ('scale', MinMaxScaler(), slice(0, None))  # Scale all columns
], remainder='passthrough')

# Step 4: Classifier
trf4 = DecisionTreeClassifier()

# Create pipeline
pipe = Pipeline([
    ('trf1', trf1),  # Step 1: Imputation
    ('trf2', trf2),  # Step 2: One-hot encoding
    ('trf3', trf3),  # Step 3: Scaling
    ('trf4', trf4)   # Step 4: Model
])

# Ensure proper ha
# Fit the pipeline
pipe.fit(X_train, y_train)
</code></pre>
<p>Error:</p>
<p><a href=""https://i.sstatic.net/TMvRgQBJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TMvRgQBJ.png"" alt=""error image is here"" /></a></p>
<p>What is the reason to the error?</p>
","-1","Question"
"79369190","","<p>I often want to view a random sample of <code>k</code> rows from a DataFrame rather than just the head/tail, for which I would use <code>df.sample(k)</code>.</p>
<p>When I chain on <code>.style</code> to this sample, the styler will only see the <code>k</code> selected rows, and the resulting colour-mapping will be inaccurate as it only considers the sample.</p>
<p>How can I shuffle, sample, and style a DataFrame, whilst ensuring the styler uses all of the data?</p>
<h2>Example</h2>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

#Data for testing
df = pd.DataFrame({
    'device_id': np.random.randint(200, 800, size=1000),
    'normalised_score': np.random.uniform(0, 2, size=1000),
    'severity_level': np.random.randint(-3, 4, size=1000),
})

#Inaccurate styling if I chain .style onto a sampled DataFrame:
df.sample(5).style.background_gradient(subset='severity_level', cmap='RdYlGn')
</code></pre>
<p>I am using a colourmap that roughly goes red-white-green over the range of <code>severity_level</code> (-3, -2, -1, 0, +1, +2, +3). A value of 0 should therefore display as <em>white</em>, but it gets coloured red in the sample below:</p>
<p><a href=""https://i.sstatic.net/3wi0kmlD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3wi0kmlD.png"" alt=""enter image description here"" /></a></p>
<p>The colouring should consider all <code>severity_level</code> values, even though I only display a few randomly-selected rows.</p>
","0","Question"
"79370034","","<p>im new in julia, and i try use PyCall in julia</p>
<p>the problem is i cant create new column or add new column. when i run it, i only got 6 columns(before i add new column).</p>
<p>This my code
using PyCall</p>
<pre class=""lang-julia prettyprint-override""><code>using Pandas
using Dates
@pyimport re
py&quot;&quot;&quot;
import pandas as pd

df = pd.read_csv(&quot; my path tweets.csv&quot;, index_col = None, encoding=&quot;ISO-8859-1&quot;)

&quot;&quot;&quot;
df_sample = df.sample(n=200, random_state=42)
df_sample[&quot;Date&quot;] = pd.to_datetime(df_sample[&quot;Date&quot;], format=&quot;%a %b %d %H:%M:%S PDT %Y&quot;)
df_sample[&quot;Month&quot;] = df_sample.Date.dt.month
df_sample[&quot;Month&quot;].sort_values()
</code></pre>
<p>i got warning :</p>
<blockquote>
<p>sys:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see <a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access</a>.</p>
</blockquote>
","-1","Question"
"79370983","","<p>I have a file path location:</p>
<pre><code>     file
df = /a/b/c/d/e/f/g/h/i/j/k/l/m/n/a.c
     /a/b/c/d/e/x/b.c
</code></pre>
<p>I am using the below logic to remove redundant path</p>
<pre><code>df[&quot;file&quot;]=  df[&quot;file&quot;].str.extract(r&quot;(?:/[\w\.-]+){7}/(.+)&quot;)
</code></pre>
<p>My output is</p>
<pre><code>df = /a/b/c/d/e/f/g/...
     /a/b/c/d/e/x/b.c
</code></pre>
<p>I am interested to display the file name <strong>a.c</strong> completely and have output to show the accurate location of a.c with atleast last 3 subdirectory and first few such as 5 subdirectories.</p>
<pre><code>/a/b/c/d/..../l/m/n/a.c
</code></pre>
<p>How can we parse correctly ?</p>
","1","Question"
"79371263","","<p>i have got a Condition Dataframe like this which has about 300 rows</p>
<pre><code>pd.DataFrame({&quot;PERSONALNR&quot;:[&quot;000009461&quot;,&quot;000009461&quot;],&quot;PERIODE&quot;:[&quot;202401&quot;,&quot;202402&quot;],&quot;MANDANT&quot;:[&quot;LB&quot;,&quot;LB&quot;],&quot;DA&quot;:[&quot;01&quot;,&quot;01&quot;]})
</code></pre>
<p>where &quot;PERSONALNR&quot; and &quot;PERIODE&quot; are the conditions i need to meet and the values &quot;MANDANT&quot; and &quot;DA&quot; need to be replaced in the second Dataframe</p>
<p>The Dataframe i want to replace values in looks similar to this one, which has about 110k rows</p>
<pre><code>pd.DataFrame({&quot;PERSONALNR&quot;:[&quot;000009461&quot;,&quot;000009461&quot;],&quot;PERIODE&quot;:[&quot;202401&quot;,&quot;202402&quot;],&quot;MANDANT&quot;:[&quot;LB&quot;,&quot;LB&quot;],&quot;DA&quot;:[&quot;01&quot;,&quot;01&quot;], &quot;KSTBEZ&quot;:[&quot;Springer pool&quot;,&quot;bla bla&quot;]})
</code></pre>
<p>and the solution i came up with is the following:</p>
<pre><code>for row in POOL.itertuples():
    LA.loc[(LA.PERSONALNR==row.PERSONALNR)&amp;(LA.PERIODE==row.PERIODE)&amp;(LA.DA==&quot;01&quot;)&amp;(LA.KSTBEZ.str.contains(&quot;pool&quot;)),[&quot;MANDANT&quot;,&quot;DA&quot;]]=[row.MANDANT,row.DA]
</code></pre>
<p>My solution works for the Dataframe above quite ok - takes about 10 seconds or so to finish, but i need to do the same operation in a dataframe with 1 Million rows - there it takes about 10 minutes...</p>
<p>can anyone come up with a better solution?</p>
","1","Question"
"79372190","","<p>I'm trying to get the data from a simple table from the following website (<a href=""https://bvmf.bmfbovespa.com.br/clube-de-investimento/clube-de-investimento.aspx?Idioma=pt-br"" rel=""nofollow noreferrer"">https://bvmf.bmfbovespa.com.br/clube-de-investimento/clube-de-investimento.aspx?Idioma=pt-br</a>). I was able to get the data from the first page, but as we can see the pagination it's not linked to the URL and I couldn't get it, even though i could find the buttons at the bottom of the page  &quot;ProximoPaginacao&quot; and &quot;MeioPaginacao&quot;, but i couldn't handle this implementation. Any ideas?</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import pandas as pd

def extract_table_data(url, table_id):

try:
    response = requests.get(url,verify=False)
    response.raise_for_status()
    html_content = response.text
    soup = BeautifulSoup(html_content, 'html.parser')
    table = soup.find('table', id=table_id)
    if not table:
        print(f&quot;Table with ID '{table_id}' not found.&quot;)
        return None

    # Extract header row
    header_row = [th.get_text(strip=True) for th in table.find_all('th')]

    # Extract data rows
    data_rows = []
    for row in table.find('tbody').find_all('tr'):
        data_rows.append([td.get_text(strip=True) for td in row.find_all('td')])

    # Create DataFrame
    df = pd.DataFrame(data_rows, columns=header_row)
    return df
except requests.exceptions.RequestException as e:
    print(f&quot;Error during requests: {e}&quot;)
    return None
except Exception as e:
    print(f&quot;An error occurred: {e}&quot;)
    return None

# Example usage
url = &quot;https://bvmf.bmfbovespa.com.br/clube-de-investimento/clube-de-investimento.aspx? 
Idioma=pt-br&quot;  # Replace with the actual URL
table_id = &quot;ctl00_contentPlaceHolderConteudo_grdAtivo_ctl01&quot;  # Replace with the actual 
table ID
table_data = extract_table_data(url, table_id)

if table_data is not None:
   print(table_data)
</code></pre>
","0","Question"
"79372381","","<p>I want to create a function to transform the datatype of all spark dataframe columns from decimal to float.<br>
I do not know my column names in advance, nor if and how many columns of the type of decimal are included. This excludes explicit casting of columns to prevent scaling limitations.<br>
Other data type columns should not be affected.<br>
NULLS might occur.<br></p>
<p>Reason behind all this madness: I need to convert the spark dataframe to pandas, to then be able to write an xlsx file. The transformation to pandas of decimal however results in an object type, which is stored in the xlsx file as text, not as a number.<br><br></p>
<p>Sample code:</p>
<pre><code>df = spark.sql(&quot;select 'text' as txt, 1.1111 as one, 2.22222 as two, CAST(3.333333333333 AS FLOAT) as three&quot;)
df.printSchema()

&gt;&gt;
root
 |-- txt: string (nullable = false)
 |-- one: decimal(5,4) (nullable = false)
 |-- two: decimal(6,5) (nullable = false)
 |-- three: float (nullable = false)
</code></pre>
<p>Transform to Pandas:</p>
<pre><code>df_pd = df.toPandas()
print(df_pd.dtypes)

&gt;&gt;
txt       object
one       object
two       object
three    float32
dtype: object
</code></pre>
<p>I need all of the decimal types to be of float type in df_pd.<br><br><br>
Ideally I have something like this:</p>
<pre><code>df = spark.sql(&quot;select 'text' as txt, 1.1111 as one, 2.22222 as two, 3.333333333333 as three&quot;)

insert magic

df.printSchema()

&gt;&gt;
root
 |-- txt: string (nullable = false)
 |-- one: float (nullable = false)
 |-- two: float (nullable = false)
 |-- three: float (nullable = false)
</code></pre>
<p>Thanks</p>
","0","Question"
"79373355","","<p>With a dataset with millions of records, I have items with various categories and measurements, and I'm trying to figure out how many of the records have changed, in particular when the category or measurement goes to NaN (or NULL from the database query) during the sequence.</p>
<p>In SQL, I'd use some PARTITION style OLAP functions to do this, but seems like it should fairly straightforward in Python with Pandas, but I can't quite wrap my head around the vectorized notation.</p>
<p>I've tried various <code>df.groupby</code> clauses and lambda functions but nothing quite gets it in the required format - basically, the <code>df.groupby('item')['measure']</code> in this example, the first row of the grouped subset of item &amp; measure always returns <code>True</code>, where I'd like to it to be <code>False</code> or <code>NaN</code>. Simply put, they are false positives. I understand from pandas' perspective, it's a change since the first <code>x.shift()</code> would be NaN, but I can't figure out how to filter that or handle it in the lambda function.</p>
<p>Sample Code:</p>
<pre><code>import pandas as pd
import numpy as np

test_df = pd.DataFrame({'item': [20, 20, 20, 20, 20, 20, 20, 20, 30, 30, 30, 30, 30, 30, 30, 30, 40, 40, 40, 40, 40, 40, 40, 40 ],
                        'measure': [1, 1, 1, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 7, 7, 7, 10, 10, 10, 10, 10, 10, 10, 10 ],
                        'cat': ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'c', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e']})

test_df['measure_change'] = test_df.groupby('item')['measure'].transform(lambda x: x.shift() != x)
test_df['cat_change'] = test_df.groupby('item')['cat'].transform(lambda x: x.shift() != x)
</code></pre>
<p>In the output below, as an example, rows 0, 8, and 16, the <code>measure_change</code> should be False. So all of item 40 would have <code>measure_change == False</code> and that would indicate no changes with that item. Any &amp; all suggestions are appreciated.</p>
<p>(cat_change set up the same way)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>#</th>
<th>item</th>
<th>measure</th>
<th>measure_change</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>20</td>
<td>1</td>
<td>True</td>
</tr>
<tr>
<td>1</td>
<td>20</td>
<td>1</td>
<td>False</td>
</tr>
<tr>
<td>2</td>
<td>20</td>
<td>1</td>
<td>False</td>
</tr>
<tr>
<td>3</td>
<td>20</td>
<td>3</td>
<td>True</td>
</tr>
<tr>
<td>4</td>
<td>20</td>
<td>3</td>
<td>False</td>
</tr>
<tr>
<td>5</td>
<td>20</td>
<td>3</td>
<td>False</td>
</tr>
<tr>
<td>6</td>
<td>20</td>
<td>3</td>
<td>False</td>
</tr>
<tr>
<td>7</td>
<td>20</td>
<td>3</td>
<td>False</td>
</tr>
<tr>
<td>8</td>
<td>30</td>
<td>6</td>
<td>True</td>
</tr>
<tr>
<td>9</td>
<td>30</td>
<td>6</td>
<td>False</td>
</tr>
<tr>
<td>10</td>
<td>30</td>
<td>6</td>
<td>False</td>
</tr>
<tr>
<td>11</td>
<td>30</td>
<td>6</td>
<td>False</td>
</tr>
<tr>
<td>12</td>
<td>30</td>
<td>6</td>
<td>False</td>
</tr>
<tr>
<td>13</td>
<td>30</td>
<td>7</td>
<td>True</td>
</tr>
<tr>
<td>14</td>
<td>30</td>
<td>7</td>
<td>False</td>
</tr>
<tr>
<td>15</td>
<td>30</td>
<td>7</td>
<td>False</td>
</tr>
<tr>
<td>16</td>
<td>40</td>
<td>10</td>
<td>True</td>
</tr>
<tr>
<td>17</td>
<td>40</td>
<td>10</td>
<td>False</td>
</tr>
<tr>
<td>18</td>
<td>40</td>
<td>10</td>
<td>False</td>
</tr>
<tr>
<td>19</td>
<td>40</td>
<td>10</td>
<td>False</td>
</tr>
<tr>
<td>20</td>
<td>40</td>
<td>10</td>
<td>False</td>
</tr>
<tr>
<td>21</td>
<td>40</td>
<td>10</td>
<td>False</td>
</tr>
<tr>
<td>22</td>
<td>40</td>
<td>10</td>
<td>False</td>
</tr>
<tr>
<td>23</td>
<td>40</td>
<td>10</td>
<td>False</td>
</tr>
</tbody>
</table></div>
","2","Question"
"79374600","","<p>I noticed in the example below, that pandas and pyspark differ in their quantile calculation.</p>
<pre><code>data = {&quot;A&quot;: [1, 2, 3, 4, 5]}

pdf = pd.DataFrame(data)
sdf = spark.createDataFrame(pdf)

cols = [&quot;A&quot;]
percentiles = [0.2, 0.8]

pdf.quantile(percentiles)  # 1.8 for 0.2, 4.2 for 0.8

sdf.pandas_api().quantile(percentiles)  # 1.0 for 0.2, 4.0 for 0.8

np.quantile([1, 2, 3, 4, 5], 0.2)  # 1.8

sdf.approxQuantile(&quot;A&quot;, percentiles, 0)  # 1.0 for 0.2, 4.0 for 0.8
</code></pre>
<p>I know that pandas has different interpolation methods (&quot;midpoint&quot;, &quot;nearest&quot;, &quot;upper&quot;, &quot;lower&quot;), but none of them give results, always equal to pyspark implementation.</p>
<p>Can someone advise on how to align pyspark and pandas implementation of quantiles? Thank you very much guys, really appreciate!</p>
","1","Question"
"79374674","","<p>I have two dataframes with identical shape and want to update df1 with df2 if some conditions are met</p>
<pre><code>import pandas as pd
from typing import Any


df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3], &quot;B&quot;: [4, 5, 6]})
print(df1, &quot;\n&quot;)
df2 = pd.DataFrame({&quot;A&quot;: [7, 8, 9], &quot;B&quot;: [10, 3, 12]})
print(df2, &quot;\n&quot;)

# Define a condition function
def condition(x: Any) -&gt; bool:
    &quot;&quot;&quot;condition function to update only cells matching the conditions&quot;&quot;&quot;
    return True if x in [2, 7, 9] else False

df1.update(df2)
print(df1)
</code></pre>
<p>but if I use filter_func <code>df1.update(df2,filter_func=condition)</code> it fails with <code>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() </code><br />
Unfortunately the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.update.html"" rel=""nofollow noreferrer"">docu</a> is not very verbose.</p>
<p>How to update a dataframe with conditions correctly?</p>
","1","Question"
"79375533","","<p>I'm trying to filter the columns by datatype and the 2 additional required columns.</p>
<pre><code>import pandas as pd
data = pd.read_csv(&quot;https://media.geeksforgeeks.org/wp-content/uploads/nba.csv&quot;)
data.head()

#to extract the object dataframe
object_cols = data.select_dtypes(include='object')#.head()
</code></pre>
<p>Please note this is a sample dataframe. my actual dataset have 450 columns</p>
<p>This is my try.</p>
<pre><code>RequiredDataframe1 = data[data.columns[&quot;Number&quot;, &quot;Age&quot;, object_cols.columns.values]]
</code></pre>
<p>But I get the following error:</p>
<pre><code>IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
</code></pre>
","0","Question"
"79376634","","<p>I have two datafames:</p>
<pre><code>df1 = pd.DataFrame({
    'from': [0, 2, 8, 26, 35, 46],
    'to': [2, 8, 26, 35, 46, 48],
    'int': [2, 6, 18, 9, 11, 2]})

df2 = pd.DataFrame({
    'from': [0, 2, 8, 17, 34],
    'to': [2, 8, 17, 34, 49],
    'int': [2, 6, 9, 17, 15]})
</code></pre>
<p>I want to create a new dataframe that looks like this:</p>
<pre><code>df = pd.DataFrame({
    'from': [0, 2, 8, 17, 26, 34, 35, 46, 48],
    'to': [2, 8, 17, 26, 34, 35, 46, 48, 49],
    'int': [2, 6, 9, 9, 8, 1, 11, 2, 1]})
</code></pre>
<p>I have tried standard merging scripts but have not been able to split the rows containing higher 'from' and 'to' numbers in either <code>df1</code> or <code>df2</code> into smaller ones.</p>
<p>How can I merge my dataframes over multiple columns and split rows?</p>
","1","Question"
"79377042","","<p>I have a dataframe that looks like:</p>
<pre><code>ID   f_1   f_2   f_3
1    1     0     1
2    0     1     1
3    1     1     0
4    1     0     1
5    0     1     1   
</code></pre>
<p>I have completely no idea as to even how to begin. And also my original dataframe is rather large (~1M rows) and hence a fast method would be highly appreciated.</p>
<p>and I would like to generate a new column <code>Result</code> which records the pair of <code>f</code>'s that have 1 in them, i.e.</p>
<pre><code>ID   f_1   f_2   f_3   Result
1    1     0     1     1_3
2    0     1     1     2_3
3    1     1     0     1_2
4    1     0     1     1_3
5    0     1     1     2_3
</code></pre>
","4","Question"
"79377876","","<p>I've got a column <code>['Duration]</code> which is an <code>int</code> datatype. I'm now trying to find out the most occurencing <code>['Duration']</code> in a  pandas dataframe.</p>
<pre><code>  duration = (inter['duration'].mode())
  print(duration)
</code></pre>
<p>Result:</p>
<pre><code>  0    94
  Name: duration, dtype: int64      
</code></pre>
<p>The answer is right, but the datatype is wrong. It should be an integer When I ran type function over the duration variable it shows this</p>
<pre><code>  type(duration)
</code></pre>
<p>Result:</p>
<pre><code>  pandas.core.series.Series
</code></pre>
<p>The variable 'duration' should be an integer and not pandas.series.
How can I convert it to that?</p>
","1","Question"
"79378007","","<p>I reviewed this question on stack overflow
<a href=""https://stackoverflow.com/questions/78562542/in-python-how-to-compare-the-value-to-each-subsequent-value-in-a-row-until-a-co"">In Python, how to compare the value to each subsequent value in a row until a condition is met</a>
and I would like to extend the criteria with other variables.</p>
<p>I have this dataframe in Python:</p>
<pre><code>import pandas as pd
data = {&quot;ID&quot;: [117, 117, 117, 117, 117, 117, 118, 118, 118, 118, 118, 118], 
        &quot;Date&quot;: [&quot;2023-11-14&quot;, &quot;2024-01-25&quot;, &quot;2024-02-01&quot;, &quot;2024-02-04&quot;, &quot;2024-02-11&quot;, &quot;2024-03-04&quot;,
        &quot;2024-01-02&quot;, &quot;2024-01-28&quot;, &quot;2024-02-04&quot;, &quot;2024-02-18&quot;, &quot;2024-03-11&quot;, &quot;2024-06-05&quot;], 
        &quot;status&quot;: ['S', 'S', 'S', 'E', 'E', 'E', 'E', 'E', 'S', 'S', 'S', 'E']}
df = pd.DataFrame(data)
</code></pre>
<p>What I would like to do is, compare the first date where variable &quot;status&quot; is 'S' to the next dates until the difference between the dates meets the threshold of 30 days. Then, once that row meets the threshold, I'd like to search to the next &quot;status&quot; is 'S' and this date will be checked to the next date and so on.
Rows within the threshold have the same integer/id/name and status does not matter.</p>
<p>I would expect an extra column 'flag' group-based on ID</p>
<p><a href=""https://i.sstatic.net/JpXK4EN2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpXK4EN2.png"" alt=""Expected output"" /></a></p>
<p>Expected results for patient 117</p>
<ul>
<li>index 0 is status S so this is the first reference date -&gt; flag will be 1</li>
<li>index 1 is more than 30 days later dan index 0 and has status S, so this is the new reference date -&gt; flag will be 2</li>
<li>index 2-4 is within 30 days, status does not matter -&gt; flag will be 2</li>
<li>index 5 is more than 30 days, but status is not S, so we will exlcude this row that is why the flag is 0 -&gt; flag will be 0</li>
</ul>
<p>Expected results for patient 118</p>
<ul>
<li>index 6-7 is the first date but has no status S, so we will exclude this row -&gt; flag will be 0</li>
<li>index 8 first date with status S, so this is the first reference date -&gt; flag will be 1</li>
<li>index 9 is within 30 days, status does not matter -&gt; flag will be 1</li>
<li>index 10 the date is more than 30 days and status S so this is the new reference date -&gt; flag will be 2</li>
<li>index 11 date is more than 30 days but has no status S so we will exclude this row -&gt; flag will be 0</li>
</ul>
<p>Python-code so far:
It is able to append the same integer as a date within 30 days and append a new integer as a date is over 30 days. But I also have to check the variable &quot;status&quot;. I am struggling to check if status is equal to 'S' for every new reference date.</p>
<pre><code>import pandas as pd
data = {&quot;ID&quot;: [117, 117, 117, 117, 117, 117, 118, 118, 118, 118, 118, 118], 
        &quot;Date&quot;: [&quot;2023-11-14&quot;, &quot;2024-01-25&quot;, &quot;2024-02-01&quot;, &quot;2024-02-04&quot;, &quot;2024-02-11&quot;, &quot;2024-03-04&quot;,
        &quot;2024-01-02&quot;, &quot;2024-01-28&quot;, &quot;2024-02-04&quot;, &quot;2024-02-18&quot;, &quot;2024-03-11&quot;, &quot;2024-06-05&quot;], 
        &quot;status&quot;: ['S', 'S', 'S', 'E', 'E', 'E', 'E', 'E', 'S', 'S', 'S', 'E']}
df = pd.DataFrame(data)

# make custom function
def get_flag(d, thresh=30):
    dates = pd.to_datetime(d['Date'])
    status = d['status']
    ref = dates.iloc[0]
    result = [1]
    n = 2
    e = 2
    
    for date in dates.iloc[1:]:
        if (date - ref).days &gt;= thresh:
            result.append(n)
            ref = date 
            n+=1
        else:
            result.append(e)
    return d.assign(flag=result)
        
# groupby + apply + custom function    
out = df.groupby('ID', group_keys=False).apply(get_flag)
out
</code></pre>
<p><strong>UPDATE:</strong></p>
<p>Let's change the dataframe to this:</p>
<pre><code>import pandas as pd data = {&quot;ID&quot;: [117, 117, 117, 117, 117, 117, 117, 117,117,117,117,118, 118, 118, 118, 118, 118], 
        &quot;Date&quot;: [&quot;2013-09-18&quot;, &quot;2016-02-07&quot;, &quot;2016-02-17&quot;, &quot;2016-02-20&quot;, &quot;2016-04-05&quot;, &quot;2016-04-12&quot;, &quot;2016-04-12&quot;, &quot;2016-04-14&quot;,  &quot;2016-04-16&quot;, &quot;2016-05-05&quot;, &quot;2016-05-16&quot;,
        &quot;2024-01-02&quot;, &quot;2024-01-28&quot;, &quot;2024-02-04&quot;, &quot;2024-02-18&quot;, &quot;2024-03-11&quot;, &quot;2024-03-12&quot;], 
        &quot;status&quot;: ['E', 'E', 'B', 'E', 'E', 'S', 'B', 'S', 'E', 'E', 'E', 
                   'E', 'S', 'E', 'S', 'E', 'S']} df = pd.DataFrame(data)
</code></pre>
<p>Applying the function to the dataframe gives flag 0 for all rows.
You expect that index 5 has flag 1 for patient 117, because status = 'S'.
Index 6,7,8,9 should also be flag 1 since the date is &lt;30 days of the reference date 2016-04-12.
For patient 18 you expect flag=1 for index 12 (because status = 'S') and 13, 14 (because &lt;30 days) and flag =2 for index 16 (status = 'S' and 30 days later than reference date of index 12 '2024-01-28')</p>
<p>The function below implies that the date of index 5 is within the threshold and create flag = 0, while status = S and should be new reference date in the loop.</p>
<pre><code>def get_flag(g, thresh=30):
    dates = pd.to_datetime(g['Date'])
    status = g['status'].eq('S').astype(int)
    ref = dates.iloc[0]
    s = status.iloc[0]
    result = [s]

    for i in range(1, len(g)):
        date = dates.iloc[i]
        stat = status.iloc[i]
        if (date - ref).days &gt;= thresh:
            result.append(result[-1]+stat if stat else 0)
            ref = date 
        else:
            result.append(result[-1])
    return g.assign(flag=result)
        
# groupby + apply + custom function    
out = df.groupby('ID', group_keys=False).apply(get_flag)
</code></pre>
","0","Question"
"79378096","","<p>I have two dataframe df_X and df_Y, and I want to merge these two (df_X has 1000 rows) with merge (how='inner'), everything is fine when df_Y has 1000 rows but it is very slow with big dataframe (expected: 40_000_000 rows).</p>
<p>I would like to speed up the calculations. I tried to use <code>df1.join(df2)</code> and sort them before the <code>merge</code> but with poor results. Any ideas to improve the code ?</p>
<p>NB : I am working with Linux or MacOS computers / python 3.12</p>
<p>Here is reproductible example of my attempts :</p>
<pre><code>import pandas as pd
import numpy as np
import time

match_columns = ['a', 'b', 'c', 'd', 'e']
extra_columns_X = ['X' + str(i) for i in range(9)]
extra_columns_Y = ['Y' + str(i) for i in range(9)]

# Define function to generate synthetic dataframes
def generate_synthetic_data():
    # Generate df1 with coarse resolution
    n_rows = 1000  # Desired number of rows
    df_X = pd.DataFrame({
        match_columns[0]: np.linspace(0, 10000, n_rows),  # Finer steps
        match_columns[1]: np.linspace(0, 5000, n_rows),
        match_columns[2]: np.linspace(10, 11000, n_rows),
        match_columns[3]: np.linspace(20, 12000, n_rows),
        match_columns[4]: np.linspace(30, 13000, n_rows),
    })
    # Add some extra columns to df1
    for col in extra_columns_X:
        df_X[col] = np.random.uniform(0, 1, len(df_X))  # Random values

    df_X = df_X.astype('float32')
    df_X = np.around(df_X, 2)
    
    # Generate df2 with finer resolution for match columns
    n_rows = 40000000  # Desired number of rows
    df_Y = pd.DataFrame({
        match_columns[0]: np.linspace(0, 10000, n_rows),  # Finer steps
        match_columns[1]: np.linspace(0, 5000, n_rows),
        match_columns[2]: np.linspace(10, 11000, n_rows),
        match_columns[3]: np.linspace(20, 12000, n_rows),
        match_columns[4]: np.linspace(30, 13000, n_rows),
    })

    # This enforce common values in match_columns
    interval = n_rows // len(df_X)
    for i, col in enumerate(match_columns):
        df_Y.iloc[::interval, i] = df_X[col].values  # Assign df_X values at regular intervals

    # Add some extra columns to df2
    for col in extra_columns_Y:
        df_Y[col] = np.random.uniform(0, 1, len(df_Y))  # Random values
    
    df_Y = df_Y.astype('float32')
    df_Y = np.around(df_Y, 2)
    
    return df_X, df_Y

# Generate the dataframes
df_X, df_Y = generate_synthetic_data()

# Timing for pd.merge
start_time = time.time()
merged_with_merge = pd.merge(df_X, df_Y, on=match_columns, how='inner')
print(&quot;Merge Time:&quot;, time.time() - start_time)

# # Timing for pd.merge
# start_time = time.time()
# df_X.sort_values(match_columns, inplace=True)
# df_Y.sort_values(match_columns, inplace=True)
# merged_with_merge = pd.merge(df_X, df_Y, on=match_columns, how='inner')
# print(&quot;Merge Time if sorted:&quot;, time.time() - start_time)

# Timing with dask
import dask.dataframe as dd
start_time = time.time()
ddf_X = dd.from_pandas(df_X, npartitions=10)
ddf_Y = dd.from_pandas(df_Y, npartitions=10)
merged_with_dask = dd.merge(ddf_X, ddf_Y, on=match_columns, how='inner').compute()
print(&quot;Merge Time with dask:&quot;, time.time() - start_time)

# Timing for set_index + join
start_time = time.time()
df_X.sort_values(match_columns, inplace=True)
df_Y.sort_values(match_columns, inplace=True)
df_X_indexed = df_X.set_index(match_columns)
df_Y_indexed = df_Y.set_index(match_columns)
merged_with_join = df_X_indexed.join(df_Y_indexed, how='inner').reset_index()
print(&quot;Join Time if sorted:&quot;, time.time() - start_time)

# Results
print(&quot;Rows Merged with Merge:&quot;, len(merged_with_merge))
print(&quot;Rows Merged with Join:&quot;, len(merged_with_join))
</code></pre>
","2","Question"
"79378938","","<p>The resample function doesn't work with 5h or 10h intervals. It starts okay and then it stops aligning on the day.</p>
<pre><code>d = {'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'}
df = read.resample('5H').agg(d)
df
</code></pre>
<p>output:</p>
<pre><code>                    Open    High    Low     Close
Date
2024-01-21 00:00:00 317.3   319.5   316.1   317.4
2024-01-21 05:00:00 317.4   319.4   317.2   319.0
2024-01-21 10:00:00 319.0   319.6   318.0   319.1
2024-01-21 15:00:00 319.1   321.6   317.5   320.6
2024-01-21 20:00:00 320.6   321.7   318.2   319.5
2024-01-22 01:00:00 319.5   320.1   314.1   317.2
2024-01-22 06:00:00 317.2   318.6   310.7   312.8
2024-01-22 11:00:00 312.8   315.3   308.4   311.1
2024-01-22 16:00:00 311.2   312.5   303.6   308.5
2024-01-22 21:00:00 308.5   308.7   304.8   308.2
2024-01-23 02:00:00 308.1   311.7   308.1   309.9
2024-01-23 07:00:00 310.0   310.2   291.7   295.9
</code></pre>
<p>It starts okay until the next day 2024-01-22.</p>
<p>I want to resample data like this:</p>
<ul>
<li>00:00 to 05:00 (this is 5 hours)</li>
<li>05:00 to 10:00 (5 hours)</li>
<li>10:00 to 15:00 (5 hours)</li>
<li>15:00 to 20:00 (5 hours)</li>
<li>20:00 to 00:00 next day (4 hours)</li>
</ul>
<p>It works correctly with 2h, 3h, 4h, or 6h intervals</p>
<pre><code>d = {'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'}
df = read.resample('4H').agg(d)
df
</code></pre>
<p>output:</p>
<pre><code>                    Open    High    Low     Close
Date
2024-01-21 00:00:00 317.3   319.5   316.1   318.5
2024-01-21 04:00:00 318.5   318.9   317.2   317.5
2024-01-21 08:00:00 317.5   319.6   317.3   319.3
2024-01-21 12:00:00 319.3   319.6   317.5   318.9
2024-01-21 16:00:00 318.9   321.6   317.9   320.6
2024-01-21 20:00:00 320.6   321.7   318.2   318.6
2024-01-22 00:00:00 318.7   320.3   315.3   316.7
2024-01-22 04:00:00 316.7   318.6   314.1   317.8
2024-01-22 08:00:00 317.9   317.9   310.7   313.5
2024-01-22 12:00:00 313.4   315.3   308.4   311.1
2024-01-22 16:00:00 311.2   312.5   303.6   306.6
</code></pre>
","1","Question"
"79379121","","<p>It should be simple,  but so far google has not been a friend, it's wasted a lot of time with non-answers, that answer the wrong question.
So, I know that we have <code>df.style.set_td_classes(classes)</code> that allows me to set a class on each 'td' based on a dataframe of class names.  However it does nothing for the index values.  I want the whole row to get a class. eg.  <code>&lt;tr class=myclass&gt;</code>, that would make it so simple,  I could create a series with class names where I wanted, exactly the same way as for set_td_classes.  But there is no function available.</p>
<p>What can I do to get pandas to set a class name onto a tr element ?</p>
<p>In response to a few comments.
Here is a small bit of python code to show the issue...</p>
<pre><code>def highlight_max(x, color):
if len(x.shape) &lt; 2:
    if any(abs(x)&gt;1.0):
        return pd.Series(f'color:{color};', x.index)
    else:
        return pd.Series(None, x.index)
else:
    return np.where(x.to_numpy()&gt;1.0, f&quot;color: {color};&quot;, None)
df = pd.DataFrame(np.random.randn(5, 2), columns=[&quot;A&quot;, &quot;B&quot;])
# 1
df.style.apply(highlight_max, color='red').to_html(Path.home()/'teststyle.htm')
# 2
df.style.apply(highlight_max, color='blue', axis=1).to_html(Path.home()/'teststyle.htm')
# 3
df.style.apply(highlight_max, color='green', axis=None).to_html(Path.home()/'teststyle.htm')  
</code></pre>
<p>If style.apply was to achieve the result I'm looking for then #2 should have given me my html (@Quang Hoang), but it doesn't.
It does highlight across all the columns in the row but not the index.
Also the html is quite ugly,  with a separate class defined for each cell!
E.g.</p>
<pre><code>&lt;html&gt;
&lt;head&gt;
&lt;style type=&quot;text/css&quot;&gt;
    #T_16f7a_row2_col0, #T_16f7a_row2_col1, #T_16f7a_row4_col0, #T_16f7a_row4_col1 {
      color: blue;
    }
    &lt;/style&gt;
    &lt;/head&gt;&lt;body&gt;&lt;table id=&quot;T_16f7a&quot;&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th class=&quot;blank level0&quot;&gt;&amp;nbsp;&lt;/th&gt;
          &lt;th id=&quot;T_16f7a_level0_col0&quot; class=&quot;col_heading level0 col0&quot;&gt;A&lt;/th&gt;
          &lt;th id=&quot;T_16f7a_level0_col1&quot; class=&quot;col_heading level0 col1&quot;&gt;B&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;th id=&quot;T_16f7a_level0_row0&quot; class=&quot;row_heading level0 row0&quot;&gt;0&lt;/th&gt;
          &lt;td id=&quot;T_16f7a_row0_col0&quot; class=&quot;data row0 col0&quot;&gt;0.570318&lt;/td&gt;
          &lt;td id=&quot;T_16f7a_row0_col1&quot; class=&quot;data row0 col1&quot;&gt;-0.791125&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;th id=&quot;T_16f7a_level0_row1&quot; class=&quot;row_heading level0 row1&quot;&gt;1&lt;/th&gt;
          &lt;td id=&quot;T_16f7a_row1_col0&quot; class=&quot;data row1 col0&quot;&gt;0.734733&lt;/td&gt;
          &lt;td id=&quot;T_16f7a_row1_col1&quot; class=&quot;data row1 col1&quot;&gt;0.344844&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;th id=&quot;T_16f7a_level0_row2&quot; class=&quot;row_heading level0 row2&quot;&gt;2&lt;/th&gt;
          &lt;td id=&quot;T_16f7a_row2_col0&quot; class=&quot;data row2 col0&quot;&gt;1.703771&lt;/td&gt;
          &lt;td id=&quot;T_16f7a_row2_col1&quot; class=&quot;data row2 col1&quot;&gt;0.693211&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;th id=&quot;T_16f7a_level0_row3&quot; class=&quot;row_heading level0 row3&quot;&gt;3&lt;/th&gt;
          &lt;td id=&quot;T_16f7a_row3_col0&quot; class=&quot;data row3 col0&quot;&gt;0.740752&lt;/td&gt;
          &lt;td id=&quot;T_16f7a_row3_col1&quot; class=&quot;data row3 col1&quot;&gt;-0.588767&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;th id=&quot;T_16f7a_level0_row4&quot; class=&quot;row_heading level0 row4&quot;&gt;4&lt;/th&gt;
          &lt;td id=&quot;T_16f7a_row4_col0&quot; class=&quot;data row4 col0&quot;&gt;-1.817743&lt;/td&gt;
          &lt;td id=&quot;T_16f7a_row4_col1&quot; class=&quot;data row4 col1&quot;&gt;0.602709&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;/body&gt;&lt;/html&gt;
</code></pre>
<p>There is a way to force your own table id (almost, it always gets the 'T_' prefix), so you could then have predictable class names for an external css file.   However it's not going to be too useful, since to change a set of rows, you are going to have to call a unique class for each row's class identifier (in each cell).  There is still no class exported into the 'tr' cell which is what I'm looking for.</p>
<p>Update:
I've found way that least allows a single class to control multiple rows (at present not including the index column). But it's not very intuitive.</p>
<pre><code>hls=df.loc[abs(df['B']) &gt; 1.0].index
classes = pd.DataFrame().reindex_like(df).astype(object)
classes.loc[hls]='shade'
classes.fillna('', inplace=True)
df.style.set_uuid('table').set_td_classes(classes).to_html(Path.home()/'teststyle.htm')
</code></pre>
<p>So this adds a class 'shade' on every 'td' cell that meets the given condition.  However, I do need to add my own extra css code into the html to control the 'shade' class.  But at least, to change color or some other style, I only change 1 class property.</p>
","0","Question"
"79379785","","<p><strong>Question</strong>: What I may be doing wrong and how we can fix the following error?</p>
<pre><code>import pandas as pd

def handle_bad_line(bad_line: list[str]) -&gt; list[str] | None:
    # Do something with the bad line, e.g., print it or modify it
    print(&quot;Bad line:&quot;, bad_line)
    # Return a modified line (if needed) or None to skip it
    return None  # Skip the bad line

df = pd.read_csv(&quot;DataFile.csv&quot;,engine=&quot;python&quot;, on_bad_lines=handle_bad_line)
</code></pre>
<p><strong>Error</strong>:</p>
<blockquote>
<p>ValueError: Argument &lt;function handle_bad_line at 0x000001D9A6683E20&gt; is invalid for on_bad_lines</p>
</blockquote>
<p><strong>Remark</strong>: The function option for <code>on_bad_lines</code> was added on Pandas 1.4.0 and above. Following is from <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer"">official document</a>:</p>
<blockquote>
<blockquote>
<p>Callable, function with signature (bad_line: list[str]) -&gt; list[str] | None that will process a single bad line. bad_line is a list of strings split by the sep. If the function returns None, the bad line will be ignored. If the function returns a new list of strings with more elements than expected, a ParserWarning will be emitted while dropping extra elements. Only supported when engine='python'</p>
</blockquote>
</blockquote>
","-1","Question"
"79381632","","<p>I have a recommendation dataset in Pandas with the following structure:</p>
<pre><code>import pandas as pd

data = {
    'consumer_id': ['C001', 'C001', 'C001', 'C002', 'C002', 'C002', 'C003', 'C003', 'C003', 'C004'],
    'resource_id': ['Amazon', 'Rappi', 'Magalu', 'Rappi', 'Amazon', 'Magalu', 'Amazon', 'Magalu', 'Eudora', 'Amazon'],
    'resource_type': ['affiliate', 'DG', 'affiliate', 'DG', 'affiliate', 'affiliate', 'affiliate', 'affiliate', 'affiliate', 'affiliate'],
    'order': [1, 2, 3, 1, 2, 3, 1, 2, 3, 1]
}

df_recommendation = pd.DataFrame(data)
</code></pre>
<p>Additionally, I have a list of consumers who purchased a specific item (e.g., &quot;Magalu&quot;) in the last 90 days:</p>
<pre><code>users_bought = {
    'consumer_id': ['C001', 'C002']
}

df_users_bought = pd.DataFrame(users_bought)
</code></pre>
<p>Objective:
I want to adjust the recommendation ranking for these customers so that if they have purchased &quot;Magalu&quot; in the last 90 days, it should appear first in their recommendation list among items with resource_type == 'affiliate'. The order of other recommendations should remain the same.</p>
<p>Expected Output:
For example, for consumer C002, the updated recommendation order should look like this:</p>
<pre><code>consumer_id  resource_id   resource_type                order
C002         Rappi         DG                           1
C002         Magalu        affiliate                    2
C002         Amazon        affiliate                    3
</code></pre>
<p>And for C001</p>
<pre><code>consumer_id  resource_id   resource_type                order
C001         Magalu        affiliate                    1
C001         Rappi          DG                          2
C001         Amazon         affiliate                   3
</code></pre>
","0","Question"
"79381686","","<p>I have an azure function with code below:</p>
<pre><code>storage_account_url = f&quot;{self.datalake_settings.STORAGE_ENDPOINT}/{parquet_folder_path}/{file_name}.parquet&quot;
storage_options = {
    &quot;account_name&quot;: self.datalake_settings.STORAGE_ACCOUNT,
    &quot;client_id&quot;: self.datalake_settings.RUNACCOUNT_ID,
    &quot;client_secret&quot;: self.datalake_settings.RUNACCOUNT_KEY.get_secret_value(),
    &quot;tenant_id&quot;: self.settings.TENANT_ID
}

df.to_parquet( storage_account_url, engine='pyarrow', compression='snappy', storage_options=storage_options )
</code></pre>
<p>This is my requirements.txt:</p>
<pre><code>azure-functions
azure-identity
azure-storage-blob
azure-monitor-opentelemetry
opentelemetry-api
opentelemetry-sdk
opentelemetry-semantic-conventions
pydantic
adlfs
azure-storage-blob
azure-storage-file-datalake
</code></pre>
<p>This is my .venv/lib:
<a href=""https://i.sstatic.net/GPkVu2sQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GPkVu2sQ.png"" alt=""enter image description here"" /></a></p>
<p>When I run this code I get following error:</p>
<blockquote>
<p>System.Private.CoreLib: Exception while executing function:
Functions.get_exchangerates_trigger. System.Private.CoreLib: Result:
Failure Exception: ImportError: Install adlfs to access Azure Datalake
Gen2 and Azure Blob Storage</p>
</blockquote>
<p>Any ideas how to troubleshoot this? It clearly looks like the adlfs and blobstorage packages are installed.</p>
","0","Question"
"79381694","","<p>I have a query that returns data in Miscrosoft SQL server management studio, but I'm getting the following error when trying to read in a query using pandas:</p>
<pre><code>sqlalchemy.exc.ResourceClosedError: This result object does not return rows. It has been closed automatically.
</code></pre>
<p>I from similar questions, I understand that using <code>SET NOCOUNT ON</code> at the start of the query resolves this (and has resolved this error for me in the past), however I have this in my query and am still getting the error.</p>
<p>My query is like:</p>
<pre><code>query = &quot;&quot;&quot;SET NOCOUNT ON;
    DECLARE @startdttm AS datetime
    DECLARE @enddttm AS datetime

    SET @startdttm = '01-APR-2024 00:00:00'
    SET @enddttm = '30-APR-2024 23:59:59'

    SELECT some_columns,
    INTO #temp1
    FROM table1
    WHERE date BETWEEN @startdttm AND @enddttm AND some_other_conditions
    GROUP BY some_columns

    SELECT some_columns,
    INTO #temp2
    FROM table2
    LEFT JOIN #temp1 ON some_column=some_column
    WHERE date BETWEEN @startdttm AND @enddttm AND some_other_conditions
    GROUP BY some_columns

    SELECT some_columns,
    INTO #temp3
    FROM table3
    WHERE date BETWEEN @startdttm AND @enddttm AND some_other_conditions
    GROUP BY some_columns

    SELECT columns,
    FROM   #temp2
    INNER JOIN #temp3 ON column=column
    WHERE  some conditions

    &quot;&quot;&quot;
</code></pre>
<p>and then I'm trying to read it into a pandas dataframe with:</p>
<pre><code>from sqlalchemy import create_engine
import pandas as pd
engine = create_engine('connection string')
df = pd.read_sql(query, engine)
</code></pre>
<p>and getting the above error.  I guess something to do with creating temporary tables, but all the solutions online suggest adding NOCOUNT, which in this case didn't resolve things.  Anyone know where to go from here?</p>
","0","Question"
"79381867","","<p>Why the script below does not work? How can I match the groupby boxplot and the DataFrame plot in the same figure?</p>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 1, figsize=(15, 15))
n = 480
ts = pd.DataFrame(np.random.randn(n), index=pd.date_range(start=&quot;2014-02-01&quot;,periods=n,freq=&quot;H&quot;))
ts.groupby(lambda x: x.strftime(&quot;%Y-%m-%d&quot;)).boxplot(subplots=False, rot=90, ax = axes)
ts.plot(ax = axes)
plt.show()
</code></pre>
","1","Question"
"79382113","","<p>I'm working with large dataframe and I'd like to read its index only, without having to load the whole dataframe.</p>
<p>I tried using <code>usecols=[0]</code> which I thought would only load the first column, and I'll be able to extract the indices. But it parses the whole dataframe...</p>
","0","Question"
"79382909","","<p>I have three sets of data (x,y) that define unique lineshapes. Using linear combination fitting I want to fit these three lineshapes to a fourth. I can set up the simple code below:</p>
<pre><code>#read in data (three components)
header_list=['Energy','Intensity']

component_1 = pd.read_csv('Lineshape_average.txt', delimiter=&quot;\t&quot;, skiprows=1,names=header_list)
component_2=pd.read_csv('Morb_11.txt', delimiter=&quot;\t&quot;,skiprows=1,names=header_list)
component_3=pd.read_csv('PM224.txt', delimiter=&quot;\t&quot;, skiprows=1,names=header_list)

c1=component_1['Intensity']
c2=component_2['Intensity']
c3=component_3['Intensity']

#read in data to be fit
df1 = pd.read_csv(file_name, delimiter=&quot;\t&quot;, skiprows=1,names=header_list)
df1['unc']=0.1

#set up model
def equation(params, obvs_intensity, uncertainty):
    a = params['a']
    b = params['b']
    c = params['c']

    calc_intensity=a*c1+b*c2+c*c3

    return (obvs_intensity-calc_intensity)/uncertainty

params = Parameters()
params.add('a', value = 0.33,min=0, max=1)
params.add('b',value =0.33,min=0, max=1)
params.add('c', value = 0.33, min=0, max=1)

#run model
result = minimize(equation, params, args =(df1['Intensity'],df1['unc']), nan_policy='omit')
</code></pre>
<p>Is there a way to allow the three components to shift in x (energy) so that I can get a better fit?</p>
","0","Question"
"79383355","","<p>I have a series with four categories A,B,C,D and their current value</p>
<p><code>s1 = pd.Series({&quot;A&quot;: 0.2, &quot;B&quot;: 0.3, &quot;C&quot;: 0.3, &quot;D&quot;: 0.9})</code></p>
<p>And a threshold against which I need to compare the categories,</p>
<p><code>threshold = {&quot;custom&quot;: {&quot;A, B&quot;: 0.6, &quot;C&quot;: 0.3}, &quot;default&quot;: 0.4}</code></p>
<p>But the threshold has two categories summed together: A &amp; B
And it has a <code>&quot;default&quot;</code> threshold to apply to each category that hasn't been specifically named.</p>
<p>I can't quite work out how to do this in a general way
I can solve two separate sub problems, but not the whole problem.</p>
<ol>
<li>I can solve the problem with single categories and a default threshold.</li>
<li>Or, I can solve combined categories, but not apply the default threshold.</li>
</ol>
<p><strong>What I need to evaluate is:</strong></p>
<pre><code>s1[A]+s1[B] &lt; threshold[&quot;custom&quot;][&quot;A,B&quot;] :: 0.2 + 0.3 &lt; 0.6
s1[C] &lt; threshold[&quot;custom&quot;][&quot;C&quot;] :: 0.3 &lt; 0.3
s1[D] &lt; threshold[&quot;default&quot;] :: 0.9 &lt; 0.4
</code></pre>
<p>And return this Series:</p>
<pre><code># A,B   True
# C    False
# D    False
</code></pre>
<p>Here is what I've got for the subproblems</p>
<p><strong>1. To apply the default threshold, I reindex and fillna with the default value:</strong></p>
<pre><code>aligned_threshold = (
    pd.Series(threshold.get(&quot;custom&quot;))
    .reindex(s1.index)
    .fillna(threshold.get(&quot;default&quot;))
)
# A    0.4
# B    0.4
# C    0.3
# D    0.4
</code></pre>
<p>then I can compare:</p>
<pre><code>s1 &lt; aligned_threshold
# A     True
# B     True
# C    False
# D    False
# dtype: bool
</code></pre>
<p><strong>2. To combine categories</strong></p>
<pre><code>threshold_s = pd.Series(threshold.get(&quot;custom&quot;))
s1_combined = pd.Series(index=threshold_s.index)
for category, threshold in threshold[&quot;custom&quot;].items():
    s1_combined[category] = sum([s1.get(k, 0) for k in category.split(&quot;, &quot;)])
# now s1_combined is:
# A,B    0.6
# C      0.3
s1_combined &lt; threshold_s
# A,B     True
# C      False
# dtype: bool
</code></pre>
<p>but I've lost category D</p>
<p><strong>To recap, what I need is:</strong></p>
<pre><code>s1[A]+s1[B]
s1[C]
s1[D]
</code></pre>
<p>So that I can compare thus:
<code>s1 &lt; threshold</code></p>
<p>And return this Series:</p>
<pre><code># A,B   True
# C    False
# D    False
</code></pre>
","3","Question"
"79383544","","<p>My data for machine learning has several variables, below is a box plot of one of the variables (call this x) against the outcome (call this y). I want to remove the outliers in x, but only for x = 0, 1, 2, 3, 4, as there are no outliers for x = 5 and above.</p>
<p><a href=""https://i.sstatic.net/nuqazLiP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nuqazLiP.png"" alt=""Box plot of variable x against outcome y"" /></a></p>
<p>I used the function below to try to remove the outliers using interquartile range (IQR) method:</p>
<pre><code>import pandas as pd 
import numpy as np

# Load the dataset
df = pd.read_csv('.xxx.csv')

# Function to remove outliers
def remove_outlier_using_IQR(df: pd.DataFrame, name_column: str, value: int) -&gt; pd.DataFrame:
    &quot;&quot;&quot;
    Remove outliers using IQR in the 'name_column'.

    Args:
    df (pd.DataFrame): The DataFrame containing the columns for outlier removal.
    name_column (str): The name of the column containing outliers to be removed.
    Value: Value in name column for outlier removal.

    Returns:
    pd.DataFrame: The DataFrame with outliers in 'name_column' removed.
    &quot;&quot;&quot;

# Detect outliers in the 'name_column'
    df2 = df[df[name_column]==value]
    Q1 = df2['final_test'].quantile(0.25)
    Q3 = df2['final_test'].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5*IQR
    upper = Q3 + 1.5*IQR
    df2.info()
    print(df2.shape)
    print(Q3)

# Create arrays of Boolean values indicating the outlier rows
    upper_array = np.where(df2['final_test'] &gt;= upper)[0]
    lower_array = np.where(df2['final_test'] &lt;= lower)[0]

    print(upper)
    print(upper_array)

# Removing the outliers
    df2.drop(index=upper_array, inplace=True)
    df2.drop(index=lower_array, inplace=True)

    df3 = df[df[name_column]!=value]
    df_merged = pd.concat([df2,df3], ignore_index=False, sort=False)

    return df_merged

# Use function to remove outliers
df = remove_outlier_using_IQR(df=df, name_column='hours_per_week', value=int(0))
df = remove_outlier_using_IQR(df=df, name_column='hours_per_week', value=int(1))
df = remove_outlier_using_IQR(df=df, name_column='hours_per_week', value=int(2))
df = remove_outlier_using_IQR(df=df, name_column='hours_per_week', value=int(3))
df = remove_outlier_using_IQR(df=df, name_column='hours_per_week', value=int(4))
</code></pre>
<p>Running the code gives:</p>
<pre class=""lang-none prettyprint-override""><code>print(df2.shape) =&gt; (56,18)
print(Q3) =&gt; 48.0
print(upper) =&gt; 55.5
print(upper_array) =&gt; [15 25 34 53]

KeyError                                  Traceback (most recent call last)
Cell In[36], line 46
     43     return df_merged
     45 # Use staticmethod fundction above to remove outliers identified from EDA boxplots
---&gt; 46 df = remove_outlier_using_IQR(df=df, name_column='hours_per_week', value=int(0))
     47 df = remove_outlier_using_IQR(df=df, name_column='hours_per_week', value=int(1))
     48 df = remove_outlier_using_IQR(df=df, name_column='hours_per_week', value=int(2))

Cell In[36], line 37
     34 print(upper_array)
     36 # Removing the outliers
---&gt; 37 df2.drop(index=upper_array, inplace=True)
     38 df2.drop(index=lower_array, inplace=True)
     40 df3 = df[df[name_column]!=value]

File c:\Users\xxx\AppData\Local\anaconda3\envs\hdbenv\Lib\site-packages\pandas\core\frame.py:5581, in DataFrame.drop(self, labels, axis, index, columns, level, inplace, errors)
   5433 def drop(
   5434     self,
   5435     labels: IndexLabel | None = None,
   (...)
   5442     errors: IgnoreRaise = &quot;raise&quot;,
   5443 ) -&gt; DataFrame | None:
   5444     &quot;&quot;&quot;
   5445     Drop specified labels from rows or columns.
...
-&gt; 7070         raise KeyError(f&quot;{labels[mask].tolist()} not found in axis&quot;)
   7071     indexer = indexer[~mask]
   7072 return self.delete(indexer)

KeyError: '[15, 25, 34, 53] not found in axis'
</code></pre>
<p>I want to use the code <code>df2.drop(index=upper_array, inplace=True)</code> to drop samples with index <code>[15, 25, 34, 53]</code> as they are outliers, however there is error saying <code>[15, 25, 34, 53]</code> not found in axis.</p>
","1","Question"
"79383833","","<p>I try the following example in Pandas 2.2.3:</p>
<pre class=""lang-py prettyprint-override""><code>outage_mask = pd.Series(([True]*5 + [False]*5)*5, index=pd.date_range(&quot;2025-01-01&quot;, freq=&quot;1h&quot;, periods=50))
[ts for ts in outage_mask.loc[outage_mask.diff().fillna(False)].index]
</code></pre>
<p>This gives me the error message:</p>
<blockquote>
<p>FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set <code>pd.set_option('future.no_silent_downcasting', True)</code></p>
</blockquote>
<p>I cannot figure out how to correctly apply this <code>infer_objects</code>. I assume the problem is that the output of <code>diff</code> becomes an 'object' dtype due do containing both <code>NaN</code>s and <code>bool</code>s, but for example this does not help:</p>
<pre class=""lang-py prettyprint-override""><code>[ts for ts in outage_mask.loc[outage_mask.diff().infer_objects(copy=False).fillna(False)].index]
</code></pre>
<p>I <em>can</em> avoid the warning by this clumsy work-around:</p>
<pre class=""lang-py prettyprint-override""><code>[ts for ts in outage_mask.loc[outage_mask.diff().astype(float).fillna(0.).astype(bool)].index]
</code></pre>
<p>but I would like to understand how to apply the solution from the warning correctly. How do I do that?</p>
","1","Question"
"79383889","","<p>I have a pandas dataframe which looks like this:</p>
<pre><code>1_2 1_3 1_4 2_3 2_4 3_4
1   5   2   8   2   2
4   3   4   5   8   5
8   8   8   9   3   3
4   3   4   4   8   3
8   0   7   4   2   2
</code></pre>
<p>where the columns are the 4C2 combinations of 1,2,3,4. And I would like to generate 4 new columns <code>f_1, f_2, f_3, f_4</code> where the values of the columns are defined to be</p>
<pre><code>df['f_1'] = df['1_2']+df['1_3']+df['1_4']
df['f_2'] = df['1_2']+df['2_3']+df['2_4']
df['f_3'] = df['1_3']+df['2_3']+df['3_4']
df['f_4'] = df['1_4']+df['2_4']+df['3_4']
</code></pre>
<p>In other words, the column <code>f_i</code> are defined to be the sum of columns <code>i_j</code> and <code>k_i</code>.</p>
<p>So I can brute force my way in this case. However, my original dataframe is a lot bigger and there are <code>20C2 = 190</code> columns instead and hence a brute force method wouldn't work.</p>
<p>So the desired outcome looks like</p>
<pre><code>1_2 1_3 1_4 2_3 2_4 3_4 f_1 f_2 f_3 f_4
1   5   2   8   2   2   8   11  15  6
4   3   4   5   8   5   11  17  13  17
8   8   8   9   3   3   24  20  20  14
4   3   4   4   8   3   11  16  10  15
8   0   7   4   2   2   15  14  6   11
</code></pre>
<p>Thank you so much.</p>
","5","Question"
"79385026","","<p>I have a dataset with 'tag-like' groupings:</p>
<pre><code>     Id       tags
0    item1    ['friends','family']
1    item2    ['friends']
2    item3    []
3    item4    ['family','holiday']
</code></pre>
<p>So a row can belong to several groups. I want to create an object similar to groupby, so that I can use agg etc.</p>
<pre><code>df.groupby('tags').count()
</code></pre>
<p>expected result</p>
<pre><code>     tags          count
0    'friends'    2
1    'family'     2
2    'holiday'    1
</code></pre>
<p>But of course it won't work because it treats the whole list as the key, rather than the individual tags. Here's an attempt</p>
<pre><code>tagset = set(df.tags.explode())
grpby  = { t: df.loc[df.tags.str.contains(t, regex=False)] 
           for t in tagset }
</code></pre>
<p>From what I understand, groupby objects are structured a bit like this.  But how to make it a <code>groupby</code> object? So that I can do things like <code>grpby.year.mean()</code> etc?</p>
","2","Question"
"79385475","","<p>So I'm reading multiple files (30 exactly). Some of them has the same columns, some has different or so.</p>
<pre><code>mycsvdir = r'C:\t\...\dict_full'

csvfiles = glob.glob(os.path.join(mycsvdir, '*.csv'))

dataframes = [] 
for csvfile in csvfiles:
    df = pd.read_csv(csvfile, encoding='UTF-16 LE', sep='\t') #, usecols=[&quot;event_rus&quot;, &quot;event_category&quot;, &quot;event_action&quot;])
    dataframes.append(df)

result = pd.concat(dataframes, ignore_index=True)

result.head()
</code></pre>
<p>With that code I get a</p>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 877 entries, 0 to 876
Data columns (total 51 columns):
</code></pre>
<p><a href=""https://i.sstatic.net/823bFfJT.png"" rel=""nofollow noreferrer"">very!</a> <a href=""https://i.sstatic.net/EDr2tcGZ.png"" rel=""nofollow noreferrer"">strange!</a> df : )</p>
<pre><code>result.columns = result.columns.str.replace(r' |  |\s|\xa0|\-|depreciated', '', regex=True).str.lower()
result.columns = result.columns.str.strip().str.replace(r' ', '')

result['event_action'] = result[['eventaction', 'eventactiondeprecated']].apply(lambda row: row.dropna().iloc[0] if not row.dropna().empty else None, axis=1)
result['event_category'] = result[['eventcategory', 'eventcategorydeprecated']].apply(lambda row: row.dropna().iloc[0] if not row.dropna().empty else None, axis=1)
result['event_label'] = result[['eventlabel', 'propertyeventlabel', 'eventlabel']].apply(lambda row: row.dropna().iloc[0] if not row.dropna().empty else None, axis=1)
...
</code></pre>
<p>After that I dropped or merged <a href=""https://i.sstatic.net/UmAeWLrE.png"" rel=""nofollow noreferrer"">not useful</a> columns, but I still have <a href=""https://i.sstatic.net/fzobg3F6.png"" rel=""nofollow noreferrer"">column duplicates</a>. <br />
My mind is blown away)) <br />
Last thing I want is to combine two column with information (and I know how to do that, I already tried and was successful), but I'd like to find out why all the above happening, and how to get kind of normal df.</p>
<p>Maybe <a href=""https://i.sstatic.net/Uts4hHED.png"" rel=""nofollow noreferrer"">the df types are needed</a>.</p>
","0","Question"
"79385781","","<p>I am calculating the difference between two datetime stamps inside an apply function.  As part of the calculations, i want to store some of the output of the calculations (seconds) for each row in the dataframe, at each pass of the apply function.  I am wondering if using the apply function vs a loop through the dataframe is correct.</p>
<p>Here is my simplified test input dataframe</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>date_time</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-01-24 12:00:00.000</td>
</tr>
<tr>
<td>2025-01-24 12:05:00.000</td>
</tr>
<tr>
<td>2025-01-24 12:10:00.000</td>
</tr>
</tbody>
</table></div>
<p>test code</p>
<pre><code>from datetime import datetime, timezone
import pandas as pd

str_event_time = '2025-01-24T011:55:00.000Z'
event_time = datetime.strptime(str_t_zero_time,'%Y-%m-%dT%H:%M:%S.%fZ')

def calculate_time_diff(date_time):

    #calculate the time diff in seconds
    time_diff_secs = (date_time - event_time).total_seconds()

    #save total second into new column in df 
    # ???
   
    # minutes and seconds
    m, s = divmod(abs(time_diff_secs), 60)

    # hours and minutes
    h, m = divmod(m, 60)

    time_diff_string = 'T{:+03.0f}:{:02.0f}:{:05.2f}'.format(h if t_time_seconds &gt;= 0 else -h, m, s)

    
    return time_diff_string 
    

def main():
    #create new columns
    df.insert(1,'time_delta',None,allow_duplicates=True)
    df.insert(1,'time_diff_s',None,allow_duplicates=True)

    time_format = '%Y-%m-%dT%H:%M:%S.%fZ'

    df['time_delta'] = df['date_time'].apply(calculate_time_diff)
    
    

</code></pre>
<p>Desired dataframe (ultimately saved to excel) is the calculations from the static, event time: '2025-01-24T011:55:00.000Z'</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>time_diff_s</th>
<th>time_delta</th>
<th>date_time</th>
</tr>
</thead>
<tbody>
<tr>
<td>300</td>
<td>T-00:05:00.00</td>
<td>2025-01-24 12:00:00.000</td>
</tr>
<tr>
<td>600</td>
<td>T-00:10:00.00</td>
<td>2025-01-24 12:05:00.000</td>
</tr>
<tr>
<td>900</td>
<td>T-00:10:00.00</td>
<td>2025-01-24 12:10:00.000</td>
</tr>
</tbody>
</table></div>
<p>how can i save to both the df['time_delta'] and df['time_diff_s'] column based the date_time value each time the apply function is called?  I don't have much experience using the apply function and was thinking it was more efficient than looping through the dataframe.  I'd like to learn the best way to update multiple columns in a dataframe based on the calulation of a different column.</p>
","0","Question"
"79387911","","<p>Say I have this data frame:</p>
<pre><code>import pandas as pd

x = pd.DataFrame([[1, 'step', 'id', 22, 33], 
                  [2, 'step', 'id', 55, 66]], 
                  columns=['time', 'head_1', 'head_2', 'value_1', 'value_2'])

print(x)

   time head_1 head_2  value_1  value_2
0     1   step     id       22       33
1     2   step     id       55       66
</code></pre>
<p>Then I use pivot table like below</p>
<pre><code>print(x.pivot_table(values=['value_1', 'value_2'], columns='time', index=['head_1', 'head_2']))

              value_1     value_2    
time                1   2       1   2
head_1 head_2                        
step   id          22  55      33  66
</code></pre>
<p>However, I really want to have value_1 and value_2 in rows instead of columns like below (a new header as head_3). That is, put value_1 and value_2 in rows and only time as column. How do I do that?</p>
<pre><code>time                       1   2
head_1 head_2 head_3                       
step   id     value_1     22  55
step   id     value_2     33  66
</code></pre>
","1","Question"
"79388899","","<pre><code>import requests
import pandas as pd
import numpy as np
import os
print(&quot;Saving file to:&quot;, os.getcwd())


# Define the URL
URL = &quot;https://web.archive.org/web/20230902185326/https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29&quot;

# Fetch the HTML content of the webpage
response = requests.get(URL)
if response.status_code == 200:
    print(&quot;Successfully fetched the page!&quot;)
    html_content = response.text
else:
    print(f&quot;Failed to fetch the page. Status code: {response.status_code}&quot;)
    exit()

# Extract tables from the webpage
try:
    tables = pd.read_html(html_content)
    if not tables:
        print(&quot;No tables found in the HTML content.&quot;)
        exit()
except ValueError as e:
    print(f&quot;Error reading HTML tables: {e}&quot;)
    exit()

# Inspect all extracted tables
for i, table in enumerate(tables):
    print(f&quot;Table {i}:&quot;)
    print(table.head())
    print(&quot;\n&quot;)

# Select the required table (adjust index if necessary)
df = tables[3]  # Replace 3 with the correct index if needed
print(&quot;Selected table:&quot;)
print(df.head())

# Dynamically rename and inspect columns
df.columns = range(df.shape[1])  # Replace headers with numerical indices
print(&quot;Columns after renaming:&quot;, df.columns)

# Handle missing columns dynamically
if 2 in df.columns:
    df = df[[0, 2]]  # Select columns 0 and 2
else:
    print(&quot;Column 2 not found. Available columns:&quot;, df.columns)
    exit()

# Retain rows for the top 10 economies
df = df.iloc[1:11, :]

# Rename columns
df.columns = ['Country', 'GDP (Million USD)']

# Convert GDP from Million USD to Billion USD and round to 2 decimal places
df['GDP (Million USD)'] = df['GDP (Million USD)'].astype(float)
df['GDP (Million USD)'] = np.round(df['GDP (Million USD)'] / 1000, 2)

# Rename the column header to 'GDP (Billion USD)'
df.rename(columns={'GDP (Million USD)': 'GDP (Billion USD)'}, inplace=True)

# Save the DataFrame to a CSV file
df.to_csv(r&quot;C:\Users\Path\Largest_economies.csv&quot;, index=False)

print(&quot;The top 10 economies by GDP have been saved to 'Largest_economies.csv'.&quot;)
</code></pre>
<p>Verified the current working directory using os.getcwd(), explicitly set the file path in the to_csv() method, tested write permissions by saving to simpler locations like the Desktop, wrapped to_csv() in a try-except block to catch errors, and ensured proper encoding and folder existence for successful file generation</p>
","1","Question"
"79388966","","<p>I have a pandas dataframe, which contains heterogeneous data variables (strings and numberical values).</p>
<p>Is there a quick way of visualising the relationships between these variables in a plot, where each column and row in the plot would correspond to the individual data variable?</p>
<p>I have tried using sns.pairplot(df), but it ignores the string variables.</p>
","0","Question"
"79389094","","<p>I have a problem with a Python script.
It creates an e-mail with a pandas datasource.
I would like to add one or more pictures but I can not.
Without the pictures it works properly.</p>
<p>I ask you to help me in order to fix the problem.</p>
<pre><code>import cx_Oracle
import pandas as pd
import sqlalchemy
import smtplib
import matplotlib

from email.mime.text import MIMEText
from email.header import Header
from email.mime.image import MIMEImage

image_path=&quot;C:\\Users\\n.restaino\\PycharmProjects\\pythonProject\\.venv\\barchart1.png&quot;

def send_simple_email(host, subject, from_addr, to_addr, body, password):
    # Instantiate MIMEText object
    message = MIMEText(body, 'html', 'utf-8')
    message['From'], message['To'] = from_addr, to_addr
    message['Subject'] = Header(subject, &quot;utf-8&quot;)
    message = message.as_string()   # Because SMTP.sendmail, required bytes-like string. Otherwise you will see error like: Error: expected string or bytes-like object

    try:
        import ssl
        context = ssl.create_default_context()
        server = smtplib.SMTP_SSL(host, 465, context=context)
        server.login(from_addr, password)
        server.sendmail(from_addr, [to_addr], message)
    except Exception as e:
        print(f&quot;Error: {e}&quot;)
    finally:
        server.quit()
        print(&quot;Email Sent!&quot;)

if __name__ == &quot;__main__&quot;:
    # Connection details
    user = 'USER'
    pw = 'PWD'
    host = '10.0.0.1'
    port = '1521'
    db = 'DB'

    engine = sqlalchemy.create_engine('oracle+cx_oracle://' + user + ':' + pw + '@' + host + ':' + port + '/?service_name=' + db)
    my_query='SELECT * FROM T_C10'
    df = pd.read_sql(my_query, engine)
    ax = df.plot.bar(x='tipo', y='numero', rot=0)

    fig = ax.get_figure()
    fig.savefig(image_path)

    host = &quot;smtp.gmail.com&quot;  # Using Gmail server to send the email
    subject = &quot;[AUTO EMAIL] Report Sales&quot;
    html_body = f&quot;&quot;&quot;&lt;h1&gt; Sales Report &lt;/h1&gt; {df.to_html()}
                    &lt;img src={image_path} alt=&quot;The Python logo&quot;&gt;
    &quot;&quot;&quot;

    from_addr, to_addr = &quot;C.D@gmail.com&quot;, &quot;A.B@gmail.com&quot;

    # Open the image file in binary mode

    # Get Passwork from user
    import getpass

    password = '1234 5678 '  # getpass.getpass()''

    # Sending Email
    send_simple_email(host, subject, from_addr, to_addr, html_body, password)
</code></pre>
<p>I have already read the associated post. But with pandas and gmail it's a little bit different. Please give me some advices. Thanks</p>
","0","Question"
"79390524","","<p>I have two issues on a dataframe:</p>
<ul>
<li>It does not have the correct headers</li>
<li>The current headers contain values that should be a &quot;simple&quot; (first) row of the dataframe
<em>How do I keep my current header as the first row of my dataframe and implement correct names for my headers ?</em></li>
</ul>
<p>My current solution consists of 4 steps :</p>
<ul>
<li>Define the list of the expected headers : <code>ref_columns = ['column 1', 'column2']</code></li>
<li>Use list comprehension and a dataframe constructor to create a dataframe from my <em>original dataframe</em> containing only one row (the old headers) and with the correct headers (<em>header dataframe</em>) : <code>df_header_row = pd.DataFrame([list(df_original.columns)], columns=ref_columns)</code></li>
<li>Force the original dataframe's headers to the expected ones : <code>df_original.columns = ref_columns</code></li>
<li>Concate the <em>header dataframe</em> and the <em>original dataframe</em> : <code>df_full = concat([df_header_row, df_original], ignore_index=True)</code></li>
</ul>
<p>It is definitely not a fluid solution and requires to recreate a full dataframe (<em>header dataframe</em>). Does anyone have a better solution?</p>
<p>Thanks</p>
","0","Question"
"79390828","","<p>Suppose I have this pandas dataset:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Question Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Q01</td>
</tr>
<tr>
<td>1</td>
<td>Q01-1</td>
</tr>
<tr>
<td>1</td>
<td>Q02</td>
</tr>
<tr>
<td>2</td>
<td>Q01</td>
</tr>
<tr>
<td>2</td>
<td>Q02</td>
</tr>
<tr>
<td>2</td>
<td>Q02-1</td>
</tr>
<tr>
<td>2</td>
<td>Q02-1-1</td>
</tr>
<tr>
<td>2</td>
<td>Q02-2</td>
</tr>
</tbody>
</table></div>
<p>I want to remove the rows based on certain hierarchical conditions between the values of question codes per ID. For example, Q01-1 is a sub-question when Q01 is answered, soI don't need to keep Q01 anymore since we already have Q01-1. By ID 2, I need to show Q01, Q02-1-1 (since it is a sub-question of Q02-1, which is also one of Q02) and Q02-2 (since it is also another sub-question of Q02).</p>
<p>The desired final result of the table above would be:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Question Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Q01-1</td>
</tr>
<tr>
<td>1</td>
<td>Q02</td>
</tr>
<tr>
<td>2</td>
<td>Q01</td>
</tr>
<tr>
<td>2</td>
<td>Q02-1-1</td>
</tr>
<tr>
<td>2</td>
<td>Q02-2</td>
</tr>
</tbody>
</table></div>
<p>Thanks in advance for the help!</p>
","1","Question"
"79392038","","<p>My organization has account numbers that are comprised of combining multiple fields. The last field is always 4 characters (typically 0000)</p>
<pre><code>Org           Account
 01       01-123-0000
 01       01-456-0000
 02       02-789-0000
 02       02-456-0000
 03       03-987-0000
 03       03-123-1234
</code></pre>
<p>I also have a dictionary mapping of how many characters the last component should be.</p>
<pre><code>MAP = {'01': 4, '02': 3, '03': 3}
</code></pre>
<p>However there also special mappings for Org 03:</p>
<pre><code>D03_SPECIAL_MAP = {'0000': '012', '1234': '123'}
</code></pre>
<p>My code to update the last component is:</p>
<pre><code>for i,r in df.iterrows():
    updated = False # Keep track if we have updated this row

    # Split off last component from the rest of the account
    Acct, last_comp = r['Account'].rsplit('-',1)

    # Check if we need to update code length and the code length does not match
    if r['Org'] in MAP and len(last_comp) != MGMT_MAP[r['Org']]:
        df.at[i,'Account'] = '-'.join(Acct) + &quot;-&quot; + last_comp.zfill(MAP[r['Org']])
        updated = True

    # Special mapping for Org 03
    if r['Org'] =='03' and last_comp in D03_SPECIAL_MAP.keys():
        df.at[i,'Account'] = '-'.join(Acct) + &quot;-&quot; + D03_SPECIAL_MAP[last_comp]
        updated = True

    if not updated: # Join Default if we have not hit either of he conditions above
        df.at[i,'Account'] = '-'.join(Acct) + &quot;-&quot; + last_comp
</code></pre>
<p>The output of this will be:</p>
<pre><code>Org           Account
 01       01-123-0000
 01       01-456-0000
 02       02-789-000
 02       02-456-000
 03       03-987-012
 03       03-123-123
</code></pre>
<p>My code works as expected except this process is a little slow to check every record. Is there a way to perform the same operation without using <code>df.iterrows()</code>?</p>
","1","Question"
"79392071","","<p>I have to match <code>Payment Slips</code> with <code>Transaction</code> data. Those two sets of data are not directly linked, as there isn't a shared ID among them.</p>
<p>I came up with the <code>Block_ID</code>,  calculated value I can use to match them as close as possible.</p>
<p>Unfortunately, there may be repeated <code>Block_ID</code> so some <code>Transaction</code> may match the same <code>Payment Slip</code> while the other <code>Payment Slips</code> with the same <code>Block_ID</code> will remain unmatched.</p>
<pre><code>transactions = pd.DataFrame(
    [[&quot;9103cd45-95a4-4671-892b-10c1192d774e&quot;, 2.40,     &quot;2023-09-20&quot;, &quot;20230920|2.40&quot;, None],
     [&quot;900899fb-13eb-483d-9fc4-aa62d18e0b8a&quot;, 2.40,     &quot;2023-09-20&quot;, &quot;20230920|2.40&quot;, None],
     [&quot;6007e789-fa40-4983-9dc7-06560c98616b&quot;, 2.40,     &quot;2023-09-20&quot;, &quot;20230920|2.40&quot;, None],
     [&quot;421714d0-b6dc-4dd3-b747-6ba2fc7b63b7&quot;, 414.66,   &quot;2023-09-20&quot;, &quot;20230920|414.66&quot;, None],
     [&quot;8845a1ef-c625-4874-89cf-0ad83abd7ec3&quot;, 429.64,   &quot;2023-09-20&quot;, &quot;20230920|429.64&quot;, None],
     [&quot;64934365-c5b6-4976-9b73-ebd0605e4ad6&quot;, 1377.50,  &quot;2023-09-20&quot;, &quot;20230920|1377.50&quot;, None],
     [&quot;571bc685-6459-4fe9-ac1c-ea4a80a2533b&quot;, 2190.00,  &quot;2023-09-20&quot;, &quot;20230920|2190.00&quot;, None],
     [&quot;20876834-40a2-42e3-80af-728d7dda1c84&quot;, 2600.00,  &quot;2023-09-20&quot;, &quot;20230920|2600.00&quot;, None],
     [&quot;e642c6b7-d154-49d2-9863-7849ff64b8c8&quot;, 10480.02, &quot;2023-09-20&quot;, &quot;20230920|10480.02&quot;, None],
     [&quot;317f414e-8570-41e3-ac4c-39a5bd20a6a4&quot;, 32856.66, &quot;2023-09-20&quot;, &quot;20230920|32856.66&quot;, None]],
    columns=[&quot;id&quot;, &quot;amount&quot;, &quot;date&quot;, &quot;block_id&quot;, &quot;slip_id&quot;])

payment_slips = pd.DataFrame(
    [[&quot;7f2176dd-68ee-4b49-9f57-9e661cb3bf41&quot;, 2.40,     &quot;2023-09-20&quot;, &quot;20230920|2.40&quot;],
     [&quot;21843656-15b1-4ea6-b9a8-6ad9672b4a2e&quot;, 10.80,    &quot;2023-09-20&quot;, &quot;20230920|10.80&quot;],
     [&quot;b0389f34-086d-4e66-ac75-a69c215d7c9a&quot;, 206.85,   &quot;2023-09-20&quot;, &quot;20230920|206.85&quot;],
     [&quot;857a161b-a8fe-4939-b951-686e73a7c5b6&quot;, 414.66,   &quot;2023-09-20&quot;, &quot;20230920|414.66&quot;],
     [&quot;f6f8305c-5260-4bce-9be8-0618ed500389&quot;, 429.64,   &quot;2023-09-20&quot;, &quot;20230920|429.64&quot;],
     [&quot;790d5425-ed53-4ecc-8e13-5d7726cae289&quot;, 1377.50,  &quot;2023-09-20&quot;, &quot;20230920|1377.50&quot;],
     [&quot;f0885f50-6617-4fa8-a2e4-79e917cb2237&quot;, 2190.00,  &quot;2023-09-20&quot;, &quot;20230920|2190.00&quot;],
     [&quot;5f24b735-8dfb-4592-bef5-6fb74d6f4e24&quot;, 2600.00,  &quot;2023-09-20&quot;, &quot;20230920|2600.00&quot;],
     [&quot;717cc01c-7520-45ad-bb72-2c0bafc79c6d&quot;, 10480.02, &quot;2023-09-20&quot;, &quot;20230920|10480.02&quot;],
     [&quot;f3b31d20-9a74-43ac-aaee-56a3b87674d8&quot;, 32856.66, &quot;2023-09-20&quot;, &quot;20230920|32856.66&quot;]],
    columns=[&quot;id&quot;, &quot;amount&quot;, &quot;date&quot;, &quot;block_id&quot;])

known_slips = set()
for transaction in transactions.itertuples():
    match = payment_slips.loc[
        (payment_slips['block_id'] == transaction.block_id)
        &amp; (~payment_slips['id'].isin(known_slips))]
    if not match.empty:
        transactions.at[transaction.Index, 'slip_id'] = match.iloc[0]['id']
        known_slips.add(match.iloc[0]['id'])
</code></pre>
<p>In the example above, you can see that the top 3 Transactions will all match the first Payment Slip, which is wrong, only the first Transaction should have a match, while the others don't.</p>
<p>This was solved by keeping track of all matched slip IDs, so a previously matched ID cannot be paired again.</p>
<p>While this approach works, I know it is inefficient because I have to iterate the whole transactions DataFrame and re-search the Slips DataFrame multiple times.</p>
<p>And while not in the example above, The data is residing in a SQL Database, and I would like to use <strong>pandas DataFrame read_sql</strong> as I can batch data using the <code>chunksize</code> parameter.</p>
<p>Currently my studies pointed me to <strong>pandas DataFrame merge</strong> to accomplish this but it seems I must perform a <strong>cross-apply</strong> and then filter.</p>
<p>This approach also seems wasteful, as I will end up with a Cartesian product of the frames which easily surpass thousands of records.</p>
<p>I realize there may be other approaches using pandas however I am not familiar with the library enough.</p>
<p>Is there a way to solve this problem with a pandas solution?</p>
","0","Question"
"79392369","","<h3>Problem:</h3>
<p>I have a two-step bioinformatics pipeline where:</p>
<ol>
<li><strong>Code 1</strong> generates output files (<code>.marked.bam</code>) and places them into a directory structure.</li>
<li><strong>Code 2</strong> processes annotated files (<code>annotated.hg38_multianno.txt</code>) generated downstream, and I need to populate the <code>Tumor_Sample_Barcode</code> column in these annotated files with the corresponding sample names.</li>
</ol>
<p>Currently, the <code>Tumor_Sample_Barcode</code> column is not being populated correctly—it ends up with placeholder values (e.g., <code>annovar_output</code>) instead of the actual sample names extracted from the <code>.marked.bam</code> filenames.</p>
<hr />
<h3>Goal:</h3>
<p>Populate the <code>Tumor_Sample_Barcode</code> column in <code>annotated.hg38_multianno.txt</code> files with the correct sample names derived from the <code>.marked.bam</code> filenames in <strong>Code 1</strong>.</p>
<hr />
<h1>Example input DataFrame</h1>
<pre><code>Sample1,Polyp
Sample2,Normal
Sample3,Polyp

BAM files:
Sample1.marked.bam
Sample2.marked.bam
Sample3.marked.bam

</code></pre>
<p>Sample names are derived from the * in the *.marked.bam. For example, Sample1.marked.bam has sample name &quot;Sample1&quot;.</p>
<h1>Desired Output</h1>
<p>For each annotated.hg38_multianno.txt file, the Tumor_Sample_Barcode column should contain the sample name.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Column A</th>
<th>Column B</th>
<th>Tumor_Sample_Barcode</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cell 1</td>
<td>Cell 2</td>
<td>Sample1</td>
</tr>
<tr>
<td>Cell 3</td>
<td>Cell 4</td>
<td>Sample2</td>
</tr>
</tbody>
</table></div>
<h4>Code 1: Generates <code>.marked.bam</code> files</h4>
<pre class=""lang-bash prettyprint-override""><code># Extract sample names from .marked.bam files
for bam_file in &quot;$OUTPUT_DIR&quot;/*.marked.bam; do
    sample_name=$(basename &quot;$bam_file&quot; | sed 's/\.marked\.bam$//')
    echo &quot;Processing BAM file for sample: $sample_name&quot;
    # Processing and generating annotated files for each sample...
done
</code></pre>
<h4>Code 2: Processes Annotated Files</h4>
<pre class=""lang-bash prettyprint-override""><code># For each annotated file, extract the sample name from its parent directory
for ANNOTATED_FILE in $(find &quot;$ANNOVAR_OUTPUT_DIR&quot; -name &quot;annotated.hg38_multianno.txt&quot;); do
    SAMPLE_NAME=$(basename &quot;$(dirname &quot;$ANNOTATED_FILE&quot;)&quot;)

    # Populate &quot;Tumor_Sample_Barcode&quot; column
    awk -v OFS='\t' -v sample_name=&quot;$SAMPLE_NAME&quot; '
        NR == 1 {
            # Add column if not present
            col = -1
            for (i = 1; i &lt;= NF; i++) if ($i == &quot;Tumor_Sample_Barcode&quot;) col = i
            if (col == -1) {
                print $0, &quot;Tumor_Sample_Barcode&quot;
                col = NF + 1
            } else {
                print $0
            }
        }
        NR &gt; 1 {
            $col = sample_name
            print $0
        }
    ' &quot;$ANNOTATED_FILE&quot; &gt; &quot;${ANNOTATED_FILE}.temp&quot;
    mv &quot;${ANNOTATED_FILE}.temp&quot; &quot;$ANNOTATED_FILE&quot;
done
</code></pre>
","2","Question"
"79392973","","<p>I have a data processing algorithm where I want to set values for a cell based on the value in another row. All within the same column. We can group these rows by the eid column.</p>
<p>I want to calculate the max value of the field for a group of rows (group by eid) and in case where a given value is no good (i.e. blank or not of type int/double) then rather than taking the max, I assign that problematic value instead. If there are multiple values with wrong data type then pick any of them.</p>
<p>The way to filter out the rows afterwards is by using a flag PF. One 1 row will have a flag value 1.</p>
<p>So, in short, assign (override, if you have to) all the max values for the group of rows to the row with the flag=1 and then once done, discard the rows where the flag is not 1.</p>
<p>Let's say we loop over these 2 data fields.</p>
<pre><code>Transform_fields = ('LV1','LV2')

for i in range(len(Transform_fields)):
        
        field_name = Transform_fields[i]
        
        data[field_name + '_org'] = data[field_name]

        try:
            data[field_name] = data[field_name].astype('float')
            
            data[field_name + '_max'] = data.groupby(['eid'])[field_name].transform(max)

            data[field_name + '_max'].fillna(data[field_name + '_org'], inplace = True)

            data[field_name] = data.groupby('eid')[field_name + '_max'].transform(lambda x: 0 if (x == 0).any() else x)
        except Exception as e:

        print('Error reading data from file ' +  str(e) + ': \n')
    
        #need to update the value of the field of the record where the PF is 1 in that group

#finally,
data = data[data['pf'] == '1']
</code></pre>
<p>Some data samples shown by this simple program below to display a sample input DF and resulting DF -</p>
<pre><code>import pandas as pd

df_in = pd.DataFrame({'eid': [10001, 10001, 10002, 10002, 10003, 10004, 10004, 10004],
                   'LV1': ['', -8888, -8888, -8888, -8888, '&amp;', 33, 'TIM'],
                   'LV2': [-8888, 'GRAY', 'blue', 9, 22, 55, 68, 166],
                   'PF': [1, 0, 1, 0, 1, 1, 0, 0]})
print('Original Dataframe:')
print(df_in)

df_out = pd.DataFrame({'eid': [10001, 10002,  10003, 10004],
                   'LV1': ['', -8888, -8888, '&amp;'],
                   'LV2': ['GRAY', 'blue',  22, 166],
                   'PF': [1, 1, 1, 1]})

print(&quot;\nTransformed Dataframe:&quot;)
print(df_out)
</code></pre>
<hr />
<p><strong>Update 3 days after first posting this question-</strong></p>
<p>Just thought to share in case anyone else reads this later on.
So I have followed suggestions made by user @mozway below and it works great for a relatively small amount of input data.</p>
<p>However, when I run this program for a larger data file (comprising 103,000 rows X 125 columns) then it does not finish running in a reasonable time. So the 'Transform_fields' is a list 125 items long, data for which is supplied in the larger input file.</p>
<p>I am reading the large input file via a call to pandas.read_csv function</p>
<p>I ran the same program for that larger file and after approx. 10 min of waiting for it to finish, I introduced a KeyboardInterrupt to terminate the execution.</p>
<p>Not saying that the suggestion provided did not work (thanks again @mozway). Just that it did not scale well even though it uses pandas (i.e. vectorization).</p>
<p>I also then tried to get rid of the for loop by changing it to this below and that also did not finish running in more than 10 mins so I had to terminate the execution again -</p>
<pre><code># Perform the groupby operation once for all columns
agg_dict = {field: cust_max for field in Transform_fields}
out = df_in.groupby('eid', as_index=False).agg(agg_dict)
</code></pre>
<p>Not sure yet what the solution is going to be. Thank you.</p>
","1","Question"
"79393122","","<p>I have a table like:</p>
<pre><code>           | 2022 | 2022 | 2021 | 2021
class      |   A  |   B  |   A  |   B
-----------|------|------|------|------
X          |   1  |   2  |   3  |   4
Y          |   5  |   6  |   7  |   8
</code></pre>
<p>How can I transform it to following form?</p>
<pre><code>year   |  category | class | value
----------------------------------
2022   |     A     |   X   |   1
2022   |     A     |   Y   |   5
2022   |     B     |   X   |   2
2022   |     B     |   Y   |   6
2021   |     A     |   X   |   3
2021   |     A     |   Y   |   7
2021   |     B     |   X   |   4
2021   |     B     |   Y   |   8
</code></pre>
<p>I tried various combinations of <code>pd.melt</code> with no success..</p>
<p>Thx in advance!</p>
","3","Question"
"79393269","","<p>I am working on a code to split one Excel file into multiple files based on some criteria,
I got it (mostly) working, but I am having big issue with headers from Excel.</p>
<p>when I use this code:</p>
<pre><code>df = pd.read_excel(file_path, sheet_name=sheet.Name, header=0)
</code></pre>
<p>it does not grab the headers correctly.
I have tried multiple times, multiple approaches,
but as you can see <a href=""https://i.sstatic.net/MS0DFdpB.png"" rel=""nofollow noreferrer"">enter image description here</a>
for all my tables, the object is readable and the range is correctly grabbed.</p>
<p>Here is my Partial code:</p>
<pre><code>            try:
                list_objects = sheet.ListObjects
                if list_objects.Count &gt; 0:
                    print(f&quot;Found {list_objects.Count} List Object(s) (visible tables) in sheet {sheet.Name}&quot;)
                    for list_object in list_objects:
                        print(f&quot;  - Table Name: {list_object.Name}&quot;)
                        print(f&quot;    - Visible: {list_object.Parent.Visible}&quot;)
                        print(f&quot;    - Range Address: {list_object.Range.Address}&quot;)
                        
                        # Only process tables that are in our required list
                        if list_object.Name not in REQUIRED_TABLES:
                            print(f&quot;    - Skipping table {list_object.Name} (not in required tables list)&quot;)
                            continue
                        
                        # Get table range address
                        table_range = list_object.Range.Address
                        # Convert Excel range address to zero-based indices
                        # Remove $ signs and split into start/end cells
                        table_range = table_range.replace('$', '')
                        start_cell, end_cell = table_range.split(':')
                        
                        # Read the sheet and extract the table data
                        df = pd.read_excel(file_path, sheet_name=sheet.Name, header=0)

</code></pre>
<p>How do I modify this line:</p>
<pre><code>df = pd.read_excel(file_path, sheet_name=sheet.Name, header=0)
</code></pre>
<p>to grab the range from List_object?</p>
<p>I have tried directly assigning Table to a df:
df = list_object.Range.Value but it was in error.</p>
","0","Question"
"79394177","","<p>I have this code that I wrote and it's taking too long to run. I was advised to vectorize this operation but so far I have found only multiplication examples. Here is my code:</p>
<pre><code>my_dict = {}
for i in list(df.index):
    my_dict[i] = myClass(df.loc[i, 'name'])
    my_dict[i].class_method({'col1': df.loc[i, 'col1']})
    my_dict[i].class_method({'col2': df.loc[i, 'col2']})
    ...
</code></pre>
<p>and so on until 'col17'. Someone reviewed my code and said to <strong>'use the fact that df is a dataframe and not loop through and don't use the expensive .loc() operation'</strong></p>
<p>The only thing I could come up with is:</p>
<pre><code>my_list = ['col1', 'col2', ..., 'col17']
my_dict = {}

for i in list(df.index):
    my_dict[i] = myClass(df.loc[i, 'name'])
        for col in my_list:
            my_dict[i].class_method({col: df.loc[i, col})
    
</code></pre>
<p>but this is not really vectorizing anything... are there any secret ways around pandas vectorization that I don't know about?</p>
","0","Question"
"79395144","","<p>Excel file #1:</p>
<pre><code>Date      Data1   Data2
1-1-2025   x1       x2
1-2-2025   x3       x4
</code></pre>
<p>Excel file #2:</p>
<pre><code>Date      Data1   Data2
1-3-2025   y1       y2
1-4-2025   y3       y4
</code></pre>
<p>I want to merge them into a single file:</p>
<pre><code>Date      Data1   Data2
1-1-2025   x1       x2
1-2-2025   x3       x4
1-3-2025   y1       y2
1-4-2025   y3       y4
</code></pre>
<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>for file_path in file_paths:
    # Get the date from the file name
    file_name = file_path
    file_name = os.path.splitext(file_name)[0]
    file_name = file_name[-8:]
    file_name = file_name.replace('_','/')
    # Read the Excel file
    df = pd.read_excel(file_path, sheet_name='Parte', usecols='O:AN', skiprows=10, nrows=110, header=[0])
    data_frames.append(df)
    combined_df = pd.concat(data_frames, ignore_index=True)
    combined_df.to_excel('rute1', index=False)
</code></pre>
<p>The result is not correct:</p>
<pre><code>Date      Data1   Data2
1-1-2025   x1       x2
1-2-2025   x3       x4
1-3-2025                          y1       y2
1-4-2025                          y3       y4
</code></pre>
","-3","Question"
"79395343","","<p>I'm aware of the several posts that cover this topic, I apologise in advance. I've been reading and trying several times.</p>
<p>Here are three example json files that I save into fildir:</p>
<pre><code>https://data.sec.gov/api/xbrl/companyfacts/CIK0000320193.json
https://data.sec.gov/api/xbrl/companyfacts/CIK0001722010.json
https://data.sec.gov/api/xbrl/companyfacts/CIK0001722606.json
</code></pre>
<p>primary keys are cik, entityName, facts.
within facts: &quot;dei&quot;, &quot;us-gaap&quot;.
within either of the two facts, you have a &quot;Concept&quot;, in quotes, that isn't assigned a key...sigh.</p>
<p>Here is a sample of one of these json files:</p>
<pre><code>{
  &quot;cik&quot;: 320193,
  &quot;entityName&quot;: &quot;Apple Inc.&quot;,
  &quot;facts&quot;: {
    &quot;dei&quot;: {
      &quot;EntityCommonStockSharesOutstanding&quot;: {
        &quot;label&quot;: &quot;Entity Common Stock, Shares Outstanding&quot;,
        &quot;description&quot;: &quot;Indicate number of shares or other units outstanding of each of registrant'.......etc....&quot;,
        &quot;units&quot;: {
          &quot;shares&quot;: [
            {
              &quot;end&quot;: &quot;2009-06-27&quot;,
              &quot;val&quot;: 895816758,
              &quot;accn&quot;: &quot;0001193125-09-153165&quot;,
              &quot;fy&quot;: 2009,
              &quot;fp&quot;: &quot;Q3&quot;,
              &quot;form&quot;: &quot;10-Q&quot;,
              &quot;filed&quot;: &quot;2009-07-22&quot;,
              &quot;frame&quot;: &quot;CY2009Q2I&quot;
            },
            }
          ]
        }
      }
    },
    &quot;us-gaap&quot;: {
      &quot;AccountsPayable&quot;: {
        &quot;label&quot;: &quot;Accounts Payable (Deprecated 2009-01-31)&quot;,
        &quot;description&quot;: &quot;Carrying value as of the balance sheet date of liabilities incurred (and for which invoices have typically been received) and payable to vendors for goods and services received that are used in an entity's business. For classified balance sheets, used to reflect the current portion of the liabilities (due within one year or within the normal operating cycle if longer); for unclassified balance sheets, used to reflect the total liabilities (regardless of due date).&quot;,
        &quot;units&quot;: {
          &quot;USD&quot;: [
            {
              &quot;end&quot;: &quot;2008-09-27&quot;,
              &quot;val&quot;: 5520000000,
              &quot;accn&quot;: &quot;0001193125-09-153165&quot;,
              &quot;fy&quot;: 2009,
              &quot;fp&quot;: &quot;Q3&quot;,
              &quot;form&quot;: &quot;10-Q&quot;,
              &quot;filed&quot;: &quot;2009-07-22&quot;,
              &quot;frame&quot;: &quot;CY2008Q3I&quot;
            },

</code></pre>
<p>here is one of the several attempts i've tried:</p>
<pre><code>import pandas as pd
import os
import glob
import json

OUTPUT_PATH = &quot;../output/concepts/csv/&quot;
fildir = '../resources/companyfacts/'
fils = os.path.join(fildir, '*.json')
filist = glob.glob(fils)

for fils in filist:
    i = open(fils, &quot;r&quot;)
    sec_data = json.loads(i.read())
    try:
        for item in sec_data[&quot;facts&quot;][&quot;dei&quot;]:
            i.close()
    except:
        continue
    try:
        for item in sec_data[&quot;facts&quot;][&quot;us-gaap&quot;]:
            i.close()
    except:
        continue
    dfs = [pd.read_json(fils) for sec_data in filist]
    data = {&quot;concepts&quot;: {}}
    for item in sec_data[&quot;facts&quot;][&quot;dei&quot;]:
        if f&quot;{item}&quot; not in data:
            data[f&quot;{item}&quot;] = {}
        data[f&quot;{item}&quot;] = item
    for item in sec_data[&quot;facts&quot;][&quot;us-gaap&quot;]:
       if f&quot;{item}&quot; not in data:
            data[f&quot;{item}&quot;] = {}
       data[f&quot;{item}&quot;] = item
    df = pd.concat(dfs, ignore_index=True)
    df = pd.DataFrame(data).transpose()
    df.to_csv(OUTPUT_PATH + &quot;CONCEPTS.csv&quot;)
</code></pre>
<p>output results:</p>
<pre><code>concepts
EntityCommonStockSharesOutstanding
EntityPublicFloat
AccruedLiabilitiesCurrent
AdditionalPaidInCapital
CommonStockParOrStatedValuePerShare
CommonStockSharesAuthorized
......
......
......etc
</code></pre>
<p>what I want:
first, I want the script to skip and process the next file in the list, for any file that doesn't match the JSON keys it searches for. (any issues that come up processing the file).
Some files are empty, and some files don't have &quot;us-gaap&quot; or &quot;dei&quot; objects in the facts key. There are a few thousand files. One error that comes up:</p>
<pre><code>    for item in sec_data[&quot;facts&quot;][&quot;us-gaap&quot;]:
                ~~~~~~~~~~~~~~~~~^^^^^^^^^^^
KeyError: 'us-gaap'

</code></pre>
<p>Then, I want my csv file to look like this:</p>
<pre><code>CIK0000320193-concepts:    CIK0001722010-concepts:   CIK0001722606-concepts:
concpet list #1 here       concept list #2 here      concept list #3 here        
</code></pre>
<p>Please forgive me, I'm not the best with code, but Im good with patterns, details, and solving problems. The closest I've gotten with checking guides and other stack questions, is an output that processes only one of the several files in my folder.</p>
<p>Edit: I got closer like this, but this way, I its only one big list. one column list of everything. I'm not sure if its all there. I can keep trying until I get columns that specify each file. Thank you.</p>
<pre><code>import pandas as pd
import os
import json
import glob

OUTPUT_PATH = &quot;../output/concepts/csv/&quot;
fildir = '../resources/companyfacts/'
fils = os.path.join(fildir, '*.json')
filist = glob.glob(fils)

data = []
for fils in filist:
    i = open(fils, &quot;r&quot;)
    sec_data = json.loads(i.read())
    cik = str(sec_data['cik'])
    padded_cik = cik.zfill(10)
    cickStr = f'CIK{padded_cik}-concepts'

    entList = []
    for k,v in sec_data['facts'].items():
        dei = list(sec_data['facts']['dei'])
        gaap = list(sec_data['facts']['us-gaap'])
        entList += list(dei)
        entList += list(gaap)
        data += list(entList)
        sec_data.update({cickStr:list(set(entList))})

#    dfs = pd.DataFrame(entList)
    dfs = pd.DataFrame(data)
    #df = pd.concat(dfs, ignore_index=True)
#    df.to_csv(OUTPUT_PATH + &quot;CONCEPTS.csv&quot;)
#    for k,v in sec_data['facts'].items():
#        entList += list(v.keys())
#df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in sec_data.items()])) 
dfs.to_csv(OUTPUT_PATH + &quot;CONCEPTS.csv&quot;)
</code></pre>
","2","Question"
"79396624","","<p>I am fairly new to Python and I am trying to build an app where I take a master file that contains product information and compares it to an older data file with the same data.</p>
<p>The aim of the app is to highlight any new products using a 'SKU' column and to then generate a new file that contains all the same columns from the master file but only the 'new' product rows listed.</p>
<p>Filename descriptions:</p>
<p>&quot;downloaded_data.csv&quot; - this is the master file which is downloaded from a URL.</p>
<p>&quot;current-data.csv&quot; - this is old data file to compare against to find new products.</p>
<p>&quot;new-products.csv&quot; - this will be the file generated to include only the new products that appear in the master file after comparing to the current data file (currnet-data.csv). This new file should also include all the columns that appear in the master file (downloaded_data.csv).</p>
<p>&quot;comparison-file.csv&quot; - this is a file that simply outputs the list of SKUs that are considered new.</p>
<p>Example columns from &quot;downloaded_data.csv&quot; file which should also appear in the new file.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Title</th>
<th>Image URL</th>
<th>SKU</th>
</tr>
</thead>
<tbody>
<tr>
<td>Product Name One</td>
<td>imageurl.jpg</td>
<td>SKU123</td>
</tr>
<tr>
<td>Product Name Two</td>
<td>imageurl.jpg</td>
<td>SKU456</td>
</tr>
</tbody>
</table></div>
<p>Example columns from &quot;current-data.csv&quot; file. Compare the above example CSV file with this using the &quot;SKU&quot; and &quot;ID&quot; columns:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Image URL</th>
<th>Product Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>SKU123</td>
<td>imageurl.jpg</td>
<td>Product Name One</td>
</tr>
<tr>
<td>SKU789</td>
<td>imageurl.jpg</td>
<td>Product Name Three</td>
</tr>
</tbody>
</table></div>
<p>Example of the &quot;new products&quot; file (same columns from &quot;downloaded_data.csv&quot; but only inputting the SKUs that are missing from the lookup file &quot;current-data.csv&quot;):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Title</th>
<th>Image URL</th>
<th>SKU</th>
</tr>
</thead>
<tbody>
<tr>
<td>Product Name Three</td>
<td>imageurl.jpg</td>
<td>SKU789</td>
</tr>
</tbody>
</table></div>
<p>Below is my Python code. After running this, the &quot;new-products.csv&quot; file ends up just being a copy of the downloaded master file.</p>
<pre><code>import pandas as pd
import requests

pd.set_option(&quot;display.max_rows&quot;, None)

# Dowload CSV file
url = &quot;URL GOES HERE&quot;
response = requests.get(url)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Save the content of the response to a local CSV file
    with open(&quot;downloaded_data.csv&quot;, &quot;wb&quot;) as f:
        f.write(response.content)
    print(&quot;CSV file downloaded successfully&quot;)
else:
    print(&quot;Failed to download csv file. Status code: &quot;, response.status_code)

# Read the CSV file into a Pandas DataFrame
master_file_compare = pd.read_csv(&quot;downloaded_data.csv&quot;, usecols=[29], names=['SKU'])
account_data = pd.read_csv(&quot;account-data.csv&quot;, usecols=[5], names=['SKU'])

# Merging both dataframe using left join
comparison_result = pd.merge(master_file_compare,account_data, on='SKU', how='left', indicator=True)

# Filtering only the rows that are available in left (master_file_compare)
comparison_result = comparison_result.loc[comparison_result['_merge'] == 'left_only']

comparison_result.to_csv('comparison-file.csv', encoding='utf8')

# Compare Comparison file to master and generate data file
with open('downloaded_data.csv', 'r', encoding='utf8') as in_file, open('new-products.csv', 'w', encoding='utf8') as out_file:
    for line in in_file:
        if line.split(',')[0].strip() not in comparison_result:
            out_file.write(line)

# print(account_data.head)
print(comparison_result)
</code></pre>
<p>To note that the current data (comparison file) will not have the same columns or even ordering of columns. All I am aiming to do is to match on the SKU columns as those are the identifiers.</p>
<p>Any pointers or suggestions on where to focus next would be greatly appreciated.</p>
<p>Thanks!</p>
","0","Question"
"79396950","","<p>I have a Pandas dataframe in wide format that looks like this:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'Class_ID': {0: 432, 1: 493, 2: 32},
                   'f_proba_1': {0: 3, 1: 8, 2: 6},
                   'f_proba_2': {0: 4, 1: 9, 2: 9},
                   'f_proba_3': {0: 2, 1: 4, 2: 1},
                   'p_proba_1': {0: 3, 1: 82, 2: 36},
                   'p_proba_2': {0: 2, 1: 92, 2: 96},
                   'p_proba_3': {0: 8, 1: 41, 2: 18},
                   'Meeting_ID': {0: 27, 1: 23, 2: 21}})

df

    Class_ID    f_proba_1   f_proba_2   f_proba_3   p_proba_1   p_proba_2   p_proba_3   Meeting_ID
0   432         3           4           2           3           2           8           27
1   493         8           9           4           82          92          41          23
2   32          6           9           1           36          96          18          21
</code></pre>
<p>and I would like to convert to long format:</p>
<pre><code>    Class_ID    Student_ID  f_proba p_proba Meeting_ID
0   432         1           3       3       27
1   432         2           4       2       27
2   432         3           2       8       27
3   493         1           8       82      23
4   493         2           9       92      23
5   493         3           4       41      23
6   32          1           6       36      21
7   32          2           9       96      21
8   32          3           1       18      21
</code></pre>
<p>So I have tried <code>.melt</code> in Pandas and here is my code</p>
<pre><code>out = pd.melt(df,
              id_vars = ['Class_ID', 'Meeting_ID'],
              value_vars = ['f_proba_1','f_proba_2','f_proba_3','p_proba_1','p_proba_2','p_proba_3'],
              var_name = 'Student_ID',
              value_name = ['f_proba', 'p_proba'])
out
</code></pre>
<p>but it didn't work.</p>
","2","Question"
"79397388","","<p>I have a dataframe containing daily data</p>
<pre><code>import pandas as pd
import numpy as np

# Set the random seed for reproducibility
np.random.seed(42)

# Generate random data
dates = pd.date_range(start='2020-06-25', periods=1679, freq='D')
open_prices = np.random.uniform(low=100, high=200, size=len(dates))
high_prices = open_prices + np.random.uniform(low=0, high=10, size=len(dates))
low_prices = open_prices - np.random.uniform(low=0, high=10, size=len(dates))
close_prices = np.random.uniform(low=low_prices, high=high_prices)

# Create the DataFrame
ohlc_data = pd.DataFrame({
    'Open': open_prices,
    'High': high_prices,
    'Low': low_prices,
    'Close': close_prices
}, index=dates)
</code></pre>

<pre><code>&gt;&gt;&gt; ohlc_data
            Open        High        Low         Close
Date        
2020-06-25  137.454012  144.403523  129.235702  143.945741
2020-06-26  195.071431  198.532428  188.476323  195.793458
2020-06-27  173.199394  182.955496  165.236980  181.584150
2020-06-28  159.865848  166.275569  157.146388  164.104760
2020-06-29  115.601864  123.826670  108.678275  117.641837
... ... ... ... ...
2025-01-24  179.003044  184.073640  173.358878  175.845545
2025-01-25  130.467914  132.347118  124.462082  130.051784
2025-01-26  108.091928  108.861645  106.429375  108.432944
2025-01-27  140.298018  147.259579  136.500644  145.721364
2025-01-28  117.352451  121.180439  111.180509  115.331552
</code></pre>
<p>I need to resample data to 3 days starting from the first day for each year in DataFrame</p>
<pre><code>agg = {'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'}
resampled = ohlc_data.resample('3D').agg(agg)
</code></pre>

<pre><code>&gt;&gt;&gt; resampled
            Open        High        Low         Close
Date
2020-06-25  137.454012  198.532428  129.235702  181.584150
2020-06-28  159.865848  166.275569  108.678275  113.720371
2020-07-01  105.808361  195.845186  96.417676   161.755865
2020-07-04  170.807258  198.739371  100.149184  192.952485
2020-07-07  183.244264  188.269925  115.280878  118.845486
... ... ... ... ...
2025-01-15  185.142496  195.768948  111.549399  191.451661
2025-01-18  111.636640  184.883239  109.047597  136.075629
2025-01-21  187.797432  191.054177  175.267831  176.687211
2025-01-24  179.003044  184.073640  106.429375  108.432944
2025-01-27  140.298018  147.259579  111.180509  115.331552
</code></pre>
<p><strong>first year:</strong></p>
<pre><code>&gt;&gt;&gt; resampled.loc['2020-01-01': '2020-06-26']
            Open        High        Low         Close
Date
2020-06-25  137.454012  198.532428  129.235702  181.584150
2020-06-28  159.865848  166.275569  108.678275  113.720371
</code></pre>
<p>This is okay for now because I don't have data before 2020-06-25</p>
<p><strong>second year:</strong></p>
<pre><code>&gt;&gt;&gt; resampled.loc['2021-01-01': '2021-01-06']
            Open        High        Low         Close
Date
2021-01-03  190.041806  192.176919  128.095988  134.056420
2021-01-06  134.920957  195.614503  129.865870  189.599085
</code></pre>
<p>resampling here start from 2021-01-03 , I need it to start from 2021-01-01</p>
<p><strong>third year:</strong></p>
<pre><code>&gt;&gt;&gt; resampled.loc['2022-01-01': '2022-01-06']
            Open        High        Low         Close
Date
2022-01-01  140.348287  147.026333  98.926715   103.562334
2022-01-04  175.513726  177.367027  158.572894  169.912020
</code></pre>
<p>resampling in this year work as what I need, starting from 2022-01-01</p>
<p><strong>I tried using origin parameter:</strong></p>
<pre><code>agg = {'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'}
ori = str(ohlc_data.index[0].date().replace(month=1, day=1))
resampled = ohlc_data.resample('3D', origin=ori).agg(agg)
</code></pre>
<p>but this works only with first year in DataFrame</p>
","0","Question"
"79397978","","<p>I am trying to plot geochemical values and my data is in a dataframe with columsn such as <code>&quot;Pt&quot;</code>, <code>&quot;Pd&quot;</code>, <code>&quot;Cu&quot;</code> etc,
I am trying to plot the <code>Cu/Pd</code> ratio of a <code>df</code> against the <code>Pd</code> values, which I've done and is fine. However then I want to make smaller subplots of the data, refining them by a third variable, <code>&quot;Mg#&quot;</code> which is a column in the same df.</p>
<p>How would I only plot values with <code>Mg# &gt;70</code> or Mg# 60-70 for example.
I understand there must be and IF or ELIF statement but I'm unsure how to order it.</p>
<p>This is my current code for the plot:</p>
<pre><code>plt.scatter(East_Greenland['Pd'], 
            East_Greenland['Cu']/East_Greenland[&quot;Pd&quot;], 
            color = &quot;purple&quot;, marker = &quot;s&quot;, s=25, 
            edgecolor = &quot;black&quot;, label = &quot;East Greenland (onshore)&quot;)
</code></pre>
<p>I am just unsure how to filter by the Mg# without creating a new column or df.
Thanks!</p>
<p>I've tried looking at other elif and if problems but they are either for discrete values or I cannot follow where to put in my statements</p>
","3","Question"
"79398404","","<p>I'm running into a situation I don't know what to do:</p>
<p>The data is a list, no index. Sample data:</p>
<pre><code>data = [
 {'fields': ['2024-10-07T21:22:01', 'USER-A', 21,  0,  0, 21]},
 {'fields': ['2024-10-07T21:18:28', 'USER-B', 20, 20,  0,  0, 0, 45]}
]
</code></pre>
<p>The column header is in another:</p>
<pre><code>cols = ['Created On', 'Created By', 'Transaction Count (ALL)',
        'X Pending', 'X Cancelled (X)', 'X Completed (Y)']
</code></pre>
<p>I have tried using <code>pandas.DataFrame</code> as well as <code>json_normalize</code>, I either get a single column table with each value as a row, or I got all values as a column, and when I try with using &quot;fields&quot;, it tells me &quot;list indices must be integers or slices, not str&quot; which I don't understand why I get this... what is the best way to have these info into a dataframe please?</p>
<p>(the number of data elements and number of column headers may not be consistent just for example sake, the real data has things aligned)</p>
","1","Question"
"79399697","","<p>I want to multiple data extract from cmip6 model data and save as netcdf file by using the following scripts:</p>
<pre><code>import pandas as pd
import xarray as xr 

from netCDF4 import Dataset 

nc_file = (r&quot;C:\Users\DELL 3090\Desktop\Projection\RF_58\pr_Amon_AWI-CM-1-1-MR_ssp585_mm_month.nc&quot;) 
NC = xr.open_dataset(nc_file)

#define locations and station id
lat=[13.3,7.5,8.5,7.9,9,7.8,11.3,14.3,14.2,7]

lon=[39.8,34.3,39.8,38.7,38.8,39.9,37.5,39.5,38.9,39.9]

name=['Abala','Abobo','Abomsa','Adamitulu','AddisAbabaBole','Adele','Adet','Adigrat','Adwa','Agarfa']

Newdf = pd.DataFrame([])

for i,j,id in zip(lat,lon,name): 
    dsloc = NC.sel(lat=i,lon=j,method='nearest') 
    DT=dsloc.to_dataframe()
    # insert the name with your preferred column title:
    Newdf=Newdf._append(DT,sort=True)
    Newdf.to_netcdf('C:/Users/DELL 3090/Desktop/Projection\RF_58/pr_Amon_AWI-CM-1-1 MR_ssp585_mm_month111.nc', index=True, header=True)
</code></pre>
<p>But finally python responds this error:</p>
<blockquote>
<p>File ~\anaconda3\envs\env_resaerch\Lib\site-packages\pandas\core\generic.py:6204 in __getattr__
return object.__getattribute__(self, name)<br />
AttributeError: 'DataFrame' object has no attribute 'to_netcdf'</p>
</blockquote>
<p>so please help me what can I do?</p>
","-1","Question"
"79399929","","<p>I am trying to write Pandas code that would allow me to sample DataFrame using a normal distribution. The most convinient way is to use random_state parameter of the sample method to draw random samples, but somehow employ numpy.random.Generator.normal to draw random samples using a normal (Gaussian) distribution.</p>
<pre><code>import pandas as pd
import numpy as np
import random

# Generate a list of unique random numbers
temp = random.sample(range(1, 101), 100)
df = pd.DataFrame({'temperature': temp})

# Sample normal
rng = np.random.default_rng()
triangle_df.sample(n=10, random_state=rng.normal())
</code></pre>
<p><strong>This obviously doesn't work. There is an issue with <em>random_state=rng.normal().</em></strong></p>
","2","Question"
"79400579","","<p>I want to assign a random float (from 0 to 1) to a column that contains unique value within a Pandas dataframe.</p>
<p>Below is a dataframe with unique value of &quot;region&quot;; I want to create a new column with a unique randomly generated float (between 0 to 1) corresponds to each region.</p>
<p>I used random function to generate a random number, but I couldn't figure out how to assign these random numbers to each region and make it a new column.</p>
<p>The goal also includes making sure the random number assigned to each region doesn't change in case of a re-run, so I set a seed.</p>
<pre><code>import pandas as pd
import numpy as np
import random

list_reg = ['region1', 'region2', 'region3', 'region4', 'region5', 'region6']

df_test = pd.DataFrame({
    'region': list_reg,
    'product1': [100, 250, 350, 555, 999999, 200000],
    'product2': [41, 111, 12.14, 16.18, np.nan, 200003],
    'product3': [7.04, 2.09, 11.14, 2000320, 22.17, np.nan],
    'product4': [236, 249, 400, 0.56, 359, 122],
    'product5': [None, 1.33, 2.54, 1, 0.9, 3.2]})

# in case of a re-run, make sure the randomly generated number doesn't change
random.seed(123)
random_genator = random.uniform(0.0001, 1.0000)
</code></pre>
<p>The desired goal would be something like below</p>
<p><a href=""https://i.sstatic.net/jQH2EmFd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jQH2EmFd.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79400992","","<p>So I'm an economics undergrad trying to learn some pandas for data manipulation.</p>
<pre><code># Making YEAR as the index
inflation_forecasts.index = pd.DatetimeIndex(inflation_forecasts['YEAR'].astype(int).astype(str)+'-Q'+inflation_forecasts['QUARTER'].astype(int).astype(str),freq = 'QS')
</code></pre>
<p>This is from my professor's notebook. The code downloads macroeconomic data from a public database as an excel file and now we're converting it into a df.
I'm good up to this point but I don't get why .astype(str) follows .astype(int)</p>
<p>I tried looking up pandas manipulation but all it did was reaffirm what the astype method does which is converts the dtype into the specified type. does this line of code
1.converts values in YEAR and QUARTER into an integer THEN a string?
- Why would you do this ??-
2.concatenates YEAR and QUARTER into one 'column'</p>
","1","Question"
"79401345","","<p>Summary: I want to be able to create list logic that can read any dataframe (varying column headers, data types etc) and assign it into the following structure list</p>
<pre><code>[{'Name': column header 1 name, 'Type': column header 1 type, 'Name': column header 2 name, 'Type': column header 2 name}]
</code></pre>
<p>I have a sample table df_stack_exchange</p>
<pre><code>data_stack_exchange = {'store': ['A','B', 'B', 'C', 'C', 'C', 'D', 'D', 'D', 'D'],
        'worker': [1,1,2,1,2,3,1,2,3,4],
        'boxes': [105, 90, 100, 80, 10, 200, 70, 210, 50, 0]}
df_stack_exchange = pandas.DataFrame(data_stack_exchange)
</code></pre>
<p>And want to create the following. I know it is not a functional dictionary with the duplicated keys but its for another defined purpose that requires this rigid structure</p>
<pre><code>[{'Name': 'store', 'Type': 'object', 'Name': 'worker', 'Type': 'int64', 'Name': 'boxes', 'Type': 'int64'}]
</code></pre>
<p><strong>Edit 1:</strong>
The end state is I want to automate filling out the InputColumns on the create_data_set boto3 command without having to type out each column of my s3 file: <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/quicksight/client/create_data_set.html"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/quicksight/client/create_data_set.html</a></p>
<pre><code>'InputColumns': [
{
'Name': 'string',
'Type': 'STRING'|'INTEGER'|'DECIMAL'|'DATETIME'|'BIT'|'BOOLEAN'|'JSON',
'SubType': 'FLOAT'|'FIXED'
},
]
</code></pre>
","0","Question"
"79402584","","<p>i have a pandas dataframe with multiple columns (on which I am computing the cumulative value).</p>
<p>I would like to get the incremental value for the same now.</p>
<p>This is my current dataset:</p>
<pre><code>Gender  Cubic_Cap   Branch  UWYear  yhat_all
  M        1000        A     2015      19
  M        1000        A     2015      20
  M        1000        A     2015      26
  M        1000        A     2015      30
  F        1500        B     2016      1
  F        1500        B     2016      25
  F        1500        B     2016      36
  F        1500        B     2016      49
</code></pre>
<p>My desired result is:</p>
<pre><code>yhat_incremental
0
1
6
4
1
24
11
13
</code></pre>
<p>I've tried the following methods (but to no avail):</p>
<pre><code>all_c1['incremental_yhat'] = all_c1.groupby(rating_factors)['yhat_all'].diff().fillna(all_c1['yhat_all'])
</code></pre>
<p>I've also tried this:</p>
<pre><code>all_c1['incremental_yhat'] = all_c1['yhat_all'].shift().where(all_c1[rating_factors].eq(all_c1[rating_factors].shift()))
</code></pre>
<p>Is there any other methods I can use to obtain this?</p>
","-2","Question"
"79402987","","<p>I can read files from Azure Storage into Pandas like this</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from azure.identity import AzureCliCredential
    
credential = AzureCliCredential()
pd.read_csv(
    &quot;abfs://my_container/my_file.csv&quot;,
    storage_options={'account_name': 'my_account', 'credential': credential}
)
</code></pre>
<p>Getting the token from AzureCliCredential is slow. Is there a way to make pandas/fsspec cache the token so that the slow token retrieval process is not repeated over and over again when I open many files?</p>
","2","Question"
"79403118","","<p>I am trying to create a rolling rank column for a float column. However the output is not as expected. Below I have an example:</p>
<pre><code>data = {
    'time': pd.date_range(start='2025-01-01', periods=5, freq='H'),
    'zone': ['A'] * 5,
    'price': [1.0, 1.5, 1.7, 1.9, 2.0],
}
dummy_df = pd.DataFrame(data)

class RollingRank:
    def __init__(self, rank_col: str, group_by_col: str, window_length: int, time_col: str):
        self.rank_col = rank_col
        self.time_col = time_col
        self.group_by_col = group_by_col
        self.window_length = window_length

    def fit_transform(self, df):
        df = df.sort_values(by=[self.group_by_col, self.time_col])
        df.set_index(self.time_col, inplace=True)
        dfg = (
            df.groupby(self.group_by_col)
            .rolling(window=f&quot;{self.window_length}H&quot;, center=True, closed=&quot;both&quot;)
            .rank(method=&quot;min&quot;)
        )
        df.loc[:, f&quot;{self.rank_col}_rank&quot;] = dfg[self.rank_col].values
        return df

df_rank = RollingRank(rank_col=&quot;price&quot;, group_by_col=&quot;zone&quot;, window_length=3, time_col=&quot;time&quot;).fit_transform(dummy_df)
</code></pre>
<p>As an output I get the rank <code>[2.0, 3.0, 3.0, 3.0, 3.0]</code> which does not make sense with <code>center=True</code> and <code>closed=&quot;both&quot;</code>.</p>
<p>As an easy example for the middel row with the timestamp <code>2025-01-01 02:00:00</code> I would expect the rank to be 2 (and not 3 as in the output) as the values used for ranking would be <code>[1.5, 1.7, 1.9]</code>.</p>
<p>Any ideas of what I am doing wrong?</p>
","0","Question"
"79404365","","<p>Suppose we have two DataFrames to be merged:</p>
<pre><code>import pandas as pd

df1 = pd.DataFrame()
df1[&quot;key&quot;] = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]
df1[&quot;low&quot;] = [0, 1, 2]
df1[&quot;high&quot;] = [2, 4, 6]

df2 = pd.DataFrame()
df2[&quot;key&quot;] = [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;]
df2[&quot;value&quot;] = [1, 2, 3, 2, 5, 0]

print(f&quot;df1:\n{df1}&quot;)
print(f&quot;df2:\n{df2}&quot;)
</code></pre>
<p>Output:</p>
<pre><code>df1:
  key  low  high
0   a    0     2
1   b    1     4
2   c    2     6
df2:
  key  value
0   a      1
1   a      2
2   a      3
3   b      2
4   b      5
5   c      0
</code></pre>
<p>If I want to merge these DataFrames such that I only keep the rows where &quot;key&quot; matches and &quot;value&quot; is between &quot;low&quot; and &quot;high&quot;, I know how to do that in two steps:</p>
<pre><code>df = df1.merge(df2, how=&quot;inner&quot;, on=&quot;key&quot;)
df = df.loc[(df[&quot;value&quot;] &gt;= df[&quot;low&quot;]) &amp; (df[&quot;value&quot;] &lt;= df[&quot;high&quot;])]
print(f&quot;df:\n{df}&quot;)
</code></pre>
<p>Output:</p>
<pre><code>df:
  key  low  high  value
0   a    0     2      1
1   a    0     2      2
3   b    1     4      2
</code></pre>
<p>But that approach might not be practical if the DataFrames are large, because merging the DataFrames in the first step could take a long time (see below for additional details).</p>
<p>In SQL we can easily do the join with a query like this:</p>
<pre><code>SELECT
    df1.key,
    df1.low,
    df1.high,
    df2.value
FROM df1
INNER JOIN df2
ON
    df1.key = df2.key AND
    df2.value BETWEEN df1.low AND df1.high
</code></pre>
<p><strong>Is there a way to execute this merge in Python in one step?</strong></p>
<p>Edit 1: I'm looking for a solution without installing any additional dependencies, such as pyjanitor or other modules besides pandas.</p>
<p>Edit 2: By &quot;one step&quot; I mean that I want to execute the merge with a single pandas operation, not two operations (like .merge and .loc) combined on a single line.</p>
<hr />
<p><strong>Why not just chain together Pandas operations?</strong></p>
<p>Some people posted comments and solutions similar to this:</p>
<pre><code>df = df1.merge(df2, how=&quot;inner&quot;, on=&quot;key&quot;).loc[lambda r: (r[&quot;value&quot;] &gt;= r[&quot;low&quot;]) &amp; (r[&quot;value&quot;] &lt;= r[&quot;high&quot;])]
</code></pre>
<p>This solution works but can be impractical because it merges the DataFrames before performing the subsetting operation. If the DataFrames are large, <code>df1.merge(df2, how=&quot;inner&quot;, on=&quot;key&quot;)</code> can be very costly to compute.</p>
<p>As an example, suppose we use the same DataFrames <code>df1</code> and <code>df2</code> with additional rows:</p>
<pre><code>import pandas as pd
add_rows = 10
random.seed(42)

df1 = pd.DataFrame()
df1[&quot;key&quot;] = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] + random.choices([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], k=add_rows)
df1[&quot;low&quot;] = [0, 1, 2] + list(range(100, 100 + add_rows))
df1[&quot;high&quot;] = df1[&quot;low&quot;] + 2

df2 = pd.DataFrame()
df2[&quot;key&quot;] = [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;] + random.choices([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], k=add_rows)
df2[&quot;value&quot;] = [1, 2, 3, 2, 5, 0] + list(range(-add_rows, 0))

print(f&quot;df1:\n{df1}&quot;)
print(f&quot;df2:\n{df2}&quot;)
</code></pre>
<p>Output:</p>
<pre><code>df1:
   key  low  high
0    a    0     2
1    b    1     3
2    c    2     4
3    b  100   102
4    a  101   103
5    a  102   104
6    a  103   105
7    c  104   106
8    c  105   107
9    c  106   108
10   a  107   109
11   b  108   110
12   a  109   111
df2:
   key  value
0    a      1
1    a      2
2    a      3
3    b      2
4    b      5
5    c      0
6    a    -10
7    b     -9
8    a     -8
9    a     -7
10   b     -6
11   b     -5
12   a     -4
13   b     -3
14   c     -2
15   a     -1
</code></pre>
<p>Notice that the values in <code>df2[&quot;value&quot;]</code> are all negative while the values in <code>df1[&quot;low&quot;]</code> and <code>df1[&quot;high&quot;]</code> are all positive, so the final result will always contain the same 3 rows no matter how many rows are added.</p>
<p>With 10 added rows, <code>df1.merge(df2, how=&quot;inner&quot;, on=&quot;key&quot;)</code> contains 74 rows and <code>df = df1.merge(df2, how=&quot;inner&quot;, on=&quot;key&quot;).loc[lambda r: (r[&quot;value&quot;] &gt;= r[&quot;low&quot;]) &amp; (r[&quot;value&quot;] &lt;= r[&quot;high&quot;])]</code> takes 0.005 seconds to compute on my machine.</p>
<p>With 1000 added rows, the merge contains 336K rows and the final result (which is unchanged, remember) takes 0.03 seconds.</p>
<p>At 10,000 added rows we have 33.3M rows and 1.03 seconds, and 100,000 added rows crashes the notebook.</p>
<p>What is needed here is a way to perform a SQL-like subsetting <em>during</em> the merge.</p>
","-1","Question"
"79404815","","<p>I am trying to map values of my columns. For gender the following mapping works:</p>
<pre><code>df['gender'] = df['gender'].map({'female': 0, 'male': 1, 'other': 2})
</code></pre>
<p>Similarly I would like to map age column based on the range they belong to, so that, each decade is a separate class. Here is my not working code:</p>
<pre><code>   df['age'] = df['age'].map({
                              '25 &lt; age &lt; 36': 1,
                              '35 &lt; age &lt; 46': 2,
                              '45 &lt; age &lt; 56': 3
                             })
   
</code></pre>
<p>How can I do the mapping correctly? Thanks in advance.</p>
","1","Question"
"79405200","","<p>I switched from NumPy arrays to Pandas DataFrames (dfs) many years ago because the latter has column names, which</p>
<ol>
<li>makes programming easier;</li>
<li>is robust in order changes when reading data from a <code>.json</code> or <code>.csv</code> file.</li>
</ol>
<p>From time to time, I need the last row (<code>[-1]</code>) of some column <code>col</code> of some <code>df1</code>, and combine it with the last row of the same column <code>col</code> of another <code>df2</code>.  I know the <em>name</em> of the column, not their position/order (I could know, but it might change, and I want to have a code that is robust against changers in the order of columns).</p>
<p>So what I have been doing for years in a number of Python scripts is something that looks like</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd

# In reality, these are read from json files - the order
# of the columns may change, their names may not:
df1 = pd.DataFrame(np.random.random((2,3)), columns=['col2','col3','col1'])
df2 = pd.DataFrame(np.random.random((4,3)), columns=['col1','col3','col2'])

df1.col2.iloc[-1] = df2.col2.iloc[-1]
</code></pre>
<p>but since some time my mailbox gets flooded with cron jobs going wrong, telling me that</p>
<blockquote>
<p>You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:</p>
<pre><code>df[&quot;col&quot;][row_indexer] = value
</code></pre>
<p>Use <code>df.loc[row_indexer, &quot;col&quot;] = values</code> instead, to perform the assignment in a single step and ensure this keeps updating the original <code>df</code>.</p>
<p>See the caveats in the documentation: <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy</a></p>
<pre><code>df1.col2.iloc[-1] = df2.col2.iloc[-1]
</code></pre>
</blockquote>
<p>Of course, this error message is incorrect, and replacing the last line in my example with either of</p>
<pre class=""lang-py prettyprint-override""><code>df1.loc[-1, 'col2'] = df2.loc[-1, 'col2']    # KeyError: -1
df1.iloc[-1, 'col2'] = df2.iloc[-1, 'col2']  # ValueError (can't handle 'col2')
</code></pre>
<p>does not work either, since <code>.iloc[]</code> cannot handle column names and <code>.loc[]</code> cannot handle relative numbers.</p>
<p>How can I handle the last (or any other relative number) row and a column with given name of a Pandas DataFrame?</p>
","1","Question"
"79406608","","<p>I have the following code:</p>
<pre class=""lang-py prettyprint-override""><code>data = pandas.read_csv('data.csv')
data['when'] = pandas.to_datetime(data['when'])
data.set_index('when', inplace=True)
print(data)
print(data.index.dtype)
</code></pre>
<p>which prints:</p>
<pre><code>                 price
when                  
2025-01-04  98259.4300
2025-01-03  98126.6400
2025-01-02  96949.1800
2025-01-01  94610.1400
2024-12-31  93647.0100
...                ...
2010-07-21      0.0792
2010-07-20      0.0747
2010-07-19      0.0808
2010-07-18      0.0858
2010-07-17      0.0500

[5286 rows x 1 columns]
datetime64[ns]
</code></pre>
<p>Then, I am trying to select a range like this:</p>
<pre class=""lang-py prettyprint-override""><code>start_date = datetime(year=2010,month=1,day=1)
end_date = datetime(year=2025,month=1,day=1)
print(data.loc[start_date:end_date])
print(data.loc[start_date:])
print(data.loc[:end_date])
</code></pre>
<p>and this prints</p>
<pre><code>Empty DataFrame
Columns: [price]
Index: []
Empty DataFrame
Columns: [price]
Index: []
               price
when                
2025-01-04  98259.43
2025-01-03  98126.64
2025-01-02  96949.18
2025-01-01  94610.14
</code></pre>
<p>Why?</p>
<p>I am using pandas 2.2.3.</p>
","-1","Question"
"79407233","","<p>I have a simple chatbot that generates SQL queries and uses them on a database. I want to store the output in a .csv file and then download that file.</p>
<p>This is usually possible when I take my output from SQL (a list of dicts), create a pd.Dataframe after evaluating that output, and finally download it as a csv file using st.download_button.</p>
<p>However, when the result is a Decimal() format from SQL, pd.Dataframe fails and both eval and literal_eval would not work on it (I got invalid object error using literal_eval).</p>
<p>I also tried to convert the data using Python's Decimal datatype.
<code>from decimal import Decimal</code>
But this recognized the output object as string and not a Decimal.</p>
<p>So I did some research and found that writerows would work with Decimal type data, but I am still not able to download the file.</p>
<pre><code>out = db.run_no_throw(query, include_columns=True) 
#This is the output returned by the database. According to the [docs][1], this returns a string with the result.

print(out, type(out)) 
# Prints [{A: Decimal(1,2)}] and str (literal_eval(out), literal_eval(str(out)) and literal_eval('&quot;' + out + '&quot;') all gave an invalid object error here)
</code></pre>
<p>This is how I am currently trying to download the output data:</p>
<pre><code>with open(filename, mode='w+', newline='') as file_to_output:
                writer = csv.writer(file_to_output, delimiter=&quot;,&quot;)
                writer.writerows(out)
                downloaded = st.download_button(
                    label=&quot;Download data as CSV&quot;,
                    data=file_to_output,
                    file_name=&quot;filename.csv&quot;
                )
</code></pre>
<p>The above code creates a file locally with the expected data, but it prints 1 line per row in the .csv file, like so -</p>
<p>[</p>
<p>{</p>
<p>A</p>
<p>:</p>
<p>D</p>
<p>e</p>
<p>...</p>
<p>However, on the server, the file I download does not even consist of that data (It is a blank file). So the streamlit download button is not getting the data from the file, even though it says <a href=""https://docs.streamlit.io/develop/api-reference/widgets/st.download_button"" rel=""nofollow noreferrer"">here</a> that it should because it is one of str, bytes, or a file.</p>
<p>What am I missing here? I would really appreciate any help. Thanks!</p>
<p>EDIT - Running eval() on out variable gave the error &quot;Could not recognize Decimal() object&quot;</p>
","2","Question"
"79407822","","<p>I have downloaded ERA5 global data which contains the following params:</p>
<ol>
<li>total precipitation</li>
<li>snow fall</li>
<li>cloud cover</li>
<li>temperature at 2m</li>
</ol>
<p>The data range is from Jan 2008 to Dec 2024.I am using xarray package in python to deal with netcdf files but I've been scratching my head this time looking at the data.</p>
<p>The issue is, the data is gridded by lat which means for every lat value there are multiple longitude values (-180 to +180) which means there are multiple dates for each lat, long (repeating dates)</p>
<p>is there any way to extract timeseries for Pakistan region along with latitude and longitude?</p>
<p><a href=""https://i.sstatic.net/82eHk94T.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>This is what the data look like</p>
<p><a href=""https://i.sstatic.net/z1ufChm5.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I am using the .sel() method from xarray but am feeling completely blank and have no clue what to do in this type of scenarios. I don't want to lose data points as I am trying to build a forecasting model.</p>
","-1","Question"
"79407952","","<p>I have a dataframe and a series, as follows:</p>
<pre><code>import pandas as pd
from itertools import permutations

df = pd.DataFrame({'a': [['a', 'b', 'c'], ['a', 'c', 'b'], ['c', 'a', 'b']]})

prob = list(permutations(['a', 'b', 'c']))
prob = [list(ele) for ele in prob]
ps = pd.Series(prob)

&gt;&gt;&gt; df
           a
0  [a, b, c]
1  [a, c, b]
2  [c, a, b]
&gt;&gt;&gt; ps
0    [a, b, c]
1    [a, c, b]
2    [b, a, c]
3    [b, c, a]
4    [c, a, b]
5    [c, b, a]
dtype: object
</code></pre>
<p>My question is how to add a column 'idx' in df, which contains the index of the value in column 'a' in series 'ps'? The desire result is:</p>
<pre><code>a     idx
[a,b,c] 0
[a,c,b] 1
[c,a,b] 4
</code></pre>
<p>The chatgpt gave me a answer, but it works very very slowly when my real data is huge.</p>
<pre><code>df['idx'] = df['a'].apply(lambda x: ps[ps.apply(lambda y: y == x)].index[0])
</code></pre>
<p>Is there a more efficient way?</p>
","1","Question"
"79408524","","<p>Good morning all</p>
<p>I am trying to process a lot of data, and I need to group data, look at the group, then set a value based on the other entries in the group, but I want to set the value in a column in the full dataset. What I can't figure out is how I can use the group to write back to the main dataframe.</p>
<p>So as an example, I created this data frame</p>
<pre><code>import pandas as pd
data = [{
    &quot;class&quot;: &quot;cat&quot;,
    &quot;name&quot;: &quot;Fluffy&quot;,
    &quot;age&quot;: 3,
    &quot;child&quot;: &quot;Whiskers&quot;,
    &quot;parents_in_group&quot;: &quot;&quot;
}, {
    &quot;class&quot;: &quot;dog&quot;,
    &quot;name&quot;: &quot;Spot&quot;,
    &quot;age&quot;: 5
}, {
    &quot;class&quot;: &quot;cat&quot;,
    &quot;name&quot;: &quot;Whiskers&quot;,
    &quot;age&quot;: 7
}, {
    &quot;class&quot;: &quot;dog&quot;,
    &quot;name&quot;: &quot;Rover&quot;,
    &quot;age&quot;: 2,
    &quot;child&quot;: &quot;Spot&quot;
}]
df = pd.DataFrame(data)
df
</code></pre>
<p>So as an example, lets say that I want to set the parrents_in_group to a list of all the parrents in the group, easy to do</p>
<pre><code>for name, group in group_by_class:
  mask = group[&quot;child&quot;].notna()
  print(&quot;This is the parrent in group&quot;)
  print(group[mask])
  parent_name = group[mask][&quot;name&quot;].values[0]
  print(f&quot;This is the parent name: {parent_name}&quot;)
  group[&quot;parents_in_group&quot;] = parent_name
  print(&quot;And now we have the name set in group&quot;)
  print(group)
</code></pre>
<p>That updates the group, but not the actual data frame. So how would I go about writing this information back to the main data frame</p>
<p><strong>Using the name and search</strong></p>
<p>This works, but seems a bit untidy</p>
<pre><code>for name, group in group_by_class:
    mask = group[&quot;child&quot;].notna()
    parent_name = group[mask][&quot;name&quot;].values[0]
    df.loc[df['class'] == name, 'parents_in_group'] = parent_name
    
df
</code></pre>
<p><strong>Using group</strong></p>
<p>How would I go about using group to set the values, rather than searching for the name that the group was created by. Or are there better ways to going about it.</p>
<p>The real challenge I'm having is that I need to get the group, find some specific values in the group, then set some fields based on the data found.</p>
<p>Any help of course welcome.</p>
","1","Question"
"79408631","","<p>I am trying to figure out percent change of values between rows and pandas.pct_change() gives me other values than when I do the math myself.
Can someone explain?
<a href=""https://i.sstatic.net/mLbSU0WD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mLbSU0WD.png"" alt=""enter image description here"" /></a></p>
<p>I can't figure out where I am doing wrong. my &quot;manual&quot; Formula is:
100 - (prev/current *100)</p>
<pre><code>close       manual calculation      Pandas ptc_change
0.7419      #VALUE!                 NaN
0.7406      -0.175533351            -0.175226
0.736       -0.625                  -0.621118
0.733       -0.409276944            -0.407609
0.7409      1.066270752             1.077763
0.7371      -0.515533849            -0.51289
0.7371      0                       0
0.7324      -0.641725833            -0.637634
0.7115      -2.937456079            -2.853632
0.7107      -0.112565077            -0.112439
</code></pre>
","0","Question"
"79409450","","<p>I've been asked to debug some code which pulls data from an API, converts it from JSON to Pandas, Pandas to Spark, and then writes to a table. The following error occurs when it tries to convert from Pandas to Spark:<br />
<code>Exception thrown when converting pandas.Series (float64) with name 'latitude' to Arrow Array (string).</code><br />
This leads to a loss of data in the table to which it writes: the API is known to contain ~30,000 records, but the end table only has 11,000 written to it. I believe this is due to the function <code>process_and_write_page()</code> failing mid-way down a page and moving onto the next one.</p>
<p>Code is included below - unfortunately I've had to anonymise the API details, but hopefully this gives enough information to diagnose the problem.</p>
<pre><code>import requests, json, time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, FloatType

import pandas as pd
from tqdm import tqdm

# Initialize Spark session with Arrow optimization and schema auto-merge enabled
spark = SparkSession.builder.appName(&quot;app&quot;) \
    .config(&quot;spark.sql.execution.arrow.pyspark.enabled&quot;, &quot;true&quot;) \
    .config(&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;, &quot;true&quot;) \
    .getOrCreate()

# API details
base_api_url = &quot;https://an/api/url&quot;
api_key = &quot;api12345&quot;

# Headers for the API request
headers = {
    &quot;subscriptionKey&quot;: api_key,
    &quot;userAgent&quot;: &quot;aUserAgent&quot;
}

# Create a session
session = requests.Session()
session.headers.update(headers)

# Function to fetch a page of IDs
def fetch_ids(page=1, per_page=100, get_total=False):
    try:
        response = session.get(f&quot;{base_api_url}?page={page}&amp;perPage={per_page}&quot;, timeout=30)
        response.raise_for_status()
        data = response.json()
        if get_total:
            return data['totalPages']
        ids = [customer['id'] for customer in data['customers']]
        next_page_uri = data.get('nextPageUri')
        return ids, next_page_uri
    except requests.exceptions.RequestException as e:
        print(f&quot;Failed to fetch data for page {page}: {e}&quot;)
        return [], None

# Function to fetch detailed information for a single customer with retry 
def fetch_details(id, retries=5):
    for attempt in range(retries):
        try:
            response = session.get(f&quot;{base_api_url}/{id}&quot;, timeout=30)
            response.raise_for_status()
            data = response.json()
 
            basic_data = {
                &quot;id&quot;: data.get(&quot;id&quot;),
                &quot;type&quot;: data.get(&quot;type&quot;),
                &quot;name&quot;: data.get(&quot;name&quot;),
                &quot;latitude&quot;: data.get(&quot;latitude&quot;),
                &quot;longitude&quot;: data.get(&quot;longitude&quot;),
                &quot;alsoKnownAs&quot;: data.get(&quot;alsoKnownAs&quot;)
            }
            return (basic_data)

        except requests.exceptions.RequestException as e:
            if hasattr(response, 'status_code') and response.status_code == 429:
                time.sleep(1.33 ** attempt)
            else:
                print(f&quot;Request exception for customer {id}: {e}&quot;)
                break
    return None

# Function to process and write a page of customer data
def process_and_write_page(ids):
    basic_data_list = []
    inspection_areas_list = []
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_id = {executor.submit(fetch_location_details, id): id for id in ids}
        for future in as_completed(future_to_id):
            id = future_to_id[future]
            try:
                basic_data = future.result()
                if basic_data:
                    basic_data_list.append(basic_data)
                        })
                    
            except Exception as e:
                print(f&quot;Exception fetching details for {id}: {e}&quot;)

    basic_schema = StructType([
        StructField(&quot;id&quot;, StringType(), True),
        StructField(&quot;type&quot;, StringType(), True),
        StructField(&quot;name&quot;, StringType(), True),
        StructField(&quot;latitude&quot;, StringType(), True),
        StructField(&quot;longitude&quot;, StringType(), True),
        StructField(&quot;alsoKnownAs&quot;, StringType(), True)
    ])   

    basic_df = pd.DataFrame(basic_data_list)

    if not basic_df.empty:
            # Problem seems to occur here, with createDataFrame()
            basic_spark_df = spark.createDataFrame(basic_df, schema=basic_schema)
            basic_spark_df.write.mode(&quot;append&quot;).option(&quot;mergeSchema&quot;, &quot;true&quot;).format(&quot;delta&quot;).saveAsTable(&quot;api_test&quot;)          

total_pages = fetch_ids(get_total=True)
display(total_pages) 
# 292 pages - at 100 records per page, should return ~29200 reocrds (API contains 29126)
# (Only ~11,000 records in table)

# Fetch and process data page by page
for p in tqdm(range(1, total_pages+1), desc='Fetching pages', unit='page'):
    ids, next_page_uri = fetch_ids(page=p)
    if not ids or not next_page_uri:
        display(&quot;Invalid ids or next_page_uri value&quot;)
        break
    # Directly process and write page data
    process_and_write_page(ids)
</code></pre>
<p>I have tried changing the <code>latitude</code> and <code>longitude</code> elements in <code>basic_schema</code> to <code>FloatType</code>; however this gives a different error:<br />
<code>AnalysisException: [DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'latitude' and 'latitude'</code></p>
","0","Question"
"79410477","","<p>I am just learning Python, and trying to load some records into an excel. Here is the code I am running.</p>
<pre class=""lang-py prettyprint-override""><code>
# Install the openpyxl module
#%pip install openpyxl

# Import pandas
import pandas as pd
import os

df = spark.sql(&quot;&quot;&quot;
select distinct BILLING_ACCOUNT_NUMBER  from       prd_pdt_general_scm.scm_core.subscriber_soc 
where SOC_CODE='P360HT' 
and SUBSCRIBER_NUMBER ='0000000000'
and curr_ind='Y'
&quot;&quot;&quot;)
#display(df)  # this is showing the results correctly.

# Convert the Spark DataFrame to a Pandas DataFrame
pandas_df = df.toPandas()

# Define the directory and file path
directory = r'C:/Users/achatte17/Documents/ISE'
excel_file_path = os.path.join(directory, 'output.xlsx')


display(excel_file_path) #Also showing the path correctly 

# Create the directory if it does not exist
os.makedirs(directory, exist_ok=True)

# Save the DataFrame to an Excel file

writer = pd.ExcelWriter(excel_file_path)    
pandas_df.to_excel(writer, index = False)    

writer.save()      
writer.close()

# Get the absolute path
file_path = os.path.abspath('output.xlsx')

# Print the file location
print(f'The location of the Excel file is: {file_path}') #this is showing a     different path as /home/spark-d377ec0c-caf2-4f42-9dc7-34/output.xlsx
</code></pre>
<p>I tried multiple ways of providing the directory, also tried to write in different ways.
there is no error message, but I am not able to find the excel.</p>
","0","Question"
"79411167","","<p>I have a Pandas dataframe:</p>
<pre><code>import pandas as pd
import numpy as np

np.random.seed(150)
df = pd.DataFrame(np.random.randint(0, 10, size=(10, 2)), columns=['A', 'B'])
</code></pre>
<p>I want to add a new column &quot;C&quot; whose values ​​are the combined-list of every three rows in column &quot;B&quot;. So I use the following method to achieve my needs, but this method is slow when the data is large.</p>
<pre><code>&gt;&gt;&gt; df['C'] = [df['B'].iloc[i-2:i+1].tolist() if i &gt;= 2 else None for i in range(len(df))]
&gt;&gt;&gt; df
   A  B          C
0  4  9       None
1  0  2       None
2  4  5  [9, 2, 5]
3  7  9  [2, 5, 9]
4  8  3  [5, 9, 3]
5  8  1  [9, 3, 1]
6  1  4  [3, 1, 4]
7  4  1  [1, 4, 1]
8  1  9  [4, 1, 9]
9  3  7  [1, 9, 7]
</code></pre>
<p>When I try to use the df.apply function, I get an error message:</p>
<pre><code>df['C'] = df['B'].rolling(window=3).apply(lambda x: list(x), raw=False)

TypeError: must be real number, not list
</code></pre>
<p>I remember that Pandas <code>apply</code> doesn't seem to return a list, so how do I do this? I searched the forum, but couldn't find a suitable topic about apply and return.</p>
","5","Question"
"79412180","","<p>hi i have a python code where i am reading some data from PostgreSQL table(<strong>schema of the tables in a  db</strong>) and  doing some transformations and then writing some data back to PostgreSQL table but in another schema so same database but under different schema. but my code is writing the data or creating table and writing data to same schema.</p>
<p>below is my code. in the below code i am giving the schema name and table name in the variable 'table_name' but its not helping me.</p>
<pre><code>import pandas as pd
import psycopg2
from sqlalchemy import create_engine, JSON
import json

# Function to convert PostgreSQL data types to Snowflake data types
def convert_data_type(pg_data_type):
    conversion_dict = {
        'integer': 'NUMBER',
        'bigint': 'NUMBER',
        'smallint': 'NUMBER',
        'serial': 'NUMBER',
        'bigserial': 'NUMBER',
        'decimal': 'NUMBER',
        'numeric': 'NUMBER',
        'real': 'FLOAT',
        'double precision': 'FLOAT',
        'money': 'FLOAT',
        'character varying': 'VARCHAR',
        'varchar': 'VARCHAR',
        'character': 'CHAR',
        'char': 'CHAR',
        'text': 'STRING',
        'bytea': 'BINARY',
        'timestamp without time zone': 'TIMESTAMP_NTZ',
        'timestamp with time zone': 'TIMESTAMP_TZ',
        'date': 'DATE',
        'time without time zone': 'TIME_NTZ',
        'time with time zone': 'TIME_TZ',
        'boolean': 'BOOLEAN'
    }
    return conversion_dict.get(pg_data_type, pg_data_type)

pg_dbname = &quot;my_db_name&quot;
pg_user = &quot;my_user&quot;
pg_password = &quot;My_pw&quot;
pg_host = &quot;My_host&quot;
pg_port = &quot;5432&quot;


# Connect to PostgreSQL database
conn = psycopg2.connect(
    dbname=pg_dbname,
    user=pg_user,
    password=pg_password,
    host=pg_host,
    port=pg_port
)

# Create a cursor object
cur = conn.cursor()

# Query to get table schemas
cur.execute(&quot;&quot;&quot;
SELECT table_schema, table_name, column_name, data_type 
FROM information_schema.columns 
WHERE table_schema = 'public'
ORDER BY table_name
&quot;&quot;&quot;)

# Fetch all results
results = cur.fetchall()

# Close the cursor and connection
cur.close()
conn.close()

# Process the results and create a DataFrame
data = []
for row in results:
    table_schema, table_name, column_name, data_type = row
    converted_data_type = convert_data_type(data_type)
    data.append([table_schema, table_name, column_name, data_type, converted_data_type])

df = pd.DataFrame(data, columns=['table_schema', 'table_name', 'column_name', 'original_data_type', 'converted_data_type'])

# Grouping data and making dictionary
result = (
    df.groupby(['table_schema', 'table_name'])
    .apply(lambda x: pd.Series({
        'columns': dict(zip(x['column_name'], x['converted_data_type'])),
        'original_columns': dict(zip(x['column_name'], x['original_data_type']))
    }))
    .reset_index()
)

# Create SQLAlchemy engine
engine = create_engine(f'postgresql+psycopg2://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_dbname}')

# Define the table name in the new schema
table_name = 'project_demo.mapping_table'

# Insert the DataFrame into the PostgreSQL table in the new schema
result.to_sql(table_name, engine, if_exists='replace', index=False, dtype={'columns': JSON, 'original_columns': JSON})

print(&quot;Data inserted successfully!&quot;)

</code></pre>
<p>i tried to change the schema name in the variable</p>
","-1","Question"
"79412275","","<p>I want to make a pandas dataframe that describes the state of a system at different times</p>
<ul>
<li>I have the initial state which describes the first row</li>
<li>Each row correspond to a time</li>
<li>I have reveserved the first two columns for &quot;household&quot; / statistics</li>
<li>The following columns are state parameters</li>
<li>At each iteration/row a number of parameters change - this could be just one or many</li>
</ul>
<p>I have created a somewhat simplified version that simulates my change data  : df_change</p>
<p><strong>Question 1</strong></p>
<p>Can you think of a more efficient way of generating the matrix than what i do in this code?
i have a state that i update in  a loop and insert</p>
<p><strong>Question 2</strong></p>
<p>This is what i discovered while trying to write the sample code for this discussion. I see 20 fold performanne boost in loop iteration performance if i do the assignments to the &quot;household&quot; columns  after the loop. Why is this? I am using python = 3.12.4 and pandas 2.2.2.</p>
<pre><code>df[&quot;product&quot;] =&quot;some_product&quot;
</code></pre>
<pre><code>#%%

import numpy as np
import pandas as pd
from tqdm import tqdm
num_cols =600
n_changes = 40000


# simulate changes

extra_colums = [&quot;n_changes&quot;,&quot;product&quot;]
columns = [chr(i+65) for i in range(num_cols)]

state = { icol : np.random.random() for icol in columns}

change_index = np.random.randint(0,4,n_changes).cumsum()
change_col =  [columns[np.random.randint(0,num_cols)] for i in range(n_changes)]
change_val= np.random.normal(size=n_changes)

# create change matrix
df_change=pd.DataFrame(index= change_index )
df_change['col'] = change_col
df_change['val'] = change_val
index = np.unique(change_index)


# %%
# Slow approach  gives 5 iterations/s
df = pd.DataFrame(index= index, columns=extra_colums + columns)
df[&quot;product&quot;] =&quot;some_product&quot;
for i in tqdm(index):
    state.update(zip(df_change.loc[[i],&quot;col&quot;] , df_change.loc[[i],&quot;val&quot;]))
    df.loc[i,columns] = pd.Series(state)

# %%
# Fast approach gives 1000 iterations/sec
df2 = pd.DataFrame(index= index, columns=extra_colums + columns)
for i in tqdm(index):
    state.update(zip(df_change.loc[[i],&quot;col&quot;] , df_change.loc[[i],&quot;val&quot;]))
    df2.loc[i,columns] = pd.Series(state)
df2[&quot;product&quot;] =&quot;some_product&quot;
</code></pre>
<p><strong>Edit</strong></p>
<p>I marked the answer by ouroboros1 as theaccepted solution - it works really well and answered Question 1.</p>
<p>I am still curios about Question 2 : the difference in pandas performance using the two methods where i iterate through the rows. I found that I can also get a performance similar to the original &quot;df2&quot; method depending on how i assign the value before the loop.</p>
<p>The interesting point here is that pre assignment changes the performance in loop that follows.</p>
<pre><code># Fast approach gives 1000 iterations/sec
df3 = pd.DataFrame(index=index, columns=extra_colums + columns)

#df3.loc[index,&quot;product&quot;] = &quot;some_product&quot; # Fast  
#df3[&quot;product&quot;] = &quot;some_product&quot;           # Slow  
df3.product = &quot;some_product&quot;               # Fast

for i in tqdm(index):
    state.update(zip(df_change.loc[[i], &quot;col&quot;], df_change.loc[[i], &quot;val&quot;]))
    df3.loc[i, columns] = np.array(list(state.values())) 



</code></pre>
","1","Question"
"79412451","","<h1>Code</h1>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import streamlit as st
import plotly.express
import plotly

data = {
    '2024-01-31' :   1044,
    '2024-02-29' :   2310,
    '2024-03-31' :    518,
    '2024-04-30' :  -1959,
    '2024-05-31' :      0,
    '2024-06-30' :  -1010,
    '2024-07-31' :   1500,
    '2024-08-31' : -15459,
    '2024-09-30' : -14153,
    '2024-10-31' : -12604,
    '2024-11-30' :  -5918,
    '2024-12-31' :  -3897
}

df = pd.DataFrame(data.items(), columns=['date', 'value'])

fig = plotly.express.bar(data_frame=df, x='date', y='value')

st.plotly_chart(fig)
</code></pre>
<h1>Issue</h1>
<p>In this screenshot, the mouse is hovering over the rightmost bar.</p>
<p>The hover label says <code>Dec 31, 2024</code>, which is expected.</p>
<p>Note that the x-axis label says <code>Jan 2025</code>.</p>
<p><a href=""https://i.sstatic.net/YXvWs4x7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YXvWs4x7.png"" alt=""enter image description here"" /></a></p>
<h1>Workaround</h1>
<p>Here's one approach to a workaround:</p>
<pre><code>df['date'] = pd.to_datetime(df['date']).dt.to_period('M').dt.to_timestamp()
</code></pre>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import streamlit as st
import plotly.express
import plotly

data = {
    '2024-01-31' :   1044,
    '2024-02-29' :   2310,
    '2024-03-31' :    518,
    '2024-04-30' :  -1959,
    '2024-05-31' :      0,
    '2024-06-30' :  -1010,
    '2024-07-31' :   1500,
    '2024-08-31' : -15459,
    '2024-09-30' : -14153,
    '2024-10-31' : -12604,
    '2024-11-30' :  -5918,
    '2024-12-31' :  -3897
}

df = pd.DataFrame(data.items(), columns=['date', 'value'])

df['date'] = pd.to_datetime(df['date']).dt.to_period('M').dt.to_timestamp()

fig = plotly.express.bar(data_frame=df, x='date', y='value')

st.plotly_chart(fig)
</code></pre>
<p>Now the x-axis labels match the data bar months:</p>
<p><a href=""https://i.sstatic.net/F3Bcw5Vo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F3Bcw5Vo.png"" alt=""enter image description here"" /></a></p>
<h1>Notes</h1>
<p>The workaround here basically changes each date value from being the month-end to the first of the month.</p>
<p>So, instead of this</p>
<pre><code>&gt;&gt;&gt; df
          date  value
0   2024-01-31   1044
1   2024-02-29   2310
2   2024-03-31    518
3   2024-04-30  -1959
4   2024-05-31      0
5   2024-06-30  -1010
6   2024-07-31   1500
7   2024-08-31 -15459
8   2024-09-30 -14153
9   2024-10-31 -12604
10  2024-11-30  -5918
11  2024-12-31  -3897
</code></pre>
<p>we have this:</p>
<pre><code>&gt;&gt;&gt; df
         date  value
0  2024-01-01   1044
1  2024-02-01   2310
2  2024-03-01    518
3  2024-04-01  -1959
4  2024-05-01      0
5  2024-06-01  -1010
6  2024-07-01   1500
7  2024-08-01 -15459
8  2024-09-01 -14153
9  2024-10-01 -12604
10 2024-11-01  -5918
11 2024-12-01  -3897
</code></pre>
<p>The change is made with this line:</p>
<pre><code>df['date'] = pd.to_datetime(df['date']).dt.to_period('M').dt.to_timestamp()
</code></pre>
<h1>Question</h1>
<p>Is this the recommended approach to resolving this issue? Or is there a more idiomatic method using plotly?</p>
","2","Question"
"79412543","","<p>I import <a href=""https://www.census.gov/construction/bps/historical/metro_units.html"" rel=""nofollow noreferrer"">these files</a> into Python using Pandas:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
url = &quot;https://www.census.gov/construction/bps/txt/tb3u201910.txt&quot;
df = pd.read_fwf(url,skiprows=8,skipfooter=6,header=1)
</code></pre>
<p>This works except for lines that contain a break (line 29 continues on line 30):</p>
<pre><code>122 12060 Atlanta-Sandy Springs-Alpharetta,
  GA                                             3728    2242       4       0    1482      20     100
</code></pre>
<p>In the DataFrame that turns into:</p>
<pre><code>122  12060  Atlanta-Sandy Springs-Alpharetta,   NaN    NaN    NaN  NaN  NaN  NaN NaN
</code></pre>
<p>With a second line that has:</p>
<pre><code> GA       NaN  3728  2242.0    4.0     0.0  1482.0   20.0   100.0
</code></pre>
<p>The output I'm hoping to achieve:</p>
<pre><code>122  12060  Atlanta-Sandy Springs-Alpharetta,GA 3728    2242.0   4.0   0.0  1482.0   20.0     100.0
</code></pre>
<p>How can I make Pandas recognize the line continuation? Alternatively, is there a way to modify my current DataFrame to fix these errors? I'm open to both Pandas and non-Pandas solutions.</p>
","2","Question"
"79414030","","<p>I'm working with a large CSV file (over 10 million rows) that I need to process in Python. However, when I try to load the entire file into a pandas DataFrame, I run into memory issues.</p>
<p>What are some efficient ways to read and process large CSV files in Python without running out of memory?</p>
<p>I've considered the following approaches:</p>
<ul>
<li>Using <code>pandas.read_csv()</code> with <code>chunksize</code>: This allows me to read the file in smaller chunks, but I'm unsure how to effectively process each chunk.</li>
<li>Using <code>dask.dataframe</code>: I've heard that Dask can handle larger-than-memory datasets. Is it a good alternative?</li>
</ul>
<p>Using <code>csv.reader</code>: Would this be a more memory-efficient way to read the file line by line?</p>
<p>Could someone provide examples or best practices for handling large CSV files in Python? Any tips on optimizing performance would also be appreciated!</p>
","0","Question"
"79414916","","<p>I new to plotly library , I want to visualize a dafarame into a plotly Bubble chart.</p>
<p>here's the code :</p>
<pre><code>import plotly.graph_objects as go
import plotly.graph_objects as px 
import streamlit as st
import pandas as pd

data = {'x': [1.5, 1.6, -1.2],
        'y': [21, 16, 46],
        'circle-size': [10, 5, 6],
        'circle-color': [&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;]
        }
# Create DataFrame
df = pd.DataFrame(data)
st.dataframe(df)
fig = px.scatter(df, x=&quot;x&quot;, y=&quot;y&quot;, color=&quot;circle-color&quot;,
                 size='circle-size')
fig.show()
st.plotly_chart(fig)
</code></pre>
<p>I have problems, the first one is how to plug the dataframe(df) with plotly to see the data ? and the second I'm lookng to implement a custom bubble chart, something with colors with negative values like this :
<a href=""https://i.sstatic.net/cHBj6PgY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cHBj6PgY.png"" alt=""enter image description here"" /></a></p>
<p>can anyone help pleas to solve these problems ?
thnaks</p>
","1","Question"
"79415141","","<p>I have an application that takes an input spreadsheet filled in by the user.  I'm trying to bug fix, and I've just noticed that in one True/False column, they've written <code>FLASE</code> instead of <code>FALSE</code>.  I'm trying to write in as many workarounds for user error as I can, as the users of this app aren't very technical, so I was wondering if there's a way to convert this column to type bool, but set any typos to False? I appreciate this would also set something like TURE to be false too.</p>
<p>For example:</p>
<pre><code>df = pd.DataFrame({'bool_col':['True', 'Flase', 'False', 'True'], 'foo':[1,2,3,4]})
</code></pre>
<p>Running <code>df['bool_col'] = df['bool_col'].astype(bool)</code> returns True for everything (as they're all non-empty strings), however I would like it to reutrn True, False, False, True.</p>
","1","Question"
"79415726","","<p>In a recent post <a href=""https://stackoverflow.com/questions/79412275/pandas-performance-while-iterating-a-state-vector"">Pandas performance while iterating a state vector</a>, I noticed a performance when slicing pandas dataframes that i do not understand.</p>
<p>The code presented here does not do anything usefull, but highlight the issue:</p>
<ul>
<li>I create a dataframe with two areas of columns named <em>extra_columns</em> and <em>columns</em></li>
<li>The part of the code which takes time to execute is the loop, where slices in <em>columns</em> are assigned.</li>
</ul>
<p>What baffles me that the way i assign values to <em>extra_columns</em> before the loop affects the loop performance</p>
<p><strong>Python code</strong></p>
<pre><code>import timeit

setup_stmt =&quot;&quot;&quot;
import pandas as pd
num_cols = 500
n_iter = 100
extra_column = [ &quot;product&quot;]
columns = [chr(i+65) for i in range(num_cols)]
index= range(n_iter)
&quot;&quot;&quot;

stmt1 =&quot;&quot;&quot;
df = pd.DataFrame(index = index, columns=extra_column + columns)
df[&quot;product&quot;] = &quot;x&quot;
for i in index:
    df.loc[i,columns] = 0
&quot;&quot;&quot;

stmt2 =&quot;&quot;&quot;
df = pd.DataFrame(index = index, columns=extra_column + columns)
df.product = &quot;x&quot;            
for i in index:
    df.loc[i,columns] = 0
&quot;&quot;&quot;

stmt3 =&quot;&quot;&quot;
df = pd.DataFrame(index= index, columns=extra_column + columns)
df.loc[index,&quot;product&quot;] = &quot;x&quot;
for i in index:
    df.loc[i,columns] = 0
&quot;&quot;&quot;

stmt4 =&quot;&quot;&quot;
df = pd.DataFrame(index = index, columns=extra_column + columns)
for i in index:
    df.loc[i,columns] = 0
df[&quot;product&quot;] = &quot;x&quot;
&quot;&quot;&quot;

print(f&quot; stmt1 takes { timeit.timeit(setup= setup_stmt, stmt= stmt1,  number=10):2.2f} seconds&quot; )
print(f&quot; stmt2 takes { timeit.timeit(setup= setup_stmt, stmt= stmt2,  number=10):2.2f} seconds&quot; )
print(f&quot; stmt3 takes { timeit.timeit(setup= setup_stmt, stmt= stmt3,  number=10):2.2f} seconds&quot; )
print(f&quot; stmt4 takes { timeit.timeit(setup= setup_stmt, stmt= stmt4,  number=10):2.2f} seconds&quot; )
</code></pre>
<p><strong>Output</strong></p>
<pre><code> stmt1 takes 20.60 seconds
 stmt2 takes 0.46 seconds
 stmt3 takes 0.46 seconds
 stmt4 takes 0.46 seconds
</code></pre>
","3","Question"
"79417684","","<p>I have a Pandas df that looks like:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Index</th>
<th style=""text-align: center;"">foo</th>
<th style=""text-align: center;"">bar</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">{'A': 123.02, 'B': 12.3}</td>
<td style=""text-align: center;"">{'A': 123.02, 'B': 12.3}</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">{'A': 123.02, 'B': 12.3}</td>
<td style=""text-align: center;"">{'A': 123.02, 'B': 12.3}</td>
</tr>
</tbody>
</table></div>
<p>I want to remove any duplicate records in my df. So I tried:</p>
<pre><code>df.drop_duplicates(inplace=True)
</code></pre>
<p>which gave an error:</p>
<pre><code>File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 7195, in pandas._libs.hashtable.PyObjectHashTable._unique
TypeError: unhashable type: 'dict'
</code></pre>
<p>Current Approach to remove duplicate records:</p>
<pre><code>df = df.map(str).T.drop_duplicates().T
df['foo' = df['foo'].apply(lambda x: ast.literal_eval(x))
</code></pre>
<p>Since the values get converted to</p>
<pre><code>&quot;{'A' : np.float32(123.02), 
  'B': np.float32(12.3)}&quot;
</code></pre>
<p>instead of</p>
<pre><code>&quot;{'A': 123.02, 'B': 12.3}&quot;
</code></pre>
<p>there's an error from <code>ast</code>.</p>
<p>Is there anyway to prevent the np.float32 from showing in the str?</p>
<p>Except replacing it in post processing.</p>
","-2","Question"
"79418256","","<p>when I use only plotly I got this :</p>
<p>code :</p>
<pre><code>import plotly.graph_objects as go
import plotly.express as px 
import pandas as pd


data = {'x': [1.5, 1.6, -1.2],
        'y': [21, -16, 46],
        'circle-size': [10, 5, 6],
        'circle-color': [&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;]
        }

# Create DataFrame
df = pd.DataFrame(data)
fig = px.scatter(
    df,
    x=&quot;x&quot;, 
    y=&quot;y&quot;, 
    color=&quot;circle-color&quot;,
    size='circle-size'
)


fig.update_layout(
    {
        'xaxis': {
            &quot;range&quot;: [-100, 100],
            'zerolinewidth': 3, 
            &quot;zerolinecolor&quot;: &quot;blue&quot;,
            &quot;tick0&quot;: -100,
            &quot;dtick&quot;: 25,
            'scaleanchor': 'y'
        },
        'yaxis': {
            &quot;range&quot;: [-100, 100],
            'zerolinewidth': 3, 
            &quot;zerolinecolor&quot;: &quot;green&quot;,
            &quot;tick0&quot;: -100,
            &quot;dtick&quot;: 25
        },
        &quot;width&quot;: 500,
        &quot;height&quot;: 500
    }
)
fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/OlCU5Sd1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OlCU5Sd1.png"" alt=""enter image description here"" /></a></p>
<p>but when I use it with streamlit :</p>
<pre><code>import streamlit as st
import plotly.graph_objects as go
import plotly.express as px 
import pandas as pd


data = {'x': [1.5, 1.6, -1.2],
        'y': [21, -16, 46],
        'circle-size': [10, 5, 6],
        'circle-color': [&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;]
        }

# Create DataFrame
df = pd.DataFrame(data)
fig = px.scatter(
    df,
    x=&quot;x&quot;, 
    y=&quot;y&quot;, 
    color=&quot;circle-color&quot;,
    size='circle-size'
)


fig.update_layout(
    {
        'xaxis': {
            &quot;range&quot;: [-100, 100],
            'zerolinewidth': 3, 
            &quot;zerolinecolor&quot;: &quot;green&quot;,
            &quot;tick0&quot;: -100,
            &quot;dtick&quot;: 25,
            &quot;scaleanchor&quot;: 'y'
        },
        'yaxis': {
            &quot;range&quot;: [-100, 100],
            'zerolinewidth': 3, 
            &quot;zerolinecolor&quot;: &quot;red&quot;,
            &quot;tick0&quot;: -100,
            &quot;dtick&quot;: 25
        },
        &quot;width&quot;: 500,
        &quot;height&quot;: 500
    }
)
event = st.plotly_chart(fig, key=&quot;iris&quot;, on_select=&quot;rerun&quot;)

event.selection
</code></pre>
<p>we got this :</p>
<p><a href=""https://i.sstatic.net/82WXFc4T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82WXFc4T.png"" alt=""enter image description here"" /></a></p>
<p>why the x axis is removed when I use streamlit?</p>
","1","Question"
"79419035","","<p>Using Pandas 2.2.1 within Python 3.12, I'm importing data from text files that have a varying number of rows of metadata and multiple tables that precede the actual data to be imported. The number of rows that need to be skipped is typically between 3500 to 4500 when using the &quot;skiprows&quot; option within the &quot;pd.read_csv&quot; approach. The rows of data that are needed are always located at the very end of the text file and are bracketed by a row of dashes.
The file structure (overly simplified) looks like this:</p>
<pre><code>Archive generated by archiveSST02e.f                                                         
GEOPHYSICAL LOG
Borehole fluid type:                                           WATER            
Borehole fluid resistivity/conductivity:                       see log          
Borehole fluid temperature:                                    see log                                                                               
CTD manufacturer and SN:                                       Sea and Sun Tech.   

***Thousands of rows of more text and tables of unused data***

Then the data that is actually needed.
The last row of dashes is the last row of the text file.
--------------------------------------------------------------------------------------        
                                         Spec.   Spec.                                                    
        Corr.                            cond.   cond.                                                       
Alt.     sub.     Sub.   Cond.   Temp.   CTD     formula    Press.     Dens.                                 
feet     feet     feet   uS/cm   deg C   uS/cm   uS/cm      dbar      kg/m^3        
--------------------------------------------------------------------------------------
0.38     0.53     0.13    1067   26.870   1028    1030       0.04     996.93 
0.24     0.67     0.27    1048   26.794   1012    1014       0.08     996.95
0.12     0.79     0.38    1014   26.762    980     981       0.11     996.95
-0.34     1.25     0.84     842   26.785    813     814       0.25     996.88
...
-0.34     1.25     0.84     825   26.774    797     798       0.25     996.87
--------------------------------------------------------------------------------------
</code></pre>
<p>Example script used to import the text file data using the following python script:</p>
<pre><code>df = pd.read_csv('Name_of_file.txt',encoding='utf-8', delimiter=r&quot;\s+&quot;, skiprows=4196, header=None)
</code></pre>
<p>I can't seem to find anything on Stack Overflow that will allow me to import the desired section of data without hard coding how many header lines to skip. Each text file I need to import has different number of rows to skip and different number or rows of data to import.</p>
","0","Question"
"79419281","","<p>The DataFrame is generated using pd.read_excel. The index values are date which I want to convert to datetime but I am not sure if the approach I found is the &quot;best&quot; one. My issue is that the string date format has month in english abbreviated name. So in my case, &quot;2024-02-10&quot; is &quot;Feb 10, 2024&quot;. Therefore, I cannot use <code>datetime.strptime(date_string, &quot;%b %d, %Y&quot;)</code> because the month abbreviated name depends on locale and I dont want to use <code>locale.setlocale()</code>.</p>
<p>The two options I found to be able to do that are:</p>
<ul>
<li>Using <a href=""https://arrow.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">arrow library</a>:</li>
</ul>
<pre><code>import arrow
date_arrow = arrow.get(&quot;Feb 10, 2024&quot;, &quot;MMM D, YYYY&quot;, locale=&quot;en_US&quot;).datetime
</code></pre>
<ul>
<li>Using <a href=""https://dateparser.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">dateparser library</a>:</li>
</ul>
<pre><code>import dateparser
date_parsed = dateparser.parse(&quot;Feb 26, 1971&quot;)
</code></pre>
<p>To to have the DataFrame as I want, I can do either:</p>
<pre><code>import pandas as pd
import arrow
path = r&quot;some_path&quot;
data = pd.read_excel(path, header=[0, 1, 2, 3], index_col=0)
data.index.map(lambda x: arrow.get(x, &quot;MMM D, YYYY&quot;, locale=&quot;en_US&quot;).datetime)
</code></pre>
<p>or</p>
<pre><code>import pandas as pd
import dateparser
path = r&quot;some_path&quot;
data = pd.read_excel(path, header=[0, 1, 2, 3], index_col=0)
data.index.map(lambda x: dateparser.parse(x))
</code></pre>
<p>Both work but <code>datepaser</code> seems a bit slower. Now my questions are the followings:</p>
<ul>
<li><p>Is it correct that I cannot do that using python standard library without relying on <code>locale</code>?</p>
</li>
<li><p>More generally, is there some better/cleaner alternative to achieve this?</p>
</li>
<li><p>Bonus question: I have been trying to do this conversion directly within pd.read_excel but nothing is being converted... I do not get why. There is not error, it just seems to not be doing anything</p>
</li>
</ul>
<pre><code>import pandas as pd
import arrow
path = r&quot;some_path&quot;
data = pd.read_excel(path, header=[0, 1, 2, 3], index_col=0, converters={0: lambda x: arrow.get(x, &quot;MMM D, YYYY&quot;, locale=&quot;en_US&quot;).datetime})
</code></pre>
","1","Question"
"79420147","","<p>I have a dataframe like below</p>
<pre><code>data = {'col7': ['Apple','Banana','-','-','-','-'],'col6': [float('NaN'),float('NaN'),'-','-','-','HA'],'col5': [float('NaN'),float('NaN'),'-','-','-','HB'],'col4': [float('NaN'),float('NaN'),'-','Egg','G1','HC'],'col3': [float('NaN'),'Cat','Dog',float('NaN'),'G2','HD'],'col2': [float('NaN'),float('NaN'),float('NaN'),float('NaN'),'G3',float('NaN')],'col1': [float('NaN'),float('NaN'),float('NaN'),float('NaN'),'G4','HD'],'col0': [float('NaN'),float('NaN'),float('NaN'),'Fish','G5',float('NaN')]}
df = pd.DataFrame(data)
</code></pre>
<p><a href=""https://i.sstatic.net/gMdzldIz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gMdzldIz.png"" alt=""enter image description here"" /></a></p>
<p>I'd like to print the string for iteration columns and rows with column name and also store in a list</p>
<p><a href=""https://i.sstatic.net/LV6KD4dr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LV6KD4dr.png"" alt=""enter image description here"" /></a></p>
<p>A For loop to print the string and save in a list :</p>
<p>Apple=output[col7:col0]</p>
<p>Banana=output[col7:col4]</p>
<p>Cat=output[col3:col0]</p>
<p>Dog=output[col3:col0]</p>
<p>.....</p>
<p>Appriciated your big help!!</p>
","0","Question"
"79421290","","<p>For the following <code>df</code> I wish to change the values in columns <code>A</code>,<code>B</code> and <code>C</code> to those in columns <code>X</code>,<code>Y</code> and <code>Z</code>, taking into account a boolean selection on column <code>B</code>.</p>
<pre><code>columns = {&quot;A&quot;:[1,2,3],
           &quot;B&quot;:[4,pd.NA,6],
           &quot;C&quot;:[7,8,9],
           &quot;X&quot;:[10,20,30],
           &quot;Y&quot;:[40,50,60],
           &quot;Z&quot;:[70,80,90]}

df = pd.DataFrame(columns)
df

    A   B      C    X   Y   Z
0   1   4      7    10  40  70
1   2   &lt;NA&gt;   8    20  50  80
2   3   6      9    30  60  90
</code></pre>
<p>However when I try to do the value reassignment I end up with NULLS.</p>
<pre><code>df.loc[~(df[&quot;B&quot;].isna()), [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]] = df.loc[~(df[&quot;B&quot;].isna()), [&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;]]
df

    A   B      C    X   Y   Z
0   NaN NaN    NaN  10  40  70
1   2.0 &lt;NA&gt;   8.0  20  50  80
2   NaN NaN    NaN  30  60  90

</code></pre>
<p>My desired result is:</p>
<pre><code>    A   B   C   X   Y   Z
0   10  40  70  10  40  70
1   2   &lt;NA&gt;    8   20  50  80
2   30  60  90  30  60  90
</code></pre>
<p>If I do the reassignment on a single column then I get my expected result:</p>
<pre><code>df.loc[~(df[&quot;B&quot;].isna()), &quot;A&quot;] = df.loc[~(df[&quot;B&quot;].isna()), &quot;X&quot;]
df
    A   B      C    X   Y   Z
0   10  4      7    10  40  70
1   2   &lt;NA&gt;   8    20  50  80
2   30  6      9    30  60  90
</code></pre>
<p>However I expected that I should be able to do this on multiple columns at once. Any ideas what I am missing here?</p>
<p>Thanks</p>
","1","Question"
"79421531","","<p>Consider the following pandas dataframe</p>
<pre><code>    reference   sicovam     label       id      date        TTM price
0   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  18  52.69
1   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  30  NaN
2   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  49  53.11
3   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  60  NaN
4   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  77  53.69
5   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  90  NaN
6   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  109 54.42
7   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  137 55.15
8   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  171 55.80
9   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  180 NaN
10  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  15  50.04
11  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  30  NaN
12  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  46  50.52
13  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  60  NaN
14  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  74  51.17
15  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  90  NaN
16  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  106 51.95
17  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  134 52.73
18  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  168 53.46
19  SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-05  180 NaN
</code></pre>
<p>After grouping by the <code>reference</code>, <code>sicovam</code>, <code>label</code>, <code>id</code> and <code>date</code> columns, I would like to fill the <code>NaN</code> values of the <code>price</code> column via linear interpolation over the <code>TTM</code> value i.e., in the context of the linear interpolation formula, <code>price</code> is the <code>y</code> and <code>TTM</code> is the <code>x</code> variable.</p>
<p>So far, I built the following lines.</p>
<pre><code>def intepolate_group(group):
    group[&quot;price&quot;] = group[&quot;price&quot;].interpolate(method='linear', limit_direction='both', axis=0)
    return group

new_df = df.groupby([&quot;reference&quot;,&quot;sicovam&quot;,&quot;label&quot;,&quot;id&quot;,&quot;date&quot;])[[&quot;TTM&quot;,&quot;price&quot;]].apply(intepolate_group)
</code></pre>
<p>Nevertheless, the result that I get is the linear interpolation over the index numbers per group. For example for the following part of the dataset, I get <code>54.06</code> instead of <code>53.99</code>. What do I still need in order to interpolate over the TTM variable?</p>
<p>PS: I want to avoid masking via loop (instead of grouping) and setting the <code>TTM</code> as the index, because the dataframe is quite big and such a scenario takes considerable amount of time.</p>
<pre><code>4   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  77  53.69
5   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  90  NaN
6   SCOM_WTI    68801903    WTI Nymex   BBG:CL  2015-01-02  109 54.42
</code></pre>
","0","Question"
"79421661","","<p>I managed to set a fixed label for each bubble in my chart. Here’s the code:</p>
<pre><code>import plotly.graph_objects as go
import plotly.express as px 
import pandas as pd
margin_factor = 1.6
data = {'x': [1.5, 1.6, -1.2],
        'y': [21, -16, 46],
        'circle-size': [10, 5, 6],
        'circle-color': [&quot;red&quot;,&quot;red&quot;,&quot;green&quot;],
        'tttt': [&quot;the last xt for MO&quot;,&quot;the last xt for MO pom&quot;,&quot;the last xt for MO %&quot;]
        }
# Create DataFrame
df = pd.DataFrame(data)
fig = px.scatter(
    df,
    x=&quot;x&quot;, 
    y=&quot;y&quot;, 
    color=&quot;circle-color&quot;,
    size='circle-size'
)


fig.update_layout(
    {
        'xaxis': {
            &quot;range&quot;: [-100, 100],
            'zerolinewidth': 3, 
            &quot;zerolinecolor&quot;: &quot;blue&quot;,
            &quot;tick0&quot;: -100,
            &quot;dtick&quot;: 25,
            'scaleanchor': 'y'
        },
        'yaxis': {
            &quot;range&quot;: [-100, 100],
            'zerolinewidth': 3, 
            &quot;zerolinecolor&quot;: &quot;green&quot;,
            &quot;tick0&quot;: -100,
            &quot;dtick&quot;: 25
        },
        &quot;width&quot;: 500,
        &quot;height&quot;: 500
    }
)
x_pad = (max(df.x) - min(df.x)) / 8
y_pad = (max(df.y) - min(df.y)) / 30

for x0, y0 in zip(data['x'], data['y']):
    fig.add_shape(type=&quot;rect&quot;,
        x0=x0 + (x_pad)/5, 
        y0=y0 - y_pad, 
        x1=x0 + x_pad, 
        y1=y0 + y_pad,
        xref='x', yref='y',
        line=dict(
            color=&quot;black&quot;,
            width=2,
        ),
        fillcolor=&quot;#1CBE4F&quot;,
        layer=&quot;below&quot;
    )

fig.add_trace(
    go.Scatter(
        x=df[&quot;x&quot;].values + (x_pad)/2,
        y=df[&quot;y&quot;],
        text=df[&quot;tttt&quot;],
        mode=&quot;text&quot;,
        showlegend=False
    )
)
fig.show()
</code></pre>
<p>The result :
<a href=""https://i.sstatic.net/3K3wZHNl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3K3wZHNl.png"" alt=""enter image description here"" /></a></p>
<p>Now what I’m trying to do is to put some css to these labels some this like this :</p>
<p><a href=""https://i.sstatic.net/InHsANWk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/InHsANWk.png"" alt=""enter image description here"" /></a></p>
<p>I know there is something called annotation, I tried it, but I did not successfully create annotations for each element.</p>
","1","Question"
"79421829","","<p>I have a DataFrame df_items and want to create combinations of its rows of size i using itertools.combinations. Each combination should maintain all columns from the original DataFrame.</p>
<p>Current approach: works but loses column names</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import combinations
combinations = np.array(list(combinations(range(len(df_items)), i)))
selected_items = df_items.values[combinations]
</code></pre>
","-1","Question"
"79423247","","<p>I created a code to insert my dataframe, called df3, into an excel file.</p>
<p>My code is working fine, but now I'd like to change background cells color in all cells based on value</p>
<p>I tried this solution, I don't get any errors but I also don't get any cells colored</p>
<p>My code:</p>
<pre><code>def cond_formatting(x):
    if x == 'OK':
        return 'background-color: lightgreen'
    elif x == 'NO':
        return 'background-color: red'
    else:
        return None
    
print(pd.merge(df, df2, left_on='uniquefield', right_on='uniquefield2', how='left').drop('uniquefield2', axis=1))
df3 = df.merge(df2, left_on='uniquefield', right_on='uniquefield2', how='left').drop(['uniquefield2', 'tournament2', 'home2', 'away2', 'result2'], axis=1) 
df3 = df3[[&quot;home&quot;,&quot;away&quot;,&quot;scorehome&quot;,&quot;scoreaway&quot;,&quot;best_bets&quot;,&quot;oddtwo&quot;,&quot;oddthree&quot;,&quot;htresult&quot;,&quot;shresult&quot;,&quot;result&quot;,&quot;over05ht&quot;,&quot;over15ht&quot;,&quot;over05sh&quot;,&quot;over15sh&quot;,&quot;over05&quot;,&quot;over15&quot;,&quot;over25&quot;,&quot;over35&quot;,&quot;over45&quot;,&quot;goal&quot;,&quot;esito&quot;,&quot;tournament&quot;,&quot;uniquefield&quot;]]
df3 = df3.sort_values('best_bets')

df3.style.applymap(cond_formatting)



# determining the name of the file
file_name = camp + '_Last_20' + '.xlsx'
 
# saving the excel
df3.to_excel(file_name, freeze_panes=(1, 0))
print('Tournament is written to Excel File successfully.')
</code></pre>
<p>How I said, code is working but all background cells color are white (no colors)
any suggestion?</p>
<p>Thanks for your help</p>
","2","Question"
"79424017","","<p>I have a dataframe which has the following columns:
<code>region_id</code>, <code>name</code>, <code>parent</code>, <code>parent_name</code>, <code>t2m</code>, <code>d2m</code>, and <code>tp</code>.</p>
<p>I want to group and aggregate column values in a specific way. To enable that, I have defined the following lists:</p>
<pre><code>w_params = ['t2m', 't2m', 't2m', 'd2m', 'tp']
operation = ['max', 'min', 'mean', 'mean', 'sum']
common_cols = ['region_id', 'name', 'parent', 'parent_name']
</code></pre>
<p>I have written the function <code>agg_daily</code> to group column values by <code>date</code> and <code>region_id</code> and aggregate.</p>
<pre><code>def agg_daily(df, common_cols, w_params, operation):
    &quot;&quot;&quot;
    Aggregate the data for each day.

    Parameters
    ----------
    df : pandas dataframe
        Dataframe containing daily data.

    Returns
    -------
    agg_daily_df : pandas dataframe
        Dataframe containing aggregated data for each day.

    &quot;&quot;&quot;
    agg_daily_df = df.groupby(['date', 'region_id']).agg(
        name=('name', 'first'),
        parent=('parent', 'first'),
        parent_name=('parent_name', 'first'),
        t2m_max=('t2m', 'max'),
        t2m_min=('t2m', 'min'),
        t2m_mean=('t2m', 'mean'),
        d2m=('d2m', 'mean'),
        tp=('tp', 'sum')
    ).reset_index()
    agg_daily_df = agg_daily_df.sort_values(['region_id', 'date'], ascending=[True, True]).reset_index(drop=True)
    return agg_daily_df
</code></pre>
<p>However, notice inside <code>agg_daily</code> that the arguments within <code>agg</code>, e.g. <code>t2m_max</code>, <code>t2m_min</code>, <code>t2m_mean</code> are hard coded. Instead, I want to pass <code>common_cols</code>, <code>w_params</code>, <code>operation</code> as arguments to <code>agg_daily</code>, avoid the hard coding, and yet get the function <code>agg_daily</code> perform the desired operation.</p>
<p>Note that for columns belonging to <code>common_cols</code>, I do not wish to create a new column name in the final output. However, for the columns belonging to <code>w_params</code>, I want to create a column corresponding to the <code>operation</code> being performed.</p>
<p>Can anyone help me get a customizable function?</p>
","0","Question"
"79424140","","<p>I realize <strong>there are working alternatives to this</strong>, I just want to understand what is going on for my own edification or anyone else who comes across this.</p>
<pre><code>df_test = pd.DataFrame({'test1':['blah1','blah2','blah3'],'test2':['blah1','blah2','blah3']})
</code></pre>
<p>When I run the below code on this above DataFrame, I get <code>TypeError: replace() argument 2 must be str, not float</code></p>
<pre><code>df_test.map(lambda x: x.replace('blah1',np.nan))
</code></pre>
<p>What exactly prevents 'argument 2' from being <code>np.nan</code> from working when of course the below works no problem</p>
<pre><code>df_test.replace('blah1',np.nan)
</code></pre>
<p>Thanks in advance</p>
","0","Question"
"79425295","","<p>A disclaimer at the start: I am not really a programmer but do have some ability to hack some things together and get a reasonable result. But my knowledge is very much a case of— get a solution to the problem at hand, find the tree but not really take a look at the forest.</p>
<p>I'm trying to put together a database for playing with ML and AI models in python. The data has heterogeneous frequencies:</p>
<ul>
<li>Stock prices and interest rates with daily data</li>
<li>Interest rates and some other macro economic data at monthly frequencies</li>
<li>Things like GDP and GNI and some other production related data at quarterly frequency</li>
</ul>
<p>Given that some of the work I need to do is related to trying to forecast estimates for things like quarterly earnings or other financial data elements from balance sheets or income statements, and macro data like GNP, that means classifying daily, weekly or monthly data into a quarterly bucket is needed.</p>
<p>I was starting to use the Pandas Periodindex function and that did what I wanted. But if I want to do something like filter a dataframe for only Q1 data for all the years turns out it is not as simple as &quot;where 'Q1' in <code>df.index</code>, do something&quot;. Looks like I can do what I want by jumping though some additional steps, but I'm thinking of simply making the index a string value rather than a Period value (just using the <code>.astype(str)</code> when using the <code>Periodindex()</code> call).</p>
<p>Would I be giving up some functionality I'm not seeing at the moment but might really like having access to by changing the index?</p>
<p>Below is a snippet of what I'm talking about:</p>
<pre><code>...
qBalSh.index = pd.PeriodIndex(qBalSh.fiscalDateEnding, freq='Q').astype(str)
qBalSh.index.name = 'Quarter'
qIncSt.index = pd.PeriodIndex(qIncSt.fiscalDateEnding, freq='Q')
qIncSt.index.name = 'Quarter'
...
</code></pre>
","0","Question"
"79425965","","<p>I'm opening a csv file using pandas.</p>
<pre><code>import pandas as pd 
df = pd.read_csv('/file/planned.csv') 
</code></pre>
<p>I'm opening a file that contains about 2,000 records collected from all over the places in the world. When I'm trying to open this file with pandas, I'm getting the following errors for</p>
<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xec in position 34: invalid continuation byte
</code></pre>
<p>After I searched through the web, I was able to put the following encoding options hoping that I could open the file. However, I'm still getting the following error messages for each encoding options I tried.</p>
<h1>utf-8</h1>
<pre><code>df_planned = pd.read_csv('/content/sample_data/planned.csv', encoding='utf-8')
    &gt; UnicodeDecodeError: 'utf-8' codec can't decode byte 0xec in position 34: invalid continuation byte
</code></pre>
<h1>utf-16</h1>
<pre><code>df_planned = pd.read_csv('/content/sample_data/planned.csv', encoding='utf-16') 
    &gt; UnicodeDecodeError: 'utf-16-le' codec can't decode bytes in position 234-235: illegal encoding
</code></pre>
<h1>euc-kr</h1>
<pre><code>df_planned = pd.read_csv('/content/sample_data/planned.csv', encoding='euc-kr')
UnicodeDecodeError: 'euc_kr' codec can't decode byte 0x84 in position 37: illegal multibyte sequence
</code></pre>
<p>I'm still not able to open the file into the dataframe using the pandas.</p>
<h1>cp949</h1>
<pre><code>df_planned = pd.read_csv('/content/sample_data/planned.csv', encoding='cp949')
UnicodeDecodeError: 'cp949' codec can't decode byte 0xe8 in position 43: illegal multibyte sequence
</code></pre>
<p>Could anyone help? Thank you so much.</p>
","0","Question"
"79426835","","<p>Im trying to use <code>pandas</code> in a dbt python model (dbt-duckdb), but I keep getting the problem <code>Python model failed: No module named 'pandas'</code>.
Here you can find my dbt model configuration:</p>
<pre><code>import boto3
import pandas as pd
def model(dbt, session):
    dbt.config(
        materialized=&quot;table&quot;,
        packages = [&quot;pandas==2.2.3&quot;],
        python_version=&quot;3.11&quot;
    )
    key = &quot;my_key&quot;
    bucket = &quot;my_bucket&quot;
    client = boto3.client('s3')
    return None
</code></pre>
<p>Also I know duckdb has a way of importing s3 files but I need to manipulate the files before duckdb reads them because they are not correct.</p>
<p>Also this is my models yaml config</p>
<pre><code>version: 2

models:
  - name: test
    config:
      packages:
       - &quot;pandas==2.2.3&quot;
</code></pre>
<p>Also I have a virtualenvironemnt with pandas installed.</p>
<p>Anyone who has experience with it, thanks in advance!</p>
","1","Question"
"79426836","","<p>I have a dataframe called &quot;base_dataframe&quot; that looks as following:</p>
<pre><code>      F_NAME      L_NAME       EMAIL     
0     Suzy        Maripol      suzy@mail.com
1     Anna        Smith        anna@mail.com
2     Flo         Mariland     flo@mail.com
3     Sarah       Linder       sarah@mail.com
4     Nala        Kross        Nala@mail.com
5     Sarosh      Fink         sarosh@mail.com
</code></pre>
<p>I would like to create a new dataframe that only contains the rows matching specific regular expressions that I define:</p>
<ul>
<li>For column &quot;F_NAME&quot; I only want to copy over the rows that contain &quot;Sar&quot;</li>
<li>For column &quot;L_NAME&quot; I only want to copy over the rows that contain &quot;Mari&quot;</li>
</ul>
<p>The way I tackle this in my code is :</p>
<pre><code>sar_df = base_dataframe[&quot;F_NAME&quot;].str.extract(r'(?P&lt;sar_content&gt;(^Sar.*))') 
mari_df = base_dataframe[&quot;L_NAME&quot;].str.extract(r'(?P&lt;mar_content&gt;(^Mari.*))') 
</code></pre>
<p>Then I copy those filtered columns/DFs over to my target dataframe &quot;new_dataframe&quot;:</p>
<pre><code>new_dataframe[&quot;selected_F_NAME&quot;] = sar_df.copy
new_dataframe[&quot;selected_L_NAME&quot;] = mari_df.copy
</code></pre>
<p>And my &quot;new_dataframe&quot; would at the end look like this :</p>
<pre><code>      F_NAME      L_NAME       EMAIL     
0     Suzy        Maripol      suzy@mail.com
2     Flo         Mariland     flo@mail.com
3     Sarah       Linder       sarah@mail.com
5     Sarosh      Fink         sarosh@mail.com
</code></pre>
<p>This works for me but it takes an extremely long time to copy over all the data to my &quot;new_dataframe&quot;, because my &quot;base_dataframe&quot; has many hundred thousands of rows. I also need to apply multiple different regular-expressions on multiples columns ( the dataframe example I gave is basically simplified, just to explain what I want to do).</p>
<p>I am pretty sure there is a more optimised way to do this, but can't figure it out right now. I would appreciate any help with this.</p>
","3","Question"
"79426845","","<p>This very simple script gives error <code>KeyError: &quot;None of [Index(['A', 1], dtype='object')] are in the [index]&quot;</code>:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
L1 = ['A','A','A','A','B','B','B','B']
L2 = [1,1,2,2,1,1,2,2]
V = [9.8,9.9,10,10.1,19.8,19.9,20,20.1]
df = pd.DataFrame({'L1':L1,'L2':L2,'V':V})

print(df)

df.groupby(['L1','L2']).boxplot(column='V')
plt.show()
</code></pre>
<p>So my dataframe is:</p>
<pre><code>  L1  L2     V
0  A   1   9.8
1  A   1   9.9
2  A   2  10.0
3  A   2  10.1
4  B   1  19.8
5  B   1  19.9
6  B   2  20.0
7  B   2  20.1
</code></pre>
<p>and I would expect a plot with four boxplot showing the values V, the labels of boxplots should be A/1, A/2, B/1, B/2.</p>
<p>I had a look at <a href=""https://stackoverflow.com/questions/55652574/how-to-solve-keyerror-unone-of-index-dtype-object-are-in-the-colum"">How To Solve KeyError: u&quot;None of [Index([..], dtype=&#39;object&#39;)] are in the [columns]&quot;</a> but I was not able to fix my error, AI tools are not helping me either.</p>
<p>What am I not understanding?</p>
","4","Question"
"79427970","","<p>I have a pandas dataframe that looks like this.</p>
<blockquote>
<pre><code>imagefile.   imagefile_mnemonic.      a.  b.   pix.   val
file1        image1                   0.  0.   v1.    55
file1        image1                   0.  1.   v1.    75
file1        image1                   0.  2.   v1.    95
file1        image1                   0.  3.   v1.   115
file1        image1                   0.  4.   v1.   135
file1        image1                   0.  5.   v1.   155
file1        image1                   0.  6.   v1.   175
...          ...                   ...
file6        image6                   23. 11.  v5.   763
file6        image6                   23. 12.  v5.   787
</code></pre>
</blockquote>
<p>I want to calculate the slopes where x values are values in column b for a combination of mnemonic, same 'a' value and same pix type. y values are values in val column. 'a' values range from 0 to 20 for file1, 0 to 41 for file2,...different for each file. For each of these 0 to 20 values in file 1 there are 7 'b' values with 7 corresponding values in column val. There are 5 pix types in each file.</p>
<p>So for file1, the data for calculating slope would be</p>
<pre><code>file1, pix type v1, 'a'=0 ==&gt;`data = {'x':[0,1,2,3,4,5,6],
'y':[55,75,95,115,135,155,175]}`
file1, pix type v1, 'a'=1 ==&gt;  data here similar to above.
file1, pix type v1, 'a'=2 ==&gt;  data here similar to above.

... 
file 1, pix type v1, 'a'=19 ==&gt;  data here similar to above
</code></pre>
<p>Then move to file1, pix type v2</p>
<pre><code>file1, pix type v2, 'a'=0 ==&gt;`data {'x'[0,1,2,3,4,5,6], &gt; 'y':[45,65,85,105,125,145]}` 
file 1, pix type v2, 'a'=1 ==&gt;  data here similar to above. 
file 1, pix type v2, 'a'=2 ==&gt;  data here similar to above. 
... 
file 1, pix type v2,'a'=19 ==&gt;  data here similar to above.
</code></pre>
<p>...
Do this for all pix types in file 1 then repeat for files 2 to 6.
Later I want to take an average of all slopes for all 'a' values in file 1.</p>
<p>I tried the following by creating a new dataframe for each pix type.</p>
<pre><code>    `from scipy.stats import linregress
     df_v1 = df[df['pix']=='v1']
     slope, intercept, r_value, p_value, std_err = linregress(df_v1['b'],        df_v1['val']).groupby(df_p['a'])`
</code></pre>
<p>but got an error that says
<code>AttributeError: 'LinregssResult' object has no attribute 'groupby' </code></p>
<p>Is there any other way I can address this and still use <code>linregress</code>? Thank you!</p>
","0","Question"
"79428080","","<p>I use pandas within visual studio, and had to switch to a new computer. I'm currently running pandas version (2.2.3), and cannot get the entire dataframe to print, although I didn't have this issue on my older laptop with an older version of pandas. I have tried setting all the <em>pd.set_options</em> below, but still the entire dataframe isn't being printed. I checked the length of the dataframe I am printing, and it is 643, but it is only printing a subset of 40 rows. The oddest bit is that the print cuts off mid row of data, which occurs regardless of how many columns I include in the print. Are there other settings within Visual Studio or with pandas that influence the print statements?</p>
<pre><code>pd.set_option('max_colwidth', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
</code></pre>
<p><a href=""https://i.sstatic.net/BWNI4wzu.png"" rel=""nofollow noreferrer"">Row cutting off mid print statement</a></p>
<p><strong>FIX</strong>: For whatever reason, restarting my computer didn't work, but forcing a restart within VSCode under &quot;<em>Help-&gt;Check for Updates</em> fixed the issue. Appreciate the help all.</p>
","1","Question"
"79428848","","<p>I have imported a JSON file in Python for extrapolate the information that I have in it for selecting every single data or principal data that contains different data. When I try to select the principal ones, to see all the data that contains, I can't see it, but instead I only see the first data of the principal paragraph.</p>
<p>How this is possible? Is it impossible to see the data transformed in a table?</p>
<p>For example, I have this index with this data but I can't see all the data that this index contains when I recall <code>AccessLevels</code> from JSON with code like this:</p>
<pre class=""lang-py prettyprint-override""><code>Accesso = df['AccessLevels']
print(Accesso)
</code></pre>
<p>JSON:</p>
<pre class=""lang-json prettyprint-override""><code>&quot;AccessLevels&quot;: {
    &quot;Home.btnBuzzer&quot;: 0,
    &quot;Home.btnLogin&quot;: 0,
    &quot;Home.btnLogout&quot;: 0,
    &quot;Home.btnMenu&quot;: 0,
    &quot;L2A_ConfEdit.btnRecSave&quot;: 2,
    &quot;L2A_ConfEdit.cbNameConf&quot;: 4,
    &quot;L2A_ConfEdit.editParameters&quot;: 4,
    &quot;L2B_ProgEdit.btnRecActivate&quot;: 1,
    &quot;L2B_ProgEdit.btnRecSave&quot;: 2,
    &quot;L2B_ProgEdit.btnRecSaveAs&quot;: 2,
    &quot;L2B_ProgEdit.cbNameProg&quot;: 1,
    &quot;L2B_ProgEdit.editParameters&quot;: 2,
    &quot;L2E_Stats.btnRstPar&quot;: 1,
    &quot;L2E_Stats.btnRstTot&quot;: 2,
    &quot;L2_Menu.btnConfEdit&quot;: 0,
    &quot;L2_Menu.btnMaint&quot;: 4,
    &quot;L2_Menu.btnProgEdit&quot;: 0,
    &quot;L2_Menu.btnRtCmds&quot;: 0,
    &quot;L2_Menu.btnStats&quot;: 0,
    &quot;L2_Menu.btnTestControl&quot;: 2
},
</code></pre>
<p>The result of printing the access level is this:</p>
<pre><code>Home.btnBuzzer  0.0 
Home.btnLogin   0.0 
Home.btnLogout  0.0 
Home.btnMenu    0.0 
L2A_ConfEdit.btnRecSave  2.0 
  ... 
5    NaN
6    NaN
7    NaN 
8    NaN 
9    NaN
Name: AccessLevels, Length: 74, dtype: float64
</code></pre>
<p>Note that the JSON file is containing about 12k lines</p>
<pre class=""lang-py prettyprint-override""><code>import json
import pandas as pd

# Replace 'your_file.json' with the path to your JSON file
with open('00_000000065.json', 'r') as file:
    data = json.load(file)

#print whole json file
#print(data)


df = pd.DataFrame(data)
#Info = df['Info']['CPU12 prog code']
#print(Info)
#print(&quot;\nPrint the whole json file as a db to see how it works&quot;)    
#print(df)

print(&quot;\nPrinting the recipes\n&quot;)
#Ricetta = df['Recipes']
#Step = df['Recipes']['Programs']['13']['Step']['007']
#print(f&quot;Print me step {Step}&quot;)

Info = df['Info']['CPU12 prog code']
print(Info)

Image = df['zImages']['0']
print(Image)


Step1 = df['Recipes']['Configs']['00']['Step']

print(Step1)

print(&quot;\nTest of the various levels\n&quot;)
Accesso = df['AccessLevels']
print(Accesso)

print(&quot;\nPrint of the counter\n&quot;)
Contatore = df['CounterEnabled']
print(Contatore)
</code></pre>
","0","Question"
"79428965","","<p>I'm trying a function in polars, and it is significantly slower than my pandas equivalent.</p>
<p>My pandas function is the following:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import time
import numpy as np

target_value = 0.5
data = np.random.rand(1000,100)
df = pd.DataFrame(data)

run_times = []
for i in range(100):
    st = time.perf_counter()
    df_filtered = df.loc[(df[0] - target_value).abs() == (df[0] - target_value).abs().min()]
    run_time = time.perf_counter() - st
    run_times.append(run_time)
print(f&quot;avg pandas run: {sum(run_times)/len(run_times)}&quot;)
</code></pre>
<p>and polars is the following</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl
import time
import numpy as np

target_value = 0.5
data = np.random.rand(1000,100)
df = pl.DataFrame(data)

run_times = []
for i in range(100):
    st = time.perf_counter()
    df = df.with_columns(abs_diff = (pl.col('column_0')-target_value).abs())
    df_filtered = df.filter(pl.col('abs_diff') == df['abs_diff'].min())
    run_time = time.perf_counter() - st
    run_times.append(run_time)
print(f&quot;avg polars run: {sum(run_times)/len(run_times)}&quot;)
</code></pre>
<p>My real datasets have 1,000 to 10,000 rows and 100 columns, and I need to filter through many different datasets. For one example of df shape (1_000, 100), I'm seeing my pandas version is magnitudes faster (0.0006s for pandas and 0.0037s for polars),  which was unexpected. Is there a more efficient way to write my polars query? Or is it just expected for pandas to outperform with smaller datasets of this size?</p>
<p>One thing to note, when I test it with 2 columns, polars is faster, and the more columns I add, the slower polars is. On the other hand, polars begins to outperform pandas after about 500_000 rows vs 100 columns.</p>
<p>Additionally in my real use case, I would need to return multiple rows that match the closest value.</p>
<p>Not sure if this is important, but for additional context, I'm running python on a linux server.</p>
","3","Question"
"79429681","","<p>I'm currently in the process  of cleaning up a large questionnaire data base.</p>
<p>I wanted to know, if in R or pandas, there was a way to graphically change the order of columns.</p>
<p>I mostly used RStudio and because I did a lot of data manipulation, now I have to reorder everything to make it logical to the human mind.</p>
<p>Is there a more intuitive way to do this than with <code>select()</code> or by dragging everything into excel?</p>
<p>Perfection would be to have a function like <code>irec()</code> or <code>iorder()</code> (from <code>questionr</code>) that launches a window in which you can click and drag columns into place. I'm tired of re-writing these long-ass variable names in select functions.</p>
","-1","Question"
"79430908","","<p>I have a <code>pandas</code> <code>DataFrame</code> <code>df</code> that has very many columns, including some named &quot;S-xx&quot; with xx ranging from 1 to 20. All these 20 columns contain labels; let's say they're A,B,C and N.  What I want to do is remove all those rows of <code>df</code> that contain label N in any of the S-xx columns.  A tiny example:</p>
<pre><code>import pandas as pd

data = {&quot;Subject&quot;: [&quot;101&quot;, &quot;102&quot;, &quot;201&quot;, &quot;202&quot;],
        &quot;S-1&quot;: [A, N, N, B],
        &quot;S-2&quot;: [B, A, N, B],
        &quot;S-3&quot;: [A, C, B, N], ... &quot;S-20&quot;: [C, A, N, N]}

df = pd.DataFrame(data)
df.set_index(&quot;Subject&quot;)
</code></pre>
<p>Which looks something like this when tabulated:</p>
<pre><code>         S-1  S-2  S-3 ... S-20
Subject            
101       A    B    A  ...  C
102       N    A    C  ...  A
201       N    N    B  ...  N
202       B    B    N  ...  N
</code></pre>
<p>I would like to only keep rows in which none of the columns <code>S-x</code> have value N.</p>
<p>Of course I can write the usual <code>df[df[&quot;S-1&quot;]!=N &amp; ... ]</code> but since I have many <code>S-x</code> columns, I wonder if there exists a better, more elegant <code>pandas</code> way of doing the same condition on all columns with name <code>S-x</code> and then gathering the results.</p>
","1","Question"
"79431751","","<p>I'm aiming to subset a df where the first two string values in a list are the same between two separate columns. Example, the list outlined in <code>first_2</code> display the values I'm interested in returning. When these values are found between <code>Letters</code> and <code>Value</code>, I want to subset these rows.</p>
<p>However, I don't want the rows where AB and DA are found. I'm only after an identical match.</p>
<pre><code>df = pd.DataFrame({
    'Letters':('AB','BD','AB','DA','EG','FA'),
    'Value':('AB','BC','DA','DA','EH','FA'),
    'Position':(1,np.nan,3,4,np.nan,6),
})

first_2 = ['AB','DA']

df1 = df[(~df['Letters'].str[0:1].isin(first_2)) &amp; (df['Value'].isin(first_2))]
</code></pre>
<p>intended:</p>
<pre><code>Letters Value  Position
0      AB    AB       1.0
3      DA    DA       4.0
</code></pre>
","0","Question"
"79431885","","<p>I have the following DataFrame:</p>
<pre><code>df = pd.DataFrame({
    &quot;One_X&quot;: [1.1, 1.1, 1.1],
    &quot;One_Y&quot;: [1.2, 1.2, 1.2],
    &quot;Two_X&quot;: [1.11, 1.11, 1.11],
    &quot;Two_Y&quot;: [1.22, 1.22, 1.22]
})
</code></pre>
<p>I want to split df.columns into a MultiIndex where:</p>
<ul>
<li>Level 0: &quot;One&quot;, &quot;Two&quot;</li>
<li>Level 1: &quot;X&quot;, &quot;Y&quot;</li>
</ul>
<p>I used the following code (which is also in the Pandas cookbook documentation):</p>
<pre><code>df.columns = pd.MultiIndex.from_tuples([tuple(c.split(&quot;_&quot;)) for c in df.columns])
</code></pre>
<p>However, I get the following error:</p>
<p>AttributeError: 'tuple' object has no attribute 'split'</p>
<p>How can I fix this issue please?</p>
<p>Note: AI generated he same code too.</p>
<p>Thanks in advance for your help!</p>
","1","Question"
"79432200","","<p>** working on the 'New York Taxi trip' Project in BigQuery in GCP.</p>
<p>** the data has 2 columns:- pickup time and dropoff time [lets consider df variable]</p>
<p>** df.info() states the format of the two columns as 'timestamp[us, tz=UTC][pyarrow]'</p>
<p>** The format does not allow me to get the 'trip_duration' value which we get after
the following operation:</p>
<p>`       dropoff - pickup [subtaction is not valid]</p>
<pre><code>   *&quot;Cannot subtract dtypes timestamp[us, tz=UTC][pyarrow] and timestamp[us, tz=UTC][pyarrow]&quot;*
</code></pre>
<p>`</p>
<p>** I have used the following:
<code> df['dropoff'] = pd.to_datetime(data['dropoff']) df['pickup'] = pd.to_datetime(data['pickup'])</code></p>
<p>to convert to 'datetime64[ns]' format but in vain.</p>
<p>** Need Suggestions.</p>
<p>** I have used the following:</p>
<p><code>     df['dropoff'] = pd.to_datetime(data['dropoff']) df['pickup'] = pd.to_datetime(data['pickup'])</code></p>
<p>to convert to 'datetime64[ns]' format but in vain.</p>
<p>**</p>
<p>from datetime import datetime</p>
<p><code> ...datetime.date(df['dropoff']) ...datetime.fromtimestamp(df['dropoff']) ...df['dropoff'].dt.tz_localize(None)</code></p>
<p>None of them work</p>
","0","Question"
"79432346","","<p>I would like to create three new columns in an existing pandas dataframe (<code>df</code>) by using the same <code>lambda</code> function on the same column only with different input. I have succeeded that by the following lines of code. Nevertheless, I would like to know whether there is a quicker way to produce the same output with fewer lines of code:</p>
<pre><code>df[&quot;1d&quot;] = (
        df
        .groupby(cols)[&quot;ln&quot;]
        .apply(lambda x: x - x.shift(1))
        .values
)
df[&quot;5d&quot;] = (
        df
        .groupby(cols)[&quot;ln&quot;]
        .apply(lambda x: x - x.shift(5))
        .values
)
df[&quot;30d&quot;] = (
        df
        .groupby(cols)[&quot;ln&quot;]
        .apply(lambda x: x - x.shift(30))
        .values
)
</code></pre>
","2","Question"
"79432482","","<p><img src=""https://i.sstatic.net/lAgpX79F.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.sstatic.net/pzs0rcMf.png"" alt=""enter image description here"" /></p>
<p>The column name of the first column ('title') is not read by pandas. In spite of the column name being there, while displaying the list of columns, in the output name of the first column is missing.</p>
<p><code>print(news.columns)</code> misses the name of first column</p>
","-1","Question"
"79432928","","<p>I am trying to parse some csv files with a few thousand rows. The csv is structured with comma as a delimiter and quotation marks enclosing each field. I want to use pd.read_csv() to parse the data without skipping the faulty lines with on_bad_line='pass' argument.</p>
<p>Example:</p>
<pre><code>&quot;Authors&quot;,&quot;Title&quot;,&quot;ID&quot;,&quot;URN&quot;
&quot;Overflow, Stack, Doe, John Society of overflowers &quot;Stack&quot; (something) &quot;,&quot;50 years of overflowing in &quot;Stack&quot; : 50 years of stacking---&quot;,&quot;&quot;117348359&quot;,&quot;URN:ND_649C52T1A9K1JJ51&quot;
&quot;Tyson, Mike&quot;,&quot;Me boxing&quot;,&quot;525166266&quot;,&quot;URN:SD_95125N2N3523N5KB&quot;
&quot;Smith, Garry&quot;,&quot;&quot;Funny name&quot; : a book about names&quot;,&quot;951992851&quot;,&quot;URN:ND_95N5J2N352BV525N25&quot;
&quot;The club of clubs&quot;,&quot;My beloved, the &quot;overflow stacker&quot;&quot;,&quot;9551236651&quot;,&quot;URN:SD_955K2B61J346F1N25&quot;
</code></pre>
<p>I have tried to illustrate the problematic structure of the csv file. In the example above, only the second row would get parsed without problems and others would have issues because of enclosed quotation marks or commas within the field bounds.</p>
<p>When I ran the script with the following command:</p>
<p><code>df = pd.read_csv(path, engine='python', delimiter=&quot;,&quot;, quotechar='&quot;', encoding=&quot;utf-8&quot;, on_bad_lines='warn')</code></p>
<p>I get errors on problematic lines: ParserWarning: Skipping line 1477: ',' expected after '&quot;'
Similar error when I try the default engine or pyarrow.</p>
<p>Now, what I want to accomplish is pass a handler function to the on_bad_lines= argument that would skip the line if it can not be parsed but at the same time store the values that are found in a new variable as a dictionary or a list so I can manually add that data later in the code.
I have tried this function and passed it as a value for on_bad_lines= but the bad_lines list just ended up empty anyway:</p>
<pre><code>bad_lines = []
def handle_bad_lines(bad_line):
    print(f&quot;Skipping bad line: {bad_line}&quot;)
    bad_lines.append(bad_line) 
    return None
</code></pre>
<p>Thank you.</p>
","1","Question"
"79433460","","<p>I have experimented and read the documentation for <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html"" rel=""nofollow noreferrer"">DataFrameGroupBy.aggregate</a>, but it is not clear to me whether - and how I can define an aggregation that works on multiple columns. It seems to me like aggregations specified with keyword argument assignment to new columns only work on single columns as input.</p>
<p>If I have a simple data frame like:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({&quot;value1&quot;: range(8), &quot;value2&quot;: range(7, -1, -1), &quot;Sample&quot;: [1, 2]*4, &quot;Year&quot;: list(range(2023, 2027))*2})
</code></pre>
<p>I would like to be able to do something like:</p>
<pre class=""lang-py prettyprint-override""><code>df.groupby([&quot;Sample&quot;, &quot;Year&quot;]).agg(agg1 = lambda group: (group[&quot;value1&quot;] * group[&quot;value2&quot;]).mean())
</code></pre>
<p>where I was hoping my callable would be given each entire group as a sub-DataFrame, but this does not work.</p>
<p>Is it possible to do this kind of aggregation and how?</p>
","2","Question"
"79433821","","<p>I'm struggling with the right combo of <a href=""https://piso.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">piso</a> functions to perform the following analysis:</p>
<p>Let's say I have a length of road with mile markers as such:</p>
<p>0---1---2---3---4---5</p>
<p>And let's say I have a record of inspections like so:</p>
<p>(mile 0 to 5 inspected 1/1/2025)</p>
<p>(mile 2 to 5 inspected 1/8/2025)</p>
<p>(mile 0 to 3 inspected 1/15/2025)</p>
<p>(mile 0 to 2 inspected 1/22/2025)</p>
<p>The table/dataframe &quot;all_df&quot; representing these inspections would look like:</p>
<pre><code>import pandas as pd

all_df = pd.DataFrame(
    data=(
        (0, 5, pd.Timestamp(&quot;2025-1-1&quot;)),
        (2, 5, pd.Timestamp(&quot;2025-1-8&quot;)),
        (0, 3, pd.Timestamp(&quot;2025-1-15&quot;)),
        (0, 2, pd.Timestamp(&quot;2025-1-22&quot;)),
    ),
    columns = [&quot;From_Mi&quot;, &quot;To_Mi&quot;, &quot;Date&quot;]
)
</code></pre>
<p>The desired dataframe &quot;recent_df&quot; showing only most recent inspections would look like:</p>
<pre><code>| From_MI | To_Mi | Last Date |
| ------- | ----- | --------- |
| 0       | 2     | 1/22/2025 |
| 2       | 3     | 1/15/2025 |
| 3       | 5     | 1/8/2025  |
</code></pre>
<p>Would this be some operation involving .split() and .intersection()?</p>
<p>Any help is appreciated!</p>
","1","Question"
"79434242","","<p>I have two dataframes like below. Some of the <code>area1_name</code> and <code>area2_name</code> overlap, and I'm trying to combine the two area names into one long list.</p>
<pre><code>df1 = pd.DataFrame({'area1_index': [0,1,2,3,4,5], 'area1_name': ['AL','AK','AZ','AR','CA','CO']})

df2 = pd.DataFrame({'area2_index': [0,1,2,3,4,5,6], 'area2_name': ['MN','AL','CT','TX','AK','AR','CA']})
</code></pre>
<p>What I want eventually is this:</p>
<pre><code>final = pd.DataFrame({'area1_index': [nan,0,nan,nan,1,2,3,4,5], 'area1_name': [nan,'AL',nan,nan,'AK','AZ','AR','CA','CO'], 'area2_index': [0,1,2,3,4,nan,5,6,nan], 'area2_name':['MN','AL','CT','TX','AK',nan,'AR','CA',nan]})
</code></pre>
<p>My first thought was to identify the overlapping area names, join the overlapping dataframe and the missing dataframe, like below:</p>
<pre><code>df1_df2_overlap = pd.DataFrame({'area1_index': [0,1,3,4], 'area2_index': [1,4,5,6], 'area1_name': ['AL','AK','AR','CA']})
df2_missing = pd.DataFrame({'area2_index': [0,2,3], 'area2_name': ['MN','CT','TX']})

df3 = pd.merge(df1, df2, &quot;outer&quot;)
df4 = pd.merge(df3, df2_missing, &quot;outer&quot;)
</code></pre>
<p>But this sorts everything by <code>area2_index</code>. I tried adding the <code>.sort_values()</code> argument using the <code>by = ['seq2_index', 'seq1_index']</code> too but had the same result. How can I order this the way I want? Or is there a better way to combine <code>df1</code> and <code>df2</code> without having to identify the overlapping/missing components?</p>
","2","Question"
"79434402","","<p>Im trying to implode by two groups &quot;Test&quot; and &quot;Name&quot;.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Test</th>
<th>Subject</th>
<th>Scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>Test1</td>
<td>Math</td>
<td>75</td>
</tr>
<tr>
<td>Test1</td>
<td>Math</td>
<td>60</td>
</tr>
<tr>
<td>Test1</td>
<td>Math</td>
<td>34</td>
</tr>
<tr>
<td>Test2</td>
<td>History</td>
<td>28</td>
</tr>
<tr>
<td>Test2</td>
<td>History</td>
<td>90</td>
</tr>
<tr>
<td>Test2</td>
<td>History</td>
<td>76</td>
</tr>
<tr>
<td>Test2</td>
<td>History</td>
<td>55</td>
</tr>
<tr>
<td>Test3</td>
<td>English</td>
<td>33</td>
</tr>
<tr>
<td>Test3</td>
<td>English</td>
<td>89</td>
</tr>
<tr>
<td>Test3</td>
<td>English</td>
<td>54</td>
</tr>
</tbody>
</table></div>
<p>I wanted to look like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Test</th>
<th>Subject</th>
<th>Scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>Test1</td>
<td>Math</td>
<td>75,60,34</td>
</tr>
<tr>
<td>Test2</td>
<td>History</td>
<td>28,90,76,55</td>
</tr>
<tr>
<td>Test3</td>
<td>English</td>
<td>33,89,54</td>
</tr>
</tbody>
</table></div>
<p>However I use this code</p>
<pre><code>Testdf = Test.groupby(['Name', 'Test'])['Score'].apply(
    lambda x: x.tolist()[0]
).reset_index()
</code></pre>
<p>And the results only gives me the first score of the list, but not the others.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Test</th>
<th>Name</th>
<th>Scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>Test1</td>
<td>Math</td>
<td>75</td>
</tr>
<tr>
<td>Test2</td>
<td>History</td>
<td>28</td>
</tr>
<tr>
<td>Test3</td>
<td>English</td>
<td>33</td>
</tr>
</tbody>
</table></div>
<p>I have tried different variations, it seems to be a problem because they are integers or inside AWS.</p>
<p>Edit: Code from that folks recommended works on local but not on AWS. Still giving me one data point for scores.</p>
","-1","Question"
"79434429","","<p>I have a dataframe with 6 columns: 'Name', 'A', 'B', 'C', 'Val', 'Category'</p>
<p>It looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>Name   A     B     C   Val   Category
 x    1.1   0     0.2  NA    NA
 y    0     0.1   0    NA    NA
 z    0.5   0.1   0.3  NA    NA
</code></pre>
<p>I want to expand the dataframe such that for each value that is not 0 in columns 'A', 'B', 'C' you get an extra row. The column 'Val' is assigned the non-zero value that led to the expansion and the 'Category' is arbitrarily based on where the value came from.</p>
<p>The result should look like this:</p>
<pre class=""lang-py prettyprint-override""><code>Name   A    B     C    Val   Category
 x    1.1   0     0.2  1.1   first
 x    1.1   0     0.2  0.2   third
 y    0     0.1   0    0.1   second
 z    0.5   0.1   0.3  0.5   fisrt
 z    0.5   0.1   0.3  0.1   second
 z    0.5   0.1   0.3  0.3   third
</code></pre>
<p>This is probably the wrong approach, but I thought since I only have three columns I should be repeating all the rows 3 times by using the repeat function on the index and then looping through the rows based on a for loop with a skip to apply 3 functions to assign the target and AICN all rows and then dropping rows where the target is 0.</p>
<pre class=""lang-py prettyprint-override""><code>def targeta(row):
    target = row
    val = 'first'
    return target, val

def targetb(row):
    target = row
    val = 'second'
    return target, val

def targetc(row):
    target = row
    val = 'third'
    return target, val

df_repeat = df.loc[df.index.repeat(3)]

for i in range(1,len(df_repeat)-3,3):
    df_repeat.iloc[i][['Target','Category']]=targeta(df_repeat.iloc[i]['A'])
    df_repeat.iloc[i+1][['Target','Category']]=targetb(df_repeat.iloc[i+1]['B'])
    df_repeat.iloc[i+2][['Target','Category']]=targetc(df_repeat.iloc[i+2]['C'])
</code></pre>
<p>I only got to this point and realized I am getting an empty dataframe. Any suggestions on what to do?</p>
","1","Question"
"79434541","","<p>I'm trying to create the &quot;Related Quantities&quot; column of a dataframe given the existing &quot;Item&quot;, &quot;Quantity&quot;, and &quot;Related Items&quot; columns.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Item</th>
<th>Quantity</th>
<th>Related Items</th>
<th>Related Quantities</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Flowers</td>
<td>1</td>
<td>['Bushes']</td>
<td>{'Bushes': 2}</td>
</tr>
<tr>
<td>1</td>
<td>Bushes</td>
<td>2</td>
<td>['Flowers']</td>
<td>{'Flowers': 1}</td>
</tr>
<tr>
<td>2</td>
<td>Cars</td>
<td>3</td>
<td>['Trucks', 'Motorcycles']</td>
<td>{'Trucks': 4, 'Motorcycles': 5}</td>
</tr>
<tr>
<td>3</td>
<td>Trucks</td>
<td>4</td>
<td>['Cars', 'Motorcycles']</td>
<td>{'Cars': 3, 'Motorcycles': 5}</td>
</tr>
<tr>
<td>4</td>
<td>Motorcycles</td>
<td>5</td>
<td>['Cars', 'Trucks']</td>
<td>{'Cars': 3, 'Trucks': 4}</td>
</tr>
</tbody>
</table></div>
<p>The values in the dictionaries will be used as the input of a function to generate another column later</p>
<hr />
<p>I believe I have a line that will make the dictionary for a single row, which I could use to fill out the column using <code>iterrows()</code> and a for loop:</p>
<pre class=""lang-none prettyprint-override""><code>dictionary = {item : df.loc[df['Item'] == item,['Quantity']][idx] for item in related_items_list}
</code></pre>
<p>(where <code>idx</code> is something to grab the corresponding index of the one row left after filtering by <code>'Item'</code>, and <code>related_items_list</code> is the value grabbed from the <code>'Related Items'</code> column of the current row in the loop)</p>
<p>But I'm trying to make something with <code>df.apply()</code> instead, in hopes that it will be more performant. Is there some way to allow the function called in <code>apply()</code> to access the whole dataframe instead of just the row passed to it?</p>
<p>I think I may be way overcomplicating this. Is trying to use <code>apply()</code> for performance a waste of time? Should I just be using a loop to feed the <code>'Quantity'</code> data into the function directly instead of making this column at all? I worry that will also hurt performance</p>
<h1>EDIT</h1>
<p>Thank you. It looks like to_dict() is somewhat faster than making the lookup dict using a for loop.</p>
<pre><code>number of rows in df:
704400

Testing with 100 iterations:
time of 'for item,qty in zip of two columns' method:
19.455784299999998

time of 'df.set_index(col1)[col2].to_dict()' method:
11.409341199999997
</code></pre>
","2","Question"
"79434994","","<p>I have an excel file, with 4 columns, namely &quot;Item&quot;, &quot;Size&quot;, &quot;Price&quot;, &quot;Quantity&quot;.</p>
<p>I would like to combine all the items with same value in &quot;Item&quot;, &quot;Size&quot;, &quot;Price&quot;, and sum up their &quot;Quantity&quot; into a single entry, and keep the sorting order.</p>
<p>For example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Item.</th>
<th>Size.</th>
<th>Price</th>
<th>Quantity</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>2-3cm</td>
<td>0.5</td>
<td>10</td>
</tr>
<tr>
<td>A</td>
<td>1-2cm</td>
<td>0.6</td>
<td>20</td>
</tr>
<tr>
<td>B</td>
<td>2cm</td>
<td>0.7</td>
<td>30</td>
</tr>
<tr>
<td>A</td>
<td>1-2cm</td>
<td>0.6</td>
<td>40</td>
</tr>
</tbody>
</table></div>
<p>Desired Output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Item.</th>
<th>Size.</th>
<th>Price</th>
<th>Quantity</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>2-3cm</td>
<td>0.5</td>
<td>10</td>
</tr>
<tr>
<td>A</td>
<td>1-2cm</td>
<td>0.6</td>
<td>60</td>
</tr>
<tr>
<td>B</td>
<td>2cm</td>
<td>0.7</td>
<td>30</td>
</tr>
</tbody>
</table></div>
<p>I tried to use groupby method, but the results seem not correct.
Any idea to solve this in excel365 + Python Pandas?</p>
","1","Question"
"79435496","","<p>I have two DataFrames:</p>
<ul>
<li><code>A</code>: Contains unique <code>(A1, A2)</code> pairs and a column <code>D</code> with numerical values.</li>
<li><code>B</code>: Contains <code>(A1, A2)</code> pairs, but allows duplicates.</li>
</ul>
<p>I need to efficiently map column <code>D</code> from <code>A</code> to <code>B</code> based on the <code>(A1, A2)</code> keys.</p>
<p>Currently, I’m using the following Pandas approach:</p>
<pre><code>import pandas as pd


A = pd.DataFrame({
    'A1': [1, 2, 3],
    'A2': ['X', 'Y', 'Z'],
    'D': [10, 20, 30]
})


B = pd.DataFrame({
    'A1': [2, 3, 4, 2],
    'A2': ['Y', 'Z', 'W', 'Y'],
})

B = B.merge(A, how='left', on=['A1', 'A2'], suffixes=('', '_A'))
B.drop(columns=[col for col in B.columns if col.endswith('_A')], inplace=True)

print(B)
</code></pre>
<p>gives the output of <code>B</code> filled with <code>D</code></p>
<pre><code>   A1 A2     D
0   2  Y  20.0
1   3  Z  30.0
2   4  W   NaN
3   2  Y  20.0
</code></pre>
<p><strong>Concerns</strong>:</p>
<p>I am looking a faster way to achieve the same mapping other than using <code>merge</code>. The output should retain all rows from B, filling missing values from A where applicable. One of the drawback of this approach is to remove unnecessary columns due to left join to make it compatible for my downstream code.</p>
<p><strong>What I’ve Tried:</strong></p>
<p>Using <code>update()</code>, but it doesn’t work well with multi-key joins.</p>
<p><strong>Question:</strong></p>
<p>Is there a more efficient way to map <code>D</code> from <code>A</code> to <code>B</code> faster without unnecessary column operations?</p>
","0","Question"
"79435770","","<p>I found this post which initially seemed to be exactly what I was looking for but it didn't help me:
<a href=""https://stackoverflow.com/questions/64139905/adding-header-and-footer-to-json-output-from-python"">Adding Header and Footer to JSON output from Python</a></p>
<p>I have a csv file which I read in as Pandas dataframe:</p>
<pre><code>import os
import csv
import json
import pandas as pd
csvFilePath = &quot;Mypath&quot;
track = pd.read_csv(csvFilePath, header = 0, skiprows = 0, delimiter = &quot;;&quot;)
</code></pre>
<p>The example csv looks like this:</p>
<pre><code>Param1;Param2;name;lat;lon;altitude;vert_rate;track;speed;category;Param3;Param4;Param5
999999;9999;rocket;57.878696;11.160667;1089;0;137;2;99;999;16;0
999999;9999;rocket;57.878796;11.160668;2543.963336;152638.0483;137;2;99;999;15;0
999999;9999;rocket;57.878896;11.160670;4226.050004;126781.7063;137;2;99;999;14;0
999999;9999;rocket;57.878796;11.160669;6091.207544;121824.349;137;2;99;999;13;0
999999;9999;rocket;57.878696;11.160667;8098.097372;121471.6581;137;2;99;999;12;0
</code></pre>
<p>Now I would like to safe this dataframe with an additional header as a JSON file:
The additional header looks like this dictionary:</p>
<pre><code>headlines={
           &quot;now&quot;: 1636008051.9,
           &quot;messages&quot;: 6236,
           }
</code></pre>
<p>The aim JSON should contain the information given by &quot;headlines&quot; (but without its name) and the content of the dataframe:</p>
<pre><code>{
  &quot;now&quot;: 1636008051.9,
  &quot;messages&quot;: 6236,
  &quot;track&quot;: [
    {      
      &quot;Param1&quot;: 999999,
      &quot;Param2&quot;: &quot;9999&quot;,
      &quot;name&quot;: &quot;rocket&quot;,
      &quot;lat&quot;:  57.878696,
      &quot;lon&quot;: 11.160667,
      &quot;altitude&quot;: 1089,
      &quot;vert_rate&quot;: 0,
      &quot;track&quot;: 137,
      &quot;speed&quot;: 2,
      &quot;category&quot;: 99,
      &quot;Param3&quot;: 999,
      &quot;Param4&quot;: 16,
      &quot;Param5&quot;: 0
    }
    {      
      &quot;Param1&quot;: 999999,
      &quot;Param2&quot;: &quot;9999&quot;,
      &quot;name&quot;: &quot;rocket&quot;,
      &quot;lat&quot;:  57.878796,
      &quot;lon&quot;: 11.160668,
      &quot;altitude&quot;: 2543.963336,
      &quot;vert_rate&quot;: 152638.0483,
      &quot;track&quot;: 137,
      &quot;speed&quot;: 2,
      &quot;category&quot;: 99,
      &quot;Param3&quot;: 999,
      &quot;Param4&quot;: 15,
      &quot;Param5&quot;: 0
    }
    {      
      &quot;Param1&quot;: 999999,
      &quot;Param2&quot;: &quot;9999&quot;,
      &quot;name&quot;: &quot;rocket&quot;,
      &quot;lat&quot;:  57.878896,
      &quot;lon&quot;: 11.160670,
      &quot;altitude&quot;: 4226.050004,
      &quot;vert_rate&quot;: 126781.7063,
      &quot;track&quot;: 137,
      &quot;speed&quot;: 2,
      &quot;category&quot;: 99,
      &quot;Param3&quot;: 999,
      &quot;Param4&quot;: 14,
      &quot;Param5&quot;: 0
    }
    {...and so on...}
  ]
}
</code></pre>
<p>The dataframe itself I can simply turn to JSON like that:</p>
<pre><code>json = track.to_json(path_out + &quot;result.json&quot;, orient='records')
</code></pre>
<p>but here I don't know how to add the preceding lines from the &quot;header&quot; dict
How can I join the dictionary and the csv to output the JSON? Or is there a simpler way? Or any hint to a post which I didn't find?
I need to do it in pandas as the csv-dataframe will be further needed.</p>
","1","Question"
"79436039","","<p>I have a dataframe that contains 1681 evenly distributed 2D grid points. Each data point has its x and y coordinates, a label representing its category (or phase), and a color for that category.</p>
<pre><code>         x     y      label    color
0    -40.0 -30.0         Fe  #660066
1    -40.0 -29.0         Fe  #660066
2    -40.0 -28.0        FeS  #ff7f50
3    -40.0 -27.0        FeS  #ff7f50
4    -40.0 -26.0        FeS  #ff7f50
...    ...   ...        ...      ...
1676   0.0   6.0  Fe2(SO4)3  #8a2be2
1677   0.0   7.0  Fe2(SO4)3  #8a2be2
1678   0.0   8.0  Fe2(SO4)3  #8a2be2
1679   0.0   9.0  Fe2(SO4)3  #8a2be2
1680   0.0  10.0  Fe2(SO4)3  #8a2be2

[1681 rows x 4 columns]
</code></pre>
<p>I want to generate a polygon diagram that shows the linear boundary of each category (in my case also known as a &quot;phase diagram&quot;). Sor far I can only show this kind of diagram in a simple scatter plot like this:</p>
<pre><code>import matplotlib.pyplot as plt
import pandas as pd

plt.figure(figsize=(8., 8.))
for color in df.color.unique():
    df_color = df[df.color==color]
    plt.scatter(
            x=df_color.x,
            y=df_color.y,
            c=color,
            s=100,
            label=df_color.label.iloc[0]
    )
plt.xlim([-40., 0.])
plt.ylim([-30., 10.])
plt.xlabel('Log pO2(g)')
plt.ylabel('Log pSO2(g)')
plt.legend(bbox_to_anchor=(1.05, 1.))
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/3PceH2lD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3PceH2lD.png"" alt=""enter image description here"" /></a>
However, what I want is a phase diagram with clear linear boundaries that looks something like this:
<a href=""https://i.sstatic.net/0ko5IPCY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0ko5IPCY.jpg"" alt=""enter image description here"" /></a></p>
<p>Is there any way I can generate such phase diagram using <code>matplotlib</code>? Note that the boundary is not deterministic, especially when the grid points are not dense enough. Hence there needs to be some kind of heuristics, for example the boundary line should always lie in the middle of two neighboring points with different categories. I imagine there will be some sort of line fitting or interpolation needed, and <code>matplotlib.patches.Polygon</code> is probably useful here.</p>
<p>For easy testing, I attach a code snippet for generating the data, but <strong>the polygon information shown below are not supposed to be used for generating the phase diagram</strong></p>
<pre><code>import numpy as np
import pandas as pd
from shapely.geometry import Point, Polygon

labels = ['Fe', 'Fe3O4', 'FeS', 'Fe2O3', 'FeS2', 'FeSO4', 'Fe2(SO4)3']
colors = ['#660066', '#b6fcd5', '#ff7f50', '#ffb6c1', '#c6e2ff', '#d3ffce', '#8a2be2']
polygons = []
polygons.append(Polygon([(-26.7243,-14.7423), (-26.7243,-30.0000), (-40.0000,-30.0000), 
(-40.0000,-28.0181)]))
polygons.append(Polygon([(-18.1347,-0.4263), (-16.6048,1.6135), (-16.6048,-30.0000),
(-26.7243,-30.0000), (-26.7243,-14.7423), (-18.1347,-0.4263)]))
polygons.append(Polygon([(-18.1347,-0.4263), (-26.7243,-14.7423),
(-40.0000,-28.0181), (-40.0000,-22.2917), (-18.1347,-0.4263)]))
polygons.append(Polygon([(0.0000,-20.2615), (0.0000,-30.0000), (-16.6048,-30.0000),
(-16.6048,1.6135), (-16.5517,1.6865), (-6.0517,-0.9385), (0.0000,-3.9643)]))
polygons.append(Polygon([(-14.2390,10.0000), (-14.5829,7.5927), (-16.5517,1.6865),
(-16.6048,1.6135), (-18.1347,-0.4263), (-40.0000,-22.2917), (-40.0000,10.0000)]))
polygons.append(Polygon([(-6.0517,-0.9385), (-16.5517,1.6865), (-14.5829,7.5927),
(-6.0517,-0.9385)]))
polygons.append(Polygon([(0.0000,-3.9643), (-6.0517,-0.9385), (-14.5829,7.5927),
(-14.2390,10.0000), (0.0000,10.0000)]))

x_grid = np.arange(-40., 0.01, 1.)
y_grid = np.arange(-30., 10.01, 1.)
xy_grid = np.array(np.meshgrid(x_grid, y_grid)).T.reshape(-1, 2).tolist()
data = []
for coords in xy_grid:
    point = Point(coords)
    for i, poly in enumerate(polygons):
        if poly.buffer(1e-3).contains(point):
            data.append({
                'x': point.x,
                'y': point.y,
                'label': labels[i],
                'color': colors[i]
            })
            break
df = pd.DataFrame(data)
</code></pre>
","11","Question"
"79436352","","<p>I want to insert a column at a specific index in a Pandas DataFrame, but only assign values to certain rows. Currently, I am doing it in two steps:</p>
<pre><code>df = pd.DataFrame({'A': [1, 2, 3, 4, 5],
                  'B': [10, 20, 30, 40, 50] })
df.insert(1, 'NewCol', None)
df.loc[[1, 3], 'NewCol'] = ['X', 'Y']
</code></pre>
<p>Is there a more concise way to achieve this in a single operation?</p>
","1","Question"
"79436611","","<p>I have an excel that extracts information from a platform called SAP, when they extract the information it comes with a format of dd/mm/yyyy, but there are times that the date is extracted as dd.mm.yyyy, the thing is that when I convert that specific column to a DataFrame using python's library Pandas, the format just goes crazy.
This is the code I've been trying:</p>
<pre><code>import pandas as pd
import re
from datetime import datetime

# Convertir xlsx a csv
excel_data = pd.read_excel(&quot;Reportes/Crudos/Reporte SAP.xlsx&quot;, header=1)
excel_data['Asiento contable (Fecha de contabilización)'].to_csv(&quot;Reportes/Crudos/1.csv&quot;, index=False)
excel_data['Asiento contable (Fecha de contabilización)'].to_excel(&quot;Reportes/Crudos/1.xlsx&quot;, index=False)
# imprime solamente valores unicos sin repeticion
print(excel_data['Asiento contable (Fecha de contabilización)'].unique())
</code></pre>
<p>The print gives me this:</p>
<pre><code>[datetime.datetime(2025, 8, 1, 0, 0) datetime.datetime(2025, 9, 1, 0, 0)
 '13/01/2025' '17/01/2025' datetime.datetime(2025, 3, 1, 0, 0)
 '18/01/2025' '20/01/2025' datetime.datetime(2025, 10, 1, 0, 0)
 '14/01/2025' datetime.datetime(2025, 2, 1, 0, 0)
 datetime.datetime(2025, 6, 1, 0, 0) datetime.datetime(2025, 7, 1, 0, 0)
 datetime.datetime(2025, 11, 1, 0, 0) datetime.datetime(2025, 4, 1, 0, 0)
 '15/01/2025' '16/01/2025' datetime.datetime(2025, 12, 1, 0, 0)
 datetime.datetime(2025, 5, 1, 0, 0) '19/01/2025'
 datetime.datetime(2025, 1, 30, 0, 0) datetime.datetime(2025, 1, 31, 0, 0)
 datetime.datetime(2025, 1, 28, 0, 0) datetime.datetime(2025, 1, 23, 0, 0)
 datetime.datetime(2025, 1, 27, 0, 0) datetime.datetime(2025, 1, 29, 0, 0)
 datetime.datetime(2025, 1, 22, 0, 0) datetime.datetime(2025, 1, 24, 0, 0)
 datetime.datetime(2025, 1, 25, 0, 0) datetime.datetime(2025, 1, 21, 0, 0)
 datetime.datetime(2025, 1, 26, 0, 0)]
</code></pre>
<p>Going in the generated csv and xlsx it gives me this:</p>
<p>csv:</p>
<pre><code>1. 01/08/2025 00:00
2. 01/09/2025 00:00
3. 13/01/2025
4. 17/01/2025
5. 01/03/2025 00:00
6. 18/01/2025
7. 20/01/2025
8. 01/10/2025 00:00
9. 14/01/2025
10. 01/02/2025 00:00
11. 01/06/2025 00:00
12. 01/07/2025 00:00
13. 01/11/2025 00:00
14. 01/04/2025 00:00
15. 15/01/2025
16. 16/01/2025
17. 01/12/2025 00:00
18. 01/05/2025 00:00
19. 19/01/2025
20. 30/01/2025 00:00
21. 31/01/2025 00:00
22. 28/01/2025 00:00
23. 23/01/2025 00:00
24. 27/01/2025 00:00
25. 29/01/2025 00:00
26. 22/01/2025 00:00
27. 24/01/2025 00:00
28. 25/01/2025 00:00
29. 21/01/2025 00:00
30. 26/01/2025 00:00
</code></pre>
<p>xlsx:</p>
<pre><code>1. 2025-08-01 00:00:00
2. 2025-09-01 00:00:00
3. 13/01/2025
4. 17/01/2025
5. 2025-03-01 00:00:00
6. 18/01/2025
7. 20/01/2025
8. 2025-10-01 00:00:00
9. 14/01/2025
10. 2025-02-01 00:00:00
11. 2025-06-01 00:00:00
12. 2025-07-01 00:00:00
13. 2025-11-01 00:00:00
14. 2025-04-01 00:00:00
15. 15/01/2025
16. 16/01/2025
17. 2025-12-01 00:00:00
18. 2025-05-01 00:00:00
19. 19/01/2025
20. 2025-01-30 00:00:00
21. 2025-01-31 00:00:00
22. 2025-01-28 00:00:00
23. 2025-01-23 00:00:00
24. 2025-01-27 00:00:00
25. 2025-01-29 00:00:00
26. 2025-01-22 00:00:00
27. 2025-01-24 00:00:00
28. 2025-01-25 00:00:00
29. 2025-01-21 00:00:00
30. 2025-01-26 00:00:00
</code></pre>
<p>Which means we have 3 types of format:</p>
<ol>
<li>dd/mm/yyyy hh:mm (csv) | dd-mm-yyyy hh:mm (excel)</li>
<li>dd/mm/yyyy</li>
<li>mm/dd/yyyy hh:mm (csv) | mm-dd-yyyy hh:mm (excel)</li>
</ol>
<p>If we make an analysis, we get that from January 1 to 12 they come out with this format:
cell: mm/dd/yyyy hh:mm
formula bar: mm/dd/yyyy hh:mm:ss a. m.</p>
<p>from 13-20 come with this format:
cell: dd/mm/yyyy
formula bar: dd/mm/yyyy</p>
<p>and from 21-31 they come with this format:
cell: dd/mm/yyyy hh:mm
Formula bar: dd/mm/yyyy hh:mm:ss a. m.</p>
<p>I've tried making a:</p>
<pre><code>df[&quot;Asiento contable (Fecha de contabilización)&quot;] = pd.to_datetime(df[&quot;Asiento contable (Fecha de contabilización)&quot;], dayfirst=True)
</code></pre>
<p>But it doesnt recognize the dates well and I end with no data between the 1st and the 12th of January</p>
<p>I want to know if there is a way to unify this 3 types of format into just one: dd/mm/yyyy</p>
","1","Question"
"79439150","","<p>I am working in databricks where I read a json file from s3 and need to convert it to a string.
The json file has multiple layers to it. For the demo, lets say it looks like so:</p>
<pre><code>{
  &quot;id&quot;: &quot;123&quot;,
  &quot;details&quot;:[
    {
      &quot;name&quot;: &quot;Bob&quot;,
      &quot;address&quot;: &quot;123 street&quot;
    },
    {
      &quot;name&quot;: &quot;Amy&quot;,
      &quot;address&quot;: &quot;XYZ street&quot;
    }
  ],
  &quot;docType&quot;: &quot;File&quot;,
  &quot;collections&quot;: [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]
}
</code></pre>
<p>Said json file is stored in s3 and so I read as so:</p>
<pre><code>aws_s3_bucket = 'my_bucket'
mount_name = '/mnt/test'

source_url = 's3a://%s' %(aws_s3_bucket)
dbutils.fs.mount(source_url,mount_name)

file_path = &quot;/dummyKey/dummyFile.json&quot;
df = spark.read.option(&quot;multiline&quot;,&quot;true&quot;).json(mount_name + file_path).cache()
</code></pre>
<p>This returns a dataframe with 4 columns: <code>id</code>,<code>details</code>,<code>docType</code> and <code>collections</code>. So a column for each top level field in my json file.</p>
<p>I now would like to convert this dataframe into a json string i.e. the file that is in s3 but without formating.</p>
<p>I tried to make use of <code>toJson()</code> function, as well as <code>to_json</code> from Pandas lib. But in both cases I got the following error:</p>
<pre><code>the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
</code></pre>
<p>which from what I understand means that it cannot do this as it is too complex.</p>
<p>My question is then as follows: how can I read a json file from s3 and convert it to a string? I suspect there is a way to directly read it in as a string without saving to dataframe but I just cannot find such function (provided it exists to begin with).</p>
","0","Question"
"79439971","","<p>I'm using something similar to this as <code>input.txt</code></p>
<pre><code>header
 040525 $$$$$   9999         12345
   random stuff
 040525 $$$$$   8888         12345
 040525 $$$$$   7777         12345
           random stuff
 040525 $$$$$   6666         12345
footer
</code></pre>
<p>Due to the way this input is being pre-processed, I cannot correctly use pd.read_csv. I must first create a list from the input; Then, create a DataFrame from the list.</p>
<pre><code>data_list = []
with open('input.txt', 'r') as data:
    for line in data:
        data_list.append(line.strip().split()) 
df = pd.DataFrame(data_list)
</code></pre>
<p>I only want to append lines that contain '$$$' in the second column. Desired output would be:</p>
<pre><code>       0      1     2      3
0  40525  $$$$$  9999  12345
1  40525  $$$$$  8888  12345
2  40525  $$$$$  7777  12345
3  40525  $$$$$  6666  12345
</code></pre>
","1","Question"
"79440315","","<p>Just curious if option (b) is more efficient than option (a)? At the first glance, option (a) will have several times of more operations than option (b). But I did some simulations for a million rows in <em>df</em>, option (b) is just a fraction faster on average. Does it mean the Pandas will group all the scalar operations in option (a) automatically?</p>
<p>(a) Variable a, b, c, d, e, f are all scalars.</p>
<pre><code>    df['val2'] = (a*b+c*d)*df['val1']*e/f
</code></pre>
<p>(b)</p>
<pre><code>    x = (a*b+c*d)*e/f
    df['val2'] = df['val1']*x
</code></pre>
","1","Question"
"79441449","","<p>I am using this <a href=""https://stackoverflow.com/a/51633278/27644906"">answer</a>, for a similar question. To make my question independent, I repeat the code here:</p>
<pre><code>scipy.stats.kruskal(*[group[&quot;variable&quot;].values for name, group in df.groupby(&quot;treatment&quot;)])
</code></pre>
<p>So now each class is grouped and a function (in this case kruskal test) is applied on the groups of different classes. Now my question is how to exclude classes with low sample seize! E.g., ignore classes with less than 5 samples?</p>
<p>Thank you in advance.</p>
","0","Question"
"79441884","","<p>I used these codes using the <code>groupby()</code> function to find the top averages, budgets, revenue etc. for movies as part of my Exploratory Data Analysis:</p>
<p><code>movies = df.groupby('Title')</code></p>
<p><code>movies.mean().sort_values(by='TMDB Vote Average', ascending=False).head()</code></p>
<p>This worked on older versions of Anaconda or Jupyter Notebook but I cannot understand why it is not working on the latest version of Anaconda or Jupyter Notebook when I upgraded towards this system.</p>
<p>Does anyone in the community know how I can rectify this problem? Thank you for your time.</p>
","0","Question"
"79442012","","<p>I am trying to Scrape Screener.in website to extract some information related to stocks.
However while trying to extract Quarterly Results section there are some field which is hidden and when click on + button it show additional information related to parent header. I need to have this information</p>
<p>I am using below python code which is giving me a dataframe but without additional information</p>
<pre><code>url = f'https://www.screener.in/company/TATAPOWER/consolidated/'
print(url)
req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
page = urlopen(req).read()
soup = BeautifulSoup(page, 'html.parser')
table = soup.find_all(&quot;table&quot;, {&quot;class&quot;: &quot;data-table responsive-text-nowrap&quot;})[0]
df = pd.read_html(StringIO(str(table)))[0]
df

</code></pre>
<p>Above code is working fine however I am not able to pull additional information</p>
<p>Can somebody help me with this?</p>
","1","Question"
"79442105","","<p>I have a dataframe that records the performance of F1-drivers and it looks like</p>
<pre><code>Driver_ID   Date         Place
1          2025-02-13   1
1          2024-12-31   1
1          2024-11-03   2
1          2023-01-01   1
2          2025-01-13   5
2          2024-12-02   1
2          2024-11-12   2
2          2023-11-12   1
2          2023-05-12   1
</code></pre>
<p>and I want to create a new columns <code>Total_wins</code> which counts the number of wins of the driver before today's race, so the desired column looks like</p>
<pre><code>Driver_ID   Date         Place  Total_wins
1          2025-02-13   1      2
1          2024-12-31   1      1
1          2024-11-03   2      1
1          2023-01-01   1      0
2          2025-01-13   5      3
2          2024-12-02   1      2
2          2024-11-12   2      2
2          2023-11-12   1      1
2          2023-05-12   1      0
</code></pre>
<p>And here is my code:</p>
<pre><code>win = (df.assign(Date=Date)
         .sort_values(['Driver_ID','Date'], ascending=[True,True])
         ['Place'].eq(1))

df['Total_wins']=(win.groupby(df['Driver_ID'], group_keys=False).apply(lambda g: g.shift(1, fill_value=0).cumsum()))
</code></pre>
<p>So the code works (mostly) fine. I used mostly because I checked the result manually and most of the results are correct, but for a few rows, it gives wrong results like</p>
<pre><code>Driver_ID   Date         Place  Total_wins
1          2025-02-13   1      2
1          2024-12-31   1      4
1          2024-11-03   2      1
1          2023-01-01   1      0
</code></pre>
<p>I tried to debug it but I couldn't find anything wrong. Is there any subtle mistake in my code that might have caused the mistake? Or what is the possible reason for this? My original dataframe is huge (~150000 rows)</p>
<p>Thank you so much in advance</p>
","2","Question"
"79443476","","<p>I want to create a constant volume chart in python. Here is an example with a constant volume of 50 and some sample data:</p>
<pre><code>import pandas as pd
import numpy as np
date_rng = pd.date_range(start='2024-01-01', end='2024-12-31 23:00:00', freq='h')

# Create a dataframe with the date range
df = pd.DataFrame(date_rng, columns=['timestamp'])

# Add the 'price' column with random floating numbers between 70 and 100
df['price'] = np.round(np.random.uniform(70, 100, size=(len(date_rng))), 2)

# Add the 'volume' column with random integers between 1 and 10
df['volume'] = np.random.randint(1, 11, size=(len(date_rng)))

constantvolume = 50
df['cumsum'] = np.cumsum(df['volume'])
df['mod'] = df['cumsum']/  constantvolume
df['whole'] = np.ceil(df['mod'])
df['next_num'] = df['whole'].shift(-1) - df['whole']
df['mod2'] = df[df['next_num'] &gt; 0]['cumsum'] % constantvolume
df['mod2'] = df['mod2'].fillna(0)
dfa = df.groupby(df['whole']).agg({'price': ['min', 'max', 'last', 'first'], 'timestamp': 'first', 'volume': 'sum'})
dfa.columns = ['low', 'high', 'close', 'open', 'timestamp', 'volume']
dfa['timestamp'] = pd.to_datetime(dfa['timestamp'])
dfa.set_index('timestamp', inplace=True)
dfa
</code></pre>
<p>Now this is very close to what I want to do. The only issue is that the volume in each row is not exactly the defined quantity of 50 because the cumsum doesnt always add to 50.</p>
<p>So what I would have to do is where next_num &gt;0, see if there is the volume = to the defined constant volume, if yes good if no then split the next row with the same timestamp and same price but split the volume in two parts so that the mod is zero and then move on.</p>
<p>The desired result is that in the final dataframe the volume = constantvolume in all rows exactly, with the exception of the last row where it could be different.</p>
<p>The only way I can think of is a loop which I dont think is the best way and will be very slow as the actual dataframe as 1mn rows...</p>
","0","Question"
"79444668","","<p>I have the following code that produces a combo chart of multiple lines (time series) and a total sum in bars:</p>
<pre><code>import plotly.graph_objects as go
import pandas as pd

data = {
    &quot;Date&quot;: pd.date_range(start=&quot;2024-01-01&quot;, periods=10, freq=&quot;D&quot;),
    &quot;Category A&quot;: [10, 15, 13, 17, 14, 19, 21, 18, 22, 24],
    &quot;Category B&quot;: [5, 8, 7, 9, 10, 11, 12, 14, 13, 15],
    &quot;Category C&quot;: [7, 9, 6, 10, 12, 13, 15, 16, 18, 20]
}

df = pd.DataFrame(data)
df[&quot;Total&quot;] = df.iloc[:, 1:].sum(axis=1)

fig = go.Figure()

fig.add_trace(go.Bar(
    x=df[&quot;Date&quot;], 
    y=df[&quot;Total&quot;], 
    name=&quot;Total&quot;, 
    marker=dict(color=&quot;lightgray&quot;, opacity=0.5),
    yaxis=&quot;y1&quot;  
))

for col in df.columns[1:-1]:  
    fig.add_trace(go.Scatter(
        x=df[&quot;Date&quot;], 
        y=df[col], 
        mode=&quot;lines+markers&quot;, 
        name=col,
        yaxis=&quot;y2&quot; 
    ))

fig.update_layout(
    title=&quot;Dual-Axis Combo Chart: Time Series + Total Sum&quot;,
    xaxis_title=&quot;Date&quot;,
    
    yaxis=dict(
        title=&quot;Total Sum&quot;, 
        side=&quot;left&quot;,
        showgrid=False
    ),
    
    yaxis2=dict(
        title=&quot;Category Values&quot;, 
        side=&quot;right&quot;,
        overlaying=&quot;y&quot;,
        showgrid=False
    ),
    
    barmode=&quot;overlay&quot;, 
    template=&quot;plotly_white&quot;
)

fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/Wvzx7uwX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wvzx7uwX.png"" alt=""Dual axis combo chart"" /></a></p>
<p>I would like the totals in the grey bar to update based on the selected series on the legend.
As can be seen in the image below, I unclick categories B and C and the bar with the totals doesn't update.
<a href=""https://i.sstatic.net/2uyXe7M6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2uyXe7M6.png"" alt=""combo chart unclicked series"" /></a></p>
<p>I understand this is the expected behaviour, but I couldn't figure out a way of getting the totals bar to update based on the legend series being selected/unselected.</p>
<p>Is there a way of achieving this in a jupyter-notebook? I understand with Dash it should be possible, but my use case is for a notebook</p>
","2","Question"
"79445067","","<p>the datetime index of a pandas dataframe have groups with the same datetime (they also have the same Id number). Plus additional problem as explained below</p>
<p>here are some stock market tick data</p>
<pre><code>2025-02-14 00:00:01.192429,6143.25,2,32178,6143.25,6143.50,271611387,C,43,01,2,14
2025-02-14 00:00:01.317655,6143.25,1,32179,6143.25,6143.50,271611388,C,43,01,2,14
2025-02-14 00:00:01.317655,6143.25,1,32180,6143.25,6143.50,271611388,C,43,01,2,14
2025-02-14 00:00:01.317655,6143.25,1,32181,6143.25,6143.50,271611388,C,43,01,2,14
2025-02-14 00:00:20.222990,6143.50,1,32182,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222990,6143.50,1,32183,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222990,6143.50,1,32184,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222990,6143.50,1,32185,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32186,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32187,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32188,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:23.891463,6143.50,1,32189,6143.50,6143.75,271611391,C,43,01,2,14
2025-02-14 00:00:23.891463,6143.50,1,32190,6143.50,6143.75,271611391,C,43,01,2,14
2025-02-14 00:00:23.891463,6143.50,1,32191,6143.50,6143.75,271611391,C,43,01,2,14
2025-02-14 00:01:00.046733,6143.50,1,32192,6143.25,6143.50,271611392,C,43,01,1,14
</code></pre>
<p>as you can see there are several groups that have the same datetime (datetime is the first column), and also have the same Id number (Id is in the 7-th column).</p>
<p>What I want to do is to add the smallest time increment (which is microseconds) when the time is the same.</p>
<p>This can be done using:</p>
<pre><code>            ttt = df.groupby(df['id']).cumcount()
            df.index += pd.to_timedelta(ttt,unit='us')
</code></pre>
<p>The problem is that the difference between 2 groups with the same time is sometimes only 1 microsecond.</p>
<p>for instance here:</p>
<pre><code>2025-02-14 00:00:20.222990,6143.50,1,32182,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222990,6143.50,1,32183,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222990,6143.50,1,32184,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222990,6143.50,1,32185,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32186,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32187,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32188,6143.25,6143.50,271611390,C,43,01,1,14
</code></pre>
<p>if I would execute the code above I would get this:</p>
<pre><code>2025-02-14 00:00:20.222990,6143.50,1,32182,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32183,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222992,6143.50,1,32184,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222993,6143.50,1,32185,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32186,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:20.222992,6143.50,1,32187,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:20.222993,6143.50,1,32188,6143.25,6143.50,271611390,C,43,01,1,14
</code></pre>
<p>Then I would have a datetime overlap. I want to get this. Then there is no overlapping datetime.</p>
<pre><code>2025-02-14 00:00:20.222990,6143.50,1,32182,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222991,6143.50,1,32183,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222992,6143.50,1,32184,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222993,6143.50,1,32185,6143.25,6143.50,271611389,C,43,01,1,14
2025-02-14 00:00:20.222994,6143.50,1,32186,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:20.222995,6143.50,1,32187,6143.25,6143.50,271611390,C,43,01,1,14
2025-02-14 00:00:20.222996,6143.50,1,32188,6143.25,6143.50,271611390,C,43,01,1,14
</code></pre>
<p>any ideas how to get it done? Thanks</p>
<p>So with the help from you and other posts I made a array and a loop version. The array version needs iterations but still it is much faster. My files have like 700000 rows (per day!) and it makes a big difference to use meth = 0 in my code</p>
<pre><code>        # unbundled
        if cons == 0:

            # 2 methods. Meth 0 is an array version (faster), Meth 1 loops through the elements
            meth = 0

            if meth == 0:
                df[&quot;Time&quot;] = df.index

                for _ in range(10000):
                    dups = df.duplicated(subset=&quot;Time&quot;) &amp; ~pd.isnull(df[&quot;Time&quot;])

                    if not dups.any():
                        break

                    # Add several ms of time to each time
                    df[&quot;Time&quot;] += pd.to_timedelta(
                        df.groupby(&quot;Time&quot;).cumcount(), unit=&quot;us&quot;
                    )

                df.set_index(&quot;Time&quot;, inplace=True)  # make Time the index
                df.index.names = [&quot;date_time&quot;]  # change name back to date_time

            if meth == 1:
                df[&quot;Time&quot;] = df.index
                cn = df.columns.get_loc(&quot;Time&quot;)  # column number
                for i in range(1, len(df), 1):
                    if df.iloc[i, cn] &lt;= df.iloc[i - 1, cn]:
                        df.iloc[i, cn] = df.iloc[i - 1, cn] + pd.to_timedelta(
                            1, unit=&quot;us&quot;
                        )

                df.set_index(&quot;Time&quot;, inplace=True)  # make Time the index
                df.index.names = [&quot;date_time&quot;]  # change name back to date_time
</code></pre>
","0","Question"
"79445156","","<p>I am grouping my data as below:</p>
<pre><code>all_groups = df.groupby('age').groups
</code></pre>
<p>Printing <code>all_groups</code> shows:</p>
<pre><code>{1.0: [11, 14, 15, 22], 2.0: [12, 13, 27], 3.0: [16, 17, 19, 20, 23, 24],
6.0: [21], 7.0: [18, 25, 26], 11.0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

</code></pre>
<p>Now I want to run <code>stats.mannwhitneyu</code> on all possible combinations of two classes. In this example, I have 6 groups, therefor, 15 combinations are possible, e.g., <code>stats.mannwhitneyu(class1, class2), stats.mannwhitneyu(class1, class3), ..., stats.mannwhitneyu(class7, class11)</code>.</p>
<p>I need a general approach to do it, specially that I don't know the number of classes in advance. What is the cleanest/smartest way to do it?
Thank you in advance.</p>
","3","Question"
"79445805","","<p>I am trying to import my data from excel in to python using Pandas. I then want to plot the data.
At the moment I am struggling with blank values in my data.
Source data looks like attached image.
When I run the code as below the data printed contains 'Nan'</p>
<pre><code>import pandas as pd
df = pd.read_excel('CSTRS Chem Eng Practicals_2.xlsx', sheet_name='Flow Rates')
df = pd.DataFrame(df)
df = df.values
Time2 = df[4:,0]
Conc1 = df[4:,2]
Conc2 = df[4:,4]
Conc3 = df[4:,6]
Conc4 = df[4:,8]
Conc5 = df[4:,10]
print (Time2)
print (Conc1)
</code></pre>
<p>If I use df.notna() then I get this error message 'AttributeError: 'numpy.ndarray' object has no attribute 'notna''
Can anyone suggest a way to remove the NaN values so I am able to plot the data</p>
<p>Thanks</p>
<p>I have tried df.notna() and I have also tried slicing the data and neither works</p>
","2","Question"
"79445857","","<p>I'm using something similar to this as <code>input.txt</code></p>
<pre><code> 040525 $$$$$   9999         12345
 040525 $$$$$   8888         12345
 040525 $$$$$   7777         12345
 040525 $$$$$   6666         12345
</code></pre>
<p>Due to the way this input is being pre-processed, I cannot correctly use pd.read_csv. I must first create a list from the input; Then, create a DataFrame from the list.</p>
<pre><code>data_list = []
with open('input.txt', 'r') as data:
    for line in data:
        data_list.append(line.strip())
df = pd.DataFrame(data_list)
</code></pre>
<p>This results in each row being considered 1 column</p>
<pre><code>print(df.shape)
print(df)
print(df.columns.tolist())

(4, 1)
                                   0
0  040525 $$$$$   9999         12345
1  040525 $$$$$   8888         12345
2  040525 $$$$$   7777         12345
3  040525 $$$$$   6666         12345
[0]
</code></pre>
<p>How can I create 4 columns in this DataFrame? Desired output would be:</p>
<pre><code>(4, 4)
       a      b     c      d
0  40525  $$$$$  9999  12345
1  40525  $$$$$  8888  12345
2  40525  $$$$$  7777  12345
3  40525  $$$$$  6666  12345
['a', 'b', 'c', 'd']
</code></pre>
","1","Question"
"79446437","","<p>I have a dictionary of lists, each with a key string value (stock ticker) and value consisting of a list of dicts which looks like this:</p>
<pre><code>data
Out[88]: 
{'NVDA': [{'open': 144.75, 'high': 144.21, 'low': 174.33, 'close': 210.47},
  {'open': 123.97, 'high': 128.5, 'low': 110.25, 'close': 154.09},
  {'open': 118.19, 'high': 134.81, 'low': 104.37, 'close': 149.72},
  {'open': 225.35, 'high': 126.81, 'low': 104.77, 'close': 209.46},
  {'open': 247.2, 'high': 243.25, 'low': 220.44, 'close': 186.01}],
 'MSFT': [{'open': 175.78, 'high': 213.98, 'low': 229.75, 'close': 206.59},
  {'open': 142.98, 'high': 168.42, 'low': 188.33, 'close': 232.52},
  {'open': 184.14, 'high': 163.42, 'low': 194.81, 'close': 153.03},
  {'open': 199.54, 'high': 130.26, 'low': 101.05, 'close': 102.1},
  {'open': 243.91, 'high': 119.21, 'low': 190.2, 'close': 223.31}],
 'AAPL': [{'open': 202.06, 'high': 162.54, 'low': 212.3, 'close': 226.78},
  {'open': 191.17, 'high': 153.49, 'low': 135.13, 'close': 151.83},
  {'open': 187.15, 'high': 149.75, 'low': 123.28, 'close': 247.32},
  {'open': 194.29, 'high': 175.34, 'low': 244.14, 'close': 207.45},
  {'open': 228.9, 'high': 133.26, 'low': 100.59, 'close': 129.35}]}


ticks = ['NVDA', 'MSFT', 'AAPL']
data = {}

for s in ticks:
    data[s] = []
    for _ in range(5):
        entry = {
            'open': round(random.uniform(100, 250), 2),
            'high': round(random.uniform(100, 250), 2),
            'low': round(random.uniform(100, 250), 2),
            'close': round(random.uniform(100, 250), 2)
        }
        data[s].append(entry)
</code></pre>
<p>I'd like to convert this to a dataframe which looks like this:</p>
<pre><code>df
Out[98]: 
    tick    open    high     low   close
0   NVDA  215.44  124.29  121.61  244.35
1   NVDA  214.89  184.49  157.39  239.31
2   NVDA  221.42  204.17  148.83  215.00
3   NVDA  182.49  104.29  175.36  226.59
4   NVDA  127.31  182.31  228.92  173.52
5   MSFT  217.79  147.98  120.40  239.97
6   MSFT  108.66  222.83  177.20  172.62
7   MSFT  138.16  116.36  241.62  231.15
8   MSFT  160.53  234.88  154.93  127.49
9   MSFT  168.22  127.77  224.75  207.59
10  AAPL  119.95  106.36  150.28  195.93
11  AAPL  117.71  142.54  210.08  116.37
12  AAPL  147.07  204.46  223.98  104.91
13  AAPL  135.71  211.83  210.11  102.34
14  AAPL  216.45  136.08  130.27  236.48
</code></pre>
","2","Question"
"79446466","","<p>I have pandas table where I want to create new column and fill data based on another columns values. I also want to know, if new columns value is updated
So I have dictionary like this:</p>
<pre><code>update_values = {&quot;Groub_A&quot;: {&quot;aadff2&quot;: &quot;Mark&quot;, &quot;aasd12&quot;: &quot;Otto&quot;, &quot;asdd2&quot;: &quot;Jhon&quot;},&quot;Groub_B&quot;: {&quot;aadfaa&quot;: &quot;Josh&quot;, &quot;aa1113&quot;: &quot;Math&quot;, &quot;967323sd&quot;: &quot;Marek&quot;}}  
</code></pre>
<p>And I want to my table look like this:</p>
<pre><code>Column_1 | Column_new_2 | Column_new_3
aadff2   | Mark         | Groub_A
aadff2   | Mark         | Groub_A
aasd12   | Otto         | Groub_A
asdd2    | Jhon         | Groub_A
967323sd | Marek        | Groub_B
967323sd | Marek        | Groub_B
aa1113   | Math         | Groub_B
</code></pre>
<p>So far I have just copied Column_1 and use <code>df.replace(&quot;Column_new_2&quot;:update_values[&quot;Groub_A&quot;])</code> and same thing with groub_B, but then don't know how to make Column_new_3?
There must be a easy solution, but I just can't figure it out.</p>
","0","Question"
"79446667","","<p>I have a simple dataframe:</p>
<pre><code>data = [[2025, 198237, 77, 18175],
        [202, 292827, 77, 292827]]
</code></pre>
<p>I only want the 1st and 4th columns and I don't want header or index labels:</p>
<pre><code>df = pd.DataFrame(data).iloc[:,[0,3]]
print(df.to_string(index=False, header=False))
</code></pre>
<p>Output is the following:</p>
<pre><code>2025  18175
 202 292827
</code></pre>
<p>How do I line up my first column in column 3 (left-justified) and line up my second column in column 10 (left-justified)?
Since i'm calling the to_string method, which is converting the dataframe to a string representation, shouldn't I be able to use <code>ljust</code>? I'm not able to produce the desired output, which would be:</p>
<pre><code>   2025   18175  
   202    292827 
</code></pre>
","1","Question"
"79447714","","<p>I have a pandas dataframe that looks like</p>
<pre><code>data = {
'Date': ['2024-07-14','2024-07-14','2024-07-14','2024-07-14','2024-07-14','2024-03-14','2024-03-14','2024-03-14','2024-02-14','2024-02-10','2024-02-10','2024-02-10','2024-04-13','2024-04-13','2023-02-11','2023-02-11','2023-02-11','2011-10-11','2011-05-02','2011-05-02'],
'Test_Number': [5,4,3,2,1,3,2,1,4,3,2,1,2,1,3,2,1,1,2,1],
'Student_ID': [2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1],
'Place': [3,5,7,3,1,9,6,3,7,8,2,1,3,4,2,1,5,6,2,7]
}
df = pd.DataFrame(data)
</code></pre>
<p>and I would like to create three new columns 'student_rec_1', 'student_rec_2', 'student_rec_3' using the following method:</p>
<p>for each Student_ID, student_rec_1 is equal to the Place of that student in the last test in the closest last date, and is equal to np.nan if it does not exist.</p>
<p>Similarly, student_rec_2 is equal to the Place of that student in the second last test in the closest last date, and is equal to np.nan if it does not exist,</p>
<p>student_rec_3 is equal to the Place of that student in the third last test in the closest last date, and is equal to np.nan if it does not exist. So the desired outcome looks like</p>
<pre><code>data_new = {
'Date': ['2024-07-14','2024-07-14','2024-07-14','2024-07-14','2024-07-14','2024-03-14','2024-03-14','2024-03-14','2024-02-14','2024-02-10','2024-02-10','2024-02-10','2024-04-13','2024-04-13','2023-02-11','2023-02-11','2023-02-11','2011-10-11','2011-05-02','2011-05-02'],
'Test_Number': [5,4,3,2,1,3,2,1,4,3,2,1,2,1,3,2,1,1,2,1],
'Student_ID': [2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1],
'Place': [3,5,7,3,1,9,6,3,7,8,2,1,3,4,2,1,5,6,2,7],
'student_rec_1': [9,9,9,9,9,7,7,7,8,np.nan,np.nan,np.nan,2,2,6,6,6,2,np.nan,np.nan],
'student_rec_2': [6,6,6,6,6,8,8,8,2,np.nan,np.nan,np.nan,1,1,2,2,2,7,np.nan,np.nan],
'student_rec_3': [3,3,3,3,3,2,2,2,1,np.nan,np.nan,np.nan,5,5,7,7,7,np.nan,np.nan,np.nan]
}
df_new = pd.DataFrame(data_new)
</code></pre>
<p>That's what I have tried:</p>
<p>df['Date'] = pd.to_datetime(df['Date'])</p>
<p>df = df.sort_values(['Date', 'Test_Number'], ascending=[False, False])</p>
<p>def get_last_n_records(group, n):
return group['Place'].shift(-n)</p>
<p>df['student_rec_1'] = df.groupby('Student_ID').apply(get_last_n_records, 1).reset_index(level=0, drop=True)
df['student_rec_2'] = df.groupby('Student_ID').apply(get_last_n_records, 2).reset_index(level=0, drop=True)
df['student_rec_3'] = df.groupby('Student_ID').apply(get_last_n_records, 3).reset_index(level=0, drop=True)</p>
<p>but it just shifted the student's place for each student and didn't account for the &quot;last day&quot; aspect and would just shift the Place irregardless.</p>
","2","Question"
"79448271","","<p>I am trying to find a clean way to create a new column in a dataframe with the ranking of the group/subgroup based on the sum of a value.
Here is a simple example :</p>
<pre><code>df = pd.DataFrame({
        &quot;group&quot;: [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;,&quot;c&quot;],
        &quot;subgroup&quot;: [&quot;i&quot;,&quot;ii&quot;,&quot;i&quot;,&quot;ii&quot;,&quot;i&quot;,&quot;ii&quot;,&quot;i&quot;,&quot;ii&quot;,&quot;i&quot;,&quot;ii&quot;,&quot;ii&quot;],
        &quot;value&quot;: [2, 4, 2, 3, 5, 1, 2, 4, 1, 5, 11] })
</code></pre>
<p>The output I'd like to have is :</p>
<pre><code>|group |subgroup| value | rank |
|------|--------|-------|------|
| &quot;a&quot;  | &quot;i&quot;    | 2     | 3    |
| &quot;a&quot;  | &quot;ii&quot;   | 4     | 4    |
| &quot;a&quot;  | &quot;i&quot;    | 2     | 3    |
| &quot;a&quot;  | &quot;ii&quot;   | 3     | 4    |
| &quot;a&quot;  | &quot;i&quot;    | 5     | 3    |
| &quot;b&quot;  | &quot;ii&quot;   | 1     | 2    |
| &quot;b&quot;  | &quot;i&quot;    | 2     | 5    |
| &quot;b&quot;  | &quot;ii&quot;   | 4     | 2    |
| &quot;b&quot;  | &quot;i&quot;    | 1     | 5    |
| &quot;b&quot;  | &quot;ii&quot;   | 5     | 2    |
| &quot;c&quot;  | &quot;i&quot;    | 11    | 1    |
</code></pre>
<p>because the sum of 'value' of the subgroup [&quot;a&quot;,&quot;i&quot;] is 9, making it the third biggest subgroup.
I know it sounds easy to do, but I can't find an efficient way to do it with Pandas...</p>
","1","Question"
"79448603","","<p>Until now I used to convert all values in a pandas dataframe with combined numerical and string entries to numeric values if possible in one easy step, using <em>.map</em> and <em>.to_numeric</em> with &quot;<em>errors = 'ignore'</em>&quot;.</p>
<p>It worked perfectly, but after updating to the latest version of Pandas (2.2.3) I get a FutureWarning.</p>
<pre><code>import pandas as pd
A = pd.DataFrame({
    'x' : ['1','2','3'],
    'y' : ['not_a_number','5',9999],
    }) # example data
B = A.map(pd.to_numeric, errors = 'ignore')

# FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing errors and catch exceptions explicitly instead B = A.map(pd.to_numeric, errors = 'ignore')
</code></pre>
<p>How could I code this future proof in an elegant, vectorised way?</p>
<p>I could not think of any solution that is not very cumbersome (looping over each individual entry of the dataframe).</p>
","2","Question"
"79450165","","<p>How can I sort the axis in descending order? I tried the following and it did not work:</p>
<pre class=""lang-py prettyprint-override""><code>from io import BytesIO
from fpdf import FPDF
import pandas as pd
import matplotlib.pyplot as plt
import io

DATA = {
    'x': ['val_1', 'val_2', 'val_3', 'val_4', 'val_5'],
    'y': [1, 2, 3, 4, 5]
}

COLUMNS = tuple(DATA.keys())

plt.figure()
df = pd.DataFrame(DATA, columns=COLUMNS)
df.sort_values(['y'], ascending=[False]).plot(x=COLUMNS[0], y=COLUMNS[1], kind=&quot;barh&quot;, legend=False)

img_buf = BytesIO()
plt.savefig(img_buf, dpi=200)

pdf = FPDF()
pdf.add_page()
pdf.image(img_buf, w=pdf.epw)
pdf.output('output.pdf')
img_buf.close()
</code></pre>
","1","Question"
"79450409","","<p>How to parse xls data to this struct, both row and column have merged cells, simply use <code>df.index.to_series().ffill()</code> cannot handle.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;time&quot;: &quot;time&quot;,
  &quot;category&quot;: &quot;A&quot;,
  &quot;variety&quot;: &quot;A1&quot;,
  &quot;specification&quot;: &quot;S1&quot;,
  &quot;unit&quot;: &quot;U1&quot;,
  &quot;average&quot;: 1.25,
  &quot;region&quot;: &quot;RegionA&quot;,
  &quot;market&quot;: &quot;MarketA&quot;,
  &quot;price&quot;: 1.1,
}
</code></pre>
<p><a href=""https://i.sstatic.net/A2cUFT08.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A2cUFT08.png"" alt=""enter image description here"" /></a></p>
","2","Question"
"79450672","","<p>I want to sort the first column according to the internal algorithm, and then sort the second column according to the custom sorting method</p>
<p>The test data is as follows：</p>
<pre><code>     A              B
Ankang Shaanxi      Ankang Southeast
Baoding Anguo       Baoding Anguo Northeast
Baoding Anguo       Baoding Anguo Southeast
Changsha Hunan      Changsha Hunan Bright
Ankang Shaanxi      Ankang Northeast
Baoding Anguo       Baoding Anguo Southwest
Baoding Anguo       Baoding Anguo Upper
Ankang Shaanxi      Ankang Southwest    
Luoyang Henan       Luoyang Henan Upper
Baoding Anguo       Baoding Anguo Northwest
Changsha Hunan      Changsha Hunan Lower
Ankang Shaanxi      Ankang Southwest Upper  
Ankang Shaanxi      Ankang Northwest
</code></pre>
<p>I hope to be able to arrange it as shown below</p>
<p>The first column is sorted together using pandas' built-in string sorting algorithm, and then the second column is sorted using the custom order algorithm of northeast, southeast, northwest, southwest,upper.</p>
<p>I used pandas' sort_values() method to sort. I had no problem sorting a single column, but it always failed when I tried to sort two columns together.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data={'A':['Ankang Shaanxi','Baoding Anguo','Baoding Anguo','Changsha Hunan','Ankang Shaanxi',
'Baoding Anguo','Baoding Anguo','Ankang Shaanxi','Luoyang Henan','Baoding Anguo',
'Changsha Hunan','Ankang Shaanxi','Ankang Shaanxi'],
'B':['Ankang Southeast','Baoding Anguo Northeast','Baoding Anguo Southeast','Changsha Hunan Bright','Ankang Northeast','Baoding Anguo Southwest','Baoding Anguo Upper','Ankang Southwest','Luoyang Henan Upper','Baoding Anguo Northwest','Changsha Hunan Lower','Ankang Southwest Upper','Ankang Northwest']}

df=pd.DataFrame(data)
def sort_fun(x):

      return x.split()[-1]

df['sort_value']=df['B'].apply(sort_fun)

sort_dicts={'Northeast':0,'Southeast':1,'Northwest':2,'Southwest':3,'Upper':4}

df.sort_values(by=['A','sort_value'],key=lambda x :x.map(sort_dicts))
</code></pre>
<p>I referred to it <a href=""https://stackoverflow.com/questions/74611496/pandas-how-to-custom-sort-on-multiple-columns"">Pandas: How to custom-sort on multiple columns?</a></p>
<pre class=""lang-py prettyprint-override""><code>      A                    B
Ankang Shaanxi      Ankang Northeast
Ankang Shaanxi      Ankang Southeast
Ankang Shaanxi      Ankang Northwest
Ankang Shaanxi      Ankang Southwest    
Ankang Shaanxi      Ankang Southwest Upper
Baoding Anguo       Baoding Anguo Northeast
Baoding Anguo       Baoding Anguo Southeast
Baoding Anguo       Baoding Anguo Northwest
Baoding Anguo       Baoding Anguo Southwest
Baoding Anguo       Baoding Anguo Upper
Changsha Hunan      Changsha Hunan Bright
Changsha Hunan      Changsha Hunan Lower
Luoyang Henan       Luoyang Henan Upper
</code></pre>
","3","Question"
"79450810","","<p>The data I am working with:</p>
<pre><code>data (140631115432592), ndim: 2, size: 3947910, shape: (232230, 17)
VIN (1-10)                                            object
County                                                object
City                                                  object
State                                                 object
Postal Code                                          float64
Model Year                                             int64
Make                                                  object
Model                                                 object
Electric Vehicle Type                                 object
Clean Alternative Fuel Vehicle (CAFV) Eligibility     object
Electric Range                                       float64
Base MSRP                                            float64
Legislative District                                 float64
DOL Vehicle ID                                         int64
Vehicle Location                                      object
Electric Utility                                      object
2020 Census Tract                                    float64
dtype: object
   VIN (1-10)    County      City State  Postal Code  ...  Legislative District DOL Vehicle ID             Vehicle Location                               Electric Utility 2020 Census Tract
0  2T3YL4DV0E      King  Bellevue    WA      98005.0  ...                  41.0      186450183   POINT (-122.1621 47.64441)  PUGET SOUND ENERGY INC||CITY OF TACOMA - (WA)      5.303302e+10
1  5YJ3E1EB6K      King   Bothell    WA      98011.0  ...                   1.0      478093654  POINT (-122.20563 47.76144)  PUGET SOUND ENERGY INC||CITY OF TACOMA - (WA)      5.303302e+10
2  5UX43EU02S  Thurston   Olympia    WA      98502.0  ...                  35.0      274800718  POINT (-122.92333 47.03779)                         PUGET SOUND ENERGY INC      5.306701e+10
3  JTMAB3FV5R  Thurston   Olympia    WA      98513.0  ...                   2.0      260758165  POINT (-122.81754 46.98876)                         PUGET SOUND ENERGY INC      5.306701e+10
4  5YJYGDEE8M    Yakima     Selah    WA      98942.0  ...                  15.0      236581355  POINT (-120.53145 46.65405)                                     PACIFICORP      5.307700e+10
</code></pre>
<p>Data in csv format:</p>
<pre><code>VIN (1-10),County,City,State,Postal Code,Model Year,Make,Model,Electric Vehicle Type,Clean Alternative Fuel Vehicle (CAFV) Eligibility,Electric Range,Base MSRP,Legislative District,DOL Vehicle ID,Vehicle Location,Electric Utility,2020 Census Tract
2T3YL4DV0E,King,Bellevue,WA,98005,2014,TOYOTA,RAV4,Battery Electric Vehicle (BEV),Clean Alternative Fuel Vehicle Eligible,103,0,41,186450183,POINT (-122.1621 47.64441),PUGET SOUND ENERGY INC||CITY OF TACOMA - (WA),53033023604
5YJ3E1EB6K,King,Bothell,WA,98011,2019,TESLA,MODEL 3,Battery Electric Vehicle (BEV),Clean Alternative Fuel Vehicle Eligible,220,0,1,478093654,POINT (-122.20563 47.76144),PUGET SOUND ENERGY INC||CITY OF TACOMA - (WA),53033022102
5UX43EU02S,Thurston,Olympia,WA,98502,2025,BMW,X5,Plug-in Hybrid Electric Vehicle (PHEV),Clean Alternative Fuel Vehicle Eligible,40,0,35,274800718,POINT (-122.92333 47.03779),PUGET SOUND ENERGY INC,53067011902
JTMAB3FV5R,Thurston,Olympia,WA,98513,2024,TOYOTA,RAV4 PRIME,Plug-in Hybrid Electric Vehicle (PHEV),Clean Alternative Fuel Vehicle Eligible,42,0,2,260758165,POINT (-122.81754 46.98876),PUGET SOUND ENERGY INC,53067012332
5YJYGDEE8M,Yakima,Selah,WA,98942,2021,TESLA,MODEL Y,Battery Electric Vehicle (BEV),Eligibility unknown as battery range has not been researched,0,0,15,236581355,POINT (-120.53145 46.65405),PACIFICORP,53077003200
3C3CFFGE1G,Thurston,Olympia,WA,98501,2016,FIAT,500,Battery Electric Vehicle (BEV),Clean Alternative Fuel Vehicle Eligible,84,0,22,294762219,POINT (-122.89166 47.03956),PUGET SOUND ENERGY INC,53067010802
5YJ3E1EA4J,Snohomish,Marysville,WA,98271,2018,TESLA,MODEL 3,Battery Electric Vehicle (BEV),Clean Alternative Fuel Vehicle Eligible,215,0,39,270125096,POINT (-122.1677 48.11026),PUGET SOUND ENERGY INC,53061052808
5YJ3E1EA3K,King,Seattle,WA,98102,2019,TESLA,MODEL 3,Battery Electric Vehicle (BEV),Clean Alternative Fuel Vehicle Eligible,220,0,43,238776492,POINT (-122.32427 47.63433),CITY OF SEATTLE - (WA)|CITY OF TACOMA - (WA),53033006600
1N4AZ0CP5E,Thurston,Yelm,WA,98597,2014,NISSAN,LEAF,Battery Electric Vehicle (BEV),Clean Alternative Fuel Vehicle Eligible,84,0,2,257246118,POINT (-122.60735 46.94239),PUGET SOUND ENERGY INC,53067012421
</code></pre>
<p>Filtering and grouping:</p>
<pre><code>filt = (data[&quot;Model Year&quot;] &gt;= 2018) &amp; (data[&quot;Electric Vehicle Type&quot;] == &quot;Battery Electric Vehicle (BEV)&quot;)
data = data[filt].groupby([&quot;State&quot;, &quot;Make&quot;], sort=False, observed=True, as_index=False).agg( avg_electric_range=pd.NamedAgg(column=&quot;Electric Range&quot;, aggfunc=&quot;mean&quot;), oldest_model_year=pd.NamedAgg(column=&quot;Model Year&quot;, aggfunc=&quot;min&quot;))
</code></pre>
<p>Currently it yields the following table:</p>
<pre><code>  State       Make  avg_electric_range  oldest_model_year
0    WA      TESLA           52.143448               2018
1    WA     NISSAN           60.051874               2018
&lt;snip&gt;
</code></pre>
<p>How do I add a <code>Count</code> column which shows the count of each group which is used for further filtering? Note: rule out <code>apply</code> as everything should stay in Pandas'land.</p>
","2","Question"
"79450950","","<p>Can someone explain what is meant by</p>
<blockquote>
<p>Both <code>loc</code> and <code>iloc</code> [in Pandas] are row-first, column-second. This is the opposite of what we do in native Python, which is column-first, row-second.</p>
</blockquote>
<p>Because I thought when accessing arrays or lists of lists, the first index always represents the row:</p>
<pre class=""lang-py prettyprint-override""><code>matrix = [
    [1,2,3], # row 1, index 0
    [4,5,6], # row 2, index 1
    [7,8,9] # row 3, index 2
]
print(matrix[1][2]) # Output = 6
</code></pre>
","1","Question"
"79452237","","<p>The basics of using Pandas <em>where</em> with callables seems simple.</p>
<pre><code>np.random.seed(0)
df = pd.DataFrame(np.random.randn(8, 4), columns=['A', 'B', 'C', 'D'])
df[&quot;test&quot;] = range(1,9)

def MyBool(x):
    print(1)
    return ( x &gt; 0 )

def MyFunc(x1):
    print(1)
    return x1['A']

df.where(
    cond  = lambda x: MyBool(x),
    other = lambda x: MyFunc(x) ,
    )
</code></pre>
<p>In the code above, I am replacing the values of all columns with the value of column A whenever the value of the col is less than 0. Note, I know I don't need to use callables for this simple example.</p>
<p>Based on my analysis, this is what is happening under the hood.</p>
<p>First, MyFunc is evaluated where the argument is the df itself. This returns a 8x1 df (=A)</p>
<p>Second, the MyBool is evaluated which returns a 8x5 boolean df.</p>
<p>Third, (not sure about this last step) for all entries (i,j) where MyBool returned False, the value of the i'th row of the output of MyFunc is used to replace the current value of the df.</p>
<p><strong>This leads me on to my question</strong>: how does this extend to the cases when MyFunc returns a dataframe with several columns and rows? How does the function determine which entries need to be replaced and with which values?</p>
<p>For illustrative purposes, suppose now that we want to divide B and C by 2 when test is equal to 5. The code I have provided below works but I don't quite understand how it determines which entries are to be replaced and with which values.</p>
<p>MyBool still returns a one dimensional vector but MyFunc returns a dataframe. If the previous logic I explained was correct, then shouldn't it replace each False entry with the dataframe? Indeed, if this were the case, the resulting dataframe should be bigger than the input df.</p>
<p>I've been reading the documentation and playing with different examples but can't figure this one out.</p>
<pre><code>def MyBool(x):
    output = x.test != 5
    return output

def MyFunc(x1):
    x1.loc[ x1.test == 5, [&quot;B&quot;, &quot;C&quot;] ] /= 2
    return x1

df.where(
    cond  = lambda x: MyBool(x),
    other = lambda x: MyFunc(x.copy()),
    axis  = 0
    )
</code></pre>
","2","Question"
"79452360","","<p>I am looking to convert a column with dates in a list [D, M, Y] to a datetime column. The below works but there must be a better way?</p>
<pre><code>new_df = pd.DataFrame({'date_parts': [[29, 'August', 2024], [28, 'August', 2024], [27, 'August', 2024]]})
display(new_df)

## Make new columns with dates
new_df = pd.concat([new_df, new_df['date_parts'].apply(pd.Series)], axis=1).rename(columns={0:'D', 1:'M', 2:'Y'})

month_map = {
'January':1,
'February':2,
'March':3,
'April':4,
'May':5,
'June':6,
'July':7,
'August':8,
'September':9,
'October':10,
'November':11,
'December':12
}

## make datetime column
new_df['release_date'] = pd.to_datetime(dict(year=new_df.Y, month=new_df.M.apply(lambda x: month_map[x]), day=new_df.D),  format='%d-%B-%Y') 
new_df.drop(columns=['D', 'M', 'Y'])
</code></pre>
<pre><code>## Input
    date_parts
0   [29, August, 2024]
1   [28, August, 2024]
2   [27, August, 2024]

## Output
    date_parts          release_date
0   [29, August, 2024]  2024-08-29
1   [28, August, 2024]  2024-08-28
2   [27, August, 2024]  2024-08-27
</code></pre>
","3","Question"
"79452715","","<p>I have a very large dataset (about 10^7 rows and 1000 columns) and need to make cuts into one of the columns, for trining/validation separation, with the bins changing based on another column. I am pretty new to python and am using this function:</p>
<p><code>SEGMENT</code> is either A, B or C, and <code>DATE</code> is what I am cutting (yes, it is a numerical column, I know it looks terrible but it was not my choice), with different bins for different values of <code>SEGMENT</code>.</p>
<pre><code>cuts = {
    &quot;A&quot;: {&quot;cut&quot;:[0,20240101,20240801,20241201,20250000], &quot;class&quot;:[&quot;out&quot;, &quot;training&quot;, &quot;validation&quot;, &quot;out&quot;]},
    &quot;B&quot;: {&quot;cut&quot;:[0,20230701,20240701,20241201,20250000], &quot;class&quot;:[&quot;out&quot;, &quot;training&quot;, &quot;validation&quot;, &quot;out&quot;]},
    &quot;C&quot;: {&quot;cut&quot;:[0,20230701,20240701,20250101,20250201], &quot;class&quot;:[&quot;out&quot;, &quot;training&quot;, &quot;validation&quot;, &quot;out&quot;]}
}
def divisions(row):
    rules = cuts[row[&quot;SEGMENT&quot;]]
    return pd.cut([row[&quot;DATE&quot;]], bins=rules[&quot;cut&quot;], labels=rules[&quot;class&quot;], right=False, ordered=False)[0]

df[&quot;CLASS&quot;] = df.apply(divisions, axis=1)
</code></pre>
<p>This seems to work but has been insanely slow even on samples of less than 0.1% of the actual dataset.</p>
<p>How can I improve this?</p>
<p>All I need is this <code>CLASS</code> column, to check if the training and validation datasets show similar behaviors. I am not yet doing the actual modeling.</p>
","2","Question"
"79453687","","<p>I have 2 DataFrame with different columns and want to merge to csv without comma for the one having single column.</p>
<p>How can we remove comma for the one having single column?</p>
<pre><code>import pandas as pd

# 1st DataFrame with single column
pd_title = pd.DataFrame(['Category: A', ''])

# 2nd DataFrame with double columns 
data = [
  [&quot;Date&quot;, &quot;Value&quot;],
  ['2025-01-01', 50],
  ['2025-01-02', 40],
  ['2025-01-03', 45]
]

result = pd_title._append(data).reset_index(drop=True)

result.to_csv('/content/test.csv', index=False, header=False)
</code></pre>
<p>The result from code :</p>
<p><a href=""https://i.sstatic.net/A2Q3atC8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A2Q3atC8.png"" alt=""enter image description here"" /></a></p>
<p>The result what I mean :</p>
<p><a href=""https://i.sstatic.net/6HFVrS6B.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6HFVrS6B.png"" alt=""enter image description here"" /></a></p>
","1","Question"
"79454185","","<p>I have a pandas dataframe that looks like this:</p>
<pre><code>import pandas as pd
df = pd.DataFrame({'id': [1,2], 'var1': [5,6], 'var2': [20,60], 'var3': [8, -2], 'model_version': ['model_a', 'model_b']})
</code></pre>
<p>I have 2 different models, saved in <code>pkl</code> files, which I load them like this:</p>
<pre><code>import pickle
with open('model_a.pkl', 'rb') as file:
    model_a = pickle.load(file)
with open('model_b.pkl', 'rb') as file:
    model_b = pickle.load(file)
</code></pre>
<p>I would like to apply and predict <code>model_a</code> for <code>id==1</code> and <code>model_b</code> for <code>id==2</code>, as indicated in the <code>model_version</code> column in the <code>df</code>.</p>
<p>How can I do that in &quot;one-go&quot; ?</p>
<p>What I mean in &quot;one-go&quot; is that: I am <strong>NOT</strong> looking for a solution that looks like this:</p>
<pre><code>df_a = df.query('model_version==&quot;model_a&quot;')
predictions_a = model_a.predict(df_a)

df_b = df.query('model_version==&quot;model_b&quot;')
predictions_b = model_b.predict(df_b)
</code></pre>
","1","Question"
"79454433","","<p>I am quite new to Pandas. I need to select/locate the records between 2 dates.</p>
<p>I have tried a range of methods, but cant seem to get it. I have included a cut down of sample of the CSV/Data I am working with.</p>
<p>Each column is a date, so all of the documentation I have found don't match this data structure</p>
<p>Thanks for any help.</p>
<p><a href=""https://i.sstatic.net/9QbEDrOK.png"" rel=""nofollow noreferrer"">sample csv file</a></p>
","1","Question"
"79455358","","<p>I have a dataframe <code>df1</code> that has a year column and numeric columns. The values of year in <code>df1</code> are not unique. I would like to normalize the numeric columns of <code>df1</code> at a yearly level using a specific function. For this purpose, I have another dataframe <code>df2</code> that has a year column, and with the same numeric columns as in <code>df1</code>. To do the renormalization, I want to divide all numeric columns in a given year with <code>df2</code> for that year.</p>
<p>Here's sample code that achieves what I want but I'm sure there's a &quot;smart&quot; way of doing the same procedure.</p>
<pre><code>import pandas as pd

df1 = pd.DataFrame({
    'year': [1, 1, 0, 0],
    'B': [4, 2, 1, 5],
    'C': [5, 3, 2, 6],
    'D': [&quot;Good 1&quot;, &quot;Good 2&quot;, &quot;Good 1&quot;, &quot;Good 2&quot;]
})
df1
</code></pre>
<pre><code>   year  B  C       D
0     1  4  5  Good 1
1     1  2  3  Good 2
2     0  1  2  Good 1
3     0  5  6  Good 2
</code></pre>
<pre><code>df2 = pd.DataFrame({'year': [1, 0], 'B': [3, 5], 'C': [4, 7]})
df2
</code></pre>
<pre><code>   year  B  C
0     1  3  4
1     0  5  7
</code></pre>
<pre><code>vars_to_replace = ['B', 'C']
</code></pre>
<p>This works:</p>
<pre><code>for year in df1.year.unique():
    df1.loc[df1.year == year, vars_to_replace] /= df2.loc[df2.year == year, vars_to_replace].values

df1
</code></pre>
<pre><code>   year         B         C       D
0     1  1.333333  1.250000  Good 1
1     1  0.666667  0.750000  Good 2
2     0  0.200000  0.285714  Good 1
3     0  1.000000  0.857143  Good 2
</code></pre>
<p>The code achieves what I want but I'm not too happy with having to use for-loops. I've tried other alternatives but can't make it work. Any ideas? I've tried the following approach but it doesn't work because <code>df1</code> isn't unique in year.</p>
<pre><code>df1.set_index(&quot;year&quot;, inplace=True)
df2.set_index(&quot;year&quot;, inplace=True)
df1.loc[:, vars_to_replace] /= df2
</code></pre>
<p>The last command returns a &quot;ValueError: cannot reindex on an axis with duplicate labels&quot;</p>
<p>NOTE: after doing research on the questions and answers concerning this topic on SO, I was <em>unable to find</em> a case that covers a multicolumn operation where the indexes (year in <code>df1</code> in my example) are not unique.</p>
","0","Question"
"79455667","","<p>I have two dataframes that I want to mix togther first two of the second table go to the every second two of, so like this:</p>
<pre><code>First table              Second table
column_1                 column_1
1                        5
2                        6
3                        7
4                        8
</code></pre>
<p>And then the new table will be like this:</p>
<pre><code>new table
column_1
1
2
5
6
3
4
7
8
</code></pre>
<p>Is there easy way to do this with pandas?</p>
","1","Question"
"79456337","","<p>I have a sample dataframe as below that has same subfix as <code>001, 002, 003</code>.</p>
<pre><code>import pandas as pd
import numpy as np

branch_names = [f&quot;Branch_{i}&quot; for i in range(1, 11)]
date_1 = '20241231'
date_2 = '20250214'
date_3 = '20250220'

data = {
    'Branch': branch_names,
    date_1 + '_001': np.random.randint(60, 90, 10),
    date_1 + '_002': np.random.randint(60, 90, 10),
    date_1 + '_003': np.random.randint(60, 90, 10),
    date_2 + '_001': np.random.randint(60, 90, 10),
    date_2 + '_002': np.random.randint(60, 90, 10),
    date_2 + '_003': np.random.randint(60, 90, 10),
    date_3 + '_001': np.random.randint(60, 90, 10),
    date_3 + '_002': np.random.randint(60, 90, 10),
    date_3 + '_003': np.random.randint(60, 90, 10)
}

# Chuyển thành DataFrame
df = pd.DataFrame(data)
</code></pre>
<p><a href=""https://i.sstatic.net/53iUjbUH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/53iUjbUH.png"" alt=""enter image description here"" /></a></p>
<p>Now I want to subtract data between columns that have same subfix as below principle:</p>
<pre><code>df['diff_1_001'] = df[date_3 + '_001'] - df[date_2 + '_001']
df['diff_2_001'] = df[date_3 + '_001'] - df[date_1 + '_001']
df['diff_1_002'] = df[date_3 + '_002'] - df[date_2 + '_002']
df['diff_2_002'] = df[date_3 + '_002'] - df[date_1 + '_002']
df['diff_1_003'] = df[date_3 + '_003'] - df[date_2 + '_003']
df['diff_2_003'] = df[date_3 + '_003'] - df[date_1 + '_003']
df
</code></pre>
<p>As you see that we have same <code>001, 002, 003</code> but prefix is different. So I want don't want to hard code the <code>001, 002, 003</code> but automatically subtract it as mentioned above.</p>
","3","Question"
"79457029","","<p>I found that setting pandas DataFrame column with numpy array whose dtype is object will cause a wierd error. I wonder why it happens.</p>
<p>The code I ran is as follows:</p>
<pre><code>import numpy as np
import pandas as pd

print(f&quot;numpy version: {np.__version__}&quot;)
print(f&quot;pandas version: {pd.__version__}&quot;)

data = pd.DataFrame({
    &quot;c1&quot;: [1, 2, 3, 4, 5],
})

print(&quot;-&quot; * 10)

t1 = np.array([[&quot;A&quot;], [&quot;B&quot;], [&quot;C&quot;], [&quot;D&quot;], [&quot;E&quot;]])
data[&quot;c1&quot;] = t1 # This works well

print(&quot;-&quot; * 10)

t2 = np.array([[&quot;A&quot;], [&quot;B&quot;], [&quot;C&quot;], [&quot;D&quot;], [&quot;E&quot;]], dtype=object)
data[&quot;c1&quot;] = t2 # This throws an error

print(&quot;-&quot; * 10)
</code></pre>
<p>The output is:</p>
<pre><code>numpy version: 1.26.4
pandas version: 2.2.2
----------
----------
Traceback (most recent call last):
  File &quot;...\test.py&quot;, line 19, in &lt;module&gt;
    data[&quot;c1&quot;] = t2 # This throws an error
    ~~~~^^^^^^
  File &quot;...\pandas\core\frame.py&quot;, line 4311, in __setitem__
    self._set_item(key, value)
  File &quot;...\pandas\core\frame.py&quot;, line 4524, in _set_item
    value, refs = self._sanitize_column(value)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;...\pandas\core\frame.py&quot;, line 5267, in _sanitize_column
    arr = sanitize_array(value, self.index, copy=True, allow_2d=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;...\pandas\core\construction.py&quot;, line 606, in sanitize_array
    subarr = maybe_infer_to_datetimelike(data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;...\pandas\core\dtypes\cast.py&quot;, line 1182, in maybe_infer_to_datetimelike
    raise ValueError(value.ndim)  # pragma: no cover
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 2
</code></pre>
","0","Question"
"79457388","","<p>Why is my python3 code to split column 4 at the bases of &quot;,&quot; to the column 7, 8, 9 with condition if the column 5 value is &quot;LUB&quot; not working?</p>
<p>source data:
<img src=""https://i.sstatic.net/OVFJsa18.png"" alt=""source data screenshot"" /></p>
<pre><code>20240801    CASH MAN1   120.00  Z20/1   0.5 , Z20/1 ; 120   1   LUB
20240801    PAYTM   15720.00    CASH MAN1   0   1   JRNLCR
20240801    PAYTM   81343.00    CASH MAN2   0   2   JRNLCR
</code></pre>
<p>my code:</p>
<pre><code>import numpy as np
import pandas as pd

var001_sf = input(&quot;Enter file name to import : &quot;)
var002_sf = var001_sf + &quot;.csv&quot;
var002_ar1 = pd.read_csv(var002_sf, sep='\t', header=None)
var002_ar1 = var002_ar1.dropna(axis=0)  # remove rows with na value
var002_ar1.iloc[:, [0, 1, 2, 3, 4, 5, 6]] = var002_ar1.iloc[:, [0, 1, 2, 3, 4, 5, 6]].map(str)
var002_ar1.loc[var002_ar1.iloc[:, 6] == 'LUB', [7, 8, 9]] = var002_ar1.iloc[:, 4].str.split(', ', n=2, expand=True)
var002_ar1.to_csv(&quot;t250221.csv&quot;, sep='\t')
</code></pre>
<p>output data:
<img src=""https://i.sstatic.net/eAWCt7kv.png"" alt=""output data screenshot"" /></p>
<pre><code>20240801    CASH MAN1   120.0   Z20/1   0.5 , Z20/1 ; 120   1   LUB         
1   20240801    PAYTM   15720.0 CASH MAN1   0   1   JRNLCR          
2   20240801    PAYTM   81343.0 CASH MAN2   0   2   JRNLCR          
</code></pre>
<p>I can't get it to split &quot;0.5 , Z20/1 ; 120&quot; in column 7, 8, 9</p>
<p>What should I do for the desired result?</p>
","-1","Question"
"79457797","","<p>I have put the following log data to a <code>pandas</code> dataframe. Now I want to get the 3 rows with <code>target_label= 'Security Alert'</code> having the most recent timestamps</p>
<p>In the following table the third column is the <code>target_label</code> column,</p>
<pre><code>1759,12/29/2025 20:33,ModernCRM,Unauthorised access attempt from 192.168.99.79 detected,Security Alert,bert
2213,9/1/2025 10:30,AnalyticsEngine,Security alert: suspicious activity from 192.168.214.63,Security Alert,bert
168,5/19/2025 7:56,ModernHR,Multiple incorrect login attempts were made by user 7918,Security Alert,bert
1844,6/10/2025 8:39,ThirdPartyAPI,&quot;Alert: server 9 experienced unusual login attempts, security risk&quot;,Security Alert,bert
961,4/26/2025 5:21,ThirdPartyAPI,API access audit trail shows unauthorized entry for user 5627,Security Alert,bert
1077,3/4/2025 9:49,AnalyticsEngine,&quot;Anomalous activity identified on server 47, security review recommended&quot;,Security Alert,bert
1356,5/3/2025 13:03,ThirdPartyAPI,Multiple rejected login attempts found for user 1805,Security Alert,bert
43,11/22/2025 11:06,BillingSystem,&quot;Abnormal system behavior on server 40, potential security breach&quot;,Security Alert,bert
2062,6/7/2025 1:22,AnalyticsEngine,&quot;Server 11 experienced potential security incident, review required&quot;,Security Alert,bert
769,4/28/2025 4:07,ModernHR,API access denied due to unauthorized credentials for user 5914,Security Alert,bert
</code></pre>
","0","Question"
"79458545","","<p>I'm new to learning Python and have a wide data file that I would like to aggregate by different variables and create a new file for use with a dashboard.  I am able to use groupby() to get the aggregation I want for one column at a time and then append the results to a csv file.  However, I'm wondering if there's a more efficient way to do this using a loop of some sort.</p>
<p>Here's a sample of what my original data file looks like:</p>
<pre><code>data = {'ID': [105, 106, 107, 108, 109, 110, 111, 112],
        'Name': ['Bill', 'Jane', 'Mary', 'Rich', 'Tomas', 'Kiki', 'Martin', 'Larry'],
        'Cohort': ['Cohort A', 'Cohort A', 'Cohort A', 'Cohort A', 'Cohort B', 'Cohort B', 'Cohort B', 'Cohort B'],
        'Program Size': ['small', 'large', 'medium', 'medium', 'large', 'small', 'large', 'medium'],
        'Rating': ['excellent', 'good', 'needs improvement', 'needs improvement', 'good', 'excellent', 'good', 'excellent']}

df = pd.DataFrame(data)
</code></pre>
<p>Looks like this:</p>
<pre><code> ID    Name    Cohort Program Size             Rating
0  105    Bill  Cohort A        small          excellent
1  106    Jane  Cohort A        large               good
2  107    Mary  Cohort A       medium  needs improvement
3  108    Rich  Cohort A       medium  needs improvement
4  109   Tomas  Cohort B        large               good
5  110    Kiki  Cohort B        small          excellent
6  111  Martin  Cohort B        large               good
7  112   Larry  Cohort B       medium          excellent
</code></pre>
<p>I always want to group by the Cohort column, plus one of the other columns and get counts for each combination.  In my new data file, I'll have a Cohort column, a &quot;Variable&quot; column (that will be the other column in the group by), the options present the &quot;Description&quot;, and then the Frequency.  Here's what my desired result would look like:</p>
<pre><code>desired = {'Cohort': ['Cohort A', 'Cohort A', 'Cohort A', 'Cohort B', 'Cohort B', 'Cohort B', 'Cohort A', 'Cohort A', 'Cohort A', 'Cohort B', 'Cohort B'],
        'Variable': ['size', 'size', 'size', 'size', 'size', 'size', 'rating', 'rating', 'rating', 'rating', 'rating'],
        'Description': ['small', 'medium', 'large', 'small', 'medium', 'large', 'excellent', 'good', 'needs improvement', 'excellent', 'good'],
        'Frequency': [1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2]}

desired_df = pd.DataFrame(desired)
</code></pre>
<pre><code> Cohort Variable        Description  Frequency
0   Cohort A     size              small          1
1   Cohort A     size             medium          2
2   Cohort A     size              large          1
3   Cohort B     size              small          1
4   Cohort B     size             medium          1
5   Cohort B     size              large          2
6   Cohort A   rating          excellent          1
7   Cohort A   rating               good          1
8   Cohort A   rating  needs improvement          2
9   Cohort B   rating          excellent          2
10  Cohort B   rating               good          2
</code></pre>
<p>I've been able to write a loop to group by Cohort and my other columns and get frequencies, but I'm not sure how to put it all together (append dataframes?) to get to my desired result.  I appreciate any guidance on next steps!  (Here is what I wrote to loop through my columns and get frequencies:</p>
<pre><code>cols = ['Program Size', 'Rating']

for i in cols:
    grouped_df = df.groupby(['Cohort', (i)], as_index=False).agg(
        frequency=('ID', 'count')
    )
    print(f&quot;Grouped by Cohort and {i}:\n{grouped_df}\n&quot;)
</code></pre>
","1","Question"
"79458590","","<p>I have an xml file with experimental data that I am trying to read out in Python with pandas.</p>
<p>The data is separated into 2 experiments, which each have 2 wavelengths (and more subnodes). I can select an attribute for the wavelength and it gives me a readout without any problems, but for both Experiments. If I try to also select an attribute for the wavelength I get the &quot;XPathEvalError: Invalid predicate&quot; error.</p>
<p>Here's the code when it works without the attribute for the Experiment Section</p>
<pre><code>import pandas as pa

a = pa.read_xml(path_or_buffer=filepath, xpath=&quot;//doc:experimentSection[@sectionName]/doc:plateSection/doc:microplateData/doc:wave[@waveID=1]/doc:well[@wellID]/doc:oneDataSet&quot;, namespaces={&quot;doc&quot;: &quot;http://moleculardevices.com/microplateML&quot;})
</code></pre>
<p>Here's the same code when it includes the experiment section attribute &quot;Experiment#1&quot; and now it returns Invalid predicate. If the attribute is changed to something that is not in the original file it returns &quot;Invalid expression&quot;. So it does seem to recognize the attribute to be present</p>
<pre><code>import pandas as pa

a = pa.read_xml(path_or_buffer=filepath, xpath=&quot;//doc:experimentSection[@sectionName=Experiment#1]/doc:plateSection/doc:microplateData/doc:wave[@waveID=1]/doc:well[@wellID]/doc:oneDataSet&quot;, namespaces={&quot;doc&quot;: &quot;http://moleculardevices.com/microplateML&quot;})
</code></pre>
<p>I've copied the contents of the <a href=""https://pastebin.com/m3BA5fMy"" rel=""nofollow noreferrer"">xml file</a> into pastebin. The file works as expected if reverted back to xml.</p>
<p>Thanks for the help</p>
<p>I already tried finding other solutions on the internet without much success. The expected output is a 2 column dataframe with rawData and timeData for a selected Experiment and Wavelength, but only the selection of Wavelength works</p>
","1","Question"
"79459705","","<p>My log file format is as below :</p>
<pre class=""lang-none prettyprint-override""><code>2016-09-28 04:30:30, Info                  CBS    Loaded Servicing Stack v6.1.7601.23505 with Core: C:\Windows\winsxs\amd64_microsoft-windows-servicingstack_31bf3856ad364e35_6.1.7601.23505_none_681aa442f6fed7f0\cbscore.dll
2016-09-28 04:30:31, Info                  CSI    00000001@2016/9/27:20:30:31.455 WcpInitialize (wcp.dll version 0.0.0.6) called (stack @0x7fed806eb5d @0x7fef9fb9b6d @0x7fef9f8358f @0xff83e97c @0xff83d799 @0xff83db2f)
2016-09-28 04:30:31, Info                  CSI    00000002@2016/9/27:20:30:31.458 WcpInitialize (wcp.dll version 0.0.0.6) called (stack @0x7fed806eb5d @0x7fefa006ade @0x7fef9fd2984 @0x7fef9f83665 @0xff83e97c @0xff83d799)
2016-09-28 04:30:31, Info                  CSI    00000003@2016/9/27:20:30:31.458 WcpInitialize (wcp.dll version 0.0.0.6) called (stack @0x7fed806eb5d @0x7fefa1c8728 @0x7fefa1c8856 @0xff83e474 @0xff83d7de @0xff83db2f)
2016-09-28 04:30:31, Info                  CBS    Ending TrustedInstaller initialization.
2016-09-28 04:30:31, Info                  CBS    Starting the TrustedInstaller main loop.
2016-09-28 04:30:31, Info                  CBS    TrustedInstaller service starts successfully.
2016-09-28 04:30:31, Info                  CBS    SQM: Initializing online with Windows opt-in: False
2016-09-28 04:30:31, Info                  CBS    SQM: Cleaning up report files older than 10 days.
2016-09-28 04:30:31, Info                  CBS    SQM: Requesting upload of all unsent reports.
</code></pre>
<p>Taken from here <a href=""https://github.com/logpai/loghub/blob/master/Windows/Windows_2k.log"" rel=""nofollow noreferrer"">https://github.com/logpai/loghub/blob/master/Windows/Windows_2k.log</a></p>
<p>I want to read this file in a pandas dataframe with proper column headings. It is having multiple delimeters as <code>,</code> <code>space</code> and <code>tab</code>.
Please share your thoughts with a running code sample.</p>
","-4","Question"
"79460679","","<p>I am using Python to read many PDFs, and they are quite large (some have 40 pages, others 3000). Therefore, I need some optimization.
As such, these PDFs have tabulated data and everything is well-structured. From these tables, I need to match a record and extract a row from each PDF, but since there are so many, my kernel crashes...</p>
<p>I was using Pdfplumber and also tried with LlamaReadMarkdown. Any suggestions?
Here is my code</p>
<pre><code>    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages[1:]:
            tables = page.extract_table()
            if tables:
                df = pd.DataFrame(tables[1:], columns=tables[0])
                for index, row in df.iterrows():
                    if rut in row.values:
                        return df.loc[[index]]
    return None
</code></pre>
<p>Im thinking to process it by batch but it will be viable?</p>
","0","Question"
"79461108","","<p>I have a Pandas dataframe with 28 columns in total. Each one has a unique number after a name. I want to drop all the numbers from the columns but keep the name. How can I do that best?</p>
<p>Here is an example of the columns:</p>
<pre><code>Miscellaneous group | 00002928  Alcoholic Beverages | 0000292   Animal fats group | 000029
</code></pre>
<p>I tried <code>.rename()</code> already but to do this for 28 columns isn't efficient and is time consuming. It  also creates a very long coding cell in Google Colab Notebook.</p>
","0","Question"
"79461783","","<p>I'm trying to figure out how to create a candle stick chart with the data from pandas resample method. But let's start with this simple example:</p>
<pre><code>import pandas as pd
from datetime import datetime
from bokeh.plotting import figure, show

data = {
    'time': [
        datetime(2025, 1, 1),
        datetime(2025, 1, 2),
        datetime(2025, 1, 3),
    ],
    'open':  [10, 40, 20],
    'close': [40, 20, 30],
    'low':   [ 5, 10, 20],
    'high':  [40, 50, 35],
}

df = pd.DataFrame(data)

inc = df.close &gt; df.open
dec = df.open &gt; df.close

p = figure()
p.segment(df.index, df.high, df.index, df.low, color=&quot;black&quot;)
p.vbar(df.index[dec], 0.6, df.open[dec], df.close[dec], color=&quot;#eb3c40&quot;)
p.vbar(df.index[inc], 0.6, df.open[inc], df.close[inc], fill_color=&quot;white&quot;,
       line_color=&quot;#49a3a3&quot;, line_width=2)
show(p)
</code></pre>
<p>This gives me an expected chart:</p>
<p><a href=""https://i.sstatic.net/TpI1qLTJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TpI1qLTJ.png"" alt=""candlestick chart"" /></a></p>
<p>But let's say that I want to create the same chart but I have data in different format. In this format I don't have specific open/close/low/high values. I have just an array of dates and values. Like here:</p>
<pre><code>data = {
    'time': [
        datetime(2025, 1, 1, 6),
        datetime(2025, 1, 1, 10),
        datetime(2025, 1, 1, 14),
        datetime(2025, 1, 1, 18),
        
        datetime(2025, 1, 2, 6),
        datetime(2025, 1, 2, 10),
        datetime(2025, 1, 2, 14),
        datetime(2025, 1, 2, 18),

        datetime(2025, 1, 3, 6),
        datetime(2025, 1, 3, 10),
        datetime(2025, 1, 3, 14),
        datetime(2025, 1, 3, 18),
        
    ],
    'price': [
        10,  5, 40, 40,
        40, 10, 50, 20,
        20, 20, 35, 30,
    ]
}
</code></pre>
<p>I know that I can use resample method to group this values by days.</p>
<pre><code>df = pd.DataFrame(data)
resampler = df.resample('D', on='time', kind='period')
</code></pre>
<p>Now I can access &quot;open&quot; as <code>resampler.first()</code>, &quot;close&quot; as <code>resampler.last()</code>, &quot;low&quot; as <code>resampler.min()</code> and &quot;high&quot; as <code>resampler.max()</code>. So looks like I have all I need but I'm struggling to get it together to draw a chart. How can I do it? What will be the equivalent of <code>df.index</code> here?</p>
","0","Question"
"79464314","","<p>Pandas <code>astype()</code> appears to unexpectedly switch to performing in-place operations after loading data from a pickle file. Concretly, for <code>astype(str)</code>, the data type of the input dataframe values is modified. What is causing this behavior?</p>
<p>Pandas version: 2.0.3</p>
<p><strong>Minimal example:</strong></p>
<pre><code>import pandas as pd
import numpy as np

# create a test dataframe
df = pd.DataFrame({'col1': ['hi']*10 + [False]*20 + [np.nan]*30})

# print the data types of the cells, before and after casting to string
print(pd.unique([type(elem) for elem in df['col1'].values]))
_ = df.astype(str)
print(pd.unique([type(elem) for elem in df['col1'].values]))

# store the dataframe as pkl and directly load it again
outpath = 'C:/Dokumente/my_test_df.pkl'
df.to_pickle(outpath)
df2 = pd.read_pickle(outpath)

# print the data types of the cells, before and after casting to string
print(pd.unique([type(elem) for elem in df2['col1'].values]))
_ = df2.astype(str)
print(pd.unique([type(elem) for elem in df2['col1'].values]))
</code></pre>
<p><strong>Output:</strong></p>
<p><a href=""https://i.sstatic.net/TMZX6icJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TMZX6icJ.png"" alt=""enter image description here"" /></a></p>
","1","Question"
"79464463","","<p>I hope I am explaining this correctly.
I have a dataframe in which i need to identify pairs of rows based on the string value of two columns. Each row in the pair must have a different string value in another column. I then need to add a new value to a new column based on TRUE or FALSE condition of that third column AND the condition of the pairing.</p>
<p>For example. A simplified version of the df would be:
<img src=""https://i.sstatic.net/zOX4c185.png"" alt=""Before"" /></p>
<p>The end result would look like this:
<img src=""https://i.sstatic.net/fznjAIF6.png"" alt=""After, with new_col"" /></p>
<p>Any help would be greatly appreciated.</p>
","2","Question"
"79464633","","<p>In the below table how can we copy the column tempx cells for each test from partition column long cell to the corresponding test cell<br></p>
<p>For example when we filter Scenario Column cell A1.results.0.test1 , it has two rows corresponding to each partition type;<br>
Here need to make sure both rows Temp1 to Temp5 columns have the same cell value as that present in the long cell (partition column)</p>
<p>**</p>
<ul>
<li><p>Input Table
<a href=""https://i.sstatic.net/wixVF5sY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wixVF5sY.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Expected Result Output Table
<a href=""https://i.sstatic.net/ykO7SYX0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ykO7SYX0.png"" alt=""enter image description here"" /></a>
Please suggest how can we copy the cells for each test</p>
</li>
</ul>
<pre><code>import openpyxl as op
import os
import datetime

input_file_path = r&quot;C:\deleteme_flat&quot;
result_filename = &quot;Report_Summary_&quot; + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + &quot;.xlsx&quot;
result_file_path = os.path.join(input_file_path, result_filename)
results = list()


def job_summary_gen():
    # this is the extension you want to detect
    extension = '.xlsx'
    file_list = []
    i = 0
    new_wb = op.Workbook()
    # sheet name update
    new_wb['Sheet'].title = &quot;Summary&quot;
    # wb2 = op.load_workbook(result_file_path)
    ws2 = new_wb['Summary']

    # If there are any rows already populated in the destination sheet start at next row otherwise start at row 1
    if ws2.max_row == 1:
        new_row = ws2.max_row
    else:
        new_row = ws2.max_row + 1

    for root, dirs_list, files_list in os.walk(input_file_path):
        for file_name in files_list:
            if os.path.splitext(file_name)[-1] == extension and file_name == 'testReport.xlsx':
                file_name_path = os.path.join(root, file_name)
                print(file_name_path)  # This is the full path of the filter file
                file_list.append(file_name_path)
                file_dir = os.path.dirname(file_name_path)
                folder_name = os.path.basename(file_dir)
                print(folder_name)
                wb1 = op.load_workbook(file_name_path)
                ws1 = wb1['Summary']
                # read flat Report
                for cell in ws1.iter_rows(min_col=3, max_col=3):
                    # print(cell[0].value)
                    # creating the header
                    if 'Partition' in cell[0].value.lower() and i &lt; 1:
                        # print(&quot;header workbook&quot;),
                        # Add 'File #' to first cell in destination row using row number as #
                        ws2.cell(row=new_row, column=1).value = &quot;File Name&quot;
                        for x in range(2, ws1.max_column + 1):
                            # Read each cell from col 1 to last used col
                            cell_value = ws1.cell(row=cell[0].row, column=x)
                            # Write last read cell to next empty row
                            ws2.cell(row=new_row, column=x).value = cell_value.value
                        # Increment to next unused row
                        new_row += 1
                        i += 1

                    # search for 'long' lower case against the cell value converted to lower case
                    if 'Partition' not in cell[0].value.lower() and 'long' in cell[0].value.lower():
                        # Add 'File name' to first cell in destination row using row number as #
                        ws2.cell(row=new_row, column=1).value = folder_name
                        # Copy cells Temp1 to Temp5
                        for x in range(2, ws1.max_column + 1):
                            # Logic to copy cells Temp1 to Temp5 from the long row to test row

                        # Increment to next unused row
                        new_row += 1
    new_wb.save(result_file_path)


# create_report()
job_summary_gen()
</code></pre>
","-1","Question"
"79464768","","<p>I am creating a catboost pool from a pandas dataframe (columns have strings as names, not sure if thats relevant) and then quantizing it and saving to disk using this code:</p>
<pre><code>import catboost as cb
import pandas as pd

data = {
    'label': np.random.randint(0, 5, 100),  # Random integers 0-4 for label
    'feature1': np.random.randn(100),      # Random normal distribution
    'feature2': np.random.randn(100),
    'feature3': np.random.randn(100),
    'feature4': np.random.randn(100),
    'feature5': np.random.randn(100),
    'feature6': np.random.randn(100),
    'feature7': np.random.randn(100),
    'feature8': np.random.randn(100),
    'feature9': np.random.randn(100)
}
train = pd.DataFrame(data)
factors = train.columns.values.tolist()[1:]
pool = cb.Pool(data = train[factors], label = train['label'])
pool.quantize()
pool.save('cbpool')
pool2 = cb.Pool('cbpool')
</code></pre>
<p>This will yield the following error on the last line:</p>
<pre><code>---------------------------------------------------------------------------

CatBoostError                             Traceback (most recent call last)

&lt;ipython-input-10-440a13636ec9&gt; in &lt;cell line: 0&gt;()
----&gt; 1 pool = cb.Pool('/content/drive/MyDrive/temp/traincb'+endtrain.strftime(&quot;%Y%m%d&quot;))

1 frames

/usr/local/lib/python3.11/dist-packages/catboost/core.py in __init__(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, graph, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)
    785                             &quot;feature_names should have None or string or pathlib.Path type when the pool is read from the file.&quot;
    786                         )
--&gt; 787                     self._read(data, column_description, pairs, graph, feature_names, delimiter, has_header, ignore_csv_quoting, thread_count)
    788                 else:
    789                     if isinstance(data, FeaturesData):

/usr/local/lib/python3.11/dist-packages/catboost/core.py in _read(self, pool_file, column_description, pairs, graph, feature_names_path, delimiter, has_header, ignore_csv_quoting, thread_count, quantization_params, log_cout, log_cerr)
   1334                     item = ''
   1335             self._check_thread_count(thread_count)
-&gt; 1336             self._read_pool(
   1337                 pool_file,
   1338                 column_description,

_catboost.pyx in _catboost._PoolBase._read_pool()

_catboost.pyx in _catboost._PoolBase._read_pool()

CatBoostError: library/cpp/string_utils/csv/csv.cpp:30: RFC4180 violation: quotation mark must be in the escaped string only
</code></pre>
","0","Question"
"79466365","","<p>I have a CSV file in AWS S3 bucket, which is having below sample data.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>First_name</th>
<th>job_level</th>
</tr>
</thead>
<tbody>
<tr>
<td>Andrew \</td>
<td>SSE</td>
</tr>
<tr>
<td>Kyle</td>
<td>SE</td>
</tr>
</tbody>
</table></div>
<p>while loading csv file to pandas, this is giving me error as the</p>
<pre><code>End of record reached while expected to parse column [&quot;job_level&quot;:2]
</code></pre>
<p>I tried to replace &quot;\&quot; from above data using <code>csv_data.replace(r'\\', '')</code> and <code>csv_data.replace(r'\,', ',')</code>. This is not working as this is giving me below result while printing</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>First_Name</th>
<th>Job_level</th>
</tr>
</thead>
<tbody>
<tr>
<td>Andrew,SSE</td>
<td>Nan</td>
</tr>
<tr>
<td>Kyle</td>
<td>SE</td>
</tr>
</tbody>
</table></div>
<p>**
Expected Result:**</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>First_Name</th>
<th>Job_level</th>
</tr>
</thead>
<tbody>
<tr>
<td>Andrew</td>
<td>SSE</td>
</tr>
<tr>
<td>Kyle</td>
<td>SE</td>
</tr>
</tbody>
</table></div>
","1","Question"
"79467071","","<p>I have code that aims to generate a graph from an adjacency matrix from a table correlating workers with their manager. The source is a table with two columns (Worker, manager). It still works perfectly from a small mock data set, but fails unexpectedly with the real data:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import networkx as nx

# Read input
df = pd.read_csv(&quot;org.csv&quot;)

# Create the input adjacency matrix
am = pd.DataFrame(0, columns=df[&quot;Worker&quot;], index=df[&quot;Worker&quot;])
# This way, it is impossible that the dataframe is not square,
# or that index and columns don't match

# Fill the matrix
for ix, row in df.iterrows():
    am.at[row[&quot;manager&quot;], row[&quot;Worker&quot;]] = 1

# At this point, am.shape returns a square dataframe (2825,2825)
# Generate the graph
G = nx.from_pandas_adjacency(am, create_using=nx.DiGraph)
</code></pre>
<p>This returns: <code>NetworkXError: Adjacency matrix not square: nx,ny=(2825, 2829)</code></p>
<p>And indeed, the dimensions reported in the error are not the same as in those of the input dataframe <code>am</code>.</p>
<p>Does anyone have an idea of what happens in <code>from_pandas_adjacency</code> that could lead to this mismatch?</p>
","2","Question"
"79467265","","<p>I want to append data from a Pandas <code>DataFrame</code> to both an existing excel sheet and an excel table (<code>ListObject</code>). I'm using <code>openpyxl</code> to do so.</p>
<p>I'm now writing the code for writing to a sheet. I have the following:</p>
<pre class=""lang-py prettyprint-override""><code>def _append_to_excel_sheet(self
                        , data_to_write: pd.DataFrame
                        , excel_file: str
                        , sheet_name: str
                        , **kwargs
                        ) -&gt; bool:
    
    try:
        if Path(excel_file).exists():
            # Load existing workbook
            self.logger.debug(f&quot;Appending {len(data_to_write)} rows to sheet {sheet_name} in {excel_file}&quot;)
            with open (excel_file, &quot;rb&quot;) as f:
                wb = load_workbook(f
                    , read_only=False
                    , keep_vba=True
                    , data_only=False
                    , keep_links=True
                    , rich_text=True
                    )
            self.logger.debug(wb.sheetnames)
            
            ws = wb[sheet_name] if sheet_name in wb.sheetnames else wb.create_sheet(sheet_name)
            
            # Find last row with data
            last_row = ws.max_row
            
            # Write new data
            for idx, row in enumerate(data_to_write.values):
                self.logger.debug(f&quot;Appending row: {row} to row: {last_row + idx + 1}&quot;)
                for col_idx, value in enumerate(row, 1):
                    ws.cell(row=last_row + idx + 1, column=col_idx, value=value)
            
            self.logger.debug(f&quot;New range: {ws.cell(row=last_row + 1, column=1).coordinate}:{ws.cell(row=last_row + len(data_to_write), column=len(data_to_write.columns)).coordinate}&quot;)
            self.logger.debug(f&quot;Saving to file {excel_file}&quot;)
            wb.save(excel_file)
            
        else:
            # Create new file if it doesn't exist
            self.logger.debug(f&quot;Creating new file {excel_file} and writing {len(data_to_write)} rows to sheet {sheet_name}&quot;)
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                data_to_write.to_excel(
                    writer,
                    sheet_name=sheet_name,
                    index=False
                )
                
    except Exception as e:
        self.logger.error(f&quot;Failed to write to Excel: {str(e)}&quot;)
        raise

    finally:
        wb.close()
</code></pre>
<p>When I run this code, the <code>logger</code> object reaches the <code>self.logger.debug(f&quot;Saving to file {excel_file}&quot;)</code> line without raising any exception. Furthermore, I never see an <code>Exception</code> raised: <code>self.logger.error(f&quot;Failed to write to Excel: {str(e)}&quot;)</code> is unreachable as far as my tests are concerned.</p>
<p>I have looked at <a href=""https://openpyxl.readthedocs.io/en/3.1.3/tutorial.html"" rel=""nofollow noreferrer"">openpyxl's documentation</a> and several answers to similar questions in SO (<a href=""https://stackoverflow.com/questions/29901558/writing-data-into-excel-sheet-using-openpyxl-isnt-working"">this one</a> and <a href=""https://stackoverflow.com/questions/56657204/cant-write-data-into-excel-sheet-using-openpyxl-workbook"">this one</a> primarily) but haven't been able to find what I'm doing wrong in my code. The <code>str</code> path I pass to the function is absolute.</p>
<p>I know I can <a href=""https://stackoverflow.com/questions/38074678/append-existing-excel-sheet-with-new-dataframe-using-python-pandas"">use pandas to append a DataFrame to an existing sheet</a>, and this would be an ideal solution but I also want this functionality with Excel tables.</p>
<p>Is there a way to enable verbose mode to see what <code>openpyxl</code> is doing behind the scenes? Am I missing an edge case in which saving a workbook with the same name is forbidden? What alternatives can I look into if I'm unable to fix the issue I have?</p>
<hr />
<p><em><strong>Edit</strong></em></p>
<p>To add more context on how I'm calling the code, the function is a method of a class called <code>ExcelOutputHandler</code>. I'm calling it from a <code>Unittest.TestCase</code> as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from datetime import datetime
from pathlib import Path
import sys
import unittest
from src.core.types import City # this is a NamedTuple
from src.output_handler import ExcelOutputHandler


import logging

class test_output_handler(unittest.TestCase):
    @classmethod
    def setUpClass(cls) -&gt; None:
        cls.logger = logging.getLogger()
        cls.logger.setLevel(logging.DEBUG)
        cls.logger.addHandler(logging.StreamHandler(sys.stdout))
        cls.test_dir = Path(__file__).parent

        cls.xl_testfile = cls.test_dir / f&quot;./output_history/{datetime.now().strftime(&quot;%Y-%m-%d %H-%M-%S&quot;)}.xlsx&quot;

        with open(cls.test_dir / &quot;./test_worksheet.xlsx&quot;, &quot;rb&quot;) as template_testfile:
            with open(cls.xl_testfile, &quot;wb+&quot;) as testfile:
                testfile.write(template_testfile.read())


    @classmethod
    def tearDownClass(cls) -&gt; None:
        pass


    def setUp(self) -&gt; None:
        
        pass


    def tearDown(self) -&gt; None:
        pass


    def test_write_to_sheet_overlay(self) -&gt; None:
        handler = ExcelOutputHandler(self.logger)

        data = [
            City('London', 'UK', 'EU', 'Rainy', 50, 'S', 5)
            , City('Paris', 'FR', 'EU', 'Sunny', 10, 'A', 6)
            , City('Berlin', 'DE', 'EU', 'Cold', 20, 'A', 3)
            , City('Brussels', 'BE', 'EU', 'Cold', 10, 'B', 6)
            , City('Lisbon', 'PT', 'EU', 'Sunny', 20, 'S+', 7)
            , City('Oslo', 'NW', 'EU', 'Cold', 10, 'S', 3)
            , City('Vienna', 'AT', 'EU', 'Cold', 10, 'A+', 8)
        ]

        handler._append_to_excel_sheet(data, str(self.xl_testfile), &quot;Sheet2&quot;)
        handler._append_to_excel_sheet(data, str(self.xl_testfile), &quot;Sheet2&quot;)

        # Verify the results
        import openpyxl
        wb = openpyxl.load_workbook(str(self.xl_testfile))
        ws = wb[&quot;Sheet2&quot;]
        
        # Get the number of rows with data (excluding header)
        row_count = sum(1 for row in ws.iter_rows(min_row=2) if any(cell.value for cell in row))
        
        # Assert we have twice the number of data rows
        self.assertEqual(row_count, len(data) * 2, 
                        f&quot;Expected {len(data) * 2} rows but found {row_count}&quot;)
        
        wb.close()
        
</code></pre>
<p>In the test, I expect the excel file to have 14 data rows, but it always fails with <code>AssertionError: 0 != 14 : Expected 14 rows but found 0</code>. From <a href=""https://stackoverflow.com/questions/23667610/what-is-the-difference-between-setup-and-setupclass-in-python-unittest"">this answer</a> I assume the code called to set up the <code>xl_testfile</code> will copy the <code>test_worksheet</code> file and then allow the tests to access it.</p>
","0","Question"
"79467663","","<p>I'm making a file compiler that takes data from csv files and after every 8 files read concatenates the data into a separate csv file. The program is taking quite long to be able to do these functions. is there a more optimized way to go about this?</p>
<p>I'm currently reading the csv file into a pandas data frame then appending said data frames into a list to compile them for <code>pd.concat()</code> after.</p>
<p>edit:
The inputs used in the <code>pd.read_csv</code> call is the root's directory and the files name that's being read since im using os.walk to jump from folder to folder. The content in each of the folders is an inconsistent amount of csv files storing data for a model's MSE RMSE and MAE. the reason why im using a data frame is because im trying to use the data in each of the csv files for further data analysis(reason why it concatenates every 8 files is because each model has 8 outputs). All csv files have one row for a header and are 6 columns by 5 rows.</p>
<p>code snippet:</p>
<pre><code>data = []

data_value = pd.read_csv(os.path.join(root, file), sep='\t') #Reading data into df

data.append(data_value) # appending df to a list

pd.concat(data) #concatenating all data in list into a data frame
</code></pre>
","1","Question"
"79467698","","<p>I have a dataframe of 10899 rows × 32 columns where there are many cells containing data that start with -9. e.g. -99.0, -9, -9.678, etc.</p>
<p>How can I create a new dataframe from the original where all values beginning with '-9' are converted to NaN?</p>
<p>The code I used returned a new dataframe with no changes.</p>
<p>Let's call the original dataframe weatherData_original.</p>
<p>Here's a piece of weatherData_original:</p>
<pre><code>    Date    Solar   MaxRH   AvgAirTemp
0   3/1/1983    -9.00   -9.0    -99.00
1   3/2/1983    -9.00   -9.0    0.31
2   3/3/1983    -9.00   -9.0    -99.00
3   3/4/1983    -9.00   -9.0    8.62
4   3/5/1983    19.97   64.6    8.91
... ... ... ... ...
10894   12/27/2012  9.67    53.9    5.99
10895   12/28/2012  10.21   89.7    0.96
10896   12/29/2012  10.25   57.9    1.89
10897   12/30/2012  3.72    86.3    3.85
10898   12/31/2012  9.68    92.9    3.53
</code></pre>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>new_df = weatherData_original.replace(regex=r'^-9\d+', value=pd.NA) 

new_df = weatherData_original.replace(regex='^[-9].*', value=pd.NA)
</code></pre>
","0","Question"
"79467759","","<p>When working with adding df1, and df2 into two different tables, I want to ensure that df1 doesn't write if df2 fails or any of the middle steps fail. But this doesn't work when using &quot;write_pandas&quot; function to write pandas DF to table.</p>
<pre><code>session.sql(&quot;commit&quot;).collect()
session.write_pandas(df,table1,db,schema,auto_create_table=True)
raise Exception(&quot;transaction Test&quot;)
session.write_pandas(df,table2,db,schema,auto_create_table=True)
session.sql(&quot;commit&quot;).collect()
</code></pre>
<p>This basically should be able to rollback table1 insert after raising Exception, but it doesn't. We see the dataframe items appended to table1. Direct SQL Statements do work</p>
<pre><code>session.sql(&quot;commit&quot;).collect()
session.sql(&quot;insert into DB.SC.T1('col1','col2') values (99,'ninenine')&quot;)
raise Exception(&quot;transaction Test&quot;)
session.sql(&quot;insert into DB.SC.T2('col1','col2') values (88,'eightyeight')&quot;)
session.sql(&quot;commit&quot;).collect()
</code></pre>
<p>So here the first table doesn't get new values when it fails before committing. Is</p>
","0","Question"
"79467796","","<p>I have a dataset that I want to groupby, and then add some calculated columns based on conditions from other columns. I want the status to only include 'open' and 'closed', and I want the state to exclude 'FL'.</p>
<p><a href=""https://i.sstatic.net/CUJejWOr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CUJejWOr.png"" alt=""enter image description here"" /></a></p>
<p>So far I've got:</p>
<pre><code>df.where(((df['status']==&quot;open&quot;) or (df['status']==&quot;closed&quot;)) 
and (df['state']!=&quot;FL&quot;).groupby(['schoolID', 'SchoolName']).agg(count=('StudentID', 'count'))
</code></pre>
<p>but this returns an error:</p>
<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>
","0","Question"
"79467944","","<p>The goal is to compare two pandas dataframes considering a margin of error.</p>
<p>To reproduce the issue:</p>
<h3>Importing pandas</h3>
<pre><code>import pandas as pd
</code></pre>
<h3>Case one - same data dataframes</h3>
<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1,1,1], &quot;B&quot;: [2,2,2], &quot;C&quot;: [3,3,3]})
df2 = pd.DataFrame({&quot;A&quot;: [1,1,1], &quot;B&quot;: [2,2,2], &quot;C&quot;: [3,3,3]})
print(df1.compare(df2, result_names=('df1', 'df2')))

# The result is an empty dataframe
Empty DataFrame
Columns: []
Index: []
</code></pre>
<h3>Case two - different data dataframes</h3>
<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1,1,1], &quot;B&quot;: [2,2,2], &quot;C&quot;: [3,3,3]})
df2 = pd.DataFrame({&quot;A&quot;: [1,1,1], &quot;B&quot;: [2,2.2,2], &quot;C&quot;: [3,3,3]}) # Note that the second B value is 2.2
print(df1.compare(df2, result_names=('df1', 'df2')))

# The result is a dataframe showing differences
     B     
   df1  df2
1  2.0  2.2
</code></pre>
<p>The issue is that I want that it only considers differences greater than 0.5</p>
<h3>How I achieved it.</h3>
<pre><code>threshold = 0.5
df3 = df1.melt().reset_index().merge(df2.melt().reset_index(), on=&quot;index&quot;)
df3[&quot;diff&quot;] = (df3[&quot;value_x&quot;] - df3[&quot;value_y&quot;]).abs()
print(df3.loc[df3[&quot;diff&quot;] &gt; threshold])

# The result is an empty dataframe
Empty DataFrame
Columns: [index, variable_x, value_x, variable_y, value_y, diff]
Index: []
</code></pre>
<p>Is there a better way to do this? It takes a lot of time for a huge DF.</p>
<p>In time:</p>
<ul>
<li>This is only a reproducible example.</li>
<li>I am opened to use other libraries as Numpy.</li>
</ul>
","2","Question"
"79468372","","<p>I have a pandas dataframe with a timestamp index, I am grouping to get only hourly values and after a series of operations on the values of that hour I need to re-write the results to the original DF:</p>
<pre><code>for name, group in df.groupby(pd.Grouper(freq=&quot;1H&quot;)):
    if group.shape[0] &gt; 0:
        results = some_function(group) # Operations on the group, returns a list of labels same length of the group
        df.loc[group.index, 'results'] = results 
</code></pre>
<p>I am getting the error <code>ValueError: Must have equal len keys and value when setting with an iterable</code> but it only happens after many successful iterations (hours) in the for loop. Any ideas?</p>
","1","Question"
"79468954","","<p>I've been dealing with Python for the last few months and it still a little bit irritate me :)</p>
<p>But hope someone can explain how to plot some subplots from dataframe grouped by 2 columns using Matplotlib. Or give a link where I can find the information about this.</p>
<p>There is the example of dataframe:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>name</th>
<th>N</th>
<th>coeff</th>
<th>X1</th>
<th>Y1</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>F2</td>
<td>F2_1</td>
<td>1</td>
<td>0</td>
<td>0.56451</td>
</tr>
<tr>
<td>1</td>
<td>F2</td>
<td>F2_1</td>
<td>5</td>
<td>0</td>
<td>0.45005</td>
</tr>
<tr>
<td>2</td>
<td>F2</td>
<td>F2_2</td>
<td>1</td>
<td>25</td>
<td>0.53641</td>
</tr>
<tr>
<td>3</td>
<td>F2</td>
<td>F2_2</td>
<td>5</td>
<td>25</td>
<td>0.53641</td>
</tr>
<tr>
<td>4</td>
<td>F2</td>
<td>F2_3</td>
<td>1</td>
<td>55</td>
<td>0.45067</td>
</tr>
<tr>
<td>5</td>
<td>F2</td>
<td>F2_3</td>
<td>5</td>
<td>55</td>
<td>0.38828</td>
</tr>
<tr>
<td>6</td>
<td>F2</td>
<td>F2_4</td>
<td>1</td>
<td>85</td>
<td>0.33279</td>
</tr>
<tr>
<td>7</td>
<td>F2</td>
<td>F2_4</td>
<td>5</td>
<td>85</td>
<td>0.35315</td>
</tr>
<tr>
<td>8</td>
<td>F2</td>
<td>F2_5</td>
<td>1</td>
<td>95</td>
<td>0.2756</td>
</tr>
<tr>
<td>9</td>
<td>F2</td>
<td>F2_5</td>
<td>5</td>
<td>95</td>
<td>0.32571</td>
</tr>
<tr>
<td>10</td>
<td>F2</td>
<td>F2_6</td>
<td>1</td>
<td>120</td>
<td>0.159</td>
</tr>
<tr>
<td>11</td>
<td>F2</td>
<td>F2_6</td>
<td>5</td>
<td>120</td>
<td>0.27583</td>
</tr>
<tr>
<td>12</td>
<td>F2</td>
<td>F2_7</td>
<td>1</td>
<td>150</td>
<td>0.05648</td>
</tr>
<tr>
<td>13</td>
<td>F2</td>
<td>F2_7</td>
<td>5</td>
<td>150</td>
<td>0.21128</td>
</tr>
<tr>
<td>14</td>
<td>F3</td>
<td>F3_1</td>
<td>1</td>
<td>0</td>
<td>0.42757</td>
</tr>
<tr>
<td>15</td>
<td>F3</td>
<td>F3_1</td>
<td>5</td>
<td>0</td>
<td>0.32409</td>
</tr>
<tr>
<td>16</td>
<td>F3</td>
<td>F3_2</td>
<td>1</td>
<td>25</td>
<td>0.36033</td>
</tr>
<tr>
<td>17</td>
<td>F3</td>
<td>F3_2</td>
<td>5</td>
<td>25</td>
<td>0.2701</td>
</tr>
<tr>
<td>18</td>
<td>F3</td>
<td>F3_3</td>
<td>1</td>
<td>55</td>
<td>0.2161</td>
</tr>
<tr>
<td>19</td>
<td>F3</td>
<td>F3_3</td>
<td>5</td>
<td>55</td>
<td>0.17014</td>
</tr>
<tr>
<td>20</td>
<td>F3</td>
<td>F3_4</td>
<td>1</td>
<td>85</td>
<td>0.08191</td>
</tr>
<tr>
<td>21</td>
<td>F3</td>
<td>F3_4</td>
<td>5</td>
<td>85</td>
<td>0.10512</td>
</tr>
<tr>
<td>22</td>
<td>F3</td>
<td>F3_5</td>
<td>1</td>
<td>95</td>
<td>0.04468</td>
</tr>
<tr>
<td>23</td>
<td>F3</td>
<td>F3_5</td>
<td>5</td>
<td>95</td>
<td>0.09686</td>
</tr>
<tr>
<td>24</td>
<td>F3</td>
<td>F3_6</td>
<td>1</td>
<td>120</td>
<td>0.07025</td>
</tr>
<tr>
<td>25</td>
<td>F3</td>
<td>F3_6</td>
<td>5</td>
<td>120</td>
<td>0.033</td>
</tr>
<tr>
<td>26</td>
<td>F3</td>
<td>F3_7</td>
<td>1</td>
<td>150</td>
<td>0.10689</td>
</tr>
<tr>
<td>27</td>
<td>F3</td>
<td>F3_7</td>
<td>5</td>
<td>150</td>
<td>0.01334</td>
</tr>
</tbody>
</table></div>
<p>I group it by columns Name and Coeff and need to plot sublots for every Name with graphs for every Coeff. In this case it has to be 2 subplots for &quot;F2&quot; and &quot;F3&quot; with 2 graphs for coeff 1 and 5.</p>
<p>There is no problem when I group it by 1 column. I don't understand how it works with grouping by some columns</p>
","1","Question"
"79469073","","<p>I have a data frame collected from a CSV in the following format:</p>
<pre><code>Book Name,Languages
&quot;Book 1&quot;,&quot;['Portuguese','English']&quot;
&quot;Book 2&quot;,&quot;['English','Japanese']&quot;
&quot;Book 3&quot;,&quot;[Spanish','Italian','English']&quot;
...
</code></pre>
<p>I was able to convert the string array representation on the column Languages to a python array using transform, but now i'm struggling to find a way to group Books by language.</p>
<p>I would like to produce from this data set a dict like this:</p>
<pre><code>{
  'Portuguese': 'Book 1'
  'English': ['Book 1', 'Book 2', 'Book 3'],
  'Spanish': 'Book 3',
  'Italian': 'Book 3',
  'Japanese': 'Book 2'
}
</code></pre>
<p>I tried to look into groupby on the array column but could not figure out how to make each entry on the array a key to be used as grouping.</p>
<p>Any pointers would be really apreciated.</p>
","3","Question"
"79469509","","<p>I have a df, in which there is a column, namely &quot;foo&quot;.
In this column, each entry is a list of numbers.
I can keep the first 4 numbers for each entry in this column by <code>df[&quot;foo&quot;] = df[&quot;foo&quot;].apply(lambda row: row[0:4])</code>.
For example, the first entry is <code>0, 1, 2, 3, 4, 5, 6</code>.
Then, the resulting first entry would become <code>0, 1, 2, 3</code>.</p>
<p>Now, I want to add a number (let's say 10) to each element in each entry.
From <a href=""https://stackoverflow.com/questions/9304408/how-to-add-an-integer-to-each-element-in-a-list"">here</a>, one can add an integer to each element in a list by <code>new_list = [x+1 for x in my_list]</code>. This is what I want, using the same example: <code>10, 11, 12, 13, 14, 15, 16</code>.</p>
<p>I would like to do something like <code>df[&quot;foo&quot;] = df[&quot;foo&quot;].apply(lambda row: x+10 for x in row)</code>. But this naive approach does not work.</p>
<p>The error is <code>NameError: name 'row' is not defined</code></p>
","0","Question"
"79469956","","<p>I have a Pandas DataFrame that contains user interactions with products in an e-commerce setting. This is the first 5 rows of the Dataframe:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>event_time</th>
<th>event_type</th>
<th>product</th>
<th>user</th>
</tr>
</thead>
<tbody>
<tr>
<td>593492</td>
<td>2019-12-31 00:27:00</td>
<td>view</td>
<td>9789601696706.0</td>
<td>671</td>
</tr>
<tr>
<td>593491</td>
<td>2019-12-31 00:27:00</td>
<td>cart</td>
<td>5903868402694.0</td>
<td>764</td>
</tr>
<tr>
<td>593490</td>
<td>2019-12-31 00:27:00</td>
<td>view</td>
<td>322198.0</td>
<td>609</td>
</tr>
<tr>
<td>593489</td>
<td>2019-12-31 00:27:00</td>
<td>view</td>
<td>8693245572504.0</td>
<td>702</td>
</tr>
<tr>
<td>593488</td>
<td>2019-12-31 00:26:59</td>
<td>view</td>
<td>301133.0</td>
<td>808</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>It has to be noted that the event_type column can have 3 values:
<code>view, cart, purchase</code></p>
<p>I want to calculate, for each purchase event, how many times the same user has viewed and added the product to the cart before that specific purchase.</p>
<p>However, if a user makes multiple purchases of the same product, views and cart events should only be counted for the most recent purchase and not be carried over from a previous purchase.</p>
<p>For example, if a user views a product multiple times, makes a purchase, and then views the product again before making another purchase, I want to count only the views that happened after the first purchase but before the second one.</p>
<p>How can I efficiently compute these values (preferably without using the .apply function because the dataset is pretty big) while ensuring that views and cart events are only counted for the most recent purchase and not past purchases?</p>
<p>I have came up with this:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(&quot;&lt;path to data&gt;&quot;)

purchases = df[df['event_type'] == &quot;purchase&quot;]
# Filter views and carts before purchase
events = df[df['event_type'].isin([&quot;view&quot;, &quot;cart&quot;])]
merged = events.merge(purchases, on=['user', 'product'], suffixes=('', '_purchase'))
filtered = merged[merged['event_time'] &lt; merged['event_time_purchase']]

# Count occurrences
event_counts = filtered.groupby(['id_purchase', 'event_type']).size().unstack(fill_value=0)
event_counts = event_counts.rename(columns={'view': 'view_before_bought', 'cart': 'cart_before_bought'})

final_df = df.merge(event_counts, left_on='id', right_index=True, how='left').fillna(0)
</code></pre>
<p>But the result of my attempt is this (for one user and one product):</p>
<pre><code>id, event_time, event_type, product, user, cart_before_bought, view_before_bought
88, 2019-10-01 00:01:46, view, 4902778716786.0, 598, 0.0, 0.0
112, 2019-10-01 00:02:14, purchase, 4902778716786.0, 598, 0.0, 1.0
134, 2019-10-01 00:02:48, view, 4902778716786.0, 598, 0.0, 0.0
145, 2019-10-01 00:03:02, view, 4902778716786.0, 598, 0.0, 0.0
9714, 2019-10-02 00:36:33, view, 4902778716786.0, 598, 0.0, 0.0
9891, 2019-10-02 00:38:12, purchase, 4902778716786.0, 598, 0.0, 4.0
</code></pre>
<p>It counts all the previous view/cart events</p>
<h2>Update</h2>
<p>Data that could be used for testing (keep in mind that there are more user, product combinations):</p>
<pre><code>id, event_time, event_type, product, user
1, 2019-10-01 00:01:46, view, 4902778716786, 598
2, 2019-10-01 00:02:00, cart, 4902778716786, 598
3, 2019-10-01 00:02:14, purchase, 4902778716786, 598
4, 2019-10-01 00:02:48, view, 4902778716786, 598
5, 2019-10-01 00:03:02, view, 4902778716786, 598
6, 2019-10-02 00:36:33, view, 4902778716786, 598
7, 2019-10-02 00:37:00, cart, 4902778716786, 598
8, 2019-10-02 00:38:12, purchase, 4902778716786, 598
</code></pre>
<p>Preferred outcome:</p>
<pre><code>id, event_time, event_type, product, user, cart_before_bought, view_before_bought
1, 2019-10-01 00:01:46, view, 4902778716786, 598, 0, 0
2, 2019-10-01 00:02:00, cart, 4902778716786, 598, 0, 0
3, 2019-10-01 00:02:14, purchase, 4902778716786, 598, 1, 1
4, 2019-10-01 00:02:48, view, 4902778716786, 598, 0, 0
5, 2019-10-01 00:03:02, view, 4902778716786, 598, 0, 0
6, 2019-10-02 00:36:33, view, 4902778716786, 598, 0, 0
7, 2019-10-02 00:37:00, cart, 4902778716786, 598, 0, 0
8, 2019-10-02 00:38:12, purchase, 4902778716786, 598, 1, 3
</code></pre>
","-1","Question"
"79470048","","<p>I developed a Qt GUI that reads data from a database and displays it on the screen in a table. The table includes conditional formatting which colors cell backgrounds based on cell contents. For example, if the cell contains the letter 'G' for good, then the cell background gets colored green.</p>
<p>I'm currently working on a function that will export that table to an HTML document which I can convert to PDF. I have the data itself in <code>dfhealth</code> and the colors defined in <code>dfdefs</code>, both of which come from <code>pd.read_sql</code> statements.</p>
<pre><code>dfhealth
  NAME  STATUS
0 A     G
1 B     N

dfdefs
  STATUS COLOR
0 G      GREEN
1 N      YELLOW
</code></pre>
<p>My issue is with the function for defining the conditional formatting of the DataFrame that will later be converted to HTML:</p>
<pre><code>def condFormat(s, df=dfdefs): # &lt;-- Error on this line
    dcolors = {&quot;GREEN&quot;: &quot;rgb(146, 208, 80)&quot;,
               &quot;YELLOW&quot;: &quot;rgb(255, 255, 153)&quot;,
               &quot;RED&quot;: &quot;rgb(218, 150, 148)&quot;,
               None: &quot;rgb(255, 255, 255)&quot;}
    dftemp = df.query(&quot;STATUS == @s&quot;)
    scolor = dcolors[dftemp[&quot;COLOR&quot;].iloc[0]]
    return &quot;background-color: &quot; + scolor

def main():
    dfhealth = pd.read_sql(&quot;SELECT name, status FROM health&quot;, conn)
    dfdefs = pd.read_sql(&quot;SELECT status, color FROM definitions&quot;, conn)
    dfhealth.style.applymap(condFormat)
    html = dfhealth.to_html()
    return html
</code></pre>
<p>I get the following error on the line shown above: &quot;NameError: name 'dfdefs' is not defined&quot;. I can't figure out how to tell the <code>condFormat</code> function that it needs to compare the contents of each cell to the STATUS column of <code>dfdefs</code> to get the color. Thank you in advance for your help.</p>
","0","Question"
"79470317","","<p>I have a <code>df</code>. I would like to expand <code>df</code> with two new columns, namely <code>new_foo</code> and <code>new_bar</code>, by two existing columns, namely <code>old_foo</code> and <code>old_bar</code>, as input.</p>
<p>In <code>old_foo</code>, <code>old_bar</code>, <code>new_foo</code> and <code>new_bar</code> columns, each entry is a list of numbers.</p>
<p>I would like to create the new columns <code>new_foo</code> and <code>new_bar</code> as following:</p>
<pre><code>df[[&quot;new_foo&quot;, &quot;new_bar&quot;]] = df.apply(lambda row: a_func(
        row[&quot;original_foo&quot;][row.foo - bar : row.foo - bar + baz], 
        row[&quot;original_bar&quot;][row.foo - bar : row.foo - bar + baz]), axis=1, result_type=&quot;expand&quot;)
</code></pre>
<p><code>a_func</code> is a function that takes two lists of numbers and returns two lists of numbers.</p>
<p>I want to find a way so I don't need to repeatedly write the similar index part inside <code>[ ]</code>.
In other words, I want to simplify the above code snippet by creating a variable <code>start = row.foo - bar</code> so that the code becomes</p>
<pre><code>df[[&quot;new_foo&quot;, &quot;new_bar&quot;]] = df.apply(lambda row: a_func(
        row[&quot;original_foo&quot;][start : start + baz], 
        row[&quot;original_bar&quot;][start : start + baz]), axis=1, result_type=&quot;expand&quot;)
</code></pre>
<p>The part that prevents me from doing this is that <code>start</code> depends on <code>foo</code>, which is different for each row (i.e., <code>row.foo</code>).</p>
","0","Question"
"79470598","","<p>I have a Pandas DataFrame with mixed scalar and array-like data of different raw types (int, float, str). The DataFrame's types look like this:</p>
<pre><code>'col1', dtype('float64')
'col2', dtype('O') &lt;-- array, item type str
'col3', dtype('int64'
'col4', dtype('bool')
...
'colA', dtype('O') &lt;-- array, item type str
...
'colB', dtype('O') &lt;-- array, item type float
...
'colC', dtype('O') &lt;-- scalar, str
...
'colD', dtype('O') &lt;-- array, item type int
...
some more mixed data type columns
</code></pre>
<p>The length of the numeric array-like data is variable fron DataFrame row to row.</p>
<p>Currently, I try to write this DataFrame naively to a HDF5 file by</p>
<pre><code>with h5py.File(self.file_path, 'w') as f:
    f.create_dataset(dataset_name, data=dataframe)
</code></pre>
<p>This operation fails with the error message</p>
<pre><code>    tid = h5t.py_create(dtype, logical=1)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;h5py/h5t.pyx&quot;, line 1669, in h5py.h5t.py_create
  File &quot;h5py/h5t.pyx&quot;, line 1693, in h5py.h5t.py_create
  File &quot;h5py/h5t.pyx&quot;, line 1753, in h5py.h5t.py_create
TypeError: Object dtype dtype('O') has no native HDF5 equivalent
</code></pre>
<p>What is a way / the best way to write this data to HDF5?</p>
","0","Question"
"79470747","","<p>I have this dataframe, already sorted by ID then SEQ:</p>
<pre><code>x = pd.DataFrame({
    'ID':  [1, 1, 1, 1, 2, 2, 2],
    'SEQ': [1, 2, 3, 4, 1, 3, 4]
})
</code></pre>
<p>Inline:</p>
<pre><code>    ID  SEQ
0   1   1
1   1   2
2   1   3
3   1   4
4   2   1
5   2   3
6   2   4
</code></pre>
<p>And I want to combine the next <code>SEQ</code> value with the actual one, if and only if they share the same ID. So <code>ROW[y].SEQ + ROW[y+1].SEQ</code>.</p>
<p>So at the moment I do:</p>
<pre><code>x['SEQ'] = x['SEQ'].astype('str')
x['ID_NEXT'] = x['ID'].shift(-1)
x['SEQ_NEXT'] = x['SEQ'].shift(-1)
x['COMBINE'] = x['SEQ']+' - '+x['SEQ_NEXT']
x = x[x['ID']==x['ID_NEXT']]
</code></pre>
<p>And I obtain what I want:</p>
<pre><code>    ID  SEQ ID_NEXT SEQ_NEXT    COMBINE
0   1   1   1       2           1 - 2
1   1   2   1       3           2 - 3
2   1   3   1       4           3 - 4
4   2   1   2       3           1 - 3
5   2   3   2       4           3 - 4
</code></pre>
<p>Is there a more efficient way to do this ?</p>
","-1","Question"
"79471155","","<p>I have a problem with Pandas Series dtype automatic conversion.
Let's take a simple Dataframe</p>
<pre><code>data = {
   &quot;t&quot;: [1, 2, 3],
   &quot;x&quot;: [1.7, 8.5, 4.3]
}
df = pd.DataFrame(data)
print(df)
</code></pre>
<p>Here I have two columns, &quot;t&quot; is of type &quot;int&quot; and &quot;x&quot; is of type &quot;float&quot;.
Now if I want to extract a single row from this df:</p>
<pre><code>row_as_ser = df.iloc[0]
t_row_as_ser = row_as_ser[&quot;t&quot;]
print(row_as_ser)
</code></pre>
<p>The value from the &quot;t&quot; column gets converted to float. How can I disable this behaviour or re-convert the values to int if possible ?</p>
<p>I know Series is a single type so I would like it to be &quot;object&quot;.</p>
<p>I tried the convert_dtypes function but it keeps a float.
I know i can re-convert each element one by one but I have a lot of columns that can possibly change name.</p>
<p>For the moment, I extract my row using the query function:</p>
<pre><code>row_as_df = df.query(&quot;t == 3&quot;) // Let s name it row_as_df because query returns a DataFrame
t_row_as_df = row_as_df[&quot;t&quot;]
wanted_row = row_as_df.iloc[0] // The data I want, but it converts values like &quot;t&quot; to float
</code></pre>
<p>The data types are preserved because row_as_df is a DataFrame, but I can't use it because the variable &quot;t_row_as_df&quot; is a Series, I would need to use t_row_as_df.values[0], but my row is an argument in a function that would preferably accept a Series by default, and t_row_as_ser is an int (so it does not have a .values attribute).</p>
<p>Just to prove that what I want is possible: if I have a column &quot;s&quot; in the original dataframe that contains strings, then the extracted wanted_row would be a Series of dtype &quot;object&quot; and the value of &quot;t&quot; would be an int.</p>
","1","Question"
"79471484","","<p>I'm using Python 3.12.6 and pandas==2.2.3.</p>
<p>This is a simple code, that I've always used and always worked to read the first sheet of an excel file:</p>
<pre><code>df = pd.read_excel(file_path, engine='openpyxl', sheet_name=0, index_col=None)
</code></pre>
<p>However, I have an excel file that is behaving strangely. This is the sheet header, it has these columns:</p>
<pre><code>&quot;NOME&quot;, &quot;DATA INSCRIÇÃO &quot;, &quot;PROVA OBJETIVA&quot;, &quot;PROVA DISCURSIVA&quot;
</code></pre>
<p>However, note that there are a few line breaks in some cells that might be strange to utf8 encoding:</p>
<p>read_excel() only reads up to the column &quot;DATA INSCRIÇÃO&quot;.</p>
<pre><code>print(df.columns)

Index(['NOME', 'DATA INSCRIÇÃO '],
      dtype='object')
</code></pre>
<p>When I save this sheet to .csv and open with notepad, this is what I see:</p>
<pre class=""lang-none prettyprint-override""><code>NOME;DATA INSCRIÇÃO ;&quot;PROVA 
OBJETIVA&quot;;&quot;PROVA 
DISCURSIVA&quot;
</code></pre>
<p>I've noticed there are line breaks, as well as quotes, precisely on the problematic columns.
Anyone has any idea why it's breaking? Or a better way to read all the columns in Python?</p>
<p>If I save the sheet to .csv and read_csv(), it breaks with an encoding error, which I suspect is the problem. BUT, if I try this:</p>
<pre><code>df = pd.read_csv(csv_path, delimiter=';', encoding='latin1')
</code></pre>
<p>It works! If I'm interpreting this correctly, this tells me that there might be a latin1 encoded line break that read_excel can't read. The problem is: read_excel() has no encoding argument. I've looked at the other possible arguments to read_excel, but nothing seems to help. Any help would be greatly appreciated.</p>
","0","Question"
"79472665","","<p>I have the following list:</p>
<pre><code>plates = [[], [], [{'plate ID': '193a', 'ra': 98.0, 'dec': 11.0, 'sources': [[3352102441297986560, 99.28418829069784, 11.821604434173034], [3352465726807951744, 100.86164898224092, 12.756149587760696]]}], [{'plate ID': '194b', 'ra': 98.0, 'dec': 11.0, 'sources': [[3352102441297986560, 99.28418829069784, 11.821604434173034], [3352465726807951744, 100.86164898224092, 12.756149587760696]]}], [], [], [], [], [], [], [], [], [], []]
</code></pre>
<p>I'd need to loop <code>plates</code>, find the key <code>'sources'</code> and store some data to another list.</p>
<pre><code>import pandas as pd

matched_plates = []
matches_sources_ra = []
matches_sources_dec =[]

plates = [[], [], [{'plate ID': '193a', 'ra': 98.0, 'dec': 11.0, 'sources': [[3352102441297986560, 99.28418829069784, 11.821604434173034], [3352465726807951744, 100.86164898224092, 12.756149587760696]]}], [{'plate ID': '194b', 'ra': 98.0, 'dec': 11.0, 'sources': [[3352102441297986560, 99.28418829069784, 11.821604434173034], [3352465726807951744, 100.86164898224092, 12.756149587760696]]}], [], [], [], [], [], [], [], [], [], []] 
plates_df = pd.DataFrame(plates)
for idx, row in plates_df.iterrows():    
    if 'sources' in row.keys():
       print(row[&quot;plate ID&quot;])
       matched_plates.append([row[&quot;plate ID&quot;],len(row[&quot;sources&quot;])])
       matches_sources_ra.append(row[&quot;sources&quot;][0][1])
       matches_sources_dec.append(row[&quot;sources&quot;][0][2])
</code></pre>
<p>This code never enters the <code>if</code>, what am I doing wrong?</p>
<p>Thank you for your help</p>
","3","Question"
"79473246","","<p>I am trying to minimize the following code to a for loop, as I have 14 of the similar columns, or something in a fewer number of code lines. What would be a pythonic way to do it?</p>
<pre><code>df['fan1_rpm'] = np.random.randint(5675, 5725, 30)
df['fan2_rpm'] = np.random.randint(5675, 5725, 30)
df['fan3_rpm'] = np.random.randint(5675, 5725, 30)
df['fan4_rpm'] = np.random.randint(5675, 5725, 30)
.
.
.
.
</code></pre>
","0","Question"
"79473319","","<p>I am very new to Python, so please bear with me. I have this dataframe called <code>df_staging</code> (sample image) that summarizes the annual sales and income for multiple counties <code>(A1,A2,A3,A4,etc.)</code>.</p>
<p>I want to do the following:</p>
<ol>
<li>Estimate sum and mean of each county.</li>
<li>For each county, plot the sales and income for each year. Each county in 1 subplot.</li>
</ol>
<p>Ideally, I could use a for loop to perform 1 and 2. I have tried (doesn't work):</p>
<pre><code>ind_county = df_staging['County Name'].drop_duplicates()
ind_county = ind_county[:2]

for i in ind_county:
    ts_year = df_staging.loc[df_staging['County Name'] == ind_county[i]]
    ts_year.plot.bar(x = 'Year', y = 'Sales')
</code></pre>
<p>How would I accomplish this? Is it possible to do it without using a for loop? I am open to any suggestions and tips that anyone has.</p>
<p>Thank you.</p>
<p><a href=""https://i.sstatic.net/mG4TwvDs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mG4TwvDs.png"" alt=""Sample image"" /></a></p>
","0","Question"
"79473657","","<p>I have following code</p>
<pre><code>display(ch2m_cpaf.groupby('PROVIDING_COMPANY_CODE').apply(lambda df: df, axis=0))
</code></pre>
<p>If I will run it with <code>axis=0</code> argument, it raises</p>
<blockquote>
<p>TypeError: () got an unexpected keyword argument 'axis'</p>
</blockquote>
<p>But why <code>axis=0</code> argument is not part of lambda expression?</p>
<p>I tried putting it in parenthesis, it should just return Pandas' Series object</p>
","0","Question"
"79473658","","<p>How to define a Python function that takes in input a DataFrame df a string str that is one of its column names, so that</p>
<pre><code>def fun(x,col):
    return x.loc[x.col == 0]
</code></pre>
<p>I know this is quite pleonastic but it is didactical. Is it possible to use variables for dataframe columns? (apparently not...)</p>
<p>The following code does not work</p>
<pre><code>df = pandas.DataFrame({'Name': ...list of Irish people names..., 'Height':.... list of people's height)

x = 'Height'
</code></pre>
<p>I saw the solution to the question in here: <a href=""https://stackoverflow.com/questions/65815475/applying-function-to-a-dataframe-using-its-columns-as-parameters"">Applying function to a DataFrame using its columns as parameters</a></p>
<p>which I liked a lot as a fan of lambda whatever... but it does not apply (at least I cannot see and please help me on this, possibly).</p>
<p>Would it be something like:</p>
<pre><code>lambda x,col: x.loc[x.col==0], DataFrame, x.col)?
</code></pre>
<p>Thank you in advance</p>
","1","Question"
"79473874","","<p>I need to group by multiple columns on a dataframe and calculate the rolling mean in the group. But the original index needs to be preserved.</p>
<p>Simple python code below :</p>
<pre><code>data = {'values': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],
        'type':['A','B','A','B','A','B','A','B','A','B','A','B','A','B','A'],
       'type2':['C','D','C','D','C','D','C','D','C','D','C','D','C','D','C']}
df = pd.DataFrame(data)

window_size = 3

df['mean'] = (df.groupby(['type','type2'])['values'].rolling(window=3).mean().reset_index(drop=True))

print(df)
</code></pre>
<p>Output :</p>
<pre><code>    values type type2  mean
0        1    A     C   NaN
1        2    B     D   NaN
2        3    A     C   3.0
3        4    B     D   5.0
4        5    A     C   7.0
5        6    B     D   9.0
6        7    A     C  11.0
7        8    B     D  13.0
8        9    A     C   NaN
9       10    B     D   NaN
10      11    A     C   4.0
11      12    B     D   6.0
12      13    A     C   8.0
13      14    B     D  10.0
14      15    A     C  12.0
</code></pre>
<p>What I need :</p>
<pre><code>      values type type2  mean
0        1    A     C   1
1        2    B     D   2
2        3    A     C   2
3        4    B     D   3
4        5    A     C   3
5        6    B     D   4
6        7    A     C   5
7        8    B     D   6
8        9    A     C   7
9       10    B     D   8
10      11    A     C   9
11      12    B     D   10
12      13    A     C   11
13      14    B     D   12
14      15    A     C   13
</code></pre>
<p>The requirement is very simple. The mean has to be calcluated in the groups. So last row is group (A,C) . So it is 15+ 13(previous) + 11 (previous to previous because window is 3) = 39 /3 =13</p>
<p>Same for other rows.</p>
<p>But when I do this with level = 0  with below code I get</p>
<pre><code>df['mean'] = (df.groupby(['type','type2'])['values'].rolling(window=3).mean().reset_index(level=0,drop=True))

</code></pre>
<hr />
<pre><code>raised in MultiIndex.from_tuples, see test_insert_error_msmgs
  12690         if not value.index.is_unique:
  12691             # duplicate axis
  12692             raise err
  12693 
&gt; 12694         raise TypeError(
  12695             &quot;incompatible index of inserted column with frame index&quot;
  12696         ) from err
  12697     return reindexed_value, None

TypeError: incompatible index of inserted column with frame index
</code></pre>
<p>How to go about this simple requirement ?</p>
","2","Question"
"79473967","","<p>I have 2 dataframes:</p>
<p>df1:</p>
<pre><code>BA  Product FixedPrice  Vintage       DeliveryPeriod
A    foo     10.00      Vintage 2025     Mar25
B    foo     11.00      Vintage 2025     Dec25
B    foo     12.00      Vintage 2024     Sep25
C    bar      2.00      None             Nov25
</code></pre>
<p>df2:</p>
<pre><code>Service             DeliveryPeriod FloatPrice
ICE - Vintage 2025      Mar25          5.00
ICE - Vintage 2025      Dec25          4.00
ICE - Vintage 2024      Sep25          6.00
ICE - Vintage 2023      Nov25          1.00
</code></pre>
<p>How can I get a result like this:</p>
<pre><code>BA  Product FixedPrice  Vintage       DeliveryPeriod FloatPrice
A    foo     10.00      Vintage 2025     Mar25           5.00
B    foo     11.00      Vintage 2025     Dec25           4.00
B    foo     12.00      Vintage 2024     Sep25           6.00
C    bar      2.00      None             Nov25           0.00
</code></pre>
<p>I was going to use <code>merge</code> but the columns don't match and neither do the values. I need to get the float price based on if <code>Service</code> contains the value from <code>Vintage</code> and matches the delivery period.</p>
","0","Question"
"79475516","","<p>I have a list of strings , that I want to search in a particular column in a dataframe. I want to output the rows which has ALL of those values.</p>
<p>I have used the code below:</p>
<pre><code>kw = ['apple','banana','kiwi']
regex_val = &quot;|&quot;.join(kw)

df['col'].str.contains(regex_val)
</code></pre>
<p>This gives the output of any of the keywords is present. I want to output when ALL are present for a record.</p>
<p>Also the keywords list is not limited to 3. It can vary based on user input.</p>
<p>I have tried the str.contains , but it works if any of the keywords are present.</p>
","1","Question"
"79476439","","<p>I have a dataframe with a column of the <code>dateutil.relativedelta</code> type. When I try grouping by this column, I get the error <code>TypeError: '&lt;' not supported between instances of 'relativedelta' and 'relativedelta'</code>. I didn't find anythind in the pandas <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer"">page</a> that indicated that there restrictions on the types of columns that can be used to group the dataframe. Am I missing something?</p>
<p>Minimal example below:</p>
<pre><code>from dateutil.relativedelta import relativedelta
import pandas as pd
import itertools

def expand_grid(data_dict):
    rows = itertools.product(*data_dict.values())
    return pd.DataFrame.from_records(rows, columns=data_dict.keys())

ref_dates = pd.date_range(start=&quot;2024-06&quot;, end=&quot;2025-02&quot;, freq=&quot;MS&quot;).tolist()
windows = [relativedelta(months=-30),relativedelta(months=-18)]
max_horizon = [relativedelta(months=2)]  
params = expand_grid({'ref_date': ref_dates, 'max_horizon': max_horizon, 'window': windows})

for name, group in params.groupby(by=['window']): print(name)
</code></pre>
","0","Question"
"79476892","","<p>I have the following df:</p>
<pre><code>| day      | first mover    |
| -------- | -------------- |
| 1        |     1        |
| 2        |     1        |
| 3        |     0        |
| 4        |     0        |
| 5        |     0        |
| 6        |     1        |
| 7        |     0        |
| 8        |     1        |
</code></pre>
<p>i want to group this Data frame in the order bottom to top with a frequency of 4 rows. Furthermore if first row of group is 1 make all other entries 0. Desired output:</p>
<pre><code>| day      | first mover    |
| -------- | -------------- |
| 1        |     1        |
| 2        |     0        |
| 3        |     0        |
| 4        |     0        |
| 5        |     0        |
| 6        |     0        |
| 7        |     0        |
| 8        |     0        |
</code></pre>
<p>The first half i have accomplished. I am confuse about how to make other entries 0 if first entry in each group is 1.</p>
<pre><code>N=4
(df.iloc[::-1].groupby(np.arange(len(df))//N
</code></pre>
","1","Question"
"79477395","","<p>I have df as below</p>
<pre><code>User  Action
AA     Page1
AA     Page1
AA     Page2
BB     Page2
BB     Page3
CC     Page3
CC     Page3
</code></pre>
<p>Is there a way to get the count for different Pages for each user as a df. something like below</p>
<pre><code>User  Page1  Page2  Page3
AA        2      1      0
BB        0      1      1
CC        0      0      2
</code></pre>
<p>So far i have tried to get the overall count all the actions per user</p>
<pre><code>df['User'].value_counts().reset_index(name='Action')
</code></pre>
<p>What should i do to get the unique values of Action column as separate columns of a df and its count as values?</p>
","2","Question"
"79478558","","<p>I want to lookup column in dataframe (Lookup item) using pandas (or anything else). If the value is found in that column then corresponding value in another column (Corresponding item) of same dataframe is picked up with both recorded into another dataframe.</p>
<p>Example cols are:</p>
<ol>
<li>lookup_id = [111, 222, 333, 444, 777 , 1089 , 3562 ]</li>
<li>id_number = [111, 23, 333, 444, 10101 ,777 , 222 ]</li>
</ol>
<pre><code>items = arr.array('i', [111, 222, 333, 444, 777 , 1089 , 3562 ])
new_df = []

for item in items:
    lookup_id = item
    
    id_number = get_score_by_id(df_past, lookup_id)
    if id_number is not None:
       df_append = (f'{lookup_id}, {id_number}')

    else:
       df_append = ('NAN')

print(df_append)

</code></pre>
<p>The required output dataframe is shown below:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Lookup item</th>
<th>Corresponding item</th>
</tr>
</thead>
<tbody>
<tr>
<td>111</td>
<td>OK</td>
</tr>
<tr>
<td>222</td>
<td>OK</td>
</tr>
<tr>
<td>333</td>
<td>OK</td>
</tr>
<tr>
<td>444</td>
<td>OK</td>
</tr>
<tr>
<td>777</td>
<td>OK</td>
</tr>
<tr>
<td>1089</td>
<td>NAN</td>
</tr>
<tr>
<td>3562</td>
<td>NAN</td>
</tr>
</tbody>
</table></div>
<p>So here all are found so 'OK' returned and 1089 and 3562 do not exist so NAN recored.</p>
<p>The script has been developed to replace an xls VLOOKUP so it's a vlookup/append but cant get the append to add new fields in next row fresh df.</p>
<p>I can get it to work just printing the output but want the new_df populated and that’s primarily intent of question.</p>
<p>Thanks.</p>
","0","Question"
"79479347","","<p>I have been searching for this solution in the official site of Plotly, Plotly forum and this forum for 3 days, and did not find it. I tried these following quetsions:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/67168846/how-to-avoid-overlapping-text-in-a-plotly-scatter-plot"">How to avoid overlapping text in a plotly scatter plot?</a></li>
<li><a href=""https://stackoverflow.com/questions/52304003/make-x-axis-wider-and-y-axis-narrower-in-plotly"">Make X axis wider and Y axis narrower in plotly</a></li>
</ul>
<p>For example, here is the image:</p>
<p><a href=""https://i.sstatic.net/oTaqbeqA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oTaqbeqA.png"" alt=""Plotly’s infographic screenshot"" /></a></p>
<p>You can see that Angola’s, Cabo Verde’s, Mozambique’s, Portugal’s, and São Tomé’s trace lines are overlapping each other on the Y-axis. On the X-axis, these markers of the numbers of population of all these countries between the years 2022 and 2025 are overlapping each other.</p>
<p>Here is the CSV data:</p>
<pre class=""lang-none prettyprint-override""><code>&quot;Country&quot;,&quot;2022&quot;,&quot;2024&quot;,&quot;2025&quot;,&quot;2030&quot;,&quot;2040&quot;,&quot;2050&quot;
&quot;Angola&quot;,&quot;35.6M&quot;,&quot;37.9M&quot;,&quot;39M&quot;,&quot;45.2M&quot;,&quot;59M&quot;,&quot;74.3M&quot;
&quot;Brasil&quot;,&quot;210M&quot;,&quot;212M&quot;,&quot;213M&quot;,&quot;216M&quot;,&quot;219M&quot;,&quot;217M&quot;
&quot;Cabo Verde&quot;,&quot;520K&quot;,&quot;525K&quot;,&quot;527K&quot;,&quot;539K&quot;,&quot;557K&quot;,&quot;566K&quot;
&quot;Moçambique&quot;,&quot;32.7M&quot;,&quot;34.6M&quot;,&quot;35.6M&quot;,&quot;40.8M&quot;,&quot;52.1M&quot;,&quot;63.5M&quot;
&quot;Portugal&quot;,&quot;10.4M&quot;,&quot;10.4M&quot;,&quot;10.4M&quot;,&quot;10.3M&quot;,&quot;10.1M&quot;,&quot;9.8M&quot;
&quot;São Tomé and Príncipe&quot;,&quot;226K&quot;,&quot;236K&quot;,&quot;240K&quot;,&quot;265K&quot;,&quot;316K&quot;,&quot;365K&quot;
</code></pre>
<p>And here is the simple and small code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

fig = go.Figure()
df = pd.read_csv('assets/csv/luso_pop_proj_who_data.csv')

for col in df.columns[1:]:
  df[col] = df[col].replace({&quot;M&quot;: &quot;e+06&quot;, &quot;K&quot;: &quot;e+03&quot;}, regex = True).astype(float)
  
df_long = df.melt(id_vars = &quot;Country&quot;, var_name = &quot;Year&quot;, value_name = &quot;Population&quot;)
df_long[&quot;Year&quot;] = df_long[&quot;Year&quot;].astype(int)

fig = px.line(
  df_long,
  color = &quot;Country&quot;,
  height = 600,
  labels = 
  {
    &quot;Country&quot;: &quot;País&quot;,
    &quot;Population&quot;: &quot;Número de população&quot;, 
    &quot;Year&quot;: &quot;Ano&quot;
  },
  template = &quot;seaborn&quot;,
  text = &quot;Population&quot;,
  title = &quot;Projeção da população nos países lusófonos (2022-2050)&quot;,
  width = 1200,
  x = &quot;Year&quot;,
  y = &quot;Population&quot;,
)

fig.update_traces(
  hovertemplate = None,
  mode = &quot;lines+markers+text&quot;,
  showlegend = True,
  textfont = dict(size = 15),
  textposition = &quot;top center&quot;,
  texttemplate = &quot;%{text:.3s}&quot;,
)

fig.update_layout(
  margin = dict(l = 0, r = 0, b = 10, pad = 0),
  hovermode = False
)

fig.show(
  config = config
)
</code></pre>
<p>I have been testing with these following codes:</p>
<pre class=""lang-py prettyprint-override""><code>fig = px.line(
  facet_row_spacing = 1,
)

fig.update_traces(
  line = dict(
    backoff = 10,
  ),
)

fig.update_xaxes(
  automargin = True,
  autorange = False,
  fixedrange = True,
  range = ['2020', '2050'],
)

fig.update_yaxes(
  automargin = True,
  fixedrange = True,
)

fig.update_layout(
  margin = dict(l = 0, r = 0, b = 10, pad = 0),
)
</code></pre>
<p>I expected that, with these configurations, the trace lines moved away with the increasing space, but it did not give any effect.</p>
","2","Question"
"79479705","","<p>I am looking for an idea how to change a diagram to make it fit to that picture:</p>
<p><a href=""https://i.sstatic.net/cwVZTYig.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cwVZTYig.png"" alt=""enter image description here"" /></a></p>
<p>Maybe it would make sense to make the output in a table to klick a row and show it in the diagram. But I have no idea which bib I can use.</p>
<p>The data I have already converted:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Excel-Datei lesen
df = pd.read_csv('Data/DrivingRange.csv', sep=',')
df = df.drop(index=0)

# Carry-Distanz
df['Carry-Distanz'] = pd.to_numeric(df['Carry-Distanz'], errors='coerce')
# Club speed in km/h
df['Schl.gsch.'] = pd.to_numeric(df['Schl.gsch.'], errors='coerce')
# Club Path
df['Schwungbahn'] = pd.to_numeric(df['Schwungbahn'], errors='coerce')
# Face Angle
df['Schlagfläche'] = pd.to_numeric(df['Schlagfläche'], errors='coerce')
# Face to Path
df['Schlagflächenstellung'] = pd.to_numeric(df['Schlagflächenstellung'], errors='coerce')

# Club speed in mph
df['Schl_gsch_mph'] = df['Schl.gsch.'] * 0.621371

# Sequence of clubs
sequence = ['Lob-Wedge', 'Sandwedge', 'Eisen 9', 'Eisen 8', 'Eisen 7', 'Eisen 6', 'Eisen 5', 'Hybrid 4', 'Driver']
df['Schlägerart'] = pd.Categorical(df['Schlägerart'], categories=sequence, ordered=True)

# Sort DataFrame by category
df_sorted = df.sort_values(by='Schlägerart')



df_sorted_filtered = df_sorted[['Schlägerart','Schl_gsch_mph', 'Carry-Distanz', 'Schwungbahn', 'Schlagfläche', 'Schlagflächenstellung']]
df_sorted_filtered.rename(columns={'Schlägerart': 'Club', 'Schl_gsch_mph': 'Club Speed', 'Carry-Distanz' : 'Carry Distance', 'Schwungbahn': 'Club Path', 'Schlagfläche' : 'Face Angle', 'Schlagflächenstellung' : 'Face to Path'}, inplace=True)
print(df_sorted_filtered)
</code></pre>
<p>Output:</p>
<pre><code>         Club  Club Speed  Carry Distance  Club Path  Face Angle  Face to Path
30  Lob-Wedge   67.779147       68.842956      -3.89        4.12          8.01
31  Lob-Wedge   67.712039       58.803586       5.71       20.79         15.08
1   Sandwedge   62.611826       76.651350      -1.75       -0.25          1.50
2   Sandwedge   62.007853       60.411199       2.02       -2.80         -4.82
3   Sandwedge   67.197544       93.361768       6.17       -5.94        -12.11
</code></pre>
","0","Question"
"79480120","","<p>I am using minmaxscaler trying to scaling each column. The scaled result for each column is always all zero.  For example , below the values of df_test_1 after finishing scaling is all zero. But even with all values of zero, using inverse_transferm from this values of zero can still revert back to original values. But why the results of scaled are shown all zero?</p>
<pre><code>from sklearn.preprocessing import MinMaxScaler
df_dict={'A':[-1,-0.5,0,1],'B':[2,6,10,18]}
df_test=pd.DataFrame(df_dict)
print('original scale data')
print(df_test)
scaler_model_list=[]
df_test_1=df_test.copy()
for col in df_test.columns:
   scaler = MinMaxScaler()
   scaler_model_list.append(scaler)  # need to save scalerfor each column since there are different if we want to use inverse_transform() later
   df_test_1.loc[:,col]=scaler.fit_transform(df_test_1.loc[:,col].values.reshape(1,-1))[0]

print('after finishing scaling')
print(df_test_1)
print('after inverse transformation')
print(scaler_model_list[0].inverse_transform(df_test_1.iloc[:,0].values.reshape(1,-1)))
print(scaler_model_list[1].inverse_transform(df_test_1.iloc[:,1].values.reshape(1,-1)))


original scale data
     A   B
0 -1.0   2
1 -0.5   6
2  0.0  10
3  1.0  18
after finishing scaling
     A  B
0  0.0  0
1  0.0  0
2  0.0  0
3  0.0  0
after inverse transformation
[[-1.  -0.5  0.   1. ]]
[[ 2.  6. 10. 18.]]

</code></pre>
","1","Question"
"79480437","","<p>On the Pandas dataframe below I use <code>max</code> to find the maximum value:</p>
<pre class=""lang-py prettyprint-override""><code>df[&quot;Max&quot;] = df[['Day 1','Day 2','Day 4']].max(axis=1)
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Car</th>
<th>Day 1</th>
<th>Day 2</th>
<th>Day 4</th>
<th>Max</th>
</tr>
</thead>
<tbody>
<tr>
<td>Car1</td>
<td>4</td>
<td>7</td>
<td>3</td>
<td>7</td>
</tr>
<tr>
<td>car2</td>
<td>8</td>
<td>2</td>
<td>1</td>
<td>8</td>
</tr>
</tbody>
</table></div>
<p>What do I do to find <em>when</em> is the maximum value instead of the value itself as the table example below?</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Car</th>
<th>Day 1</th>
<th>Day 2</th>
<th>Day 4</th>
<th>When</th>
</tr>
</thead>
<tbody>
<tr>
<td>Car1</td>
<td>4</td>
<td>7</td>
<td>3</td>
<td>Day 2</td>
</tr>
<tr>
<td>car2</td>
<td>8</td>
<td>2</td>
<td>1</td>
<td>Day 1</td>
</tr>
</tbody>
</table></div>
","1","Question"
"79481317","","<p>Context: Given a piece of paper with text/drawings sporadically printed on it, determine how many unmarked strips of a given width could be cut from it (without doing anything clever like offsetting the cuts according to text position). Data comes in the form of a pandas DataFrame with x0 and x1 columns, which are the bounding box values in the relevant dimension.</p>
<p>My initial attempt to solve this was to use <code>pd.cut()</code> and <code>DataFrame.groupby()</code> to bin everything by an <code>np.arange()</code> and count the empty bins, but this only works if x1-x0 &lt; strip_width:</p>
<pre><code>def count_unmarked_strips(df, total_width, strip_width):
    bins = np.arange(0, total_width, strip_width)
    count_min = df.groupby(pd.cut(df['x0'], bins=bins))['x0'].count().to_numpy()
    count_max = df.groupby(pd.cut(df['x1'], bins=bins))['x1'].count().to_numpy()
    return ((count_min + count_max) == 0).sum()
</code></pre>
<p>This already seems like a bad way to do this and I cannot come up with a way to extend this to work beyond the above limitation. More generally, is there a good/reasonably performant way to compare two sets of intervals like this?</p>
<p>Update with alternate solution:</p>
<pre><code>def count_unmarked_strips_alt(df, total_width, strip_width):
    bin_edges = np.arange(0, total_width, strip_width)
    bins = np.zeros(len(bin_edges)-1, dtype=bool)
    for i in range(len(bins)):
        bin_count = df.loc[
            ( # if writing starts in strip
                (df['x0'] &gt;= bin_edges[i])
                &amp; (df['x0'] &lt;= bin_edges[i+1])
            )
            | ( # if writing ends in strip
                (df['x1'] &gt;= bin_edges[i])
                &amp; (df['x1'] &lt;= bin_edges[i+1])
            )
            | ( # if writing crosses strip boundary
                (df['x0'] &lt;= bin_edges[i])
                &amp; (df['x1'] &gt;= bin_edges[i])
            )
        ].shape[0]
        bins[i] = bin_count == 0
    return bins.sum()
</code></pre>
<p>This seems to work, but is much slower to run, which I would like to improve.</p>
<p>Example data:</p>
<pre><code>total_width = 20.
strip_width = 2. # both functions give the same result
# strip_width = 1. # first function overcounts unmarked strips

df = pd.DataFrame()
df['x0'] = [0.1, 6.3, 2.5, 17.2, 11.8, 5.3]
df['x1'] = [0.7, 7.9, 5.4, 19.0, 12.2, 5.7]
</code></pre>
<p>so, e.g. we have a 20cm width of paper with 6 markings on of different sizes, bounded on the left hand side by x0 and on the right hand side by x1. If we were to cut the paper into equal strips with 2cm width, we would find 2 of them would be unmarked.</p>
","0","Question"
"79481878","","<p>How can I format numbers in a <code>pandas</code> table, when a cell displays a list of floating point numbers instead of a single float value?</p>
<p>Here's a code example:</p>
<pre><code>import pandas as pd
df = pd.DataFrame(data={'Values':[[0.1231245678,0,0],[1e-10,0,0]]})
df
</code></pre>
<p>I would like to format the numbers in the table as <code>%.2f</code>. So the table data should be displayed as</p>
<pre><code>[0.12, 0.00, 0.00]
[0.00, 0.00, 0.00]
</code></pre>
<p>The usual options:</p>
<pre><code>pd.set_option('float_format','{:20,.2f}'.format)
pd.set_option('display.chop_threshold', 0.001)
</code></pre>
<p>only work when table cells contain single numbers.</p>
","0","Question"
"79482097","","<p>So I am trying to style a pandas dataframe on a streamlit app. I was able to get it to style appropriately with the following code:</p>
<pre><code>df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9], 'X': [10, 11, 12], 'Y': [13, 14, 15], 'Z': [16, 17, 18]})

default_colors = {
    &quot;A&quot;: &quot;brown&quot;,
    &quot;B&quot;: &quot;blue&quot;,
    &quot;C&quot;: &quot;green&quot;,
    &quot;X&quot;: &quot;orange&quot;,
    &quot;Y&quot;: &quot;red&quot;,
    &quot;Z&quot;: &quot;purple&quot;
}


def highlight_columns(val, color):
    return f'color: {color}; font-weight: bold;' if val else ''

if 'A' in df.columns:
    styled_df.applymap(lambda val: highlight_columns(val, default_colors['A']), subset=['A'])

if 'B' in df.columns:
    styled_df.applymap(lambda val: highlight_columns(val, default_colors['B']), subset=['B'])

if 'C' in df.columns:
    styled_df.applymap(lambda val: highlight_columns(val, default_colors['C']), subset=['C'])

if 'X' in df.columns:
    styled_df.applymap(lambda val: highlight_columns(val, default_colors['X']), subset=['X'])

if 'Y' in df.columns:
    styled_df.applymap(lambda val: highlight_columns(val, default_colors['Y']), subset=['Y'])

if 'Z' in df.columns:
    styled_df.applymap(lambda val: highlight_columns(val, default_colors['Z']), subset=['Z'])
</code></pre>
<p>But upon refactoring with:</p>
<pre><code>for column in 'ABCXYZ':
    if column in df.columns:
        styled_df.applymap(lambda val: highlight_columns(val, default_colors[column]), subset=[column])
</code></pre>
<p>All columns iterated over are updated to the the final columns color. In this case column 'Z'. What is going on here? Why do the 6 if statements work fine but the loop doesn't?</p>
","0","Question"
"79482376","","<p>I want to drop the first group of rows based on a column's value. Here is an example of a table</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>stage</th>
<th>h1</th>
<th>h2</th>
<th>h3</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>4</td>
<td>55</td>
<td>55</td>
</tr>
<tr>
<td>0</td>
<td>5</td>
<td>66</td>
<td>44</td>
</tr>
<tr>
<td>0</td>
<td>4</td>
<td>66</td>
<td>33</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>33</td>
<td>55</td>
</tr>
<tr>
<td>0</td>
<td>5</td>
<td>44</td>
<td>33</td>
</tr>
</tbody>
</table></div>
<p>Get the column stage, get all the first group of rows that start with 0, and drop the rows in the table. The table will look like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>stage</th>
<th>h1</th>
<th>h2</th>
<th>h3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3</td>
<td>33</td>
<td>55</td>
</tr>
<tr>
<td>0</td>
<td>5</td>
<td>44</td>
<td>33</td>
</tr>
</tbody>
</table></div>
<p>This is what I did:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {'stage': [0, 0, 0, 1, 0],
        'h1': [4, 5, 4, 3, 5],
        'h2': [55, 66, 66, 33, 44],
        'h3': [55, 44, 33, 55, 33]}

df = pd.DataFrame(data)

# Find indices of the first group of rows with uiwp_washing_stage = 0
indices_to_drop = []
for i in range(len(df)):
    if df['stage'].iloc[i] == 0:
        indices_to_drop.append(i)
    else:
        break

df = df.drop(indices_to_drop)
df = df.reset_index(drop=True)
print(df)
</code></pre>
<p>The above seems to work, but if the file is too big it takes a while, is there a Pands way of doing this?</p>
","1","Question"
"79483085","","<p>I have a Pandas dataframe with some values and a Series with corresponding maximum allowed values.<br />
Next, I check dataframe and see, that some values are greater maximum allowed values and I need to replace them with values from Series.</p>
<p>How can I do this maximum Python and Pandas way?</p>
<p>Thank you!</p>
<p>Example code:</p>
<pre><code>import pandas as pd

max_vals = pd.Series([3, 4, 5, 6, 7], index=['a', 'b', 'c', 'd', 'e'])
data = pd.DataFrame([[1,2,7,4,5],
                     [2,3,4,5,6],
                     [7,8,5,10,11]], 
                     columns=max_vals.index)

mask = data &gt; max_vals
data[mask]

    a   b   c   d   e
0   NaN NaN 7.0 NaN NaN
1   NaN NaN NaN NaN NaN
2   7.0 8.0 NaN 10.0 11.0

# How can I do something like this?

data[mask] = max_vals[mask]
</code></pre>
","1","Question"
"79483656","","<p>All,
The following function <code>get_frequency_of_events</code> detects the frequency of consecutive numbers, for example,</p>
<pre><code>import numpy as np
aa=np.array([1,2,2,3,3,3,4,4,4,4,5,5,5,5,5])
get_frequency_of_events(aa) 
</code></pre>
<p>this yields the following:</p>
<p>list of indices @ the beginning of each group [1, 3, 6, 10]</p>
<p>frequency of each group [2, 3, 4, 5]</p>
<p>another example,</p>
<pre><code>aa=np.array([1,1,1,np.nan,np.nan,1,1,np.nan])
idx, feq= get_frequency_of_events(aa)
</code></pre>
<p>list of indices @ the beginning of each group  [0, 5]</p>
<p>frequency of each group [3, 2]</p>
<p>Yet, this function is slow, especially when iterating it over 3D data. How can I vectorize such a function to achieve faster processing?</p>
<p>Here is the function</p>
<pre><code>def get_frequency_of_events(mydata):
    &quot;&quot;&quot; 
    Author  :  Shaaban
    Date    : Jan 22, 2025 
    Purpose : get the frequency of repeated consecutive numbers and their indices, this is important when finding the frequency of heatwaves and etc ... All we have to do is to build matrix of ones (or any other number), and NAN. One refers to the existence of the EVENT, and nan refers to the inexistence of the event. Then this function could give you a summary of the the frequency of the events and their associated indices. 
    tests : 
        
    aa=np.array([1,1,0,0,0,1,0,1,1,1,1,0,1,1])
    get_frequency(aa)
    
    aa=np.array([1,2,2,3,3,3,4,4,4,4,5,5,5,5,5])
    get_frequency(aa)

    aa=np.array([1,1,1,1,0,0,1,1,1])
    get_frequency(aa)

    aa=np.arange(10)
    get_frequency(aa)

    aa=np.ones(10)
    get_frequency(aa)
    
    # CAUTION CAUTION CAUTION 
    #For heatwave numbers, etc , make your array consits of fixed number (any number) that is associated with an evens and Nan for days/hours/month not associated with events. The trick here is that no nan could ever be equal to another nan. 
    
    aa=np.array([1,1,1,np.nan,np.nan,1,1,np.nan])
    idx, feq= get_frequency(aa)
    &quot;&quot;&quot;
    
    index_list=[]
    events_frequency_list=[]
    
    idx_last_num=len(mydata)-1
    
    counter=0
    ii=0
    while(ii &lt;= idx_last_num-1):
        #print( '@ index = '+str(ii) )
        counter=0
        while(mydata[ii] == mydata[ii+1]):
            print(' Find match @ '+str(ii)+' &amp; '+str(ii+1)+\
                  ' data are '+str(mydata[ii])+' &amp; '+str(mydata[ii+1]))
            # store the index of the first match of each group.
            if counter == 0:
                index_list.append(ii)
            ii=ii+1
            counter=counter+1
            # break from while if this is the last element in the array.
            if ii==idx_last_num:
                break
        # if we just were iniside loop, store the no of events
        if counter != 0:
            no_events=counter+1
            events_frequency_list.append(no_events)
        
        # counter if there is no match at all for the outer while. 
        ii=ii+1
    print('list of indices @ the begining of each group  ')
    print(index_list)
    print(' frequency of each group.')
    print(events_frequency_list)
    return index_list, events_frequency_list
</code></pre>
","1","Question"
"79483911","","<p>i have a data frame with two columns (name1, name2)
i would like to use a dictionary of column names and then do a for loop that compares if the values are the same and specifically show the values that are not the same</p>
<p>when i try the following i get an error &quot;ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()&quot;</p>
<pre><code># create df
test2 = {'NAME1': ['Tom', 'nick', 'krish', 'jack'],
        'NAME': ['Tom', 'nick', 'Carl', 'Bob']}
dfx = pd.DataFrame(test2)

#create dictionary
thisdict = {
  &quot;NAME1&quot;: &quot;NAME&quot;
}

#loop and display differences
for a, b in thisdict.items():
    if dfx[a] != dfx[b]:
        x = dfx[[a, b]]
        print(x)
</code></pre>
","-1","Question"
"79484795","","<p>I've done my due diligence and still can't find what I need to make this happen.</p>
<p>This is a sample dataset of what I'm using.  I need to add the column &quot;date_price_change&quot; for that particular Store/SKU combination where the &quot;price&quot; changed last.</p>
<p>I've tried-</p>
<p>df['date_price_change'] = df.groupby['store_nbr', 'product_sku']['date'].max()</p>
<p>but that's giving me an error, plus I need to make sure that the &quot;date change&quot; is less than the row's date.</p>
<p>TIA</p>
<p><a href=""https://i.sstatic.net/fzexpNT6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fzexpNT6.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79485072","","<p>I am attempting to unit test a function of mine that checks if something is in a database based on a bunch of &quot;identifiers&quot;. This is my very first time doing anything with sqlite3 or SQL at all so it's possible I'm making some very stupid mistake. This function has worked when I ran it on actual data, but for some reason is failing in this testing version. I am using Python 3.9, and sqlite3.</p>
<p>After adding several statements to try and debug, I've ended up with this in my code: (there's actually more columns but I shortened it, it just adds more AND to the WHERE statement, also these are obviously not the variable names in my actual code)</p>
<pre><code>import sqlite3

class TestObject:
  def __init__(self, db_loc):
    self.con = sqlite3.connect(db_loc)
    self.cur = self.con.cursor()

  def get_thingy(self, row):
    # this is applied to a pandas dataframe with .apply(lambda row: self.get_thingy(row), axis=1) so row is a pd.Series
    params = (row.col1, row.col2, str(row.col3), row.col4, row.col5)
    print(params)
    res = self.cur.execute(&quot;&quot;&quot;
    SELECT thingy FROM thingies
    WHERE (col1 = ? or col2 = ?) AND col3 = ? AND col4 = ? AND col5 = ?&quot;&quot;&quot;, params).fetchone()
    print(res)
    print(self.cur.execute(&quot;&quot;&quot;SELECT thingy FROM thingies 
      WHERE (col1 = 1 or col2 = 2) AND col3 = '3' AND col4 = 4 AND col5 = 5&quot;&quot;&quot;).fetchone())
    # and then more code that does stuff with the retrieved thingy

  def get_thingies(self):
    self.thingy_table['thingy'] = self.thingy_table.apply(lambda row: self.get_thingy(row), axis=1)
</code></pre>
<p>while my test looks like</p>
<pre><code>import pytest
import os
import pandas as pd
from [path to the other file] import TestObject

class TestClass:
  @pytest.fixture
  def testobj(self):
    db_path = 'test.db'
    test = TestObject(db_path)
    yield test
    if test.con: test.con.close()
    os.remove(db_path)
 
  def test_get_thingy(self, testobj):
    testobj.cur.execute(&quot;&quot;&quot;CREATE TABLE thingies(col1, col2, col3, col4, col5, thingy)&quot;&quot;&quot;)
    testobj.cur.execute(&quot;&quot;&quot;INSERT INTO thingies VALUES (1, 2, '3', 4, 5, &quot;yay thingy&quot;)&quot;&quot;&quot;)
    testobj.con.commit()
    testobj.get_thingy(pd.Series({'col1': 1, 'col2': 2, 'col3': 3, 'col4': 4, 'col5': 5}))
</code></pre>
<p>Running this test prints</p>
<pre><code>(1, 2, '3', 4, 5)
None
('yay thingy',)
</code></pre>
<p>I have tried copy/pasting the printed <code>params</code> list in to replace the <code>params</code> keyword in the .execute() command, and this caused both queries to return <code>yay thingy</code>. So something is strange about the <em>way</em> that I am accessing these values.</p>
<p>Why does it not work with <code>params</code> as an input, but if I print <code>params</code> and copy what is printed, that works? Shouldn't it be the exact same? Is something strange about taking these values from a pd.Series? The get_thingies function worked on actual data in a DataFrame, is a row of a DataFrame not actually a Series like I thought it was? I'm losing my mind.</p>
","0","Question"
"79485775","","<p>I would like to read in a csv-file. Sadly this is created by a buggy software: My header row contains multiple separators &quot;;&quot; and &quot;,&quot;. Which wouldn't be a problem if my dataset wouldn't have a decimal &quot;,&quot;.</p>
<p>I would like to automatically read in the files without changing the header by hand but I did not find something according to in the Pandas documentation.</p>
<p>The example data:</p>
<pre><code>some file header row number 1
some file header row number two
some more information about data in that file
column1;column2;column3,column4,column5
1,234;2,665;0,888;3,891;3,762
2,232;1,233;0,888;3,789;3,524
</code></pre>
<p>Can I specify a different separator for the header than for the dataset itself?
My code:</p>
<pre><code>df = pd.read_csv(
    &quot;mypath\test.csv&quot;, 
    skiprows = 3;header= 0, delimiter=';,', decimal = &quot;,&quot;)
</code></pre>
<p>which results in reading everything into one column</p>
<pre><code>    column1;column2;column3,column4,column5
0   1,234;2,665;0,888;3,891;3,762
1   2,232;1,233;0,888;3,789;3,524
</code></pre>
<p>But the result should be five columns:</p>
<pre><code>    column1     column2     column3     column4     column5
0   1.234   2.665   0.888   3.891   3.762
1   2.232   1.233   0.888   3.789   3.524
</code></pre>
","0","Question"
"79485898","","<p>I have a pandas dataframe that contains two date columns, a start date and an end date that defines a range. I'd like to be able to collect a total count for all dates across all rows in the dataframe, as defined by these columns.</p>
<pre><code>index     date_from       date_to
    0  '2019-08-01'  '2019-08-05'
    1  '2019-08-04'  '2019-08-07'
    2  '2019-08-07'  '2019-08-09'
</code></pre>
<p>And I need to calculate the number of dates for all ranges.
The result should be like this:</p>
<pre><code>date           count
'2019-08-01'     1
'2019-08-02'     1
'2019-08-03'     1
'2019-08-04'     2
'2019-08-05'     2
'2019-08-06'     1
'2019-08-07'     2
'2019-08-08'     1
'2019-08-09'     1
</code></pre>
<p>I used a for-loop to solve the problem, but the calculations take a very long time because the original dataframe is quite large.</p>
","4","Question"
"79487265","","<p>I have a csv-file I read in with pandas to a dataframe. The time column is formatted in days since 01-01-1900.
The aim is to gain an isoformatted column with the &quot;T&quot; between date and time.</p>
<p>The csv-file looks like:</p>
<pre><code>DateTime,column1,column2,column3,column4,column5
43621.6210327662,1.234,2.665,0.888,3.891,3.762
43621.6210445023,2.232,1.233,0.888,3.789,3.524
</code></pre>
<p>My code to read in is:</p>
<pre><code>import pandas as pd
df = pd.read_csv(myfile.csv)
df
</code></pre>
<p>and the dataframe contains days since 01-01-1900:</p>
<pre><code>    DateTime    column1     column2     column3     column4     column5
0   43621.621033    1.234   2.665   0.888   3.891   3.762
1   43621.621045    2.232   1.233   0.888   3.789   3.524
</code></pre>
<p>Now I convert the days-formatted date to a more common look:</p>
<pre><code>from datetime import datetime, timedelta
df['DateTime'] =  pd.to_datetime('1900-01-01') + pd.to_timedelta(df['DateTime'],'D')
df
    DateTime    column1     column2     column3     column4     column5
0   2019-06-07 14:54:17.230999464   1.234   2.665   0.888   3.891   3.762
1   2019-06-07 14:54:18.244998936   2.232   1.233   0.888   3.789   3.524
</code></pre>
<p>My aim is to have a isoformatted DateTime column that has the &quot;T&quot; between date and time like:</p>
<pre><code>    DateTime    column1     column2     column3     column4     column5
0   2019-06-07T14:54:17.230999464   1.234   2.665   0.888   3.891   3.762
1   2019-06-07T14:54:18.244998936   2.232   1.233   0.888   3.789   3.524
</code></pre>
<p>But the DateTime column seems to be a series instead of a date? I can`t apply something like:</p>
<pre><code>df['DateTime'].isoformat()
</code></pre>
<p>How would I proceed?</p>
","-1","Question"
"79489149","","<p>I am working with data from 174 subjects, stored in a dataframe (df_behavioral) where one row represents one subject. Some subjects are related to one another, as indicated by a variable called 'Family_ID', which assigns each subject to a family.</p>
<p>I need to split the sample into two subsamples of approximately equal size while ensuring that there are only unrelated subjects in one subsample. In other words: Subjects from the same family cannot be in the same subsample.</p>
<p>Additionally, the split should be stratified by Neuroticism scores (variable 'NEOFAC_N'), so that the distribution of Neuroticism scores is approximately equal in the two subsamples.</p>
<p>I would greatly appreciate your help!</p>
<p>This is my code so far. I was able to receive two subsamples of unrelated subjects but still need help in implementing the stratification by Neuroticism scores.</p>
<pre><code>import numpy as np
import pandas as pd

# Set seed for reproducibility
np.random.seed(42)

# Generate sample dataset
num_subjects = 174
num_families = 87  # 87 unique families

# Create Family_IDs (maximally two subjects per family)
family_ids = np.repeat(np.arange(num_families), 2)[:num_subjects]
np.random.shuffle(family_ids)

# Generate Neuroticism scores (normally distributed, scale 0-40, integers)
neuroticism_scores = np.clip(np.random.normal(loc=20, scale=5, size=num_subjects), 0, 40).astype(int)

# Create random subject names
subject_ids = [f'Subject_{i}' for i in range(num_subjects)]

# Create dataframe
df_behavioral = pd.DataFrame({
    'Subject_ID': subject_ids,
    'Family_ID': family_ids,
    'NEOFAC_N': neuroticism_scores
})

print(df_behavioral.head())  # Preview dataset

siblings_list = []
 
for family in df_behavioral[&quot;Family_ID&quot;].unique():
    siblings_list.append(df_behavioral.query(&quot;Family_ID==@family&quot;).index.values)
 
print(&quot;There are {} unique families in the dataset&quot;.format(len(siblings_list)))
 
subj_set1 = [family[0] for family in siblings_list]
subj_set2 = [family[1] for family in siblings_list if len(family)&gt;1]
 
# Make sure subjects are sorted
subj_set1.sort()
subj_set2.sort()
 
print(&quot;There are {} unrelated subjects in set 1&quot;.format(len(subj_set1)))
print(&quot;There are {} unrelated subjects in set 2&quot;.format(len(subj_set2)))

# Save subsamples as DataFrames 
sample_1 = df_behavioral.loc[subj_set1].reset_index(drop=True)
sample_2 = df_behavioral.loc[subj_set2].reset_index(drop=True)
</code></pre>
","3","Question"
"79489723","","<p>This is a MWE of some code I'm writing to do some monte carlo exercises. I need to estimate models across draws and I'm parallelizing across models. In the MWE a &quot;model&quot; is just parametrized by a number of draws and a seed.</p>
<p>I define the functions below.</p>
<pre><code>import time
import pandas as pd
import numpy as np
import multiprocessing as mp
    
def linreg(df):
    y = df[['y']].values
    x = np.hstack([np.ones((df.shape[0], 1)), df[['treat']].values])
    
    xx_inv = np.linalg.inv(x.T @ x)
    beta_hat = xx_inv @ (x.T @ y)
    
    return pd.Series(beta_hat.flat, index=['intercept', 'coef'])

def shuffle_treat(df):
    df['treat'] = df['treat'].sample(frac=1, replace=False).values
    return df
    
def run_analysis(draws, seed, sleep=0):
    
    N = 5000
    df = pd.DataFrame({'treat':np.random.choice([0,1], size=N, replace=True)})
    df['u'] = np.random.normal(size=N)
    df['y'] = df.eval('10 + 5*treat + u')

    np.random.seed(seed)
    
    time.sleep(sleep)
    
    est = [linreg(shuffle_treat(df)) for k in range(draws)]

    est = pd.concat(est, axis=0, sort=False, keys=range(draws), names=['k', 'param'])
    
    return est
</code></pre>
<p>I then test them and show that running in serial takes a similar amount of time as running in parallel. I can confirm they are running in parallel because if I force some sleep time there is a clear gain from parallelization. I know the problem is coming from this list comprehension: <code>[linreg(shuffle_treat(df)) for k in range(draws)]</code>, but I don't understand why I don't achieve gains from parallelization across models. I've tried to parallelize across draws instead, but the results were even worse.</p>
<pre><code>param_list = [dict(draws=500, seed=1029), dict(draws=500, seed=1029)]
param_list_sleep = [dict(draws=500, seed=1029, sleep=5), dict(draws=500, seed=1029, sleep=5)]

def run_analysis_wrapper(params):
    run_analysis(**params)

start = time.time()
for params in param_list:
    run_analysis_wrapper(params)
end = time.time()
print(f'double run 1 process: {(end - start):.2f} sec')

start = time.time()
with mp.Pool(processes=2) as pool:
    pool.map(run_analysis_wrapper, param_list)
end = time.time()
print(f'double run 2 processes: {(end - start):.2f} sec')

start = time.time()
for params in param_list_sleep:
    run_analysis_wrapper(params)
end = time.time()
print(f'double run 1 process w/ sleep: {(end - start):.2f} sec')

start = time.time()
with mp.Pool(processes=2) as pool:
    pool.map(run_analysis_wrapper, param_list_sleep)
end = time.time()
print(f'double run 2 processes w/ sleep: {(end - start):.2f} sec')
</code></pre>
<p>Output:</p>
<pre><code>double run 1 process: 2.52 sec
double run 2 processes: 2.94 sec
double run 1 process w/ sleep: 12.30 sec
double run 2 processes w/ sleep: 7.71 sec
</code></pre>
<p>For reference machine is Linux-based EC2 instance with <code>nproc --a</code> showing 48 CPUs. I'm running within a conda environment with Python 3.9.16.</p>
","3","Question"
"79490056","","<p>Among other descriptive statistics, I want to get some quantiles out of my pandas <code>DataFrame</code>. I can get the quantiles I want a couple of different ways, but I can't find the right way to do it with <code>aggregate</code>. I'd like to use aggregate because it'd be tidy and maybe computationally efficient to get all my stats in one go.</p>
<pre><code>rng = np.random.default_rng(seed=18860504)
df = pd.DataFrame({
    &quot;dummy&quot;: 1,
    &quot;bell&quot;: rng.normal(loc=0, scale=1, size=100),
    &quot;fish&quot;: rng.poisson(lam=10, size=100),
    &quot;cabin&quot;: rng.lognormal(mean=0, sigma=1.0, size=100),
})
quants = [x/5 for x in range(6)]
quantiles = pd.DataFrame({
    &quot;quantile&quot; : [f&quot;q{100*q:02n}&quot; for q in quants],
    &quot;bell&quot; : df.groupby(&quot;dummy&quot;)[&quot;bell&quot;].quantile(quants),
    &quot;fish&quot; : df.groupby(&quot;dummy&quot;)[&quot;fish&quot;].quantile(quants),
}) 
print(quantiles)
</code></pre>
<p>Output:</p>
<pre><code>          quantile      bell  fish
dummy                             
1     0.0     q000 -2.313461   4.0
      0.2     q020 -0.933831   7.0
      0.4     q040 -0.246860   9.0
      0.6     q060  0.211076  10.0
      0.8     q080  0.685958  13.0
      1.0     q100  3.017258  20.0
</code></pre>
<p>I'd like to get these quantiles using <code>groupby().agg()</code>, ideally with programmatically named columns like &quot;bell_q90&quot;. Here's an example of the <code>aggregate</code> syntax that feels natural to me:</p>
<pre><code>df.groupby(&quot;dummy&quot;).agg(
    bell_med=(&quot;bell&quot;, &quot;median&quot;),
    bell_mean=(&quot;bell&quot;, &quot;mean&quot;),
    fish_med=(&quot;fish&quot;, &quot;median&quot;),
    fish_mean=(&quot;fish&quot;, &quot;mean&quot;),
    # fish_q10=(&quot;fish&quot;, &quot;quantile(0.1)&quot;), # nothing like it 
    # fish_q10=(&quot;fish&quot;, &quot;quantile&quot;, 0.1), # nothing like it 
    # fish_q10=(&quot;fish&quot;, &quot;quantile&quot;, kwargs({&quot;q&quot;:0.1}), # nothing like it 
) 
</code></pre>
<p>I can imagine generating the columns by iterating over <code>quants</code> and a list of named columns, using <code>Series.agg</code> and than stitching them together, but this seems like a hack. (For example, it would require me to do my &quot;normal&quot; aggregation first and then add quantiles on afterwards.)</p>
<pre><code>my_aggs = dict()
for q in quants:
    for col in [&quot;bell&quot;, &quot;fish&quot;]:
        my_aggs[f&quot;{col}_q{100*q:03n}&quot;] = df.groupby(&quot;dummy&quot;)[col].quantile(q) 

print(pd.DataFrame(my_aggs)) # numbers equivalent to those above
</code></pre>
<p>Is there a better way?</p>
","3","Question"
"79491662","","<p>Just leaving this for posterity, in case someone ever comes across this somewhat specific problem. Though if anyone has any technical insights into the problem, feel free to share.</p>
<p><strong>Setup</strong></p>
<p>I have 3 cascading tables in Postgres. Master, a level1 (with the master id as FK) and a level 2, with the leve1 id as FK).
In a python script, data is inserted successively within a transaction (<em>try-except</em> and <em>with</em> blocks). The last insert uses a pandas to_sql() method as it is a large multi column table.</p>
<pre><code>try:
    with engine.connect() as conn:
       conn.execute(insert_master)
       conn.execute(insert_1)
       df.to_sql(sql, con=conn())
</code></pre>
<p><strong>Symptoms</strong></p>
<p>The setup as above actually worked quite fine. In the context of the connection, if something failed, nothing was commited, just as intended.</p>
<p>Then I moved from connections to sessions, so the setup changed to:</p>
<pre><code>session = dbSession() #sqlAlchemy Sessionmaker object

try:
    with session.begin():
       session.execute(insert_master)
       session.execute(insert_1)
       df.to_sql(sql, session.bind)
</code></pre>
<p>And this is where the error started to occur. I tried both <code>session.bind</code> and <code>session.get_bind()</code> but to no avail.</p>
<p>See my &quot;solution&quot; below.</p>
","0","Question"
"79492024","","<p>I am trying to make dashboard in flask by connecting it with SQL server and getting these errors. I confirm there are no null values and I checked by removing the column as well from query but still not working. Code is -</p>
<pre><code>import pandas as pd
import pyodbc
from flask import Flask, render_template, jsonify
app = Flask(__name__)

# SQL Server Connection Details
conn_str = (
    &quot;DRIVER={SQL Server};&quot;
    &quot;SERVER=xyz;&quot;
    &quot;DATABASE=xyz;&quot;
    &quot;UID=xyz;&quot;
    &quot;PWD=xyz;&quot;
)

# Fetch Data from SQL Server
def fetch_data():
    try:
        conn = pyodbc.connect(conn_str)
        query = &quot;&quot;&quot;
            SELECT TicketDate, Technician, Open_Tickets, Closed_Tickets, Created_Today, Closed_Today, Created_Hourly
            FROM Technician_Ticket_Stats
        &quot;&quot;&quot;
        df = pd.read_sql(query, conn)
        conn.close()
        # Debugging logs
        print(&quot;Fetched data successfully:&quot;)
        print(df.head()) 
        df['TicketDate'] = df['TicketDate'].astype(str)  # Convert date for JSON
        return df.to_dict(orient=&quot;records&quot;)
    except Exception as e:
        print(&quot;Error fetching data:&quot;, e)  
        return []
@app.route(&quot;/&quot;)
def index():
    return render_template(&quot;index.html&quot;)
@app.route(&quot;/get_data&quot;)
def get_data():
    try:
        data = fetch_data()
        return jsonify(data)
    except Exception as e:
        return jsonify({&quot;error&quot;: str(e)}), 500
if __name__ == &quot;__main__&quot;:
    app.run(host='127.0.0.1', port=8050, debug=True)here
</code></pre>
","1","Question"
"79492749","","<p>I have a simple hvplot example where I want to demonstrate 3 charts side by side (line plot, histogram &amp; box plot). Is there an easy way to give each box plot a colour matching the other charts (i.e. from the default colour cycle)?</p>
<pre><code>import pandas as pd
import hvplot.pandas

data = pd.read_csv('https://raw.githubusercontent.com/ChrisWalshaw/DataViz/master/Data/Products/DailySales.csv', index_col=0)
data.index = pd.to_datetime(data.index)
print(data.head())

selected = ['A', 'F', 'L']

plot1 = data[selected].hvplot.line(
    frame_height=300, frame_width=300,
)
plot2 = data[selected].hvplot.hist(
    frame_height=300, frame_width=300,
)
plot3 = data[selected].hvplot.box(
    frame_height=300, frame_width=300,
)
plot = plot1 + plot2 + plot3
hvplot.show(plot)
</code></pre>
<p><a href=""https://i.sstatic.net/fzU1Ff26.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fzU1Ff26.png"" alt=""output - line plot, histogram, box plots"" /></a></p>
","0","Question"
"79493179","","<p>I have a API response, which I'm normalizing the Json response. While trying to convert from pandas to spark dataframe, it's failing with below error,</p>
<p><strong>pyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_MERGE_TYPE] Can not merge type <code>BooleanType</code> and <code>DoubleType</code>.</strong></p>
<pre><code>response = requests.get('https://urloftherequest')
data = response.json()
df = pd.json_normalize(data[field])

df_aws = spark.createDataFrame(df)
</code></pre>
<p>If it is a csv, I can convert the type of all the columns to string, not sure want would be the solution for a Json response.</p>
","0","Question"
"79493514","","<p>I have a python script which needs to be executed by passing the input using command line. The command is as follows</p>
<pre class=""lang-bash prettyprint-override""><code>python script.py --input [{\\&quot;A\\&quot;:\\&quot;322|985\\&quot;,\\&quot;B\\&quot;:3}]
</code></pre>
<p>The idea is to convert the input to a pandas DataFrame. Code below does convert it to Pandas DataFrame but only creates a single column named <code>0</code> and the value for that column is <code>[{\A\:\322|985\,\B\:3}]</code>.</p>
<pre><code>import json
import pandas as pd
import argparse


def validate_input(input_data):

    if isinstance(input_data, pd.DataFrame):
        return input_data  # Already a DataFrame, return as is
    
    json_conv = json.dumps(input_data)
    json_data = json.loads(json_conv)
    
    return pd.DataFrame([json_data])  # Convert JSON serializable to DataFrame

def process_data(input_data):
    &quot;&quot;&quot;
    Function that processes data, only called if dtype is valid.
    &quot;&quot;&quot;
    validated_data = validate_input(input_data)
    print(validated_data)
    print(&quot;Processing data:\n&quot;, validated_data)

def main():
    parser = argparse.ArgumentParser(description=&quot;Validate and process JSON or Pandas DataFrame input.&quot;)
    parser.add_argument(&quot;--input&quot;, type=str, help=&quot;Input data as a JSON string&quot;)
    args = parser.parse_args()
    
    try:
        process_data(args.input)  # Proceed with processing only after validation
    except json.JSONDecodeError:
        raise TypeError(&quot;Invalid JSON input. Please provide a valid JSON string.&quot;)
    

if __name__ == &quot;__main__&quot;:
    main()

</code></pre>
<p>Run code below to get expected output</p>
<pre><code>pd.DataFrame([{&quot;A&quot;:&quot;322|985&quot;,&quot;B&quot;:3}])
</code></pre>
","0","Question"
"79493638","","<p>Looking for suggestions on how to compress this code into a couple of lines.
One line for assigning columns, and the other for data.</p>
<pre><code>df_input = pd.DataFrame(columns=['supply_temp', 'liquid_mass_flow','air_inlet_temp'])

flow = 60
inputs = np.array([45,flow*988/60000,35])


df_input['supply_temp'] = inputs[0]
df_input['liquid_mass_flow'] = inputs[1]
df_input['air_inlet_temp'] = inputs[2]
</code></pre>
","-5","Question"
"79494025","","<pre><code>url =&quot;https://www.dsebd.org/top_20_share.php&quot;
r =requests.get(url) 
soup = BeautifulSoup(r.text,&quot;lxml&quot;)
table = soup.find(&quot;table&quot;,class_=&quot;table table-bordered background-white shares-table&quot;)
top = table.find_all(&quot;th&quot;)
header = []
for x in top:
    ele = x.text
    header.append(ele)
    
df = pd.DataFrame(columns= header)
print(df)

row1 =  table.find_all(&quot;tr&quot;)
row2 =[]
for r1 in row1[1:]:
    ftd= r1.find_all(&quot;td&quot;)[1].find(&quot;a&quot;, class_=&quot;ab1&quot;).text.strip()
    data=r1.find_all(&quot;td&quot;)[1:]
    r2 = [ele.text for ele in data]
    r2.insert(0,ftd)
    l= len(df)
    df.loc[l]= r2
print(df)

df.to_csv(&quot;top_20_share_value_22.csv&quot;)
</code></pre>
<p>How to see / make data visible after converting to csv and view via excel?</p>
<p>I have gone through above mentioned code.</p>
","1","Question"
"79495612","","<p>I'm converting pandas dataframe to spark dataframe, but it is failing with</p>
<p><strong>Can not merge type &lt;class 'pyspark.sql.types.StringType'&gt; and &lt;class 'pyspark.sql.types.DoubleType'&gt;</strong></p>
<p>I can infer the schema and convert the types. But I have array type and I don't want to infer array type. Is there a way to infer particular column (Id) alone to double and remain other columns untouched.</p>
<pre><code> |-- Id: string (nullable = true)
 |-- Field: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- key: string (nullable = true)
 |    |    |-- value: string (nullable = true)
</code></pre>
","-1","Question"
"79495724","","<p>I am working on a school project in which I am supposed to clip unnecessary data from a log file and we were told to use python.
First, I am supposed to divide the data of the log into columns and then proceed with everything else.</p>
<p>So this is the solution I came up with, though it's not quite working:</p>
<pre><code>import pandas as pd
import pytz
from datetime import datetime
import re

def parse_str(x):
    if x is None:
        return '-'
    else:
        return x[1:-1]

def parse_datetime(x):
    try:
        dt = datetime.strptime(x[1:-7], '%d/%b/%Y:%H:%M:%S')
        dt_tz = int(x[-6:-3])*60+int(x[-3:-1])
        return dt.replace(tzinfo=pytz.FixedOffset(dt_tz))
    except ValueError:
        return datetime.now()

def parse_int(x):
    return int(x) if x is not None else 0

data = pd.read_csv(
    'Log_jeden_den.log',
    sep=r'\s(?=(?:[^&quot;]*&quot;[^&quot;]/&quot;)*[^&quot;]*$)(?![^\\[]*\\])',
    engine='python',
    na_values='-',
    header=None,
    usecols=['ip', 'request', 'status', 'size', 'referer', 'user_agent'],
    names=['ip', 'time', 'request', 'status', 'size', 'referer', 'user_agent'],
    converters={'time': parse_datetime,
                'request': parse_str,
                'status': parse_int,
                'size': parse_int,
                'referer': parse_str,
                'user_agent': parse_str})
print(data.head())
</code></pre>
<p>This is what I get:
<a href=""https://i.sstatic.net/8xSnuRTK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8xSnuRTK.png"" alt=""Output"" /></a></p>
<p>Basically I need each part of the log to be split into the mentioned columns.</p>
<p>Log file looks like this:</p>
<p>193.87.12.30 - - [19/Feb/2020:06:25:50 +0100] &quot;GET /navbar/navbar-ukf.html HTTP/1.0&quot; 200 7584 &quot;-&quot; &quot;-&quot;</p>
<p>193.87.12.30 - - [19/Feb/2020:06:25:55 +0100] &quot;GET /navbar/navbar-ukf.html HTTP/1.0&quot; 200 7584 &quot;-&quot; &quot;-&quot;</p>
<p>193.87.12.30 - - [19/Feb/2020:06:25:56 +0100] &quot;GET /navbar/navbar-ukf.html HTTP/1.0&quot; 200 7584 &quot;-&quot; &quot;-&quot;</p>
<p>193.87.12.30 - - [19/Feb/2020:06:25:57 +0100] &quot;GET /navbar/navbar-ukf.html HTTP/1.0&quot; 200 7584 &quot;-&quot; &quot;-&quot;</p>
<p>193.87.12.30 - - [19/Feb/2020:06:25:49 +0100] &quot;GET / HTTP/1.1&quot; 200 20925 &quot;-&quot; &quot;libwww-perl/6.08&quot;</p>
<p>23.100.232.233 - - [19/Feb/2020:06:25:49 +0100] &quot;GET /media-a-marketing/dianie-na-univerzite/kalendar-udalosti/815-den-otvorenych-dveri-2018 HTTP/1.1&quot; 200 26802 &quot;-&quot; &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0;  Trident/5.0)&quot;</p>
<p>193.87.12.30 - - [19/Feb/2020:06:25:46 +0100] &quot;GET / HTTP/1.1&quot; 200 20925 &quot;-&quot; &quot;libwww-perl/6.08&quot;</p>
<p>I tried this:</p>
<pre><code>usecols=[0, 3, 4, 5, 6, 7, 8]
</code></pre>
<p>But I am getting error:</p>
<p><a href=""https://i.sstatic.net/26WyYUCM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/26WyYUCM.png"" alt=""ParserError"" /></a></p>
<p>To sum up. I need to break the log data to broken down to columns. My code somewhat does that but incorrectly. It creates the columns but does not divide the data inbetween them, it puts them all into the first column. Naming the columns didn't help and marking them with numbers caused ParserError.</p>
<p>I know I can just open the file in excel and split it into columns but I really would like to do it &quot;proper&quot; hard way. Is it possible, though? Thanks for any advice in advance.</p>
","0","Question"
"79496133","","<p>I have a Python script that reads content from JSON file and then write to excel file using pandas with that code and format:</p>
<pre><code>fichero_excel_salida = 'Estad.xlsx'
nombre_pagina = 'Jornada_' + str(num_jornada)
count = 0

    # Generamos los dataframe y Volcamos la info al fichero excel
    with pd.ExcelWriter(fichero_excel_salida, engine=&quot;openpyxl&quot;) as writer:

        for key,contenido_dict in  dict_dict_excels.items():
            count = count +1
            key = pd.DataFrame(data=contenido_dict)
            if (count % 2) == 0:
                pass
            else:
                key = (key.T)
            print(key)
    
            if count == 1:
                key.style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name=nombre_pagina, startcol=0, startrow=1)
            elif count == 2:
                key.style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name=nombre_pagina, startcol=0, startrow=6, header=True, index=True)
            elif count == 3:
                key.style.set_properties(**{'text-align': 'center'}).to_excel(writer, sheet_name=nombre_pagina, startcol=0, startrow=11)
</code></pre>
<p>So the result of the code above, give me this with some format cells:</p>
<pre><code>   A    B    C    D    E    F    G    H    I
1
2       10
3  AA   2
4  BB   3
5
6
7       XX  YY   ZZ
8  AA    5   3   6
9  BB    6   2   6
10
11
12      NUM  MIN  PTS
13 John  3   24    6
14 Dave  4   12    0
15 Ko    5   30    2
16 Dam   10  20    2
17
</code></pre>
<p>Now, I need to get new data from next week, and append this new data to new sheet in the same excel.</p>
<p>What is the best form to do it?</p>
<ol>
<li><p>Reading the existing Excel, create new Excel file and put all data again (oldest and newest) using this code?</p>
<pre><code>dict_all_xlsx_file = pd.read_excel(fichero_excel_salida, sheet_name=None)
</code></pre>
<p>How I can put the data with the same format as was in the previous excel file (with cell formats)?</p>
</li>
<li><p>Append the new data of each week in the Excel?</p>
<p>How I can do it, because using my Python script in Linux with pandas 2.2.3, all time create new Excel document and I lost old sheets.</p>
</li>
</ol>
","0","Question"
"79496136","","<p>I have a dataframe whose index is a multiindex where axes[0] is the date, and axis[1] is the rank. Rank starts with 1 and ends at 100, but there can be a variable number of ranks in between as below.
Here are the ranks</p>
<pre><code>dx = pd.DataFrame({
    &quot;date&quot;: [
        pd.to_datetime('2025-02-24'), pd.to_datetime('2025-02-24'), pd.to_datetime('2025-02-24'), pd.to_datetime('2025-02-24'),
        pd.to_datetime('2025-02-25'), pd.to_datetime('2025-02-25'), pd.to_datetime('2025-02-25'), 
        pd.to_datetime('2025-02-26'), pd.to_datetime('2025-02-26'), pd.to_datetime('2025-02-26'), pd.to_datetime('2025-02-26'), pd.to_datetime('2025-02-26')
    ],
     &quot;rank&quot;: [0.0,1.0,2.0,100.0,0.0,1.0,100.0,0.0,1.0,2.0,3.0,100.0],
    &quot;value&quot;: [2.3, 2.5, 2.4, 2.36, 2.165, 2.54, 2.34, 2.12, 2.32, 2.43, 2.4, 2.3]
})

dx.set_index([&quot;date&quot;, &quot;rank&quot;], inplace=True)

</code></pre>
<p>I want to plot this df, and <code>df.plot()</code> works fine creating a reasonable x-axis. However, I want to add a grid or vertical lines at all the <code>rank=1</code>, and all the <code>rank=100(different color)</code>.</p>
<p>I tried this :</p>
<pre><code>
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(30, 5))

dx.plot(ax=axes[0])
axes[0].tick_params('x', labelrotation=90)
xs = [x for x in dx.index if x[1]==0]

for xc in xs:
    axes[0].axvline(x=xc, color='blue', linestyle='-')
</code></pre>
<p>but get this error:</p>
<pre><code>ConversionError: Failed to convert value(s) to axis units: (Timestamp('2025-02-24 00:00:00'), 0.0)
</code></pre>
<p>I also want to only show x labels for <code>rank=0</code>, and not all of them. Currently, if i set label rotation to 90, it results in that but not sure if this is the best way to ensure that.</p>
<pre><code>axes[0].tick_params('x', labelrotation=90)
</code></pre>
<p>So looking for 2 answers</p>
<ol>
<li>How to set vertical lines at specific points with this type of multiindex</li>
<li>How to ensure only certain x labels show on the chart</li>
</ol>
","1","Question"
"79496178","","<p>I have a CSV file like this:</p>
<pre class=""lang-none prettyprint-override""><code>Ngày(Date),Số(Number)
07/03/2025,8
07/03/2025,9
...
06/03/2025,6
06/03/2025,10
06/03/2025,18
06/03/2025,14
...
</code></pre>
<p>(Each day has 27 numbers)</p>
<p>I want to predict a list of 27 numbers on the next day using LSTM.
It keeps getting an error on this step:</p>
<pre><code>data_matrix = np.array(grouped_data.loc[:, &quot;Số&quot;].tolist())
</code></pre>
<p>with</p>
<pre><code>KeyError: 'Số'
</code></pre>
<p>(which means 'Number')</p>
<p>Here is my code:</p>
<pre><code>import numpy as np
import pandas as pd

df = pd.read_csv(&quot;C:/Users/Admin/lonum_fixed.csv&quot;, encoding=&quot;utf-8&quot;, sep=&quot;,&quot;)
df.columns = df.columns.str.strip()

grouped_data = df.groupby(&quot;Ngày&quot;)[[&quot;Số&quot;]].apply(lambda x: list(map(int, x[&quot;Số&quot;].values))).reset_index()
grouped_data[&quot;Số&quot;] = grouped_data[&quot;Số&quot;].apply(lambda x: eval(x) if isinstance(x, str) else x)

data_matrix = np.array(grouped_data.loc[:, &quot;Số&quot;].tolist())
</code></pre>
","-1","Question"
"79496746","","<ul>
<li><p>I have downloaded a .xpt file from: <a href=""https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DPQ_L.xpt"" rel=""nofollow noreferrer"">https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DPQ_L.xpt</a></p>
</li>
<li><p>This file comes with a description of what the values should be expected to be: <a href=""https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DPQ_L.htm"" rel=""nofollow noreferrer"">https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DPQ_L.htm</a></p>
</li>
<li><p>The data is from a participant filling out a survey of questions.</p>
</li>
<li><p>The values should be between 0-9 and &quot;.&quot; if missing.</p>
</li>
</ul>
<p>I have run the following code to interpret the file:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Reading the .xpt file (SAS Transport file)
df = pd.read_sas('C:\\Users\\wsaj5\\Downloads\\DPQ_L.xpt', format='xport')

# Print the first few rows to inspect the data
print(df.head())

# Check the data types of the columns
print(df.dtypes)
</code></pre>
<p>This gives me:</p>
<pre class=""lang-py prettyprint-override""><code>       SEQN        DPQ010        DPQ020  ...        DPQ080        DPQ090        DPQ100
0  130378.0           NaN           NaN  ...           NaN           NaN           NaN
1  130379.0  5.397605e-79  5.397605e-79  ...  5.397605e-79  5.397605e-79  5.397605e-79
2  130380.0  5.397605e-79  5.397605e-79  ...  5.397605e-79  5.397605e-79  5.397605e-79
3  130386.0  5.397605e-79  5.397605e-79  ...  5.397605e-79  5.397605e-79  5.397605e-79
4  130387.0  5.397605e-79  5.397605e-79  ...  5.397605e-79  5.397605e-79           NaN

[5 rows x 11 columns]
SEQN      float64
DPQ010    float64
DPQ020    float64
DPQ030    float64
DPQ040    float64
DPQ050    float64
DPQ060    float64
DPQ070    float64
DPQ080    float64
DPQ090    float64
DPQ100    float64
dtype: object
</code></pre>
<ul>
<li><p>5.40e-79 isn't an expected value and I have had this value for other files that I have been looking at from the same source too.</p>
</li>
<li><p>Not all values in columns containing 5.40e-79 are 5.40e-79, just some values seem wrong.</p>
</li>
<li><p>There are still other values in the same rows for different columns which would suggest that the participant has completed the survey so there shouldn't be answers missing.</p>
</li>
<li><p>ChatGPT suggests that this is a placeholder value for missing values.</p>
</li>
<li><p>Can these values be ignored? I am worried that I will be ignoring data that I shouldn't be.</p>
</li>
</ul>
","1","Question"
"79497048","","<p>I have a column with <code>dtype</code> = <code>object</code>. It has either <code>NaN</code> or <code>123 - 456</code>. I need to extract min and max into it's own columns:</p>
<pre><code>import numpy
df[&quot;min&quot;] = df[&quot;values&quot;].map(lambda x: minimum if x and (minimum:=str(x).split(&quot; - &quot;)[0]) else numpy.nan)
df[&quot;max&quot;] = df[&quot;values&quot;].map(lambda x: maximum if x and &quot; - &quot; in str(x) and (maximum:=str(x).split(&quot; - &quot;)[1]) else numpy.nan)
</code></pre>
<p>(1) Is this the most efficient way?</p>
<p>(2) This code snippet results in <code>string</code> column types. How to obtain <code>int</code> type?</p>
","2","Question"
"79497570","","<p>I have a DataFrame that looks something like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">C1</th>
<th style=""text-align: left;"">C2</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">10</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">20</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">30</td>
<td style=""text-align: left;"">16</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td style=""text-align: left;"">23</td>
</tr>
<tr>
<td style=""text-align: left;"">6</td>
<td style=""text-align: left;"">23</td>
</tr>
<tr>
<td style=""text-align: left;"">8</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">10</td>
</tr>
</tbody>
</table></div>
<p>I would like to calculate the mean value in column C1 depending on the values in column C2. The mean value is to be calculated over all values in column C1 until the value in column C2 changes again. The result table should look like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">C1</th>
<th style=""text-align: left;"">C2</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">15</td>
<td style=""text-align: left;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">30</td>
<td style=""text-align: left;"">16</td>
</tr>
<tr>
<td style=""text-align: left;"">5.5</td>
<td style=""text-align: left;"">23</td>
</tr>
<tr>
<td style=""text-align: left;"">4.67</td>
<td style=""text-align: left;"">10</td>
</tr>
</tbody>
</table></div>
","1","Question"
"79497724","","<p>Using numpy, one can subset an array with one boolean array per dimension like:</p>
<pre><code>In [10]: aa = np.array(range(9)).reshape(-1, 3)

In [11]: aa
Out[11]:
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])

In [12]: conditions = (np.array([True, True, False]), np.array([True, False, True]))

In [13]: aa[np.ix_(*conditions)]
Out[13]:
array([[0, 2],
       [3, 5]])
</code></pre>
<p>Is there a way to do this in Pandas? I've looked in their docs<br />
<a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing</a><br />
but didn't find it. (I would have posted 4 relevant links, but then the automatic question checks think I've posted code that is not properly formatted.)</p>
<p>This<br />
<a href=""https://github.com/pandas-dev/pandas/issues/11290"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/issues/11290</a><br />
github issue is close, but I want to pick entire rows and columns.</p>
","2","Question"
"79499156","","<p>I have a Pandas dataframe with checkbox widget. I would like to process the values from the row when the checkbox is checked.</p>
<p>Can anyone help me with this? My code is below.</p>
<pre><code>        df = pagination.data_editor(data=pages[current_page - 1], use_container_width=True,
                        disabled=['Date','Day','Project','Task','StartTime','EndTime','Note','Total_Hours'],
                        column_order=['Date','Day','Project','Task','StartTime','EndTime','Total_Hours','Note','Delete','Edit'],
                        column_config=config,
                        key='edit_df')

            
            if not df.loc[df[&quot;Delete&quot;]].empty:
                row = df.loc[df[&quot;Delete&quot;] != df.loc[df[&quot;Delete&quot;]].empty]
                entryid = row['id'].values
                
                
</code></pre>
<p>What I get here is a DataFrame object and not the value of the id column. What do I need to do to get just the value of id column?</p>
","0","Question"
"79499286","","<p>My requirement is simple,  I need to select the rows from a Pandas DataFrame when one of two couple columns are populated or both.  The attributes contain integer foreign keys.
This works:</p>
<pre><code>df_ofComment   = df_obsFinding.loc[ (None if   df_obsFinding['Comments'] is None  else df_obsFinding['Comments'].apply(len) != 0 )     ]
</code></pre>
<p>gives me the rows from df_obsFindings  where there is data in Comments.  Good</p>
<p>This fails:</p>
<pre><code>df_ofComment   = df_obsFinding.loc[ (None if   df_obsFinding['Rejection_Comments'] is None  else df_obsFinding['Rejection_Comments'].apply(len) != 0 ) 
</code></pre>
<p>Tosses this error:</p>
<pre><code> TypeError: object of type 'NoneType' has no len()
</code></pre>
<p>I believe the data in 'Rejection_Comments' is dirtyer than 'Comments'</p>
<p>Under debug in the Comments col I see:  [], [1234] , [1234], [ 456]   etc....
Looks to me like lists and empty lists.</p>
<p>Under debug in Rejection_Comments I see  None and Empty Boxes.<br />
Silly me I thought checking for None would handle this.</p>
<p>In the end I was looking for a statement like this:</p>
<pre><code>df_ofComment   = df_obsFinding.loc[ (None if   df_obsFinding['Comments'] is None  else df_obsFinding['Comments'].apply(len) != 0 )    | 
                                    ( None if   df_obsFinding['Rejection_Comments'] is None  else   df_obsFinding['Rejection_Comments'].apply(len)!= 0 )  ]
</code></pre>
<p>Maybe I am not going about this in a &quot;Python&quot;  way</p>
<p>Many thanks for your attention to this matter.</p>
<p>With kind regard.</p>
<p>KD</p>
","0","Question"
"79499303","","<p>Let us assume we have this dataframe:</p>
<p><code>df = pd.DataFrame.from_dict({1:{&quot;a&quot;: 10, &quot;b&quot;:20, &quot;c&quot;:30}, 2:{&quot;a&quot;:100, &quot;b&quot;:200, &quot;c&quot;:300}}, orient=&quot;index&quot;)</code></p>
<p>Further, let us assume I want to apply a function to each row that adds 1 to the values in columns a and b</p>
<pre><code>def add(x):
    return x[&quot;a&quot;] +1, x[&quot;b&quot;] +1
</code></pre>
<p>Now, if I use the apply function to mod and overwrite the columns twice, some values are flipped:</p>
<pre><code>&gt;&gt;&gt; df.loc[:, [&quot;a&quot;, &quot;b&quot;]] = df[[&quot;a&quot;, &quot;b&quot;]].apply(lambda x: add(x), axis=1)
&gt;&gt;&gt; df
    a    b    c
1  11  101   30
2  21  201  300
&gt;&gt;&gt; 
&gt;&gt;&gt; df.loc[:, [&quot;a&quot;, &quot;b&quot;]] = df[[&quot;a&quot;, &quot;b&quot;]].apply(lambda x: add(x), axis=1)
&gt;&gt;&gt; df
     a    b    c
1   12   22   30
2  102  202  300
&gt;&gt;&gt; 
</code></pre>
<p>Could somebody explain to me why b1 and a2 get flipped?</p>
","1","Question"
"79500226","","<p>I want to create a summary file of a few dozen original files, of which I only extract the last row (df.tail(1)) of each original file with 1000s of columns, which might be int, float, str, bool.</p>
<p>My problem is, that sometimes column names differ some (which is fine, I just have to retain the information), the majority of column names are the same. <strong>When using pd.concat on these dfs, nans are created for all columns which do not exist in all files, which leads to a change in datatype for ints.</strong></p>
<pre><code>df1 = pd.DataFrame([(2, 3, True, &quot;Test&quot;)], columns=[&quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;a&quot;])
df2 = pd.DataFrame([(4, 6)], columns=[&quot;x&quot;, &quot;b&quot;])

df3 = pd.concat([df1, df2], ignore_index = True)
</code></pre>
<p>--&gt; df3:</p>
<p><a href=""https://i.sstatic.net/IYM5s7FW.png"" rel=""nofollow noreferrer"">resulting df3</a></p>
<p>But datatype preservation is necessary for further file handling (ints have to stay ints)!</p>
<p>I tried adding the information of the subsequent dfs by using dicts</p>
<pre><code>df3 = df1.copy(deep=True)

res_dict = {&quot;x&quot;: 4, &quot;y&quot;: 6}
df3.loc[len(df3.index)] = res_dict
</code></pre>
<p>but this doesn't preserve datatype either.</p>
<p>The only solution I found so far, is to add an all empty row when adding values of a new original file, accessing cells explicitely, and if a new column is to be added, to also add the column all empty before adding information to the specific cell</p>
<pre><code>for loop_no, key in enumerate(res_dict.keys()):
    if loop_no == 0:
        row_no = len(df3)
        df3.loc[row_no] = [&quot;&quot;] * len(df3.columns)

    if key in df3.columns:
        df3.loc[row_no, key] = res_dict[key]
    else:
        df3[key] = &quot;&quot;
        df3.loc[row_no, key] = res_dict[key]
</code></pre>
<p>But this is really tedious and with the amount of data I have to assemble not exactly efficient.</p>
<p>If I use convert_dtypes() as suggested, I will receive a warning of</p>
<p>pandas\core\dtypes\cast.py: 1080 RuntimeWarning: invalid value encountered in cast if (arr.astype(int) == arr).all()</p>
<p>for every column which is not &quot;int&quot;. Can this be simply ignored?</p>
<p>Also i wonder if I will run into:
FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation</p>
","2","Question"
"79501838","","<pre><code>deatils=[{&quot;names&quot;:[&quot;ramu&quot;,&quot;Ravi&quot;,&quot;Naik&quot;],&quot;ages&quot;:[39,40,45],&quot;department&quot;:&quot;admin&quot;,&quot;location&quot;:&quot;Bangalore&quot;},

{&quot;names&quot;:[&quot;Kiran&quot;,&quot;Kumar&quot;,&quot;Joseph&quot;],&quot;ages&quot;:[35,26,52],&quot;department&quot;:&quot;IT&quot;,&quot;location&quot;:&quot;Hyderabad&quot;},

{&quot;names&quot;:[&quot;Rajesh&quot;,&quot;Nakul&quot;],&quot;ages&quot;:[34,40],&quot;department&quot;:&quot;Payroll&quot;,&quot;location&quot;:&quot;Chennai&quot;},

{&quot;names&quot;:[&quot;Mukesh&quot;,&quot;Kundan&quot;,&quot;Kishore&quot;],&quot;ages&quot;:[27,33,37],&quot;department&quot;:&quot;IT&quot;,&quot;location&quot;:&quot;Bangalore&quot;}]
</code></pre>
<p>I'm looking for the data to be saved in CSV file like below:</p>
<p><a href=""https://i.sstatic.net/pBXF2Apf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBXF2Apf.png"" alt=""enter image description here"" /></a></p>
","-5","Question"
"79502746","","<p>I'm on Pandas <code>2.2.1</code> with Python <code>3.9.19</code> trying to sample 5 rows per group using <code>groupby().apply()</code>, but I keep getting this DeprecationWarning in pandas 2.2+:</p>
<pre><code>DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns.  
This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation.  
Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
</code></pre>
<p>This is a reproducible example:</p>
<pre><code>import pandas as pd

# Sample DataFrame
df = pd.DataFrame({
    'locale': ['en_US', 'en_US', 'fr_FR', 'fr_FR', 'fr_FR', 'zh_CN', 'zh_CN'],
    'query': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],
    'score': [1, 2, 3, 4, 5, 6, 7]
})

# Function to sample 5 rows per group
def sample_per_locale(df):
    sampled_df = df.groupby(&quot;locale&quot;, group_keys=False) \
                   .apply(lambda x: x.sample(min(5, len(x)), random_state=42)) \
                   .reset_index(drop=True)
    
    return sampled_df  # Keeping locale but still getting the warning

# Run function
sampled_df = sample_per_locale(df)
print(sampled_df)
</code></pre>
<p><strong>What I’ve Tried:</strong>
Using <code>include_groups=False</code> (but my pandas' version doesn't support it):</p>
<pre><code>df.groupby(&quot;locale&quot;, group_keys=False, include_groups=False)  # TypeError
</code></pre>
<p>Explicitly selecting locale after apply():</p>
<pre><code>sampled_df[['locale'] + [col for col in sampled_df.columns if col != 'locale']]
</code></pre>
<p>It still triggers the warning.</p>
<p><strong>Expected Output:</strong>
I just want <code>groupby().apply()</code> to sample 5 rows per locale without triggering this warning.</p>
<p>How do I properly exclude the grouping column in Pandas 2.2+?
What's the correct way to handle this change in future versions?
Any help would be greatly appreciated!</p>
","0","Question"
"79503619","","<p>I want to add columns in my df with values based on the sample list in one column and the next column headers as sample numbers. In detail: based on the <code>11</code> column, I want to add 3 columns designed as <code>11_1</code>, <code>11_2</code> and <code>11_3</code> with values according to the sample list in the <code>11</code> and then the same for <code>00</code>.</p>
<p>My tiny part of input data:</p>
<pre><code>df_matrix_data = {'11': [['P4-1', 'P4-2', 'P4-3'], ['P4-1', 'P4-3', 'P4-4']],
                  '00': [['P4-4', 'P4-6', 'P4-7',], ['P4-2', 'P4-5', 'P4-7']],
                  'P4-1': [1, 2], 'P4-2': [6, 8], 'P4-3': [5, 2], 'P4-4': [2, 3], 'P4-5': [np.nan, 2], 'P4-6': [6, np.nan],
                  'P4-7': [3, 2]}
df_matrix = pd.DataFrame.from_dict(df_matrix_data)
</code></pre>
<p>will look like this:</p>
<pre><code>                   11                  00  P4-1  P4-2  P4-3  P4-4  P4-5  P4-6  P4-7
0  [P4-1, P4-2, P4-3]  [P4-4, P4-6, P4-7]     1     6     5     2   NaN   6.0     3
1  [P4-1, P4-3, P4-4]  [P4-2, P4-5, P4-7]     2     8     2     3   2.0   NaN     2
</code></pre>
<p>and desired output should look like this:</p>
<pre><code>                   11                  00  P4-1  P4-2  P4-3  P4-4  P4-5  P4-6  P4-7  11_1  11_2  11_3  00_1  00_2  00_3
0  [P4-1, P4-2, P4-3]  [P4-4, P4-6, P4-7]     1     6     5     2   NaN   6.0     3     1     6     5     2     6     3
1  [P4-1, P4-3, P4-4]  [P4-2, P4-5, P4-7]     2     8     2     3   2.0   NaN     2     2     2     3     8     2     2
</code></pre>
<p>Any ideas on how to perform it?</p>
","1","Question"
"79504101","","<p>I have a dataframe like the following:</p>
<pre><code>ID    Parm_1    Parm_2    Result
0     100       100       0.2
1     100       200       0.4
3     100       300       0.9
4     100       400       0.45

5     200       100       0.01
6     200       200       ...
7     200       300       ...
8     200       400       ...

9     300       100       ...
</code></pre>
<p>As you can see there is a repeating sequence with <code>Parm_1</code> and <code>Parm_2</code>; when <code>Parm_1</code> increases, the same 4 <code>Parm_2</code> values are cycled. However, I don't know how to represent this with matplotlib. I want to draw this where the x-axis shows this increment, as the (crude) drawing shows below.</p>
<p>Any ideas?</p>
<p>Thanks in advance</p>
<p><a href=""https://i.sstatic.net/kYl3awb8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kYl3awb8.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79505221","","<p>I have a dataframe that I like to add a column of values from array of tuples. The tuple contains the coordinates (position, value). An example:</p>
<pre><code>import pandas as pd
import numpy as np

alpha = [chr(i) for i in range(ord('A'), ord('K')+1)]

dt = pd.date_range(start='2025-1-1', freq='1h', periods = len(alpha))

df = pd.DataFrame ( alpha , index = dt )
df.index.name = 'timestamp'
df.columns = ['item']

c = np.array( [(1, 100), (2, 202), (6, 772)] )
</code></pre>
<p>which gives:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>timestamp</th>
<th>item</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-01-01 00:00:00</td>
<td>A</td>
</tr>
<tr>
<td>2025-01-01 01:00:00</td>
<td>B</td>
</tr>
<tr>
<td>2025-01-01 02:00:00</td>
<td>C</td>
</tr>
<tr>
<td>2025-01-01 03:00:00</td>
<td>D</td>
</tr>
<tr>
<td>2025-01-01 04:00:00</td>
<td>E</td>
</tr>
<tr>
<td>2025-01-01 05:00:00</td>
<td>F</td>
</tr>
<tr>
<td>2025-01-01 06:00:00</td>
<td>G</td>
</tr>
<tr>
<td>2025-01-01 07:00:00</td>
<td>H</td>
</tr>
<tr>
<td>2025-01-01 08:00:00</td>
<td>I</td>
</tr>
<tr>
<td>2025-01-01 09:00:00</td>
<td>J</td>
</tr>
<tr>
<td>2025-01-01 10:00:00</td>
<td>K</td>
</tr>
</tbody>
</table></div>
<p>I am trying to join column c, in such a way that ROW[1] contains [B and 100].</p>
<p>I have accomplished what I want with the following:</p>
<pre><code>df.reset_index(inplace = True) 
df.index.name = 'pos'

for x,y in c:
    df.loc[ int(x) , 'price'] = y

df.set_index(&quot;timestamp&quot;, inplace=True)

</code></pre>
<p>This gave me the desired results:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>timestamp</th>
<th>item</th>
<th>price</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-01-01 00:00:00</td>
<td>A</td>
<td>nan</td>
</tr>
<tr>
<td>2025-01-01 01:00:00</td>
<td>B</td>
<td>100</td>
</tr>
<tr>
<td>2025-01-01 02:00:00</td>
<td>C</td>
<td>202</td>
</tr>
<tr>
<td>2025-01-01 03:00:00</td>
<td>D</td>
<td>nan</td>
</tr>
<tr>
<td>2025-01-01 04:00:00</td>
<td>E</td>
<td>nan</td>
</tr>
<tr>
<td>2025-01-01 05:00:00</td>
<td>F</td>
<td>nan</td>
</tr>
<tr>
<td>2025-01-01 06:00:00</td>
<td>G</td>
<td>772</td>
</tr>
<tr>
<td>2025-01-01 07:00:00</td>
<td>H</td>
<td>nan</td>
</tr>
<tr>
<td>2025-01-01 08:00:00</td>
<td>I</td>
<td>nan</td>
</tr>
<tr>
<td>2025-01-01 09:00:00</td>
<td>J</td>
<td>nan</td>
</tr>
<tr>
<td>2025-01-01 10:00:00</td>
<td>K</td>
<td>nan</td>
</tr>
</tbody>
</table></div>
<p>However, the idea of dropping and recreating the index for this feels a bit awkward, especially if I have multiple indexes.</p>
<p>My question, is there a better way that dropping and recreating an index to add a column with missing values, using position ?</p>
","0","Question"
"79505931","","<p>I have a CSV (or rather TSV) I got from stripping the header off a gVCF with</p>
<pre class=""lang-bash prettyprint-override""><code>bcftools view foo.g.vcf -H &gt; foo.g.vcf.csv
</code></pre>
<p>A <code>head</code> gives me this, so everything looks like expected so far</p>
<pre><code>chr1H   1       .       T       &lt;*&gt;     0       .       END=1000        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
chr1H   1001    .       T       &lt;*&gt;     0       .       END=1707        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
chr1H   1708    .       C       &lt;*&gt;     0       .       END=1763        GT:GQ:MIN_DP:PL 0/0:6:2:0,6,59
chr1H   1764    .       T       &lt;*&gt;     0       .       END=2000        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
chr1H   2001    .       A       &lt;*&gt;     0       .       END=3000        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
chr1H   3001    .       G       &lt;*&gt;     0       .       END=4000        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
chr1H   4001    .       T       &lt;*&gt;     0       .       END=5000        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
chr1H   5001    .       T       &lt;*&gt;     0       .       END=6000        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
chr1H   6001    .       A       &lt;*&gt;     0       .       END=7000        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
chr1H   7001    .       G       &lt;*&gt;     0       .       END=8000        GT:GQ:MIN_DP:PL 0/0:1:0:0,0,0
</code></pre>
<p>When I know try to read the file as a dataframe in a Jupyter Notebook like this</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl

df = pl.read_csv(&quot;foo.g.vcf.csv&quot;, has_header=False,
                 new_columns=[&quot;CHROM&quot;, &quot;POS&quot;, &quot;ID&quot;, &quot;REF&quot;, &quot;ALT&quot;, &quot;QUAL&quot;, &quot;FILTER&quot;, &quot;INFO&quot;, &quot;FORMAT&quot;, &quot;SAMPLE&quot;],
                 separator=&quot;\t&quot;)
</code></pre>
<p>I get a compute error &quot;Original error: <code>remaining bytes non-empty</code>&quot;. However, when I do</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import polars as pl

df = pd.read_csv(&quot;foo.g.vcf.csv&quot;, header=None, sep=&quot;\t&quot;,
                 names=[&quot;CHROM&quot;, &quot;POS&quot;, &quot;ID&quot;, &quot;REF&quot;, &quot;ALT&quot;, &quot;QUAL&quot;, &quot;FILTER&quot;, &quot;INFO&quot;, &quot;FORMAT&quot;, &quot;SAMPLE&quot;])
df = pl.DataFrame(df)
</code></pre>
<p>every works as intended.</p>
<p>Why can I read with pandas without problems and convert to polars, but not read with polars directly?</p>
<p>The other VCF I want to compare with, which I stripped the same way, works with polars.</p>
","1","Question"
"79507131","","<p>I encounter problems trying to fill all null values on a specific column of a data frame.</p>
<p>Here is an example of dataframe and my expected outcome.</p>
<p>Example data frame:</p>
<pre><code>Column_1 Column_2
    F      A
   None    B
   None   None
    G      C
   None   None
   None    D 
    H      D
</code></pre>
<p>I want to get the first value from the column 1 to all null value from column 2</p>
<p>Expected Outcome:</p>
<pre><code>Column_1 Column_2
    F      A
   None    B
   None    G #First value from the left column
    G      C
   None    H #First value from the left column
   None    D 
    H      D
</code></pre>
<p>I'm getting error when I try this code.</p>
<p><code>df['Colunmn_2'].ffill(df.loc[df['Column_1'].first_valid_index(), 'Column_1'],inplace=True)</code></p>
<p>Thanks in advance!</p>
","2","Question"
"79507978","","<p>I have a csv file of the following format:
<code>12;0;5/15/2008;1:01:09;1;0;0;None;97;39;0.279;;;0;;;;;0;0;0;;;;;;;;;;;;;;;;</code></p>
<p>Then I read the file into a pandas dataframe:</p>
<pre><code>df = pd.read_csv(config.get(&quot;OS&quot;, &quot;output_file&quot;), header=None, delimiter=config.get(&quot;OS&quot;, &quot;delimiter&quot;), decimal=&quot;.&quot;)
df=df.fillna(value='')
print(df)
</code></pre>
<p>and in the output I only have 13 columns ending with the last non-null column.</p>
<p>How can I preserve the number of columns in the dataframe and have the trailing columns in the dataframe set to null?</p>
","0","Question"
"79508495","","<p>I want to create (for example) a small multi-index dataframe with 3 level index and one column of values.  There will only be a handful of rows.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Category</strong></td>
<td><strong>Sub Category</strong></td>
<td><strong>Sub Sub Category</strong></td>
<td></td>
</tr>
<tr>
<td>Machine</td>
<td>Car</td>
<td>Sedan</td>
<td>10</td>
</tr>
<tr>
<td>Machine</td>
<td>Bike</td>
<td>2 wheel</td>
<td>5</td>
</tr>
<tr>
<td>Machine</td>
<td>Bike</td>
<td>3 wheel</td>
<td>4</td>
</tr>
<tr>
<td>Animal</td>
<td>Donkey</td>
<td>Big</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>A requirement is that the data has to be easy to enter, a row at a time, from left to right (not top to bottom), so my first step is to make a list of lists.</p>
<p>Then I can use DataFrame method and then the set-index method like this:</p>
<pre class=""lang-py prettyprint-override""><code>data=[[&quot;Machine&quot;,&quot;Car&quot;,&quot;Sedan&quot;,10], [&quot;Machine&quot;,&quot;Bike&quot;,&quot;2 Wheel&quot;,5], [&quot;Machine&quot;,&quot;Bike&quot;,&quot;3 Wheel&quot;,4], [&quot;Animal&quot;,&quot;Donkey&quot;,&quot;Big&quot;,2]]
column_names=[&quot;Category&quot;,&quot;Sub Category&quot;,&quot;Sub Sub Category&quot;,&quot;Value&quot;]
df=pd.DataFrame(data,columns=column_names)
df.set_index([&quot;Category&quot;,&quot;Sub Category&quot;,&quot;Sub Sub Category&quot;])
</code></pre>
<p>But is there a way to make the multi-index DataFrame directly from a list, or something like it?</p>
","0","Question"
"79509728","","<p>I'm slowly migrating to polars from pandas and I have found that in some cases the polars syntax is tricky.</p>
<p>I'm seeking help to do a <a href=""https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.group_by.html"" rel=""nofollow noreferrer""><code>group_by</code></a> followed by a <a href=""https://docs.pola.rs/api/python/stable/reference/series/api/polars.Series.describe.html"" rel=""nofollow noreferrer""><code>describe</code></a> using less (or more readable) code.</p>
<p>See this example:</p>
<pre><code>from io import BytesIO
import pandas as pd
import polars as pl

S = b'''group,value\n3,245\n3,28\n3,48\n1,113\n1,288\n1,165\n2,90\n2,21\n2,109'''

pl_df = pl.read_csv(BytesIO(S))
pd_df = pd.read_csv(BytesIO(S))

# Polars' way
pl_df.group_by('group').map_groups(
    lambda df: (
        df['value']
        .describe()
        .with_columns(
            group=pl.lit(df['group'][0])
        )
    )
).pivot(index='group', on='statistic')
</code></pre>
<p>Something similar in pandas would be:</p>
<pre><code># pandas' 
pd_df.groupby('group').value.describe()
</code></pre>
","2","Question"
"79509755","","<p>I have data * in the following format in a text file.</p>
<pre><code>x1 y1 z1 x2 y2 z2 x3 y3 z3 data1x data1y data1z data2x data2y data2z data3x data3y data3z
</code></pre>
<p>I'm trying to reformat the file such the output looks like this:</p>
<pre><code>x1 y1 z1 data1x data1y data1z 
x2 y2 z2 data2x data2y data2z
x3 y3 z3 data3x data3y data3z
</code></pre>
<p>I'm sure that there's a smart way to do the same using for example pandas. I tried with pivot and pivot_table, but the output wasn't quite right. Could you please help me?</p>
<ul>
<li>It's node coordinates and the corresponding vector field values (all floats), and in reality each row consists of 10 nodes, and there are thousands of rows in the file</li>
</ul>
<p>This</p>
<pre><code>all_data = []
with open(old_file, &quot;r&quot;) as file1:
    skip_first_row = file1.readline()

    for line in file1:
        line_list = line.split()

        all_data.append(line_list[0:3]+line_list[30:33])
        all_data.append(line_list[3:6]+line_list[33:36])
        all_data.append(line_list[6:9]+line_list[36:39])


with open(new_file, &quot;w&quot;) as file2:
    write_headline = file2.write(&quot;%s\n&quot; % headline)
    for data in all_data:
        file2.write(' '.join(data) + '\n')

</code></pre>
<p>does the job, but I'm sure there a smarter way!</p>
","1","Question"
"79512283","","<p>First of all, sorry about my english is not good.</p>
<p>My problem is: I want to find products that can be sold together at one address by similar selling locations in using language Python pandas. For example, my dataframe like:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>PRODUCT</th>
<th>SELLING LOCATIONS</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1, 2, 3</td>
</tr>
<tr>
<td>B</td>
<td>2,5</td>
</tr>
<tr>
<td>C</td>
<td>7,8,9,10</td>
</tr>
<tr>
<td>D</td>
<td>5,4</td>
</tr>
<tr>
<td>E</td>
<td>10,11</td>
</tr>
</tbody>
</table></div>
<p>Result is:</p>
<p>A,B,D</p>
<p>C,E</p>
<p>Reason:</p>
<ul>
<li>Product A and B has same similar selling locations is 2; Product B and D has same similar selling locations is 5. Therefor, A,B,D is can be sold together in one address.</li>
<li>Similar to product C and E.</li>
</ul>
","0","Question"
"79512793","","<p>I have a df_source.
It contains either three columns, namely A_1, B_1 and C_1, or
six columns, namely A_1, A_2, B_1, B_2, C_1 and C_2.</p>
<p>I want to select only the A_* and B_* and save them into another df, namely df_new.
The resulting df_new may contain either two columns or four columns.</p>
<p>I am thinking of doing something like the following:</p>
<pre><code>df_new = df_source[[&quot;A_1&quot;, &quot;B_1&quot;]]
if 'A_2' in df_new.columns:
    # Also add A_2, B_2 columns in df_new, but I don't know how to add the columns to the created df_new.
</code></pre>
<p>Instead of</p>
<pre><code>df_new = df_source[[&quot;A_1&quot;, &quot;B_1&quot;]]
if 'A_2' in df_new.columns:
  df_new = df_source[[&quot;A_1&quot;, &quot;B_1&quot;, &quot;A_2&quot;, &quot;B_2&quot;]]
</code></pre>
","1","Question"
"79512868","","<p>I need to read data from an Excel file. The first cell contains the property name, and the second cell contains the property value. However, some of the property names in the first column are merged across two or more columns, and the corresponding values are in the next cell. For example, the property name &quot;Ref&quot; is in columns A and B, and its value is in column C.</p>
<p>I want to retrieve the value of the &quot;Ref&quot; property from column C in my Excel file.</p>
<p>Here is my excel image:</p>
<p><a href=""https://i.sstatic.net/cwtXNjag.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cwtXNjag.png"" alt=""enter image description here"" /></a></p>
<p>I am using python. Here is the output:</p>
<pre><code>Approval Memo of : SHILPI AKTER
Name of the Applicant : SHILPI AKTER
Name of Territory : Comilla
Total Family Expenses : 30000
Ref : N/A
Amount : N/A
Total Amount : 3000
</code></pre>
<p><strong>Ref and Amount</strong> Properties value not found.
Here is my code:</p>
<pre><code>     import os
    import openpyxl
    from openpyxl.utils import column_index_from_string

    file_path = r&quot;D:\file\input\example.xlsx&quot;  
    if os.path.exists(file_path):
        print(&quot;File exists!&quot;)
    else:
        print(&quot;File not found! Check the path.&quot;)
        exit()

    target_sheet = &quot;Output Approval Templete&quot;

    # Define the properties to extract
    properties = [ 
        &quot;Approval Memo of&quot;,
        &quot;Name of the Applicant&quot;,
        &quot;Name of Territory&quot;,
        &quot;Total Family Expenses&quot;,
        &quot;Ref&quot;,
        &quot;Amount&quot;,
        &quot;Total Amount&quot;
    ]

    # Function to get the actual value from a merged cell
    def get_merged_cell_value(sheet, row, col):
        for merged_range in sheet.merged_cells.ranges:
            min_row, min_col, max_row, max_col = merged_range.bounds  # Extract merged cell bounds
            if min_row &lt;= row &lt;= max_row and min_col &lt;= col &lt;= max_col:
                return sheet.cell(min_row, min_col).value  # Return the first cell's value of the merged range
        return sheet.cell(row, col).value  

    # Function to format numeric values properly
    def format_value(value):
        if isinstance(value, float) and value &gt; 1e10:  # Large numbers like NID
            return str(int(value))  # Convert to integer and string to avoid scientific notation
        elif isinstance(value, (int, float)):  # General number formatting
            return str(value)
        elif value is None:
            return &quot;N/A&quot;  # Handle missing values
        return str(value).strip()

    try:
        # Load the workbook
        wb = openpyxl.load_workbook(file_path, data_only=True)

        if target_sheet not in wb.sheetnames:
            print(f&quot;Sheet '{target_sheet}' not found in the file.&quot;)
        else:
            ws = wb[target_sheet]
            extracted_data = {}

            # Iterate over rows to extract data
            for row in ws.iter_rows():
                for cell in row:
                    # Check if the cell value is a property we are looking for
                    if cell.value and isinstance(cell.value, str) and cell.value.strip() in properties:
                        prop_name = cell.value.strip()
                        col_idx = cell.column  # Get column index (1-based)
                        next_col_idx = col_idx + 1  # Next column index

                        # Ensure next column exists within sheet bounds
                        if next_col_idx &lt;= ws.max_column:
                            # Check if the cell is merged, and get its value
                            next_value = get_merged_cell_value(ws, cell.row, next_col_idx)
                            
                            # Store the formatted value for the property
                            extracted_data[prop_name] = format_value(next_value)  # Store extracted value

            # Print extracted values
            for key, value in extracted_data.items():
                print(f&quot;{key} : {value}&quot;)

    except Exception as e:
        print(f&quot;Error loading workbook: {e}&quot;)
</code></pre>
<p>Please help me to find out merge cell properties value.</p>
","1","Question"
"79513050","","<p>Suppose I have a DataFrame with the following format of strings separated by commas:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>ColumnName</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>apple,peach,orange,pear,</td>
</tr>
<tr>
<td>1</td>
<td>orange, pear,apple</td>
</tr>
<tr>
<td>2</td>
<td>pear</td>
</tr>
<tr>
<td>3</td>
<td>peach,apple</td>
</tr>
<tr>
<td>4</td>
<td>orange</td>
</tr>
</tbody>
</table></div>
<p>The actual number of rows will be greater than 10,000.</p>
<p>I want to expand the DataFrame and sort the DataFrame by row 0.</p>
<p>My expected output is below, where None is of type NoneType:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>apple</td>
<td>peach</td>
<td>orange</td>
<td>pear</td>
</tr>
<tr>
<td>1</td>
<td>apple</td>
<td>None</td>
<td>orange</td>
<td>pear</td>
</tr>
<tr>
<td>2</td>
<td>None</td>
<td>None</td>
<td>None</td>
<td>pear</td>
</tr>
<tr>
<td>3</td>
<td>apple</td>
<td>peach</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>4</td>
<td>None</td>
<td>None</td>
<td>orange</td>
<td>None</td>
</tr>
</tbody>
</table></div>
<p>I have expanded the data using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>df = df['ColumnName'].str.split(',', expand=True)  # Expand initial DataFrame
</code></pre>
<p>However, I am unable to sort or reorder the data as desired despite trying various combinations of <code>df.sort_values()</code>.</p>
","2","Question"
"79513586","","<p>Say I have a dataset with multiindex i1 i2 i3 and columns A B1 .. B_n where A stores the i3 level index of a value I want to look up from another observation. I want to use this value to look up the B2 value. There can be pointers to a value not in the df or multiple pointers to the same value. However I am not getting what I want and suspect merging might not be the best way of doing this.</p>
<p>For example row (1,10,102) should have looked up i3=101's B2_x value here:</p>
<pre><code>            A  B1  B2_x  c1  B2_y  b2_wanted
i1 i2 i3                                     
1  10 101  103   2     1   1   2.0        2.0
      102  101   2     2   2   NaN        1.0
      103  100   1     2   2   1.0        NaN
   11 100  102   1     2   2   NaN        NaN
      101  103   1     2   2   NaN        NaN
   12 101  103   2     1   2   1.0        1.0
      103  101   1     1   2   1.0        1.0
2  10 100  103   1     2   1   NaN        NaN
      101  103   1     1   2   1.0        NaN
      102  101   1     1   1   NaN        1.0
</code></pre>
<p>This is the code for my attempt</p>
<pre><code># generate test df
import pandas as pd
import numpy as np
np.random.seed(2)
range1 = range(1, 4) 
range2 = range(10, 13) 
range3 = range(100, 104)  
multi_index = pd.MultiIndex.from_product([range1, range2, range3], names=['i1', 'i2', 'i3'])

df = pd.DataFrame(index=multi_index, data={'A': np.random.randint(100, 104, size=len(multi_index)),
                                           'B1': np.random.randint(1, 3, size=len(multi_index)),
                                           'B2': np.random.randint(1, 3, size=len(multi_index)),
                                           'c1': np.random.randint(1, 3, size=len(multi_index))})

df = df[df.index.get_level_values(&quot;i3&quot;)!=df[&quot;A&quot;]].head(10).copy()

# attempt to solve starts here

vars_to_lookup=df[[&quot;A&quot;,&quot;B2&quot;]].copy().reset_index().set_index([&quot;i1&quot;,&quot;i2&quot;,&quot;A&quot;])
vars_to_lookup.index.names=[&quot;i1&quot;,&quot;i2&quot;,&quot;i3&quot;]
vars_to_lookup.drop(&quot;i3&quot;,axis=1,inplace=True)

temp_df=pd.merge(df,vars_to_lookup,left_index=True,right_index=True,how=&quot;left&quot;)
temp_df[&quot;b2_wanted&quot;]= pd.Series([2,1,np.nan,np.nan,np.nan,1,1,np.nan,np.nan,1 ],index=df.index)
</code></pre>
","0","Question"
"79513720","","<p><a href=""https://i.sstatic.net/zEEi3W5n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zEEi3W5n.png"" alt=""Sample of the dataframe"" /></a></p>
<p>The three columns on the left are day month, and year. I am trying to fill in <code>NaN</code> values in the last column which I am calling <code>'C'</code>. For each week of each month there is one non-empty value in the last column, and I would like to assign the <code>NaN</code> values with the non-empty value.</p>
<p>So far I have tried doing it with the first week with the following code:</p>
<pre class=""lang-py prettyprint-override""><code>for year in range(2013, 2023):
  for month in range(1, 13):
    for day in range(1, 8):
      df.loc[pd.isnull(df['C']), 'C'] = df.loc[(df['year'] == year) &amp; (df['month'] == month) &amp; (df['day'] == 3), 'C']
</code></pre>
","0","Question"
"79513766","","<p>I'm trying to calculate a 2d variable z = x + y where x and y are 1d arrays of unequal dimensions (say, x- and y-coordinate points on a spatial grid). I'd like to display the result row-by-row in which the values of x and y are in the first two columns and the corresponding value of z calculated from these x and y values are in the third, something like the following for x = [1, 2] and y = [3, 4, 5]:</p>
<pre><code>x  y  z
1  3  4
1  4  5
1  5  6
2  3  5
2  4  6
2  5  7
</code></pre>
<p>The code below works (using lists here, but will probably need numpy arrays later):</p>
<pre><code>import pandas as pd

x = [1, 2]
y = [3, 4, 5]
col1 = []
col2 = []
z = []
for i in range(len(x)):
    for j in range(len(y)):
        col1.append(x[i])
        col2.append(y[j])
        z.append(x[i]+y[j])

df = pd.DataFrame(zip(col1, col2, z), columns=[&quot;x&quot;, &quot;y&quot;, &quot;z&quot;])
print(df)
</code></pre>
<p>Just wondering, is there a better way of doing this without using the loop by some combination of meshgrid, indices, flatten, v/hstack, and reshape? The size of x and y will typically be around 100.</p>
","2","Question"
"79515072","","<p>I have a dataframe that has information about employee's employment info and I am trying to combine with another dataframe that has their Employee ID #.</p>
<p><code>df</code></p>
<pre><code>Name             SSN
Doe, John A      XXXX-XX-1234
Doe, Jane B      XXXX-XX-9876
Test, Example    XXXX-XX-0192
</code></pre>
<p><code>Employee_Info</code></p>
<pre><code>First_Name    Last_Name            SSN     EmployeeID
      John          Doe    999-45-1234             12
      JANE          DOE    999-45-9876             13
   Example         Test    999-45-0192             14
</code></pre>
<p>My desired output is:</p>
<pre><code>Name             SSN          EmployeeID
Doe, John A      XXX-XX-1234          12
Doe, Jane B      XXX-XX-9876          13
Test, Example    XXX-XX-0192          14
</code></pre>
<p>The <code>df</code> dataframe actually has the SSN masked except for the last 4 characters. Here is the code I have currently:</p>
<pre><code>df['SSN_Last_4'] = df['SSN4'].str[-4:]
Employee_Info['SSN_Last_4'] = Employee_Info['SSN'].str[-4:]
df2 = pd.merge(df, Employee_Info, on='SSN', how='left')
</code></pre>
<p>However because some employees might have the same last 4 digits of SSN, I need to also match based on name. However the caveat is that the <code>Name</code> in <code>df</code> is the employee fullname (which might include middle initial) and the case might be different. My original idea was to split the Name on <code>, </code> and drop middle initial, and then convert all the name columns to be lowercase and modify the join. However I feel that there are better methods to join the data.</p>
","0","Question"
"79515992","","<p>Ok I cannot wrap my head around that.</p>
<p>I have two dataframes, <code>first_df</code> and <code>second_df</code>, where <code>first_df</code> contains information about street segments, including the street name, start and end numbers of street segments, and whether the segment is for even or odd numbers. <code>second_df</code> contains street numbers and their corresponding addresses.</p>
<p>The goal is to assign the correct &quot;Secteur Elementaire&quot; (elementary sector) to each street number in <code>second_df</code> based on the information in <code>first_df</code>. So it needs to pass three checks:</p>
<ol>
<li>Check if streetname exists (and create a subset dataframe containing only the elemnts corresponding to this streetname)</li>
<li>Check if streetnumber is within the range defined by <code>first_df['Début']</code> and <code>first_df['Fin']</code></li>
<li>Check if it is even or odd, and assign the sector based on its whether the number is part of the even or odd sector.</li>
</ol>
<p>The issue I'm facing is that the logic to determine the correct sector based on whether the street number is even or odd is not working as expected. The current implementation is assigning the wrong sectors in some cases.</p>
<pre class=""lang-py prettyprint-override""><code>first_df = pd.DataFrame({
    'Voie': ['AVENUE D EPERNAY', 'AVENUE D EPERNAY', 'AVENUE D EPERNAY', 'AVENUE D EPERNAY'],
    'Périmètre élémentaire': ['PROVENCAUX', 'AVRANCHES', 'MAISON BLANCHE', 'SCULPTEURS JACQUES'],
    'Périmètre maternelle': ['AUVERGNATS-PROVENCAUX', 'AVRANCHES', 'MAISON BLANCHE', 'SCULPTEURS JACQUES'],
    'Début': [142, 1, 73, 2],
    'Fin': [998, 71, 999, 140],
    'Partie': ['Pair', 'Impair', 'Impair', 'Pair']
})

second_df = pd.DataFrame({
    'numero': [1, 2, 6, 7, 8, 9, 10, 12],
    'nom_afnor': ['AVENUE D EPERNAY', 'AVENUE D EPERNAY', 'AVENUE D EPERNAY', 'AVENUE D EPERNAY', 'AVENUE D EPERNAY', 'AVENUE D EPERNAY', 'AVENUE D EPERNAY', 'AVENUE D EPERNAY'],
    'Secteur Elementaire': ['tbd', 'tbd', 'tbd', 'tbd', 'tbd', 'tbd', 'tbd', 'tbd']
})


def sort_df(first_df, second_df):
    for bano_index, row in second_df.iterrows():
        street_number = row['numero']
        street_name = row['nom_afnor']
        #print(f'first loop: {num_de_rue}')

        if street_name in first_df['Voie'].values: # check if street name exists
            reims_filtered = first_df.loc[first_df['Voie'] == street_name] # if it does create a dataframe containing only the elements matching the street name
            #print(reims_filtered)
            for reims_index, reims_matching_row in reims_filtered.iterrows(): # iterate over the rows the filtered dataframe
                #print(reims_matching_row)
                if street_number &gt;= reims_matching_row['Début'] and street_number &lt;= reims_matching_row['Fin']: # check if street number is between range of street segment
                    #print(f'Check range {street_number} {reims_matching_row['Périmètre élémentaire']}')
                    if street_number % 2 == 0: # check if street number is even
                        print(reims_index, street_number, reims_matching_row['Partie'])
                        if reims_matching_row['Partie'] == 'Pair': # if it is even, then check which sector should be taken to be assigned based on street segment and its corresponding odd/even sector
                            print(f'Check column {street_number} {reims_matching_row['Périmètre élémentaire']}')
                            sector_to_assign = reims_matching_row['Périmètre élémentaire']
                            second_df.at[bano_index, 'Secteur Elementaire'] = sector_to_assign
                            break
                    else:
                        print(f'Check odd {street_number} {reims_matching_row['Périmètre élémentaire']}')
                        sector_to_assign = reims_matching_row['Périmètre élémentaire']
                        second_df.at[bano_index, 'Secteur Elementaire'] = sector_to_assign
                    break

    return second_df

sort_df(first_df, second_df)
</code></pre>
<p>Output of running this function:</p>
<pre><code>Check odd 1 AVRANCHES
1 2 Impair
1 6 Impair
Check odd 7 AVRANCHES
1 8 Impair
Check odd 9 AVRANCHES
1 10 Impair
1 12 Impair
   numero         nom_afnor Secteur Elementaire
0       1  AVENUE D EPERNAY           AVRANCHES
1       2  AVENUE D EPERNAY                 tbd
2       6  AVENUE D EPERNAY                 tbd
3       7  AVENUE D EPERNAY           AVRANCHES
4       8  AVENUE D EPERNAY                 tbd
5       9  AVENUE D EPERNAY           AVRANCHES
6      10  AVENUE D EPERNAY                 tbd
7      12  AVENUE D EPERNAY                 tbd
</code></pre>
<p>Clearly there is an issue with the odd or even logic, especially when trying to assign <code>reims_matching_row['Partie'] == 'Pair'</code>.</p>
<p>If someone has any leads…</p>
","2","Question"
"79516578","","<p>I'm working with a mileage dataset and am having trouble programming a way to filter out clearly wrong mileages (e.g. 350,000 and 1234).</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>VIN</th>
<th>MILEAGE</th>
<th>DATE</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>10000</td>
<td>2024-01-01</td>
</tr>
<tr>
<td>a</td>
<td>20000</td>
<td>2024-02-01</td>
</tr>
<tr>
<td>a</td>
<td>350000</td>
<td>2024-03-01</td>
</tr>
<tr>
<td>a</td>
<td>30000</td>
<td>2024-04-01</td>
</tr>
<tr>
<td>a</td>
<td>1234</td>
<td>2024-05-01</td>
</tr>
<tr>
<td>a</td>
<td>40000</td>
<td>2024-06-01</td>
</tr>
</tbody>
</table></div>
<p>I first tried filtering out decreasing mileages by grouping by VIN and getting the difference in one mileage to the next, but because of the 350,000 that filters out 30,000 and 40,000 (which should remain).</p>
<p>My next thought was to check the next few rows and see if removing the 350,000 (basically if mileage decreases, remove the previous row and check:) would make mileage follow a slightly increasing trend, but then I'm not sure how I would treat the 1234. My last resort would be to say that mileage can't increase/decrease by more than X amount, but I was wondering if there was a programmatic approach to filtering out these errors.</p>
<p>Thank you!</p>
","0","Question"
"79517122","","<p>Even when I directly copy a movie name from my own dataset and provide it as input, it still cannot find the movie name in the dataset and always returns <code>None</code> as the output.</p>
<pre><code>from surprise import SVD, Dataset
from surprise import Reader
import pandas as pd
from fuzzywuzzy import process


def find_book(user_input,books):
    try:
        
        result = process.extractOne(user_input, books)
        if result:  
            match, score = result  
            if score &gt; 70:  
                return match
        return None  
    except:
        return None

movies_pd = pd.read_csv('movies.csv')
rating_pd = pd.read_csv('ratings.csv')

movie_stats = rating_pd.groupby(&quot;movieId&quot;).agg(
    avg_rating = (&quot;rating&quot;,&quot;mean&quot;),
).reset_index()

pd1 = pd.merge(movies_pd,movie_stats,on=&quot;movieId&quot;)


books = pd1[&quot;title&quot;].str.strip().str.lower()
user_input = input(&quot;what movie have you watched? &quot;).strip().lower()
match_book = find_book(user_input,books)
print(match_book)
</code></pre>
","0","Question"
"79517222","","<p>here's the dataset and the problem I'm working with</p>
<pre><code>import numpy as np
import pandas as pd

couples = pd.DataFrame({
    'man': [
        ['fishing', 'biking', 'reading'],
        ['hunting', 'mudding', 'fishing'],
        ['reading', 'movies', 'running'],
        ['running', 'reading', 'biking', 'mudding'],
        ['movies', 'reading', 'yodeling']
    ],
    'woman': [
        ['biking', 'reading', 'movies'],
        ['fishing', 'drinking'],
        ['knitting', 'reading'],
        ['running', 'biking', 'fishing', 'movies'],
        ['movies']
    ]
})

print(couples)
#                                    man                               woman
# 0           [fishing, biking, reading]           [biking, reading, movies]
# 1          [hunting, mudding, fishing]                 [fishing, drinking]
# 2           [reading, movies, running]                 [knitting, reading]
# 3  [running, reading, biking, mudding]  [running, biking, fishing, movies]
# 4          [movies, reading, yodeling]                            [movies]
</code></pre>
<p><em>For each couple, determine what hobbies each man has that his wife doesn’t and what hobbies each woman has that her husband doesn’t.</em></p>
<p>I tried solving it the way shown below:
<code>couples['hobbies than man has that woman doesnt'] = couples['man'][~couples['man'].isin(couples['woman'])] </code></p>
<p>but instead of returning fishing in the 1st line, hunting and mudding in the 2nd and so on, it simply returns the first column:</p>
<pre><code>man woman   hobbies than man has that woman doesnt
0   [fishing, biking, reading]  [biking, reading, movies]   [fishing, biking, reading]
1   [hunting, mudding, fishing] [fishing, drinking] [hunting, mudding, fishing]
2   [reading, movies, running]  [knitting, reading] [reading, movies, running]
3   [running, reading, biking, mudding] [running, biking, fishing, movies]  [running, reading, biking, mudding]
4   [movies, reading, yodeling] [movies]    [movies, reading, yodeling]
</code></pre>
<p>I've also tried converting both columns to str type before creating the third column but the result is the same. why does that happen and how do I fix it?</p>
","2","Question"
"79518311","","<p>I have panda series as the following :</p>
<pre><code>    1   1
    2   2
    3   3 
    4   4
    5   0
    6   0
    7   1
    8   2
    9   3
   10   0
   11   0
   12   0
   13   0
   14   1
   15   2
</code></pre>
<p>I have to arrange this in following format :</p>
<pre><code>    1   1
    2   2
    3   3 
    4   4
    5   0
    6   0
    7   3  ---&gt; 4-2+1 (previous non zero value - amount of previous zeroes + current value)
    8   4  ---&gt; 4-2+2 (previous non zero value - amount of previous zeroes + current value)
    9   5  ---&gt; 4-2+3 (previous non zero value - amount of previous zeroes + current value)
   10   0
   11   0
   12   0
   13   0
   14   2 ---&gt; 5-4+1 (previous non zero value - amount of previous zeroes + current value)
   15   3 ---&gt; 5-4+2 (previous non zero value - amount of previous zeroes + current value)
</code></pre>
<p>I am stuck at this. Till now I am able to produce a data frame with consecutive zeroes.</p>
<pre><code>zero = ser.eq(0).groupby(ser.ne(0).cumsum()).cumsum()
</code></pre>
<p>which gave me:</p>
<pre><code>    1   0
    2   0
    3   0 
    4   0
    5   1
    6   2
    7   0
    8   0
    9   0
   10   1
   11   2
   12   3
   13   4
   14   0
   15   0
</code></pre>
<p>if someone willing to assist on this. i am dropping cookie cutter for this problem which will create the above series.</p>
<pre><code>d = {'1': 1, '2': 2, '3': 3, '4':4, '5':0, '6':0, '7':1, '8':2, '9':3, '10':0, '11':0, '12':0, '13':0, '14':1, '15':2}
ser = pd.Series(data=d)
</code></pre>
","4","Question"
"79518641","","<p>I am performing a join where I'd like to have a variable name for the column to join on.</p>
<p>For example, <code>DF1</code> is an income statement that uses raw names for line items. <code>DF2</code> contains a mapping of the raw names to cleaned up names that depend on which company's income statement we are looking at. I'd like to have a variable <code>CO</code> that determines which column to join on. The end result should be bringing the cleaned up names into <code>DF1</code>.</p>
<p><code>DF1</code>:</p>
<p><img src=""https://i.sstatic.net/bpk7sxUr.png"" alt=""enter image description here"" /></p>
<p><code>DF2</code>:</p>
<p><img src=""https://i.sstatic.net/fiH9kB6t.png"" alt=""enter image description here"" /></p>
<p>An example join is:</p>
<pre><code>DF1.join(DF2, DF1.Company_A = DF2.Final)
</code></pre>
<p>How do I define a variable <code>CO</code> to specify the <code>DF1</code> column in the join? So the join would be:</p>
<pre><code>DF1.join(DF2, DF1.CO = DF2.Final)
</code></pre>
<p>I am not sure how to write this in a way that Snowflake doesn't think the variable <code>CO</code> is a column name in <code>DF1</code>.</p>
<p>Snowpark solution preferred, but Pandas is OK as long as it works in Snowflake.</p>
","0","Question"
"79518999","","<p>I have a pandas Series <code>s</code>, and when I call <code>s.std(skipna=True)</code> and <code>s.std(skipna=False)</code> I get different results even when there are no NaN/null values in <code>s</code>, why? Did I misunderstand the <code>skipna</code> parameter? I'm using pandas 1.3.4</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

s = pd.Series([10.0]*4800000, index=range(4800000), dtype=&quot;float32&quot;)

# No NaN/null in the Series
print(s.isnull().any()) # False
print(s.isna().any()) # False

# Why the code below prints different results?
print(s.std(skipna=False)) # 0.0
print(s.std(skipna=True)) # 0.61053276
</code></pre>
","3","Question"
"79519830","","<p>This is my dataframe:</p>
<pre><code>df = pd.DataFrame({
    'a': [0, 0, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0]
})
</code></pre>
<p>Expected output is creating column <code>b</code>:</p>
<pre><code>    a  b
0   0  0
1   0  0
2   1  0
3  -1  1
4  -1 -1
5   0 -1
6   0 -1
7   0 -1
8   0  0
9   0  0
10 -1  0
11  0 -1
12  0 -1
13  1 -1
14  0  1
</code></pre>
<p>Logic:</p>
<p>I explain the logic by some examples:</p>
<p>I want to create column b  to df</p>
<p>I want to have a window of three rows</p>
<p>for example for row number 3 I want to look at three previous rows and capture the last non 0 value. if all of the values are 0 then 'b' is 0. in this case the last non zero value is 1. so column b is 1</p>
<p>for example for row number 4 . The last non zero value is -1 so column b is -1</p>
<p>I want to do the same for all rows.</p>
<p>This is what I have tried so far. I think there must be a better way.</p>
<pre><code>import pandas as pd
df = pd.DataFrame({
    'a': [0, 0, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0]
})

def last_nonzero(x):
    # x is a pandas Series representing a window
    nonzero = x[x != 0]
    if not nonzero.empty:
        # Return the last non-zero value in the window (i.e. the one closest to the current row)
        return nonzero.iloc[-1]
    return 0

# Shift by 1 so that the rolling window looks only at previous rows.
# Use a window size of 3 and min_periods=1 to allow early rows.
df['b'] = df['a'].shift(1).rolling(window=3, min_periods=1).apply(last_nonzero, raw=False).astype(int)
</code></pre>
","3","Question"
"79521718","","<p>I have a pandas dataset I would like to sort in a semi-random way. The data contains names and I need to sort them in a pattern so that the names are cycled through before being repeated, and I would also love for that order to be random.
When working down the dataset I need to encounter each name as evenly as possible as I do not always get through the entire list. When I have the list randomized, the data set is hundreds of lines long I often don't encounter the less frequent names as I only expect to check the first 20ish% of the data and need an equal chance of encountering each name regardless of how often they are in the list.
I would like the order of the names to be random so the same name isn't always choosen first with a fresh dataset if that's possible. Also I need to keep every entry in the list and can not drop lines.</p>
<p>For example</p>
<p>{
'Name' : ['Sam', 'John', 'Amy', 'Sam', 'Sam', 'Amy', 'Sam', 'Amy', 'Sarah', 'John', 'Sam'],</p>
<p>'Score' :['2','3','5','4','6','10','8','15','7','9', '12']
}</p>
<pre><code>    Name   Score
1   Sarah  7
2   Amy    15
3   Sam    4
4   John   3
5   Amy    5
6   Sam    2
7   John   9
8   Amy    10
9   Sam    8
10  Sam    12
11  Sam    6
</code></pre>
","0","Question"
"79522783","","<p>I need to calculate the floor of a localized timestamp with daily resolution, but I get an exception when the daylight saving time starts.</p>
<pre><code>&gt;&gt;&gt; pd.Timestamp('2024-09-08 12:00:00-0300', tz='America/Santiago').floor(&quot;D&quot;)
NonExistentTimeError: 2024-09-08 00:00:00
</code></pre>
<p>I understand that midnight does not exist on that day, clocks are moved to 1am after 11:59pm. Still I would have expected floor to return  <code>pd.Timestamp('2024-09-08 01:00:00-0300', tz='America/Santiago')</code>.</p>
<p>The same happens with ceil applied to the previous day:</p>
<pre><code>&gt;&gt;&gt; pd.Timestamp('2024-09-07 12:00:00-0300', tz='America/Santiago').ceil(&quot;D&quot;)
NonExistentTimeError: 2024-09-08 00:00:00
</code></pre>
<p>I made two attempts at solving this but I either get the same exception or the wrong answer:</p>
<pre><code>&gt;&gt;&gt; pd.Timestamp('2024-09-08 12:00:00').floor(&quot;D&quot;).tz_localize('America/Santiago')
NonExistentTimeError: 2024-09-08 00:00:00
&gt;&gt;&gt; pd.Timestamp('2024-09-08 12:00:00-0300').floor(&quot;D&quot;).tz_convert('America/Santiago')
Timestamp('2024-09-07 23:00:00-0400', tz='America/Santiago')  # Wrong answer
</code></pre>
","2","Question"
"79524057","","<p>Say I have a dataframe such as</p>
<pre><code>df = pd.DataFrame(data = {'col1': [1,np.nan, '&gt;3', 'NA'], 'col2':[&quot;3.&quot;,&quot;&lt;5.0&quot;,np.nan, 'NA']})
</code></pre>
<pre><code>out:
    col1    col2
0   1   3.
1   NaN &lt;5.0
2   &gt;3  NaN
3   NA  NA
</code></pre>
<p>What I would like is to strip stuff like &quot;&lt;&quot; or &quot;&gt;&quot;, and get to floats only</p>
<pre><code>out:
    col1    col2
0   1   3.
1   NaN 5.0
2   3   NaN
3   NaN NaN
</code></pre>
<p>I thought about something like</p>
<pre><code>df['col2'].replace({'.*' : r&quot;[-+]?(?:\d*\.*\d+)&quot;}, regex=True, inplace=True)
</code></pre>
<p>the idea being, replace anything with the regex for float, (I think), but this fails</p>
<pre><code>error: bad escape \d at position 8
</code></pre>
<p>I tried along the lines of</p>
<pre><code>df['col2'].replace({r&quot;[-+]?(?:\d*\.*\d+)&quot;: r&quot;\1&quot;}, regex=True, inplace=True)
</code></pre>
<p>assuming &quot;&gt;&quot;or &quot;&lt;&quot; come before the float number, but then this fails (does not catch anything) if the field is a string or <code>np.nan</code>.</p>
<p>Any suggestions please?</p>
","6","Question"
"79525802","","<pre><code>import plotly.graph_objects as go
fig=go.Figure()
cols=['NO2 mug/m^3','NO mug/m^3']

for idx,col in enumerate(cols):
    for df in [ag,snd]:
        fig.add_trace(go.Bar(x=df['Date'],y=df[col],
                             name=f'{str(df.Municipality.unique()[0])} {col}'))
        #fig.add_trace(go.Bar(x=ag['Date'],y=ag['NO mug/m^3']))
        fig.update_layout(barmode='group',xaxis=dict(title='Date'))
       # print(df)
fig
</code></pre>
<p>In this snippet i take two columns (which are common in both dataframes) and use a bar chart which give me the following result
<a href=""https://i.sstatic.net/fygLv6ta.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fygLv6ta.png"" alt=""enter image description here"" /></a></p>
<p>in this photo we get 4 distinct bars that do not overlay.
what i want is to keep this format but to also add a second y axis.
But because of the dynamic nature that this format will be used i want it to be versatile.
When lets say we have one column ( NO mug/m^3) to not have the second axis.</p>
<p>i tried something like this</p>
<pre><code>fig = go.Figure()
cols = ['NO2 mug/m^3', 'NO mug/m^3']  # Selected gases
dfs = [ag, snd]  # List of dataframes

for df_idx, df in enumerate(dfs):  # Iterate over DataFrames (locations)
    for col_idx, col in enumerate(cols):  # Iterate over gases
        fig.add_trace(go.Bar(
            x=df['Date'],
            y=df[col],
            name=f'{df.Municipality.unique()[0]} - {col}',
            offsetgroup=str(df_idx),  # Group bars per location
            marker=dict(opacity=0.8),
            yaxis=&quot;y&quot; if col_idx == 0 else &quot;y2&quot;  # Assign second gas to secondary y-axis
        ))

# Layout adjustments
layout_args = {
    &quot;barmode&quot;: &quot;group&quot;,  # Ensures bars are placed side-by-side
    &quot;xaxis&quot;: {&quot;title&quot;: &quot;Date&quot;},
    &quot;legend_title&quot;: &quot;Location - Gas&quot;
}

if len(cols) == 1:
    layout_args[&quot;yaxis&quot;] = {&quot;title&quot;: cols[0]}  # Single Y-axis case
else:
    layout_args[&quot;yaxis&quot;] = {&quot;title&quot;: cols[0]}
    layout_args[&quot;yaxis2&quot;] = {
        &quot;title&quot;: cols[1],
        &quot;overlaying&quot;: &quot;y&quot;,  # Overlay on primary y-axis
        &quot;side&quot;: &quot;right&quot;,
        &quot;showgrid&quot;: False
    }

fig.update_layout(**layout_args)
fig.show()
</code></pre>
<p>From what i tried i got something like the following<a href=""https://i.sstatic.net/pzIydR1f.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pzIydR1f.png"" alt=""enter image description here"" /></a></p>
<p>which is not desireable. is there any way to keep the format of my first image(4 distinct non overlaying bars per x point) while using some condition in order to achive my second axis?</p>
<p>thank for your patience</p>
","3","Question"
"79525897","","<p>I am trying to transpose 2 columns into 4 rows, but it's been extremely tricky as there is duplicate lines, making pivoting difficult.</p>
<p>What I have is this data as <code>df</code>:</p>
<pre><code>   Device      Serial   Property      Value
0  Computer2A  ABCSFT2  Display Name  Microsoft Edge
1  Computer2A  ABCSFT2  Install Date  2/22/2025
2  Computer2A  ABCSFT2  Publisher     Microsoft
3  Computer2A  ABCSFT2  Version       133.0.3065.92
4  Computer2A  ABCSFT2  Display Name  MSVC++ 2022 X64
5  Computer2A  ABCSFT2  Install Date  8/13/2024
6  Computer2A  ABCSFT2  Publisher     Microsoft
7  Computer2A  ABCSFT2  Version       14.40.33810
8  Computer2A  ABCSFT2  Display Name  MSVC++ 2022 X86
9  Computer2A  ABCSFT2  Install Date  8/13/2024
10 Computer2A  ABCSFT2  Publisher     Microsoft
11 Computer2A  ABCSFT2  Version       14.40.33810
[...]
</code></pre>
<p>What I want is this:</p>
<pre><code>Device      Serial   Display Name     Install Date  Publisher  Version
Computer2A  ABCSFT2  Microsoft Edge   2/22/2025     Microsoft  133.0.3065.92
Computer2A  ABCSFT2  MSVC++ 2022 X64  8/13/2024     Microsoft  14.40.33810
Computer2A  ABCSFT2  MSVC++ 2022 X86  8/13/2024     Microsoft  14.40.33810
</code></pre>
<p>The closest I've been able to come up with is using categoricals and unstacking, but it doesn't produce wanted results.</p>
<p>Code with <code>df</code> already defined (see <a href=""https://stackoverflow.com/questions/49028069/how-to-pivot-one-column-containing-strings-in-a-dataframe"">here</a> for origin):</p>
<pre><code>cats = ['Display Name','Install Date','Publisher','Version']
df['Property'] = pd.Categorical(df['Property'], categories=cats, ordered=True)

df = (df.set_index(['Device','Serial','Property'], append=True)['Value']
       .unstack()
       .reset_index()
       .rename_axis(None, axis=1)
       .sort_values(['Device','Serial']))
</code></pre>
<p>What is produced:</p>
<pre><code>       Device      Serial   Display Name     Install Date  Publisher  Version
12510  Computer01  STWF2G3  Microsoft Edge   NaN           NaN        NaN
12511  Computer01  STWF2G3  NaN              2/22/2025     NaN        NaN
12512  Computer01  STWF2G3  NaN              NaN           Microsoft  NaN
12513  Computer01  STWF2G3  NaN              NaN           NaN        133.0.3065.92
12514  Computer01  STWF2G3  MSVC++ 2022 X64  NaN           NaN        NaN
12515  Computer01  STWF2G3  NaN              8/13/2024     NaN        NaN
12516  Computer01  STWF2G3  NaN              NaN           Microsoft  NaN
12517  Computer01  STWF2G3  NaN              NaN           NaN        14.40.33810
12518  Computer01  STWF2G3  MSVC++ 2022 X86  NaN           NaN        NaN
12519  Computer01  STWF2G3  NaN              8/13/2024     NaN        NaN
12520  Computer01  STWF2G3  NaN              NaN           Microsoft  NaN
12521  Computer01  STWF2G3  NaN              NaN           NaN        14.40.33810
[...]
</code></pre>
<p>What am I missing here? Does anyone know?</p>
<p>I've tried pivot_table() and melt(), but they aren't viable options because of various reasons. Pivoting seems to be designed for numbers, not text.</p>
<p>I have a feeling the issue lies with using Append=True, but I have duplicate identifiers with the left two columns, so I often get this error using many recommended code I've found, and I have no idea how to work with the data:</p>
<p><code>ValueError: Index contains duplicate entries, cannot reshape</code></p>
<p>Any help would be appreciated. Thanks.</p>
","0","Question"
"79526121","","<p>I'm struck with a problem, i have date frame as below, it has data for distributor who supply the items for different locations, now i want to calculate, <strong>for a particular day, does any item ( example: apple) had different cost at a different location ( for the case of apple the price in KA and TN is same but AP it differs)</strong>, how to solve this, i tried grouping with date,item,price and location not getting how to proceed or what i'm trying is right or wrong.</p>
<p>is it also possible to get the difference cost if there a difference?</p>
<pre><code>   distributor  Location    Date    item    cost
0   GEC           KA    2025-02-01  Apple   10.0
1   GEC           TN    2025-02-01  Apple   10.0
2   GEC           AP    2025-02-01  Apple   9.0
3   GEC           KA    2025-02-02  Orange  8.0
4   GEC           TN    2025-02-02  Orange  7.0
5   GEC           AP    2025-02-02  Orange  8.5
6   GEC           KA    2025-02-03  Banana  6.0
7   GEC           TN    2025-02-03  Banana  6.0
8   GEC           AP    2025-02-03  Banana  6.0
</code></pre>
<p>P.S: if you need any more info please ask, please don't don vote the question i badly need an answer</p>
","0","Question"
"79526325","","<p>I am working with a pandas DataFrame where one of the columns contains datetime values, and I need to identify duplicate entries in the &quot;Data&quot; column. The datetime values include both the date and the exact time (hours, minutes, and seconds). However, I noticed an issue when I read the data from a .csv file — pandas does not seem to consider the time down to the second when identifying duplicates.</p>
<p>Interestingly, when I create synthetic data directly in pandas (like in the example below), the expected output works correctly, and it identifies the duplicates as I would expect. But when I read the same data from a .csv file, it marks even datetime values that are different by the hour as duplicates, which is not what I want.</p>
<p>Here is an example of my synthetic DataFrame:</p>
<pre><code>import pandas as pd

# Creating synthetic data with random IDs and names
data = {
    'ID': ['ID-1001', 'ID-1002', 'ID-1003', 'ID-1004', 'ID-1005', 'ID-1006', 'ID-1007', 'ID-1008', 'ID-1009', 'ID-1010'],
    'Name': ['Sensor-A', 'Sensor-B', 'Sensor-C', 'Sensor-D', 'Sensor-E', 'Sensor-F', 'Sensor-G', 'Sensor-H', 'Sensor-I', 'Sensor-J'],
    'Code': [330735, 330736, 330737, 330738, 330739, 330740, 330741, 330742, 330743, 330744],
    'Date': [
        '2022-01-01 12:00:00', '2022-01-01 12:00:00', '2022-01-01 13:00:00', '2022-01-01 14:00:00', 
        '2022-01-02 12:00:00', '2022-01-02 13:00:00', '2022-01-02 14:00:00', '2022-01-02 15:00:00', 
        '2022-01-03 12:00:00', '2022-01-03 13:00:00'
    ]
}

# Convert to DataFrame
dd_csv = pd.DataFrame(data)

# Ensure 'Date' is in datetime format
dd_csv['Date'] = pd.to_datetime(dd_csv['Date'])
</code></pre>
<p>In this dataset, the following rows have exact duplicate datetime values (same date and time):</p>
<p>2022-01-01 12:00:00 for Sensor-A and Sensor-B (these are duplicates).
Now, I want to check for duplicates in the &quot;Data&quot; column based on the exact datetime value, including both date and time. It works ok for the synthetic data above.</p>
<pre><code>duplicates_all = dd_csv['Date'].duplicated(keep=False)
print(dd_csv[duplicates_all])
</code></pre>
<pre><code>      ID      Name    Code                Date
0  ID-1001  Sensor-A  330735  2022-01-01 12:00:00
1  ID-1002  Sensor-B  330736  2022-01-01 12:00:00
</code></pre>
<p>However, when the data is read from a .csv file (real data), the time is not correctly recognized down to the second. This results in pandas marking entries with the same date but different times (down to the hour) as duplicates, even if I set the format before:</p>
<pre><code>import pandas as pd

# URL of the CSV file in the GitHub repository
url = 'https://raw.githubusercontent.com/jc-barreto/Data/main/test_data.csv'

# Read the CSV file directly from the URL
real_data = pd.read_csv(url)

# Convert the 'Date' column to datetime format
real_data['Date'] = pd.to_datetime(real_data['Date'], format=&quot;%Y-%m-%d %H:%M:%S&quot;, errors='coerce')

# Identify rows with duplicate dates
duplicates_all = real_data['Date'].duplicated(keep=False)

# Print the rows with duplicate dates
print(real_data[duplicates_all])
</code></pre>
<p>and the output is:</p>
<pre><code>
        Unnamed: 0 ID                Date         T
11774        11774  A 2017-05-25 12:00:00  20.55000
11775        11775  A 2017-05-25 13:00:00  20.56000
11776        11776  A 2017-05-25 14:00:00  20.56000
11777        11777  A 2017-05-25 15:00:00  20.57000
11778        11778  A 2017-05-25 16:00:00  20.57000

</code></pre>
<p>where clear the dates are not repeated since it have different times.</p>
<p>I have tried the suggestion from the answer below, but didn't work neither:</p>
<pre><code>real_data['date_only'] = [x.date() for x in real_data['Date']]
real_data['time_only'] = [x.time() for x in real_data['Date']]

duplicates_all2 = real_data[['date_only', 'time_only']].duplicated(keep=False)
print(real_data[duplicates_all2])
</code></pre>
<p><strong>How do I fix that?</strong>
I need to fix because I'm going to use the ID + Data as a key for a database update, to make sure I only update data that is not in the database.</p>
","0","Question"
"79526398","","<p>I have a pandas dataframe as follows:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Athlete ID</th>
<th>City</th>
<th>No. of Sport Fields</th>
</tr>
</thead>
<tbody>
<tr>
<td>1231</td>
<td>LA</td>
<td>81</td>
</tr>
<tr>
<td>4231</td>
<td>NYC</td>
<td>80</td>
</tr>
<tr>
<td>2234</td>
<td>NJ</td>
<td>64</td>
</tr>
<tr>
<td>1223</td>
<td>SF</td>
<td>75</td>
</tr>
<tr>
<td>4531</td>
<td>LA</td>
<td>81</td>
</tr>
<tr>
<td>2345</td>
<td>NYC.</td>
<td>80</td>
</tr>
<tr>
<td>...</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I want to print the City and No. of Sport Fields columns and group by City and sort by No. of Sport Fields. groupby() won't work here because I am not calculating anything.</p>
","2","Question"
"79526965","","<p>One problem with a pandas DataFrame is that it needs some data to create its structure. Hence, it can be a problem to represent the no-row case.</p>
<p>For example, suppose I have a function that returns a list of records represented as dictionaries: <code>get_data() -&gt; list[dict[str, Any]]</code> and I want to have a function that returns a DataFrame of the same data:</p>
<pre class=""lang-py prettyprint-override""><code>def get_dataframe() -&gt; pd.DataFrame:
    l = get_data()
    df = pd.DataFrame(l)
    return df
</code></pre>
<p>This works well except when <code>len(l)=0</code> because pandas needs at least one record to infer the number of columns and column types. It is not great to return None in this case because you would likely need to write a ton of if/else statements downstream to handle the zero-record case. Ideally, it would be nice to return an empty DataFrame with the correct number of columns and column types so that we don't have to do special treatment for the no record case in the downstream code. But it is very tedious to do, because:</p>
<ol>
<li>In <code>get_dataframe()</code>, I need to specify the number of columns and column types to create an empty DataFrame, but such information is already specified somewhere else. It is tedious to specify the same things twice.</li>
<li>Because I specify the same information twice, they may not be consistent. So I would need to add code to check consistency.</li>
<li>Believe it or not, the DataFrame constructor does not take a list of dtypes. There are workarounds to specify a type for each column, but it is not convenient.</li>
</ol>
<p>One idea to remove the redundancy is that instead of representing the raw data as a list of dict, I represent them as a list of dataclass, which allows me to annotate the type of each field. I can then use the annotation information to create the column types. This is not ideal either because type annotation is optional, and also the mapping of Python types to <code>dtype</code> is not one-to-one.</p>
<p>I wonder how is the situation of no data usually handled.</p>
","1","Question"
"79527518","","<p>I am having problem with install pandasai, i have tried conda install, remove and re-install.
link: <a href=""https://invoice-reader-llm-1.streamlit.app/"" rel=""nofollow noreferrer"">https://invoice-reader-llm-1.streamlit.app/</a></p>
<pre><code>      duckdb/extension/jemalloc/jemalloc/include\jemalloc/internal/jemalloc_internal_macros.h(36): warning C4005: 'restrict': macro redefinition
  duckdb/extension/jemalloc/jemalloc/include\jemalloc/internal/jemalloc_internal_decls.h(87): note: see previous definition of 'restrict'
  error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.43.34808\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
  [end of output]
</code></pre>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
ERROR: Failed building wheel for duckdb
Running setup.py clean for duckdb
Failed to build duckdb
ERROR: Failed to build installable wheels for some pyproject.toml based projects (duckdb)</p>
<p><a href=""https://i.sstatic.net/z1QqXAY5.png"" rel=""nofollow noreferrer"">enter image description here</a>
<a href=""https://i.sstatic.net/26NHxKrM.png"" rel=""nofollow noreferrer"">when I use pandasai</a></p>
","0","Question"
"79529354","","<p>I have 2 dataframes that are the results of 2 queries, returned in JSON. One of the dataframes <code>sap_scrapped</code> looks like this and was generated by:</p>
<pre><code>sap_scrapped = pd.DataFrame(response.json()[&quot;d&quot;][&quot;results&quot;]))
</code></pre>
<pre class=""lang-none prettyprint-override""><code>       Material  Scrapped
0  08-008099-00         2
1  10-000001-00         5
</code></pre>
<p>The other dataframe <code>sap_warehouse</code> looks like this and was generated by:</p>
<pre><code>sap_warehouse = pd.DataFrame(response.json()[&quot;d&quot;][&quot;results&quot;]))
</code></pre>
<pre class=""lang-none prettyprint-override""><code>       Material  SSP  Scrapped
0  10-000001-00    0         0
1  10-789001-00   10         7
</code></pre>
<p>I want to compare each value in the <code>sap_scrapped</code> 'Material' column to the <code>sap_warehouse</code> 'Material' column.  If the scrapped material is not found anywhere in <code>sap_warehouse</code>, then I want to insert that missing material to the <code>sap_warehouse</code> dataframe. Using the data above, I'd want to insert <code>08-008099-00</code> into the <code>sap_warehouse</code> dataframe into the 'Material' column, zero in the SSP column and copy the 'Scrapped' value from the <code>sap_scrapped</code> dataframe (2) into 'Scrapped' in <code>sap_warehouse</code>.</p>
<p>How can I do that?</p>
","0","Question"
"79530224","","<p>I'm processing a dataset with over 10 thousand rows in <code>pandas</code>, and I need to perform groupby and aggregation operations to summarize the data. However, my system has limited memory (8GB RAM), and the current approach is running into performance issues. I've tried chunking the data and using <code>Dask</code> for parallel processing, but the improvements are minimal.
For example, my dataset looks like this:</p>
<pre><code>ID  | Value  
----|------  
1   | 100  
2   | 200  
1   | 300  
</code></pre>
<p>and my code of aggregation:</p>
<pre><code>import pandas as pd

data = pd.read_csv('large_file.csv')
result = data.groupby('ID').agg({'Value': 'sum'})
</code></pre>
<p>SO my question is, are there alternative approaches or tools I could use to further optimize the performance of these operations?</p>
","1","Question"
"79531352","","<p>I have dataframes that show the codes belonging to each ID.</p>
<pre><code>import pandas as pd

data_group = {
    'id': ['0111','0123'],
    'code': [['1', '2', '3'],['1','2']]
}
df_group = pd.DataFrame(data_group)
</code></pre>
<p>and i have dataframes with ids and code and dates this is sample of dataframe</p>
<pre><code>data = {
    'codice': ['1', '2', '3', '1','1','1'],
    'id': ['0111', '0111', '0111', '0111','0123','0123'],
    'data1': ['2025-02-03 02:16:00', '2025-02-03 02:18:00', '2025-02-03 02:17:00', '2025-02-03 12:02:00','2025-02-03 12:02:00','2025-02-03 12:02:00'],
    'data2': ['2025-02-03 02:44:00', '2025-02-03 02:44:00', '2025-02-03 02:39:00', '2025-02-03 12:05:00','2025-02-03 12:06:00','2025-02-03 12:04:00']
}
df = pd.DataFrame(data) 
</code></pre>
<p>I want to identify the overlapping date ranges within the entire group of code IDs and return the common date ranges.(ex: for '0111' common range of date between codes 1,2,3 not just 2,3 or 1,2)
the result that i want is :</p>
<pre><code>result = {
    'id' :['0111'],
    'data1': ['2025-02-03 02:18:00'],
    'data2': ['2025-02-03 02:39:00']
</code></pre>
","-1","Question"
"79531444","","<p>There are two dataframes: <code>current</code> and <code>base</code>. <code>current</code> is an incremental update to <code>base</code>.</p>
<p>Whenever there is an update made to <code>current</code>, we want to run checks to see if it adheres to rules. If it does, <code>current</code> becomes <code>base</code>.</p>
<p>Updates to column <code>Issr</code> is not expected except empty value. So, there are two checks:</p>
<ul>
<li>New entry must not add any value to <code>Issr</code> column (<code>''</code> is accepted)</li>
<li>Existing row's value for column <code>Issr</code> shouldn't be updated (if updated to empty, it is fine). Update to any other column is ok.</li>
</ul>
<p><code>base</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>SNo</th>
<th>Rank</th>
<th>Ctry</th>
<th>Cat</th>
<th>Issr</th>
<th>Ref</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>10</td>
<td>A</td>
<td>Book</td>
<td>Y</td>
<td>100</td>
</tr>
<tr>
<td>2</td>
<td>14</td>
<td>B</td>
<td>Laptop</td>
<td>C</td>
<td>101</td>
</tr>
<tr>
<td>3</td>
<td>15</td>
<td>C</td>
<td>Pen</td>
<td>J</td>
<td>102</td>
</tr>
<tr>
<td>4</td>
<td>50</td>
<td>D</td>
<td>Pen</td>
<td></td>
<td>103</td>
</tr>
</tbody>
</table></div>
<p><code>current</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>SNo</th>
<th>Rank</th>
<th>Ctry</th>
<th>Cat</th>
<th>Issr</th>
<th>Ref</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>10</td>
<td>A</td>
<td>Book1</td>
<td>Y</td>
<td>100</td>
</tr>
<tr>
<td>2 (updated)</td>
<td>14</td>
<td>B</td>
<td>Laptop</td>
<td></td>
<td>101</td>
</tr>
<tr>
<td>3</td>
<td>15</td>
<td>C</td>
<td>Pen</td>
<td>J</td>
<td>102</td>
</tr>
<tr>
<td>4 (updated)</td>
<td>50</td>
<td>D</td>
<td>Pen</td>
<td>U</td>
<td>103</td>
</tr>
<tr>
<td>5 (new entry)</td>
<td>24</td>
<td>K</td>
<td>Pencil</td>
<td>W</td>
<td>101</td>
</tr>
<tr>
<td>6 (new entry)</td>
<td>24</td>
<td>RT</td>
<td>Pencil</td>
<td></td>
<td>201</td>
</tr>
</tbody>
</table></div>
<p>Above <code>current</code> fails checks because of multiple issues:</p>
<ul>
<li>Row# 4 in <code>base</code>: Existing row's column <code>Issr</code> got updated to a non-empty values (while existing Row# 2 update is fine as it made the value empty)</li>
<li>Row# 5 is a new entry in <code>current</code> with a non-empty value. This is violation. Row#6 is also new entry but ok since the value for <code>Issr</code> is empty.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

base = {
    'Rank': [10,14,15,50],
    'Ctry': ['A', 'B', 'C', 'D'],
    'Cat': ['Book', 'Laptop', 'Pen', 'Pen'],
    'Issr': ['Y', 'C', 'J', ''],
    'Ref': ['100', '101', '102', '103']
}

current = {
    'Rank': [10,14,15,50, 24, 24],
    'Ctry': ['A', 'B', 'C', 'D', 'K', 'RT'],
    'Cat': ['Book', 'Laptop', 'Pen', 'Pen', 'Pencil', 'Pencil'],
    'Issr': ['Y', '', 'J', 'U', 'W', ''],
    'Ref': ['100', '101', '102', '103', '101', '201']
}

base_df = pd.DataFrame(base)
current_df = pd.DataFrame(current)


merged_df = pd.merge(current_df, base_df, how=&quot;outer&quot;, indicator=True)

print(merged_df)

Rank Ctry     Cat   Issr    Ref      _merge
10    A      Book    Y      100        both
14    B     Laptop          101   left_only     
15    C     Pen     J       102        both 
50    D     Pen     U       103   left_only   &lt;--- how to know this got marked `left_only` as value of `issr` col is different 
24    K     Pencil  W       101   left_only   &lt;--- Invalid - new entry has no-empty value for `Issr` col
24    RT    Pencil          201   left_only
14    B     Laptop  C       101  right_only
50    D     Pen             103  right_only


</code></pre>
<p>I can get <code>left_only</code> (indicating update/new rows in <code>current</code> df) but how to know because of which column/columns the row got marked as <code>left_only</code>?</p>
<p>If I get to know that pandas marked <code>left_only</code> as it saw a diff in <code>Issr</code> column, I can just check its value (empty or not), and pass/fail the job.</p>
<p>How to get this column info?</p>
","1","Question"
"79531484","","<p>I have a very simple for loop which was working last week but when I run it this week on exactly the same data it is not working... when I run the code outside of the loop it works without an error. I don't understand...</p>
<p>The code is:
'''</p>
<pre><code>survey = pd.read_csv(folderin + 'survey.csv')
CSsurvey = pd.read_csv(folderin + 'CS_survey.csv')

sports = ['Value MTB related (e.g. inner tubes, water bottles etc)',
'Value Running','Value Roaming and other outdoor related (e.g. climbing, kayaking)',
'Value Outdoor sports event related (e.g.race)']

sport = []
for p in sports:
    item = survey[p].sum()
    CSitem = CSsurvey[p].sum()
    total = item + CSitem
    sport.append(total)'''
</code></pre>
<p>I get the error:</p>
<p><a href=""https://i.sstatic.net/VMV0lIth.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VMV0lIth.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/59CFNVHO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/59CFNVHO.png"" alt=""enter image description here"" /></a></p>
<p>If I run the code for each 'p' outside of the loop I don't get an error</p>
","0","Question"
"79531715","","<p>When I use <code>pd.read_html()</code> to load a table into a pandas dataframe and then run <code>print(dataframe)</code> the output starts with <code>&quot;Empty DataFrame&quot;</code>.</p>
<p>Also, I'm having trouble accessing the elements of the dataframe.</p>
<p>Here is the html:</p>
<pre><code>&lt;HTML&gt;
&lt;HEAD&gt;

&lt;TITLE&gt;System Information 10.8.129.1&lt;/TITLE&gt;

&lt;/HEAD&gt;

&lt;BODY&gt;


&lt;br&gt;&lt;H4&gt;Port Statistics&lt;/H4&gt;



&lt;table BORDER COLS=5 WIDTH=&quot;80%&quot;&gt;

&lt;tr&gt;&lt;th&gt;Slot/Port&lt;/th&gt;&lt;th&gt;Intf.&lt;/th&gt;&lt;th&gt;TX Frm.&lt;/th&gt;&lt;th&gt;TX Oct.&lt;/th&gt;&lt;th&gt;RX Frm.&lt;/th&gt;&lt;th&gt;RX Oct.&lt;/th&gt;&lt;th&gt;RX BC&lt;/th&gt;&lt;th&gt;RX MC&lt;/th&gt;&lt;th&gt;CRC Align.&lt;/th&gt;&lt;th&gt;Unders.&lt;/th&gt;&lt;th&gt;Overs.&lt;/th&gt;&lt;th&gt;Frag.&lt;/th&gt;&lt;th&gt;Jabbers&lt;/th&gt;&lt;th&gt;Total Coll.&lt;/th&gt;&lt;th&gt;Late Coll.&lt;/th&gt;&lt;tr&gt;&lt;th&gt;1/1&lt;/th&gt;&lt;th&gt;1&lt;/th&gt;&lt;th&gt;66967821&lt;/th&gt;&lt;th&gt;3429650783&lt;/th&gt;&lt;th&gt;96815811&lt;/th&gt;&lt;th&gt;2328791105&lt;/th&gt;&lt;th&gt;39571627&lt;/th&gt;&lt;th&gt;3960333&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;th&gt;1/2&lt;/th&gt;&lt;th&gt;2&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;th&gt;2/1&lt;/th&gt;&lt;th&gt;9&lt;/th&gt;&lt;th&gt;17533&lt;/th&gt;&lt;th&gt;1899146&lt;/th&gt;&lt;th&gt;13646&lt;/th&gt;&lt;th&gt;1821221&lt;/th&gt;&lt;th&gt;416&lt;/th&gt;&lt;th&gt;34&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;th&gt;2/2&lt;/th&gt;&lt;th&gt;10&lt;/th&gt;&lt;th&gt;12941919&lt;/th&gt;&lt;th&gt;1909511968&lt;/th&gt;&lt;th&gt;3896084&lt;/th&gt;&lt;th&gt;1687222693&lt;/th&gt;&lt;th&gt;415948&lt;/th&gt;&lt;th&gt;78&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;th&gt;0&lt;/th&gt;&lt;/tr&gt;



&lt;/table&gt;


&lt;/BODY&gt;

&lt;/HTML&gt;
</code></pre>
<p>I try to read extract the dataframe by running:</p>
<pre><code>import os
import pandas as pd

url='C:\\WebFiles\switch_stats.html'

dataframe = pd.read_html(url)

print(dataframe)
</code></pre>
<p>The output is:</p>
<pre><code>[Empty DataFrame
Columns: [(Slot/Port, 1/1, 1/2, 2/1, 2/2), (Intf., 1, 2, 9, 10), (TX Frm., 66967821, 0, 17533, 12941919), (TX Oct., 3429650783, 0, 1899146, 1909511968), (RX Frm., 96815811, 0, 13646, 3896084), (RX Oct., 2328791105, 0, 1821221, 1687222693), (RX BC, 39571627, 0, 416, 415948), (RX MC, 3960333, 0, 34, 78), (CRC Align., 0, 0, 0, 0), (Unders., 0, 0, 0, 0), (Overs., 0, 0, 0, 0), (Frag., 0, 0, 0, 0), (Jabbers, 0, 0, 0, 0), (Total Coll., 0, 0, 0, 0), (Late Coll., 0, 0, 0, 0)]
Index: []]
</code></pre>
","0","Question"
"79532217","","<p>Say I have three scaler components (Vx, Vy, Vz) of some value V, as well as the 2D location coordinates where these values exist (x, y). This is arranged in a pandas dataframe (df) such that the data is arranged as:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Vx</th>
<th>Vy</th>
<th>Vz</th>
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>30</td>
<td>20</td>
<td>10</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>50</td>
<td>70</td>
<td>40</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>Here x and y have the same range [0:5], and there exists value for Vx,y,z at every coordinate.</p>
<p>I want to compute the gradient of my values. How do I go about doing this? Would I need to put the values in a meshgrid? Do I need a timeseries?</p>
","0","Question"
"79532506","","<p>I have a dataframe with a column multi-index, <code>df1</code>, with a datetime index and 2 levels: level 0, called Capitals, has columns A, B, C, and level 1, called Smalls, has columns a, b, c, d, e.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Capitals</th>
<th>A</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>B</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>C</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Smalls</td>
<td>a</td>
<td>b</td>
<td>c</td>
<td>d</td>
<td>e</td>
<td>a</td>
<td>b</td>
<td>c</td>
<td>d</td>
<td>e</td>
<td>a</td>
<td>b</td>
<td>c</td>
<td>d</td>
<td>e</td>
</tr>
<tr>
<td>Date</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-01-25</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-02-25</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-03-25</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-04-25</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I have a second dataframe, <code>df2</code>, with the same datetime index and three columns, X, Y and Z.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>X</th>
<th>Y</th>
<th>Z</th>
</tr>
</thead>
<tbody>
<tr>
<td>Date</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-01-25</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-02-25</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-03-25</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-04-25</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Is there a way to:</p>
<p>i) multiply B of df1 by Z of df2 (Ba * Z, Bb * Z, Bc * Z, Bd * Z Be * Z) and</p>
<p>ii) add the 5 new (Smalls: a, b, c, d ,e) columns to a new Capitals column called D in <code>df1</code>?</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Capitals</th>
<th>A</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>B</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>C</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>D</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Smalls</td>
<td>a</td>
<td>b</td>
<td>c</td>
<td>d</td>
<td>e</td>
<td>a</td>
<td>b</td>
<td>c</td>
<td>d</td>
<td>e</td>
<td>a</td>
<td>b</td>
<td>c</td>
<td>d</td>
<td>e</td>
<td>a</td>
<td>b</td>
<td>c</td>
<td>d</td>
<td>e</td>
</tr>
<tr>
<td>Date</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-01-25</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-02-25</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-03-25</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>01-04-25</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>The method I'm using first creates an empty multi-index data frame with a similar structure to df1, with the new columns I want to add, which is connected to the original multi-index data frame.</p>
<p>Then it iterates through the level 1 values of B, multiplying by the value of Z in the second df.</p>
<pre><code># Extract level 1 tickers from df1.columns
smalls = df1.columns.get_level_values(1).unique()

# Create new MultiIndex for the empty columns
new_columns = pd.MultiIndex.from_product(['D', smalls],names=df1.columns.names)

# Create an empty DataFrame with the new columns
empty_df = pd.DataFrame(0, index=df1.index, columns=new_columns)

# Concatenate with the original DataFrame
df1 = pd.concat([df1, empty_df], axis=1)

# Multiply dfs and populate D
for small in smalls:
    df1[('D', small)] = df1[('B', small)] / df2['Z']
</code></pre>
<p>Is there a more streamlined way to do this, using vectors rather than iterating?</p>
","2","Question"
"79532783","","<p>I have a dataframe with a column called &quot;education&quot; which has over 100 unique values. I want to create a new column and categorize &quot;education&quot; into four levels: high school, undergrad, masters, and doctorate.</p>
<p>The following example is oversimplified but this is what I <strong>don't</strong> want to do:</p>
<pre><code>df['level'] = df['education'].map({'grade 9' : 'high school', 
                                   'grade 10' : 'high school', 
                                   'grade 11' : 'high school', 
                                   'BS' : 'undergrad', 
                                   'B.Com' : 'undergrad', 
                                   'BA' : 'undergrad', 
                                   'MBA' : 'masters', 
                                   'PhD' : 'doctorate'
                                  })
</code></pre>
<p>The above worked well for another column but I find it too tedious in this instance. Is there a way that I could use an if-else statement with the following lists (or is there a better way)? Although suggested in a similar question, it would not be sufficient to use .startswith() because a bachelor's won't always start with &quot;B&quot;, a master's won't always start with &quot;M&quot;, etc.</p>
<pre><code>high_school = ['grade 9', 'grade 10', 'grade 11', 'grade 12']
undergrad = ['BS', 'B.Com', 'B.Ed', 'BA', 'LLB']
masters = ['MA', 'MBA', 'M.Ed', 'MPA', 'LLM', 'MTech', 'M.Tech']
doctorate = ['PhD', 'MD', 'DO']

</code></pre>
<p>This is an example of my desired outcome:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>education</th>
<th>level</th>
</tr>
</thead>
<tbody>
<tr>
<td>BS</td>
<td>undergrad</td>
</tr>
<tr>
<td>LLM</td>
<td>masters</td>
</tr>
<tr>
<td>DO</td>
<td>doctorate</td>
</tr>
<tr>
<td>5.47</td>
<td>NaN</td>
</tr>
<tr>
<td>grade 9</td>
<td>high school</td>
</tr>
<tr>
<td>MBA</td>
<td>masters</td>
</tr>
<tr>
<td>John</td>
<td>NaN</td>
</tr>
</tbody>
</table></div>
<p>Thank you for your help in advance.</p>
","1","Question"
"79532998","","<p>I have two Dataframes that look like this:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({'PERSONALNUMMER': {4756: '0209740',4820: '0234212',4855: '0251297',4750: '0209326',4992: '4000404'},
 'MANDANT': {4756: 'OM', 4820: 'OM', 4855: 'OM', 4750: 'OM', 4992: 'OM'},
 'Fachabteilung': {4756: 'HA2300',4820: 'HA2300',4855: 'HA2300',4750: 'HA2300',4992: 'HA2300'},
 'FACHEXPERTISE': {4756: 'AQ10',4820: 'AQ10',4855: 'AQ10',4750: 'AQ10',4992: 'AQ10'},
 'Leistungsgruppe': {4756: pd.NA,4820: 'Endoprothetik Knie',4855: 'Allgemeine Chirurgie',4750: 'Wirbelsaeuleneingriffe',4992: pd.NA}})


MAP = pd.DataFrame({'MANDANT': {238: 'OM', 239: 'OM', 240: 'OM', 241: 'OM'},
 'Fachabteilung': {238: 'HA2300', 239: 'HA2300', 240: 'HA2300', 241: 'HA2300'},
 'FACHEXPERTISE': {238: 'AQ10', 239: 'AQ10', 240: 'AQ10', 241: 'AQ10'},
 'Leistungsgruppe': {238: 'Allgemeine Chirurgie',
  239: 'Endoprothetik Huefte',240: 'Endoprothetik Knie',241: 'Revision Huefte'},
 'VK': {238: 3,239: 14,240: 28,241: 22}})
</code></pre>
<p>I want to do the following:
I have empty entries in my df Dataframe in column <code>Leistungsgruppe</code> and I need to pick <em>random</em> elements out of my MAP Dataframe based on conditions.</p>
<p>So, I want to iterate over <code>df</code> and pick element(s)</p>
<p>The conditions are like: the <code>MANDANT</code>, <code>Fachabteilung</code>, and <code>FACHEXPERTISE</code> need to be the same, and if there are more than 3 entries for that element in my MAP Dataframe, I need to narrow it down to 3 elements.</p>
<p>Right now I have no clue how to do this because my Python knowledge when it comes to iterating on single rows is almost non-existent.</p>
<p>I tried to think of a way to do this with <code>iterrows</code> but I can not come up with a quick solution right now.</p>
","0","Question"
"79534147","","<p>I have a column with names of songs. I would like to create a new column in the data frame called Song Category based on words contained in the string of a column called Name.</p>
<p>If the name contains words like &quot;Evil&quot;, &quot;Night&quot;, &quot;Problem&quot;, &quot;Sorrow&quot;, &quot;Dead&quot;, &quot;Curse&quot; then the category should be negative
If the name contains words like &quot;Amazing&quot;, &quot;Angel&quot;, &quot;Perfect&quot;, &quot;Sunshine&quot;, &quot;Home&quot;, &quot;Live&quot;, &quot;Friends&quot; then the category should be negative
Else the category in the new columns should be neither.</p>
<p>I have tried the below but I get negative for all rows in in the new column.</p>
<pre class=""lang-none prettyprint-override""><code>    if [df_song['Name'].isin([&quot;Evil&quot;, &quot;Night&quot;, &quot;Problem&quot;, &quot;Sorrow&quot;, &quot;Dead&quot;, &quot;Curse&quot;, &quot;Venom&quot;,&quot;Pain&quot;, &quot;Lonely&quot;, &quot;Beast&quot;])]:  
        category.append('Negative') 
    elif [df_song['Name'].isin([&quot;Amazing&quot;, &quot;Angel&quot;, &quot;Perfect&quot;, &quot;Sunshine&quot;, &quot;Home&quot;, &quot;Live&quot;, &quot;Friends&quot;])]: 
        category.append(&quot;Positive&quot;) 
    else: 
        category.append(&quot;Neither&quot;)

df_song['Song Category'] = category 
</code></pre>
","0","Question"
"79534293","","<p>I have a dataframe of 40000 row and 64 columns. The columns are indexed with a 2 level index (16 labels in level 0 and 4 in level 1), while the rows have a simple index.
I want the columns labels of level 0 to be transformed into row labels of level 0.</p>
<p>I have tried the <code>.stack</code> method but it transforms columns labels of level 0 into row labels of level 1, which is not what I want.</p>
<p>Below the minimal reproducible example.</p>
<pre><code>import pandas as pd
import numpy as np

# This is the intial dataframe.
df_init = pd.DataFrame( np.linspace(0, 89, 90, dtype='int').reshape(10, 9), columns=pd.MultiIndex.from_product([['A', 'B', 'C'], ['a', 'b', 'c']]))

# This is an intermediate transformation.
df1 = df_init.stack(level=0)

# &quot;df_final&quot; is what I want.
df_final = pd.concat([df1.xs(key='A', level=1), df1.xs(key='B', level=1), df1.xs(key='C', level=1)], keys=['A', 'B', 'C'])

</code></pre>
<p>Is there any pandas idiom to have the desired result without using <code>.xs</code> which, in this case, is inconvenient?</p>
","0","Question"
"79534705","","<p>I have:</p>
<pre><code>keys = [&quot;panda1&quot;, &quot;panda2&quot;, &quot;panda3&quot;]
values = [[&quot;eats&quot;, &quot;shoots&quot;], [&quot;shoots&quot;, &quot;leaves&quot;], [&quot;eats&quot;, &quot;leaves&quot;]]
df = pd.DataFrame({&quot;keys&quot;: keys, &quot;values&quot;: values})

     keys      values
0   panda1  [eats, shoots]
1   panda2  [shoots, leaves]
2   panda3  [eats, leaves]
</code></pre>
<p>Then I exploded the values in the values col:</p>
<pre><code>df1 = df.explode(olumn = 'values')

     keys   values
0   panda1  eats
0   panda1  shoots
1   panda2  shoots
1   panda2  leaves
2   panda3  eats
2   panda3  leaves
</code></pre>
<p>My question is how to get this output directly please:</p>
<pre><code>  panda1  panda2  panda3
0   eats   shoots   eats
1  shoots  leaves  leaves
</code></pre>
<ul>
<li>Can I get this directly in one step from the original df before explode()? if not</li>
<li>Is there a chance I can do this in 1 step with the explode function or no please?</li>
</ul>
<p>Thank you all in advance :-)</p>
","1","Question"
"79534710","","<p>I have code as below to write a json from the below dictionary
Looks like there is issue in my group by or aggregate.
It is not generating the JSON as expected as give below.</p>
<pre><code>import json

data = { &quot;some_id&quot;: &quot;123456&quot;,
 &quot;some_email&quot;: &quot;xyz.abc@acacceptance.com&quot;,
 &quot;some_number&quot; : 123456}
df = pd.DataFrame(data, index=[0])
#print(df)


for idx, row in df.iterrows():
  # convert each record to a dictionary
  record_dict = row.to_dict()
  json_data = json.dumps(record_dict, indent=1)
  #print(json_data)
  
phone_number = { &quot;some_number&quot; : 123456, &quot;Contact&quot; :{&quot;phone_number&quot;: 45464464646,&quot;contact?&quot;: &quot;Y&quot;}}
df_phonnumber = pd.DataFrame(phone_number)
#print(df_phonnumber)

merged_df = pd.merge(df, df_phonnumber, left_on='some_number', right_on='some_number', how='left')
#print(merged_df)

#single columns def_size and dep_name
d = (merged_df.groupby(['some_number','some_email','some_id']).apply(lambda x: x[['Contact']]
      .to_dict('r'))
      .reset_index(name='dont_contact'))

json_str = d.to_json(orient='records')

pretty_json = json.dumps(json.loads(json_str), indent=4)
print(pretty_json)
</code></pre>
<p>Expecting the JSON as below</p>
<pre><code>{
        &quot;some_number&quot;: 123456,
        &quot;some_email&quot;: &quot;xyz.abc@acacceptance.com&quot;,
        &quot;some_id&quot;: &quot;123456&quot;,
        &quot;dont_contact&quot;: [
            &quot;Phone_Number&quot;: &quot;45464464646&quot;,
            &quot;Contact?&quot;: &quot;Y&quot;
            ]
}
</code></pre>
<p>But I am getting the Json as below. What is missing?</p>
<pre><code>{
        &quot;some_number&quot;: 123456,
        &quot;some_email&quot;: &quot;xyz.abc@acacceptance.com&quot;,
        &quot;some_id&quot;: &quot;123456&quot;,
        &quot;dont_contact&quot;: [
            {
                &quot;Contact&quot;: &quot;Y&quot;
            },
            {
                &quot;Contact&quot;: 45464464646
            }
        ]
    }
</code></pre>
<p>can some please help on this to get the json in the above format?</p>
","1","Question"
"79535482","","<p>What is the meaning of below line. I'm especially confused about how [:,0:6] is working?</p>
<pre><code>X = pd.get_dummies(df.iloc[:,0:6])
</code></pre>
<p><em>Above code is transforming categorical data into a format that can be provided to machine learning algorithms.</em></p>
","0","Question"
"79536363","","<p>When I do an operation in STATA, for example removing duplicated rows, it will tell me the number of rows removed, for instance:</p>
<pre><code>. sysuse auto.dta
(1978 automobile data)

. drop if mpg&lt;15
(8 observations deleted)

. drop if rep78==.
(4 observations deleted)
</code></pre>
<p>For the tidyverse, the package <a href=""https://github.com/elbersb/tidylog"" rel=""nofollow noreferrer"">tidylog</a> implements a similar feature, providing feedback on the operation (e.g. for a join, number of joined and unjoined rows, for a filter, nubmer of removed rows, etc.), with the little disadvantage that you will lose the autocompletion of your editor, as it wraps tidyverse functions with definitions like <code>filter(...)</code>, to accomodate for the fact that the tidyverse upstream definition could change over time.</p>
<p>Is there something similar for pandas?</p>
<p>I found <a href=""https://github.com/eyaltrabelsi/pandas-log"" rel=""nofollow noreferrer"">pandas-log</a> but seems abandoned.</p>
<p><a href=""https://stackoverflow.com/questions/71729310/how-to-print-the-number-of-observations-dropped-by-tidyverses-functions-like-fi"">Related question for R</a>.</p>
","3","Question"
"79537356","","<p>I have a dataframe with weekly product sales</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>product_id</th>
<th>week_number</th>
<th>sales</th>
</tr>
</thead>
<tbody>
<tr>
<td>A1</td>
<td>1</td>
<td>1000</td>
</tr>
<tr>
<td>A1</td>
<td>2</td>
<td>2000</td>
</tr>
<tr>
<td>A1</td>
<td>3</td>
<td>3000</td>
</tr>
<tr>
<td>A2</td>
<td>1</td>
<td>8000</td>
</tr>
<tr>
<td>A2</td>
<td>2</td>
<td>4000</td>
</tr>
<tr>
<td>A2</td>
<td>3</td>
<td>2000</td>
</tr>
</tbody>
</table></div>
<p>I want to add a column that identifies rows where the total sales were the highest for the given product:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>product_id</th>
<th>week_number</th>
<th>sales</th>
<th>product_max</th>
</tr>
</thead>
<tbody>
<tr>
<td>A1</td>
<td>1</td>
<td>1000</td>
<td>FALSE</td>
</tr>
<tr>
<td>A1</td>
<td>2</td>
<td>2000</td>
<td>FALSE</td>
</tr>
<tr>
<td>A1</td>
<td>3</td>
<td>3000</td>
<td>TRUE</td>
</tr>
<tr>
<td>A2</td>
<td>1</td>
<td>8000</td>
<td>TRUE</td>
</tr>
<tr>
<td>A2</td>
<td>2</td>
<td>4000</td>
<td>FALSE</td>
</tr>
<tr>
<td>A2</td>
<td>3</td>
<td>2000</td>
<td>FALSE</td>
</tr>
</tbody>
</table></div>
<p>Since A1 had its highest sales on week 3, that row is tagged as True. But week 3 wasn't the highest for A2. (And so for A2, week 1 is tagged as True.)</p>
<p>I know I could write a loop to do this and cycle through each of the product IDs one by one, but I am wondering if there is a way to do this with a different function - possibly Pandas Groupby?</p>
<p>Thank you!</p>
","2","Question"
"79537565","","<p>For example, given regex</p>
<pre><code>r&quot;(?P&lt;D&gt;.8)(?P&lt;foo&gt;\d+)(?P&lt;bar&gt;[a-z]+)&quot;
</code></pre>
<p>I want to return the dictionary</p>
<pre><code>{'D': r'.8', 'foo': r'\d+', 'bar': r'[a-z]+'}
</code></pre>
<p>that maps group name to group regex. Getting the groups is straightforward (with groupindex), but there doesn't seem to be a direct way to get the corresponding regexes.</p>
","3","Question"
"79537934","","<p>My aim is to implement a class that inherits from <code>pd.Series</code> class and acts as a container object. This object will hold varied objects as its elements in the form of</p>
<pre><code>container = Container(
   a = 12,
   b = [12, 10, 20],
   c = 'string',
   name='this value',
)
</code></pre>
<p>I will be accessing these elements via a dot notation:</p>
<pre><code>print(container.a)     # Output: 12
print(container.b)     # Output: [12, 10, 20]
print(container.c)     # Output: string
print(container.name)  # Output: None
</code></pre>
<p>I have tried the following implementation:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

class Container(pd.Series):
    def __init__(self, **kwargs):
        super().__init__(data=kwargs)
</code></pre>
<p>This works to a large extent.</p>
<p>However, there would be special cases. If <code>index</code> of one of the elements in the container is also a property in <code>pd.Series()</code> then the property would be returned. In the case of above example, <code>container.name</code> would return <code>None</code> value as it returns the pd.Series().name property of the series. I want container.name to return <code>'this value'</code>.</p>
<p>So, maybe I need to overerite the <code>__getattribute__</code> or <code>__getattr__</code> to make this work.</p>
<p>I have tried the following:</p>
<pre><code>def __getattr__(self, item):
    if item in self:
        return self[item]
    else:
        raise AttributeError(f&quot;'{self.__class__.__name__}' object has no attribute '{item}'&quot;)

def __getattribute__(self, item):
    try:
        # Try to get an item using dot notation when it's available in the series itself
        value = super().__getattribute__('__getitem__')(item)
        if item in self:
            return value
    except (KeyError, AttributeError):
        pass
    # Fallback to the default __getattribute__
    return super().__getattribute__(item)
</code></pre>
<p>However, this would always return <code>RecursionError: maximum recursion depth exceeded</code>. I have tried different ways.</p>
<p>Note that I like to inherit from <code>pd.Series()</code> to have access to all other functionalities of the series.</p>
","0","Question"
"79538310","","<p>I have a Python script, that basically looks like this:</p>
<pre><code>import mypackage

# this function generates always the same pandas.DataFrame
df = mypackage.create_the_dataframe()

# write the DataFrame to xlsx and csv
df.to_excel(&quot;the_dataframe_as.xlsx&quot;, index=False, engine=&quot;openpyxl&quot;)
df.to_csv(&quot;the_dataframe_as.csv&quot;, index=False)
</code></pre>
<p>I was trying to write a test for the <code>create_the_dataframe</code> function. So I checked the hash of the resulting xlsx and csv files and found that for two different runs of the script, the hash and file size of the resulting xlsx file changes. The hash for the csv remains the same.</p>
<p>Although I can live with this, I am very curious to understand why this is the case?</p>
","0","Question"
"79538323","","<p>I'm unable to send PySpark data frame as an attachment in Excel.</p>
<p>I'm able to do easily with CSV file using below,</p>
<pre><code>email.add_attachment(df.toPandas().to_csv(index=False).encode('utf-8')
, maintype='application', subtype='csv', filename=file)
</code></pre>
<p>Unable do the same using excel,</p>
<pre><code>email.add_attachment(df.toPandas().to_excel(file, index=False, sheet_name='FileReport'))
</code></pre>
<p>Any suggestion would be appreciated.
Below is the code</p>
<pre><code>from datetime import datetime, date 
import pandas as pd 
from pyspark.sql import Row 
from pyspark.sql import SparkSession 
spark = SparkSession.builder.getOrCreate() 
from email.message import EmailMessage

email = EmailMessage()
email['From'] = &quot;dkfhskg&quot;
email['To'] = &quot;dsjagjsg&quot;
email['Subject'] = &quot;Report&quot;

mail = 'Report'

email.set_content(mail , subtype='html')

_smtp_server = &quot;kdfhsajgfjdsg&quot;
_smtp_port = 10

smtp = smtplib.SMTP(_smtp_server, _smtp_port)

df = spark.createDataFrame([ 
    Row(Sno=1, Rank=4., RollNo='1000', Date=date(2000, 8, 1), 
        e=datetime(2000, 8, 1, 12, 0)), 
    Row(Sno=2, Rank=8., RollNo='1001', Date=date(2000, 6, 2),  
        e=datetime(2000, 6, 2, 12, 0)), 
    Row(Sno=4, Rank=5., RollNo='1002', Date=date(2000, 5, 3), 
        e=datetime(2000, 5, 3, 12, 0)) 
]) 


file = 'Report_' + str(datetime.now()) + '.xlsx'

email.add_attachment(df.toPandas().to_excel(file, index=False, sheet_name='FileReport'))
</code></pre>
","0","Question"
"79539480","","<p>Minimum working example:</p>
<pre><code>from datetime import time
import pandas as pd

df = pd.DataFrame(index=['ABC','DEF'], data={time(9):[2,4],time(10):[6,8]})
df.to_parquet('MWE.parquet')
df1 = pd.read_parquet('MWE.parquet')
</code></pre>
<p>Error message:</p>
<pre class=""lang-none prettyprint-override""><code>TypeError: data type 'time' not understood
</code></pre>
<p>Is there a simple workaround for this?</p>
","1","Question"
"79540085","","<p>I have a data frame with products and their variances that I want to plot as a histogram:</p>
<pre><code>import pandas as pd
data = {'product_name': ['link', 'zelda', 'impa', 'rauru', 'saria', 'darunia'], 'variance': [0.95, -0.85, 0.3, 0.2, 0.03, 0.02]}
df = pd.DataFrame(data)
</code></pre>
<p>I can use plotly to create a histogram:</p>
<pre><code>px.histogram(df, 'variance')
</code></pre>
<p><a href=""https://i.sstatic.net/LiL4BSdr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LiL4BSdr.png"" alt=""enter image description here"" /></a></p>
<p>But I want to create five bins, with custom start and ends (like in the graph below) rather than what plotly defaults to. This seems weirdly difficult to do using plotly express - is there a way to customize the bin sizes at all?</p>
<p><a href=""https://i.sstatic.net/JzlZgF2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JzlZgF2C.png"" alt=""enter image description here"" /></a></p>
<p>Thanks</p>
","1","Question"
"79540595","","<p>Below is what I'm trying to accomplish:</p>
<ol>
<li>Get data from API</li>
<li>Clean the data</li>
<li>Insert the data into SQL Server</li>
</ol>
<p>Issue that I'm facing:</p>
<ul>
<li>I convert the data that I receive from api to a pandas dataframe</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>response = requests.get(API_SCHEME_DATA, headers=headers, data=payload, timeout=20)
if response.status_code == 200:
    data = response.json()[&quot;Data&quot;]
    df = pd.DataFrame(data, dtype=str)
</code></pre>
<ul>
<li>Now I, clean this data in a very simple step</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># bigint list contains column names that I can treat as numeric
for col in bigint_list:
    df[col] = pd.to_numeric(df[col], errors='coerce')
    df[col] = df[col].fillna(0.0)

df = df.astype(str)
df = df.where(pd.notnull(df), None)   
</code></pre>
<ul>
<li>I then use pyodbc connection to insert dataframe into SQL Server</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># Prepare columns and placeholders for the SQL query
columns = [f&quot;[{col}]&quot; for col in df.columns]
placeholders = ', '.join(['?' for _ in range(len(df.columns))])

# SQL Insert query
insert_data_query = f&quot;&quot;&quot;
    INSERT INTO {table_name} ({', '.join(columns)})
    VALUES ({placeholders})
&quot;&quot;&quot;
            
# Convert the dataframe rows to a list of tuples for insertion
rows = [tuple(x) for x in df.replace({None: None}).values]
# Execute the insert query for multiple rows
connection_cursor.executemany(insert_data_query, rows)
connection_cursor.connection.commit()
</code></pre>
<ul>
<li>While inserting I get the below error:</li>
</ul>
<pre><code>pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]Error converting data type nvarchar to numeric. (8114) (SQLExecDirectW)')
</code></pre>
<ul>
<li>I believe, I have narrowed down the culprit to a parameter coming in the api</li>
</ul>
<pre><code>&quot;CurrentValue&quot;: &quot;0.00002&quot;
</code></pre>
<ul>
<li>While inserting this parameter, the parameter value has been changed to scientific notation, and I can see the value being changed to <code>'2e-05'</code>. The column that I'm inserting in the SQL server is of <code>DECIMAL</code> type. And I believe this is why I'm getting the error.</li>
</ul>
<p>What I have done so far to resolve:</p>
<ul>
<li>I have tried to suppress the scientific notation by:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>pd.options.display.float_format = '{:.8f}'.format
</code></pre>
<p>and also tried to round off the columns like below during cleaning step:</p>
<pre class=""lang-py prettyprint-override""><code># bigint list contains column names that I can treat as numeric
for col in bigint_list:
    df[col] = pd.to_numeric(df[col], errors='coerce')
    df[col] = df[col].fillna(0.0)
    df[col] = df[col].round(10)

df = df.astype(str)
df = df.where(pd.notnull(df), None) 
</code></pre>
<p>However, nothing seems to work and I'm still seeing the value converted to scientific notation. Any help would be appreciated.</p>
","0","Question"
"79541633","","<p>I want to add aliases to consecutive occurrences of the same gene name in column <code>gene_id</code>. If the <code>gene_id</code> value is unique, it should be unchanged.</p>
<p>Here is my example input:</p>
<pre><code>df_genes_data = {'gene_id': ['g0', 'g1', 'g1', 'g2', 'g3', 'g4', 'g4', 'g4']}
df_genes = pd.DataFrame.from_dict(df_genes_data)
print(df_genes.to_string())

  gene_id
0      g0
1      g1
2      g1
3      g2
4      g3
5      g4
6      g4
7      g4
</code></pre>
<p>and there is the desired output:</p>
<pre><code>  gene_id
0      g0
1  g1_TE1
2  g1_TE2
3      g2
4      g3
5  g4_TE1
6  g4_TE2
7  g4_TE3
</code></pre>
<p>Any ideas on how to perform it? I've been looking for solutions but found only ways to count consecutive occurrences, not to label them with aliases.</p>
<p>EDIT:</p>
<p>I've tried to find <code>gene_id</code> values which occur more than once in my data:</p>
<pre><code>rep = []
gene_list = df_genes['gene_id']
for idx in range(0, len(gene_list) - 1):
    if gene_list[idx] == gene_list[idx + 1]:
        rep.append(gene_list[idx])
rep = list(set(rep))
print(&quot;Consecutive identical gene names are : &quot; + str(rep))
</code></pre>
<p>but I have no idea how to add desired aliases to them.</p>
","3","Question"
"79542528","","<p>I'm using the <a href=""https://pandas.pydata.org/docs/reference/index.html"" rel=""nofollow noreferrer"">pandas library</a> to convert CSVs to other data types. My CSV has the fields quoted, like this:</p>
<pre class=""lang-none prettyprint-override""><code>&quot;Version&quot;, &quot;Date&quot;, &quot;Code&quot;, &quot;Description&quot;, &quot;Tracking Number&quot;, &quot;Author&quot;
&quot;0.1&quot;, &quot;22AUG2022&quot;, , &quot;Initial Draft&quot;, &quot;NR&quot;, &quot;Sarah Marshall&quot;
&quot;0.2&quot;, &quot;23SEP2022&quot;, &quot;D&quot;, &quot;Update 1&quot;, &quot;N-1234&quot;, &quot;Bill Walter&quot;
&quot;0.3&quot;, &quot;09MAY2023&quot;, &quot;A\, C&quot;, &quot;New update.&quot;, &quot;N-1235&quot;, &quot;George Orwell&quot;
</code></pre>
<p>The problem is that when I read the CSV with <code>pandas.read_csv('myfile.csv')</code>, the quotes are included in the values:</p>
<pre><code>   Version        &quot;Date&quot;   &quot;Code&quot;     &quot;Description&quot;  &quot;Tracking Number&quot;           &quot;Author&quot;
0      0.1   &quot;22AUG2022&quot;            &quot;Initial Draft&quot;               &quot;NR&quot;   &quot;Sarah Marshall&quot;
1      0.2   &quot;23SEP2022&quot;      &quot;D&quot;        &quot;Update 1&quot;           &quot;N-1234&quot;      &quot;Bill Walter&quot;
2      0.3   &quot;09MAY2023&quot;   &quot;A, C&quot;     &quot;New update.&quot;           &quot;N-1235&quot;    &quot;George Orwell&quot;
</code></pre>
<p>So these quotes are included when converting to HTML:</p>
<pre class=""lang-html prettyprint-override""><code>&lt;table&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Version&lt;/th&gt;
      &lt;th&gt;&quot;Date&quot;&lt;/th&gt;
      &lt;th&gt;&quot;Code&quot;&lt;/th&gt;
      &lt;th&gt;&quot;Description&quot;&lt;/th&gt;
      &lt;th&gt;&quot;Tracking Number&quot;&lt;/th&gt;
      &lt;th&gt;&quot;Author&quot;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;&quot;22AUG2022&quot;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&quot;Initial Draft&quot;&lt;/td&gt;
      &lt;td&gt;&quot;NR&quot;&lt;/td&gt;
      &lt;td&gt;&quot;Sarah Marshall&quot;&lt;/td&gt;
    &lt;/tr&gt;
...
</code></pre>
<p>I've tried <code>quoting=csv.QUOTE_NONE</code>, but it didn't fix it--in fact, it actually added quotes to the <code>Version</code> column.</p>
<p>I found <a href=""https://stackoverflow.com/questions/31281699/python-strip-every-double-quote-from-csv"">this question</a>--the answers essentially says to strip out any quotes in post processing.</p>
<p>I can of course loop through the CSV rows and strip out the leading/trailing quotes for each of the values, but since quoted values are common with CSV I'd expect that there would be a parameter or easy way to enable/disable the quotes in the rendered output but I can't find something like that.</p>
<p>Is there a way to accomplish this?</p>
","1","Question"
"79543011","","<p>I'm trying to assign a dataframe cell with a nested list:</p>
<pre><code>df.loc['y','A'] = [[2]]
</code></pre>
<p>However, the actual assigned value is <code>[2]</code>.</p>
<p>It works expected for <code>[2], [[[2]]], [[[[2]]]]</code>, but just not for <code>[[2]]</code></p>
<p>See the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

df = pd.DataFrame({&quot;A&quot;: [[1], [[2]], [[[3]]], [[[[4]]]], np.array([[2]]), np.array([[[2]]]), [[1],[2]]], 
                   &quot;B&quot;: [[1], [[2]], [[[3]]], [[[[4]]]], np.array([[2]]), np.array([[[2]]]), [[1],[2]]],
                   &quot;C&quot;: [1,2,3,4,5,6,7]
                   }, 
                   index=[&quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;w&quot;,&quot;a&quot;,&quot;b&quot;,&quot;c&quot;])


# initial assing works
print(df)

df.loc['x','A'] = [1] # good
df.loc['y','A'] = [[2]] # buggy, actual assigned value [2]
df.loc['z','A'] = [[[3]]] # good
df.loc['w','A'] = [[[[4]]]] #good

df.loc['a','A'] = np.array([[2]], dtype=object) # buggy, actual assign value [2]
df.loc['b','A'] = np.array([[[2]]], dtype=object) # good


#df.loc['b','A'] = [1,2] # error: Must have equal len keys and value when setting with an iterable
df.loc['c','A'] = [[1],[2]] # buggy, actual assigned value [1,2]

print(df)
</code></pre>
<p>The output:</p>
<pre class=""lang-none prettyprint-override""><code>            A           B  C
x         [1]         [1]  1
y       [[2]]       [[2]]  2
z     [[[3]]]     [[[3]]]  3
w   [[[[4]]]]   [[[[4]]]]  4
a       [[2]]       [[2]]  5
b     [[[2]]]     [[[2]]]  6
c  [[1], [2]]  [[1], [2]]  7
           A           B  C
x        [1]         [1]  1
y        [2]       [[2]]  2
z    [[[3]]]     [[[3]]]  3
w  [[[[4]]]]   [[[[4]]]]  4
a        [2]       [[2]]  5
b    [[[2]]]     [[[2]]]  6
c     [1, 2]  [[1], [2]]  7
</code></pre>
<p>What is even more strange is,
if we remove the col &quot;C&quot; , there will be no buggy, no error in all the code comments above.</p>
","1","Question"
"79543945","","<p>I am trying to down-sample a time series in Pandas from 8 seconds to 10 seconds. For the purposes of this example, I've generated fake data that linearly increases with the number of seconds, over a minute. Importantly, for this example, the time intervals of the two time series are not multiples of each other.</p>
<p>When using .resample().interpolate() in Pandas, it seems unable to interpolate for the first few points, for which there is sufficient data. How can I work around it? Here's the example:</p>
<pre><code>import numpy as np
import pandas as pd
import datetime

a = datetime.datetime(2025, 12, 2, 17, 39, 6)
interval8df = pd.DataFrame(np.linspace(60, 124, 9), columns=['Hi'], index=pd.date_range(a, periods=9, freq='8s'))
interval8df['Hi']

2025-12-02 17:39:06     60.0
2025-12-02 17:39:14     68.0
2025-12-02 17:39:22     76.0
2025-12-02 17:39:30     84.0
2025-12-02 17:39:38     92.0
2025-12-02 17:39:46    100.0
2025-12-02 17:39:54    108.0
2025-12-02 17:40:02    116.0
2025-12-02 17:40:10    124.0
Freq: 8s, Name: Hi, dtype: float64
</code></pre>
<p>When using resample interpolate, this is the result:</p>
<pre><code>interval8df.resample('10s').interpolate(method='time')['Hi']

2025-12-02 17:39:00      NaN
2025-12-02 17:39:10      NaN
2025-12-02 17:39:20      NaN
2025-12-02 17:39:30     84.0
2025-12-02 17:39:40     94.0
2025-12-02 17:39:50    104.0
2025-12-02 17:40:00    114.0
2025-12-02 17:40:10    124.0
Freq: 10s, Name: Hi, dtype: float64
</code></pre>
<p>While I can understand the first 17:39:00 going NaN, both 17:39:10 and 17:39:20 are both surrounded by points in the original time series (6 and 14 seconds, then 14 and 20 seconds respectively). Why is it occurring?</p>
<p>I've tried using mean, but that produced no NaNs.</p>
<pre><code>interval8df.resample('10s').mean()['Hi']

2025-12-02 17:39:00     60.0
2025-12-02 17:39:10     68.0
2025-12-02 17:39:20     76.0
2025-12-02 17:39:30     88.0
2025-12-02 17:39:40    100.0
2025-12-02 17:39:50    108.0
2025-12-02 17:40:00    116.0
2025-12-02 17:40:10    124.0
Freq: 10s, Name: Hi, dtype: float64
</code></pre>
<p>Additionally, changing the interpolate method does not seem to have improved the solution.</p>
<p>The workaround I've been using is up-sampling from 8 seconds to 1 second using interpolate, then down-sampling from 1 second to 10 seconds using the mean, which is obviously clunky. I would like to be able to do this directly in one step.</p>
","1","Question"
"79544212","","<p>I have a dataframe with below data.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">DateTime</th>
<th style=""text-align: center;"">Tag</th>
<th style=""text-align: right;"">Qty</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">2025-01-01 13:00</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">270</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-01-03 13:22</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">32</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-01-10 12:33</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">44</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-01-22 10:04</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">120</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-01-29 09:30</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: right;"">182</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-02-02 15:05</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">216</td>
</tr>
</tbody>
</table></div>
<p><strong>To be achieved</strong>: 2 new columns, first with cumulative sum of Qty until the DateTime on each row when Tag is not equal to 2, second with cumulative sum of Qty until the DateTime on each row when Tag is equal to 2. Below is the result I am looking for.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">DateTime</th>
<th style=""text-align: center;"">Tag</th>
<th style=""text-align: right;"">Qty</th>
<th style=""text-align: right;"">RBQ</th>
<th style=""text-align: right;"">RSQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">2025-01-01 13:00</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">270</td>
<td style=""text-align: right;"">270</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-01-03 13:22</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">32</td>
<td style=""text-align: right;"">302</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-01-10 12:33</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">44</td>
<td style=""text-align: right;"">302</td>
<td style=""text-align: right;"">44</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-01-22 10:04</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">120</td>
<td style=""text-align: right;"">302</td>
<td style=""text-align: right;"">164</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-01-29 09:30</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: right;"">182</td>
<td style=""text-align: right;"">484</td>
<td style=""text-align: right;"">164</td>
</tr>
<tr>
<td style=""text-align: left;"">2025-02-02 15:05</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">216</td>
<td style=""text-align: right;"">600</td>
<td style=""text-align: right;"">164</td>
</tr>
</tbody>
</table></div>
<p>I've been searching for a method, but doesn't seem to be getting it right. May I please get help on getting it right?</p>
<p>Thanks,</p>
","2","Question"
"79544423","","<p>I am not sure title is well describing the problem but I will explain it step by step.</p>
<p>I have a correlation matrix of genes (10k x 10k)
I convert this correlation matrix to pairwise dataframe (upper triangle) (around 100m row)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>gene1</th>
<th>gene2</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gene3450</td>
<td>Gene9123</td>
<td>0.999706</td>
</tr>
<tr>
<td>Gene5219</td>
<td>Gene9161</td>
<td>0.999691</td>
</tr>
<tr>
<td>Gene27</td>
<td>Gene6467</td>
<td>0.999646</td>
</tr>
<tr>
<td>Gene3255</td>
<td>Gene4865</td>
<td>0.999636</td>
</tr>
<tr>
<td>Gene2512</td>
<td>Gene5730</td>
<td>0.999605</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>Then I have gold-standard TERMS table around 5k rows and columns are ID and used_genes</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>used_genes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Complex 1</td>
<td>[Gene3629, Gene8048, Gene9660, Gene4180, Gene1...]</td>
</tr>
<tr>
<td>2</td>
<td>Complex 2</td>
<td>[Gene3944, Gene931, Gene3769, Gene7523, Gene61...]</td>
</tr>
<tr>
<td>3</td>
<td>Complex 3</td>
<td>[Gene8236, Gene934, Gene5902, Gene165, Gene664...]</td>
</tr>
<tr>
<td>4</td>
<td>Complex 4</td>
<td>[Gene2399, Gene2236, Gene8932, Gene6670, Gene2...]</td>
</tr>
<tr>
<td>5</td>
<td>Complex 5</td>
<td>[Gene3860, Gene5792, Gene9214, Gene7174, Gene3...]</td>
</tr>
</tbody>
</table></div>
<p>What I do:</p>
<ul>
<li><p>I iterate of each Gold-standard complex row.</p>
</li>
<li><p>Convert used_gene list to pairwise, like geneA-geneB, geneA-geneC
etc.</p>
</li>
<li><p>Check those complex-row gene pairs in the stacked correlation pairs.</p>
</li>
<li><p>If they are exist I put column TP=1, if not TP=0.</p>
</li>
<li><p>Based on the TP counts I calculate precision, recall, and area under
the curve score.</p>
</li>
</ul>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>used_genes</th>
<th>auc_score</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multisubunit ACTR coactivator complex</td>
<td>[CREBBP, KAT2B, NCOA3, EP300]</td>
<td>0.001695</td>
</tr>
<tr>
<td>Condensin I complex</td>
<td>[SMC4, NCAPH, SMC2, NCAPG, NCAPD2]</td>
<td>0.009233</td>
</tr>
<tr>
<td>BLOC-2 (biogenesis of lysosome-related organel...)</td>
<td>[HPS3, HPS5, HPS6]</td>
<td>0.000529</td>
</tr>
<tr>
<td>NCOR complex</td>
<td>[TBL1XR1, NCOR1, TBL1X, GPS2, HDAC3, CORO2A]</td>
<td>0.000839</td>
</tr>
<tr>
<td>BLOC-1 (biogenesis of lysosome-related organel...)</td>
<td>[DTNBP1, SNAPIN, BLOC1S6, BLOC1S1, BLOC1S5, BL...]</td>
<td>0.002227</td>
</tr>
</tbody>
</table></div>
<p>So, in the end, for each of gold-standard row, I have PR-AUC score.</p>
<p>I will share my function below, with 100m stacked df, and 5k terms It takes around 25 minutes, and I am trying to find a way to reduce the time.</p>
<p>PS: for the calculation of PR-AUC part, I have compiled C++ code, I just give the ordered TP numbers as a input to C++ function and return the score, still runtime is the same. I guess the problem is iteration part.</p>
<pre><code>from sklearn import metrics
def compute_per_complex_pr(corr_df, terms_df):

    pairwise_df = binary(corr_df)
    pairwise_df = quick_sort(pairwise_df).reset_index(drop=True)
    
    # Precompute a mapping from each gene to the row indices in the pairwise DataFrame where it appears.
    gene_to_pair_indices = {}
    for i, (gene_a, gene_b) in enumerate(zip(pairwise_df[&quot;gene1&quot;], pairwise_df[&quot;gene2&quot;])):
        gene_to_pair_indices.setdefault(gene_a, []).append(i)
        gene_to_pair_indices.setdefault(gene_b, []).append(i)
    
    # Initialize AUC scores (one for each complex) with NaNs.
    auc_scores = np.full(len(terms_df), np.nan)
    
    # Loop over each gene complex
    for idx, row in terms_df.iterrows():
        gene_set = set(row.used_genes)
   
        
        # Collect all row indices in the pairwise data where either gene belongs to the complex.
        candidate_indices = set()
        for gene in gene_set:
            candidate_indices.update(gene_to_pair_indices.get(gene, []))
        candidate_indices = sorted(candidate_indices)
        
        if not candidate_indices:
            continue
        
        # Select only the relevant pairwise comparisons.
        sub_df = pairwise_df.loc[candidate_indices]
        # A prediction is 1 if both genes in the pair are in the complex; otherwise 0.
        predictions = (sub_df[&quot;gene1&quot;].isin(gene_set) &amp; sub_df[&quot;gene2&quot;].isin(gene_set)).astype(int)
        
        if predictions.sum() == 0:
            continue
        
        # Compute cumulative true positives and derive precision and recall.
        true_positive_cumsum = predictions.cumsum()
        precision = true_positive_cumsum / (np.arange(len(predictions)) + 1)
        recall = true_positive_cumsum / true_positive_cumsum.iloc[-1]
        
        if len(recall) &lt; 2 or recall.iloc[-1] == 0:
            continue
        
        auc_scores[idx] = metrics.auc(recall, precision)
    
    # Add the computed AUC scores to the terms DataFrame.
    terms_df[&quot;auc_score&quot;] = auc_scores
    return terms_df
    
    
 
def binary(corr):
    stack = corr.stack().rename_axis(index=['gene1', 'gene2']).reset_index(name='score')
    stack = drop_mirror_pairs(stack)
    return stack
    
    
def quick_sort(df, ascending=False):
    order = 1 if ascending else -1
    sorted_df = df.iloc[np.argsort(order * df[&quot;score&quot;].values)].reset_index(drop=True)
    return sorted_df


def drop_mirror_pairs(df):
    gene_pairs = np.sort(df[[&quot;gene1&quot;, &quot;gene2&quot;]].to_numpy(), axis=1)
    df.loc[:, [&quot;gene1&quot;, &quot;gene2&quot;]] = gene_pairs
    df = df.loc[~df.duplicated(subset=[&quot;gene1&quot;, &quot;gene2&quot;], keep=&quot;first&quot;)]
    return df
</code></pre>
<p>for dummy data (corr matrix, terms_df)</p>
<pre><code>import numpy as np
import pandas as pd

# Set a random seed for reproducibility
np.random.seed(0)

# -------------------------------
# Create the 10,000 x 10,000 correlation matrix
# -------------------------------
num_genes = 10000
genes = [f&quot;Gene{i}&quot; for i in range(num_genes)]


rand_matrix = np.random.uniform(-1, 1, (num_genes, num_genes))
corr_matrix = (rand_matrix + rand_matrix.T) / 2
np.fill_diagonal(corr_matrix, 1.0)

corr_df = pd.DataFrame(corr_matrix, index=genes, columns=genes)


num_terms = 5000
terms_list = []

for i in range(1, num_terms + 1):
    # Randomly choose a number of genes between 10 and 40 for this term
    n_genes = np.random.randint(10, 41)
    used_genes = np.random.choice(genes, size=n_genes, replace=False).tolist()
    term = {
        &quot;id&quot;: i,
        &quot;name&quot;: f&quot;Complex {i}&quot;,
        &quot;used_genes&quot;: used_genes
    }
    terms_list.append(term)

terms_df = pd.DataFrame(terms_list)

# Display sample outputs (for verification, you might want to show the first few rows)
print(&quot;Correlation Matrix Sample:&quot;)
print(corr_df.iloc[:5, :5])  # print a 5x5 sample

print(&quot;\nTerms DataFrame Sample:&quot;)
print(terms_df.head())
</code></pre>
<p>to run the function <code>compute_per_complex_pr(corr_df, terms_df)</code></p>
","3","Question"
"79544960","","<p>I have a Pandas dataframe with columns that need to be sequentially calculated. Column A helps calculate Column B which helps calculate Column F, etc. Processing can be slow as Python is using only 1 thread because of the GIL.</p>
<p>I'm trying to have:</p>
<ol>
<li>First block of code run and finish normally.</li>
<li>Functions 1-3 run <strong>after first block of code</strong>, use multiprocessing, reference code above and can be used in code below.</li>
<li>2nd block of code runs <strong>after Function 3</strong> and references all lines above.</li>
</ol>
<p>Tried putting resource heavy blocks of code in functions and using multiprocessing on them only, but ran into 2 big issues:</p>
<ol>
<li>New columns created in functions couldn’t reference code above and couldn’t be referenced in code below.</li>
<li>When creating local variables in each function to reference the global dataframe, multiprocessing took much longer than normal processing.</li>
</ol>
<p>Part of my code so far. Newish to Python but trying.</p>
<pre><code>import pandas as pd
import multiprocessing as mp

## I pull some data

df['Date'] = df['timestamp'].apply(lambda x: pd.to_datetime(x*1000000))
df['volume'] = df['volume_og']
...

def functionone():
    df = pd.DataFrame()
    df['market_9'] = df.apply(lambda x : &quot;9&quot; if x['Date'] &gt;= x['market_9_start'] and x['Date'] &lt; x['market_9_end'] else None, axis=1)
    ...

def functiontwo():
    df = pd.DataFrame()
    ...

def functionthree():
    df = pd.DataFrame()
    df['nine_score'] = df.apply(lambda x : x['strength'] if x['market_9'] == &quot;9&quot; else None, axis=1)
    ...

fig = make_subplots(specs=[[{&quot;secondary_y&quot;: True}]])

fig.add_trace( 
    go.Bar( 
        x=df['Date'],
        y=df['volume'],
        ...
    ), secondary_y=True,
)


if __name__ == '__main__':

    p1 = mp.Process(target=functionone)
    p2 = mp.Process(target=functontwo)
    p3 = mp.Process(target=functionthree)

    p1.start()
    p2.start()
    p3.start()
    ...
</code></pre>
","1","Question"
"79545304","","<p>I have a dataset with over 60k rows of data of an imaginary list of purchases.
I need to filter out the names used in certain columns such as Name, City, Store, etc.</p>
<p>A lot of the names mentioned in the dataset have &quot;typos&quot; which consist of 3 random letters added to the end of the string. An example would be that the name &quot;New York&quot; is repeated 70 times but 10 out of those instances are similar to &quot;New Yorkxyz&quot; with &quot;xyz&quot; being random letters; Or &quot;Chicago&quot; that is repeated twice and one of those instances is &quot;Chicagoijk&quot;.
What I've tried so far with my unfamiliarity with python, is:</p>
<ul>
<li><p>First check if a string is repeated multiple times, and replace all the other instances that start with the letters of the said string, with it.</p>
</li>
<li><p>If no other instances were found, check again with the same string but without the last 3 letters.</p>
</li>
</ul>
<p>I'm not sure if the algorithm I came up with is correct because so far It hasn't worked, but it might be how I'm unfamiliar with python.</p>
<p>The libraries that we were told we can use are <strong>pandas, matplotlib, seaborn, nltk,
sklearn, numpy, imblearn, and scipy</strong>.</p>
<pre><code>import pandas as pd  
import re
from fuzzywuzzy import process


df = pd.read_csv(&quot;purchases.csv&quot;)  
 

columns_to_clean = [&quot;City&quot;, &quot;Name&quot;, &quot;Store&quot;]

def fix_typos_smart(df, columns):
    for col in columns:
        if col in df.columns:
            df[col] = df[col].astype(str)  #ensure all values are strings
            
            #count occurrences of full words
            full_word_counts = df[col].value_counts()

            #create a mapping of words to replace
            correction_map = {}

            for word, count in full_word_counts.items():
                if count &gt; 1:  #if the full word appears more than once
                    for typo in df[col].unique():
                        if typo.startswith(word):  #find all variants that start with the full word
                            correction_map[typo] = word
            
            #if no full-word matches, check for shortened words
            if not correction_map:
                short_word_counts = df[col].apply(lambda x: x[:-3] if len(x) &gt; 3 else x).value_counts()
                valid_shortened_words = set(short_word_counts[short_word_counts &gt; 1].index)

                for word in df[col].unique():
                    if len(word) &gt; 3:
                        shortened = word[:-3]
                        if shortened in valid_shortened_words:
                            correction_map[word] = shortened
            
            
            df[col] = df[col].replace(correction_map)

    return df

#apply the typo correction function
df = fix_typos_smart(df, columns_to_clean)

df.to_csv(&quot;purchasesfixed.csv&quot;, index=False) #save
</code></pre>
","-1","Question"
"79545386","","<p>I have irregular 3D point data that looks something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd

xx, yy = np.meshgrid(
    np.linspace(-50, 50, 101),
    np.linspace(-50, 50, 101),
)
rng = np.random.default_rng(12345)
xx += rng.normal(size=101 * 101).reshape((101, 101))
yy += rng.normal(size=101 * 101).reshape((101, 101))

df3d = pd.DataFrame({
    &quot;X&quot;: np.broadcast_to(xx, (11, 101, 101)).T.flatten(),
    &quot;Y&quot;: np.broadcast_to(yy, (11, 101, 101)).T.flatten(),
    &quot;Z&quot;: np.broadcast_to(np.arange(11, dtype=float), (101, 101, 11)).flatten(),
})
</code></pre>
<p><code>df3d</code></p>
<pre><code>                X          Y     Z
0      -51.423825 -51.287428   0.0
1      -51.423825 -51.287428   1.0
2      -51.423825 -51.287428   2.0
3      -51.423825 -51.287428   3.0
4      -51.423825 -51.287428   4.0
          ...        ...   ...
112206  51.593733  50.465087   6.0
112207  51.593733  50.465087   7.0
112208  51.593733  50.465087   8.0
112209  51.593733  50.465087   9.0
112210  51.593733  50.465087  10.0

[112211 rows x 3 columns]
</code></pre>
<p>With my analysis, I need to group these into 2D locations with 1 or more Z measures (it's not always 11 for my real-world data):</p>
<pre class=""lang-py prettyprint-override""><code>gb2d = df3d.groupby([&quot;X&quot;, &quot;Y&quot;])
df2d = gb2d[&quot;Z&quot;].count().to_frame(&quot;count&quot;)
df2d[&quot;Zmin&quot;] = gb2d[&quot;Z&quot;].min()
df2d[&quot;Zmax&quot;] = gb2d[&quot;Z&quot;].max()
</code></pre>
<p><code>df2d.reset_index()</code></p>
<pre><code>               X          Y  count  Zmin  Zmax
0     -51.995857 -49.653017     11   0.0  10.0
1     -51.939229  24.073164     11   0.0  10.0
2     -51.740996  -5.415639     11   0.0  10.0
3     -51.645503  21.830189     11   0.0  10.0
4     -51.639759 -42.850923     11   0.0  10.0
         ...        ...    ...   ...   ...
10196  51.593733  50.465087     11   0.0  10.0
10197  51.905789  37.538099     11   0.0  10.0
10198  51.989935 -32.464752     11   0.0  10.0
10199  52.530599 -40.110744     11   0.0  10.0
10200  52.902015  -6.111877     11   0.0  10.0

[10201 rows x 5 columns]
</code></pre>
<p><strong>Question:</strong> How would I assign the integer index from df2d (shown above) back to the parent df3d frame?</p>
<p>My best attempt works, but does not scale well with larger frames. E.g.:</p>
<pre class=""lang-py prettyprint-override""><code>idx2d = pd.Series(np.arange(len(df2d)), index=df2d.index)
df3d[&quot;idx2d&quot;] = idx2d.loc[df3d[[&quot;X&quot;, &quot;Y&quot;]].to_records(index=False).tolist()].values
</code></pre>
<p>works for this sample size, but takes up beyond my 32 GB RAM with my real-world data of 24 million points. What's a better way that won't eat up all of my RAM?</p>
","0","Question"
"79545484","","<p><a href=""https://i.sstatic.net/mLAi2j2D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mLAi2j2D.png"" alt=""enter image description here"" /></a></p>
<p>I just updated conda and Rstudio and now dataframes will not show up correctly in RStudio. R dataframes show up correctly but not pandas (image attached). The pandas dataframes are correctly displayed in the console. I have updated all pandas and all of my RStudio packages.</p>
","0","Question"
"79545762","","<p>I'd like some help understanding this behavior:</p>
<pre><code>import pandas as pd
pd.date_range(&quot;2016-09-01&quot;, &quot;2006-03-01&quot;, freq=&quot;-6MS&quot;, inclusive=&quot;left&quot;)
</code></pre>
<p>This returns:</p>
<pre><code>DatetimeIndex(['2016-09-01', '2016-03-01', '2015-09-01', '2015-03-01',
               '2014-09-01', '2014-03-01', '2013-09-01', '2013-03-01',
               '2012-09-01', '2012-03-01', '2011-09-01', '2011-03-01',
               '2010-09-01', '2010-03-01', '2009-09-01', '2009-03-01',
               '2008-09-01', '2008-03-01', '2007-09-01', '2007-03-01',
               '2006-09-01'],
              dtype='datetime64[ns]', freq='-6MS')
</code></pre>
<p>Note that here <code>'2006-03-01'</code> is missing.</p>
<p>When I move the end date forward to <code>2006-03-02</code>... <code>2006-03-01</code> now IS included:</p>
<pre><code>import pandas as pd
pd.date_range(&quot;2016-09-01&quot;, &quot;2006-03-02&quot;, freq=&quot;-6MS&quot;, inclusive=&quot;left&quot;)
</code></pre>
<p>Returns:</p>
<pre><code>DatetimeIndex(['2016-09-01', '2016-03-01', '2015-09-01', '2015-03-01',
               '2014-09-01', '2014-03-01', '2013-09-01', '2013-03-01',
               '2012-09-01', '2012-03-01', '2011-09-01', '2011-03-01',
               '2010-09-01', '2010-03-01', '2009-09-01', '2009-03-01',
               '2008-09-01', '2008-03-01', '2007-09-01', '2007-03-01',
               '2006-09-01', '2006-03-01'],
              dtype='datetime64[ns]', freq='-6MS')
</code></pre>
<p>I expected <code>2006-03-01</code> to be excluded and the result to be the same result in both cases, why is this happening? It's counting backwards from 2016-09-01 by 6 month intervals, so it shouldn't include 2006-03-01 when the last date is set to a value greater than that (e.g. 2006-03-02), right?</p>
","1","Question"
"79546240","","<p>I have the following Dataframe, which contains, among others, UserID and rank_group as attribute:</p>
<pre><code>  UserID  Col2  Col3  rank_group 
0    1     2     3     1
1    1     5     6     1
...
20   1     8     9     2
21   1    11    12     2
...
45   1    14    15     3
46   1    17    18     3
47   2     2     3     1
48   2     5     6     1
...
60   2     8     9     2
61   2    11    12     2
...
70   2    14    15     3
71   2    17    18     3
</code></pre>
<p>The dataframe has got an UserID, and for each user, it has rows with rank_group 1 on the top, followed by the rows with rank_group 2, etc. In other words, rank_group follows a specific progressive order, 1,2,3,4,etc</p>
<p>I would like to shuffle the order of the Dataframe's rows such that rank_group follow a random one. For example, if we compute the rank_group from 1 to n for each user, we should obtain after shuffling, the dataset reflecting any permutation from 1 to n.</p>
<p>I tried df.sample(frac=1) but it does not take into account the rank_group block but it mixes any row with any row. It is not what I am looking for. In my case, it has to maintain the same order within a fixed rank_group. Also, looked into the np.random.permutation, same issue here. Any help?</p>
","2","Question"
"79546920","","<p>My df has 19 columns but for simplicity the df will have the following columns [['Gene_name','Genes_in_same_transcription_unit']]</p>
<p>The column 'Gene_name' list a bunch of genes in <em>E. coli</em> and the corresponding entry in the column 'Genes_in_same_transcription_unit' lists the genes that are under the same promoter. An entry in this column contains multiple genes in a list that are separated by a comma and a space (example: pphA, sdsR). I want to know if there is a gene that is listed in the 'Genes_in_same_transcription_unit' that isn't in the 'Gene_name' column and for it to be highlighted or the font color for that gene, and only that gene, be in red or any other color beside black. My ultimate goal is to be able to quickly look at the column 'Genes_in_same_transcription_unit' and see if a whole operon of genes have been affected or if only 1 or 2 genes are affected and where they are in the operon (first gene, middle, or last).</p>
<p>Below is an example of the modified dataframe</p>
<p><a href=""https://i.sstatic.net/jy2Uj1iF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jy2Uj1iF.png"" alt=""enter image description here"" /></a></p>
<p>This is what I want to happen</p>
<p><a href=""https://i.sstatic.net/cpHCu0gY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cpHCu0gY.png"" alt=""enter image description here"" /></a></p>
<p>I have tried a bunch of stuff and nothing seems to work. Below is the closest I have gotten to getting what I want. With the code below I can make a new column called 'Highlighted_Genes' where the genes that should be in red be placed between two asterisks and then highlight what is in between the two asterisks in red but when I save the file the font color styling doesn't get saved.</p>
<pre><code># Convert Gene_name column to a set for quick lookup
gene_set = set(df['Gene_name'].dropna())  # Drop NaNs from Gene_name to avoid issues

# Function to highlight genes
def highlight_genes(row):
    genes_str = row['Genes_in_same_transcription_unit']
    
    # Handle NaN or missing values
    if pd.isna(genes_str):
        return None  # Keep NaNs as is
    
    genes_list = genes_str.split(', ')  # Split into list

    # Add asterisks to genes NOT in the Gene_name column
    highlighted_list = [f&quot;*{gene}*&quot; if gene not in gene_set else gene for gene in genes_list]
    
    return ', '.join(highlighted_list)  # Join back into string

# Apply function
df['Highlighted_Genes'] = df.apply(highlight_genes, axis=1)

# Function to highlight words inside asterisks
def highlight_genes(val):
    if not isinstance(val, str):  # Ensure it's a string
        return val  # Return as is (preserves NaN or other values)
    
    def replace_func(match):
        return f'&lt;span style=&quot;color: red;&quot;&gt;{match.group(1)}&lt;/span&gt;'
    
    # Replace text between * * with a red-colored span tag
    highlighted_text = re.sub(r'\*(.*?)\*', replace_func, val)
    
    return highlighted_text

# Apply the function using Styler
df_styled = df.style.format({'Highlighted_Genes': lambda x: highlight_genes(x)})

# Display in Jupyter Notebook
df_styled

df_styled.to_excel('highlighted_genes.xlsx')

</code></pre>
","0","Question"
"79547724","","<p>I'm working with a CSV file representing the sampling of a radio signal detector. My columns represent time, each one being 0.25 seconds, for a total of 3,600 columns. The rows represent the frequency at which the signal was captured, for a total of 200. For the next phase, my managers asked me to reduce the sampling resolution so that each column is equivalent to 1 second.</p>
<p>For those with more experience with Pandas, is there a method to reduce the CSV in this way?</p>
<p>I found this way to use the .mean() function:</p>
<p>row_avg = df.mean(axis=1)</p>
<p>But this works for the complete row. Is any way to use this or any other method to get the mean of 4 columns per row?</p>
","0","Question"
"79548243","","<p>I would like to define a loss function for the CVXPy optimization that minimizes differences from the reference grouped target:</p>
<pre><code>import cvxpy as cp
import pandas as pd

# toy example for demonstration purpose
target = pd.DataFrame(data={'a': ['X', 'X', 'Y', 'Z', 'Z'], 'b': [1]*5})
w = cp.Variable(target.shape[0])
beta = cp.Variable(target.shape[0])
def loss_func(w, beta):
    x = pd.DataFrame(data={'a': target['a'], 'b': w @ beta}).groupby('a')['b'].sum()
    y = target.groupby('a')['b'].sum()
    return cp.norm2(x - y)**2 # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; ValueError: setting an array element with a sequence.
</code></pre>
<p>but this gives me the following error</p>
<p><code>ValueError: setting an array element with a sequence.</code></p>
<p>What would be the way to cover this use-case using CVXPy?</p>
","1","Question"
"79548332","","<p>I have written a python method to form a pandas df and upsert it into sqlserver. This method works perfectly fine when I try to execute the python file alone. But it throws collation exception when I try to run via Azure functions.</p>
<p>Here is my code:</p>
<pre><code>import pandas as pd
from sqlalchemy import create_engine, types, inspect, MetaData, Table

eng = sqlAlchemy_getConnection(sql_alchemy_connection_url)
inspector = inspect(eng)
target_table = &quot;my_users&quot;
get_colss = inspector.get_columns(target_table,&quot;dbo&quot;)
dtype_mapping = {
                column['name']: column['type']
                for column in get_colss
            }
src_df.to_sql(temp_table,sqlAlchemy_conn,schema=&quot;dbo&quot;,if_exists='replace',index=False,dtype=dtype_mapping)
</code></pre>
<p>Error when trying to execute from Azure function:</p>
<blockquote>
<p>Exception user_name (VARCHAR(100) COLLATE
&quot;SQL_Latin1_General_CP1_CI_AS&quot;) not a string</p>
</blockquote>
<p>Drivers:</p>
<ul>
<li>ODBC Driver 17 for SQL Server</li>
</ul>
<p>What could be the cause of this issue?</p>
","0","Question"
"79549461","","<p>I have a function that returns multiple variables and I am trying to map it to a dataframe to speed up data processing. However when I run the code, it does not unpack the data and add it to the columns.</p>
<p>code</p>
<pre><code>import pandas as pd

def last_3_divisions(number: int, divide_by: int):
    last_3 = {3: None, 2: None, 1: None}
    for x in range(3, 0,-1):
        number = number / divide_by
        last_3[x] = number 
    return last_3[3], last_3[2], last_3[1]

mydf = pd.DataFrame([8,6,7,5,3,0,9], columns=[&quot;mynum&quot;])

mydf[&quot;d3&quot;], mydf[&quot;d2&quot;], mydf[&quot;d1&quot;] = mydf[&quot;mynum&quot;].map(lambda x: last_3_divisions(x, 2))
</code></pre>
<p>running the code works fine and will generate an output, however when I try to output the tuples to different columns, it produces a value error</p>
<blockquote>
<p>ValueError: too many values to unpack (expected 3)</p>
</blockquote>
<p>How do I unpack the tuples to sort them into different columns?</p>
","0","Question"
"79550255","","<p>I have browsed multiple other related questions about Pandas, and read documentation for <code>groupby</code> and <code>duplicated</code>, but I cannot seem to find a way to fit all the pieces together in Pandas. I could do this by iterating over the rows in my CSV multiple times and doing pairwise comparison, but it seems like I should be able to do it in Pandas.</p>
<p>Essentially, I would like to define my own function for comparing two rows and deciding if they are a 'duplicate'. If two rows are 'duplicates' of each other, then merge by keeping columns 0 and 1 of the first row, and use <code>sum()</code> on the rest of the columns.</p>
<p>I have a CSV file that has entries something like the following:</p>
<pre><code>   English Spanish    Count AF ... # other numerical columns
0 'hello' 'hola'      23    0
1 'helo'  'hola'      2     1
2 'hello' 'hola_a'    1     0
3 'hallo' 'a_aureola' 1     1
...
</code></pre>
<p>I would like to consider rows as 'duplicated' based on these criteria:</p>
<ul>
<li>If the Levenshtein string edit distance between two rows' English entries is below a threshold, and the Spanish is an exact match, then they are duplicates
<ul>
<li>e.g. rows 0 and 1 have an English edit distance of 1, and the Spanish is an exact match</li>
</ul>
</li>
<li>If the English of two rows is an exact match, and the Spanish entries have a non-zero overlap when split into lists on <code>_</code>, then they are duplicates
<ul>
<li>e.g. rows 0 and 2 match in English, and row0 <code>'hola'.split('_') -&gt; ['hola']</code> has an overlap with row2 <code>'hola_a'.split('_') -&gt; ['hola','a']</code> since both lists have <code>'hola'</code></li>
</ul>
</li>
<li><strong>However</strong>, if neither the English nor the Spanish is an exact match, then they ar enot duplicates
<ul>
<li>e.g. row 2 and row 3 should not be counted as duplicates, even though the English edit distance is 1, and the Spanish lists overlap when split on <code>_</code></li>
</ul>
</li>
</ul>
<p>The duplicates should then be merged, with the non-language column entries added up. (I think I could figure this part out with <code>groupby</code>, the main difficulty is getting the duplicates.) The final output should look like:</p>
<pre><code>   English Spanish    Count AF ... # other numerical columns
0 'hello' 'hola'      26    1
1 'hallo' 'a_aureola' 1     1
</code></pre>
","1","Question"
"79550403","","<p>I would like</p>
<ul>
<li>to gain a subset of a Pandas Dataframe</li>
<li>based on query, if possible</li>
<li>giving several conditions based on column values</li>
<li>where only rows have to be selected until conditions appear for the first time.</li>
</ul>
<p>Probably this is nothing new. I do just not find the right answers from other posts.</p>
<p>The example Dataframe:</p>
<pre><code>import pandas as pd
df_GPS = pd.DataFrame([['2024-06-21 06:22:38', 22958, 605.968389, 1, 2, 1],
                       ['2024-06-21 06:22:39', 22959, 606.009398, 3, 0, 1],
                       ['2024-06-21 06:22:40', 22960, 605.630573, 1, 2, 0],
                       ['2024-06-21 06:22:41', 22961, 605.476367, 3, 3, 0],
                       ['2024-06-21 06:22:42', 22962, 605.322161, 2, 1, 1],
                       ['2024-06-21 06:22:43', 22963, 605.268389, 4, 1, 0],
                       ['2024-06-21 06:22:44', 22964, 605.559398, 1, 3, 1],
                       ['2024-06-21 06:22:45', 22965, 606.630573, 2, 9 , 0],
                       ['2024-06-21 06:22:46', 22966, 607.476367, 15, 13, 3],
                       ['2024-06-21 06:22:47', 22967, 609.322161, 23, 19, 12],
                       ['2024-06-21 06:22:48', 22968, 607.155939, 20, 21, 16],
                       ['2024-06-21 06:22:49', 22969, 606.763057, 18, 14, 8],
                       ['2024-06-21 06:22:50', 22970, 605.333781, 1, 1, 1],
                       ['2024-06-21 06:22:50', 22971, 604.333781, 15, 1, 1]
                      ], columns=['time', '__UTCs__','Altitude', 's01[m]', 's5.5[m]', 's10[m]'])
df_GPS

    time    __UTCs__    Altitude    s01[m]  s5.5[m]     s10[m]
0   2024-06-21 06:22:38     22958   605.968389  1   2   1
1   2024-06-21 06:22:39     22959   606.009398  3   0   1
2   2024-06-21 06:22:40     22960   605.630573  1   2   0
3   2024-06-21 06:22:41     22961   605.476367  3   3   0
4   2024-06-21 06:22:42     22962   605.322161  2   1   1
5   2024-06-21 06:22:43     22963   605.268389  4   1   0
6   2024-06-21 06:22:44     22964   605.559398  1   3   1
7   2024-06-21 06:22:45     22965   606.630573  2   9   0
8   2024-06-21 06:22:46     22966   607.476367  15  13  3
9   2024-06-21 06:22:47     22967   609.322161  23  19  12
10  2024-06-21 06:22:48     22968   607.155939  20  21  16
11  2024-06-21 06:22:49     22969   606.763057  18  14  8
12  2024-06-21 06:22:50     22970   605.333781  1   1   1
13  2024-06-21 06:22:50     22971   604.333781  15  1   1
</code></pre>
<p>The result I am aiming at looks like:</p>
<pre><code>    time    __UTCs__    Altitude    s01[m]  s5.5[m]     s10[m]
1   2024-06-21 06:22:40     22960   605.630573  1   2   0
2   2024-06-21 06:22:41     22961   605.476367  3   3   0
3   2024-06-21 06:22:42     22962   605.322161  2   1   1
4   2024-06-21 06:22:43     22963   605.268389  4   1   0
5   2024-06-21 06:22:44     22964   605.559398  1   3   1
6   2024-06-21 06:22:45     22965   606.630573  2   9   0
7   2024-06-21 06:22:46     22966   607.476367  15  13  3
</code></pre>
<p>I tried with <code>query</code> (what I thought should be the most elegant way):</p>
<pre><code>df_sub = df_GPS.query('__UTCs__ &gt;= 22960 &amp; s01[m] &lt; 16')
</code></pre>
<p>which gives an <code>UndefinedVariableError: name 's01' is not defined</code> maybe due to the underlines or the brackets in the column names? How would I define that these are columns of df_GPS?<br>
On the other side</p>
<pre><code>df_sub = df_GPS[((df_GPS['__UTCs__'] &gt;= 22960) &amp; (df_GPS['s01[m]'] &lt; 16))].copy()
</code></pre>
<p>Which results in:</p>
<pre><code>    time    __UTCs__    Altitude    s01[m]  s5.5[m]     s10[m]
2   2024-06-21 06:22:40     22960   605.630573  1   2   0
3   2024-06-21 06:22:41     22961   605.476367  3   3   0
4   2024-06-21 06:22:42     22962   605.322161  2   1   1
5   2024-06-21 06:22:43     22963   605.268389  4   1   0
6   2024-06-21 06:22:44     22964   605.559398  1   3   1
7   2024-06-21 06:22:45     22965   606.630573  2   9   0
8   2024-06-21 06:22:46     22966   607.476367  15  13  3
12  2024-06-21 06:22:50     22970   605.333781  1   1   1
13  2024-06-21 06:22:50     22971   604.333781  15  1   1
</code></pre>
<p>works in principle but leaves all rows meeting the last criterion. I want to stop the query after the first finding of all meeting criteria.
Is there a way without undertaking a groupby of ['s01[m]']?</p>
<p>The last way I tried is with <code>loc</code>. This would also reset the index but results in the same row content:</p>
<pre><code>df_sub = df_GPS.loc[(df_GPS['__UTCs__'] &gt;= 0) &amp; (df_GPS['s01[m]'] &lt;= 16)]

    time    __UTCs__    Altitude    s01[m]  s5.5[m]     s10[m]
0   2024-06-21 06:22:38     22958   605.968389  1   2   1
1   2024-06-21 06:22:39     22959   606.009398  3   0   1
2   2024-06-21 06:22:40     22960   605.630573  1   2   0
3   2024-06-21 06:22:41     22961   605.476367  3   3   0
4   2024-06-21 06:22:42     22962   605.322161  2   1   1
5   2024-06-21 06:22:43     22963   605.268389  4   1   0
6   2024-06-21 06:22:44     22964   605.559398  1   3   1
7   2024-06-21 06:22:45     22965   606.630573  2   9   0
8   2024-06-21 06:22:46     22966   607.476367  15  13  3
12  2024-06-21 06:22:50     22970   605.333781  1   1   1
13  2024-06-21 06:22:50     22971   604.333781  15  1   1
</code></pre>
<p>How may I finish the query? with a while-loop?</p>
","2","Question"
"79550639","","<p>Latest pandas version casts types into <code>np</code> types. To cast a series of integer to strings I thought <code>astype(str)</code> would have been enough:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np


list_of_str = list(pd.Series([1234, 123, 345]).to_frame()[0].unique().astype(str))
list_of_str
</code></pre>
<p>returns <code>[np.str_('1234'), np.str_('123'), np.str_('345')]</code>.</p>
<p>And also</p>
<pre class=""lang-py prettyprint-override""><code>list_of_str = list(pd.Series([1234, 123, 345]).to_frame()[0].unique().astype(np.str_))
list_of_str
</code></pre>
<p>returns <code>[np.str_('1234'), np.str_('123'), np.str_('345')]</code>.</p>
<p>Is there an efficient way to cast to python type string, that does not require list comprehension, like:</p>
<pre class=""lang-py prettyprint-override""><code>list_of_str = [str(s) for s in list_of_str]
list_of_str
</code></pre>
<p>that finally returns <code>['1234', '123', '345']</code>
?</p>
","0","Question"
"79550795","","<p>I am trying to get user inputs to create a file where users can store website, username, and password in table format whenever users hit a button. I made the function below, and it looks okay to me. However, when a user enters the second and third entries, the data frame structure is broken. Any idea why it happens?
You may see the print result each time adding a row to my data.</p>
<p>Code:</p>
<pre><code>from tkinter import *
import pandas as pd
import os

def save_password():
    website_name = input(&quot;Website: &quot;)
    username = input(&quot;Username: &quot;)
    password = input(&quot;Password: &quot;)
    # password_details = f&quot;website: {website_name};username: {username};password: {password}&quot;
    input_entries_dict = {&quot;Website&quot;: [website_name],
                          &quot;Username/Email&quot;: [username],
                          &quot;Password&quot;: [password]}
    input_entries_df = pd.DataFrame(input_entries_dict)
    if not os.path.isfile(&quot;MyPassword_test.txt&quot;):
        input_entries_df.to_csv(&quot;MyPassword_test.txt&quot;, index=False)
        print(input_entries_df)
    else:
        data = pd.read_csv(&quot;MyPassword_test.txt&quot;)
        data = data._append(input_entries_df, ignore_index=True, sort=True)
        print(data)
        data.to_csv(&quot;MyPassword_test.txt&quot;, sep=&quot;;&quot;, index=False)

save_password()
</code></pre>
<p>Outputs for each time:</p>
<pre><code>First entry: ALL FINE
  Website Username/Email Password
0  d32d23        f7324f2  f3223f2

Second Entry: Column names are shifted
       Password Username/Email  Website
0       f3223f2        f7324f2   d32d23
1  ddwefddsfds5       32fwefw5  48sfd4s

Third Entry:Colum of &quot;Password;Username/Email;Website&quot; created!
  Password Password;Username/Email;Website Username/Email    Website
0      NaN          f3223f2;f7324f2;d32d23            NaN        NaN
1      NaN   ddwefddsfds5;32fwefw5;48sfd4s            NaN        NaN
2   154152                             NaN      f32f23f23  2f23f2332
</code></pre>
","1","Question"
"79550891","","<p>I am looking to combine rows in a DataFrame based on a grouping - but struggling to even get started with it.</p>
<p>For one field, I want to concatenate values. For another I want to sum.</p>
<p>Example:</p>
<ul>
<li>I want to group on <code>col_a</code> and <code>col_d</code></li>
<li>I want to concatenate <code>col_b</code> with a <code>|</code> separator</li>
<li>I want to sum <code>col_c</code></li>
</ul>
<p>Input</p>
<pre><code>col_a    col_b    col_c    col_d
Alex     Milk     5        UK
Alex     Sugar    4        USA
David    Rice     3        Spain
Alex     Wheat    1        UK
</code></pre>
<p>Output:</p>
<pre><code>col_a    col_b          col_c    col_d
Alex     Milk | Wheat   6        UK
Alex     Sugar          4        USA
David    Rice           3        Spain
</code></pre>
","0","Question"
"79551328","","<p>I have a large dataset (5m+ records) of NYC crimes. I want to select 5 crimes from the column 'OFNS_DESC' and group the crimes according to the number of occurrences within a month so that I can plot a time series, in a more efficient way.</p>
<p><a href=""https://i.sstatic.net/xSH9fEiI.png"" rel=""nofollow noreferrer"">sample dataset</a></p>
<p>I currently have the below. The issue is that it takes quite a while to compute because, for each value, it goes through the entire dataset and it does the same for the grouping. I want to make this process more efficient</p>
<pre><code># convert ARREST_DATE format to YYYY/MM
arrest_ym = df[&quot;ARREST_DATE&quot;].dt.to_period('M')

# filter out crime types
dngr_drug = df[df[&quot;OFNS_DESC&quot;] == &quot;DANGEROUS DRUGS&quot;]
aslt3 = df[df[&quot;OFNS_DESC&quot;] == &quot;ASSAULT 3 &amp; RELATED OFFENSES&quot;]
pt_lrcy = df[df[&quot;OFNS_DESC&quot;] == &quot;PETIT LARCENY&quot;]
flny_aslt = df[df[&quot;OFNS_DESC&quot;] == &quot;FELONY ASSAULT&quot;]
dngr_wpns = df[df[&quot;OFNS_DESC&quot;] == &quot;DANGEROUS WEAPONS&quot;]

# group and count 
dngr_drug_gc = dngr_drug.groupby(arrest_ym)[&quot;OFNS_DESC&quot;].count()
aslt3_gc = aslt3.groupby(arrest_ym)[&quot;OFNS_DESC&quot;].count()
pt_lrcy_gc = pt_lrcy.groupby(arrest_ym)[&quot;OFNS_DESC&quot;].count()
flny_aslt_gc = flny_aslt.groupby(arrest_ym)[&quot;OFNS_DESC&quot;].count()
dngr_wpns_gc = dngr_wpns.groupby(arrest_ym)[&quot;OFNS_DESC&quot;].count()
</code></pre>
<p>Below is the code for my plot. I believe it would be affected based on the changes made to the above.</p>
<pre><code># multi-line plot
x_labels = dngr_drug_gc.index.astype(str)

plt.figure(figsize = (20, 10))
plt.plot(dngr_drug_gc.index.astype(str), dngr_drug_gc.values, 'b-', label = &quot;Dangerous Drugs&quot;)
plt.plot(aslt3_gc.index.astype(str), aslt3_gc.values, 'g-', label = &quot;Assault 3 &amp; Related Offenses&quot;)
plt.plot(pt_lrcy_gc.index.astype(str), pt_lrcy_gc.values, 'r-', label = &quot;Petit Larceny&quot;)
plt.plot(flny_aslt_gc.index.astype(str), flny_aslt_gc.values, 'c-', label = &quot;Felony Assault&quot;)
plt.plot(dngr_wpns_gc.index.astype(str), dngr_wpns_gc.values, 'm-', label = &quot;Dangerous Weapons&quot;)

plt.xticks(x_labels[::6], rotation=45)

plt.xlabel(&quot;Month&quot;)
plt.ylabel(&quot;Crime Count&quot;)
plt.title(&quot;Monthly Crime Frequency&quot;)
plt.legend(loc = &quot;best&quot;)
</code></pre>
<p>My results are fine but I just want to optimise the code. One way I saw was to do the code below but I don't know how to proceed from there.</p>
<pre><code>crime_select = [&quot;DANGEROUS DRUGS&quot;, &quot;ASSAULT 3 &amp; RELATED OFFENSES&quot;, &quot;PETIT LARCENY&quot;, &quot;FELONY ASSAULT&quot;, &quot;DANGEROUS WEAPONS&quot;]
filtered_crime = df[df[&quot;OFNS_DESC&quot;].isin(crime_select)]
</code></pre>
<p>Please bear in mind that I am absolutely new to coding so any explanations would be very helpful. Thanks.</p>
<p><a href=""https://i.sstatic.net/IYeazhVW.png"" rel=""nofollow noreferrer"">result plot</a></p>
","0","Question"
"79551824","","<p>I am making calculations in the database and want to validate results against <code>Pandas</code>' calculations.</p>
<p>I want to calculate the 25th, 50th and 75th percentile.  The end use is a statistical calculation so <code>percentile_cont()</code> is the <code>Postgres</code> function I'm using.</p>
<p>My query:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT percentile_cont(0.25) WITHIN GROUP (order by obs1) as q1
     , percentile_cont(0.5) WITHIN GROUP (order by obs1) as med
     , percentile_cont(0.75) WITHIN GROUP (order by obs1) as q3
FROM my_table;
</code></pre>
<p>It returns <code>q1</code>=<code>73.99</code>, <code>med</code>=<code>74.0</code> and <code>q3</code>=<code>74.0085</code> as the result.</p>
<p>However, when I do the calculations in <code>Pandas</code> the <code>q3</code> value is different.  I searched and the consensus online is the <code>interpolation='linear'</code> argument in <code>Pandas</code> causes the calculation method to match that of <code>percentile_cont()</code>.</p>
<pre><code>dataframe = pandas.read_sql(sql='SELECT obs1 FROM my_table', con='my_connection_info')
dataframe = dataframe.sort_values(by='obs1')
percentiles = dataframe.quantile(q=[0.25, 0.5, 0.75],
                                         axis=0,
                                         numeric_only=False,
                                         interpolation='linear',
                                         method='single')
</code></pre>
<p>The results are:</p>
<pre><code>        obs1
0.25  73.992
0.50  74.000
0.75  74.006
</code></pre>
<p>I'm confused because only the 75th percentile is off.  <code>q1</code> and <code>q2</code> look like a rounding issue, but <code>q3</code> is simply off.</p>
<p>When I do my calculation method (25 sorted values * 0.75 = 18.75 position) and look at the sorted values below, the 18th and 19th position (index values 13 and 17) are both <code>74.006</code>.</p>
<p>I do not see how <code>Postgres</code> gets the <code>q3</code> result that it does, nor how to get the results from <code>Postgres</code> and <code>Pandas</code> to match for testing purposes.</p>
<p>Update 1 per answer suggestions from  @Jose Luis Dioncio:</p>
<ol>
<li>Added <code>::numeric</code>, no change to the results.</li>
<li>The column datatype is <code>float8</code></li>
<li>Confirmed <code>count</code> of data; <code>SELECT COUNT (obs1) FROM my_table;</code> = <code>25</code> and <code>len(dataframe)</code> = <code>25</code>.</li>
<li>End use is dictating the use <code>percentile_cont()</code>.  As a test I tried <code>percentile_disc()</code> instead and it returned <code>74.009</code>.  Still strikes me as a strange result.</li>
<li>Confirming the ordering inside the <code>Postgres</code> and <code>Pandas</code> methods is beyond me, but based on what I can observe it looks look it is working because <code>q1</code> and <code>q2</code> match.</li>
</ol>
<p>What should I try next?</p>
<p>Data directly from my database using <code>DBeaver</code> interface and <code>SELECT obs1 FROM my_table;</code>:</p>
<pre><code>      obs1
24  73.982
12  73.983
18  73.984
7   73.985
2   73.988
20  73.988
4   73.992
10  73.994
16  73.994
1   73.995
6   73.995
9   73.998
15  74.000
19  74.000
3   74.002
11  74.004
21  74.004
13  74.006
17  74.006
8   74.008
5   74.009
22  74.010
14  74.012
23  74.015
0   74.030
</code></pre>
","1","Question"
"79551904","","<p><strong>Description:</strong></p>
<ol>
<li><p>Input is a CSV file</p>
</li>
<li><p>CSV file contains columns of different data types: Ordinal Values, Nominal Values, Numerical Values and Multi Value</p>
</li>
<li><p>For the multivalue columns. Minimum is 1, maximum is 5 values. The input is similar to this:</p>
</li>
</ol>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Job Perks</th>
<th>Insurance Benefits</th>
</tr>
</thead>
<tbody>
<tr>
<td>Online Courses; Certification Programs; Cross Training</td>
<td>Life Insurance; Dental Insurance</td>
</tr>
<tr>
<td>Leadership Development Programs; Online Courses</td>
<td>Life Insurance; Accident Insurance</td>
</tr>
</tbody>
</table></div>
<ol start=""4"">
<li>Multivalue Expected Output:</li>
</ol>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Job Perks_Online Courses</th>
<th>Job Perks_Certification Programs</th>
<th>Job Perks_Cross Training</th>
<th>Job Perks_Leadership Development Programs</th>
<th>Insurance Benefits_Life Insurance</th>
<th>Insurance Benefits_Dental Insurance</th>
<th>Insurance Benefits_Accident Insurance</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<ol start=""5"">
<li>How do I preprocess the CSV input and save it to a dataframe with the above expected output? I am able to preprocess nominal attributes to the expected output(sample code below), but finding it hard to convert multivalues</li>
</ol>
<p><strong>Input:</strong></p>
<p>CSV Dataset: <a href=""https://github.com/omnislayer/WorkDataSet/blob/main/ECP_Unedited.csv"" rel=""nofollow noreferrer"">https://github.com/omnislayer/WorkDataSet/blob/main/ECP_Unedited.csv</a></p>
<p><strong>Sample Code:</strong></p>
<pre><code>#For the nominal:
import pandas as pd
import numpy as np
import dtale #Better STDOUT for dataframes

nominalColumns = [&quot;Gender&quot;, &quot;Marital Status&quot;, &quot;Educational Attainment&quot;, &quot;Employment Status&quot;, &quot;Company Bonus Structure&quot;, &quot;Company Medical Plan Type&quot;]
multivalueColumns = [&quot;Job Perks&quot;, &quot;Professional Development Opportunities&quot;, &quot;Insurance Benefits&quot;]

df = pd.read_csv('ECP_Unedited.csv')

#Convert Nominal Columns
newCols = pd.get_dummies(df[nominalColumns], dtype=int)
df = df.drop(columns=nominalColumns)
df = pd.concat([df, newCols], axis=1)
dtale.show(df)

#Convert Multivalue Columns
#INSERT CODE HERE!

</code></pre>
","3","Question"
"79552670","","<p>Consider the following dataframe example:</p>
<pre><code>maturity_date   simulation  simulated_price realized_price
30/06/2010      1           0.539333333     0.611
30/06/2010      2           0.544           0.611
30/06/2010      3           0.789666667     0.611
30/06/2010      4           0.190333333     0.611
30/06/2010      5           0.413666667     0.611
</code></pre>
<p>Apart from setting aside the value of the last column and concatenating, is there any other way to adjust the dataframe such that the last column becomes row?</p>
<p>Here is the desired output:</p>
<pre><code>maturity_date   simulation      simulated_price
30/06/2010      1               0.539333333     
30/06/2010      2               0.544           
30/06/2010      3               0.789666667     
30/06/2010      4               0.190333333     
30/06/2010      5               0.413666667     
30/06/2010      realized_price  0.611           
</code></pre>
","1","Question"
"79552829","","<p>I have a dataframe with location, the date a person is departing and the date their replacement is arriving.</p>
<pre><code>x = pd.DataFrame({
    'Location': ['Paris', 'Dallas', 'Tokyo', 'London'],
    'Departing': ['MAY2025', 'JUN2025', 'APR2025', 'JUN2025'],
    'Arriving': ['MAR2025', 'JUN2025', 'JUN2025', 'APR2025'],
})

    Location  Departing  Arriving
0    Paris     MAY2025    MAR2025
1    Dallas    JUN2025    JUN2025
2    Tokyo     APR2025    JUN2025
3    London    JUN2025    APR2025
</code></pre>
<p>I want to create a column for each of the next 3 months that tallies how many people will be at each location each month. For each month column up to the Departing date, a 1 should be added to the column value. And, for each month starting from the Arriving date onward, another 1 is added. So, if I was creating the table for April 2025, it should look like this:</p>
<pre><code>    Location  Departing  Arriving  APR2025  MAY2025  JUN2025
0    Paris     MAY2025    MAR2025     2        1        1
1    Dallas    JUN2025    JUN2025     1        1        1
2    Tokyo     APR2025    JUN2025     0        0        1
3    London    JUN2025    APR2025     2        2        1
</code></pre>
<p>For the first row, someone is leaving Paris May 2025. Each month up to May gets a 1, so a 1 is added to APR2025. The next person coming to Paris arrived in March 2025, so March and every month after it gets a 1 added to the column value. For the Dallas row, someone is leaving in June, so all months up to June (April and May) get a 1. The next person coming to Dallas arrives in June, so June gets a 1.</p>
<p>I need to reuse the code throughout the year, so the date months will change each month. I created the columns by setting a variable for the start month, using pd.datetime to get the next 3 months and then adding them to the dataframe as new columns. But, I'm not sure how to go about filling the columns now.</p>
","0","Question"
"79553359","","<p>I have a pandas dataframe where i want to obtain the unique 'task type' (column) not executed by the 'users' (column) found in the 'leavers' list.</p>
<pre><code>import pandas as pd

src = {'task type': ['install', 'upgrade', 'install', 'remove','upgrade'],
       'users': ['Dave', 'Carol', 'Mike', 'Alice', 'Dave']
       }

leavers = ['Carol', 'Alice']
</code></pre>
<p>e.g., I only want the tasks done by Dave and Mike. Since Dave has done an upgrade which ussualy are managed by Alice and Carol i don't want it show up.</p>
<p>Right now i can identitfy the tasks carried out by the leavers, but i'm stuck at how to substrac those task types from the main dataframe.</p>
<pre><code>df_leavers = df.loc[df['users'].isin(leavers)]
</code></pre>
<p>Some help would be much appriciated.
Sorry if it is a dumb question but i' pretty new to pandas and i can not find the answer anywhere else.</p>
<p>Edit: Finally solved it. Here i leave the answer that worked for me. Probably there are more elegant ways to do the same.</p>
<pre><code>df2 = df.loc[df['users'].isin(leavers)]
df = df.loc[~df['task type'].isin(df2['task type'])]
print(df.groupby(['task type']).size().sort_values(ascending=False))
</code></pre>
","1","Question"
"79553487","","<p>I'm dipping my toes into network visualizations in Python. I have a dataframe like the following:</p>
<pre><code>| user | nodes    |
| -----| ---------|
| A    | [0, 1, 3]|
| B    | [1, 2, 4]|
| C    | [0, 3]   |
|...   |          |
</code></pre>
<p>Is there a way to easily plot a network graph (NetworkX?) from data that contains the list of nodes on each row? The presence of a node in a row would increase the prominence of that node on the graph (or the prominence/weight of the edge in the relationship between two nodes).</p>
<p><a href=""https://i.sstatic.net/bZsrCGcU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZsrCGcU.png"" alt=""enter image description here"" /></a></p>
<p>I assume some transformation would be required to get the data into the appropriate format for NetworkX (or similar) to be able to create the graph relationships.</p>
<p>Thanks!</p>
","1","Question"
"79554434","","<p>I have a dataframe and a list below.</p>
<pre><code>import pandas as pd

my_df = pd.DataFrame({'fruits': ['apple', 'banana', 'cherry', 'durian'], 
                      'check': [False, False, False, False]})
my_list = ['pp', 'ana', 'ra', 'cj', 'up', 'down', 'pri']

&gt;&gt;&gt; my_df
   fruits     check
0     apple     False
1    banana     False
2    cherry     False
3    durian     False
</code></pre>
<p>I can make a result with nested for loops.</p>
<pre><code>for fruit in my_df['fruits']:
    for v in my_list:
        if v in fruit:
            my_df.loc[my_df['fruits']==fruit, 'check'] = True

&gt;&gt;&gt; my_df
   fruits     check
0     apple     True
1    banana     True
2    cherry     False
3    durian     False
</code></pre>
<p>I tried below.</p>
<pre><code>my_df['fruits'].apply(lambda x: True for i in my_list if i in x)
</code></pre>
<p>but, it spat out <code>Type Error: 'generator' object is not cllable</code></p>
<p>I want to remove nested for loops and replace these with apply function.
How can i do this?</p>
","0","Question"
"79554601","","<p>I'm having issues with a Palantir pipeline in a PySpark repository.</p>
<p>Specifically, I'm using Pandas and leveraging to_excel with ExcelWriter to write multiple transformed tables (as DataFrames) into different sheets of an Excel file.</p>
<p>The problem is that when writing with to_excel, I can't figure out how to convert float or double values that use a dot (.) as a decimal separator into a comma (,).</p>
<p>For example, I have code like this:</p>
<p>with pd.ExcelWriter(&quot;output.xlsx&quot;) as writer:
df.to_excel(writer, sheet_name=&quot;Sheet1&quot;, index=False)</p>
<p>But in the Excel output, I find strings like 324.45, which are not recognized as numbers because Excel expects 324,45 based on my locale.</p>
<p>I tried converting those float fields to strings using a for loop and replacing . with ,, and although the value is written to Excel with the comma, Excel still doesn't treat it as a number. Instead, I get the small green triangle (error indicator) suggesting to convert it to a number (like using Excel's Text to Columns feature).</p>
<p>I also tried using the float_format option in to_excel, like this:</p>
<p>df.to_excel(writer, sheet_name=&quot;Sheet1&quot;, index=False, float_format=&quot;%.2f&quot;)</p>
<p>But I couldn't manage to get it to use the comma as a decimal separator either.</p>
<p>This issue arises because my Excel is set to use the comma as the decimal separator (according to my regional settings), and I cannot change these settings due to company policies.</p>
<p>How can I write numbers in Excel using a comma as a decimal separator in a way that Excel recognizes them as numeric values and not as text?
Thanks a lot for any help!</p>
","0","Question"
"79555053","","<p>Consider the following dataframe example:</p>
<pre><code>id  date        hrz tenor   1       2       3       4
AAA 16/03/2010  2   6m      0.54    0.54    0.78    0.19
AAA 30/03/2010  2   6m      0.05    0.67    0.20    0.03
AAA 13/04/2010  2   6m      0.64    0.32    0.13    0.20
AAA 27/04/2010  2   6m      0.99    0.53    0.38    0.97
AAA 11/05/2010  2   6m      0.46    0.90    0.11    0.14
AAA 25/05/2010  2   6m      0.41    0.06    0.96    0.31
AAA 08/06/2010  2   6m      0.19    0.73    0.58    0.80
AAA 22/06/2010  2   6m      0.40    0.95    0.14    0.56
AAA 06/07/2010  2   6m      0.22    0.74    0.85    0.94
AAA 20/07/2010  2   6m      0.34    0.17    0.03    0.77
AAA 03/08/2010  2   6m      0.13    0.32    0.39    0.95
AAA 16/03/2010  2   1y      0.54    0.54    0.78    0.19
AAA 30/03/2010  2   1y      0.05    0.67    0.20    0.03
AAA 13/04/2010  2   1y      0.64    0.32    0.13    0.20
AAA 27/04/2010  2   1y      0.99    0.53    0.38    0.97
AAA 11/05/2010  2   1y      0.46    0.90    0.11    0.14
AAA 25/05/2010  2   1y      0.41    0.06    0.96    0.31
AAA 08/06/2010  2   1y      0.19    0.73    0.58    0.80
AAA 22/06/2010  2   1y      0.40    0.95    0.14    0.56
AAA 06/07/2010  2   1y      0.22    0.74    0.85    0.94
AAA 20/07/2010  2   1y      0.34    0.17    0.03    0.77
AAA 03/08/2010  2   1y      0.13    0.32    0.39    0.95
</code></pre>
<p>How can I <code>grouby</code> the variables <code>id, hrz</code> and <code>tenor</code> and apply the following custom functions across the dates?</p>
<pre><code> def ks_test(x):
    return scipy.stats.kstest(np.sort(x), 'uniform')[0]

 def cvm_test(x):
    n = len(x)
    i = np.arange(1, n + 1)
    x = np.sort(x)
    w2 = (1 / (12 * n)) + np.sum((x - ((2 * i - 1) / (2 * n))) ** 2)
    return w2
</code></pre>
<p>The desired output is the following dataframe (figure results are just examples):</p>
<pre><code>id   hrz    tenor   test        1       2       3       4
AAA  2      6m      ks_test     0.04    0.06    0.02    0.03
AAA  2      6m      cvm_test    0.09    0.17    0.03    0.05
AAA  2      1y      ks_test     0.04    0.06    0.02    0.03
AAA  2      1y      cvm_test    0.09    0.17    0.03    0.05
</code></pre>
","1","Question"
"79555775","","<p>I have the following dataframe:</p>
<pre><code>import pandas as pd
d = {'Name': ['DataSource', 'DataSource'], 'DomainCode': ['Pr', 'Gov'], 'DomainName': ['Private', 'Government']}
df = pd.DataFrame(data=d)
</code></pre>
<p>So the dataframe is as follows:</p>
<pre><code>         Name DomainCode  DomainName
0  DataSource         Pr     Private
1  DataSource        Gov  Government
</code></pre>
<p>I need to group it by the name to receive two lists:</p>
<pre><code>         Name DomainCode    DomainName
0  DataSource [Pr, Gov]     [Private, Government]
</code></pre>
<p>I understand how to do it for a single column:</p>
<pre><code>df = df.groupby(&quot;Name&quot;)[&quot;DomainCode&quot;].apply(list).reset_index()
</code></pre>
<p>when I receive</p>
<pre><code>           Name DomainCode
0  A_DataSource  [GOV, PR]
</code></pre>
<p>but I cannot add the second column there whatever I tried. How to do this?</p>
<p>One more question is that the list returned by the previous command is somehow not a list as it has a length of 1, and not two.</p>
","1","Question"
"79555826","","<p>I would like to run groupby and aggregation over a dataframe where the aggregation joins strings with the same <code>id</code>.
The df looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>In [1]: df = pd.DataFrame.from_dict({'id':[1,1,2,2,2,3], 'name':['a','b','c','d','e','f']})
In [2]: df
Out[2]:
   id name
0   1    a
1   1    b
2   2    c
3   2    d
4   2    e
5   3    f
</code></pre>
<p>I have this working in Pandas thus:</p>
<pre class=""lang-py prettyprint-override""><code>def list_aggregator(x):
    return '|'.join(x)

df2 = pd.DataFrame.from_dict('id':[], 'name':[])
df2['id'] = df['id'].drop_duplicates()
df2['name'] = df['name'].groupby(df['id']).agg(list_aggregator).values
</code></pre>
<p>Produces:</p>
<pre class=""lang-py prettyprint-override""><code>In [26]: df2
Out[26]:
   id   name
0   1    a|b
2   2  c|d|e
5   3      f
</code></pre>
<p>For Dask, my understanding (from <a href=""https://docs.dask.org/en/latest/dataframe-groupby.html#aggregate"" rel=""nofollow noreferrer"">the docs</a>) is you have to tell Dask what to do to aggregate within chunks, and then what to do with those aggregated chunks. In both cases, I want to do the equivalent of <code>'|'.join()</code>. So:</p>
<pre class=""lang-py prettyprint-override""><code>ddf = dd.from_pandas(df, 2)
ddf2 = dd.from_pandas(pd.DataFrame.from_dict({'id':[],'name':[]}))
ddf2['id'] = ddf['id'].drop_duplicates()

dd_list_aggregation = dd.Aggregation(
    'list_aggregation',
    list_aggregator,  # chunks are aggregated into strings with 1 string per chunk
    list_aggregator,  # per-chunk strings are aggregated into a single string per id
)

ddf2['name'] = ddf['name'].groupby(ddf['id']).agg(dd_list_aggregation).values
</code></pre>
<p>Expected result is as above (or, indeed, nothing as I haven't called <code>ddf2.compute()</code> yet), but I receive this error:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask_expr/_core.py:446, in Expr.__getattr__(self, key)
    445 try:
--&gt; 446     return object.__getattribute__(self, key)
    447 except AttributeError as err:

File ~/miniconda3/envs/test/lib/python3.10/functools.py:981, in cached_property.__get__(self, instance, owner)
    980 if val is _NOT_FOUND:
--&gt; 981     val = self.func(instance)
    982     try:

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask_expr/_groupby.py:206, in GroupByApplyConcatApply._meta_chunk(self)
    205 meta = meta_nonempty(self.frame._meta)
--&gt; 206 return self.chunk(meta, *self._by_meta, **self.chunk_kwargs)

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask/dataframe/groupby.py:1200, in _groupby_apply_funcs(df, *by, **kwargs)
   1199 for result_column, func, func_kwargs in funcs:
-&gt; 1200     r = func(grouped, **func_kwargs)
   1202     if isinstance(r, tuple):

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask/dataframe/groupby.py:1276, in _apply_func_to_column(df_like, column, func)
   1275 if column is None:
-&gt; 1276     return func(df_like)
   1278 return func(df_like[column])

Cell In[88], line 2
      1 def dd_list_aggregator(x):
----&gt; 2     return '|'.join(x[1])

File ~/miniconda3/envs/test/lib/python3.10/site-packages/pandas/core/base.py:245, in SelectionMixin.__getitem__(self, key)
    244     raise KeyError(f&quot;Column not found: {key}&quot;)
--&gt; 245 ndim = self.obj[key].ndim
    246 return self._gotitem(key, ndim=ndim)

AttributeError: 'str' object has no attribute 'ndim'

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
Cell In[96], line 1
----&gt; 1 ddf2['name'] = ddf['name'].groupby(ddf['id']).agg(dd_list_aggregation).values

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask_expr/_groupby.py:1907, in GroupBy.agg(self, *args, **kwargs)
   1906 def agg(self, *args, **kwargs):
-&gt; 1907     return self.aggregate(*args, **kwargs)

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask_expr/_groupby.py:1891, in GroupBy.aggregate(self, arg, split_every, split_out, shuffle_method, **kwargs)
   1888 if arg == &quot;size&quot;:
   1889     return self.size()
-&gt; 1891 return new_collection(
   1892     GroupbyAggregation(
   1893         self.obj.expr,
   1894         arg,
   1895         self.observed,
   1896         self.dropna,
   1897         split_every,
   1898         split_out,
   1899         self.sort,
   1900         shuffle_method,
   1901         self._slice,
   1902         *self.by,
   1903     )
   1904 )

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask_expr/_collection.py:4440, in new_collection(expr)
   4438 def new_collection(expr):
   4439     &quot;&quot;&quot;Create new collection from an expr&quot;&quot;&quot;
-&gt; 4440     meta = expr._meta
   4441     expr._name  # Ensure backend is imported
   4442     return get_collection_type(meta)(expr)

File ~/miniconda3/envs/test/lib/python3.10/functools.py:981, in cached_property.__get__(self, instance, owner)
    979 val = cache.get(self.attrname, _NOT_FOUND)
    980 if val is _NOT_FOUND:
--&gt; 981     val = self.func(instance)
    982     try:
    983         cache[self.attrname] = val

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask_expr/_groupby.py:432, in GroupbyAggregation._meta(self)
    430 @functools.cached_property
    431 def _meta(self):
--&gt; 432     return self._lower()._meta

File ~/miniconda3/envs/test/lib/python3.10/functools.py:981, in cached_property.__get__(self, instance, owner)
    979 val = cache.get(self.attrname, _NOT_FOUND)
    980 if val is _NOT_FOUND:
--&gt; 981     val = self.func(instance)
    982     try:
    983         cache[self.attrname] = val

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask_expr/_reductions.py:425, in ApplyConcatApply._meta(self)
    423 @functools.cached_property
    424 def _meta(self):
--&gt; 425     meta = self._meta_chunk
    426     aggregate = self.aggregate or (lambda x: x)
    427     if self.combine:

File ~/miniconda3/envs/test/lib/python3.10/site-packages/dask_expr/_core.py:451, in Expr.__getattr__(self, key)
    447 except AttributeError as err:
    448     if key.startswith(&quot;_meta&quot;):
    449         # Avoid a recursive loop if/when `self._meta*`
    450         # produces an `AttributeError`
--&gt; 451         raise RuntimeError(
    452             f&quot;Failed to generate metadata for {self}. &quot;
    453             &quot;This operation may not be supported by the current backend.&quot;
    454         )
    456     # Allow operands to be accessed as attributes
    457     # as long as the keys are not already reserved
    458     # by existing methods/properties
    459     _parameters = type(self)._parameters

RuntimeError: Failed to generate metadata for DecomposableGroupbyAggregation(frame=df['name'], arg=&lt;dask.dataframe.groupby.Aggregation object at 0x7f052960b850&gt;, observed=False, split_out=1). This operation may not be supported by the current backend.
</code></pre>
<p>My thinking is numerical objects are expected, but the backend is pandas, so string manipulations should be possible, right?</p>
","2","Question"
"79557064","","<p>I'm noticing an inconsistency in how <code>pd.isnull</code> behaves.</p>
<p>Given that <code>pd.isnull('nan')</code> returns <code>False</code>, but <code>pd.isnull(float('nan'))</code> returns <code>True</code>, I would have expected that applying <code>pd.isnull</code> to a DataFrame column previously converted to the 'Float64' dtype would return <code>True</code> for any value that was originally the string 'nan'. But that's not what I'm seeing.</p>
<p>Here's an example:</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame([['1', pd.NA, 'nan'], ['re', 'ui1', np.nan]], columns=['a', 'b', 'c'])

pd.isnull(df['c'])  # [False, True] ==&gt; expected
pd.isnull(df.loc[0, 'c'])  # False ==&gt; expected
pd.isnull(df[['c']].astype('Float64'))  # [False, True] ==&gt; unexpected?
pd.isnull(df[['c']].astype('Float64').values)  # [True, True] ==&gt; expected
pd.isnull(df[['c']].astype('Float64').loc[0, 'c'])  # True ==&gt; expected
</code></pre>
<p>Why does <code>pd.isnull</code> exhibit different behavior when applied to an entire column or dataset compared to a single element?
Am I missing something?</p>
<ul>
<li>python 3.9.12</li>
<li>pandas 2.2.3</li>
<li>numpy 1.26.4</li>
</ul>
<p><strong>EDIT</strong><br />
As pointed out in a comment below, interestingly, we don't encounter the same issue when converting to the 'float' dtype:</p>
<pre><code>pd.isnull(df[['c']].astype('float'))  # [True, True]  ==&gt; expected
</code></pre>
","2","Question"
"79557429","","<p>I am using Google Colab.
I wanted to import pandas, however I received the following error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)

&lt;ipython-input-15-7dd3504c366f&gt; in &lt;cell line: 0&gt;()
----&gt; 1 import pandas as pd

6 frames

/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in &lt;module&gt;
     30 
     31 from pandas._libs import lib
---&gt; 32 from pandas._libs.parsers import STR_NA_VALUES
     33 from pandas.errors import (
     34     AbstractMethodError,

parsers.pyx in init pandas._libs.parsers()

AttributeError: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)
</code></pre>
<p>How can I resolve it?</p>
","0","Question"
"79558903","","<p>I've been working on this for two weeks. I have a data frame with 3,600 columns and 200 rows, which correspond to a frequency sweep performed with a detector. The file's original format is .fit, so I'm working with the astropy.fits library. The .fit file doesn't add a time stamp, which is necessary for the analysis I'm performing. I tried adding it by creating another data frame, but when I inserted the time stamp values, I got the following error:</p>
<p>ValueError: Shape of passed values is (200, 3600), indices imply (3600, 3600)</p>
<pre><code>hdu = fits.open(file)
data = hdu[0].data
minusMeans = data - data.mean()
freqs = hdu[1].data['Frequency'][0]
spectraTimeStamp = pd.date_range( start= start_time, end= end_time, periods=3600)
finalData = pd.DataFrame(minusMeans, index=spectra_time)
</code></pre>
","-1","Question"
"79560135","","<p>I have a dataframe that looks like this:</p>
<pre><code>test = pd.DataFrame(
    {'onset': [1,3,18,33,35,50],
     'duration': [2,15,15,2,15,15],
     'type': ['Instr', 'Remember', 'SocTestString', 'Rating', 'SelfTestString', 'XXX']
    }
)
</code></pre>
<p>I want to create a new dataframe such that when <code>type</code> contains &quot;TestString&quot;,</p>
<ul>
<li>two new rows are created below that row, such that the row is is now split into three rows with (for example) SocTestString_1, SocTestString_2, SocTestString_3</li>
<li>for those three rows, change duration columns to the value 5</li>
<li>for those three rows, also change the onset column such that it is the onset value of the previous column + 5</li>
</ul>
<p>The final dataframe should look like this:</p>
<pre><code>test_final = pd.DataFrame(
    {'onset': [1,3,18,23,28,33,35,40,45,50],
     'duration': [2,15,5,5,5,2,5,5,5,15],
     'type': ['Instr', 'Remember', 'SocTestString_1', 'SocTestString_2', 'SocTestString_3', 'Rating', 'SelfTestString_1', 'SelfTestString_2', 'SelfTestString_3', 'XXX']
    })
</code></pre>
<p>How may I accomplish this?</p>
","2","Question"
"79561773","","<p>I want to add color to cells of Status Pass and Fail. I tried following code it will print a colored data frame but <strong>exported csv was not colored</strong>.</p>
<p>And when I applied two rules (green and red), only green shows.</p>
<pre><code>df = pd.DataFrame([{'Status':'Pass', 'Value': '0'}, {'Status':'Pass', 'Value': '1'}, {'Status':'FAIL', 'Value': '2'}])
# add background color green to all lines with status 'PASS'
df.to_csv('test.csv')
df.style.map(lambda x: 'background-color: green' if x == 'Pass' else '', subset=['Status'])
</code></pre>
","1","Question"
"79562296","","<p>I am trying to find the last of a weekday in a month. For example, let us say the last Sunday in October.</p>
<p>I try to do like this:</p>
<pre><code>pd.Timestamp(&quot;2025-10-31&quot;) - pd.DateOffset(weekday=6)
</code></pre>
<p>The resulting date is <code>Timestamp('2025-11-02 00:00:00')</code>, i.e. the result is a later date despite the minus. The result is also identical if I add in stead of subtract.</p>
<p>This is in contrast to:</p>
<pre><code>&gt;&gt;&gt; pd.Timestamp(&quot;2025-10-31&quot;) - pd.DateOffset(days=2)
Timestamp('2025-10-29 00:00:00')

&gt;&gt;&gt; pd.Timestamp(&quot;2025-10-31&quot;) - pd.DateOffset(day=2)
Timestamp('2025-10-02 00:00:00')
</code></pre>
<p>which result in an earlier date as I expect, so the behaviour is just different for 'weekday' than for other offsets.</p>
<p>If I attempt to use the <code>DateOffset.rollback</code> method instead, it simply does not change the date:</p>
<pre><code>&gt;&gt;&gt; pd.DateOffset(weekday=6).rollback(pd.Timestamp(&quot;2025-10-31&quot;))
Timestamp('2025-10-31 00:00:00')
</code></pre>
<p>I cannot find anything in the DateOffset or DateOffset.rollback documentation that describes this.</p>
<p>What can I do to make this work?</p>
","1","Question"
"79562359","","<p>I have a dataset in the following form:</p>
<pre><code>2024-10-27T00:00    1   A
2024-10-27T00:15    2   B
2024-10-27T00:30    3   C
2024-10-27T00:45    4   D
2024-10-27T01:00    5   E
2024-10-27T01:15    6   F
2024-10-27T01:30    7   G
2024-10-27T01:45    8   H
2024-10-27T02:00    9   I
2024-10-27T02:00    10  J
2024-10-27T02:15    11  K
2024-10-27T02:15    12  L
2024-10-27T02:30    13  M
2024-10-27T02:30    14  N
2024-10-27T02:45    15  O
2024-10-27T02:45    16  P
2024-10-27T03:00    17  Q
2024-10-27T03:15    18  R
2024-10-27T03:30    19  S
2024-10-27T03:45    20  T

</code></pre>
<p>This is in local CE(S)T time, I want to let pandas infer the DST change. However it throws an error that there are 4 DST changes where it expects only one.</p>
<p>Interestingly, the code below does work:</p>
<pre><code>import pandas as pd

data = [
    [&quot;2024-10-27T00:00&quot;, 1, &quot;A&quot;],
    [&quot;2024-10-27T00:15&quot;, 2, &quot;B&quot;],
    [&quot;2024-10-27T00:30&quot;, 3, &quot;C&quot;],
    [&quot;2024-10-27T00:45&quot;, 4, &quot;D&quot;],
    [&quot;2024-10-27T01:00&quot;, 5, &quot;E&quot;],
    [&quot;2024-10-27T01:15&quot;, 6, &quot;F&quot;],
    [&quot;2024-10-27T01:30&quot;, 7, &quot;G&quot;],
    [&quot;2024-10-27T01:45&quot;, 8, &quot;H&quot;],
    [&quot;2024-10-27T02:00&quot;, 9, &quot;I&quot;],
    [&quot;2024-10-27T02:15&quot;, 10, &quot;J&quot;],
    [&quot;2024-10-27T02:30&quot;, 11, &quot;K&quot;],
    [&quot;2024-10-27T02:45&quot;, 12, &quot;L&quot;],
    [&quot;2024-10-27T02:00&quot;, 13, &quot;M&quot;],
    [&quot;2024-10-27T02:15&quot;, 14, &quot;N&quot;],
    [&quot;2024-10-27T02:30&quot;, 15, &quot;O&quot;],
    [&quot;2024-10-27T02:45&quot;, 16, &quot;P&quot;],
    [&quot;2024-10-27T03:00&quot;, 17, &quot;Q&quot;],
    [&quot;2024-10-27T03:15&quot;, 18, &quot;R&quot;],
    [&quot;2024-10-27T03:30&quot;, 19, &quot;S&quot;],
    [&quot;2024-10-27T03:45&quot;, 20, &quot;T&quot;],
]

df = pd.DataFrame(data, columns=[&quot;timestamp&quot;, &quot;number&quot;, &quot;letter&quot;])
df.set_index(&quot;timestamp&quot;, inplace=True)
df.index=pd.to_datetime(df.index)

df.index=df.index.tz_localize('Europe/Amsterdam',ambiguous='infer')

</code></pre>
<p>In the latter example Pandas seems to interpret (rightfully so) that the first occurence indeed is summertime and the second is wintertime. Is there a way to deal with this (easily)? Hardcoding the re-order would be a way, but seems cumbersome (honestly wouldn't even really know how, something like filtering for duplicates and then looping and swapping the rows?).</p>
<p>Anybody have thoughts on this?</p>
","1","Question"
"79563104","","<p>I'm doing the following leetcode problem and have been experimenting with different solutions: <a href=""https://leetcode.com/problems/customer-who-visited-but-did-not-make-any-transactions/description/"" rel=""nofollow noreferrer"">https://leetcode.com/problems/customer-who-visited-but-did-not-make-any-transactions/description/</a></p>
<p>In the problem, we have the following data:</p>
<pre><code>data = [[1, 23], [2, 9], [4, 30], [5, 54], [6, 96], [7, 54], [8, 54]]
visits = pd.DataFrame(data, columns=['visit_id', 'customer_id']).astype({'visit_id':'Int64', 'customer_id':'Int64'})
data = [[2, 5, 310], [3, 5, 300], [9, 5, 200], [12, 1, 910], [13, 2, 970]]
transactions = pd.DataFrame(data, columns=['transaction_id', 'visit_id', 'amount']).astype({'transaction_id':'Int64', 'visit_id':'Int64', 'amount':'Int64'})
</code></pre>
<p>Printed out, the data is:</p>
<pre><code>Visits
+----------+-------------+
| visit_id | customer_id |
+----------+-------------+
| 1        | 23          |
| 2        | 9           |
| 4        | 30          |
| 5        | 54          |
| 6        | 96          |
| 7        | 54          |
| 8        | 54          |
+----------+-------------+

Transactions
+----------------+----------+--------+
| transaction_id | visit_id | amount |
+----------------+----------+--------+
| 2              | 5        | 310    |
| 3              | 5        | 300    |
| 9              | 5        | 200    |
| 12             | 1        | 910    |
| 13             | 2        | 970    |
+----------------+----------+--------+
</code></pre>
<p>The problem asks for a dataframe where for each <code>customer_id</code>, the number of visits with no transactions is recorded (if it is non-zero). Thus, with the previous sample data, we expect the following output:</p>
<pre><code>+-------------+----------------+
| customer_id | count_no_trans |
+-------------+----------------+
| 54          | 2              |
| 30          | 1              |
| 96          | 1              |
+-------------+----------------+
</code></pre>
<p>My idea is to:</p>
<ol>
<li>Remove rows with duplicate <code>visit_id</code> from <code>transactions</code></li>
<li>Left merge <code>visits</code> with <code>transactions</code> on <code>visit_id</code></li>
<li>Group by <code>customer_id</code> and count the number of null rows in each group (i.e. the number of visits where no transactions were made).</li>
</ol>
<p>For Steps 1 and 2, I do the following:</p>
<pre><code>df = pd.merge(left=visits,
    right = transactions.drop_duplicates(subset = ['visit_id']),
    how = 'left',
    on = 'visit_id')
</code></pre>
<p>For Step 3, this works with the custom aggregation function:</p>
<pre><code>df1 = (df[df['transaction_id'].isna()]
    .groupby('customer_id', as_index = False)['transaction_id']
    .agg(lambda x: x.isna().sum())).rename(columns = {'transaction_id':'count_no_trans'})
</code></pre>
<p>but if I just try to use the built in <code>.isna().sum()</code> methods, it doesn't work:</p>
<pre><code>df2 = (df[df['transaction_id'].isna()]
    .groupby('customer_id', as_index = False)['transaction_id']
    .isna().sum().rename(columns = {'transaction_id':'count_no_trans'}))
</code></pre>
<p>I get the error:</p>
<p><code>AttributeError: 'SeriesGroupBy' object has no attribute 'isna'</code></p>
<p>Why does this happen?</p>
","0","Question"
"79563265","","<p>I have a column in Pandas DataFrame(Names) with a large collection of names. I have another DataFrame(Title) text column and in between text, the names in Name frame are there. What would be the fastest way to extract matching Names from Title? The below shows my working so far and it is very slow for the actual large dataset.</p>
<pre><code>import pandas as pd
import re

Names = pd.DataFrame({
    'ENTITY_NAME': ['XYZ', 'ABC', 'NGA', 'METRO','DPAC']
})

Titles = pd.DataFrame({
    'title': ['testing some text XYZ testing some text.',
              'XYZ, ABC some random text',
              'some text DPAC random random']
})

# Function to extract ENTITY_NAME if found in title
def extract_entity_name(title, entity_names):
    pattern = '|'.join([r'\b' + re.escape(entity) + r'\b' for entity in entity_names])
    matches = re.findall(pattern, title)
    return ', '.join(matches)
  



Titles['extracted_entity_name'] = Titles['title'].apply(lambda x: extract_entity_name(x, Names['ENTITY_NAME'].tolist()))

display(Titles.head())
</code></pre>
","2","Question"
"79565121","","<p>When using TensorFlow's <code>tf.data.Dataset.take(1)</code> to get the first element of a dataset and then attempting to retrieve this element using <code>iter()</code> and <code>next()</code>, the <code>next()</code> call enters an infinite loop if the <code>pandas</code> library is imported <em>before</em> <code>tensorflow</code>.</p>
<p>This behavior has been observed in both VS Code Jupyter Notebook and standard Jupyter Notebook environments, suggesting an interaction at the Python runtime level.</p>
<p>Code to reproduce:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd  
import tensorflow as tf

# Assume your original dataset looks something like this 
def generator():
  for i in range(5):
    yield (tf.random.uniform(shape=(2, 7, 11)), tf.random.uniform(shape=(2, 1, 1)))

original_dataset = tf.data.Dataset.from_generator(
    generator,
    output_signature=(
        tf.TensorSpec(shape=(None, 7, 11), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 1, 1), dtype=tf.float32)
    )
)

# take the first element 
first_element_ds = original_dataset.take(1)
iterator = iter(first_element_ds)

# get the first element with next() will run into an infinite loop
first_element_tuple = next(iterator)

tensor1, tensor2 = first_element_tuple
print(&quot;Tensor 1:&quot;, tensor1.numpy().shape)
print(&quot;Tensor 2:&quot;, tensor2.numpy().shape)
</code></pre>
<p>Environment:</p>
<ul>
<li>Pandas Version: 2.2.2</li>
<li>TensorFlow Version: 2.17.0</li>
<li>Python Version: 3.11.9</li>
<li>NumPy Version: 1.26.4</li>
<li>Jupyter Core Version: 5.7.2</li>
<li>Operating System: macOS 13.7.4</li>
<li>Virtual Environment: conda 24.11.0, cPython 3.11.5</li>
</ul>
<p>Question:</p>
<p>Why does the execution of <code>next()</code> runs into an infinite loop?</p>
","2","Question"
"79566113","","<p>I would like to bring to your attention my straightforward code. However, I am encountering an issue where the code does not return the data from the first column after applying the group by function. Could you please advise me on how I can extract the data from the first column or the primary grouping column to create a list or a new data frame?</p>
<pre><code>
import pandas as pd

data = {
    'Region': ['North America', 'North America', 'North America', 'Europe', 'Europe', 'Asia', 'Asia'],
    'Sale': [76445, 45555, 73356, 54467, 65758, 544456, 75556],
    }

df = pd.DataFrame(data)
df2 = df.groupby('Region')['Sale'].sum()
test = df2[['Region']]
print(test) 

</code></pre>
","-1","Question"
"79567429","","<p>I guess this must be rather simple, but I'm struggling to find the easy way of doing it.</p>
<p>I have a pandas DataFrame with the columns A to D and need to copy some of the columns to new ones. The trick is that it not just involves renaming, I also need to duplicate the values to new columns as well.</p>
<p>Here is an example of the input:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.DataFrame({
    'A': [1,2,3], 
    'B':['2025-10-01', '2025-10-02', '2025-10-01'], 
    'C': ['2025-02-10', '2025-02-15', '2025-02-20'], 
    'D': [0, 5, 4],
    'values': [52.3, 60, 70.6]
})

mapping_dict = {
    'table_1': {
        'id': 'A',
        'dt_start': 'B',
        'dt_end': 'B',
    },
    'table_2': {
        'id': 'D',
        'dt_start': 'C',
        'dt_end': 'C',
    },
}

</code></pre>
<p>I'd like to have as output for <code>table_1</code> a DataFrame as follows:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">id</th>
<th style=""text-align: center;"">dt_start</th>
<th style=""text-align: center;"">dt_end</th>
<th style=""text-align: center;"">values</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">2025-10-01</td>
<td style=""text-align: center;"">2025-10-01</td>
<td style=""text-align: center;"">52.3</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">2025-10-02</td>
<td style=""text-align: center;"">2025-10-02</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2025-10-01</td>
<td style=""text-align: center;"">2025-10-01</td>
<td style=""text-align: center;"">80.6</td>
</tr>
</tbody>
</table></div>
<p>And I guess it is possible to infer the expected output for <code>table_2</code>.</p>
<p>Note that the column <code>values</code>, which is not included in the mapping logic, should remain in the dataframe.</p>
<p>I was able to achieve this by using a for loop, but I feel that should be a natural way of doing this directly on pandas without manually looping over the mapping dict and then dropping the extra columns.</p>
<p>Here is my solution so far:</p>
<pre class=""lang-py prettyprint-override""><code>table_name = 'table_1'

new_df = df.copy()
for new_col, old_col in mapping_dict[table_name].items():
    new_df[new_col] = df[old_col]

new_df = new_df.drop(mapping_dict[table_name].values(), axis='columns')
</code></pre>
<p>Any help or suggestion will be appreciated!</p>
","2","Question"
"79567767","","<p>I'm doing the following problem on Leetcode: <a href=""https://leetcode.com/problems/monthly-transactions-i/"" rel=""nofollow noreferrer"">https://leetcode.com/problems/monthly-transactions-i/</a></p>
<p>My question doesn't deal much with the actual problem, all you need to know is that there is a column <code>'trans_date'</code> that has dates in the YYYY-MM-DD format, and I'd like to convert this date to the YYYY-MM format.</p>
<p>But, if I write:</p>
<pre><code>import pandas as pd

def monthly_transactions(transactions: pd.DataFrame) -&gt; pd.DataFrame:
    transactions['trans_date'] = transactions['trans_date'].dt.to_period('M')
    return transactions
</code></pre>
<p>I get a Runtime error when running on Leetcode:</p>
<pre><code>OverflowError: Maximum recursion level reached
    return ujson_dumps(
Line 263 in write (/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py)
    ).write()
Line 210 in to_json (/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py)
    return json.to_json(
Line 2702 in to_json (/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py)
    return func(*args, **kwargs)
Line 333 in wrapper (/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py)
    json_file = simplejson.loads(result_table.to_json(orient='split'))
Line 48 in main (_driver.py)
    main()
Line 62 in &lt;module&gt; (_driver.py)
</code></pre>
<p>But, this is fine (it runs and gives the wrong answer):</p>
<pre><code>import pandas as pd

def monthly_transactions(transactions: pd.DataFrame) -&gt; pd.DataFrame:
    transactions['trans_date'].dt.to_period('M')
    return transactions
</code></pre>
<p>I'm pretty confused here, since running this on my own computer and not through Leetcode is fine.</p>
<p>Can anyone else reproduce this behavior and/or suggest how to fix it?</p>
<p>edit: Ok, people said to post a minimal (non-working) example. I assumed it was obvious from the context of Leetcode that you copy and paste the function definition and then run it in Leetcode, but alas. Here are some minimal examples that fail:</p>
<pre><code>import pandas as pd

data = [[121, 'US', 'approved', 1000, '2018-12-18'], [122, 'US', 'declined', 2000, '2018-12-19'], [123, 'US', 'approved', 2000, '2019-01-01'], [124, 'DE', 'approved', 2000, '2019-01-07']]
transactions = pd.DataFrame(data, columns=['id', 'country', 'state', 'amount', 'trans_date']).astype({'id':'Int64', 'country':'object', 'state':'object', 'amount':'Int64', 'trans_date':'datetime64[ns]'})

def monthly_transactions(transactions: pd.DataFrame) -&gt; pd.DataFrame:
    transactions['trans_date'] = transactions['trans_date'].dt.to_period('M')
    return transactions

print(monthly_transactions(transactions))
</code></pre>
<p>with error:</p>
<pre><code>OverflowError: Maximum recursion level reached
    return ujson_dumps(
Line 263 in write (/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py)
    ).write()
Line 210 in to_json (/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py)
    return json.to_json(
Line 2702 in to_json (/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py)
    return func(*args, **kwargs)
Line 333 in wrapper (/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py)
    json_file = simplejson.loads(result_table.to_json(orient='split'))
Line 48 in main (_driver.py)
    main()
Line 62 in &lt;module&gt; (_driver.py)
</code></pre>
<p>I'm looking to get the same result as running on my computer:</p>
<pre><code>    id country     state  amount trans_date
0  121      US  approved    1000    2018-12
1  122      US  declined    2000    2018-12
2  123      US  approved    2000    2019-01
3  124      DE  approved    2000    2019-01
</code></pre>
<p>Someone mentioned that there is potentially an issue with overwriting the same column, but making a new column also fails with the same error as above:</p>
<pre><code>import pandas as pd

data = [[121, 'US', 'approved', 1000, '2018-12-18'], [122, 'US', 'declined', 2000, '2018-12-19'], [123, 'US', 'approved', 2000, '2019-01-01'], [124, 'DE', 'approved', 2000, '2019-01-07']]
transactions = pd.DataFrame(data, columns=['id', 'country', 'state', 'amount', 'trans_date']).astype({'id':'Int64', 'country':'object', 'state':'object', 'amount':'Int64', 'trans_date':'datetime64[ns]'})

def monthly_transactions(transactions: pd.DataFrame) -&gt; pd.DataFrame:
    transactions['trans_date_1'] = transactions['trans_date'].dt.to_period('M')
    return transactions

print(monthly_transactions(transactions))
</code></pre>
<p>I'm looking to get (from running on my computer):</p>
<pre><code>    id country     state  amount trans_date trans_date_1
0  121      US  approved    1000 2018-12-18      2018-12
1  122      US  declined    2000 2018-12-19      2018-12
2  123      US  approved    2000 2019-01-01      2019-01
3  124      DE  approved    2000 2019-01-07      2019-01
</code></pre>
","-1","Question"
"79568474","","<p>I am calculating a binary result of whether or not values in columns have at least one match. It may chance that they have multiple matches. I want to return a result of 1 if they have &gt;1 common value.</p>
<p>MRE below:</p>
<pre><code>df=pd.DataFrame({'col1':['a|b|c','a|b|c','a|b|c'],'col2':['b|d|e|a','d|e|f','a']})
df['new']=df[['col1','col2']].apply(lambda x: len(set(x['col1'].split('|')).intersection(set(x['col2'].split('|')))),axis=1)
</code></pre>
<p>I want to change the return of any value &gt;1 with 1, ie</p>
<pre><code>pd.DataFrame({'col1':['a|b|c','a|b|c','a|b|c'],'col2':['b|d|e|a','d|e|f','a'],'new':[1,0,1]})
</code></pre>
<p>SOLVED</p>
<pre><code>df['new']=df[['col1','col2']].apply(lambda x: 1 if len(set(x['col1'].split('|')).intersection(set(x['col2'].split('|')))) &gt;=1 else 0,axis=1)
</code></pre>
","0","Question"
"79569195","","<p>Here is my dataframe</p>
<pre><code>my_df = pd.DataFrame({'col_1': ['A', 'A', 'B', 'B', 'C', 'C'],
                           'col_2': [1, 2, 1, 2, 1, 2]})
</code></pre>
<p>I would like to group by <code>col_1</code> and filter out anything strictly greater than one using <code>col_2</code>. The final result should look like:</p>
<pre><code>final_df = pd.DataFrame({'col_1': ['A',  'B',  'C'],
                               'col_2': [1,  1,  1, ]})
</code></pre>
<p>Here is what I tried:</p>
<pre><code>df_ts = my_df.groupby('col_1').filter(lambda x: (x['col_2'] &lt;= 1).any())
</code></pre>
<p>It returns the same dataframe</p>
<p>I also tried:</p>
<pre><code>df_ts = my_df.groupby('col_1').filter(lambda x: x['col_2'] &lt;= 1)
</code></pre>
<p>It generates error.</p>
","-2","Question"
"79569269","","<p>I am a newbie to Python and pandas and would appreciate any help I can get. I have the below code and would like to know whether there is a more efficient way to write it to improve performance. I tried using cumsum but it does not give me the same output.</p>
<p>Context: I need to calculate the total vesting_value_CAD for each trancheID by summing all trancheIDs having the same employeeID, groupName, and vesting_year with agreementDate &lt;= the current trancheID's agreementDate, excluding the current row.</p>
<p>Code:</p>
<pre><code>import pandas as pd
from datetime import datetime

# Create the dataframe
data = {
    'employeeID': [2, 2, 2, 2, 2, 2, 2],
    'groupName': ['A', 'A', 'A', 'A', 'A', 'B', 'A'],
    'agreementID': [7, 7, 1, 1, 8, 9, 6],
    'agreementDate': ['3/1/2025', '3/1/2025', '4/1/2025', '3/1/2025', '2/1/2025', '3/1/2025', '3/1/2025'],
    'trancheID': [28, 29, 26, 27, 30, 31, 32],
    'vesting_year': [2025, 2026, 2026, 2027, 2026, 2026, 2026],
    'vesting_value_CAD': [200, 300, 400, 500, 50, 30, 40]
}

df = pd.DataFrame(data)

# Convert agreementDate to datetime
df['agreementDate'] = pd.to_datetime(df['agreementDate'], format='%m/%d/%Y')

# Function to calculate total vesting_value_CAD for each trancheID
def calculate_total_vesting_value(row):
    # Filter the dataframe based on the conditions
    filtered_df = df[(df['employeeID'] == row['employeeID']) &amp; 
                     (df['groupName'] == row['groupName']) &amp; 
                     (df['vesting_year'] == row['vesting_year']) &amp; 
                     (df['agreementDate'] &lt;= row['agreementDate']) &amp; 
                     (df['trancheID'] != row['trancheID'])]
    
    # Calculate the sum of vesting_value_CAD
    total_vesting_value = filtered_df['vesting_value_CAD'].sum()
    
    return total_vesting_value

# Apply the function 
df['total_vesting_value_CAD'] = df.apply(calculate_total_vesting_value, axis=1)


print(df)
</code></pre>
","4","Question"
"79569500","","<p>I have a pandas dataframe <code>df</code>. It has multi-index with Gx.Region and Scenario_Model.
The Scenario_Model index is ordered in alphabetical order des, pes, tes. When I plot it, it comes in the same order. However, I want to reorder it as pes, tes and des, and plot it accordingly. Is it possible to achieve it in Python pandas dataframe?</p>
<pre><code>    dict = {('Value', 2023, 'BatteryStorage'): {('Central Africa', 'des'): 0.0,
  ('Central Africa', 'pes'): 0.0,
  ('Central Africa', 'tes'): 0.0,
  ('Eastern Africa', 'des'): 0.0,
  ('Eastern Africa', 'pes'): 0.0,
  ('Eastern Africa', 'tes'): 0.0,
  ('North Africa', 'des'): 0.0,
  ('North Africa', 'pes'): 0.0,
  ('North Africa', 'tes'): 0.0,
  ('Southern Africa', 'des'): 504.0,
  ('Southern Africa', 'pes'): 100.0,
  ('Southern Africa', 'tes'): 360.0,
  ('West Africa', 'des'): 0.0,
  ('West Africa', 'pes'): 0.0,
  ('West Africa', 'tes'): 0.0},
 ('Value', 2023, 'Biomass PP'): {('Central Africa', 'des'): 0.0,
  ('Central Africa', 'pes'): 0.0,
  ('Central Africa', 'tes'): 0.0,
  ('Eastern Africa', 'des'): 40,
  ('Eastern Africa', 'pes'): 10,
  ('Eastern Africa', 'tes'): 50,
  ('North Africa', 'des'): 0.0,
  ('North Africa', 'pes'): 0.0,
  ('North Africa', 'tes'): 0.0,
  ('Southern Africa', 'des'): 90.0,
  ('Southern Africa', 'pes'): 43.0,
  ('Southern Africa', 'tes'): 50.0,
  ('West Africa', 'des'): 200.0,
  ('West Africa', 'pes'): 150.0,
  ('West Africa', 'tes'): 100}}

df_sample = pd.DataFrame.from_dict(dict)
df_sample.plot(kind = &quot;bar&quot;,
               stacked = True)
</code></pre>
<p><a href=""https://i.sstatic.net/DUTRGb4E.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DUTRGb4E.png"" alt=""enter image description here"" /></a></p>
","3","Question"
"79569831","","<p>I have a dataset that I need to pull a random sample from. This is the data:</p>
<pre><code>{'character': {0: 'mario',
  1: 'luigi',
  2: 'yoshi',
  3: 'peach',
  4: 'bowser',
  5: 'boo',
  6: 'toad',
  7: 'lakitu',
  8: 'koopa'},
 'location': {0: 'castle',
  1: 'castle',
  2: 'forest',
  3: 'castle',
  4: 'dungeon',
  5: 'dungeon',
  6: 'forest',
  7: 'forest',
  8: 'dungeon'},
 'color': {0: 'red',
  1: 'green',
  2: 'green',
  3: 'yellow',
  4: 'blue',
  5: 'white',
  6: 'blue',
  7: 'yellow',
  8: 'green'}}
</code></pre>
<p>Which I have arranged a data frame named df1.</p>
<p>What I need to do is pull a random sample of characters from the dataframe, making sure that the random sample has at least 2 (if possible) characters from each location and at least 1 (if possible) characters from each color</p>
<p>Ordinarily I would use df1.sample(n = 2), but in this case there are two criteria that need to be met, and n = 2 only applies to the location criteria, not the color criteria</p>
<p>Is it possible to do all of this in one function?</p>
<p>Thanks!</p>
","1","Question"
"79570345","","<p><em>I have climate data dataframe in cs format but no headers, so I read data as follows:</em></p>
<pre><code>    df_MERRA_data = pd.read_csv(fname,delimiter=',',header=None)
    df_MERRA_data.columns = \
        ['Date-time','Temperature','Wind speed','Sun   shine','Precipitation','Relative Humidity']

</code></pre>
<p><em>Then I create an index:</em></p>
<pre><code>    def extract_YYYYMMDD(Date_val):
        year_val = np.rint(Date_val/1000000).astype(int)
        month_val = np.rint(Date_val/10000).astype(int) - year_val*100
        day_val = np.rint(Date_val/100).astype(int) - year_val*10000 - month_val*100
        hour_val = np.rint(Date_val).astype(int) - year_val*1000000 - month_val*10000 - day_val*100
        return year_val, month_val, day_val, hour_val

    year, month, day, hour = extract_YYYYMMDD(df_MERRA_data['Date-time'])
    df_MERRA_data['Year'] = year
    df_MERRA_data['Month'] = month
    df_MERRA_data['Day'] = day
    df_MERRA_data['Hour'] = hour

</code></pre>
<h1>Now make an index based on date and time</h1>
<pre><code>    df_MERRA_data['Date-time'] =   pd.to_datetime(df_MERRA_data[['Year','Month','Day','Hour']], errors='coerce')\
    .dt.strftime('%Y-%m-%d %H')
    df_MERRA_data.set_index(['Date-time'], inplace=True)

*Then later when I want to extract data on the index I get the error:*

    df_MERRA_year = df_MERRA_data['Date-time'].dt.year

KeyError: 'Date-time'

</code></pre>
<p><em>These are the top three rows in the dataframe:</em></p>
<pre><code>    df_MERRA_data.head(3)
</code></pre>
<p>Out[3]:</p>
<pre><code>               Temperature  Wind speed  Sun shine  ...  Month  Day  Hour
Date-time                                          ...                  
1985-01-01 00         17.5        14.4         87  ...      1    1     0
1985-01-01 01         17.3        14.4         88  ...      1    1     1
1985-01-01 02         17.1        14.4         88  ...      1    1     2

[3 rows x 9 columns]
</code></pre>
<p>Can someone please help me to successfully access the records on the index so that I can determine yearly, monthly, weekly and daily averages, maximum, minmum, etc</p>
<p>Thank you in advance, I am a Pandas Novice.</p>
<p>I have changed the Date-time column name to Date_time and datetime with no success, I have also tried the code without</p>
<pre><code>    df_MERRA_data.set_index(['Date-time'], inplace=True)
</code></pre>
<p>Thank you</p>
","1","Question"
"79570508","","<p>I have column <code>column1</code> of type string with values like <code>&quot;some1,some2,some3&quot;</code>. I need to create boolean columns based on <code>column1</code> like: <code>some1</code>, <code>some2</code>, <code>some3</code>.</p>
<p>For example, I have the dataframe with one column <code>column1</code>:</p>
<pre><code>column1            |
-------------------+
&quot;some1,some2,some3&quot;|
&quot;some2,some3&quot;      |
&quot;some1&quot;            |
&quot;some1,some3&quot;      |
</code></pre>
<p>I need to get the dataframe with three boolean columns:</p>
<pre><code>some1 | some2 | some3 |
------+-------+-------+
True  | True  | True  |
False | True  | True  |
True  | False | False |
True  | False | True  |
</code></pre>
","1","Question"
"79570561","","<p>I'm facing an issue while pushing data into Excel using Python.
I need to write a string like &quot;4/10&quot; to a cell, but Excel automatically converts it into a date.
However, &quot;4/10&quot; is not a date in my case — it's the name of a material, and it must stay exactly as &quot;4/10&quot;.
I'm using pandas, xlwings, and openpyxl.
How can I force Excel to treat the value as a plain string and prevent it from being converted into a date?
Thanks in advance!
i tried <code>str()</code> and some other things, but didn't workde</p>
","0","Question"
"79570591","","<p>I'm trying to load a 10GB csv from <a href=""https://world.openfoodfacts.org/"" rel=""nofollow noreferrer"">openfoodfacts</a> -&gt; exact csv is <a href=""https://world.openfoodfacts.org/data/en.openfoodfacts.org.products.csv"" rel=""nofollow noreferrer""><code>en.openfoodfacts.org.products.csv</code></a>.</p>
<p>If I try directly to read the CSV using pandas:</p>
<pre><code>df_open_food = pd.read_csv('fr.openfoodfacts.org.products.csv')
</code></pre>
<p>I get the following message error:</p>
<pre><code>ParserError                               Traceback (most recent call last)
Cell In[2], line 2
      1 # Loading CSV file (file coming from data.gouv)
----&gt; 2 df_open_food = pd.read_csv('fr.openfoodfacts.org.products.csv')

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)
   1013 kwds_defaults = _refine_defaults_read(
   1014     dialect,
   1015     delimiter,
   (...)   1022     dtype_backend=dtype_backend,
   1023 )
   1024 kwds.update(kwds_defaults)
-&gt; 1026 return _read(filepath_or_buffer, kwds)

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\pandas\io\parsers\readers.py:626, in _read(filepath_or_buffer, kwds)
    623     return parser
    625 with parser:
--&gt; 626     return parser.read(nrows)

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\pandas\io\parsers\readers.py:1923, in TextFileReader.read(self, nrows)
   1916 nrows = validate_integer(&quot;nrows&quot;, nrows)
   1917 try:
   1918     # error: &quot;ParserBase&quot; has no attribute &quot;read&quot;
   1919     (
   1920         index,
   1921         columns,
   1922         col_dict,
-&gt; 1923     ) = self._engine.read(  # type: ignore[attr-defined]
   1924         nrows
   1925     )
   1926 except Exception:
   1927     self.close()

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\pandas\io\parsers\c_parser_wrapper.py:234, in CParserWrapper.read(self, nrows)
    232 try:
    233     if self.low_memory:
--&gt; 234         chunks = self._reader.read_low_memory(nrows)
    235         # destructive to chunks
    236         data = _concatenate_chunks(chunks)

File parsers.pyx:838, in pandas._libs.parsers.TextReader.read_low_memory()

File parsers.pyx:905, in pandas._libs.parsers.TextReader._read_rows()

File parsers.pyx:874, in pandas._libs.parsers.TextReader._tokenize_rows()

File parsers.pyx:891, in pandas._libs.parsers.TextReader._check_tokenize_status()

File parsers.pyx:2061, in pandas._libs.parsers.raise_parser_error()

ParserError: Error tokenizing data. C error: Expected 52 fields in line 3, saw 95
</code></pre>
<h2>Solutions' I've tried:</h2>
<p>I've re-used a part of the code from
<a href=""https://fleuryc.github.io/oc_ingenieur-ia_P3-Preparez-des-donnees-pour-un-organisme-de-sante-publique/notebook.html"" rel=""nofollow noreferrer"">https://fleuryc.github.io/oc_ingenieur-ia_P3-Preparez-des-donnees-pour-un-organisme-de-sante-publique/notebook.html</a>
but I encounter memory issue, here under code + error message</p>
<pre><code>import os.path
import pandas as pd

# define csv path
local_path = os.getcwd() + '\\'
csv_filename = 'fr.openfoodfacts.org.products.csv'
csv_path = local_path + csv_filename

# define name of corrected file
corrected_file = 'OFF_corrected.csv'
corrected_file_path = local_path + corrected_file

# Apply correction and save it
if not os.path.isfile(clean_local_path):
    with open(csv_local_path, 'r',encoding='utf-8') as csv_file, open(clean_local_path, 'w') as clean_file:
        data= csv_file.read()
        clean_file.write(data.replace('\n\t', '\t'))


MemoryError                               Traceback (most recent call last)
Cell In[14], line 3
      1 if not os.path.isfile(clean_local_path):
      2     with open(csv_local_path, 'r',encoding='utf-8') as csv_file, open(clean_local_path, 'w') as clean_file:
----&gt; 3         data= csv_file.read()
      4         clean_file.write(data.replace('\n\t', '\t'))

File &lt;frozen codecs&gt;:325, in decode(self, input, final)

MemoryError:
</code></pre>
<p>I was looking to apply chunk approach in order to replace issues per batch and save it as a new file using write with mode='a'
Hereunder, my try</p>
<pre><code>import os.path
import pandas as pd

# define csv path
local_path = os.getcwd() + '\\'
csv_filename = 'fr.openfoodfacts.org.products.csv'
csv_path = local_path + csv_filename

# define the name of the file where I will record dataset corrected
corrected_file = 'OFF_corrected.csv'
corrected_file_path = local_path + corrected_file

    # Apply correction and save it
if not os.path.isfile(clean_local_path):
    with open(csv_local_path, 'r',encoding='utf-8') as csv_file, open(clean_local_path, 'a') as clean_file:
        chunk_size=300
        for chunk in csv_file (chunksize=chunk_size):
            clean_file.write(chunk.replace('\n\t', '\t'))
</code></pre>
<p>Error message</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[12], line 4
      2 with open(csv_local_path, 'r',encoding='utf-8') as csv_file, open(clean_local_path, 'a') as clean_file:
      3     chunk_size=300
----&gt; 4     for chunk in csv_file (chunksize=chunk_size):
      5         clean_file.write(chunk.replace('\n\t', '\t'))

TypeError: '_io.TextIOWrapper' object is not callable
</code></pre>
<blockquote>
<p>Solution found:using &quot;for&quot; loop and mode = append</p>
</blockquote>
<pre><code>if not os.path.isfile(clean_local_path): 
    with open(csv_local_path, 'r',encoding='utf-8') as csv_file, open(clean_local_path, 'a', encoding='utf-8') as clean_file: 
        for row in csv_file: 
            clean_file.write(row.replace('\n\t', '\t'))
</code></pre>
","-3","Question"
"79572227","","<p>I'm using Pandera to define a schema for a pandas DataFrame where the index represents calendar dates (without time). I want to type-annotate the index as holding datetime.date values. Here's what I tried:</p>
<pre><code># mypy.ini
[mypy]
plugins = pandera.mypy
</code></pre>
<pre class=""lang-py prettyprint-override""><code># schema.py
from datetime import date
import pandera as pa
from pandera.typing import Index

class DateIndexModel(pa.DataFrameModel):
    date: Index[date]
</code></pre>
<p>But running <code>mypy</code> gives the following error:</p>
<pre><code>error: Type argument &quot;date&quot; of &quot;Index&quot; must be a subtype of &quot;bool | int | str | float | ExtensionDtype | &lt;30 more items&gt;&quot;  [type-var]
Found 1 error in 1 file (checked 1 source file)
</code></pre>
<p>I know that <code>datetime64[ns]</code> or <code>pandas.Timestamp</code> work fine, but I specifically want to model just <strong>dates without time</strong>. Is there a type-safe way to do this with <code>Pandera</code> and <code>mypy</code>?</p>
<p>Any workaround that lets me enforce date-only index semantics (with or without <code>datetime.date</code>) while keeping <code>mypy</code> happy?</p>
<p>Colab example notebook:<br />
<a href=""https://colab.research.google.com/drive/1AdiztxHlyvEMo6B3CzYnvzlnh6a0GfUQ?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1AdiztxHlyvEMo6B3CzYnvzlnh6a0GfUQ?usp=sharing</a></p>
","1","Question"
"79573037","","<p>So I do not know how to save the csv which this code creates to a specific folder in my directory. Any help would be appreciated!</p>
<pre class=""lang-py prettyprint-override""><code>#Store rows which do not conform to the relationship in a new dataframe
subset = df[df['check_total_relationship'] == False]`

subset.to_csv('false_relationships.csv', index=False, header=True, encoding='utf-8')`
</code></pre>
","1","Question"
"79573192","","<p>Doing the same operation using plain <code>numpy</code>, the <code>numpy_nullable</code> backend and the <code>pyarrow</code> backend shows that <code>numpy_nullable</code> behaves differently than the other two and in a rather counter-intuitive way.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; sr = pd.Series([1.5, 0.0])
&gt;&gt;&gt; (sr / sr).max()
1.0
&gt;&gt;&gt; sr = pd.Series([1.5, 0.0]).convert_dtypes()
&gt;&gt;&gt; (sr / sr).max()
&lt;NA&gt;
&gt;&gt;&gt; sr = pd.Series([1.5, 0.0]).convert_dtypes(dtype_backend='pyarrow')
&gt;&gt;&gt; (sr / sr).max()
1.0
</code></pre>
<p>Is there a good reason, or at least an explanation, for this inconsistent behavior? Is there a way around it or should we simply avoid the <code>numpy_nullable</code> backend?</p>
","1","Question"
"79574803","","<p>I have a dataset that is structured as a multiindex in pandas, with first index &quot;Date&quot; and second index &quot;TagId&quot;. So I would access the data by using: df.loc['01/06/2022'].loc['0x24025F44AD21'][...]</p>
<p>Here is a snippet of what that looks like:</p>
<pre class=""lang-py prettyprint-override""><code>                                  X         Y                    Time
Date       TagId                                                     
01/06/2022 0x24025F44AD21  3.121672  5.188564 1900-01-01 08:37:17.000
           0x24025F44AD21  3.121672  5.188564 1900-01-01 08:37:17.200
           0x24025F44AD21  3.121672  5.188564 1900-01-01 08:37:17.400
           0x24025F44AD21  3.121672  5.188564 1900-01-01 08:37:17.600
           0x24025F44AD21  3.121672  5.188564 1900-01-01 08:37:17.800
                           ...       ...                     ...
           0x24025F44AD21  [......]  [......] 1900-01-01 11:59:26.400

           0x24046130B076  [......]  [......] 1900-01-01 08:58:10:200
                           ...       ...                     ...
           0x24046130B076  7.611438  2.346377 1900-01-01 12:31:05.800
           0x24046130B076  7.611438  2.346377 1900-01-01 12:31:06.000
           0x24046130B076  7.533170  2.035381 1900-01-01 12:31:06.200
           0x24046130B076  7.533170  2.035381 1900-01-01 12:31:06.400
</code></pre>
<p>I want an easy and efficient way to make sure all the first and last times (from the &quot;Time&quot; Columns) lign up. So, if you will, I want to find the latest first time and the earliest last time so that I can trim the other datasets accordingly.</p>
<p>Additionally, which you can't see in the data above, there is a gap in the data. A break of sorts, where no measurements were taken. I want those &quot;breaks&quot; to align as well.</p>
<p>So my goal is that the output of the above dataset looks something like this:</p>
<pre class=""lang-py prettyprint-override""><code>                                  X         Y                    Time
Date       TagId                                                     
01/06/2022 0x24025F44AD21  [......]  [......]  1900-01-01 08:58:10:200
                           ...       ...                     ...
           0x24025F44AD21  [......]  [......]  1900-01-01 11:59:26.400

           0x24046130B076  [......]  [......]  1900-01-01 08:58:10:200
                           ...       ...                     ...
           0x24025F44AD21  [......]  [......]  1900-01-01 11:59:26.400
</code></pre>
<p>(Here I used [......] because I don't know what these values are. It is mainly about the time column)</p>
<p>It would be great if somebody could help me. Thanks in advance!</p>
","0","Question"
"79575173","","<p>I have a CSV file that I am reading to a Pandas dataframe. I am then writing the dataframe to a Microsoft Delta table in a Fabric Lakehouse. The CSV file has multiple string columns that hold datetimes in the format <code>DD/MM/YYYY HH:MM:SS</code>.</p>
<p>The inital CSV column data (Reported);</p>
<pre><code>19/02/2025 00:00:00
</code></pre>
<p>When I create the dataframe and do the conversion using;</p>
<pre><code>df_pandas['Reported'] = pd.to_datetime(df_pandas['Reported'])
</code></pre>
<p>I get the output of;</p>
<pre><code>2025-02-19 00:00:00
</code></pre>
<p>However, when I write the data frame to a delta table, it changes the datatype from datetime to complex and outputs the data as this:</p>
<pre><code>4/2/2024 12:00:00 AM
</code></pre>
<p>What would I need to do to retain the datetime format when writing it to a Delta Table? I am using the delta-rs library with Pandas instead of Spark.</p>
","1","Question"
"79575741","","<p>Give the following dataframe:</p>
<pre><code>_BETTER   _SAME     _WORSE    ___dataset     Metric
0.373802  0.816794  0.568783      Train      precision
0.391304  0.865229  0.519324      Train      recall
0.382353  0.840314  0.542929      Train      f1-score
0.500000  1.000000  0.583333      Val        precision
0.333333  1.000000  0.736842      Val        recall
0.400000  1.000000  0.651163      Val        f1-score
0.000000  0.000000  0.666667      Test       precision
0.000000  0.000000  0.500000      Test       recall
0.000000  0.000000  0.571429      Test       f1-score
</code></pre>
<p>would like to add the followings:</p>
<pre><code>_BETTER   _SAME     _WORSE    ___dataset     Metric
0.373802  0.816794  0.568783      Train      precision
0.391304  0.865229  0.519324      Train      recall
0.382353  0.840314  0.542929      Train      f1-score
0.500000  1.000000  0.583333      Val        precision
0.333333  1.000000  0.736842      Val        recall
0.400000  1.000000  0.651163      Val        f1-score
0.000000  0.000000  0.666667      Test       precision
0.000000  0.000000  0.500000      Test       recall
0.000000  0.000000  0.571429      Test       f1-score
mean_p_b  mean_p_s  mean_p_w       All       precision_avg
mean_r_b  mean_r_s  mean_r_w       All       recall_avg
mean_f1_b mean_f1_s mean_f1_w      All       f1_score_avg
</code></pre>
<p>where <code>mean_p_b</code>  <code>mean_p_s</code>  <code>mean_p_w</code> is obtained by the average of the precision row, w.r.t the three colums, respectively. Likewise the <code>mean_r_b</code>  <code>mean_r_s</code>  <code>mean_r_w</code> and <code>mean_f1_b</code> <code>mean_f1_s</code> <code>mean_f1_w</code>.</p>
<p>Applying each separately:</p>
<pre><code>df_avg_precision[&quot;BETTER&quot;] = (df_train_precision['_BETTER'].values + df_val_precision['_BETTER'].values + df_test_precision['_BETTER'].values)/3
df_avg_precision[&quot;Metric&quot;] = &quot;precision_avg&quot;
df_avg_recall[&quot;BETTER&quot;] = (df_train_recall['_BETTER'].values + 
df_val_recall['_BETTER'].values + df_test_recall['_BETTER'].values)/3
df_avg_recall[&quot;Metric&quot;] = &quot;recall_avg&quot;
df_avg_f1[&quot;BETTER&quot;] = (df_train_f1['_BETTER'].values + 
df_val_f1['_BETTER'].values + df_test_f1['_BETTER'].values)/3
df_avg_f1[&quot;Metric&quot;] = &quot;f1_avg&quot;
</code></pre>
","0","Question"
"79576076","","<p>I'm new to Python and pandas, and I need some help with a problem I'm facing.</p>
<p>I have the following table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>employeeID</th>
<th>groupName</th>
<th>agreementID</th>
<th>agreementDate</th>
<th>vesting_value_CAD</th>
<th>expiryDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>A</td>
<td>8</td>
<td>2/1/2025</td>
<td>50</td>
<td>3/31/2025</td>
</tr>
<tr>
<td>2</td>
<td>A</td>
<td>7</td>
<td>3/1/2025</td>
<td>200</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>A</td>
<td>6</td>
<td>3/1/2025</td>
<td>40</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>A</td>
<td>1</td>
<td>1/1/2025</td>
<td>500</td>
<td>1/31/2025</td>
</tr>
<tr>
<td>2</td>
<td>B</td>
<td>9</td>
<td>3/1/2025</td>
<td>30</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I need to sum the vesting_value_CAD for each employee, group, and agreement date, where the expiryDate of other rows is greater than or equal to the current row's agreementDate. For example, for the first row (employeeID 2 in group A with agreementDate 2/1/2025), the sum of vesting_value_CAD should be 200 + 40, that is the sum of agreement IDs 7 and 6.</p>
<p>Could someone please guide me on how to achieve this without using a function or a for loop?</p>
<p>Thank you in advance for your help!</p>
","0","Question"
"79576300","","<p>I have a scatter plot. I want to add some metadata to each point in my scatter plot. looking at documentation , i found annotate function( matplotlib.pyplot.annotate) , has anyone use this or some other function , such that we can add metadata to each point or are there are similar libraries, like matplotlib, which can display metadata , on clicking/hovering individual points in the scatterplot?</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np

# Generate random data for x and y axes
x = np.random.rand(20)
y = np.random.rand(20)
colors = np.random.rand(20)
area = (30 * np.random.rand(20))**2  # point radii

# Create the scatter plot
plt.scatter(x, y, s=area, c=colors, alpha=0.5)

# Add labels and title
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Scatter Plot Example')

# Display the plot
plt.show()
</code></pre>
","1","Question"
"79577437","","<p>I have written this code just to read a csv file:</p>
<pre><code>import pandas as pd
df = pd.read_table('C:\\XXXXX\\Python_Learn\\pandas.csv')
print(df.to_string())
</code></pre>
<p>and output to this is coming as below:</p>
<pre class=""lang-none prettyprint-override""><code>  Date|Invoice ID|Customer Name|Product|Category|Quantity|Unit Price|Total Amount|Region|Salesperson
0      2025-04-01|INV1001|John Smith|Apple iPhone 14|Electronics|1|999.00|999.00|North|Alice Johnson
1           2025-04-01|INV1002|Jane Doe|Samsung TV 55|Electronics|2|600.00|1200.00|NULL|Bob Williams
2               2025-04-02|INV1003|Michael Lee|Nike Sneakers|Apparel|3|120.00|360.00|West|Carol Chen
3             2025-04-02|INV1004|Emma Brown|Office Chair|Furniture|1|150.00|150.00|South|David Patel
4          2025-04-03|INV1005|Olivia Green|HP Laptop|Electronics|1|850.00|850.00|North|Alice Johnson
5             2025-04-03|INV1006|Noah White|Dining Table|Furniture|1|450.00|450.00|NULL|Bob Williams
6                    2025-04-04|INV1007|Ava Scott|Levis Jeans|Apparel|2|80.00|160.00|West|Carol Chen
7             2025-04-04|INV1008|Liam Davis|AirPods Pro|Electronics|2|250.00|500.00|NULL|David Patel
</code></pre>
<p>Why is my output getting aligned to the right?</p>
<p>I added <code>.strip()</code> but it still gives same output.</p>
<p>Update: I updated my code with .read_csv() instead of .read_table() &amp; it improved the output in terms of alignment, but now it doesn't show characters on the right side of the file.</p>
<pre><code>import pandas as pd
df = pd.read_csv('C:\\XXXXX\\Python_Learn\\pandas.csv')
print(df)
</code></pre>
<p>Output as below,</p>
<pre><code>  Date|Invoice ID|Customer Name|Product|Category|Quantity|Unit Price|Total Amount|Region|Salesperson
0  2025-04-01|INV1001|John Smith|Apple iPhone 14|...
1  2025-04-01|INV1002|Jane Doe|Samsung TV 55|Elec...
2  2025-04-02|INV1003|Michael Lee|Nike Sneakers|A...
3  2025-04-02|INV1004|Emma Brown|Office Chair|Fur...
4  2025-04-03|INV1005|Olivia Green|HP Laptop|Elec...
5  2025-04-03|INV1006|Noah White|Dining Table|Fur...
6  2025-04-04|INV1007|Ava Scott|Levis Jeans|Appar...
7  2025-04-04|INV1008|Liam Davis|AirPods Pro|Elec...
</code></pre>
<p>Why does it show ... instead of reading content on right side of the file?</p>
","1","Question"
"79577490","","<p>I have a data set like the following and want to scale the data using any of the scalers in <code>sklearn.preprocessing</code>.</p>
<p>Is there an easy way to fit this scaler not over the whole data set, but per group? My current solution can't be included in a Pipeline:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

df = pd.DataFrame({'group': [1, 1, 1, 2, 2, 2], 'x': [1,2,3,10,20,30]})
def scale(x):
    # see https://stackoverflow.com/a/72408669/3104974
    scaler = StandardScaler()
    return scaler.fit_transform(x.values[:,np.newaxis]).ravel()

df['x_scaled'] = df.groupby('group').transform(scale)
</code></pre>
<pre><code>   group   x  x_scaled
0      1   1 -1.224745
1      1   2  0.000000
2      1   3  1.224745
3      2  10 -1.224745
4      2  20  0.000000
5      2  30  1.224745
</code></pre>
","2","Question"
"79578293","","<p>I am looking for guidance on how to check for missing values in a DataFrame that are not the typical &quot;NaN&quot; or &quot;np.nan&quot; in Python. I have a dataset/DataFrame that has a string literal &quot;?&quot; representing missing data. How can I identify this string as a missing value?</p>
<p>When I run usual commands using Pandas like:</p>
<p><code>missing_values = df.isnull().sum()</code></p>
<p><code>print(missing_values[missing_values &gt; 0])</code></p>
<p>Python doesn't pick up on these cells as missing and returns 0s for the sum of null values. It also doesn't return anything for printing missing values &gt; 0.</p>
","1","Question"
"79578969","","<p>Sample dataframe:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame([[1, {'t': 1}], [2, {'t': 2}], [3, np.nan], [4, np.nan]], columns=['A', 'B'])
</code></pre>
<pre><code>   A         B
0  1  {'t': 1}
1  2  {'t': 2}
2  3       NaN
3  4       NaN
</code></pre>
<p>What I want:</p>
<pre><code>   A         B
0  1  {'t': 1}
1  2  {'t': 2}
2  3  {'t': 0}
3  4  {'t': 0}
</code></pre>
<p>For some reason, the below does not work -</p>
<pre><code>df.fillna({'B': {'t': 0}}, inplace=True)
</code></pre>
<p>Is there a restriction on what values can be used for fillna?</p>
<p>I have this solution which works,</p>
<pre><code>df['B'] = df['B'].mask(df['B'].isna(), {'t': 0})
</code></pre>
<p>but I want to know if there is a better way to do this? For example, this fails if 'B' does not exist in the dataframe, whereas the fillna doesn't fail in similar case</p>
","0","Question"
"79580309","","<p>I want to divide a large dataset into 12-hour periods of data starting/ending at midnight and noon each day.</p>
<p>I was planning to use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Period.html"" rel=""nofollow noreferrer"">Pandas.Period</a> for this but I noticed that it converts an arbitrary datetime to a 12-hour period beginning in the current hour, whereas what I want is the 12-hour period starting at 00:00 or 12:00 hours.</p>
<pre><code>import pandas as pd

dt = pd.to_datetime(&quot;2025-04-17 18:35&quot;)
current_period = dt.to_period(freq='12h')
print(current_period)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>2025-04-17 18:00
</code></pre>
<p>What I want is the following period:</p>
<pre class=""lang-none prettyprint-override""><code>2025-04-17 12:00
</code></pre>
","0","Question"
"79580520","","<p>I have a function used by an application which returns two str. I want to assign the output values ​​to two columns of a dataframe. But I don't understand how to assign to get the values, I get NaN values.</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    'nombre': [10, 25, 42, 18, 73]
})
df[&quot;COMMENT&quot;] = df[&quot;S&quot;] = None

def verif(row):
    if row[&quot;nombre&quot;] % 10 == 0:
        c = &quot;Ends with 0&quot;
        e = &quot;OK&quot;
    else:
        c = &quot;Do not end with 0&quot;
        e = &quot;KO&quot;
    return pd.Series({&quot;e&quot;: e, &quot;c&quot;: c})

mask = df[&quot;nombre&quot;] % 2 == 0
df.loc[mask, [&quot;S&quot;, &quot;COMMENT&quot;]] = df.loc[mask].apply(verif, axis=1)
</code></pre>
","0","Question"
"79583054","","<p>I am trying to clone SCOTT.EMP table with TEST table. However datatypes are not matching.</p>
<p>Please advise how to create a clone of EMP with new Table &quot;TEST&quot; with Same datatype in python pandas.</p>
<pre><code>    DESC EMP:
    Name     Null?    Type         
    -------- -------- ------------ 
    EMPNO    NOT NULL NUMBER(4)    
    ENAME             VARCHAR2(10) 
    JOB               VARCHAR2(9)  
    MGR               NUMBER(4)    
    HIREDATE          DATE         
    SAL               NUMBER(7,2)  
    COMM              NUMBER(7,2)  
    DEPTNO            NUMBER(2) 

    import pandas as pd
    import oracledb
    from sqlalchemy import create_engine
    # create engine
    engine = create_engine(&quot;oracle+oracledb://scott:tiger@LAPTOP-QMH68LT9:1521?service_name=ORCLPDB&quot;)
    # query Oracle and put the result into a pandas Dataframe
    df= pd.read_sql('SELECT * FROM EMP', engine) 
    &gt;&gt;&gt; df_test = df.copy()
    &gt;&gt;&gt; df_test.to_sql('test', con=engine,if_exists='replace',index=False,dtype={'sal':sa.types.Float,'mgr':sa.types.Float,'comm':sa.types.Float})
    -1
       
    DESC TEST        
    Name     Null? Type       
    -------- ----- ---------- 
    EMPNO          NUMBER(19) 
    ENAME          CLOB       
    JOB            CLOB       
    MGR            FLOAT(126) 
    HIREDATE       DATE       
    SAL            FLOAT(126) 
    COMM           FLOAT(126) 
    DEPTNO         NUMBER(19) 
</code></pre>
<p>Observation:
EMPNO    :  NUMBER(4) is changed to NUMBER(19) - Is this default in Pandas df Precesion ? where we can get list of default values.</p>
<p>ENAME             VARCHAR2(10) : -&gt; Changed to CLOB</p>
<p>SAL               NUMBER(7,2)  : -&gt; Changed to FLOAT(126) . How we can load with Same Precesion/Scale value.</p>
","-1","Question"
"79583700","","<p>I'm playing around with the pipe <code>|</code> and ampersand <code>&amp;</code> operators, as well as the <code>.gt()</code> and <code>.lt()</code> built-in functions to see how they work together.</p>
<p>I'm looking at a column in a DataFrame with values from 0.00 to 1.00.</p>
<p>I can use the <code>&gt;</code>, <code>&lt;</code>, and <code>&amp;</code> operators together and find no problem, same with using <code>.gt()</code>, <code>.lt()</code>, and <code>&amp;</code>. However, if I try to chain <code>.gt().lt()</code> it gives another result.</p>
<p>In my example I'm using <code>.gt(0.7).lt(0.9)</code>, but this yields values &lt;=0.7. If I change the order to <code>.lt(0.9).gt(0.7)</code>, I get values &lt;=0.9.</p>
<p>I can always just write it like this <code>df['column'].gt(0.7)&amp;df['column'].lt(0.9)</code>, just wondering if there's a way of chaining .gt().lt()</p>
","3","Question"
"79584468","","<p>I have a collection of user data as follows:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">user</th>
<th style=""text-align: center;"">start</th>
<th style=""text-align: center;"">end</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">John Doe</td>
<td style=""text-align: center;"">2025-03-21 11:30:35</td>
<td style=""text-align: center;"">2025-03-21 13:05:26</td>
</tr>
<tr>
<td style=""text-align: center;"">...</td>
<td style=""text-align: center;"">...</td>
<td style=""text-align: center;"">...</td>
</tr>
<tr>
<td style=""text-align: center;"">Jane Doe</td>
<td style=""text-align: center;"">2023-12-31 01:02:03</td>
<td style=""text-align: center;"">2024-01-02 03:04:05</td>
</tr>
</tbody>
</table></div>
<p>Each user has a start and end datetime of some activity. I would like to place this temporal range in the index so I can quickly query the dataframe to see which users were active during a certain date/time range like so:</p>
<pre class=""lang-py prettyprint-override""><code>df['2024-01-01:2024-01-31']
</code></pre>
<p>Pandas has <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Period.html"" rel=""nofollow noreferrer""><code>Period</code></a> objects, but these seem to only support a specific year, day, or minute, not an arbitrary start and end datetime. Pandas also has <a href=""https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.html"" rel=""nofollow noreferrer""><code>MultiIndex</code></a> indices, but these seem to be designed for hierarchical categorical labels, not for time ranges. Any other ideas for how to represent this time range in an index?</p>
","2","Question"
"79585017","","<p>I have two datasets from an animal shelter that include Animal IDs and dates. One set is for intakes, one is for outcomes. Some Animal IDs are duplicated, typically because the same animal has entered/exited the shelter multiple times. I want to add a column indicating how many times that animal has entered/exited so far (the dataframes are organized by date). How would I do this?</p>
<p>Example intake data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Animal ID</th>
<th>Intake Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>12/07/2017 02:07:00 PM</td>
</tr>
<tr>
<td>2</td>
<td>12/08/2017 01:10:00 PM</td>
</tr>
<tr>
<td>1</td>
<td>01/06/2018 12:03:00 PM</td>
</tr>
<tr>
<td>3</td>
<td>01/07/2018 01:10:00 PM</td>
</tr>
<tr>
<td>2</td>
<td>01/08/2018 04:15:00 PM</td>
</tr>
</tbody>
</table></div>
<p>What I want is to add this column to each dataframe:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Animal ID</th>
<th>Intake Date</th>
<th>Shelter Visits</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>12/07/2017 02:07:00 PM</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>12/08/2017 01:10:00 PM</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>01/06/2018 12:03:00 PM</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>01/07/2018 01:10:00 PM</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>01/08/2018 04:15:00 PM</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>This would be to clarify join conditions when I want to combine the datasets (match each Animal ID not only with itself, but with what visit it represents). How do I do this?</p>
","0","Question"
"79585207","","<p>I'd like to use a <code>fillna</code> command to fillna multiple columns of a Pandas dataframe with the same series, matching by index:</p>
<pre><code>import numpy as np
import pandas as pd

df_1 = pd.DataFrame(index = [0, 1, 2], columns = ['a', 'b', 'c'], data = [[1, np.NaN, np.NaN], [2, np.NaN, 4], [np.NaN, 2, -3]])
series_1 = pd.Series([6, 5, 4], index = [0, 1, 2])

df_2 = df_1.fillna(series_2) # ???
</code></pre>
<p>Desired output is</p>
<pre><code>df_2 = pd.DataFrame(index = [0, 1, 2], columns = ['a', 'b', 'c'], data = [[1, 6, 6], [2, 5, 4], [4, 2, -3]])
</code></pre>
<p>Is there a pythonic way to do this without individually <code>fillna</code>ing every column?</p>
","2","Question"
"79585340","","<p>Let's say I have the following dataframe:</p>
<pre><code>+----+------------------------------------------------+-------------+----------+----------+
|    | String                                         | Substring   | Result 1 | Result 2 |
+----+------------------------------------------------+-------------+----------+----------+
|  0 | fooDisplay &quot;screen 1&quot; other text               | Screen 1    |          |          |
|  1 | foobar Display &amp;quot;Screen2&amp;quot; more text   | GFX         |          |          |
|  2 | barDisplay &amp;quot;Screen2&amp;quot;useless text     | Screen 2    |          |          |
|  3 | Link=&quot;Screen 1&quot;                                | Screen 1    |          |          |
+----+------------------------------------------------+-------------+----------+----------+
</code></pre>
<p>I am trying to achieve the following:</p>
<pre><code>+----+------------------------------------------------+-------------+----------+----------+
|    | String                                         | Substring   | Result 1 | Result 2 |
+----+------------------------------------------------+-------------+----------+----------+
|  0 | foo&quot;Display screen 1&quot; other text               | Screen 1    | True     | False    |
|  1 | foobar Display &amp;quot;Screen2&amp;quot; more text   | GFX         | False    | False    |
|  2 | barDisplay &amp;quot;Screen2&amp;quot;useless text     | Screen 2    | False    | True     |
|  3 | Link=&quot;Screen 1&quot;                                | Screen 1    | False    | False    |
+----+------------------------------------------------+-------------+----------+----------+
</code></pre>
<p>From a pseudocode standpoint, this is what I am trying to do:</p>
<pre><code>search_string = 'Display ' + row['Substring']
if row['String'].containsCaseInsensitive search_string:
    row['Result 1'] = True
else:
    row['Result 1'] = False

search_string2 = 'Display &amp;quot;' + row['Substring'] + '&amp;quot;'
if row['String'].containsCaseInsensitive search_string2:
    row['Result 2'] = True
else:
    row['Result 2'] = False
</code></pre>
<p>I know that I can accomplish this by iterating through each row, but I am trying to be a good Python user and avoid that as I have hundreds of thousands of rows in my dataframe, and multiple &quot;Result&quot; columns (upwards of 10) in some cases.</p>
<p>I see plenty of examples online where &quot;static&quot; substrings are located within a dataframe string, however, I can't seem to find an example that uses the contents of a dataframe cell as the substring used to search another cell in the same row. Here is one such example: <a href=""https://stackoverflow.com/questions/51026771/pandas-case-insensitive-in-a-string-or-case-ignore"">pandas-case-insensitive-in-a-string-or-case-ignore</a></p>
<p>So, how do I efficiently update dataframe rows using a combination of dynamic and static queries for a given row?</p>
","2","Question"
"79585543","","<p>I'm working with a very large CSV file (around 10GB) that doesn't fit into my computer's memory. When I try to load it into a pandas DataFrame using <code>pd.read_csv()</code>, I get a <code>MemoryError</code>.</p>
<p>What's the most efficient way to process this file using pandas without loading the entire thing into memory at once? I need to perform operations like:</p>
<ul>
<li>Calculating the sum of a specific column.</li>
<li>Filtering rows based on certain conditions.</li>
<li>Grouping and aggregating data.</li>
</ul>
<p>My code:</p>
<pre><code>import pandas as pd
import numpy as np

# Simulate a large CSV file
with open('large_file.csv', 'w') as f:
    for i in range(10000000):  # 10 million rows
        f.write(f'{i},{np.random.rand()},{np.random.randint(0, 10)}\n')


# This line causes a MemoryError
df = pd.read_csv('large_file.csv', names=['id', 'value', 'category'])

# Desired operations (example)
# total_value = df['value'].sum()
# filtered_df = df[df['category'] &gt; 5]
# grouped_df = df.groupby('category')['value'].mean()

print(&quot;Processing Complete&quot;) # Never Reaches Here.
</code></pre>
<p>I've tried using <code>chunksize</code> in <code>pd.read_csv()</code>, but I'm not sure how to efficiently perform the desired operations on the chunks and combine the results. Are there other techniques or libraries I should consider?</p>
","1","Question"
"79587011","","<p>I'm running physical simulations and storing the data for later analysis.  Up to this point, I've been storing the data as a pickled data frame, and manipulating it using pandas in jupyter lab.  However, as my simulations are getting larger, unpickling the entire dataset is making the notebook run very slowly.  Often, I only need a subset of the data - a few columns.  I've resorted to saving some of the datasets as csv's, since I can import specific columns from that format. The full datasets can be in the 1-2 GB range - potentially larger in the future, but that's the extent right now.</p>
<p>Is there a format, short of a database, that can allow quick access to a specified subset of a large data frame?  Or is a database the way to go?</p>
","1","Question"
"79587477","","<p>I’m working on a data analysis project using Python. I want to check which columns have missing (null) values and how many. How can I do this in Pandas?</p>
","-1","Question"
"79588678","","<p>I have a large Excel spreadsheet. I'm only interested in certain columns. Furthermore, I'm only interested in rows where specific columns meet certain criteria.</p>
<p>The following works:</p>
<pre><code>import pandas as pd
import warnings

# this suppresses the openpyxl warning that we're seeing
warnings.filterwarnings(&quot;ignore&quot;, category=UserWarning, module=&quot;openpyxl&quot;)

# These are the columns we're interested in
COLUMNS = [
    &quot;A&quot;,
    &quot;B&quot;,
    &quot;C&quot;
]

# the source file
XL = &quot;source.xlsx&quot;
# sheet name in the source file
SHEET = &quot;Sheet1&quot;
# the output file
OUTPUT = &quot;target.xlsx&quot;
# the sheet name to be used in the output file
OUTSHEET = &quot;Sheet1&quot;

# This loads the entire spreadsheet into a pandas dataframe
df = pd.read_excel(XL, sheet_name=SHEET, usecols=COLUMNS).dropna()
# this replaces the original dataframe with rows where A contains &quot;FOO&quot;
df = df[df[&quot;A&quot;].str.contains(r&quot;\bFOO\b&quot;, regex=True)]
# now isolate those rows where the B contains &quot;BAR&quot;
df = df[df[&quot;B&quot;].str.contains(r&quot;\bBAR\b&quot;, regex=True)]
# output to the new spreadsheet
df.to_excel(OUTPUT, sheet_name=OUTSHEET, index=False)
</code></pre>
<p>This works. However, I can't help thinking that there might be a better way to manage the selection criteria especially if / when they get more complex.</p>
<p>Or is it a case of &quot;step-by-step&quot; is good?</p>
","1","Question"
"79589524","","<h2>The issue</h2>
<p>In SQL it is very easy to apply different aggregate functions to different columns, e.g. :</p>
<pre><code>select item, sum(a) as [sum of a], avg(b) as [avg of b], min(c) as [min of c]
</code></pre>
<p>In Python, not so much. For a simple groupby, this <a href=""https://stackoverflow.com/questions/66195952/pythonic-way-to-apply-different-aggregate-functions-to-different-columns-of-a-pa/66197013"">answer</a> provides an elegant and pythonic way to do it.</p>
<h2>Desired output</h2>
<p>The answer linked above shows calculations ( sum of a, weighted avg of b, etc) for each city.
Now I need to add another dimension - let's call it colour - and show the intersection / pivot / crosstab of city and colour. I want to create the two tables below:</p>
<p><a href=""https://i.sstatic.net/YjBXhUtx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YjBXhUtx.png"" alt=""enter image description here"" /></a></p>
<h2>What I have tried (with a toy example)</h2>
<p>Using <code>groupby()</code> and <code>unstack()</code> gives me Version 2. I am not sure how to obtain version 1 of the screenshot above. See toy example below.</p>
<pre><code>import pandas as pd
import numpy as np

np.random.seed(100) # so results are always the same
df = pd.DataFrame(columns =['a','b','c','d'], data = np.random.rand(300,4))
df['city'] = np.repeat(['London','New York','Buenos Aires'], 100)
df['colour'] = np.random.choice(['green','red'],300)


def func(x, df):
    # func() gets called within a lambda function; x is the row, df is the entire table   
    s_dict = {}

    s_dict['sum of a'] = x['a'].sum()
    s_dict['% of a'] = x['a'].sum() / df['a'].sum() if df['a'].sum() !=0 else np.nan
    
    s_dict['avg of b'] = x['b'].mean()
    
    s_dict['weighted avg of a, weighted by b'] = ( x['a'] * x['b']).sum() / x['b'].sum() if x['b'].sum() &gt;0 else np.nan
    
    s_dict['sum of c'] = x['c'].sum()
    s_dict['sum of d'] = x['d'].sum()
    
    return pd.Series( s_dict  ) 

out = df.groupby(['city','colour']).apply(lambda x: func(x,df)).unstack()
</code></pre>
<h2>Why this is not a duplicate question</h2>
<p>The answer linked above does not address the case of an additional dimension.</p>
<p>Most of the examples I have seen with pivot() and crosstab() do not contain custom functions, but more basic functions like count, sum, average. There are examples with custom functions, but not similar to my case, e.g. operating on one field only, not 2 (see <a href=""https://stackoverflow.com/questions/67304225/in-a-pandas-pivot-table-how-do-i-define-a-function-for-a-subset-of-data"">here</a> ) or without the bi-dimensional aspect (repeat the calculation for each colour) - e.g. <a href=""https://stackoverflow.com/questions/45440895/pandas-crosstab-with-own-function"">here</a>
If you have found a question which is more similar to my case, I'd appreciate it if you could first flag it in a comment or answer, as it is not uncommon for questions to be erroneously marked as duplicate. Thank you.</p>
","0","Question"
"79589664","","<p>I have weather data in a python dict that I'm trying to convert to a pandas df.  (From there I'll load it into SQLServer but I've got that part working)</p>
<pre><code>my_dict = {
'data.outdoor.temperature': {'unit': '℃', 'list': 
{'datetime.datetime(2025, 4, 23, 10, 0): 22.3', 
'datetime.datetime(2025, 4, 23, 14, 0): 21.3', 
'datetime.datetime(2025, 4, 23, 18, 0): 18.2', }}, 
'data.indoor.temperature': {'unit': '℃', 'list': 
{'datetime.datetime(2025, 4, 23, 10, 0): 23.2', 
'datetime.datetime(2025, 4, 23, 14, 0): 23.5', 
'datetime.datetime(2025, 4, 23, 18, 0): 22.9'}}}
</code></pre>
<p>The output I'm after is:</p>
<pre><code>outdoor.temperature | 2025_04_20 | 14 | 28.1 | ℃  
outdoor.temperature | 2025_04_20 | 18 | 23.8 | ℃  
.....etc  
indoor.temperature | 2025_04_20 | 14 | 23.5 | ℃  
indoor.temperature | 2025_04_20 | 18 | 23.8 | ℃  
.....etc
</code></pre>
<p>The list of <code>datetime</code> &amp; <code>temperatures</code> could be 100s of records</p>
<p>I've tried <code>from_dict</code>, <code>concat</code> / <code>unstack</code>, <code>normalize</code>, <code>flatten</code> and combinations of the lot but I'm beaten</p>
","-1","Question"
"79590117","","<p>When loading a csv file in pandas I've encountered the bellow error message:</p>
<pre class=""lang-none prettyprint-override""><code>DtypeWarning: Columns have mixed types. Specify dtype option on import  
or set low_memory=False
</code></pre>
<p>Reading online I found few solutions.</p>
<p>One, to set <code>low_memory=False</code>, but I understand that this is not a good practice and it doesn't really resolve the problem.</p>
<p>Second solution is to set a data type for each column (or each column with mixed data types):</p>
<pre><code>pd.read_csv(csv_path_name, dtype={'first_column': 'str', 'second_column': 'str'})
</code></pre>
<p>Again, from what I read, not the ideal solution if we have a big dataset.</p>
<p>Third solution - create a converter function.
To my understanding this might be the most appropriate solution. I found code which works for me, but I am trying to better understand <strong>what is this function exactly doing:</strong></p>
<pre><code>def convert_dtype(x):
    if not x:
        return ''
    try:
        return str(x)
    except:
        return ''

df = pd.read_csv(csv_path_name, converters={'first_col':convert_dtype, 'second_col':convert_dtype, etc.... } )
</code></pre>
<p>Can someone please explain the function code to me?</p>
<p>Thanks</p>
","1","Question"
"79591383","","<p>I have a Pandas Dataframe that I derive from a process like this:</p>
<pre><code>df1 = pd.DataFrame({'c1':['A','B','C','D','E'],'c2':[1,2,3,4,5]})
df2 = pd.DataFrame({'c1':['A','B','C'],'c2':[1,2,3],'c3': [np.array((1,2,3,4,5,6)),np.array((6,7,8,9,10,11)),np.full((6,),np.nan)]})
df3 = df1.merge(df2,how='left',on=['c1','c2'])
</code></pre>
<p>This looks like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>c1</th>
<th>c2</th>
<th>c3</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1</td>
<td><code>[1,2,3,4,5,6]</code></td>
</tr>
<tr>
<td>B</td>
<td>2</td>
<td><code>[6,7,8,9,10,11]</code></td>
</tr>
<tr>
<td>C</td>
<td>3</td>
<td><code>[nan,nan,nan,nan,nan,nan]</code></td>
</tr>
<tr>
<td>D</td>
<td>4</td>
<td><code>NaN</code></td>
</tr>
<tr>
<td>E</td>
<td>5</td>
<td><code>NaN</code></td>
</tr>
</tbody>
</table></div>
<p>In order to run the next step of my code, I need all of the arrays in <code>c3</code> to have a consistent length. For the inputs coming in that were present in the join (i.e. row 1 through 3) this was already taken care of. However, for the rows that were missing from <code>df2</code> where I now have only a single <code>NaN</code> value (rows 4 and 5) I need to replace those <code>NaN</code>'s with an array of <code>NaN</code> values like in row 3. The problem is that I can't figure out how to do that.</p>
<p>I've tried a number of things, starting with the obvious:</p>
<pre><code>df3.loc[pd.isnull(df3.c3),'c3'] = np.full((6,),np.nan)
</code></pre>
<p>Which gave me a</p>
<pre><code>ValueError: Must have equal len keys and value when setting with an iterable
</code></pre>
<p>Fair enough; I understand this error and why python is confused about what I'm trying to do. How about this?</p>
<pre><code>for i in df3.index:
    df3.at[i,'c3'] = np.full((6,),np.nan) if all(pd.isnull(df3.c3)) else df3.c3
</code></pre>
<p>That code runs without error but then when I go to print out df3 (or use it) I get this error:</p>
<pre><code>RecursionError: maximum recursion depth exceeded
</code></pre>
<p>That one I don't understand, but moving on, what if I preassign a column full of my NaN arrays and then I can do some logic after the join:</p>
<pre><code>for i in df1.index: df1.at[i,'c4'] = np.full((6,),np.nan)
</code></pre>
<p>This gives me the understandable error:</p>
<pre><code>ValueError: setting an array element with a sequence 
</code></pre>
<p>How about another variation of the same idea:</p>
<pre><code>df1['c4'] = np.full((6,),np.nan)
</code></pre>
<p>This one gives a different, also understandable error:</p>
<pre><code>ValueError: Length of values (6) does not match length of index (5)
</code></pre>
<p>Hence, the question: How do I replace values in my dataframe (in this case null values) with an empty numpy array of a given length?</p>
<p>For clarity, the desired final result is this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>c1</th>
<th>c2</th>
<th>c3</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1</td>
<td><code>[1,2,3,4,5,6]</code></td>
</tr>
<tr>
<td>B</td>
<td>2</td>
<td><code>[6,7,8,9,10,11]</code></td>
</tr>
<tr>
<td>C</td>
<td>3</td>
<td><code>[nan,nan,nan,nan,nan,nan]</code></td>
</tr>
<tr>
<td>D</td>
<td>4</td>
<td><code>[nan,nan,nan,nan,nan,nan]</code></td>
</tr>
<tr>
<td>E</td>
<td>5</td>
<td><code>[nan,nan,nan,nan,nan,nan]</code></td>
</tr>
</tbody>
</table></div>
","2","Question"
"79591534","","<p>I have a question very similar to <a href=""https://stackoverflow.com/questions/62607115/in-pandas-how-to-assign-the-result-of-a-groupby-aggregate-to-the-next-group-in"">this one</a> but 1) I want this to work for a Series also and 2) I don't have a column representing my grouping values; the values are in the index.</p>
<p>Imagine I have a Series which looks as follows:</p>
<pre><code>2023-08-01    1515000.08
2023-09-01    2629410.80
2023-10-01    2548748.40
2023-11-01    2494398.04
2023-12-01    3397805.34
2024-01-01    3285501.49
2024-02-01    3173978.74
2024-03-01    3139235.65
2024-04-01    2927895.84
2024-05-01    2750708.29
dtype: float64
</code></pre>
<p>I would like to create a new Series where the values represent <em>the value of the last month of the previous quarter</em>, like so:</p>
<pre><code>2023-08-01    NaN
2023-09-01    NaN
2023-10-01    2629410.80 &lt;--- the old value from 2023-09-01, which was the end of Q3
2023-11-01    2629410.80
2023-12-01    2629410.80
2024-01-01    3397805.34 &lt;--- the old value from 2023-12-01
2024-02-01    3397805.34
2024-03-01    3397805.34
2024-04-01    3139235.65 &lt;--- the old value from 2024-03-01
2024-05-01    3139235.65
dtype: float64
</code></pre>
<p>I am finding it very cumbersome to find an elegant solution here. My code looks something like:</p>
<pre><code>period_to_val = (
    series.groupby(
        lambda x: get_quarter(date=x)
    )
    .last()
    .shift()
)

data = series.index.map(
    lambda x: period_to_val[get_end_of_period(date=x, term_length=term_length, fiscal_year_start=fiscal_year_start)]
)

result = pd.Series(data=data, index=series.index)
</code></pre>
<p>But this feels too verbose and ugly. I want an operation on the <code>SeriesGroupBy</code> which is a mix between a <code>.shift()</code> and a <code>.transform(&quot;last&quot;)</code>, but such a thing seems not to exist.</p>
<p>Any ideas for how to improve? Thanks!</p>
","1","Question"
"79591713","","<p>Now I have a time<em>lon</em>lat 3D data where time is recorded as year, month and day. I need to split time in the form of year*month+day. So that the data becomes 4 dimensional. How should I do this?</p>
<p>I have given a simple data below:</p>
<pre class=""lang-py prettyprint-override""><code>import xarray as xr
import numpy as np
import pandas as pd

time = pd.date_range(&quot;2000-01-01&quot;, &quot;2001-12-31&quot;, freq=&quot;D&quot;)
time = time[~((time.month == 2) &amp; (time.day == 29))] 

lon = np.linspace(100, 110, 5)
lat = np.linspace(30, 35, 4)
data = np.random.rand(len(time), len(lon), len(lat))

da = xr.DataArray(
    data,
    coords={&quot;time&quot;: time, &quot;lon&quot;: lon, &quot;lat&quot;: lat},
    dims=[&quot;time&quot;, &quot;lon&quot;, &quot;lat&quot;],
    name=&quot;pr&quot;
)
</code></pre>
<p>except dim:</p>
<p>year: 2000, 2001</p>
<p>monthly: 01-01, 01-02,...12-31</p>
<p>lon: ...</p>
<p>lat: ...</p>
<hr />
<p>One additional question:
Why is <code>.first</code> and <code>.last</code> reporting errors? How should I use them?</p>
<pre><code>da.assign_coords(year = da.time.dt.year, monthday = da.time.dt.strftime(&quot;%m-%d&quot;)).groupby(['year', 'monthday']).first()
da.assign_coords(year = da.time.dt.year, monthday = da.time.dt.strftime(&quot;%m-%d&quot;)).groupby(['year', 'monthday']).last()
</code></pre>
","0","Question"
"79592693","","<p>Why does <code>pd.to_datetime('2025175', format='%Y%W%w')</code> and <code>pd.Timestamp.fromisocalendar(2025, 17, 5)</code> gives different output?</p>
<p>I expected to obtain <code>Timestamp('2025-04-25 00:00:00')</code> for both cases.
But the first approach resulted on a Friday one week ahead.</p>
<p>Minimum example</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd


friday_datetime = pd.to_datetime('2025175', format='%Y%W%w')
friday_timestamp = pd.Timestamp.fromisocalendar(2025, 17, 5)
assert friday_datetime == friday_timestamp, (friday_datetime, friday_timestamp)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>assert friday_datetime == friday_timestamp, (friday_datetime, friday_timestamp) 
AssertionError: (Timestamp('2025-05-02 00:00:00'), Timestamp('2025-04-25 00:00:00'))
</code></pre>
","0","Question"
"79594460","","<p>I am trying to use a dataframe that contains a bunch of URLs and run each individual URL through a function. Note: the function works perfectly if called with a single URL i.e. myfunction(<a href=""https://www.example.com"" rel=""nofollow noreferrer"">https://www.example.com</a>).</p>
<pre><code># Example Dataframe (df)
#    Page Links
# 0  https://www.example.com
# 1  https://www.example2.com
# 2  https://www.example3.com
# 3  https://www.example4.com

def myfunction(url):
...code...

new_df = df.apply(myfunction, axis=1)
</code></pre>
<p>This is the error I get:
<a href=""https://i.sstatic.net/Jp36cIx2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jp36cIx2.png"" alt=""enter image description here"" /></a></p>
","-1","Question"
"79595489","","<p>I have a bunch of csv files read from a teensy adc onto an SD card and am trying to extract them to be able to do some basic stats over each row.</p>
<p>I have tried everything I can think of to try and fix this, but I cannot get my csv to be read correctly. The column names won't line up correctly. Heres the code I'm using:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats


###     Manual input of csv file and a short name for plot title
filename = &quot;data.csv&quot;

###     Read in data to a data frame with the correct formatting. index_col=0 was not working for all data files tested
data = pd.read_csv(filename,skiprows=1,header=1,index_col=None)

print(data.head())      #   To check that the columns are correctly lined up
</code></pre>
<p>For some reason I cannot get the code to read the header correctly and it keeps reading the header as one column longer than the data, resulting in an entire column of NaN's. This same thing happens when I do index_col=0 and index_col=&quot;SampleNumber&quot; also.</p>
<p>I've tried several iterations of the read_csv line (changing the header=,index_col=, etc) but haven't been able to correct this. The only solution I have is to manually go through and delete the first column of all my CSV files, but that does not seem efficient. Ideally I should have the &quot;SampleNumber&quot; column become the index column (since not all data.csv files have consistent numbering for the SampleNumber), but if that doesn't work it is fine to remove them altogether.</p>
<p>How do I get the SampleNumber column to be read in correctly? I suspect this is mostly an issue with how my csv files are being created but I couldn't figure out a way to upload one of them for someone else to try.</p>
<p>What is currently being output:</p>
<pre><code>   SampleNumber    C0    C1    C2    C3    C4    C5    C6    C7    C8    C9   C10   C11   C12   C13   C14  C15
0          3472  3030  2813  2695  2649  2636  2634  2632  2635  2635  2626  2624  2625  2623  2633  2597  NaN
1          2582  2581  2576  2561  2538  2511  2498  2490  2487  2484  2481  2481  2475  2475  2469  2475  NaN
2          2472  2474  2472  2474  2474  2474  2478  2474  2476  2484  2485  2490  2484  2485  2478  2486  NaN
3          2485  2483  2488  2488  2485  2486  2485  2484  2485  2483  2485  2483  2485  2483  2490  2473  NaN
4          2475  2472  2474  2477  2479  2482  2482  2482  2483  2487  2483  2482  2484  2483  2477  2483  NaN
</code></pre>
<p>What I want to be outputted:</p>
<pre><code>              C0    C1    C2    C3    C4    C5    C6    C7    C8    C9   C10   C11   C12   C13   C14  C15
SampleNumber    
0             3472  3030  2813  2695  2649  2636  2634  2632  2635  2635  2626  2624  2625  2623  2633  2597  
1             2582  2581  2576  2561  2538  2511  2498  2490  2487  2484  2481  2481  2475  2475  2469  2475  
2             2472  2474  2472  2474  2474  2474  2478  2474  2476  2484  2485  2490  2484  2485  2478  2486  
3             2485  2483  2488  2488  2485  2486  2485  2484  2485  2483  2485  2483  2485  2483  2490  2473  
4             2475  2472  2474  2477  2479  2482  2482  2482  2483  2487  2483  2482  2484  2483  2477  2483
</code></pre>
<p>Raw CSV:</p>
<pre><code>Start of new file:,,,,,,,,,,,,,,,,
MISCOUNT: 0,,,,,,,,,,,,,,,,
SampleNumber,C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,C15
0,3472,3030,2813,2695,2649,2636,2634,2632,2635,2635,2626,2624,2625,2623,2633,2597
1,2582,2581,2576,2561,2538,2511,2498,2490,2487,2484,2481,2481,2475,2475,2469,2475
2,2472,2474,2472,2474,2474,2474,2478,2474,2476,2484,2485,2490,2484,2485,2478,2486
3,2485,2483,2488,2488,2485,2486,2485,2484,2485,2483,2485,2483,2485,2483,2490,2473
4,2475,2472,2474,2477,2479,2482,2482,2482,2483,2487,2483,2482,2484,2483,2477,2483
5,2481,2482,2482,2465,2455,2450,2442,2443,2441,2448,2444,2465,2470,2467,2440,2467
</code></pre>
","1","Question"
"79595804","","<p>I have a dataset with world population (For clarity, countries are limmited to Brazil, Canada, Denmark):</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

world = pd.read_csv(&quot;../data/worldstats.csv&quot;)

cond = world[&quot;country&quot;].isin([&quot;Brazil&quot;,&quot;Canada&quot;,&quot;Denmark&quot;])
world = world[cond]
world = world.set_index([&quot;year&quot;,&quot;country&quot;]).sort_index().head(10)

world
</code></pre>
<p><a href=""https://i.sstatic.net/LPYk6udr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LPYk6udr.png"" alt=""enter image description here"" /></a></p>
<p>I want to reshapa my dataframe so the output would look like this</p>
<pre class=""lang-none prettyprint-override""><code>
year 1960
country     Population       GDP
Brazil      72493585.0       1.516557e+10
Canada      17909009.0       4.109345e+10
Denmark      4579603.0       6.248947e+09
year 1961   
country     Population       GDP
Brazil      74706888.0       1.523685e+10
Canada      18271000.0       4.076797e+10
Denmark      4611687.0       6.933842e+09
Year 1962   
country     Population       GDP
Brazil      77007549.0       1.992629e+10
Canada      18614000.0       4.197885e+10
Denmark      4647727.0       7.812968e+09
</code></pre>
<p>I've look into and tried functions such as <code>stack()</code>, <code>transpose()</code>, <code>pivot()</code> and <code>pivot_table()</code>, not I'm unable to come up with a code that will generate the output that I want.</p>
<p>I can use loops to generate the output but I want to export my output to an Excel spreadsheet. What would be the best way to it?</p>
<p>Thank you.</p>
","0","Question"
"79595809","","<p>I have a dataset of olive oil samples and the goal of creating a classification model for oil quality. I'm having trouble deciding how to deal with missing data. have a look at the data here if you like : <a href=""https://data.mendeley.com/datasets/thkcz3h6n6/6"" rel=""nofollow noreferrer"">https://data.mendeley.com/datasets/thkcz3h6n6/6</a>.</p>
<p>The issue is that the data is missing systematically from low quality oil samples. It seems that the company that collected the data skipped testing UV absorption and FAEES for samples already deemed as poor. I can't Impute based on other samples categorised as poor quality (&quot;Lampante oil&quot;) because there actually is none, its all missing. I have looked at trying to use &quot;regression-based imputation&quot; but there is not really a strong relationship between UV and FAEES and other columns.</p>
<p>So what would my course of action be for the missing values. I don't want to remove the columns completely and I can't remove the rows since it would mean removing all the Lampante (Poor quality) oil sample data.</p>
","0","Question"
"79596458","","<p>I recently upgraded my OS and had to rebuild several packages (I use a virtual env) starting from pip itself. However, pandas_ods_reader fails to build now (had it working earlier):</p>
<pre class=""lang-none prettyprint-override""><code>      copying pandas/tests/io/data/gbq_fake_job.txt -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data
      creating build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/fixed_width
      copying pandas/tests/io/data/fixed_width/fixed_width_format.txt -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/fixed_width
      creating build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/legacy_pickle/1.2.4
      copying pandas/tests/io/data/legacy_pickle/1.2.4/empty_frame_v1_2_4-GH#42345.pkl -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/legacy_pickle/1.2.4
      creating build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/parquet
      copying pandas/tests/io/data/parquet/simple.parquet -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/parquet
      creating build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/pickle
      copying pandas/tests/io/data/pickle/test_mi_py27.pkl -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/pickle
      copying pandas/tests/io/data/pickle/test_py27.pkl -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/pickle
      creating build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/xml
      copying pandas/tests/io/data/xml/baby_names.xml -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/xml
      copying pandas/tests/io/data/xml/books.xml -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/xml
      copying pandas/tests/io/data/xml/cta_rail_lines.kml -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/xml
      copying pandas/tests/io/data/xml/doc_ch_utf.xml -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/xml
      copying pandas/tests/io/data/xml/flatten_doc.xsl -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/xml
      copying pandas/tests/io/data/xml/row_field_output.xsl -&gt; build/lib.linux-x86_64-cpython-313/pandas/tests/io/data/xml
      copying pandas/io/sas/_sas.pyi -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/sas
      copying pandas/io/sas/sas.pyx -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/sas
      creating build/lib.linux-x86_64-cpython-313/pandas/io/formats/templates
      copying pandas/io/formats/templates/html.tpl -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/formats/templates
      copying pandas/io/formats/templates/html_style.tpl -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/formats/templates
      copying pandas/io/formats/templates/html_table.tpl -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/formats/templates
      copying pandas/io/formats/templates/latex.tpl -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/formats/templates
      copying pandas/io/formats/templates/latex_longtable.tpl -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/formats/templates
      copying pandas/io/formats/templates/latex_table.tpl -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/formats/templates
      copying pandas/io/formats/templates/string.tpl -&gt; build/lib.linux-x86_64-cpython-313/pandas/io/formats/templates
      UPDATING build/lib.linux-x86_64-cpython-313/pandas/_version.py
      set build/lib.linux-x86_64-cpython-313/pandas/_version.py to '1.5.3'
      running build_ext
      building 'pandas._libs.algos' extension
      creating build/temp.linux-x86_64-cpython-313/pandas/_libs
      x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -fPIC -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/src/klib -I/tmp/pip-build-env-dcnotb5s/overlay/lib/python3.13/site-packages/numpy/_core/include -I/tmp/pip-build-env-dcnotb5s/overlay/lib/python3.13/site-packages/numpy/_core/include -I/tmp/pip-build-env-dcnotb5s/overlay/lib/python3.13/site-packages/numpy/_core/include -I/tmp/pip-build-env-dcnotb5s/overlay/lib/python3.13/site-packages/numpy/_core/include -I/tmp/pip-build-env-dcnotb5s/overlay/lib/python3.13/site-packages/numpy/_core/include -I/tmp/pip-build-env-dcnotb5s/overlay/lib/python3.13/site-packages/numpy/_core/include -I/tmp/pip-build-env-dcnotb5s/overlay/lib/python3.13/site-packages/numpy/_core/include -I/home/spock/Private/python/include -I/usr/include/python3.13 -c pandas/_libs/algos.c -o build/temp.linux-x86_64-cpython-313/pandas/_libs/algos.o
      In file included from pandas/_libs/algos.c:809:
      pandas/_libs/src/klib/khash_python.h: In function ‘kh_complex128_hash_func’:
      pandas/_libs/src/klib/khash_python.h:140:36: error: request for member ‘real’ in something not a structure or union
        140 |     return kh_float64_hash_func(val.real)^kh_float64_hash_func(val.imag);
            |                                    ^
      pandas/_libs/src/klib/khash_python.h:140:67: error: request for member ‘imag’ in something not a structure or union
...
      error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for pandas
Successfully built ezodf
Failed to build lxml pandas
ERROR: Failed to build installable wheels for some pyproject.toml based projects (lxml, pandas)
</code></pre>
<p>This is on Debian Sid. I have the latest python version (3.13.3),  C compiler (14.2.0) installed. I have also upgraded pip itself to the latest version, as well all the installed packages. Current versions:</p>
<pre><code>Package            Version
------------------ -----------
acres              0.3.0
aiofiles           24.1.0
anyio              4.9.0
certifi            2025.4.26
chardet            5.2.0
charset-normalizer 3.4.1
ci-info            0.3.0
click              8.1.8
configobj          5.0.9
configparser       7.2.0
contourpy          1.3.2
cycler             0.12.1
defusedxml         0.7.1
docopt             0.6.2
etelemetry         0.3.1
filelock           3.18.0
fonttools          4.57.0
forex-python       1.8
frontend           0.0.3
h11                0.16.0
httplib2           0.22.0
idna               3.10
isodate            0.7.2
itsdangerous       2.2.0
kiwisolver         1.4.8
looseversion       1.3.0
lxml               5.4.0
matplotlib         3.10.1
networkx           3.4.2
nibabel            5.3.2
nipype             1.10.0
num2words          0.5.14
numpy              2.2.5
odfpy              1.4.1
opencv-python      4.11.0.86
packaging          25.0
pandas             2.2.3
pathlib            1.0.1
pillow             11.2.1
pip                25.1
prov               2.0.1
puremagic          1.28
pydot              3.0.4
PyMuPDF            1.25.5
pyparsing          3.2.3
PyPDF2             3.0.1
pytesseract        0.3.13
python-dateutil    2.9.0.post0
pytz               2025.2
pyxnat             1.6.3
pyzbar             0.1.9
qrcode             8.1
rdflib             7.1.4
reportlab          4.4.0
requests           2.32.3
scipy              1.15.2
simplejson         3.20.1
six                1.17.0
sniffio            1.3.1
starlette          0.46.2
traits             7.0.2
tzdata             2025.2
urllib3            2.4.0
uvicorn            0.34.2

</code></pre>
<p>I have code that depends on read_ods method in this module. I need it working as before.</p>
","0","Question"
"79596631","","<p>I have this function which converts an English month to a French month:</p>
<pre><code>def changeMonth(month):
    global CurrentMonth
    match month:
        case &quot;Jan&quot;:
            return &quot;Janvier&quot;
        case &quot;Feb&quot;:
            return &quot;Février&quot;
        case &quot;Mar&quot;:
            return &quot;Mars&quot;
        case &quot;Apr&quot;:
            return &quot;Avril&quot;
        case &quot;May&quot;:
            return &quot;Mai&quot;
        case &quot;Jun&quot;:
            return &quot;Juin&quot;
        case &quot;Jul&quot;:
            return &quot;Juillet&quot;
        case &quot;Aug&quot;:
            return &quot;Août&quot;
        case &quot;Sep&quot;:
            return &quot;Septembre&quot;
        case &quot;Oct&quot;:
            return &quot;Octobre&quot;
        case &quot;Nov&quot;:
            return &quot;Novembre&quot;
        case &quot;Dec&quot;:
            return &quot;Décembre&quot;

        # If an exact match is not confirmed, this last case will be used if provided
        case _:
            return &quot;&quot;
</code></pre>
<p>and I have a pandas col <code>df[&quot;month&quot;]= df['ic_graph']['month'].tolist()</code>:</p>
<p><a href=""https://i.sstatic.net/pqhuUmfg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pqhuUmfg.png"" alt=""enter image description here"" /></a></p>
<p>now what I'm looking for is to pass the df[&quot;month&quot;] col through the changeMonth function to display the df[&quot;month&quot;] in frensh months</p>
<p>By the way, I do not want to use the</p>
<pre><code>&gt;&gt;&gt; import locale
&gt;&gt;&gt; locale.setlocale(locale.LC_ALL, 'fr_FR')
</code></pre>
","3","Question"
"79597131","","<p>Using the <code>pandas.read_parquet()</code> method to read a file, pandas interprets a column with mixed values as type <code>Object</code>.  I want instead for pandas to interpret the column types as they are specified in the file.</p>
<p>A parquet file has meta information that states what the type of each column is and using <code>pyarrow</code> I can read the type using <code>pyarrow.parquet.read_schema(path_raw)</code>.</p>
<p>With pandas <code>read_parquet()</code>, I tried using the <code>dtype_backend</code> parameter ie. <code>pd.read_parquet(&lt;path&gt;, dtype_backend='pyarrow')</code> to try to force the correct types but that only sets the type to either <code>pyarrow</code> or <code>numpy_nullable</code> so:</p>
<pre><code>ORIGINAL FILE SCHEMA
rowid: int64
txid: int64
debit: int64
credit: int64
effective_date: date32[day] not null
entered_date: date32[day] not null
account: string

PANDAS read_parquet() WITH BACKEND='nullable_numpy'

 #   Column          Dtype 
---  ------          ----- 
 0   rowid           Int64 
 1   txid            Int64 
 2   debit           Int64 
 3   credit          Int64 
 4   effective_date  object  &lt;===== 
 5   entered_date    object   &lt;=====
 6   account         string
 
PANDAS read_parquet() WITH BACKEND='pyarrow'

 #   Column          Dtype               
---  ------          -----               
 0   rowid           int64[pyarrow]      
 1   txid            int64[pyarrow]      
 2   debit           int64[pyarrow]      
 3   credit          int64[pyarrow]      
 4   effective_date  date32[day][pyarrow]  &lt;=== why not pandas datetime type?
 5   entered_date    date32[day][pyarrow]
 6   account         string[pyarrow]

PANDAS W/O 'BACKEND='
 #   Column          Dtype 
---  ------          ----- 
 0   rowid           int64 
 1   txid            int64 
 2   debit           int64 
 3   credit          int64 
 4   effective_date  object   &lt;====
 5   entered_date    object   &lt;====
 6   account         object   &lt;====
</code></pre>
<p>Questions:</p>
<ul>
<li>why doesn't pandas use that schema information to set the dtypes of the columns in the dataframe, am I using it improperly?</li>
<li>is there a way to force pandas to adopt the schema specified in the file?</li>
</ul>
","0","Question"
"79597604","","<p>I am trying to merge three dataframes using intersection(). How can we check that all dataframes exists/initialised before running the intersection() without multiple if-else check blocks. If any dataframe is not assigned, then don't take it while doing the intersection().
Sometimes I am getting error - <strong>UnboundLocalError: local variable 'df_2' referenced before assignment</strong>, because file2 does not exist.</p>
<p>OR is there any other easy way to achieve below?</p>
<p>Below is my approach:</p>
<pre><code>if os.path.exists(file1):
        df_1 = pd.read_csv(file1, header=None, names=header_1, sep=',', index_col=None)
if os.path.exists(file2):
        df_2 = pd.read_csv(file2, header=None, names=header_2, sep=',', index_col=None)
if os.path.exists(file3):
        df_3 = pd.read_csv(file3, header=None, names=header_3, sep=',', index_col=None)

common_columns = df_1.columns.intersection(df_2.columns).intersection(df_3.columns)
filtered_1 = df_1[common_columns]
filtered_2 = df_2[common_columns]
filtered_3 = df_3[common_columns]
concatenated_df = pd.concat([filtered_1, filtered_2, filtered_3], ignore_index=True)
</code></pre>
","0","Question"
"79598340","","<p>How can I compute time to first target event per user using Pandas efficiently (with edge cases)?</p>
<p>I'm analyzing user behavior using a Pandas DataFrame that logs events on an app. Each row includes a <code>user_id</code>, <code>event_type</code>, and <code>timestamp</code>. I want to calculate the time (in seconds) from each user's first recorded event to their first occurrence of a target event (e.g., &quot;purchase&quot;).</p>
<p>However, there are a few requirements that complicate things:</p>
<p>Some users never trigger the target event, so I want to exclude or mark them as <code>NaN</code>.</p>
<p>The <code>timestamp</code> column is a <code>datetime</code>.</p>
<p>I’d like this to be vectorized and efficient (not using for loops).</p>
<p>I want to return a DataFrame with <code>user_id</code> and <code>seconds_to_first_purchase</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = [
    {'user_id': 'u1', 'event_type': 'login', 'timestamp': '2023-01-01 10:00:00'},
    {'user_id': 'u1', 'event_type': 'purchase', 'timestamp': '2023-01-01 10:05:00'},
    {'user_id': 'u2', 'event_type': 'login', 'timestamp': '2023-01-01 09:00:00'},
    {'user_id': 'u2', 'event_type': 'scroll', 'timestamp': '2023-01-01 09:03:00'},
    {'user_id': 'u3', 'event_type': 'login', 'timestamp': '2023-01-01 11:00:00'},
    {'user_id': 'u3', 'event_type': 'purchase', 'timestamp': '2023-01-01 11:20:00'},
]

df = pd.DataFrame(data)
df['timestamp'] = pd.to_datetime(df['timestamp'])
</code></pre>
<p>What’s the cleanest and most efficient way to compute the time to first &quot;purchase&quot; event per user?</p>
<p>What I tried:
I grouped the DataFrame by <code>user_id</code> and tried to extract the first <code>timestamp</code> for each user using <code>groupby().first()</code>, and then did the same for the first &quot;purchase&quot; event using a filtered DataFrame.</p>
<p>Then I tried merging both results to calculate the time difference like this:</p>
<pre class=""lang-py prettyprint-override""><code>first_event = df.groupby('user_id')['timestamp'].min()
first_purchase = df[df['event_type'] == 'purchase'].groupby('user_id')['timestamp'].min()
result = (first_purchase - first_event).dt.total_seconds()
</code></pre>
<p>What I expected:
I expected this to give me a clean Series or DataFrame with <code>user_id</code> and the number of seconds between the user's first event and their first &quot;purchase&quot;.</p>
<p>What went wrong:
It mostly works, but:</p>
<ul>
<li>Users who never purchased are missing from the result and I want to keep them (with <code>NaN</code>).</li>
<li>I'm not sure this is the most efficient or cleanest approach.</li>
</ul>
<p>I’m also wondering if there's a better way to avoid intermediate merges or repetitive groupby operations.</p>
","0","Question"
"79598498","","<p>For Python 3 How can I install pandas on a windows 11 ARM machine without downloading and compiling the source?</p>
<p>I get various errors, such as</p>
<blockquote>
<p>could not find a version that satisfies the requirement pandas</p>
</blockquote>
<p>I tried <code>pip3 install --upgrade --force-reinstall pandas</code>.
in this case it replies</p>
<blockquote>
<p>Using cached pandas-2.2.3.tar.gz.</p>
</blockquote>
<p>I tried adding a no-cache.</p>
","-1","Question"
"79599081","","<p>I'm simply trying to create a nice border for my dataset. It applies it nicely to the entire dataset expect to the first row where the data actually starts.</p>
<pre><code>import pandas as pd
import io

# In-memory buffer
output = io.BytesIO()

with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
    # Start data from row 4 (Excel), which is index 3
    df.to_excel(writer, sheet_name='Sheet1', startrow=3, startcol=1, index=False, header=False)

    workbook = writer.book
    worksheet = writer.sheets['Sheet1']

    # Formats
    header_format = workbook.add_format({'bold': True, 'align': 'center', 'valign': 'vcenter', 'border': 1})
    subheader_format = workbook.add_format({'bold': True, 'align': 'center', 'border': 1})
    bordered_format = workbook.add_format({'border': 1})

    # Merged headers in row 2 (index 1)
    worksheet.merge_range('F2:G2', 'portfolio', header_format)
    worksheet.merge_range('H2:I2', 'inversion information', header_format)
    worksheet.merge_range('J2:K2', '% of inversion', header_format)

    # Column headers in row 3 (index 2)
    start_col = 1  # Column B
    for col_num, column_name in enumerate(df.columns):
        worksheet.write(2, col_num + start_col, column_name, subheader_format)

    # Get dimensions
    num_rows, num_cols = df.shape
    first_row = 1                    # Merged headers (Excel row 2)
    last_row = 3 + num_rows          # Data ends at Excel row 4 + num_rows
    first_col = start_col
    last_col = start_col + num_cols - 1

    # Apply borders from B2 to K[bottom row]
    for row in range(first_row, last_row + 1):
        for col in range(first_col, last_col + 1):
            if row &gt;= 4:  # Data rows start from Excel row 4
                value = df.iloc[row - 4, col - start_col]
                worksheet.write(row, col, value, bordered_format)
            elif row == 2:
                continue  # Subheaders already formatted
            elif row == 1 and col &lt; 5:  # Empty cells before merged headers
                worksheet.write_blank(row, col, None, bordered_format)

# Save file
with open('mypath/rbd_inversion.xlsx', 'wb') as f:
    f.write(output.getvalue())

</code></pre>
<p>this is my code, can someone please help me debug the problem. I have also attached a screenshot of the row without it applying the border to it <a href=""https://i.sstatic.net/fztAis36.png"" rel=""nofollow noreferrer"">borderless screenshot</a></p>
","1","Question"
"79599116","","<p>I am working on a project that requires me to compare 2 rows (1 and 2, 3 and 4, etc...) and output the differences to a table. Now I have been able to compare the columns and create the table with differences. What I need to clean it up now is a way to show which row identifiers are being compared. Each set of rows has the same identifier. Please see the code and output below.</p>
<p>Source data example:</p>
<p><a href=""https://i.sstatic.net/wu2ZLXY8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wu2ZLXY8.png"" alt=""Example data table"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

def compare_rows(df):
    &quot;&quot;&quot;
    Compares each pair of consecutive rows in a Pandas DataFrame and outputs the differences.

    Args:
      df: The input Pandas DataFrame.

    Returns:
      A new Pandas DataFrame containing the differences between consecutive rows.
    &quot;&quot;&quot;

    diff_list = []

    for i in range(0,len(df) - 1,2):
        row1 = df.iloc[i]
        row2 = df.iloc[i + 1]

        print(i)

        diff = {}

        for col in df.columns:
            if row1[col] != row2[col]:
                diff[col] = (row1[col], row2[col])

            diff_list.append(diff)

    return (diff_list)

 
# Convert spark dataframe to pandas

strSQL = &quot;&quot;&quot;select * table
            where batch_id = (select max(batch_id)
                  from table)&quot;&quot;&quot;

df_spark = spark.sql(strSQL)

df = df_spark.toPandas()

diff_df = compare_rows(df)

df = pd.DataFrame.from_dict(diff_df)
 
df.to_excel('your_excel_file.xlsx', sheet_name='Sheet1', index=False)

</code></pre>
<p>What it outputs now:</p>
<p><a href=""https://i.sstatic.net/AJA2Trb8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJA2Trb8.png"" alt=""Actual output"" /></a></p>
<p>What I want:</p>
<p><a href=""https://i.sstatic.net/e8MCciAv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/e8MCciAv.png"" alt=""Desired output"" /></a></p>
","0","Question"
"79599353","","<p>I am trying to iterate through a Pandas dataframe's column values one by one to detect a substring with a RegEx pattern and replace it wherever it shows up.
The string values in the dataframe's target column have the following variations:</p>
<ol>
<li>'func(something)...'</li>
<li>'func(something,something)...'</li>
</ol>
<p>I am attempting to target the values with the 2nd variation and replace the comma-separated substring by a value from another column in the same row.</p>
<p>My sample code looks like:</p>
<pre><code>import re
for i, x in enumerate(df['col1']):
  df.loc[i,'col2'] = re.sub(r'^(func\().+,.+(\).*)', r'\1%s\2'%(x), df.loc[i,'col2'])
</code></pre>
<p>I have had a successful output on a terminal when I replace the <code>*</code> in the pattern with <code>+</code>:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; val = 'func(y,z) and func2(a,b)'
&gt;&gt;&gt; val
'func(y,z) and func2(a,b)'
&gt;&gt;&gt; pattern1 = r'^(func\().+,.+(\).*)'
&gt;&gt;&gt; replacement = r'\1%s\2'%('x')
&gt;&gt;&gt; val2 = re.sub(pattern1, replacement, val) 
&gt;&gt;&gt; val2
'func(x)'
&gt;&gt;&gt; pattern2 = r'^(func\().+,.+(\).+)'
&gt;&gt;&gt; val2 = re.sub(pattern2, replacement, val) 
&gt;&gt;&gt; val2
'func(x) and func2(a,b)'
&gt;&gt;&gt; val3 = 'func(y) and func2(a,b)'
&gt;&gt;&gt; val4 = re.sub(pattern2, replacement, val3) 
&gt;&gt;&gt; val4
'func(y) and func2(a, b)'
</code></pre>
<p>I thought the <code>.*</code> in <code>pattern1</code> should have accommodated for 0 or more instances of any character after the final captured parenthesis <code>\)</code>, but at least using <code>.+</code> in <code>pattern2</code> works.</p>
<p>Problem is, when I try to replicate it on VSCode with Python on the dataframe's column values, I am not getting the desired output. I think my regex pattern is catching other parts of the strings I did not intend, so I am getting mixed results.</p>
<p>Am I missing anything with <code>pattern2</code>?</p>
<p><strong>Update:</strong>
Here is an example code I used in VSCode in which the result is promising, but when the string has more conditions added the pattern doesn't hold up.</p>
<pre><code>df = pd.DataFrame({'col1':['func(a,b) and func(c,d)','func(a) and func(c)','func(b) and func(c,d)','func(a,b,c) and func(d,e,f)'],
                   'col2':['e','b','a','g']})
df
&gt;&gt;&gt;
    col1    col2
0   func(a,b) and func(c,d) e
1   func(a) and func(c) b
2   func(b) and func(c,d)   a
3   func(a,b,c) and func(d,e,f) g
</code></pre>
<pre><code>import re
df['col3'] = ''
for i,x in enumerate(df['col2']):
    pattern = r'^(func\().+,.+(\).+)'
    replacement = fr'\1{x}\2'
    df.loc[i,'col3'] = re.sub(pattern,replacement,df.loc[i,'col1'])
    if df.loc[i,'col3'] != df.loc[i,'col1']:
        print(df.loc[i,'col1'],'---&gt;',df.loc[i,'col3'])
&gt;&gt;&gt;
func(a,b) and func(c,d) ---&gt; func(e) and func(c,d)
func(a,b,c) and func(d,e,f) ---&gt; func(g) and func(d,e,f)
</code></pre>
<p>The above is what I expected to see, but in the attempt below, you can see additional <code>func()</code> conditions get dropped:</p>
<pre><code>df = pd.DataFrame({'col1':['func(a,b) and func(c,d) and func2(z)','func(a) and func(c) and func2(z)','func(b) and func(c,d) and func2(z)','func(a,b,c) and func(d,e,f) and func2(z)'],
                   'col2':['e','b','a','g']})
df
&gt;&gt;&gt;
    col1    col2
0   func(a,b) and func(c,d) and func2(z)    e
1   func(a) and func(c) and func2(z)    b
2   func(b) and func(c,d) and func2(z)  a
3   func(a,b,c) and func(d,e,f) and func2(z)    g
</code></pre>
<pre><code>import re
df['col3'] = ''
for i,x in enumerate(df['col2']):
    pattern = r'^(func\().+(?:,.+)+(\).+)'
    replacement = fr'\1{x}\2'
    df.loc[i,'col3'] = re.sub(pattern,replacement,df.loc[i,'col1'])
    if df.loc[i,'col3'] != df.loc[i,'col1']:
        print(df.loc[i,'col1'],'---&gt;',df.loc[i,'col3'])

&gt;&gt;&gt;
func(a,b) and func(c,d) and func2(z) ---&gt; func(e) and func2(z)
func(b) and func(c,d) and func2(z) ---&gt; func(a) and func2(z)
func(a,b,c) and func(d,e,f) and func2(z) ---&gt; func(g) and func2(z)
</code></pre>
<p>The pattern seems to leave out the middle <code>func()</code> expressions.</p>
","0","Question"
"79600043","","<p>I'm trying to read an <code>.arrow</code> format file with Python pandas.</p>
<p><code>pandas</code> does not have a <code>read_arrow</code> function. However, it does have <code>read_csv</code>, <code>read_parquet</code>, and other similarly named functions.</p>
<p>How can I read an Apache Arrow format file?</p>
<p>I have read the documentation and know how to convert between the <code>pandas.DataFrame</code> type and the <code>pyarrow</code> <code>Table</code> type, which appears to be some kind of <code>pyarrow</code> equivalent of a <code>pandas</code> <code>DataFrame</code>. However, this may or may not be relevant information.</p>
<p>What I cannot find is a way to either</p>
<ul>
<li>read a <code>pyarrow.Table</code> from an Arrow file</li>
<li>read a <code>pandas.DataFrame</code> from an Arrow file</li>
</ul>
","-4","Question"
"79600418","","<p>I have this pandas dataframe :
df :</p>
<pre><code>import pandas as pd

data = {

  &quot;function&quot;: [&quot;test1&quot;,&quot;test2&quot;,&quot;test3&quot;,&quot;test4&quot;,&quot;test5&quot;,&quot;test6&quot;,&quot;test7&quot;,&quot;test8&quot;,&quot;test9&quot;,&quot;test10&quot;,&quot;test11&quot;,&quot;test12&quot;,

],
   &quot;service&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;AO&quot;, &quot;M&quot; ,&quot;A&quot;, &quot;PO&quot;, &quot;MP&quot;, &quot;YU&quot;, &quot;Z&quot;, &quot;R&quot;, &quot;E&quot;, &quot;YU&quot;],
  &quot;month&quot;: [&quot;January&quot;,&quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot;]
}

#load data into a DataFrame object:
df = pd.DataFrame(data)

print(df)
</code></pre>
<p>the result :</p>
<pre><code>   function service      month
0     test1       A    January
1     test2       B   February
2     test3      AO      March
3     test4       M      April
4     test5       A        May
5     test6      PO       June
6     test7      MP       July
7     test8      YU     August
8     test9       Z  September
9    test10       R    October
10   test11       E   November
11   test12      YU   December
</code></pre>
<p>I have a slider which has a variable where i can select a month, imagine this variable called <strong>var</strong>. now what I'm looking for is, when I select a month in the slider I want to filter the dataframe but i want to get <strong>always six rows</strong> where the month selected is appeared in the filtered data(wherever where it appeared  in the result, in begining or in the middle or at the end)</p>
<p>if you could please help ?</p>
<p>which i have triend :</p>
<pre><code>def selectDataRange(var:str,df):
    if var==&quot;January&quot;:
        df.iloc[0: 6,]
    if var==&quot;February&quot;:
        df.iloc[1: 6,]
    if var==&quot;March&quot;:
        df.iloc[2: 6,]
</code></pre>
<p>i have tried this methode(only for the third months)..but it doesnt work</p>
","1","Question"
"79600443","","<p>My source dataframe:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Source</th>
<th>Description</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>John</td>
<td>A</td>
<td>Text1</td>
<td>1</td>
</tr>
<tr>
<td>John</td>
<td>B</td>
<td>Longer text</td>
<td>4</td>
</tr>
<tr>
<td>Bob</td>
<td>B</td>
<td>Text2</td>
<td>2</td>
</tr>
<tr>
<td>Alice</td>
<td>Z</td>
<td>Longer text</td>
<td>5</td>
</tr>
<tr>
<td>Alice</td>
<td>Y</td>
<td>The Longest text</td>
<td>3</td>
</tr>
<tr>
<td>Alice</td>
<td>X</td>
<td>Text3</td>
<td>6</td>
</tr>
</tbody>
</table></div>
<p>I want to drop duplicates from column <code>Name</code> with the following criteria:</p>
<ol>
<li>Keep the row where <code>Description</code> is the longest</li>
<li>Have a column that merges all the <code>Source</code> values, sorted alphabetically</li>
</ol>
<p>Output I want to achieve:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Source</th>
<th>Description</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>John</td>
<td>A, B</td>
<td>Longer text</td>
<td>4</td>
</tr>
<tr>
<td>Bob</td>
<td>B</td>
<td>Text2</td>
<td>2</td>
</tr>
<tr>
<td>Alice</td>
<td>X, Y, Z</td>
<td>The Longest text</td>
<td>3</td>
</tr>
</tbody>
</table></div>
<p>Here's what I have so far:</p>
<pre><code># Create new column with Description length
df['Description_Length'] = df['Description'].str.len().fillna(0) 

# Drop duplicates from Name based on Description Length
df = df.sort_values('Description_Length',ascending=False).drop_duplicates('Name')

</code></pre>
<p>What I'm missing is how to join the <code>Source</code> data before dropping the duplicates? Thanks!</p>
","2","Question"
"79600747","","<p>I am working through creating unit tests for a Python application that handles data via Pandas DataFrames. One section of this application formats data in various DataFrame columns. I am looking to mock the <em>Series.apply</em> function to format column values into strings.</p>
<p>The line I am looking to mock is:</p>
<pre><code>source_table_df[&quot;col1&quot;] = source_table_df[&quot;col1&quot;].apply(self._format_col)
</code></pre>
<p>I am mocking this with the following code:</p>
<pre><code>with patch(&quot;pandas.core.series.Series.apply&quot;) as mock_apply:
    mock_apply.side_effect = (lambda val: str(val))
</code></pre>
<p>For my unit test, I have a table that looks like so:</p>
<pre><code>col1
===
1
1
1
</code></pre>
<p>And I am looking for this to be converted to:</p>
<pre><code>col1
===
&quot;1&quot;
&quot;1&quot;
&quot;1&quot;
</code></pre>
<p>However, I am recieving the following outcome:</p>
<pre><code>col1
===
&lt;function MyClass._format_col at 0x000001AA25FD0A40&gt;
&lt;function MyClass._format_col at 0x000001AA25FD0A40&gt;
&lt;function MyClass._format_col at 0x000001AA25FD0A40&gt;
</code></pre>
<p>How can I properly mock the conversion of the column values? Thank you for your time.</p>
","1","Question"
"79600924","","<p>There are loads of answers on this topic, but for the life of me, I cannot find a solution to my issue.</p>
<p>Say I have a JSON like</p>
<pre><code>json_2_explode = [
    {
        &quot;scalar&quot;: &quot;43&quot;,
        &quot;units&quot;: &quot;m&quot;,
        &quot;parameter&quot;: [{&quot;no_1&quot;: &quot;45&quot;, &quot;no_2&quot;: &quot;1038&quot;, &quot;no_3&quot;: &quot;356&quot;}],
        &quot;name&quot;: &quot;Foo&quot;,
    },
    {
        &quot;scalar&quot;: &quot;54.1&quot;,
        &quot;units&quot;: &quot;s&quot;,
        &quot;parameter&quot;: [{&quot;no_1&quot;: &quot;78&quot;, &quot;no_2&quot;: &quot;103&quot;, &quot;no_3&quot;: &quot;356&quot;}],
        &quot;name&quot;: &quot;Yoo&quot;,
    },
    {
        &quot;scalar&quot;: &quot;1123.1&quot;,
        &quot;units&quot;: &quot;Hz&quot;,
        &quot;parameter&quot;: [{&quot;no_1&quot;: &quot;21&quot;, &quot;no_2&quot;: &quot;43&quot;, &quot;no_3&quot;: &quot;3577&quot;}],
        &quot;name&quot;: &quot;Baz&quot;,
    },
]
</code></pre>
<p>documenting some readings for attributes <code>Foo</code>, <code>Yoo</code> and <code>Baz</code>. For each I detail a number, that is, the value itself, some parameters, and the name.</p>
<p>Say this JSON is a column in a dataframe,</p>
<pre><code>df = pd.DataFrame(data = {'col1': [11, 9, 23, 1],
                         'col2': [7, 3, 1, 12],
                         'col_json' : [json_2_explode,
                                       json_2_explode,
                                       json_2_explode,
                                       json_2_explode]}, index=[0, 1, 2, 3])
</code></pre>
<pre><code>    col1    col2    col_json
0   11      7        [{'scalar': '43', 'units': 'MPa', 'parameter':...
1   9       3        [{'scalar': '43', 'units': 'MPa', 'parameter':...
2   23      1        [{'scalar': '43', 'units': 'MPa', 'parameter':...
3   1       12       [{'scalar': '43', 'units': 'MPa', 'parameter':...
</code></pre>
<p>The issue I have is that if I try</p>
<pre><code>df = pd.json_normalize(df['col_json'].explode())
</code></pre>
<p>I get</p>
<pre><code>    scalar  units            parameter                          name
0   43      m   [{'no_1': '45', 'no_2': '1038', 'no_3': '356'}] Foo
1   54.1    s   [{'no_1': '78', 'no_2': '103', 'no_3': '356'}]  Yoo
2   1123.1  Hz  [{'no_1': '21', 'no_2': '43', 'no_3': '3577'}]  Baz
3   43      m   [{'no_1': '45', 'no_2': '1038', 'no_3': '356'}] Foo
4   54.1    s   [{'no_1': '78', 'no_2': '103', 'no_3': '356'}]  Yoo
5   1123.1  Hz  [{'no_1': '21', 'no_2': '43', 'no_3': '3577'}]  Baz
6   43      m   [{'no_1': '45', 'no_2': '1038', 'no_3': '356'}] Foo
7   54.1    s   [{'no_1': '78', 'no_2': '103', 'no_3': '356'}]  Yoo
8   1123.1  Hz  [{'no_1': '21', 'no_2': '43', 'no_3': '3577'}]  Baz
9   43      m   [{'no_1': '45', 'no_2': '1038', 'no_3': '356'}] Foo
10  54.1    s   [{'no_1': '78', 'no_2': '103', 'no_3': '356'}]  Yoo
11  1123.1  Hz  [{'no_1': '21', 'no_2': '43', 'no_3': '3577'}]  Baz
</code></pre>
<p>So it is exploding each JSON in 3 rows (admittedly each JSON does contain 3 sub-dicts, so to say). I actually would like <code>Foo</code>, <code>Yoo</code> and <code>Baz</code> to be documented in the same row, adding columns.
Is there maybe solution not involving manually manipulating rows/piercing it as desired? I would love to see one of your fancy one-liners.</p>
<p><strong>EDIT</strong></p>
<p>The desired outcome would look like this, column names free to assign.</p>
<p>So for each row the JSON is exploded and assigned to new columns. json_normalise creates a new row for each element in the list of json, as undesired behaviour in the example above</p>
<pre><code>0   11  7   43  m   45  1038    356 Foo 54.1    s   78  103 356 Yoo 1123.1  Hz  21  43  3577    Baz
1   9   3   43  m   45  1038    356 Foo 54.1    s   78  103 356 Yoo 1123.1  Hz  21  43  3577    Baz
2   23  12  43  m   45  1038    356 Foo 54.1    s   78  103 356 Yoo 1123.1  Hz  21  43  3577    Baz
</code></pre>
<p>Note in this example each json list in the original dataframe is the same, but of course the idea here is to explode each json to its neighbouring new columns, and they could be different of course.</p>
<p>** EDIT 2** Attempt to implement a suggested <a href=""https://stackoverflow.com/a/79601014/14909621"">solution by Jonatahn Leon</a></p>
<p>First I use his code to define a function, to be mapped to the column containing the json object</p>
<pre><code>def json_flattener(json_blurp):
    prefix_json_2_explode = {}
    for d in json_blurp:
        prefix_json_2_explode.update({d['name'] + '_' + key: value for key, value in d.items()})
        dict_flattened = (flatten(d, '.') for d in [prefix_json_2_explode])
    df = pd.DataFrame(dict_flattened)
    return df


new_cols = ['Foo_scalar', 'Foo_units', 'Foo_parameter.0.no_1',
       'Foo_parameter.0.no_2', 'Foo_parameter.0.no_3', 'Foo_name',
       'Yoo_scalar', 'Yoo_units', 'Yoo_parameter.0.no_1',
       'Yoo_parameter.0.no_2', 'Yoo_parameter.0.no_3', 'Yoo_name',
       'Baz_scalar', 'Baz_units', 'Baz_parameter.0.no_1',
       'Baz_parameter.0.no_2', 'Baz_parameter.0.no_3', 'Baz_name']


df[new_cols]= df[['col_json']].apply(lambda x: json_flattener(x.iloc[0]), axis=1,result_type='expand')
</code></pre>
<p>But I ran into the error:</p>
<pre><code>ValueError: If using all scalar values, you must pass an index
</code></pre>
<p>Trying to sort it out.</p>
","4","Question"
"79601117","","<p>I am trying to make a movie showing the evolution of an estimator. The idea is to draw the history of the estimator after each update. I want to do this as a movie as some updates come in out of sequence and this has been the easiest way to display this.</p>
<p>The <code>animation_frame</code> option for scatter or line in <code>ploty.express</code> has worked well but when I try and color points, or use anything that makes a legend, I am running into issues.</p>
<ol>
<li>The legend only includes items from the first frame. This also leads to any color not on the first frame not drawing.</li>
<li>If I make a &quot;zeroeth&quot; frame with all the data to get the legend correct a color is only updated correctly sometimes. For example when a frame updates a blue point all blue points are correct but red points are not updated, even if they should change.</li>
</ol>
<p>Here is a code snippet showing the first problem. It is a simple scatter of some <code>y=x</code> points. Each frame has one new point compared to the last and points above <code>(3, 3)</code> are red. Doing this in what I think is the simplest way leads to red points not being displayed in the legend or on the graph.</p>
<pre><code>import numpy as np
import pandas as pd
import plotly.express as px

# Make a y=x line that grows from origin (0, 0), to (9, 9)
x = [val for stop in range(10) for val in np.arange(0, stop, 1)]
frame_id = [stop for stop in range(10) for _ in np.arange(0, stop, 1)]
y = x
color = ['blue' if val &lt; 3 else 'red' for val in x]


simple_df = pd.DataFrame().assign(
    x=x,
    y=y,
    color=color,
    frame_id=frame_id,
).sort_values(by='frame_id')

px.scatter(
    simple_df, x='x', y='y', color='color', animation_frame='frame_id'
).update_layout(
    title='Animation of y=x line growing from (0, 0) to (9, 9)',
).update_yaxes(range=[-1, 10]).update_xaxes(range=[-1, 10]).show()
</code></pre>
<p>I have sort of fixed this by adding a starting frame with all points displayed. I have made this less ugly in real code by setting the size of the points to zero, but am displaying them here for clarity. The issue now is only colors that are added on a given frame are handled correctly. So on frames 1-3 blue points only are updated. This leads to the red points hanging around until the first frame where a red point is added. This leads to further issues when going forwards and backwards using the slider but I think this movie rolling forward displays the core of the issue.</p>
<pre><code>zero_frame = pd.concat([simple_df.assign(frame_id=0), simple_df])
px.scatter(
    zero_frame, x='x', y='y', color='color', animation_frame='frame_id'
).update_layout(
    title='Animation of y=x line growing from (0, 0) to (9, 9)',
).update_yaxes(range=[-1, 10]).update_xaxes(range=[-1, 10]).show()
</code></pre>
<p>Does anyone have any fixes for this? To re-iterate, in the toy example I would like a line of blue dots to grow from <code>(1, 1)</code> and have dots after <code>(3, 3)</code> be red, with one dot added per frame. I think if this worked I could get it to behave on real data.</p>
","1","Question"
"79601755","","<p>I have a simple question relating to creating 'case when' SQL equivalent in Python.
I have a list of twitter statements from which I need to extract and categorize windows version in new column.</p>
<p>Currently I created below function, but it does not work creating error message:</p>
<pre class=""lang-none prettyprint-override""><code>ValueError: The truth value of a Series is ambiguous. 
Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>
<pre><code>def windows_ver(size):
    if df['twitter'].str.lower().str.contains('windows 11'):
        return 'windows 11'
    elif df['twitter'].str.lower().str.contains('windows 10') :
        return 'windows 10'
    return 'windows 8 or older'

df['windows_version'] = df['twitter'].apply(windows_ver)
</code></pre>
<p>Can you please advise how may I correctly create new column 'windows_version' categorizing tweets relating to particular system?
Thank you in advance for help!</p>
","-3","Question"
"79601812","","<p>I have two address columns and I want to extract the last word from the first column and the first word from the second column.
In the provided example there aren't two words in column 'Address2', but I want to build the code in such a way that it will work regardless of how the dataset will look like.
Sometimes the address2 can be one word, something it will have 2, etc..</p>
<pre><code>data = {
    'Address1': ['3 Steel Street', '1 Arnprior Crescent', '40 Bargeddie Street Blackhill'],
    'Address2': ['Saltmarket', 'Castlemilk', 'Blackhill']
}

df = pd.DataFrame(data)
</code></pre>
<p>I have no problem with column 'Address1':</p>
<pre><code>df[['StringStart', 'LastWord']] = df['Address1'].str.rsplit(' ', n=1, expand=True)
</code></pre>
<p>The problem comes with column 'Address2' where if I apply the above code I an error:
<em>Columns must be same length as key</em></p>
<p>I  understand where the problem is coming from - I am trying to split one column which has one element into two columns.
I am sure there is a way in which this can be handled to allow the split anyway and return Null if there isn't a word and a value if there is.</p>
","4","Question"
"79601886","","<p>Consider the following basic dataframe example:</p>
<pre><code>           min      mean    max
ALU        -0.008   0.000   0.034
BRENT      -0.017   0.000   0.023
CU         -0.011   0.000   0.013
DTD_BRENT  -0.011   0.000   0.019
GASOIL     -0.009   0.000   0.035
GOLD       -0.008   0.000   0.009
HH         -0.033  -0.001   0.009
JET_CCN    -0.009   0.000   0.033
</code></pre>
<p>I would like to produce a heatmap plot for the above dataframe using <code>seaborn.heatplot</code> where values ranging between <code>-0.03</code> and <code>0.03</code> will have variations (depending on the value) of the green color and values outside this interval will have red color variations.</p>
<p>While I can generally produce the plot, I am struggling with the interval-dependent color map.</p>
<pre><code>plt.figure(figsize=(12, 8))
sns.heatmap(df, cmap=sns.color_palette(['red', 'green', 'red']), center= 0, annot=True)
plt.title(&quot;Heatmap&quot;)
plt.show()
</code></pre>
","0","Question"
"79603293","","<p>I have data in Pandas, where the temperatures are the column headers and the COP values are in the matrix.</p>
<p>If I interpolate using pandas interpolate function I end up with linear interpolate, not weighted to the temperature at the top.</p>
<p>For example, here is my original data:
<a href=""https://i.sstatic.net/YL4KHPx7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YL4KHPx7.png"" alt=""enter image description here"" /></a></p>
<p>I have added the needed columns and I need to interpolate with respect to the column header version.. If I use the interpol function, the values are interpolated but not with respect to the temperature. See example, incorrect interpolation.
<a href=""https://i.sstatic.net/xFyTOtei.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFyTOtei.png"" alt=""enter image description here"" /></a></p>
<p>Is this possible in Pandas or should I remove it our of a dataframe and rather do it in Scipy?</p>
","-1","Question"
"79603298","","<p>I'm working with a day of year column that ranges from 1 to 366 (to account for leap years). I need to convert this column into a date for a specific task and I would like to set it to a year that is very unlikely to appear in my time series.</p>
<p>Is there a way to set it to the oldest leap year of pandas?</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# here is an example where the data have already been converted to datetime object
# I just missed the year to set
dates = pd.Series(pd.to_datetime(['2023-05-01', '2021-12-15', '2019-07-20']))
first_leap_year = 2000  # this is where I don't know what to set
new_dates = dates.apply(lambda d: d.replace(year=first_leap_year))
</code></pre>
","2","Question"
"79604001","","<p>I am doing the below but getting memory issues.</p>
<p>make frame</p>
<pre><code>data = {'link': [1,2,3,4,5,6,7],
        'code': ['xx', 'xx', 'xy', '', 'aa', 'ab', 'aa'],
        'Name': ['Tom', 'Tom', 'Tom', 'Tom', 'nick', 'nick', 'nick'],
        'Age': [20,20,20,20, 21, 21, 21]}

# Create DataFrame
df = pd.DataFrame(data)

print(df)
</code></pre>
<p>output</p>
<pre><code>   link code  Name  Age
0     1   xx   Tom   20
1     2   xx   Tom   20
2     3   xy   Tom   20
3     4        Tom   20
4     5   aa  nick   21
5     6   ab  nick   21
6     7   aa  nick   21
</code></pre>
<p>minimal code example that works on subset of data but not on full dataset.</p>
<pre><code>temp = df.groupby(['Name', 'Age'])['code'].apply(list).reset_index()
pd.merge(df, temp, on=['Name', 'Age']).explode('code_y').replace(r'^\s*$', np.nan, regex=True).dropna(subset='code_y').drop_duplicates()
</code></pre>
<p>output
<a href=""https://i.sstatic.net/3BeMZ1lD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3BeMZ1lD.png"" alt=""enter image description here"" /></a></p>
<p>error when used on full dataset.</p>
<pre><code>### MemoryError: Unable to allocate 5.34 TiB for an array with shape (733324768776,) and data type object
</code></pre>
<p>The apply list makes a big long list with duplicates. Is there a way to drop dups from the lists or is there maybe a better way to do this?</p>
<p><strong>update</strong></p>
<p>Going with this code as it seem to work best.</p>
<pre><code># Select relevant columns and drop duplicates directly
d = df[['code', 'Name', 'Age']].replace('', pd.NA).drop_duplicates()

# Perform the merge and drop rows with missing 'code_y' in one step
df.merge(d, how='outer', on=['Name', 'Age']).dropna(subset=['code_y'])
</code></pre>
","1","Question"
"79604038","","<p>Is there a way to make a Bar Chart not be this small? I have thousands of horizontal bars in my chart, so I want to scroll up/down and every time I add a new plot the chart data keeps shrinking? I want to keep scrolling without any changes in size, is this possible?</p>
<pre><code>import pandas as pd
import plotly.graph_objects as go
from dash import Dash, html, dcc

# Load data from CSV
csv_file = 'prices.csv'
df = pd.read_csv(csv_file)

# Create the Dash app
app = Dash(__name__)

# Define the layout of the app
app.layout = html.Div(
    [
        dcc.Graph(
            figure=go.Figure(
                data=[go.Bar(y=df['price'], x=df['list'], orientation='h')],
                layout={
                    'height': 2500 # Adjust height to control scrollbar visibility
                }
            )
        ),
    ],
    style={
        'overflow-y': 'auto',
        'height': 800  # Set a fixed height for the scrollable area
    }
)

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
<p>What it presently looks like:
<a href=""https://i.sstatic.net/fpE5Du6t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fpE5Du6t.png"" alt=""enter image description here"" /></a></p>
<p>At the end I want to achieve a plot like below (<strong>Source: Random Image on the Internet</strong>)`
<a href=""https://i.sstatic.net/yo8ose0w.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yo8ose0w.png"" alt=""enter image description here"" /></a></p>
","-6","Question"
"79604183","","<p>I have a time series dataframe, I would like to take x random samples from column &quot;temperature&quot; from each day. I am able to do this with:</p>
<pre><code>daily_groups = df.groupby([pd.Grouper(key='time', freq='D')])['temperature'].apply(lambda x: x.sample(10))
</code></pre>
<p>This works if there are at least x samples for each day. If there are not I get the error &quot;a must be greater than 0 unless no samples are taken&quot;. What I would like is, lets say I want 10 samples, get 10 if available, if not get as many as possible and if there are none skip this day. I don't want to up sample data.</p>
<p>Also I don't know if its possible to return the original dataframe with the values filtered for items mention above, instead of returning the groupby series object. Thanks for any help.</p>
","2","Question"
"79605688","","<p>I'm working with a Pandas DataFrame where I need to track a boolean flag (<code>in_position</code>)<br />
that becomes <code>True</code> after an <code>entry_signal</code> and resets to <code>False</code> only after an <code>exit_signal</code>.</p>
<p>This must be done vectorized only — no loops — to support millions of rows efficiently.</p>
<hr />
<h3>✅ Expected behavior:</h3>
<ul>
<li><code>in_position</code> should turn <code>True</code> only when entering a new state (after a clean entry)</li>
<li>It should <strong>stay True</strong>, even if additional <code>entry_signal</code>s appear inside that position</li>
<li>It should return to <code>False</code> <strong>only</strong> after a valid <code>exit_signal</code></li>
</ul>
<p>In other words, <strong><code>entry_signal</code> values during an open position must be ignored</strong>.</p>
<hr />
<h3>❌ The problem:</h3>
<p>The issue seems to come from the way cumulative comparison works:</p>
<pre class=""lang-py prettyprint-override""><code>df['in_position'] = df['final_entry'].cumsum() &gt; df['final_exit'].cumsum()
</code></pre>
<p>Even when both cumulative values reach 1, this comparison still evaluates to True on the row after the final_exit,
because both values are equal, and &gt; is no longer satisfied only on the same row, but not before it.</p>
<p>This results in in_position remaining True until the next row — or forever if no new exit appears</p>
<p>I understand that this is a side effect of how <code>.cumsum()</code> and <code>&gt;</code> behave in vectorized logic.<br />
What I'm struggling with is:<br />
➡️ <strong>How to replace this logic with a vectorized alternative that truly toggles the position on entry and resets it exactly at the exit?</strong></p>
<p>Any help would be appreciated!</p>
<p>Minimal reproducible example:</p>
<pre class=""lang-py prettyprint-override""><code># Step 1: Create minimal dataset with relevant values
data = {
    'Close':   [110, 111, 112, 113, 114, 115, 116],
    'RSI_10':  [None, 20, None, 41, 20, None, 61],
    'SMA_200': [100]*7,
}
index = pd.to_datetime([
    '2025-04-21', '2025-04-22', '2025-04-23', 
    '2025-04-24', '2025-04-25', '2025-04-27', '2025-04-28'
])
df = pd.DataFrame(data, index=index)

# Step 2: Signals
df['entry_signal']         = (df['RSI_10'] &lt; 30) &amp; (df['Close'] &gt; df['SMA_200'])
df['partial_exit_signal']  = df['RSI_10'] &gt; 40
df['exit_signal']          = df['RSI_10'] &gt; 60

# Step 3: Cumulative logic - the main problem
entry_cumsum  = df['entry_signal'].cumsum()
exit_cumsum   = df['exit_signal'].cumsum()

in_position   = entry_cumsum &gt; exit_cumsum
in_position_prev = in_position.shift(1).fillna(False).astype(bool)

# Step 4: Final signals
df['final_entry']   = df['entry_signal'] &amp; ~in_position_prev
df['partial_exit']  = df['partial_exit_signal'] &amp; in_position_prev
df['final_exit']    = df['exit_signal'] &amp; in_position_prev
df['in_position']   = in_position
df['in_position_prev'] = in_position_prev

print(df[['RSI_10', 'Close', 'entry_signal', 'final_entry', 'final_exit', 'in_position']])
</code></pre>
","1","Question"
"79605997","","<p>I have this <a href=""https://i.sstatic.net/3KE6KGpl.png"" rel=""nofollow noreferrer"">dataframe</a>.</p>
<p>I want to keep the first 'ENZYME' column and delete the other instances. I am using:</p>
<pre class=""lang-py prettyprint-override""><code>df_CL_HHA_2=df_CL_HHA.drop(df_CL_HHA.columns[[2,4,6]],axis=1)
</code></pre>
<p>But this leads to this <a href=""https://i.sstatic.net/8M6vL8jT.png"" rel=""nofollow noreferrer"">result</a>.</p>
<p>I am not sure why the first column is getting deleted despite setting 2,4,6 as my column indexes for dropping. Any help is much appreciated.</p>
","1","Question"
"79606593","","<p>I have an excel dataset with date-wise sheets for a month (April has 30 sheets). Each sheet has top two rows as header.</p>
<p>The top row is a merged one with empty cells in a number of columns. The columns are attached as picture for ready reference. How can I convert the top two rows as a single header for all the sheets in a single dataframe?</p>
<p><a href=""https://i.sstatic.net/KptVQFGy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KptVQFGy.png"" alt=""enter image description here"" /></a></p>
","0","Question"
"79606785","","<p>I have a dataframe, I need to select a range of data based on a month value, but the result expected is always showing six rows where the month selected appears in the filtered data , here's the code :</p>
<pre><code>import pandas as pd

data = {
    &quot;function&quot;: [&quot;test1&quot;,&quot;test2&quot;,&quot;test3&quot;,&quot;test4&quot;,&quot;test5&quot;,&quot;test6&quot;,&quot;test7&quot;,&quot;test8&quot;,&quot;test9&quot;,&quot;test10&quot;,&quot;test11&quot;,&quot;test12&quot;],
    &quot;service&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;AO&quot;, &quot;M&quot; ,&quot;A&quot;, &quot;PO&quot;, &quot;MP&quot;, &quot;YU&quot;, &quot;Z&quot;, &quot;R&quot;, &quot;E&quot;, &quot;YU&quot;],
    &quot;month&quot;: [&quot;January&quot;,&quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot;]
}

df = pd.DataFrame(data)

selected_month = &quot;January&quot;
selected_month_idx = df[df[&quot;month&quot;] == selected_month].index[0]
six_months_indices = [i % len(df) for i in range(selected_month_idx - 2, selected_month_idx + 4)]
six_months_df = df.loc[six_months_indices] # add .reset_index(drop=True) if needed
print(six_months_df)
</code></pre>
<p>output :</p>
<pre><code>   function service     month
10   test11       E  November
11   test12      YU  December
0     test1       A   January
1     test2       B  February
2     test3      AO     March
3     test4       M     April
</code></pre>
<p>the issue with this code is when I select <code>January</code> for example, the order of the months is not good, what I expet is somthing like that :</p>
<pre><code>   function service     month
0     test1       A   January
1     test2       B  February
2     test3      AO     March
3     test4       M     April
3     test5       A     May
3     test6       PO    June
</code></pre>
<p>when I select <code>December</code> for example the output should be :</p>
<pre><code> function service      month
    6     test7      MP       July
    7     test8      YU     August
    8     test9       Z  September
    9    test10       R    October
    10   test11       E   November
    11   test12      YU   December
</code></pre>
<p>when I select <code>Octobre</code> the output should be for example :</p>
<pre><code>   function service      month
6     test7      MP       July
7     test8      YU     August
8     test9       Z  September
9    test10       R    October
10   test11       E   November
11   test12      YU   December
</code></pre>
<p>the order of the month displated matters, always <code>min(month)</code> =&gt; <code>max(month)</code> output like <code>this is not expected</code> :</p>
<p>output :</p>
<pre><code>   function service     month
10   test11       E  November
11   test12      YU  December
0     test1       A   January
1     test2       B  February
2     test3      AO     March
3     test4       M     April
</code></pre>
<p>anyone please could to adjust the code above,</p>
<p>thanks</p>
","3","Question"
"79607513","","<p>I am running an extraction of a table in Python using Pandas. I am getting this output:</p>
<p>PS: The extraction works with other tables, this is happening with a specific one, but it's not the first time I face this error. It has already happened with other table in the past.</p>
<pre class=""lang-bash prettyprint-override""><code>Pandas version: 2.2.3
SQLAlchemy version: 2.0.37
Oracledb version: 2.5.1
Python 3.10.12
Ubuntu 22.04.4 LTS

Traceback (most recent call last):
  File &quot;/SECRET/test.py&quot;, line 23, in &lt;module&gt;
    df = pd.read_sql(f'SELECT * FROM {origin_schema}.{table}', oracle_engine)
  File &quot;/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py&quot;, line 734, in read_sql
    return pandas_sql.read_query(
  File &quot;/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py&quot;, line 1853, in read_query
    data = result.fetchall()
  File &quot;/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/result.py&quot;, line 1315, in fetchall
    return self._allrows()
  File &quot;/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/result.py&quot;, line 548, in _allrows
    rows = self._fetchall_impl()
  File &quot;/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/cursor.py&quot;, line 2130, in _fetchall_impl
    return self.cursor_strategy.fetchall(self, self.cursor)
  File &quot;/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/cursor.py&quot;, line 1140, in fetchall
    self.handle_exception(result, dbapi_cursor, e)
  File &quot;/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/cursor.py&quot;, line 1081, in handle_exception
    result.connection._handle_dbapi_exception(
  File &quot;/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py&quot;, line 2355, in _handle_dbapi_exception
    raise exc_info[1].with_traceback(exc_info[2])
  File &quot;/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/cursor.py&quot;, line 1136, in fetchall
    rows = dbapi_cursor.fetchall()
  File &quot;/usr/local/lib/python3.10/dist-packages/oracledb/cursor.py&quot;, line 779, in fetchall
    row = fetch_next_row(self)
  File &quot;src/oracledb/impl/base/cursor.pyx&quot;, line 552, in oracledb.base_impl.BaseCursorImpl.fetch_next_row
  File &quot;src/oracledb/impl/thin/cursor.pyx&quot;, line 186, in oracledb.thin_impl.ThinCursorImpl._fetch_rows
  File &quot;src/oracledb/impl/thin/protocol.pyx&quot;, line 443, in oracledb.thin_impl.Protocol._process_single_message
  File &quot;src/oracledb/impl/thin/messages.pyx&quot;, line 1388, in oracledb.thin_impl.MessageWithData.postprocess
  File &quot;/usr/local/lib/python3.10/dist-packages/sqlalchemy/dialects/oracle/cx_oracle.py&quot;, line 1284, in _detect_decimal
    return self._to_decimal(value)
decimal.InvalidOperation: [&lt;class 'decimal.ConversionSyntax'&gt;]

</code></pre>
<p>The code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine, text
import oracledb

from os import getenv, system
from dotenv import load_dotenv
load_dotenv()

oracle_user = getenv('oracle_user')
oracle_password = getenv('oracle_password')
oracle_dsn = &quot;SECRET&quot;

oracle_engine = create_engine(f'oracle+oracledb://{oracle_user}:{oracle_password}@{oracle_dsn}')

origin_schema = getenv('origin_schema')
table = getenv('table')

# Show package version
print(f'Pandas version: {pd.__version__}')
print(f'SQLAlchemy version: {sqlalchemy.__version__}')
print(f'Oracledb version: {oracledb.__version__}')
print(system('python --version'))
print(system('cat /etc/issue'))

df = pd.read_sql(f'SELECT * FROM {origin_schema}.{table}', oracle_engine)
</code></pre>
<p>Probably is an error in some column, how do I resolve this?</p>
<p>I've already tried with <code>coerce_float=False</code>:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_sql(f'SELECT * FROM {origin_schema}.{table}', oracle_engine, coerce_float=False)
</code></pre>
<p>Or <code>astype(str)</code>:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_sql(f'SELECT * FROM {origin_schema}.{table}', oracle_engine, coerce_float=False).astype(str)
</code></pre>
<p>I have no idea what column is the problem, because the traceback does not return this information.</p>
<p><a href=""https://i.sstatic.net/Evs22WZP.png"" rel=""nofollow noreferrer"">This is a <code>describe</code> of the table in DBeaver</a></p>
<p><strong>Edit</strong>: I'm doing SELECT clauses for every column individually to see which column is returning the error, but it'll take a lot of time, because the table is huge.</p>
<p><strong>Edit 2</strong>: The error is in QT_OC, a number type column. Here is a sample of the data (separated by semicolons):</p>
<blockquote>
<p>1; -2; 11904.72; 5019.1860465116; 2.1060606061; 116550; 0.0055438144; -0.0000907615</p>
</blockquote>
<p>I've tried to parse float manually and it works <strong>without</strong> errors, I don't know why pandas is struggling with this (test.json is all the values of column QT_OC exported by DBeaver):</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_json('test.json')

for i, item in enumerate(df['QT_OC']):
    print(i, float(item))
</code></pre>
","0","Question"
"79608124","","<p>I want to make a chart in altair with the following properties:</p>
<ul>
<li>It shows multiple time series</li>
<li>I can select which time series are displayed by clicking on the legend</li>
<li>If a series is unselected in the legend, then it is not displayed in the plot</li>
<li>If a series is unselected in the legend, then you can still see it in the legend</li>
<li>Colours of lines remain consistent, no matter which time series are selected or unselected</li>
</ul>
<p>An LLM supplied me with the following code which doesn't do what I want</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import altair as alt

dates = pd.date_range(start=&quot;2023-01-01&quot;, periods=10)
data = {
    &quot;Date&quot;: dates.tolist() * 3,
    &quot;Value&quot;: [10, 20, 15, 30, 25, 35, 40, 45, 50, 55] +
             [5, 15, 10, 20, 15, 25, 30, 35, 40, 45] +
             [2, 12, 8, 18, 14, 22, 28, 32, 38, 42],
    &quot;Device&quot;: [&quot;Device A&quot;] * 10 + [&quot;Device B&quot;] * 10 + [&quot;Device C&quot;] * 10
}
df = pd.DataFrame(data)

# Create a selection object for the legend
aselection = alt.selection_multi(fields=[&quot;Device&quot;], bind=&quot;legend&quot;)

# Create the Altair chart
achart = alt.Chart(df).mark_line().encode(
    x=&quot;Date:T&quot;,
    y=&quot;Value:Q&quot;,
    color=&quot;Device:N&quot;  # Keep the color consistent
).transform_filter(
    aselection  # Filter data based on the selection
).add_selection(
    aselection
).properties(
    title=&quot;Interactive Time Series Plot&quot;
)

achart
</code></pre>
<p>It seems that the <code>transform_filter</code> makes an unselected series vanish from the chart, but this also forces it to vanish from the legend. I want it to vanish from the chart but stay in the legend.</p>
<p>Alternatively, I have the following code</p>
<pre class=""lang-py prettyprint-override""><code>chart = alt.Chart(df).mark_line().encode(
    x=&quot;Date:T&quot;,
    y=&quot;Value:Q&quot;,
    color=&quot;Device:N&quot;,  # Keep the color consistent
    opacity=alt.condition(aselection, alt.value(1), alt.value(0.2))  # Hide unselected lines
).add_selection(
    aselection
).properties(
    title=&quot;Interactive Time Series Plot&quot;
)
</code></pre>
<p>Now the legend works in the way I want (you can select multiple series by shift-clicking) but an unselected time series is still displayed on the chart, just in a paler colour. If I make the chart interactive, then the end user will often end up clicking on an unselected series by accident, so this is definitely not what I want either.</p>
<p>Edit: the following almost works, but the problem is that the user can still affect the selection by clicking on the plot. Is there a way to make the plot interactive but disable clicking?</p>
<pre class=""lang-py prettyprint-override""><code>achart = alt.Chart(df).mark_line().encode(
    x=&quot;Date:T&quot;,
    y=&quot;Value:Q&quot;,
    color=&quot;Device:N&quot;,  # Keep the color consistent
).transform_filter(
    aselection
).add_selection(aselection)

chart2 = alt.Chart(df).encode(
    x=&quot;Date:T&quot;,
    y=&quot;Value:Q&quot;,
    color=&quot;Device:N&quot;,  # Keep the color consistent  # Hide unselected lines
).mark_line(opacity=0).properties(
    title=&quot;Interactive Time Series Plot&quot;
)

chart2 + achart.interactive()
</code></pre>
<p>Is there a way to do what I want?</p>
","1","Question"
"79608369","","<p>I do generate that figure with <code>seaborn.distplot()</code>. My problem is that the ticks on the X-axis do not fit to the bars, in all cases. I would expect a relationship between bars and ticks like you can see at 11 and 15.</p>
<p><a href=""https://i.sstatic.net/BOCA6W8z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BOCA6W8z.png"" alt=""seaborn distplot"" /></a></p>
<p>This is the MWE</p>
<pre><code>import numpy as np
import pandas as pd
import seaborn as sns

# Data
np.random.seed(42)
n = 5000
df = pd.DataFrame({
    'PERSON': np.random.randint(100000, 999999, n),
    'Fruit': np.random.choice(['Banana', 'Strawberry'], n),
    'Age': np.random.randint(9, 18, n)
})

fig = sns.displot(
    data=df,
    x='Age',
    hue='Fruit',
    multiple='dodge').figure

fig.show() 
</code></pre>
","3","Question"
"79608662","","<p>I currently have Pandas 1.4.2 installed. I want to update to the latest version, so I entered this into the command prompt:</p>
<blockquote>
<p>pip install -U pandas</p>
</blockquote>
<p>However, it just returns this message:</p>
<blockquote>
<p>Requirement already satisfied: pandas in
c:\users\my_username\appdata\local\programs\python\python39\lib\site-packages
(1.4.2)</p>
</blockquote>
<p>Which sounds like it won't install the new version because I already have Pandas installed. But I thought the -U specifies that you want to update an existing package?</p>
<p>Where am I going wrong? I'm using pip 22.0.4 and Python 3.9.12 if that helps.</p>
","0","Question"
"79608752","","<p>I have a bubble chart developed using plotly library and  here’s the data :</p>
<pre><code>import plotly.express as px
import pandas as pd
data = {
    &quot;lib_acte&quot;:[&quot;test 98lop1&quot;, &quot;test9665 opp1&quot;, &quot;test QSDFR1&quot;, &quot;test ABBE1&quot;, &quot;testtest21&quot;,&quot;test23&quot;],
    &quot;x&quot;:[12.6, 10.8, -1, -15.2, -10.4, 1.6],
    &quot;y&quot;:[15, 5, 44, -11, -35, -19],
    &quot;circle_size&quot;:[375, 112.5, 60,210, 202.5, 195],
    &quot;color&quot;:[&quot;green&quot;, &quot;green&quot;, &quot;green&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;]
}

#load data into a DataFrame object:
df = pd.DataFrame(data)

fig = px.scatter(
        df,
        x=&quot;x&quot;, 
        y=&quot;y&quot;, 
        color=&quot;color&quot;,
        size='circle_size',
        text=&quot;lib_acte&quot;,
        hover_name=&quot;lib_acte&quot;,
        color_discrete_map={&quot;red&quot;: &quot;red&quot;, &quot;green&quot;: &quot;green&quot;},
        title=&quot;chart&quot;
      )
fig.update_traces(textposition='middle right', textfont_size=14, textfont_color='black', textfont_family=&quot;Inter&quot;, hoverinfo=&quot;skip&quot;)
newnames = {'red':'red title', 'green': 'green title'}

fig.update_layout(
        {
            
            'yaxis': {
                &quot;range&quot;: [-200, 200],
                'zerolinewidth': 2, 
                &quot;zerolinecolor&quot;: &quot;red&quot;,
                &quot;tick0&quot;: -200,
                &quot;dtick&quot;:45,
            },
            'xaxis': {
                &quot;range&quot;: [-200, 200],
                'zerolinewidth': 2, 
                &quot;zerolinecolor&quot;: &quot;gray&quot;,
                &quot;tick0&quot;: -200,
                &quot;dtick&quot;: 45,
                #  &quot;scaleanchor&quot;: 'y'
            },
           
            &quot;height&quot;: 800,
            
            
        }
    )
fig.add_scatter(
        x=[0, 0, -200, -200],
        y=[0, 200, 200, 0],
        fill=&quot;toself&quot;,
        fillcolor=&quot;gray&quot;,
        zorder=-1,
        mode=&quot;markers&quot;,
        marker_color=&quot;rgba(0,0,0,0)&quot;,
        showlegend=False,
        hoverinfo=&quot;skip&quot;
    )
fig.add_scatter(
        x=[0, 0, 200, 200],
        y=[0, -200, -200, 0],
        fill=&quot;toself&quot;,
        fillcolor=&quot;yellow&quot;,
        zorder=-1,
        mode=&quot;markers&quot;,
        marker_color=&quot;rgba(0,0,0,0)&quot;,
        showlegend=False,
        hoverinfo=&quot;skip&quot;
    )
fig.update_layout(
   
            paper_bgcolor=&quot;#F1F2F6&quot;,
        )
fig.show()
</code></pre>
<p>output :</p>
<p><a href=""https://i.sstatic.net/9BLQjlKN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9BLQjlKN.png"" alt=""enter image description here"" /></a></p>
<p>now what I’m looking for please a way to add space between bubble if they are tight like (test 981op1 and test9665 opp1), and also a way to to increase the each bubble size 4% of its size for example.
thanks for your help.</p>
","3","Question"
"79610568","","<p>I want to store a numpy array in pandas cell.</p>
<p>This does not work:</p>
<pre><code>import numpy as np
import pandas as pd
bnd1 = np.random.rand(74,8)
bnd2 = np.random.rand(74,8)

df = pd.DataFrame(columns = [&quot;val&quot;, &quot;unit&quot;])
df.loc[&quot;bnd&quot;] = [bnd1, &quot;N/A&quot;]
df.loc[&quot;bnd&quot;] = [bnd2, &quot;N/A&quot;]
</code></pre>
<p>But this does:</p>
<pre><code>import numpy as np
import pandas as pd
bnd1 = np.random.rand(74,8)
bnd2 = np.random.rand(74,8)

df = pd.DataFrame(columns = [&quot;val&quot;])
df.loc[&quot;bnd&quot;] = [bnd1]
df.loc[&quot;bnd&quot;] = [bnd2]
</code></pre>
<p>Can someone explain why, and what's the solution?</p>
<p>Edit:</p>
<p>The first returns:</p>
<blockquote>
<p>ValueError: setting an array element with a sequence. The requested
array has an inhomogeneous shape after 1 dimensions. The detected
shape was (2,) + inhomogeneous part.</p>
</blockquote>
<p>The complete traceback is below:</p>
<pre><code>&gt; --------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
&gt; last) File
&gt; ~/anaconda3/envs/py38mats/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3185,
&gt; in ndim(a)    3184 try:
&gt; -&gt; 3185     return a.ndim    3186 except AttributeError:
&gt; 
&gt; AttributeError: 'list' object has no attribute 'ndim'
&gt; 
&gt; During handling of the above exception, another exception occurred:
&gt; 
&gt; ValueError                                Traceback (most recent call
&gt; last) Cell In[10], line 8
&gt;       6 df = pd.DataFrame(columns = [&quot;val&quot;, &quot;unit&quot;])
&gt;       7 df.loc[&quot;bnd&quot;] = [bnd1, &quot;N/A&quot;]
&gt; ----&gt; 8 df.loc[&quot;bnd&quot;] = [bnd2, &quot;N/A&quot;]
&gt; 
&gt; File
&gt; ~/anaconda3/envs/py38mats/lib/python3.8/site-packages/pandas/core/indexing.py:849,
&gt; in _LocationIndexer.__setitem__(self, key, value)
&gt;     846 self._has_valid_setitem_indexer(key)
&gt;     848 iloc = self if self.name == &quot;iloc&quot; else self.obj.iloc
&gt; --&gt; 849 iloc._setitem_with_indexer(indexer, value, self.name)
&gt; 
&gt; File
&gt; ~/anaconda3/envs/py38mats/lib/python3.8/site-packages/pandas/core/indexing.py:1835,
&gt; in _iLocIndexer._setitem_with_indexer(self, indexer, value, name)   
&gt; 1832 # align and set the values    1833 if take_split_path:    1834   
&gt; # We have to operate column-wise
&gt; -&gt; 1835     self._setitem_with_indexer_split_path(indexer, value, name)    1836 else:    1837     self._setitem_single_block(indexer,
&gt; value, name)
&gt; 
&gt; File
&gt; ~/anaconda3/envs/py38mats/lib/python3.8/site-packages/pandas/core/indexing.py:1872,
&gt; in _iLocIndexer._setitem_with_indexer_split_path(self, indexer, value,
&gt; name)    1869 if isinstance(value, ABCDataFrame):    1870    
&gt; self._setitem_with_indexer_frame_value(indexer, value, name)
&gt; -&gt; 1872 elif np.ndim(value) == 2:    1873     # TODO: avoid np.ndim call in case it isn't an ndarray, since    1874     #  that will
&gt; construct an ndarray, which will be wasteful    1875    
&gt; self._setitem_with_indexer_2d_value(indexer, value)    1877 elif
&gt; len(ilocs) == 1 and lplane_indexer == len(value) and not
&gt; is_scalar(pi):    1878     # We are setting multiple rows in a single
&gt; column.
&gt; 
&gt; File &lt;__array_function__ internals&gt;:200, in ndim(*args, **kwargs)
&gt; 
&gt; File
&gt; ~/anaconda3/envs/py38mats/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3187,
&gt; in ndim(a)    3185     return a.ndim    3186 except AttributeError:
&gt; -&gt; 3187     return asarray(a).ndim
&gt; 
&gt; ValueError: setting an array element with a sequence. The requested
&gt; array has an inhomogeneous shape after 1 dimensions. The detected
&gt; shape was (2,) + inhomogeneous part.
</code></pre>
<p>I'm using <code>pandas 2.0.3</code> and <code>numpy 1.24.4</code></p>
","2","Question"
"79611884","","<p>I'm working on a Snakemake workflow where I need to combine multiple CSV files into a single Pandas DataFrame. The list of CSV files is dynamic—it depends on upstream rules and wildcard patterns.</p>
<p>Here's a simplified version of what I have in my Snakefile:</p>
<pre><code>rule combine_tables:
    input:
        expand(&quot;results/{sample}/data.csv&quot;, sample=SAMPLES)
    output:
        &quot;results/combined/all_data.csv&quot;
    run:
        import pandas as pd
        dfs = [pd.read_csv(f) for f in input]
        combined = pd.concat(dfs)
        combined.to_csv(output[0], index=False)

</code></pre>
<p>This works when the files exist, but I’d like to know:</p>
<p>What's the best practice for handling missing or corrupt files in this context?</p>
<p>Is there a more &quot;Snakemake-idiomatic&quot; way to dynamically list and read input files for Pandas operations?</p>
<p>How do I ensure proper file ordering or handle metadata like sample names if not all CSVs are structured identically?</p>
","2","Question"
"79611944","","<p>I have a function that returns a data frame and a date string.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>pattern</th>
<th>2025-05-07</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>104</td>
</tr>
<tr>
<td>b</td>
<td>87</td>
</tr>
<tr>
<td>c</td>
<td>34</td>
</tr>
</tbody>
</table></div>
<p>I am applying the function on multiple dates to generate similar data frames as above.
The resulting dataframes are concatenated to generate a final dataframe as below.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>pattern</th>
<th>2025-05-07</th>
<th>2025-05-08</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>104.0</td>
<td>7.0</td>
</tr>
<tr>
<td>b</td>
<td>87.0</td>
<td>0.0</td>
</tr>
<tr>
<td>c</td>
<td>34.0</td>
<td>2.0</td>
</tr>
</tbody>
</table></div>
<p>I am trying to generate a filter condition using the date strings received as a output of the functions.
Since, I do not have any control on the input dates, I am trying to generate a filter condition dynamically as below -</p>
<pre><code>for index, date_str in enumerate(date_str_list):
    if index == 0:
        filter_condition = final_df[date_str] &gt; 1
    else:
        filter_condition = filter_condition &amp; final_df[date_str] &gt; 1
print('filter_condition',filter_condition)
final_filtered_df = final_df[filter_condition]
print('final_filtered_df\n',final_filtered_df)
</code></pre>
<p>When I apply the above filter I get an empty dataframe.</p>
<p>But when I run the hardcoded filter I get proper data.</p>
<pre><code>final_filtered_df = final_df[(final_df['2025-05-07'] &gt; 1) &amp; (final_df['2025-05-08'] &gt; 1)]
</code></pre>
<p>Can anyone let me know where I am going wrong. Would appreciate help on this.</p>
","0","Question"
"79613039","","<p>I have a long list of items that I want to assign a number to that increases by one every time the value in the list changes. Basically I want to categorize the values in the list.</p>
<p>It can be assumed that the values in the list are always lumped together, but I don't know the number of instances it's repeating. The list is stored in a dataframe as of now, but the output needs to be a dataframe.
Example:</p>
<pre><code>my_list = ['Apple', 'Apple', 'Orange', 'Orange','Orange','Banana']
grouping = pd.DataFrame(my_list, columns=['List'])
</code></pre>
<p>Expected output:</p>
<pre><code>     List  Value
0   Apple      1
1   Apple      1
2  Orange      2
3  Orange      2
4  Orange      2
5  Banana      3
</code></pre>
<p>I have tried with a <code>for</code> loop, where it checks if the previous value is the same as the current value, but I imagine that there should be a nicer way of doing this.</p>
","4","Question"
"79614376","","<p>I have a dataframe 'surv_both' that's 604x660 and a list 'consolidate_ptids' of 538 record ids. I am trying to generate a dataframe that isolates those 538 records using the following code:</p>
<pre><code>surv_both = surv_both[surv_both['record_id'].isin(consolidate_ptids)]
</code></pre>
<p>The resulting dataframe is 540x660, two rows more than it should be. What am I doing wrong?</p>
<p>'consolidate_ptids' is a list generated from sets of shared record ids. I have tried resetting indexes and dropping duplicates.</p>
","0","Question"
"79614700","","<p>I'm plotting date vs frequency horizontal bar charts that compares the monthly distribution pattern over time for a selection of crimes as subplots. The problem is the tick labels of the y-axis, which represents the date, display all the months over period of 2006-2023. I want to instead display the year whilst preserving the monthly count of the plot. Basically change the scale from month to year without changing the data being plotted.</p>
<p>Here's a sample of my code below:</p>
<p>Dataset: <a href=""https://drive.google.com/file/d/11MM-Vao6_tHGTRMsLthoMGgtziok67qc/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/11MM-Vao6_tHGTRMsLthoMGgtziok67qc/view?usp=sharing</a></p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

df = pd.read_csv('NYPD_Arrests_Data__Historic__20250113_111.csv')

df['ARREST_DATE'] = pd.to_datetime(df['ARREST_DATE'], format = '%m/%d/%Y')
df['ARREST_MONTH'] = df['ARREST_DATE'].dt.to_period('M').dt.to_timestamp()

# crimes, attributes and renames
crimes = ['DANGEROUS DRUGS', 'DANGEROUS WEAPONS', 'ASSAULT 3 &amp; RELATED OFFENSES', 'FELONY ASSAULT']
attributes = ['PERP_RACE']
titles = ['Race']

# loops plot creation over each attribute
for attr, title in zip(attributes, titles):
    fig, axes = plt.subplots(1, len(crimes), figsize = (4 * len(crimes), 6), sharey = 'row')
    
    for i, crime in enumerate(crimes):
        ax = axes[i]
        crime_df = df[df['OFNS_DESC'] == crime]

        pivot = pd.crosstab(crime_df['ARREST_MONTH'], crime_df[attr])

        # plots stacked horizontal bars
        pivot.plot(kind = 'barh', stacked = True, ax = ax, width = 0.9, legend = False)
        ax.set_title(crime)
        ax.set_xlabel('Frequency')
        ax.set_ylabel('Month' if i == 0 else '')  # shows the y-axis only on first plot
        ax.xaxis.set_tick_params(labelsize = 8)
        ax.set_yticks(ax.get_yticks())
        
    # adds one common legend accoss plots
    handles, labels = ax.get_legend_handles_labels()
    fig.legend(handles, labels, title = title, loc = 'upper center', ncol = len(df[attr].unique()), bbox_to_anchor = (0.5, 0.94))
    
    fig.suptitle(f'Crime Frequency Distribution by Year and {title}', fontsize = 20)
    plt.tight_layout(rect = [0, 0, 1, 0.90])
    plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/65TZ2pYB.png"" rel=""nofollow noreferrer"">Here's an image of what I currently see.</a></p>
","1","Question"
"79614935","","<p>I have 2 Dataframe of different lengths (number of rows and columns are different )but having some same column names
I want to filter out rows of DF1 where column value of Dataframe 1 is present in column value of Dataframe 2</p>
<p><strong>eg: DF1:</strong></p>
<p><a href=""https://i.sstatic.net/gYjJNgwI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gYjJNgwI.png"" alt=""enter image description here"" /></a></p>
<p><strong>DF2</strong>:</p>
<p><a href=""https://i.sstatic.net/xFQApjQi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFQApjQi.png"" alt=""enter image description here"" /></a></p>
<p><strong>Result DF:</strong></p>
<p><a href=""https://i.sstatic.net/8slx0xTK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8slx0xTK.png"" alt=""enter image description here"" /></a></p>
<p>I tried using <code>df.loc</code> but it is not working as the lengths of the dataframes are different. If <code>Itow</code> in <code>DF1</code> is present in <code>DF2</code> then it should filter out that row from <code>DF1</code>.</p>
<pre><code>res = x.loc[(x[&quot;_iTOW&quot;]) == (act_3d_fixes_spoof[&quot;_iTOW&quot;])]
</code></pre>
","0","Question"
"79615284","","<p>I have a dataframe as below and I want remove the duplicates and want the output as mentioned below. Tried few things but not working as expected. New to pandas.</p>
<pre><code>import pandas as pd
# Sample DataFrame
data = {
&quot;some_id&quot;: &quot;xxx&quot;,
&quot;some_email&quot;: &quot;abc.xyz@somedomain.com&quot;,
&quot;This is Sample&quot;: [
  {
   &quot;a&quot;: &quot;22&quot;,
   &quot;b&quot;: &quot;Y&quot;,
   &quot;c&quot;: &quot;33&quot;,
   &quot;d&quot;: &quot;x&quot;
  },
  {
   &quot;a&quot;: &quot;44&quot;,
   &quot;b&quot;: &quot;N&quot;,
   &quot;c&quot;: &quot;55&quot;,
   &quot;d&quot;: &quot;Y&quot;
  },
  {
   &quot;a&quot;: &quot;22&quot;,
   &quot;b&quot;: &quot;Y&quot;,
   &quot;c&quot;: &quot;33&quot;,
   &quot;d&quot;: &quot;x&quot;
  },
  {
   &quot;a&quot;: &quot;44&quot;,
   &quot;b&quot;: &quot;N&quot;,
   &quot;c&quot;: &quot;55&quot;,
   &quot;d&quot;: &quot;Y&quot;
  },
  {
   &quot;a&quot;: &quot;22&quot;,
   &quot;b&quot;: &quot;Y&quot;,
   &quot;c&quot;: &quot;33&quot;,
   &quot;d&quot;: &quot;x&quot;
  },
  {
   &quot;a&quot;: &quot;44&quot;,
   &quot;b&quot;: &quot;N&quot;,
   &quot;c&quot;: &quot;55&quot;,
   &quot;d&quot;: &quot;Y&quot;
  }
]
}

df = pd.DataFrame(data)
print(df)
</code></pre>
<p>The output is</p>
<pre><code>  some_id              some_email                              This is Sample
0     xxx  abc.xyz@somedomain.com  {'a': '22', 'b': 'Y', 'c': '33', 'd': 'x'}
1     xxx  abc.xyz@somedomain.com  {'a': '44', 'b': 'N', 'c': '55', 'd': 'Y'}
2     xxx  abc.xyz@somedomain.com  {'a': '22', 'b': 'Y', 'c': '33', 'd': 'x'}
3     xxx  abc.xyz@somedomain.com  {'a': '44', 'b': 'N', 'c': '55', 'd': 'Y'}
4     xxx  abc.xyz@somedomain.com  {'a': '22', 'b': 'Y', 'c': '33', 'd': 'x'}
5     xxx  abc.xyz@somedomain.com  {'a': '44', 'b': 'N', 'c': '55', 'd': 'Y'}
</code></pre>
<p>I want to remove duplicates and the output should look like</p>
<pre><code>  some_id              some_email                              This is Sample
0     xxx  abc.xyz@somedomain.com  {'a': '22', 'b': 'Y', 'c': '33', 'd': 'x'}
1     xxx  abc.xyz@somedomain.com  {'a': '44', 'b': 'N', 'c': '55', 'd': 'Y'}
</code></pre>
<p>How can this be achieved? I tried multiple ways some times it fails with unhashable dict.
I have pretty big nested data frame like this. I am using pandas dataframe and python. New to this technology</p>
","2","Question"
"79615560","","<p>I have financial data where I need to save / find rows that have multiple same value and a condition where the same value happened more than / = 2 and not (value)equal to 0 or &lt; 1.</p>
<p>Say I have this:</p>
<pre><code>                A       B       C       D       E       F       G       H       I
5/7/2025 21:00  0   0   0   0   0   0   0   0
5/7/2025 21:15  0   0   19598.8 0   19598.8 0   0   0
5/7/2025 21:30  0   0   0   0   0   0   0   0
5/7/2025 21:45  0   0   0   19823.35    0   0   0   0
5/7/2025 22:00  0   0   0   0   0   0   0   0
5/7/2025 22:15  0   0   0   0   0   0   0   0
5/7/2025 22:30  0   0   0   19975.95    0   19975.95    0   19975.95
5/7/2025 23:45  0   0   0   0   0   0   0   0
5/8/2025 1:00   0   0   19830.2 0   0   0   0   0
5/8/2025 1:15   0   0   0   0   0   0   0   0
5/8/2025 1:30   0   0   0   0   0   0   0   0
5/8/2025 1:45   0   0   0   0   0   0   0   0
</code></pre>
<p>I want this along with other datas in those rows:</p>
<pre><code>                A       B       C       D       E       F       G       H       I
5/7/2025 21:15  0   0   19598.8 0   19598.8 0   0   0
5/7/2025 22:30  0   0   0   19975.95    0   19975.95    0   19975.95
</code></pre>
","2","Question"
"79615698","","<p>I have a DataFrame with sales data that includes a 'date' and a 'sales' column. I want to group the data by date and calculate the total sales per day.</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({
    'date': ['2024-01-01', '2024-01-01', '2024-01-02'],
    'sales': [100, 150, 200]
})
</code></pre>
","-5","Question"
"79615990","","<p>I'm looking to transform a dataframe containing</p>
<p><code>[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</code></p>
<p>into <code>[[1, 2, 3, []], [4, 5, 6, [1, 2, 3, 4, 5, 6]], [7, 8, 9, [4, 5, 6, 7, 8, 9]], [10, 11, 12, [7, 8, 9, 10, 11, 12]]]</code></p>
<p>So far the only working solution I've come up with is:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

# Create the DataFrame
df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]))

# Initialize an empty list to store the result
result = []

# Iterate over the rows in the DataFrame
for i in range(len(df)):
    # If it's the first row, append the row with an empty list
    if i == 0:
        result.append(list(df.iloc[i]) + [[]])
    # If it's not the first row, concatenate the current and previous row
    else:
        current_row = list(df.iloc[i])
        previous_row = list(df.iloc[i-1])
        concatenated_row = current_row + [previous_row + current_row]
        result.append(concatenated_row)

# Print the result
print(result)
</code></pre>
<p>Is there no build in Pandas function that can roll a window, and add the results to current row, like the above can?</p>
","3","Question"
"79617077","","<p>I am trying to figure out how to code a reverse look up in pandas dataframe using groupby and looking for the owner of a max time.</p>
<p>`
import pandas as pd</p>
<pre><code>df = {'Name': ['Mike', 'Lilly', 'Frank', 'Jane', 'John', 'Matt', &quot;Alice&quot;, &quot;Ben&quot;],
  'Gender': [&quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot; ],
  'Grade': [10, 10, 11, 11, 10, 11,10,11],
  'Time': [10, 20, 15, 25, 12, 18, 15, 17]}
df = pd.DataFrame(df)

maxValues = df.groupby([&quot;Gender&quot;,&quot;Grade&quot;])[&quot;Time&quot;].max()
</code></pre>
<p>`</p>
<p>This gets my data sorted correctly into the maxValues DF and identifies the max time within the group.  I am stuck on how to go from here to correctly identify the &quot;Name&quot; or index that belongs to the max time in the original df.</p>
<p>I have made a little progress on the name side</p>
<p><code>theNames = df[(df['Gender'] == &quot;M&quot;) &amp; (df['Grade'] == 10) &amp; (df['Time'] == 10)] print(&quot;The Matching Name is %s&quot; % theNames[&quot;Name&quot;][0])</code></p>
<p>I just need some help on how to use parse the maxValues dataframe to get the values to put into my &quot;theNames&quot; call above.</p>
","1","Question"
"79618258","","<p>I tried to create a histogram with a legend outside the axes. Here is my code:</p>
<pre><code>import pandas as pd
import seaborn as sns

df_long = pd.DataFrame({
    &quot;Category&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;],
    &quot;Round&quot;: [&quot;Round1&quot;, &quot;Round1&quot;, &quot;Round1&quot;, &quot;Round1&quot;, &quot;Round2&quot;, &quot;Round2&quot;, &quot;Round2&quot;, &quot;Round2&quot;, &quot;Round3&quot;, &quot;Round3&quot;, &quot;Round3&quot;, &quot;Round3&quot;, &quot;Round4&quot;, &quot;Round4&quot;, &quot;Round4&quot;, &quot;Round4&quot;],
    &quot;Value&quot;: [10, 20, 10, 30, 20, 25, 15, 25, 12, 15, 19, 6, 10, 29, 13, 19]
  })
ax = sns.histplot(df_long, x=&quot;Category&quot;, hue=&quot;Round&quot;, weights=&quot;Value&quot;,
                  multiple=&quot;stack&quot;,
                  shrink=.8,
                  )
ax.set_ylabel('Weight')
legend = ax.get_legend()
legend.set_bbox_to_anchor((1, 1))
</code></pre>
<p>It works fine in jupyter notebook:</p>
<p><a href=""https://i.sstatic.net/H3jYqFCO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H3jYqFCO.png"" alt=""enter image description here"" /></a></p>
<p>But, if I try to create a png or pdf using matplotlib, the legend is not displayed completely.</p>
<pre><code>import matplotlib.pyplot as plt

plt.savefig(&quot;histogram.png&quot;)
plt.savefig(&quot;histogram.pdf&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/cWekRQ2g.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cWekRQ2g.png"" alt=""enter image description here"" /></a></p>
<p>I've already tried to adjust the size of the graph by using <code>plt.figure(figsize=(4, 4))</code> and the problem still exist.</p>
","2","Question"
"79618357","","<p>Package Versions:</p>
<ul>
<li>SQLAlchemy 2.0.40</li>
<li>pandas 2.2.3</li>
<li>psycopg2-binary 2.9.10</li>
</ul>
<p>I am trying to run a query using pandas' native param substitution, but I can't seem to get it to run without erroring. I tried simplifying the query to:</p>
<pre><code>select *
FROM public.bq_results br 
WHERE cast(&quot;eventDate&quot; as date) between 
  TO_DATE('%test_start_date', 'YYYYMMDD') AND TO_DATE('%test_end_date', 'YYYYMMDD')
limit 10000
</code></pre>
<p>but I get error:</p>
<pre><code>TypeError: dict is not a sequence
</code></pre>
<p>when running:</p>
<pre><code>df = pd.read_sql_query(query, self.__engine, params={&quot;test_start_date&quot;: &quot;20250101&quot;, &quot;test_end_date&quot;: &quot;20250131&quot;})
</code></pre>
<p>where</p>
<pre><code>self.__engine = create_engine(f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}')
</code></pre>
","2","Question"
"79618972","","<p>There is hourly data for 2 categories, and the value is the same for certain consecutive hours. To save space, the table is compressed as follows -</p>
<p>Input Table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Category</th>
<th>Datetime</th>
<th>Value</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1/1/2024 0:00</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>A</td>
<td>1/1/2024 1:00</td>
<td>6</td>
<td>3</td>
</tr>
<tr>
<td>A</td>
<td>1/1/2024 4:00</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>A</td>
<td>1/1/2024 5:00</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 0:00</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 2:00</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 4:00</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 5:00</td>
<td>5</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p>Desired Output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Category</th>
<th>Datetime</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1/1/2024 0:00</td>
<td>5</td>
</tr>
<tr>
<td>A</td>
<td>1/1/2024 1:00</td>
<td>6</td>
</tr>
<tr>
<td>A</td>
<td>1/1/2024 2:00</td>
<td>6</td>
</tr>
<tr>
<td>A</td>
<td>1/1/2024 3:00</td>
<td>6</td>
</tr>
<tr>
<td>A</td>
<td>1/1/2024 4:00</td>
<td>4</td>
</tr>
<tr>
<td>A</td>
<td>1/1/2024 5:00</td>
<td>2</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 0:00</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 1:00</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 2:00</td>
<td>2</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 3:00</td>
<td>2</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 4:00</td>
<td>4</td>
</tr>
<tr>
<td>B</td>
<td>1/1/2024 5:00</td>
<td>5</td>
</tr>
</tbody>
</table></div>
","0","Question"
"79619061","","<p>I have this kind of dataframe:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({
  &quot;A1&quot;: [1, 11, 111],
  &quot;A2&quot;: [2, 22, 222],
  &quot;A3&quot;: [3, 33, 333],
  &quot;A4&quot;: [4, 44, 444],
  &quot;A5&quot;: [5, 55, 555]
})

    A1   A2   A3   A4   A5
0    1    2    3    4    5
1   11   22   33   44   55
2  111  222  333  444  555
</code></pre>
<p>and this kind of mapping:</p>
<pre class=""lang-py prettyprint-override""><code>mapping = {
  &quot;A1&quot;: [&quot;A2&quot;, &quot;A3&quot;],
  &quot;A4&quot;: [&quot;A5&quot;]
}
</code></pre>
<p>which means that I want all columns in list to have values from key column so: A2 and A3 should be populated with values from A1, and A5 should be populated with values from A4. Resulting dataframe should look like this:</p>
<pre class=""lang-py prettyprint-override""><code>    A1   A2   A3   A4   A5
0    1    1    1    4    4
1   11   11   11   44   44
2  111  111  111  444  444
</code></pre>
<p>I managed to do it pretty simply like this:</p>
<pre class=""lang-py prettyprint-override""><code>for k, v in mapping.items():
  for col in v:
    df[col] = df[k]
</code></pre>
<p>but I was wondering if there is vectorized way of doing it (more pandactic way)?</p>
","5","Question"
"79619577","","<p>I am working with multi-index bar charts. My implementation uses python/pandas/hvplot. For example, the example on the hvplot documentation works for my case - <a href=""https://hvplot.holoviz.org/reference/tabular/bar.html"" rel=""nofollow noreferrer"">https://hvplot.holoviz.org/reference/tabular/bar.html</a></p>
<p>But, I also need to add value labels on top of the bars. This is working with a simple bar graphs like so -</p>
<pre><code>import pandas as pd
import hvplot.pandas # noqa
from bokeh.sampledata.autompg import autompg_clean as autompg
import hvplot
import holoviews as hv

autompg_long_form = autompg.groupby(&quot;yr&quot;).mean(numeric_only=True).reset_index()
bar=autompg_long_form.hvplot.bar(x='yr', y=&quot;mpg&quot;, width=1000)
labels = hv.Labels(data=autompg_long_form, kdims=['yr', 'mpg'], vdims='mpg')
hvplot.show(bar*labels)
</code></pre>
<p>However, expanding the approach, like this, to the multi-index case, is failing -</p>
<pre><code># Multi-index bar plot  with labels
autompg_multi_index = autompg.query(&quot;yr&lt;=80&quot;).groupby(['yr', 'origin']).mean(numeric_only=True)
plot=autompg_multi_index.hvplot.bar(width=1000, rot=90)
multi_labels = hv.Labels(data=autompg_multi_index, kdims=['origin', 'mpg'], vdims='mpg')
hvplot.show(plot*multi_labels)
</code></pre>
<p>The error is -</p>
<pre><code>raise ValueError(f&quot;failed to validate {obj_repr}.{name}: {error}&quot;)
ValueError: failed to validate FactorRange(id='p1003', ...).factors: expected an element of either Seq(String), Seq(Tuple(String, String)) or Seq(Tuple(String, String, String)), got [('70', 'Asia'), ('70', 'Europe'), ('70', 'North America'), ('71', 'Asia'), ('71', 'Europe'), ('71', 'North America'), ('72', 'Asia'), ('72', 'Europe'), ('72', 'North America'), ('73', 'Asia'), ('73', 'Europe'), ('73', 'North America'), ('74', 'Asia'), ('74', 'Europe'), ('74', 'North America'), ('75', 'Asia'), ('75', 'Europe'), ('75', 'North America'), ('76', 'Asia'), ('76', 'Europe'), ('76', 'North America'), ('77', 'Asia'), ('77', 'Europe'), ('77', 'North America'), ('78', 'Asia'), ('78', 'Europe'), ('78', 'North America'), ('79', 'Asia'), ('79', 'Europe'), ('79', 'North America'), ('80', 'Asia'), ('80', 'Europe'), ('80', 'North America'), np.str_('Asia'), np.str_('Europe'), np.str_('North America')]
</code></pre>
<p>How do I remedy this issue to add labels to the multi-index bar chart? Has anyone done this before or knows how to do it?</p>
<p>Thanks! in advance.</p>
<p>PS: This post is a copy of the one I have asked on the holoviz forum with no luck.</p>
","0","Question"
"79619950","","<p>In the below dataframe I would like to filter the columns based on a list called 'animals' to select all the columns that include the list elements.</p>
<pre><code>animal_data = {
    &quot;date&quot;: [&quot;2023-01-22&quot;,&quot;2023-11-16&quot;,&quot;2024-06-30&quot;,&quot;2024-08-16&quot;,&quot;2025-01-22&quot;],
    &quot;cats_fostered&quot;: [1,2,3,4,5],
    &quot;cats_adopted&quot;:[1,2,3,4,5],
    &quot;dogs_fostered&quot;:[1,2,3,4,5],
    &quot;dogs_adopted&quot;:[1,2,3,4,5],
    &quot;rabbits_fostered&quot;:[1,2,3,4,5],
    &quot;rabbits_adopted&quot;:[1,2,3,4,5]
}

animals = [&quot;date&quot;,&quot;cat&quot;,&quot;rabbit&quot;]

animal_data = {
    &quot;date&quot;: [&quot;2023-01-22&quot;,&quot;2023-11-16&quot;,&quot;2024-06-30&quot;,&quot;2024-08-16&quot;,&quot;2025-01-22&quot;],
    &quot;cats_fostered&quot;: [1,2,3,4,5],
    &quot;cats_adopted&quot;:[1,2,3,4,5],
    &quot;rabbits_fostered&quot;:[1,2,3,4,5],
    &quot;rabbits_adopted&quot;:[1,2,3,4,5]
}
</code></pre>
<p>I have tried some approaches below but they either don't work with lists or return no columns as it is looking for an exact match with 'cats' or 'rabbits' and not just columns that contain the strings</p>
<pre><code>animal_data[animal_data.columns.intersection(animals)]  # returns an empty df
animal_data.filter(regex=animals)  # returns an error: not able to use regex with a list
</code></pre>
","2","Question"
"79620333","","<p>I have an existing dataframe:</p>
<pre><code>data = [[5011025, 234], [5012025, 937], [5013025, 625]]
df = pd.DataFrame(data)
</code></pre>
<p>output:</p>
<pre><code>         0    1
0  5011025  234
1  5012025  937
2  5013025  625
</code></pre>
<p>What I need to do is insert a new column at <code>0</code> (the same # of rows) that contains 3 spaces. Recreating the dataframe, from scratch, it would be something like this:</p>
<pre><code>data = [['   ',5011025, 234], ['   ',5012025, 937], ['   ',5013025, 625]]
df = pd.DataFrame(data)
</code></pre>
<p>desired output:</p>
<pre><code>     0        1    2
0       5011025  234
1       5012025  937
2       5013025  625
</code></pre>
<p>What is the best way to <code>insert()</code> this new column into an existing dataframe, that may be hundreds of rows?
Ultimately, i'm trying to figure out how to write a function that will shift all columns of a dataframe x number of spaces to the right.</p>
","2","Question"
"79620822","","<p>Just simply read a csv using pd.read_csv(&quot;xxx.csv&quot;). Has anyone experienced the following error?</p>
<p>Python 3.9 works. And I am sure the csv has no issue.</p>
<pre><code>Python 3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.read_csv(&quot;xxxxxx.csv&quot;)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/io/parsers/readers.py&quot;, line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/io/parsers/readers.py&quot;, line 626, in _read
    return parser.read(nrows)
           ^^^^^^^^^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/io/parsers/readers.py&quot;, line 1968, in read
    df = DataFrame(
         ^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/core/frame.py&quot;, line 778, in __init__
    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/core/internals/construction.py&quot;, line 443, in dict_to_mgr
    arrays = Series(data, index=columns, dtype=object)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/core/series.py&quot;, line 490, in __init__
    index = ensure_index(index)
            ^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/core/indexes/base.py&quot;, line 7647, in ensure_index
    return Index(index_like, copy=copy, tupleize_cols=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/core/indexes/base.py&quot;, line 565, in __new__
    arr = sanitize_array(data, None, dtype=dtype, copy=copy)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/core/construction.py&quot;, line 654, in sanitize_array
    subarr = maybe_convert_platform(data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/hhung/miniconda3/envs/rag_poc/lib/python3.11/site-packages/pandas/core/dtypes/cast.py&quot;, line 138, in maybe_convert_platform
    arr = lib.maybe_convert_objects(arr)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;lib.pyx&quot;, line 2538, in pandas._libs.lib.maybe_convert_objects
TypeError: Cannot convert numpy.ndarray to numpy.ndarray
</code></pre>
<p>I created the virtual env using <code>conda create -n test_env python=3.11 -y</code>. Any idea?</p>
","0","Question"
"79620883","","<p>I want to combine two DataFrames of unequal length to a new DataFrame with the size of the larger one.
Now, specifically, I want to pad the values of the shorter array by repeating it until it is large enough.</p>
<p>I know this is possible for lists using <code>itertools.cycle</code> as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import cycle

x = range(7)
y = range(43)

combined = zip(cycle(x), y)
</code></pre>
<p>Now I want to do the same for DataFrames:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df1 = pd.DataFrame(...)  # length 7
df2 = pd.DataFrame(...)  # length 43

df_comb = pd.concat([cycle(df1),df2], axis=1)
</code></pre>
<p>Of course this doesn't work, but I don't know if there is an option to do this or to just manually repeat the array.</p>
","2","Question"
"79621650","","<p>I want my code to generate a set of timestamps with a particular density throughout a year. The idea was to plot the desired density function, chop it up into steps of difference, then cumulative sum them to get the actual points in time (from the start of the year) - nothing fancy.</p>
<p>Previously I tried to implement a code from <code>ChatGPT</code> (I marked the lines that I left in) but basically I had to rewrite most of it manually because it wasn't flexible enough.</p>
<pre class=""lang-py prettyprint-override""><code>    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    
    # number of data points
    event_num = 50000
    
    # desired density functions
    lin_up = np.linspace(0, 365, event_num)
    lin_down = lin_up[::-1]
    exp_up = np.exp(-lin_up / 100)[::-1]
    exp_down = np.exp(-lin_up / 100)
    harm_peak = 0.5 * (np.sin((lin_up / 58)-(np.pi/2)) + 1)
    harm_valley = 0.5 * (np.sin((lin_up / 58)+(np.pi/2)) + 1)
    ps_sigm_up = 0.5 * (np.sin((lin_up / 116)-(np.pi/2)) + 1)
    ps_sigm_down = 0.5 * (np.sin((lin_up / 116)+(np.pi/2)) + 1)
    
    act = harm_peak # so I don't have to manually update the actual function everywhere
    a_magsimum = np.max(act) # normalization 1
    act = act/a_magsimum # normalization 2
    
    # difference gives the steps (the smaller the difference, the larger the density)
    diff = 1 - act # here changeing the number changes the shape but the baseline too
    diff_magsimum = np.max(diff)
    
    # cumulative sum gives the actual days
    cum = np.cumsum(diff)
    cum_magsimum = np.max(cum)
    cum = 365 * cum/cum_magsimum
    
    plt.plot(act)
    plt.title(&quot;Desired density function&quot;)
    plt.show()
    
    plt.plot(diff)
    plt.title(&quot;Difference&quot;)
    plt.show()
    
    plt.plot(cum)
    plt.title(&quot;Cumulative sum&quot;)
    plt.show()
    
    # saving into CSV
    #df = pd.DataFrame({&quot;Date&quot;: pd.to_datetime(cum, unit=&quot;D&quot;, origin=&quot;2024-01-01&quot;)})
    #df.to_csv(&quot;events_new.csv&quot;, index=False)
    
    # dates daily (chatGPT)
    dates = pd.to_datetime(cum, unit=&quot;D&quot;, origin=&quot;2024-01-01&quot;).date
    density_per_day = pd.Series(dates).value_counts().sort_index()
    
    # dates weekly (chatGPT)
    #dates = pd.to_datetime(cum, unit=&quot;D&quot;, origin=&quot;2024-01-01&quot;)
    #density_per_week = dates.to_series().dt.to_period(&quot;W&quot;).value_counts().sort_index()
    
    # plotting graph - daily breakdown
    plt.plot(density_per_day.index, density_per_day.values, marker='o', linestyle='-')
    plt.xlabel(&quot;Date&quot;)
    plt.ylabel(&quot;Number of events / day&quot;)
    plt.title(&quot;Eventdensity by time&quot;)
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.show()
    
    # plotting graph - weekly breakdown
    #plt.plot(density_per_week.index.astype(str), density_per_week.values, marker='o', linestyle='-')
    #plt.xlabel(&quot;Week&quot;)
    #plt.ylabel(&quot;Number of events / week&quot;)
    #plt.title(&quot;Weekly event density&quot;)
    #plt.xticks(rotation=45)
    #plt.grid(True)
    #plt.show()
</code></pre>
<p>But the result density function is really different than the original - squished for some reason - and I don't seem to be able to figure out why.
<a href=""https://i.sstatic.net/LhRZ4Q5d.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LhRZ4Q5d.jpg"" alt=""enter image description here"" /></a></p>
<p>I observed that if I change the number from 1 to 2 at line 23 it will become a much better looking curve but the baseline will jump from 0 to half maximum (and I don't want that - don't see why that would be necessary).
<a href=""https://i.sstatic.net/OljIsTz1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OljIsTz1.jpg"" alt=""enter image description here"" /></a></p>
<p>I am currently using <code>Anaconda Navigator</code> and <code>Jupyter Notebook</code> but might use <code>VSCode</code> in the foreseeable future.</p>
","0","Question"
"79621793","","<p>I made a pretty simple script with python for a project, where i wanted to create a graph which shows water-production on the y-axis, and years on the x-axis. The graph is made by using a csv-file. However, for some reason the y-axis is only showing the chosen values for the x-values, instead of starting at 0 and increasing like usually. Basically it starts with 108 at the bottom, since that's the amount of produced water in the first year. Am i missing something?
Here's my code:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
filename = &quot;Waterproduction.csv&quot;
data = pd.read_csv(filename, sep=&quot;\\t&quot;, encoding=&quot;Latin-1&quot;, engine=&quot;python&quot;)
year = data.iloc[:, 0]
water =data.iloc[:, 2]
plt.figure()
plt.plot(year, water, marker='o')
plt.xlabel(&quot;Years&quot;)
plt.ylabel(&quot;Waterproduction (mill. m³)&quot;) 
plt.title(&quot;Waterproduction&quot;)
plt.show() 
</code></pre>
","0","Question"
"79621955","","<pre><code>import pandas as pd
df = pd.DataFrame(
    {
        &quot;group&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;],
        &quot;value&quot;: [10, 14, 3, 4, 9, 20],
    }
)

</code></pre>
<p><code>groupby.apply</code> is extremely slow on 10M+ rows. What is the simplest way and a memory-friendly way to subtract each group's mean from its rows while preserving the original order?</p>
<p>Is <code>groupby.transform(&quot;mean&quot;)</code> the right pattern, or is there a better-performing alternative I am missing.</p>
","0","Question"
"79623174","","<p><strong>Scenario:</strong> I have a pandas series that contains 3 values. These values can vary between nan, 0 and any value above zero. I am trying to get the pct_change among the series whenever possible.</p>
<p><strong>Examples:</strong></p>
<pre><code>[0,nan,50]
[0,0,0]
[0,0,50]
[nan,nan,50]
[nan,nan,0]
[0,0,nan]
[0,nan,0]
</code></pre>
<p><strong>What I tried:</strong> from other SO questions I was able to come up with methods either trying to ignore the nan or shifting, but these can potentially yield a result with empty values. Ideally, if a result cannot be calculated, I would like to output a 0.</p>
<p><strong>Code tried:</strong></p>
<pre><code>series_test = pd.Series([0,None,50])
series_test.pct_change().where(series_test.notna()) # tested but gives only NaN or inf

series_test.pct_change(fill_method=None)[series_test.shift(2).notnull()].dropna() # tested but gives empty result
</code></pre>
<p><strong>Question:</strong> What would be the correct way to approach this?</p>
<p><strong>Expected outputs:</strong></p>
<pre><code>[0,nan,50] - 0 (undefined case)
[0,0,0] - 0 (undefined case)
[0,0,50] - 0 (undefined case)
[nan,nan,50] - 0 (undefined case)
[nan,nan,0] - 0 (undefined case)
[0,0,nan] - 0 (undefined case)
[0,nan,0] - 0 (undefined case)
[1,nan,5] - 400%
[0,1,5] - 400%
[1,2,nan] - 100%
[1,1.3,1.8] - 80%
</code></pre>
","1","Question"
"79623688","","<p>I am working with Pandas DataFrames in <code>.py</code> files. I would like to have column name autocompletions in VSCode, similar to how it works with <code>.ipynb</code> files.</p>
<p>I remember reading I could subclass a <code>DataFrame</code> to achieve this. The post showed a code snippet similar to the following:</p>
<pre class=""lang-py prettyprint-override""><code>class TypedDataFrame(pd.DataFrame):
    day: int
    country: str
    weather: str

df_path: str = &quot;...&quot;
df: TypedDataFrame = pd.read_csv(df_path, delimiter=&quot;,&quot;)

# now typing &quot;df[&quot; would pull up autocomplete suggestions for the above annotations
</code></pre>
<p>I haven't been able to find the post again. Most of the questions relating this issue date back to 2023 and suggest using <code>Pandera</code>, <code>StaticFrame</code> and other typing libraries, so I think they may be updated now.</p>
<p>Is there a way to provide type hinting using only Pandas in 2025?</p>
","0","Question"
"79623716","","<p>Given a dataframe such as this:</p>
<pre><code>df = pd.DataFrame({'Drink': ['Beer', 'Beer', 'Wine', 'Wine', 'Wine', 'Whisky', 'Whisky'], 'Units': [14, 5, 9, 15, 7, 12, 17]})
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Drink</th>
<th>Units</th>
</tr>
</thead>
<tbody>
<tr>
<td>Beer</td>
<td>14</td>
</tr>
<tr>
<td>Beer</td>
<td>5</td>
</tr>
<tr>
<td>Wine</td>
<td>9</td>
</tr>
<tr>
<td>Wine</td>
<td>15</td>
</tr>
<tr>
<td>Wine</td>
<td>7</td>
</tr>
<tr>
<td>Whisky</td>
<td>12</td>
</tr>
<tr>
<td>Whisky</td>
<td>17</td>
</tr>
</tbody>
</table></div>
<p>How can I sort the Drink column using a list like this one?</p>
<pre><code>order = ['Wine', 'Beer', 'Whisky', 'Beer', 'Wine', 'Whisky']
</code></pre>
<p>So that the resulting dataframe looks like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Drink</th>
<th>Units</th>
</tr>
</thead>
<tbody>
<tr>
<td>Wine</td>
<td>9</td>
</tr>
<tr>
<td>Beer</td>
<td>14</td>
</tr>
<tr>
<td>Whisky</td>
<td>12</td>
</tr>
<tr>
<td>Beer</td>
<td>5</td>
</tr>
<tr>
<td>Wine</td>
<td>15</td>
</tr>
<tr>
<td>Whisky</td>
<td>17</td>
</tr>
</tbody>
</table></div>
<p>The initial dataframe has more rows than elements in the list, so once everything in the list has matched to a row, the remaining rows can be dropped.</p>
","4","Question"
"79623819","","<p>I'm using Anaconda Spyder and the following line of code is failing recently:</p>
<pre><code>url = 'https://intranet2.sbs.gob.pe/estadistica/financiera/2024/Julio/C-4252-jl2024.XLS'
df = pd.read_excel(url)
</code></pre>
<p>Last week I had no problems with this program; however, now I have the following problem (I already tried setting engine='xlrd'):</p>
<ul>
<li>Initial Error: ValueError: Excel file format cannot be determined,
you must specify an engine manually.</li>
<li>Error when I used engile=xlrd: XLRDError: Unsupported format, or
corrupt file: Expected BOF record; found b'\r\n'</li>
</ul>
<p>How can I go about fixing the error?</p>
","-1","Question"
"79624103","","<p>I have two linked dataframes which contain data on the status of something but both the data sources are poor. I have the status (e.g. 1=lost, 2=found, 3=unknown) and the date if found.</p>
<p>For instance, say <code>dfA</code> is so:</p>
<pre><code>ID, status, dateFound, registerDate
1,  1,      NaN ,      5/3/24
2,  2,      1/1/24,    1/1/24
3,  2,      2/1/24,    3/1/24
</code></pre>
<p>and 'dfB' is so:</p>
<pre><code>ID, thing_status, dateFound, date_registered
1,  2,            6/4/24,    23/4/24
2,  1,            NaN,       24/12/23
3,  2,            4/1/24,    5/1/24
</code></pre>
<p>I wish to create a new dataframe taking the last row from either <code>dfA</code> or <code>dfB</code> based on the latest registerDate/date_registered.</p>
<p>i.e. the ideal output would be:</p>
<pre><code>ID, status, dateFound, registerDate
1,  2,      6/4/24,    23/4/24
2,  2,      1/1/24,    1/1/24
3,  2,      4/1/24,    5/1/24
</code></pre>
<p>I am currently doing this using a for-loop but suspect there is a simpler way.</p>
","1","Question"
"79624224","","<p>I have a Pandas MultiIndex DataFrame set up like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Ticker</th>
<th>A</th>
<th>A</th>
<th>A</th>
<th>ZTS</th>
<th>ZTS</th>
<th>ZTS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Price</td>
<td>close</td>
<td>open</td>
<td>low</td>
<td>close</td>
<td>open</td>
<td>low</td>
</tr>
<tr>
<td>Date</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-07-01</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
</tr>
<tr>
<td>2022-07-02</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
</tr>
</tbody>
</table></div>
<p>Note the case of the &quot;open&quot;, &quot;close&quot;, and &quot;low&quot; headers. I would like to change the 'Price' level of the multiIndex column to be Title Case, like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Ticker</th>
<th>A</th>
<th>A</th>
<th>A</th>
<th>ZTS</th>
<th>ZTS</th>
<th>ZTS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Price</td>
<td>Close</td>
<td>Open</td>
<td>Low</td>
<td>Close</td>
<td>Open</td>
<td>Low</td>
</tr>
<tr>
<td>Date</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2022-07-01</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
</tr>
<tr>
<td>2022-07-02</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
<td>-data-</td>
</tr>
</tbody>
</table></div>
<p>I am doing this because I am changing to a new API to pull data and I'd like the formats to match the previous format so that my code will still work. <strong>Please know that my real dataset is much larger than this, so changing it by hand is not an option.</strong></p>
<p>I tried various things but I can't make heads or tails of how the pandas MultiIndexing works. I haven't found the docs applicable to large datasets already in hand.</p>
<p>How can I change just the case of these column names?</p>
<p>Entering df.columns shows this:</p>
<pre><code>    MultiIndex([(   'A',       'close'),
                (   'A',        'open'),
                (   'A',         'low'),
                ( 'ZTS',         'low'),
                ( 'ZTS',       'close'),
                ( 'ZTS',        'open'),
                names=['Ticker', 'Price'], length=6)
</code></pre>
","1","Question"
"79624229","","<pre><code>import numpy as np
import pandas as pd 

df = pd.read_csv('spotify_data.csv')
​
df.head()
</code></pre>
<p>Getting an error</p>
<blockquote>
<p>FileNotFoundError: [Errno 2] No such file or directory: 'spotify_data.csv'</p>
</blockquote>
<p>I have that dataset downloaded to my computer and I see it is showing in the Jupyter Notebook, but I don't have any idea why is not working. I am been looking into other similar cases, but cant see to find the answer.</p>
","-2","Question"
"79624459","","<p>I have 2 dataframes, dfA &amp; dfB. dfA contains purchases of certain products &amp; dfB contains info on said products.</p>
<p>For instance, dfA:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>purchaseID</th>
<th>productID</th>
<th>quantity</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>432</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>432</td>
<td>4</td>
</tr>
<tr>
<td>3</td>
<td>567</td>
<td>7</td>
</tr>
</tbody>
</table></div>
<p>and dfB:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>productID</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>432</td>
<td>'mower'</td>
</tr>
<tr>
<td>567</td>
<td>'cat'</td>
</tr>
</tbody>
</table></div>
<p>I wish to merge the two datasets on productID to produce something like:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>purchaseID</th>
<th>productID</th>
<th>quantity</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>432</td>
<td>1</td>
<td>'mower'</td>
</tr>
<tr>
<td>2</td>
<td>432</td>
<td>4</td>
<td>'mower'</td>
</tr>
<tr>
<td>3</td>
<td>567</td>
<td>7</td>
<td>'cat'</td>
</tr>
</tbody>
</table></div>
<p>In actual fact, dfA &amp; dfB are much larger.</p>
<p>How can I do this?</p>
<p>I understand how to do normal one-one merges, but struggling to see how to do one-many.</p>
","1","Question"
"79625976","","<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

my_data2 = pd.read_csv(&quot;heart.csv&quot;)

type(my_data2)

x = my_data2[&quot;trestbps&quot;]
y = my_data2[&quot;chol&quot;]

plt.xlabel(&quot;Resting Blood Pressure&quot;)
plt.ylabel(&quot;Cholesterol&quot;)

#Create and Display the Plot

plt.plot(x,y,&quot;*&quot;)
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.20, random_state = 0)

from sklearn.linear_model import LinearRegression as LNR
from sklearn.metrics import r2_score
type(X_train)

my_regression = LNR()
my_regression.fit(X_train, y_train) #HERE
</code></pre>
<p>I got &quot;Expected a 2-dimensional container but got &lt;class 'pandas.core.series.Series'&gt; instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.&quot;</p>
","1","Question"
"79626316","","<p>I am processing data from a form. I have imported the data as a dataframe.</p>
<p>The form has an initial date and time that have been converted into a datetime object. Partway through the form is a second time that is stored as a time object.</p>
<p>I wish to create a datetime object for the second time. However, the date may be the next day.</p>
<p>For instance, if the initial date was 2025-1-1 and the time was 10:25 PM, and the next time was 01:25 AM I wish to create a datetime for 2025-1-2 at 1:25 AM.</p>
<p>Is there a straightforward way to do this within a dataframe?</p>
<p>Example (not in a dataframe):</p>
<pre><code>initialDate = date(2025, 1, 1)
initialTime = time(22, 25)
initialDatetime = datetime(2025, 1, 1, 22, 25)
secondTime = time(1, 25)
</code></pre>
","1","Question"
"79627670","","<p>For the same compression level of 10 using zstd with Parquet, I get significantly better performance in Pandas than from Apache Spark. The following files, for example, were first generated using Spark and then loaded and saved using python shell. Why is there so much discrepancy in performance? I am using native filesystem (ext4) on Ubuntu.</p>
<pre><code>Within Spark:
    df
      .coalesce(1)
      .write
        .option(&quot;compression&quot;, &quot;zstd&quot;)
        .option(&quot;compressionLevel&quot;, &quot;10&quot;)
        .mode(&quot;overwrite&quot;)
        .parquet(parquetPath)


In Python:
    &gt;&gt;&gt; import pandas as pd
    &gt;&gt;&gt; df = pd.read_parquet('results/data1.parquet')
    &gt;&gt;&gt; df.to_parquet('data1.parquet', engine='pyarrow', compression=&quot;zstd&quot;, compression_level=10, index=False)
    &gt;&gt;&gt; df = pd.read_parquet('results/data2.parquet')
    &gt;&gt;&gt; df.to_parquet('data2.parquet', engine='pyarrow', compression=&quot;zstd&quot;, compression_level=10, index=False)

Stats:
    file                   | Apache Spark | Pandas    | Pandas/Spark sizes
    ---------------------------------------------------------------------
    results/data1.parquet  |   237780532  | 172442433 | 0.72
    results/data2.parquet  |    62052301  |  41917063 | 0.67

Software:
    Apache Spark-4.0.0-preview1
    scala-2.13.14
    Java 21.0.4
    python-3.12.4
    pandas-2.2.3
    pyarrow-19.0.1
</code></pre>
<p>PS: Metadata information is as follows</p>
<pre><code>pqt$ ls -l
-r--r--r-- 1 user user 237780532 May 20 11:55 spark.parquet

pqt$ python3
Python 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import pyarrow.parquet as pq
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.read_parquet('spark.parquet')
&gt;&gt;&gt; df.to_parquet('pandas.parquet', engine='pyarrow', compression=&quot;zstd&quot;, compression_level=10, index=False)
&gt;&gt;&gt; parquet_file = pq.ParquetFile('spark.parquet')
&gt;&gt;&gt; print(parquet_file.metadata)
&lt;pyarrow._parquet.FileMetaData object at 0x11e8091de2a0&gt;
  created_by: parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)
  num_columns: 3
  num_rows: 20000
  num_row_groups: 2
  format_version: 1.0
  serialized_size: 1001
&gt;&gt;&gt; parquet_file = pq.ParquetFile('pandas.parquet')
&gt;&gt;&gt; print(parquet_file.metadata)
&lt;pyarrow._parquet.FileMetaData object at 0x11e808d57fb0&gt;
  created_by: parquet-cpp-arrow version 19.0.1
  num_columns: 3
  num_rows: 20000
  num_row_groups: 1
  format_version: 2.6
  serialized_size: 1905
&gt;&gt;&gt;

pqt$ ls -l
-rw-rw-r-- 1 user user 172442433 May 20 12:01 pandas.parquet
-r--r--r-- 1 user user 237780532 May 20 11:55 spark.parquet

pqt$ bc
scale = 2
172442433 / 237780532
.72

pqt$
</code></pre>
","3","Question"
"79627856","","<p>I have a Dataframe with 2 columns.
I want to compare the value of first column with some threshold for 5 iterations and if it exceeds that value, check the corresponding value of the another column</p>
<p>DF:
In the below example I need to check at what value of '<strong>Inst</strong>' where '<strong>Error</strong>' was less than 2.5 for next consecutive 5 iterations</p>
<p>Expected result: Inst was 204273 204302</p>
<p>Below is what I tried but it did not work. Any pointers or better way of implementing it would be nice</p>
<pre><code>count = 0
for i in range(len(df[&quot;Inst&quot;])):
    while count &lt; 6:
        if df[&quot;Error&quot;][i] &lt; 2.5:
            count += 1
            continue
    result = df[&quot;Inst&quot;][i-5]
</code></pre>
<p>Below is my DF:</p>
<pre><code>       Error    Inst
0   2.595795  204267
1   2.568556  204268
2   2.562618  204269
4   2.538956  204271
5   2.520247  204272
6   2.498345  204273  #
7   2.474890  204274
8   2.467736  204275
9   2.471115  204276
10  2.466424  204280
11  2.495388  204284
12  2.520301  204285
13  2.604358  204291
14  2.553243  204299
15  2.490774  204302  #
16  2.452384  204303
17  2.434171  204304
18  2.404764  204305
19  2.388775  204306
20  2.384337  204307
</code></pre>
","3","Question"
"79627995","","<p>I am writing some python code using typehints and using mypy to check them.  I have a variable <code>period</code> that I stated was a string.  I later use that variable to instantiate a <code>pandas.Timedelta</code> object, setting the units to <code>period</code>.  Here is a minimally reproducible example:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

period: str = &quot;minute&quot;
my_timedelta = pd.Timedelta( 30, unit=period)

print(my_timedelta)
</code></pre>
<p>The above code runs fine and creates a Timedelta object of 30 minutes:</p>
<pre class=""lang-bash prettyprint-override""><code>$ python example.py
0 days 00:30:00
$
</code></pre>
<p>When I run <strong>mypy</strong> on the above code, I get the following error:</p>
<pre class=""lang-bash prettyprint-override""><code>(pdmkt) dino@DINO(24):~/code/pdmkt$ mypy example.py
example.py:5: error: Argument &quot;unit&quot; to &quot;Timedelta&quot; has incompatible type &quot;str&quot;; expected &quot;Literal['W', 'w', 'D', 'd', 'days', 'day', 'hours', 'hour', 'hr', 'h', 'm', 'minute', 'min', 'minutes', 's', 'seconds', 'sec', 'second', 'ms', 'milliseconds', 'millisecond', 'milli', 'millis', 'us', 'microseconds', 'microsecond', 'µs', 'micro', 'micros', 'ns', 'nanoseconds', 'nano', 'nanos', 'nanosecond']&quot;  [arg-type]
Found 1 error in 1 file (checked 1 source file)
</code></pre>
<hr />
<p>Obviously I could declare a type using the Literal, and modify my code so that <code>period</code> is that type:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import typing

PERIOD_TYPE = typing.Literal['W', 'w', 'D', 'd', 'days', 'day', 'hours', 'hour', 'hr', 'h', 'm', 'minute', 'min', 'minutes', 's', 'seconds', 'sec', 'second', 'ms', 'milliseconds', 'millisecond', 'milli', 'millis', 'us', 'microseconds', 'microsecond', 'µs', 'micro', 'micros', 'ns', 'nanoseconds', 'nano', 'nanos', 'nanosecond']

period: PERIOD_TYPE = &quot;minute&quot;
my_timedelta = pd.Timedelta( 30, unit=period)

print(my_timedelta)
</code></pre>
<p>The above code works the same as the first, <em><strong>and mypy reports:</strong></em> &quot;Success: no issues found in 1 source file&quot;</p>
<p>However, rather than declare my own type, it makes sense to me to somehow import the type from Pandas.  I looked through the pandas code and found this file:
<a href=""https://github.com/pandas-dev/pandas/blob/main/pandas/core/tools/timedeltas.py"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/blob/main/pandas/core/tools/timedeltas.py</a>
which contains the line:</p>
<pre class=""lang-py prettyprint-override""><code>from pandas._libs.tslibs.timedeltas import UnitChoices
</code></pre>
<p>In timedeltas.py the variable <code>unit</code> is of type <code>UnitChoices</code>, so obviously <code>UnitChoices</code> is the type that I want to use.</p>
<p><strong>HERE'S MY QUESTION:</strong></p>
<p><em>How can I import and use <code>UnitChoices</code> from Pandas?</em></p>
<hr />
<hr />
<p><strong>UPDATE:</strong></p>
<p>I tried the suggestion in the answer by @InSync and I am able to successfully import <code>UnitChoices</code> from Pandas, <em>however</em> it appears that the <code>UnitChoices</code> that I get is slightly different than what <code>mypy</code> sees in Pandas.</p>
<p>Here is my latest code example:</p>
<pre class=""lang-py prettyprint-override""><code>from __future__ import annotations

import pandas as pd
import typing

if typing.TYPE_CHECKING:
    from pandas._libs.tslibs.timedeltas import UnitChoices

period: UnitChoices = &quot;minute&quot;

my_timedelta = pd.Timedelta( 30, unit=period)

print(my_timedelta)
</code></pre>
<p>And here is the error that I now see (with some formatting added to the single-line error message below to make the error message easier to read):</p>
<pre class=""lang-bash prettyprint-override""><code>mypy example6.py
example6.py:11: error: Argument &quot;unit&quot; to &quot;Timedelta&quot; has incompatible type

&quot;Literal['Y', 'y', 'M'] | Literal['W', 'w', 'D', 'd', 'days', 'day', 'hours', 'hour', 'hr', 'h', 'm', 'minute', 'min', 'minutes', 's', 'seconds', 'sec', 'second', 'ms', 'milliseconds', 'millisecond', 'milli', 'millis', 'us', 'microseconds', 'microsecond', 'µs', 'micro', 'micros', 'ns', 'nanoseconds', 'nano', 'nanos', 'nanosecond']&quot;; 

expected 

&quot;Literal['W', 'w', 'D', 'd', 'days', 'day', 'hours', 'hour', 'hr', 'h', 'm', 'minute', 'min', 'minutes', 's', 'seconds', 'sec', 'second', 'ms', 'milliseconds', 'millisecond', 'milli', 'millis', 'us', 'microseconds', 'microsecond', 'µs', 'micro', 'micros', 'ns', 'nanoseconds', 'nano', 'nanos', 'nanosecond']&quot;  [arg-type]
Found 1 error in 1 file (checked 1 source file)
</code></pre>
<p>The definitions of <code>UnitChoices</code> appear the same <em><strong>except for</strong></em> the one that I am getting in my example code has the following extra type at the beginning: <code>Literal['Y', 'y', 'M'] | ...</code></p>
<p><strong>Any ideas?</strong></p>
","3","Question"
"79628859","","<p>I have a diagonal metric that loops over multiple cohorts which i'm trying to convert into a vertical table, the reason for this is for future computations extracting the entire column would be simpler then applying offsets.</p>
<p>The age is set at the age of entry, with a minimum age of 18. for a 18yr old joiner, in year 1 then are &quot;1&quot;, in year 2 they would be 19, year 3 then would be 20.... for the example data i've assigned the value based on the age at commencement to hopefully show this better.</p>
<p>my actual data has 63 cohorts, ages 18-60 with year 1 to year 20, which may change over time.</p>
<p>Original table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Cohort</th>
<th>age</th>
<th>Year 1</th>
<th>year 2</th>
<th>year 3</th>
<th>year 4</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>18</td>
<td><strong>1</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>19</td>
<td>2</td>
<td><strong>1</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>20</td>
<td>3</td>
<td>2</td>
<td><strong>1</strong></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>21</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td><strong>1</strong></td>
</tr>
<tr>
<td>2</td>
<td>18</td>
<td>1.5</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>19</td>
<td>2.5</td>
<td>1.5</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>20</td>
<td>3.5</td>
<td>2.5</td>
<td>1.5</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>21</td>
<td>4.5</td>
<td>3.5</td>
<td>2.5</td>
<td>1.5</td>
</tr>
</tbody>
</table></div>
<p>Required output, I will need Cohort and Age at commencement as hierarchical headers but couldn't get the markdown to format correctly:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Cohort <br/> Age at Commencement</th>
<th><em>1</em> <br/> 18</th>
<th><em>1</em> <br/> 19</th>
<th><em>1</em> <br/> 20</th>
<th><em>1</em> <br/> 21</th>
<th><em>2</em> <br/> 18</th>
<th><em>2</em> <br/> 19</th>
<th><em>2</em> <br/> 20</th>
<th><em>2</em> <br/> 21</th>
</tr>
</thead>
<tbody>
<tr>
<td>Year 1</td>
<td><strong>1</strong></td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>1.5</td>
<td>2.5</td>
<td>3.5</td>
<td>4.5</td>
</tr>
<tr>
<td>year 2</td>
<td><strong>1</strong></td>
<td>2</td>
<td>3</td>
<td></td>
<td>1.5</td>
<td>2.5</td>
<td>3.5</td>
<td></td>
</tr>
<tr>
<td>year 3</td>
<td><strong>1</strong></td>
<td>2</td>
<td></td>
<td></td>
<td>1.5</td>
<td>2.5</td>
<td></td>
<td></td>
</tr>
<tr>
<td>year 4</td>
<td><strong>1</strong></td>
<td></td>
<td></td>
<td></td>
<td>1.5</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I've tried pivot, melt, etc but am struggling to then format into the desired output.</p>
","0","Question"
"79628910","","<p>I have a dataframe where each column starts and finished with certain number of nan values. Somewhere in the middle of a column there is a continuous list of values. It can happen that a nan value &quot;interrupts&quot; the data. I want to iterate over each column, find such values and then remove the whole row.</p>
<p>For example, I want to find the <code>np.nan</code> between <code>9</code> and <code>13</code> and remove it:</p>
<pre><code>[np.nan, np.nan, np.nan, 1, 4, 6, 6, 9, np.nan, 13, np.nan, np.nan]
</code></pre>
<p>Conditions for removal:</p>
<ol>
<li>if value has at least one data point before</li>
<li>if value has at least one data point after</li>
<li>if value is nan</li>
</ol>
<p>I wrote code that does this already, but it's slow and kind of wordy.</p>
<pre><code>import pandas as pd
import numpy as np


data = {'A': [np.nan, np.nan, np.nan, 1, 4, 6, 6, 9, np.nan, 13, np.nan, np.nan], 'B': [np.nan, np.nan, np.nan, 11, 3, 16, 13, np.nan, np.nan, 12, np.nan, np.nan]}
df = pd.DataFrame(data)

def get_nans(column):
    output = []
    for index_to_check, value in column.items():
        has_value_before = not column[:index_to_check].isnull().all()
        has_value_after = not column[index_to_check + 1:].isnull().all()
        is_nan = np.isnan(value)
        output.append(not( has_value_before and has_value_after and is_nan))
    return output

for column in df.columns:
    df = df[get_nans(df[column])]
    
print(df)

</code></pre>
<p>How can I improve my code, vectorize it etc?</p>
","3","Question"
"79629420","","<p>I'm upgrading my repo from Python 3.7 to 3.13. This is a very big jump, so I need to update many packages as well. One such package is pandas.</p>
<p>Upon upgrading to pandas 2.2.3 (the first version which can support 3.13 from what I understand), some of my code doesn't work anymore. Could anyone help me debug what exactly is wrong?</p>
<p>The code:</p>
<p><strong>dbservice.py</strong>:</p>
<pre><code>import pandas as pd
class DatabaseExtension(DBExtension):
    _pd = pd

    def get_dataframe(self, query, *params, **kwargs):
        query = self.build_query(query, *params)
        return self._pd.read_sql(query, self.engine, params=params, **kwargs)
</code></pre>
<p><strong>The error:</strong></p>
<pre><code>  File &quot;C:\Users\wackowskin\PycharmProjects\Webapp\app\resources\database\dbservice.py&quot;, line 149, in get_dataframe
    return self._pd.read_sql(sql=query, con=self.engine, params=params, **kwargs)
           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\wackowskin\PycharmProjects\webapp\venv\Lib\site-packages\pandas\io\sql.py&quot;, line 706, in read_sql
    return pandas_sql.read_query(
    
  File &quot;C:\Users\wackowskin\PycharmProjects\webapp\venv\Lib\site-packages\pandas\io\sql.py&quot;, line 2738, in read_query
    cursor = self.execute(sql, params)
  File &quot;C:\Users\wackowskin\PycharmProjects\webapp\venv\Lib\site-packages\pandas\io\sql.py&quot;, line 2672, in execute
    cur = self.con.cursor()
          ^^^^^^^^^^^^^^^
AttributeError: 'Engine' object has no attribute 'cursor'
</code></pre>
<p>I've tried changing how the parameters are called in the function call for self._pd.read_sql() and I've tried adjusting what version of pandas I'm using, but I can't go lower than 2.2.3 since it appears none of the earlier ones support Python 3.13. Does anyone have any idea what I might need to change?</p>
","1","Question"
"79629556","","<p>I have a Pandas DataFrame representing data grouped by an ID, and I need to update values in one column based on conditions that involve values in other rows <em>within the same group</em>. Specifically, I want to update a column <code>status</code> to 'resolved' under these conditions:</p>
<ol>
<li>The <code>status</code> column for the current row is initially 'pending'.</li>
<li>There is at least one other row within the same <code>group_id</code> where <code>status</code> is 'active' and the <code>related_id</code> matches the <code>id</code> of the current row.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {'id': [1, 2, 3, 4, 5, 6, 7],
        'group_id': ['A', 'A', 'A', 'B', 'B', 'B', 'B'],
        'status': ['pending', 'active', 'pending', 'pending', 'active', 'pending', 'pending'],
        'related_id': [None, None, 1, None, None, 4, 4]}
df = pd.DataFrame(data)
print(df)
</code></pre>
<pre><code>   id group_id   status  related_id
0   1        A  pending        None
1   2        A   active        None
2   3        A  pending           1
3   4        B  pending        None
4   5        B   active        None
5   6        B  pending           4
6   7        B  pending           4
</code></pre>
<p>In this example, rows with <code>id</code> 3, 6, and 7 should have their <code>status</code> updated to 'resolved' because they are 'pending', belong to the same <code>group_id</code> as a row with 'active' status, and their <code>related_id</code> matches the <code>id</code> of that 'active' row (row 1 for id 3, row 4 for id 6 &amp; 7).</p>
<p>The output I want:</p>
<pre><code>   id group_id    status  related_id
0   1        A   pending        None
1   2        A    active        None
2   3        A  resolved           1
3   4        B   pending        None
4   5        B    active        None
5   6        B  resolved           4
6   7        B  resolved           4
</code></pre>
<p>I tried using <code>groupby()</code> and <code>apply()</code>, but I'm struggling to apply the condition check across rows within each group.</p>
","1","Question"
"79630359","","<p>When outputting a DataFrame to Jupyter interactive window, the last three columns are wrapped under the other columns even though there seems to be enough space to have them all aligned. Is there a setting to change this behavior?</p>
<pre class=""lang-py prettyprint-override""><code>#%%
import pandas as pd
df = pd.DataFrame({
    **{chr(i): [1000*i, 1000*i + 1, 1000*i + 2] for i in range(65, 80)},
})
df
</code></pre>
<p>prints the following when using <code>text/plain</code> render mode:</p>
<pre><code>       A      B      C      D      E      F      G      H      I      J  \
0  65000  66000  67000  68000  69000  70000  71000  72000  73000  74000   
1  65001  66001  67001  68001  69001  70001  71001  72001  73001  74001   
2  65002  66002  67002  68002  69002  70002  71002  72002  73002  74002   

       K      L      M      N      O  
0  75000  76000  77000  78000  79000  
1  75001  76001  77001  78001  79001  
2  75002  76002  77002  78002  79002  
</code></pre>
","1","Question"
"79630693","","<p>I have a dataframe, and a list of potentially different filters.</p>
<p>Is there a way to apply a filter that comes from a list so that the dataframe is filtered accordingly?</p>
<p>MRE:</p>
<p>In this example, the <code>df</code> is filtered based on the conditions set out in <code>filter_ls[0]</code>, ie, I want to run <code>df[df.col1&gt;3]</code>. the filter used depends on earlier logic, which specifies which element to take from filter_ls, ie element 0</p>
<pre><code>df=pd.DataFrame({'col1':[1,2,3,4,5], 'col2':[6,7,8,9,9], 'col3':[11,12,13,14,15]})
filter_ls=['df[df.col1&gt;3]', 'df[(df.col1&lt;4) &amp; (df.col2&lt;8)]', 'df[(df.col2&gt;7) &amp; (df.col3&gt;14)]']
filter_ls[0]
</code></pre>
<p>which returns <code>KeyError: 'df.col1&gt;3'</code></p>
","0","Question"
"79631026","","<p><a href=""https://pandas.pydata.org/pandas-docs/stable/development/extending.html#define-original-properties"" rel=""nofollow noreferrer"">_metadata original properties</a> are not pased to pyjanitor manipulation results</p>
<p>Take the following MWE:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import janitor # noqa: F401
import pandas_flavor as pf

# See: https://pandas.pydata.org/pandas-docs/stable/development/extending.html#define-original-properties
class MyDataFrame(pd.DataFrame):

    # normal properties
    _metadata = [&quot;myvar&quot;]

    @property
    def _constructor(self):
        return MyDataFrame

@pf.register_dataframe_method
def regvar(self):
    obj = MyDataFrame(self)
    obj.myvar = 2
    return obj

@pf.register_dataframe_method
def printvar(self):
    print(self.myvar)
    return self

df = pd.DataFrame(
     {
         &quot;Year&quot;: [1999, 2000, 2004, 1999, 2004],
         &quot;Taxon&quot;: [
             &quot;Saccharina&quot;,
             &quot;Saccharina&quot;,
             &quot;Saccharina&quot;,
             &quot;Agarum&quot;,
             &quot;Agarum&quot;,
         ],
         &quot;Abundance&quot;: [4, 5, 2, 1, 8],
     }
 )
</code></pre>
<p>Now:</p>
<pre><code>df2 = df.regvar().query(&quot;Taxon=='Saccharina'&quot;).printvar()
</code></pre>
<p>This correctly returns <code>2</code>.</p>
<p>However:</p>
<pre><code>index = pd.Index(range(1999,2005),name='Year')
df2 = df.regvar().complete(index, &quot;Taxon&quot;, sort=True).printvar()
</code></pre>
<p>Returns an Exception:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_4412\627945022.py in ?()
     39 
     40 df2 = df.regvar().query(&quot;Taxon=='Saccharina'&quot;).printvar()
     41 
     42 index = pd.Index(range(1999,2005),name='Year')
---&gt; 43 df2 = df.regvar().complete(index, &quot;Taxon&quot;, sort=True).printvar()

HOME\venvs\base\Lib\site-packages\pandas_flavor\register.py in ?(self, *args, **kwargs)
    160                     object: The result of calling of the method.
    161                 &quot;&quot;&quot;
    162                 global method_call_ctx_factory
    163                 if method_call_ctx_factory is None:
--&gt; 164                     return method(self._obj, *args, **kwargs)
    165 
    166                 return handle_pandas_extension_call(
    167                     method, method_signature, self._obj, args, kwargs

~\AppData\Local\Temp\ipykernel_4412\627945022.py in ?(self)
     21 @pf.register_dataframe_method
     22 def printvar(self):
---&gt; 23     print(self.myvar)
     24     return self

HOME\venvs\base\Lib\site-packages\pandas\core\generic.py in ?(self, name)
   6295             and name not in self._accessors
   6296             and self._info_axis._can_hold_identifiers_and_holds_name(name)
   6297         ):
   6298             return self[name]
-&gt; 6299         return object.__getattribute__(self, name)

AttributeError: 'DataFrame' object has no attribute 'myvar'
</code></pre>
","2","Question"
"79631295","","<p>I have a 'pandas' data-frame containing timeline year recorded observations for vehicles. The data is organised in a particular way which is explained more below: A sample of such a data-frame for <strong>one vehicle</strong> is shown here:</p>
<pre><code>import pandas as pd
import numpy as np

df1 = pd.DataFrame({&quot;Vehicle Type&quot;: [&quot;truck&quot;, &quot;truck&quot;, &quot;truck&quot;, &quot;truck&quot;, &quot;truck&quot;],
  &quot;Vehicle ID&quot;: [&quot;XYZ&quot;, &quot;XYZ&quot;, &quot;XYZ&quot;, &quot;XYZ&quot;, &quot;XYZ&quot;],
  &quot;Year&quot;: [1, 2, 3, 4, 5],
  &quot;Earliest Fact&quot;: [pd.NaT, pd.NaT, &quot;2018-04-18&quot;, &quot;2019-01-02&quot;, &quot;2020-01-02&quot;],
  &quot;Latest Fact&quot;: [pd.NaT, pd.NaT, &quot;2019-01-01&quot;, &quot;2020-01-01&quot;, &quot;2020-12-31&quot;],
  &quot;Fact History&quot;: [np.nan, np.nan, 11.5, 11.7, 5],
  &quot;Days Worked&quot;: [np.nan, np.nan, 234, 256, 43],
  &quot;Days Available&quot;: [np.nan,  np.nan, 260, 272, 57]
})

df[[&quot;Earliest Fact&quot;, &quot;Latest Fact&quot;]] = 
df[[&quot;Earliest Fact&quot;, &quot;Latest Fact&quot;]].apply(pd.to_datetime, errors=&quot;coerce&quot;)
</code></pre>
<p>For this particular vehicle, it has history beginning in its 3rd year up till its 5th year - you'll notice this because the value for <code>Fact History</code> is less than 12 (less than 12 months worth of history). Its 5th year corresponds to its current year where data is still being recorded for it.</p>
<p>It has no history in years 1 and 2.</p>
<p>Other vehicles in my data-frame are in similar situations whereby they will have missing history in some years.</p>
<p>Another situation will be something like this:</p>
<pre><code>df2 = pd.DataFrame({&quot;Vehicle Type&quot;: [&quot;van&quot;, &quot;van&quot;, &quot;van&quot;, &quot;van&quot;, &quot;van&quot;, &quot;van&quot;, &quot;van&quot;],
  &quot;Vehicle ID&quot;: [&quot;ABC&quot;, &quot;ABC&quot;, &quot;ABC&quot;, &quot;ABC&quot;, &quot;ABC&quot;, &quot;ABC&quot;, &quot;ABC&quot;],
  &quot;Year&quot;: [1, 2, 3, 4, 5, 6, 7],
  &quot;Earliest Fact&quot;: [pd.NaT, pd.NaT, &quot;2018-04-18&quot;, &quot;2019-01-02&quot;, &quot;2020-01-02&quot;, &quot;2021-01-01&quot;,    &quot;2022-01-01&quot;],
  &quot;Latest Fact&quot;: [pd.NaT, pd.NaT, &quot;2019-01-01&quot;, &quot;2020-01-01&quot;, &quot;2020-12-31&quot;, &quot;2021-12-31&quot;, &quot;2023-01-01&quot;],
  &quot;Fact History&quot;: [np.nan, np.nan, 5, 11.7, 12, 12, 5.7],
  &quot;Days Worked&quot;: [np.nan, np.nan, 100, 256, 273, 300, 94],
  &quot;Days Available&quot;: [np.nan,  np.nan, 130, 272, 290, 320, 141]
})
</code></pre>
<p>Here, this vehicle only has a partially recorded year in its first recorded year (its 3rd month, with 5 months worth of history). Its 7th year corresponds to its current year where data is still being recorded for it.</p>
<p>I need some systematic way to obtain the missing years for each vehicles so that I can fill them in with a function I am creating.</p>
<p>I had this solution but the code for it is far too verbose:</p>
<pre><code># concatenate dfs from above into one data-frame
df = pd.concat([df1, df], axis = &quot;index&quot;)

full = df[[&quot;Vehicle ID&quot;, &quot;Year&quot;, &quot;Fact History&quot;]].loc[df[&quot;Fact History&quot;].notnull()]

reference = pd.merge(
    full.groupby(&quot;Vehicle ID&quot;)[&quot;Year&quot;].min().reset_index().rename(columns={&quot;Year&quot;: &quot;Begins At&quot;}),
    full.groupby(&quot;Vehicle ID&quot;)[&quot;Year&quot;].max().reset_index().rename(columns={&quot;Year&quot;: &quot;Current Year&quot;}),
    on=&quot;Vehicle ID&quot;
)

reference = pd.merge(
    reference,
    full.rename(columns={&quot;Year&quot;: &quot;Begins At&quot;, &quot;Fact History&quot;: &quot;History Begins&quot;}),
    on=[&quot;Vehicle ID&quot;, &quot;Begins At&quot;],
    how=&quot;left&quot;
)

reference[&quot;Missing Years&quot;] = reference.apply(
    lambda row: &quot;, &quot;.join(map(str, range(1, row[&quot;Begins At&quot;] + 1))) 
    if row[&quot;History Begins&quot;] &lt; 10 
    else np.nan,
    axis=1
)
</code></pre>
<p>so the missing year values should correspond to where:</p>
<ul>
<li>there is no recorded history in prior years to the first recorded year; or</li>
<li>there is partially recorded history in the first recorded year</li>
</ul>
<p>the missing year values CANNOT correspond to the vehicle's current year where there will obviously be only partially recorded history</p>
<p>If there is a smoother way to do this, I would appreciate the help</p>
","0","Question"
"79631672","","<p>I have two dataframes with timestamp data. It is sensor readouts from different sources.
I want to combine them. The left dataframe (df1) can be quite large as it will be a combination of multiple sources, the right dataframe (df2) will have max. 8 columns.
Some cols of df2 may already be in df1, but there might be more or less timestamps with values. The timestamps may also be double. Some columns in df2 will be new to df1.</p>
<p>E.g.</p>
<pre><code>df1 = pd.DataFrame(
    {
        &quot;PT1&quot;: [&quot;A0&quot;, &quot;A1&quot;, &quot;A2&quot;],
        &quot;PT2&quot;: [&quot;B0&quot;, &quot;B1&quot;, &quot;B2&quot;],
        &quot;PT3&quot;: [&quot;C0&quot;, &quot;C1&quot;, &quot;C2&quot;],
    },
    index=pd.DatetimeIndex([&quot;2025-05-01 10:00&quot;, &quot;2025-05-01 10:01&quot;, &quot;2025-05-01 10:02&quot;]),
)

df2 = pd.DataFrame(
    {
        &quot;PT1&quot;: [&quot;A0&quot;, &quot;A1&quot;, &quot;A3&quot;],
        &quot;PT4&quot;: [&quot;D0&quot;, &quot;D1&quot;, &quot;D3&quot;],
    },
    index=pd.DatetimeIndex([&quot;2025-05-01 10:00&quot;, &quot;2025-05-01 10:01&quot;, &quot;2025-05-01 10:03&quot;]),
)
</code></pre>
<p>I tried concat &amp; merge, but either I don't get the Timestamps combined or I loose the index. :-/</p>
<p>Expected output would be:</p>
<pre><code>df1updated = pd.DataFrame(
    {
        &quot;PT1&quot;: [&quot;A0&quot;, &quot;A1&quot;, &quot;A2&quot;, &quot;A3&quot;],
        &quot;PT2&quot;: [&quot;B0&quot;, &quot;B1&quot;, &quot;B2&quot;, nan ],
        &quot;PT3&quot;: [&quot;C0&quot;, &quot;C1&quot;, &quot;C2&quot;, nan ],
        &quot;PT4&quot;: [&quot;D0&quot;, &quot;D1&quot;, nan,  &quot;D3&quot;],
    },
    index=pd.DatetimeIndex([&quot;2025-05-01 10:00&quot;, &quot;2025-05-01 10:01&quot;, &quot;2025-05-01 10:02&quot;, , &quot;2025-05-01 10:03&quot;]),
)
</code></pre>
<p>Update after @ouroboros1 comment:
Usually, there should only be double entries in the two dataframes, when the value is either the same or one of them is nan.
Two different values could happen, but can be solved from the data source side.
If the two values are different, it is because the source filled df2 with data from an earlier timestamp for that sensor. So I need to detect that somehow. But my plan was to do that on df2 before combining it with df1. E.g. by checking for duplicate values in df2 per column and repalcing them with nan again.</p>
","1","Question"
"79631713","","<p>I have two dataframes which I am trying to join along the columns. Both the dataframes have columns of type <code>pandas.MultiIndex</code>, i.e. they both have multilevel columns. The indices of the dataframes have no <code>NaN</code>/<code>NaT</code> values but they are not equal, i.e. there are some elements which are present in only one of the two indices. I am trying to achieve my join as follows:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.concat([left_df, right_df], axis=1, sort=False)
</code></pre>
<p>On doing so, I get the warning:</p>
<pre class=""lang-py prettyprint-override""><code>RuntimeWarning: '&lt;' not supported between instances of 'int' and 'tuple', sort order is undefined for incomparable objects.
</code></pre>
<p>On further inspection, I notice that the columns of the first dataframe (observed using <code>left_df.columns</code>) are tuples where the first element is a string and the second element is a string for some columns and a tuple of two strings for some columns. The columns of the second dataframe are tuples where the first element is a string and the second element is an integer.</p>
<p>As an alternative to <code>pd.concat</code>, I tried using <code>pd.merge</code> as follows:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.merge(left=left_df, right=right_df, left_index=True, right_index=True)  #sort is False and how is 'inner' by default
</code></pre>
<p>I get the same error on running the above line as well. It seems like I get this error because Pandas is trying to compare columns to arrange them where one of the elements is a tuple and the other is an integer. <strong>But this should not be happening because we have explicitly mentioned <code>sort=False</code>.</strong></p>
<p>As an alternative, I tried doing the same for two dataframes where the first has columns of type <code>Tuple[str, str]</code> and the second has columns of type <code>Tuple[str, Tuple[str, str]]</code> and I do not get the warning in this case.</p>
<h3>Minimum reproducible example</h3>
<p>The following piece of code generates some random data which mimics the kind of dataframes I am working with. It also shows the three solutions I tried out which all give the same warning I am trying to avoid.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

left_data = np.random.rand(1000, 5)
# Randomly set few columns as NaN
for row in np.random.choice(np.arange(1000), size=10, replace=False):
    left_data[row, :] = np.nan
    # print(f&quot;Making row {row} nan&quot;)
# Making random values as nan
grid = np.array(np.meshgrid(np.arange(0, 1000), np.arange(0, 5))).T.reshape(-1, 2)
for coord in grid[np.random.choice(grid.shape[0], size=100, replace=False)]:
    # print(f&quot;Making coordinate {coord} nan&quot;)
    left_data[coord] = np.nan
left_index = pd.date_range(start='20240101 09:00:00', periods=1000, freq='min')
left_columns = pd.MultiIndex.from_tuples([
    ('price', 'A'),                   # Tuple[str, str]
    ('price', 'B'),                   # Tuple[str, str]
    ('price', 'C'),                     # Tuple[str, str]
    ('diff', ('high', 'low')),         # Tuple[str, Tuple[str, str]]
    ('diff', ('open', 'close'))                   # Tuple[str, Tuple[str, str]]
])
left_df = pd.DataFrame(data=left_data, index=left_index, columns=left_columns)

right_data = np.random.rand(990, 3)
# Randomly set few columns as NaN
for row in np.random.choice(np.arange(990), size=5, replace=False):
    right_data[row, :] = np.nan
    # print(f&quot;Making row {row} nan&quot;)
# Making random values as nan
grid = np.array(np.meshgrid(np.arange(0, 990), np.arange(0, 3))).T.reshape(-1, 2)
for coord in grid[np.random.choice(grid.shape[0], size=50, replace=False)]:
    # print(f&quot;Making coordinate {coord} nan&quot;)
    left_data[coord] = np.nan
right_index = pd.date_range(start='20240101 12:00:00', periods=990, freq='min')
right_columns = pd.MultiIndex.from_tuples([
    ('X', 1),
    ('X', 2),
    ('X', 3),
])
right_df = pd.DataFrame(data=right_data, columns=right_columns, index=right_index)

print(&quot;Left&quot;)
print(left_df)
print(&quot;Right&quot;)
print(right_df)

df1 = pd.concat([left_df, right_df], axis=1, sort=False)
print(df1)  # Similar to outer join
df2 = pd.merge(left_df, right_df, left_index=True, right_index=True)
print(df2)  # Inner join by default
df3 = left_df.join(right_df)
print(df3)  # Left join by default
</code></pre>
<p>This throws the aforementioned warning thrice for all three alternatives.</p>
<p>While the simple solution here would be to create a new dataframe from scratch and populate it column by column, I wish to avoid using for loops and do this operations in 1-2 lines of code using library functions. How do I elegantly without explicitly creating a dataframe and populating it with the columns of each dataframe one by one in a for loop? Is this an unresolved bug in Pandas?</p>
<h3>Specifications</h3>
<p>Python version: 3.12.3</p>
<p>Pandas version: 2.2.3</p>
<p>NumPy version: 2.1.2</p>
<p>OS: Ubuntu 24.04</p>
","1","Question"
"79631997","","<p>I accidentally created a <code>pandas.Series</code> using a list as part of the index, and to my surprise, it worked without any error:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

series = pd.Series([1, 2, 3], index=[['A'], 'B', 'C'])
print(series)
print(type(list(series.index)[0]))  # outputs: &lt;class 'list'&gt;
</code></pre>
<p>This creates a Series object where the first index value is a Python list (<code>['A']</code>), , which surprised me because I assumed that all index labels in a pandas Series must be hashable types (like strings, numbers, or tuples). Lists, being mutable, are not hashable — so I expected an error.</p>
<p>However, I can still print and access the Series — at least partially — without an immediate error.</p>
<p>When I try this:</p>
<pre class=""lang-py prettyprint-override""><code>series.loc[['A']]
</code></pre>
<p>I get <code>TypeError: unhashable type: 'list'</code></p>
<p><strong>Questions:</strong></p>
<ul>
<li>Why does pandas allow a list as an index label in the first place?</li>
<li>Is this behavior intentional or just a side effect of how Index objects are handled internally?</li>
</ul>
<p>I'm using pandas version 2.2.3.</p>
<p>Any insights or references would be greatly appreciated!</p>
","2","Question"
"79632060","","<p>I'm trying to compute percentage changes in a pandas Series, while handling NaN values correctly and avoiding any <code>FutureWarning</code>. However, I'm stuck between two conflicting warnings.</p>
<pre><code>sub_df[f&quot;Δ {col}&quot;] = sub_df[col].ffill().pct_change()
</code></pre>
<p>This seems like the proper way to handle the deprecation of internal fill behavior in pct_change() — I explicitly call .ffill() before pct_change().</p>
<p>Problem is that despite that, I still get this warning:</p>
<blockquote>
<p>FutureWarning: The default fill_method='pad' in Series.pct_change is
deprecated and will be removed in a future version. Call ffill before
calling pct_change to retain current behavior and silence this
warning.</p>
</blockquote>
<p>It seems contradictory, because I already am calling .ffill().</p>
<p>So I tried the alternative:</p>
<pre><code>sub_df[f&quot;Δ {col}&quot;] = sub_df[col].ffill().pct_change(fill_method=None)
</code></pre>
<p>This now raises a different warning:</p>
<blockquote>
<p>FutureWarning: The 'fill_method' and 'limit' keywords in
Series.pct_change are deprecated and will be removed in a future
version. Call ffill before calling pct_change instead.</p>
</blockquote>
<p>Calling <code>.ffill().pct_change()</code> raises a warning that suggests I should call .<code>ffill()</code> — which I already do.</p>
<p>Setting <code>fill_method=None</code> to suppress that warning raises another <code>FutureWarning</code> that fill_method itself is deprecated.</p>
<p>How can I use <code>pct_change()</code> in a way that:</p>
<ol>
<li>handles missing values as before (with forward-fill),</li>
<li>avoids any <code>FutureWarning</code>, and aligns with future pandas behavior?</li>
</ol>
<p>Is this a bug in pandas, or am I misunderstanding something?</p>
","0","Question"
"79632701","","<p>Is there still a way to save an xls file in modern pandas (i.e. since <a href=""https://github.com/pandas-dev/pandas/pull/49296"" rel=""nofollow noreferrer"">they dropped xlwt</a>)? For example, I can save an xlsx file like so:</p>
<pre><code>import pandas as pd


def create_excel_files():
    # Create first sheet data
    df1 = pd.DataFrame(
        {
            &quot;A&quot;: [&quot;This is the first sentence.&quot;, &quot;This is the second sentence.&quot;],
            &quot;B&quot;: [&quot;This is a third sentence.&quot;, &quot;This is a fourth sentence.&quot;],
        }
    )

    # Create second sheet data
    df2 = pd.DataFrame(
        {
            &quot;A&quot;: [&quot;This is from sheet 2.&quot;, &quot;Another sentence from sheet 2.&quot;],
            &quot;B&quot;: [&quot;More content here.&quot;, &quot;Final sentence from sheet 2.&quot;],
        }
    )

    # Create XLSX file
    with pd.ExcelWriter(&quot;example.xlsx&quot;) as writer:
        df1.to_excel(writer, sheet_name=&quot;Sheet1&quot;, index=False)
        df2.to_excel(writer, sheet_name=&quot;Sheet2&quot;, index=False)
    print(&quot;Created example.xlsx&quot;)
</code></pre>
<p>but if I try to do this:</p>
<pre><code>with pd.ExcelWriter(&quot;example.xls&quot;, engine='xlwt') as writer:
    df1.to_excel(writer, sheet_name=&quot;Sheet1&quot;, index=False)
    df2.to_excel(writer, sheet_name=&quot;Sheet2&quot;, index=False)
print(&quot;Created example.xls&quot;)
</code></pre>
<p>I get this error: <code>ValueError: No engine for filetype: 'xls'</code></p>
<p>If I try this:</p>
<pre><code>with pd.ExcelWriter(&quot;example.xls&quot;) as writer:
    df1.to_excel(writer, sheet_name=&quot;Sheet1&quot;, index=False)
    df2.to_excel(writer, sheet_name=&quot;Sheet2&quot;, index=False)
print(&quot;Created example.xls&quot;)
</code></pre>
<p>I get this error: <code>ValueError: No engine for filetype: 'xls'</code></p>
","3","Question"
"79632776","","<p>Why does <code>Ctrl+I</code> only show help for pandas DataFrames but not GroupBy objects (specifically <code>pandas.core.groupby.generic.DataFrameGroupBy</code> objects)?</p>
<p>Here is an example entered into the Spyder console:</p>
<pre><code>import numpy as np
import pandas as pd
ColHead = ['A','B','C']
Data = np.random.randint(low=0,high=3,size=(12,3))
df=pd.DataFrame(Data,columns=ColHead)
gb=df.groupby('A',as_index=False)
</code></pre>
<p>If I place the insertion point at the end of <code>df</code> and press <code>Ctrl+I</code>, I see the documentation for the <code>DataFrame</code> class.  If I do the same at the end of <code>gb</code>, the Help pane shows <code>No documentation available</code>.  However, I can see the doc string in the source file:</p>
<pre><code>c:/Users/User.Name/AppData/Local/anaconda3/envs/py39/lib/site-packages/pandas/core/groupby/generic.py
-----------------------------------------------------------------------------------------------------

    &lt;...snip...&gt;

class DataFrameGroupBy(GroupBy[DataFrame]):
    _agg_examples_doc = dedent(
        &quot;&quot;&quot;
    Examples
    --------
    &gt;&gt;&gt; data = {&quot;A&quot;: [1, 1, 2, 2],
    ...         &quot;B&quot;: [1, 2, 3, 4],
    ...         &quot;C&quot;: [0.362838, 0.227877, 1.267767, -0.562860]}
    &gt;&gt;&gt; df = pd.DataFrame(data)
    &gt;&gt;&gt; df
       A  B         C
    0  1  1  0.362838
    1  1  2  0.227877
    2  2  3  1.267767
    3  2  4 -0.562860

    The aggregation is for each column.

    &gt;&gt;&gt; df.groupby('A').agg('min')
       B         C
    A
    1  1  0.227877
    2  3 -0.562860

    Multiple aggregations

    &gt;&gt;&gt; df.groupby('A').agg(['min', 'max'])

    &lt;...snip...&gt;
</code></pre>
<p>I am using Spyder 6.0.5, installed as part of the Anaconda platform.</p>
","0","Question"
"79632857","","<p>Update:
My question is related to how to find and replace in a particular column of a large csv file.  There are certain strings I'm looking for in the &quot;cells&quot; of that column that I intend to search on, in order to replace that entire &quot;cell's&quot; contents with a different string.  For the network guys, I'm trying to key on strings in 'show version' outputs so that I can replace all of that garbage with a ConnectHandler device_type.</p>
<hr />
<p>I'm trying to edit a csv file (in place would be preferrable).  I've tried multiple things with various modules. I've given up at this point.</p>
<p>Original file:</p>
<pre class=""lang-none prettyprint-override""><code>A,B,sometext_1_sometext,C,D
E,F,sometext_2_sometext,G,H
I,J,sometext_3_sometext,K,L
</code></pre>
<p>...</p>
<p>Desired result:</p>
<pre class=""lang-none prettyprint-override""><code>A,B,ONE,C,D
E,F,TWO,G,H
I,J,THREE,K,L
</code></pre>
<pre><code>import os
import csv
import pandas as pd

infile = 'Infrastructure.csv'
df = pd.read_csv('Infrastructure.csv')
with open('Infrastructure.csv', 'r') as f:
    reader = csv.reader(f)
    for line in reader:
        for i, row in enumerate(reader):
            for j, column in enumerate(row):
                if &quot;IOS&quot; in column:
                    if &quot;XE&quot; in column:
                        #print(f&quot;XE{(i,j)}&quot;)
                        df.loc[(i), 2] = &quot;,cisco_xe&quot;
                    else:
                        #print(f&quot;IOS{(i,j)}&quot;)
                        df.loc[(i), 2] = &quot;,cisco_ios&quot;
                if &quot;NX&quot; in column:
                    #print(f&quot;NX{(i,j)}&quot;)
                    df.loc[(i), 2] = &quot;,cisco_nxos&quot;
                if &quot;JUNOS&quot; in column:
                    #print(f&quot;JUNOS{(i,j)}&quot;)
                    df.loc[(i), 2] = &quot;,juniper_junos&quot;
</code></pre>
","-2","Question"
"79632877","","<p>I use python to read a large csv file, transform the data within (mostly string operations), and then write the results to a single parquet file. In the transformation process, rows are independent. 1 row in the csv file yields 1 row in the parquet file.</p>
<p>In my current implementation, I divide the data into chunks (via pandas.read_csv()) and then transform the chunked data, allocating the work across most of the cores in my CPU (via multiprocessing.ProcessPoolExecutor). Once all tasks are complete, I concatenate the results into a single, large pandas dataframe and then write it to a parquet file.</p>
<p>The code below provides a minimal example.</p>
<p>Concatenating the results maxes out memory usage. Is there a safe, more efficient process that writes each chunk of transformed data once it becomes available? I've read about ThreadPoolExecutor, Queues, and futures but I haven't found a clean example of how to combine all of these multiprocessing techniques in a case like mine.</p>
<pre><code>import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import regex as re
from concurrent.futures import ProcessPoolExecutor
from multiprocessing import cpu_count, Queue

filename = 'input.csv'

# Create example of input file
n_rows = int(25 * 1e6)  # a mid-sized dataset/ I am anticipating much larger files
input_df = pd.DataFrame([f'User_id_{i}_says_&quot;hello!&quot;.' for i in range(n_rows)], columns=['A'])
input_df.to_csv(filename, header=True, index=False)

# Example of data transformation
def clean_text(s):
    regex = re.compile('[^a-zA-Z]')
    return regex.sub('', s)

def process(df):
    # Split string into 5 columns.
    out = pd.DataFrame(df['A'].str.split('_').to_list())

    # Convert numeric id to string
    out[2] = out[2].astype(str)

    # Remove non-alphanumerics from column 4 entries.
    out[4] = out[4].apply(lambda x: clean_text(x))

    # Rename columns
    out.rename(columns={i: f'A{i}' for i in out.columns }, inplace=True)

    return out

if __name__ == &quot;__main__&quot;:

    # Set parquet data schema
    schema = pa.schema((f'A{i}', pa.string()) for i in range(5))

    # Multiprocessing implementation
    core_count = cpu_count() - 3  # OP's machine has 24 cores

    # Read input data in chunks
    chunk_size = int(1e5)
    chunks = pd.read_csv('input.csv', header=0, na_filter=False, chunksize=chunk_size)

    # Allocate transformation task
    with pq.ParquetWriter('processed.parquet', schema=schema, compression='GZIP') as writer:
        with ProcessPoolExecutor(core_count) as executor:
            results = executor.map(process, chunks)
        df = pd.concat(results, axis=0)

        # Convert df to record batch
        transformed_batch = pa.RecordBatch.from_pandas(df, schema=schema)
        writer.write_batch(transformed_batch)
</code></pre>
","0","Question"
"79633258","","<p>I'm trying to make a graph using <code>plotly</code> library and I want to make some texts in <strong>bold</strong> here's the code used :</p>
<pre class=""lang-py prettyprint-override""><code>import plotly.express as px
import pandas as pd
data = {
    &quot;lib_acte&quot;:[&quot;test 98lop1&quot;, &quot;test9665 opp1&quot;, &quot;test QSDFR1&quot;, &quot;test ABBE1&quot;, &quot;testtest21&quot;,&quot;test23&quot;],
    &quot;x&quot;:[12.6, 10.8, -1, -15.2, -10.4, 1.6],
    &quot;y&quot;:[15, 5, 44, -11, -35, -19],
    &quot;circle_size&quot;:[375, 112.5, 60,210, 202.5, 195],
    &quot;color&quot;:[&quot;green&quot;, &quot;green&quot;, &quot;green&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;],
    &quot;textfont&quot;:[&quot;normal&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;bold&quot;, &quot;bold&quot;, &quot;bold&quot;],
}

#load data into a DataFrame object:
df = pd.DataFrame(data)

fig = px.scatter(
        df,
        x=&quot;x&quot;, 
        y=&quot;y&quot;, 
        color=&quot;color&quot;,
        size='circle_size',
        text=&quot;lib_acte&quot;,
        hover_name=&quot;lib_acte&quot;,
        color_discrete_map={&quot;red&quot;: &quot;red&quot;, &quot;green&quot;: &quot;green&quot;},
        title=&quot;chart&quot;
      )
fig.update_traces(textposition='middle right', textfont_size=14, textfont_color='black', textfont_family=&quot;Inter&quot;, hoverinfo=&quot;skip&quot;)
newnames = {'red':'red title', 'green': 'green title'}

fig.update_layout(
        {
            
            'yaxis': {
                &quot;range&quot;: [-200, 200],
                'zerolinewidth': 2, 
                &quot;zerolinecolor&quot;: &quot;red&quot;,
                &quot;tick0&quot;: -200,
                &quot;dtick&quot;:45,
            },
            'xaxis': {
                &quot;range&quot;: [-200, 200],
                'zerolinewidth': 2, 
                &quot;zerolinecolor&quot;: &quot;gray&quot;,
                &quot;tick0&quot;: -200,
                &quot;dtick&quot;: 45,
                #  &quot;scaleanchor&quot;: 'y'
            },
           
            &quot;height&quot;: 800,
        }
    )
fig.add_scatter(
        x=[0, 0, -200, -200],
        y=[0, 200, 200, 0],
        fill=&quot;toself&quot;,
        fillcolor=&quot;gray&quot;,
        zorder=-1,
        mode=&quot;markers&quot;,
        marker_color=&quot;rgba(0,0,0,0)&quot;,
        showlegend=False,
        hoverinfo=&quot;skip&quot;
    )
fig.add_scatter(
        x=[0, 0, 200, 200],
        y=[0, -200, -200, 0],
        fill=&quot;toself&quot;,
        fillcolor=&quot;yellow&quot;,
        zorder=-1,
        mode=&quot;markers&quot;,
        marker_color=&quot;rgba(0,0,0,0)&quot;,
        showlegend=False,
        hoverinfo=&quot;skip&quot;
    )
fig.update_layout(
   
            paper_bgcolor=&quot;#F1F2F6&quot;,
        )
fig.show()
</code></pre>
<p>and here's the output of above code:
<a href=""https://i.sstatic.net/9BLQjlKN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9BLQjlKN.png"" alt=""output"" /></a></p>
<p>What I'm trying to do is to make <strong>&quot;test ABBE1&quot;, &quot;testtest21&quot;,&quot;test23&quot;</strong> in bold on the graph, could anyone please help how to do that ?</p>
","1","Question"
"79633336","","<p>In pandas, there are two obvious ways to represent timespans: <code>Interval[datetime64]</code> and <code>Period</code>. When do I use which? Is there a prefered one in all cases?</p>
<p>I couldn't find this in the documentation - this may be either me being insufficient at finding it, or a gap in the documentation.</p>
","1","Question"
"79633459","","<p>I observe that I cannot open a parquet file with ClickHouse if it contains a column that contains only <code>None</code> or <code>NaNs</code>.
My goal is to dump my raw files in my data warehouse, without having to define data types for each column (I want to do ELT transformation with dbt downstream).
Here is an example with a ClickHouse container, and pandas code:</p>
<h4>Setup ClickHouse</h4>
<p>You can replace <code>podman</code> with <code>docker</code>.</p>
<pre class=""lang-bash prettyprint-override""><code>podman run -d \
--name clickhouse-server \
-p 8123:8123 -p 9000:9000 \
-v $(shell pwd)/clickhouse_data:/var/lib/clickhouse \
-v $(shell pwd)/data/parquet:/var/lib/clickhouse/user_files/ \
clickhouse/clickhouse-server
</code></pre>
<p>Then in a python script, you can connect to the ClickHouse server:</p>
<pre class=""lang-py prettyprint-override""><code>import clickhouse_driver
client = clickhouse_driver.Client( # Uses a default docker clickhouse image
        host='localhost',
        port=9000,
        user='default',
        password='',
        database='default'
    )
</code></pre>
<h4>Generate parquet files and read them:</h4>
<h5>Working case:</h5>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import clickhouse_driver
# Create a toy dataframe with one column containing None
toy_df = pd.DataFrame({
    'Column1': [1, None, 2, 3],
    'Column2': ['A', 'B', 'C', 'D']
})
toy_df.to_parquet('../data/parquet/toy_df.parquet')
client.execute(
'''
CREATE TABLE toy ENGINE = MergeTree() ORDER BY tuple()
AS SELECT * FROM file('toy_df.parquet', 'Parquet')
'''
)
client.execute('SELECT * FROM toy')
</code></pre>
<h5>Non-working case:</h5>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import clickhouse_driver
# Create a toy dataframe with one column containing None
toy_df = pd.DataFrame({
    'Column1': [None, None, None, None], # works the same for [pd.NA, pd.NA, pd.NA, pd.NA],
    'Column2': ['A', 'B', 'C', 'D']
})
toy_df.to_parquet('../data/parquet/toy_df.parquet')
client.execute(
'''
CREATE TABLE toy ENGINE = MergeTree() ORDER BY tuple()
AS SELECT * FROM file('toy_df.parquet', 'Parquet')
'''
)
client.execute('SELECT * FROM toy')
</code></pre>
<p>Returns the following error:</p>
<pre class=""lang-bash prettyprint-override""><code>ServerException: Code: 636.
DB::Exception: The table structure cannot be extracted from a Parquet format file. Error:
Code: 50. DB::Exception: Unsupported Parquet type 'null' of an input column 'Column1'. If it happens during schema inference and you want to skip columns with unsupported types, you can enable setting input_format_parquet_skip_columns_with_unsupported_types_in_schema_inference. (UNKNOWN_TYPE) (version 25.4.3.22 (official build)).
You can specify the structure manually: (in file/uri /var/lib/clickhouse/user_files/toy_df.parquet).
</code></pre>
<h4>Question:</h4>
<p>What is the best way to handle this case?</p>
","0","Question"
"79633910","","<p>I have a large data set of binary data from a sensor that I'm trying to plot. I want to see the bit stream but for some of the columns of data, the output is visually shown flipped where 0 is placed at the top of the y axis and 1 is at the bottom. Im plotting each column and only some of the columns have this behavior (4 out of 18), most of them are plotted with 1 at the top of the y-axis and 0 at the bottom. I have taken the first 25 rows and used them below. Here is the code I'm using to plot.</p>
<pre><code># Plot Image

# Create figure
fig = go.Figure()

for col in df_test.columns:
    fig.add_trace(
        go.Scatter(
            x=df_test.index, 
            y=df_test[col], 
            name=col,
            visible=&quot;legendonly&quot;,
            line_shape='hv'))

        
fig.update_layout(
    autosize=False,
    width=1550,
    height=800,
    xaxis_title=df_test.index.name,
    yaxis_title=&quot;State&quot;,
    yaxis=dict(range=[0, 1])
)

fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/wnSysgY8.png"" rel=""nofollow noreferrer"">Dataframe data</a></p>
<p><a href=""https://i.sstatic.net/nSTFQfZP.png"" rel=""nofollow noreferrer"">showing column 1, data shown upside down</a></p>
<p><a href=""https://i.sstatic.net/oTQmagIA.png"" rel=""nofollow noreferrer"">showing column 2, y axis is displayed correctly</a></p>
<p><a href=""https://i.sstatic.net/Z4OpkFUm.png"" rel=""nofollow noreferrer"">showing columns 1 and 2 with the data displayed upside down</a></p>
<p>Just updated to plotly version 6.1.1</p>
<p>Any ideas why some of the columns are shown upside down and how to fix it?</p>
","-1","Question"
"79634020","","<p>Instead of using the below code, is there a method which can produce <code>n_apple</code>, <code>n_cherry</code>, and <code>n_banana</code> for the current row in a single line?</p>
<pre><code>for i, row in df.iterrows():
    date = row['date']
    n_apple = row['apple']
    n_cherry = row['cherry']
    n_banana = row['banana']
    # the list goes on...

</code></pre>
<p>Edit for clarity: My actual column names are much longer, so this won't work</p>
<pre><code>n_apple, n_cherry, n_banana = row['apple'], row['cherry'], row['banana']
</code></pre>
<p>EDIT 2: Desired behavior:</p>
<pre><code>n_apple, n_cherry, n_banana = row._unknown_method(['apple', 'cherry', 'banana'])
</code></pre>
","0","Question"
"79634157","","<p>I apply the following code to dataframe <code>model_data_utsset</code> (size <code>(36484909, 113)</code> - thoughts on making this publicly available for problem reproduction are welcome.) The same application of <code>.loc()</code> works fine for the third and fourth lines of code, but the fifth (final) line returns <code>TypeError: Must provide strings.</code> (Complete traceback at the end.) I have verified that the list <code>feats</code> contains only strings, and that the dataframe returned by <code>.loc()</code> has the same shape as the array <code>dummy</code>. Pandas version is 2.2.0.</p>
<p>How can I resolve this error?</p>
<pre><code>from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
trans = scaler.fit(model_data_utsset.loc[model_data_utsset['cmdb_ci']==ci, feats])
dummy = trans.transform(model_data_utsset.loc[ model_data_utsset['cmdb_ci']==ci, feats])
model_data_utsset.loc[model_data_utsset['cmdb_ci']==ci, feats] = dummy
</code></pre>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_3506448/3789644235.py in &lt;module&gt;
      5     dummy = trans.transform(model_data_utsset.loc[ model_data_utsset['cmdb_ci']==ci, feats])
----&gt; 6     model_data_utsset.loc[model_data_utsset['cmdb_ci']==ci, feats] = dummy
      7     print(len(model_data_utsset.loc[model_data_utsset['cmdb_ci']==ci, feats]))

~/.local/lib/python3.9/site-packages/pandas/core/indexing.py in __setitem__(self, key, value)
    910 
    911         iloc = self if self.name == &quot;iloc&quot; else self.obj.iloc
--&gt; 912         iloc._setitem_with_indexer(indexer, value, self.name)
    913 
    914     def _validate_key(self, key, axis: AxisInt):

~/.local/lib/python3.9/site-packages/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value, name)
   1944         if take_split_path:
   1945             # We have to operate column-wise
-&gt; 1946             self._setitem_with_indexer_split_path(indexer, value, name)
   1947         else:
   1948             self._setitem_single_block(indexer, value, name)

~/.local/lib/python3.9/site-packages/pandas/core/indexing.py in _setitem_with_indexer_split_path(self, indexer, value, name)
   1984                 # TODO: avoid np.ndim call in case it isn't an ndarray, since
   1985                 #  that will construct an ndarray, which will be wasteful
-&gt; 1986                 self._setitem_with_indexer_2d_value(indexer, value)
   1987 
   1988             elif len(ilocs) == 1 and lplane_indexer == len(value) and not is_scalar(pi):

~/.local/lib/python3.9/site-packages/pandas/core/indexing.py in _setitem_with_indexer_2d_value(self, indexer, value)
   2059                 # casting to list so that we do type inference in setitem_single_column
   2060                 value_col = value_col.tolist()
-&gt; 2061             self._setitem_single_column(loc, value_col, pi)
   2062 
   2063     def _setitem_with_indexer_frame_value(self, indexer, value: DataFrame, name: str):

~/.local/lib/python3.9/site-packages/pandas/core/indexing.py in _setitem_single_column(self, loc, value, plane_indexer)
   2166             # set value into the column (first attempting to operate inplace, then
   2167             #  falling back to casting if necessary)
-&gt; 2168             self.obj._mgr.column_setitem(loc, plane_indexer, value)
   2169 
   2170         self.obj._clear_item_cache()

~/.local/lib/python3.9/site-packages/pandas/core/internals/managers.py in column_setitem(self, loc, idx, value, inplace_only)
   1336             col_mgr.setitem_inplace(idx, value)
   1337         else:
-&gt; 1338             new_mgr = col_mgr.setitem((idx,), value)
   1339             self.iset(loc, new_mgr._block.values, inplace=True)
   1340 

~/.local/lib/python3.9/site-packages/pandas/core/internals/managers.py in setitem(self, indexer, value, warn)
    414             self = self.copy()
    415 
--&gt; 416         return self.apply(&quot;setitem&quot;, indexer=indexer, value=value)
    417 
    418     def diff(self, n: int) -&gt; Self:

~/.local/lib/python3.9/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs)
    362                 applied = b.apply(f, **kwargs)
    363             else:
--&gt; 364                 applied = getattr(b, f)(**kwargs)
    365             result_blocks = extend_blocks(applied, result_blocks)
    366 

~/.local/lib/python3.9/site-packages/pandas/core/internals/blocks.py in setitem(self, indexer, value, using_cow)
   2054 
   2055         try:
-&gt; 2056             values[indexer] = value
   2057         except (ValueError, TypeError):
   2058             if isinstance(self.dtype, IntervalDtype):

~/.local/lib/python3.9/site-packages/pandas/core/arrays/string_.py in __setitem__(self, key, value)
    467                 value = np.asarray(value, dtype=object)
    468             if len(value) and not lib.is_string_array(value, skipna=True):
--&gt; 469                 raise TypeError(&quot;Must provide strings.&quot;)
    470 
    471             mask = isna(value)

TypeError: Must provide strings.
</code></pre>
","0","Question"
"79634831","","<p>I have a pd.DataFrame which has a column entitled 'TRAN_DATE' which contains datetime.datetime objects.</p>
<p>I want to change these to a specific date format <strong>&quot;%Y-%m-%dT17:00:00&quot;</strong>. I want the date to be a changeable variable and the <strong>&quot;T17:00:00&quot;</strong> to remain static.</p>
<p>When running the below code however I get</p>
<pre><code>for i in range(0, len(df)):
    df.loc[i,'TRAN_DATE'] = df.loc[i,'TRAN_DATE'].strftime(&quot;%Y-%m-%dT17:00:00&quot;)
</code></pre>
<p>The type of the information comes back as a pandas._libs.tslibs.timestamps.Timestamp and the datetime as a whole with no T. i.e. 2024-11-11 17:00:00</p>
<p>Curiously when I run the following, this produces exactly what the format looks like, but I do not want the Z. i.e. 2024-11-11T17:00:00Z</p>
<pre><code>for i in range(0, len(df)):
    df.loc[i,'TRAN_DATE'] = df.loc[i,'TRAN_DATE'].strftime(&quot;%Y-%m-%dT17:00:00Z&quot;)
</code></pre>
<p>any thought</p>
","1","Question"
"79635981","","<p>I have data that can have different sized arrays per row like:</p>
<pre class=""lang-py prettyprint-override""><code>data = {
    'a': [np.array([1.,2.]), np.array([6.,7.,.6]), np.array([np.nan])],
    'b': np.array([99., 66., 88.])
}
df = pd.DataFrame(data)
</code></pre>
<p>I want to save this to a hdf5 file for archiving purposes and be able to reuse it in Matlab.</p>
<p>Saving it with</p>
<pre class=""lang-py prettyprint-override""><code>df.to_hdf('df.h5', mode='w', key='data', format='fixed')
</code></pre>
<p>is possible but not reusable, as it saves it in pandas specific format.</p>
<p>Saving it with</p>
<pre class=""lang-py prettyprint-override""><code>df.to_hdf('df.h5', mode='w', key='data', format='table')
</code></pre>
<p>is not possible and results in</p>
<pre><code>TypeError: Cannot serialize the column [a]
because its data contents are not [string] but [mixed] object dtype
</code></pre>
<p>also trying something like:</p>
<pre class=""lang-py prettyprint-override""><code>with h5py.File('df.h5', 'w') as h5f:
    h5f.create_dataset('data', data=df.to_numpy().tolist())
</code></pre>
<p>doesn't work and results in:</p>
<pre><code>ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (3, 2) + inhomogeneous part.
</code></pre>
<p>I also tried <code>pytables</code> and <code>hdf5storage</code> without much success.
Is there a straight forward way to achieve saving my <code>df</code> to a <code>hd5</code> file in a reusable way or should I move to a different file format. If so, which file format would be recommended for my purpose?</p>
","0","Question"
"79635993","","<p>Simple one but tired brain is letting me see the solution. I can not pivot because there are repeat values in 'number'.</p>
<pre><code>pd.DataFrame({'KeyID':[1,1,1,1,2,2,2,3,3,3], 'number':['a','a','c','d','a','b','c','a','b','c']})
</code></pre>
<p>I need each KeyID to have a 'd' if it does not then a row with NAN needs to be added. Both KeyId 2 and 3 are missing a 'd' so need a blank row adding.</p>
<p><a href=""https://i.sstatic.net/lPoGGk9F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lPoGGk9F.png"" alt=""enter image description here"" /></a></p>
","1","Question"
"79636672","","<p><kbd>pandas 2.2.3</kbd></p>
<p>Let’s consider two variants of defining a custom subtype of <code>pandas.Series</code>. In the first one, no <a href=""https://pandas.pydata.org/pandas-docs/stable/development/extending.html#define-original-properties"" rel=""nofollow noreferrer"">custom properties</a> are added, while in the second one, custom metadata is included:</p>
<pre><code>import pandas as pd


class MySeries(pd.Series):

    @property
    def _constructor(self):
        return MySeries


seq = MySeries([*'abc'], name='data')

print(f'''Case without _metadata:
  {isinstance(seq[0:1], MySeries) = }
  {isinstance(seq[[0, 1]], MySeries) = }
  {seq[0:1].name = }  
  {seq[[0, 1]].name = }
''')

class MySeries(pd.Series):

    _metadata = ['property']

    @property
    def _constructor(self):
        return MySeries


seq = MySeries([*'abc'], name='data')
seq.property = 'MyProperty'

print(f'''Case with defined _metadata:
  {isinstance(seq[0:1], MySeries) = }
  {isinstance(seq[[0, 1]], MySeries) = }
  {seq[0:1].name = }  
  {seq[[0, 1]].name = }
  {getattr(seq[0:1], 'property', 'NA') = }  
  {getattr(seq[[0, 1]], 'property', 'NA') = }  
''')
</code></pre>
<p>The output will be:</p>
<pre class=""lang-none prettyprint-override""><code>Case without _metadata:
  isinstance(seq[0:1], MySeries) = True
  isinstance(seq[[0, 1]], MySeries) = True
  seq[0:1].name = 'data'  
  seq[[0, 1]].name = 'data'

Case with defined _metadata:
  isinstance(seq[0:1], MySeries) = True
  isinstance(seq[[0, 1]], MySeries) = True
  seq[0:1].name = 'data'  
  seq[[0, 1]].name = None         &lt;&lt;&lt; Problematic result of indexing
  getattr(seq[0:1], 'property', 'NA') = 'MyProperty'  
  getattr(seq[[0, 1]], 'property', 'NA') = 'MyProperty'  
</code></pre>
<p>So, if <code>_metadata</code> is defined, the sequence name is preserved when slicing, but <strong>lost when indexing with a list</strong>, whereas without <code>_metadata</code> the name is preserved in both cases.</p>
<p><strong>Question:</strong>
What needs to be done to preserve the name of a custom <code>Series</code> subclass with added metadata when indexing?</p>
","1","Question"
"79637569","","<p>In pandas, I have the following long format dataframe with 1 binary variable « Metric » with 2 modalities (Nb of rooms in residence, squared meters of the residence) :</p>
<pre><code>pd.DataFrame({'State': {0: 'New York', 1: 'California', 2: 'Illinois', 3: 'Texas', 4: 'Arizona', 5: 'Pennsylvania', 6: 'Texas', 7: 'California', 8: 'New York', 9: 'California', 10: 'Illinois', 11: 'Texas', 12: 'Arizona', 13: 'Pennsylvania', 14: 'Texas', 15: 'California'}, 'Capital': {0: 'New York City', 1: 'Los Angeles', 2: 'Chicago', 3: 'Houston', 4: 'Phoenix', 5: 'Philadelphia', 6: 'San Antonio', 7: 'San Diego', 8: 'New York City', 9: 'Los Angeles', 10: 'Chicago', 11: 'Houston', 12: 'Phoenix', 13: 'Philadelphia', 14: 'San Antonio', 15: 'San Diego'}, 'Year': {0: 2021, 1: 2022, 2: 2023, 3: 2024, 4: 2016, 5: 2017, 6: 2018, 7: 2019, 8: 2021, 9: 2022, 10: 2023, 11: 2024, 12: 2016, 13: 2017, 14: 2018, 15: 2019}, 'Type of residence': {0: 'House', 1: 'Apartment', 2: 'House', 3: 'Apartment', 4: 'House', 5: 'Apartment', 6: 'House', 7: 'Apartment', 8: 'House', 9: 'Apartment', 10: 'House', 11: 'Apartment', 12: 'House', 13: 'Apartment', 14: 'House', 15: 'Apartment'}, 'Metric': {0: 'Nb rooms', 1: 'Nb rooms', 2: 'Nb rooms', 3: 'Nb rooms', 4: 'Nb rooms', 5: 'Nb rooms', 6: 'Nb rooms', 7: 'Nb rooms', 8: 'Squared meters', 9: 'Squared meters', 10: 'Squared meters', 11: 'Squared meters', 12: 'Squared meters', 13: 'Squared meters', 14: 'Squared meters', 15: 'Squared meters'}, 'Amount': {0: '3', 1: '4', 2: '5', 3: '6', 4: '3', 5: '6', 6: '5', 7: '4', 8: '48', 9: '59', 10: '100', 11: '250', 12: '276', 13: '340', 14: '405', 15: '470'}})
</code></pre>
<p>I would like to compute a « Squared meters / room » ratio by dividing the surface (numerator) by the number of rooms (denominator), for each observation :</p>
<pre><code>pd.DataFrame({'State': {0: 'New York', 1: 'California', 2: 'Illinois', 3: 'Texas', 4: 'Arizona', 5: 'Pennsylvania', 6: 'Texas', 7: 'California'}, 'Capital': {0: 'New York City', 1: 'Los Angeles', 2: 'Chicago', 3: 'Houston', 4: 'Phoenix', 5: 'Philadelphia', 6: 'San Antonio', 7: 'San Diego'}, 'Date': {0: 2021, 1: 2022, 2: 2023, 3: 2024, 4: 2016, 5: 2017, 6: 2018, 7: 2019}, 'Type of residence': {0: 'House', 1: 'Apartment', 2: 'House', 3: 'Apartment', 4: 'House', 5: 'Apartment', 6: 'House', 7: 'Apartment'}, 'Squared meters / room': {0: '16.0', 1: '14.8', 2: '20.0', 3: '41.7', 4: '92.0', 5: '56.8', 6: '81.1', 7: '117.5'}})
</code></pre>
<p>I would like to get this ratio df ideally without having to transpose the df.</p>
<p>I guess groupby would be the appropriate function to use, but cannot manage to get the proper table. Any help would be very much appreciated. Thanks very much in advance !</p>
","1","Question"
"79638042","","<p>I have imported as dataframe the following xlsx table:</p>
<p><a href=""https://i.sstatic.net/HIAPACOy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HIAPACOy.png"" alt=""enter image description here"" /></a></p>
<p>I try to round numbers from a column in float64 type but round() does not work, although I’ve tried several syntaxes:</p>
<pre><code>df [&quot;column&quot;]. round(1)
df [&quot;column&quot;].astype(&quot;float&quot;).round(1)
df [&quot;column&quot;].astype(&quot;float64&quot;).round(1)
df [&quot;column&quot;].astype(float).round(1)
</code></pre>
<p>I can manage to get the result with the following formula:</p>
<pre><code>df[&quot;column&quot;].map('{:,.1f}'.format)
</code></pre>
<p>however it is less practical and straightforward that round().</p>
","1","Question"
"79638487","","<p>I would like to extract the MIN and MAX from multiple columns (start_1, end_1, start_2, end_2) group by &quot;Name&quot;</p>
<p>I have data that looks like this:</p>
<pre><code>start_1  end_1  start_2  end_2  name
 100      200    300      400    ABC
 100      200    300      400    ABC
 150      250    300      400    ABC
 300      200    300      900    DEF
 50       200    300      1000   DEF
</code></pre>
<p>The output should be like this:</p>
<pre><code>start  end  name
 100   400  ABC
 50    1000 DEF
</code></pre>
<p>Looked into following answers already:
<a href=""https://stackoverflow.com/questions/44776593/group-by-two-columns-and-max-value-of-third-in-pandas-python"">Group by pandas Column</a></p>
<p><a href=""https://stackoverflow.com/questions/74761136/using-pandas-want-to-group-by-multiple-columns-for-min-max-and-add-another-colu"">using-pandas-want-to-group-by-multiple-columns-for-min-max-and-add-another-colu</a></p>
<p>Looking forward for your asistance</p>
","0","Question"
"79638655","","<p>I'm learning Pandas through a tutorial on Kaggle. On <a href=""https://www.kaggle.com/code/residentmario/summary-functions-and-maps"" rel=""nofollow noreferrer"">this page</a>, the following example appears (reviews is a DataFrame):</p>
<pre><code>review_points_mean = reviews.points.mean()
reviews.points.map(lambda p: p - review_points_mean)

def remean_points(row):
    row.points = row.points - review_points_mean
    return row

reviews.apply(remean_points, axis='columns')
</code></pre>
<p>I see that the function <code>remean_points</code> mutates the row it takes as input (in addition to returning it).</p>
<p>However, on Pandas docs (see <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer"">this page</a>), there is the following note regarding DataFrame.apply:</p>
<blockquote>
<p>Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.</p>
</blockquote>
<p>The example on <a href=""https://pandas.pydata.org/docs/user_guide/gotchas.html#gotchas-udf-mutation"" rel=""nofollow noreferrer"">this Pandas docs page</a> is of a function which removes an element from the row (rather than just change an element).</p>
<p>I am new to Pandas, and so I suspect I might not understand the situation correctly.</p>
<blockquote>
<ol>
<li>Is the <code>remean_points</code> function from the Kaggle tutorial supported by Pandas, even though it does change the argument <code>row</code>?</li>
<li>Is the <code>remean_points</code> function not officially allowed by Pandas, but works with the present implementation of DataFrame because it only changes an element of the row?</li>
<li>Maybe the situation is none of the previous two, and there's just something I don't understand correctly?</li>
</ol>
</blockquote>
<p>The example in the Kaggle tutorial works. I tried it. The question is whether this implementation of <code>remean_points</code> is likely to work with future versions of Pandas, and whether it is the recommended way to work with DataFrame.apply when we only need to change a value (another way would be to make a copy of <code>row</code> before changing it).</p>
","1","Question"
"79638867","","<p>I have a very weird error when creating a hdf5 file with pandas in table mode.
I create the file with the store.put() function:</p>
<pre><code>storage_file.put(some_name, current_dataset.meta_data, format=&quot;table&quot;, data_columns=True,min_itemsize={&quot;values&quot;: 100})
</code></pre>
<p>The meta_data is - obviously - a pandas dataframe with the column names:</p>
<pre><code>[
    'index',
    'xPosition',
    'yPosition',
    'approachID',
    'segment_0_type',
    'segment_0_style',
    'segment_0_duration',
    'segment_0_num_points',
    'segment_0_z_start',
    'segment_0_z_end',
    'segment_0_setpoint',
    'segment_1_type',
    'segment_1_style',
    'segment_1_duration',
    'segment_1_num_points',
    'segment_1_z_start',
    'segment_1_z_end',
    'segment_1_setpoint',
    'segment_2_type',
    'segment_2_style',
    'segment_2_duration',
    'segment_2_num_points',
    'segment_2_height_limit',
    'pauseBeforeFirst',
    'pauseOnTipsaver',
    'segmentIndex',
    'heightMultiplier',
    'heightOffset',
    'sensitivity',
    'springConstant',
    'measurementID'
]
</code></pre>
<p>I get the following error message:</p>
<pre><code>IndexError: tuple index out of range
</code></pre>
<p>together with</p>
<pre><code>File &quot;PATH\Lib\site-packages\pandas\io\pytables.py&quot;, line 4473, in write_data
    new_shape = (nrows,) + self.dtype[names[nindexes + i]].shape
</code></pre>
<p>I am sure, that there is something off with the column names, cause when I change the column names to [&quot;0&quot; ... &quot;30&quot;] with &quot;...&quot; the respective numbers in between, the error vanishes. The length of the column names shouldn't be a problem either cause I renamed every coumn names with</p>
<pre><code>for index in range(meta_data.shape[1]):    
   meta_data.rename({meta_data.columns[index]: &quot;&quot;.join([str(index) for i in range(25)])}, axis=1, inplace=True)
</code></pre>
<p>which is incredibly ugly, but works.</p>
<p>Any thoughts about this?</p>
","0","Question"
"79639167","","<p>I'm trying to scrape sectional times for horse races from RacingTV (e.g., <a href=""https://www.racingtv.com/results/2025-05-11/leopardstown/1310"" rel=""nofollow noreferrer"">https://www.racingtv.com/results/2025-05-11/leopardstown/1310</a>) using Python and Selenium, and I need the output to be standardized to 18 furlongs (18 sectional columns) for consistent column alignment across multiple days. Despite hours of work, the script isn't producing the expected results, and I'm not sure why.
What I'm Trying to Achieve
Goal: Scrape sectional times, finishing order, and race distance for all horses in races from specific meetings (e.g., Leopardstown) across multiple days (e.g., May 11–13, 2025).</p>
<p>Output: A CSV file with columns: Meeting, Race Time, Finishing Order, Horse, Distance, Sectional 1 to Sectional 18, Total Time, Sectional 1 Diff to Sectional 18 Diff, Finish Time Diff, Cumulative_At_3F_To_Go, Rank_At_3F_To_Go, Last 3F Section 1 to Last 3F Section 3, FS % 3F, FS % 2F, FS % 1F.</p>
<p>Standardization: Every race should have 18 sectional columns (padded with NaN for shorter races) to align data when combining multiple days.</p>
<p>Challenges:
Sectional times are often missing or not scraped correctly.</p>
<p>Popups (e.g., cookie consent, &quot;Race IQ Metrics&quot;) interfere with scraping.</p>
<p>The script doesn't handle multi-day scraping effectively.</p>
<p>The output CSV has inconsistent columns or missing data for some races.
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
import pandas as pd</p>
<pre><code>def init_driver():
    driver = webdriver.Firefox()
    return driver

def time_to_seconds(time_str):
    try:
        return float(time_str.replace(&quot;s&quot;, &quot;&quot;))
    except:
        return float('nan')

def handle_popups(driver, wait):
    try:
        btn = wait.until((By.ID, &quot;onetrust-accept-btn-handler&quot;))
        btn.click()
    except:
        pass

def scrape_race_sectionals(driver, url, wait):
    data = []
    driver.get(url)
    handle_popups(driver, wait)
    try:
        tab = wait.until((By.XPATH, &quot;//div[contains(text(), 'SECTIONALS')]&quot;))
        tab.click()
        rows = driver.find_elements(By.XPATH, &quot;//div[contains(text(), '1st')]&quot;)
        for row in rows:
            horse = row.find_element(By.XPATH, &quot;.//div[contains(@style, 'TitilliumWeb_')]&quot;).text
            times = [time_to_seconds(t.text) for t in row.find_elements(By.XPATH, &quot;.//div[contains(@style, 'TitilliumWeb_600')]&quot;)]
            times += [float('nan')] * (18 - len(times))
            data.append([&quot;Leopardstown&quot;, &quot;13:10&quot;, horse] + times[:18])
    except:
        print(&quot;No sectionals&quot;)
    return data

driver = init_driver()
wait = WebDriverWait(driver, 10)
try:
    data = scrape_race_sectionals(driver, &quot;https://www.racingtv.com/results/2025-05-11/leopardstown/1310&quot;, wait)
    df = pd.DataFrame(data, columns=[&quot;Meeting&quot;, &quot;Race Time&quot;, &quot;Horse&quot;] + [f&quot;Sectional {i}&quot; for i in range(1, 19)])
    df.to_csv(&quot;race_data.csv&quot;)
finally:
    driver.quit()
</code></pre>
","-1","Question"
"79639319","","<p>I have the following dataframe:</p>
<pre><code>df = pd.DataFrame({&quot;A&quot;: [&quot;sell&quot;, np.nan, np.nan, np.nan, np.nan, &quot;buy&quot;, np.nan, np.nan, np.nan, np.nan],
                   &quot;B&quot;: [&quot;buy&quot;, &quot;buy&quot;, &quot;sell&quot;, &quot;buy&quot;, &quot;buy&quot;, np.nan, &quot;buy&quot;, &quot;buy&quot;, &quot;buy&quot;, np.nan],
                   &quot;C&quot;: [&quot;sell&quot;, &quot;sell&quot;, &quot;sell&quot;, &quot;sell&quot;, &quot;buy&quot;, &quot;sell&quot;, &quot;sell&quot;, &quot;buy&quot;, &quot;buy&quot;, np.nan]},
                   index=pd.date_range(&quot;2025-05-22&quot;, periods=10, freq=&quot;15min&quot;))


print df

                        A     B     C
2025-05-22 00:00:00  sell   buy  sell
2025-05-22 00:15:00   NaN   buy  sell
2025-05-22 00:30:00   NaN  sell  sell
2025-05-22 00:45:00   NaN   buy  sell
2025-05-22 01:00:00   NaN   buy   buy
2025-05-22 01:15:00   buy   NaN  sell
2025-05-22 01:30:00   NaN   buy  sell
2025-05-22 01:45:00   NaN   buy   buy
2025-05-22 02:00:00   NaN   buy   buy
2025-05-22 02:15:00   NaN   NaN   buy
</code></pre>
<p>I want to forward fill the NaN in &quot;A&quot; column until all 3 columns are equals. It should look like this</p>
<pre><code>                        A     B     C
2025-05-22 00:00:00  sell   buy  sell
2025-05-22 00:15:00  sell   buy  sell
2025-05-22 00:30:00  sell  sell  sell
2025-05-22 00:45:00   NaN   buy  sell
2025-05-22 01:00:00   NaN   buy   buy
2025-05-22 01:15:00   buy   NaN  sell
2025-05-22 01:30:00   buy   buy  sell
2025-05-22 01:45:00   buy   buy   buy
2025-05-22 02:00:00   NaN   buy   buy
2025-05-22 02:15:00   NaN   NaN   buy 
</code></pre>
<p>Any suggestion will be greatly appreciated</p>
","3","Question"
"79639750","","<p>I have a dataframe containing rows which describe financial stocks.  The following is a simplified version:</p>
<pre><code>df = pd.DataFrame(
    {
        &quot;stockprice&quot;: [100, 103, 240],
        &quot;Characteristic1&quot;: [1, 3, 3],
        &quot;Characteristic2&quot;: [5, 7, 1],
        &quot;Characteristic3&quot;: [1, 4, 6],
    },
    index=[&quot;Company A&quot;, &quot;Company B&quot;, &quot;Company C&quot;],
)

#            stockprice  Characteristic1  Characteristic2  Characteristic3
# Company A  100         1                5                1              
# Company B  103         3                7                4              
# Company C  240         3                1                6            
</code></pre>
<p>I would like to add a column which should contain for each cell a long dictionary which will be generated based on some of these characteristics - a series of cashflows.  Later I will want to do some calculation on this generated dictionary.</p>
<p>Here is a sample function which generates the dictionary, and then the assign function to put it into my dataframe:</p>
<pre><code>def cashflow_series(ch1=1, ch2=2):
    return {0: ch1, 0.5: ch2, 1: 7, 2: 8, 3: 9}


df.assign(
    cashflows=lambda x: cashflow_series(
        ch1=x[&quot;Characteristic1&quot;], ch2=x[&quot;Characteristic3&quot;]
    )
)
</code></pre>
<p>This returns</p>
<pre><code>           stockprice  Characteristic1  Characteristic2  Characteristic3 cashflows
Company A  100         1                5                1                NaN     
Company B  103         3                7                4                NaN     
Company C  240         3                1                6                NaN
</code></pre>
<p>How can I fix this?</p>
<p>I want the new column 'cashflows' to contain a dictionary for each row, not a NaN.</p>
<p>I want something like this:</p>
<pre><code>           stockprice  Characteristic1  Characteristic2  Characteristic3 cashflows
Company A  100         1                5                1                {0:1,..3:9}
Company B  103         3                7                4                {0:3,..3:9}
Company C  240         3                1                6                {0:3,..3:9}
</code></pre>
","4","Question"
"79640408","","<p>Here is my problem. I have a very large file with about 4m rows, where each row has something like a paragraph of text. Then I have a word list with about 150 terms, some are 1-grams and some are 2-grams. I need to make some very fast Python function to get counts of words as dicts for each row of data.</p>
<p>Here's an MRE to get started.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Sample data

# word_list has 1-grams and 2-grams
word_list = ['apple', 'banana', 'dog', 'machine learning', 'data science', 'big data']

# the real data has 4m rows
texts = 5000 * ['I love apple and banana', 'The dog runs fast', 'machine learning is great', 'apple banana dog', 'data science and big data are related']

# we have it in a df for now with a col called `text`
df = pd.DataFrame({'idx': range(len(texts)), 'text': texts})

# need some fast function to return a dict for each `text` that has word count
def count_words_in_texts(df, word_list):
    &quot;&quot;&quot;
    Return list of dicts with word counts for each row
    df: DataFrame with 'text' column
    word_list: list of words/phrases (1-grams and 2-grams)
    Returns: list of dicts, one per row
    &quot;&quot;&quot;
    # TODO: Implement efficient word counting
    pass

# Expected output format:
# [{'apple': 1, 'banana': 1, 'dog':0...}...]

results = count_words_in_texts(df, word_list)
print(results)
</code></pre>
","-1","Question"
"79641325","","<p>I am using <code>win32com</code> to pull some data out of large excel files and am running into an issue with <code>pywintypes.datetime</code> and subsequent Pandas DataFrame creation.  <code>DataBody = tbl.DataBodyRange()</code> gives a large tuple which I convert into a numpy array using <code>DataBody_array = np.array(DataBody)</code>.</p>
<p>The issue is that the <code>.DataBodyRange()</code> from <code>win32com</code> gets the time data column as <code>pywintypes.datetime</code> and when I try and get this array into a pandas dataframe directly, there is an <code>AttributeError: 'NoneType' object has no attribute 'total_seconds'</code></p>
<p>Based on my understanding, <code>pywintypes.datetime</code> are COM objects representing date/time in windows, but when working with NumPy, these should get converted to <code>numpy.datetime64</code>. <strong>What would be the best method to convert these objects from <code>pywintypes.datetime</code> to <code>numpy.datetime64</code>?</strong> Since the excel tables contain a large amount of data I would like to avoid iteration, I was thinking of using something like <code>np.where</code> but I am struggling to get it to do the conversion/replacements.</p>
<p>Below is a full sample of the code and an image of the very simple excel table for the example.</p>
<p><strong>Python Code</strong></p>
<pre><code>import win32com.client as MyWinCOM
import numpy as np
import pandas as pd

# Open excel instance
xl = MyWinCOM.gencache.EnsureDispatch('Excel.Application')
xl.Visible = True
# Open workbook
wb = xl.Workbooks.Open('Test Excel.xlsx')
# Get worksheet object
ws = wb.Worksheets('Sheet1')
# Get table object
tbl = ws.ListObjects('Table1')
# Get the table headers, convert to list
ColumnNames = tbl.HeaderRowRange()
ColumnNamesList = list(np.array(ColumnNames))

# Get the data body range, convery to np array
DataBody = tbl.DataBodyRange()
DataBody_array = np.array(DataBody)

# Print databody tuple
for item in DataBody:
    print (item)
# Print databody array
for item in DataBody_array:
    print (item)

# Attempt to put into dataframe
Data_df = pd.DataFrame(DataBody_array, columns=ColumnNamesList)

print (Data_df)

</code></pre>
<p><strong>Output:</strong></p>
<pre><code>('a', pywintypes.datetime(2022, 6, 22, 0, 0, tzinfo=TimeZoneInfo('GMT Standard Time', True)), 1.0)
('b', pywintypes.datetime(2023, 10, 18, 0, 0, tzinfo=TimeZoneInfo('GMT Standard Time', True)), None)
('c', pywintypes.datetime(2022, 6, 21, 0, 0, tzinfo=TimeZoneInfo('GMT Standard Time', True)), 3.0)
['a'
 pywintypes.datetime(2022, 6, 22, 0, 0, tzinfo=TimeZoneInfo('GMT Standard Time', True))
 1.0]
['b'
 pywintypes.datetime(2023, 10, 18, 0, 0, tzinfo=TimeZoneInfo('GMT Standard Time', True))
 None]
['c'
 pywintypes.datetime(2022, 6, 21, 0, 0, tzinfo=TimeZoneInfo('GMT Standard Time', True))
 3.0]
</code></pre>
<p><strong>Excel File/Table</strong>
<a href=""https://i.sstatic.net/byohECUr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/byohECUr.png"" alt=""Test Excel.xlsx"" /></a></p>
<p><strong>Traceback</strong></p>
<pre><code>Traceback (most recent call last):
  File &quot;***.py&quot;, line 32, in &lt;module&gt;
    print (Data_df)
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\core\frame.py&quot;, line 1011, in __repr__
    return self.to_string(**repr_params)
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\core\frame.py&quot;, line 1192, in to_string
    return fmt.DataFrameRenderer(formatter).to_string(
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\format.py&quot;, line 1128, in to_string
    string = string_formatter.to_string()
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\string.py&quot;, line 25, in to_string
    text = self._get_string_representation()
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\string.py&quot;, line 40, in _get_string_representation
    strcols = self._get_strcols()
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\string.py&quot;, line 31, in _get_strcols
    strcols = self.fmt.get_strcols()
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\format.py&quot;, line 611, in get_strcols
    strcols = self._get_strcols_without_index()
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\format.py&quot;, line 875, in _get_strcols_without_index
    fmt_values = self.format_col(i)
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\format.py&quot;, line 889, in format_col
    return format_array(
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\format.py&quot;, line 1316, in format_array
    return fmt_obj.get_result()
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\format.py&quot;, line 1347, in get_result
    fmt_values = self._format_strings()
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\io\formats\format.py&quot;, line 1810, in _format_strings
    values = self.values.astype(object)
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\core\arrays\datetimes.py&quot;, line 666, in astype
    return dtl.DatetimeLikeArrayMixin.astype(self, dtype, copy)
  File &quot;C:\Program Files\Python39\lib\site-packages\pandas\core\arrays\datetimelike.py&quot;, line 415, in astype
    converted = ints_to_pydatetime(
  File &quot;pandas\_libs\tslibs\vectorized.pyx&quot;, line 158, in pandas._libs.tslibs.vectorized.ints_to_pydatetime
  File &quot;pandas\_libs\tslibs\timezones.pyx&quot;, line 266, in pandas._libs.tslibs.timezones.get_dst_info
AttributeError: 'NoneType' object has no attribute 'total_seconds'
</code></pre>
","1","Question"
"79641353","","<p>This questions relates to a similar question which can be found here:
<a href=""https://stackoverflow.com/questions/54965009/how-to-create-a-variability-plot-with-a-multi-level-grouped-label-x-axis"">How to create a variability plot with a multi-level grouped label x-axis</a></p>
<p>I have stolen code from this post to get myself started:
<a href=""https://stackoverflow.com/questions/74062380/plot-both-multi-index-labels-on-x-axis-in-pandas-plot"">Plot both multi-index labels on x-axis in pandas plot</a></p>
<p>...but I am currently stuck</p>
<p>my data is in this format:</p>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# for same set of random variates
np.random.seed(seed = 123)
dates = pd.date_range(start = &quot;28/05/2024&quot;, end = &quot;28/05/2025&quot;, freq = &quot;D&quot;)
vehicles = np.random.choice([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;], size = len(dates))
kilometres = np.random.normal(loc = 170, scale = 76, size = len(dates)).round(2)
df = pd.DataFrame({&quot;date&quot;:dates, &quot;fleet no&quot;:vehicles, &quot;kilometres&quot;:kilometres})
# groupby to get data into appropriate format
df = df.groupby(by = [df[&quot;date&quot;].dt.to_period(&quot;M&quot;), &quot;fleet no&quot;]).agg({&quot;date&quot;:&quot;count&quot;, &quot;kilometres&quot;:&quot;sum&quot;})\
    .rename(columns = {&quot;date&quot;:&quot;days&quot;}).reset_index(level = None)
</code></pre>
<p>I want to be able to produce a graph whereby the x-axis is grouped by both date and fleet no, the same way they are formatted in the subplots produced by this code:</p>
<pre><code>groups = df.groupby(by = &quot;date&quot;)
figure, axes = plt.subplots(nrows = 1, ncols = len(groups), sharey = True, figsize = (16, 5))

for ax, (year, group) in zip(axes, groups):
    group.set_index(&quot;fleet no&quot;).rename_axis(year)[&quot;days&quot;].plot(kind = &quot;bar&quot;, ax = ax)
</code></pre>
<p>except it should be <strong>all in one plot</strong></p>
<p>Additionally, the year-month axis labels should be formatted in the &quot;%Y-%b&quot; format.</p>
<p>any help would be greatly appreciated</p>
","0","Question"
"79641847","","<p>I am using a PostgreSQL database and a big pandas DataFrame (~3500 records) that has to be uploaded into one of the tables in the database.</p>
<p>First, I have to test for existing records in the database and, afterwards, upload non existing ones.</p>
<p>The problem is that the CPU gets heavily loaded and sometimes my old &quot;server&quot; crashes.</p>
<p>So I divided the queries into smaller ones.</p>
<p>In the case of the uploading, I use the <code>df.to_sql()</code> method with the option <code>chunksize=150</code>, so that the upload takes place in chunks of 150 records.</p>
<p>In the case of the testing for existing records, I use the column in the dataframe that is defined as primary key in the database table (it is of type datetime, or Timestamp). So I have a long <code>where</code> clause like this:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT datetime FROM my_table
WHERE datetime = datetime_row1_df
   OR datetime = datetime_row2_df
   ...
   OR datetime = datetime_row3500_df
</code></pre>
<p>In order to reduce the CPU load, I divided this query into smaller ones, so that the first one tests for the existence in the database of the dataframe rows from 1 to 150, the second query tests for the existence of the dataframe rows from 151 to 300, and so on.</p>
<p>So I have two questions:</p>
<ul>
<li>Regarding the <code>df.to_sql()</code> method, is it a good method to reduce the CPU load?</li>
<li>Regarding the partition of the <code>where</code> clause in the SQL query, I'm wondering if what I'm doing is putting even more strain on the CPU. I think that with the first chunk, the server seeks the entire datatable, with the second one, again an entire search, and so on. Would it be better to keep it as a single and long query? If the answer is affirmative, what alternatives do I have here in order to reduce CPU load for this query?</li>
</ul>
","0","Question"
"79642757","","<p>I am using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>pandas.read_csv</code></a> to load a CSV file. I want to have my file read mostly as-is to avoid any <a href=""https://stackoverflow.com/q/16988526/3357935"">automatic data type conversions</a>, except for a specific column (<code>send_date</code>) I want parsed as a datetime.</p>
<p>The reason I want most columns read as strings or objects is to preserve data like zip codes with leading zeros (04321) and Boolean-like values (<code>true</code>, <code>false</code>, <code>unknown</code>) that are stored as strings.</p>
<h2>Problem</h2>
<p>Using <code>read_csv</code> without specifying <code>dtype</code> causes unwanted type conversions.</p>
<pre><code>df = pandas.read_csv(&quot;test.csv&quot;, parse_dates=['send_date'])
# name: Madeline (type: object)                         - correct
# zip_code: 4321 (type: int64)                          - wrong (missing leading 0)
# send_date: 2025-04-13 00:00:00 (type: datetime64[ns]) - correct
# is_customer: True (type: bool)                        - wrong (not a string)
</code></pre>
<p>Using <code>dtype=object</code> correctly preserves <code>zip_code</code> and <code>is_customer</code> as string-like values, but it prevents <code>send_date</code> from being set to type <code>datetime64[ns]</code>.</p>
<pre><code>df = pandas.read_csv(&quot;test.csv&quot;, dtype=object, parse_dates=['send_date'])
# name: Madeline (type: object)                 - correct
# zip_code: 04321 (type: object)                - correct
# send_date: 2025-04-13 00:00:00 (type: object) - wrong (not datetime)
# is_customer: true (type: object)              - correct
</code></pre>
<p>Manually setting the <code>dtype</code> for <code>send_date</code> to <code>datetime64</code> raises an error.</p>
<pre><code>df = pandas.read_csv(&quot;test.csv&quot;, dtype={&quot;send_date&quot;:&quot;datetime64&quot;}, parse_dates=['send_date'])
# TypeError: the dtype datetime64 is not supported for parsing, pass this column using parse_dates instead
</code></pre>
<p>Setting <code>dtype=str</code> causes <code>send_date</code> to be interpreted as an integer timestamp.</p>
<pre><code>df = pandas.read_csv(&quot;test.csv&quot;, dtype=str, parse_dates=['send_date'])
# name: Madeline (type: object)                 - correct
# zip_code: 04321 (type: object)                - correct
# send_date: 1744502400000000000 (type: object) - wrong (not a date)
# is_customer: true (type: object)              - correct
</code></pre>
<h2>Sample Data (test.csv)</h2>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>zip_code</th>
<th>send_date</th>
<th>is_customer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Madeline</td>
<td>04321</td>
<td>2025-04-13</td>
<td>true</td>
</tr>
<tr>
<td>Theo</td>
<td>32255</td>
<td>2025-04-08</td>
<td>true</td>
</tr>
<tr>
<td>Granny</td>
<td>84564</td>
<td>2025-04-15</td>
<td>false</td>
</tr>
</tbody>
</table></div>
<h2>Desired output</h2>
<ul>
<li>name: Madeline (type: object)</li>
<li>zip_code: 04321 (type: object)</li>
<li>send_date: 2025-04-13 00:00:00 (type: datetime64[ns])</li>
<li>is_customer: true (type: object)</li>
</ul>
<h2>Attempted Code</h2>
<pre class=""lang-py prettyprint-override""><code>import pandas

def print_first_row_value_and_dtype(df: pandas.DataFrame):
    row = df.iloc[0]
    for col in df.columns:
        print(f&quot;{col}: {row[col]} (type: {df[col].dtype})&quot;)

filename = 'test.csv'

df = pandas.read_csv(filename, parse_dates=['send_date'])  
print_first_row_value_and_dtype(df)

df = pandas.read_csv(filename, dtype=object, parse_dates=['send_date'])
print_first_row_value_and_dtype(df)

df = pandas.read_csv(filename, dtype=str, parse_dates=['send_date'])
print_first_row_value_and_dtype(df)

dtypes = {&quot;name&quot;:&quot;object&quot;, &quot;zip_code&quot;:&quot;object&quot;, &quot;send_date&quot;:&quot;datetime64&quot;, &quot;is_customer&quot;:&quot;object&quot;}
df = pandas.read_csv(filename, dtype=dtypes, parse_dates=['send_date']) # raises TypeError
</code></pre>
<h2>Question</h2>
<p>How can I make <code>pandas.read_csv()</code> parse one column (<code>send_date</code>) as a datetime while treating all other columns as strings or objects to avoid unwanted data type conversions?</p>
","1","Question"
"79643566","","<p>I am using Python 3.11.3 on Windows and Pandas 2.0.0.</p>
<p>I have this code with</p>
<pre><code>from datetime import datetime
import pandas as pd

pd.__version__
'2.0.0'

rng = pd.date_range(start='2017-01-01', end='2018-01-01', freq='B')

rng[-10:]

DatetimeIndex(['2017-12-19', '2017-12-20', '2017-12-21', '2017-12-22',
               '2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',
               '2017-12-29', '2018-01-01'],
              dtype='datetime64[ns]', freq='B')

rng[-10:].is_year_end

array([False, False, False, False, False, False, False, False,  True,
       False])

rng[rng.is_year_end]

DatetimeIndex(['2017-12-29'], dtype='datetime64[ns]', freq='B')
</code></pre>
<p>While it is true that '2017-12-29' <strong>was the last business day of the year</strong>, it was not the last day of the year.</p>
<p>However,</p>
<pre><code>rng2 = pd.date_range(start='2017-01-01', end='2018-01-01')

rng2[-10:]

DatetimeIndex(['2017-12-23', '2017-12-24', '2017-12-25', '2017-12-26',
               '2017-12-27', '2017-12-28', '2017-12-29', '2017-12-30',
               '2017-12-31', '2018-01-01'],
              dtype='datetime64[ns]', freq='D')

rng2[-10:].is_year_end

array([False, False, False, False, False, False, False, False,  True,
       False])

rng2[rng2.is_year_end]

DatetimeIndex(['2017-12-31'], dtype='datetime64[ns]', freq='D')
</code></pre>
<p>Which makes more sense to me.</p>
<p>The documentation on <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.is_year_end.html#pandas.DatetimeIndex.is_year_end"" rel=""nofollow noreferrer"">pandas.DatetimeIndex.is_year_end</a> or <a href=""https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.Series.dt.is_year_end.html"" rel=""nofollow noreferrer"">pandas.Series.dt.is_year_end</a> does not specify anything about business days or any other constraint.</p>
","2","Question"
"79644117","","<p>I have an excel file where one line contains id that is 16-digit number.<br />
It looks like:</p>
<pre><code>item1               item2               item3  
1234567890123456    1234567866623000    1234567877722000  
1320                800                 201  
2019-01-05          2020-01-04          2017-06-05  
</code></pre>
<p>I use python and pandas to read it.<br />
When I read it by pandas some ids are malformed - last digits are different, i.e. <code>1234567866623000</code> becomes <code>1234567866622992</code>.<br />
I think it is because excel has 15-digits precision, but my ids are 16 digits.<br />
Code that reads excel file is:</p>
<pre><code>import pandas as pd
...
df = pd.read_excel(filepath, sheet_name='MyList', header=None)
</code></pre>
<p>When I convert cells to str type in pandas some ids are still malformed.<br />
But when I add apostrophe <code>'</code> before each id in excel file it becomes the text type in excel and then pandas reads values correctly without loss of precision.<br />
How do I add apostrophe in the begginging of every cell in one row using pandas?<br />
Or is it any better way to read 16-digits id from excel?</p>
","0","Question"
"79644324","","<p>I have a dataframe column containing values of 0 and 1. Values of 0 indicate a piece of equipment is offline, while 1 indicates the equipment is running.  To calculate the days online between outages, I used:</p>
<pre class=""lang-py prettyprint-override""><code>df['col2'] = df[col1].groupby(df_proc[col1].eq(0).cumsum()).cumcount()
</code></pre>
<p><code>df['col2']</code> contains a cumulative total of the days online between outages.</p>
<p>Example:</p>
<p><img src=""https://i.sstatic.net/pmcBcvfg.png"" alt=""Dataframe"" /></p>
<p>I need to extract the date and the cumumlative total before each of the outages into a separate dataframe.  From the example above I would want:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>DaysOnline</th>
</tr>
</thead>
<tbody>
<tr>
<td>6/2/2025</td>
<td>4</td>
</tr>
<tr>
<td>6/10/2025</td>
<td>5</td>
</tr>
</tbody>
</table></div>
<p>I obviously I am a novice at Python and Stack Overflow.</p>
","1","Question"
"79644379","","<p><strong>CONTEXT</strong></p>
<p>I am trying to parse dates in a DataFrame and want to catch all rows that aren't able to be converted and then log those rows/ID's.  I want to ignore any rows that fail to parse because it is a null or empty value and only include those that fail such as '|199/142/4' in the example DataFrame below.</p>
<p><strong>CURRENT CODE</strong></p>
<pre><code># Create a dataframe with float values
data = {
'ID': [1, 2, 3, 4, 5],
'Date': ['09/02/1999', '11/12/2001', '', '', '|199/142/4']
}

df = pd.DataFrame(data)

original_null = df[col].isnull()
    try:
        df['Date'] = pd.to_datetime(df['Date'], format='mixed', errors='coerce').dt.strftime('%m/%d/%Y')
        invalid_dates = df[df['Date'].isnull()]
        invalid_dates = invalid_dates[~invalid_dates.index.isin(original_null[original_null].index)]
        if not invalid_dates.empty:
            with open(data_path + client_alias + '_removed_ids.log', 'w') as log_file:
                    log_file.write(f&quot;Removed IDs for column {col}:\n&quot;)
                    log_file.write(invalid_dates['ID'].to_string(index=False) + '\n')
                
                
            df = df[~df['ID'].isin(invalid_dates['ID'])]
    except Exception as e:
        None
</code></pre>
<p><strong>EXPECTED OUTCOME</strong></p>
<p>A DataFrame that only contains ID 5 and the corresponding date in a string value that could not be parsed.</p>
<pre><code>ID    Date
5     '|199/142/4'
</code></pre>
","2","Question"
"79647209","","<p>I am downloading some finance data and then adding new columns to a dataframe. But, when I saved the dataframe to an Excel file, the new columns I created are not getting saved in Excel.</p>
<pre><code>import yfinance as yf
import pandas as pd
import time


stock_data = yf.download(tickers=&quot;AAPL&quot;, interval=&quot;1d&quot;, period=&quot;3y&quot;) 

stock_data['SMA50'] = stock_data['Close'].rolling(window=50).mean()
stock_data['SMA200'] = stock_data['Close'].rolling(window=200).mean()
stock_data['buy?'] = &quot;&quot;

df = pd.DataFrame(stock_data)

for row in df.itertuples():
    sma200 = row[7]
    close = row[1]
    if(sma200 &gt; close):
        print(&quot;buy!&quot;)
        row[8] = &quot;buy!&quot;


writer = pd.ExcelWriter('AAPL.xlsx', engine='xlsxwriter')
df.to_excel(writer, sheet_name=&quot;datas&quot;, startrow=1, header=False, index=True)
workbook = writer.book
worksheet = writer.sheets[&quot;datas&quot;]

writer.close()
</code></pre>
","1","Question"
"79648028","","<p>I have a csv file which has data like the following:</p>
<p><a href=""https://i.sstatic.net/LhS83Eod.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LhS83Eod.png"" alt=""enter image description here"" /></a></p>
<p>I'm trying to get that into a JSON format as shown below:</p>
<pre><code>[
    {
        &quot;customerInformation&quot;: {
            &quot;customerNumber&quot;: &quot;000202&quot;
        },
        &quot;itemInformation&quot;: [
            {
                &quot;itemNumber&quot;: &quot;1660100110857&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;510520144&quot;
            }
        ]
    },
    {
        &quot;customerInformation&quot;: {
            &quot;customerNumber&quot;: &quot;001040&quot;
        },
        &quot;itemInformation&quot;: [
            {
                &quot;itemNumber&quot;: &quot;0171100243&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;0171100288&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;017110021212&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;017110561010&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;2028G1006&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;2028G206&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;0669406015&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;181902000&quot;
            },
            {
                &quot;itemNumber&quot;: &quot;0669401020&quot;
            }
        ]
    }
]
</code></pre>
<p>I almost have it perfect, but I can't figure out how to insert the nested customerNumber under customerInformation.  Here is what I have so far, but it's not the exact output I need.</p>
<pre><code>import pandas as pd
import os
import sys
import json

directory = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop')

dtype_dic = {'customerNumber' : str, 'itemNumber':str}

if os.path.isfile(directory + '/' + 'Price_Checker.csv'):
    print(&quot;Program Running.... please standby&quot;)
else:
    print(&quot;Required file Price_Checker.csv not found on user's Desktop&quot;)
    sys.exit()

df = pd.read_csv(directory + r'\\Price_Checker.csv', dtype=dtype_dic)


print(df)
result = []

for cust_id, cust_df in df.groupby('customerNumber'):
    cust_dict = {'customerInformation': cust_id}
    cust_dict['itemInformation'] = []
    for item_id, item_df in cust_df.groupby('itemNumber'):
        item_dict = {'itemNumber': item_id}
        cust_dict['itemInformation'].append(item_dict)
    result.append(cust_dict)

print(json.dumps(result, indent=4))
</code></pre>
<p>Thank you in advance for any assistance you can lend.</p>
","2","Question"
"79649939","","<p>A pandas dataframe was pickled on another machine via</p>
<pre class=""lang-py prettyprint-override""><code>df.to_pickle('data.pkl')
</code></pre>
<p>I tried loading that pickle both through</p>
<pre class=""lang-py prettyprint-override""><code>with open('path/to/data.pkl', 'rb') as handle:
    data = pickle.load(handle)
</code></pre>
<p>and</p>
<pre class=""lang-py prettyprint-override""><code>data = pd.read_pickle('path/to/data.pkl')
</code></pre>
<p>both yielded</p>
<pre class=""lang-py prettyprint-override""><code>ModuleNotFoundError: No module named 'numpy._core.numeric'
</code></pre>
<p>Both computers run pandas 2.2.2 and numpy 1.26.4, going to these versions worked for <a href=""https://www.reddit.com/r/learnpython/comments/1dnzcpi/no_module_numpy_core_i_need_urgent_help/"" rel=""nofollow noreferrer"">some others</a> that faced this error, not me.</p>
","0","Question"
"79650039","","<p>I have a Taipy application where I want to, when selected a value from the drop-down component, to dynamically render tables.</p>
<p>I have data frame that I load from csv file with columns:
<em>partnerName, regionName, userId, groupName, depositAmount, withdrawalAmount, totalRevenue</em>.<br />
My drop-down component will contain values: <em>partnerName, regionName, userId, groupName</em>. After each selection of a value from a drop-down, the value will be removed and the table will be rendered, <strong>along with the other tables that are already rendered</strong>, for previously selected values.</p>
<p>For each selected value from drop-down, the <strong>original (default)</strong> data frame will be grouped by that value and all numeric columns will be aggregated. The table should have data from this aggregated data frame.</p>
<p>My idea so far was to save each of those aggregated data frames to a list and then iterate through that list and display it in a table using <strong>partial</strong> from Taipy. When tried to use index</p>
<pre><code>for i, _ in enumerate(state.tables_data):
                with builder.part(&quot;card&quot;):
                    builder.text(f&quot;Filter applied: {state.filter_display_texts[i]}&quot;, class_name=&quot;h2&quot;)
                    builder.table(data=state.tables_data[i], page_size=10)
</code></pre>
<p>, it always says that</p>
<blockquote>
<pre><code>&gt; WARNING:root:
--- 1 warning(s) were found for page 'TaiPy_partials_0'  ---
 - Warning 1: table.data property should be bound.
-------------------------------------------------------------

return attrgetter(name)(gui._get_data_scope())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ AttributeError: 'types.SimpleNamespace' object has no attribute ''
</code></pre>
</blockquote>
<p>and if I try to iterate by data frame it says:</p>
<blockquote>
<p>--- 2 warning(s) were found for page 'TaiPy_partials_0'  ---</p>
<ul>
<li>Warning 1: table.data property should be bound.</li>
<li>Warning 2: Can't find Data Accessor for type &lt;class 'str'&gt;.</li>
</ul>
</blockquote>
<p>and an error</p>
<blockquote>
<p>Traceback (most recent call last):   File
&quot;taipy_apps\taipy-app\Lib\site-packages\taipy\gui\gui.py&quot;, line 676,
in _manage_message
self.__request_data_update(str(message.get(&quot;name&quot;)), message.get(&quot;payload&quot;))   File
&quot;taipy_apps\taipy-app\Lib\site-packages\taipy\gui\gui.py&quot;, line 1134,
in __request_data_update
newvalue = _getscopeattr_drill(self, var_name)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;taipy_apps\taipy-app\Lib\site-packages\taipy\gui\utils_attributes.py&quot;,
line 26, in _getscopeattr_drill
return attrgetter(name)(gui._get_data_scope())
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ AttributeError: 'types.SimpleNamespace' object has no attribute ''<br />
_warn(f&quot;Decoding Message has failed: {message}&quot;, e)</p>
</blockquote>
<p>I am using:
Python v3.12.0
Taipy v4.0.3
Pandas v2.2.2</p>
<pre><code>My code so far: 

from taipy.gui import Gui
import taipy.gui.builder as builder
import pandas as pd
from datetime import date

# Load data
data = pd.read_csv('my_data.csv')
data.fillna(0, inplace=True)

# Initialize variables
selected_partners = None
partners = data['partnerName'].unique().tolist()  

starting = date(2025, 1, 1)
ending = date(2025, 5, 1)
dates = [starting, ending]

selected_filter = None
filters = [ 'partnerName', 'regionName', 'userId', 'groupName' ]  

filter_display_texts = []
tables_data = []

def load_data(state):
    print(state.selected_partners)
    print(state.dates)
    state.data = data[data['partnerName'].isin(state.selected_partners)] if state.selected_partners else data

def filter_category(state):
    if not state.selected_filter:
        return
    
    print('Filtering data by:', state.selected_filter)
    state.filter_display_texts.append(state.selected_filter)
    value = state.data.groupby(state.selected_filter)[['depositAmount', 'withdrawalAmount', 'totalRevenue']].sum().reset_index() 
    state.tables_data.append(value)
    
    with builder.Page() as table_data:
        with builder.layout(&quot;1&quot;):
            for val in state.tables_data:
                with builder.part(&quot;card&quot;):
                    builder.table(data=val, class_name=&quot;h2&quot;)

    tables_partial.update_content(state, table_data)
    state.filters = [f for f in state.filters if f != state.selected_filter]

def reset_state(state):
    pass

with builder.Page() as page:
    builder.date_range(dates=&quot;{dates}&quot;, label_start=&quot;Start Date&quot;, label_end=&quot;End Date&quot;)
    builder.selector(value=&quot;{selected_partners}&quot;, lov=&quot;{partners}&quot;, dropdown=True, multiple=True, filter=True, label=&quot;Select region (if none all will be displayed)&quot;)
    builder.button(label=&quot;Submit&quot;, on_action=load_data)
    builder.button(label=&quot;Reset&quot;, on_action=reset_state)
    builder.html(&quot;br&quot;)
    
    builder.selector(value='{selected_filter}', lov=&quot;{filters}&quot;, dropdown=True, label='Select filter', on_change=filter_category)
    builder.part(partial=&quot;{tables_partial}&quot;)

gui = Gui(page=page)
tables_partial = gui.add_partial(&quot;&quot;)
gui.run(title=&quot;Analytics Dashboard&quot;, dark_mode=True, use_reloader=True, port=5001, debug=True)
</code></pre>
","-1","Question"
"79651091","","<pre><code>!python -V
#%%
import pandas as pd
import pickle
from sklearn.metrics import root_mean_squared_error
from sklearn.feature_extraction import DictVectorizer
import mlflow

def read_dataframe1(filename):
    df = pd.read_parquet(filename)

    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime
    df.duration = df.duration.apply(lambda td: td.total_seconds()/60)

    df = df[df.duration &gt;= 1] &amp; (df.duration &lt;= 60)

    categorical = ['PULocationID', 'DOLocationID']
    df[categorical] = df[categorical].astype(str)

    return df

#%%
train_df = read_dataframe1(&quot;https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet&quot;) 
</code></pre>
<p>When I run the last line for loading the parquet file with the function read_dataframe1, it does not load even after waiting over 30 minutes and the cell keeps running</p>
<p>What can be the reason?</p>
","-3","Question"
"79651120","","<p>I've read <a href=""https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.format.html"" rel=""nofollow noreferrer"">the documentation</a> and simply cannot understand why I can't seem to achieve my objective.</p>
<p>All I want to do is output integers with a thousands separator where appropriate.</p>
<p>I'm loading a spreadsheet from my local machine that is in the public domain <a href=""https://www.nsandi.com/files/asset/xlsx/prize-june-2025.xlsx"" rel=""nofollow noreferrer"">here</a></p>
<p>Here's my MRE:</p>
<pre><code>import pandas as pd

WORKBOOK = &quot;/Volumes/Spare/Downloads/prize-june-2025.xlsx&quot;

def my_formatter(v):
    return f&quot;{v:,d}&quot; if isinstance(v, int) else v

df = pd.read_excel(WORKBOOK, header=2, usecols=&quot;B,C,E:H&quot;)
print(df.dtypes)
df.style.format(my_formatter)
print(df.head())
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Prize Value                    int64
Winning Bond NO.              object
Total V of Holding             int64
Area                          object
Val of Bond                    int64
Dt of Pur             datetime64[ns]
dtype: object
   Prize Value Winning Bond NO.  Total V of Holding                Area  Val of Bond  Dt of Pur
0      1000000      103FE583469               50000           Stockport         5000 2005-11-29
1      1000000      352AC359547               50000  Edinburgh, City Of         5000 2019-02-11
2       100000      581WF624503               50000          Birmingham        20000 2024-06-03
3       100000      265SM364866               50000       Hertfordshire        32500 2016-01-31
4       100000      570HE759643               11000       Hertfordshire        11000 2024-02-22
</code></pre>
<p>I have determined that <em>my_formatter()</em> is never called and I have no idea why.</p>
","2","Question"
"79651472","","<p>Given a dataframe with three columns <code>x</code>, <code>y</code>, and <code>strata</code> what is the simplest way to compute the correlation between <code>x</code> and <code>y</code> for each strata?</p>
<p>For example,</p>
<pre><code>example_df = pd.DataFrame({
    'strata': ['A', 'A', 'A', 'B', 'B'],
    'x': [-1, 0, 1, -1, 1],
    'y': [-1, 0, 1, 1, -1],
})
</code></pre>
<p>For 'A' correlation is <code>1.0</code> and for 'B' correlation is <code>-1.0</code>.</p>
","0","Question"
"79652595","","<p>I am new to python and pandas library and not able to find solution to this simple problem. I am totally stuck at this point. I have a dataframe that contains a 'timestamp' column (datatype is <code>datetime64[ns, 'Asia/Kolkata']</code>. Originally the data in 'timestamp' column was in in64 format (1748174400000, 1748188800000 etc..)</p>
<p>I converted this column in dataframe to Datetime Aware column using the below command:</p>
<pre><code>df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc = True)
</code></pre>
<p>Output was --</p>
<pre><code>(2025-05-25 12:00:00+00:00, 2025-05-25 16:00:00+00:00 etc..)
</code></pre>
<p>Then I converted the timezone to 'Asia/Kolkata' using the below command:</p>
<pre><code>df['timestamp'] = df['timestamp'].dt.tz_convert('Asia/Kolkata')
</code></pre>
<p>Output was --</p>
<pre><code>(2025-05-25 17:30:00+05:30, 2025-05-25 21:30:00+05:30 etc..)
</code></pre>
<p>I want to convert the 'timestamp' column values to the actual local date and time of Kolkata, instead of converting it to +05:30 added to UTC. How can I achieve this?</p>
","2","Question"
"79653054","","<p>I want to update the <code>base</code> pandas DataFrame with data from the <code>update</code> DataFrame. This applies to multiple rows of the base DataFrame (ie the merge features are not unique so can't be used as an index) and I cannot use the index matching techniques of <code>pd.join()</code> or <code>pd.merge()</code> because I want to overwrite the columns. Using <code>df.loc[]</code> and <code>df.update()</code> doesn't work because usind <code>['id','date']</code> as an index of base is non unique.</p>
<p>Previous solutions end up with null values or using list comprehensions to create bool lists for indexing. There must be a better way!</p>
<p>Note, I often do this where base has up to 20M rows and update has up to 5K rows, and often the matching is on 3-5 columns. Using list comps can be sloooow unless I do laborious jiggery pokery of multiple separate list comps for each conditional column</p>
<pre><code>base = pd.DataFrame({
    'grp':['A','A','B','B','A','B','C'],
    'id': ['a','b','a','b','a','a','c'], 
    'date': ['2025-01-01']*4+['2025-01-02']*3, 
    'cat1': [0]*7, 
    'cat2': [0]*7,
    })
base:
    grp id  date    cat1    cat2
0   A   a   2025-01-01  0   0
1   A   b   2025-01-01  0   0
2   B   a   2025-01-01  0   0
3   B   b   2025-01-01  0   0
4   A   a   2025-01-02  0   0
5   B   a   2025-01-02  0   0
6   C   c   2025-01-02  0   0

update = pd.DataFrame({
    'date': ['2025-01-01', '2025-01-01', '2025-01-02', '2025-01-02'],
    'id': ['a', 'b', 'a', 'b'],
    'cat1': [1, 0, 1, 0],
    'cat2': [0, 1, 1, 1],
    })
update:
    date    id  cat1    cat2
0   2025-01-01  a   1   0
1   2025-01-01  b   0   1
2   2025-01-02  a   1   1
3   2025-01-02  b   0   1

desired_output = pd.DataFrame({
    'grp':['A','A','B','B','A','B','C'],
    'id': ['a','b','a','b','a','a','c'], 
    'date': ['2025-01-01']*4+['2025-01-02']*3, 
    'cat1': [1,0,1,0,1,1,0], 
    'cat2': [0,1,0,1,1,1,0],
    })
desired_output:
    grp id  date    cat1    cat2
0   A   a   2025-01-01  1   0
1   A   b   2025-01-01  0   1
2   B   a   2025-01-01  1   0
3   B   b   2025-01-01  0   1
4   A   a   2025-01-02  1   1
5   B   a   2025-01-02  1   1
6   C   c   2025-01-02  0   0
</code></pre>
","0","Question"
"79653120","","<p>I am trying to connect and write a Pandas Dataframe to a Postgres custom schema on Supabase. Below is the code.</p>
<pre><code>from sqlalchemy import create_engine

def server_access():
    # Create SQLAlchemy connection string
    conn_str = (
        f&quot;postgresql+psycopg2://{&quot;[USER]&quot;}:{&quot;[PASSWORD]&quot;}&quot;
        f&quot;@{&quot;[HOST]&quot;}:{[PORT]}/{&quot;[SCHEMA]&quot;}?client_encoding=utf8&quot;
    )
    engine = create_engine(url = conn_str)
    return engine
    
engine = server_access()
df.to_sql('tbl', engine, if_exists='append', index=False)
</code></pre>
<p>As per the <a href=""https://supabase.com/docs/guides/api/using-custom-schemas"" rel=""nofollow noreferrer"">documentation</a> I've followed the steps to expose custom schema but it didn't worked. However, if I try using public schema the same code works fine without defining <code>client_encoding</code> in connection string.</p>
<p>How to connect and write a Pandas Dataframe using SQLAlchemy on a custom schema on Supabase?</p>
<p>Edit:</p>
<p>Sample connection string: <code>postgresql+psycopg2://user:password@host:6543/custom_schema?client_encoding=utf8</code></p>
<p>After replacing <code>custom_schema</code> with <code>postgres</code> it works fine.</p>
<p>Error:</p>
<pre><code>conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) server didn't return client encoding
(Background on this error at: https://sqlalche.me/e/20/e3q8)
</code></pre>
","2","Question"
"79653549","","<p>I have very limited knowledge of pandas.</p>
<p>The data I'm using covers 2 dst (daylight saving) transitions for the UK (from 1 Sep 24 to 30 Apr 25), and consist of timestamps in milliseconds with values. The transition dates are 27 Oct 24 and 30 Mar 25.</p>
<p>Using df.resample and a rule of 86400000ms I resample the data to get one value per day.</p>
<p>If I then log the dataframe data I appear to have one value per day, with no omitted/missing days in the period.</p>
<p>I then use df.to_json to convert the data to json.</p>
<p>The resulting json data does not have a data point for the date 30th March 25. The value for the 30th is instead been given against the timestamp for the 31st.</p>
<p>Any help much appreciated.</p>
","0","Question"
"79653826","","<p>I have a dataset representing a parameter measured over well depth intervals:</p>
<pre><code>df = pd.DataFrame(
    {
        &quot;depth in&quot;: [1000, 1020, 1040],
        &quot;depth out&quot;: [1020, 1040, 1060],
        &quot;Parameter&quot;: [10, 15, 5],
    }
)
</code></pre>
<p>How can I create a 2D chart showing Depth vs Parameter, where vertical bars represent the parameter value over each depth interval?</p>
<p>I'd like the plot to show each parameter value as a vertical bar spanning from <code>'depth in'</code> to <code>'depth out'</code>. I tried using <code>matplotlib</code>, but I'm having trouble configuring the axes correctly.</p>
","2","Question"
"79653909","","<p>I'm using streamlit's <code>st.data_editor</code>, and I have this <code>DataFrame</code>:</p>
<pre><code>import streamlit as st
import pandas as pd

df = pd.DataFrame(
    [
       {&quot;command&quot;: &quot;test 1&quot;, &quot;rating&quot;: 4, &quot;is_widget&quot;: True},
       {&quot;command&quot;: &quot;test 2&quot;, &quot;rating&quot;: 5, &quot;is_widget&quot;: False},
       {&quot;command&quot;: &quot;test 3&quot;, &quot;rating&quot;: 3, &quot;is_widget&quot;: True},
   ]
)
edited_df = st.data_editor(df)
</code></pre>
<p>is there a way to color a specific cell based on a condition? I want to colorize in yellow the cell (in the rating col) when the value is less then 2.</p>
","2","Question"
"79654798","","<p>I have a csv that has 162 rows, but around 3 million columns. I want to read it with pandas. I have enough RAM available, but <code>pd.read_csv(file.csv, header=None, dtype=str)</code> takes forever. The cells contain either one letter or 0. Are there any helpful options to make this feasible/faster?</p>
","1","Question"
"79655324","","<p>Suppose I have this:</p>
<pre class=""lang-py prettyprint-override""><code>ISresult = h25.groupby(['month','impactedservice']).agg({'resolvetime': ['count','median','mean', 'min', 'max','std']})
</code></pre>
<p>The column list looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>[('resolvetime', 'count'),
 ('resolvetime', 'median'),
 ('resolvetime', 'mean'),
 ('resolvetime', 'min'),
 ('resolvetime', 'max'),
 ('resolvetime', 'std')]
</code></pre>
<p>What is a good way to only show the Top N or the Bottom N <code>{Month, ImpactedService}</code> pairs going against a measure such as Mean?
We need to keep Month Sort order, but the secondary sort should be on the Mean measure.  And then we only want to output the highest let's say 5 of those measures per month or the lowest 5.</p>
<p>Example data snippet showing part of Month 1, but we have 5 months in the dataset.</p>
<pre><code>                                                        resolvetime   \
                                                               count   
month impactedservice                                                  
1     ABCD                            7   
      ALD                            18   
      ANE                            24   
      ARP                            31   
      AZZ                            14   
...                                                              ...   
                                                                      \
                                                              median   
month impactedservice                                                  
1     ABCD                       23.2150   
      AMAM                        4.4400   
      BRN                         2.15
...

\ 

mean 

month impactedservice                                                  
1     ABCD                       14.215000   
      AMAM                        2.33   
      BRN                         1.15

\ ... and so on, for the other measures for month 1 and 
      the other months measures 
      We need to keep the month sorted ascending.
</code></pre>
","2","Question"
"79655383","","<p>I have a dataset with two columns: <code>sID</code> (string identifier) and <code>sum_count</code> (numeric value). My goal is to partition these sIDs into 5 groups with the following requirements:</p>
<ol>
<li><p>The sum of sum_count values in each group should be as equal as possible (exact equality is not required, but close approximations
are preferred).</p>
</li>
<li><p>Within each group, the sIDs should be merged into a comma-separated string.</p>
</li>
</ol>
<p>Example Input:</p>
<pre><code>sID    sum_count
A      10
B      5
C      8
D      12
E      9
F      13
</code></pre>
<p>Expected Output Format:</p>
<pre><code>Group1: sID=A,C,E | sum_count_total=27  
Group2: sID=B,D,F | sum_count_total=30
...  # Continuing for 5 groups
</code></pre>
<p>I tried naive methods like:</p>
<ul>
<li>Random sampling</li>
<li>Sorting by sum_count followed by round-robin/cyclic
distribution but these produced imbalanced group sums (e.g., ±20%
variance).</li>
</ul>
<p>Is there a more efficient algorithm or existing tool (e.g., Python library, DolphinDB function) to achieve this?</p>
","2","Question"
"79655669","","<p>I am using pyomo to generate my model which depends on input data from a pandas data frame. In the model, I need to add a binary variable for every pair (column, row) in the data frame for which the corresponding entry is greater than zero.</p>
<p>So far I am doing the following (which works) but it generates too many variables, obviously.</p>
<pre><code>df = pd.read_csv(...)    
cols = df.columns.tolist()
rows = df.index.tolist()

model = ConcreteModel()
model.myVars = Var(cols, rows, within=Binary, name=&quot;myVars&quot;)
</code></pre>
<p>What's the easiest and most elegant way to only generate a variable in <code>myVars</code> for <code>c in cols</code> and <code>r in rows</code> if <code>df[c][r] &gt; 0</code>?</p>
","1","Question"
"79656093","","<p>I am trying to come up with a frequency in Pandas that represents the start of a calendar week (configurable by week start). For example, for all dates from 2025-01-06 (Monday) to 2025-01-13 (Sunday), I want them to be categorized under Monday's date of 2025-01-06.</p>
<p>However, because of how Pandas <a href=""https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.DateOffset.html"" rel=""nofollow noreferrer"">DateOffset</a> rolls dates forward to the next period, not all the dates in that week are categorized under 2025-01-06.</p>
<p>You can see what I'm describing in this sample example:</p>
<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; 
&gt;&gt;&gt; range_dex = pd.date_range(start=&quot;2025-01-06&quot;, end=&quot;2025-01-30&quot;)
&gt;&gt;&gt; sample_series = pd.Series(1, index=range_dex)
&gt;&gt;&gt; sample_series
2025-01-06    1
2025-01-07    1
2025-01-08    1
...
2025-01-28    1
2025-01-29    1
2025-01-30    1
Freq: D, dtype: int64
&gt;&gt;&gt; 
&gt;&gt;&gt; week_start = 0  # 0 for Monday, ..., 6 for Sunday
&gt;&gt;&gt; freq_pd = pd.offsets.Week(n=1, weekday=week_start)
&gt;&gt;&gt; 
&gt;&gt;&gt; sample_series.groupby(pd.Grouper(freq=freq_pd)).size()
2025-01-06    1
2025-01-13    7
2025-01-20    7
2025-01-27    7
2025-02-03    3
Freq: W-MON, dtype: int64
</code></pre>
<p>The result I am hoping to achieve instead is:</p>
<pre><code>&gt;&gt;&gt; sample_series.groupby(pd.Grouper(freq=freq_pd)).size()
2025-01-06    7
2025-01-13    7
2025-01-20    7
2025-01-27    4
Freq: W-MON, dtype: int64
</code></pre>
<p>I've also tried <code>pd.offsets.Week(n=0, weekday=week_start)</code> and <code>pd.offsets.Week(n=-1, weekday=week_start)</code>, which both error, and I can't do the typical math I could do on a Series with something like <code>series + pd.offsets.Week(n=1, weekday=week_start) - pd.offsets.Week(n=1, weekday=week_start)</code>. I also have a use case for <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.asfreq.html"" rel=""nofollow noreferrer""><code>.asfreq</code></a> so really want this specified in terms of a frequency, if possible. Thanks!</p>
<p>For reference, this is the calendar for January 2025:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Sun</th>
<th style=""text-align: center;"">Mon</th>
<th style=""text-align: center;"">Tue</th>
<th style=""text-align: center;"">Wed</th>
<th style=""text-align: center;"">Thu</th>
<th style=""text-align: center;"">Fri</th>
<th style=""text-align: center;"">Sat</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">29</td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: center;"">31</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">4</td>
</tr>
<tr>
<td style=""text-align: center;"">5</td>
<td style=""text-align: center;"">6</td>
<td style=""text-align: center;"">7</td>
<td style=""text-align: center;"">8</td>
<td style=""text-align: center;"">9</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: center;"">11</td>
</tr>
<tr>
<td style=""text-align: center;"">12</td>
<td style=""text-align: center;"">13</td>
<td style=""text-align: center;"">14</td>
<td style=""text-align: center;"">15</td>
<td style=""text-align: center;"">16</td>
<td style=""text-align: center;"">17</td>
<td style=""text-align: center;"">18</td>
</tr>
<tr>
<td style=""text-align: center;"">19</td>
<td style=""text-align: center;"">20</td>
<td style=""text-align: center;"">21</td>
<td style=""text-align: center;"">22</td>
<td style=""text-align: center;"">23</td>
<td style=""text-align: center;"">24</td>
<td style=""text-align: center;"">25</td>
</tr>
<tr>
<td style=""text-align: center;"">26</td>
<td style=""text-align: center;"">27</td>
<td style=""text-align: center;"">28</td>
<td style=""text-align: center;"">29</td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: center;"">31</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table></div>
","1","Question"
"79656511","","<p>I have a DataFrame with a handful of date columns, I want to create a new column &quot;MaxDate&quot; that contains the maximum date.  I tried using <code>apply</code>, but my various code patterns for the lambda function yield errors.</p>
<pre><code>import pandas as pd
import datetime as dt
df=pd.DataFrame(
   [ [
      dt.date(2025,6,5), dt.date(2025,6,6) ],[
      dt.date(2025,6,7), dt.date(2025,6,8) ]
   ],
   columns=['A','B'], index=['Row1','Row2']
)

# Explicitly find maximum of row 0 (WORKS)
max( df.loc[ df.index[0], ['A','B'] ] )

# None of the following 3 code patterns work for &quot;apply&quot;

if False:

   df['MaxDate'] = df.apply( lambda row: max(
      row.loc[ row.index[0], ['A','B'] ]
   )  )

   # IndexingError: &quot;Too many indexers&quot;

elif False:

   df['MaxDate'] = df.apply( lambda row: max(
      row['A','B']
   )  )

   # KeyError:
   # &quot;key of type tuple not found and not a MultiIndex&quot;

elif False:

   df['MaxDate'] = df.apply( lambda row: max(
      row['A'],row['B']
   )  )

   # KeyError: 'A'
</code></pre>
<p>I tried determining whether the <code>row</code> variable was a DataFrame or a Series, but the result was <code>nan</code></p>
<pre><code># Querying class of &quot;row&quot; yields a column of &quot;nan&quot;
df['MaxDate'] = df.apply( lambda row: type(row) )
</code></pre>
<p>Of the 3 code patterns above, I would like to avoid th3 last one because it requires too many repetitions of the word <code>row</code>, making my code &quot;noisy&quot;.</p>
<p>What am I doing wrong?</p>
<p>Others have cited <a href=""https://stackoverflow.com/questions/12169170"">this duplicate question</a>, which I appreciate.  To me, however, this is more than just a question of how to achieve the end effect.  It is also sussing out my understand of the <code>apply</code> method.  What is wrong with my use of its mechanics?  And why doesn't <code>type(row)</code> show the class of the <code>row</code> object?  Without visibility into its type, it's hard to smartly come up with code patterns that are likely to work.  I've re-titled the question to reflect this.</p>
","2","Question"
"79656645","","<p>I have a pandas data frame with a MultiIndex, where any row @ index (i,j) should be equal to the row @ index (j,i). This is a unique MultiIndex, i.e. - no duplicate index values, and originally the above constraint is not satisfied, i.e. - there are rows (i,j) which are not equal to row (j,i). I am trying to fix this by using .loc and setting the values manually, but I'm not sure what I'm doing wrong. Can anyone explain why the snippet below doesn't work and how to achieve what I'm trying to achieve? Note: since I create the data with np.random there is a VERY SMALL chance you will get a True instead of a False at the end. Just run it again and you should get the &quot;correct&quot; outcome.</p>
<pre><code>#make a dataframe with a multi index
np.random.seed(0)
idx1 = pd.MultiIndex.from_product([list(range(3)),list(range(1,4))])
df = pd.DataFrame(np.random.randint(0,100,9*2).reshape(9,2),index=idx1)

#get a new multiindex that is the &quot;reverse&quot; of the original
idx2 = pd.MultiIndex.from_frame(idx1.to_frame().iloc[:,[1,0]])

#filter the first index by membership in the &quot;reverse&quot; index  
idx1 = idx1[idx2.isin(idx1)]

#do the same for the &quot;reverse&quot; index
idx2 = idx2[idx2.isin(idx1)]

#set the values of the dataframe for idx2 with idx1 values
df.loc[idx2] = df.loc[idx1].values

#compare the dataframe at the overlapping indices (should be True but it's False)
print(np.all(df.loc[idx1].values == df.loc[idx2].values))
</code></pre>
","1","Question"
"79657102","","<p>This code works:</p>
<pre><code>cohort = r'priority' 
result2025 = df.groupby([cohort],as_index=False).agg({'resolvetime': ['count','mean']})
</code></pre>
<p>and this code works</p>
<pre><code>cohort = r'impactedservice' 
result2025 = df.groupby([cohort],as_index=False).agg({'resolvetime': ['count','mean']})
</code></pre>
<p>and this code works</p>
<pre><code>result2025 = df.groupby(['impactedservice','priority'],as_index=False).agg({'resolvetime': ['count','mean']})
</code></pre>
<p>but what is not working for me is defining the cohort variable to be</p>
<pre><code>cohort = r'impactedservice,priority' # a double-cohort
result2025 = df.groupby([cohort],as_index=False).agg({'resolvetime': ['count','mean']})
</code></pre>
<p>That gives error:</p>
<pre><code>KeyError: 'impactedservice,priority'
</code></pre>
<p>How to properly define the cohort variable in this case?</p>
","3","Question"
"79657477","","<p>I have a dataframe that looks like this:</p>
<blockquote>
<p>[('Close', 'AAPL'), ('High', 'AAPL'), ('Low', 'AAPL'), ('Open', 'AAPL'), ('Volume', 'AAPL')] [Ticker            AAPL        AAPL        AAPL   AAPL      AAPL] [Date     ] [2025-06-05  200.630005  204.750000  200.149994  203.5  55126100] [2025-06-06  203.919998  205.699997  202.050003  203.0  46539200]
<a href=""https://i.sstatic.net/K5TmW7Gy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/K5TmW7Gy.png"" alt=""enter image description here"" /></a></p>
</blockquote>
<p>What I need is a list of dictionaries with 'Close', 'High', 'Low', and 'Volume'. I'm dropping 'AAPL' and the df is already ordered by 'Date'.</p>
<blockquote>
<p>{'2025-06-05'{'Close':200.05, 'High':204.75,...},'2025-06-06-:{'Close':203.92,'High':...}}</p>
</blockquote>
<p>UPDATE:
There was a request for a reproducible sample. What I'm doing is getting the data from yFinance and trying to make it into a form usable by ta.</p>
<pre><code>import yfinance as yf
from ta import add_all_ta_features

df = yf.download('AAPL', start='2025-06-04', end='2025-06-06', interval='1d')

records = df.to_dict('records') #Here is where code needs to be added to transform df
data_ind = add_all_ta_features(records, open=&quot;Open&quot;, high=&quot;High&quot;, low=&quot;Low&quot;,
                close=&quot;Close&quot;, volume=&quot;Volume&quot;)
</code></pre>
","-1","Question"
"79657712","","<p>I am trying to download financial data using <code>yfinance</code> which seems to return a panda, and try to convert that to a simple python list. Here is the complete code:</p>
<pre><code>import yfinance as yf

# Download 1-minute intraday data for Airbus 
df = yf.download(&quot;AIR.DE&quot;, interval=&quot;1m&quot;, period=&quot;1d&quot;)

# Get datetime list and close prices
x = df.index.tolist()          
y = df[&quot;Close&quot;].tolist()       
</code></pre>
<p>However I get an error</p>
<pre><code>AttributeError: 'DataFrame' object has no attribute 'tolist'
</code></pre>
<p>So how to convert the float values of the panda to a list of floats?</p>
","0","Question"
"79658047","","<p>I am new to <code>pandas</code> and I am trying to catch up on its API design. What I am most interested in is to get a good rule of thumb to predict wheter calling a method on a dataframe will return a new copy of it (that I must assign to a variable) or will modify it inplace.</p>
<p>The documentation mentions everywhere that <a href=""https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html"" rel=""nofollow noreferrer"">Copy-On-Write</a> will be the future standard, therefore I have enabled it setting <code>pd.options.mode.copy_on_write = True</code> and I am only interested in its behaviour when copy on write is active.</p>
<p>Here is an example of the transformations I need to apply to a data set loaded from an Excel sheet.
Although the snippet below seems to do what I need, I always have to reassign to the variable <code>df</code> the modified dataframe returned by each method.</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_excel(&quot;my_excel_file.xls&quot;, sheet_name=&quot;my_sheet&quot;, usecols=&quot;A:N&quot;)  # load dataframe from excel sheet
df = df.dropna(how='all')            # remove empty rows
df = df.iloc[:-1,:]                  # remove last row
df.columns.array[0] = &quot;Resource&quot;     # change name of the first column
df = df.astype({&quot;Resource&quot;: int})    # change column type
df.columns = df.columns.str.replace('Avg of ', '').str.replace('Others', 'others')  # alter column names
df = df.set_index(&quot;Resource&quot;)        # use 'Resource' column as index
df = df.sort_index(axis=0)           # sort df by index value
df = df / 100                        # divide each entry by 100
df = df.round(4)                     # round to 4 decimals
df = df.reindex(columns=sorted(df))  # order columns in ascending alphabetical order
</code></pre>
<p>What is the recommended way to carry out the operations in the snippet above? Is it correct to assume that each method that modifies the dataframe is not applied inplace and returns a new dataframe object that I need to assign to a variable? More generally, is reassigning the variable <code>df</code> after each step the recommended way to use pandas API?</p>
","0","Question"
"79659380","","<p>I have a column in my dataframe that is called delivery period. The delivery period is supposed to be in the format 'Month Year' (January 2025). It shows as the string &quot;2025-01-01 to 2025-12-31&quot;.</p>
<p>I need to identify where this occurs and create a new row for each month with the same data.
For instance:</p>
<pre><code>Price Curve     Text            Delivery Period
TEST            TEST        2025-01-01 to 2025-12-31
some curve      some text       June 2025
some curve      some text       July 2025
some curve      some text       August 2025
some curve      some text       September 2025
some curve      some text       October 2025
some curve      some text       November 2025
some curve      some text       December 2025 
</code></pre>
<p>to</p>
<pre><code>Price Curve       Text            Delivery Period
TEST              TEST              January 2025
TEST              TEST              February 2025
TEST              TEST              March 2025
TEST              TEST              April 2025
TEST              TEST              May 2025
TEST              TEST              June 2025
TEST              TEST              July 2025
TEST              TEST              August 2025
TEST              TEST              September 2025
TEST              TEST              October 2025
TEST              TEST              November 2025
TEST              TEST              December 2025
some curve      some text           June 2025
some curve      some text           July 2025
some curve      some text           August 2025
some curve      some text           September 2025
some curve      some text           October 2025
some curve      some text           November 2025
some curve      some text           December 2025 

</code></pre>
<p>MRE:</p>
<pre><code>data_dict_cols = {
    'Price Curve': [
        &quot;TEST&quot;,
        &quot;some curve&quot;,
        &quot;some curve&quot;,
        &quot;some curve&quot;,
        &quot;some curve&quot;,
        &quot;some curve&quot;,
        &quot;some curve&quot;,
        &quot;some curve&quot;
    ],
    'Text': [
        &quot;TEST&quot;,
        &quot;some text&quot;,
        &quot;some text&quot;,
        &quot;some text&quot;,
        &quot;some text&quot;,
        &quot;some text&quot;,
        &quot;some text&quot;,
        &quot;some text&quot;
    ],
    'Delivery Period': [
        &quot;2025-01-01 to 2025-12-31&quot;,
        &quot;June 2025&quot;,
        &quot;July 2025&quot;,
        &quot;August 2025&quot;,
        &quot;September 2025&quot;,
        &quot;October 2025&quot;,
        &quot;November 2025&quot;,
        &quot;December 2025&quot;
    ]
}

df = pd.DataFrame(data_dict_cols)
</code></pre>
","1","Question"
"79660030","","<p>I have a dataset like the below which was got by traversing through a hierarchy:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>rank</th>
<th>manager_id</th>
<th>manager_id_1</th>
<th>manager_id_2</th>
<th>...</th>
<th>manager_id_20</th>
</tr>
</thead>
</table></div>
<p>There is not a set number of levels, so the data might well go to manager_id_25 in future.</p>
<p>For every row, I want to find the first column from manager_id onwards which satisfies a condition. The condition is to check if a specific substring is present in the column.</p>
<p>For example, if the row data is:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">id</th>
<th style=""text-align: left;"">name</th>
<th style=""text-align: left;"">rank</th>
<th style=""text-align: left;"">manager_id</th>
<th style=""text-align: left;"">manager_id_1</th>
<th style=""text-align: left;"">manager_id_2</th>
<th style=""text-align: left;"">manager_id_3</th>
<th style=""text-align: left;"">...</th>
<th style=""text-align: left;"">result</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">xx1</td>
<td style=""text-align: left;"">namexx</td>
<td style=""text-align: left;"">T</td>
<td style=""text-align: left;"">xx2-TR</td>
<td style=""text-align: left;"">xx3-FR</td>
<td style=""text-align: left;"">xx4-FR</td>
<td style=""text-align: left;"">xx22-ER</td>
<td style=""text-align: left;"">...</td>
<td style=""text-align: left;"">xx3-FR</td>
</tr>
</tbody>
</table></div>
<p>and I want the first column that contains &quot;FR&quot; in the data, then &quot;xx3-FR&quot; has to be populated in the result column.</p>
","0","Question"
"79660430","","<p>I am trying to copy the data in <strong>data.csv</strong> from <code>A1</code> to <code>T1</code> and append this data in <strong>Mergeddata.xlsx</strong> in cells from <code>D2</code> to <code>W2</code>. With below Code I am able to write the data in the specific cells.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import os

df=pd.read_csv('data.csv', header=0)
with pd.ExcelWriter('Mergeddata.xlsx', engine=&quot;openpyxl&quot;, mode='a', if_sheet_exists='overlay') as writer:
   df.to_excel(writer, sheet_name='Sheet1',startcol=3,startrow=1,index=False)
</code></pre>
<p><strong>Data.csv(Initial content) data that needs to copy:</strong></p>
<p>90.77 90.7 90.8 81 69.3 90  0.72    0.71    0.72    64.46   90  64.54   63.64   65  5808.68                             90         0.98 <strong>0.98</strong>     <strong>0.98</strong>   88.24   <strong>90</strong></p>
<p><img src=""https://i.sstatic.net/H3EbyB8O.png"" alt=""enter image description here"" /></p>
<p><strong>Initial data(MergeData.xlsx) :</strong></p>
<pre class=""lang-none prettyprint-override""><code>Time    NE  Index   DuVoltageAvg(V) DuVoltageMin(V) DuVoltageMax(V) DuVoltageTot(V) DuVoltageCnt(count) DuCurrentAvg(A) DuCurrentMin(A) DuCurrentMax(A) DuCurrentTot(A) DuCurrentCnt(count) DuPowerAvg(W)   DuPowerMin(W)   DuPowerMax(W)   DuPowerTot(W)   DuPowerCnt(count)   PowerFactorAvg(PF)  PowerFactorMin(PF)  PowerFactorMax(PF)  PowerFactorTot(PF)  PowerFactorCnt(count)

2025-06-08 15:15    SC_401B ShelfID0/SlotID0    90.73   90.6    90.8    8165.7  90  0.72    0.71    0.72    64.44   90  64.46   63.64   65.45   5801.3  90  0.98    0.98    0.98    88.24   90
</code></pre>
<p><strong>Data After Excelwriter (MergeData.xlsx):</strong><br>
90.77   90.7    90.8    8169.3  90.0    0.72    0.71    0.72.1  64.46   <strong>90.0.1</strong>  64.54   63.64   65.0    5808.68 <strong>90.0.2</strong>  0.98    <strong>0.98.1</strong>  <strong>0.98.2</strong>  88.24   <strong>90.0.3</strong></p>
<p><img src=""https://i.sstatic.net/nu4Me6LP.png"" alt=""enter image description here"" /></p>
<p>After the Excelwriter, somehow 2 additional decimal points (dots) started coming in the cell values.</p>
<p>I need help with this issue</p>
","1","Question"
"79661119","","<p>Trying to understand behavior in Pandas <code>apply</code> with <code>raw=True</code> and underlying numpy <code>apply_along_axis</code>.</p>
<p>(More context: the reason I'm using Pandas UDF with <code>raw</code> is to make a UDF within a PySpark job, with <code>@pandas_udf</code> for performance without overhead of creating <code>pd.Series</code> per row.)</p>
<p>The following code will break with an error:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from typing import Optional

def func(a: int) -&gt; Optional[int]:
  if a % 3 == 0: return 1
  if a % 3 == 1: return 0
  else: return None 

df = pd.DataFrame([[1], [2], [3], [4], [5], [6]])

print(df.apply(lambda row: func(row[0]), axis=1, raw=True))
</code></pre>
<p>Error:</p>
<pre><code>TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
</code></pre>
<p>This is because the underlying numpy code creates a buffer as an array based on the <em>first</em> result of the function, which is dtype=int64 in this case, and can't handle saving a None to the array.</p>
<p>Now, if the <em>very first</em> result from the function is <code>None</code>, numpy will create a buffer of dtype=object instead, and I get a valid result:</p>
<pre><code>df = pd.DataFrame([[2], [3], [4], [5], [6]])

# output:
0    None
1       1
2       0
3    None
4       1
</code></pre>
<p>Is this expected behavior -- can you just not use <code>apply</code> with <code>raw</code> if the applied Python function returns <code>Optional[int]</code>?</p>
","0","Question"
"79662505","","<p>So i have a Series with year index (1901,1902,1903 etc) and values. My  challenge in course is &quot;Show a tick mark on the x-axis for every 5 years from 1900 to 2020. (Hint: you'll need to use NumPy).&quot; I cant find a method, how can i do this?</p>
<pre><code>prize_year = df_data.groupby('year').agg({'prize': pd.Series.count})
prize_year.rolling(window=5).mean().head(10)
</code></pre>
<p>I dont know how to use NumPy exactly here to achieve it.</p>
<p>I also have a matplotlib chart:</p>
<pre class=""lang-py prettyprint-override""><code>plt.xlim(1900, 2020)
plt.scatter(prize_year.index,prize_year.values)
</code></pre>
<p>This is the final result that I need:</p>
<p><img src=""https://i.sstatic.net/TKr8o3Jj.webp"" alt=""This is the final result that i need"" /></p>
","-1","Question"
"79662638","","<p>I am using graph-tool to plot an adjacency matrix from a dataframe. The adjacency matrix looks correct and symmetric, but when I plot it using graph-tool, the resulting visualization is incorrect. (I can look at the dataframe/array to see where there should be connections, but there are none. For example, I know that node 100 should have more than one connection, but it is currently only connected to 99.)</p>
<p>I created a dataframe with about 133 columns, that looks approximately like this:</p>
<p><img src=""https://i.sstatic.net/4agwkl3L.png"" alt=""an adjacency matrix in a dataframe"" /></p>
<p>I used <a href=""https://stackoverflow.com/questions/66079457/building-and-adjacency-matrix-from-2-column-pandas-df"">this</a> code to create a symmetric matrix, and it doesn't look like there are any problems there in terms of the matrix. (I think I don't technically need a symmetric matrix if it's undirected, but I thought that was what was causing my issue of missing connections and nodes, initially.)</p>
<p>However, when trying to graph the matrix using this code, borrowing from graph-tool,</p>
<pre><code>new_array = symmetric_adjacency_matrix.to_numpy()
print(new_array)
new_g = Graph(scipy.sparse.lil_matrix(new_array),directed=False)
graph_draw(new_g,vertex_text=new_g.vertex_index)
</code></pre>
<p>it creates this image:</p>
<p><img src=""https://i.sstatic.net/8MS0arrT.png"" alt=""a graph with connected nodes"" /></p>
<p>Unfortunately, when looking at this, I know that the node 100 should be connected to node 116 as well, along with others. But this is not happening, and I am not sure why. Moreover, it is missing nodes, such as 133. 133 exists in my dataframe, but it is not showing up in the graph.</p>
<p>Is it due to the conversion from a dataframe to an array? I'm not sure what my issue is here, nor how to fix it.</p>
<p>Edit: Reproducible example (I used Google Colab, so that is why the import for graph-tool is like that. Need an exclamation point for shell commands.):</p>
<pre><code>#installed conda
!pip install -q condacolab
import condacolab
condacolab.install()

#installed graph-tool
!mamba install -q graph-tool

#imported from graph-tool
from graph_tool.all import *

#below is pulled from their google colab installation 
g = collection.data[&quot;celegansneural&quot;]
state = minimize_nested_blockmodel_dl(g)

#imports
import numpy as np
import scipy
import pandas as pd

#dataframe taken from my data and shortened
df = pd.DataFrame({'p1': [1, 1, 2, 2, 2, 2, 3, 3, 3, 3], 
                   'p2': [2, 4, 3, 4, 5, 14, 4, 5, 14, 17]})

#creates symmetrical adjacency matrix
def get_adjacency_matrix(df, col1, col2):
    df = pd.crosstab(df[col1], df[col2])
    idx = df.columns.union(df.index)
    df = df.reindex(index = idx, columns=idx, fill_value=0)
    return df

a_to_b = get_adjacency_matrix(df, &quot;p1&quot;, &quot;p2&quot;)
b_to_a = get_adjacency_matrix(df, &quot;p2&quot;, &quot;p1&quot;)

symmetric_adjacency_matrix = a_to_b + b_to_a
symmetric_adjacency_matrix

#turns matrix into array, graphs it
new_array = symmetric_adjacency_matrix.to_numpy()
print(new_array)
new_g = Graph(scipy.sparse.lil_matrix(new_array),directed=False)
graph_draw(new_g,vertex_text=new_g.vertex_index)
</code></pre>
","1","Question"
"79663044","","<p>I need to calculate pairwise correlations between 3,000 stocks using minute-level returns derived from high-frequency trade data. I have a working DolphinDB implementation and want to compare its performance with an equivalent Python implementation.
DolphinDB Implementation (Reference)</p>
<pre><code>// 1. Calculate minute-level VWAP
priceMatrix = exec wavg(Trade_Price, Trade_Volume) 
             from trades 
             where Time between 09:30:00.000000000 : 16:00:00.000000000 
             pivot by minute(Time) as minute, Symbol

// 2. Forward fill missing values
priceMatrix.ffill!()

// 3. Compute minute-level returns
retMatrix = each(def(x): ratios(x)-1, priceMatrix)

// 4. Calculate pairwise correlations
corrMatrix = cross(corr, retMatrix)
</code></pre>
<p>I'm aware of pandas/numpy but need guidance on the most performant approach:</p>
<pre><code>import pandas as pd
import numpy as np
from datetime import datetime, time
import time as tm

# 1. Generate sample trade data (replace with your actual data)
num_stocks = 3000
symbols = [f'S_{i}' for i in range(num_stocks)]
times = pd.date_range(&quot;2023-01-01 09:30:00&quot;, &quot;2023-01-01 16:00:00&quot;, freq=&quot;s&quot;)

np.random.seed(42)
trades = pd.DataFrame({
    'Time': np.random.choice(times, size=10_000_000),
    'Symbol': np.random.choice(symbols, size=10_000_000),
    'Trade_Price': np.random.uniform(10, 500, size=10_000_000),
    'Trade_Volume': np.random.randint(1, 10000, size=10_000_000)
})

# Filter market hours
trades = trades[trades['Time'].dt.time.between(time(9,30), time(16,0))]

start = tm.time()
# 2. Calculate minute-level VWAP
vwap = (trades.groupby([pd.Grouper(key='Time', freq='1min'), 'Symbol'])
          .apply(lambda x: np.average(x['Trade_Price'], weights=x['Trade_Volume']))
          .unstack()
          .ffill())
print(f&quot;VWAP calculation completed in {tm.time() - start:.2f} seconds&quot;)  # 158.85s

start = tm.time()
# 3. Compute returns
returns = vwap.pct_change()
print(f&quot;Returns calculation completed in {tm.time() - start:.2f} seconds&quot;) # 0.25s

start = tm.time()
# 4. Calculate correlation matrix (optimized)
corr_matrix = returns.corr()
print(f&quot;Correlation matrix calculation completed in {tm.time() - start:.2f} seconds&quot;)   #11.54s
</code></pre>
<p>The above Python code executes very slowly when calculating pairwise correlations across 3,000 stocks. Are there more efficient approaches for large-scale correlation computations in Python?</p>
","0","Question"
"79664817","","<p>Could some one help me out with an efficient solution to convert a tabular data into a numpy matrix?</p>
<p>The input is tabular data with header in the following format:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">D1</th>
<th style=""text-align: right;"">D2</th>
<th style=""text-align: left;"">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">A</td>
<td style=""text-align: right;"">B</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: right;"">C</td>
<td style=""text-align: right;"">D</td>
<td style=""text-align: left;"">2</td>
</tr>
<tr>
<td style=""text-align: right;"">E</td>
<td style=""text-align: right;"">F</td>
<td style=""text-align: left;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">A</td>
<td style=""text-align: right;"">D</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: right;"">E</td>
<td style=""text-align: right;"">B</td>
<td style=""text-align: left;"">4</td>
</tr>
</tbody>
</table></div>
<p>The desired output is a numeric numpy array:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: right;""></th>
<th style=""text-align: right;""></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">3</td>
</tr>
</tbody>
</table></div>
<p>Here, D1 column values of table &quot;transforms&quot; to rows in the array (representing A/C/E in order) and D2 column values of table &quot;transforms&quot; to column in the array (representing B/D/F in order) and the values are arranged appropriately.</p>
","0","Question"
"79664986","","<p>I have a list of lists and I need to create groups: each group should be start by a pattern (the word &quot;START&quot; in this case) and should be end the line before the next pattern, here below the example:</p>
<pre><code>lst = [
    [&quot;abc&quot;],
    [&quot;START&quot;],
    [&quot;cdef&quot;],
    [&quot;START&quot;],
    [&quot;fhg&quot;],
    [&quot;cdef&quot;],
]

group_a = [
    [&quot;START&quot;],
    [&quot;cdef&quot;],
]

group_b = [
    [&quot;START&quot;],
    [&quot;fhg&quot;],
    [&quot;cdef&quot;],
]
</code></pre>
<p>I tried with numpy and pandas too without any success.
Many thanks in advance for your support.
Regards
Tommaso</p>
","-5","Question"
"79665073","","<p>I have a .csv file that I want to convert into numpy arrays. I want one numpy array containing the header's names, and a second numpy array containing the data.
I managed to read my file and put it in a dataframe format. But how can i convert this dataframe into two arrays please ?</p>
<p>Here is what I wrote to convert it into a dataframe and please find on this link my .csv file <a href=""https://www.dropbox.com/scl/fi/xr0sry5tiiw9arhgt3dml/data.csv?rlkey=efdgosd8zekmbmyi1ipslatl6&amp;st=bs81dzf8&amp;dl=0"" rel=""nofollow noreferrer"">text</a></p>
<pre><code>import pandas as pd

from pathlib import Path



universal_path = Path(r&quot;file\file\data.csv&quot;)

data = pd.read_csv(universal_path , sep = ';'  , skiprows = 0,  
                   encoding = 'ISO-8859-1', engine = 'python', header = 0 ) 
</code></pre>
","1","Question"
"79665475","","<p>I am trying to scrape data from the following web page, &quot;https://www.cocorahs.org/ViewData/StationPrecipSummary.aspx&quot; using html_session(). I need the &quot;Date&quot; and &quot;Precip In&quot; in two separate columns in a table for Station 1 : MD-BL-13..</p>
<p>Can someone help me out !!</p>
<p>I found this code on stack overflow but it is giving me an error: &quot;NoneType object has no attribute html&quot; on the second last line (df = pd.read_html..). What do I change in the below code ?</p>
<pre><code>import requests
from requests_html import HTMLSession
import pandas as pd

with HTMLSession() as s: 
    r = s.get('https://www.cocorahs.org/ViewData/StationPrecipSummary.aspx')

hiddens = r.html.find('input[name=__VIEWSTATE]', first=True).attrs.get('value')

payload = {
    '__EVENTTARGET': '',
    '_VIEWSTATE': hiddens,
    'obsSwitcher:ddlObsUnits': 'usunits',
    'tbStation1': 'MD-BL-13',
    'ucDateRangeFilter:dcStartDate': '8/1/2019',
    'ucDateRangeFilter_dcStartDate_p': '2019-8-1-0-0-0-0',
    'ucDateRangeFilter:dcEndDate': '8/10/2019',
    'ucDateRangeFilter_dcEndDate_p': '2019-8-10-0-0-0-0',
    'btnSubmit': 'Get Summary'
    }

r = s.post('https://www.cocorahs.org/ViewData/StationPrecipSummary.aspx', data=payload)
table = r.html.find('table.Grid', first=True)
df = pd.read_html(table.html, header=0)[0]
print(df)
</code></pre>
","0","Question"
"79665487","","<p>SSLCertVerificationError                  Traceback (most recent call last)
File /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1344, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)
1343 try:
-&gt; 1344     h.request(req.get_method(), req.selector, req.data, headers,
1345               encode_chunked=req.has_header('Transfer-encoding'))
1346 except OSError as err: # timeout error</p>
<p>File /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1336, in HTTPConnection.request(self, method, url, body, headers, encode_chunked)
1335 &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;
-&gt; 1336 self._send_request(method, url, body, headers, encode_chunked)</p>
<p>File /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1382, in HTTPConnection._send_request(self, method, url, body, headers, encode_chunked)
1381     body = _encode(body, 'body')
-&gt; 1382 self.endheaders(body, encode_chunked=encode_chunked)</p>
<p>File /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1331, in HTTPConnection.endheaders(self, message_body, encode_chunked)
1330     raise CannotSendHeader()
-&gt; 1331 self._send_output(message_body, encode_chunked=encode_chunked)</p>
<p>File /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1091, in HTTPConnection._send_output(self, message_body, encode_chunked)
1090 del self._buffer[:]
-&gt; 1091 self.send(msg)
1093 if message_body is not None:
1094
...
-&gt; 1347         raise URLError(err)
1348     r = h.getresponse()
1349 except:</p>
<p>URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)&gt;</p>
","1","Question"
"79665533","","<p>I'm confused by this pandas warning. I'm already using the recommended <code>.loc[,]</code> format, but the warning persists. Can someone explain why this warning is appearing?</p>
<pre><code>df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
sub_df = df.loc[df.A &gt; 1]
sub_df.loc[:, 'C'] = sub_df.A + sub_df.B
</code></pre>
<p>Here is the warning message:</p>
<pre><code>&lt;ipython-input-203-8af37ac9de96&gt;:3: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  sub_df.loc[:, 'C'] = sub_df.A + sub_df.B
</code></pre>
","0","Question"
"79665790","","<h1>Context</h1>
<p>I have a Pandas DataFrame I receive from a transform stage in an ETL process, and I am trying to load it to a Postgres database. This has been fine for most of the tables, but the final assignment table is causing me problems.</p>
<p>The <code>player_match_event_df</code> DataFrame looks like this:</p>
<pre><code>    match_event_id  player_id  related_player_id
0        150573493   37623454                NaN
1        150573495     787556                NaN
2        150573550   37736097                NaN
3        150573587     978873         37677332.0
4        150573592     323669         37757705.0
5        150573591     191273           788849.0
6        150573605      73555           787556.0
7        150573625     191273            32113.0
8        150573634   29763093            35095.0
9        150573635     320908           320430.0
10       150573633     320715         37493565.0
11       150573672   37677530         12844068.0
12       150573646        820         37425064.0
13       150573648   12447486            73555.0
14       150573681   37677330         29723379.0
15       150573689     220042                NaN
</code></pre>
<p><code>player_match_event_df.info()</code>:</p>
<pre><code>Data columns (total 3 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   match_event_id     16 non-null     int64  
 1   player_id          16 non-null     int64  
 2   related_player_id  12 non-null     float64
dtypes: float64(1), int64(2)
</code></pre>
<p>The <code>schema.sql</code> defines the table player_match_event having the following columns:
player_match_event_id, match_event_id, player_id, related_player_id. Snippet:</p>
<pre><code>CREATE TABLE player_match_event (
    player_match_event_id INT GENERATED ALWAYS AS IDENTITY,
    player_id INT NOT NULL,
    related_player_id BIGINT,
    match_event_id INT NOT NULL,
    PRIMARY KEY (player_match_event_id),
    FOREIGN KEY (player_id) REFERENCES player(player_id),
    FOREIGN KEY (related_player_id) REFERENCES player(player_id),
    FOREIGN KEY (match_event_id) REFERENCES match_event(match_event_id)
);
</code></pre>
<h1>Problem</h1>
<p>The related_player_id column is being problematic, and whenever I try to insert into the table I get the following error: <code>Error inserting data: bigint out of range</code>. As you can see from the <code>player_match_event_df</code>, none of the numerical values are out of range. I think the issue must be with the fact it is of type float and contains <code>NaN</code> values.</p>
<h1>Expected result</h1>
<p>When loading to player_match_event, I insert the relevant player_id and match_event_id always, and related_player_id is happy with a null or empty value when there wasn't a relevant_player_id associated with this event.</p>
<h2>Fixes tried</h2>
<p>So I've tried this:</p>
<pre><code>player_match_event_df[&quot;related_player_id&quot;] = player_match_event_df[&quot;related_player_id&quot;].apply(
        lambda x: int(x) if pd.notna(x) else None
    )
</code></pre>
<p>In the hope to force all values to be an integer, and na values to be <code>None</code>. This doesn't work though as I think None can't exist in an int dtype column, so nothing actually changes in the dataframe when I run that.</p>
<p>I found <a href=""https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int"">another Stack Overflow thread</a> which suggested using</p>
<pre><code>    player_match_event_df[&quot;related_player_id&quot;] = player_match_event_df[&quot;related_player_id&quot;].astype(
        float).astype('Int64')
</code></pre>
<p>Running this does change how the DataFrame looks:</p>
<pre><code>    match_event_id  player_id  related_player_id
0        150573493   37623454               &lt;NA&gt;
1        150573495     787556               &lt;NA&gt;
2        150573550   37736097               &lt;NA&gt;
3        150573587     978873           37677332
4        150573592     323669           37757705
5        150573591     191273             788849
6        150573605      73555             787556
7        150573625     191273              32113
8        150573634   29763093              35095
9        150573635     320908             320430
10       150573633     320715           37493565
11       150573672   37677530           12844068
12       150573646        820           37425064
13       150573648   12447486              73555
14       150573681   37677330           29723379
15       150573689     220042               &lt;NA&gt;
</code></pre>
<p>But when I try to load this to the database, using <code>execute_values</code> I get columns and values this way:</p>
<pre><code>columns = &quot;, &quot;.join(df.columns)
values = df.to_records(index=False).tolist()
</code></pre>
<p>And when I print values, this is the output:</p>
<pre><code>[(150573493, 37623454, nan), (150573495, 787556, nan), (150573550, 37736097, nan), (150573587, 978873, 37677332.0), (150573592, 323669, 37757705.0), (150573591, 191273, 788849.0), (150573605, 73555, 787556.0), (150573625, 191273, 32113.0), (150573634, 29763093, 35095.0), (150573635, 320908, 320430.0), (150573633, 320715, 37493565.0), (150573672, 37677530, 12844068.0), (150573646, 820, 37425064.0), (150573648, 12447486, 73555.0), (150573681, 37677330, 29723379.0), (150573689, 220042, nan)]
</code></pre>
<p>Which still shows floats and nan, and then still results in the same error message. What can I try next?</p>
","2","Question"
"79665899","","<p><code>df.dropna(how=&quot;all&quot;, inplace=True)</code> does not remove all full <code>NaN</code> columns/rows that are in my dataframe.</p>
<p>I created a matrix of every distance between the values in a list.</p>
<pre><code>     0        1        2        3        4
0    0.000000,1.003090,7.006241,8.985008,9.012505
1    1.003090,0.000000,6.003151,7.981918,8.009415
2    7.006241,6.003151,0.000000,1.978767,2.006264
3    8.985008,7.981918,1.978767,0.000000,0.027496
</code></pre>
<p>I then filtered this list by values that don't fit within a specific interval and turned the values that don't fit into <code>NaN</code>s.</p>
<pre><code>     0        1        2        3        4
0    Nan     ,Nan     ,7.006241,8.985008,9.012505
1    Nan     ,Nan     ,6.003151,7.981918,8.009415
2    7.006241,6.003151,Nan     ,Nan     ,2.006264
3    8.985008,7.981918,1.978767,Nan     ,Nan
</code></pre>
<p>Additionally I turned the lower half (diagonally) to <code>Nans</code> because it is essentially the same as the upper half (diagonally).</p>
<pre><code>     0        1        2        3        4
0    Nan     ,Nan     ,7.006241,8.985008,9.012505
1    Nan     ,Nan     ,6.003151,7.981918,8.009415
2    Nan     ,Nan     ,Nan     ,Nan     ,2.006264
3    Nan     ,Nan     ,Nan     ,Nan     ,Nan
</code></pre>
<p>Now I am trying to remove all columns and rows that are fully turned into <code>Nans</code> with:</p>
<pre><code>df.dropna(how=&quot;all&quot;, inplace=True)
</code></pre>
<p>but it only removes like 6 out of 100 rows that are full of <code>Nans</code> :,(.</p>
<p>This is my full code:</p>
<pre><code>mz_array = [1,2,3,4,5,6,7] # my list with data, actually 603 long.
distance_matrix = []
for column, i in zip(mz_array, range(len(mz_array))):
    distance_matrix.append([])
    for row in mz_array:
        distance_matrix[i].append(np.abs(row - column))

df_dm = pd.DataFrame(distance_matrix)
df_dm.columns = df_dm.columns.map(str)

for i in range(len(mz_array)):
    for j in range(len(mz_array)):
        if j &lt;= i:
            df_dm.iat[i,j] = None
        if df_dm.iat[i,j] &lt; 55 or df_dm.iat[i,j] &gt; 480:
            df_dm.iat[i,j] = None

df_dm.dropna(how=&quot;all&quot;, inplace=True)
</code></pre>
","0","Question"
"79321985","79321942","<p>You should not use <code>apply</code>+<code>query</code> for this type of operations in pandas, it is inefficient.</p>
<p>The correct approach would be to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a>:</p>
<pre><code>pop = population_2024.set_index('Country')['Population (M)']

new_murders_df['Homicide rate per 100,000'] = (
    new_murders_df['Number of murders']
    .div(new_murders_df['Country'].map(pop))
    .mul(0.1)
)
</code></pre>
<p>Output:</p>
<pre><code>   Country  Year  Number of murders  Homicide rate per 100,000
0  Jamaica  2023               1393                  49.292286
1  Jamaica  2024               1138                  40.268931
2    Japan  2023                912                   0.732530
3    Japan  2024                912                   0.732530
</code></pre>
<p>A fix of your approach (but really, don't) using a f-string:</p>
<pre><code>new_murders_df[&quot;Homicide rate per 100,000&quot;] = new_murders_df.apply(
    lambda row: (row[&quot;Number of murders&quot;] / population_2024.query(f&quot;Country=='{row['Country']}'&quot;)[&quot;Population (M)&quot;].squeeze()) * 100000,
    axis=1
)
</code></pre>
<p>or using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>loc</code></a>:</p>
<pre><code>new_murders_df[&quot;Homicide rate per 100,000&quot;] = new_murders_df.apply(
    lambda row: (row[&quot;Number of murders&quot;] / population_2024.loc[lambda r: r[&quot;Country&quot;]==row['Country']][&quot;Population (M)&quot;]) * 100000,
    axis=1
)
</code></pre>
","3","Answer"
"79322535","79322295","<p>If df is the dataframe you got from pd.read_csv(), you can filter on the strings inside the column 'production_countries' by using str.contains().<br />
Note that &quot;|&quot; means &quot;OR&quot;, that na=False is to ignore the few missing data in this column and case=False to ignore the case(lower/upper) though it seems that case is not necessary for this dataset.</p>
<pre><code>df = df[df['production_countries'].str.contains(&quot;United States|United Kingdom&quot;, na=False, case=False)]
</code></pre>
","0","Answer"
"79323842","79323746","<p>You are getting this error as you are passing negative values into <code>np.log()</code> when you do the below:</p>
<pre><code>np.log(1 - df['ebit']) * -1
</code></pre>
<p>and</p>
<pre><code>np.log(1 - df['sale']) * -1
</code></pre>
<p>I imagine the <code>* -1</code> part was you trying to avoid passing in a negative, however you are doing this outside of the log function, hence the error.  For example, if <code>1 - df['ebit'] = n</code>, your code is first trying to do <code>log(n)</code> then multiply that by <code>-1</code>.  If n is negative (as it often is in your code), this is not possible.</p>
<p>You want to re-write your log calls such that the <code>* -1</code> is inside the log, like:</p>
<pre><code>np.log((1 - df['sale']) * -1)
</code></pre>
<p><strong>Edit thanks to @Quang Hoang</strong></p>
<p>Using:</p>
<pre><code>np.log((1 - df['sale']).abs())
</code></pre>
<p>Is a more robust way of achieving what you're after, as using <code>* -1</code> will still cause issues with negative values if there is a value in <code>df['sale']</code> that is less than 1. Using <code>.abs()</code> takes the absolute value of a column, so the value regardless of sign, which will avoid any negative values being passed into <code>np.log()</code></p>
","1","Answer"
"79324412","79323746","<p>The problem is in this block of code:</p>
<pre><code>    choices_ebit = [
        np.log(1 + df['ebit']),
        np.log(1 - df['ebit']) * -1
    ]
</code></pre>
<p>Here, you are calculating both formulas, for when ebit is positive and when it's negative, and storing them in <code>choices_ebit</code>. However, when ebit&gt;=1, the second one will give you the runtime warning, and when ebit&lt;=-1, the first one will give your the runtime warning.</p>
<p>In order to avoid calculating both formulas, you can factor them out into one with <code>abs()</code> on the one hand, and <code>np.sign()</code> on the other:</p>
<pre><code>    df['lnebit'] = np.log(1 + df['ebit'].abs()) * np.sign(df['ebit'])
</code></pre>
<p>This meets your requirements:</p>
<ul>
<li>when ebit&gt;=0, sign(ebit) == 1 and abs(ebit) == ebit, so that resolves to log(1+ebit)</li>
<li>when ebit&lt;=, sign(ebit) == -1 and abs(ebit) == -ebit, so that resolves to -log(1-ebit)</li>
</ul>
","2","Answer"
"79325924","79325079","<p>One approach is to skip the rows and define the columns</p>
<pre><code>column_names = ['Column1', 'Column2', 'Column3']
df = pd.read_csv(file_path, skiprows=18, names=column_names)
</code></pre>
","1","Answer"
"79327414","79325094","<blockquote>
<p>I want to remove all the cafe's plots that are far away from Transjakarta's line. I just want to show the cafe that intersects with Transjakarta's line because the radius range is only 200m from Transjakarta's line. How can I do that?</p>
</blockquote>
<p>You just need to intersect your cafes' points with a 200-meter buffer around your transit lines.</p>
<pre class=""lang-py prettyprint-override""><code>import osmnx as ox

# get the transit lines near some point
point = (34.05, -118.25)
tags = {&quot;railway&quot;: [&quot;light_rail&quot;, &quot;subway&quot;]}
gdf_rail = ox.features.features_from_point(point, tags, dist=1500)
bbox = gdf_rail.union_all().envelope
gdf_rail = ox.projection.project_gdf(gdf_rail)

# get the cafes within 200 meters of the transit lines
tags = {&quot;amenity&quot;: &quot;cafe&quot;}
gdf_poi = ox.features.features_from_polygon(bbox, tags).to_crs(gdf_rail.crs)
gdf_poi = gdf_poi[gdf_poi.intersects(gdf_rail.union_all().buffer(200))]
gdf_poi.shape

# plot the transit lines and nearby cafes
ax = gdf_rail.plot()
ax = gdf_poi.plot(ax=ax)

</code></pre>
<blockquote>
<p>How can I show the legend title?</p>
</blockquote>
<p>Previously answered at <a href=""https://stackoverflow.com/q/44620013/7321942"">Title for matplotlib legend</a></p>
","1","Answer"
"79325578","79325561","<p>You need to edit this line:</p>
<pre><code>df.loc[df.index[0], 'age'] = df.loc[df.index[-1], 'age']
</code></pre>
<p>Complete code:</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'name': ['tom', 'jon', 'sam', 'jane', 'bob'],
               'age': [np.nan, 25, 18, 26, 17],
               'sex': ['male', 'male', 'male', 'female', 'male']})

df.loc[df.index[0], 'age'] = df.loc[df.index[-1], 'age']
</code></pre>
","2","Answer"
"79325798","79325219","<p>Use the API endpoint to get the data on upcoming events.</p>
<p>Here's how:</p>
<pre class=""lang-py prettyprint-override""><code>import requests
from tabulate import tabulate
import pandas as pd

url = 'https://search.raceroster.com/search?q=5k&amp;t=upcoming'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
}

events = requests.get(url,headers=headers).json()['data']

loc_keys = [&quot;address&quot;, &quot;city&quot;, &quot;country&quot;]

table = [
    [
        event[&quot;name&quot;],
        event[&quot;url&quot;],
        &quot; &quot;.join([event[&quot;location&quot;][key] for key in loc_keys if key in event[&quot;location&quot;]])
    ] for event in events
]

columns = [&quot;Name&quot;, &quot;URL&quot;, &quot;Location&quot;]
print(tabulate(table, headers=columns))

df = pd.DataFrame(table, columns=columns)
df.to_csv('5k_events.csv', index=False, header=True)
</code></pre>
<p>This should print:</p>
<pre><code>Name                                         URL                                                                                         Location
-------------------------------------------  ------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------
Credit Union Cherry Blossom                  https://raceroster.com/events/2025/72646/credit-union-cherry-blossom                        Washington, D.C. Washington United States
Big Cork Wine Run 5k                         https://raceroster.com/events/2025/98998/big-cork-wine-run-5k                               Big Cork Vineyards, 4236 Main Street, Rohrersville, MD 21779, U.S. Rohrersville United States
3rd Annual #OptOutside Black Friday Fun Run  https://raceroster.com/events/2025/98146/3rd-annual-number-optoutside-black-friday-fun-run  Grain H2O, Summit Harbour Place, Bear, DE, USA Bear United States
Ryan's Race 5K walk Run                      https://raceroster.com/events/2025/97852/ryans-race-5k-walk-run                             Odessa High School, Tony Marchio Drive, Townsend, DE Townsend United States
13th Annual Delaware  Tech Chocolate Run 5k  https://raceroster.com/events/2025/98542/13th-annual-delaware-tech-chocolate-run-5k         Delaware Technical Community College - Charles L. Terry Jr. Campus - Dover, Campus Drive, Dover, DE, USA Dover United States
Builders Dash 5k                             https://raceroster.com/events/2025/99146/builders-dash-5k                                   Rail Haus - Beer Garden, North West Street, Dover, DE Dover United States
The Ivy Scholarship 5k                       https://raceroster.com/events/2025/96874/the-ivy-scholarship-5k                             Hare Pavilion, River Place, Wilmington, DE Wilmington United States
39th Firecracker 5k Run Walk                 https://raceroster.com/events/2025/96907/39th-firecracker-5k-run-walk                       Rockford Tower, Lookout Drive, Wilmington, DE Wilmington United States
24th Annual John D Kelly Logan House 5k      https://raceroster.com/events/2025/97364/24th-annual-john-d-kelly-logan-house-5k            Kelly's Logan House, Delaware Avenue, Wilmington, DE, USA Wilmington United States
2nd Annual Scott Trot 5K                     https://raceroster.com/events/2025/96904/2nd-annual-scott-trot-5k                           American Legion Post 17, American Legion Road, Lewes, DE Lewes United States
</code></pre>
<p>Bonus:</p>
<blockquote class=""spoiler"">
<p> To get more events data, just paginate the API with these parameters: <code>l=10&amp;p=1</code>. For example, <a href=""https://search.raceroster.com/search?q=5k&amp;l=10&amp;p=1&amp;t=upcoming"" rel=""nofollow noreferrer"">https://search.raceroster.com/search?q=5k&amp;l=10&amp;p=1&amp;t=upcoming</a> Also, note there's a field in <code>meta -&gt; hits</code> that holds the number of found events. For your query that's <code>1465</code>.</p>
</blockquote>
","3","Answer"
"79325957","79325219","<p>I've added the raw urls and title in my <a href=""https://github.com/nikitimi/selenium-scraper/tree/issue/eventContacts"" rel=""nofollow noreferrer"">GitHub repository</a>, just change the <code>user_name</code> in <code>save_to_document.py</code> line 21.</p>
<p>Then run it to start the process in creating spreadsheet with the outcomes you've stated.</p>
<p>highly recommend to initialize virtual environment (venv).</p>
<p>Output looks like this:</p>
<p><a href=""https://i.sstatic.net/xFmGAT0i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFmGAT0i.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79326187","79326157","<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>df.reindex</code></a>, single out column <code>'Branch'</code> and apply <a href=""https://docs.python.org/3/library/functions.html#sorted"" rel=""nofollow noreferrer""><code>sorted</code></a> to the remainder, <code>df.columns[1:]</code>:</p>
<pre class=""lang-py prettyprint-override""><code>out = df.reindex(['Branch'] + sorted(df.columns[1:]), axis=1)

out.equals(df2)
# True
</code></pre>
<p>Or directly:</p>
<pre class=""lang-py prettyprint-override""><code>out2 = df[['Branch'] + sorted(df.columns[1:])]

out2.equals(df2)
# True
</code></pre>
","4","Answer"
"79326454","79325633","<p>The statsmodels documentation has a dedicated example about using statsmodels OLS with dummy variables. You can check it <a href=""https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html#OLS-estimation"" rel=""nofollow noreferrer"">here</a>.</p>
<p>They suggest to input not a dataframe, but a numpy array, and they even have a code example, explaining how to produce input. This code is from their example:</p>
<pre><code>nsample = 50
groups = np.zeros(nsample, int)
groups[20:40] = 1
groups[40:] = 2

dummy = pd.get_dummies(groups).values
x = np.linspace(0, 20, nsample)
# drop reference category
X = np.column_stack((x, dummy[:, 1:]))
X = sm.add_constant(X, prepend=False)

res2 = sm.OLS(y, X).fit()
</code></pre>
","1","Answer"
"79327394","79327355","<p>You can use Python's formatted string feature <code>(f&quot;{x:0{n_bits}b}&quot;)</code> to convert numbers to their binary equivalent, ensuring a fixed bit-width.</p>
<pre><code>import pandas as pd
import numpy as np

s = pd.Series([1, 2, 3, 4, 5])

# Find the largest value to determine the number of bits required
max_value = s.max()
n_bits = int(np.ceil(np.log2(max_value + 1)))

binary_df = pd.DataFrame(
    s.apply(lambda x: list(map(int, f&quot;{x:0{n_bits}b}&quot;))).tolist(),
    columns=[f&quot;bit_{i}&quot; for i in range(n_bits)]
)

print(binary_df)
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>   bit_0  bit_1  bit_2
0      0      0      1
1      0      1      0
2      0      1      1
3      1      0      0
4      1      0      1
</code></pre>
","2","Answer"
"79329234","79326157","<p>One option is to set your index as <code>Branch</code>, then <code>sort_index()</code> on <code>axis=1</code></p>
<pre><code>df.set_index('Branch').sort_index(axis=1).reset_index()
</code></pre>
","0","Answer"
"79333886","79326029","<blockquote>
<p>the existing code reads the feather file for this entity into memory</p>
</blockquote>
<p>You are using the wrong tool for the job.
You explain that you have a lot of data
and that you operate within a memory constraint.
Clearly the problem is feasible,
but your existing tooling has failed to
properly address the RAM constraint, which is
one of the requirements.</p>
<p>Outsource such details to an RDBMS like sqlite or postgres.
Doing JOINs is their bread and butter.
Hang onto &quot;intermediate&quot; rows locally, and only serialize
them out to S3 or a feather file once post-processing
such as merging is complete.
The pandas ecosystem has rich support for DB I/O.</p>
","1","Answer"
"79327447","79327355","<p>If your values are less than 255, you could <a href=""https://numpy.org/doc/stable/reference/generated/numpy.unpackbits.html"" rel=""nofollow noreferrer""><code>unpackbits</code></a>:</p>
<pre><code>s = pd.Series([1, 2, 3, 4, 5])

N = int(np.log2(s.max()))
powers = 2**np.arange(N, -1, -1)
out = pd.DataFrame(np.unpackbits(s.to_numpy(np.uint8)[:, None], axis=1)[:, -N-1:],
                   index=s.index, columns=powers)
</code></pre>
<p>If your have larger numbers, compute a mask with <code>&amp;</code> and an array of powers of 2:</p>
<pre><code>s = pd.Series([1, 2, 3, 4, 5])

powers = 2**np.arange(int(np.log2(s.max())), -1, -1)

out = pd.DataFrame((s.to_numpy()[:, None] &amp; powers).astype(bool).astype(int),
                   index=s.index, columns=powers)
</code></pre>
<p>Output:</p>
<pre><code>   4  2  1
0  0  0  1
1  0  1  0
2  0  1  1
3  1  0  0
4  1  0  1
</code></pre>
","1","Answer"
"79327609","79327355","<p>One solution is to use bin() function to get the binary and zfill() to pad the numbers with 0 on the left. Finally use to_list() to transform the list of digits into new columns:</p>
<pre><code>s = pd.Series(list(range(1, 6)), name=&quot;nb&quot;)
df = pd.DataFrame(s)

# Get the binary format (like 0b11)
df[&quot;bin&quot;] = df['nb'].map(bin)  

# max number of characters for largest number
nmax= max(df[&quot;bin&quot;].map(len)) - 2  

# Prefill each number with 0 to get nmax length and get the list of digits
df[&quot;bin2&quot;] = df['bin'].map(lambda x: list(f&quot;{x[2:].zfill(int(nmax))}&quot;))

# Use to_list() to &quot;explode&quot; the lists into new columns
res = pd.DataFrame(df[&quot;bin2&quot;].to_list(), index=df[&quot;nb&quot;])
display(res)
</code></pre>
<p><a href=""https://i.sstatic.net/Fyd5uhTV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fyd5uhTV.png"" alt=""res"" /></a></p>
","1","Answer"
"79327818","79327761","<p>You may be better off using something like the attached.  The trick is the recursive call to <code>process_msg()</code> to handle emails that are attachment.</p>
<pre><code>import email

def process_msg(mail):
    from = email.utils.parseaddr(mail[&quot;From&quot;])[1].lower()
    if (mail.is_multipart()):
       msgs =  mail.get_payload()
       for part in msgs:
           content_type = msg.get_content_type()
           content = msg.get_payload()
           if content_type == &quot;text/plain&quot;:
              # Do something
           elif content_type == &quot;message/rfc822&quot;:
              process_msg(content);
           elif # more cases here

mail = email.message_from_file(filename)
process_msg(mail)
</code></pre>
","0","Answer"
"79327835","79327563","<p>For code clarity, durability, and maintainability, avoid relying on side-effects but explicitly assign objects. One approach can be to define a method to handle the multiple step operations and call it in a list comprehension:</p>
<pre class=""lang-py prettyprint-override""><code>def proc_frame(df):
    df = df.dropna(how='all', axis=1)
    df['Valid From Converted'] = pd.to_datetime(df['Valid From'], format='%Y%m%d')
    df['to Converted'] = pd.to_datetime(df['to'], format='%Y%m%d')
    return df[df['to Converted'] &gt;= now]

new_df_list = [proc_frame(df) for df in df_list]
</code></pre>
<p>Additionally, consider chaining lines in right hand side methods like <code>assign</code> and <code>query</code> to avoid the <code>df</code> calls.</p>
<pre class=""lang-py prettyprint-override""><code>def proc_frame(df):
    return (
        df.dropna(how='all', axis=1)
          .assign(**{
              'Valid From Converted': lambda x: pd.to_datetime(x['Valid From'], format='%Y%m%d'),
              'to Converted': lambda x: pd.to_datetime(x['to'], format='%Y%m%d')
          })
          .query('`to Converted` &gt;= @now')
    )
new_df_list = [proc_frame(df) for df in df_list]
</code></pre>
","0","Answer"
"79327999","79327904","<p>Your vision is a  pivot table, here is howto do what you want:</p>
<pre><code>df = pd.DataFrame({
'month': ['JAN', 'JAN', 'JAN', 'FEB','FEB','FEB','MAR','MAR', 'MAR'],
'electricty': [12, 15, 10, 20, 20, 20, 20, 20, 20],
'water': [30000, 40000, 35000, 45000, 45000, 45000, 45000, 45000, 20],
'heating': [30000, 40000, 35000, 45000, 20, 20, 20, 20, 20],
</code></pre>
<p>})</p>
<pre><code>pivot_table = df.pivot_table(index=['month'],
                              values=['electricty', 'water','heating' ],
                              aggfunc=np.sum)
print(pivot_table)
</code></pre>
<p>Output:</p>
<pre><code>       electricty  heating   water
month                             
FEB            60    45040  135000
JAN            37   105000  105000
MAR            60       60   90020
</code></pre>
<p>also you can just make a pivot table in excel and import it to pandas</p>
","1","Answer"
"79329967","79329930","<p>you need sort</p>
<pre><code>sample_df = sample_df.set_index(&quot;Date&quot;).sort_index()  # sort

# same with your code
sample_df.index = pd.to_datetime(sample_df.index)

# Basic plot.
fig = plt.figure( figsize = (10,5) )
plt.plot(sample_df.index, sample_df[&quot;Energy_MW&quot;], 'b')
plt.grid(True)
plt.show()
</code></pre>
","1","Answer"
"79330035","79329991","<p>Try this code:</p>
<pre><code>tolerance = 0.01

sorted_vals = sorted(df['val'])

groups = []
current_group = [sorted_vals[0]]

for value in sorted_vals[1:]:
    if value - current_group[-1] &lt;= tolerance:
        current_group.append(value)
    else:
        groups.append(current_group)
        current_group = [value]

groups.append(current_group)

group_counts = pd.DataFrame({
    'Group': groups,
    'Count': [len(group) for group in groups]
})

print(group_counts)
</code></pre>
<p>Output:</p>
<pre><code>                Group  Count
0        [5.01, 5.01]      2
1              [5.08]      1
2  [5.54, 5.55, 5.56]      3
3               [6.1]      1
4               [6.3]      1
5               [6.7]      1
</code></pre>
","2","Answer"
"79330128","79329991","<p>There are two ways to go about this: grouping the sorted elements (which doesn't use <code>.value_counts()</code>) and binning.</p>
<h2>Grouping sorted elements</h2>
<p>Sort the values, compare each pair (using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.diff.html"" rel=""nofollow noreferrer""><code>.diff()</code></a>), then assign group numbers (using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>.cumsum()</code></a>).</p>
<p>Then you can <code>.groupby()</code> and aggregate, getting the unique elements of each group and their size of course.</p>
<pre><code>tolerance = 0.01
vals_sorted = df['val'].sort_values()
group_numbers = (
    vals_sorted
    .diff()
    .gt(tolerance)
    .cumsum()
    .rename('group_number')
)

vals_sorted.groupby(group_numbers).agg(['unique', 'size'])
</code></pre>
<pre><code>                          unique  size
group_number                          
0                         [5.01]     2
1                         [5.08]     1
2             [5.54, 5.55, 5.56]     3
3                          [6.1]     1
4                          [6.3]     1
5                          [6.7]     1
</code></pre>
<h2>Binning</h2>
<p>Create equally-sized bins and pass them to <code>.value_counts()</code>. This is a shortcut for <a href=""https://pandas.pydata.org/docs/reference/api/pandas.cut.html"" rel=""nofollow noreferrer""><code>pd.cut()</code></a>.</p>
<p>Lastly, since this is a categorical value count, zeroes are included, so filter them out.</p>
<p>I also sorted the result by the index so it's easier to compare it against the first solution.</p>
<pre><code>import numpy as np

tolerance = 0.01
start = df['val'].min()
stop = df['val'].max()
step = 3 * tolerance
bins = np.arange(start, stop+step, step)

df['val'].value_counts(bins=bins)[lambda s: s &gt; 0].sort_index()
</code></pre>
<pre><code>val
(5.0089999999999995, 5.04]    2
(5.07, 5.1]                   1
(5.52, 5.55]                  2
(5.55, 5.58]                  1
(6.09, 6.12]                  1
(6.27, 6.3]                   1
(6.69, 6.72]                  1
Name: count, dtype: int64
</code></pre>
<p>The result isn't quite what you want, but it's close. Maybe you'd want to adjust the start value, e.g. <code>start = df['val'].min() - 2*tolerance</code>.</p>
","1","Answer"
"79330314","79330288","<p><strong>Example</strong></p>
<p>The reason we need an example is to make your question reproducible. Please provide a complete sample with no omissions. This time I made one.</p>
<pre><code>import pandas as pd
import numpy as np

np.random.seed(42)
date = pd.date_range(start=&quot;2023-01-01&quot;, end=&quot;2023-01-31 23:00:00&quot;, freq=&quot;h&quot;)
values = np.random.randint(0, 100, len(date))
s = pd.Series(values, index=date)
</code></pre>
<p>s</p>
<pre><code>2023-01-01 00:00:00    51
2023-01-01 01:00:00    92
2023-01-01 02:00:00    14
                       ..
2023-01-31 21:00:00    15
2023-01-31 22:00:00    86
2023-01-31 23:00:00    56
Freq: h, Length: 744, dtype: int64
</code></pre>
<p><strong>Answer</strong></p>
<p>use <code>groupby</code> + <code>rolling</code></p>
<pre><code>grp = [s.index.weekday, s.index.hour]

out = (
    s.groupby(grp).rolling(4).mean()
     .droplevel([0, 1]).sort_index()
     .groupby(grp).shift()
)
</code></pre>
<p>out:</p>
<pre><code>2023-01-01 00:00:00      NaN
2023-01-01 01:00:00      NaN
2023-01-01 02:00:00      NaN
                       ...  
2023-01-31 21:00:00    35.75
2023-01-31 22:00:00    45.00
2023-01-31 23:00:00    34.25
Length: 744, dtype: float64
</code></pre>
","1","Answer"
"79330356","79330288","<p>The correct approach is indeed to use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html"" rel=""nofollow noreferrer""><code>rolling</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html"" rel=""nofollow noreferrer""><code>shift</code></a>, like done by Kim.</p>
<p>However, I would rather use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a> here as a wrapper, which seamlessly allows to use a period as window and will maintain the index directly. Also it should be more efficient as one only needs to group once:</p>
<pre><code>out = (s.groupby([s.index.weekday, s.index.hour])
        .transform(lambda x: x.rolling('28D').mean().shift())
      )
</code></pre>
<p>If you do have 0 for missing values and want to ignore them, remove the rows prior to processing:</p>
<pre><code>s = s.loc[lambda x: x!=0]
out = (s.groupby([s.index.weekday, s.index.hour])
        .transform(lambda x: x.rolling('28D').mean().shift())
      )
</code></pre>
<p>Example output (using Kim's input):</p>
<pre><code>2023-01-01 00:00:00      NaN
2023-01-01 01:00:00      NaN
2023-01-01 02:00:00      NaN
2023-01-01 03:00:00      NaN
2023-01-01 04:00:00      NaN
                       ...  
2023-01-31 19:00:00    83.75
2023-01-31 20:00:00    54.00
2023-01-31 21:00:00    35.75
2023-01-31 22:00:00    45.00
2023-01-31 23:00:00    34.25
Freq: h, Length: 744, dtype: float64
</code></pre>
","1","Answer"
"79331186","79329759","<p>This is it:</p>
<pre><code>import numpy as np
import pandas as  pd
amb_temp = [np.nan, np.nan, 32, 32]
in_temp = [ 29, 27, 23, 22]
volts = [np.nan, 13, 11, 11]

dict1 = {'ambient_temperature': amb_temp, 'temperature_inside': in_temp, 'volts': volts} 
keys = list(dict1.keys())

for k in keys:
    data_array = np.array(dict1[k])
    print(&quot;1   {}&quot;.format(data_array))
    not_nan = ~np.isnan(data_array)
    indices = np.arange(len(data_array))
    dict1[k] = np.interp(indices, indices[not_nan], data_array[not_nan])
print(dict1)
</code></pre>
","0","Answer"
"79331482","79331430","<p>Pretty interesting, so going down the rabbit hole with your code <a href=""https://github.com/pandas-dev/pandas/blob/0691c5cf90477d3503834d983f69350f250a6ff7/pandas/_testing/asserters.py#L410"" rel=""nofollow noreferrer"">per pandas codebase</a>.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df_pa[&quot;product_id&quot;].dtype.__class__, df[&quot;product_id&quot;].dtype.__class__
(&lt;class 'pandas.core.arrays.string_.StringDtype'&gt;, &lt;class 'pandas.core.dtypes.dtypes.ArrowDtype'&gt;)
</code></pre>
<p>Then if just casually use common pandas functions (which are part of <code>pandas</code> forever):</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df_pa[&quot;product_id&quot;].astype('string').dtype.__class__, df[&quot;product_id&quot;].astype('string').dtype.__class__
(&lt;class 'pandas.core.arrays.string_.StringDtype'&gt;, &lt;class 'pandas.core.arrays.string_.StringDtype'&gt;)
</code></pre>
<p>Which makes me think <code>df_pa</code> version is more appropriate. Then going through <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html"" rel=""nofollow noreferrer"">pandas.DataFrame.convert_dtypes</a> for pyarrow is says:</p>
<blockquote>
<p>&quot;pyarrow&quot;: returns pyarrow-backed nullable ArrowDtype DataFrame.</p>
</blockquote>
<p>Then following <a href=""https://pandas.pydata.org/docs/reference/api/pandas.ArrowDtype.html#pandas.ArrowDtype"" rel=""nofollow noreferrer"">ArrowDtype</a> ('experimental' is key there), which just confirms my earlier suspicion - likely function <code>convert_dtypes</code> is the culprit of this debacle, also the one producing &quot;less appropriate&quot; results.</p>
","2","Answer"
"79331681","79331481","<p>You can create a dataframe (with one column) having the datetime as index by using the method from_dict() with orient='index'. Then you can use the plot() method from Pandas for a quick drawing of the data divided in 2 parts:</p>
<pre><code>df = pd.DataFrame.from_dict({&quot;1578286800000&quot;:71,&quot;1578373200000&quot;:72,&quot;1578459600000&quot;:72,&quot;1578546000000&quot;:74,
                             &quot;1578632400000&quot;:7,&quot;1578891600000&quot;:7,&quot;1578978000000&quot;:6,&quot;1579064400000&quot;:7,&quot;1579150800000&quot;:6},
                            orient='index')
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7,4))
df[:5].plot(ax=ax1)
df[5:].plot(ax=ax2)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/XIMvZ8bc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XIMvZ8bc.png"" alt=""plot"" /></a></p>
","1","Answer"
"79332490","79329991","<p>try this:</p>
<pre><code>mask = df['val'].sort_values().diff().gt(0.01)
result_df = df.groupby(mask.cumsum())['val'].agg([set, 'count'])
print(result_df)
</code></pre>
<pre><code>                    set  count
val                           
0                {5.01}      2
1                {5.08}      1
2    {5.55, 5.54, 5.56}      3
3                 {6.1}      1
4                 {6.3}      1
5                 {6.7}      1
</code></pre>
","1","Answer"
"79332950","79331430","<p>I think you should to use <code>pd.ArrowDtype(pa.string())</code> not <code>pd.StringDtype(&quot;pyarrow&quot;)</code>.</p>
","1","Answer"
"79333084","79331481","<ol>
<li>Load the key-value pairs using the json module from the json file.</li>
<li>Create a <code>DataFrame</code> using the key-value pairs, with <code>orient=&quot;index&quot;</code> which means keys will be the index and the values will be the rows.</li>
<li>Now simply create your plot from the dataframe.</li>
</ol>
<pre><code>import pandas as pd
import json
import matplotlib.pyplot as plt

with open('temp.json', 'r') as file:
    data_pairs = json.load(file)

dataframe = pd.DataFrame.from_dict(data_pairs, orient=&quot;index&quot;)

dataframe[:5].plot(legend=False, figsize=(3 , 3))
_ = plt.xticks(rotation=45)
dataframe[5:].plot(legend=False, figsize=(3 , 3))
_ = plt.xticks(rotation=45)
</code></pre>
<h3>Plots</h3>
<p><a href=""https://i.sstatic.net/nOaynUPN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nOaynUPN.png"" alt=""Graph For first 5 pairs"" /></a></p>
<p><a href=""https://i.sstatic.net/lGcsndY9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lGcsndY9.png"" alt=""Graph For first 5 pairs"" /></a></p>
<h3>References:</h3>
<ol>
<li><a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_dict.html#pandas.DataFrame.from_dict"" rel=""nofollow noreferrer"">Orient</a></li>
</ol>
","1","Answer"
"79331931","79331776","<p>Assuming:</p>
<pre><code>df = pd.DataFrame({
    'A': [1, None, None, -1, 1, None, None, None],
    'B': [-1, 1, None, 1, None, 1, None, 1],
    'C': [None, -1, 1, None, -1, None, 1, -2],
    'D': [None, None, -1, None, None, -1, None, 1],
    'E': [None, None, None, None, None, None, -1, None]
}, index=['Key 1', 'Key 2', 'Key 3', 'Key 4', 'Key 5', 'Key 6', 'Key 7', 'Key 8'])

final = pd.Series([None, 1, -1, None, None], index=['A', 'B', 'C', 'D', 'E'])
</code></pre>
<p>You could use <a href=""https://docs.python.org/3/library/itertools.html"" rel=""nofollow noreferrer""><code>itertools</code></a> to produce the combinations, and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>reindex</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html"" rel=""nofollow noreferrer""><code>sum</code></a> to compare to the expected output:</p>
<pre><code># helper function to produce the combinations
from itertools import combinations, chain
def powerset(iterable, max_set=None):
    s = list(iterable)
    if max_set is None:
        max_set = len(s)
    return chain.from_iterable(combinations(s, r) for r in range(1, max_set+1))

# loop over the combinations 
# reindex and sum
# compare to final with nulls as 0
MAX_N = 3
for c in powerset(df.index, MAX_N):
    if df.reindex(c).sum().eq(final, fill_value=0).all():
        print(c)
</code></pre>
<p>Output:</p>
<pre><code>('Key 2',)
('Key 3', 'Key 8')
('Key 4', 'Key 5')
('Key 1', 'Key 2', 'Key 4')
</code></pre>
<p>Note that this produces Key1/Key2/Key4 as a valid combination since Key1/Key4 cancel themselves and Key2 alone is valid.</p>
<p>To avoid this, you could keep track of the produced combinations and only retain those that are not a superset of already seen valid combinations:</p>
<pre><code>MAX_N = 3
seen = set()
for c in powerset(df.index, MAX_N):
    if df.reindex(c).sum().eq(final, fill_value=0).all():
        f = frozenset(c)
        if not any(f &gt; s for s in seen):
            seen.add(f)
            print(c)
</code></pre>
<p>Output:</p>
<pre><code>('Key 2',)
('Key 3', 'Key 8')
('Key 4', 'Key 5')
</code></pre>
","1","Answer"
"79333090","79333079","<p>You could create a mask for <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html"" rel=""nofollow noreferrer""><code>drop</code></a>:</p>
<pre><code>m = df_web.columns.str.startswith('post_') &amp; (df_web.columns!='post_title')
# array([ True,  True, False, False, False,  True,  True, False, False,
#        False, False,  True, False])

df_web.drop(columns=df_web.columns[m], inplace=True)
</code></pre>
<p>Or for <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a>:</p>
<pre><code>df_web = df_web.loc[:, (df_web.columns=='post_title')
                       | ~df_web.columns.str.startswith('post_')]
</code></pre>
<p>Or using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.filter.html"" rel=""nofollow noreferrer""><code>filter</code></a> with a (nested) negative match:</p>
<pre><code>df_web = df_web.filter(regex='^(?!post_(?!title))')
</code></pre>
<p><a href=""https://regex101.com/r/zSZv4d/1"" rel=""nofollow noreferrer"">regex demo</a></p>
<p>Output columns:</p>
<pre><code>Index(['sku', 'total_sales', 'product_type', 'post_title', 'guid'], dtype='object')
</code></pre>
","2","Answer"
"79334384","79334351","<p>You can convert the keys first.  They seem to be in milliseconds:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import datetime as dt

data = {&quot;1707195600000&quot;:1,&quot;1707282000000&quot;:18,&quot;1707368400000&quot;:1,&quot;1707454800000&quot;:13,&quot;1707714000000&quot;:18,&quot;1707800400000&quot;:12,&quot;1707886800000&quot;:155,&quot;1707973200000&quot;:1}
cvt_data = {dt.datetime.fromtimestamp(int(k) / 1000):v for k, v in data.items()}
df = pd.DataFrame.from_dict(cvt_data, orient='index')
print(df)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>                       0
2024-02-05 21:00:00    1
2024-02-06 21:00:00   18
2024-02-07 21:00:00    1
2024-02-08 21:00:00   13
2024-02-11 21:00:00   18
2024-02-12 21:00:00   12
2024-02-13 21:00:00  155
2024-02-14 21:00:00    1
</code></pre>
<p>The above converts the timestamps to time zone-unaware local time which was Pacific Standard Time(UTC-8) in my case.  Below converts to to time zone-aware datetimes in UTC.  Note the <code>+00:00</code> indicating the zone offset in the output:</p>
<pre><code>cvt_data = {dt.datetime.fromtimestamp(int(k) / 1000, tz=dt.UTC):v for k, v in data.items()}
</code></pre>
<p>It is much faster to use <code>pd.to_datetime</code> (about 6-7x in my testing), and defaults to a tz-unaware UTC conversion for this data.  Use <code>utc=True</code> to make it tz-aware:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame.from_dict(data, orient='index')
df.index = pd.to_datetime(df.index.astype(dtype='int64'), utc=True, unit='ms')
</code></pre>
<p>Output (both UTC versions):</p>
<pre class=""lang-none prettyprint-override""><code>                             0
2024-02-06 05:00:00+00:00    1
2024-02-07 05:00:00+00:00   18
2024-02-08 05:00:00+00:00    1
2024-02-09 05:00:00+00:00   13
2024-02-12 05:00:00+00:00   18
2024-02-13 05:00:00+00:00   12
2024-02-14 05:00:00+00:00  155
2024-02-15 05:00:00+00:00    1
</code></pre>
<pre class=""lang-none prettyprint-override""><code></code></pre>
","1","Answer"
"79334426","79334351","<p>You can use Pandas' <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>to_datetime</code></a> to convert the index. I haven't done any benchmarking, but I would guess that it is likely to be faster when you have a bigger amount of data:</p>
<pre><code>dataframe.index = pd.to_datetime(dataframe.index, unit=&quot;ms&quot;)
# or
dataframe.index = pd.to_datetime(dataframe.index.astype(dtype='int64'))
</code></pre>
<p>Note: &quot;The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated.&quot; - hence the second option, with the index first converted to int.</p>
","0","Answer"
"79335145","79334864","<p>To update all the columns names (from the 1st to last column):</p>
<pre><code>df.columns = ['First Name', 'Last Name', 'Age', 'Gender']
</code></pre>
<p>To update the name of one or more columns:</p>
<pre><code>df = df.rename(columns={1:&quot;Last Name&quot;})
</code></pre>
","0","Answer"
"79335215","79334864","<p>Since you have criteria to determine the likelihood of a column to be a specific header, you could apply tests on each one and select the one with highest probability using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html"" rel=""nofollow noreferrer""><code>sum</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.idxmax.html"" rel=""nofollow noreferrer""><code>idxmax</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a>:</p>
<pre><code>first_names = ['Eve', 'Jane', 'John', 'Lucy', 'Tom']
last_names = ['Black', 'Doe', 'White']

# Age is a numeric column with values between 0-100
col_age = (df.select_dtypes('number')
             .apply(lambda x: x.between(0, 100)).sum().idxmax()
           )

# Gender contains Male/Female/Other
col_gender = df.isin(['Male', 'Female', 'Other']).sum().idxmax()

# First/Last names contain values from list of known first/last names
col_first = df.isin(first_names).sum().idxmax()
col_last = df.isin(last_names).sum().idxmax()

out = df.rename(columns={col_first: 'First Name',
                         col_last: 'Last Name',
                         col_age: 'Age',
                         col_gender: 'Gender',
                        })
</code></pre>
<p>Output:</p>
<pre><code>  First Name  Gender  Age Last Name
0        Tom    Male   25     Smith
1       John    Male   18       Doe
2       Lucy  Female    7     Black
3       Jane  Female   48       Doe
</code></pre>
<p>Intermediates for the Last Name check:</p>
<pre><code># df.isin(last_names)
       0      1      2      3
0  False  False  False  False
1  False  False  False   True
2  False  False  False   True
3  False  False  False   True

# df.isin(last_names).sum()
0    0
1    0
2    0
3    3
dtype: int64

# df.isin(last_names).sum().idxmax()
3
</code></pre>
<h5>generalization</h5>
<p>You could generalize by defining a dictionary of column name: (dtype, function) to automate the tests and pairing with the maximum likelihood column:</p>
<pre><code>cols = {'Fist Name': (None, lambda x: x.isin(first_names)),
        'Last Name': (None, lambda x: x.isin(last_names)),
        'Age': ('number', lambda x: x.between(0, 100)),
        'Gender': (None, lambda x: x.isin(['Male', 'Female', 'Other'])),
        }

out = df.rename(columns={df.pipe(lambda x: x.select_dtypes(dtype) if dtype else x)
                           .apply(f).sum().idxmax(): c
                         for c, (dtype, f) in cols.items()})
</code></pre>
<p>Note that a column will necessarily be found even if there is no match, in case of equal probability the first available column will be used. You might want to add additional tests if the selection criteria could be ambiguous.</p>
","0","Answer"
"79335589","79335580","<p>You should use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a>, not <code>apply</code>:</p>
<pre><code>df['flag'] = df.groupby(df['datecol'].dt.weekday)['val'].transform(is_outlier)
</code></pre>
<p>Alternatively, explicitly return a Series and use <code>group_keys=False</code>:</p>
<pre><code>def is_outlier(x):
    iqr = x.quantile(.75) - x.quantile(.25)
    outlier = (x &lt;= x.quantile(.25) - 1.5*iqr) | (x &gt;= x.quantile(.75) + 1.5*iqr)
    return pd.Series(np.where(outlier, 1, 0), index=x.index)

df['flag'] = (df.groupby(df['datecol'].dt.weekday, group_keys=False)
              ['val'].apply(is_outlier)
              )
</code></pre>
<p><em>Note that with a single condition, <code>np.where</code> should be preferred to <code>np.select</code>.</em></p>
<p>You could also use a vectorial approach with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.quantile.html"" rel=""nofollow noreferrer""><code>groupby.quantile</code></a>:</p>
<pre><code>wd = df['datecol'].dt.weekday
g = df.groupby(wd)['val']
q25 = g.quantile(.25)
q75 = g.quantile(.75)
iqr = wd.map(q75-q25)
df['flag'] = 1 - df['val'].between(wd.map(q25) - 1.5*iqr, wd.map(q75) + 1.5*iqr)
</code></pre>
<p>Output:</p>
<pre><code>       datecol   val  flag
0   2024-01-01  3193     0
1   2024-01-02  1044     0
2   2024-01-03  2963     0
3   2024-01-04  4448     0
4   2024-01-05  1286     0
..         ...   ...   ...
361 2024-12-27  1531     0
362 2024-12-28  4565     0
363 2024-12-29  3396     0
364 2024-12-30  1870     0
365 2024-12-31  3818     0
</code></pre>
","1","Answer"
"79336231","79336210","<p>The issue with your approach is that you rely on the <a href=""https://en.wikipedia.org/wiki/Topographic_prominence"" rel=""nofollow noreferrer"">prominence</a>, which is the local height of the peaks, and not a good fit with your type of data.</p>
<p>From your total dataset, it <em>looks</em> indeed clear to the naked eye that there are high &quot;peaks&quot; relative to the top of the large blue area, but this is no longer obvious once we consider the exact local data:</p>
<p><a href=""https://i.sstatic.net/652t9GTB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/652t9GTB.png"" alt=""zoomed peaks"" /></a></p>
<p><em>NB. the scale of the insets' Y-axis is the same.</em></p>
<p>Also, let's compute the prominence of all peaks (see how the middle peak has a much greater prominence):</p>
<p><a href=""https://i.sstatic.net/nSWRqb6P.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nSWRqb6P.png"" alt=""enter image description here"" /></a></p>
<p>As you can see, there are peaks everywhere and what you would define as a peak in the left inset is actually a relatively small peak compared to peaks that you would not want to detect in the right inset.</p>
<p>What you want is a peak that is higher than the surrounding peaks, and you want to fully ignore the baseline, thus your approach of using a smoothing function to get the local trend is good.</p>
<p>Since your issue seems to be about speed, you can greatly improve it by using the native <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.window.rolling.Rolling.quantile.html"" rel=""nofollow noreferrer""><code>rolling.quantile</code></a> over a custom <code>rolling.apply</code> with <code>np.percentile</code>:</p>
<pre><code>from scipy.signal import find_peaks

percentile_80 = series.rolling(window=61, center=True, min_periods=1).quantile(0.8)
smoothed_series = series.sub(percentile_80).clip(lower=0)

peaks, peak_data = find_peaks(smoothed_series, prominence=np.max(smoothed_series) * 0.1, distance=48)

series.plot()
series.loc[smoothed_series.iloc[peaks].nlargest(7).index].plot(ls='', marker='o')
</code></pre>
<p>This runs in just a few milliseconds compared to more than one second for the custom <code>apply</code>:</p>
<pre><code># series.rolling(window=61, center=True, min_periods=1).apply(lambda x: np.percentile(x, 80))
1.47 s ± 25 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# series.rolling(window=61, center=True, min_periods=1).quantile(0.8)
3.9 ms ± 56.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/kZvM0nob.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kZvM0nob.png"" alt=""improved peak detection"" /></a></p>
<p>I also added a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.clip.html"" rel=""nofollow noreferrer""><code>clip</code></a> step after smoothing to get the following intermediate:</p>
<p><a href=""https://i.sstatic.net/E6h9prZP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E6h9prZP.png"" alt=""smoothed series"" /></a></p>
","5","Answer"
"79337708","79337606","<p>First sorted  the dataframe like you did and then you can group by <code>Employee</code> and fetch the <code>first</code> value in a group ordered by <code>FTE</code>.</p>
<pre><code>df_sorted = df.sort_values(['Employee', 'FTE'], ascending=[True, False])


result = df_sorted.groupby('Employee').agg({
    'Firstname': 'first',
    'Lastname': 'first',
    'Type': 'first',
    'Title': 'first',
    'Department': 'first',  
    'Starting': 'first',
    'Ending': 'first',
    'FTE': 'first',
    'Department': lambda x: ', '.join(x) 
}).reset_index()

result = result.rename(columns={'Department': 'Description'})

# Department is then added back for highest FTE for each employee
result['Department'] = df_sorted.groupby('Employee')['Department'].first().values
</code></pre>
<p>Output</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Employee#</th>
<th>Firstname</th>
<th>Lastname</th>
<th>Type</th>
<th>Title</th>
<th>Description</th>
<th>Starting</th>
<th>Ending</th>
<th>FTE</th>
<th>Department</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>Tom</td>
<td>Jones</td>
<td>CONP</td>
<td>TEACHER</td>
<td>School 2, School 1, School 3</td>
<td>20240826</td>
<td>99999999</td>
<td>0.533</td>
<td>School 2</td>
</tr>
</tbody>
</table></div>
","1","Answer"
"79338119","79337606","<p>No need for a <code>groupby</code> operation, just pick the row with the max FTE (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.idxmax.html"" rel=""nofollow noreferrer""><code>idxmax</code></a>), <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html"" rel=""nofollow noreferrer""><code>drop</code></a> the unwanted columns and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>assign</code></a> the new Description:</p>
<pre><code>out = (df.loc[[df['FTE'].idxmax()]]
         .drop(columns=['EmployeeEntryID', 'ChangeDate'])
         .assign(Description=', '.join(df.sort_values('FTE', ascending=False)['Department']))
       )
</code></pre>
<p>Output:</p>
<pre><code>   Employee# Firstname Lastname  Type    Title Department  Starting    Ending    FTE                   Description
0       1234       Tom    Jones  CONP  TEACHER   School 2  20240826  99999999  0.533  School 2, School 1, School 3
</code></pre>
","-1","Answer"
"79338241","79338219","<p>First remove iterating by <code>iterrows</code>, then is possible simplify a generalize solution by:</p>
<pre><code>cols = ['F','D']

for col in cols:
    s = df[f'{col}_x'].sub(df[f'{col}_y'])
    df[f'{col}_mul'] = s.mul(s.shift())

df['+'.join(cols)] = df.filter(like='mul').sum(axis=1, min_count=1)
print (df)
         C  F_x  D_x  F_y  D_y  F_mul  D_mul   F+D
0    Spark    2    3    1    1    NaN    NaN   NaN
1  PySpark    4    4    2    2    2.0    4.0   6.0
2   Python    3    6    1    2    4.0    8.0  12.0
3   pandas    5    5    2    2    6.0   12.0  18.0
4     Java    4    5    1    1    9.0   12.0  21.0
</code></pre>
<p>Another idea is processing all columns together - advantage is dont need specify columns for processing:</p>
<pre><code>df1 = (df.filter(like='x').rename(columns=lambda x: x.replace('x','mul'))
         .sub(df.filter(like='y').rename(columns=lambda x: x.replace('y','mul'))))

df2 = df1.mul(df1.shift())
df = df.join(df2)

df['+'.join(x.replace('_mul','') for x in df2.columns)] = df2.sum(axis=1, min_count=1)
print (df)
         C  F_x  D_x  F_y  D_y  F_mul  D_mul   F+D
0    Spark    2    3    1    1    NaN    NaN   NaN
1  PySpark    4    4    2    2    2.0    4.0   6.0
2   Python    3    6    1    2    4.0    8.0  12.0
3   pandas    5    5    2    2    6.0   12.0  18.0
4     Java    4    5    1    1    9.0   12.0  21.0
</code></pre>
","2","Answer"
"79338287","79338219","<p>You can remove the iteration trough rows entirely.
Your code doesn't do anything row specific inside the <code>iterrows()</code> loop -- neither <code>index</code> nor <code>row</code> values are used. What happens is, that you just end up recalculating the entire column (F_mul and D_mul) in each iteration.</p>
<p>Example</p>
<pre class=""lang-py prettyprint-override""><code>...

# Multiply with shifted values instead of using iterrows()
df['F_mul'] = df['F_x-F_y'] * df['F_x-F_y'].shift()
df['D_mul'] = df['D_x-D_y'] * df['D_x-D_y'].shift()
...
</code></pre>
","1","Answer"
"79338373","79338219","<p>You should really not use loops or <code>iterrows</code> in pandas.</p>
<p>What you want to do can be achieved vectorially. You should not have the key <strong>C</strong> as column but rather as <strong>index</strong> to take advantage of automatic alignment of the indices:</p>
<pre><code># ensure C is the index
df = df.set_index('C')
df1 = df1.set_index('C')

# compute the difference
sub = df.sub(df1)

# combine df/df1/sub and compute the shifted multiplication
out = (pd.concat([df.add_suffix('_x'),
                  df1.add_suffix('_y'),
                  sub.add_suffix('_sub'),
                  sub.mul(sub.shift()).add_suffix('_mul')
                 ], axis=1)
      )

# compute the total sum
out['+'.join(df)] = out.filter(like='_mul').sum(axis=1, min_count=1)

# reset_index
out = out.reset_index()
</code></pre>
<p>You could even have everything as a single expression (except sub):</p>
<pre><code>sub = df.sub(df1)

out = (pd.concat([df.add_suffix('_x'),
                  df1.add_suffix('_y'),
                  sub.add_suffix('_sub'),
                  sub.mul(sub.shift()).add_suffix('_mul')
                 ], axis=1)
         .assign(**{'+'.join(df): lambda x: x.filter(like='_mul')
                                             .sum(axis=1, min_count=1)})
         .reset_index()
      )
</code></pre>
<p>Output:</p>
<pre><code>         C  F_x  D_x  F_y  D_y  F_sub  D_sub  F_mul  D_mul   F+D
0    Spark    2    3    1    1      1      2    NaN    NaN   NaN
1  PySpark    4    4    2    2      2      2    2.0    4.0   6.0
2   Python    3    6    1    2      2      4    4.0    8.0  12.0
3   pandas    5    5    2    2      3      3    6.0   12.0  18.0
4     Java    4    5    1    1      3      4    9.0   12.0  21.0
</code></pre>
<p>And if you don't need the intermediate <code>_sub</code>/<code>_mul</code> columns, this is even easier:</p>
<pre><code>sub = df.sub(df1)

out = pd.concat([df.add_suffix('_x'),
                 df1.add_suffix('_y'),
                 sub.mul(sub.shift())
                    .sum(axis=1, min_count=1).rename('+'.join(df))
                ], axis=1).reset_index()
</code></pre>
<p>Or:</p>
<pre><code>sub = df.sub(df1)

out = (df.join(df1, lsuffix='_x', rsuffix='_y')
         .assign(**{'+'.join(df): sub.mul(sub.shift())
                                     .sum(axis=1, min_count=1)})
         .reset_index()
      )
</code></pre>
<p>Output:</p>
<pre><code>         C  F_x  D_x  F_y  D_y   F+D
0    Spark    2    3    1    1   NaN
1  PySpark    4    4    2    2   6.0
2   Python    3    6    1    2  12.0
3   pandas    5    5    2    2  18.0
4     Java    4    5    1    1  21.0
</code></pre>
","3","Answer"
"79338894","79336443","<p>To get the table data you need to use the wp-json endpoint combined with the country id. You can retrieve the country id from the website you are already requesting by finding it in the raw text response.</p>
<p>Next you need to request the wp-json endpoint. There you will recieve a json object including the table html.</p>
<pre><code>def request_table(country_id: str):
    url = &quot;https://www.worldgovernmentbonds.com/wp-json/country/v1/main&quot;

    payload = {
    &quot;GLOBALVAR&quot;:
        {
            &quot;JS_VARIABLE&quot;: &quot;jsGlobalVars&quot;,
            &quot;FUNCTION&quot;: &quot;Country&quot;,
            &quot;DOMESTIC&quot;: True,
            &quot;ENDPOINT&quot;: &quot;https://www.worldgovernmentbonds.com/wp-json/country/v1/historical&quot;,
            &quot;DATE_RIF&quot;: &quot;2099-12-31&quot;,
            &quot;OBJ&quot;: None,
            &quot;COUNTRY1&quot;:
                {
                    &quot;SYMBOL&quot;: country_id
                },
            &quot;COUNTRY2&quot;: None,
            &quot;OBJ1&quot;: None,
            &quot;OBJ2&quot;:None
        }
    }
    headers = {
      'accept': '*/*',
      'content-type': 'application/json; charset=UTF-8',
      'origin': 'https://www.worldgovernmentbonds.com',
    }
    response = requests.request(&quot;POST&quot;, url, headers=headers, data=json.dumps(payload))

    data = response.json()
    return data.get(&quot;mainTable&quot;)



def ZCCWord(Date, country):
    # Site URL
    url = &quot;http://www.worldgovernmentbonds.com/country/&quot; + country
    html_content = requests.get(url).text

    # extract country id
    start_index = html_content.find(&quot;\&quot;SYMBOL\&quot;:\&quot;&quot;)
    end_index = html_content[start_index + 10:].find(&quot;\&quot;,&quot;)
    country_id = html_content[start_index + 10:start_index + 10 + end_index]
    # request table
    table_html = request_table(country_id)

    soup = BeautifulSoup(table_html, &quot;lxml&quot;)
    # gdp = soup.find_all(&quot;table&quot;, attrs={&quot;class&quot;: &quot;w3-table w3-white table-padding-custom w3 small font-family-arial table-valign-middle&quot;})
    gdp = soup.find_all(&quot;table&quot;)  # , attrs={&quot;class&quot;: &quot;w3-table money pd44 -f15&quot;})
    table1 = gdp[0]

    # ... Rest of your code ...
</code></pre>
","2","Answer"
"79339764","79339719","<p>You need to make visualization using Gregorian dates - which is standard for plotting libraries.</p>
<p>Use this - <a href=""https://pypi.org/project/hijri-converter/2.1.1/"" rel=""nofollow noreferrer"">https://pypi.org/project/hijri-converter/2.1.1/</a></p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from hijri_converter import Hijri, Gregorian


data = {
    'Date': [
        '1446/05/25', '1446/05/26', '1446/05/26', '1446/05/26', 
        '1446/05/26', '1446/05/26', '1446/05/27', '1446/05/27',
        '1446/05/28', '1446/05/28', '1446/05/29'
    ],
    'Counts': [12, 2, 6, 1, 1, 6, 6, 6, 4, 6, 9]
}


df = pd.DataFrame(data)


def hijri_to_gregorian(date_str):
    year, month, day = map(int, date_str.split('/'))
    hijri_date = Hijri(year, month, day)
    greg_date = hijri_date.to_gregorian()
    return pd.Timestamp(greg_date.year, greg_date.month, greg_date.day)


df['Gregorian_Date'] = df['Date'].apply(hijri_to_gregorian)
df['Hijri_Date'] = df['Date']


grouped_df = df.groupby('Gregorian_Date')['Counts'].sum().reset_index()


plt.figure(figsize=(12, 6))
plt.plot(grouped_df['Gregorian_Date'], grouped_df['Counts'], marker='o')
plt.gcf().autofmt_xdate()  
plt.title('Counts by Date (Gregorian Calendar)')
plt.xlabel('Date')
plt.ylabel('Total Counts')
plt.grid(True)
plt.tight_layout()
plt.show()
</code></pre>
<p>Code reference - <a href=""https://stackoverflow.com/q/58466932/15358800"">Convert Hijri (Islamic Date) to Gregorian</a></p>
<p>The aggrigated data will look like this</p>
<pre><code>Gregorian Date | Total Counts
------------------------------
2024-11-27 (1446/05/25) | 12
2024-11-28 (1446/05/26) | 16
2024-11-29 (1446/05/27) | 12
2024-11-30 (1446/05/28) | 10
2024-12-01 (1446/05/29) | 9
</code></pre>
<p><a href=""https://i.sstatic.net/rUuRXnTk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUuRXnTk.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79340387","79339822","<p>You need to put all your code inside the loop, now u are only generating the variable &quot;dt&quot; inside the loop, so, you end the loop with the final dt,that is the timedelta 108.</p>
<p>I made a litle change that makes u able to set how many 6 hours intervals u want.</p>
<p>Try this:</p>
<pre><code>from datetime import datetime
from datetime import timedelta
import pandas as pd
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import xarray as xr
import numpy as np
import metpy.calc as mpcalc
from metpy.plots import USCOUNTIES
import netCDF4
from netCDF4 import Dataset
from netCDF4 import num2date
from metpy.units import units
from scipy.ndimage import gaussian_filter
import scipy.ndimage as ndimage
from siphon.catalog import TDSCatalog

start_time = datetime(2025, 1, 7, 12, 0, 0)

hours_intervals = 40

for k in range(0,hours_intervals):
        dt = start_time + timedelta(hours= 6*(k+1))

        #dt = datetime(2025,1,4,12)
        best_gfs = TDSCatalog('https://thredds.ucar.edu/thredds/catalog/grib/NCEP/GFS/Global_0p25deg/catalog.xml?dataset=grib/NCEP/GFS/Global_0p25deg/Best')
        best_ds = best_gfs.datasets[0]
        ncss = best_ds.subset()
        query = ncss.query()
        query.accept('netcdf')
        query.lonlat_box(north=75, south=15, east=320, west=185)
        query.time(dt)
        query.variables('Geopotential_height_isobaric', 'Pressure_reduced_to_MSL_msl', 'Precipitation_rate_surface', 'Snow_depth_surface', 'Categorical_Snow_surface','Categorical_Freezing_Rain_surface', 'Categorical_Ice_Pellets_surface')
        
        data = ncss.get_data(query)
        print(list(data.variables))
        
        plev = list(data.variables['isobaric'][:])
        
        lat = data.variables['latitude'][:].squeeze()
        lon = data.variables['longitude'][:].squeeze()
        time1 = data['time']
        vtime = num2date(time1[:].squeeze(), units=time1.units)
        emsl_var = data.variables['Pressure_reduced_to_MSL_msl']
        preciprate = data.variables['Precipitation_rate_surface'][:].squeeze()
        snowdepth = data.variables['Snow_depth_surface'][:].squeeze()
        catsnow = data.variables['Categorical_Snow_surface'][:].squeeze()
        catice = data.variables['Categorical_Freezing_Rain_surface'][:].squeeze()
        catsleet = data.variables['Categorical_Ice_Pellets_surface'][:].squeeze()
        EMSL = units.Quantity(emsl_var[:], emsl_var.units).to('hPa')
        mslp = gaussian_filter(EMSL[0], sigma=3.0)
        hght_1000 = data.variables['Geopotential_height_isobaric'][0, plev.index(100000)]
        hght_500 = data.variables['Geopotential_height_isobaric'][0, plev.index(50000)]
        thickness_1000_500 = gaussian_filter((hght_500 - hght_1000)/10, sigma=3.0)
        lon_2d, lat_2d = np.meshgrid(lon, lat)
        
        precip_inch_hour = preciprate * 141.73228346457
        precip2 = mpcalc.smooth_n_point(precip_inch_hour, 5, 1)
        
        precip_colors = [
           &quot;#bde9bf&quot;,  # 0.01 - 0.02 inches 1
           &quot;#adddb0&quot;,  # 0.02 - 0.03 inches 2
           &quot;#9ed0a0&quot;,  # 0.03 - 0.04 inches 3
           &quot;#8ec491&quot;,  # 0.04 - 0.05 inches 4
           &quot;#7fb882&quot;,  # 0.05 - 0.06 inches 5
           &quot;#70ac74&quot;,  # 0.06 - 0.07 inches 6
           &quot;#60a065&quot;,  # 0.07 - 0.08 inches 7
           &quot;#519457&quot;,  # 0.08 - 0.09 inches 8
           &quot;#418849&quot;,  # 0.09 - 0.10 inches 9
           &quot;#307c3c&quot;,  # 0.10 - 0.12 inches 10
           &quot;#1c712e&quot;,  # 0.12 - 0.14 inches 11
           &quot;#f7f370&quot;,  # 0.14 - 0.16 inches 12
           &quot;#fbdf65&quot;,  # 0.16 - 0.18 inches 13
           &quot;#fecb5a&quot;,  # 0.18 - 0.2 inches 14
           &quot;#ffb650&quot;,  # 0.2 - 0.3 inches 15
           &quot;#ffa146&quot;,  # 0.3 - 0.4 inches 16
           &quot;#ff8b3c&quot;,   # 0.4 - 0.5 inches 17
           &quot;#f94609&quot;,   # 0.5 - 0.6 inches 18
        ]
        
        precip_colormap = mcolors.ListedColormap(precip_colors)
        
        clev_precip =  np.concatenate((np.arange(0.01, 0.1, .01), np.arange(.1, .2, .02), np.arange(.2, .61, .1)))
        norm = mcolors.BoundaryNorm(clev_precip, 18)
        
        datacrs = ccrs.PlateCarree()
        plotcrs = ccrs.LambertConformal(central_latitude=35, central_longitude=-100,standard_parallels=(30, 60))
        bounds = ([-105, -90, 30, 40])
        fig = plt.figure(figsize=(14,12))
        ax = fig.add_subplot(1,1,1, projection=plotcrs)
        ax.set_extent(bounds, crs=ccrs.PlateCarree())
        ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth = 0.75)
        ax.add_feature(cfeature.STATES, linewidth = 1)
        ax.add_feature(USCOUNTIES, edgecolor='grey', linewidth = .5)
        clevs = (np.arange(0, 540, 6),
                 np.array([540]),
                 np.arange(546, 700, 6))
        colors = ('tab:blue', 'b', 'tab:red')
        kw_clabels = {'fontsize': 11, 'inline': True, 'inline_spacing': 5, 'fmt': '%i',
                      'rightside_up': True, 'use_clabeltext': True}
        
        # Plot MSLP
        clevmslp = np.arange(800., 1120., 2)
        cs2 = ax.contour(lon_2d, lat_2d, mslp, clevmslp, colors='k', linewidths=1.25,
                         linestyles='solid', transform=ccrs.PlateCarree())
        
        cf = ax.contourf(lon_2d, lat_2d, precip2, clev_precip, cmap=precip_colormap, norm=norm, extend='max', transform=ccrs.PlateCarree())
        
        ax.set_title('GFS Precip Type, Rate(in/hr), MSLP (hPa), &amp; 1000-500mb Thickness (dam)', loc='left', fontsize=10, weight = 'bold')
        ax.set_title('Valid Time: {}z'.format(vtime), loc = 'right', fontsize=8)

        fig.savefig('path/to/save/image/to.png')
</code></pre>
","0","Answer"
"79342011","79341984","<p>This looks like Brazilian Portuguese, you should install the <code>pt_BR</code> locale on your machine, then run:</p>
<pre><code>import locale
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')
df['Data_converted'] = pd.to_datetime(df['Data'], format='%d %b',
                                      errors='coerce')
</code></pre>
<p>Output:</p>
<pre><code>     Data Data_converted
0  28 JUL     1900-07-28
1  04 AGO     1900-08-04
</code></pre>
<p>And, if you want to force the year:</p>
<pre><code>df['Data_converted'] = pd.to_datetime('2025 ' + df['Data'],
                                      format='%Y %d %b', errors='coerce')
</code></pre>
<p>Output:</p>
<pre><code>     Data Data_converted
0  28 JUL     2025-07-28
1  04 AGO     2025-08-04
</code></pre>
","4","Answer"
"79343504","79343315","<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 4, 5, 6, 7],
    'field_a': list('AABBCCDEFG'),
    'sibling_id': [2, 3, 1, 3, 1, 2, np.nan, np.nan, 7, 6],
    'sibling_field_a': ['B', 'C', 'A', 'C' , 'A', 'B', np.nan, np.nan, 'G', 'F']
})

# fillna with -1 as a placeholder since I know my ids are never negative
df['sibling_id'] = df['sibling_id'].fillna(-1).astype(int)

# Group by id and aggregate values into a list
grouped_df = df.groupby('id')['sibling_id'].agg(list).to_frame()
# Append index values to the lists
grouped_df = grouped_df.apply(lambda row: sorted([row.name] + row['sibling_id']), axis=1).reset_index()
grouped_df[0] = grouped_df[0].astype(str)
# Create the family id
df['family_id'] = df.merge(grouped_df, on=['id'], how='left').groupby(0).ngroup()
# Remove the -1 placeholder for nulls
df['sibling_id'] = df['sibling_id'].replace(-1, pd.NA)

   id field_a sibling_id sibling_field_a  family_id
0   1       A          2               B          2
1   1       A          3               C          2
2   2       B          1               A          2
3   2       B          3               C          2
4   3       C          1               A          2
5   3       C          2               B          2
6   4       D       &lt;NA&gt;             NaN          0
7   5       E       &lt;NA&gt;             NaN          1
8   6       F          7               G          3
9   7       G          6               F          3
</code></pre>
","1","Answer"
"79343618","79343427","<p>One way of doing this it to define a function that will look at each row of <code>dfA</code> as a list and compare with the same thing from <code>dfB</code>:</p>
<pre><code>import pandas as pd

data_A = {'Index': [0, 1, 2], 'Column 1': ['Albuquerque', 'New York', 'Miami'], 'Column 2': ['NM', 'NY', 'FL'], 'Column 3': ['87101', '10009', '33101']}
dfA = pd.DataFrame(data_A).set_index('Index')
data_B = {'Index': [0, 1, 2, 3], 'Column 1': ['NM', 'Atlanta', 'San Francisco', '10009'], 'Column 2': ['Albuquerque', 'GA', 'CA', 'NY'], 'Column 3': ['87101', '30033', '94016', 'New York']}
dfB = pd.DataFrame(data_B).set_index('Index')
print(dfA)
print(dfB)

def match_rows(dfA, dfB):
    in_both = []
    not_in_both = []
    for index_a, row_a in dfA.iterrows():
        match_found = False
        for _, row_b in dfB.iterrows():
            if set(row_a) == set(row_b):
                in_both.append((index_a, row_a.to_list()))
                match_found = True
                break
        if not match_found:
            not_in_both.append((index_a, row_a.to_list()))
    
    for index_b, row_b in dfB.iterrows():
        if not any(set(row_b) == set(row_a) for _, row_a in dfA.iterrows()):
            not_in_both.append((index_b, row_b.to_list()))
    
    return in_both, not_in_both

matches, non_matches = match_rows(dfA, dfB)
matches, non_matches

def format_output(matches, non_matches):
    formatted_matches = [
        f&quot;Matching from dfA, Index {index}: {values}&quot;
        for index, values in matches
    ]
    formatted_non_matches = [
        f&quot;Not Matching from {'dfA' if index in dfA.index else 'dfB'}, Index {index}: {values}&quot;
        for index, values in non_matches
    ]
    return formatted_matches, formatted_non_matches

formatted_matches, formatted_non_matches = format_output(matches, non_matches)
formatted_matches, formatted_non_matches
</code></pre>
<p>I introduced a second function here to format the output in a more understandable way:</p>
<pre><code>([&quot;Matching from dfA, Index 0: ['Albuquerque', 'NM', '87101']&quot;,
  &quot;Matching from dfA, Index 1: ['New York', 'NY', '10009']&quot;],
 [&quot;Not Matching from dfA, Index 2: ['Miami', 'FL', '33101']&quot;,
  &quot;Not Matching from dfA, Index 1: ['Atlanta', 'GA', '30033']&quot;,
  &quot;Not Matching from dfA, Index 2: ['San Francisco', 'CA', '94016']&quot;])
</code></pre>
","1","Answer"
"79343697","79343427","<p>You can also do something like this:</p>
<pre><code>dfB = dfB.reset_index(drop= False) # Only if you need the index 

common_list = []
uncommon_list = []

# Check if a value in column 2 (or any other) of dfB exists anywhere in dfA

i = 0
for item in dfB['Column 2']:
    rowB = dfB.iloc[i]
    
    if dfA.isin([item]).any().any():
        common_list.append(rowB.tolist())
    else:
        uncommon_list.append(rowB.tolist())
    
    i+=1
</code></pre>
<p>Output:</p>
<p><code>print(common_list)</code></p>
<pre><code>[[0, 'NM', 'Albuquerque', 87101], [3, 10009, 'NY', 'New York']]
</code></pre>
<p><code>print(uncommon_list)</code></p>
<pre><code>[[1, 'Atlanta', 'GA', 30033], [2, 'San Francisco', 'CA', 94016]]
</code></pre>
","1","Answer"
"79343789","79343427","<p>One solution without explicit for loop:</p>
<pre><code>resA = dfA[dfA['Column 1'].map(lambda x: x in dfB.values)]

       Column1 Column2  Column3
0  Albuquerque      NM    87101
1     New-York      NY    10009 
</code></pre>
<hr />
<pre><code>inB = dfA['Column 1'].map(lambda x: np.where(dfB == x)[0]) 
inB = [x[0] for x in inB if len(x)&gt;0]
resB = dfB.loc[[x for x in dfB.index if x not in inB]]

         Column1 Column2 Column3
1        Atlanta      GA   30033
2  San-Francisco      CA   94016
</code></pre>
<p>And if you also want here, the cities in dfA which are not in dfB:</p>
<pre><code>resAnotB = dfA.loc[[x for x in dfA.index if x not in resA.index]]
resB2 = pd.concat([resB, resAnotB])

         Column1 Column2 Column3
1        Atlanta      GA   30033
2  San-Francisco      CA   94016
2          Miami      FL   33101
</code></pre>
","1","Answer"
"79343843","79343315","<p>This is a graph problem, it cannot be solved easily with pandas only if you have complex relationships (e.g. half-sibling of half-siblings).</p>
<p>You can use <a href=""https://networkx.org"" rel=""nofollow noreferrer""><code>networkx</code></a>'s <a href=""https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html"" rel=""nofollow noreferrer""><code>connected_components</code></a> after converting the DataFrame to graph:</p>
<pre><code># pip install networkx
import networkx as nx

# convert to graph
G = nx.from_pandas_edgelist(df,
                            source='field_a',
                            target='sibling_field_a')
# ensure NaNs are not a node
G.remove_nodes_from([np.nan])

# compute the groups/families
groups = {n: i for i, c in enumerate(nx.connected_components(G)) for n in c}

# map the groups to the ids
df['family_id'] = df['field_a'].map(groups)
</code></pre>
<p>Output:</p>
<pre><code>   id field_a  sibling_id sibling_field_a  family_id
0   1       A           2               B          0
1   1       A           3               C          0
2   2       B           1               A          0
3   2       B           3               C          0
4   3       C           1               A          0
5   3       C           2               B          0
6   4       D        &lt;NA&gt;             NaN          1
7   5       E        &lt;NA&gt;             NaN          2
8   6       F           7               G          3
9   7       G           6               F          3
</code></pre>
<p>Graph:</p>
<p><a href=""https://i.sstatic.net/eA2PpE1v.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eA2PpE1v.png"" alt=""pandas form families connected components"" /></a></p>
","1","Answer"
"79343883","79343427","<p>A common way to compare such <code>dfs</code> would be to use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>df.merge</code></a> with <code>indicator=True</code>. Problem: <code>dfB</code> is in bad shape. So, let's fix that first.</p>
<pre class=""lang-py prettyprint-override""><code>dfB = (dfB.stack()
       .str.extract(r'(^\d{5}$)|(^[A-Z]{2}$)|(.+)')
       .groupby(level=0)
       .first()
       .astype({0: int})
       .iloc[:, ::-1]
       .set_axis(dfA.columns, axis='columns')
       )

out = dfA.merge(dfB, how='outer', indicator=True)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>        Column 1 Column 2  Column 3      _merge
0    Albuquerque       NM     87101        both # in `dfA` and `dfB`
1        Atlanta       GA     30033  right_only # only in `dfB`
2          Miami       FL     33101   left_only # only in `dfA`
3       New York       NY     10009        both
4  San Francisco       CA     94016  right_only
</code></pre>
<p>I.e., now you can use <code>how='inner'</code> to get <code>dfA</code> present in <code>dfB</code>:</p>
<pre class=""lang-py prettyprint-override""><code>dfA.merge(dfB, how='inner')

      Column 1 Column 2  Column 3
0  Albuquerque       NM     87101
1     New York       NY     10009
</code></pre>
<p>And <code>how='outer'</code> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html"" rel=""nofollow noreferrer""><code>df.query</code></a> to get all rows not in <code>'both'</code> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html"" rel=""nofollow noreferrer""><code>df.drop</code></a> to drop <code>'_merge'</code> afterwards:</p>
<pre class=""lang-py prettyprint-override""><code>(dfA.merge(dfB, how='outer', indicator=True).query('_merge != &quot;both&quot;')
 .drop('_merge', axis=1))

        Column 1 Column 2  Column 3
1        Atlanta       GA     30033
2          Miami       FL     33101
4  San Francisco       CA     94016
</code></pre>
<hr />
<p><strong>Explanation / Intermediates</strong></p>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>df.stack</code></a> to turn <code>dfB</code> into a series and apply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html"" rel=""nofollow noreferrer""><code>Series.str.extract</code></a> to get zip code (5 digits), state (2 capital letters), city (anything else) into different columns. Explanation of the regex pattern <a href=""https://regex101.com/r/I27xpe/1"" rel=""nofollow noreferrer"">here</a>.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>dfB.stack().str.extract(r'(^\d{5}$)|(^[A-Z]{2}$)|(.+)')

                0    1              2
0 Column 1    NaN   NM            NaN
  Column 2    NaN  NaN    Albuquerque
  Column 3  87101  NaN            NaN
1 Column 1    NaN  NaN        Atlanta
  Column 2    NaN   GA            NaN
  Column 3  30033  NaN            NaN
2 Column 1    NaN  NaN  San Francisco
  Column 2    NaN   CA            NaN
  Column 3  94016  NaN            NaN
3 Column 1  10009  NaN            NaN
  Column 2    NaN   NY            NaN
  Column 3    NaN  NaN       New York
</code></pre>
<ul>
<li>Now, use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>df.groupby</code></a> on index level 0 and get <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.first.html"" rel=""nofollow noreferrer""><code>groupby.first</code></a>.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># .groupby(level=0).first()

       0   1              2
0  87101  NM    Albuquerque
1  30033  GA        Atlanta
2  94016  CA  San Francisco
3  10009  NY       New York
</code></pre>
<ul>
<li>Apply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html"" rel=""nofollow noreferrer""><code>df.astype</code></a> to convert the zip codes to <code>int</code>.</li>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>df.iloc</code></a> to fix the order (city, state, zip code) and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_axis.html"" rel=""nofollow noreferrer""><code>df.set_axis</code></a> to align the column names with <code>dfA</code>.</li>
<li>Finally, apply different versions of <code>df.merge</code>.</li>
</ul>
<hr />
<p><strong>Data used</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

dataA = {'Column 1': {0: 'Albuquerque', 1: 'New York', 2: 'Miami'}, 
         'Column 2': {0: 'NM', 1: 'NY', 2: 'FL'}, 
         'Column 3': {0: 87101, 1: 10009, 2: 33101}}
dfA = pd.DataFrame(dataA)

dataB = {'Column 1': {0: 'NM', 1: 'Atlanta', 2: 'San Francisco', 3: '10009'}, 
         'Column 2': {0: 'Albuquerque', 1: 'GA', 2: 'CA', 3: 'NY'}, 
         'Column 3': {0: '87101', 1: '30033', 2: '94016', 3: 'New York'}}
dfB = pd.DataFrame(dataB)
</code></pre>
","1","Answer"
"79343992","79343929","<p>You can give specific aggregations for each column, and just leave out the non-numeric columns.</p>
<pre><code>planets.groupby(['type', 'magnetic_field']).agg(
    mean_radius=('radius_km', 'mean'), max_radius=('radius_km', 'max'), 
    mean_moons=('moons', 'mean'), max_moons=('moons', 'max'), 
    mean_temp=('mean_temp_c', 'mean'), max_temp=('mean_temp_c', 'max')
)
                            mean_radius  max_radius  mean_moons  max_moons  mean_temp  max_temp
type        magnetic_field                                                                     
gas giant   yes                 64071.5       69911        81.5         83     -125.0      -110
ice giant   yes                 24992.0       25362        20.5         27     -197.5      -195
terrestrial no                   4721.0        6052         1.0          2      199.5       464
            yes                  4405.5        6371         0.5          1       91.0       167
</code></pre>
","1","Answer"
"79343993","79343929","<p>Not sure what is the exact expected output, but you can convert the yes/no to booleans, select the desired dtypes before aggregation:</p>
<pre><code>planets = (planets
           .replace({'yes': True, 'no': False})
           .convert_dtypes()
           )

cols = planets.select_dtypes(['number', 'boolean']).columns

P = (planets.groupby(['type', 'magnetic_field'])[cols]
     .agg(['mean', 'max'])
     )
</code></pre>
<p>Output:</p>
<pre><code>                           radius_km        moons     rings        mean_temp_c       magnetic_field       
                                mean    max  mean max  mean    max        mean   max           mean    max
type        magnetic_field                                                                                
gas giant   True             64071.5  69911  81.5  83   1.0   True      -125.0  -110            1.0   True
ice giant   True             24992.0  25362  20.5  27   1.0   True      -197.5  -195            1.0   True
terrestrial False             4721.0   6052   1.0   2   0.0  False       199.5   464            0.0  False
            True              4405.5   6371   0.5   1   0.0  False        91.0   167            1.0   True
</code></pre>
","3","Answer"
"79344026","79343427","<p>You could aggregate each row as a <a href=""https://docs.python.org/3/library/stdtypes.html#frozenset"" rel=""nofollow noreferrer""><code>frozenset</code></a> and use it to create keys to map the indices of dfB.</p>
<p>Prerequisite: let's ensure Index is the index and if the dtypes are mixed, convert everything to string</p>
<pre><code># optional, only if Index is a column
dfA.set_index('Index', inplace=True)
dfB.set_index('Index', inplace=True)

# only if dfA has 87101 (int) and dfB has &quot;87101&quot; (str)
dfA = dfA.astype(str)
dfB = dfB.astype(str)
</code></pre>
<p>Then aggregate, create the keys and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a>:</p>
<pre><code>keyA = dfA.apply(frozenset, axis=1)
keyB = dfB.apply(frozenset, axis=1)

dfA['matching_row'] = keyA.map(pd.Series(dfB.index, index=keyB))
</code></pre>
<p>Output:</p>
<pre><code>          Column 1 Column 2 Column 3  matching_row
Index                                             
0      Albuquerque       NM    87101           0.0
1         New York       NY    10009           3.0
2            Miami       FL    33101           NaN
</code></pre>
<p>If your goal is just to split dfA without identifying the specific rows from dfB, you could just use the above created keys with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.isin.html"" rel=""nofollow noreferrer""><code>isin</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a>:</p>
<pre><code>isin_dfB = dict(list(dfA.groupby(keyA.isin(keyB))))

isin_dfB[True]
#           Column 1 Column 2 Column 3
# Index                               
# 0      Albuquerque       NM    87101
# 1         New York       NY    10009

isin_dfB[False]
#       Column 1 Column 2 Column 3
# Index                           
# 2        Miami       FL    33101
</code></pre>
","1","Answer"
"79344218","79343929","<p>You can select the numeric columns with .select_dtypes('number') [and .select_dtypes('object') for the categorical ones]:</p>
<pre><code>numeric_cols = planets.select_dtypes('number').columns
P = planets.groupby(['type', 'magnetic_field'])[numeric_cols].agg(['mean', 'max'])
display(P)
</code></pre>
<p><a href=""https://i.sstatic.net/m9kxmVDs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/m9kxmVDs.png"" alt=""df"" /></a></p>
","1","Answer"
"79344613","79344594","<p>the second dfa looks very unorganised, let us sort that first</p>
<pre><code>df = df.sort_values(by=['identifier', 'year'])
dfa = dfa.sort_values(by=['identifier', 'achievement_year'])
</code></pre>
<p>looks like there are some dups too, lets try that too..</p>
<pre><code>A   3   2015
A   3   2015
A   4   2019
A   4   2019

df = df.sort_values(by=['identifier', 'year']).drop_duplicates(subset=['identifier', 'year'], keep='last')
</code></pre>
<p>lets go with merge func</p>
<pre><code>merged_df = pd.merge_asof(
    dfa,
    df,
    by='identifier',
    left_on='achievement_year',
    right_on='year',
    direction='backward'
)
</code></pre>
<p>add this to code and run, any error let me know..</p>
","0","Answer"
"79346374","79346256","<ol>
<li>Just create an intermediate timestamp as a merge key.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import random

# let dataset 1 be the weather data
# let dataset 2 be the pv voltage &amp; pics data

# define datasets
END_EPOCH_TIME = 1400
dataset_1_timestamps = np.arange(0, END_EPOCH_TIME, 30)
dataset_2_timestamps = np.arange(0, END_EPOCH_TIME)[np.array([(random.random() &gt; 0.8) for _ in range(END_EPOCH_TIME)])]

df_1 = pd.DataFrame({&quot;timestamp&quot;: dataset_1_timestamps, &quot;weather&quot;: [f&quot;A_{i}&quot; for i in range(len(dataset_1_timestamps))]})
df_2 = pd.DataFrame({&quot;timestamp&quot;: dataset_2_timestamps, &quot;pv_voltage&quot;: [f&quot;B_{i}&quot; for i in range(len(dataset_2_timestamps))]})

# combine them without losing rows
df_2[&quot;current_timestamp&quot;] = df_2[&quot;timestamp&quot;]
df_2[&quot;timestamp&quot;] =  df_2[&quot;timestamp&quot;].apply(lambda t: t - (t % 30))
combined_df = df_2.merge(df_1, on=&quot;timestamp&quot;, how=&quot;right&quot;)

print(df_1.head())
print(df_2.head())
print(combined_df.head())
</code></pre>
<p>Output</p>
<pre><code>   timestamp weather
0          0     A_0
1         30     A_1
2         60     A_2
3         90     A_3
4        120     A_4
   timestamp pv_voltage  current_timestamp
0          0        B_0                  3
1          0        B_1                  6
2          0        B_2                 14
3          0        B_3                 17
4          0        B_4                 18
   timestamp pv_voltage  current_timestamp weather
0          0        B_0                  3     A_0
1          0        B_1                  6     A_0
2          0        B_2                 14     A_0
3          0        B_3                 17     A_0
4          0        B_4                 18     A_0
</code></pre>
<ol start=""2"">
<li>The inconsistent time-window for your image data just means you have to be selective about picking which data is needed for inference. Eg. Is it reasonable for your model to make an educated guess based only on the single latest image? Does it need 10 images instead? How many total training/validation/testing samples would you have if a minimum of 10 is required? Since the time granularity of the images is short, can you interpolate between the images somehow in instances where the data gaps are short? (1-2mins), etc. I'd recommend researching similar problems to what you're trying to solve to get ideas.</li>
</ol>
","0","Answer"
"79346980","79344594","<p>See rule #5 in this <a href=""https://angwalt12.medium.com/the-hidden-rules-of-pandas-merge-asof-e67293a5318e"" rel=""nofollow noreferrer"">link</a>:
<strong>Sort both dataframes according to the column listed for the ‘on’ parameter.</strong></p>
<p>This is also a rather hidden rule given its necessity, especially if you data was sorted initially. Regardless, just like watching out for nonetypes before running a merge ensure you <strong>sort both data frames according to the on column</strong> and all should be well. This is actually a rule that merge_asof shares with merge_ordered() so ensure you sort before that kind of merge also. To find out more about merge_ordered() go here.</p>
<p>I had sorted the frames on <code>'identifier'</code>, and <code>'year'</code></p>
","0","Answer"
"79360531","79346496","<p>This issue occurs because the punkt resource download was incomplete or corrupted. The punkt tokenizer relies on several underlying files, including punkt_tab. When these files are missing, you encounter a LookupError. While downloading punkt should include all necessary files, explicitly downloading punkt_tab resolved your issue by filling in the missing dependency. To fix this, ensure a complete download of punkt using nltk.download('punkt'). If problems persist, clear existing downloads and re-download punkt. This ensures all required files are available for the tokenizer to function correctly.
For me explicity installing punkt_tab resolved the issue for me.</p>
","0","Answer"
"79451539","79335633","<p>The file location was synchronized with OneDrive.</p>
<p>Using another location that is not synchronized solved the issue.</p>
","0","Answer"
"79346705","79346684","<p>What about a log scale on Y ?</p>
<pre><code>plt.yscale('log')
</code></pre>
<p><a href=""https://i.sstatic.net/2fDg4KqM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2fDg4KqM.png"" alt=""plot"" /></a></p>
","0","Answer"
"79346857","79346829","<p>You can use <code>groupby</code> and <code>unique</code></p>
<pre><code># read csv
df = pd.read_csv(&quot;Example_File&quot;)

# Identify duplicates where col2 values differ for the same col1
mismatched = df.groupby('col1').filter(lambda x: len(x['col2'].unique()) &gt; 1)

# Get the unique col1 values with mismatched col2 values
result = mismatched['col1'].unique()
</code></pre>
","0","Answer"
"79346986","79346829","<p>based on this <a href=""https://stackoverflow.com/a/15411596/10452700"">answer</a> <code>.nunique()</code> should be worked within groupby:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# reproduce and generate the example DataFrame
data = {
    &quot;Col1&quot;: [&quot;555-555-5555&quot;, &quot;555-555-5555&quot;, &quot;555-123-4567&quot;, &quot;555-123-4567&quot;, &quot;777-555-5555&quot;],
    &quot;Col2&quot;: [1, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Step 1: Identify groups with inconsistent values in Col2
mismatched _duplicates = df.groupby(&quot;Col1&quot;)[&quot;Col2&quot;].nunique()
mismatched _duplicates = mismatched _duplicates[inconsistent_duplicates &gt; 1].index

# Step 2: Filter the DataFrame for these inconsistent groups
result = df[df[&quot;Col1&quot;].isin(mismatched _duplicates)]

# Display the result
print(result)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>           Col1  Col2
2  555-123-4567     0
3  555-123-4567     1
</code></pre>
<hr />
<p>alternatively, I triedsolve this problem using <code>.value_counts()</code> in conjunction with other operations. Here's how:</p>
<pre class=""lang-py prettyprint-override""><code># Step 1: Create a new column combining Col1 and Col2
df[&quot;Combined&quot;] = df[&quot;Col1&quot;] + &quot;_&quot; + df[&quot;Col2&quot;].astype(str)

# Step 2: Use value_counts() to count occurrences of each unique combination
counts = df[&quot;Combined&quot;].value_counts()

# Step 3: Identify phone numbers with multiple unique Col2 values
mismatched_phones = (
    counts.index.str.split(&quot;_&quot;).str[0]  # Extract Col1 part of the combined values
    .value_counts()[lambda x: x &gt; 1]   # Count occurrences of each phone number and filter duplicates
    .index
)

# Step 4: Filter the original DataFrame for these inconsistent phone numbers
result = df[df[&quot;Col1&quot;].isin(mismatched_phones)]

# Display the result
print(result[[&quot;Col1&quot;, &quot;Col2&quot;]])
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>           Col1  Col2
2  555-123-4567     0
3  555-123-4567     1
</code></pre>
<hr />
<p>you can use <code>duplicated()</code> to solve this problem effectively by checking for rows with the same <code>Col1</code> values and different <code>Col2</code> values as below:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.DataFrame(data)

# Step 1: Find rows with duplicate Col1 values but different Col2 values
duplicates = df[df.duplicated(subset=&quot;Col1&quot;, keep=False)]  # Keep all duplicates based on Col1
mismatched = duplicates.groupby(&quot;Col1&quot;).filter(lambda x: x[&quot;Col2&quot;].nunique() &gt; 1)

# Display the result
print(mismatched)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>           Col1  Col2
2  555-123-4567     0
3  555-123-4567     1
</code></pre>
","0","Answer"
"79347248","79346838","<p>With Openpyxl it is easy enough to add extra rows to a Table.<br>
Just add or insert the new row(s) in to the Table and update the table's reference <code>table.ref</code> <br></p>
<p>If you are adding an extra Column(s) there is an extra step needed. That is to update the Table's columns reference. If you just update the table reference the table will be corrupted and Excel cannot recover it.</p>
<p>The following code uses your original table with three columns,</p>
<ul>
<li>Adds the 4th column</li>
<li>Updates the table references</li>
<li>Updates the Conditional Formatting rules to cover the new range</li>
</ul>
<p>I am also using a different method to update the Conditional Formatting range. <br>
Your existing method works fine however this method just saves you having to determine and re-write the existing rules.<br>
This method gets and holds the rules for existing range i.e. <code>old_rule_range</code> 'A2:C4' in the variable <code>rules</code> and deletes those rules from the sheet.<br>
Then for each of the rules held in the variable, re-adds to the Sheet using the new range.<br>
<em><strong>Deleting the existing rules in the old range is necessary as the update will not and you would otherwise end up with the same rule on both the new and old ranges.</strong></em><br></p>
<pre><code>import openpyxl
from openpyxl.formatting.formatting import ConditionalFormattingList
from openpyxl.formatting.rule import ColorScaleRule
from openpyxl.styles import Alignment, Font

path = &quot;test_colors.xlsx&quot;
wb = openpyxl.load_workbook(path)   
sheet = wb['Tab1']

# New Column details
column_number = 4
column_header = 'd'

# Adding data for Column D 
for num, row in enumerate(sheet.iter_rows(min_col=column_number, max_col=column_number)):
    cell = row[0]
    if cell.row == 1:
        cell.value = column_header
        cell.alignment = Alignment(horizontal=&quot;center&quot;)
        cell.font = Font(bold=True)
    else:
        cell.value = num*1.1

### Update the Table
# Select the Table
table = sheet.tables['tab1_table']  # Table selected by name 'tab1_table'
# Add the new column details
table.tableColumns.append(openpyxl.worksheet.table.TableColumn(name=column_header, id=column_number))
# Update the table reference
table.ref = &quot;A1:D4&quot;

### Update the Conditional Formatting
new_rule_range = &quot;A2:D4&quot;
old_rule_range = &quot;A2:C4&quot;
&quot;&quot;&quot;
rule = ColorScaleRule(start_type='min', start_color='00FF00',
                      mid_type='percentile', mid_value=50, mid_color='FFFF00',
                      end_type='max', end_color='AA4A44')

sheet.conditional_formatting = ConditionalFormattingList()
sheet.conditional_formatting.add(new_rule_range, rule)
&quot;&quot;&quot;

# Get rules in the existing CF range 
rules = sheet.conditional_formatting[old_rule_range]
# Delete existing rules
del sheet.conditional_formatting[old_rule_range]
# For each rule re-add with the new range, new_rule_range 
for rule in rules:
    sheet.conditional_formatting.add(new_rule_range, rule)

wb.save(f&quot;new_{path}&quot;)
</code></pre>
<p>The resulant table;<br>
<sub>I have not included table style in this example but whatever style/formatting the original Table has should be retained.</sub>
<a href=""https://i.sstatic.net/VPrDhmth.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VPrDhmth.png"" alt=""enter image description here"" /></a><br></p>
","1","Answer"
"79347390","79347389","<p>It seems like the problem was duplicate header names. I had the exact header name 8 times. So pandas just counted up. The question mark was never the problem. Removing the question mark made the header unique, so a number wasn't added by pandas anymore.</p>
","0","Answer"
"79347922","79347619","<p>Since your data is unknown, add multiple column names to custom_data in list form based on this reference <a href=""https://plotly.com/python/pie-charts/#pie-chart-with-plotly-express"" rel=""nofollow noreferrer"">example</a>. Specify the location of that list for the hover template specification and add the numeric display format.</p>
<pre><code>import plotly.express as px
df = px.data.gapminder().query(&quot;year == 2007&quot;).query(&quot;continent == 'Asia'&quot;)
df = df.sort_values('pop', ascending=False).head(10)

fig = px.pie(df,
             values='pop',
             names='country',
             custom_data=['lifeExp', 'gdpPercap'],
             title='Top 10 population of Asia continent',
             hole=0.5,)
    
fig.update_traces(
    hovertemplate=
    &quot;LifeExp: %{customdata[0][0]:.02f}&lt;br&gt;&quot; +
    &quot;GDPperCap: %{customdata[0][1]:.02f}&quot; +
    &quot;&lt;extra&gt;&lt;/extra&gt;&quot;
)
fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/IYlrSmDW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IYlrSmDW.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79348409","79348389","<p>You can use regular expressions to pull out text without looping through each row. Here's a way to do it using <code>str.extract</code> in pandas:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Assuming df is your DataFrame and 'Name' is the column with the names
df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.')
</code></pre>
","0","Answer"
"79348598","79348067","<p>It's better to avoid mixing pip and conda but sometimes we have no choice. Looks like it is your case, as on conda-forge, the latest version of your library is quite old.</p>
<p>I assume that you are working in a dedicated environment for your project (you should if it is not the case). If you were already in an environment, as you have installed an old version of dataframe_image with conda, uninstall it with conda.<br />
Then be sure to install most libraries you will need with conda and install also pip (using conda).
Now you can install dataframe_image with pip to get the latest version.</p>
<p>References:</p>
<ul>
<li><a href=""https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html#install-non-conda-packages"" rel=""nofollow noreferrer"">non-conda packages in conda env</a></li>
<li><a href=""https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#pip-in-env"" rel=""nofollow noreferrer"">pip in conda env</a></li>
</ul>
","0","Answer"
"79348749","79348389","<p>According to the image, the schema seems to be &quot;LastName, Title. FirstName Other&quot; with no comma or dot in Other. So we can split the name first by the comma and take the group 1 (2nd element) and then split by the dot and take the first group. So you can use:</p>
<pre><code>data['Title'] = data['Name'].map(lambda s: s.split(&quot;,&quot;)[1].split(&quot;.&quot;)[0].strip())
</code></pre>
","0","Answer"
"79348883","79348168","<p>The <code>apply</code> method has a <code>raw</code> argument:</p>
<pre><code> raw:bool, default False

        False : passes each row or column as a Series to the function.

        True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance.
</code></pre>
<p>So using <code>raw=True</code> should pass all the data you need to the function.</p>
","1","Answer"
"79349184","79349155","<p>Those are lists, not arrays. You can convert them to arrays (e.g. with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.array.html"" rel=""nofollow noreferrer""><code>np.array()</code></a>) then simply multiply the columns together.</p>
<pre><code>import numpy as np

df['c'] = df['a'] * df['b'].map(np.array)
</code></pre>
<p>Result:</p>
<pre><code>   a       b        c
0  3     [4]     [12]
1  2  [7, 2]  [14, 4]
</code></pre>
","2","Answer"
"79349284","79349123","<p>use <code>transform</code> instead <code>apply</code>. This should give you some speedup. And <code>sort_index</code> and <code>reset_index</code> are not needed to create columns.</p>
<pre><code>td['output'] = (
    td.sort_values(by=&quot;week&quot;)
    .groupby(&quot;slug&quot;)[&quot;valuation&quot;]
    .transform(lambda x: x.shift().expanding().mean())
)
</code></pre>
<p>To answer your further question, you can see that <code>td.groupby('slug')['valuation'].shift()</code> is not a groupby object, it's just a series. So adding <code>.expanding().mean()</code> doesn't do any grouping operations. If you don't use lambda functions, you'll need to perform the groupby operation one more time with code like this <code>td.groupby('slug')['valuation'].shift().groupby('slug').expanding().mean()</code></p>
","1","Answer"
"79349398","79349347","<p>Use <em><strong>df.at</strong></em> for <em>single-cell assignments</em>: This method avoids ambiguity and is efficient for scalar updates.</p>
<p><em><strong>df.loc</strong></em> is better for <em>bulk operations</em>: Use it when updating multiple cells at once or for filtering.</p>
<p>The issue doesn't occur when the DataFrame is simple (e.g., no join), as the structure is straightforward.</p>
","1","Answer"
"79349515","79349155","<p>Your approach would have been correct with <code>axis=1</code> (= row-wise, the default apply is column-wise):</p>
<pre><code>df['c'] = df.apply(lambda row: [row['a'] * x for x in row['b']], axis=1)
</code></pre>
<p>Using <code>apply</code> is however quite slow since pandas creates an intermediate Series for each row. It will be more efficient to use pure python: a list comprehension is well suited.</p>
<pre><code>df['c'] = [[a * x for x in b] for a, b in zip(df['a'], df['b'])]
</code></pre>
<p>Output:</p>
<pre><code>   a       b        c
0  3     [4]     [12]
1  2  [7, 2]  [14, 4]
</code></pre>
<h4>Comparison of timings</h4>
<p>(on 200k rows)</p>
<pre><code># list comprehension
# df['c'] = [[a * x for x in b] for a, b in zip(df['a'], df['b'])]
98.3 ms ± 3.47 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# conversion to numpy arrays
# df['c'] = df['a'] * df['b'].map(np.array)
371 ms ± 75.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# apply with axis=1
# df['c'] = df.apply(lambda row: [row['a'] * x for x in row['b']], axis=1)
1.65 s ± 65.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
","3","Answer"
"79349665","79349347","<p>one option to avoid the error message is to create an array of tuples, then assign:</p>
<pre class=""lang-py prettyprint-override""><code># idea from here 
# https://stackoverflow.com/a/66248758/7175713
ss=np.array([(1,2)], dtype='i,i')
df.loc[1,'1'] = ss
</code></pre>
<p>I'd stick to using <code>.at</code> since it is very specific to single cell assignments, and as such you avoid the ambiguity.</p>
","2","Answer"
"79349718","79349015","<p>I demonstrate a solution below using <a href=""https://seaborn.pydata.org/"" rel=""nofollow noreferrer""><code>seaborn</code></a>.</p>
<p><a href=""https://i.sstatic.net/TpuyVblJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TpuyVblJ.png"" alt=""enter image description here"" /></a></p>
<p>The sample data represents three algorithms, four tasks classes (imbalanced), and the number of successes for the algorithm-task pair. There are 500 entries so each algorithm-task pair occurs multiple times.</p>
<pre class=""lang-py prettyprint-override""><code>            algo            task  n_successes
0    algorithm A      regression           24
1    algorithm B  classification           16
2    algorithm A      regression           35
...
497  algorithm C     compression           43
498  algorithm B    segmentation            3
499  algorithm C     compression            8
</code></pre>
<p>I used <a href=""https://seaborn.pydata.org/generated/seaborn.displot.html"" rel=""nofollow noreferrer""><code>sns.displot</code></a>. <code>displot</code> can create various distributional plots as specified by <code>kind=</code>, and it defaults to histograms. It supports faceting - <code>col='algo'</code> splits the figure out by <code>'algo'</code> (so internally it runs <code>sns.histplot</code> for each subfigure).</p>
<pre class=""lang-py prettyprint-override""><code>sns.displot(
    df,
    kind='hist',

    x='n_successes',
    hue='task',
    col='algo',
    
    multiple='fill',
    stat='proportion',

    #Optional formatting:
    # limit number of bins, shrink bars, set figure size
    ...
)
</code></pre>
<p>The main parameter for the plotting is <code>multiple='fill'</code> - this stacks the tasks for each bin and normalises the length. Including <code>stat='proprtion'</code> is somewhat redundant here since we've already obtained the proportions (I included it because it labels the figure with &quot;Proportion&quot;)</p>
<h2>Reproducible example</h2>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

np.random.seed(0)

# Sample data
n_samples = 500
data = {
    'algo': np.random.choice(['algorithm A', 'algorithm B', 'algorithm C',], size=n_samples),
    'task': np.random.choice(3*['classification'] + 2*['regression'] + ['segmentation',  'compression'], size=n_samples),
    'n_successes': np.random.randint(45, size=n_samples)
}
df = pd.DataFrame(data)

#Solution
import seaborn as sns

sns.displot(
    df,
    kind='hist',

    x='n_successes',
    hue='task',
    col='algo',
    
    multiple='fill',
    stat='proportion',

    #Optional formatting:
    # limit number of bins, shrink bars, set figure size
    bins=6,
    shrink=0.8,
    height=2.7, aspect=0.8,
)
</code></pre>
","1","Answer"
"79349873","79348168","<p>I found two ways to do it, but one of them using numba and rolling <code>method='table'</code> doesn't work because numba is a bit obscure and doesn't understand the outside context of the callback function.</p>
<p>However, the solution based on <a href=""https://stackoverflow.com/a/60918101/1142881"">this answer</a> works perfectly:</p>
<pre><code>df['force_index_close'] = df['close'].rolling(window=window).\
    apply(args=(df['volume'],), 
          func=lambda close, dfv: ta.volume.ForceIndexIndicator(close=close, volume=dfv.loc[close.index], window=13, fillna=True).force_index().iloc[-1])
print(df['force_index_close'])
df['force_index_close'].plot()
</code></pre>
<p>this is what's happening:</p>
<ol>
<li>I perform the rolling on a single column <code>close</code>, otherwise the apply is computed twice, i.e. once per column</li>
<li>apply gets an additional context <code>args</code> with the series made of the other column <code>volume</code>, if your use-case would require additional columns then they could be injected here into the apply</li>
<li>in the apply func I simply narrow the context <code>volume</code> series to align to the <code>close</code> index</li>
</ol>
","0","Answer"
"79350219","79350196","<p>You need to specify the level of the column index that you want to drop.</p>
<p>Since the date column you want to drop is in level 1 you need to mention it explicitly.</p>
<pre><code>df2.drop(columns=[&quot;July 1, 2024 (est.)&quot;], level=1, axis=1)
</code></pre>
","2","Answer"
"79350314","79348687","<p>Consider database-side, instead of client-side, chunking using SQL Server's <code>OFFSET n ROWS</code> and <code>FETCH NEXT n ROWS ONLY</code>. Be sure to properly set <code>ORDER BY</code> clause and enough <code>range</code> limit to cover all rows.</p>
<pre class=""lang-py prettyprint-override""><code>def call_my_query(path_to_query, query_name, chunk, connection):
    # Read SQL string from file
    file_path = os.path.join(path_to_query, query_name)
    with open(file_path, &quot;r&quot;) as file:
        base_query = file.read()

    main_query = (
        f&quot;SELECT * FROM ({base_query}) &quot;
        &quot;ORDER BY id &quot;
        &quot;OFFSET ? ROWS &quot;
        &quot;FETCH NEXT ? ROWS ONLY&quot;
    )

    # Build list of data frames from SQL parameterized chunks
    chunk_frames = []
    start_time = time.time()

    for i in range(0, int(5e6), chunk):
        chunk_frames.append(
            pd.read_sql_query(
                sql = main_query, con = connection, params = [i, chunk]
            )
        )

    # Concatenating the chunks - joining all the chunks together
    final_frame = pd.concat(chunk_frames, ignore_index=True)

    # Process end-time
    end_time = time.time()

    print(&quot;Data loaded successfully!&quot;)
    print(
        f'Processed {len(final_frame)} rows '
        f'in {end_time - start_time:.2f} seconds'
    )

    return final_frame
</code></pre>
","1","Answer"
"79350751","79349155","<p>if you are willing to include another library, you may get good performance via the <a href=""https://akimbo.readthedocs.io/en/latest/quickstart.html"" rel=""nofollow noreferrer"">akimbo</a> library, and should be faster than the list comprehension:</p>
<pre class=""lang-py prettyprint-override""><code># pip install akimbo
import pandas as pd
import akimbo.pandas
import awkward as ak

df['c'] = df.b.ak * df.a
df

   a       b        c
0  3     [4]     [12]
1  2  [7, 2]  [14  4]
</code></pre>
","1","Answer"
"79350758","79350477","<p>By default, you get a multi-level indexed DataFrame with yf.download so you have 2 options:</p>
<ul>
<li><p>Load the DF with simple indexing:</p>
<pre class=""lang-py prettyprint-override""><code>data = yf.download(tickers, period='1y', interval='1d', multi_level_index=False)
</code></pre>
</li>
<li><p>Use the double-index on the downloaded columns:</p>
<pre class=""lang-py prettyprint-override""><code>data['above_200_SMA'] = (data['Close', 'ABBV'] &gt; data['SMA200'])
</code></pre>
</li>
</ul>
","3","Answer"
"79352699","79352565","<p>I'm still not sure what your input variables look like for your function, but I'll start by using a random <code>pd.DataFrame</code> to illustrate the concept:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd

# Fill a dataframe with random integers
# and set a date range as the index
idx = pd.date_range(start='2024-01-01', end='2025-01-01', freq='h')
df = pd.DataFrame(
    np.random.randint(0, 100, size=(len(idx), 2)), 
    columns=['A', 'B'], 
    index=idx
)

df.head()
                    A   B
2024-01-01 00:00:00 5   87
2024-01-01 01:00:00 51  55
2024-01-01 02:00:00 87  40
2024-01-01 03:00:00 85  90
2024-01-01 04:00:00 38  52
</code></pre>
<p>And now to get your mean per day, use a <code>pandas.Grouper</code> to group the index by day, and aggregate the groups with the <code>mean</code> function.</p>
<pre class=""lang-py prettyprint-override""><code>result = df.groupby(pd.Grouper(freq='D')).agg('mean')
result.head()


            A           B
2024-01-01  49.875000   51.750000
2024-01-02  42.708333   54.791667
2024-01-03  46.416667   43.833333
2024-01-04  58.541667   55.791667
2024-01-05  53.625000   44.125000



</code></pre>
","1","Answer"
"79353186","79353068","<p>If the dtype becoming <code>float</code> is not a concern, then <code>np.ma</code> might be useful for working with this:</p>
<pre><code>(
    np.ma.masked_invalid(df['a']) == np.ma.masked_invalid(df['b'])
).astype(float).filled(np.nan)
</code></pre>
<p>This masks <code>nan</code> in the comparison, then replaces masked values back with <code>nan</code>.</p>
","0","Answer"
"79353188","79353068","<p>One way is to use a mask to check where your NaN values are postprocess your result:</p>
<pre class=""lang-py prettyprint-override""><code>result = df['a'] == df['b']
print(result)

# Check where you NaN values are and set them to NaN afterwards
nan_mask = df[&quot;a&quot;].isna() | df[&quot;b&quot;].isna()

result[nan_mask] = float(&quot;nan&quot;)
print(result)

# 0    NaN
# 1    NaN
# 2    1.0
# dtype: float64
</code></pre>
<p>Note: You cannot have a dtype of int or bool if you want to have NaN values.</p>
","2","Answer"
"79353208","79353068","<p>You can use <code>numpy.where</code> and <code>pandas.isna</code> to replace the comparisons involving NaN with NaN:</p>
<ul>
<li><code>pd.isna</code> checks if any of the elements in the columns 'a' or 'b' are
NaN.</li>
<li><code>numpy.where</code> allows you to replace values based on a condition. If
either element is NaN, it replaces the comparison result with NaN;
otherwise, it performs the equality check.</li>
</ul>
<p>Here's the fixed code:</p>
<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame(columns=['a', 'b'], index=[0, 1, 2], data={'a': [np.NaN, np.NaN, 1], 'b': [np.NaN, 1, 1]}) 

# Compare the columns with numpy.where and pandas.isna
# checks if any of the elements in the columns 'a' or 'b' are NaN.
comparison = np.where(pd.isna(df['a']) | pd.isna(df['b']), np.NaN, df['a'] == df['b'])

# Convert the result to a Series in order to have your excepted output
result = pd.Series(comparison, index=df.index)

print(result)
</code></pre>
<p>As output, you get:</p>
<pre><code>0    NaN
1    NaN
2    1.0
dtype: float64
</code></pre>
","1","Answer"
"79353229","79353068","<p>Pandas has extension dtypes that support three-valued logic <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html#nullable-integer"" rel=""nofollow noreferrer"">for integers</a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html#nullable-integer"" rel=""nofollow noreferrer"">for floats</a>.</p>
<ul>
<li><p>You can use them on-demand:</p>
<pre><code>df.astype('Float64').pipe(lambda d: d['a'] == d['b'])
</code></pre>
<pre><code>0    &lt;NA&gt;
1    &lt;NA&gt;
2    True
dtype: boolean
</code></pre>
</li>
<li><p>Or on df creation:</p>
<pre><code>df = pd.DataFrame(
    {'a': [np.NaN, np.NaN, 1], 'b': [np.NaN, 1, 1]},
    dtype='Int64')
#       a     b
# 0  &lt;NA&gt;  &lt;NA&gt;
# 1  &lt;NA&gt;     1
# 2     1     1

df['a'] == df['b']
</code></pre>
<pre><code>0    &lt;NA&gt;
1    &lt;NA&gt;
2    True
dtype: boolean
</code></pre>
</li>
</ul>
<p>See also: <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html"" rel=""nofollow noreferrer"">Nullable integer data type</a> (User Guide)</p>
<hr />
<p>There's also a <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html#api-arrays-bool"" rel=""nofollow noreferrer"">nullable boolean</a>, which works the same in this particular case.</p>
<pre><code>df.astype('boolean').pipe(lambda d: d['a'] == d['b'])
</code></pre>
<pre><code>0    &lt;NA&gt;
1    &lt;NA&gt;
2    True
dtype: boolean
</code></pre>
<p>See also: <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/boolean.html"" rel=""nofollow noreferrer"">Nullable Boolean data type</a> (User Guide)</p>
","2","Answer"
"79353282","79353192","<p><code>super().reindex</code> calls the reindex method from Series' parent class. Series class definition is</p>
<pre><code>class Series(base.IndexOpsMixin, NDFrame):
</code></pre>
<p>So there are 2 parent classes. In the import at the top of the file you can see that <code>base</code> is imported from <code>pandas.core</code> and <code>NDFrame</code> from <code>pandas.core.generic</code></p>
<p>In the base.py there is no reindex function but in the class <code>NDFrame</code> there is. You can find the <code>reindex()</code> function there:</p>
<p><a href=""https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/generic.py#L5347"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/generic.py#L5347</a> (<a href=""https://github.com/pandas-dev/pandas/blob/0691c5cf90477d3503834d983f69350f250a6ff7/pandas/core/generic.py#L5347"" rel=""nofollow noreferrer"">permalink</a>)</p>
","2","Answer"
"79354154","79354118","<p>Main issue here is that the <code>table</code> is hidden in the comments, so you have to bring it up first, before BeautifulSoup respectively pandas, that use it under the hood, could find it - simplest solution in my opinion is to replace the specific characters in this case:</p>
<pre><code>.replace('&lt;!--','').replace('--&gt;','')
</code></pre>
<p>Example with requests and pandas:</p>
<pre><code>import requests
import pandas as pd

df = pd.read_html(
    requests.get(
        'https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml').text.replace('&lt;!--','').replace('--&gt;',''), 
    attrs={'id':'ArizonaDiamondbackspitching'}
    )[0]
df
</code></pre>
<p>Check also <a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/#special-strings"" rel=""nofollow noreferrer""><code>Special Strings</code></a> in BeautifulSoup docs:</p>
<blockquote>
<p>Tag, NavigableString, and BeautifulSoup cover almost everything you’ll
see in an HTML or XML file, but there are a few leftover bits. The
main one you’ll probably encounter is the Comment.</p>
</blockquote>
","0","Answer"
"79355018","79354633","<p>You could minimize the reshaping by using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html"" rel=""nofollow noreferrer""><code>melt</code></a> and a higher level plotting library like <a href=""https://seaborn.pydata.org"" rel=""nofollow noreferrer""><code>seaborn</code></a>:</p>
<pre><code>import seaborn as sns

sns.catplot(data=pd.concat(data_dict, names=['section', None])
                    [['Data_1', 'Data_4']]
                   .melt(ignore_index=False, var_name='dataset')
                   .reset_index(),
            row='dataset',
            x='section', y='value',
            kind='violin',
           )
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/jyBX2jpF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jyBX2jpF.png"" alt=""enter image description here"" /></a></p>
<p>Another approach to reshape:</p>
<pre><code>tmp = (pd
   .concat(data_dict, names=['section', None])
                    [['Data_1', 'Data_4']]
   .pipe(lambda x: x.set_axis(pd.MultiIndex.from_arrays([x.index.get_level_values('section'),
                                                         x.groupby('section').cumcount()])))
   .T.stack()
)

# then access the datasets
tmp.loc['Data_1']
# section         5         7
# 0        0.235954  0.415159
# 1        0.739301  0.834547
# 2        0.443639  0.170972
# 3        0.806812  0.923432
# 4        0.224321  0.622174
# 5        0.504660  0.185039
</code></pre>
","2","Answer"
"79355380","79355334","<p>You could apply the logic to the full DataFrame at once using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a>, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nsmallest.html"" rel=""nofollow noreferrer""><code>nsmallest</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.add_suffix.html"" rel=""nofollow noreferrer""><code>add_suffix</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>join</code></a>:</p>
<pre><code>tmp = df.filter(like='feature')

out = df.join(tmp.div(tmp.groupby(df['Group_ID'])
                         .transform(lambda x: x.nsmallest(2).mean()))
                 .add_suffix('_rel')
             )
</code></pre>
<p>Output:</p>
<pre><code>    Group_ID  feature1  feature2  label  feature1_rel  feature2_rel
0          1         3         2      0      1.500000      2.000000
1          1         5         7      0      2.500000      7.000000
2          1         2         4      1      1.000000      4.000000
3          1         9         9      1      4.500000      9.000000
4          1         2         0      1      1.000000      0.000000
5          2         4         1      1      0.888889      0.333333
6          2         8         8      0      1.777778      2.666667
7          2         5         5      0      1.111111      1.666667
8          3         0         9      1      0.000000      3.600000
9          3         4         7      1      4.000000      2.800000
10         3         2         3      0      2.000000      1.200000
11         3         7         2      0      7.000000      0.800000
</code></pre>
","2","Answer"
"79356197","79352565","<p>Per <a href=""https://tutorial.xarray.dev/fundamentals/03.2_groupby_with_xarray.html#split-step"" rel=""nofollow noreferrer"">this documentation</a>, to group the data in daily buckets, you can use similar syntax to the pandas option:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import xarray as xr

x = (
    xr
    .tutorial
    .load_dataset(&quot;air_temperature&quot;)
    .groupby(&quot;time.day&quot;)
    .mean()
)
</code></pre>
<h1>Edit</h1>
<p>Since you want two things, one to see if there are any non-contiguous dates and <em>then</em> to do the analysis, here's how that can be accomplished:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import xarray as xr

x = (
    xr
    .tutorial
    .load_dataset(&quot;air_temperature&quot;)
    .resample(time='D')
)

idx = pd.Series(x.groups).index

# Check that all of the range is contiguous, raise an error otherwise
# dropna because the first element will always be NaT
if not (idx.diff().dropna() == '1 days').all():
    raise ValueError(&quot;Got a non-contiguous date range!&quot;)

# do the calculation
x.mean()
</code></pre>
<p>And to show that the index check will fail on a non-contiguous date range:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd 

a = pd.date_range('2023-01-01', periods=5)
b = pd.date_range('2024-01-01', periods=5)

if not (a.union(b).diff().dropna() == '1 days').all():
    raise Exception(&quot;Broken date range!&quot;)

Exception                                 Traceback (most recent call last)
Cell In[8], line 5
      2 b = pd.date_range('2024-01-01', periods=5)
      4 if not (a.union(b).diff().dropna() == '1 days').all():
----&gt; 5     raise Exception(&quot;Broken date range!&quot;)

Exception: Broken date range!
</code></pre>
","1","Answer"
"79356553","79356240","<p><strong>SOLUTION 1</strong></p>
<p>A possible solution, whose steps are:</p>
<ul>
<li><p>First, the dataframe values are reshaped using <a href=""https://numpy.org/doc/stable/reference/generated/numpy.reshape.html"" rel=""nofollow noreferrer""><code>reshape</code></a> into a two-column numpy array and assigned to the variable <code>a</code>.</p>
</li>
<li><p>Then, a new dataframe is created using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>DataFrame</code></a>, taking every second element from <code>a</code> (i.e., <code>a[:, 1]</code>) and reshaping it into a four-column array.</p>
</li>
<li><p>The columns of the new dataframe are labeled with the first four elements of the first column of <code>a</code> (i.e., <code>a[:4, 0]</code>).</p>
</li>
</ul>
<pre><code>a = df.values.reshape(-1, 2)
pd.DataFrame(a[:, 1].reshape(-1, 4), columns=a[:4,0])
</code></pre>
<hr />
<p><strong>SOLUTION 2</strong></p>
<p>Another possible solution, whose steps are:</p>
<ul>
<li><p>Split the dataframe into two parts using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>iloc</code></a>: <code>df.iloc[:, :2]</code> (first two columns) and <code>df.iloc[:, 2:]</code> (remaining columns).</p>
</li>
<li><p>Rename the second part's columns with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_axis.html"" rel=""nofollow noreferrer""><code>set_axis</code></a> to match <code>df.columns[:2]</code>.</p>
</li>
<li><p>Concatenate the two parts using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>.</p>
</li>
<li><p>Set column <code>'0'</code> as the index with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a>.</p>
</li>
<li><p>Reshape with two <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html"" rel=""nofollow noreferrer""><code>unstack</code></a> operations to flatten the structure.</p>
</li>
<li><p>Remove axis names with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename_axis.html"" rel=""nofollow noreferrer""><code>rename_axis</code></a>.</p>
</li>
<li><p>Reset the index with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a>, dropping the original index.</p>
</li>
</ul>
<pre><code>(pd.concat([df.iloc[:, :2], df.iloc[:,2:].set_axis(df.columns[:2], axis=1)])
 .set_index('0').unstack().unstack(1)
 .rename_axis(None, axis=1)
 .reset_index(drop=True))
</code></pre>
<hr />
<p>Output:</p>
<pre><code>   Name Age Salary       phone
0  khan  42   5000  1783232575
</code></pre>
","0","Answer"
"79356575","79355334","<p>IIUC, you can try:</p>
<pre><code>col = pd.Index(['feature1', 'feature2'])
df[col+'_rel']= df.groupby('Group_ID')[col].transform(lambda x: x/x.nsmallest(2).mean())
</code></pre>
<p>Output:</p>
<pre><code>    Group_ID  feature1  feature2  label  feature1_rel  feature2_rel
0          1         3         2      0      1.500000      2.000000
1          1         5         7      0      2.500000      7.000000
2          1         2         4      1      1.000000      4.000000
3          1         9         9      1      4.500000      9.000000
4          1         2         0      1      1.000000      0.000000
5          2         4         1      1      0.888889      0.333333
6          2         8         8      0      1.777778      2.666667
7          2         5         5      0      1.111111      1.666667
8          3         0         9      1      0.000000      3.600000
9          3         4         7      1      4.000000      2.800000
10         3         2         3      0      2.000000      1.200000
11         3         7         2      0      7.000000      0.800000
</code></pre>
","1","Answer"
"79356620","79356240","<p>Since I don't know if your actual data expands in rows or columns, I generated code so that the allocation is dynamic for both.</p>
<pre><code>cols = df.columns[::2].tolist()
out = (df.assign(index=0).pivot(columns=cols, index='index')
         .set_axis(df[cols].melt()['value'].rename(None), axis=1)
)
</code></pre>
<p>out</p>
<pre><code>        Name    Age Salary  phone
index               
0       khan    42  5000    1783232575
</code></pre>
","0","Answer"
"79356634","79356240","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>arr = df.stack().values

out = pd.DataFrame(data=[arr[1::2]], columns=arr[::2])
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>   Name  Salary Age       phone
0  khan    5000  42  1783232575
</code></pre>
<p><strong>Explanation</strong></p>
<ul>
<li>Create an array with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>df.stack</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.values.html"" rel=""nofollow noreferrer""><code>Series.values</code></a></li>
</ul>
<pre class=""lang-py prettyprint-override""><code>array(['Name', 'khan', 'Salary', 5000, 'Age', '42', 'phone', 1783232575],
      dtype=object)
</code></pre>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>pd.DataFrame</code></a> and pass odd indices (<code>[1::2]</code>) to <code>data</code> in a list, and even ones (<code>[::2]</code>) to <code>columns</code>.</li>
</ul>
<hr />
<p>If the order of columns is of particular concern, via <a href=""https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html"" rel=""nofollow noreferrer""><code>.T</code></a> + <a href=""https://numpy.org/doc/stable/reference/generated/numpy.ravel.html"" rel=""nofollow noreferrer""><code>np.ravel</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>out2 = pd.DataFrame(data=[df.values[:, 1::2].T.ravel()], 
                    columns=df.values[:, ::2].T.ravel())
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>   Name Age  Salary       phone
0  khan  42    5000  1783232575
</code></pre>
","1","Answer"
"79356797","79356690","<pre class=""lang-py prettyprint-override""><code># your dataframe
df = pd.DataFrame({'EMPLID': [12, 13, 14, 15, 16, 17, 18],
    'V1': [2,3,4,50,6,7,8],
    'V2': [3,3,3,3,3,3,3],
    'V3': [7,15,8,9,10,11,12],
    'X': [2,3,1,3,3,1,2]
})
</code></pre>
<p>First, we get the columns names that we want to change and their indices on the original dataframe.</p>
<pre class=""lang-py prettyprint-override""><code># column name
x = df['X'][(df['X'] &gt; 1)]
# column names mapped to your scenario
columns = [f'V{v}' for v in x]
# desired indexes
positions = x.index.values
#Then we convert the column names to indices and use these indices to update the positions matching the conditions.
column_indices = [df.columns.get_loc(col) for col in columns]
</code></pre>
<p>Now, we can use two approaches here.</p>
<h1>A vectorized approach</h1>
<p>Convert the dataframe to numpy array, change the desired positions all at once and change the result back to a dataframe.</p>
<pre class=""lang-py prettyprint-override""><code>
import numpy as np

# the original dataframe column names
column_names = df.columns
# convert the dataframe to numpy (this will 'remove' the column names)
df_array = df.values
# put the columns back (axis=0 will stack the columns at the top of the array)
df_array = np.concatenate([[column_names], df_array], axis=0)
# position+1 because when using pandas to get the row index, we ignore the columns (which would have index 0)
df_array[positions+1, column_indices] = 9
# convert the result back to a dataframe
df = pd.DataFrame(df_array[1:], columns=column_names)

print(df)
</code></pre>
<p>Output:</p>
<pre><code> EMPLID  V1  V2  V3  X
0      12   2   9   7  2
1      13   3   3   9  3
2      14   4   3   8  1
3      15  50   3   9  3
4      16   6   3   9  3
5      17   7   3  11  1
6      18   8   9  12  2
</code></pre>
<h1>A loop approach</h1>
<p>The easiest way would be just to loop over the rows and columns, changing one value at a time.</p>
<pre class=""lang-py prettyprint-override""><code>for row, column in zip(positions, column_indices):
    df.iat[row,column] = 9
</code></pre>
<p>If your dataframe is small, the vectorized approach won't have as much of an advantage over the for loop.</p>
","1","Answer"
"79357424","79356690","<p>One simple solution with np.where:</p>
<pre><code>for i in range(2, 4):
    df[f&quot;V{i}&quot;] = np.where(df[&quot;X&quot;] == i, 9, df[f&quot;V{i}&quot;])
</code></pre>
","1","Answer"
"79357533","79357226","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>pay_days_gross = (raise_series.add(1)
                  .cumprod()
                  .reindex(pay_days, 
                           method='ffill', 
                           fill_value=1)
                  .mul(start_rate)
                  )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>2025-08-03    1060.000
2025-08-17    1060.000
2025-08-31    1060.000
2025-09-14    1060.000
2025-09-28    1060.000
2025-10-12    1113.000
2025-10-26    1113.000
2025-11-09    1113.000
2025-11-23    1113.000
2025-12-07    1113.000
2025-12-21    1113.000
2026-01-04    2287.215
2026-01-18    2287.215
2026-02-01    2287.215
Freq: 2W-SUN, dtype: float64
</code></pre>
<hr />
<p>Equality check OP's method:</p>
<pre class=""lang-py prettyprint-override""><code>pay_days_gross1.set_axis(pay_days_gross.index).equals(pay_days_gross)

# True
</code></pre>
<hr />
<p><strong>Explanation / Intermediates</strong></p>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.add.html"" rel=""nofollow noreferrer""><code>Series.add</code></a> to add <code>1</code>, turning percentages into multipliers, and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.cumprod.html"" rel=""nofollow noreferrer""><code>Series.cumprod</code></a> to calculate their cumulative effect over time.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>raise_series.add(1).cumprod()

2024-10-01    1.060000
2025-10-01    1.113000
2026-01-01    2.287215
2026-10-01    2.378704
2027-10-01    2.473852
2028-10-01    2.572806
2029-10-01    2.675718
dtype: float64
</code></pre>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.reindex.html"" rel=""nofollow noreferrer""><code>Series.reindex</code></a> with <code>pay_days</code>, using <code>method='ffill'</code> to forward-fill values and <code>fill_value=1</code> to set the multiplier to <code>1</code> if <code>start</code> precedes the first date in <code>raise_series</code>.</li>
<li>Multiply by <code>start_rate</code> with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.mul.html"" rel=""nofollow noreferrer""><code>Series.mul</code></a>.</li>
</ul>
","3","Answer"
"79358004","79357992","<p><code>$$</code> is the beginning of a LaTeX math formula. So matplotlib crashes because you have a math formula opened and not closed.</p>
<p>Note that it did not crashes, strictly speaking. It correctly raises an error (it is not as if it broke the python interpreter with a segfault). The error message (that you should have included in your question) clearly states</p>
<pre><code>ParseException: Expected end of text, found '$'  (at char 0), (line:1, col:1)
</code></pre>
<p>(Well, not that clearly. But that is a very classical error for many parsers: it complains about what it expected)</p>
","2","Answer"
"79358007","79357992","<p>You need to escape the <code>$</code> as it will be <a href=""https://matplotlib.org/stable/users/explain/text/usetex.html"" rel=""nofollow noreferrer"">interpreted as LaTeX formatting</a> (specifically, <code>$...$</code> is the LaTeX math mode, which is useful if you want to type complex mathematical terms, try for instance <code>l[49] = r'$x_1 \cdot \delta t$'</code>):</p>
<pre><code>l[50] = r'\$\$'
l.value_counts(normalize=False).plot(kind='bar')
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/K8NJkXGy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/K8NJkXGy.png"" alt=""enter image description here"" /></a></p>
","5","Answer"
"79358533","79348687","<p>The dask docstring for read_sql_query includes:</p>
<pre><code>sql : SQLAlchemy Selectable
    SQL query to be executed. TextClause is not supported

</code></pre>
<p>but you are passing text anyway, which is why you get that error. To use dask, and its abililty to automatically find partitioning points in the data, you must phrase the query in terms of sqlalchemy functions, e.g., starting off in sqlalchemy.sql and its submodules ( <a href=""https://docs.sqlalchemy.org/en/20/core/sqlelement.html"" rel=""nofollow noreferrer"">https://docs.sqlalchemy.org/en/20/core/sqlelement.html</a> ).</p>
","-1","Answer"
"79360632","79360275","<p>Perhaps the reason is that <code>sql=</code> parameter expects only the query to be executed, but when you use <code>select(products)</code> the structure changes. Try formatting and the query itself seems wrong as seen in the output. The tables to be selected are not mentioned. Look at 'https://docs.sqlalchemy.org/en/20/core/metadata.html' on how to specify which tables to select.</p>
","1","Answer"
"79361257","79361211","<p>You seem to be relying on a view of the Index, this is generally a bad practice and prone to hidden issues like yours.</p>
<p>You can be explicit, modify your names and reassign them to the DataFrame:</p>
<pre><code>header = list(df)
header[1:] = [c.replace('word1_word2', 'word1 word2') for c in header[1:]]
df.columns = header
</code></pre>
<p>Or use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a> with a dictionary (if the column names are unique):</p>
<pre><code>df.rename(columns={c: c.replace('word1_word2', 'word1 word2')
                   for c in df.columns[1:]}, inplace=True)
</code></pre>
<p>Output:</p>
<pre><code>   word_skip  word1 word2  word3_word4  word5_word6
0          1           10          100         1000
1          2           20          200         2000
2          3           30          300         3000
3          4           40          400         4000
4          5           50          500         5000
</code></pre>
","0","Answer"
"79361512","79361494","<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html"" rel=""nofollow noreferrer""><code>query</code></a> could be a good solution:</p>
<pre><code>filter_A = 'A == 1'
filter_B = 'B == 2'

print(df.query(f'({filter_A}) &amp; ({filter_B})'))
</code></pre>
<p>Alternatively:</p>
<pre><code>filters = [filter_A, filter_B]
df.query('&amp;'.join(f'({f})' for f in filters))

# or
df.query('&amp;'.join(map('({})'.format, filters)))
</code></pre>
<p>Output:</p>
<pre><code>   A  B
1  1  2
</code></pre>
","2","Answer"
"79361536","79361494","<p>You can use the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.eval.html"" rel=""nofollow noreferrer""><code>.eval()</code></a> method, which allows for the evaluation of a string describing operations on dataframe columns:</p>
<ol>
<li><p>Evaluate these string expressions on the dataframe <code>df</code>.</p>
</li>
<li><p>Combine the results of these evaluations using the bitwise AND operator (<code>&amp;</code>), which performs element-wise logical AND operation.</p>
</li>
<li><p>Use the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>.loc</code></a> accessor to filter the dataframe based on the combined condition.</p>
</li>
</ol>
<pre><code>filter_A = 'A == 1'
filter_B = 'B == 2'
df.loc[df.eval(filter_A) &amp; df.eval(filter_B)]
</code></pre>
<p>Output:</p>
<pre><code>   A  B
1  1  2
</code></pre>
","2","Answer"
"79361544","79361495","<p>Seems that mapping works and focus is on dataframe - Extract the ids from your dataframe as a series, iterate over them and record the results in a dictionary, which you can then easily transfer back into a dataframe.</p>
<pre><code># list or series of your ids
client_id_series = ['R_111','R_222']

pd.DataFrame(
    [
        {'ID':client_id,'Phone':get_client_phone(client_id)} 
        for client_id 
        in client_id_series
    ]
)
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ID</th>
<th style=""text-align: right;"">Phone</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">R_111</td>
<td style=""text-align: right;"">+77777777777</td>
</tr>
<tr>
<td style=""text-align: left;"">R_222</td>
<td style=""text-align: right;"">+88888888888</td>
</tr>
</tbody>
</table></div>
<hr />
<p>Or simply iterate your existing dataframe directly and only add the column with the result of the phone number</p>
<pre><code>data = {
    'Date': ['20240514 May, 14 22:00', '20240514 May, 14 23:00'],
    'ID': ['R_111', 'R_222'],
    'Comment': ['Le client ne répond pas', None]
}

df = pd.DataFrame(data)

df['Phone'] = df['ID'].apply(get_client_phone)

print(df)
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">Date</th>
<th style=""text-align: left;"">ID</th>
<th style=""text-align: left;"">Comment</th>
<th style=""text-align: right;"">Phone</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">20240514 May, 14 22:00</td>
<td style=""text-align: left;"">R_111</td>
<td style=""text-align: left;"">Le client ne répond pas</td>
<td style=""text-align: right;"">+77777777777</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">20240514 May, 14 23:00</td>
<td style=""text-align: left;"">R_222</td>
<td style=""text-align: left;""></td>
<td style=""text-align: right;"">+88888888888</td>
</tr>
</tbody>
</table></div>
","0","Answer"
"79362247","79361494","<p>If you're only ever combining based on logical AND, you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html"" rel=""nofollow noreferrer""><code>.pipe()</code></a> with your existing functions, or <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>.loc</code></a> with callables (i.e. functions) that produce boolean masks.</p>
<p>Otherwise, you can use mask functions and combine them in a function you pass to <code>.loc</code>.</p>
<p><em>Note: Here I've changed the name of the dataframe to <code>df_example</code> to avoid confusion with the function parameter <code>df</code>.</em></p>
<h3><code>.pipe()</code></h3>
<pre><code>&gt;&gt;&gt; df_example.pipe(filter_A).pipe(filter_B)
   A  B
1  1  2
</code></pre>
<h3><code>.loc</code> with mask functions</h3>
<pre><code>def mask_A(df):
    return df[&quot;A&quot;] == 1

def mask_B(df):
    return df[&quot;B&quot;] == 2
</code></pre>
<pre><code>&gt;&gt;&gt; df_example.loc[mask_A].loc[mask_B]
   A  B
1  1  2
</code></pre>
<h3><code>.loc</code> with combined function</h3>
<pre><code>&gt;&gt;&gt; df_example.loc[lambda df: mask_A(df) &amp; mask_B(df)]
   A  B
1  1  2
&gt;&gt;&gt; # now OR
&gt;&gt;&gt; df_example.loc[lambda df: mask_A(df) | mask_B(df)]
   A  B
0  1  1
1  1  2
</code></pre>
","0","Answer"
"79362331","79348640","<p>I found a solution. The problem was that I had trained the model incorrectly and it was unable to predict data outside the set. The code below works correctly:</p>
<pre><code>def learn(self, dataset_path: str) -&gt; Sequential:
    df = pd.read_csv(dataset_path)
    y = df['close'].fillna(method='ffill')
    y = y.values.reshape(-1, 1)

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaler = scaler.fit(y)
    y = scaler.transform(y)

    n_lookback = int(len(y) * 0.24)
    n_forecast = int(len(y) * 0.12)

    X = []
    Y = []

    for i in range(n_lookback, len(y) - n_forecast + 1):
        X.append(y[i - n_lookback: i])
        Y.append(y[i: i + n_forecast])

    X = np.array(X, dtype=np.float16)
    Y = np.array(Y, dtype=np.float16)

    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=(n_lookback, 1)))
    model.add(LSTM(units=50))
    model.add(Dense(n_forecast))

    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X, Y, epochs=30, batch_size=128)

    return model
</code></pre>
<p>Call predict method:</p>
<pre><code>def predict(self, model: Sequential, df: pd.DataFrame) -&gt; pd.DataFrame:
    y = df['close'].fillna(method='ffill')
    y = y.values.reshape(-1, 1)

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaler = scaler.fit(y)
    y = scaler.transform(y)

    n_lookback = int(len(y) * 0.24)
    n_forecast = int(len(y) * 0.12)

    X_ = y[- n_lookback:]
    X_ = X_.reshape(1, n_lookback, 1)

    Y_ = model.predict(X_).reshape(-1, 1)
    Y_ = scaler.inverse_transform(Y_)

    timestamp_step = 1_000_000

    df_future = pd.DataFrame(columns=['unix', 'Forecast'])
    unix_range = np.array(
        range(int(df['unix'].iloc[0] / timestamp_step), int(df['unix'].iloc[-1] / timestamp_step) + 1)
    )
    df_future['unix'] = np.array(range(unix_range[-1], (unix_range[-1] + n_forecast) - 1))
    df_future['Forecast'] = pd.Series(Y_.flatten())

    return df_future[df_future['Forecast'].notna()]
</code></pre>
","0","Answer"
"79362543","79362404","<p>That's a fairly complex task. The individual steps are straightforward though.</p>
<p>You need:</p>
<ul>
<li><a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-lookup"" rel=""nofollow noreferrer"">indexing lookup</a> to find the cap values based on class</li>
<li><a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a> to match the max_BKT</li>
<li><a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.mask.html"" rel=""nofollow noreferrer""><code>mask</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a> to identify the rows to mask</li>
</ul>
<pre><code>idx, cols = pd.factorize(df['class']+'_Cap')

group = ['orig', 'dest', 'type']

out = (
    df.merge(override, on=group, how='left')
    .assign(
        value=lambda x: x['value'].mask(
            x['BKT_order'].ge(
                x['BKT_order']
                .where(x['BKT'].eq(x['max_BKT']))
                .groupby([x[c] for c in group])
                .transform('first')
            ),
            x.reindex(cols, axis=1).to_numpy()[np.arange(len(x)), idx],
        )
    )
    .reindex(columns=df.columns)
)
</code></pre>
<p>Output:</p>
<pre><code>  orig dest type class BKT  BKT_order  value  fc_Cap  sc_Cap
0  AMD  TRY   SA    fc  MA          1  12.04      20      50
1  AMD  TRY   SA    fc  TY          2  20.00      20      50
2  AMD  TRY   SA    fc  NY          3  20.00      20      50
3  AMD  TRY   SA    fc  MU          4  20.00      20      50
4  AMD  TRY   PE    fc  RE          1   9.70      20      50
5  AMD  TRY   PE    sc  EW          5   7.70      20      50
6  NCL  MNK   PE    sc  PO          2  50.00      20      50
7  NCL  MNK   PE    sc  TU          3  50.00      20      50
8  NCL  MNK   PE    sc  MA          1  16.70      20      50
</code></pre>
<p>Intermediates:</p>
<pre><code>tmp = df.merge(override, on=group, how='left')
tmp['cap'] = tmp.reindex(cols, axis=1).to_numpy()[np.arange(len(tmp)), idx]
tmp['mask'] = tmp['BKT'].eq(tmp['max_BKT'])
tmp['masked_BKT'] = tmp['BKT_order'].where(tmp['mask'])
tmp['ref_BKT'] = tmp.groupby(group)['masked_BKT'].transform('first')
tmp['&gt;= ref_BKT'] = tmp['BKT_order'].ge(tmp['ref_BKT'])

  orig dest type class BKT  BKT_order  value  fc_Cap  sc_Cap max_BKT  cap   mask  masked_BKT  ref_BKT  &gt;= ref_BKT
0  AMD  TRY   SA    fc  MA          1  12.04      20      50      TY   20  False         NaN      2.0       False
1  AMD  TRY   SA    fc  TY          2  11.50      20      50      TY   20   True         2.0      2.0        True
2  AMD  TRY   SA    fc  NY          3  17.70      20      50      TY   20  False         NaN      2.0        True
3  AMD  TRY   SA    fc  MU          4   9.70      20      50      TY   20  False         NaN      2.0        True
4  AMD  TRY   PE    fc  RE          1   9.70      20      50     NaN   20  False         NaN      NaN       False
5  AMD  TRY   PE    sc  EW          5   7.70      20      50     NaN   50  False         NaN      NaN       False
6  NCL  MNK   PE    sc  PO          2   8.70      20      50      PO   50   True         2.0      2.0        True
7  NCL  MNK   PE    sc  TU          3  12.50      20      50      PO   50  False         NaN      2.0        True
8  NCL  MNK   PE    sc  MA          1  16.70      20      50      PO   50  False         NaN      2.0       False
</code></pre>
","2","Answer"
"79362770","79362404","<p>Here one solution with merge and apply (df1 and df2 are the 2 input dataframes).</p>
<pre><code>df2[&quot;max_BKT_order&quot;] = 0
df1 = df1.set_index([&quot;orig&quot;, &quot;dest&quot;, &quot;type&quot;, &quot;BKT&quot;])
df2 = df2.set_index([&quot;orig&quot;, &quot;dest&quot;, &quot;type&quot;, &quot;max_BKT&quot;])
df2[&quot;max_BKT_order&quot;] = df2.apply(
    lambda x: df1.loc[x.name, &quot;BKT_order&quot;] if x.name in df1.index else np.nan,
    axis=1)
df1 = df1.reset_index()
df2 = df2.reset_index()

# intermediate df2:
  orig dest type max_BKT  max_BKT_order
0  AMD  TRY   SA      TY            2.0
1  AMD  TRY   PE      PO            NaN
2  NCL  MNK   PE      PO            2.0

# Join both dataframes on the first 3 columns
res = df1.merge(df2, on=[&quot;orig&quot;, &quot;dest&quot;, &quot;type&quot;])
res = res.set_index([&quot;orig&quot;, &quot;dest&quot;, &quot;type&quot;]).sort_index()

res[&quot;value&quot;] = res.apply(
    lambda x: 
        res.loc[x.name].loc[
            res.loc[x.name][&quot;BKT_order&quot;] == x[&quot;max_BKT_order&quot;], 
            f&quot;{x['class']}_Cap&quot;
        ][0] 
        if x[&quot;BKT_order&quot;] &gt;= x[&quot;max_BKT_order&quot;]
        else x[&quot;value&quot;],
    axis=1)
# Format the result
res = res.reset_index()[df1.columns]
</code></pre>
<pre><code>  orig dest type BKT class  BKT_order  value  fc_Cap  sc_Cap
0  AMD  TRY   PE  RE    fc          1   9.70      20      50
1  AMD  TRY   PE  EW    sc          5   7.70      20      50
2  AMD  TRY   SA  MA    fc          1  12.04      20      50
3  AMD  TRY   SA  TY    fc          2  20.00      20      50
4  AMD  TRY   SA  NY    fc          3  20.00      20      50
5  AMD  TRY   SA  MU    fc          4  20.00      20      50
6  NCL  MNK   PE  PO    sc          2  50.00      20      50
7  NCL  MNK   PE  TU    sc          3  50.00      20      50
8  NCL  MNK   PE  MA    sc          1  16.70      20      50
</code></pre>
","1","Answer"
"79363505","79363433","<p>Set <code>C</code> as the index then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>.stack('number')</code></a>.</p>
<p>Then to make it look like you want, reset the index and sort by <code>number</code>.</p>
<pre><code>(
    df.set_index('C')
    .stack('number')
    .reset_index()
    .sort_values('number')
)
</code></pre>
<pre><code>   C number  A  B
0  X    one  1  3
2  Y    one  5  7
1  X    two  2  4
3  Y    two  6  8
</code></pre>
<p>Note: To sort by <code>number</code> more correctly:</p>
<pre><code>    .sort_values('number', key=lambda s: s.map({'one': 1, 'two': 2}))
</code></pre>
","2","Answer"
"79364114","79364078","<p>A similar issue was raised on <a href=""https://github.com/chanzuckerberg/single-cell-curation/issues/1165"" rel=""nofollow noreferrer"">https://github.com/chanzuckerberg/single-cell-curation/issues/1165</a>. Seems to be a bug with scipy version &gt;= 1.15.0. The user stated that rolling back to scipy version 1.14.1 fixes the issue. It's still an open bug issue on the repository.</p>
","0","Answer"
"79364429","79364338","<p>The <code>bins</code> parameter is actually just a convenience that calls <a href=""https://pandas.pydata.org/docs/reference/api/pandas.cut.html"" rel=""nofollow noreferrer""><code>cut</code></a> internally.</p>
<p>You can therefor <a href=""https://pandas.pydata.org/docs/reference/api/pandas.cut.html"" rel=""nofollow noreferrer""><code>cut</code></a> the values manually and use the <code>labels</code> parameter before <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html#pandas.Series.value_counts"" rel=""nofollow noreferrer""><code>value_counts</code></a>:</p>
<pre><code>out = pd.cut(df['A'], mBins, labels=mLabels).value_counts()
</code></pre>
<p>If you don't want to provide the labels, you could also infer them from the intervals and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a>:</p>
<pre><code>from math import ceil

out = (df['A'].value_counts(bins=mBins)
       .rename(lambda x: f'{ceil(x.left+1)}-{ceil(x.right)}')
      )
</code></pre>
<p><em>NB. this is an example for integer values/bounds.</em></p>
<p>Output:</p>
<pre><code>A
0-2    2
3-4    1
5-6    1
Name: count, dtype: int64
</code></pre>
","1","Answer"
"79364558","79364338","<p>value_counts returns a series so you can simply rename its index with:</p>
<pre class=""lang-py prettyprint-override""><code>    simple_VC.index = mLabels

0-2    2
3-4    1
5-6    1

</code></pre>
","1","Answer"
"79364603","79364551","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.first.html"" rel=""nofollow noreferrer""><code>GroupBy.first</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.last.html"" rel=""nofollow noreferrer""><code>GroupBy.last</code></a> first, subtract necessary columns with add <code>points</code> columns in <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>:</p>
<pre><code>g = df.groupby(['match_id','round'])
df1 = g.first()
df2 = g.last()

cols = ['A','B','C','D','E']
out = pd.concat([df1['points'].rename('points_home'),
                df2['points'].rename('points_away'), 
                df1[cols].sub(df2[cols])], axis=1).reset_index()
print (out)
   match_id  round  points_home  points_away  A  B  C  D  E
0      5890      1           10            9  0  0  0 -2  1
1      5890      2           10            9  0  0  0 -4  0
2      5890      3           10            9  0  8  0 -2  1
</code></pre>
<p>Alternative with <code>MultiIndex</code> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.agg.html"" rel=""nofollow noreferrer""><code>GroupBy.agg</code></a>:</p>
<pre><code>df3 = (df.groupby(['match_id','round'])
         .agg(['first','last'])
         .rename(columns={'first':'home', 'last':'away'}))

cols = ['A','B','C','D','E']

out = pd.concat([df3['points'].add_prefix('points_'), 
                 df3.xs('home', axis=1, level=1)[cols]
                    .sub(df3.xs('away', axis=1, level=1)[cols])], axis=1).reset_index()
print (out)
   match_id  round  points_home  points_away  A  B  C  D  E
0      5890      1           10            9  0  0  0 -2  1
1      5890      2           10            9  0  0  0 -4  0
2      5890      3           10            9  0  8  0 -2  1
</code></pre>
","3","Answer"
"79366300","79365680","<p>Adding to the discussion, here are those tests in the <a href=""https://gouthamanbalaraman.com/blog/numpy-vs-pandas-comparison.html"" rel=""nofollow noreferrer"">linked page</a> reproduced with some minor changes to see if anything has changed since that original post was made almost 8 years ago and python and many of its libraries have upgraded quite a bit since then. <a href=""https://devguide.python.org/versions/"" rel=""nofollow noreferrer"">According to python.org</a> the newest version of python available at the time of his post was 3.6 .</p>
<p>Here is the source code, copied from the linked page and updated to be runnable as posted here, plus a few minor changes for convenience.</p>
<pre><code>import pandas
import matplotlib.pyplot as plt
import seaborn
import numpy

import sys
import time

NUMBER_OF_ITERATIONS = 10
FIGURE_NUMBER = 0

def bench_sub(mode1_inputs: list, mode1_statement: str, mode2_inputs: list, mode2_statement: str) -&gt; tuple[bool, list[float], list[float]]:

    mode1_results = []
    mode1_times = []
    mode2_results = []
    mode2_times = []

    for inputs, statementi, results, times in (
            (mode1_inputs, mode1_statement, mode1_results, mode1_times),
            (mode2_inputs, mode2_statement, mode2_results, mode2_times)
    ):
        for inputi in inputs:
            ast = compile(statementi, '&lt;string&gt;', 'exec')
            ast_locals = {'data': inputi}
            start_time = time.perf_counter_ns()
            for _ in range(NUMBER_OF_ITERATIONS):
                exec(ast, locals=ast_locals)
            end_time = time.perf_counter_ns()

            results.append(ast_locals['res'])
            times.append((end_time - start_time) / 10 ** 9 / NUMBER_OF_ITERATIONS)

    passing = True
    for results1, results2 in zip(mode1_results, mode2_results):
        if not passing:
            break
        try:
            if type(results1) in [pandas.Series, numpy.ndarray] and type(results2) in [pandas.Series, numpy.ndarray]:
                if type(results1[0]) is str:
                    isclose = set(results1) == set(results2)
                else:
                    isclose = numpy.isclose(results1, results2).all()
            else:
                isclose = numpy.isclose(results1, results2)
            if not isclose:
                passing = False
                break
        except (ValueError, TypeError):
            print(type(results1))
            print(results1)
            print(type(results2))
            print(results2)
            raise
    return passing, mode1_times, mode2_times

def bench_sub_plot(mode1_inputs: list, mode1_statement: str, mode2_inputs: list, mode2_statement: str, title: str, label1: str, label2: str, save_fig: bool = True) -&gt; tuple[bool, list[float], list[float]]:
    passing, mode1_times, mode2_times = bench_sub(mode1_inputs, mode1_statement, mode2_inputs, mode2_statement)

    fig, ax = plt.subplots(2, dpi=100, figsize=(8, 6))
    mode1_x = [len(x) for x in mode1_inputs]
    mode2_x = [len(x) for x in mode2_inputs]

    ax[0].plot(mode1_x, mode1_times, marker='o', markerfacecolor='none', label=label1)
    ax[0].plot(mode2_x, mode2_times, marker='^', markerfacecolor='none', label=label2)
    ax[0].set_xscale('log')
    ax[0].set_yscale('log')
    ax[0].legend()
    ax[0].set_title(title + f' : {&quot;PASS&quot; if passing else &quot;FAIL&quot;}')
    ax[0].set_xlabel('Number of records')
    ax[0].set_ylabel('Time [s]')

    if mode1_x == mode2_x:
        mode_comp = [x / y for x, y in zip(mode1_times, mode2_times)]
        ax[1].plot(mode1_x, mode_comp, marker='o', markerfacecolor='none', label=f'{label1} / {label2}')
        ax[1].plot([min(mode1_x), max(mode1_x)], [1.0, 1.0], linestyle='dashed', color='#AAAAAA', label='parity')
        ax[1].set_xscale('log')
        ax[1].legend()
        ax[1].set_title(title + f' (ratio)\nValues &lt;1 indicate {label1} is faster than {label2}')
        ax[1].set_xlabel('Number of records')
        ax[1].set_ylabel(f'{label1} / {label2}')
    plt.tight_layout()
    # plt.show()

    if save_fig:
        global FIGURE_NUMBER
        # https://stackoverflow.com/a/295152
        clean_title = ''.join([x for x in title if (x.isalnum() or x in '_-. ')])
        fig.savefig(f'outputs/{FIGURE_NUMBER:06}_{clean_title}.png')
        FIGURE_NUMBER += 1

    return passing, mode1_times, mode2_times

def _print_result_comparison(success: bool, times1: list[float], times2: list[float], input_lengths: list[int], title: str, label1: str, label2: str):
    print(title)
    print(f'  Test result: {&quot;PASS&quot; if success else &quot;FAIL&quot;}')
    field_width = 15
    print(f'{&quot;# of records&quot;:&gt;{field_width}} {label1 + &quot; [ms]&quot;:&gt;{field_width}} {label2 + &quot; [ms]&quot;:&gt;{field_width}} {&quot;ratio&quot;:&gt;{field_width}}')
    for input_length, time1, time2 in zip(input_lengths, times1, times2):
        print(f'{input_length:&gt;{field_width}} {time1 * 1000:&gt;{field_width}.03f} {time2 * 1000:&gt;{field_width}.03f} {time1 / time2:&gt;{field_width}.03f}')
    print()

def bench_sub_plot_print(mode1_inputs: list, mode1_statement: str, mode2_inputs: list, mode2_statement: str, title: str, label1: str, label2: str, all_lengths: list[int], save_fig: bool = True) -&gt; tuple[bool, list[float], list[float]]:
    success, times1, times2 = bench_sub_plot(
        mode1_inputs,
        mode1_statement,
        mode2_inputs,
        mode2_statement,
        title,
        label1,
        label2,
        True
    )
    _print_result_comparison(success, times1, times2, all_lengths, title, label1, label2)
    return success, times1, times2


def _main():

    start_time = time.perf_counter_ns()

    # In [2]:
    iris = seaborn.load_dataset('iris')


    # In [3]:
    data_pandas: list[pandas.DataFrame] = []
    data_numpy: list[numpy.rec.recarray] = []
    all_lengths = [10_000, 100_000, 500_000, 1_000_000, 5_000_000, 10_000_000, 15_000_000]
    # all_lengths = [10_000, 100_000, 500_000] #, 1_000_000, 5_000_000, 10_000_000, 15_000_000]
    for total_len in all_lengths:
        data_pandas_i = pandas.concat([iris] * (total_len // len(iris)))
        data_pandas_i = pandas.concat([data_pandas_i, iris[:total_len - len(data_pandas_i)]])
        data_pandas.append(data_pandas_i)
        data_numpy.append(data_pandas_i.to_records())

    # In [4]:
    print('Input sizes [count]:')
    print(f'{&quot;#&quot;:&gt;4} {&quot;pandas&quot;:&gt;9} {&quot;numpy&quot;:&gt;9}')
    for i, (data_pandas_i, data_numpy_i) in enumerate(zip(data_pandas, data_numpy)):
        print(f'{i:&gt;4} {len(data_pandas_i):&gt;9} {len(data_numpy_i):&gt;9}')
    print()

    # In [5]:
    mb_size_in_bytes = 1024 * 1024
    print('Data sizes [MB]:')
    print(f'{&quot;#&quot;:&gt;4} {&quot;pandas&quot;:&gt;9} {&quot;numpy&quot;:&gt;9}')
    for i, (data_pandas_i, data_numpy_i) in enumerate(zip(data_pandas, data_numpy)):
        print(f'{i:&gt;4} {int(sys.getsizeof(data_pandas_i) / mb_size_in_bytes):&gt;9} {int(sys.getsizeof(data_numpy_i) / mb_size_in_bytes):&gt;9}')
    print()

    # In [6]:
    print(data_pandas[0].head())
    print()

    # In [7]:
    # ...

    # In [8]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.loc[:, &quot;sepal_length&quot;].mean()',
        data_numpy,
        'res = numpy.mean(data.sepal_length)',
        'Mean on Unfiltered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [9]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = numpy.log(data.loc[:, &quot;sepal_length&quot;])',
        data_numpy,
        'res = numpy.log(data.sepal_length)',
        'Vectorised log on Unfiltered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [10]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.loc[:, &quot;species&quot;].unique()',
        data_numpy,
        'res = numpy.unique(data.species)',
        'Unique on Unfiltered String Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [11]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.loc[(data.sepal_width &gt; 3) &amp; (data.petal_length &lt; 1.5), &quot;sepal_length&quot;].mean()',
        data_numpy,
        'res = numpy.mean(data[(data.sepal_width &gt; 3) &amp; (data.petal_length &lt; 1.5)].sepal_length)',
        'Mean on Filtered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [12]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = numpy.log(data.loc[(data.sepal_width &gt; 3) &amp; (data.petal_length &lt; 1.5), &quot;sepal_length&quot;])',
        data_numpy,
        'res = numpy.log(data[(data.sepal_width &gt; 3) &amp; (data.petal_length &lt; 1.5)].sepal_length)',
        'Vectorised log on Filtered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [13]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data[data.species == &quot;setosa&quot;].sepal_length.mean()',
        data_numpy,
        'res = numpy.mean(data[data.species == &quot;setosa&quot;].sepal_length)',
        'Mean on (String) Filtered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [14]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.petal_length * data.sepal_length + data.petal_width * data.sepal_width',
        data_numpy,
        'res = data.petal_length * data.sepal_length + data.petal_width * data.sepal_width',
        'Vectorized Math on Unfiltered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [16]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.loc[data.sepal_width * data.petal_length &gt; data.sepal_length, &quot;sepal_length&quot;].mean()',
        data_numpy,
        'res = numpy.mean(data[data.sepal_width * data.petal_length &gt; data.sepal_length].sepal_length)',
        'Vectorized Math in Filtering Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    end_time = time.perf_counter_ns()
    print(f'Total run time: {(end_time - start_time) / 10 ** 9:.3f} s')

if __name__ == '__main__':
    _main()
</code></pre>
<p>Here is the console output it generates:</p>
<pre class=""lang-none prettyprint-override""><code>Input sizes [count]:
   #    pandas     numpy
   0     10000     10000
   1    100000    100000
   2    500000    500000
   3   1000000   1000000
   4   5000000   5000000
   5  10000000  10000000
   6  15000000  15000000

Data sizes [MB]:
   #    pandas     numpy
   0         0         0
   1         9         4
   2        46        22
   3        92        45
   4       464       228
   5       928       457
   6      1392       686

   sepal_length  sepal_width  petal_length  petal_width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa

Mean on Unfiltered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.061           0.033           1.855
         100000           0.160           0.148           1.081
         500000           0.653           1.074           0.608
        1000000           1.512           2.440           0.620
        5000000          11.633          12.558           0.926
       10000000          23.954          25.360           0.945
       15000000          35.362          40.108           0.882

Vectorised log on Unfiltered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.124           0.056           2.190
         100000           0.507           0.493           1.029
         500000           3.399           3.441           0.988
        1000000           5.396           6.867           0.786
        5000000          27.187          38.121           0.713
       10000000          55.497          72.609           0.764
       15000000          88.406         112.199           0.788

Unique on Unfiltered String Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.332           1.742           0.191
         100000           2.885          21.833           0.132
         500000          14.769         125.961           0.117
        1000000          29.687         264.521           0.112
        5000000         147.359        1501.378           0.098
       10000000         295.118        3132.478           0.094
       15000000         444.365        4882.316           0.091

Mean on Filtered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.355           0.130           2.719
         100000           0.522           0.672           0.777
         500000           1.797           4.824           0.372
        1000000           4.602          10.827           0.425
        5000000          22.116          57.945           0.382
       10000000          43.076         116.028           0.371
       15000000          68.893         177.658           0.388

Vectorised log on Filtered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.361           0.128           2.821
         100000           0.576           0.758           0.760
         500000           2.066           5.199           0.397
        1000000           5.259          11.523           0.456
        5000000          22.785          59.581           0.382
       10000000          47.527         121.882           0.390
       15000000          75.080         187.954           0.399

Mean on (String) Filtered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.636           0.192           3.304
         100000           4.068           1.743           2.334
         500000          20.954           9.306           2.252
        1000000          41.938          18.522           2.264
        5000000         217.254          97.929           2.218
       10000000         434.242         197.289           2.201
       15000000         657.205         297.919           2.206

Vectorized Math on Unfiltered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.168           0.049           3.415
         100000           0.385           0.338           1.140
         500000           3.193           5.018           0.636
        1000000           6.028           9.539           0.632
        5000000          32.640          48.235           0.677
       10000000          69.748          99.893           0.698
       15000000         107.528         159.040           0.676

Vectorized Math in Filtering Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.350           0.234           1.500
         100000           0.926           2.494           0.371
         500000           6.093          15.007           0.406
        1000000          12.641          30.021           0.421
        5000000          71.714         163.060           0.440
       10000000         145.373         326.206           0.446
       15000000         227.817         490.991           0.464

Total run time: 183.198 s
</code></pre>
<p>And here are the plots it generated:</p>
<p><a href=""https://i.sstatic.net/rELP0TIk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rELP0TIk.png"" alt=""mean on unfiltered column"" /></a></p>
<hr />
<p><a href=""https://i.sstatic.net/wHFUPbY8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wHFUPbY8.png"" alt=""vectorised log on unfiltered column"" /></a></p>
<hr />
<p><a href=""https://i.sstatic.net/ZMtc3GmS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZMtc3GmS.png"" alt=""unique on unfiltered string column"" /></a></p>
<hr />
<p><a href=""https://i.sstatic.net/IxC6dB0W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IxC6dB0W.png"" alt=""mean on filtered column"" /></a></p>
<hr />
<p><a href=""https://i.sstatic.net/CObuIork.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CObuIork.png"" alt=""vectorised log on filtered column"" /></a></p>
<hr />
<p><a href=""https://i.sstatic.net/LjXEyYdr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LjXEyYdr.png"" alt=""mean on string filtered column"" /></a></p>
<hr />
<p><a href=""https://i.sstatic.net/p16ibJfg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/p16ibJfg.png"" alt=""vectorised math on unfiltered column"" /></a></p>
<hr />
<p><a href=""https://i.sstatic.net/g1kwPGIz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/g1kwPGIz.png"" alt=""vectorised math in filtering column"" /></a></p>
<hr />
<p>These results were generated with Windows 10, Python 3.13, on i9-10900K, and never got close to running out of memory so swap should not be a factor.</p>
","1","Answer"
"79366830","79366813","<p>The simplest way to solve this is to create a new virtual environment, VScode usually recognizes that a new venv has been created and allows you to switch to that new environment.</p>
<p>in your vscode terminal, run <code>rm -r {your env folder}</code>, then <code>python venv .venv</code> (the name should be &quot;.venv&quot;, sometimes it helps vscode recognize that this is a venv). You should receive a small modal at the bottom right of the screen for switching to the new environment. If you don't receive it, use <code>source ./.venv/Scripts/Activate</code>.</p>
<p>If all of this doesn't work, try running <code>wsl code .</code> to open vscode using wsl. It's very much possible that vscode is looking for a windows venv and fails to find a linux venv.</p>
","0","Answer"
"79367049","79366943","<p>You must be looking for stack():</p>
<pre class=""lang-py prettyprint-override""><code>    df = pd.DataFrame(np.arange(12).reshape((4,3)), columns=list(&quot;ABC&quot;))
    
       A   B   C
    0  0   1   2
    1  3   4   5
    2  6   7   8
    3  9  10  11
    
    res = (df.stack()
             .reset_index(level=1)
             .sort_values(by=&quot;level_1&quot;)
             .reset_index(drop=True)
             .rename(columns={&quot;level_1&quot;:&quot;labels&quot;, 0:&quot;entries&quot;})
    )
    
       labels  entries
    0       A        0
    1       A        3
    2       A        6
    3       A        9
    4       B        1
    5       B        4
    6       B        7
    7       B       10
    8       C        2
    9       C        5
    10      C        8
    11      C       11
</code></pre>
","2","Answer"
"79367303","79366709","<p>Your <code>rows</code> variable is a list of tuples, so you want to use the <code>executemany</code> function to insert all of the rows.</p>
<pre><code>rows = [tuple(x) for x in dataf.values]
insert_sql = &quot;INSERT INTO Payments VALUES (?, ?, ?, ?)&quot;
cursor.executemany(insert_sql, rows)
cursor.commit()
</code></pre>
","0","Answer"
"79367492","79360275","<p>As @AlwaysLearning pointed out, the option to load some or all table information from an existing database is called reflection (<a href=""https://docs.sqlalchemy.org/en/20/tutorial/metadata.html#table-reflection"" rel=""nofollow noreferrer"">https://docs.sqlalchemy.org/en/20/tutorial/metadata.html#table-reflection</a>).</p>
<pre><code>from sqlalchemy import select, Table, MetaData, create_engine, URL
import pandas as pd

metadata_obj = MetaData()
url_object = URL.create(
    &quot;mssql+pyodbc&quot;,
    host=&quot;abgsql.xx.xx.ac.uk&quot;,
    database=&quot;ABG&quot;,
    query={
        &quot;driver&quot;: &quot;ODBC Driver 18 for SQL Server&quot;,
        &quot;TrustServerCertificate&quot;: &quot;yes&quot;,
    },
)
engine = create_engine(url_object)
products = Table(&quot;products&quot;, metadata_obj, autoload_with=engine)
stmt = select(products)
df = pd.read_sql(sql=stmt, con=engine)
</code></pre>
","0","Answer"
"79369253","79356690","<p>You can first construct the column header then get the expected condition by <code>crosstab</code> the index and column header.</p>
<pre><code>col = df.loc[df['X'] &gt; 1, 'X'].map('V{}'.format)
m = pd.crosstab(col.index, col).astype(bool)
</code></pre>
<pre><code>print(m)

X         V2     V3
row_0
0       True  False
1      False   True
3      False   True
4      False   True
6       True  False
</code></pre>
<p>Finally, you can either boolean indexing the dataframe or mask the dataframe.</p>
<pre><code>df[m] = 9

# or

out = df.mask(m.reindex_like(df).fillna(0).astype(bool), 9)
</code></pre>
<pre><code>print(out)

   EMPLID  V1  V2  V3  X
0      12   2   9   7  2
1      13   3   3   9  3
2      14   4   3   8  1
3      15  50   3   9  3
4      16   6   3   9  3
5      17   7   3  11  1
6      18   8   9  12  2
</code></pre>
","1","Answer"
"79371167","79356088","<p><code>Connection.exec_driver_sql</code> was added to SQLAlchemy in the 1.4 series.</p>
<p>if you want to use SQLA 1.3.x with Pandas you will need to install Pandas 1.3 (this likely to cause a lot of dependency management work because it is so old.</p>
<p>If you want to use Pandas 2.0.x then you will need to install SQLAlchemy 1.4.16.</p>
<p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/2.0/getting_started/install.html"" rel=""nofollow noreferrer"">Pandas installation docs</a> which lists compatible dependency versions (there is a version dropdown at the top right of the page).</p>
","0","Answer"
"79377165","79362783","<p>Interesting issue. I am able to reproduce it, and it seems it's related how data is ingested by modin-pandas and (regular) pandas.</p>
<p>According to my tests, write_pandas uses COPY INTO command, and specify the column names. Therefore you don't get any error even if your dataframe doesn't have all columns in the table.</p>
<p>The modin-pandas uses INSERT INTO command, and it doesn't specify the column names, therefore your dataframe should have all the columns in the table.</p>
<p>Please submit a Support case, explain the business impact and the error you get. So Support can engage Engineering to fix this issue.</p>
","1","Answer"
"79645279","79346496","<p>Install punkt_tab separitly by using nltk.download('punkt_tab')</p>
","0","Answer"
"79367395","79367366","<p>Given your description, you could use <a href=""https://numpy.org/doc/2.1/reference/generated/numpy.select.html"" rel=""nofollow noreferrer""><code>np.select</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html"" rel=""nofollow noreferrer""><code>ffill</code></a>. This way you'll ensure that even if you have multiple peaks before a valley or conversely this will keep the order:</p>
<pre><code>m1 = df['valley'].eq(1)
m2 = df['peak'].eq(1)
df['grp'] = pd.Series(np.select([m1, m2], ['A', 'B'], pd.NA),
                      index=df.index).ffill()
</code></pre>
<p>Variant with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.case_when.html"" rel=""nofollow noreferrer""><code>case_when</code></a>:</p>
<pre><code>m1 = df['valley'].eq(1)
m2 = df['peak'].eq(1)
df['grp'] = df['valley'].case_when([(m1, 'A'), (m2, 'B'), (~(m1|m2), pd.NA)]
                                   ).ffill()
</code></pre>
<p>Or with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.from_dummies.html"" rel=""nofollow noreferrer""><code>from_dummies</code></a> after adding a new column:</p>
<pre><code>df['grp'] = (pd.from_dummies(df[['valley', 'peak']]
                             .assign(other=df[['valley', 'peak']]
                                     .sum(axis=1).rsub(1)))
               .squeeze().map({'valley': 'A', 'peak': 'B'}).ffill()
            )
</code></pre>
<p>Or with reshaping:</p>
<pre><code>df['grp'] = (df[['valley', 'peak']]
 .rename_axis(columns='col')
 .replace(0, pd.NA).stack().reset_index(-1, name='val')
 .replace({'col': {'valley': 'A', 'peak': 'B'}})['col']
 .reindex(df.index).ffill()
)
</code></pre>
<p>Alternatively, if there is always a valley then peak then valley... you could use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.select.html"" rel=""nofollow noreferrer""><code>cumsum</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.mod.html"" rel=""nofollow noreferrer""><code>mod</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> the group after identify which one of peak/valley is the first:</p>
<pre><code>df['grp'] = (df[['valley', 'peak']].max(axis=1).cumsum()
            .add(df[['valley', 'peak']].idxmax().idxmin() == 'peak')
            .mod(2).map({0: 'B', 1: 'A'})
           )
</code></pre>
<p>Output:</p>
<pre><code>     x     y  valley  peak grp
0    1  5.69       1     0   A
1    2  6.03       0     0   A
2    3  6.03       0     1   B
3    4  6.03       0     0   B
4    5  6.03       0     0   B
5    6  6.03       0     0   B
6    7  6.03       0     0   B
7    8  5.38       0     0   B
8    9  5.21       1     0   A
9   10  5.40       0     0   A
10  11  5.24       0     0   A
11  12  5.40       0     0   A
12  13  5.36       0     0   A
13  14  5.47       0     0   A
14  15  5.58       0     0   A
15  16  5.50       0     0   A
16  17  5.61       0     1   B
17  18  5.53       0     0   B
18  19  5.40       0     0   B
19  20  5.51       0     0   B
20  21  5.47       0     0   B
21  22  5.44       0     0   B
22  23  5.39       0     0   B
23  24  5.27       0     0   B
24  25  5.38       0     0   B
25  26  5.35       0     0   B
26  27  5.32       0     0   B
27  28  5.09       1     0   A
</code></pre>
","1","Answer"
"79367402","79367389","<p>Pandas <code>.str.contains</code> performs a regex search rather than a substring search, by default. That means that characters like <code>*</code> or <code>+</code> get treated as regex metacharacters instead of a literal asterisk or plus sign.</p>
<p>It looks like you're trying to perform a substring search, not a regex search. Your <code>x</code> isn't a valid regex, and even if it was, it wouldn't mean what you want. You need to specify <code>regex=False</code>:</p>
<pre><code>dataframe[x] = dataframe[column].str.contains(x, na=False, regex=False).astype(int)
</code></pre>
","0","Answer"
"79367743","79367707","<p>First of all, you have to do:</p>
<pre><code>newIndex, indexer = a.reindex(b)
</code></pre>
<p><code>reindex</code> returns two things. You need/want to get only the indexer.</p>
<p>So now you can get what you want:</p>
<pre><code>indexerWithNan = np.where(indexer == -1, np.nan, indexer)
</code></pre>
<p>Which is:</p>
<pre><code>[ 0.  1.  2.  3.  4.  5. nan nan nan  6.  7.]
</code></pre>
<p>Why was your initial code wrong?  The <code>reindex()</code> method does not support the <code>fill_value</code> parameter for <code>pandas.Index</code> objects as it does for <code>pandas.Series</code> or <code>pandas.DataFrame</code>.</p>
","1","Answer"
"79367754","79367707","<p>If you just want to know which values of <code>b</code> are missing from <code>a</code> use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.isin.html"" rel=""nofollow noreferrer""><code>Index.isin</code></a>:</p>
<pre><code>~b.isin(a)
</code></pre>
<p>Output:</p>
<pre><code>array([False, False, False, False, False, False,  True,  True,  True,
       False, False])
</code></pre>
<p>If you want the missing values:</p>
<pre><code>b[~b.isin(a)]
</code></pre>
<p>Output:</p>
<pre><code>DatetimeIndex(['2006-01-01', '2007-01-01', '2008-01-01'], dtype='datetime64[ns]', freq=None)
</code></pre>
<p>and for the indices:</p>
<pre><code>np.where(~b.isin(a))[0]
</code></pre>
<p>Output: <code>array([6, 7, 8])</code></p>
","1","Answer"
"79368666","79368140","<p>If you look at the signature of the <code>generate</code> function in the <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/modeling_blip.py#L1153"" rel=""nofollow noreferrer"">source</a> code, you will find what arguments it expects:</p>
<pre><code>def generate(
    self,
    pixel_values: torch.FloatTensor,
    input_ids: Optional[torch.LongTensor] = None,
    attention_mask: Optional[torch.LongTensor] = None,
    interpolate_pos_encoding: bool = False,
    **generate_kwargs,
) -&gt; torch.LongTensor:
</code></pre>
<p>And if you print the dictionnary that returns the preprocess step (<code>print(input_descriptions)</code>), you will find the first arguments expected by the generate function: ['pixel_values', 'input_ids', 'attention_mask']. In particular in your code the first argument is input_ids (dim. 2) whereas it should be pixel_values (dim. 4). The following call should work:</p>
<pre><code>description_ids = model.generate(
        **inputs_description,
        max_length=15,
        num_beams=5,
        early_stopping=True
    )
</code></pre>
<p>and similarly for the other call to generate().</p>
","1","Answer"
"79369180","79369011","<p>It is generally not a very good idea to use multiple <code>ColumnTransformer()</code> stages in a row as the column index control becomes tedious, if even possible.</p>
<p>I'd advise wrapping transforms into a <code>Pipeline()</code> for each transformation group and thus sticking to a single <code>ColumnTransformer()</code>.
Wrapping a scaler into a <code>ColumnTransformer()</code> is also redundant (and the effect of a scaler on a tree model is likely insignificant).</p>
<p>Revised code, assuming the titanic column naming:</p>
<pre><code>from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Step 1: Preprocessing
trf1 = ColumnTransformer(
    [
        ('impute', SimpleImputer(), ['Age']),
        ('impute_and_encode', Pipeline([
            ('impute', SimpleImputer(strategy='most_frequent')),
            ('encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')),
        ]), ['Embarked']),
        ('encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first'), ['Sex']),
    ],
    remainder='passthrough',
    force_int_remainder_cols=False,
)

# Step 2: Scaling
trf2 = MinMaxScaler()

# Step 3: Classifier
trf3 = DecisionTreeClassifier(random_state=177013)

# Create pipeline
pipe = Pipeline([
    ('trf1', trf1),  # Step 1: Preprocessing
    ('trf2', trf2),  # Step 2: Scaling
    ('trf3', trf3)   # Step 3: Model
])

# Fit the pipeline
pipe.fit(X_train, y_train)
</code></pre>
","0","Answer"
"79369191","79369190","<p>You would need to pipe <code>df</code> into the styler first, and then chain on <a href=""https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.hide.html"" rel=""nofollow noreferrer""><code>.hide</code></a>, whereat you select a random subset of rows and hide the rest using <code>.hide(df.sample(frac=1.).index[k:])</code>.</p>
<p><code>.hide</code> doesn't take <code>lambda</code> functions, so you can't shuffle before <code>.style</code> and then access the shuffled DataFrame later in the chain.</p>
<pre class=""lang-py prettyprint-override""><code>#... data from OP
(
    df
    .style
    .background_gradient(subset='severity_level', cmap='RdYlGn')

    #Shuffle and select k indices (by hiding rows coming after k)
    .hide(df.sample(frac=1.).index[k:])
)
</code></pre>
<blockquote>
<p>A value of 0 should therefore display as white, but it gets coloured red because the styler only gets part of the data</p>
</blockquote>
<p>The styler now uses all values of <code>severity_level</code>, irrespective of the sample displayed</p>
<p><a href=""https://i.sstatic.net/WnNpCWwX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WnNpCWwX.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/UDaRYKOE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UDaRYKOE.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79371033","79370983","<p>You might want to use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html"" rel=""nofollow noreferrer""><code>str.replace</code></a>:</p>
<pre><code>df['new_file'] = df['file'].str.replace(r'^(/(?:[^/]+/){5}).*((?:/[^/]+){4})$',
                                        r'\1...\2', regex=True)
</code></pre>
<p>Output:</p>
<pre><code>                               file                  new_file
0  /a/b/c/d/e/f/g/h/i/j/k/l/m/n/a.c  /a/b/c/d/e/.../l/m/n/a.c
1                  /a/b/c/d/e/x/b.c          /a/b/c/d/e/x/b.c
</code></pre>
<p><a href=""https://regex101.com/r/O9m4vg/2"" rel=""nofollow noreferrer"">regex demo</a></p>
","0","Answer"
"79371048","79370983","<p>Using <a href=""https://docs.python.org/3/library/pathlib.html"" rel=""nofollow noreferrer"">pathlib</a>, you can parse the path as a filesystem path:</p>
<pre><code>&gt;&gt;&gt; import pathlib
&gt;&gt;&gt; path = pathlib.Path(&quot;/a/b/c/d/e/f/g/h/i/j/k/l/m/n/a.c&quot;)
</code></pre>
<p>Now you can access it as an object:</p>
<pre><code>&gt;&gt;&gt; path.parts[0:4]
('/', 'a', 'b', 'c')
&gt;&gt;&gt; path.parts[-4:]
('l', 'm', 'n', 'a.c')
</code></pre>
<p>And with that, you can format the path as you wish (not the most elegant code, but you probably catch my drift):</p>
<pre><code>f&quot;{os.path.join(*path.parts[0:4])}...{os.path.join(*path.parts[-5:])}&quot;
&gt;&gt;&gt; '/a/b/c...k/l/m/n/a.c'
</code></pre>
","1","Answer"
"79371282","79371263","<p>Assuming you don't have duplicated combinations of PERSONALNR/PERIODE in POOL.</p>
<p>You can use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a> and <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a>:</p>
<pre><code>mask = LA['DA'].eq('01') &amp; LA['KSTBEZ'].str.contains('pool')
tmp = (LA[['PERSONALNR', 'PERIODE']].reset_index()
       .merge(POOL, on=['PERSONALNR', 'PERIODE'], how='left')
       .set_index('index')
      )

LA.loc[mask, ['MANDANT', 'DA']] = tmp.loc[mask, ['MANDANT', 'DA']]
</code></pre>
<p>Output (using a trailing <code>x</code> in the MANDANT/DA values from POOL for the demo):</p>
<pre><code>  PERSONALNR PERIODE MANDANT   DA         KSTBEZ
0  000009461  202401     LBx  01x  Springer pool
1  000009461  202402      LB   01        bla bla
</code></pre>
<h4>Why do we need <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a>?</h4>
<p>After the <code>merge</code>, the index is lost, which wouldn't allow correct alignment with the mask or for the assignment to the input <code>LA</code>.</p>
<p>Example:</p>
<pre><code>LA = pd.DataFrame({'PERSONALNR': ['000009461', '000009461'],
                   'PERIODE': ['202401', '202402'],
                   'MANDANT': ['LB', 'LB'],
                   'DA': ['01', '01'],
                   'KSTBEZ': ['Springer pool', 'bla bla']},
                  index=[2, 0]) # Note the non-range index
POOL = pd.DataFrame({'PERSONALNR': ['000009461', '000009461'],
                     'PERIODE': ['202401', '202402'],
                     'MANDANT': ['LBx', 'LBx'],
                     'DA': ['01x', '01x']})

mask = LA['DA'].eq('01') &amp; LA['KSTBEZ'].str.contains('pool')
tmp = (LA[['PERSONALNR', 'PERIODE']]#.reset_index()
       .merge(POOL, on=['PERSONALNR', 'PERIODE'], how='left')
       #.set_index('index')
      )

LA.loc[mask, ['MANDANT', 'DA']] = tmp.loc[mask, ['MANDANT', 'DA']]
</code></pre>
<p>Output:</p>
<pre><code>IndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).
</code></pre>
","0","Answer"
"79372843","79372381","<p>To resolve your issue, please follow below code. For the sample, I use the above four columns as a data frame and convert it into a Temp table.</p>
<p><strong>Code:</strong></p>
<pre><code>from pyspark.sql.functions import col
from pyspark.sql.types import DecimalType, FloatType

df1 = spark.sql(&quot;&quot;&quot;
    SELECT 'text' AS txt, 
           CAST(1.1111 AS DECIMAL(5,4)) AS one, 
           CAST(2.22222 AS DECIMAL(6,5)) AS two, 
           CAST(3.333333333333 AS FLOAT) AS three
&quot;&quot;&quot;)

df1.createOrReplaceTempView(&quot;deci_table&quot;)

def convert_decimal_to_float_from_table(table_name):
    df12 = spark.sql(f&quot;SELECT * FROM {table_name}&quot;)

    # check the decimal columns values
    decimal_columns = [field.name for field in df12.schema.fields if isinstance(field.dataType, DecimalType)]

    # using below for loop you can convert decimal columns to float 
    for col_name in decimal_columns:
        df12 = df12.withColumn(col_name, col(col_name).cast(FloatType()))

    return df12

df1.printSchema()
df12_conv = convert_decimal_to_float_from_table(&quot;deci_table&quot;)

df12_conv.printSchema()

df_pd = df12_conv.toPandas()
print(df_pd.dtypes)
</code></pre>
<p>Output:</p>
<p><img src=""https://i.imgur.com/QeCFqoE.png"" alt=""enter image description here"" /></p>
","0","Answer"
"79373361","79373355","<p>You can use a combination of <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.diff.html"" rel=""nofollow noreferrer""><code>groupby.diff</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer""><code>fillna</code></a> to achieve this. We compare the row difference with 0 to find any rows where <code>measure</code> changed:</p>
<pre class=""lang-py prettyprint-override""><code>test_df['measure_change'] = test_df.groupby('item')['measure'].diff().fillna(0) != 0
</code></pre>
<p>Result:</p>
<pre><code>    item  measure  measure_change
0     20        1           False
1     20        1           False
2     20        1           False
3     20        3            True
4     20        3           False
5     20        3           False
6     20        3           False
7     20        3           False
8     30        6           False
9     30        6           False
10    30        6           False
11    30        6           False
12    30        6           False
13    30        7            True
14    30        7           False
15    30        7           False
16    40       10           False
17    40       10           False
18    40       10           False
19    40       10           False
20    40       10           False
21    40       10           False
22    40       10           False
23    40       10           False
</code></pre>
<hr />
<p>Alternativly, if you have strings to compare as well you can add a secondary condition checking the <code>shift</code> value for nans: <code>x.shift().notna()</code>.</p>
<pre class=""lang-py prettyprint-override""><code>test_df['measure_change'] = test_df.groupby('item')['measure'].transform(lambda x: (x != x.shift()) &amp; (x.shift().notna()))
</code></pre>
","3","Answer"
"79373662","79372190","<p>As mentioned in the comments, content is dynamically reloaded - In order to access it via <code>requests</code>, you would have to replicate the mechanism, but it would be easier to do it via an official api or <code>selenium</code>.</p>
<p>Here is an approach that calls the page initil and takes over the paging as long as a next button is available. At the end, the tables are concatenated into a final dataframe:</p>
<pre><code>import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, StaleElementReferenceException


driver = webdriver.Chrome()

url = f'https://bvmf.bmfbovespa.com.br/clube-de-investimento/clube-de-investimento.aspx?Idioma=pt-br'
driver.get(url)

data = []

while True:
    try:
        WebDriverWait(driver, 30).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, 'a[id=&quot;ctl00_contentPlaceHolderConteudo_tabIbovespa_TabAdmnistradores&quot;]'))
        ).click()

        table = pd.read_html(driver.page_source)[0]
        data.append(table)

        next_button = WebDriverWait(driver, 30).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, 'input[id=&quot;ctl00_contentPlaceHolderConteudo_grdAtivo_ctl01_ctl03_ctl01_ctl00_lnkNext&quot;]'))
        )
        driver.execute_script(&quot;arguments[0].click();&quot;, next_button)

    except TimeoutException:
        print(&quot;no next page element&quot;)
        break
    except StaleElementReferenceException:
        print(&quot;dom chenged try again&quot;)
        continue


driver.close()

result = pd.concat(data, ignore_index=True)
result[(~result['Administrador'].str.endswith('...')) &amp; (~result['Administrador'].str.startswith('...'))]
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">Clube de Investimento</th>
<th style=""text-align: left;"">Administrador</th>
<th style=""text-align: left;"">CNPJ</th>
<th style=""text-align: right;"">Nº Registro</th>
<th style=""text-align: left;"">Registrado em</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">A40 Clube de Investimento</td>
<td style=""text-align: left;"">XP INVESTIMENTOS CCTVM S/A</td>
<td style=""text-align: left;"">53.086.076/0001-48</td>
<td style=""text-align: right;"">9058</td>
<td style=""text-align: left;"">19/10/2023</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: left;"">Abaetê Clube de Investimento</td>
<td style=""text-align: left;"">XP INVESTIMENTOS CCTVM S/A</td>
<td style=""text-align: left;"">52.143.244/0001-27</td>
<td style=""text-align: right;"">9006</td>
<td style=""text-align: left;"">04/05/2023</td>
</tr>
<tr>
<td style=""text-align: right;"">...</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: right;""></td>
<td style=""text-align: left;""></td>
</tr>
<tr>
<td style=""text-align: right;"">4968</td>
<td style=""text-align: left;"">Zodiacus Clube de Investimentos</td>
<td style=""text-align: left;"">BTG PACTUAL SERVICOS FINANCEIROS S/A DTVM</td>
<td style=""text-align: left;"">43.193.144/0001-14</td>
<td style=""text-align: right;"">8742</td>
<td style=""text-align: left;"">11/08/2021</td>
</tr>
<tr>
<td style=""text-align: right;"">4969</td>
<td style=""text-align: left;"">ZTA CLUBE DE INVESTIMENTO</td>
<td style=""text-align: left;"">GUIDE INVESTIMENTOS S.A. CV</td>
<td style=""text-align: left;"">58.848.525/0001-61</td>
<td style=""text-align: right;"">9171</td>
<td style=""text-align: left;"">17/10/2024</td>
</tr>
</tbody>
</table></div>
","2","Answer"
"79374058","79370034","<p>I would not suggest using df_sample.Date in Pandas because it can lead to errors due to potential existing attributes in the library.</p>
<p>Try this column assignment:</p>
<pre><code>df_sample[&quot;Month&quot;] = df_sample[&quot;Date&quot;].dt.month
</code></pre>
<p>Also, the last line of code sort_values() returns a sorted column but might not update the data frame. This can be solved this way:</p>
<pre><code>df_sample[&quot;Month&quot;] = df_sample[&quot;Month&quot;].sort_values()
</code></pre>
","0","Answer"
"79374687","79374674","<p>Your function will receive a 1D numpy array (per column), it should be vectorized and return a boolean 1D array (<em>callable(1d-array) -&gt; bool 1d-array</em>).</p>
<p>Use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.isin.html"" rel=""nofollow noreferrer""><code>numpy.isin</code></a> to test membership:</p>
<pre><code>def condition(x):
    &quot;&quot;&quot;condition function to update only cells matching the conditions&quot;&quot;&quot;
    return np.isin(x, [2, 7, 9])

df1.update(df2, filter_func=condition)
</code></pre>
<p>with a lambda:</p>
<pre><code>df1.update(df2, filter_func=lambda x: np.isin(x, [2, 7, 9]))
</code></pre>
<p>Alternatively, if you really can't vectorize with pure numpy functions (<strong>this should not be done here!</strong>), decorate with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html"" rel=""nofollow noreferrer""><code>numpy.vectorize</code></a>:</p>
<pre><code>@np.vectorize
def condition(x: Any) -&gt; bool:
    &quot;&quot;&quot;condition function to update only cells matching the conditions&quot;&quot;&quot;
    return True if x in [2, 7, 9] else False

df1.update(df2, filter_func=condition)
</code></pre>
<p>Updated <code>df1</code>:</p>
<pre><code>   A  B
0  1  4
1  8  5
2  3  6
</code></pre>
","3","Answer"
"79374766","79374600","<p>First of all, <a href=""https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html"" rel=""nofollow noreferrer""><code>approxQuantile</code></a> computes an approximation of the quantiles if <code>relativeError</code> is not <code>0</code>.</p>
<p>In your case, since you're using <code>0</code>, it looks like you can get the same value with <code>interpolation='lower'</code>.</p>
<pre><code>np.random.seed(0)
pdf = pd.DataFrame({'A': np.random.random(1000)})
sdf = spark.createDataFrame(pdf)

quantiles = [0.2, 0.5, 0.8]

np.allclose(pdf['A'].quantile(quantiles, interpolation='lower'),
            sdf.approxQuantile('A', quantiles, 0)
           ) # True
</code></pre>
<p><em>Note that using <code>sdf.pandas_api().quantile(percentiles)</code> still goes with the pyspark function.</em></p>
","1","Answer"
"79375611","79375533","<p>You were not so far, you can expand the values with:</p>
<pre><code>RequiredDataframe1 = data[['Number', 'Age', *object_cols.columns.values]]
</code></pre>
<p>Or actually, just:</p>
<pre><code>RequiredDataframe1 = data[['Number', 'Age', *object_cols]]
</code></pre>
<p>Or use an Index <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.union.html"" rel=""nofollow noreferrer""><code>union</code></a>:</p>
<pre><code>RequiredDataframe1 = data[pd.Index(['Number', 'Age']).union(object_cols.columns, sort=False)]
</code></pre>
<p>Output:</p>
<pre><code>   Number   Age           Name            Team Position Height            College
0     0.0  25.0  Avery Bradley  Boston Celtics       PG    6-2              Texas
1    99.0  25.0    Jae Crowder  Boston Celtics       SF    6-6          Marquette
2    30.0  27.0   John Holland  Boston Celtics       SG    6-5  Boston University
3    28.0  22.0    R.J. Hunter  Boston Celtics       SG    6-5      Georgia State
4     8.0  29.0  Jonas Jerebko  Boston Celtics       PF   6-10                NaN
...
</code></pre>
","1","Answer"
"79375895","79372190","<p>You can get all the pages using requests &amp; BeautifulSoup, without selenium:</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import pandas as pd


def extract_all_tables(url):
    requests.packages.urllib3.disable_warnings()
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36',
    }
    data = {}

    tables = []
    while True:
        print(f'Scraping table #{len(tables) + 1}')
        response = requests.post(url, headers=headers, data=data, verify=False)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        table = soup.select_one('table#ctl00_contentPlaceHolderConteudo_grdAtivo_ctl01')
        header_row = [th.get_text(strip=True) for th in table.thead.select('th')]
        data_rows = [[td.get_text(strip=True) for td in tr.select('td')] for tr in table.tbody.select('tr')]
        
        df = pd.DataFrame(data_rows, columns=header_row)
        tables.append(df)

        next_button = table.tfoot.select_one('td.ProximoPaginacao &gt; input')
        if not next_button:
            break

        data['__VIEWSTATE'] = soup.select_one('input#__VIEWSTATE').get('value')
        data['__EVENTTARGET'] = next_button.get('name').replace('$', ':')

    return tables


url = 'https://bvmf.bmfbovespa.com.br/clube-de-investimento/clube-de-investimento.aspx?idioma=pt-br'
tables = extract_all_tables(url)
print(f'{len(tables) = }')
</code></pre>
","1","Answer"
"79376674","79376634","<p>Frirst, combine all unique <code>from</code> and <code>to</code> values from both <code>df1</code> and <code>df2</code> to create a set of breakpoints:</p>
<pre class=""lang-py prettyprint-override""><code>breakpoints = set(df1['from']).union(df1['to']).union(df2['from']).union(df2['to'])
breakpoints = sorted(breakpoints)
</code></pre>
<p>In the example, this is <code>[0, 2, 8, 17, 26, 34, 35, 46, 48, 49]</code>. Now, create a new dataframe with these <code>from</code> and <code>to</code> values, then compute the intervals:</p>
<pre class=""lang-py prettyprint-override""><code>new_df = pd.DataFrame({'from': breakpoints[:-1], 'to': breakpoints[1:]})
new_df['int'] = new_df['to'] - new_df['from']
</code></pre>
<p>Result:</p>
<pre><code>  from  to  int
0    0   2    2
1    2   8    6
2    8  17    9
3   17  26    9
4   26  34    8
5   34  35    1
6   35  46   11
7   46  48    2
8   48  49    1
</code></pre>
","2","Answer"
"79377048","79377042","<p>You could use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dot.html"" rel=""nofollow noreferrer""><code>dot</code></a> product after renaming the columns with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html"" rel=""nofollow noreferrer""><code>str.replace</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.removeprefix.html"" rel=""nofollow noreferrer""><code>str.removeprefix</code></a>:</p>
<pre><code>tmp = df.drop(columns='ID')
df['Result'] = (tmp @ tmp.columns.str.replace('^f', '', regex=True)).str[1:]

# variant
df['Result'] = (tmp @ tmp.columns.str.removeprefix('f')).str[1:]
</code></pre>
<p>Alternatively,  a more classical pandas (much slower) approach with reshaping (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html#pandas.DataFrame.melt"" rel=""nofollow noreferrer""><code>melt</code></a>), filtering (with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html"" rel=""nofollow noreferrer""><code>query</code></a>), and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a>:</p>
<pre><code>df['Result'] = (df.melt('ID', ignore_index=False).query('value == 1')
                  .groupby(level=0)['variable']
                  .agg(lambda x: '_'.join(x.str.extract('_(\d+)', expand=False)))
               )
</code></pre>
<p>Or with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>stack</code></a>:</p>
<pre><code>s = df.drop(columns='ID').stack()
df['Result'] = (s[s==1].reset_index(-1).groupby(level=0)['level_1']
                .agg(lambda x: '_'.join(x.str.extract('_(\d+)', expand=False)))
               )
</code></pre>
<p>Output:</p>
<pre><code>   ID  f_1  f_2  f_3 Result
0   1    1    0    1    1_3
1   2    0    1    1    2_3
2   3    1    1    0    1_2
3   4    1    0    1    1_3
4   5    0    1    1    2_3
</code></pre>
<h4>Timings</h4>
<p>On 20K rows:</p>
<pre><code># dot product
2.96 ms ± 161 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# melt + groupby.agg
965 ms ± 59.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# stack + groupby.agg
928 ms ± 43.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<p>On 1M rows:</p>
<pre><code># dot product
359 ms ± 56.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# melt/stack + groupby.agg
did not run under a few minutes
</code></pre>
","3","Answer"
"79377920","79377876","<p>There can be more than one value that is the mode.</p>
<pre><code>inter = pd.DataFrame({'duration': [1, 2, 3, 1, 1, 2, 2]})
inter['duration'].mode()

# 0    1
# 1    2
# Name: duration, dtype: int64
</code></pre>
<p>If you want to pick the first one, just slice:</p>
<pre><code>inter['duration'].mode().iloc[0]
</code></pre>
<p>Example:</p>
<pre><code>inter = pd.DataFrame({'duration': [1, 2, 3, 1, 1, 3, 2]})
inter['duration'].mode().iloc[0]

# 1
</code></pre>
<p>Otherwise, you will need to decide what to do with the multiple values (e.g. aggregate them as the <code>mean</code> if this makes sense).</p>
","1","Answer"
"79378107","79378007","<p>Since your logic is iterative, a loop is indeed a way to go. You can change your function to use the status as 0/1 and to refer to the last result. If you're over the date either set 0 or increment the result:</p>
<pre><code>def get_flag(g, thresh=30):
    dates = pd.to_datetime(g['Date'])
    status = g['status'].eq('S').astype(int)
    ref = dates.iloc[0]
    s = status.iloc[0]
    result = [s]

    for i in range(1, len(g)):
        date = dates.iloc[i]
        stat = status.iloc[i]
        if (date - ref).days &gt;= thresh:
            result.append(result[-1]+stat if stat else 0)
            ref = date 
        else:
            result.append(result[-1])
    return g.assign(flag=result)
        
# groupby + apply + custom function    
out = df.groupby('ID', group_keys=False).apply(get_flag)
</code></pre>
<p>Output:</p>
<pre><code>     ID        Date status  flag
0   117  2023-11-14      S     1
1   117  2024-01-25      S     2
2   117  2024-02-01      S     2
3   117  2024-02-04      E     2
4   117  2024-02-11      E     2
5   117  2024-03-04      E     0
6   118  2024-01-02      E     0
7   118  2024-01-28      E     0
8   118  2024-02-04      S     1
9   118  2024-02-18      S     1
10  118  2024-03-11      S     2
11  118  2024-06-05      E     0
</code></pre>
","1","Answer"
"79379220","79378938","<p>If you want to <code>resample</code> with 5h periods except the last one, so that it always starts at midnight, you should <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.normalize.html"" rel=""nofollow noreferrer""><code>normalize</code></a> your dates and use them as grouper for <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.resample.html"" rel=""nofollow noreferrer""><code>groupby.resample</code></a>:</p>
<p><strong>Setup for this example</strong></p>
<pre><code>N = 1000
df = pd.DataFrame(np.random.random((N, 4)),
                  columns=['Open', 'High', 'Low', 'Close'],
                  index=pd.date_range('2024-01-21', periods=N, freq='10min')
                          .rename('Date')
                 )

d = {'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'}
</code></pre>
<p><strong>Code</strong></p>
<pre><code>out = (df.groupby(df.index.normalize(), group_keys=False)
         .resample('5h', origin='start_day').agg(d)
      )
</code></pre>
<p>Example output (first three days):</p>
<pre><code>                         Open      High       Low     Close
Date                                                       
2024-01-21 00:00:00  0.854852  0.991030  0.001056  0.434836
2024-01-21 05:00:00  0.423933  0.993614  0.024808  0.970102
2024-01-21 10:00:00  0.114825  0.992917  0.000970  0.423687
2024-01-21 15:00:00  0.368081  0.934378  0.019684  0.474279
2024-01-21 20:00:00  0.618224  0.991313  0.004855  0.828402
2024-01-22 00:00:00  0.314505  0.981417  0.015882  0.672777
2024-01-22 05:00:00  0.637003  0.928499  0.018432  0.081630
2024-01-22 10:00:00  0.798733  0.939966  0.002545  0.779787
2024-01-22 15:00:00  0.786353  0.980841  0.007235  0.622986
2024-01-22 20:00:00  0.455437  0.985423  0.207164  0.492761
2024-01-23 00:00:00  0.759215  0.943299  0.012884  0.610557
2024-01-23 05:00:00  0.491669  0.907216  0.022135  0.204769
2024-01-23 10:00:00  0.035259  0.950254  0.020136  0.533030
2024-01-23 15:00:00  0.977045  0.958560  0.000002  0.791182
2024-01-23 20:00:00  0.327000  0.953044  0.021118  0.559102
</code></pre>
","2","Answer"
"79379863","79379785","<p>I tried running below code, in windows with vscode on some dummy data I created. It will run only if you add engine = &quot;python&quot; as below:</p>
<pre><code>import pandas as pd

def handle_bad_line(bad_line: list[str]) -&gt; list[str] | None:
# Do something with the bad line, e.g., print it or modify it
    print(&quot;Bad line:&quot;, bad_line)
    # Return a modified line (if needed) or None to skip it
    return None  # Skip the bad line

df = pd.read_csv(&quot;DataFile.csv&quot;, engine=&quot;python&quot;,on_bad_lines=handle_bad_line)
</code></pre>
<p><a href=""https://i.sstatic.net/65xgAQAB.png"" rel=""nofollow noreferrer"">SS of code run</a></p>
","0","Answer"
"79381783","79381632","<p>First filter the consumers to keep with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.isin.html"" rel=""nofollow noreferrer""><code>isin</code></a>, then build two boolean arrays to sort each group, one to identify the values before the first &quot;affiliate&quot;, and one to identify the &quot;Magalu&quot;. Sort the values to have (1) initial non-affiliate, (2) &quot;Magalu&quot;, (3) rest of the data with help of <a href=""https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html"" rel=""nofollow noreferrer""><code>numpy.lexsort</code></a>:</p>
<pre><code>df = df_recommendation[df_recommendation['consumer_id']
                       .isin(df_users_bought['consumer_id'])]
def sorter(g):
    m1 = g['resource_type'].eq('affiliate').cummax()
    m2 = g['resource_id'].ne('Magalu')
    return (g.iloc[np.lexsort([m2, m1])]
            .assign(order=sorted(g['order']))
           )

out = (df.groupby('consumer_id', group_keys=False, sort=False)
       [df.columns].apply(sorter)
      )
</code></pre>
<p>Output:</p>
<pre><code>  consumer_id resource_id resource_type  order
2        C001      Magalu     affiliate      1
0        C001      Amazon     affiliate      2
1        C001       Rappi            DG      3
3        C002       Rappi            DG      1
5        C002      Magalu     affiliate      2
4        C002      Amazon     affiliate      3
</code></pre>
","1","Answer"
"79381941","79381867","<p>Reason why you got the boxplots crammed to the left and the line plot crammed to the right:</p>
<p>Matplotlib internally transforms strings/categories on the x-axis to integers starting from 0. But for dates, it transforms them to float values corresponding to the number of days since 01/01/1970. That's why I use 16102 (I added 0.5 to put the box in the middle of the month instead of the beginning).</p>
<pre><code>fig, axes = plt.subplots(1, 1, figsize=(10, 5))
n = 480
ts = pd.DataFrame(np.random.randn(n), index=pd.date_range(start=&quot;2014-02-01&quot;, periods=n,freq=&quot;H&quot;))
g = ts.groupby(lambda x: x.strftime(&quot;%Y-%m-%d&quot;))
g.boxplot(subplots=False, ax=axes, positions=np.arange(16102.5, 16122.5))
ts.plot(ax = axes)

# To format the x-tick labels
labels = pd.date_range(start=&quot;2014-02-01&quot;, periods=20, freq=&quot;D&quot;)
labels = [t.strftime('%Y-%m-%d') for t in labels]
axes.set_xticklabels(labels=labels, rotation=70)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/xFIX3S3i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFIX3S3i.png"" alt=""plot"" /></a></p>
","1","Answer"
"79382683","79381686","<p>I found another approach that works:</p>
<pre><code>    credential = ClientSecretCredential(
        tenant_id=self.settings.TENANT_ID,
        client_id=self.datalake_settings.RUNACCOUNT_ID,
        client_secret=self.datalake_settings.RUNACCOUNT_KEY.get_secret_value()
    )
    
    # Create blob service client
    account_url = f&quot;https://{self.datalake_settings.STORAGE_ACCOUNT}.blob.core.windows.net&quot;
    blob_service_client = BlobServiceClient(
        account_url=account_url,
        credential=credential
    )
    
    # Get container name from the EXTRACT_ROOT (assuming it's in format &quot;container/path&quot;)
    container_name = &quot;st-xx-lake-xxx-dev-ctn&quot;
    
    # Get the blob path (everything after container name)
    blob_path = f&quot;{parquet_folder_path}/{file_name}&quot;
    
    # Get container client
    container_client = blob_service_client.get_container_client(container_name)
    
    # Write parquet to bytes buffer
    parquet_buffer = io.BytesIO()
    df.to_parquet(parquet_buffer, engine='pyarrow', compression='snappy')
    parquet_buffer.seek(0)
    
    # Upload the parquet file
    blob_client = container_client.upload_blob(
        name=blob_path,
        data=parquet_buffer,
        overwrite=True
    )
</code></pre>
","0","Answer"
"79383377","79383355","<p>You could build a mapper to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.SeriesGroupBy.sum.html"" rel=""nofollow noreferrer""><code>groupby.sum</code></a> and compare to the reference thresholds:</p>
<pre><code>mapper = {x: k for k in threshold['custom']
          for x in k.split(', ')}
# {'A': 'A, B', 'B': 'A, B', 'C': 'C'}

s2 = (s1.rename(mapper)
        .groupby(level=0).sum()
     )

out = s2.lt(s2.index.to_series()
               .map(threshold['custom'])           
               .fillna(threshold['default'])
           )
</code></pre>
<p>Alternative for the last step if you don't have NaNs:</p>
<pre><code>out = s2.lt(s2.index.map(threshold['custom']).values,
            fill_value=threshold['default'])
</code></pre>
<p>Output:</p>
<pre><code>A, B     True
C       False
D       False
dtype: bool
</code></pre>
","3","Answer"
"79383561","79383544","<p>You need to convert the positional indices to correspondin labels in <code>df2.index</code>. You can do that by indexing <code>df2.index</code> with those arrays:</p>
<p>So do this:</p>
<pre class=""lang-py prettyprint-override""><code>df2.drop(df2.index[upper_array], inplace=True)
df2.drop(df2.index[lower_array], inplace=True)
</code></pre>
<p>That way, you are telling <code>.drop()</code> actual row labels from <code>df2.index</code> rather than numeric positions.</p>
<p><code>np.where(...)</code> giving you positional indices (0,1,2, …) relative to df2 rather than actual row labels in <code>df2.</code> When you do</p>
<pre class=""lang-py prettyprint-override""><code>df2.drop(index=upper_array, inplace=True)
</code></pre>
<p>pandas interprets upper_array as actual row labels that must exist in <code>df2.index</code>. So KeyError [15, 25, 34,53] not found in axis simply means that df2 does not have row labels [15, 25, 34, 53].</p>
","1","Answer"
"79383648","79383544","<p>You can simplify solution for remove values in <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.between.html"" rel=""nofollow noreferrer""><code>Series.between</code></a>:</p>
<pre><code>upper_array = np.where(df2['final_test'] &gt;= upper)[0]
lower_array = np.where(df2['final_test'] &lt;= lower)[0]

print(upper)
print(upper_array)

# Removing the outliers
df2.drop(index=upper_array, inplace=True)
df2.drop(index=lower_array, inplace=True)
</code></pre>
<p>change to:</p>
<pre><code>df2 = df2[df2['final_test'].between(lower, upper, inclusive='neither')]
</code></pre>
","0","Answer"
"79383741","79382909","<p>You could try to fit your data using linear interpolation. You can for instance use <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html"" rel=""nofollow noreferrer"">scipy.interpolate.interp1d</a>.</p>
<pre class=""lang-py prettyprint-override""><code>from scipy.interpolate import interp1d

f_intensity_1 = interp1d(component_1[&quot;Energy&quot;], component_1[&quot;Intensity&quot;])
f_intensity_2 = interp1d(component_2[&quot;Energy&quot;], component_2[&quot;Intensity&quot;])
f_intensity_3 = interp1d(component_3[&quot;Energy&quot;], component_3[&quot;Intensity&quot;])

f_intensity_df = interp1d(df[&quot;Energy&quot;], df[&quot;Intensity&quot;])
f_uncertainty_df = interp1d(df[&quot;Energy&quot;], df[&quot;unc&quot;]) # this probably will be constant in your case
</code></pre>
<p>Then you can add the energy as a parameter:</p>
<pre class=""lang-py prettyprint-override""><code>params = Parameters()
params.add('a', value = 0.33,min=0, max=1)
params.add('b',value =0.33,min=0, max=1)
params.add('c', value = 0.33, min=0, max=1)

# You can improve that using your knowledge of the system
params.add('e', value=component_1[&quot;Energy&quot;].mean(), min=component_1[&quot;Energy&quot;].min(), max=component_1[&quot;Energy&quot;].max()) 
</code></pre>
<p>And update your equation as</p>
<pre class=""lang-py prettyprint-override""><code>def equation(params):
    a = params['a']
    b = params['b']
    c = params['c']
    e = params['e']

    c1 = f_intensity_1(e)
    c2 = f_intensity_2(e)
    c3 = f_intensity_3(e)

    obs_intensity = f_intensity_df(e)
    uncertainty = f_uncertainty_df(e)

    calc_intensity=a*c1+b*c2+c*c3

    return (obvs_intensity-calc_intensity)/uncertainty
</code></pre>
<p>Would something like that work for you?</p>
","0","Answer"
"79383870","79383833","<p>I would use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.convert_dtypes.html"" rel=""nofollow noreferrer""><code>convert_dtypes</code></a> here, which will force the nullable boolean pandas dtype on a mix of True/False/NaN:</p>
<pre><code>[
    ts
    for ts in outage_mask.loc[
        outage_mask.diff().convert_dtypes().fillna(False)
    ].index
]
</code></pre>
<p>You actually don't even need the <code>fillna</code> since a nullable boolean NaN behaves like <code>False</code> and you could skip the list comprehension:</p>
<pre><code>list(outage_mask.loc[outage_mask.diff().convert_dtypes()].index)
</code></pre>
<p>Output:</p>
<pre><code>[Timestamp('2025-01-01 05:00:00'),
 Timestamp('2025-01-01 10:00:00'),
 Timestamp('2025-01-01 15:00:00'),
 Timestamp('2025-01-01 20:00:00'),
 Timestamp('2025-01-02 01:00:00'),
 Timestamp('2025-01-02 06:00:00'),
 Timestamp('2025-01-02 11:00:00'),
 Timestamp('2025-01-02 16:00:00'),
 Timestamp('2025-01-02 21:00:00')]
</code></pre>
","1","Answer"
"79383876","79383833","<p>Try explicitly cast to <code>boolean</code> or <code>float</code> first</p>
<pre class=""lang-py prettyprint-override""><code>mask_diff =outage_mask.diff().astype(&quot;boolean&quot;).fillna(False)
outage_mask.loc[mask_diff]
</code></pre>
","0","Answer"
"79383968","79383889","<p>Build a dictionary of the columns with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>str.split</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a>+<code>Index.groupby</code>, and process them in a loop:</p>
<pre><code>s = df.columns.to_series().str.split('_').explode()
d = s.index.groupby(s)

for k, v in d.items():
    df[f'f_{k}'] = df[v].sum(axis=1)
</code></pre>
<p>You could also use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.eval.html"" rel=""nofollow noreferrer""><code>eval</code></a> instead of the loop once you have <code>d</code>:</p>
<pre><code>query = '\n'.join(f'f_{k} = {&quot;+&quot;.join(map(&quot;`{}`&quot;.format, v))}'
                  for k,v in d.items())
out = df.eval(query)
</code></pre>
<p>Output:</p>
<pre><code>   1_2  1_3  1_4  2_3  2_4  3_4  f_1  f_2  f_3  f_4
0    1    5    2    8    2    2    8   11   15    6
1    4    3    4    5    8    5   11   17   13   17
2    8    8    8    9    3    3   24   20   20   14
3    4    3    4    4    8    3   11   16   10   15
4    8    0    7    4    2    2   15   14    6   11
</code></pre>
<p>Intermediate <code>d</code>:</p>
<pre><code>{'1': ['1_2', '1_3', '1_4'],
 '2': ['1_2', '2_3', '2_4'],
 '3': ['1_3', '2_3', '3_4'],
 '4': ['1_4', '2_4', '3_4'],
}
</code></pre>
<p>Pure python approach to build <code>d</code>:</p>
<pre><code>d = {}
for c in df:
    for k in c.split('_'):
        d.setdefault(k, []).append(c)
</code></pre>
<p>You could also imagine a pure pandas approach based on reshaping with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html"" rel=""nofollow noreferrer""><code>melt</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html"" rel=""nofollow noreferrer""><code>pivot_table</code></a>, but this is most likely much less efficient:</p>
<pre><code>out = df.join(df
   .set_axis(df.columns.str.split('_'), axis=1)
   .melt(ignore_index=False).explode('variable')
   .reset_index()
   .pivot_table(index='index', columns='variable',
                values='value', aggfunc='sum')
   .add_prefix('f_')
)
</code></pre>
","3","Answer"
"79385082","79385026","<p>A possible solution, whose steps are:</p>
<ul>
<li><p>The <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a> method is used to transform each item of a list-like to a row, replicating index values.</p>
</li>
<li><p>The <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html"" rel=""nofollow noreferrer""><code>value_counts</code></a> function is then applied to count the occurrences of each unique value in the <code>tags</code> column.</p>
</li>
</ul>
<p><em>NB: I thank @user19077881 for suggesting, in a comment below, to use <code>reset_index()</code> here. I also thank @ouroboros1 for letting me know that <code>to_frame()</code> is not needed.</em></p>
<pre><code>df.explode('tags').value_counts('tags').reset_index()
</code></pre>
<p>Output:</p>
<pre><code>      tags  count
0   family      2
1  friends      2
2  holiday      1
</code></pre>
","1","Answer"
"79385171","79385026","<p>You can't have a row belong to multiple groups like your <code>grpby</code> object. Thus what you want to do is impossible in pure pandas, unless you duplicate the rows with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a>, then you will be able to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a>:</p>
<pre><code>out = (df.explode('tags')
         .groupby('tags', as_index=False)
         .agg(**{'count': ('tags', 'size')})
      )
</code></pre>
<p>Output:</p>
<pre><code>      tags  count
0   family      2
1  friends      2
2  holiday      1
</code></pre>
<p>With a more meaningful aggregation:</p>
<pre><code>out = (df.explode('tags')
         .groupby('tags', as_index=False)
         .agg({'Id': frozenset})
      )
</code></pre>
<p>Output:</p>
<pre><code>      tags              Id
0   family  (item4, item1)
1  friends  (item2, item1)
2  holiday         (item4)
</code></pre>
<p>Note however that <code>explode</code> is quite expensive, so if you just want to count the tags, better use pure python:</p>
<pre><code>from collections import Counter
from itertools import chain

out = Counter(chain.from_iterable(df['tags']))
</code></pre>
<p>Output:</p>
<pre><code>Counter({'friends': 2, 'family': 2, 'holiday': 1})
</code></pre>
<p>And if you want to split the DataFrame like your <code>grpby</code> object:</p>
<pre><code>tmp = df.assign(group=df['tags']).explode('group')
group = tmp.pop('group')

out = dict(list(tmp.groupby(group)))
</code></pre>
<p>Output:</p>
<pre><code>{'family':       Id               tags
           0  item1  [friends, family]
           3  item4  [family, holiday],
 'friends':       Id               tags
           0  item1  [friends, family]
           1  item2          [friends],
 'holiday':       Id               tags
           3  item4  [family, holiday]}
</code></pre>
","1","Answer"
"79385867","79385781","<p>One way of doing this is by spliting your function into to separate functions:</p>
<pre><code>def calculate_time_diff_in_seconds(date_time):

    #calculate the time diff in seconds
    return (date_time - event_time).total_seconds()

def seconds_to_time_diff_format(seconds)
   
    # minutes and seconds
    m, s = divmod(abs(seconds), 60)

    # hours and minutes
    h, m = divmod(m, 60)

    time_diff_string = 'T{:+03.0f}:{:02.0f}:{:05.2f}'.format(h if t_time_seconds &gt;= 0 else -h, m, s)

    return time_diff_string 
</code></pre>
<p>And then simply do this:</p>
<pre><code>df['time_diff_s'] = df['date_time'].apply(calculate_time_diff)
df['time_delta'] = df['time_diff_s'].apply(seconds_to_time_diff_format)
</code></pre>
<p>An other way of doing this (and i think it is faster for large scale) is making your function dealing with the whole dataframe and returning series:</p>
<pre><code>from datetime import datetime
import pandas as pd

str_event_time = '2025-01-24T11:55:00.000Z'
event_time = datetime.strptime(str_event_time,'%Y-%m-%dT%H:%M:%S.%fZ')

def calculate_time_diff_in_seconds(date_time):

    #calculate the time diff in seconds
    return (date_time - event_time).total_seconds()

def seconds_to_time_diff_format(Tseconds):
   
    # minutes and seconds
    m, s = divmod(abs(Tseconds), 60)

    # hours and minutes
    h, m = divmod(m, 60)

    time_diff_string = 'T{:+03.0f}:{:02.0f}:{:05.2f}'.format(h if Tseconds &gt;= 0 else -h, m, s)
    
    return time_diff_string 

def calculate_time_diff(mydataseries):
    mydataseries['time_diff_s'] = calculate_time_diff_in_seconds(mydataseries['date_time'])
    mydataseries['time_delta'] = seconds_to_time_diff_format(mydataseries['time_diff_s'])
    return mydataseries
    


def main():
    # Create the DataFrame
    data = {
        &quot;date_time&quot;: [
            &quot;2025-01-24 12:00:00.000&quot;,
            &quot;2025-01-24 12:05:00.000&quot;,
            &quot;2025-01-24 12:10:00.000&quot;
        ]
    }
    df = pd.DataFrame(data)

    df['date_time'] = pd.to_datetime(df['date_time'])

    print(df)

    df.insert(1,'time_delta',None,allow_duplicates=True)
    df.insert(1,'time_diff_s',None,allow_duplicates=True)

    time_format = '%Y-%m-%dT%H:%M:%S.%fZ'

    df = df.apply(calculate_time_diff, axis=1)

    #df['time_diff_s'] = df['date_time'].apply(calculate_time_diff_in_seconds)
    #df['time_delta'] = df['time_diff_s'].apply(seconds_to_time_diff_format)
    

    print(df)

main()
</code></pre>
<p>or if you want to keep it in one function (i don't recommend it) but:</p>
<pre><code>def calculate_time_diff(series):

#calculate the time diff in seconds
series['time_diff_s'] = (series['date_time'] - event_time).total_seconds()


# minutes and seconds
m, s = divmod(abs(series['time_diff_s']), 60)

# hours and minutes
h, m = divmod(m, 60)

series['time_delta'] = 'T{:+03.0f}:{:02.0f}:{:05.2f}'.format(h if series['time_diff_s'] &gt;= 0 else -h, m, s)


return series
</code></pre>
<p>and then apply it to the df: <code>df = df.apply(calculate_time_diff, axis=1)</code></p>
","1","Answer"
"79385879","79378096","<p>The <code>pd.merge</code> function is not so slow for a <strong>generic function</strong>. One can write a faster generic implementation, but not in Python (this would be far to difficult IMHO): only in a native language (e.g. C++ or Rust). That being said, we can compute the operation more efficiently by writing a <strong>specialized function</strong> optimized for your specific use-case. This function can be written in <strong>Numba</strong>. Indeed, there is AFAIK no way to reimplement this efficiently in pure-Python, Pandas or even Numpy.</p>
<p>In order to be faster, the specialized implementation assumes:</p>
<ul>
<li><code>df_X</code> is much smaller than <code>df_Y</code></li>
<li>output dataframe is significantly smaller than <code>df_Y</code></li>
</ul>
<p>The idea is to first <strong>build a dictionary <code>x_keys</code></strong> where each key is a tuple built from the rows of <code>df_X[match_columns]</code>. Then, we <strong>iterate over <code>df_Y</code> in parallel</strong> so to check if key-related items of a row are found in <code>x_keys</code>. If this is the case, we set a flag <code>matches[i]</code> to <code>True</code> (because we cannot easily build a list of variable-size in parallel, especially in Numba). Then, we iterate over <code>matches</code> to extract the indices of the matching rows in both <code>df_X</code> and <code>df_Y</code>. We do not directly do that in sequential because the expression <code>key in x_keys</code> is pretty expensive. This is why this part is parallelized. The last step is fast because checking if <code>matches[i]</code> is <code>True</code> is a very cheap operation. Once we extracted the indices of each dataframe, we can easily build the left and right part of the output dataframe.</p>
<hr />
<h2>Actual faster code</h2>
<p>Here is the code of the faster Numba implementation previously described:</p>
<pre class=""lang-py prettyprint-override""><code>import numba as nb

KeyType = nb.types.UniTuple(nb.types.float32, 5)
IntList = nb.types.ListType(nb.types.int32)

@nb.njit('(f4[:], f4[:], f4[:], f4[:], f4[:], f4[:], f4[:], f4[:], f4[:], f4[:])', parallel=True)
def get_merge_index(xa, xb, xc, xd, xe, ya, yb, yc, yd, ye):
    n, m = xa.size, ya.size
    assert (xb.size, xc.size, xd.size, xe.size) == (n, n, n, n)
    assert (yb.size, yc.size, yd.size, ye.size) == (m, m, m, m)
    x_keys = nb.typed.Dict.empty(key_type=KeyType, value_type=IntList)
    for i in range(n):
        key = (xa[i], xb[i], xc[i], xd[i], xe[i])
        if key in x_keys:
            x_keys[key].append(np.int32(i))
        else:
            x_keys[key] = nb.typed.typedlist.List([np.int32(i)])
    matches = np.empty(m, dtype=np.bool_)
    for i in nb.prange(m):
        key = (ya[i], yb[i], yc[i], yd[i], ye[i])
        matches[i] = key in x_keys
    x_index = []
    y_index = []
    cur = 0
    for i in range(m):
        if matches[i]:
            key = (ya[i], yb[i], yc[i], yd[i], ye[i])
            for j in x_keys[key]:
                x_index.append(np.int32(j))
                y_index.append(np.int32(i))
    return (np.array(x_index), np.array(y_index))

def compute(df_X, df_Y):
    gen_args = lambda df, cols: [df[col].to_numpy() for col in cols]
    x_index, y_index = get_merge_index(*gen_args(df_X, match_columns), *gen_args(df_Y, match_columns))
    y_col_ids = [col_id for col_id, col_name in enumerate(df_Y.columns) if col_name not in match_columns]
    # Indexing pandas functions are apparently insanely slow so we need to do all the work rather manually...
    left_df = df_X.iloc[x_index].reset_index(drop=True)
    right_df = df_Y.iloc[y_index].iloc[:,y_col_ids].reset_index(drop=True)
    return pd.concat([left_df, right_df], axis=1)

merged_df = compute(df_X, df_Y)
</code></pre>
<p>Note I only checked the size of the output dataframe is correct and the values looked approximately correct (i.e. no thorough validation).</p>
<hr />
<h2>Performance evaluation</h2>
<p>Here are performance results on my i5-9600KF CPU (6-cores) with Python 3.12.8 and Pandas 2.2.3:</p>
<pre><code>Join Time if sorted:      75.64 s
Merge Time with dask:     16.51 s
Merge Time:                6.68 s
Numba Time:                2.11 s     &lt;----------
Optimal time:             ~0.06 s
</code></pre>
<p>We can see that the Numba implementation is about <strong>3 times faster</strong> than the <code>pd.merge</code> function. This is mainly because of the parallel loop (which does not scale certainly due to internal allocations).</p>
<hr />
<h2>Further optimizations</h2>
<p>If this is not fast enough then I strongly advise you to <strong>reimplement this logic in a native language</strong> like C++ or even Rust. The dictionary accesses should be faster and the code should also scale. In the end, I expect this to be at least be 2-4 times faster than the Numba code on my machine.</p>
<p>If this is still not fast enough, then you need to <strong>choose a faster hash-map implementation</strong> than the one provided in the standard library of the target language (especially in C++). I advise you to pick one caring about CPU caches and implementing <a href=""https://en.wikipedia.org/wiki/Open_addressing"" rel=""nofollow noreferrer"">open-addressing</a>. <a href=""https://github.com/Tessil/robin-map"" rel=""nofollow noreferrer"">Here</a> is an example in C++.</p>
<p>If this is still not enough then you can filter rows very efficiently using <a href=""https://en.wikipedia.org/wiki/Bloom_filter"" rel=""nofollow noreferrer""><strong>Bloom filters</strong></a>. This can actually be done in Numba completely in Python but this is not easy. You can find an example in <a href=""https://stackoverflow.com/questions/74075122/the-most-efficient-way-rather-than-using-np-setdiff1d-and-np-in1d-to-remove-com/74083928#74083928"">this post</a>.</p>
<p>Once all aforementioned optimizations have been applied, the resulting code should be close to <strong>optimal</strong>: it should be bound by the time to read the input from the main memory (slow DRAM).</p>
","1","Answer"
"79387956","79387911","<p>One straightforward way is to melt your dataframe so <code>value_1</code> and value_2 become labels in single column and then pivot on that. Like:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

x = pd.DataFrame(
    [
        [1, 'step', 'id', 22, 33],
        [2, 'step', 'id', 55, 66]
    ],
    columns=['time','head_1', 'head_2', 'value_1', 'value_2']
)

melted = x.melt(
    id_vars=['time','head_1', 'head_2'],
    value_vars=['value_1','value_2'],
    var_name='head_3',
    value_name='val'
)

result = melted.pivot_table(
    index=['head_1','head_2','head_3'],
    columns='time',
    values='val',
    aggfunc='first'
)
</code></pre>
<p>You’ll get:</p>
<pre><code>time                 1   2
head_1 head_2 head_3        
step   id     value_1  22  55
               value_2 33  66
</code></pre>
<p>This places <code>value_1</code> and `value_2 in rows as you wanted with only time going across columns</p>
","3","Answer"
"79388039","79387911","<p>You can <code>unstack</code> + <code>stack</code> here:</p>
<pre class=""lang-py prettyprint-override""><code>out = (x.rename_axis(columns='head_3')
       .set_index(['time', 'head_1', 'head_2'])
       .unstack('time')
       .stack('head_3', future_stack=True)
       )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>time                    1   2
head_1 head_2 head_3         
step   id     value_1  22  55
              value_2  33  66
</code></pre>
<p><strong>Explanation</strong></p>
<ul>
<li>Add a name for existing column labels with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename_axis.html"" rel=""nofollow noreferrer""><code>df.rename_axis</code></a>.</li>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>df.set_index</code></a> to set appropriate columns as the index.</li>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.unstack.html"" rel=""nofollow noreferrer""><code>df.unstack</code></a> to move index level 'time' to columns.</li>
<li>Finally, use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>df.stack</code></a> to move column level 'head_3' in the opposite direction.</li>
</ul>
","3","Answer"
"79388543","79385475","<p>So I came up with renaming almost all columns in the very beginning manually to have them with names without surpraises, and it all (dropping, combining, etc.) went well after : )</p>
<p><a href=""https://i.sstatic.net/BHH7uAtz.png"" rel=""nofollow noreferrer"">Here is my &quot;perfect&quot; df</a></p>
","1","Answer"
"79388561","79382113","<p>You can use the <code>index_col</code> parameter like this:</p>
<pre><code>import pandas as pd

df = pd.read_csv('yours_file.csv', index_col=0, usecols=[0])

index = df.index
print(index)
</code></pre>
","0","Answer"
"79389554","79388899","<p>I think the error is in the <code>if 2 in df.columns</code> condition (and then calling <code>exit()</code> to exit the script prematurely).</p>
<p>To load the table with GDP use slightly different approach - simply read the table using <code>pd.read_html</code> and don't convert the columns to numbers.</p>
<p>Also, the correct table has index <code>2</code> not <code>3</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import re
import requests
from io import StringIO

import pandas as pd


url = 'https://web.archive.org/web/20230902185326/https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29'

data = requests.get(url).text
df = pd.read_html(StringIO(data))[2]

# change/flatten the multi-index columns
df.columns = [re.sub(r'\[\d+\]', '', a if a == b else f'{a}_{b}') for a, b in df.columns]

# clean the data
df = df.map(lambda x: int(re.sub(r'\[.*\]', '', x)) if isinstance(x, str) and x.startswith('[') else x)

# get only top 10 economies
df = df.loc[1:11].reset_index(drop=True)

# further filter/sort the data
# ...

print(df.head())
df.to_csv('data.csv', index=False)
</code></pre>
<p>Prints:</p>
<pre class=""lang-none prettyprint-override""><code>  Country/Territory UN region IMF_Estimate IMF_Year World Bank_Estimate World Bank_Year United Nations_Estimate United Nations_Year
0     United States  Americas     26854599     2023            25462700            2022                23315081                2021
1             China      Asia     19373586     2023            17963171            2022                17734131                2021
2             Japan      Asia      4409738     2023             4231141            2022                 4940878                2021
3           Germany    Europe      4308854     2023             4072192            2022                 4259935                2021
4             India      Asia      3736882     2023             3385090            2022                 3201471                2021
</code></pre>
<p>and saves <code>data.csv</code>.</p>
","1","Answer"
"79390546","79390524","<p>Save the current headers as the first row of the dataframe using <code>df.loc[-1] = df.columns</code> and adjust the index.</p>
<p>Rename the columns to the correct ones with <code>df.columns = ['column 1', 'column 2']</code>.</p>
<p>Done!</p>
","3","Answer"
"79390918","79390828","<p>You could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html"" rel=""nofollow noreferrer""><code>extract</code></a> the part before the trailing <code>-xxx</code> and use this to identify the levels to drop with <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a>. Perform per group with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.SeriesGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a>:</p>
<pre><code>out = df[df.groupby('ID')['Question Code']
           .transform(lambda x: ~x.isin(x.str.extract('(.*)-[^-]+$',
                                        expand=False).dropna()))]
</code></pre>
<p>Output:</p>
<pre><code>   ID Question Code
1   1         Q01-1
2   1           Q02
3   2           Q01
6   2       Q02-1-1
7   2         Q02-2
</code></pre>
<p>Example intermediates for ID 2:</p>
<pre><code># s.str.extract('(.*)-[^-]+$', expand=False).dropna()
5      Q02
6    Q02-1
7      Q02
Name: Question Code, dtype: object

# ~s.isin(s.str.extract('(.*)-[^-]+$', expand=False).dropna())
3     True
4    False
5    False
6     True
7     True
Name: Question Code, dtype: bool
</code></pre>
","1","Answer"
"79392121","79392038","<p>Since working with strings is hardly vectorizable, just use a simple python function and a list comprehension:</p>
<pre><code>def replace(org, account):  
    a, b = account.rsplit('-', maxsplit=1)
    if org == '03':
        return f'{a}-{D03_SPECIAL_MAP[b]}'
    return f'{a}-{b[:MAP[org]]}'
    
df['Account'] = [replace(o, a) for o, a in zip(df['Org'], df['Account'])]
</code></pre>
<p>Output:</p>
<pre><code>  Org      Account
0  01  01-123-0000
1  01  01-456-0000
2  02   02-789-000
3  02   02-456-000
4  03   03-987-012
5  03   03-123-123
</code></pre>
","2","Answer"
"79393067","79392973","<p>You could write a custom function to compute your custom maximum (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html"" rel=""nofollow noreferrer""><code>pandas.to_numeric</code></a> to identify the numeric values, and a boolean mask, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.idxmax.html"" rel=""nofollow noreferrer""><code>idxmax</code></a> to pick one of the invalid values), and combine it to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a>:</p>
<pre><code>def cust_max(s):
    s2 = pd.to_numeric(s, errors='coerce')
    m = (s2.isna()&amp;s.notna())
    if m.any():
        return s[m.idxmax()]
    return s2.max()
    
out = df.groupby('eid', as_index=False).agg(cust_max)
</code></pre>
<p>Although not fully clear, if you want to ignore the rows with <code>PF==0</code>, just pre-filter:</p>
<pre><code>out = df.query('PF!=0').groupby('eid', as_index=False).agg(cust_max)
</code></pre>
<p>Output:</p>
<pre><code>     eid    LV1   LV2  PF
0  10001         GRAY   1
1  10002  -8888  blue   1
2  10003  -8888    22   1
3  10004      &amp;    68   1
</code></pre>
<p>Reproducible input:</p>
<pre><code>df = pd.DataFrame({'eid': [10001, 10001, 10002, 10002, 10003, 10004, 10004, 10004],
                   'LV1': ['', -8888, -8888, -8888, -8888, '&amp;', 33, 'TIM'],
                   'LV2': [-8888, 'GRAY', 'blue', 9, 22, 55, 68, 166],
                   'PF': [1, 0, 1, 0, 1, 1, 0, 0]})
</code></pre>
","1","Answer"
"79393190","79393122","<p>You could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html"" rel=""nofollow noreferrer""><code>melt</code></a> with <code>ignore_index=False</code> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename_axis.html"" rel=""nofollow noreferrer""><code>rename_axis</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a>:</p>
<pre><code>out = (df.rename_axis(columns=['year', 'category'])
         .melt(ignore_index=False)
         .reset_index()
      )
</code></pre>
<p>Or:</p>
<pre><code>out = (df.melt(ignore_index=False)
         .rename(columns={'variable_0': 'year',
                          'variable_1': 'category'})
         .reset_index()
      )
</code></pre>
<p>Output:</p>
<pre><code>  class  year category  value
0     X  2022        A      1
1     Y  2022        A      5
2     X  2022        B      2
3     Y  2022        B      6
4     X  2021        A      3
5     Y  2021        A      7
6     X  2021        B      4
7     Y  2021        B      8
</code></pre>
<p>Reproducible input:</p>
<pre><code>df = pd.DataFrame.from_dict({'index': ['X', 'Y'],
                             'columns': [('2022', 'A'), ('2022', 'B'), ('2021', 'A'), ('2021', 'B')],
                             'data': [[1, 2, 3, 4], [5, 6, 7, 8]],
                             'index_names': ['class'],
                             'column_names': [None, None]},
                            orient='tight')
</code></pre>
","2","Answer"
"79393325","79393269","<p>To correctly read the table data including the headers based on the range of each list_object, you need to modify the line:</p>
<pre><code>df = pd.read_excel(file_path, sheet_name=sheet.Name, header=0)
</code></pre>
<p>to</p>
<p>Determine the number of columns in the table</p>
<pre><code>start_col, start_row = start_cell[:-1], start_cell[-1:]
end_col, end_row = end_cell[:-1], end_cell[-1:]
</code></pre>
<p>Convert column letters to zero-based indices</p>
<pre><code>start_col_idx = ord(start_col.upper()) - ord('A')
end_col_idx = ord(end_col.upper()) - ord('A')
usecols = list(range(start_col_idx, end_col_idx + 1))
</code></pre>
<p>Read the sheet and extract the table data</p>
<pre><code>df = pd.read_excel(file_path, sheet_name=sheet.Name, usecols=usecols, 
skiprows=int(start_row)-1, nrows=int(end_row)-int(start_row)+1, header=0)
</code></pre>
","0","Answer"
"79394611","79394177","<p><code>.loc</code> can be expensive as it needs to look up if you are passing a slice or an iterable over keys. Converting your dataframe to a dict of dict should bring faster lookups:</p>
<pre class=""lang-py prettyprint-override""><code>my_list = ['col1', 'col2', ..., 'col17']
my_dict = {}

for row_key, row in df.T.to_dict().items():
    my_dict[row_key] = myClass(row['name'])
        for col in my_list:
            my_dict[row_key].class_method({col: row[col})
</code></pre>
","0","Answer"
"79395084","79392071","<p>From what I understand, you want to pair up payment slips with transactions <strong>by occurrence</strong> of the block ID, so that means all you need to do is enumerate the occurrences of each block ID. See <a href=""https://stackoverflow.com/a/52263160/4518341"">piRSquared's answer on &quot;enumerate items in each group&quot;</a>. Then you can merge.</p>
<p>That would look like this:</p>
<pre><code>join_key = 'block_id'

def _enum(df: pd.DataFrame) -&gt; pd.Series:
    return df.groupby(join_key).cumcount()

result = (
    transactions
    .drop(columns=['slip_id'])  # Will be merged instead
    .assign(_n=_enum)
    .merge(
        payment_slips
        .rename(columns={'id': 'slip_id'})
        .assign(_n=_enum),
        how='left',
    )
    # .drop(columns=['block_id', '_n'])  # Drop merge keys
)

print(result.drop(columns=['id', 'block_id']))  # To show result on one page
</code></pre>
<pre class=""lang-none prettyprint-override""><code>     amount        date  _n                               slip_id
0      2.40  2023-09-20   0  7f2176dd-68ee-4b49-9f57-9e661cb3bf41
1      2.40  2023-09-20   1                                   NaN
2      2.40  2023-09-20   2                                   NaN
3    414.66  2023-09-20   0  857a161b-a8fe-4939-b951-686e73a7c5b6
4    429.64  2023-09-20   0  f6f8305c-5260-4bce-9be8-0618ed500389
5   1377.50  2023-09-20   0  790d5425-ed53-4ecc-8e13-5d7726cae289
6   2190.00  2023-09-20   0  f0885f50-6617-4fa8-a2e4-79e917cb2237
7   2600.00  2023-09-20   0  5f24b735-8dfb-4592-bef5-6fb74d6f4e24
8  10480.02  2023-09-20   0  717cc01c-7520-45ad-bb72-2c0bafc79c6d
9  32856.66  2023-09-20   0  f3b31d20-9a74-43ac-aaee-56a3b87674d8
</code></pre>
","1","Answer"
"79396038","79381694","<p>While this hasn't solved my problem, I found <a href=""https://stackoverflow.com/questions/57154620/select-failed-because-the-following-set-options-have-incorrect-settings-ansi-w"">this question</a> which suggests adding <code>SET ANSI_WARNINGS OFF</code> might be a solution.  In my case, although my query is returning the data I want in MSSMS, it is also returning the below warning in the messages tab:</p>
<pre><code>Warning: Null value is eliminated by an aggregate or other SET operation.
</code></pre>
<p>As I understand it, when calling a query in python it takes the first thing returned by that query. In this case, that is the warning message, hence the <code>object does not return rows error</code>.  Adding the <code>SET ANSI_WARNINGS OFF</code> to the start of the query should suppress the warnings and allow python to read in the data.</p>
<p>I got different errors using this, so it didn't work for me but I've posted this answer in case it might help anyone else with this issue.  In my case I think I will have to rework my query so that I don't get any error messages.</p>
","0","Answer"
"79396380","79389094","<p>I fixed something.
Now the images are attached.
But I can not include them in the html body of the email.</p>
<p>I attach as always my code.
I would like to thank you in advance</p>
<pre><code>import smtplib
import mimetypes
from email.mime.image import MIMEImage
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders

import oracledb
import pandas as pd
import sqlalchemy
import smtplib
import matplotlib


image_path=&quot;C:\\Users\\n.restaino\\PycharmProjects\\pythonProject\\.venv\\barchart1.png&quot;

if __name__ == &quot;__main__&quot;:
    # Connection details
    user = 'a'
    pw = 'a'
    host = 'SRV-1'
    port = '1521'
    db = '2'


    engine = sqlalchemy.create_engine('oracle+cx_oracle://' + user + ':' + pw + '@' + host + ':' + port + '/?service_name=' + db)
    my_query='SELECT tc10_clfo tipo, count(*) numero FROM T_C10 GROUP BY tc10_clfo'
    df = pd.read_sql(my_query, engine)
    ax = df.plot.bar(x='tipo', y='numero', rot=0)

    fig = ax.get_figure()
    fig.savefig(image_path)



email_user = 'a.b@gmail.com'
password_user = '1234 5678 9012 3456'
email_send = 'c.d@gmail.com'
subject = 'Python'

msg = MIMEMultipart()
msg['From'] = email_user
msg['To'] = email_send
msg['Subject'] = subject

body = f&quot;&quot;&quot;&lt;h1&gt; Sales Report &lt;/h1&gt; {df.to_html()}
                    &lt;image src=&quot;cid:'image'&quot;/&gt;
        &quot;&quot;&quot;
msg.attach(MIMEText(body,'html'))

msgRoot = MIMEMultipart('mixed')
msgAlternative = MIMEMultipart('mixed')

fp='C:\\Users\\n.restaino\\PycharmProjects\\pythonProject\\.venv\\barchart1.png'
attachment =open(fp,'rb')
part = MIMEBase('application','octet-stream')
part.set_payload((attachment).read())
encoders.encode_base64(part)
part.add_header('Content-Disposition',&quot;attachment; filename= &quot;+fp, cid='image')
msg.attach(part)

fp='C:\\Users\\n.restaino\\PycharmProjects\\pythonProject\\.venv\\barchart1 - Copia.png'
attachment =open(fp,'rb')
part = MIMEBase('application','octet-stream')
part.set_payload((attachment).read())
encoders.encode_base64(part)
part.add_header('Content-Disposition',&quot;attachment; filename= &quot;+fp)
msg.attach(part)



part = MIMEBase('application','octet-stream')
part.set_payload((attachment).read())
encoders.encode_base64(part)
part.add_header('Content-Disposition',&quot;attachment; filename= &quot;+fp)

msg.attach(part)
text = msg.as_string()
server = smtplib.SMTP('smtp.gmail.com',587)
server.starttls()
server.login(email_user, password_user)

server.sendmail(email_user,email_send,text)
server.quit
</code></pre>
","0","Answer"
"79396841","79395144","<p>It looks like the <code>Date</code> column isn't recognized as a datetime object. Convert the <code>Date</code> column to a datetime format. After combining the data, sort the rows by <code>Date</code>:</p>
<pre><code>df['Date'] = pd.to_datetime(df['Date'])
combined_df = pd.concat(data_frames, ignore_index=True)
combined_df = combined_df.sort_values(by='Date')
</code></pre>
","0","Answer"
"79396866","79396624","<p>If I understood correctly, you want to get the <code>SKU</code> values that are in the new <code>csv</code> file (let's call it <code>v2.csv</code>) but not in the former one (let's call it <code>v1.csv</code>).</p>
<p>You can do it in many different ways, the way I'd do it would be by converting the list of <code>SKU</code> values of each file to <code>set</code> objects and take the difference between the two to get the ones that are in <code>set_sku_v2</code> but not in <code>set_sku_v1</code>.</p>
<p>Sample code:</p>
<pre class=""lang-py prettyprint-override""><code>v1_df = pd.read_csv(&quot;v1.csv&quot;)
v2_df = pd.read_csv(&quot;v2.csv&quot;)

set_sku_v1 = set(v1_df[&quot;SKU&quot;])  # e.g. {12, 15, 19, 22}
set_sku_v2 = set(v2_df[&quot;SKU&quot;])  # e.g. {12, 19, 31}

new_sku = list(set_sku_v2 - set_sku_v1)  # e.g. [15, 22]
new_products_df = v2_df.query(f&quot;SKU in {new_sku}&quot;)
</code></pre>
","0","Answer"
"79396996","79396950","<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.wide_to_long.html"" rel=""nofollow noreferrer""><code>pd.wide_to_long</code></a> for this:</p>
<pre class=""lang-py prettyprint-override""><code>out = (pd.wide_to_long(df, 
                       stubnames=['f_proba', 'p_proba'], 
                       i=['Class_ID', 'Meeting_ID'], 
                       j='Student_ID', 
                       sep='_')
       .reset_index()
       )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>   Class_ID  Meeting_ID  Student_ID  f_proba  p_proba
0       432          27           1        3        3
1       432          27           2        4        2
2       432          27           3        2        8
3       493          23           1        8       82
4       493          23           2        9       92
5       493          23           3        4       41
6        32          21           1        6       36
7        32          21           2        9       96
8        32          21           3        1       18
</code></pre>
","5","Answer"
"79397571","79397388","<p>If I'm reading this right, you're generating a daily model, then only taking every third day. The reason it doesn't work clean is because there are 365 days in non-leap years, which doesn't divide cleanly by 3, offsetting the resample dates by one for every year.</p>
<p>The easiest solution to implement would be to break the data down by year:</p>
<pre><code>year1 = ohlc_data.loc['2020-01-01': '2020-12-31'] 
year2 = ohlc_data.loc['2021-01-01': '2021-12-31']
</code></pre>
<p>etc.....</p>
<p>and then sort them each by three day increments:</p>
<pre><code>agg = {'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'}
resample1 = year1.resample('3D').agg(agg)
resample2 = year2.resample('3D').agg(agg) 
</code></pre>
<p>etc...</p>
<p>Then collapse them into a single pandas file:</p>
<pre><code>resample = pd.concat([resample1, resample2, resample3, etc]) 
</code></pre>
<p>By breaking it down this way, you also avoid having to write unique code for pulling leap years.</p>
<p>If you wanted to be fancy with it, write a loop along the lines of:</p>
<pre><code># Empty Dataframe
resample = pd.Dataframe()
# Breakdown control
agg = {'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'} 
# List of years in data
years_in_data = ['2020', '2021', '2022', '2023', '2024', '2025']

for year in years_in_data: 
    #                                   Jan01          Dec31
    temporary_1D = ohlc_data.loc['%year-01-01', '%year-12-31'] 
    temporary_3D = year1.resample('3D').agg(agg)
    resample = pd.concat([resample, temporary_3D]) 
</code></pre>
<p>This should give you the output where every years starts on the first of January.
You can simplify it or make it more reactive from there.</p>
","-1","Answer"
"79397590","79397388","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>agg = {'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'}

out = (
    ohlc_data.groupby(ohlc_data.index.year, group_keys=False)
    .apply(
        lambda g: g.resample('3D', origin=pd.Timestamp(g.name, 1, 1))
        .agg(agg)
    )
)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>out[~out.index.year.duplicated()]

                  Open        High         Low       Close
2020-06-23  137.454012  144.403523  129.235702  143.945741
2021-01-01  109.310277  197.899429  106.286082  188.085355
2022-01-01  140.348287  147.026333   98.926715  103.562334
2023-01-01  186.846798  189.998543  144.825695  189.588286
2024-01-01  151.771164  160.336537  101.736241  128.839795
2025-01-01  130.312836  137.712451  124.656259  135.872983
</code></pre>
<p><strong>Explanation</strong></p>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>df.groupby</code></a> to group by year (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.year.html"" rel=""nofollow noreferrer""><code>DatetimeIndex.year</code></a>).</li>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html"" rel=""nofollow noreferrer""><code>groupby.apply</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html"" rel=""nofollow noreferrer""><code>df.resample</code></a>. This way we can access <code>.name</code> for each group to create the appropriate <code>origin</code>.</li>
</ul>
<hr />
<p><em>Edit</em> (in response to <a href=""https://stackoverflow.com/questions/79397388/how-do-i-make-pandas-resample-starting-first-day-of-each-year-in-dataframe/79397590?noredirect=1#comment140050474_79397590"">comment</a> by <a href=""https://stackoverflow.com/users/4518341/wjandrea"">@wjandrea</a>):</p>
<p>The <code>.name</code> attribute doesn't appear to be documented (cf. <a href=""https://stackoverflow.com/a/37037622"">this post</a>). It gets set when the <code>apply_groupwise</code> method of the <code>BaseGrouper</code> is called, which happens when you use <code>groupby.apply</code> (<a href=""https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/groupby/groupby.py#L1785"" rel=""nofollow noreferrer"">groupby.py#L1785</a>) via <code>_python_apply_general</code> (e.g. <a href=""https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/groupby/groupby.py#L1851"" rel=""nofollow noreferrer"">#L1851</a> and then <a href=""https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/groupby/groupby.py#L1885"" rel=""nofollow noreferrer"">#L1885</a>).</p>
<p>The relevent part for <code>apply_groupwise</code> is in <a href=""https://github.com/pandas-dev/pandas/blob/main/pandas/core/groupby/ops.py#L1006-L1014"" rel=""nofollow noreferrer"">ops.py#L1006-L1014</a>:</p>
<pre class=""lang-py prettyprint-override""><code>        zipped = zip(group_keys, splitter)

        for key, group in zipped:
            # Pinning name is needed for
            #  test_group_apply_once_per_group,
            #  test_inconsistent_return_type, test_set_group_name,
            #  test_group_name_available_in_inference_pass,
            #  test_groupby_multi_timezone
            object.__setattr__(group, &quot;name&quot;, key)
</code></pre>
<p>Seems safe to assume that the attribute is not going anywhere soon, given the reasons for pinning it. Using <code>g.index.year[0]</code> to create the appropriate timestamp for <code>origin</code> will normally be a suitable alternative. It certainly is here, but one can of course use <code>group_keys</code> that aren't directly retrievable from the data contained in <code>g</code>.</p>
<p>A more verbose, but generic and &quot;documented&quot; alternative could then be:</p>
<pre class=""lang-py prettyprint-override""><code>gr_by = ohlc_data.groupby(ohlc_data.index.year, group_keys=False)
keys_iter = iter(gr_by.groups.keys())

out2 = (
    gr_by
    .apply(
        lambda g: g.resample('3D', origin=pd.Timestamp(next(keys_iter), 1, 1))
        .agg(agg)
    )
)

out2.equals(out)
# True
</code></pre>
<p>Using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.groups.html"" rel=""nofollow noreferrer""><code>groupby.groups</code></a> + <a href=""https://docs.python.org/3/library/functions.html#iter"" rel=""nofollow noreferrer""><code>iter</code></a> + <a href=""https://docs.python.org/3/library/functions.html#next"" rel=""nofollow noreferrer""><code>next</code></a>.</p>
","1","Answer"
"79397672","79396624","<p>Using the following example:</p>
<pre><code>display(downloaded_data)
Title   Image_URL   SKU
0   Product_Name_One    imageurl1.jpg   SKU123
1   Product_Name_Two    imageurl2.jpg   SKU456
2   Product_Name_Four   imageurl4.jpg   SKU457

display(current_data)
ID  Image_URL   Product_Name
0   SKU123  imageurl1.jpg   Product_Name_One
1   SKU789  imageurl3.jpg   Product_Name_Three
</code></pre>
<p>And adapting your original code:</p>
<pre><code># Merging both dataframe using left join
comparison_result = pd.merge(downloaded_data, current_data, 
                             left_on='SKU', right_on='ID', 
                             how='left', indicator=True)

# Filtering only the rows that are available in left (master_file_compare)
comparison_result = comparison_result.loc[comparison_result['_merge'] == 'left_only']   
new_products = downloaded_data.loc[downloaded_data['SKU'].isin(comparison_result['SKU'])]

display(new_products)
    Title   Image_URL   SKU
1   Product_Name_Two    imageurl2.jpg   SKU456
2   Product_Name_Four   imageurl4.jpg   SKU457
</code></pre>
","0","Answer"
"79398025","79395343","<p>Try this:</p>
<p><strong>Code:</strong></p>
<pre><code>data = {}
for fils in filist:
    i = open(fils, &quot;r&quot;)
    sec_data = json.loads(i.read())
    cik = str(sec_data['cik'])
    padded_cik = cik.zfill(10)
    cickStr = f'CIK{padded_cik}-concepts'
    
    entList = []
    for k,v in sec_data['facts'].items():
        entList += list(v.keys())
    
    data.update({cickStr:list(set(entList))})
    
    
df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()])) 
</code></pre>
<p>I can only base off the information you gave, but provided that those links are the same as the json files, the code works as seen in the output below:</p>
<pre><code>import requests
import pandas as pd


headers = {
    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 Edg/132.0.0.0',
    }

filist = ['https://data.sec.gov/api/xbrl/companyfacts/CIK0000320193.json',
'https://data.sec.gov/api/xbrl/companyfacts/CIK0001722010.json',
'https://data.sec.gov/api/xbrl/companyfacts/CIK0001722606.json']


data = {}
for fils in filist:
    w=1
    #i = open(fils, &quot;r&quot;)
    #sec_data = json.loads(i.read())
    sec_data = requests.get(fils, headers=headers).json()
    cik = str(sec_data['cik'])
    padded_cik = cik.zfill(10)
    cickStr = f'CIK{padded_cik}-concepts'
    
    entList = []
    for k,v in sec_data['facts'].items():
        entList += list(v.keys())
    
    data.update({cickStr:list(set(entList))})
    
    
df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()]))
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>print(df)
                                CIK0000320193-concepts  ...                             CIK0001722606-concepts
0           NetCashProvidedByUsedInFinancingActivities  ...  AdjustmentsForUnrealisedForeignExchangeLossesG...
1                       FiniteLivedIntangibleAssetsNet  ...      CurrentReceivablesFromTaxesOtherThanIncomeTax
2                                    VariableLeaseCost  ...  NumberOfShareOptionsExercisedInSharebasedPayme...
3    BusinessAcquisitionCostOfAcquiredEntityPurchas...  ...  WeightedAverageExercisePriceOfShareOptionsExer...
4    OtherComprehensiveIncomeReclassificationAdjust...  ...  IncomeTaxesPaidRefundClassifiedAsOperatingActi...
..                                                 ...  ...                                                ...
495                       PropertyPlantAndEquipmentNet  ...                                                NaN
496                            CommonStockSharesIssued  ...                                                NaN
497                        CommitmentsAndContingencies  ...                                                NaN
498                            IncomeTaxExpenseBenefit  ...                                                NaN
499  OperatingLeasesFutureMinimumPaymentsDueInFourY...  ...                                                NaN

[500 rows x 3 columns]
</code></pre>
","1","Answer"
"79399163","79398404","<p>You could combine two <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>DataFrame</code></a> constructors:</p>
<pre><code>data = [{'fields': ['2024-10-07T21:22:01', 'USER-A', 21, 0, 0, 21]},
        {'fields': ['2024-10-07T21:18:28', 'USER-B', 20, 20, 0, 0, 0, 45]},
       ]

out = pd.DataFrame(pd.DataFrame(data)['fields'].tolist())
</code></pre>
<p>Output:</p>
<pre><code>                     0       1   2   3  4   5    6     7
0  2024-10-07T21:22:01  USER-A  21   0  0  21  NaN   NaN
1  2024-10-07T21:18:28  USER-B  20  20  0   0  0.0  45.0
</code></pre>
<p>If you also have a list of columns <code>cols</code>, you could truncate the columns:</p>
<pre><code>cols = ['Created On', 'Created By', 'Transaction Count (ALL)',
        'X Pending', 'X Cancelled (X)', 'X Completed (Y)']

out = pd.DataFrame(pd.DataFrame(data)['fields'].str[:len(cols)].tolist(),
                   columns=cols)
</code></pre>
<p>Output:</p>
<pre><code>            Created On Created By  Transaction Count (ALL)  X Pending  X Cancelled (X)  X Completed (Y)
0  2024-10-07T21:22:01     USER-A                       21          0                0               21
1  2024-10-07T21:18:28     USER-B                       20         20                0                0
</code></pre>
<p>Or <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a> to keep the extra columns:</p>
<pre><code>out = (pd.DataFrame(pd.DataFrame(data)['fields'].tolist())
         .rename(columns=dict(enumerate(cols)))
       )
</code></pre>
<p>Output:</p>
<pre><code>            Created On Created By  Transaction Count (ALL)  X Pending  X Cancelled (X)  X Completed (Y)    6     7
0  2024-10-07T21:22:01     USER-A                       21          0                0               21  NaN   NaN
1  2024-10-07T21:18:28     USER-B                       20         20                0                0  0.0  45.0
</code></pre>
<p>But, honestly, better pre-process in pure python, this will be more efficient/explicit:</p>
<pre><code># truncation
out = pd.DataFrame((dict(zip(cols, d['fields'])) for d in data))

# alternative truncation
out = pd.DataFrame([d['fields'][:len(cols)] for d in data], columns=cols)

# renaming
out = (pd.DataFrame([d['fields'] for d in data])
         .rename(columns=dict(enumerate(cols)))
      )
</code></pre>
","3","Answer"
"79399710","79399697","<p><code>to_netcdf</code> is a method of <a href=""https://docs.xarray.dev/en/stable/generated/xarray.Dataset.to_netcdf.html"" rel=""nofollow noreferrer""><code>xarray.Dataset</code></a>. Therefore, first convert it to an <code>xarray</code> with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_xarray.html"" rel=""nofollow noreferrer""><code>DataFrame.to_xarray</code></a>.</p>
<p>In your case this should be:</p>
<pre class=""lang-py prettyprint-override""><code>...
Newdf.to_xarray().to_netcdf(...)
</code></pre>
","2","Answer"
"79399737","79388966","<p>When it comes to the categorical data, you need to encode them. As for the visualization, you can either do scatterplots of correlation heatmaps. Here is an example with fictive data that you could adapt to fit your needs:</p>
<pre><code>import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

np.random.seed(42)
df = pd.DataFrame({
    'Category': np.random.choice(['A', 'B', 'C'], size=100),
    'Numerical1': np.random.randn(100) * 10 + 50,
    'Numerical2': np.random.randn(100) * 5 + 100,
    'String_Var': np.random.choice(['Low', 'Medium', 'High'], size=100),
    'Numerical3': np.random.randint(1, 100, size=100)
})

label_encoders = {}
for col in ['Category', 'String_Var']:
    le = LabelEncoder()
    df[col + '_Encoded'] = le.fit_transform(df[col])
    label_encoders[col] = le

df_encoded = df.drop(columns=['Category', 'String_Var'])

sns.pairplot(df_encoded, diag_kind='kde')
plt.show()

plt.figure(figsize=(8,6))
sns.heatmap(df_encoded.corr(), annot=True, cmap='coolwarm', fmt=&quot;.2f&quot;, linewidths=0.5)
plt.title(&quot;Correlation Heatmap&quot;)
plt.show()
</code></pre>
<p>which renders</p>
<p><a href=""https://i.sstatic.net/6cVvhIBM.png"" rel=""nofollow noreferrer"">scatterplot</a></p>
<p>and</p>
<p><a href=""https://i.sstatic.net/BHenpkQz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BHenpkQz.png"" alt=""heatmap"" /></a></p>
","0","Answer"
"79399969","79399929","<p>Passing a <code>Generator</code> to <code>sample</code> just changes the way the generator is initialized, <strong>it won't change the distribution that is used</strong>. Random sampling is <strong>uniform</strong> (<a href=""https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html"" rel=""nofollow noreferrer""><code>choice</code></a> is used internally [<a href=""https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/sample.py#L152"" rel=""nofollow noreferrer"">source</a>]) and you can't change that directly with the <code>random_state</code> parameter.</p>
<p>Also note that normal sampling doesn't really make sense for discrete values (like the rows of a DataFrame).</p>
<p>Now let's assume that you want to sample the rows of your DataFrame in a non-uniform way (for example with weights that follow a normal distribution) you could use the <code>weights</code> parameter to pass custom weights for each row.</p>
<p>Here is an example with normal weights (although I'm not sure if this makes much sense):</p>
<pre><code>rng = np.random.default_rng()
weights = abs(rng.normal(size=len(df)))

sampled = df.sample(n=10000, replace=True, weights=weights)
</code></pre>
<p>Another example based on <a href=""https://stackoverflow.com/questions/37411633/how-to-generate-a-random-normal-distribution-of-integers"">this Q/A</a>. Here we'll give higher probabilities to the rows from the middle of the DataFrame:</p>
<pre><code>from scipy.stats import norm

N = len(df)
weights = norm.pdf(np.arange(N)-N//2, scale=5)
df.sample(n=10, weights=weights).sort_index()
</code></pre>
<p>Output (mostly rows around 50):</p>
<pre><code>    temperature
43           94
44           50
47           80
48           99
50           63
51           52
52            1
53           20
54           41
63            3
</code></pre>
<p>Probabilities of sampling with a bias for the center (and sampled points):</p>
<p><a href=""https://i.sstatic.net/tCrMi1Oy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tCrMi1Oy.png"" alt=""non-uniform sampling of rows pandas DataFrame"" /></a></p>
","2","Answer"
"79401330","79400579","<p>To add the column to an existing DF, you can generate a list of the correct size using a comprehension:</p>
<pre><code>df_test['random_genator'] = [random.uniform(0.0001, 1.0000) for _ in range(len(list_reg))]
</code></pre>
<p>which gives (for example):</p>
<pre><code>    region  product1   product2    product3  product4  product5  random_genator
0  region1       100      41.00        7.04    236.00       NaN        0.052458
1  region2       250     111.00        2.09    249.00      1.33        0.087278
2  region3       350      12.14       11.14    400.00      2.54        0.407301
3  region4       555      16.18  2000320.00      0.56      1.00        0.107789
4  region5    999999        NaN       22.17    359.00      0.90        0.901209
5  region6    200000  200003.00         NaN    122.00      3.20        0.038250
</code></pre>
","1","Answer"
"79401489","79401345","<p>Another dataframe might facilitate subsequent operations more easily.</p>
<pre><code>df_new = df_stack_exchange.dtypes.to_frame('Type').reset_index()
df_new.rename(columns={'index':'Name'}, inplace=True)
df_new

    Name    Type
0   store   object
1   worker  int64
2   boxes   int64
</code></pre>
","0","Answer"
"79402620","79402584","<p>If need set only <code>0</code> to first value and gount difference per groups:</p>
<pre><code>rating_factors = ['Var1','Var2','Var3','Var4']
all_c1['incr Value'] = (all_c1.groupby(rating_factors, dropna=False)['CumValue'].diff()
                              .fillna(all_c1['CumValue']))
all_c1.loc[0, 'incr Value'] = 0
print (all_c1)
  Var1 Var2 Var3 Var4  CumValue  incr Value
0   V1   X1   L1   R1        19         0.0
1   V1   X1   L1   R1        20         1.0
2   V1   X1   L1   R1        26         6.0
3   V1   X1   L1   R1        30         4.0
4   V2   X1   L1   R1         1         1.0
5   V2   X1   L1   R1        25        24.0
6   V2   X1   L1   R1        36        11.0
7   V2   X1   L1   R1        49        13.0
</code></pre>
<p>Alternative:</p>
<pre><code>rating_factors = ['Var1','Var2','Var3','Var4']
all_c1['incr Value'] = all_c1['CumValue']
                         .sub(all_c1.groupby(rating_factors, dropna=False)['CumValue']
                                    .shift(fill_value=0))

all_c1.loc[0, 'incr Value'] = 0
print (all_c1)
  Var1 Var2 Var3 Var4  CumValue  incr Value
0   V1   X1   L1   R1        19           0
1   V1   X1   L1   R1        20           1
2   V1   X1   L1   R1        26           6
3   V1   X1   L1   R1        30           4
4   V2   X1   L1   R1         1           1
5   V2   X1   L1   R1        25          24
6   V2   X1   L1   R1        36          11
7   V2   X1   L1   R1        49          13
</code></pre>
","0","Answer"
"79404233","79403118","<p>Cf. <a href=""https://stackoverflow.com/q/72752927"">this post</a>, which notes the same problem with plain <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html"" rel=""nofollow noreferrer""><code>df.rolling</code></a>. I would suggest creating an iterator with the timestamps using <a href=""https://docs.python.org/3/library/functions.html#iter"" rel=""nofollow noreferrer""><code>iter</code></a> and selecting the correct timestamp for each window rank via <a href=""https://docs.python.org/3/library/functions.html#next"" rel=""nofollow noreferrer""><code>next</code></a> inside <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.window.rolling.Rolling.apply.html"" rel=""nofollow noreferrer""><code>rolling.apply</code></a>. Basically:</p>
<pre class=""lang-py prettyprint-override""><code># adding another group, with prices reversed

dummy_df = pd.concat([dummy_df, dummy_df.assign(
    zone='B', price=dummy_df['price'].values[::-1]
    )], ignore_index=True)

gr_by = dummy_df.set_index('time').groupby('zone')
ts_iter = iter(dummy_df.index.tolist())

s = (gr_by.rolling(window='3h', center=True, closed='both')['price']
     .apply(lambda x: x.rank(method='min')[next(ts_iter)])
     .rename('price_rank')
     )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>zone  time               
A     2025-01-01 00:00:00    1.0
      2025-01-01 01:00:00    2.0
      2025-01-01 02:00:00    2.0
      2025-01-01 03:00:00    2.0
      2025-01-01 04:00:00    2.0
B     2025-01-01 00:00:00    2.0
      2025-01-01 01:00:00    2.0
      2025-01-01 02:00:00    2.0
      2025-01-01 03:00:00    2.0
      2025-01-01 04:00:00    1.0
Name: price_rank, dtype: float64
</code></pre>
<p>You can then use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>df.merge</code></a> to add the result to <code>dummy_df</code>. So, for your <code>class</code> that could be:</p>
<pre class=""lang-py prettyprint-override""><code>class RollingRank:
    def __init__(self, rank_col: str, group_by_col: str, window_length: int, 
                 time_col: str):
        self.rank_col = rank_col
        self.time_col = time_col
        self.group_by_col = group_by_col
        self.window_length = window_length

    def fit_transform(self, df):
        df = df.sort_values(by=[self.group_by_col, self.time_col])
        df.set_index(self.time_col, inplace=True)
        
        gr_by = df.groupby(self.group_by_col)
        
        ts_iter = iter(df.index.tolist())

        dfg = (gr_by.rolling(window=f'{self.window_length}h', 
                         center=True, 
                         closed='both')['price']
               .apply(lambda x: x.rank(method='min')[next(ts_iter)])
               .rename(f'{self.rank_col}_rank')
               )
        
        df = df.merge(dfg, on=['zone', 'time'], how='left')
        return df

df_rank = (RollingRank(rank_col=&quot;price&quot;, 
                       group_by_col=&quot;zone&quot;, 
                       window_length=3, 
                       time_col=&quot;time&quot;)
           .fit_transform(dummy_df)
           )
</code></pre>
","0","Answer"
"79404398","79404365","<p>You can use <a href=""https://pyjanitor-devs.github.io/pyjanitor/api/functions/#janitor.functions.conditional_join"" rel=""nofollow noreferrer"">conditional_join from pyjanitor</a> to merge based on your conditions.</p>
<pre><code># pip install pyjanitor
import janitor

df = df1.conditional_join(df2, ('low', 'value', '&lt;='), 
                          ('high', 'value', '&gt;='), ('key', 'key', '=='), 
                          right_columns='value')
</code></pre>
<p>End result:</p>
<pre><code>key  low  high  value
  a    0     2      1
  b    1     4      2
</code></pre>
","0","Answer"
"79404869","79404815","<p>A possible solution, which is base on <a href=""https://numpy.org/doc/stable/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>numpy.where</code></a>:</p>
<pre><code>np.where((df['age'] &gt; 25) &amp; (df['age'] &lt; 36), 1, 
         np.where((df['age'] &gt; 35) &amp; (df['age'] &lt; 46), 2, 
                  np.where((df['age'] &gt; 45) &amp; (df['age'] &lt; 56), 3, 0)))
</code></pre>
","0","Answer"
"79405237","79405200","<p>You can try to use the following snippet.</p>
<pre class=""lang-py prettyprint-override""><code>df1.loc[df1.index[-1], 'col1'] = df2.loc[df2.index[-1], 'col1']
</code></pre>
<p>On my machine with pandas version 2.2.3, it gives no warnings.</p>
","1","Answer"
"79406633","79406608","<p>convert start date and end date to date objects by date() method</p>
<pre><code>start_date = datetime(year=2010, month=1, day=1).date()
end_date = datetime(year=2025, month=1, day=1).date()
</code></pre>
","-2","Answer"
"79406648","79406608","<p>I discovered that for this to work, the index has to be ordered, though I'm not sure whether this is the expected behavior. So adding this before doing the selection fixes the issue:</p>
<pre class=""lang-py prettyprint-override""><code>data.sort_index(inplace=True)
</code></pre>
","0","Answer"
"79407167","79406608","<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>df.loc</code></a> slices by index order, not by chronological date order. I.e., reverse <code>start_date</code> and <code>end_date</code>:</p>
<p><strong><a href=""https://stackoverflow.com/help/minimal-reproducible-example"">Minimal, Reproducible Example</a></strong></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from datetime import datetime

idx = pd.date_range('2025-01-04', '2010-01-01', freq='-1d', name='when')
data = pd.DataFrame({'price': np.random.default_rng(0).random(len(idx))}, 
                    index=idx)

data.loc[end_date:start_date] # reversed
</code></pre>
<p>Result:</p>
<pre class=""lang-py prettyprint-override""><code>               price
when                
2025-01-01  0.016528
2024-12-31  0.813270
2024-12-30  0.912756
2024-12-29  0.606636
2024-12-28  0.729497
             ...
2010-01-05  0.288949
2010-01-04  0.608021
2010-01-03  0.111751
2010-01-02  0.385724
2010-01-01  0.172269
</code></pre>
<p>Similarly, you need:</p>
<ul>
<li><code>data.loc[:start_date]</code> instead of <code>data.loc[start_date:]</code></li>
<li><code>data.loc[end_date:]</code> instead of <code>data.loc[:end_date]</code></li>
</ul>
<hr />
<p>However, note that <code>df.loc</code> is optimized for a monotonically <em>increasing</em> <code>DatetimeIndex</code>.
I.e., this works:</p>
<pre class=""lang-py prettyprint-override""><code>data.sort_index().loc['2025-01-03 23:59':]

               price
when                
2025-01-04  0.636962
</code></pre>
<p>Yet the following throws an error:</p>
<pre class=""lang-py prettyprint-override""><code>data.loc[:'2025-01-03 23:59']
</code></pre>
<blockquote>
<p>KeyError: 'Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is not allowed.'</p>
</blockquote>
<p>That's a poorly phrased message, seeing that:</p>
<pre class=""lang-py prettyprint-override""><code>data.index.is_monotonic_decreasing
# True
</code></pre>
<p>Our index is monotonic, just not monotonically <em>increasing</em>. As a result, it fails <a href=""https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/indexes/datetimes.py#L677-L681"" rel=""nofollow noreferrer"">this condition</a>:</p>
<pre class=""lang-py prettyprint-override""><code>        if (
            check_str_or_none(start)
            or check_str_or_none(end)
            or self.is_monotonic_increasing     # False!
        )
</code></pre>
<p>Subsequently, the operation errors out a few lines later. You would need:</p>
<pre class=""lang-py prettyprint-override""><code>data.loc[data.index &gt;= '2025-01-03 23:59']

# or: data.truncate(before='2025-01-03 23:59')
</code></pre>
<p>Best practice is to work with a monotonically increasing timeseries.</p>
","1","Answer"
"79407332","79407233","<p>In python, you can directly use Pandas dataframe to handle Decimal objects.
Also, I tried using json.loads() instead of eval() or literal_eval() as the data in your example seems to be JSON-like.</p>
<p>Next, try passing the data either as bytes or as a file-like object to use streamlit download.</p>
","0","Answer"
"79407973","79407952","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>DataFrame.merge</code></a> with <code>DataFrame</code> constructor:</p>
<pre><code>#if possible duplicates in ps remove them
ps = ps.drop_duplicates()

df = df.merge(pd.DataFrame({'idx': ps.index, 'a':ps.values}), on='a')
print (df)
           a  idx
0  [a, b, c]    0
1  [a, c, b]    1
2  [c, a, b]    4
</code></pre>
<p>Solution for oldier pandas versions - converting lists to tuples before <code>merge</code>:</p>
<pre><code>df1 = ps.apply(tuple).reset_index().drop_duplicates(0)
print (df1)
   index          0
0      0  (a, b, c)
1      1  (a, c, b)
2      2  (b, a, c)
3      3  (b, c, a)
4      4  (c, a, b)
5      5  (c, b, a)

df = (df.merge(df1, left_on=df['a'].apply(tuple),right_on=df1[0])
       .drop(['key_0',0], axis=1))
print (df)
           a  index
0  [a, b, c]      0
1  [a, c, b]      1
2  [c, a, b]      4
</code></pre>
","1","Answer"
"79408275","79402987","<blockquote>
<p>Getting the token from AzureCliCredential is slow. Is there a way to make pandas/fsspec cache the token so that the slow token retrieval process is not repeated over and over again when I open many files?</p>
</blockquote>
<p>I agree with mdurant's comment, that if you read multiple files within the same session, <code>fsspec</code> should reuse the original filesystem instance and its credentials.</p>
<p>When using <code>AzureCliCredential</code>, the token lifetime depends on the Azure AD configuration, and it typically lasts for <code>1 hour</code> before expiring.</p>
<p>You can use the below code that explains to persist and reuse <code>tokens</code> across sessions by caching them to <code>disk</code>.</p>
<p><strong>Code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>import json
import os
from datetime import datetime, timezone
import pandas as pd

from azure.core.credentials import AccessToken, TokenCredential
from azure.identity import AzureCliCredential

TOKEN_CACHE_FILE = &quot;azure_token_cache.json&quot;

class CachedCredential(TokenCredential):
    def __init__(self, underlying_credential):
        self.underlying_credential = underlying_credential
        self._token = None
        self._expires_on = None
        self.load_cached_token()

    def load_cached_token(self):
        if os.path.exists(TOKEN_CACHE_FILE):
            try:
                with open(TOKEN_CACHE_FILE, &quot;r&quot;) as f:
                    cache = json.load(f)
                expiry_datetime = datetime.fromtimestamp(cache[&quot;expires_on&quot;], timezone.utc)
                if expiry_datetime &gt; datetime.now(timezone.utc):
                    self._token = cache[&quot;token&quot;]
                    self._expires_on = cache[&quot;expires_on&quot;]
                    print(&quot;Loaded cached token, expires at:&quot;, expiry_datetime)
            except Exception as e:
                print(&quot;Failed to load cached token:&quot;, e)

    def save_token(self):
        cache = {&quot;token&quot;: self._token, &quot;expires_on&quot;: self._expires_on}
        with open(TOKEN_CACHE_FILE, &quot;w&quot;) as f:
            json.dump(cache, f)

    def get_token(self, *scopes, **kwargs):
        now_ts = datetime.now(timezone.utc).timestamp()
        if self._token is None or now_ts &gt;= self._expires_on:
            token_obj = self.underlying_credential.get_token(*scopes, **kwargs)
            self._token = token_obj.token
            self._expires_on = token_obj.expires_on
            self.save_token()
            expiry_datetime = datetime.fromtimestamp(self._expires_on, timezone.utc)
            print(&quot;Fetched new token, expires at:&quot;, expiry_datetime)
        return AccessToken(self._token, self._expires_on)

def main():
    underlying_credential = AzureCliCredential()
    cached_credential = CachedCredential(underlying_credential)
    
    token_obj = cached_credential.get_token(&quot;https://storage.azure.com/.default&quot;)
    token_str = token_obj.token
    expiry_datetime = datetime.fromtimestamp(token_obj.expires_on, tz=timezone.utc)
    
    print(&quot;\nAccess Token:&quot;)
    print(token_str)
    print(&quot;\nExpires On:&quot;)
    print(expiry_datetime)
    storage_options = {
        &quot;account_name&quot;: &quot;xxxxx&quot;,  # Replace with your actual storage account name.
        &quot;credential&quot;: cached_credential  # Pass the credential object.
    }
    try:
        df = pd.read_csv(&quot;abfs://sample/001.csv&quot;, storage_options=storage_options)
        print(&quot;\nDataFrame Head:&quot;)
        print(df.head())
    except Exception as e:
        print(&quot;\nError reading file:&quot;, e)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Fetched new token, expires at: 2025-02-03 09:48:59+00:00

Access Token:
xxxxx

Expires On:
2025-02-03 xxxxx9+00:00

DataFrame Head:
   PassengerId  Survived  Pclass  \
0            1         0       3   
1            2         1       1   
2            3         1       3   
3            4         1       1   
4            5         0       3   

                                                Name     Sex   Age  SibSp  \
0                            Braund, Mr. Owen Harris    male  22.0      1   
1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   
2                             Heikkinen, Miss. Laina  female  26.0      0   
3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   
4                           Allen, Mr. William Henry    male  35.0      0   

   Parch            Ticket     Fare Cabin Embarked  
0      0         A/5 21171   7.2500   NaN        S  
1      0          PC 17599  71.2833   C85        C  
2      0  STON/O2. 3101282   7.9250   NaN        S  
3      0            113803  53.1000  C123        S  
4      0            373450   8.0500   NaN        S  
</code></pre>
<p>But I will suggest you use SAS token as an alternative to using credentials like <code>AzureCliCredential</code></p>
<p><strong>Code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

storage_options = {
    &quot;account_name&quot;: &quot;your_account_name&quot;,
    &quot;sas_token&quot;: &quot;your_sas_token&quot;
}

df = pd.read_csv(&quot;abfs://your_container/your_file.csv&quot;, storage_options=storage_options)
print(df.head())
</code></pre>
<p>You can generate <code>sas</code> token for long time expiration to read the csv files.</p>
<p><strong>Reference:</strong></p>
<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/tutorial-use-pandas-spark-pool"" rel=""nofollow noreferrer"">Use Pandas to read/write ADLS data in serverless Apache Spark pool in Synapse Analytics - Azure Synapse Analytics | Microsoft Learn</a></li>
<li><a href=""https://stackoverflow.com/questions/79304635/azure-synapse-time-out-token-expire/79305035#79305035"">timeout - Azure Synapse time out - Token expire - Stack Overflow</a> by JayashankarGS</li>
</ul>
","2","Answer"
"79408492","79392369","<p>Possible cause:</p>
<p>your awk script's forelast line is:</p>
<pre><code>mv &quot;${ANNOTATED_FILE}.temp&quot; &quot;$ANNOTATED_FILE&quot;`
</code></pre>
<p>with a <em>parasit, unclosed backquote</em> at its end.</p>
<p>I first thought it was a copy-paste error (hence my <a href=""https://stackoverflow.com/review/suggested-edits/36700062"">edit proposal</a>),<br />
however, if the script you run <em>actually</em> contains this backquote,<br />
the first thing you could do is to <strong>improve the error handling</strong> of your toolchain,<br />
so that a fatal error in one of the <code>.sh</code> of that chain doesn't go unnoticed,<br />
making you think that the (untouched, still containing the placeholders) annotated.hg38_multianno.txt is the result of your awk having correctly run.</p>
<p>… And once done, of course you can just <strong>remove that unclosed backquote</strong> to unleash your chain.</p>
","1","Answer"
"79408525","79408524","<p>Figured it out trying to write this, but if anyone have the same issue, the key is in the group.index, and using .loc not .iloc</p>
<pre><code>for name, group in group_by_class:
    mask = group[&quot;child&quot;].notna()
    parent_name = group[mask][&quot;name&quot;].values[0]
    print(group.index)
    df.loc[group.index, 'parents_in_group'] = parent_name
    

    
df
</code></pre>
","0","Answer"
"79408536","79408524","<p>A loop-less approach would be to compute a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.first.html"" rel=""nofollow noreferrer""><code>groupby.first</code></a> after <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html"" rel=""nofollow noreferrer""><code>dropna</code></a>, then to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> the output:</p>
<pre><code>df['parents_in_group'] = df['class'].map(
    df.dropna(subset='child').groupby('class')['name'].first()
)

# variant
df['parents_in_group'] = df['class'].map(
    df['name'].where(df['child'].notna()).groupby(df['class']).first()
)
</code></pre>
<p>Or, with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow noreferrer""><code>drop_duplicates</code></a> in place of <code>groupby</code> (for efficiency):</p>
<pre><code>df['parents_in_group'] = df['class'].map(
    df.dropna(subset='child')
      .drop_duplicates(subset='class')
      .set_index('class')['name']
)
</code></pre>
<p>Output:</p>
<pre><code>  class      name  age     child parents_in_group
0   cat    Fluffy    3  Whiskers           Fluffy
1   dog      Spot    5       NaN            Rover
2   cat  Whiskers    7       NaN           Fluffy
3   dog     Rover    2      Spot            Rover
</code></pre>
<p>Or, if efficiency doesn't really matter with a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html"" rel=""nofollow noreferrer""><code>groupy.apply</code></a>:</p>
<pre><code>out = (df.groupby('class', sort=False, group_keys=False)
         .apply(lambda x: x.assign(parents_in_group=x.loc[x['child'].notna(),
                                                          'name']
                                                     .iloc[:1].squeeze()),
               include_groups=False)
      )
</code></pre>
","1","Answer"
"79408660","79408631","<p>You are getting the wrong values because you are using the wrong manual calculation formula. Instead of using <code>100-(prev/current*100)</code>, you should use <code>((current-prev)/prev)*100</code>. Then, it will be same with <code>Pandas .pct_change()</code></p>
","1","Answer"
"79408689","79408631","<p>You inverted <code>prev</code> and <code>current</code> in your formula, which means that your % change doesn't have the same reference in both cases (<code>current</code> or <code>prev</code>).</p>
<p>This is the same as if you consider two numbers 100 and 110. If you take 100 as reference you have 10% increase, if you take 110 you have -9.0909...% decrease.</p>
<p>Your formula should thus be:</p>
<pre><code>100 - (current/prev *100)
</code></pre>
<p>If you want pandas's <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.pct_change.html"" rel=""nofollow noreferrer""><code>pct_change</code></a> to match your computation you should reverse the Series (then shift):</p>
<pre><code>df['rev_pct_change'] = df.loc[::-1, 'close'].pct_change().mul(-100).shift(-1)
</code></pre>
<p>which can also be obtained by setting the <code>periods</code> parameter to <code>-1</code>:</p>
<pre><code>df['rev_pct_change'] = df['close'].pct_change(-1).mul(-100).shift()
</code></pre>
<p>Example:</p>
<pre><code>    close    manual  pct_change  rev_pct_change
0  0.7419       NaN         NaN             NaN
1  0.7406 -0.175533   -0.175226       -0.175533
2  0.7360 -0.625000   -0.621118       -0.625000
3  0.7330 -0.409277   -0.407609       -0.409277
4  0.7409  1.066271    1.077763        1.066271
5  0.7371 -0.515534   -0.512890       -0.515534
6  0.7371  0.000000    0.000000       -0.000000
7  0.7324 -0.641726   -0.637634       -0.641726
8  0.7115 -2.937456   -2.853632       -2.937456
9  0.7107 -0.112565   -0.112439       -0.112565
</code></pre>
","1","Answer"
"79409674","79407233","<p>pandas <code>.read_sql_query()</code> method can be used to directly create a DataFrame from an SQL query. Then the DataFrame can be written to a CSV file using the <code>.to_csv()</code> method.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import sqlalchemy as sa

engine = sa.create_engine(&quot;postgresql://scott:tiger@192.168.0.199/test&quot;)

sql = &quot;&quot;&quot;\
SELECT 'widget' AS item, CAST(2.99 AS Decimal(18, 4)) AS price
UNION ALL
SELECT 'gadget' AS item, CAST(9.99 AS Decimal(18, 4)) AS price
&quot;&quot;&quot;
df = pd.read_sql_query(sql, engine, coerce_float=False)
print(df)
&quot;&quot;&quot;
     item   price
0  widget  2.9900
1  gadget  9.9900
&quot;&quot;&quot;
print(repr(df.loc[0, &quot;price&quot;]))  # Decimal('2.9900')

df.to_csv(&quot;products.csv&quot;, header=True, index=False)
with open(&quot;products.csv&quot;, &quot;r&quot;) as csv:
    print(csv.read())
&quot;&quot;&quot;
item,price
widget,2.9900
gadget,9.9900
&quot;&quot;&quot;
</code></pre>
","0","Answer"
"79410406","79407233","<p>Try this, it worked for me this way.</p>
<pre><code>import pandas as pd
from decimal import Decimal
import csv
import streamlit as st
import json


db_output = '[{&quot;A&quot;: &quot;Decimal(1.2)&quot;}]' #using this example


def parse_decimal(obj):
    if &quot;Decimal&quot; in obj: #checking and converting to decimal
        return Decimal(obj.replace(&quot;Decimal(&quot;, &quot;&quot;).replace(&quot;)&quot;, &quot;&quot;))
    return obj


data = json.loads(db_output, parse_float=parse_decimal) #converting from json to python obj


df = pd.DataFrame(data)

# converting DataFrame to CSV (in-memory)
csv_data = df.to_csv(index=False).encode('utf-8')

# Streamlit download button
st.download_button(
    label=&quot;Downloading data as CSV&quot;,
    data=csv_data,
    file_name=&quot;data_inp.csv&quot;,
    mime='text/csv'
)
</code></pre>
","0","Answer"
"79410677","79410477","<p>Try setting the directory to the current directory itself, so you can just see if it is properly saved.</p>
<pre class=""lang-py prettyprint-override""><code>directory = r'./'
excel_file_path = os.path.join(directory, 'output.xlsx')
</code></pre>
","0","Answer"
"79411182","79411167","<p>You can use <code>numpy</code>'s <a href=""https://numpy.org/devdocs/reference/generated/numpy.lib.stride_tricks.sliding_window_view.html"" rel=""noreferrer""><code>sliding_window_view</code></a>:</p>
<pre><code>from numpy.lib.stride_tricks import sliding_window_view as swv

N = 3
df['C'] = pd.Series(swv(df['B'], N).tolist(), index=df.index[N-1:])
</code></pre>
<p>Output:</p>
<pre><code>   A  B          C
0  4  9        NaN
1  0  2        NaN
2  4  5  [9, 2, 5]
3  7  9  [2, 5, 9]
4  8  3  [5, 9, 3]
5  8  1  [9, 3, 1]
6  1  4  [3, 1, 4]
7  4  1  [1, 4, 1]
8  1  9  [4, 1, 9]
9  3  7  [1, 9, 7]
</code></pre>
","8","Answer"
"79411511","79411167","<p>I guess you can change your thinking from another way around, say, not <code>row-wise</code> but <code>column-wise</code> sliding windowing, and probably your code could speed up unless you have a large window size <code>N</code>.</p>
<p>For example, you can try</p>
<pre><code>N = 3
nr = len(df)
df['C'] = [None]*(N-1) + np.column_stack([df['B'].iloc[k:nr-N+1+k] for k in range(N)]).tolist()
</code></pre>
<p>and you will obtain</p>
<pre><code>    A   B   C
0   4   9   None
1   0   2   None
2   4   5   [9, 2, 5]
3   7   9   [2, 5, 9]
4   8   3   [5, 9, 3]
5   8   1   [9, 3, 1]
6   1   4   [3, 1, 4]
7   4   1   [1, 4, 1]
8   1   9   [4, 1, 9]
9   3   7   [1, 9, 7]
</code></pre>
","2","Answer"
"79412307","79412180","<p>I googled &quot;pandas to_sql&quot;, which pointed me here: <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html</a>. There is a <code>schema</code> keyword argument listed. So, just do <code>result.to_sql('mapping_table', engine, schema='project_demo', ...)</code>. By passing a dotted schema.table as the first arg, your current code will instead try to write to a table called <code>&quot;project_demo.mapping_table&quot;</code> in the <code>public</code> schema.</p>
","1","Answer"
"79412498","79412451","<p>The dates you're using are all the last days of each month (e.g., 2024-01-31, 2024-02-29, etc.), which can cause the x-axis to shift slightly, especially when formatting. Try adjusting the date to the month start instead.</p>
<pre><code>import pandas as pd
import plotly.express as px

data = {
    '2024-01-31' :   1044,
    '2024-02-29' :   2310,
    '2024-03-31' :    518,
    '2024-04-30' :  -1959,
    '2024-05-31' :      0,
    '2024-06-30' :  -1010,
    '2024-07-31' :   1500,
    '2024-08-31' : -15459,
    '2024-09-30' : -14153,
    '2024-10-31' : -12604,
    '2024-11-30' :  -5918,
    '2024-12-31' :  -3897
}

df = pd.DataFrame(data.items(), columns=['date', 'value'])
# convert to datetime
df['date'] = pd.to_datetime(df['date'])
# Adjust the dates to the first day of the month
df['date'] = df['date'].dt.to_period('M').dt.start_time

fig = px.bar(df, x='date', y='value')

fig.show()
</code></pre>
<p>You can also try the following if you want to preserve the hover data but update the x-axis labels</p>
<pre><code># Convert 'date' column to datetime format
df['date'] = pd.to_datetime(df['date'])
# Create the bar plot
fig = px.bar(df, x='date', y='value')
# Store original 'date' column in customdata for hover
fig.update_traces(
    customdata=df['date']  # Store the original date for hover text
)

# Update x-axis to show only month and year
fig.update_xaxes(
    tickvals=df['date'],
    ticktext=df['date'].dt.strftime('%b %Y')  # Format: abbreviated month and full year (e.g., Jan 2024)
)

# Customize hover template to show the original date (from customdata)
fig.update_traces(
    hovertemplate=&quot;&lt;b&gt;Date:&lt;/b&gt; %{customdata|%b %d, %Y}&lt;br&gt;&lt;b&gt;Value:&lt;/b&gt; %{y}&lt;extra&gt;&lt;/extra&gt;&quot;
)

fig.show()
</code></pre>
","2","Answer"
"79413038","79412275","<p>Here's one approach that should be much faster:</p>
<p><strong>Data sample</strong></p>
<pre class=""lang-py prettyprint-override""><code>num_cols = 4
n_changes = 6
np.random.seed(0) # reproducibility

# setup ...
</code></pre>
<pre><code>df_change

   col       val
1    C  0.144044
4    A  1.454274
5    A  0.761038
7    A  0.121675
7    C  0.443863
10   B  0.333674
</code></pre>
<pre><code>state

{'A': 0.5488135039273248,
 'B': 0.7151893663724195,
 'C': 0.6027633760716439,
 'D': 0.5448831829968969}
</code></pre>
<p><strong>Code</strong></p>
<pre class=""lang-py prettyprint-override""><code>out = (df_change.reset_index()
       .pivot_table(index='index', 
                    columns='col', 
                    values='val',
                    aggfunc='last')
       .rename_axis(index=None, columns=None)
       .assign(product='some_product')
       .reindex(columns=extra_colums + columns)
       .fillna(pd.DataFrame(state, index=[index[0]]))
       .ffill()
       )
</code></pre>
<p>Output</p>
<pre class=""lang-py prettyprint-override""><code>    n_changes       product         A         B         C         D
1         NaN  some_product  0.548814  0.715189  0.144044  0.544883
4         NaN  some_product  1.454274  0.715189  0.144044  0.544883
5         NaN  some_product  0.761038  0.715189  0.144044  0.544883
7         NaN  some_product  0.121675  0.715189  0.443863  0.544883
10        NaN  some_product  0.121675  0.333674  0.443863  0.544883

# note:
# A updated in 4, 5, 7
# B updated in 10
# C updated in 1, 7
</code></pre>
<p><strong>Explanation / Intermediates</strong></p>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>df.reset_index</code></a> to access 'index' inside <a href=""https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html"" rel=""nofollow noreferrer""><code>df.pivot_table</code></a>. For the <code>aggfunc</code> use <code>'last'</code>. I.e., we only need to propagate the <em>last</em> value in case of duplicate 'col' values per index value.</li>
<li>Cosmetic: use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename_axis.html"" rel=""nofollow noreferrer""><code>df.rename_axis</code></a> to reset index and columns <em>names</em> to <code>None</code>.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># df_chagne.reset_index().pivot_table(...).rename_axis(...)

           A         B         C
1        NaN       NaN  0.144044
4   1.454274       NaN       NaN
5   0.761038       NaN       NaN
7   0.121675       NaN  0.443863
10       NaN  0.333674       NaN
</code></pre>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>df.assign</code></a> to add column 'product' with a scalar ('some_product').</li>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>df.reindex</code></a> to get the columns in the desired order (with <code>extra_columns</code> up front). Not yet existing column 'n_changes' will be added with <code>NaN</code> values.</li>
<li>Now, apply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer""><code>df.fillna</code></a> and use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>pd.DataFrame</code></a> with <code>state</code> for <em>only</em> the first index value (<code>index[0]</code>), to fill the first row (alternatively, use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.combine_first.html"" rel=""nofollow noreferrer""><code>df.combine_first</code></a>).</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># after .fillna(...)

    n_changes       product         A         B         C         D
1         NaN  some_product  0.548814  0.715189  0.144044  0.544883
4         NaN  some_product  1.454274       NaN       NaN       NaN
5         NaN  some_product  0.761038       NaN       NaN       NaN
7         NaN  some_product  0.121675       NaN  0.443863       NaN
10        NaN  some_product       NaN  0.333674       NaN       NaN
</code></pre>
<ul>
<li>Finally, we want to forward fill: <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html"" rel=""nofollow noreferrer""><code>df.ffill</code></a>.</li>
</ul>
<hr />
<p>Performance comparison:</p>
<pre class=""lang-py prettyprint-override""><code>num_cols = 100
n_changes = 100
np.random.seed(0) # reproducibility

# out:
7.01 ms ± 435 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# df2 (running this *afterwards*, as you are updating `state`
93.7 ms ± 3.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
<p>Equality check:</p>
<pre class=""lang-py prettyprint-override""><code>df2.astype(out.dtypes).equals(out)
# True
</code></pre>
","2","Answer"
"79413523","79412451","<p>The first proposal allows specifying the range of the x-axis, so setting <code>range_x</code> will change the scale of the x-axis that is automatically adjusted. This explanation can be found <a href=""https://plotly.com/python/time-series/#time-series-plot-with-custom-date-range"" rel=""nofollow noreferrer"">here</a>.</p>
<pre><code>import plotly.express as px

fig = px.bar(data_frame=df, x='date', y='value', range_x=['2024-01-31','2024-12-31'])
fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/nScRzYlP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nScRzYlP.png"" alt=""enter image description here"" /></a></p>
<p>My second suggestion is to customize the display of the x-axis. To make it equivalent to the x-axis in the question, change the display criteria to 'M2'. There is a ticklabelmode, and there is an instant and a period. Changing this to 'period' will set the string between the scales. See <a href=""https://plotly.com/python/time-series/#moving-tick-labels-to-the-middle-of-the-period"" rel=""nofollow noreferrer"">here</a> for details. The rest is adjusting the position of the scale. This adjustment needs to be adjusted depending on the environment in which it will be used.</p>
<pre><code>fig = px.bar(data_frame=df, x='date', y='value')
fig.update_xaxes(dtick=&quot;M2&quot;, tickformat=&quot;%b %Y&quot;, ticklabelmode=&quot;period&quot;, ticklabelshift=40) 
fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/4B4UnRLj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4B4UnRLj.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79414046","79414030","<p>Here is example how effectively process each chunk with pandas:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

filename = &quot;large_data.csv&quot;

chunksize = 10_000
total_sum = 0

for chunk in pd.read_csv(filename,chunksize=chunksize):
    total_sum += chunk[&quot;some_numeric_column&quot;].sum()

print(&quot;Grand total:&quot;, total_sum)
</code></pre>
<p>And here is example with Dask:</p>
<pre class=""lang-py prettyprint-override""><code>import dask.dataframe as dd

filename = &quot;large_data.csv&quot;

ddf = dd.read_csv(filename,blocksize=&quot;64MB&quot;)  

result = ddf[&quot;some_numeric_column&quot;].mean().compute()
print(&quot;Mean:&quot;, result)
</code></pre>
","1","Answer"
"79414404","79409450","<p>With thanks to @Paulo Marques for the suggestion: I converted the latitude and longitude fields to <code>str</code> type while the data were still in a pandas dataframe.\</p>
<p>The block that converts to Spark and writes to table now looks like this:</p>
<pre><code>basic_df = pd.DataFrame(basic_data_list)
basic_df.latitude = basic_df.latitude.astype(str)
basic_df.longitude = basic_df.longitude.astype(str)

    if not basic_df.empty:
            basic_spark_df = spark.createDataFrame(basic_df, schema=basic_schema)
            basic_spark_df.write.mode(&quot;append&quot;).option(&quot;mergeSchema&quot;, &quot;true&quot;).format(&quot;delta&quot;).saveAsTable(&quot;api_test&quot;)          
</code></pre>
<p>And all the data now pull through without any errors.</p>
","1","Answer"
"79415154","79415141","<p>I was overeager asking this, but in case it helps anyone else:</p>
<pre><code>df1['bool_col'] = df1['bool_col'] == 'True'
</code></pre>
<p>Will set only True vales to true, and anything else as False.</p>
","1","Answer"
"79415176","79415141","<p>If you want a generic approach, you could use fuzzy matching, for example with <a href=""https://github.com/seatgeek/thefuzz"" rel=""nofollow noreferrer""><code>thefuzz</code></a>:</p>
<pre><code>from thefuzz import process

def to_bool(s, threshold=60):
    bools   = [True, False]
    choices = list(map(str, bools))
    match, score = process.extractOne(s, choices)
    d = dict(zip(choices, bools))
    if score &gt; threshold:
        return d[match]

df['bool'] = df['bool_col'].map(to_bool)
</code></pre>
<p>Example:</p>
<pre><code>  bool_col  foo   bool
0     True    1   True
1    Flase    2  False
2    False    3  False
3     True    4   True
4     ture    5   True
5   banana    6   None
</code></pre>
","1","Answer"
"79415201","79414030","<p>Accordingly to Pandas documentation, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv"" rel=""nofollow noreferrer"">read_csv</a> invoked with the chunksize argument returns a collection of dataframes.</p>
<pre><code>import pandas as pd

df_collection = pd.read_csv('large_file.gz',
                 chunksize=1000000)


for df in df_collection:
    #your process here
</code></pre>
<p>You can search about out-of-core methods for loading a large datasets in python. There are other ways to deal with this situations as well</p>
","0","Answer"
"79415414","79414916","<p>You were not far from getting it right. I just made a few adjustments to your code:</p>
<pre><code>import streamlit as st
import plotly.express as px
import pandas as pd

st.title(&quot;Custom Bubble Chart with Negative Value Highlighting&quot;)

data = {'x': [1.5, 1.6, -1.2, -2.0, 2.3, -3.5],
        'y': [21, 16, 46, 30, 10, 55],
        'circle-size': [10, 5, 6, 8, 12, 9]}

df = pd.DataFrame(data)

df[&quot;color&quot;] = df[&quot;x&quot;].apply(lambda val: &quot;red&quot; if val &lt; 0 else &quot;blue&quot;)

st.dataframe(df)

fig = px.scatter(df, x=&quot;x&quot;, y=&quot;y&quot;, 
                 size=&quot;circle-size&quot;,
                 color=&quot;color&quot;,
                 color_discrete_map={&quot;red&quot;: &quot;red&quot;, &quot;blue&quot;: &quot;blue&quot;},
                 title=&quot;Bubble Chart (Red = Negative X, Blue = Positive X)&quot;)

st.plotly_chart(fig)

</code></pre>
<p>which gives</p>
<p><a href=""https://i.sstatic.net/6HGhUZuB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6HGhUZuB.png"" alt=""streamlit image"" /></a></p>
","0","Answer"
"79415610","79401345","<p>Bit late to the party, but what you are really missing is a mapping from the pandas columns' types to the types you provide in your edit.</p>
<p>I've written the below dictionary to do that, and to build a string like you specified. You'll want to modify it to add SubType where necessary, and may want to modify it to construct a json instead of a string.</p>
<pre><code>mapping_dict = {
&quot;object&quot;: &quot;STRING&quot;,
&quot;string&quot;: &quot;STRING&quot;,
&quot;boolean&quot;: &quot;BOOLEAN&quot;,
&quot;category&quot;: &quot;STRING&quot;,
&quot;int8&quot;: &quot;INTEGER&quot;,
&quot;int16&quot;: &quot;INTEGER&quot;,
&quot;int32&quot;: &quot;INTEGER&quot;,
&quot;int64&quot;: &quot;INTEGER&quot;,
&quot;uint8&quot;: &quot;INTEGER&quot;,
&quot;uint16&quot;: &quot;INTEGER&quot;,
&quot;uint32&quot;: &quot;INTEGER&quot;,
&quot;uint64&quot;: &quot;INTEGER&quot;,
&quot;float16&quot;: &quot;DECIMAL&quot;,
&quot;float32&quot;: &quot;DECIMAL&quot;,
&quot;float64&quot;: &quot;DECIMAL&quot;,
&quot;datetime64&quot;: &quot;DATETIME&quot;,
&quot;timedelta64&quot;: &quot;STRING&quot;,
&quot;bool&quot;: &quot;BIT&quot;,
&quot;str&quot;: &quot;STRING&quot;,
&quot;unicode&quot;: &quot;STRING&quot;,
&quot;bytes&quot;: &quot;STRING&quot;,
&quot;dict&quot;: &quot;JSON&quot;,
&quot;list&quot;: &quot;JSON&quot;,
}


result_string=&quot;&quot;
for name, dtype in df_stack_exchange.dtypes.to_dict().items():
    result_string +=(f'{{Name: {name}, Type: {mapping_dict[str(dtype)]}}}, ')

print(result_string)
</code></pre>
","0","Answer"
"79416380","79412543","<p>This method will not be fast if you have a big file (but fast enough with the file you use):</p>
<pre><code>df = pd.read_fwf(url,skiprows=8,skipfooter=6,header=1)
# Correct the first-half line by filling with the second-half values on next line
for i in range(len(df)):
    if df[&quot;Name&quot;].isna().iloc[i]:  # check if it is the second-half of a broken line
        df.iloc[i - 1, 2:10] = df.iloc[i, 2:10]
        if type(df.iloc[i][&quot;CSA CBSA&quot;]) is str:
            df.iloc[i - 1, 1] += df.iloc[i, 0]

# Remove the second-half line
df = df.dropna(subset=[&quot;Name&quot;])

display(df[16:20])
</code></pre>
<p><a href=""https://i.sstatic.net/CU1tolMr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CU1tolMr.png"" alt=""df"" /></a></p>
","1","Answer"
"79417385","79415726","<p><strong>TL;DR</strong></p>
<p>Your original <code>df</code> has a single block in memory.</p>
<p>With <code>stmt1</code>, use of <code>df[&quot;product&quot;] = &quot;x&quot;</code> makes the <code>BlockManager</code> (an internal memory manager) add a new block. Having multiple blocks adds overhead, as <code>pandas</code> needs to check and consolidate them each time a row gets modified.</p>
<p>With <code>stmt3</code>, you do not have this issue, as <code>df.loc[index,&quot;product&quot;] = &quot;x&quot;</code> is an <em>in-place modification</em>, that keeps the original, single block intact.</p>
<p><code>stmt2</code> should be ignored (see note at the end). <code>stmt4</code> is irrelevant, as the second block is created only after the for loop.</p>
<hr />
<p><strong>Answer</strong></p>
<p>The difference in performance between your <code>stmt1</code> and <code>stmt3</code> has to do with the so-called <code>BlockManager</code>, which is an <a href=""https://github.com/pandas-dev/pandas/blob/main/pandas/core/internals/managers.py#L142"" rel=""nofollow noreferrer"">internal manager</a> that tries to keep columns with compatible dtypes together as <em>blocks</em> in memory.</p>
<ul>
<li><strong>Initial situation: one block</strong></li>
</ul>
<p>Useful information about the use of the <code>BlockManager</code> for a specific <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>pd.DataFrame</code></a> can be retrieved by accessing <a href=""https://github.com/pandas-dev/pandas/blob/main/pandas/core/frame.py#L653"" rel=""nofollow noreferrer""><code>df._mgr</code></a>. With your example:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

num_cols = 3
n_iter = 3
extra_column = [&quot;product&quot;]
columns = [chr(i+65) for i in range(num_cols)]
index= range(n_iter)

df = pd.DataFrame(index=index, columns=extra_column + columns)

  product    A    B    C
0     NaN  NaN  NaN  NaN
1     NaN  NaN  NaN  NaN
2     NaN  NaN  NaN  NaN
</code></pre>
<pre><code>df._mgr

BlockManager
Items: Index(['product', 'A', 'B', 'C'], dtype='object')
Axis 1: RangeIndex(start=0, stop=3, step=1)
NumpyBlock: slice(0, 4, 1), 4 x 3, dtype: object    # all cols
</code></pre>
<p>So, here we see that the <code>BlockManager</code> is working with a single block in memory.</p>
<ul>
<li><code>stmt1</code>: <strong>adding a new column / replacing one adds a block</strong></li>
</ul>
<p>If now we use bracket notation (<code>[]</code>) to assign &quot;x&quot; to column &quot;product&quot;, we are really re-creating that column. As a result, a second block is created:</p>
<pre class=""lang-py prettyprint-override""><code>df[&quot;product&quot;] = &quot;x&quot;
print(df._mgr)

BlockManager
Items: Index(['product', 'A', 'B', 'C'], dtype='object')
Axis 1: RangeIndex(start=0, stop=3, step=1)
NumpyBlock: slice(1, 4, 1), 3 x 3, dtype: object    # cols &quot;A, &quot;B&quot;, &quot;C&quot; 
NumpyBlock: slice(0, 1, 1), 1 x 3, dtype: object    # col &quot;product&quot;
</code></pre>
<p>The important thing here is that this column is <em>replacing</em> the old column &quot;product&quot;: it's a new column. E.g., if we use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>df.loc</code></a> to create a <em>new</em> column, the same thing happens:</p>
<pre class=""lang-py prettyprint-override""><code>df.loc[:, &quot;new_col&quot;] = &quot;x&quot;
print(df._mgr)

BlockManager
Items: Index(['product', 'A', 'B', 'C', 'new_col'], dtype='object')
Axis 1: RangeIndex(start=0, stop=3, step=1)
NumpyBlock: slice(1, 4, 1), 3 x 3, dtype: object    # cols &quot;A, &quot;B&quot;, &quot;C&quot;
NumpyBlock: slice(0, 1, 1), 1 x 3, dtype: object    # col &quot;product&quot;
NumpyBlock: slice(4, 5, 1), 1 x 3, dtype: object    # col &quot;new_col&quot;
</code></pre>
<ul>
<li><code>stmt3</code>: <strong>in-place modification keeps block intact</strong></li>
</ul>
<p>Here we see the difference with <code>df.loc[index,&quot;product&quot;] = &quot;x&quot;</code>, because in this case we are not re-creating &quot;product&quot;, we are simply updating its values. This does not create a new block:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame(index = index, columns=extra_column + columns)
df.loc[index,&quot;product&quot;] = &quot;x&quot;

print(df._mgr)

BlockManager
Items: Index(['product', 'A', 'B', 'C'], dtype='object')
Axis 1: RangeIndex(start=0, stop=3, step=1)
NumpyBlock: slice(0, 4, 1), 4 x 3, dtype: object    # &quot;product&quot; still here
</code></pre>
<hr />
<p><strong>Key takeaways</strong></p>
<p>The upshot of all this for the different versions you use:</p>
<ul>
<li><code>stmt1</code> with <code>df[&quot;product&quot;] = &quot;x&quot;</code> internally has <strong>two</strong> blocks</li>
<li><code>stmt3</code> with <code>df.loc[index,&quot;product&quot;] = &quot;x&quot;</code> internally keeps <strong>one</strong> block</li>
<li><code>stmt4</code> with <code>df[&quot;product&quot;] = &quot;x&quot;</code> <em>after</em> the for loop only has <strong>two</strong> blocks after that loop.</li>
</ul>
<p>The significant delay for <code>stmt1</code> is caused by <code>pandas</code> needing to reconcile multiple blocks each time <code>df.loc[i,columns] = 0</code> is executed in the loop. These internal checks trigger extra memory operations, as <code>pandas</code> must align modified rows across separate blocks. This results in a sizeable slowdown compared to the single-block <code>df</code>.</p>
<p>Interestingly, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html"" rel=""nofollow noreferrer""><code>df.copy</code></a> leads to a reset of the blocks. Consequently, adding <code>df = df.copy()</code> gets the performance of <code>stmt1</code> very close to <code>stmt3</code> again:</p>
<pre class=""lang-py prettyprint-override""><code># adding: `df = df.copy()`
stmt1 =&quot;&quot;&quot;
df = pd.DataFrame(index = index, columns=extra_column + columns)
df[&quot;product&quot;] = &quot;x&quot;
df = df.copy()
for i in index:
    df.loc[i,columns] = 0
&quot;&quot;&quot;

print(f&quot; stmt1 takes { timeit.timeit(setup= setup_stmt, stmt= stmt1,  number=10):2.2f} seconds&quot; )
print(f&quot; stmt3 takes { timeit.timeit(setup= setup_stmt, stmt= stmt3,  number=10):2.2f} seconds&quot; )
</code></pre>
<p>Prints:</p>
<pre class=""lang-py prettyprint-override""><code> stmt1 takes 1.00 seconds
 stmt3 takes 0.99 seconds
</code></pre>
<hr />
<p><strong>Further reading</strong></p>
<p>Some interesting reads on this complex topic and the difficulty of establishing its influence for specific use cases:</p>
<ul>
<li><a href=""https://dkharazi.github.io/blog/blockmanager"" rel=""nofollow noreferrer"">Internal Structure of Pandas DataFrames</a>, by Darius Kharazi (2020-05-15).</li>
<li><a href=""https://uwekorn.com/2020/05/24/the-one-pandas-internal.html"" rel=""nofollow noreferrer"">The one pandas internal I teach all my new colleagues: the BlockManager</a>, blog by Uwe Korn (2020-05-24).</li>
</ul>
<p>There are plans to replace the <code>BlockManager</code>: see <a href=""https://pandas.pydata.org/pandas-docs/version/2.0/development/roadmap.html#block-manager-rewrite"" rel=""nofollow noreferrer"">here</a>, cf. <a href=""https://github.com/wesm/pandas2/blob/master/source/internal-architecture.rst#removal-of-blockmanager--new-dataframe-internals"" rel=""nofollow noreferrer"">here</a>.</p>
<hr />
<p><strong>A note on</strong> <code>stmt2</code></p>
<p><code>stmt2</code> should be ignored here, because it is not doing what you think it does. &quot;dot notation&quot; is a convenience feature that can provide <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#attribute-access"" rel=""nofollow noreferrer"">attribute access</a> to a <code>df</code> column. But this method comes with a few caveats. One being:</p>
<blockquote>
<p>The attribute will not be available if it conflicts with an existing method name, e.g. <code>s.min</code> is not allowed, but <code>s['min']</code> is possible.</p>
</blockquote>
<p>This applies here, because <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.product.html"" rel=""nofollow noreferrer""><code>df.product</code></a> is a method of class <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>pd.DataFrame</code></a>. I.e., when you do <code>df.product = &quot;x&quot;</code>, you are simply overwriting the method and storing the <em>string</em> &quot;x&quot; in its place:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({&quot;product&quot;: [1]})
type(df.product)

method
</code></pre>
<pre class=""lang-py prettyprint-override""><code>df.product = &quot;x&quot;
type(df.product)

str
</code></pre>
<p>I.e., we never updated the actual <code>df</code>:</p>
<pre class=""lang-py prettyprint-override""><code>   product
0        1      # nothing changed
</code></pre>
","2","Answer"
"79417777","79417684","<p>Probably you are after for this?</p>
<pre><code>d = {&quot;A&quot; : 123.02, &quot;B&quot;: 12.3}
for key in d.keys():
    d[key] = str(d[key])
</code></pre>
<p>or a one-liner</p>
<pre><code>dict(zip(d.keys(), map(str, d.values())))
</code></pre>
<p>such that <code>print(d)</code> shows</p>
<pre><code>{'A': '123.02', 'B': '12.3'}
</code></pre>
<hr />
<p>If you want to encapsulate the raw data in a string, you can simply run</p>
<pre><code>str(d)
</code></pre>
<p>which gives</p>
<pre><code>&quot;{'A': 123.02, 'B': 12.3}&quot;
</code></pre>
","1","Answer"
"79418367","79400992","<p>Yes, the code converts YEAR and QUARTER into integers first, then to strings.</p>
<ol>
<li><code>.astype(int)</code> ensures that the values are treated as integers, which removes any potential non-numeric values or floating points.</li>
<li><code>.astype(str)</code> then converts them into strings so that they can be concatenated properly into a single string, forming the date in the 'YYYY-QX' format.</li>
</ol>
<p>The line concatenates YEAR and QUARTER into one string column.
It combines the <code>YEAR</code> and <code>QUARTER</code> columns to create a single string representing the quarter (e.g., <code>'2021-Q1'</code>), which is then used as the index for the DataFrame.</p>
","0","Answer"
"79419098","79419035","<p>You could write a script to extract the data into a separate file. Take a look at <code>readlines()</code>:</p>
<pre><code>with open('Name_of_file.txt', 'r', encoding='utf-8') as file:
        lines = file.readlines()
</code></pre>
<p>Now you have a list of all lines in the file. Get a list of the indices of the lines that are dashes: (using <a href=""https://www.w3schools.com/python/ref_string_startswith.asp"" rel=""nofollow noreferrer"">https://www.w3schools.com/python/ref_string_startswith.asp</a>)</p>
<pre><code>dash_lines = [i for i, line in enumerate(lines) if line.startswith('-')]
</code></pre>
<p>Now you can easily get the index where your data starts:</p>
<pre><code>start_index = dash_lines[-2] + 1  # one after second-to-last dashed line
data_lines = lines[start_index:-1] #don't include last line, which is also dashed
</code></pre>
","1","Answer"
"79419967","79411167","<p>The code slices out the 'B' column of a DataFrame, then forms windows of size three over it. Each sliding window is stored in a list format under a new column ‘C’. The first two rows of ‘C’ have None because the first two elements do not have enough preceding elements to form a window. This process is made easier by the function sliding_window_view, which avoids copying data and instead creates views of the original array.</p>
<pre><code>import pandas as pd
import numpy as np

# Use sliding_window_view for fast rolling window extraction
from numpy.lib.stride_tricks import sliding_window_view

# Sample Data 
np.random.seed(150)

df = pd.DataFrame(np.random.randint(0, 10, size=(10, 2)), columns=['A', 'B'])
print(df)

'''
  A  B
0  4  9
1  0  2
2  4  5
3  7  9
4  8  3
5  8  1
6  1  4
7  4  1
8  1  9
9  3  7
'''

# Convert column to NumPy array
B_values = df['B'].values

'''
 Apply sliding window
Imagine a window of size 3 sliding across the array. 
For each position of the window, it extracts the elements 
within the window.
'''
windows = sliding_window_view(B_values, window_shape=3)


# Create a new column, filling the first two rows with None
df['C'] = [None, None] + windows.tolist()

print(df.head(10))

'''
   A  B          C
0  4  9       None
1  0  2       None
2  4  5  [9, 2, 5]
3  7  9  [2, 5, 9]
4  8  3  [5, 9, 3]
5  8  1  [9, 3, 1]
6  1  4  [3, 1, 4]
7  4  1  [1, 4, 1]
8  1  9  [4, 1, 9]
9  3  7  [1, 9, 7]
'''
</code></pre>
","1","Answer"
"79420172","79420147","<p>You could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html"" rel=""nofollow noreferrer""><code>ffill</code></a> then reshape with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>stack</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html"" rel=""nofollow noreferrer""><code>pivot</code></a>:</p>
<pre><code>out = (df
   .ffill(axis=1).rename_axis(index='idx', columns='col')
   .replace('-', pd.NA)
   .stack().reset_index(name='value')
   .groupby(['idx', 'value'], as_index=False)['col']
   .agg(lambda x: f'=output[{x.min()}:{x.max()}]')
   .assign(n = lambda x: x.groupby('idx').cumcount(),
           value=lambda x: x['value']+x.pop('col'))
   .pivot(index='idx', columns='n', values='value')
   .rename_axis(index=None, columns=None)
)
</code></pre>
<p>Output:</p>
<pre><code>                          0                       1                     2                     3                     4
0   Apple=output[col0:col7]                     NaN                   NaN                   NaN                   NaN
1  Banana=output[col4:col7]   Cat=output[col0:col3]                   NaN                   NaN                   NaN
2     Dog=output[col0:col3]                     NaN                   NaN                   NaN                   NaN
3     Egg=output[col1:col4]  Fish=output[col0:col0]                   NaN                   NaN                   NaN
4      G1=output[col4:col4]    G2=output[col3:col3]  G3=output[col2:col2]  G4=output[col1:col1]  G5=output[col0:col0]
5      HA=output[col6:col6]    HB=output[col5:col5]  HC=output[col4:col4]  HD=output[col0:col3]                   NaN
</code></pre>
<p>Or, since you have strings here, for once you could leverage <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a>:</p>
<pre><code>def label(x):
    return f'{x.iloc[0]}=output[{x.index[0]}:{x.index[-1]}]'

out = (df
   .ffill(axis=1).replace('-', pd.NA)
   .apply(lambda s: s.groupby(s).agg(label).tolist(), axis=1)
   .pipe(lambda s: pd.DataFrame(s.tolist(), index=s.index))
)
</code></pre>
<p>Output:</p>
<pre><code>                          0                       1                     2                     3                     4
0   Apple=output[col7:col0]                    None                  None                  None                  None
1  Banana=output[col7:col4]   Cat=output[col3:col0]                  None                  None                  None
2     Dog=output[col3:col0]                    None                  None                  None                  None
3     Egg=output[col4:col1]  Fish=output[col0:col0]                  None                  None                  None
4      G1=output[col4:col4]    G2=output[col3:col3]  G3=output[col2:col2]  G4=output[col1:col1]  G5=output[col0:col0]
5      HA=output[col6:col6]    HB=output[col5:col5]  HC=output[col4:col4]  HD=output[col3:col0]                  None
</code></pre>
<p>Or if you really just want to loop and get strings:</p>
<pre><code>for _, row in df.ffill(axis=1).replace('-', pd.NA).iterrows():
    row.dropna(inplace=True)
    for _, s in row.groupby(row.ne(row.shift()).cumsum()):
        print(f'{s.iloc[0]}=output[{s.index[0]}:{s.index[-1]}]')
</code></pre>
<p>Output:</p>
<pre><code>Apple=output[col7:col0]
Banana=output[col7:col4]
Cat=output[col3:col0]
Dog=output[col3:col0]
Egg=output[col4:col1]
Fish=output[col0:col0]
G1=output[col4:col4]
G2=output[col3:col3]
G3=output[col2:col2]
G4=output[col1:col1]
G5=output[col0:col0]
HA=output[col6:col6]
HB=output[col5:col5]
HC=output[col4:col4]
HD=output[col3:col0]
</code></pre>
<p>For this last approach, if you have extra columns to use in the output, you could set them as index and retrieve them in the loop:</p>
<pre><code>for (x, y), row in (df.set_index(['X', 'Y'])
                      .ffill(axis=1)
                      .replace('-', pd.NA).iterrows()):
    row.dropna(inplace=True)
    for _, s in row.groupby(row.ne(row.shift()).cumsum()):
        print(f'{x}.{s.iloc[0]}=output[{y}][{s.index[0]}:{s.index[-1]}]')
</code></pre>
<p>Output:</p>
<pre><code>1.Apple=output[1][col7:col0]
54.Banana=output[1][col7:col4]
54.Cat=output[1][col3:col0]
7.Dog=output[2][col3:col0]
8.Egg=output[2][col4:col1]
8.Fish=output[2][col0:col0]
22.G1=output[3][col4:col4]
22.G2=output[3][col3:col3]
22.G3=output[3][col2:col2]
22.G4=output[3][col1:col1]
22.G5=output[3][col0:col0]
21.HA=output[3][col6:col6]
21.HB=output[3][col5:col5]
21.HC=output[3][col4:col4]
21.HD=output[3][col3:col0]
</code></pre>
","3","Answer"
"79421025","79418256","<p>You can ensure that all line are shown with <code>zeroline=True</code> for both axes and set <code>showline=True</code> to make sure the <code>y-axis</code> and <code>x-axis</code> lines are always drawn.</p>
<pre><code>import streamlit as st
import plotly.express as px
import pandas as pd

data = {
    'x': [1.5, 1.6, -1.2],
    'y': [21, -16, 46],
    'circle-size': [10, 5, 6],
    'circle-color': [&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;]
}

df = pd.DataFrame(data)

fig = px.scatter(
    df,
    x=&quot;x&quot;, 
    y=&quot;y&quot;, 
    color=&quot;circle-color&quot;,
    size='circle-size'
)

fig.update_layout(
    xaxis={
        &quot;range&quot;: [-100, 100],
        'zeroline': True, 
        'zerolinewidth': 3, 
        &quot;zerolinecolor&quot;: &quot;green&quot;,
        &quot;tick0&quot;: -100,
        &quot;dtick&quot;: 25,
        &quot;constrain&quot;: &quot;domain&quot;,
        &quot;showline&quot;: True
    },
    yaxis={
        &quot;range&quot;: [-100, 100],
        'zeroline': True, 
        'zerolinewidth': 3, 
        &quot;zerolinecolor&quot;: &quot;red&quot;,
        &quot;tick0&quot;: -100,
        &quot;dtick&quot;: 25,
        &quot;constrain&quot;: &quot;domain&quot;,
        &quot;showline&quot;: True
    },
    width=500,
    height=500
)



st.plotly_chart(fig, use_container_width=False)

</code></pre>
<p>which gives</p>
<p><a href=""https://i.sstatic.net/822EKzzT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/822EKzzT.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79421302","79421290","<p>Your columns are not matching on the two sides of the assignment.</p>
<p>Since you use a mask on both sides, don't perform index alignment and directly pass the values:</p>
<pre><code>m = ~df['B'].isna()
df.loc[m, ['A', 'B', 'C']] = df.loc[m, ['X', 'Y', 'Z']].values
</code></pre>
<p>For your approach to work, you could also <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a> the columns. In this case you would not need the mask on both sides and could benefit from the index alignment:</p>
<pre><code>cols_left = ['A', 'B', 'C']
cols_right = ['X', 'Y', 'Z']
df.loc[~df['B'].isna(), cols_left] = df[cols_right].rename(
    columns=dict(zip(cols_right, cols_left))
)
</code></pre>
<p>Output:</p>
<pre><code>    A     B   C   X   Y   Z
0  10    40  70  10  40  70
1   2  &lt;NA&gt;   8  20  50  80
2  30    60  90  30  60  90
</code></pre>
","2","Answer"
"79421642","79421531","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>cols = ['reference', 'sicovam', 'label', 'id', 'date']

df['price'] = (
    df.set_index('TTM')
    .groupby(cols)['price']
    .transform(lambda x: x.interpolate(method='index'))
    .values
    )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>df.iloc[4:7]

            reference sicovam  label      id        date  TTM      price
4 SCOM_WTI   68801903     WTI  Nymex  BBG:CL  2015-01-02   77  53.690000
5 SCOM_WTI   68801903     WTI  Nymex  BBG:CL  2015-01-02   90  53.986562
6 SCOM_WTI   68801903     WTI  Nymex  BBG:CL  2015-01-02  109  54.420000
</code></pre>
<p><strong>Explanation</strong></p>
<ul>
<li>Set column 'TTM' as the index with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>df.set_index</code></a> and apply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>df.groupby</code></a>.</li>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a> and use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.interpolate.html"" rel=""nofollow noreferrer""><code>pd.Series.interpolate</code></a> with <code>method='index'</code>.</li>
<li>Finally, chain <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.values.html"" rel=""nofollow noreferrer""><code>Series.values</code></a> to assign the result back to <code>df['price']</code>.</li>
</ul>
","1","Answer"
"79422091","79421829","<p>If you want independent DataFrames for each combination of rows, the best is to use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>iloc</code></a> in a loop:</p>
<pre><code>for c in combinations(range(len(df_items)), 2):
    print(df_items.iloc[list(c)])
</code></pre>
<p>Example output:</p>
<pre><code>   A  B
0  0  0
1  1  1
   A  B
0  0  0
2  2  2
   A  B
1  1  1
2  2  2
</code></pre>
<p>Used input:</p>
<pre><code>df_items = pd.DataFrame({'A': range(3),
                         'B': range(3)})
</code></pre>
<p>You could also <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> but this will be less efficient:</p>
<pre><code>from itertools import combinations, chain

i = 2

tmp = df_items.iloc[list(chain.from_iterable(combinations(range(len(df_items)), i)))]

tmp.groupby(np.arange(len(tmp))//i)
</code></pre>
","0","Answer"
"79424187","79421661","<p>You can add the rectangles using plotly <a href=""https://plotly.com/python/shapes/#rectangles-positioned-relative-to-the-axis-data"" rel=""nofollow noreferrer"">shapes</a>. I have adjusted the size and coordinates of the rectangle to scale to the <code>range = max - min</code> of your x and y values in your data since this is how plotly determines the scale of the figures (rather than hardcoding the size of the rectangles which won't work if your data changes). The length of the box in the x-direction is then scaled to the length of the text to be placed inside the box.</p>
<pre><code>import plotly.graph_objects as go
import plotly.express as px 
import pandas as pd


data = {'x': [1.5, 1.6, -1.2],
        'y': [21, -16, 46],
        'circle-size': [10, 5, 6],
        'circle-color': [&quot;red&quot;,&quot;red&quot;,&quot;green&quot;],
        'tttt': [&quot;ggg&quot;,&quot;vvvv&quot;,&quot;really really long string&quot;],

        }

# Create DataFrame
df = pd.DataFrame(data)
df['length'] = df['tttt'].str.len()
fig = px.scatter(
    df,
    x=&quot;x&quot;, 
    y=&quot;y&quot;, 
    color=&quot;circle-color&quot;,
    size='circle-size',
    # text=&quot;tttt&quot;,
    # hover_name=&quot;tttt&quot;,
    color_discrete_map={&quot;red&quot;: &quot;red&quot;, &quot;green&quot;: &quot;green&quot;}
)
fig.update_traces(textposition='middle right', textfont_size=14, textfont_color='black', textfont_family=&quot;Inter&quot;, hoverinfo=&quot;skip&quot;)

xrange_min = -100
xrange_max = 100
yrange_min = -100
yrange_max = 100

fig.update_layout(
    {
        'xaxis': {
            &quot;range&quot;: [xrange_min, xrange_max],
            'zerolinewidth': 3, 
            &quot;zerolinecolor&quot;: &quot;blue&quot;,
            &quot;tick0&quot;: -100,
            &quot;dtick&quot;: 25,
            'scaleanchor': 'y'
        },
        'yaxis': {
            &quot;range&quot;: [yrange_min, yrange_max],
            'zerolinewidth': 3, 
            &quot;zerolinecolor&quot;: &quot;green&quot;,
            &quot;tick0&quot;: -100,
            &quot;dtick&quot;: 25
        },
        &quot;width&quot;: 500,
        &quot;height&quot;: 500
    }
)

x_pad = (xrange_max - xrange_min) / 25
y_pad = (yrange_max - yrange_min) / 30

for (x0, y0, text, length) in zip(df['x'], df['y'], df['tttt'], df['length']):
    fig.add_shape(type=&quot;rect&quot;,
        x0=x0 + 1.2*x_pad, 
        y0=y0 - y_pad, 
        x1=x0 + (length)*x_pad, 
        y1=y0 + y_pad,
        xref='x', yref='y',
        line=dict(
            color=&quot;black&quot;,
            width=2,
        ),
        fillcolor=&quot;#1CBE4F&quot;,
        layer=&quot;below&quot;
    )

    fig.add_trace(
        go.Scatter(
            x=[x0 + 1.2*x_pad + (length*x_pad/4)],
            y=[y0 + (y_pad / 2)],
            text=[text],
            mode=&quot;text&quot;,
            showlegend=False
        )
    )

fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/26n5t1VM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/26n5t1VM.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79428355","79419281","<p><strong>Have you tried <em>pandas.to_datetime()</em>?</strong>
I suggest you just convert your string index/column to datetime using:</p>
<pre><code>import pandas as pd

test_string = &quot;Feb 10, 2024&quot;

test_result = pd.to_datetime(test_string)

print(test_result)
</code></pre>
<p>Output:</p>
<pre><code>2024-02-10 00:00:00
</code></pre>
<p>If you need to, you can change it to another string format using strftime.</p>
","0","Answer"
"79440080","79421290","<p>I think this method is the Fastest for Large DataFrames</p>
<pre><code>import pandas as pd
import numpy as np


columns = {&quot;A&quot;: [1, 2, 3],
           &quot;B&quot;: [4, pd.NA, 6],
           &quot;C&quot;: [7, 8, 9],
           &quot;X&quot;: [10, 20, 30],
           &quot;Y&quot;: [40, 50, 60],
           &quot;Z&quot;: [70, 80, 90]}

df = pd.DataFrame(columns)
print(df)
'''
   A     B  C   X   Y   Z
0  1     4  7  10  40  70
1  2  &lt;NA&gt;  8  20  50  80
2  3     6  9  30  60  90
'''

targetCols = [0,1,2]  # A, B, C

sourceCols = [3,4,5]  # X, Y, Z

mask = ~pd.isna(df['B']).to_numpy()
data = df.to_numpy()

data[np.ix_(mask,targetCols)] = data[np.ix_(mask,sourceCols)]

df= pd.DataFrame(data,columns = df.columns)
print(df)
'''
    A     B   C   X   Y   Z
0  10    40  70  10  40  70
1   2  &lt;NA&gt;   8  20  50  80
2  30    60  90  30  60  90

'''
</code></pre>
","0","Answer"
"79451247","79407822","<p>First of all, you need to preprocess a little you dataset. Look at the coordinates. The latitude starts at 90º and ends at -90. Imagine you want to select latitude from 23.5° to 37° (which I think corresponds to Pakistan). If you try to select <code>.sel(latitude=slice(23.5, 37))</code> you will obtain 0 dimensions because latitude goes from 90º to -90º.</p>
<p>You need first to sort the latitude dimension:</p>
<pre><code>data = data.sortby(data.latitude)
</code></pre>
<p>Also, your longitude is between 0º and 360º, so I recommend you to change it to -180º to 180º using:</p>
<pre><code>data = data.assign_coords(longitude=(((data.longitude + 180) % 360) - 180))
data = data.sortby(data.longitude)
</code></pre>
<p>I think Pakistan is between 23.5°N to 37.0°N and 60.5°E to 77.5°E, but change the following code to the region you are interested in. To obtain the timeseries that you want you will need to:</p>
<pre><code># Select region
data_pak = data.sel(latitude=slice(23.5, 37.0), longitude=slice(60.5, 77.5))

# Transform to dataframe and change dimensions
stacked = data_pak.stack(latlon=('latitude', 'longitude'))
df = stacked.to_dataframe()
df_unstacked = df.unstack('latlon')
df_unstacked.columns = [f'{var}_{lat}_{lon}' for var, lat, lon in df_unstacked.columns]
df_result = df_unstacked.reset_index()
</code></pre>
<p>And then, df_result is the result. If this doesn't work, you can try a more &quot;manual&quot; way to do it:</p>
<pre><code># Select region
data_pak = data.sel(latitude=slice(23.5, 37.0), longitude=slice(60.5, 77.5))

# Transform to dataframe and change dimensions
df = data_pak.to_dataframe()
df['lat_lon'] = df.apply(lambda row: f&quot;lat={row['lat']}lon={row['lon']}&quot;, axis=1)
df_melted = df.melt(id_vars=['time', 'lat_lon'], value_vars=['expver', 'tp', 'e', 'sf'], var_name='variable', value_name='value')
df_melted['new_col'] = df_melted['variable'] + &quot;-&quot; + df_melted['lat_lon']
df_result = df_melted.pivot(index='time', columns='new_col', values='value')
df_result.reset_index(inplace=True)
</code></pre>
","0","Answer"
"79480228","79397978","<p>To filter by the <code>Mg#</code> without creating a new column you can use <strong>boolean indexing</strong>. The created variables hold temporary filtered DataFrames, essential for plotting the subsets. They do not change the original dataframe.<br/><br/>Filtering and plotting for Mg# &gt; 70</p>
<pre><code>high_mg_data = East_Greenland[East_Greenland['Mg#'] &gt; 70]

plt.scatter(high_mg_data['Pd'], 
            high_mg_data['Cu']/high_mg_data[&quot;Pd&quot;], 
            color = &quot;red&quot;, marker = &quot;o&quot;, s=25, 
            edgecolor = &quot;black&quot;, label = &quot;Mg# &gt; 70&quot;)
</code></pre>
<p>Filtering and plotting for 60 &lt;= Mg# &lt;= 70</p>
<pre><code>mid_mg_data = East_Greenland[(East_Greenland['Mg#'] &gt;= 60) &amp; (East_Greenland['Mg#'] &lt;= 70)]

plt.scatter(mid_mg_data['Pd'], 
            mid_mg_data['Cu']/mid_mg_data[&quot;Pd&quot;], 
            color = &quot;blue&quot;, marker = &quot;^&quot;, s=25, 
            edgecolor = &quot;black&quot;, label = &quot;60 &lt;= Mg# &lt;= 70&quot;)
</code></pre>
<p>Note that in the context of filtering Pandas DataFrames using boolean indexing, you must use the <code>&amp;</code> (bitwise AND) operator, not the <code>and</code> (logical AND) operator.</p>
","0","Answer"
"79482389","79411167","<p>Here is another way:</p>
<pre><code>df.assign(C = [s.tolist() if len(s) == 3 else None for s in df['B'].rolling(3)])
</code></pre>
<p>Output:</p>
<pre><code>   A  B          C
0  4  9       None
1  0  2       None
2  4  5  [9, 2, 5]
3  7  9  [2, 5, 9]
4  8  3  [5, 9, 3]
5  8  1  [9, 3, 1]
6  1  4  [3, 1, 4]
7  4  1  [1, 4, 1]
8  1  9  [4, 1, 9]
9  3  7  [1, 9, 7]
</code></pre>
","0","Answer"
"79492442","79379121","<p>Actually you don't need to add the CSS to your external HTML file, you can use <code>set_table_styles</code> to add the class CSS directly.</p>
<pre class=""lang-py prettyprint-override""><code>from pandas.io.formats.style import Styler
s = Styler(df, cell_ids=False)

hls=df.loc[abs(df['B']) &gt; 0.5].index
classes = pd.DataFrame().reindex_like(df).astype(object)
classes.loc[hls]='shade'
classes.fillna('', inplace=True)

html = s.set_uuid('table').set_td_classes(classes).set_table_styles([
    {&quot;selector&quot;: &quot;.shade&quot;, &quot;props&quot;: &quot;font-weight:bold; color: green;&quot;}
], overwrite=False).to_html()
</code></pre>
<p>HTML:</p>
<pre><code>&lt;style type=&quot;text/css&quot;&gt;
#T_table .shade {
  font-weight: bold;
  color: green;
}
&lt;/style&gt;
&lt;table id=&quot;T_table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th class=&quot;blank level0&quot; &gt;&amp;nbsp;&lt;/th&gt;
      &lt;th class=&quot;col_heading level0 col0&quot; &gt;A&lt;/th&gt;
      &lt;th class=&quot;col_heading level0 col1&quot; &gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th class=&quot;row_heading level0 row0&quot; &gt;0&lt;/th&gt;
      &lt;td class=&quot;data row0 col0&quot; &gt;-0.584093&lt;/td&gt;
      &lt;td class=&quot;data row0 col1&quot; &gt;0.309629&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th class=&quot;row_heading level0 row1&quot; &gt;1&lt;/th&gt;
      &lt;td class=&quot;data row1 col0 shade&quot; &gt;0.997064&lt;/td&gt;
      &lt;td class=&quot;data row1 col1 shade&quot; &gt;-0.779591&lt;/td&gt;
    &lt;/tr&gt;
...
</code></pre>
<p><strong>Alternatively</strong> you can use <code>set_table_styles</code> for each row.</p>
<pre class=""lang-py prettyprint-override""><code>from pandas.io.formats.style import Styler

s = Styler(df, cell_ids=False)
hls=df.loc[abs(df['B']) &gt; 1.0].index

s.set_table_styles({
    row: [{&quot;selector&quot;: &quot;td&quot;, &quot;props&quot;: &quot;color: red; font-weight: bold;&quot;}]
    for row in hls
}, axis=1)
</code></pre>
<p>This will give:</p>
<pre><code>&lt;style type=&quot;text/css&quot;&gt;
#T_35571 td.row2 {
  color: red;
  font-weight: bold;
}
#T_35571 td.row3 {
  color: red;
  font-weight: bold;
}
#T_35571 td.row4 {
  color: red;
  font-weight: bold;
}
&lt;/style&gt;
&lt;table id=&quot;T_35571&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th class=&quot;blank level0&quot; &gt;&amp;nbsp;&lt;/th&gt;
      &lt;th class=&quot;col_heading level0 col0&quot; &gt;A&lt;/th&gt;
      &lt;th class=&quot;col_heading level0 col1&quot; &gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th class=&quot;row_heading level0 row0&quot; &gt;0&lt;/th&gt;
      &lt;td class=&quot;data row0 col0&quot; &gt;-0.584093&lt;/td&gt;
      &lt;td class=&quot;data row0 col1&quot; &gt;0.309629&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th class=&quot;row_heading level0 row1&quot; &gt;1&lt;/th&gt;
      &lt;td class=&quot;data row1 col0&quot; &gt;0.997064&lt;/td&gt;
      &lt;td class=&quot;data row1 col1&quot; &gt;-0.779591&lt;/td&gt;
    &lt;/tr&gt;
...

</code></pre>
<p>You are correct in saying that there is no <code>set_tr_classes</code>. This would require an enhancement to the API.</p>
","0","Answer"
"79497971","79393122","<pre><code>import pandas as pd

df = pd.DataFrame.from_dict({'index': ['X', 'Y'],
                             'columns': [('2022', 'A'), ('2022', 'B'), ('2021', 'A'), ('2021', 'B')],
                             'data': [[1, 2, 3, 4], [5, 6, 7, 8]],
                             'index_names': ['class'],
                             'column_names': [None, None]},
                            orient='tight')

df = df.stack(level=[0, 1], future_stack=True).reset_index()
df.columns = ['class', 'year', 'category', 'value']

'''
  class  year category  value
0     X  2022        A      1
1     X  2022        B      2
2     X  2021        A      3
3     X  2021        B      4
4     Y  2022        A      5
5     Y  2022        B      6
6     Y  2021        A      7
7     Y  2021        B      8
'''
</code></pre>
","0","Answer"
"79501107","79383889","<p>Method 1</p>
<pre><code>import numpy as np
import pandas as pd

#DataFrame representing connections between nodes
df = pd.DataFrame({
    '1_2': [1, 4, 8, 4, 8],  # Edge between node 1 and 2
    '1_3': [5, 3, 8, 3, 0],  # Edge between node 1 and 3
    '1_4': [2, 4, 8, 4, 7],  # Edge between node 1 and 4
    '2_3': [8, 5, 9, 4, 4],  # Edge between node 2 and 3
    '2_4': [2, 8, 3, 8, 2],  # Edge between node 2 and 4
    '3_4': [2, 5, 3, 3, 2]   # Edge between node 3 and 4
})

# Extract the column names (edges) from the DataFrame
cols = np.array(df.columns)

# Split the column names into node pairs (e.g., &quot;1_2&quot; becomes [1, 2])
pairs = np.array([col.split('_') for col in cols], dtype=int)

# Find the unique nodes present in the graph
unique_nodes = np.unique(pairs)

# Create a matrix to represent node-edge connections (initially filled with zeros)
node_matrix = np.zeros((unique_nodes.size, len(cols)))

# Create an array of row indices for node_matrix
# This array tells us which rows of node_matrix should be set to 1
# Each node in each edge needs to be marked as connected
row_indices = np.concatenate([np.where(unique_nodes == node)[0]
                               for node in pairs.flatten()])
# Example row_indices: [0 1 0 2 0 3 1 2 1 3 2 3]

# Create an array of column indices for node_matrix
# Each column (edge) connects two nodes, so we repeat each column index twice
col_indices = np.repeat(np.arange(len(cols)), 2)
# Example col_indices: [0 0 1 1 2 2 3 3 4 4 5 5]

# Set the corresponding elements in node_matrix to 1, 
#indicating node-edge connections
node_matrix[row_indices, col_indices] = 1
# Resulting node_matrix:
# [[1. 1. 1. 0. 0. 0.]
#  [1. 0. 0. 1. 1. 0.]
#  [0. 1. 0. 1. 0. 1.]
#  [0. 0. 1. 0. 1. 1.]]

# Convert the DataFrame values to a NumPy array for matrix multiplication
dfValues = df.to_numpy()

# Perform matrix multiplication to calculate the 
# sum of edge values for each node
res = dfValues @ node_matrix.T
'''
 Resulting res:
 [[ 8. 11. 15.  6.]
  [11. 17. 13. 17.]
  [24. 20. 20. 14.]
  [11. 16. 10. 15.]
  [15. 14.  6. 11.]]
'''
# Create a DataFrame from the result, with node names as column labels
res_df = pd.DataFrame(res, columns=[f'f_{node}' for node in unique_nodes], 
         dtype=int)

# Concatenate the original DataFrame with the result DataFrame
df_final = pd.concat([df, res_df], axis=1)
'''
   1_2  1_3  1_4  2_3  2_4  3_4  f_1  f_2  f_3  f_4
0    1    5    2    8    2    2    8   11   15    6
1    4    3    4    5    8    5   11   17   13   17
2    8    8    8    9    3    3   24   20   20   14
3    4    3    4    4    8    3   11   16   10   15
4    8    0    7    4    2    2   15   14    6   11
'''
</code></pre>
","1","Answer"
"79502956","79385026","<pre><code>import pandas as pd

df = pd.DataFrame({
    'Id': ['item1', 'item2', 'item3', 'item4'],
    'tags': [['friends', 'family'], ['friends'], [], ['family', 'holiday']]
})

res = df.explode('tags').value_counts('tags').reset_index()
print(res)
'''
      tags  count
0  friends      2
1   family      2
2  holiday      1
'''


</code></pre>
","0","Answer"
"79508849","79377042","<pre><code>import pandas as pd
import torch
import numpy as np

# Create a sample DataFrame with binary features (1s and 0s)
df = pd.DataFrame({
    'ID': [1, 2, 3, 4, 5],  # Unique identifier for each row
    'Col_1': [1, 0, 1, 1, 0],  # Binary feature column 1
    'Col_2': [0, 1, 1, 0, 1],  # Binary feature column 2
    'Col_3': [1, 1, 0, 1, 1]   # Binary feature column 3
})

# Determine if a GPU is available, otherwise use CPU
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Extract the names of the feature columns (excluding 'ID')
featureCols = df.columns[1:]
featureNames = np.array(featureCols)  # Convert to NumPy array for easy indexing

# Convert the feature columns to a PyTorch tensor (efficient numerical data)
tensor = torch.tensor(df[featureCols].values, dtype=torch.int8, device=device)
'''
tensor([[1, 0, 1],  # Represents the binary features for row 1
        [0, 1, 1],  # Represents the binary features for row 2
        [1, 1, 0],  # Represents the binary features for row 3
        [1, 0, 1],  # Represents the binary features for row 4
        [0, 1, 1]], dtype=torch.int8) #represents the binary features for row 5.
'''

# Find the indices of the '1' values (where features are active)
rowIndices, colIndices = torch.nonzero(tensor, as_tuple=True)
'''
# Row indices where features are '1'
rowIndices: tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])  
# Column indices where features are '1'
colIndices: tensor([0, 2, 1, 2, 0, 1, 0, 2, 1, 2])  
'''

# Map the column indices back to the original feature names
mappedFeatures = featureNames[colIndices.cpu().numpy()]
'''
['Col_1' 'Col_3' 'Col_2' 'Col_3' 'Col_1' 'Col_2' 'Col_1' 'Col_3' 'Col_2' 'Col_3']
'''

# Find the unique row indices and their starting positions in 'mappedFeatures'
uniqueRows, rowPositions = np.unique(rowIndices.cpu().numpy(), return_index=True)
'''
# Unique row indices
uniqueRows: [0 1 2 3 4] 
# Starting positions of feature names for each row in 'mappedFeatures' 
rowPositions: [0 2 4 6 8]  
'''

# Group the feature names for each row and join them with ' and '
joinedRes = np.array([' and '.join(gr) for gr in np.split(mappedFeatures, rowPositions[1:])])
'''
['Col_1 and Col_3' 'Col_2 and Col_3' 'Col_1 and Col_2' 'Col_1 and Col_3' 'Col_2 and Col_3']
'''

# Add the joined feature names as a new column 'res' to the DataFrame
df['res'] = joinedRes

# Print the final DataFrame with the 'res' column
print(df)
</code></pre>
","0","Answer"
"79522256","79367366","<p>If you are working on a huge dataset then, Use Pytorch and numpy. I think the following code shall help you a lot. But Only use this if you are using on huge datasets.</p>
<pre><code>import torch
import pandas as pd

# Create DataFrame representing time series data with valley and peak flags
data = {'x': torch.arange(1, 29).tolist(),  # Time points (1 to 28)
        'y': [5.69, 6.03, 6.03, 6.03, 6.03, 6.03, 6.03, 5.38, 5.21, 5.4, 5.24,
              5.4, 5.36, 5.47, 5.58, 5.5, 5.61, 5.53, 5.4, 5.51, 5.47, 5.44,
              5.39, 5.27, 5.38, 5.35, 5.32, 5.09], # Corresponding y values
        'valley_flag': [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], # Flags indicating valley points
        'peak_flag':   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]} # Flags indicating peak points
df = pd.DataFrame(data)
print(&quot;Original DataFrame:\n&quot;, df)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def to_tensor(series):
    &quot;&quot;&quot; Convert Pandas Series to PyTorch tensor &quot;&quot;&quot;
    return torch.tensor(series.values,device = device)

# Convert valley and peak flags to PyTorch tensors
valley_flags_tensor = to_tensor(df['valley_flag'])
peak_flags_tensor   = to_tensor(df['peak_flag'])



# Find indices where valley and peak flags are set (1)
valley_indices = torch.nonzero(valley_flags_tensor, as_tuple=True)[0]
peak_indices = torch.nonzero(peak_flags_tensor, as_tuple=True)[0]
print(&quot;Peak Indices:&quot;, peak_indices)

# Combine and sort valley and peak indices to get event indices
event_indices = torch.sort(torch.cat((valley_indices, peak_indices)))[0]
print(&quot;Event Indices:&quot;, event_indices)

# Create a tensor representing the length of the DataFrame
dataframe_length_tensor = torch.tensor([len(df)], device=device)
print(&quot;DataFrame Length Tensor:&quot;, dataframe_length_tensor)

# Calculate the lengths of segments between events
segment_lengths = torch.diff(torch.cat((event_indices, dataframe_length_tensor)))
print(&quot;Segment Lengths:&quot;, segment_lengths)

# Generate alternating group labels (0 and 1) for each event
group_labels = (torch.arange(len(event_indices), device=device) % 2).to(torch.int8)
print(&quot;Group Labels:&quot;, group_labels)

# Initialize a tensor to store group assignments, with -1 indicating unassigned
group_assignments_container = torch.full((len(df),), -1, dtype=torch.int8, device=device)
print(&quot;Initial Group Assignments Container:\n&quot;, group_assignments_container)

# Assign initial group labels at event positions
group_assignments_container[event_indices] = group_labels
print(&quot;Group Assignments after Initial Assignment:\n&quot;, group_assignments_container)

# Repeat group labels according to segment lengths for propagation
repeated_group_labels = group_assignments_container[event_indices].repeat_interleave(segment_lengths)

# Create a tensor of indices for the entire DataFrame
dataframe_indices = torch.arange(len(df), device=device)
print(&quot;DataFrame Indices:&quot;, dataframe_indices)

# Propagate group labels to fill the entire container using scatter
group_assignments_container = group_assignments_container.scatter(0, dataframe_indices, repeated_group_labels)
print(&quot;Final Group Assignments Container:\n&quot;, group_assignments_container)

# Convert group assignments (0 and 1) to 'A' and 'B' labels and add to DataFrame
df['grp'] = ['A' if g == 0 else 'B' for g in group_assignments_container.cpu().numpy()]
print(&quot;DataFrame with Group Labels:\n&quot;, df)

'''
Peak Indices: tensor([ 2, 16])
Event Indices: tensor([ 0,  2,  8, 16, 27])
DataFrame Length Tensor: tensor([28])
Segment Lengths: tensor([ 2,  6,  8, 11,  1])
Group Labels: tensor([0, 1, 0, 1, 0], dtype=torch.int8)
Initial Group Assignments Container:
 tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.int8)
Group Assignments after Initial Assignment:
 tensor([ 0, -1,  1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1,  1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1,  0], dtype=torch.int8)
DataFrame Indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27])
Final Group Assignments Container:
 tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0], dtype=torch.int8)
DataFrame with Group Labels:
      x     y  valley_flag  peak_flag grp
0    1  5.69            1          0   A
1    2  6.03            0          0   A
2    3  6.03            0          1   B
3    4  6.03            0          0   B
4    5  6.03            0          0   B
5    6  6.03            0          0   B
6    7  6.03            0          0   B
7    8  5.38            0          0   B
8    9  5.21            1          0   A
9   10  5.40            0          0   A
10  11  5.24            0          0   A
11  12  5.40            0          0   A
12  13  5.36            0          0   A
13  14  5.47            0          0   A
14  15  5.58            0          0   A
15  16  5.50            0          0   A
16  17  5.61            0          1   B
17  18  5.53            0          0   B
18  19  5.40            0          0   B
19  20  5.51            0          0   B
20  21  5.47            0          0   B
21  22  5.44            0          0   B
22  23  5.39            0          0   B
23  24  5.27            0          0   B
24  25  5.38            0          0   B
25  26  5.35            0          0   B
26  27  5.32            0          0   B
27  28  5.09            1          0   A

'''
</code></pre>
","0","Answer"
"79583422","79390828","<pre><code>import pandas as pd
import warnings


data = {
    'ID': [1, 1, 1, 2, 2, 2, 2, 2],
    'Question Code': ['Q01', 'Q01-1', 'Q02', 'Q01', 'Q02', 'Q02-1', 'Q02-1-1', 'Q02-2']
}


question_df = pd.DataFrame(data)
'''
   ID Question Code
0   1           Q01
1   1         Q01-1
2   1           Q02
3   2           Q01
4   2           Q02
5   2         Q02-1
6   2       Q02-1-1
7   2         Q02-2
'''

# Define a function to process each group of questions (grouped by 'ID')
def filter_child_questions(group):
  
    # Get a set of all question codes within the current group
    all_codes = set(group['Question Code'])

    # Identify parent question codes. A parent code is one that appears
    # before a hyphen in another question code within the same group.
    parent_codes = {code.rsplit('-', 1)[0] for code in all_codes if '-' in code}

    # Filter the group to keep only the question codes that are NOT in the set of parent codes.
    # This selects 'child' questions (like 'Q01-1' where 'Q01' is a parent)
    # and top-level questions that don't have any sub-questions (like 'Q02' in ID 1).
    child_or_standalone_questions = group[~group['Question Code'].isin(parent_codes)]
    return child_or_standalone_questions

# Suppress the DeprecationWarning related to DataFrameGroupBy.apply
with warnings.catch_warnings():
    warnings.simplefilter('ignore', category = DeprecationWarning)
    # Group the DataFrame by 'ID' and apply the 'filter_child_questions' function to each group.
    # group_keys=False prevents the group keys ('ID') from becoming part of the index.
    filtered_result = question_df.groupby('ID', group_keys=False).apply(filter_child_questions)#.reset_index(drop=True)

print(&quot;\nFiltered Result:&quot;)
print(filtered_result)
'''
   ID Question Code
1   1         Q01-1
2   1           Q02
3   2           Q01
6   2       Q02-1-1
7   2         Q02-2
'''
</code></pre>
","0","Answer"
"79583549","79390828","<p>Without creating a function :</p>
<pre><code>import pandas as pd
import warnings

data = {
    'ID': [1, 1, 1, 2, 2, 2, 2, 2],
    'Question Code': ['Q01', 'Q01-1', 'Q02', 'Q01', 'Q02', 'Q02-1', 'Q02-1-1', 'Q02-2']
}

df = pd.DataFrame(data)

with warnings.catch_warnings():
    warnings.simplefilter(action ='ignore', category = DeprecationWarning)

    res = df.groupby('ID',group_keys = False).apply(
          lambda gr : gr[~gr['Question Code'].isin(
          {code.rsplit('-',1)[0] for code in set(gr['Question Code']) if '-' in code})]
).reset_index(drop =True)

print(res)
'''
  ID Question Code
0   1         Q01-1
1   1           Q02
2   2           Q01
3   2       Q02-1-1
4   2         Q02-2
'''

</code></pre>
","0","Answer"
"79584792","79383889","<p>If the Dataset is Huge and Sparse, then use this :</p>
<pre><code>import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix

df = pd.DataFrame({
    '1_2': [1, 4, 8, 4, 8],
    '1_3': [5, 3, 8, 3, 0],
    '1_4': [2, 4, 8, 4, 7],
    '2_3': [8, 5, 9, 4, 4],
    '2_4': [2, 8, 3, 8, 2],
    '3_4': [2, 5, 3, 3, 2]
})

# Step 1: Extract node pairs from column names as integers
edges = df.columns.str.extract(r'(\d+)_(\d+)').astype(int).values
'''
Output (edges):
[[1 2]
 [1 3]
 [1 4]
 [2 3]
 [2 4]
 [3 4]]
'''

unique_nodes = np.unique(edges)
'''
Output (unique_nodes):
[1 2 3 4]
'''

# Step 3: Create row indices for the incidence matrix
rowIdx = np.searchsorted(unique_nodes, edges.ravel())
'''
Output (rowIdx):
[0 1 0 2 0 3 1 2 1 3 2 3]
'''
# Explanation: For each node in the 'edges' array, we find its index
# in the sorted 'unique_nodes' array. This gives us the row number
# in the incidence matrix corresponding to that node.

# Step 4: Create column indices for the incidence matrix
colIdx = np.repeat(np.arange(edges.shape[0]), 2)
'''
Output (colIdx):
[0 0 1 1 2 2 3 3 4 4 5 5]
'''
# Explanation: Each original edge in the DataFrame corresponds to a column
# in the incidence matrix. Since each edge connects two nodes, we repeat
# the column index twice.

# data for the sparse incidence matrix (all connections are 1)
data = np.ones(len(rowIdx), dtype=np.uint8)

# Step 6: Build the sparse incidence matrix and convert it to a dense array
incidence_sparse_matrix = csr_matrix(
    (data, (rowIdx, colIdx)),
    shape=(len(unique_nodes), len(df.columns))
).toarray()
'''
Output (incidence_sparse_matrix):
[[1 1 1 0 0 0]
 [1 0 0 1 1 0]
 [0 1 0 1 0 1]
 [0 0 1 0 1 1]]
'''
# Explanation: This matrix represents the connections between nodes (rows)
# and edges (columns). A '1' indicates that the node is part of the edge.
# For example, row 0 (node 1) has '1's in columns 0 ('1_2'), 1 ('1_3'), and 2 ('1_4'),
# indicating that node 1 is connected to these three edges.

csr_shape = incidence_sparse_matrix.T.shape

'''
Output (csr_shape):
(6, 4)
'''
df_shape = df.to_numpy().shape
'''
Output (df_shape):
(5, 6)
'''
node_sums = df.to_numpy() @ incidence_sparse_matrix.T
'''
Output (node_sums):
[[ 8 11 15  6]
 [11 17 13 17]
 [24 20 20 14]
 [11 16 10 15]
 [15 14  6 11]]
'''
# Explanation: This matrix multiplication effectively sums the weights of all
# edges connected to each node for each row in the original DataFrame.
# The shape of 'df.to_numpy()' is (5, 6) and the shape of 'incidence_sparse_matrix.T'
# is (6, 4), resulting in a matrix of shape (5, 4), where each row corresponds
# to a row in the original DataFrame and each column corresponds to a unique node.

# Step 10: Create a DataFrame for the node sums with meaningful column names
nodeSum_df = pd.DataFrame(node_sums, columns=[f'f_{node}' for node in unique_nodes])
'''
Output (nodeSum_df):
    f_1  f_2  f_3  f_4
0    8   11   15    6
1   11   17   13   17
2   24   20   20   14
3   11   16   10   15
4   15   14    6   11
'''

result_df = pd.concat([df, nodeSum_df], axis=1)
'''
Output (result_df):
   1_2  1_3  1_4  2_3  2_4  3_4  f_1  f_2  f_3  f_4
0    1    5    2    8    2    2    8   11   15    6
1    4    3    4    5    8    5   11   17   13   17
2    8    8    8    9    3    3   24   20   20   14
3    4    3    4    4    8    3   11   16   10   15
4    8    0    7    4    2    2   15   14    6   11
'''
</code></pre>
","0","Answer"
"79603118","79390828","<p>Polars with Regex :</p>
<pre><code>import polars as pl

df = pl.DataFrame({
    'ID': [1, 1, 1, 2, 2, 2, 2, 2],
    'QuestionCode': ['Q01', 'Q01-1', 'Q02', 'Q01', 'Q02', 'Q02-1', 'Q02-1-1', 'Q02-2']
})

df= df.with_columns(
pl.col('QuestionCode').str.replace(r'-[^-]+$','').alias('BaseCode')    
)
print(df)
'''
┌─────┬──────────────┬──────────┐
│ ID  ┆ QuestionCode ┆ BaseCode │
│ --- ┆ ---          ┆ ---      │
│ i64 ┆ str          ┆ str      │
╞═════╪══════════════╪══════════╡
│ 1   ┆ Q01          ┆ Q01      │
│ 1   ┆ Q01-1        ┆ Q01      │
│ 1   ┆ Q02          ┆ Q02      │
│ 2   ┆ Q01          ┆ Q01      │
│ 2   ┆ Q02          ┆ Q02      │
│ 2   ┆ Q02-1        ┆ Q02      │
│ 2   ┆ Q02-1-1      ┆ Q02-1    │
│ 2   ┆ Q02-2        ┆ Q02      │
└─────┴──────────────┴──────────┘
'''
df = df.join(df.filter(pl.col('BaseCode') != pl.col('QuestionCode')).unique(),
left_on = ['ID','QuestionCode'], right_on = ['ID','BaseCode'],
how = 'anti'
)
'''
shape: (5, 3)
┌─────┬──────────────┬──────────┐
│ ID  ┆ QuestionCode ┆ BaseCode │
│ --- ┆ ---          ┆ ---      │
│ i64 ┆ str          ┆ str      │
╞═════╪══════════════╪══════════╡
│ 1   ┆ Q01-1        ┆ Q01      │
│ 1   ┆ Q02          ┆ Q02      │
│ 2   ┆ Q01          ┆ Q01      │
│ 2   ┆ Q02-1-1      ┆ Q02-1    │
│ 2   ┆ Q02-2        ┆ Q02      │
└─────┴──────────────┴──────────┘
'''

df = df.drop('BaseCode')
print(df) 
'''
shape: (5, 2)
┌─────┬──────────────┐
│ ID  ┆ QuestionCode │
│ --- ┆ ---          │
│ i64 ┆ str          │
╞═════╪══════════════╡
│ 1   ┆ Q01-1        │
│ 1   ┆ Q02          │
│ 2   ┆ Q01          │
│ 2   ┆ Q02-1-1      │
│ 2   ┆ Q02-2        │
└─────┴──────────────┘
'''
</code></pre>
","0","Answer"
"79616638","79377042","<p>Solution without Pytorch :</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({
    'ID': [1, 2, 3, 4, 5],
    'Col_1': [1, 0, 1, 1, 0],
    'Col_2': [0, 1, 1, 0, 1],
    'Col_3': [1, 1, 0, 1, 1]
})

featureCols = df.columns[1:]
#Index(['Col_1', 'Col_2', 'Col_3'], dtype='object')

featureData = df[featureCols].values

rowIdx,colIdx = np.nonzero(featureData)

featureNames_cols = np.array(featureCols)
#['Col_1' 'Col_2' 'Col_3']

mapped_featureNames_cols = featureNames_cols[colIdx]

uniqueRows, rowPositions = np.unique(rowIdx, return_index = True)

df['res'] = np.array([' , '.join(gr) for gr 
                      in np.split(mapped_featureNames_cols,rowPositions[1:])])
'''
   ID  Col_1  Col_2  Col_3            res
0   1      1      0      1  Col_1 , Col_3
1   2      0      1      1  Col_2 , Col_3
2   3      1      1      0  Col_1 , Col_2
3   4      1      0      1  Col_1 , Col_3
4   5      0      1      1  Col_2 , Col_3
'''
</code></pre>
","0","Answer"
"79619919","79390828","<p>Here, we're using a clever data structure called <strong><code>marisa_trie (</code></strong><code>pip install marisa-trie</code><strong><code>)</code></strong>. Think of it like a super-efficient way to store and quickly check for prefixes in a list of words or codes.</p>
<pre><code>import numpy as np
import pandas as pd
import marisa_trie

data = {
    'id': [1, 1, 1, 2, 2, 2, 2, 2],
    'code': ['Q01', 'Q01-1', 'Q02', 'Q01', 'Q02', 'Q02-1', 'Q02-1-1', 'Q02-2']
}
df = pd.DataFrame(data)

df = df.sort_values(['id','code'],ignore_index = True)
'''
   id     code
0   1      Q01
1   1    Q01-1
2   1      Q02
3   2      Q01
4   2      Q02
5   2    Q02-1
6   2  Q02-1-1
7   2    Q02-2
'''
keep_mask = np.ones(len(df),dtype = bool)
id_np = df['id'].to_numpy()
code_np = df['code'].to_numpy()
unique_ids = np.unique(id_np)

for uid in unique_ids :
    idx = np.where(id_np == uid)
    gr_codes = code_np[idx]
    gr_codes_list = gr_codes.tolist()
    trie = marisa_trie.Trie(gr_codes_list)
    has_descendant = [any(trie.keys(code + '-')) for code in gr_codes_list]
    has_descendant_np = np.array(has_descendant)
    keep_mask[idx] = ~has_descendant_np

res = df[keep_mask]#.reset_index(drop =True)
print(res)
'''
   id     code
1   1    Q01-1
2   1      Q02
3   2      Q01
6   2  Q02-1-1
7   2    Q02-2
''' 
 
</code></pre>
","0","Answer"
"79650458","79369190","<p>The <a href=""https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.background_gradient.html"" rel=""nofollow noreferrer""><code>.background_gradient</code></a> function accepts <code>vmin</code> and <code>vmax</code> arguments to define the range for the gradient. When these parameters are left unspecified, the minimum and maximum values are pulled from the data (or gmap) <a href=""https://github.com/pandas-dev/pandas/blob/0691c5cf90477d3503834d983f69350f250a6ff7/pandas/io/formats/style.py#L2878"" rel=""nofollow noreferrer"">ref</a>, but it is also possible to specify these values directly.</p>
<p>The appropriate gradient colours can be achieved in the sampled version, even when using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html"" rel=""nofollow noreferrer""><code>.sample</code></a> on the DataFrame first, by passing the min/max values from the <em>original</em> DataFrame's 'severity_level' column to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.background_gradient.html"" rel=""nofollow noreferrer""><code>.background_gradient</code></a>.</p>
<pre class=""lang-python prettyprint-override""><code>k = 5
(
    df
    .sample(n=k)
    .style
    .background_gradient(
        subset='severity_level',
        cmap='RdYlGn',
        vmin=df['severity_level'].min(),
        vmax=df['severity_level'].max()
    )
)
</code></pre>
<p><a href=""https://i.sstatic.net/pzzqdIaf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pzzqdIaf.png"" alt=""Styled DataFrame"" /></a></p>
","1","Answer"
"79423293","79423247","<p>If you provided complete code, it would be easier for me to set colors for the cells for your excel file. But an example script given below might assist you:</p>
<pre><code>import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

data = {
    &quot;ID&quot;: [1, 2, 3, 4, 5],
    &quot;Status&quot;: [&quot;OK&quot;, &quot;NO&quot;, &quot;OK&quot;, &quot;NO&quot;, &quot;OK&quot;]
}

df = pd.DataFrame(data)

excel_filename = &quot;status.xlsx&quot;
df.to_excel(excel_filename, index=False)

wb = load_workbook(excel_filename)
ws = wb.active

# Define fill colors
green_fill = PatternFill(start_color=&quot;00FF00&quot;, end_color=&quot;00FF00&quot;, fill_type=&quot;solid&quot;)  # Green for &quot;OK&quot;
red_fill = PatternFill(start_color=&quot;FF0000&quot;, end_color=&quot;FF0000&quot;, fill_type=&quot;solid&quot;)    # Red for &quot;NO&quot;

# Apply color based on cell value in the &quot;Status&quot; column
for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=2, max_col=2):  
    for cell in row:
        if cell.value == &quot;OK&quot;:
            cell.fill = green_fill
        elif cell.value == &quot;NO&quot;:
            cell.fill = red_fill

wb.save(excel_filename)

print(&quot;Excel file created successfully!&quot;)
</code></pre>
","3","Answer"
"79424250","79423247","<p>To do what you want using Pandas to colour fill, you can do something like the following;</p>
<p>This uses the example dataframe from the other answer.<br>
<sub>assumes Pandas version 2</sub></p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    &quot;ID&quot;: [1, 2, 3, 4, 5],
    &quot;Status&quot;: [&quot;OK&quot;, &quot;NO&quot;, &quot;OK&quot;, &quot;NO&quot;, &quot;OK&quot;]
})


def highlight_ok(t):
    text_is_OK = t == &quot;OK&quot;

    return ['background-color: lightgreen' if v else 'background-color: red' for v in text_is_OK]

# determining the name of the file
camp = 'A'
file_name = camp + '_Last_20' + '.xlsx'
 
# saving the excel
df.style.apply(highlight_ok, subset=['Status']).to_excel(file_name, freeze_panes=(1, 0), index=False)

print('Tournament is written to Excel File successfully.')
</code></pre>
<p>However Openpyxl can be more flexible. If you do use Openpyxl there is no need to write the workbook with to_excel then reload the Excel Workbook into Openpyxl. <br>
Just change the engine to Openpyxl and the writer will be Openpyxl objects.<br>
Assumes Pandas version 2 is being used.</p>
<p><sub>Using the same dataframe example</sub></p>
<pre><code>import pandas as pd
from openpyxl.styles import PatternFill

df = pd.DataFrame({
    &quot;ID&quot;: [1, 2, 3, 4, 5],
    &quot;Status&quot;: [&quot;OK&quot;, &quot;NO&quot;, &quot;OK&quot;, &quot;NO&quot;, &quot;OK&quot;]
})

camp = 'A'  # Dummy value for the file_name creation
file_name = camp + '_Last_20' + '.xlsx'
excel_sheet = 'Sheet1'

with pd.ExcelWriter(file_name, mode='w', engine=&quot;openpyxl&quot;) as writer:
    df.to_excel(writer, sheet_name=excel_sheet, index=False, freeze_panes=(1, 0))

    ws = writer.sheets[excel_sheet]  # This is the Openpyxl worksheet

    # Apply color based on cell value in the &quot;Status&quot; column
    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=2, max_col=2):
        value = df[&quot;Status&quot;].iloc[row[0].row - 2]  # value is &quot;OK&quot; or &quot;NO&quot;
        row[0].fill = PatternFill(&quot;solid&quot;, start_color=(&quot;90EE90&quot; if value == &quot;OK&quot; else 'FF0000'))


print('Tournament is written to Excel File successfully.')
</code></pre>
<p>If the highlighting is preferred to be dynamic you can use Conditional Formatting on the range of values (in this case B2:B6). <br>
Obviously this means the cell value will change the highlight colour as the cell value is changed from 'OK' to 'NO' and vice versa.<br></p>
<p>Just use this section of code rather than the fill loop<br>
<code># Apply color based on cell value in the &quot;Status&quot; column</code> .</p>
<pre><code># Use Conditional formatting
# Import CellIsRule --&gt; from openpyxl.formatting.rule import CellIsRule

# Conditional Format range
cf_range = 'B2:B6'
# Define fill colors
green_fill = PatternFill(start_color=&quot;90EE90&quot;, end_color=&quot;90EE90&quot;, fill_type=&quot;solid&quot;)  # Green for &quot;OK&quot;
red_fill = PatternFill(start_color=&quot;FF0000&quot;, end_color=&quot;FF0000&quot;, fill_type=&quot;solid&quot;)    # Red for &quot;NO&quot;

ws.conditional_formatting.add(cf_range, CellIsRule(operator='equal', formula=['&quot;OK&quot;'], fill=green_fill))
ws.conditional_formatting.add(cf_range, CellIsRule(operator='equal', formula=['&quot;NO&quot;'], fill=red_fill))

</code></pre>
<p>You could also use [Xlsxwriter] for doing this instead of Openpyxl though Openpyxl will allow you to write in to an existing workbook whereas Xlsxwriter can only create a new workbook.</p>
","2","Answer"
"79424261","79424140","<p>you should only change the &quot;map&quot; function to &quot;apply&quot; function, as:</p>
<pre><code>df_test.apply(lambda x: x.replace('blah1',np.nan))
</code></pre>
","0","Answer"
"79424371","79424017","<p>unpack a dictionary created from the pairing of w_params and operation within the named aggregation:</p>
<pre><code>def agg_daily(df, common_cols, w_params, operation):
    mapped = zip(w_params, operation)
    mapped = {f&quot;{col}_{func}&quot;: (col, func) for col, func in mapped}
    outcome = df.groupby(common_cols, as_index=False).agg(**mapped)
    return outcome
</code></pre>
<p>Application:</p>
<pre><code>data = {'model': {0: 'Mazda RX4', 1: 'Mazda RX4 Wag', 2: 'Datsun 710'},
 'mpg': {0: 21.0, 1: 21.0, 2: 22.8},
 'cyl': {0: 6, 1: 6, 2: 4},
 'disp': {0: 160.0, 1: 160.0, 2: 108.0},
 'hp': {0: 110, 1: 110, 2: 93},
 'drat': {0: 3.9, 1: 3.9, 2: 3.85},
 'wt': {0: 2.62, 1: 2.875, 2: 2.32},
 'qsec': {0: 16.46, 1: 17.02, 2: 18.61},
 'vs': {0: 0, 1: 0, 2: 1},
 'am': {0: 1, 1: 1, 2: 1},
 'gear': {0: 4, 1: 4, 2: 4},
 'carb': {0: 4, 1: 4, 2: 1}}

mtcars = pd.DataFrame(data)
agg_daily(df=mtcars, 
          common_cols='cyl', 
          w_params=['disp','hp','drat'], 
          operation=['min','max','min'])
   cyl  disp_min  hp_max  drat_min
0    4     108.0      93      3.85
1    6     160.0     110      3.90
</code></pre>
<p>ideally, you'll add some checks - w_params should be same length as operation, the entries in operation should be strings (if they are not you have to consider how to grab the names - <code>.__name__()</code> possibly), ...</p>
","2","Answer"
"79424396","79424140","<p>Since <code>map()</code> is applied element-wise, each element (x) is a string- when <code>replace()</code> is called, it is invoking Python's built-in string method, instead of Panda's.</p>
<p>The error is raised because Python's built-in method requires both arguments to be strings.</p>
","1","Answer"
"79425561","79425295","<p><code>PeriodIndex</code> has time-specific features. See the user guide: <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html"" rel=""nofollow noreferrer"">Time series / date functionality</a></p>
<p>In any case, you don't need to set the index as string, since:</p>
<p>A) <code>PeriodIndex</code> provides a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.PeriodIndex.quarter.html"" rel=""nofollow noreferrer""><code>.quarter</code></a> attribute</p>
<pre><code>qIncSt[qIncSt.index.quarter == 1]
</code></pre>
<p>B) Even if it didn't, you can always do <code>.astype(str)</code> on-demand and use that for filtering.</p>
<pre><code>qIncSt[qIncSt.index.astype(str).str.contains('Q1')]
</code></pre>
<h3>Example</h3>
<pre><code>months = [&quot;2023-01&quot;, &quot;2023-02&quot;, &quot;2023-03&quot;, &quot;2023-04&quot;, &quot;2023-05&quot;]
qIncSt = pd.DataFrame({
    'fiscalDateEnding': pd.to_datetime(months),
    'foobar': range(len(months))})
qIncSt.index = pd.PeriodIndex(
    qIncSt['fiscalDateEnding'],
    freq='Q',
    name='Quarter')
</code></pre>
<pre><code>&gt;&gt;&gt; qIncSt
        fiscalDateEnding  foobar
Quarter                         
2023Q1        2023-01-01       0
2023Q1        2023-02-01       1
2023Q1        2023-03-01       2
2023Q2        2023-04-01       3
2023Q2        2023-05-01       4

&gt;&gt;&gt; qIncSt[qIncSt.index.quarter == 1]
        fiscalDateEnding  foobar
Quarter                         
2023Q1        2023-01-01       0
2023Q1        2023-02-01       1
2023Q1        2023-03-01       2

&gt;&gt;&gt; qIncSt[qIncSt.index.astype(str).str.contains('Q1')]
        fiscalDateEnding  foobar
Quarter                         
2023Q1        2023-01-01       0
2023Q1        2023-02-01       1
2023Q1        2023-03-01       2
</code></pre>
","0","Answer"
"79426011","79425965","<p>You will have to find encoding of CSV file first with the following:</p>
<pre><code>import chardet
import pandas as pd

with open('your_file.csv', 'rb') as f:
   enc = chardet.detect(f.read())  # or readline if the file is large

encoding = enc['encoding']
</code></pre>
<p>Once you know the encoding then you can use your method to read the file with.</p>
<p>df_planned = pd.read_csv('/content/sample_data/planned.csv', encoding=encoding)</p>
<p>(replace with found ending)</p>
","0","Answer"
"79426931","79426845","<p>You can use <code>boxplot</code> for the grouping as well</p>
<pre><code>df.boxplot(column='V', by=['L1', 'L2'])
</code></pre>
<p><a href=""https://i.sstatic.net/ZLH2rXFm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZLH2rXFm.png"" alt=""enter image description here"" /></a></p>
","3","Answer"
"79426991","79426836","<p>Since you goal seems to be filtering, couldn't you replace your extract logic by a simple <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a>?:</p>
<pre><code># identify rows with F_NAME starting with &quot;Sar&quot;
m1 = base_dataframe['F_NAME'].str.startswith('Sar')
# identify rows with L_NAME starting with &quot;Mari&quot;
m2 = base_dataframe['L_NAME'].str.startswith('Mari')

# keep rows with either match
out = base_dataframe[m1|m2]
</code></pre>
<p>Or, if you have multiple conditions:</p>
<pre><code>conditions = [base_dataframe['F_NAME'].str.startswith('Sar'),
              base_dataframe['L_NAME'].str.startswith('Mari'),
             # ... other conditions,
             ]

out = base_dataframe[np.logical_xor.reduce(conditions)]
</code></pre>
<p>Output:</p>
<pre><code>   F_NAME    L_NAME            EMAIL
0    Suzy   Maripol    suzy@mail.com
2     Flo  Mariland     flo@mail.com
3   Sarah    Linder   sarah@mail.com
5  Sarosh      Fink  sarosh@mail.com
</code></pre>
","2","Answer"
"79427053","79426845","<p>If you want to use <code>groupby</code> anyway, probaly you should create a single index for grouping, e.g.,</p>
<pre><code>df.groupby(df['L1']+',' +df['L2'].astype(str)).boxplot(column = 'V',figsize = (8,8))
</code></pre>
<p><a href=""https://i.sstatic.net/nuERbGBP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nuERbGBP.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79427355","79426835","<p>Found it, make sure that the venv you are using is called <code>dbt-env</code> dbt will automatically take this venv where you have installed <code>pandas</code> or whatever package you needed!</p>
","1","Answer"
"79428152","79428080","<p>Here are a few that I've used over the years from time to time, have not put together a reason for why some of these fix it and some don't, but try and play around with these:</p>
<ol>
<li>Set max sequence items to None as well</li>
</ol>
<p><code>pd.options.display.max_seq_items = None</code></p>
<ol start=""2"">
<li>Not using <code>set_option()</code></li>
</ol>
<pre><code># can also try setting to very large numbers e.g. 1000000000
pd.options.display.max_rows = None
pd.options.display.max_columns = None
pd.options.display.width = None
</code></pre>
","0","Answer"
"79428246","79425965","<p>I solved the problem with brute force:</p>
<pre><code>x=open('iuput.txt','rb')
for line in x:
    for char in line:
       try:
           print(chr(char),end='')
       except UnicodeEncodeError:
           print('\nOffending character:',char)
           exit()
</code></pre>
<p>Finally, the shortest way was to correct the input files with a text editor.</p>
","2","Answer"
"79429019","79428965","<p>Testing your &quot;function&quot; with <code>pandas</code>, <code>polars</code> and <code>numpy</code></p>
<pre><code>import pandas as pd
import time
import numpy as np
import polars as pl


def test(func, argument):
    run_times = []
    for i in range(100):
        st = time.perf_counter()
        df = func(argument)
        run_time = time.perf_counter() - st
        run_times.append(run_time)
    return np.mean(run_times)

def f_pandas(df):
    min_abs_diff = (df[0] - target_value).abs().min()
    return df.loc[(df[0] - target_value).abs() == min_abs_diff]

def f_pandas_vectorized(df):
    return df.loc[(df[0] - target_value).abs().idxmin()]

def f_polars(df):
    min_abs_diff = (df[&quot;column_0&quot;] - target_value).abs().min() 
    return df.filter((df[&quot;column_0&quot;] - target_value).abs() == min_abs_diff)

def f_numpy(data):
    abs_diff = np.abs(data[:, 0] - target_value)
    min_idx = np.argmin(abs_diff)
    return pd.DataFrame(data[[min_idx]])


target_value = 0.5
data = np.random.rand(100000, 1000)
df = pd.DataFrame(data)
df_pl = pl.DataFrame(data)

print(f&quot;average pandas runtime: {test(f_pandas, df)}&quot;)
print(f&quot;average pandas runtime with idxmin(): {test(f_pandas_vectorized, df)}&quot;)
print(f&quot;average polars runtime: {test(f_polars, df_pl)}&quot;)
print(f&quot;average numpy runtime: {test(f_numpy, data)}&quot;)
</code></pre>
<p>I got this results running in a Jupyter Notebook on a Linux machine.</p>
<pre><code>average pandas runtime: 0.00989325414002451
average pandas runtime with idxmin(): 0.005005129760029377
average polars runtime: 0.006758741329904296
average numpy runtime: 0.004175669220221607

average pandas runtime: 0.009967705049803044
average pandas runtime with idxmin(): 0.005097740050114225
average polars runtime: 0.006972378070222476
average numpy runtime: 0.004102102290034964

average pandas runtime: 0.010020545769948512
average pandas runtime with idxmin(): 0.004993948210048984
average polars runtime: 0.007027968560159934
average numpy runtime: 0.004024256040174805
</code></pre>
<p>You see <code>polars</code> is faster than your <code>panda</code> code, but using vectorized operations like idxmin() in <code>pandas</code> at least in this case is better than <code>polars</code>. <code>numpy</code> is often faster in this type of numerical work.</p>
","3","Answer"
"79429197","79427970","<p>You can use the <code>linregress</code> function from <code>scipy.stats</code> in combination with pandas's <code>groupby</code>. Group the data by <code>imagefile_mnemonic</code>, <code>a</code>, and <code>pix</code>, then apply a custom function to each group to calculate the slope using <code>linregress</code>.</p>
<p>It should be something like this</p>
<pre><code>import pandas as pd
from scipy.stats import linregress

data = {
    'imagefile': ['file1']*7 + ['file6']*2,
    'imagefile_mnemonic': ['image1']*7 + ['image6']*2,
    'a': [0]*7 + [23]*2,
    'b': [0, 1, 2, 3, 4, 5, 6, 11, 12],
    'pix': ['v1']*7 + ['v5']*2,
    'val': [55, 75, 95, 115, 135, 155, 175, 763, 787]
}
df = pd.DataFrame(data)

print(df)

def calculate_slope(group):
    slope, intercept, r_value, p_value, std_err = linregress(group['b'], group['val'])
    return pd.Series({'slope': slope, 'intercept': intercept, 'r_value': r_value, 'p_value': p_value, 'std_err': std_err})

results = df.groupby(['imagefile_mnemonic', 'a', 'pix']).apply(calculate_slope).reset_index()

print(results)
</code></pre>
<p>Which gives</p>
<pre><code>  imagefile_mnemonic   a pix  slope  intercept  r_value       p_value  std_err
0             image1   0  v1   20.0       55.0      1.0  1.920675e-50      0.0
1             image6  23  v5   24.0      499.0      1.0  0.000000e+00      0.0
</code></pre>
","0","Answer"
"79429845","79429681","<p>You can do it using a Shiny App and <code>DT</code>. The resulting column order will be shown on top with column indexes. I used <code>mtcars</code>, but you can do use your own data. The columns are drag &amp; dropable. Right now, I put the max drag-n-drop time to 900 ms.</p>
<p><a href=""https://i.sstatic.net/HlVcZ2CO.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HlVcZ2CO.gif"" alt=""out"" /></a></p>
<pre><code>library(shiny)
library(DT)

callback &lt;- JS(&quot;
  var finalOrder;
  
  table.on('column-reorder.dt', function(e, settings, details) {
    clearTimeout(finalOrder);
    
    // Wait for all reordering to finish before sending the final order
    finalOrder = setTimeout(function() {
      var order = table.colReorder.order();
      Shiny.setInputValue('columnsOrder', order);
    }, 900); // update to time you need for dragging
  });
&quot;)

df &lt;- mtcars

shinyApp(
  ui = fluidPage(
    p('Use this command to reorder your data.frame:'),
    fluidRow(column(12, verbatimTextOutput('reorder'))),
    p('Drag and Drop your columns in the correct order'),
    fluidRow(column(12, DTOutput('table')))
  ),
  
  server = function(input, output) {
    tableData &lt;- reactiveVal(df)
    lastOrder &lt;- reactiveVal(NULL)
    
    output$table &lt;- renderDT({
      datatable(
        tableData(),
        extensions = 'ColReorder',
        options = list(
          dom = 'Rlfrtip',
          pageLength = nrow(tableData()),
          colReorder = TRUE
        ),
        callback = callback,
        rownames = FALSE
      )
    })
    
    observeEvent(input$columnsOrder, {
      req(input$columnsOrder)
      
      # Get the new order
      new_order &lt;- as.numeric(input$columnsOrder) + 1
      
      # Only update if the order is different from the last one
      if (!identical(new_order, lastOrder())) {
        lastOrder(new_order)
        current_data &lt;- tableData()
        tableData(current_data[, new_order])
      }
    })
    
    output$reorder &lt;- renderPrint({
      current_cols &lt;- colnames(tableData())
      paste0(
        &quot;df &lt;- df[, c(&quot;,
        paste0(&quot;'&quot;, current_cols, &quot;'&quot;, collapse = &quot;, &quot;),
        &quot;)]&quot;
      )
    })
  }
)
</code></pre>
","0","Answer"
"79429882","79429681","<p>A solution is to create a simple Shiny app that uses the <code>sortable</code> R package to enable you to reorder the columns of tables you upload using a drag and drop interface:</p>
<pre><code>library(shiny)
library(sortable)

ui &lt;- fluidPage(
  titlePanel(&quot;Drag-and-Drop Column Reordering&quot;),
  fileInput(&quot;target_upload&quot;, h5(strong(&quot;Click to Upload CSV File&quot;), style = &quot;color:#007acc;&quot;),
            accept = c(&quot;text/csv&quot;),
            buttonLabel = strong(&quot;Select File&quot;, style = &quot;color:#007acc;&quot;),
            placeholder = &quot;No file selected&quot;),
  
  fluidRow(
    column(
      width = 4,
      h4(&quot;Reorder Columns:&quot;),
      uiOutput(&quot;rank_list_ui&quot;)
    ),
    column(
      width = 8,
      h4(&quot;Data Frame Preview:&quot;),
      tableOutput(&quot;table&quot;),
      downloadButton(&quot;download&quot;, strong(&quot;Download&quot;), icon = icon(&quot;download&quot;))
    )
  )
)

server &lt;- function(input, output, session) {
  
  
  file_upload &lt;- reactive({
    inFile &lt;- input$target_upload
    if (is.null(inFile)) {
      return(NULL)
    }
    data &lt;- read.csv(inFile$datapath, header = TRUE, sep = &quot;,&quot;)
    return(data)
  })
  
  output$rank_list_ui &lt;- renderUI({
    data &lt;- file_upload()
    if (is.null(data)) {
      return(NULL)
    }
    column_names &lt;- colnames(data)
    rank_list(
      input_id = &quot;column_order&quot;,
      labels = column_names,
      options = sortable_options(multiDrag = TRUE)
    )
  })
  
  reordered_df &lt;- reactive({
    data &lt;- file_upload()
    order &lt;- input$column_order
    if (is.null(data) || is.null(order)) {
      return(NULL)
    }
    data[, order, drop = FALSE]
  })
  
  
  output$table &lt;- renderTable({
    reordered_df()
  })
  
  
  output$download &lt;- downloadHandler(
    filename = function() {
      paste(&quot;data&quot;, &quot;.csv&quot;, sep = &quot;&quot;)
    },
    content = function(file) {
      write.csv(reordered_df(), file, row.names = FALSE)
    }
  )
}

shinyApp(ui, server)
</code></pre>
<p><a href=""https://i.sstatic.net/exBJsBvI.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/exBJsBvI.gif"" alt=""enter image description here"" /></a></p>
","3","Answer"
"79430203","79429681","<p>You can use a text editor to cut and paste column names to avoid re-typing column names.  These are all very simple involving just 1 or 2 lines of R code and do not depend on R Studio.</p>
<p><strong>1) edit</strong> Use <code>edit</code> to put the names vector of the data frame, here <code>mtcars</code>, into a text editor, edit the names to be in the order you want and then exit the editor.</p>
<pre><code>mtcars2 &lt;- mtcars[edit(names(mtcars))]
</code></pre>
<p>The names will appear like this in the text editor:</p>
<pre><code>c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;drat&quot;, &quot;wt&quot;, &quot;qsec&quot;, &quot;vs&quot;, &quot;am&quot;, 
&quot;gear&quot;, &quot;carb&quot;)
</code></pre>
<p><strong>2) cat/readLines</strong> A variation of that is the following.  names.dat will contain one name per line and those lines can be rearranged in a text editor. This makes the editing slightly easier than in (1) at the expense of an extra line of R code to invoke.</p>
<pre><code>cat(names(mtcars), file = &quot;names.dat&quot;, sep = &quot;\n&quot;)
# in a text editor edit names.dat rearranging the names
mtcars2 &lt;- mtcars[readLines(&quot;names.dat&quot;)]
</code></pre>
<p>The names will appear like this in the text editor making them even easier to cut and paste</p>
<pre><code>mpg
cyl
disp
hp
drat
wt
qsec
vs
am
gear
carb
</code></pre>
<p><strong>3) Programmatic rearrangement</strong> In some cases it may be possible to sort the columns programmatrically.  For example if we have <code>dat</code> shown below and want to sort the columns so that all the a's sort together and similarly for the b's.</p>
<pre><code>library(gtools)

dat &lt;- data.frame(a1 = 1, b1 = 2, a2 = 3, b2 = 4, a3 = 5, b3 = 6)

dat[mixedsort(names(dat))]

##   a1 a2 a3 b1 b2 b3
## 1  1  3  5  2  4  6
</code></pre>
<p>or this which has the advantage that it keeps other columns in place</p>
<pre><code>dat2 &lt;- data.frame(id = 0, a1 = 1, b1 = 2, a2 = 3, b2 = 4, a3 = 5, b3 = 6)
root &lt;- sub(&quot;\\d+$&quot;, &quot;&quot;, names(dat2))
dat2[order(match(root, root))]

##   id a1 a2 a3 b1 b2 b3
## 1  0  1  3  5  2  4  6
</code></pre>
","0","Answer"
"79430225","79428965","<p>You could optimize your polars query a bit, specially use expressions instead of df[&quot;col&quot;]. Possibly even more so if you don't mind only getting one row out of the query instead of including all values tying for the minimal.</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl
import time
import numpy as np

target_value = 0.5
data = np.random.rand(1000,100)
df = pl.DataFrame(data)

run_times = []
for i in range(100):
    st = time.time()
    abs_diff = (pl.col('column_0') - target_value).abs()
    # Option A - keep original behaviour but just better optimized
    # df_filtered = df.filter(abs_diff == abs_diff.min())
    # Option B - only get the row with minimal index instead of filtering
    df_filtered = df.row(df.select(abs_diff.arg_min()).item())
    run_time = time.time() - st
    run_times.append(run_time)

print(f&quot;avg polars run: {sum(run_times)/len(run_times)}&quot;)
</code></pre>
<p>Like others said, numpy (or jax etc.) may as well be better suited for this kind of work though</p>
","2","Answer"
"79430953","79430908","<p>Select the inverse of what you want to drop:</p>
<p>List all the column names you care about with <code>colnames = [f&quot;S-{i}&quot; for i in range(1,21)]</code> (I've modified this to fit the example data in the below demo)</p>
<p><code>functools.reduce</code> with <code>operator.or_</code> handles the fact that any column could have an &quot;N&quot; and the <code>~</code> handles the negation of that condition so you only select the rows where no column has an &quot;N&quot;.</p>
<pre><code>In [67]: df
Out[67]:
  Subject S-1 S-2 S-3 S-20
0     101   A   B   A    C
1     102   N   A   C    A
2     201   N   N   B    N
3     202   B   B   N    N

In [68]: colnames = ['S-1', 'S-2', 'S-3', 'S-20']

In [69]: df[~functools.reduce(operator.or_, (df[col].eq(&quot;N&quot;) for col in colnames))]
Out[69]:
  Subject S-1 S-2 S-3 S-20
0     101   A   B   A    C
</code></pre>
<p>Don't forget to <code>import fuctools, operator</code></p>
","0","Answer"
"79431473","79428848","<pre><code>import json
import pandas as pd

# Load file JSON
with open('00_000000065.json', 'r') as file:
    data = json.load(file)

# Print file JSON
print(data)

# Create DataFrame and set Key like index
access_levels = pd.DataFrame(list(data['AccessLevels'].items()), columns=['Key', 'Value'])
access_levels.set_index(&quot;Key&quot;, inplace=True)
#Stampa Tutto
print(&quot;\nAccessLevels:&quot;)
print(access_levels)

# Access the value directly using the key
print(&quot;Value of Home.btnBuzzer:&quot;)
print(access_levels.loc[&quot;Home.btnBuzzer&quot;, &quot;Value&quot;])

print(&quot;Ciao Amico, sono anch'io italiano. :D, spero di averti aiutato&quot;)
</code></pre>
<p>The problem is in the JSON, that is, when importing AccessLevels it contains a dictionary (a nested structure) and pandas does not flatten the JSON correctly, leading to non-direct access to the values.</p>
<p>To solve it, create the DataFrame transforming it into a two-column table, setting the Key column as an index so you access the value via key.</p>
","0","Answer"
"79431775","79431751","<pre><code>s = df['Letters'].str[:2]
out = df[s.isin(first_2) &amp; s.eq(df['Value'])]
</code></pre>
<p>out</p>
<pre><code>    Letters Value   Position
0   AB      AB      1.0
3   DA      DA      4.0
</code></pre>
","2","Answer"
"79431906","79431885","<p>use pandas' str.split function, with the keyword argument: <code>expand=True</code>:</p>
<pre class=""lang-py prettyprint-override""><code>df.columns = df.columns.str.split('_', expand=True)
df
   One        Two
     X    Y     X     Y
0  1.1  1.2  1.11  1.22
1  1.1  1.2  1.11  1.22
2  1.1  1.2  1.11  1.22
</code></pre>
","2","Answer"
"79432375","79432346","<p>You have to loop over the shifts, but you could reuse the grouper:</p>
<pre><code>shifts = [1, 5, 30]
g = df.groupby(cols)['ln']
for s in shifts:
    df[f'{s}d'] = df['ln']-g.shift(s)
</code></pre>
<p>Also, your approach to use <code>groupby.apply</code> + <code>.values</code> is incorrect. This would only work if the rows are already sorted by group. If you use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.shift.html"" rel=""nofollow noreferrer""><code>groupby.shift</code></a> you will maintain a correct index/alignment.</p>
<p>If you really need to use a custom function, go with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a>:</p>
<pre><code>shifts = [1, 5, 30]
g = df.groupby(cols)['ln']
for s in shifts:
    df[f'{s}d'] = g.transform(lambda x: x-x.shift(s))
</code></pre>
<p>Example with a dummy input and <code>cols = ['A', 'B']</code>:</p>
<pre><code>    A  B  ln    1d    5d  30d
0   0  0   0   NaN   NaN  NaN
1   0  0   2   2.0   NaN  NaN
2   0  0  10   8.0   NaN  NaN
3   0  1   6   NaN   NaN  NaN
4   0  1   8   2.0   NaN  NaN
5   0  1   9   1.0   NaN  NaN
6   0  1  15   6.0   NaN  NaN
7   1  0   4   NaN   NaN  NaN
8   1  0  19  15.0   NaN  NaN
9   1  1   1   NaN   NaN  NaN
10  1  1   3   2.0   NaN  NaN
11  1  1  12   9.0   NaN  NaN
12  1  1  16   4.0   NaN  NaN
13  1  1  17   1.0   NaN  NaN
14  1  1  18   1.0  17.0  NaN
15  2  0   7   NaN   NaN  NaN
16  2  0  11   4.0   NaN  NaN
17  2  0  13   2.0   NaN  NaN
18  2  1   5   NaN   NaN  NaN
19  2  1  14   9.0   NaN  NaN
</code></pre>
","2","Answer"
"79432382","79432200","<p>The issue occurs because PyArrow timestamps behave differently in Pandas. To perform arithmetic operations, you need to convert them explicitly to datetime64[ns] and remove the timezone</p>
<pre><code>import pandas as pd

df['pickup'] = df['pickup'].astype('datetime64[ns]').dt.tz_localize(None)
df['dropoff'] = df['dropoff'].astype('datetime64[ns]').dt.tz_localize(None)

df['trip_duration'] = df['dropoff'] - df['pickup']
</code></pre>
<p>If .astype('datetime64[ns]') does not work, try using pd.to_datetime() explicitly:</p>
<pre><code>df['pickup'] = pd.to_datetime(df['pickup']).dt.tz_convert(None)
df['dropoff'] = pd.to_datetime(df['dropoff']).dt.tz_convert(None)

df['trip_duration'] = df['dropoff'] - df['pickup']
</code></pre>
","0","Answer"
"79432493","79432482","<p>Maybe your title column is not actually a column but an index trying using this.</p>
<pre><code>print(df.index)
</code></pre>
<p>You can define your own columns and skip the headers from csv file</p>
<pre><code>import pandas as pd

column_names = [&quot;title&quot;, &quot;text&quot;, &quot;subject&quot;, &quot;date&quot;, &quot;label&quot;]  # Define the expected column names
df = pd.read_csv(&quot;your_file.csv&quot;, names=column_names, header=0) 
</code></pre>
","1","Answer"
"79433287","79432928","<p>You should try to filter the file first, by looking for an odd number of quotes (&quot;) or counting the number of commas in each line, or using any other method you prefer. After filtering you can do:</p>
<pre><code>data = pd.read_csv('bad_data.csv', header=0,names=[&quot;Authors&quot;,&quot;Title&quot;,&quot;ID&quot;,&quot;URN&quot;], delimiter=',', quotechar='&quot;', doublequote=False, encoding='utf-8')
print(data)
</code></pre>
<p>Or you can try something like this code, which filters the lines, separates the bad ones and prints them, saves the good ones in a dictionary, and then creates the DataFrame at the end.</p>
<p>Either way you will need to create your own filter.</p>
<pre><code>import pandas as pd
# Manual filter
with open('bad_data.csv', mode='r', encoding='utf-8') as f:
    data_csv = f.readlines()

dict_filtered = {&quot;Authors&quot;:[], &quot;Title&quot;:[], &quot;ID&quot;: [], &quot;URN&quot;: []}
for line in data_csv:
    if &quot;Authors&quot; not in line: # Skip header line
        # Some handle to recognize bad data
        # Example -&gt; data with more than 5 commas 
        list_data = line.split(',')
        if len(list_data) &gt; 5:
            print(f'Bad line:{line}')
        else:
            
            author = f&quot;{list_data[0]}, {list_data[1]}&quot;
            title = list_data[2]
            ID = list_data[3]
            URN = list_data[4]

            dict_filtered['Authors'].append(author)
            dict_filtered['Title'].append(title)
            dict_filtered['ID'].append(ID)
            dict_filtered['URN'].append(URN)

print(&quot;DATAFRAME&quot;)
df = pd.DataFrame(dict_filtered)
print(df)
</code></pre>
","0","Answer"
"79433291","79432928","<p>So the problem here as far as I understand is that the CSV file has some rows with issues — things like unmatched quotes or commas inside quoted fields — which mess up parsing when using pd.read_csv() and instead of letting pd.read_csv() throw errors or skip those bad rows, you want to handle them manually. Here's the solution that I think serves your purpose.</p>
<pre><code>import pandas as pd
import csv

data = []
bad_lines = []

with open(&quot;path.csv&quot;, mode=&quot;r&quot;, encoding=&quot;utf-8&quot;) as file:
    reader = csv.reader(file, delimiter=&quot;,&quot;, quotechar='&quot;')
    headers = next(reader)  

    for i, row in enumerate(reader, start=2):
        try:
            if row.count('&quot;') % 2 != 0:
                raise ValueError(&quot;Unmatched quotes in row&quot;)
        
            if any(',' in field and '&quot;' in field for field in row): #(indicating possible unescaped commas)
                raise ValueError(&quot;Commas inside quoted field.&quot;)
        
            data.append(row) #append if no issues exist
    
        except Exception as e:
            # Store bad lines
            bad_lines.append((i, row, str(e)))

uncorrupted_df = pd.DataFrame(data, columns=headers)

# Print bad lines encountered
print(&quot;Following bad lines encountered:&quot;)
for line in bad_lines:
    print(f&quot;Line {line[0]}: {line[1]} (Error: {line[2]})&quot;)
</code></pre>
<p>This approach will catch and handle malformed rows instead of letting read_csv() silently skip them or throw errors etc.
I hope this works for you!</p>
","0","Answer"
"79433482","79433460","<p>No you can't with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a>. You would need to create a column prior to the <code>groupby</code>:</p>
<pre><code>(df.eval('prod = value1 * value2')
   .groupby(['Sample', 'Year'])
   .agg(agg1=('prod', 'mean'))
)
</code></pre>
<p>Output:</p>
<pre><code>             agg1
Sample Year      
1      2023   6.0
       2025   8.0
2      2024   8.0
       2026   6.0
</code></pre>
<p>Note that to be able to use multiple columns from within <code>groupby</code> you could also use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html"" rel=""nofollow noreferrer""><code>groupby.apply</code></a>, but for the shown example this is less elegant and less efficient:</p>
<pre><code>(df.groupby(['Sample', 'Year'])
   .apply(lambda x: x['value1'].mul(x['value2']).mean())
)
</code></pre>
<p>Output:</p>
<pre><code>Sample  Year
1       2023    6.0
        2025    8.0
2       2024    8.0
        2026    6.0
dtype: float64
</code></pre>
","3","Answer"
"79433945","79432928","<p><strong>TLDR:</strong></p>
<p>Looking at the pandas code for 1.4.1 and I assume for 1.4.0 There seems to be a bug that simply bypasses the <code>callable</code> specification. You can verify that by putting a break point inside your method and see that it is never called.</p>
<p><strong>Apparently this has been resolved in newer (v2) pandas and the pyarrow engine.</strong></p>
<p>So the solution is likely to be upgrade to a newer version of pandas or patch your version.</p>
<p><strong>Background:</strong></p>
<p>In python_parser.py (about line 744) we find this method that works for the <code>on_bad_lines</code> &quot;error&quot; and &quot;warn&quot; conditions but fails to handle the callable path and thus simply returns <code>None</code> in the event of a parse error. When this method returns <code>None</code> the caller (<code>_next_line(self)</code>) just ignores that line.</p>
<p>I'm guessing here that this is because getting the actual data from <code>line = next(self.data)</code> is tricky as <code>self.data</code> is a <code>csv.reader</code> object and this iterator throws without exposing the actual data.</p>
<pre class=""lang-py prettyprint-override""><code>    def _next_iter_line(self, row_num: int) -&gt; list[Scalar] | None:
        &quot;&quot;&quot;
        Wrapper around iterating through `self.data` (CSV source).

        When a CSV error is raised, we check for specific
        error messages that allow us to customize the
        error message displayed to the user.

        Parameters
        ----------
        row_num: int
            The row number of the line being parsed.
        &quot;&quot;&quot;
        try:
            # assert for mypy, data is Iterator[str] or None, would error in next
            assert self.data is not None
            line = next(self.data)  ### &lt;---- This throws with your data
            # for mypy
            assert isinstance(line, list)
            return line
        except csv.Error as e:
            if (
                self.on_bad_lines == self.BadLineHandleMethod.ERROR
                or self.on_bad_lines == self.BadLineHandleMethod.WARN
            ):
                msg = str(e)

                if &quot;NULL byte&quot; in msg or &quot;line contains NUL&quot; in msg:
                    msg = (
                        &quot;NULL byte detected. This byte &quot;
                        &quot;cannot be processed in Python's &quot;
                        &quot;native csv library at the moment, &quot;
                        &quot;so please pass in engine='c' instead&quot;
                    )

                if self.skipfooter &gt; 0:
                    reason = (
                        &quot;Error could possibly be due to &quot;
                        &quot;parsing errors in the skipped footer rows &quot;
                        &quot;(the skipfooter keyword is only applied &quot;
                        &quot;after Python's csv library has parsed &quot;
                        &quot;all rows).&quot;
                    )
                    msg += &quot;. &quot; + reason

                self._alert_malformed(msg, row_num)
            return None
</code></pre>
","0","Answer"
"79434326","79432928","<p>I wonder if you can fix your bad CSV first, then run the good CSV through Pandas; don't try to fix it in Pandas (wrong tool for the job).</p>
<p>I don't know how representative your sample CSV really is, but it looks like someone hand-rolled their own CSV encoder by doing something like enclosing every item in a list with double quotes, then joining those elements with commas:</p>
<pre class=""lang-py prettyprint-override""><code>data = [
    [&quot;amy bob&quot;, &quot;i&quot;],
    [&quot;cam, deb&quot;, &quot;ii&quot;],
    ['&quot;el&quot; fay', &quot;iii&quot;],
]

for row in data:
    quoted = [f'&quot;{x}&quot;' for x in row]
    print(&quot;,&quot;.join(quoted))
</code></pre>
<p>which prints:</p>
<pre class=""lang-none prettyprint-override""><code>&quot;amy bob&quot;,&quot;i&quot;
&quot;cam, deb&quot;,&quot;ii&quot;
&quot;&quot;el&quot; fay&quot;,&quot;iii&quot;
</code></pre>
<p>Given your sample, would the original data look something like?</p>
<pre class=""lang-py prettyprint-override""><code>[
    ['Authors',        'Title',                 'ID',         'URN'                      ],
    ['...&quot;Stack&quot;...',  '...in &quot;Stack&quot;...',      '&quot;117348359', 'URN:ND_649C52T1A9K1JJ51'  ],
    ['Tyson, Mike',    'Me boxing',             '525166266',  'URN:SD_95125N2N3523N5KB'  ],
    ['Smith, Garry',   '&quot;Funny name&quot;...',       '951992851',  'URN:ND_95N5J2N352BV525N25'],
    ['The club of...', '...&quot;overflow stacker&quot;', '9551236651', 'URN:SD_955K2B61J346F1N25' ],
]
</code></pre>
<p><em>Not sure about the double quote in the ID of the first record, &quot;117348359. I assume that's a typo you made typing up the sample.</em></p>
<p>If so, you might start by assuming that all lines:</p>
<ul>
<li>have a beginning and ending double quote (&quot;bookend quotes&quot;)</li>
<li>the sequence <code>&quot;,&quot;</code> only appears in between fields, you don't expect to see that sequence in the data itself</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>N_COLS = 4

with open(&quot;input-bad.csv&quot;) as f:
    for i, line in enumerate(f, start=1):
        line = line.strip()

        if line[0] != '&quot;' or line[-1] != '&quot;':
            print(f&quot;line {i} doesn't have bookend quotes: {line}&quot;)

        if (n := line.count('&quot;,&quot;')) != N_COLS - 1:
            print(f&quot;line {i} appears to have {n} cols: {line}&quot;)
</code></pre>
<p>Adding a &quot;bad record&quot; at the end of your sample:</p>
<pre class=""lang-py prettyprint-override""><code>['Mr Baz', 'foo&quot;,&quot; the bar', '99999999', 'URN:TX_77777777']
</code></pre>
<pre class=""lang-none prettyprint-override""><code>Mr Baz&quot;,&quot;foo&quot;,&quot; the bar&quot;,&quot;99999999&quot;,&quot;URN:TX_77777777
</code></pre>
<p>would print:</p>
<pre class=""lang-none prettyprint-override""><code>line 6 doesn't have bookend quotes: Mr Baz&quot;,&quot;foo&quot;,&quot; the bar&quot;,&quot;99999999&quot;,&quot;URN:TX_77777777
line 6 appears to have 4 cols:      Mr Baz&quot;,&quot;foo&quot;,&quot; the bar&quot;,&quot;99999999&quot;,&quot;URN:TX_77777777
</code></pre>
<p>Hopefully that doesn't print out anything, or only a small number of records you could deal with (I don't how you'd deal with them, though).</p>
<p>If so, then you can fix the file and output good CSV:</p>
<pre class=""lang-py prettyprint-override""><code>import csv, sys


def process_line(line: str) -&gt; str:
    &quot;&quot;&quot;
    Strip surrounding whitespace, remove bookend quotes.
    &quot;&quot;&quot;
    return line.strip()[1:-1]


writer = csv.writer(sys.stdout)

with open(&quot;input-bad.csv&quot;) as f:
    for line in f:
        line = process_line(line)
        fields = line.split('&quot;,&quot;')
        writer.writerow(fields)
</code></pre>
<p>Running that on your sample I get (with leading spaces for readability):</p>
<pre class=""lang-none prettyprint-override""><code>Authors,           Title,                        ID,            URN
&quot;...&quot;&quot;Stack&quot;&quot;...&quot;, &quot;...in &quot;&quot;Stack&quot;&quot;...&quot;,         &quot;&quot;&quot;117348359&quot;, URN:ND_649C52T1A9K1JJ51
&quot;Tyson, Mike&quot;,     Me boxing,                    525166266,     URN:SD_95125N2N3523N5KB
&quot;Smith, Garry&quot;,    &quot;&quot;&quot;Funny name&quot;&quot;...&quot;,          951992851,     URN:ND_95N5J2N352BV525N25
The club of...,    &quot;...&quot;&quot;overflow stacker&quot;&quot;...&quot;, 9551236651,    URN:SD_955K2B61J346F1N25
</code></pre>
<p><em>Again, &quot;&quot;117348359 looks weird, but I'll leave that for you.</em></p>
","0","Answer"
"79434436","79434402","<p>A possible solution:</p>
<pre><code>(df.groupby(['Subject', 'Test'], as_index=False)
    .agg({'Scores': lambda x: ','.join(x.astype(str))})
</code></pre>
<p>This code groups by both <code>Subject</code> and <code>Test</code> columns, then aggregates the <code>Scores</code> column by converting the values to strings and joining them with commas.</p>
<p>Output:</p>
<pre><code>   Subject   Test       Scores
0  English  Test3     33,89,54
1  History  Test2  28,90,76,55
2     Math  Test1     75,60,34
</code></pre>
","0","Answer"
"79434458","79434429","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>tmp = (df[[*'ABC']].where(lambda x: x != 0)
       .stack()
       .rename_axis([None, 'Category'])
       .reset_index(1, name='Val')
       .iloc[:, [1,0]]
       )

out = (df.iloc[:, :4]
       .merge(tmp, 
              left_index=True, 
              right_index=True, 
              how='right')
       )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>  Name    A    B    C  Val Category
0    x  1.1  0.0  0.2  1.1        A
0    x  1.1  0.0  0.2  0.2        C
1    y  0.0  0.1  0.0  0.1        B
2    z  0.5  0.1  0.3  0.5        A
2    z  0.5  0.1  0.3  0.1        B
2    z  0.5  0.1  0.3  0.3        C
</code></pre>
<p><strong>Explanation / intermediates</strong></p>
<ul>
<li>Select the relevant columns (<code>['A','B','C']</code>) and apply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.where.html"" rel=""nofollow noreferrer""><code>df.where</code></a> to replace <code>0</code> with <code>np.nan</code> (or indeed: use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html"" rel=""nofollow noreferrer""><code>df.replace</code></a>) and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>df.stack</code></a>. Note that <code>NaN</code> values will be dropped.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>df[[*'ABC']].where(lambda x: x != 0).stack()

0  A    1.1
   C    0.2
1  B    0.1
2  A    0.5
   B    0.1
   C    0.3
dtype: float64
</code></pre>
<ul>
<li>Next, use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.rename_axis.html"" rel=""nofollow noreferrer""><code>Series.rename_axis</code></a> to rename level 1 to 'Category' and add it as a column via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.reset_index.html"" rel=""nofollow noreferrer""><code>Series.reset_index</code></a>. Adding <code>name</code> sets the values to 'Val' column.</li>
<li>Add <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>df.iloc</code></a> to switch the order:</li>
</ul>
<pre><code>tmp

   Val Category
0  1.1        A
0  0.2        C
1  0.1        B
2  0.5        A
2  0.1        B
2  0.3        C
</code></pre>
<ul>
<li>Ignoring <code>['Val', 'Category']</code> from the original <code>df</code>, apply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>df.merge</code></a> on the indices with <code>how='right'</code>.</li>
</ul>
<hr />
<p>To customize the categories, you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>out['Category'] = out['Category'].map({'A': 'first', ...})
</code></pre>
<p>Or, since your columns are in alphabetical order, you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Categorical.html"" rel=""nofollow noreferrer""><code>pd.Categorical</code></a> with <a href=""https://pandas.pydata.org/docs/dev/reference/api/pandas.Categorical.rename_categories.html"" rel=""nofollow noreferrer""><code>Categorical.rename_categories</code></a>.</p>
<pre class=""lang-py prettyprint-override""><code>pd.Categorical(out['Category'])

['A', 'C', 'B', 'A', 'B', 'C']
Categories (3, object): ['A', 'B', 'C']
</code></pre>
<p>Hence:</p>
<pre class=""lang-py prettyprint-override""><code>out['Category'] = (pd.Categorical(out['Category'])
                   .rename_categories(['first', 'second', 'third']))
</code></pre>
<p><strong>Data used</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

data = {'Name': {0: 'x', 1: 'y', 2: 'z'}, 
        'A': {0: 1.1, 1: 0.0, 2: 0.5}, 
        'B': {0: 0.0, 1: 0.1, 2: 0.1}, 
        'C': {0: 0.2, 1: 0.0, 2: 0.3}, 
        'Val': {0: np.nan, 1: np.nan, 2: np.nan}, 
        'Category': {0: np.nan, 1: np.nan, 2: np.nan}
        }
df = pd.DataFrame(data)
</code></pre>
","1","Answer"
"79434461","79434402","<p>Here is an example of how to do what you want:</p>
<pre><code>df2 = df.groupby([&quot;Test&quot;, &quot;Subject&quot;]).agg(
    Scores=(&quot;Scores&quot;, pd.Series.to_list),
)

print(df2)
</code></pre>
<p>The output is:</p>
<pre><code>| Test  | Subject | Scores              |
|-------|---------|---------------------|
| Test1 | Math    | [75, 60, 34]        |
| Test2 | History | [28, 90, 76, 55]    |
| Test3 | English | [33, 89, 54]        |
</code></pre>
<p>Setup for this example:</p>
<pre><code>import pandas as pd

data = {
    &quot;Test&quot;: [&quot;Test1&quot;, &quot;Test1&quot;, &quot;Test1&quot;, &quot;Test2&quot;, &quot;Test2&quot;, &quot;Test2&quot;, &quot;Test2&quot;, &quot;Test3&quot;, &quot;Test3&quot;, &quot;Test3&quot;],
    &quot;Subject&quot;: [&quot;Math&quot;, &quot;Math&quot;, &quot;Math&quot;, &quot;History&quot;, &quot;History&quot;, &quot;History&quot;, &quot;History&quot;, &quot;English&quot;, &quot;English&quot;, &quot;English&quot;],
    &quot;Scores&quot;: [75, 60, 34, 28, 90, 76, 55, 33, 89, 54],
}

df = pd.DataFrame(data)
</code></pre>
","0","Answer"
"79434512","79434429","<p>You could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html"" rel=""nofollow noreferrer""><code>replace</code></a> the 0s with NaNs, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a> the columns to your categories, reshape to long with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>stack</code></a>, and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>join</code></a> back to the original to duplicate the rows:</p>
<pre><code>out = (df
       .drop(columns=['Val', 'Category'])
       .join(df[['A', 'B', 'C']]
             .set_axis(['first', 'second', 'third'], axis=1)
             .rename_axis(columns='Category')
             .replace(0, pd.NA)
             .stack()
             .rename('Val')
             .reset_index(-1)
            )
       )
</code></pre>
<p>Output:</p>
<pre><code>  Name    A    B    C Category  Val
0    x  1.1  0.0  0.2    first  1.1
0    x  1.1  0.0  0.2    third  0.2
1    y  0.0  0.1  0.0   second  0.1
2    z  0.5  0.1  0.3    first  0.5
2    z  0.5  0.1  0.3   second  0.1
2    z  0.5  0.1  0.3    third  0.3
</code></pre>
","1","Answer"
"79434603","79434541","<p>Not sure if this is the thing you are after (in terms of speed) but you can have a try</p>
<pre><code>lut = {item: qty for item, qty in zip(df['Item'],df['Quantity'])}

df['Related Quantities'] = df['Related Items'].apply(lambda x: {k:lut[k] for k in x})
</code></pre>
<p>and you will see</p>
<pre><code>Item  Quantity          Related Items  \
0      Flowers         1               [Bushes]   
1       Bushes         2              [Flowers]   
2         Cars         3  [Trucks, Motorcycles]   
3       Trucks         4    [Cars, Motorcycles]   
4  Motorcycles         5         [Cars, Trucks]   

                Related Quantities  
0                    {'Bushes': 2}  
1                   {'Flowers': 1}  
2  {'Trucks': 4, 'Motorcycles': 5}  
3    {'Cars': 3, 'Motorcycles': 5}  
4         {'Cars': 3, 'Trucks': 4}
</code></pre>
","1","Answer"
"79434621","79434242","<p>Compute a key Series from the keys of df2 that are in df1 and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.ffill.html"" rel=""nofollow noreferrer""><code>ffill</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.bfill.html"" rel=""nofollow noreferrer""><code>bfill</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>, and restore the originally missing values with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mask.html"" rel=""nofollow noreferrer""><code>mask</code></a>:</p>
<pre><code>m = df2['area2_name'].isin(df1['area1_name'])
key2 = df2['area2_name'].where(m).ffill().bfill()

out = df1.merge(df2.assign(m=m), left_on='area1_name', right_on=key2, how='outer')

out[df1.columns] = out[df1.columns].mask(out.pop('m').eq(False))
</code></pre>
<p>Since the <code>merge</code> proceeds in order of left and right whenever the keys match, you'll have the desired order.</p>
<p>Output:</p>
<pre><code>   area1_index area1_name  area2_index area2_name
0          NaN        NaN          0.0         MN
1          0.0         AL          1.0         AL
2          NaN        NaN          2.0         CT
3          NaN        NaN          3.0         TX
4          1.0         AK          4.0         AK
5          2.0         AZ          NaN        NaN
6          3.0         AR          5.0         AR
7          4.0         CA          6.0         CA
8          5.0         CO          NaN        NaN
</code></pre>
<p>Intermediate:</p>
<pre><code>   area1_index area1_name  area2_index area2_name      m key2
0            0         AL          0.0         MN  False   AL
1            0         AL          1.0         AL   True   AL
2            0         AL          2.0         CT  False   AL
3            0         AL          3.0         TX  False   AL
4            1         AK          4.0         AK   True   AK
5            2         AZ          NaN        NaN    NaN  NaN
6            3         AR          5.0         AR   True   AR
7            4         CA          6.0         CA   True   CA
8            5         CO          NaN        NaN    NaN  NaN
</code></pre>
","0","Answer"
"79434639","79434429","<p><strong>Categories without word-based representations of numbers</strong></p>
<p>Another possible solution:</p>
<pre><code>cols = ['Val', 'Category']

df[cols] = df.loc[:, 'A':'C'].replace(0, np.nan).apply(
    lambda x: pd.Series(
        [x.dropna(), (1 + np.where(x.notna())[0])]), 
    axis=1)

df.explode(cols)
</code></pre>
<p>The steps are:</p>
<ul>
<li><p>The code selects columns <code>A</code> to <code>C</code> from the dataframe and replaces all <code>0</code> values with <code>NaN</code> values using the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html"" rel=""nofollow noreferrer""><code>replace</code></a> method.</p>
</li>
<li><p>Then, it applies a lambda function to each row using the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a> method. Inside the lambda function, the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.dropna.html"" rel=""nofollow noreferrer""><code>dropna</code></a> method is used to remove any <code>NaN</code> values from the row. The <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.notna.html"" rel=""nofollow noreferrer""><code>notna</code></a> method is then used to identify the positions of the non-<code>NaN</code> values, and these positions are incremented by 1.</p>
</li>
<li><p>The results assigned to columns <code>Val</code> and <code>Category</code>.</p>
</li>
<li><p>Finally, the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a> method is used to transform each list-like element in the <code>Val</code> and <code>Category</code> columns into individual rows.</p>
</li>
</ul>
<p>Output:</p>
<pre><code>  Name    A    B    C  Val Category
0    x  1.1  0.0  0.2  1.1        1
0    x  1.1  0.0  0.2  0.2        3
1    y  0.0  0.1  0.0  0.1        2
2    z  0.5  0.1  0.3  0.5        1
2    z  0.5  0.1  0.3  0.1        2
2    z  0.5  0.1  0.3  0.3        3
</code></pre>
<hr />
<p><strong>Categories with word-based representations of numbers</strong></p>
<p>In case, we really need the categories as words, we can use <code>inflect</code> library as follows:</p>
<pre><code># pip install inflect
import inflect

p = inflect.engine()

df.explode(cols).assign(
    Category=lambda x: x['Category']
    .map(lambda n: p.number_to_words(p.ordinal(n))))
</code></pre>
<p>Output:</p>
<pre><code>  Name    A    B    C  Val Category
0    x  1.1  0.0  0.2  1.1    first
0    x  1.1  0.0  0.2  0.2    third
1    y  0.0  0.1  0.0  0.1   second
2    z  0.5  0.1  0.3  0.5    first
2    z  0.5  0.1  0.3  0.1   second
2    z  0.5  0.1  0.3  0.3    third
</code></pre>
","0","Answer"
"79434645","79433821","<p>Maybe you can convert the <code>From_Mi</code>/<code>To_Mi</code> to <code>range()</code>, explode it and use <code>.groupby</code> to get the <strong>first</strong>/<strong>last</strong> inspection dates, e.g.:</p>
<pre class=""lang-py prettyprint-override""><code># sort if necessary:
# all_df = all_df.sort_values(by='Date')

all_df['mile'] = all_df.apply(lambda row: range(row['From_Mi'], row['To_Mi']), axis=1)
all_df = all_df.explode('mile')

out = (all_df
    .groupby('mile')['Date']
    .last()
    .reset_index()
)

out = (out
    .groupby((out['Date'] != out['Date'].shift()).cumsum())
    .agg({'Date': 'first', 'mile':'first'})
    .reset_index(drop=True)
    .rename({'mile':'From_Mi'}, axis=1)
)

out['To_Mi'] = out['From_Mi'].shift(-1).fillna(all_df['To_Mi'].max()).astype(int)

print(out)
</code></pre>
<p>Prints:</p>
<pre class=""lang-none prettyprint-override""><code>        Date  From_Mi  To_Mi
0 2025-01-22        0      2
1 2025-01-15        2      3
2 2025-01-08        3      5
</code></pre>
","0","Answer"
"79434649","79433821","<p>I wasn't able to work out how to use a more vectorised approach with <code>piso</code> and <code>IntervalArrays</code>, but you could use something like the following:</p>
<pre class=""lang-py prettyprint-override""><code># ensure sorted
all_df = all_df.sort_values(&quot;Date&quot;, ascending=True)
# interval array
arr = pd.arrays.IntervalArray.from_arrays(all_df.From_Mi, all_df.To_Mi)
# unique intervals
unique_intervals = piso.split(arr, set(arr.left).union(arr.right)).unique()

# dates with interval index
dates = all_df.set_index(pd.IntervalIndex(arr)).Date
# create dataframe with left and right intervals and last date where interval overlaps
output = pd.DataFrame(
    [
        {
            &quot;From_Mi&quot;: ui.left,
            &quot;To_Mi&quot;: ui.right,
            &quot;Date&quot;: dates[dates.index.overlaps(ui)].iloc[-1]
        }
        for ui in unique_intervals
    ]
)
</code></pre>
","0","Answer"
"79434839","79433821","<p>When I think of intervals, associated with values, I tend to think step functions.  I think an approach with piso could exist but it'd be easier to do with <a href=""https://www.staircase.dev/en/latest/"" rel=""nofollow noreferrer"">staircase</a>.  If you have piso installed then you also have staircase because piso is built upon it.</p>
<p>The values of the step function need to be numbers though, not dates, so this means converting the dates into a number - time passed since some (arbitrary) start date or &quot;epoch&quot;:</p>
<pre><code>import staircase as sc

epoch = pd.Timestamp(&quot;2024-1-1&quot;)
series_stairs = all_df.apply(
    lambda row: sc.Stairs(
        start=row.From_Mi,
        end=row.To_Mi,
        value=(row.Date-epoch)/pd.Timedelta(&quot;1D&quot;),
    ),
    axis=1,
)
</code></pre>
<p><code>series_stairs</code> is then a pandas Series of Stairs objects.  Each Stairs object is an abstraction of a step function:</p>
<pre><code>0    &lt;staircase.Stairs, id=4673826608&gt;
1    &lt;staircase.Stairs, id=4697665760&gt;
2    &lt;staircase.Stairs, id=4703875344&gt;
3    &lt;staircase.Stairs, id=4703876688&gt;
dtype: object
</code></pre>
<p>and you can plot them:</p>
<pre><code>import matplotlib.pyplot as plt

_, ax = plt.subplots(figsize=(5,2))
for s in series_stairs:
    s.plot(ax=ax)
</code></pre>
<p><a href=""https://i.sstatic.net/pBC73tFf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBC73tFf.png"" alt=""plots of step functions"" /></a></p>
<p>If we take the maximum of these step functions then the result will be a step function which gives the latest point in time that each point on the road is checked - any point in the road, not only integer points, so this extends to using fractional values for From_Mi and To_Mi if you wish.</p>
<pre><code>max_stairs = sc.max(series_stairs)
max_stairs.plot()
</code></pre>
<p><a href=""https://i.sstatic.net/tr8cSTGy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tr8cSTGy.png"" alt=""max step function"" /></a></p>
<p>You can then convert this step function back to dataframe format, remove and 0-valued intervals (these will correspond to any interval on the road which have never been inspected according to the data) and convert the step function values back to datetimes:</p>
<pre><code>recent_df = max_stairs.to_frame().query(&quot;value != 0&quot;)
recent_df[&quot;Last-Date&quot;] = recent_df[&quot;value&quot;].apply(
    lambda n: epoch + pd.Timedelta(days=n)
)
</code></pre>
<p><code>recent_df</code> then looks like this:</p>
<pre><code>start end  value   Last-Date
    0   2  387.0  2025-01-22
    2   3  380.0  2025-01-15
    3   5  373.0  2025-01-08
</code></pre>
<p>and you can clean it up to your liking.</p>
<p>Disclaimer: I am the author of piso and staircase.</p>
","0","Answer"
"79434847","79434429","<p>Using numpy indexing:</p>
<pre><code>nms = np.array(['first', 'second', 'third'])
d = df[['A', 'B', 'C']]
row, col = np.argwhere(d.gt(0)).T
res = df.iloc[row].assign(Val = d.to_numpy()[row, col], Category = nms[col])
res

 Name    A    B    C  Val Category
0    x  1.1  0.0  0.2  1.1    first
0    x  1.1  0.0  0.2  0.2    third
1    y  0.0  0.1  0.0  0.1   second
2    z  0.5  0.1  0.3  0.5    first
2    z  0.5  0.1  0.3  0.1   second
2    z  0.5  0.1  0.3  0.3    third
</code></pre>
","0","Answer"
"79435018","79434994","<p>What do you mean &quot;seems wrong&quot;?  The following line works fine:</p>
<pre><code>.groupby(['Item', 'Size', 'Price'], as_index=False)['Quantity'].sum()
</code></pre>
<p><a href=""https://i.sstatic.net/8MY7smNT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8MY7smNT.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79435019","79434994","<p>Use groupby while preserving order:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    &quot;Item&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;],
    &quot;Size&quot;: [&quot;2-3cm&quot;, &quot;1-2cm&quot;, &quot;2cm&quot;, &quot;1-2cm&quot;],
    &quot;Price&quot;: [0.5, 0.6, 0.7, 0.6],
    &quot;Quantity&quot;: [10, 20, 30, 40],
})

df = df.groupby([&quot;Item&quot;, &quot;Size&quot;, &quot;Price&quot;], sort=False, as_index=False).agg({&quot;Quantity&quot;: &quot;sum&quot;})
print(df)
</code></pre>
<p>Output:</p>
<pre class=""lang-css prettyprint-override""><code>
  Item   Size  Price  Quantity
0    A  2-3cm    0.5       10
1    A  1-2cm    0.6       60
2    B    2cm    0.7       30
</code></pre>
<p>This consolidates duplicates while maintaining order.</p>
<h2>Explanation:</h2>
<p>groupby([&quot;Item&quot;, &quot;Size&quot;, &quot;Price&quot;], sort=False): Groups data by &quot;Item&quot;, &quot;Size&quot;, and &quot;Price&quot; while keeping the original order.</p>
<p>agg({&quot;Quantity&quot;: &quot;sum&quot;}): Sums the &quot;Quantity&quot; values for duplicate rows.</p>
<p>as_index=False: Ensures the result remains a proper DataFrame instead of using a hierarchical index.</p>
","0","Answer"
"79435111","79434541","<p>Using <code>df.apply</code> with a lookup dictionary.</p>
<pre><code>import pandas as pd

data = {'Item': ['Flowers', 'Bushes', 'Cars', 'Trucks', 'Motorcycles'],
        'Quantity': [1, 2, 3, 4, 5],
        'Related Items': [['Bushes'], ['Flowers'], ['Trucks', 'Motorcycles'], ['Cars', 'Motorcycles'], ['Cars', 'Trucks']]}
df = pd.DataFrame(data)


# Creates a dictionary for fast quantity lookups
item_quantities = df.set_index('Item')['Quantity'].to_dict()

def create_related_quantities(row):
    related_quantities = {}
    for item in row['Related Items']:
        quantity = item_quantities.get(item)  # Get quantity from lookup dictionary or None if not found
        if quantity is not None:  # Only add to dict if quantity exists
            related_quantities[item] = quantity
    return related_quantities

# Applies the create_related_quantities function to each row (axis=1) of the DataFrame
df['Related Quantities'] = df.apply(create_related_quantities, axis=1)

print(df)
</code></pre>
<p>The new column created</p>
<pre><code>          Item  ...               Related Quantities
0      Flowers  ...                    {'Bushes': 2}
1       Bushes  ...                   {'Flowers': 1}
2         Cars  ...  {'Trucks': 4, 'Motorcycles': 5}
3       Trucks  ...    {'Cars': 3, 'Motorcycles': 5}
4  Motorcycles  ...         {'Cars': 3, 'Trucks': 4}
</code></pre>
","1","Answer"
"79435546","79435496","<p>Your question is not fully clear and without specific about your real dataset it's not easy to help you.</p>
<p>That said, in response to &quot;<em>I have downsteam code that needs to remove all unnecessary columns, which is one of the main reason to ask for a faster version</em>&quot;.</p>
<p>You are performing a <code>merge</code> using all columns of <code>A</code> to immediately remove those that are common with <code>B</code> (except for the merging keys).</p>
<p>You could therefore improve efficiency by pre-filtering <code>A</code> to only keep the keys and unique columns:</p>
<pre><code>keys = ['A1', 'A2']
A_keep = A.columns.difference(B.columns).union(keys)

out = B.merge(A[A_keep], how='left', on=['A1', 'A2'])
</code></pre>
<p>Example input:</p>
<pre><code>A = pd.DataFrame({'A1': [1,1,1,2,2,2,3,3,3],
                  'A2': [1,2,3,1,2,3,1,2,3],
                  'D' : range(9),
                  'w' : 'wA',
                  'x' : 'xA',  # we don't want to keep this column
                  'y' : 'yA',  # we don't want to keep this column
                 })
B = pd.DataFrame({'A1': [1,1,1,2,2,2,3],
                  'A2': [1,2,3,1,1,2,4],
                  'x' : 'xB',
                  'y' : 'yB',
                  'z' : 'zB',
                 })
</code></pre>
<p>Output, which is identical to that of your code:</p>
<pre><code>   A1  A2   x   y   z    D    w
0   1   1  xB  yB  zB  0.0   wA
1   1   2  xB  yB  zB  1.0   wA
2   1   3  xB  yB  zB  2.0   wA
3   2   1  xB  yB  zB  3.0   wA
4   2   1  xB  yB  zB  3.0   wA
5   2   2  xB  yB  zB  4.0   wA
6   3   4  xB  yB  zB  NaN  NaN
</code></pre>
<p>If you have many common columns, there can be a large gain in speed.</p>
<p>For instance, using 1M rows and a variable number of common columns:</p>
<p><a href=""https://i.sstatic.net/6HWOmotB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6HWOmotB.png"" alt=""pandas merge with many common columns"" /></a></p>
<p>Relative to pre-filtering:</p>
<p><a href=""https://i.sstatic.net/eIeADWvI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eIeADWvI.png"" alt=""pandas merge with many common columns (relative)"" /></a></p>
","1","Answer"
"79435781","79435770","<p>Create a dictionary, assign it as a new dictionary key and export with <a href=""https://docs.python.org/3/library/json.html#json.dump"" rel=""nofollow noreferrer""><code>json.dump</code></a>:</p>
<pre><code>import json

headlines['track'] = df.to_dict(orient='records')

with open(path_out + 'result.json', 'w') as f:
    json.dump(headlines, f)
</code></pre>
<p>Or as a string:</p>
<pre><code>import json

headlines['track'] = df.to_dict(orient='records')

print(json.dumps(headlines, indent=2))
</code></pre>
<p>Output:</p>
<pre><code>{
  &quot;now&quot;: 1636008051.9,
  &quot;messages&quot;: 6236,
  &quot;track&quot;: [
    {
      &quot;Param1&quot;: 999999,
      &quot;Param2&quot;: 9999,
      &quot;name&quot;: &quot;rocket&quot;,
      &quot;lat&quot;: 57.878696,
      &quot;lon&quot;: 11.160667,
      &quot;altitude&quot;: 1089.0,
      &quot;vert_rate&quot;: 0.0,
      &quot;track&quot;: 137,
      &quot;speed&quot;: 2,
      &quot;category&quot;: 99,
      &quot;Param3&quot;: 999,
      &quot;Param4&quot;: 16,
      &quot;Param5&quot;: 0
    },
    {
      &quot;Param1&quot;: 999999,
      &quot;Param2&quot;: 9999,
      &quot;name&quot;: &quot;rocket&quot;,
      &quot;lat&quot;: 57.878796,
      &quot;lon&quot;: 11.160668,
      &quot;altitude&quot;: 2543.963336,
      &quot;vert_rate&quot;: 152638.0483,
      &quot;track&quot;: 137,
      &quot;speed&quot;: 2,
      &quot;category&quot;: 99,
      &quot;Param3&quot;: 999,
      &quot;Param4&quot;: 15,
      &quot;Param5&quot;: 0
    },
    {
      &quot;Param1&quot;: 999999,
      &quot;Param2&quot;: 9999,
      &quot;name&quot;: &quot;rocket&quot;,
      &quot;lat&quot;: 57.878896,
      &quot;lon&quot;: 11.16067,
      &quot;altitude&quot;: 4226.050004,
      &quot;vert_rate&quot;: 126781.7063,
      &quot;track&quot;: 137,
      &quot;speed&quot;: 2,
      &quot;category&quot;: 99,
      &quot;Param3&quot;: 999,
      &quot;Param4&quot;: 14,
      &quot;Param5&quot;: 0
    },
    {
      &quot;Param1&quot;: 999999,
      &quot;Param2&quot;: 9999,
      &quot;name&quot;: &quot;rocket&quot;,
      &quot;lat&quot;: 57.878796,
      &quot;lon&quot;: 11.160669,
      &quot;altitude&quot;: 6091.207544,
      &quot;vert_rate&quot;: 121824.349,
      &quot;track&quot;: 137,
      &quot;speed&quot;: 2,
      &quot;category&quot;: 99,
      &quot;Param3&quot;: 999,
      &quot;Param4&quot;: 13,
      &quot;Param5&quot;: 0
    },
    {
      &quot;Param1&quot;: 999999,
      &quot;Param2&quot;: 9999,
      &quot;name&quot;: &quot;rocket&quot;,
      &quot;lat&quot;: 57.878696,
      &quot;lon&quot;: 11.160667,
      &quot;altitude&quot;: 8098.097372,
      &quot;vert_rate&quot;: 121471.6581,
      &quot;track&quot;: 137,
      &quot;speed&quot;: 2,
      &quot;category&quot;: 99,
      &quot;Param3&quot;: 999,
      &quot;Param4&quot;: 12,
      &quot;Param5&quot;: 0
    }
  ]
}
</code></pre>
","2","Answer"
"79436096","79436039","<p>I am not sure if you can easily get a representation with contiguous polygons, however you could easily get the bounding polygon from a set of points using <a href=""https://shapely.readthedocs.io/en/2.0.6/reference/shapely.convex_hull.html"" rel=""noreferrer""><code>shapely.convex_hull</code></a>:</p>
<pre><code>import shapely
import matplotlib.pyplot as plt

f, ax = plt.subplots(figsize=(8, 8))

for (name, color), coords in df.groupby(['label', 'color'])[['x', 'y']]:
    polygon = shapely.convex_hull(shapely.MultiPoint(coords.to_numpy()))
    ax.fill(*polygon.exterior.xy, color=color)
    ax.annotate(name, polygon.centroid.coords[0],
                ha='center', va='center')
</code></pre>
<p>If you want the shapely polygons:</p>
<pre><code>polygons = {k: shapely.convex_hull(shapely.MultiPoint(g.to_numpy()))
            for k, g in df.groupby(['label', 'color'])[['x', 'y']]}
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/7oCQVsfe.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/7oCQVsfe.png"" alt=""matplotlib shapely polygon from list of points convex hull"" /></a></p>
<h4>contiguous polygons</h4>
<p>To have contiguous polygons you can use the same strategy after adding points with a greater density and assigning them to their closest counterpart with a <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html"" rel=""noreferrer""><code>KDTree</code></a>:</p>
<pre><code>from scipy.spatial import KDTree

# interpolate points on the initial polygons
polygons = {k: shapely.convex_hull(shapely.MultiPoint(g.to_numpy()))
            for k, g in df.groupby('label')[['x', 'y']]}

def interp_ext(shape):
    try:
        return np.c_[shape.xy].T
    except NotImplementedError:
        pass
    e = shape.exterior if hasattr(shape, 'exterior') else shape
    points = e.interpolate(np.linspace(0, e.length, 1000))
    return np.c_[Polygon(points).exterior.xy].T

df2 = (pd.DataFrame([(l, *interp_ext(p)) for l, p in polygons.items()],
                    columns=['label', 'x', 'y'])
         .merge(df[['label', 'color']], on='label') 
         .explode(['x', 'y'])
      )

# get bounding values
xmin, ymin, xmax, ymax = df[['x', 'y']].agg(['min', 'max']).values.ravel()

# create a grid with a higher density (here 10x)
Xs = np.arange(xmin, xmax, 0.1)
Ys = np.arange(ymin, ymax, 0.1)
Xs, Ys = (x.ravel() for x in np.meshgrid(Xs, Ys))
grid = np.c_[Xs, Ys]

# indentify closest reference point
_, idx = KDTree(df2[['x', 'y']]).query(grid)

# create new DataFrame with labels/colors
df3 = pd.DataFrame(np.c_[grid, df2[['label', 'color']].to_numpy()[idx]],
                   columns=['x', 'y', 'label', 'color']
                  )

# plot
f, ax = plt.subplots(figsize=(8, 8))

for (name, color), coords in df3.groupby(['label', 'color'])[['x', 'y']]:
    polygon = shapely.convex_hull(shapely.MultiPoint(coords.to_numpy()))
    ax.fill(*polygon.exterior.xy, color=color)
    ax.annotate(name, polygon.centroid.coords[0],
                ha='center', va='center')
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/cwLSPIig.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/cwLSPIig.png"" alt=""enter image description here"" /></a></p>
<p>Another, faster, option could be to use a <a href=""https://en.wikipedia.org/wiki/Voronoi_diagram"" rel=""noreferrer"">Voronoi diagram</a> based on the original shapes. I found a library (<a href=""https://github.com/longavailable/voronoi-diagram-for-polygons"" rel=""noreferrer""><code>voronoi-diagram-for-polygons</code></a>) that does this but requires GeoPandas:</p>
<pre><code>import geopandas as gpd
from longsgis import voronoiDiagram4plg
from shapely import Polygon, convex_hull, coverage_union

# create the initial convex hulls
tmp = (df.groupby(['label', 'color'])
         .apply(lambda x: convex_hull(Polygon(x[['x', 'y']].to_numpy())))
         .reset_index(name='geometry')
      )

# convert to geodataframe
gdf = gpd.GeoDataFrame(tmp, geometry='geometry')

# Split using a Voronoi diagram
mask = Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)])
tmp = voronoiDiagram4plg(gdf, mask, densify=True)

# plot
tmp.plot(color=gdf['color'])
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/5QpfwHOp.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/5QpfwHOp.png"" alt=""enter image description here"" /></a></p>
","13","Answer"
"79436355","79436352","<p>Provide a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.html"" rel=""nofollow noreferrer""><code>Series</code></a> with the correct indices to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.insert.html"" rel=""nofollow noreferrer""><code>insert</code></a>:</p>
<pre><code>df.insert(1, 'NewCol', pd.Series(['X', 'Y'], index=[1, 3]))
</code></pre>
<p>Output:</p>
<pre><code>   A NewCol   B
0  1    NaN  10
1  2      X  20
2  3    NaN  30
3  4      Y  40
4  5    NaN  50
</code></pre>
","2","Answer"
"79440047","79439971","<p>Load only needed column using <code>usecols</code> arg and filter according to some criteria.</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(fl,
            names=['first_name', 'last_name', 'job', 'company', 'address', 'city',
       'country', 'email', 'PostingDate'], usecols=['city'],nrows=100)

print(df[df['city'].str.contains('mouth')])
</code></pre>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(fl,
            names=['a','b','c','d'], usecols=['c'],nrows=100)

print(df[df['c'].str.contains('******')])
</code></pre>
","2","Answer"
"79440061","79439971","<p>I made a test to look for values that have 'ab' in them:</p>
<pre><code>test_df = pd.DataFrame({'a':['**ab**','**cd**'],
                        'b':['**ef**','**gh**'],
                        'c':['**ij**','**kl**']})

test_df[test_df['b'].str.contains('ef')]
</code></pre>
<p>We get:</p>
<pre><code>   index a       b       c
   0    **ab**  **ef**  **ij**
</code></pre>
","1","Answer"
"79440434","79440315","<p>Yes, it is better to pre-compute <code>x</code>. Actually what matters is the <a href=""https://docs.python.org/3/reference/expressions.html#operator-precedence"" rel=""nofollow noreferrer"">operator precedence</a> and the order in which the operations are performed.</p>
<p>Assuming <code>s</code> your Series, when you run <code>(a*b+c*d)*s*e/f</code> you perform two multiplications and one division of the full Series. If you pre-compute or use <code>(a*b+c*d)*e/f*s</code>, then there is only one operation involving the Series.</p>
<p>Example:</p>
<pre><code>%timeit x*s
1.19 ms ± 73.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

%timeit (a*b+c*d)*s*e/f
3.45 ms ± 133 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

%timeit s*(a*b+c*d)*e/f
3.63 ms ± 84.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# now let's force the scalar operation to be grouped
%timeit s*((a*b+c*d)*e/f)
1.21 ms ± 29.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

%timeit (a*b+c*d)*e/f*s
1.14 ms ± 80.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</code></pre>
<p>Setup:</p>
<pre><code>s = pd.Series(np.arange(1_000_000))
a=b=c=d=e=f=2
x = (a*b+c*d)*e/f
</code></pre>
<p>In the initial <code>(a*b+c*d)*df['val1']*e/f</code>, the order or the operations is:</p>
<pre><code>a*b       # ab      #
c*d       # cd      # scalars
ab + cd   # abcd    #
s * abcd  # sabcd      #
e * sabcd # esabcd     # Series
esabcd / f             #
</code></pre>
","0","Answer"
"79440588","79439971","<p>Assuming such an example:</p>
<pre><code>some header

another header

012121 ***************************************************    3317        129862
012121 ***************************************************    3317        129862
012121 ***************************************************    3317        129862

a footer
</code></pre>
<p>You could build a wrapper with a loop and <a href=""https://docs.python.org/3/library/io.html#io.StringIO"" rel=""nofollow noreferrer""><code>io.StringIO</code></a> to pre-process the file and only pass the valid lines to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>read_csv</code></a>:</p>
<pre><code>from io import StringIO

def filter_rows(fname):
    def wrap(fname):
        with open(fname) as f:
            for line in f:
                if '******' in line:
                    yield line
                
    return StringIO(''.join(wrap(fname)))

df = pd.read_csv(filter_rows('file.txt'),
                 sep=r'\s+', names=['a', 'b', 'c', 'd'])
</code></pre>
<p>Output:</p>
<pre><code>       a                                                  b     c       d
0  12121  **********************************************...  3317  129862
1  12121  **********************************************...  3317  129862
2  12121  **********************************************...  3317  129862
</code></pre>
<p>And if you only want the first and third columns:</p>
<pre><code>df = pd.read_csv(filter_rows('file.txt'),
                 sep=r'\s+', usecols=[0, 2],
                 names=['a', 'c'])
</code></pre>
<p>Output:</p>
<pre><code>       a     c
0  12121  3317
1  12121  3317
2  12121  3317
</code></pre>
","1","Answer"
"79441701","79441449","<p>Ok it was easier than what I thought! Inspired by <a href=""https://www.geeksforgeeks.org/python-remove-array-item/"" rel=""nofollow noreferrer"">this example</a>
I changed the code to:</p>
<pre><code>scipy.stats.kruskal(*[group[&quot;variable&quot;].values for name, group in df.groupby(&quot;treatment&quot;) if group[&quot;variable&quot;].values.size &gt; 4])

</code></pre>
","0","Answer"
"79441968","79441884","<p>If your values are non-numeric (ie. 5 vs &quot;5&quot;), then <code>print(df.dtypes)</code> might be able to help. You might also need <code>df['TMDB Vote Average'] = pd.to_numeric(df['TMDB Vote Average'], errors='coerce')</code>.</p>
","0","Answer"
"79442152","79442012","<p>As already commented, the content is reloaded on demand, but it is precisely these requests that can be replicated in order to obtain the content as well.</p>
<p>To do this, you have to iterate over the rows of the table and make the request if necessary.</p>
<pre><code>import requests
import pandas as pd
from bs4 import BeautifulSoup

url = f'https://www.screener.in/company/TATAPOWER/consolidated/'
soup = BeautifulSoup(requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}).text)

keys = ['Item'] + list(soup.select_one('#quarters thead tr').stripped_strings)

data = []

for row in soup.select('#quarters tbody tr')[:-1]:
    if row.td.button:
        data.append(dict(zip(keys,[c.text for c in row.select('td')])))
        d = requests.get(f'https://www.screener.in/api/company/3371/schedules/?parent={row.td.button.text.strip(&quot; +&quot;)}&amp;section=quarters&amp;consolidated=', headers={'User-Agent': 'Mozilla/5.0'}).json()
        first_key = next(iter(d))
        data.append({&quot;Item&quot;: first_key, **d[first_key]})     
    else:
        data.append(dict(zip(keys,row.stripped_strings)))

pd.DataFrame(data)
</code></pre>
<p>Result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Item</th>
<th style=""text-align: left;"">Dec 2021</th>
<th style=""text-align: left;"">Mar 2022</th>
<th style=""text-align: left;"">Jun 2022</th>
<th style=""text-align: left;"">Sep 2022</th>
<th style=""text-align: left;"">Dec 2022</th>
<th style=""text-align: left;"">Mar 2023</th>
<th style=""text-align: left;"">Jun 2023</th>
<th style=""text-align: left;"">Sep 2023</th>
<th style=""text-align: left;"">Dec 2023</th>
<th style=""text-align: left;"">Mar 2024</th>
<th style=""text-align: left;"">Jun 2024</th>
<th style=""text-align: left;"">Sep 2024</th>
<th style=""text-align: left;"">Dec 2024</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Sales +</td>
<td style=""text-align: left;"">10,913</td>
<td style=""text-align: left;"">11,960</td>
<td style=""text-align: left;"">14,495</td>
<td style=""text-align: left;"">14,031</td>
<td style=""text-align: left;"">14,129</td>
<td style=""text-align: left;"">12,454</td>
<td style=""text-align: left;"">15,213</td>
<td style=""text-align: left;"">15,738</td>
<td style=""text-align: left;"">14,651</td>
<td style=""text-align: left;"">15,847</td>
<td style=""text-align: left;"">17,294</td>
<td style=""text-align: left;"">15,698</td>
<td style=""text-align: left;"">15,391</td>
</tr>
<tr>
<td style=""text-align: left;"">YOY Sales Growth %</td>
<td style=""text-align: left;"">43.63%</td>
<td style=""text-align: left;"">15.41%</td>
<td style=""text-align: left;"">43.06%</td>
<td style=""text-align: left;"">43.02%</td>
<td style=""text-align: left;"">29.47%</td>
<td style=""text-align: left;"">4.13%</td>
<td style=""text-align: left;"">4.95%</td>
<td style=""text-align: left;"">12.17%</td>
<td style=""text-align: left;"">3.69%</td>
<td style=""text-align: left;"">27.24%</td>
<td style=""text-align: left;"">13.67%</td>
<td style=""text-align: left;"">-0.26%</td>
<td style=""text-align: left;"">5.05%</td>
</tr>
<tr>
<td style=""text-align: left;"">Expenses +</td>
<td style=""text-align: left;"">9,279</td>
<td style=""text-align: left;"">10,091</td>
<td style=""text-align: left;"">12,812</td>
<td style=""text-align: left;"">12,270</td>
<td style=""text-align: left;"">11,810</td>
<td style=""text-align: left;"">10,526</td>
<td style=""text-align: left;"">12,500</td>
<td style=""text-align: left;"">12,967</td>
<td style=""text-align: left;"">12,234</td>
<td style=""text-align: left;"">13,540</td>
<td style=""text-align: left;"">14,232</td>
<td style=""text-align: left;"">12,427</td>
<td style=""text-align: left;"">12,312</td>
</tr>
<tr>
<td style=""text-align: left;"">Material Cost %</td>
<td style=""text-align: left;"">8.67%</td>
<td style=""text-align: left;"">13.38%</td>
<td style=""text-align: left;"">6.74%</td>
<td style=""text-align: left;"">4.04%</td>
<td style=""text-align: left;"">6.55%</td>
<td style=""text-align: left;"">12.13%</td>
<td style=""text-align: left;"">6.00%</td>
<td style=""text-align: left;"">6.09%</td>
<td style=""text-align: left;"">9.29%</td>
<td style=""text-align: left;"">13.86%</td>
<td style=""text-align: left;"">5.50%</td>
<td style=""text-align: left;"">3.59%</td>
<td style=""text-align: left;"">6.75%</td>
</tr>
<tr>
<td style=""text-align: left;"">Operating Profit</td>
<td style=""text-align: left;"">1,634</td>
<td style=""text-align: left;"">1,869</td>
<td style=""text-align: left;"">1,683</td>
<td style=""text-align: left;"">1,760</td>
<td style=""text-align: left;"">2,319</td>
<td style=""text-align: left;"">1,928</td>
<td style=""text-align: left;"">2,713</td>
<td style=""text-align: left;"">2,771</td>
<td style=""text-align: left;"">2,417</td>
<td style=""text-align: left;"">2,307</td>
<td style=""text-align: left;"">3,062</td>
<td style=""text-align: left;"">3,271</td>
<td style=""text-align: left;"">3,079</td>
</tr>
<tr>
<td style=""text-align: left;"">OPM %</td>
<td style=""text-align: left;"">15%</td>
<td style=""text-align: left;"">16%</td>
<td style=""text-align: left;"">12%</td>
<td style=""text-align: left;"">13%</td>
<td style=""text-align: left;"">16%</td>
<td style=""text-align: left;"">15%</td>
<td style=""text-align: left;"">18%</td>
<td style=""text-align: left;"">18%</td>
<td style=""text-align: left;"">16%</td>
<td style=""text-align: left;"">15%</td>
<td style=""text-align: left;"">18%</td>
<td style=""text-align: left;"">21%</td>
<td style=""text-align: left;"">20%</td>
</tr>
<tr>
<td style=""text-align: left;"">Other Income +</td>
<td style=""text-align: left;"">865</td>
<td style=""text-align: left;"">62</td>
<td style=""text-align: left;"">1,227</td>
<td style=""text-align: left;"">1,502</td>
<td style=""text-align: left;"">1,497</td>
<td style=""text-align: left;"">1,352</td>
<td style=""text-align: left;"">877</td>
<td style=""text-align: left;"">567</td>
<td style=""text-align: left;"">1,092</td>
<td style=""text-align: left;"">1,407</td>
<td style=""text-align: left;"">578</td>
<td style=""text-align: left;"">632</td>
<td style=""text-align: left;"">589</td>
</tr>
<tr>
<td style=""text-align: left;"">Exceptional items</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">-618</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">235</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">39</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">-140</td>
<td style=""text-align: left;"">0</td>
</tr>
<tr>
<td style=""text-align: left;"">Interest</td>
<td style=""text-align: left;"">953</td>
<td style=""text-align: left;"">1,015</td>
<td style=""text-align: left;"">1,026</td>
<td style=""text-align: left;"">1,052</td>
<td style=""text-align: left;"">1,098</td>
<td style=""text-align: left;"">1,196</td>
<td style=""text-align: left;"">1,221</td>
<td style=""text-align: left;"">1,182</td>
<td style=""text-align: left;"">1,094</td>
<td style=""text-align: left;"">1,136</td>
<td style=""text-align: left;"">1,176</td>
<td style=""text-align: left;"">1,143</td>
<td style=""text-align: left;"">1,170</td>
</tr>
<tr>
<td style=""text-align: left;"">Depreciation</td>
<td style=""text-align: left;"">758</td>
<td style=""text-align: left;"">846</td>
<td style=""text-align: left;"">822</td>
<td style=""text-align: left;"">838</td>
<td style=""text-align: left;"">853</td>
<td style=""text-align: left;"">926</td>
<td style=""text-align: left;"">893</td>
<td style=""text-align: left;"">926</td>
<td style=""text-align: left;"">926</td>
<td style=""text-align: left;"">1,041</td>
<td style=""text-align: left;"">973</td>
<td style=""text-align: left;"">987</td>
<td style=""text-align: left;"">1,041</td>
</tr>
<tr>
<td style=""text-align: left;"">Profit before tax</td>
<td style=""text-align: left;"">788</td>
<td style=""text-align: left;"">71</td>
<td style=""text-align: left;"">1,062</td>
<td style=""text-align: left;"">1,373</td>
<td style=""text-align: left;"">1,864</td>
<td style=""text-align: left;"">1,158</td>
<td style=""text-align: left;"">1,476</td>
<td style=""text-align: left;"">1,231</td>
<td style=""text-align: left;"">1,489</td>
<td style=""text-align: left;"">1,537</td>
<td style=""text-align: left;"">1,490</td>
<td style=""text-align: left;"">1,773</td>
<td style=""text-align: left;"">1,457</td>
</tr>
<tr>
<td style=""text-align: left;"">Tax %</td>
<td style=""text-align: left;"">30%</td>
<td style=""text-align: left;"">-794%</td>
<td style=""text-align: left;"">17%</td>
<td style=""text-align: left;"">32%</td>
<td style=""text-align: left;"">44%</td>
<td style=""text-align: left;"">19%</td>
<td style=""text-align: left;"">23%</td>
<td style=""text-align: left;"">17%</td>
<td style=""text-align: left;"">28%</td>
<td style=""text-align: left;"">32%</td>
<td style=""text-align: left;"">20%</td>
<td style=""text-align: left;"">38%</td>
<td style=""text-align: left;"">18%</td>
</tr>
<tr>
<td style=""text-align: left;"">Net Profit +</td>
<td style=""text-align: left;"">552</td>
<td style=""text-align: left;"">632</td>
<td style=""text-align: left;"">884</td>
<td style=""text-align: left;"">935</td>
<td style=""text-align: left;"">1,052</td>
<td style=""text-align: left;"">939</td>
<td style=""text-align: left;"">1,141</td>
<td style=""text-align: left;"">1,017</td>
<td style=""text-align: left;"">1,076</td>
<td style=""text-align: left;"">1,046</td>
<td style=""text-align: left;"">1,189</td>
<td style=""text-align: left;"">1,093</td>
<td style=""text-align: left;"">1,188</td>
</tr>
<tr>
<td style=""text-align: left;"">Profit after tax</td>
<td style=""text-align: left;"">552</td>
<td style=""text-align: left;"">632</td>
<td style=""text-align: left;"">884</td>
<td style=""text-align: left;"">935</td>
<td style=""text-align: left;"">1,052</td>
<td style=""text-align: left;"">939</td>
<td style=""text-align: left;"">1,141</td>
<td style=""text-align: left;"">1,017</td>
<td style=""text-align: left;"">1,076</td>
<td style=""text-align: left;"">1,046</td>
<td style=""text-align: left;"">1,189</td>
<td style=""text-align: left;"">1,093</td>
<td style=""text-align: left;"">1,188</td>
</tr>
<tr>
<td style=""text-align: left;"">EPS in Rs</td>
<td style=""text-align: left;"">1.33</td>
<td style=""text-align: left;"">1.57</td>
<td style=""text-align: left;"">2.49</td>
<td style=""text-align: left;"">2.56</td>
<td style=""text-align: left;"">2.96</td>
<td style=""text-align: left;"">2.43</td>
<td style=""text-align: left;"">3.04</td>
<td style=""text-align: left;"">2.74</td>
<td style=""text-align: left;"">2.98</td>
<td style=""text-align: left;"">2.80</td>
<td style=""text-align: left;"">3.04</td>
<td style=""text-align: left;"">2.90</td>
<td style=""text-align: left;"">3.23</td>
</tr>
</tbody>
</table></div>
","3","Answer"
"79442869","79442105","<p>I'm not sure what was wrong with your original code, but it seems you were quite close. Here's a modified version which works:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = (
    pd.DataFrame(
        {
            &quot;Driver_ID&quot;: [1, 1, 1, 1, 2, 2, 2, 2, 2],
            &quot;Date&quot;: [
                &quot;2025-02-13&quot;,
                &quot;2024-12-31&quot;,
                &quot;2024-11-03&quot;,
                &quot;2023-01-01&quot;,
                &quot;2025-01-13&quot;,
                &quot;2024-12-02&quot;,
                &quot;2024-11-12&quot;,
                &quot;2023-11-12&quot;,
                &quot;2023-05-12&quot;,
            ],
            &quot;Place&quot;: [1, 1, 2, 1, 5, 1, 2, 1, 1],
        }
    )
    .sort_values(by=[&quot;Driver_ID&quot;, &quot;Date&quot;])
    .reset_index(drop=True)
)

df[&quot;Win?&quot;] = df[&quot;Place&quot;] == 1

df[&quot;Total_wins&quot;] = df.groupby(&quot;Driver_ID&quot;, group_keys=False)[&quot;Win?&quot;].apply(
    lambda g: g.shift(1, fill_value=0).cumsum()
)

df.drop(columns=&quot;Win?&quot;, inplace=True)

print(df)
</code></pre>
<p>This produces the following:</p>
<pre class=""lang-none prettyprint-override""><code>   Driver_ID        Date  Place Total_wins
0          1  2023-01-01      1          0
1          1  2024-11-03      2          1
2          1  2024-12-31      1          1
3          1  2025-02-13      1          2
4          2  2023-05-12      1          0
5          2  2023-11-12      1          1
6          2  2024-11-12      2          2
7          2  2024-12-02      1          2
8          2  2025-01-13      5          3
</code></pre>
<p>If you want to keep the original order, you can remove the <code>reset_index(drop=True)</code> part and then re-sort after the computation is done.</p>
","1","Answer"
"79444962","79443476","<p>Here's one approach:</p>
<p><strong><a href=""https://stackoverflow.com/help/minimal-reproducible-example"">Minimal reproducible example</a></strong></p>
<pre class=""lang-py prettyprint-override""><code>np.random.seed(42) # for reproducibility
date_rng = pd.date_range(start='2024-01-01', periods=5, freq='h')

# ... as above

df['cumsum'] = df['volume'].cumsum()
</code></pre>
<p><strong>Data</strong></p>
<p>Using <code>cv = 10</code> (constantvolume), we want to use index <code>3</code> for both the <code>20</code> and <code>27</code> groups:</p>
<pre class=""lang-py prettyprint-override""><code>            timestamp  price  volume  cumsum
0 2024-01-01 00:00:00  81.24       3       3    # 10
1 2024-01-01 01:00:00  98.52       7      10    # 10
2 2024-01-01 02:00:00  91.96       8      18    # 20
3 2024-01-01 03:00:00  87.96       5      23    # split: 20 | 27
4 2024-01-01 04:00:00  74.68       4      27    # 27
</code></pre>
<p><strong>Code</strong></p>
<pre class=""lang-py prettyprint-override""><code>cv = 10

cv_rng = range(cv, df['cumsum'].max(), cv)

s_rng = pd.Series(list(set(cv_rng).difference(df['cumsum'])), 
              dtype=df['cumsum'].dtype, 
              name='cumsum')

df = (pd.concat([df, s_rng])
      .sort_values('cumsum')
      .bfill()
      .assign(
          group=lambda x: np.minimum(np.ceil(x['cumsum'] / cv) * cv, 
                                 x['cumsum'].max())
          )
      )

out = (df.groupby('group').agg(
    **{k:('price', v) for k, v in zip(['low', 'high', 'close', 'open'], 
                                      ['min', 'max', 'last', 'first'])},
    volume=('group', 'max'),
    timestamp=('timestamp', 'first')
    )
    .assign(volume=lambda x: np.where(x['volume'] % cv == 0, 
                                      cv, 
                                      x['volume'] % cv)
            )
    .set_index('timestamp')
    )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>                       low   high  close   open  volume
timestamp                                              
2024-01-01 00:00:00  81.24  98.52  98.52  81.24    10.0
2024-01-01 02:00:00  87.96  91.96  87.96  91.96    10.0
2024-01-01 03:00:00  74.68  87.96  74.68  87.96     7.0 # ts/high/open = index `3`
</code></pre>
<p><strong>Explanation / intermediates</strong></p>
<ul>
<li>First, create a <a href=""https://docs.python.org/3/library/functions.html#func-range"" rel=""nofollow noreferrer"">range</a> with <code>start=cv</code>, <code>stop=df['cumsum'].max()</code> and <code>step=cv</code>.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>list(cv_rng)
[10, 20]
</code></pre>
<ul>
<li>We want to add rows to <code>df</code> for values from <code>cv_rng</code> if they do not already exist in <code>df['cumsum']</code>: use <a href=""https://docs.python.org/3/library/functions.html#func-set"" rel=""nofollow noreferrer"">set</a> + <a href=""https://docs.python.org/3/library/stdtypes.html#frozenset.difference"" rel=""nofollow noreferrer"">set.difference</a> + <code>df['cumsum']</code> and create a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.html"" rel=""nofollow noreferrer""><code>pd.Series</code></a> (<code>s_rng</code>).</li>
<li>Now, use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>pd.concat</code></a> to add <code>s_rng</code> to <code>df</code> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html"" rel=""nofollow noreferrer""><code>df.sort_values</code></a> to order 'cumsum' values + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html"" rel=""nofollow noreferrer""><code>df.bfill</code></a> to add the rows from <code>s_rng</code> correctly sorted and taking the values of the <em>next</em> row.</li>
<li>Also add 'group' column with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>df.assign</code></a> based on <a href=""https://numpy.org/doc/2.2/reference/generated/numpy.ceil.html"" rel=""nofollow noreferrer""><code>np.ceil</code></a> or the max from 'cumsum' for the last group (via <a href=""https://numpy.org/doc/2.2/reference/generated/numpy.minimum.html"" rel=""nofollow noreferrer""><code>np.minimum</code></a>).</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>(pd.concat([df, s_rng], keys=['df', 's_rng'])
 .sort_values('cumsum')
 .bfill()
 .assign(...))
# including `keys` for demonstration purposes only; not needed

                  timestamp  price  volume  cumsum  group
df    0 2024-01-01 00:00:00  81.24     3.0       3   10.0
      1 2024-01-01 01:00:00  98.52     7.0      10   10.0
      2 2024-01-01 02:00:00  91.96     8.0      18   20.0
s_rng 0 2024-01-01 03:00:00  87.96     5.0      20   20.0 # vals from next row
df    3 2024-01-01 03:00:00  87.96     5.0      23   27.0 # note 27 last group
      4 2024-01-01 04:00:00  74.68     4.0      27   27.0
</code></pre>
<ul>
<li>Finally, we use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>df.groupby</code></a> with 'group' and use <a href=""https://pandas.pydata.org/docs/user_guide/groupby.html#named-aggregation"" rel=""nofollow noreferrer"">named aggregation</a>.</li>
<li>For 'volume', we chain <code>df.assign</code> to overwrite <code>max</code> for each group with <code>cv</code> if remainder equals 0, else remainder via <a href=""https://numpy.org/doc/2.2/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>np.where</code></a>.</li>
<li>Lastly, use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>df.set_index</code></a> to get 'timestamp' as the index.</li>
</ul>
","2","Answer"
"79445182","79445156","<p>You could compute a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.groupby.html"" rel=""nofollow noreferrer""><code>GroupBy</code></a> object, then apply your test on all <a href=""https://docs.python.org/3/library/itertools.html#itertools.combinations"" rel=""nofollow noreferrer""><code>combinations</code></a>:</p>
<pre><code>from itertools import combinations
from scipy.stats import mannwhitneyu

groups = df.groupby('age')['value']
out = pd.DataFrame.from_dict({(a[0], b[0]): mannwhitneyu(a[1], b[1])
                              for a, b in combinations(groups, 2)},
                            orient='index')
</code></pre>
<p>Example:</p>
<pre><code>          statistic    pvalue
(0, 1)         17.0  0.939860
(0, 2)         14.0  1.000000
(0, 3)         61.0  0.205667
(0, 4)         28.0  0.757692
(0, 5)         20.0  0.797203
...             ...       ...
(16, 18)        8.0  1.000000
(16, 19)       13.0  0.380952
(17, 18)       17.0  0.420635
(17, 19)       21.0  0.329004
(18, 19)       18.0  0.662338

[190 rows x 2 columns]
</code></pre>
<p>Used input:</p>
<pre><code>np.random.seed(0)
df = pd.DataFrame({'age': np.random.randint(0, 20, 100),
                   'value': np.random.random(100)
                  })
</code></pre>
<p>If you want a square matrix of pvalues as output, using <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.squareform.html"" rel=""nofollow noreferrer""><code>squareform</code></a>:</p>
<pre><code>from scipy.spatial.distance import squareform

idx = sorted(df['age'].unique())
out = pd.DataFrame(squareform([mannwhitneyu(a[1], b[1]).pvalue
                               for a, b in combinations(groups, 2)]),
                   index=idx, columns=idx).sort_index().sort_index(axis=1)
</code></pre>
<p>Output:</p>
<pre><code>          0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15        16        17        18        19
0   0.000000  0.939860  1.000000  0.205667  0.757692  0.797203  0.297702  0.330070  0.863636  0.035964  0.260140  0.727273  0.148252  1.000000  0.898102  0.114161  1.000000  0.898102  0.699301  0.528671
1   0.939860  0.000000  0.857143  0.187812  0.787879  0.730159  0.285714  0.485714  0.857143  0.066667  0.485714  1.000000  0.200000  1.000000  0.904762  0.163636  1.000000  0.555556  1.000000  0.609524
2   1.000000  0.857143  0.000000  0.286713  0.666667  0.571429  0.250000  1.000000  1.000000  0.095238  0.400000  1.000000  0.228571  0.800000  1.000000  0.266667  1.000000  0.392857  1.000000  0.714286
3   0.205667  0.187812  0.286713  0.000000  0.193233  0.055278  0.953047  0.733267  0.468531  0.313187  0.839161  0.216783  0.023976  0.363636  0.439560  0.417318  0.216783  0.055278  0.206460  0.792458
4   0.757692  0.787879  0.666667  0.193233  0.000000  0.876263  0.267677  0.315152  1.000000  0.073427  0.230303  0.833333  0.315152  0.888889  0.530303  0.164918  1.000000  1.000000  0.431818  0.533800
5   0.797203  0.730159  0.571429  0.055278  0.876263  0.000000  0.150794  0.190476  1.000000  0.017316  0.063492  0.785714  0.555556  0.857143  0.690476  0.106061  1.000000  1.000000  0.309524  0.246753
6   0.297702  0.285714  0.250000  0.953047  0.267677  0.150794  0.000000  0.555556  0.571429  0.428571  1.000000  0.250000  0.063492  0.380952  0.309524  0.755051  0.392857  0.095238  0.222222  0.930736
7   0.330070  0.485714  1.000000  0.733267  0.315152  0.190476  0.555556  0.000000  0.400000  0.114286  0.685714  0.857143  0.028571  0.800000  0.412698  0.527273  0.400000  0.111111  0.555556  0.914286
8   0.863636  0.857143  1.000000  0.468531  1.000000  1.000000  0.571429  0.400000  0.000000  0.166667  0.628571  1.000000  0.400000  1.000000  0.571429  0.266667  1.000000  1.000000  1.000000  0.904762
9   0.035964  0.066667  0.095238  0.313187  0.073427  0.017316  0.428571  0.114286  0.166667  0.000000  0.609524  0.047619  0.009524  0.285714  0.051948  0.730769  0.166667  0.004329  0.051948  0.240260
10  0.260140  0.485714  0.400000  0.839161  0.230303  0.063492  1.000000  0.685714  0.628571  0.609524  0.000000  0.228571  0.057143  0.533333  0.412698  0.927273  0.400000  0.111111  0.285714  0.761905
11  0.727273  1.000000  1.000000  0.216783  0.833333  0.785714  0.250000  0.857143  1.000000  0.047619  0.228571  0.000000  0.228571  1.000000  0.785714  0.266667  1.000000  0.571429  1.000000  0.714286
12  0.148252  0.200000  0.228571  0.023976  0.315152  0.555556  0.063492  0.028571  0.400000  0.009524  0.057143  0.228571  0.000000  0.533333  0.063492  0.024242  0.628571  0.285714  0.063492  0.171429
13  1.000000  1.000000  0.800000  0.363636  0.888889  0.857143  0.380952  0.800000  1.000000  0.285714  0.533333  1.000000  0.533333  0.000000  1.000000  0.333333  0.800000  0.857143  1.000000  0.642857
14  0.898102  0.904762  1.000000  0.439560  0.530303  0.690476  0.309524  0.412698  0.571429  0.051948  0.412698  0.785714  0.063492  1.000000  0.000000  0.343434  0.785714  0.841270  0.841270  0.792208
15  0.114161  0.163636  0.266667  0.417318  0.164918  0.106061  0.755051  0.527273  0.266667  0.730769  0.927273  0.266667  0.024242  0.333333  0.343434  0.000000  0.266667  0.073232  0.202020  0.365967
16  1.000000  1.000000  1.000000  0.216783  1.000000  1.000000  0.392857  0.400000  1.000000  0.166667  0.400000  1.000000  0.628571  0.800000  0.785714  0.266667  0.000000  1.000000  1.000000  0.380952
17  0.898102  0.555556  0.392857  0.055278  1.000000  1.000000  0.095238  0.111111  1.000000  0.004329  0.111111  0.571429  0.285714  0.857143  0.841270  0.073232  1.000000  0.000000  0.420635  0.329004
18  0.699301  1.000000  1.000000  0.206460  0.431818  0.309524  0.222222  0.555556  1.000000  0.051948  0.285714  1.000000  0.063492  1.000000  0.841270  0.202020  1.000000  0.420635  0.000000  0.662338
19  0.528671  0.609524  0.714286  0.792458  0.533800  0.246753  0.930736  0.914286  0.904762  0.240260  0.761905  0.714286  0.171429  0.642857  0.792208  0.365967  0.380952  0.329004  0.662338  0.000000
</code></pre>
","5","Answer"
"79445184","79445067","<p><strong>Solution:</strong></p>
<p>You can iterate over the whole dataframe and replace the date with the date 1 row above and add 1 mircosecond if the date is smaller than the date before:</p>
<pre><code>for e in range(len(df)):
    if e != 0 and df.iloc[e,0] &lt;= df.iloc[e-1, 0]:
        df.iloc[e,0] = df.iloc[e-1, 0] + pd.to_timedelta(1,unit='us')
</code></pre>
<p><strong>Explanation:</strong></p>
<ul>
<li><code>for e in range(len(df)):</code> Iterates over the dataframe by the indices of the rows</li>
<li><code>if e != 0 and df.iloc[e,0] &lt;= df.iloc[e-1, 0]:</code> Checks whether a date is in the first row of the dataframe or is smaller than the date 1 row above</li>
<li><code>df.iloc[e,0] = df.iloc[e-1, 0] + pd.to_timedelta(1,unit='us')</code> Replaces the date with the date 1 row above and adds 1 mircosecond to ensure it is higher</li>
</ul>
<p>I used <code>dataframe.iloc[]</code> to access the values in dataframe by the x and y coordinate.</p>
","0","Answer"
"79445247","79439150","<p>So, after some more research I didn't find anything that can achieve that. Instead, I resorted to read json file as text and concatenate rows of my df to produce json string. The code to achieve that is as follows:</p>
<pre><code>def create_json_str(collection):
    output_str = &quot;&quot;
    for row in collection:
        output_str = output_str + row[0]
    return output_str.replace(&quot; &quot;, &quot;&quot;)

aws_s3_bucket = 'my_bucket'
mount_name = '/mnt/test'

source_url = 's3a://%s' %(aws_s3_bucket)
dbutils.fs.mount(source_url,mount_name)

file_path = &quot;/dummyKey/dummyFile.json&quot;
df = spark.read.option(&quot;multiline&quot;,&quot;true&quot;).text(mount_name + file_path).cache()

df_collection = df.collect()
json_str = create_json_str(df_collection)
</code></pre>
<p>Not the best solution, I think using schemas &amp; reading it as json would be better, but for my purposes it is enough.</p>
","0","Answer"
"79445252","79445067","<p>You could perform a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.SeriesGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a> on the index to add 1 whenever the successive values are identical and propagate the change with a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>cumsum</code></a>:</p>
<pre><code># ensure the index is datetime
df.index = pd.to_datetime(df.index)

# groupby &quot;id&quot;, perform a transform on the index
# add 1µs whenever the dates are identical
# propagate the change with cumsum
df.index += pd.to_timedelta(df.groupby('id')['id']
                              .transform(lambda x: (x.index.diff() == '0').cumsum()),
                            unit='us')
</code></pre>
<p>Output:</p>
<pre><code>                                  1  2      3        4        5          6  7   8  9  id  11
2025-02-14 00:00:01.192429  6143.25  2  32178  6143.25  6143.50  271611387  C  43  1   2  14
2025-02-14 00:00:01.317655  6143.25  1  32179  6143.25  6143.50  271611388  C  43  1   2  14
2025-02-14 00:00:01.317656  6143.25  1  32180  6143.25  6143.50  271611388  C  43  1   2  14
2025-02-14 00:00:01.317657  6143.25  1  32181  6143.25  6143.50  271611388  C  43  1   2  14
2025-02-14 00:00:20.222990  6143.50  1  32182  6143.25  6143.50  271611389  C  43  1   1  14
2025-02-14 00:00:20.222991  6143.50  1  32183  6143.25  6143.50  271611389  C  43  1   1  14
2025-02-14 00:00:20.222992  6143.50  1  32184  6143.25  6143.50  271611389  C  43  1   1  14
2025-02-14 00:00:20.222993  6143.50  1  32185  6143.25  6143.50  271611389  C  43  1   1  14
2025-02-14 00:00:20.222994  6143.50  1  32186  6143.25  6143.50  271611390  C  43  1   1  14
2025-02-14 00:00:20.222995  6143.50  1  32187  6143.25  6143.50  271611390  C  43  1   1  14
2025-02-14 00:00:20.222996  6143.50  1  32188  6143.25  6143.50  271611390  C  43  1   1  14
2025-02-14 00:00:23.891465  6143.50  1  32189  6143.50  6143.75  271611391  C  43  1   2  14
2025-02-14 00:00:23.891466  6143.50  1  32190  6143.50  6143.75  271611391  C  43  1   2  14
2025-02-14 00:00:23.891467  6143.50  1  32191  6143.50  6143.75  271611391  C  43  1   2  14
2025-02-14 00:01:00.046738  6143.50  1  32192  6143.25  6143.50  271611392  C  43  1   1  14
</code></pre>
","0","Answer"
"79445340","79445156","<p>Not as elegant as <a href=""https://stackoverflow.com/a/79445182/12158757"">@mozway's <code>combinations</code> solution</a>, but a possible option with nested <code>for</code> loops</p>
<pre><code>from scipy.stats import mannwhitneyu

idx = []
res = []
d = list(all_groups.items())
for i in range(len(d)-1):
  for j in range(i, len(d)):
    idx.append((d[i][0], d[j][0]))
    res.append(mannwhitneyu(d[i][1], d[j][1]))
pd.DataFrame(res, index=idx)
</code></pre>
<p>and you will see</p>
<pre><code>             statistic    pvalue
(1.0, 1.0)         8.0  1.000000
(1.0, 2.0)         6.0  1.000000
(1.0, 3.0)         4.0  0.114286
(1.0, 6.0)         1.0  0.800000
(1.0, 7.0)         1.0  0.114286
(1.0, 11.0)       44.0  0.001465
(2.0, 2.0)         4.5  1.000000
(2.0, 3.0)         6.0  0.547619
(2.0, 6.0)         1.0  1.000000
(2.0, 7.0)         3.0  0.700000
(2.0, 11.0)       33.0  0.005495
(3.0, 3.0)        18.0  1.000000
(3.0, 6.0)         2.0  0.857143
(3.0, 7.0)         4.0  0.261905
(3.0, 11.0)       66.0  0.000162
(6.0, 6.0)         0.5  1.000000
(6.0, 7.0)         1.0  1.000000
(6.0, 11.0)       11.0  0.166667
(7.0, 7.0)         4.5  1.000000
(7.0, 11.0)       33.0  0.005495
</code></pre>
","1","Answer"
"79445864","79445857","<p>In your loop, you should split the strings into a list of substrings for the fields:</p>
<pre><code>for line in input_txt:
    data_list.append(line.strip().split())
</code></pre>
<p>This will give you the correct number of columns.</p>
<p>Alternatively, keep your loop as it is, but create a <code>Series</code> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>str.split</code></a> with <code>expand=True</code>. This might be less efficient, but could be more robust if you don't have a consistent number of fields:</p>
<pre><code>data_list = []
with open('input.txt', 'r') as data:
    for line in data:
        data_list.append(line.strip())
df = pd.Series(data_list).str.split(expand=True)
</code></pre>
<p>Output:</p>
<pre><code>        0      1     2      3
0  040525  $$$$$  9999  12345
1  040525  $$$$$  8888  12345
2  040525  $$$$$  7777  12345
3  040525  $$$$$  6666  12345
</code></pre>
<p>For the first approach, if you want column names:</p>
<pre><code>df = pd.DataFrame(data_list, columns=['a', 'b', 'c', 'd'])
</code></pre>
<p>Output:</p>
<pre><code>        a      b     c      d
0  040525  $$$$$  9999  12345
1  040525  $$$$$  8888  12345
2  040525  $$$$$  7777  12345
3  040525  $$$$$  6666  12345
</code></pre>
","1","Answer"
"79445895","79444668","<p>Interactive renderers display figures using Plotly.js, so you can actually use Javascript+Plotly.js to hook into the <a href=""https://plotly.com/javascript/plotlyjs-events/#legend-click-events"" rel=""nofollow noreferrer""><code>plotly_legendclick</code></a> event and override the default behavior (see also <a href=""https://plotly.com/javascript/plotlyjs-function-reference"" rel=""nofollow noreferrer"">Function Reference in JavaScript</a>) :</p>
<pre class=""lang-js prettyprint-override""><code>const gd = document.getElementById('{plot_id}');

gd.on('plotly_legendclick', function(eventData) {
    const i = eventData.curveNumber;
    const traces = eventData.fullData;

    if (traces[i].name === 'Total') {
        // Apply default behavior
        return true;
    }

    // Show/Hide clicked trace
    traces[i].visible = traces[i].visible === true ? 'legendonly' : true;
    Plotly.restyle(gd, { visible: traces[i].visible }, [i]);

    // Update total based on visible line traces
    const visible = traces.filter(t =&gt; t.name != 'Total' &amp;&amp; t.visible === true);
    const totalY = traces[0].y.map((_, xi)=&gt; visible.reduce((sum, t) =&gt; sum + t.y[xi], 0));
    Plotly.restyle(gd, { y: [totalY] }, [0]);

    // Prevent default behavior
    return false;
});
</code></pre>
<p>To do this in python, you just need to pass the js code as a string to plotly's <code>Figure.show()</code> method via the <a href=""https://plotly.github.io/plotly.py-docs/generated/plotly.html#plotly.basedatatypes.BaseFigure.to_html"" rel=""nofollow noreferrer""><code>post_script</code></a> parameter (the js snippet(s) are executed just after plot creation) :</p>
<pre class=""lang-py prettyprint-override""><code>js = '''
const gd = document.getElementById('{plot_id}');
gd.on('plotly_legendclick', function(eventData) {...});
'''

fig.show(post_script=[js])
</code></pre>
","1","Answer"
"79446489","79446437","<p>One way to to it</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {'NVDA': [{'open': 144.75, 'high': 144.21, 'low': 174.33, 'close': 210.47},
  {'open': 123.97, 'high': 128.5, 'low': 110.25, 'close': 154.09},
  {'open': 118.19, 'high': 134.81, 'low': 104.37, 'close': 149.72},
  {'open': 225.35, 'high': 126.81, 'low': 104.77, 'close': 209.46},
  {'open': 247.2, 'high': 243.25, 'low': 220.44, 'close': 186.01}],
 'MSFT': [{'open': 175.78, 'high': 213.98, 'low': 229.75, 'close': 206.59},
  {'open': 142.98, 'high': 168.42, 'low': 188.33, 'close': 232.52},
  {'open': 184.14, 'high': 163.42, 'low': 194.81, 'close': 153.03},
  {'open': 199.54, 'high': 130.26, 'low': 101.05, 'close': 102.1},
  {'open': 243.91, 'high': 119.21, 'low': 190.2, 'close': 223.31}],
 'AAPL': [{'open': 202.06, 'high': 162.54, 'low': 212.3, 'close': 226.78},
  {'open': 191.17, 'high': 153.49, 'low': 135.13, 'close': 151.83},
  {'open': 187.15, 'high': 149.75, 'low': 123.28, 'close': 247.32},
  {'open': 194.29, 'high': 175.34, 'low': 244.14, 'close': 207.45},
  {'open': 228.9, 'high': 133.26, 'low': 100.59, 'close': 129.35}]}

dfs = []
for k,v in data.items():
    for d in v:
        d['tick'] = k
        dfs.append(d)

df = pd.DataFrame(dfs, columns=['tick', 'open', 'high', 'low', 'close'])

print(df)
</code></pre>
<p>Result</p>
<pre class=""lang-none prettyprint-override""><code>    tick    open    high     low   close
0   NVDA  144.75  144.21  174.33  210.47
1   NVDA  123.97  128.50  110.25  154.09
2   NVDA  118.19  134.81  104.37  149.72
3   NVDA  225.35  126.81  104.77  209.46
4   NVDA  247.20  243.25  220.44  186.01
5   MSFT  175.78  213.98  229.75  206.59
6   MSFT  142.98  168.42  188.33  232.52
7   MSFT  184.14  163.42  194.81  153.03
8   MSFT  199.54  130.26  101.05  102.10
9   MSFT  243.91  119.21  190.20  223.31
10  AAPL  202.06  162.54  212.30  226.78
11  AAPL  191.17  153.49  135.13  151.83
12  AAPL  187.15  149.75  123.28  247.32
13  AAPL  194.29  175.34  244.14  207.45
14  AAPL  228.90  133.26  100.59  129.35
</code></pre>
","2","Answer"
"79446490","79446437","<p>This is a simple transformation to do if you want to get it into a format the <code>pd.DataFrame</code> constructor understands (a list of dicts):</p>
<pre><code>df = pd.DataFrame(
    [
        {&quot;ticker&quot;:k, **v}
        for k, vs in data.items()
        for v in vs
    ]
)
</code></pre>
<p>This will require auxiliary memory though.</p>
","4","Answer"
"79446516","79446466","<pre><code>import pandas as pd

# Original DataFrame
df = pd.DataFrame({
    'Column_1': ['aadff2', 'aadff2', 'aasd12', 'asdd2', '967323sd', '967323sd', 'aa1113']
})

# Dictionary with values to update
update_values = {&quot;Groub_A&quot;: {&quot;aadff2&quot;: &quot;Mark&quot;, &quot;aasd12&quot;: &quot;Otto&quot;, &quot;asdd2&quot;: &quot;Jhon&quot;},
                 &quot;Groub_B&quot;: {&quot;aadfaa&quot;: &quot;Josh&quot;, &quot;aa1113&quot;: &quot;Math&quot;, &quot;967323sd&quot;: &quot;Marek&quot;}}

# Create an empty list to store new rows
new_rows = []

# Iterate over the dictionary and create a new DataFrame
for group, values in update_values.items():
    for key, value in values.items():
        new_rows.append([key, value, group])

# Create a DataFrame from the new rows
df_update = pd.DataFrame(new_rows, columns=['Column_1', 'Column_new_2', 'Column_new_3'])

# Merge the original DataFrame with the new DataFrame
df = df.merge(df_update, on='Column_1', how='left')

print(df)
</code></pre>
","0","Answer"
"79446584","79439971","<pre><code>data_list = []
with open('input.txt', 'r') as data:
    for line in data:
         #create split_row to check
         split_row = line.strip().split()
         #check if the second substring in split_row starts with &quot;*****&quot;
         if split_row[1].startswith(&quot;*****&quot;): 
             data_list.append(split_row) 
df = pd.DataFrame(data_list)
</code></pre>
","1","Answer"
"79446630","79445805","<p>I can't test since I don't have the Excel file but the following code should works:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
df = (
    pd.read_excel('CSTRS Chem Eng Practicals_2.xlsx', sheet_name='Flow Rates')
    .dropna()
)
</code></pre>
","0","Answer"
"79446660","79446466","<p>Another possible solution, which uses <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> to create the two new columns:</p>
<pre><code>df.assign(
    **dict(zip(
        ['Column_2', 'Column_3'], 
        zip(*df['Column_1'].map(
            lambda x: [update_values[&quot;Groub_A&quot;].get(x), 'Group_A'] 
            if x in update_values[&quot;Groub_A&quot;] 
            else [update_values[&quot;Groub_B&quot;].get(x), 'Group_B']
        ))
    ))
)
</code></pre>
<p>Output:</p>
<pre><code>   Column_1 Column_2 Column_3
0    aadff2     Mark  Group_A
1    aadff2     Mark  Group_A
2    aasd12     Otto  Group_A
3     asdd2     Jhon  Group_A
4  967323sd    Marek  Group_B
5  967323sd    Marek  Group_B
6    aa1113     Math  Group_B
</code></pre>
","0","Answer"
"79446662","79446437","<p>Here is one option with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html#"" rel=""nofollow noreferrer""><code>pd.concat</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>pd.join</code></a></p>
<pre><code>pd.concat(
    map(lambda x: pd.DataFrame({'tick':[x[0]]*len(x[1])})
        .join(pd.DataFrame(x[1])),
        data.items())).reset_index(drop = True)
</code></pre>
<p>or as suggested by @juanpa.arrivillaga in the comment to avoid <code>np.join</code></p>
<pre><code>(df := pd.concat(
    [pd.DataFrame(vs).assign(tick=k) for k, vs in data.items()], ignore_index=True
))[np.roll(df.columns,1)]
</code></pre>
<p>which gives</p>
<pre><code>    tick    open    high     low   close
0   NVDA  144.75  144.21  174.33  210.47
1   NVDA  123.97  128.50  110.25  154.09
2   NVDA  118.19  134.81  104.37  149.72
3   NVDA  225.35  126.81  104.77  209.46
4   NVDA  247.20  243.25  220.44  186.01
5   MSFT  175.78  213.98  229.75  206.59
6   MSFT  142.98  168.42  188.33  232.52
7   MSFT  184.14  163.42  194.81  153.03
8   MSFT  199.54  130.26  101.05  102.10
9   MSFT  243.91  119.21  190.20  223.31
10  AAPL  202.06  162.54  212.30  226.78
11  AAPL  191.17  153.49  135.13  151.83
12  AAPL  187.15  149.75  123.28  247.32
13  AAPL  194.29  175.34  244.14  207.45
14  AAPL  228.90  133.26  100.59  129.35
</code></pre>
","3","Answer"
"79446753","79446667","<p>Define a formatting function with a single arg and apply to relevant columns</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

mw=7
def left_align(x):
        return f&quot;{x: &lt;{mw}}&quot;


data = [[2025, 198237, 77, 18175],
        [202, 292827, 77, 292827]]


df = pd.DataFrame(data).iloc[:,[0,3]]
# get length of max value
mw = len(str(df.max(numeric_only=True).max()))

#print(mw)
print(df.to_string(index=False, header=False, formatters=[left_align, left_align]))
</code></pre>
<p>Result</p>
<pre class=""lang-none prettyprint-override""><code>2025   18175 
202    292827
</code></pre>
","1","Answer"
"79446893","79446466","<p>Consider a nested list/dict comprehension to build a list of dictionaries to pass into <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_records.html"" rel=""nofollow noreferrer""><strong><code>pandas.DataFrame.from_records</code></strong></a>. Then, left <code>merge</code> against current data frame for New columns.</p>
<pre class=""lang-py prettyprint-override""><code>new_data = [
    {&quot;Column_1&quot;: k, &quot;Column_new_2&quot;: v, &quot;Column_new_3&quot;: gk}
    for gk, gv in update_values.items()
    for k, v in gv.items()
]

current_df = current_df.merge(
    pd.DataFrame.from_records(new_data), on = &quot;Column_1&quot;, how = &quot;left&quot;
)
    
</code></pre>
","3","Answer"
"79446989","79446437","<p>You can read this with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html"" rel=""nofollow noreferrer""><code>json_normalize</code></a> if you make the input a list of records:</p>
<pre><code>df = pd.json_normalize([{'tick': k, 'data': v} for k, v in data.items()],
                       'data', meta='tick')
</code></pre>
<p>This should be relatively lightweight since the lists will be shared in memory with the original ones of <code>data</code>.</p>
<p>Output:</p>
<pre><code>      open    high     low   close  tick
0   202.53  159.85  192.78  159.08  NVDA
1   161.14  165.17  189.66  155.31  NVDA
2   216.04  194.22  127.27  114.98  NVDA
3   204.64  137.89  103.44  111.93  NVDA
4   245.47  131.42  138.11  177.44  NVDA
5   197.37  140.20  190.76  180.82  MSFT
6   213.40  237.43  118.40  238.46  MSFT
7   127.91  192.21  186.09  221.07  MSFT
8   216.28  249.58  162.59  111.86  MSFT
9   100.44  149.07  223.15  185.34  MSFT
10  138.62  215.26  107.22  110.75  AAPL
11  188.77  104.89  193.78  183.34  AAPL
12  151.65  128.45  239.33  249.28  AAPL
13  151.82  142.17  241.76  134.61  AAPL
14  239.02  180.75  158.85  184.81  AAPL
</code></pre>
<p>Another option, <a href=""https://docs.python.org/3/library/itertools.html#itertools.chain"" rel=""nofollow noreferrer""><code>chain</code></a> and read the data, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.insert.html"" rel=""nofollow noreferrer""><code>insert</code></a> the tickers afterwards:</p>
<pre><code>from itertools import chain
import numpy as np

df = pd.DataFrame(chain.from_iterable(data.values()))
df.insert(0, 'tick', np.repeat(list(data), [len(l) for l in  data.values()]))
</code></pre>
<p>Output:</p>
<pre><code>    tick    open    high     low   close
0   NVDA  202.53  159.85  192.78  159.08
1   NVDA  161.14  165.17  189.66  155.31
2   NVDA  216.04  194.22  127.27  114.98
3   NVDA  204.64  137.89  103.44  111.93
4   NVDA  245.47  131.42  138.11  177.44
5   MSFT  197.37  140.20  190.76  180.82
6   MSFT  213.40  237.43  118.40  238.46
7   MSFT  127.91  192.21  186.09  221.07
8   MSFT  216.28  249.58  162.59  111.86
9   MSFT  100.44  149.07  223.15  185.34
10  AAPL  138.62  215.26  107.22  110.75
11  AAPL  188.77  104.89  193.78  183.34
12  AAPL  151.65  128.45  239.33  249.28
13  AAPL  151.82  142.17  241.76  134.61
14  AAPL  239.02  180.75  158.85  184.81
</code></pre>
","3","Answer"
"79447843","79447714","<p>First convert column <code>Date</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>to_datetime</code></a>, create helper <code>DataFrame</code> with rename columns <code>df_cand</code> so possible use left join to original (for avoid remove original index is used <code>rename</code>). Then filter
by datetimes, sorting and create counter by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>GroupBy.cumcount</code></a> for get <code>3</code> last values, which are merged to original <code>df</code>:</p>
<pre><code>df['Date'] = pd.to_datetime(df['Date'])

df = df.reset_index().rename(columns={'index':'orig_index'})

df_cand = (df.rename(columns={'Date':'cand_Date',
                             'Test_Number':'cand_Test_Number',
                             'Place':'cand_Place'})
             .drop(['orig_index'], axis=1))

merged = df.merge(df_cand, on='Student_ID', how='left')

merged = merged[merged['cand_Date'].lt(merged['Date'])]
merged = merged.sort_values(['Student_ID','orig_index','cand_Date','cand_Test_Number'],
                             ascending=[True,True,False,False])

merged['cand_rank'] = merged.groupby('orig_index').cumcount().add(1)

pivot = (merged[merged['cand_rank'].le(3)]
          .pivot(index='orig_index',columns='cand_rank',values='cand_Place')
          .add_prefix('student_rec'))

out = df.join(pivot).drop('orig_index', axis=1)
</code></pre>
<hr />
<pre><code>print(out)

         Date  Test_Number  Student_ID  Place  student_rec_1  student_rec_2  \
0  2024-07-14            5           2      3            9.0            6.0   
1  2024-07-14            4           2      5            9.0            6.0   
2  2024-07-14            3           2      7            9.0            6.0   
3  2024-07-14            2           2      3            9.0            6.0   
4  2024-07-14            1           2      1            9.0            6.0   
5  2024-03-14            3           2      9            7.0            8.0   
6  2024-03-14            2           2      6            7.0            8.0   
7  2024-03-14            1           2      3            7.0            8.0   
8  2024-02-14            4           2      7            8.0            2.0   
9  2024-02-10            3           2      8            NaN            NaN   
10 2024-02-10            2           2      2            NaN            NaN   
11 2024-02-10            1           2      1            NaN            NaN   
12 2024-04-13            2           1      3            2.0            1.0   
13 2024-04-13            1           1      4            2.0            1.0   
14 2023-02-11            3           1      2            6.0            2.0   
15 2023-02-11            2           1      1            6.0            2.0   
16 2023-02-11            1           1      5            6.0            2.0   
17 2011-10-11            1           1      6            2.0            7.0   
18 2011-05-02            2           1      2            NaN            NaN   
19 2011-05-02            1           1      7            NaN            NaN   

    student_rec_3  
0             3.0  
1             3.0  
2             3.0  
3             3.0  
4             3.0  
5             2.0  
6             2.0  
7             2.0  
8             1.0  
9             NaN  
10            NaN  
11            NaN  
12            5.0  
13            5.0  
14            7.0  
15            7.0  
16            7.0  
17            NaN  
18            NaN  
19            NaN  
</code></pre>
<p>EDIT: For better performance is possible use solution working per groups with numpy - compare dates for all previous to <code>mask</code>, create order by cumulative sum by <a href=""https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html"" rel=""nofollow noreferrer""><code>numpy.cumsum</code></a>, so possible get <code>N</code> top ordering with <a href=""https://numpy.org/doc/2.2/reference/generated/numpy.argmax.html"" rel=""nofollow noreferrer""><code>numpy.argmax</code></a>. Because there is possible some values not exist is necessary add condition with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.any.html"" rel=""nofollow noreferrer""><code>numpy.any</code></a> and return necessary columns:</p>
<pre><code>df['Date'] = pd.to_datetime(df['Date'])

N = 3

def f(x):

    dates = x['Date'].to_numpy()        
    places = x['Place'].astype(float).to_numpy() 

    mask = dates &lt; dates[:, None]  
    cs = np.cumsum(mask, axis=1) 
    targets = np.array(range(1, N+1))[None, :] 
    cs_ext = cs[..., None]

    cond = cs_ext == targets
    first_idx = np.argmax(cond, axis=1)
    m = np.any(cond, axis=1) 

    arr = places[first_idx]  
    arr[~m] = np.nan

    return pd.DataFrame(arr, 
                        index=x.index, 
                        columns=[f'student_rec_{i+1}' for i in range(N)])


out = df.join(df.groupby('Student_ID', group_keys=False)[['Place','Date']].apply(f))
</code></pre>
<hr />
<pre><code>print(out)
         Date  Test_Number  Student_ID  Place  student_rec_1  student_rec_2  \
0  2024-07-14            5           2      3            9.0            6.0   
1  2024-07-14            4           2      5            9.0            6.0   
2  2024-07-14            3           2      7            9.0            6.0   
3  2024-07-14            2           2      3            9.0            6.0   
4  2024-07-14            1           2      1            9.0            6.0   
5  2024-03-14            3           2      9            7.0            8.0   
6  2024-03-14            2           2      6            7.0            8.0   
7  2024-03-14            1           2      3            7.0            8.0   
8  2024-02-14            4           2      7            8.0            2.0   
9  2024-02-10            3           2      8            NaN            NaN   
10 2024-02-10            2           2      2            NaN            NaN   
11 2024-02-10            1           2      1            NaN            NaN   
12 2024-04-13            2           1      3            2.0            1.0   
13 2024-04-13            1           1      4            2.0            1.0   
14 2023-02-11            3           1      2            6.0            2.0   
15 2023-02-11            2           1      1            6.0            2.0   
16 2023-02-11            1           1      5            6.0            2.0   
17 2011-10-11            1           1      6            2.0            7.0   
18 2011-05-02            2           1      2            NaN            NaN   
19 2011-05-02            1           1      7            NaN            NaN   

    student_rec_3  
0             3.0  
1             3.0  
2             3.0  
3             3.0  
4             3.0  
5             2.0  
6             2.0  
7             2.0  
8             1.0  
9             NaN  
10            NaN  
11            NaN  
12            5.0  
13            5.0  
14            7.0  
15            7.0  
16            7.0  
17            NaN  
18            NaN  
19            NaN  
</code></pre>
","3","Answer"
"79448330","79448271","<p>Use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.sum.html"" rel=""nofollow noreferrer""><code>groupby.sum</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.rank.html"" rel=""nofollow noreferrer""><code>rank</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>:</p>
<pre><code>out = df.merge(df.groupby(['group', 'subgroup'])['value']
                 .sum().rank(ascending=False)
                 .rename('rank').reset_index()
              )
</code></pre>
<p>Output:</p>
<pre><code>   group subgroup  value  rank
0      a        i      2   3.0
1      a       ii      4   4.0
2      a        i      2   3.0
3      a       ii      3   4.0
4      a        i      5   3.0
5      b       ii      1   2.0
6      b        i      2   5.0
7      b       ii      4   2.0
8      b        i      1   5.0
9      b       ii      5   2.0
10     c       ii     11   1.0
</code></pre>
<p>Intermediates:</p>
<pre><code>  group subgroup  sum  rank
0     a        i    9   3.0
1     a       ii    7   4.0
2     b        i    3   5.0
3     b       ii   10   2.0
4     c       ii   11   1.0
</code></pre>
","2","Answer"
"79448626","79448603","<p>When you use <code>errors='ignore'</code>, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html"" rel=""nofollow noreferrer""><code>to_numeric</code></a> returns the original Series.</p>
<p>As mentioned in the documentation:</p>
<blockquote>
<p>errors {‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’</p>
<ul>
<li>If ‘raise’, then invalid parsing will raise an exception.</li>
<li>If ‘coerce’, then invalid parsing will be set as NaN.</li>
<li>If ‘ignore’, then invalid parsing will return the input.</li>
</ul>
<p><strong>Changed in version 2.2.  “ignore” is deprecated. Catch exceptions explicitly instead.</strong></p>
</blockquote>
<p>Catch the error explicitly if you want to keep the previous behavior:</p>
<pre><code>def to_numeric(s):
    try:
        return pd.to_numeric(s, errors='raise')
    except ValueError:
        return s
    
A.apply(to_numeric)
</code></pre>
<p><em>NB. use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a> rather than <code>map</code> for a vectorial operation.</em></p>
<p>Relevant issues: <a href=""https://github.com/pandas-dev/pandas/issues/54467"" rel=""nofollow noreferrer"">#54467</a>, <a href=""https://github.com/pandas-dev/pandas/issues/59221"" rel=""nofollow noreferrer"">#59221</a>.</p>
","3","Answer"
"79449236","79428965","<p>Per Polars Support:</p>
<blockquote>
<p>First, you need way more iterations than just 100 for such a small
timewindow. With 10,000 iterations I get the following:</p>
<p>avg polars run: 0.0005123567976988852 avg pandas run:
0.00012923809615895151 But we can rewrite the polars query to be more efficient:</p>
<pre><code>df_filtered = (
    df.lazy()
      .with_columns(abs_diff = (pl.col.column_0 - target_value).abs())
      .filter(pl.col.abs_diff == pl.col.abs_diff.min())
      .collect()
)
</code></pre>
<p>Then we get:</p>
<p>avg polars run: 0.00018435594723559915 Ultimately Polars isn't
optimized for doing many tiny tiny horizontally wide datasets though.</p>
</blockquote>
<p>Unfortunately, I didn't experience much of a performance boost when I tried the version above. It does seem the speeds are very machine dependent. I will continue with pandas for this specific use case. Thanks all for looking.</p>
","2","Answer"
"79449770","79446437","<p>Another possible solution, which is also based on <a href=""https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html"" rel=""nofollow noreferrer""><code>json_normalize</code></a>:</p>
<pre><code>out = pd.concat([pd.json_normalize(d[x]).assign(ticker = x) for x  in d])
</code></pre>
<p>If the <code>ticker</code> column <em>really</em> needs to be the first one, please use:</p>
<pre><code>out[np.roll(out.columns, 1)]
</code></pre>
<p>Output:</p>
<pre><code>  ticker    open    high     low   close
0   NVDA  144.75  144.21  174.33  210.47
1   NVDA  123.97  128.50  110.25  154.09
2   NVDA  118.19  134.81  104.37  149.72
3   NVDA  225.35  126.81  104.77  209.46
4   NVDA  247.20  243.25  220.44  186.01
0   MSFT  175.78  213.98  229.75  206.59
1   MSFT  142.98  168.42  188.33  232.52
2   MSFT  184.14  163.42  194.81  153.03
3   MSFT  199.54  130.26  101.05  102.10
4   MSFT  243.91  119.21  190.20  223.31
0   AAPL  202.06  162.54  212.30  226.78
1   AAPL  191.17  153.49  135.13  151.83
2   AAPL  187.15  149.75  123.28  247.32
3   AAPL  194.29  175.34  244.14  207.45
4   AAPL  228.90  133.26  100.59  129.35
</code></pre>
","3","Answer"
"79450374","79450165","<p>You can try <code>matplotlib.axes.Axes.invert_yaxis</code></p>
<pre><code>plt.gca().invert_yaxis()
</code></pre>
<p>or for x axis</p>
<pre><code>plt.gca().invert_xaxis()
</code></pre>
<p>The code</p>
<pre><code>from io import BytesIO
import pandas as pd
import matplotlib.pyplot as plt

DATA = {
    'x': ['val_1', 'val_2', 'val_3', 'val_4', 'val_5'],
    'y': [1, 2, 3, 4, 5]
}
COLUMNS = tuple(DATA.keys())

plt.figure()
df = pd.DataFrame(DATA, columns=COLUMNS)

df_sorted = df.sort_values(['y'], ascending=[False]).reset_index(drop=True)

df_sorted.plot(x=COLUMNS[0], y=COLUMNS[1], kind=&quot;barh&quot;, legend=False)

img_buf = BytesIO()
plt.gca().invert_xaxis()

plt.show()
</code></pre>
<p>The graph <code>plt.gca().invert_xaxis()</code></p>
<p><a href=""https://i.sstatic.net/lGSnjFM9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lGSnjFM9.png"" alt=""enter image description here"" /></a></p>
<p>The graph <code>plt.gca().invert_yaxis()</code></p>
<p><a href=""https://i.sstatic.net/AXPpdy8J.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AXPpdy8J.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79450535","79434429","<pre><code>import pandas as pd

data = {
    'Name': ['x', 'y', 'z'],
    'A': [1.1, 0, 0.5],
    'B': [0, 0.1, 0.1],
    'C': [0.2, 0, 0.3],
}
df = pd.DataFrame(data)
print(df)
'''
  Name    A    B    C
0    x  1.1  0.0  0.2
1    y  0.0  0.1  0.0
2    z  0.5  0.1  0.3
'''
# All columns except 'Name'
value_vars = [col for col in df.columns if col != 'Name'] 
'''
['A', 'B', 'C']
'''
category_map = {col: f&quot;{i+1}&quot; for i, col in enumerate(value_vars)}
'''
{'A': '1', 'B': '2', 'C': '3'}
'''

res = (
pd.melt(df,id_vars = ['Name'], value_vars = value_vars, var_name = 'Category',value_name = 'Val')
.query('Val != 0')
.assign(Category = lambda x : x['Category'].map(category_map) )
.reset_index(drop=True)    
)

print(res)
'''
 Name Category  Val
0    x        1  1.1
1    z        1  0.5
2    y        2  0.1
3    z        2  0.1
4    x        3  0.2
5    z        3  0.3
'''
</code></pre>
<p>Method 2(Better) :</p>
<pre><code>import pandas as pd
import numpy as np

data = {
    'Name': np.array(['x', 'y', 'z']),
    'A': np.array([1.1, 0, 0.5]),
    'B': np.array([0, 0.1, 0.1]),
    'C': np.array([0.2, 0, 0.3]),
}
df = pd.DataFrame(data)
print(df)
'''
  Name    A    B    C
0    x  1.1  0.0  0.2
1    y  0.0  0.1  0.0
2    z  0.5  0.1  0.3
'''

valueVars = df.columns[df.columns != 'Name']
#Index(['A', 'B', 'C'], dtype='object')

categoryLabels = np.array(
[f&quot;{i +1}&quot; for i in range(len(valueVars))]    
)
#['1' '2' '3']

namesExpanded = np.repeat(df['Name'].values,len(valueVars))
#['x' 'x' 'x' 'y' 'y' 'y' 'z' 'z' 'z']

categoriesExpanded = np.tile(categoryLabels,len(valueVars))
#['1' '2' '3' '1' '2' '3' '1' '2' '3']

valuesExpanded = df[valueVars].values.ravel()
#[1.1 0.  0.2 0.  0.1 0.  0.5 0.1 0.3]

mask = valuesExpanded != 0 

df1 = pd.DataFrame(
{ 
'Name' : namesExpanded[mask],
'Category' : categoriesExpanded[mask],
'Val' : valuesExpanded[mask]   
})

print(df1)
'''
  Name Category  Val
0    x        1  1.1
1    x        3  0.2
2    y        2  0.1
3    z        1  0.5
4    z        2  0.1
5    z        3  0.3
'''
</code></pre>
","0","Answer"
"79450688","79450672","<p>The basic logic you can use for column <code>'B'</code>:</p>
<ul>
<li><a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>Series.str.split</code></a> + access <code>str[-1]</code> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a></li>
</ul>
<pre class=""lang-py prettyprint-override""><code>df['B'].str.split().str[-1].map(sort_dicts)

0     1.0
1     0.0
2     1.0
3     NaN
4     0.0
5     3.0
6     4.0
7     3.0
8     4.0
9     2.0
10    NaN
11    4.0
12    2.0
Name: B, dtype: float64
</code></pre>
<p>Couple of ways to sort using this logic:</p>
<p><strong>Option 1</strong></p>
<p>Chain calls to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html"" rel=""nofollow noreferrer""><code>df.sort_values</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code># note 'B' first

def sort_fun(s):
    return s.str.split().str[-1].map(sort_dicts)

out = (df.sort_values('B', key=sort_fun)
       .sort_values('A', ignore_index=True)
       )
</code></pre>
<p><strong>Option 2</strong></p>
<p>Adjust <code>sort_fun</code> to only affect col <code>'B'</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def sort_fun2(s, name):
    if s.name == name: # for 'B'
        return s.str.split().str[-1].map(sort_dicts)
    return s

out2 = df.sort_values(['A', 'B'], key=lambda x: sort_fun2(x, 'B'),
                      ignore_index=True)
</code></pre>
<p>Indeed, your original approach also applied the function passed to <code>key</code> to <code>df['A']</code> (i.e., <code>df['A'].map(sort_dicts)</code>), leading to a series with <code>NaN</code> values to &quot;sort&quot;.</p>
<p><strong>Option 3</strong></p>
<p>Use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html"" rel=""nofollow noreferrer""><code>np.lexsort</code></a> as suggested by <a href=""https://stackoverflow.com/users/16343464/mozway"">@mozway</a> in the linked <a href=""https://stackoverflow.com/a/74611554/18470692"">post</a>:</p>
<pre class=""lang-py prettyprint-override""><code># again: note 'B' goes first

import numpy as np

sort = np.lexsort((df['B'].str.split().str[-1].map(sort_dicts), 
                   df['A']))

out3 = df.iloc[sort].reset_index(drop=True)
</code></pre>
<hr />
<p><strong>Output</strong></p>
<pre class=""lang-py prettyprint-override""><code>out

                 A                        B
0   Ankang Shaanxi         Ankang Northeast
1   Ankang Shaanxi         Ankang Southeast
2   Ankang Shaanxi         Ankang Northwest
3   Ankang Shaanxi         Ankang Southwest
4   Ankang Shaanxi   Ankang Southwest Upper
5    Baoding Anguo  Baoding Anguo Northeast
6    Baoding Anguo  Baoding Anguo Southeast
7    Baoding Anguo  Baoding Anguo Northwest
8    Baoding Anguo  Baoding Anguo Southwest
9    Baoding Anguo      Baoding Anguo Upper
10  Changsha Hunan    Changsha Hunan Bright
11  Changsha Hunan     Changsha Hunan Lower
12   Luoyang Henan      Luoyang Henan Upper
</code></pre>
<hr />
<p>Equality check with desired output:</p>
<pre class=""lang-py prettyprint-override""><code>data2 = {'A': ['Ankang Shaanxi', 'Ankang Shaanxi', 'Ankang Shaanxi', 
               'Ankang Shaanxi', 'Ankang Shaanxi', 'Baoding Anguo', 
               'Baoding Anguo', 'Baoding Anguo', 'Baoding Anguo', 
               'Baoding Anguo', 'Changsha Hunan', 'Changsha Hunan', 
               'Luoyang Henan'], 
         'B': ['Ankang Northeast', 'Ankang Southeast', 'Ankang Northwest', 
               'Ankang Southwest', 'Ankang Southwest Upper', 
               'Baoding Anguo Northeast', 'Baoding Anguo Southeast', 
               'Baoding Anguo Northwest', 'Baoding Anguo Southwest', 
               'Baoding Anguo Upper', 'Changsha Hunan Bright', 
               'Changsha Hunan Lower', 'Luoyang Henan Upper']}
desired = pd.DataFrame(data2)

all(df.equals(desired) for df in [out, out2, out3])
# True
</code></pre>
","1","Answer"
"79450689","79450672","<p>You can <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>assign</code></a> an new column based on the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html"" rel=""nofollow noreferrer""><code>extract</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a>:</p>
<pre><code>out = (df
   .assign(sort_value=df['B'].str.extract(r'(\w+)$', expand=False).map(sort_dicts))
   .sort_values(by=['A', 'sort_value'])
   .drop(columns='sort_value')
)
</code></pre>
<p><em>NB. <code>df['B'].str.extract(r'(\w+)$', expand=False)</code> is equivalent to <code>df['B'].str.split().str[-1]</code> but usually faster.</em></p>
<p>In such cases, having multiple conditions with different sorting keys, I prefer to use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html"" rel=""nofollow noreferrer""><code>numpy.lexsort</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>iloc</code></a>, which avoids the need to assign a temporary column:</p>
<pre><code>out = df.iloc[np.lexsort([df['B'].str.extract(r'(\w+)$', expand=False)
                                 .map(sort_dicts),
                          df['A']])]
</code></pre>
<p>Alternatively, if you don't want to use <code>lexsort</code>, you could also use a dictionary of custom sorting keys, or keep the original values by default:</p>
<pre><code>sorter = {'B': df['B'].str.extract(r'(\w+)$', expand=False).map(sort_dicts)}

out = df.sort_values(['A', 'B'], key=lambda x: sorter.get(x.name, x))
</code></pre>
<p>Output:</p>
<pre><code>                 A                        B
4   Ankang Shaanxi         Ankang Northeast
0   Ankang Shaanxi         Ankang Southeast
12  Ankang Shaanxi         Ankang Northwest
7   Ankang Shaanxi         Ankang Southwest
11  Ankang Shaanxi   Ankang Southwest Upper
1    Baoding Anguo  Baoding Anguo Northeast
2    Baoding Anguo  Baoding Anguo Southeast
9    Baoding Anguo  Baoding Anguo Northwest
5    Baoding Anguo  Baoding Anguo Southwest
6    Baoding Anguo      Baoding Anguo Upper
3   Changsha Hunan    Changsha Hunan Bright
10  Changsha Hunan     Changsha Hunan Lower
8    Luoyang Henan      Luoyang Henan Upper
</code></pre>
<h3>Why did the original approach fail?</h3>
<p>Your issue in <code>df.sort_values(by=['A', 'sort_value'], key=lambda x :x.map(sort_dicts))</code> is that you're passing a key that will be applied to both columns, which gives you an intermediate that cannot sort on <code>A</code>:</p>
<pre><code>       A  sort_value
0   None         1.0
1   None         0.0
2   None         1.0
3   None         NaN
4   None         0.0
5   None         3.0
6   None         4.0
7   None         3.0
8   None         4.0
9   None         2.0
10  None         NaN
11  None         4.0
12  None         2.0
</code></pre>
","1","Answer"
"79450710","79450409","<p>I figured out this solution:</p>
<pre class=""lang-py prettyprint-override""><code>def test_xls_parse():
    file_path = 'test.xls'
    df = pd.read_excel(file_path, engine='xlrd')

    time_label = df.iloc[0, 0]


    categories = df.iloc[1, 2:]
    varieties = df.iloc[2, 2:]
    specifications = df.iloc[3, 2:]
    units = df.iloc[4, 2:]
    averages = df.iloc[5, 2:]


    regions = df.iloc[6:, 0].ffill()
    markets = df.iloc[6:, 1]
    prices = df.iloc[6:, 2:]

    result = []
    for i in range(len(categories)):
        for j in range(len(regions)):
            obj = {
                &quot;date&quot;: time_label,
                &quot;category&quot;: categories.iloc[i],
                &quot;variety&quot;: varieties.iloc[i],
                &quot;specification&quot;: specifications.iloc[i],
                &quot;unit&quot;: units.iloc[i],
                &quot;average&quot;: None if averages.iloc[i] == '-' else float(averages.iloc[i]),
                &quot;region&quot;: regions.iloc[j],
                &quot;market&quot;: markets.iloc[j],
                &quot;price&quot;: None if prices.iloc[j, i] == '-' else float(prices.iloc[j, i])
            }
            result.append(obj)

    return pd.DataFrame(result)
</code></pre>
","1","Answer"
"79450841","79450810","<p>I'm not sure if this is what you are looking for but you can create a custom agg function like:</p>
<pre><code>pd.NamedAgg(column=&quot;Model Year&quot;, aggfunc=lambda x: np.count(x))
</code></pre>
<p>or</p>
<pre><code>pd.NamedAgg(column=&quot;Model Year&quot;, aggfunc=lambda x: len(x))
</code></pre>
","0","Answer"
"79450846","79450810","<p>Your question would benefit from a minimal reproducible example.</p>
<p>That said, the count doesn't really depend on a particular column, as long as you don't have missing values, thus pick any one that matches this criterion and add another aggregation (you can use one of the grouping columns or <code>Model Year</code> since you know it must be a valid number):</p>
<pre><code>out = (data[filt].groupby([&quot;State&quot;, &quot;Make&quot;], sort=False, observed=True, as_index=False)
        .agg(avg_electric_range=pd.NamedAgg(column=&quot;Electric Range&quot;, aggfunc=&quot;mean&quot;),
             oldest_model_year=pd.NamedAgg(column=&quot;Model Year&quot;, aggfunc=&quot;min&quot;),
             count=pd.NamedAgg(column=&quot;Model Year&quot;, aggfunc=&quot;count&quot;),
            )
       )
</code></pre>
<p>Example output:</p>
<pre><code>  State Make  avg_electric_range  oldest_model_year  count
0    WA    X                 0.5               2018      2
1    WA    Y                 3.0               2018      3
</code></pre>
","1","Answer"
"79450963","79450950","<p>This means that if you pass two coordinates, the first one indexes the rows, and the second the columns.</p>
<p>Assuming this example:</p>
<pre><code>   0  1  2
0  0  1  2
1  3  4  5
2  6  7  8
</code></pre>
<pre><code>df.iloc[0, 2]  # first (0) row, third (2) column
# 2
</code></pre>
<p>You would actually get the same in pure python:</p>
<pre><code>lst = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]

lst[0][2] # 2
</code></pre>
<p>The side effect is that, with a single coordinate you default to the rows:</p>
<pre><code>df.iloc[0] # first row
</code></pre>
<p>And if you want only a column, you should explicitly request all rows with <code>:</code>:</p>
<pre><code>df.iloc[:, 2] # all rows, third column
</code></pre>
","2","Answer"
"79450979","79450950","<p>I would say that statement is incorrect or, at least, very misleading and likely to cause confusion.</p>
<p>Both <code>iloc</code> and <code>loc</code> are <strong>row-first</strong> &amp; <strong>column-second</strong>, but this is exactly the <strong>same as</strong> how indexing works in <strong>native Python</strong> and your example. First index refers to the row, and the second index refers to the column.</p>
<p>Your example in pandas using <code>iloc/loc</code> also outputs <strong>6</strong>:</p>
<pre><code>import pandas as pd

data = [
    [1, 2, 3], # row 0
    [4, 5, 6], # row 1
    [7, 8, 9]  # row 2
]

df = pd.DataFrame(data)

print(df.iloc[1, 2])

# Output: 6
</code></pre>
<p>There has already been some discussion about this exact statement in this <a href=""https://www.kaggle.com/discussions/getting-started/138866"" rel=""nofollow noreferrer"">Kaggle discussion</a>, but to me is still not clear to what the author was referring to.</p>
<p>As per <em>Siraz Naorem</em> understanding, the <strong>statement might be referring</strong> to the <strong>creation of DataFrames</strong> <strong>from</strong> column-oriented data, e.g. <strong>dictionaries</strong>, where each list or array represents a column, not a row.</p>
<p>If we replicate again your example but create the DataFrame from a dictionary like this:</p>
<pre><code>df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})

print(df)
# Output:    
#    A  B  C
# 0  1  4  7
# 1  2  5  8
# 2  3  6  9
</code></pre>
<p>Now, when we access index <code>[1,2]</code>, we do not get 6:</p>
<pre><code>print(df.iloc[1, 2]) 
# Output: 8

print(df.iloc[2, 1]) 
# Output: 6
</code></pre>
<p>In this case, the row and column indices might seem reversed and may lead to the mistaken idea that indexing is different: <code>iloc[1,2]</code> give us now <strong>8</strong>, and we have to use <code>iloc[2,1]</code> to get the value <strong>6</strong>.</p>
<p>However, <strong><code>iloc/loc</code> indexing itself has not changed</strong>, is still row-first &amp; column-second, and what is <strong>different</strong> is the <strong>structure</strong> of the DataFrame, since pandas internally has treated each list in the dictionary as a column.</p>
","2","Answer"
"79451519","79450165","<p>Or just change <code>ascending = [False]</code> to <code>ascending = [True]</code>:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt

DATA = {
    'x': ['val_1', 'val_2', 'val_3', 'val_4', 'val_5'],
    'y': [1, 2, 3, 2.5, 0.5]
}

COLUMNS = tuple(DATA.keys())


df = pd.DataFrame(DATA, columns=COLUMNS)
df.sort_values(['y'], ascending=[True]).plot(x=COLUMNS[0], y=COLUMNS[1], kind=&quot;barh&quot;, legend=False)

plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/jtP7WTnF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jtP7WTnF.png"" alt=""enter image description here"" /></a></p>
","2","Answer"
"79452288","79452237","<p>The logic is quite simple, for each <code>False</code> in the output of the <code>cond</code> callable, the <strong>matching value</strong> in the result of <code>other</code> will be used as replacement. If <code>other</code> is a scalar, this value is used.</p>
<p>The matching value is identified by position if the callable returns an array, and by alignment for a DataFrame:</p>
<pre><code>df = pd.DataFrame({'A': [1, 0], 'B': [0, 1]})
df.where(cond=lambda x: x==0,
         other=lambda x: pd.DataFrame({'A': [10, 20]}, index=[1, 0]))
#     A    B
# 0  20  0.0
# 1   0  NaN

df.where(cond=lambda x: x==0, other=lambda x: [[7,8],[9,10]])
#    A   B
# 0  7   0
# 1  0  10
</code></pre>
<p>Therefore, your <code>MyFunc</code> functions should return a scalar, a DataFrame (that will be aligned), or an array of the same shape as the input.</p>
<p>You can modify it to broadcast the values to all columns:</p>
<pre><code>def MyFunc(x1):
    print(1)
    return np.broadcast_to(x1['A'].to_frame().values, df.shape)
</code></pre>
<p>Example:</p>
<pre><code>df = pd.DataFrame([[1, -1,  0,  0],
                   [2,  0, -1,  0],
                   [3,  0,  0, -1]],
                  columns=['A', 'B', 'C', 'D'])
def MyBool(x):
    return x &gt;= 0

def MyFunc(x):
    return np.broadcast_to(x['A'].to_frame().values, df.shape)

out = df.where(cond=MyBool, other=MyFunc)

#    A  B  C  D
# 0  1  1  0  0
# 1  2  0  2  0
# 2  3  0  0  3
</code></pre>
<p>Note that the callables <strong>should NOT</strong> modify the DataFrame in place.</p>
<p>This should be avoided:</p>
<pre><code>def MyFunc(x1):
    x1.loc[ x1.test == 5, [&quot;B&quot;, &quot;C&quot;] ] /= 2
    return x1
</code></pre>
<p>and could be replaced by a simple (without using <code>where</code>):</p>
<pre><code>df.loc[df['test'] == 5, ['B', 'C']] /= 2
</code></pre>
","2","Answer"
"79452379","79452360","<p>Just combine the parts into a single string, and pass to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>to_datetime</code></a>:</p>
<pre><code>new_df['release_date'] = pd.to_datetime(new_df['date_parts']
                                        .apply(lambda x: '-'.join(map(str, x))),
                                        format='%d-%B-%Y')
</code></pre>
<p>Output:</p>
<pre><code>           date_parts release_date
0  [29, August, 2024]   2024-08-29
1  [28, August, 2024]   2024-08-28
2  [27, August, 2024]   2024-08-27
</code></pre>
<p>You could also convert the list of list to DataFrame with day/month/year columns:</p>
<pre><code>new_df['release_date'] = pd.to_datetime(
    pd.DataFrame(
        new_df['date_parts'].to_list(),
        index=new_df.index,
        columns=['day', 'month', 'year'],
    ).replace({'month': month_map})
)
</code></pre>
","3","Answer"
"79452596","79436039","<p>I suggest the following algorithm to construct straight lines along borders between phases:</p>
<ul>
<li><p>Find points in between sample grid points that have different phases on each side.  Store these points ALONG with the pair of phases that they separate.</p>
</li>
<li><p>Sort the boundary points by their phase pairs.  ( This ensures that all the points that separate any particular phase pair are stored in adjacent memory locations )</p>
</li>
<li><p>LOOP over each phase pair that share boundary points</p>
<ul>
<li><p>LOOP over points that separate this phase pair</p>
<ul>
<li><p>Find the two points between phase pair that are the furthest apart from each other</p>
</li>
<li><p>construct boundary line connecting two furthest apart points</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>merge close boundary line endpoints at their centroid</p>
</li>
</ul>
<p>Applying this algorithm to your sample data gives</p>
<p><a href=""https://i.sstatic.net/Hl05Jh7O.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Hl05Jh7O.png"" alt=""enter image description here"" /></a></p>
<p>I implemented the algorithm in C++ ( my MATLAB coding skills are atrophied ).  Here is the routine that detects the boundaries and constructs the straight lines</p>
<pre class=""lang-none prettyprint-override""><code>void cSampleGrid::findBoundaries()
{
    // locate grid points on boundaries between phases
    for (int r = 0; r &lt; Grid.size() - 1; r++)
        for (int c = 0; c &lt; Grid[0].size() - 1; c++)
        {
            if (Grid[r][c] != Grid[r][c + 1])
                addBoundaryPoint(c + 0.5, r, Grid[r][c], Grid[r][c + 1]);

            if (Grid[r][c] != Grid[r + 1][c])
                addBoundaryPoint(c, r + 0.5, Grid[r][c], Grid[r + 1][c]);
        }

    // sort boundary points by the phases they are between
    std::sort(
        myBoundary.begin(), myBoundary.end(),
        [](const sBoundary &amp;a, const sBoundary &amp;b)
        {
            return a.phase &lt; b.phase;
        });

    // calculate end points of straight line though
    // all points between each pair of phases
    std::pair&lt;int, int&gt; prevPhase(INT_MAX, INT_MAX);
    int i1 = 0;
    for (int bk = 0; bk &lt; myBoundary.size(); bk++)
    {
        if (bk == 0)
        {
            prevPhase = myBoundary[bk].phase;
            continue;
        }
        if (myBoundary[bk].phase != prevPhase)
        {
            // found all points between two particular phases

            // create line between too points with greatest separation
            addBoundaryLine( i1, bk );

            // setup looking for points between next two phases
            prevPhase = myBoundary[bk].phase;

            i1 = bk;
        }
    }
    // draw line for seperation of last two phases
    addBoundaryLine(i1,myBoundary.size());
}
</code></pre>
<p>The complete code for the application that generates the sample grid, detects the boundaries, generates the boundary lines and displays the result is at <a href=""https://codeberg.org/JamesBremner/Phaser"" rel=""nofollow noreferrer"">https://codeberg.org/JamesBremner/Phaser</a></p>
<p>=========================================================</p>
<p>Thank you for including the defined polygons to be used for generating  a test dataset - they make this question both easier to work with and more interesting.</p>
<p>In the above answer I have used the same grid resolution as in your code snippet, i.e. 40 by 40 = 1,600 data points.</p>
<p>That is a lot of data points!  I would guess that finding the stable phase at each of 1600 different conditions would be tedious, arduous and very expensive.  Would so many data points be available in the real world?</p>
<p>I tried lowering the resolution and thus reducing the data requirements.</p>
<p><a href=""https://i.sstatic.net/FDFiSmVo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FDFiSmVo.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/pQjTpKfg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pQjTpKfg.png"" alt=""enter image description here"" /></a></p>
<p>So, if the resolution is significantly reduced, things begin to fall apart.  Yet, even asking your lab people to do 200 odd measurements seems to me like a big ask.</p>
<p>I conclude that while my algorithm is successful in answering your question as posted, I have doubts that it will be useful IRL.</p>
<p>Please let me know if you are interested in developing code that will produce useful phase diagram boundaries for lower resolution data sets.</p>
","5","Answer"
"79452894","79452715","<p>You could use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html"" rel=""nofollow noreferrer""><code>groupby.apply</code></a> to handle all rows of a SEGMENT simultaneously:</p>
<pre><code>df['CLASS'] = (df.groupby('SEGMENT', group_keys=False)['DATE']
                 .apply(lambda x: pd.cut(x, bins=cuts[x.name]['cut'],
                                         labels=cuts[x.name]['class'],
                                         ordered=False))
              )
</code></pre>
<p>Example output:</p>
<pre><code>  SEGMENT      DATE       CLASS
0       A  20240102    training
1       A  20241215         out
2       A  20231201         out
3       B  20240102    training
4       B  20241215         out
5       C  20231201    training
6       C  20240102    training
7       C  20241215  validation
</code></pre>
<p>Testing on 10K rows, this is ~1000x faster that the original approach:</p>
<pre><code># groupby
3.68 ms ± 76.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# original
3.34 s ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<p>Testing on 1M rows, this takes about half a second:</p>
<pre><code>529 ms ± 277 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
","2","Answer"
"79453694","79453687","<p>There is no direct way to do this in pandas since you're using rows of data as header.</p>
<p>You could however convert to CSV string and post-process it:</p>
<pre><code>import re

with open('/content/test.csv', 'w') as f:
    f.write(re.sub(',*\n,+\n', '\n\n', result.to_csv(index=False, header=False)))
</code></pre>
<p>A better option would be to first create the output file with the header, then export a dataframe with normal data/header and append it to the file with the <code>mode='a'</code> of <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer""><code>to_csv</code></a>:</p>
<pre><code>filename = '/content/test.csv'
with open(filename, 'w') as f:
    f.write('Category: A\n\n')
    
df = pd.DataFrame(data[1:], columns=data[0])
#          Date  Value
# 0  2025-01-01     50
# 1  2025-01-02     40
# 2  2025-01-03     45

df.to_csv(filename, index=False, mode='a')
</code></pre>
<p>Output:</p>
<pre><code>Category: A

Date,Value
2025-01-01,50
2025-01-02,40
2025-01-03,45
</code></pre>
","2","Answer"
"79453720","79453687","<p>Assuming that you want to merge one empty DataFrame with one column named &quot;Category: A&quot;, and the other non-empty DataFrame with columns &quot;Date&quot; and &quot;Value&quot;, I would do it like this:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# 1st DataFrame with single column
df1 = pd.DataFrame(columns=['Category: A'])
# 2nd DataFrame with double columns 
data = [
  [&quot;Date&quot;, &quot;Value&quot;],
  ['2025-01-01', 50],
  ['2025-01-02', 40],
  ['2025-01-03', 45]
]
df2 = pd.DataFrame(data[1:], columns=data[0])

# Merge the two DataFrames
result = pd.concat([df1, df2], axis=1)
print(result)
result.to_csv('/content/test.csv', index=False, header=False)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>Category: A,Date,Value
NaN,2025-01-01,50
NaN,2025-01-02,40
NaN,2025-01-03,45

</code></pre>
","0","Answer"
"79454209","79454185","<p>You can use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html"" rel=""nofollow noreferrer""><code>groupby.apply</code></a> with a dictionary of your model_version/model:</p>
<pre><code>models = {'model_a': model_a,
          'model_b': model_b,
         }

out = (df.groupby('model_version')
         .apply(lambda x: models[x.name].predict(x))
      )
</code></pre>
<p>Or <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> in a dictionary comprehension to get independent DataFrames in a dictionary as output:</p>
<pre><code>models = {'model_a': model_a,
          'model_b': model_b,
         }

out = {k: models[g.name].predict(g)
       for k, g in df.groupby('model_version')}
</code></pre>
<p><em>NB. if you want to use the <code>id</code> as criterion, then change the dictionary to <code>models = {1: model_a, 2: model_b}</code> and use <code>groupby('id')</code> in place of <code>groupby('model_version')</code>.</em></p>
","4","Answer"
"79454455","79454433","<p>Assuming you have non-date columns and date-like columns, you could convert them to date with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>pd.to_datetime</code></a> and <code>errors='coerce'</code>. Select the non-date columns with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.isna.html"" rel=""nofollow noreferrer""><code>isna</code></a>, and the wanted dates with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.between.html"" rel=""nofollow noreferrer""><code>between</code></a>, then perform <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a> on the columns and select them:</p>
<pre><code>dates = pd.to_datetime(df.columns, errors='coerce', format='%d/%m/%Y')
m = dates.to_series().between(pd.Timestamp('2025-01-04'),
                              pd.Timestamp('2025-01-06'),
                              inclusive='both')

out = df.loc[:, dates.isna() | m.values]
</code></pre>
<p>Output:</p>
<pre><code>     Fname       Lname  04/01/2025  05/01/2025  06/01/2025
0     Owen  Richardson         128         114         239
1   Edward       Jones         148         144         182
2   Steven     Cameron         228         272         140
3    Aldus      Turner         281         139         171
4  Dainton      Wright         269         176         142
5    Sofia    Harrison         100         103         154
6  Heather       Evans         155         163         201
7   Stella      Harris         126         183         157
8    Joyce       Smith         251         143         229
9    Tyler        Hill         299         293         218
</code></pre>
<p>If you just want the date-like:</p>
<pre><code>df[df.columns[m]]

   04/01/2025  05/01/2025  06/01/2025
0         128         114         239
1         148         144         182
2         228         272         140
3         281         139         171
4         269         176         142
5         100         103         154
6         155         163         201
7         126         183         157
8         251         143         229
9         299         293         218
</code></pre>
","0","Answer"
"79454505","79454433","<p>Here is the script to select the records between dates. This code should be little bit faster:</p>
<pre><code>import pandas as pd

file_path = &quot;file.xlsx&quot;  # Update with the correct file path
df = pd.read_excel(file_path)

# Please change the dates according to your need (04/01/2025 to 06/01/2025).
selected_columns = df[[&quot;Fname&quot;] + [&quot;Lname&quot;] + list(df.loc[:, &quot;04/01/2025&quot;:&quot;06/01/2025&quot;].columns)]

print(selected_columns)
</code></pre>
<p>If you don't need Fname and Lname please remove &quot;[&quot;Fname&quot;] + [&quot;Lname&quot;] + &quot;. Just use the line below</p>
<pre><code>selected_columns = df[list(df.loc[:, &quot;04/01/2025&quot;:&quot;06/01/2025&quot;].columns)]
</code></pre>
<p>If you want to run the script preventing an error if any column is missing, please use:</p>
<pre><code>try:
    date_columns = list(df.loc[:, &quot;04/01/2025&quot;:&quot;06/01/2025&quot;].columns)
except KeyError:
    print(&quot;Error: The specified date range columns do not exist in the dataset.&quot;)
    date_columns = []  # Prevents errors in the next step

selected_columns = df[[&quot;Fname&quot;] + date_columns]
</code></pre>
<p>Output</p>
<p><a href=""https://i.sstatic.net/lGWHXQy9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lGWHXQy9.png"" alt=""Output"" /></a></p>
","1","Answer"
"79455562","79455358","<p>You were really close in your attempt. Just don't set <code>df1</code>'s index to <code>year</code> (since it has duplicates). You could consider two alternatives:</p>
<h2>Use <code>year</code> <em>plus</em> something else</h2>
<p>e.g. keep the original index with <code>append=True</code></p>
<pre><code>df1.set_index(&quot;year&quot;, inplace=True, append=True)
df2.set_index(&quot;year&quot;, inplace=True)
df1[vars_to_replace] /= df2
df1
</code></pre>
<pre><code>               B         C       D
  year                            
0 1     1.333333  1.250000  Good 1
1 1     0.666667  0.750000  Good 2
2 0     0.200000  0.285714  Good 1
3 0     1.000000  0.857143  Good 2
</code></pre>
<h2>Avoid changing <code>df1</code>'s index altogether</h2>
<pre><code>df2.set_index(&quot;year&quot;, inplace=True)
df1[vars_to_replace] /= df2.reindex(df1['year']).reset_index(drop=True)
df1
</code></pre>
<pre><code>   year         B         C       D
0     1  1.333333  1.250000  Good 1
1     1  0.666667  0.750000  Good 2
2     0  0.200000  0.285714  Good 1
3     0  1.000000  0.857143  Good 2
</code></pre>
<p><em>Note</em>: I'm using <code>.reset_index(drop=True)</code> since <code>df1</code>'s index is the default <code>RangeIndex</code>. If <code>df1</code> had a non-default index, then the correct option would be <code>.set_axis(df1.index)</code>; however if <code>df1.index</code> has duplicates, you'll need to find another way.</p>
","1","Answer"
"79455686","79455667","<p>Craft an indexer with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.argsort.html"" rel=""nofollow noreferrer""><code>numpy.argsort</code></a> (and the <code>stable=True</code> option), <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>, reorder:</p>
<pre><code>df1 = pd.DataFrame({'col': [1,2,3,4]})
df2 = pd.DataFrame({'col': [5,6,7,8]})

order = np.argsort(
  np.r_[np.arange(len(df1))//2,
        np.arange(len(df2))//2
       ],
       stable=True # required in numpy &gt;= 2
)

out = pd.concat([df1, df2]).iloc[order]
</code></pre>
<p>Output:</p>
<pre><code>   col
0    1
1    2
0    5
1    6
2    3
3    4
2    7
3    8
</code></pre>
<p>You can generalize to other steps:</p>
<pre><code>N = 3
order = np.argsort(
  np.r_[np.arange(len(df1))//N,
        np.arange(len(df2))//N
      ]
    )
</code></pre>
<p>Output:</p>
<pre><code>   col
0    1
1    2
2    3
0    5
1    6
2    7
3    4
3    8
</code></pre>
","1","Answer"
"79456356","79456337","<p>You would typically use a MultiIndex here, which makes operations much easier than relying on substrings:</p>
<pre><code># set &quot;Branch&quot; as index, convert columns to MultiIndex
df2 = df.set_index('Branch')
df2.columns = df2.columns.str.split('_', expand=True).rename(['date', 'id'])

# perform the operation and join
out = df2.join(df2[date_3].sub(df2.drop(columns=[date_3]))
               .rename(lambda x: f'diff_{x}', level=0, axis=1)
              )
</code></pre>
<p>Output:</p>
<pre><code>date      20241231         20250214         20250220         diff_20241231         diff_20250214        
id             001 002 003      001 002 003      001 002 003           001 002 003           001 002 003
Branch                                                                                                  
Branch_1        82  72  68       62  89  86       89  64  77             7  -8   9            27 -25  -9
Branch_2        72  66  80       87  63  78       81  60  76             9  -6  -4            -6  -3  -2
Branch_3        84  63  70       79  63  72       61  71  63           -23   8  -7           -18   8  -9
Branch_4        89  82  82       85  67  63       72  62  84           -17 -20   2           -13  -5  21
Branch_5        84  89  71       83  69  69       62  65  87           -22 -24  16           -21  -4  18
Branch_6        63  65  65       81  69  70       62  81  68            -1  16   3           -19  12  -2
Branch_7        78  83  89       79  69  87       84  76  80             6  -7  -9             5   7  -7
Branch_8        75  71  88       74  83  73       61  68  64           -14  -3 -24           -13 -15  -9
Branch_9        63  60  75       80  63  67       65  89  76             2  29   1           -15  26   9
Branch_10       70  71  68       81  74  67       68  61  85            -2 -10  17           -13 -13  18
</code></pre>
<p>If needed you can always come back to a flat index later:</p>
<pre><code>out.columns = out.columns.map('_'.join)
out.reset_index(inplace=True)
</code></pre>
<p>Output:</p>
<pre><code>      Branch  20241231_001  20241231_002  20241231_003  20250214_001  20250214_002  20250214_003  20250220_001  20250220_002  20250220_003  diff_20241231_001  diff_20241231_002  diff_20241231_003  diff_20250214_001  diff_20250214_002  diff_20250214_003
0   Branch_1            63            89            62            67            69            86            68            67            88                  5                -22                 26                  1                 -2                  2
1   Branch_2            67            80            75            78            85            60            84            83            64                 17                  3                -11                  6                 -2                  4
2   Branch_3            88            89            87            88            78            82            87            73            85                 -1                -16                 -2                 -1                 -5                  3
3   Branch_4            63            62            81            71            60            76            89            86            60                 26                 24                -21                 18                 26                -16
4   Branch_5            78            65            67            79            87            70            87            77            70                  9                 12                  3                  8                -10                  0
5   Branch_6            89            65            67            77            69            64            74            84            74                -15                 19                  7                 -3                 15                 10
6   Branch_7            77            72            71            69            88            84            83            80            82                  6                  8                 11                 14                 -8                 -2
7   Branch_8            61            72            82            89            71            80            60            83            88                 -1                 11                  6                -29                 12                  8
8   Branch_9            78            81            77            74            77            63            79            60            80                  1                -21                  3                  5                -17                 17
9  Branch_10            77            89            66            81            69            79            68            71            78                 -9                -18                 12                -13                  2                 -1
</code></pre>
","2","Answer"
"79457043","79457029","<p>I'm not sure why this is causing an error with <code>dtype=object</code>, but your arrays are 2D.</p>
<p>A Series is a 1D object.</p>
<p>If you convert them to 1D this works fine:</p>
<pre><code>data['c1'] = t2.ravel()    # works fine
data['c1'] = t2.squeeze()  # also works fine
</code></pre>
","1","Answer"
"79458164","79457797","<p>First get all rows with <code>target_label='Security Alert'</code></p>
<pre><code>selected = df[ df['target_label']=='Security Alert' ]
</code></pre>
<p>Next sort values by date (doc: <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values"" rel=""nofollow noreferrer"">pandas.DataFrame.sort_values</a>)</p>
<pre><code>selected = selected.sort_values(by=&quot;date&quot;)  # .sort_values(by=&quot;date&quot;, ascending=True)
</code></pre>
<p>Finally get last (or first) three rows (depends on sorting &quot;ascending&quot; or &quot;descending&quot;)</p>
<pre><code>selected = selected[-3:]  # selected[:3]
</code></pre>
<p>So it can be</p>
<pre><code>selected = df[ df['target_label'] == 'Security Alert' ].sort_values(by='date')[-3:]
</code></pre>
<p>Of course you have to check if <code>pandas</code> keeps <code>date</code> as <code>datetime</code> object - or you will have to convert them on your own (<code>pandas.to_datetime()</code>)</p>
<hr />
<p><strong>Minimal working example.</strong></p>
<p>I use <code>io</code> only to create <code>file like object</code> in memory - so everyone can simply copy and run this example (but you should use <code>filename</code>)</p>
<p>I had to use <code>pandas.to_datetime()</code> to convert string with date to object <code>datetime</code>.</p>
<pre class=""lang-py prettyprint-override""><code>data = '''1760,12/29/2025 20:33,ModernCRM,Unauthorised access attempt from 192.168.99.79 detected,Security Alert,bert
2213,9/1/2025 10:30,AnalyticsEngine,Security alert: suspicious activity from 192.168.214.63,Security Alert,bert
168,5/19/2025 7:56,ModernHR,Multiple incorrect login attempts were made by user 7918,Security Alert,bert
1844,6/10/2025 8:39,ThirdPartyAPI,&quot;Alert: server 9 experienced unusual login attempts, security risk&quot;,Security Alert,bert
961,4/26/2025 5:21,ThirdPartyAPI,API access audit trail shows unauthorized entry for user 5627,Security Alert,bert
1077,3/4/2025 9:49,AnalyticsEngine,&quot;Anomalous activity identified on server 47, security review recommended&quot;,Security Alert,bert
1356,5/3/2025 13:03,ThirdPartyAPI,Multiple rejected login attempts found for user 1805,Security Alert,bert
43,11/22/2025 11:06,BillingSystem,&quot;Abnormal system behavior on server 40, potential security breach&quot;,Security Alert,bert
2062,6/7/2025 1:22,AnalyticsEngine,&quot;Server 11 experienced potential security incident, review required&quot;,Security Alert,bert
769,4/28/2025 4:07,ModernHR,API access denied due to unauthorized credentials for user 5914,Security Alert,bert
'''

import pandas as pd
import io

file_like_object = io.StringIO(data)
df = pd.read_csv(file_like_object, names=['number', 'date', 'system', 'description', 'target_label', 'person'])

print('data type for column &quot;date&quot;:', df['date'].dtype)

# convert string with date to datetime object
df['date'] = pd.to_datetime(df['date'])   

print('data type for column &quot;date&quot;:', df['date'].dtype)

#selected = df[ df['target_label']=='Security Alert' ]
#selected = selected.sort_values(by=&quot;date&quot;)
#selected = selected[-3:]

selected = df[ df['target_label'] == 'Security Alert' ].sort_values(by='date')[-3:]
print(selected)
</code></pre>
<p>Result</p>
<pre class=""lang-none prettyprint-override""><code>data type for column &quot;date&quot;: object
data type for column &quot;date&quot;: datetime64[ns]

   number                date           system                                        description    target_label person
1    2213 2025-09-01 10:30:00  AnalyticsEngine  Security alert: suspicious activity from 192.1...  Security Alert   bert
7      43 2025-11-22 11:06:00    BillingSystem  Abnormal system behavior on server 40, potenti...  Security Alert   bert
0    1760 2025-12-29 20:33:00        ModernCRM  Unauthorised access attempt from 192.168.99.79...  Security Alert   bert
</code></pre>
<hr />
<p><strong>EDIT:</strong></p>
<p>If you need it as timestamps</p>
<pre><code># divide by `10**9` to convert `nanoseconds` to `seconds`

timestamps = selected['date'].astype(int) // (10**9)
print(timestamps)

# check if timestamp gives correct datetime (BTW: problem can be timezone)

import datetime

print(datetime.datetime.fromtimestamp( timestamps.iloc[0] ))
</code></pre>
<p>See other methods on Stackoverflow: <a href=""https://stackoverflow.com/questions/54313463/pandas-datetime-to-unix-timestamp-seconds"">pandas datetime to unix timestamp seconds</a></p>
","0","Answer"
"79458587","79458545","<p>You can use a list comprehension.</p>
<ul>
<li>For each grouping column.
<ul>
<li>Group by <code>['Cohort', col]</code>.</li>
<li>Count IDs</li>
<li>Rename grouped column to &quot;Description&quot;</li>
<li>Add Variable column</li>
</ul>
</li>
<li><code>pd.concat()</code> to combine dataframes.</li>
<li>Sort</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>col_mapping = {'Program Size': 'size', 'Rating': 'rating'}

final_df = pd.concat([
    df.groupby(['Cohort', col], as_index=False)
    .agg(Frequency=('ID', 'count'))
    .rename(columns={col: 'Description'})
    .assign(Variable=col_mapping[col])
    [['Cohort', 'Variable', 'Description', 'Frequency']]
    for col in col_mapping.keys()
], ignore_index=True).sort_values(['Cohort', 'Variable'])
</code></pre>
","1","Answer"
"79458605","79458590","<p>The xpath needs quotes for attribute values if they include special characters such as &quot;#&quot;.</p>
<pre class=""lang-py prettyprint-override""><code>a = pa.read_xml(
    path_or_buffer=filepath,
    xpath=(
        &quot;//doc:experimentSection[@sectionName='Experiment#1']&quot;
        &quot;/doc:plateSection/doc:microplateData&quot;
        &quot;/doc:wave[@waveID='1']/doc:well[@wellID]/doc:oneDataSet&quot;
    ),
    namespaces={&quot;doc&quot;: &quot;http://moleculardevices.com/microplateML&quot;}
)
</code></pre>
","0","Answer"
"79459135","79434242","<pre><code>import pandas as pd
import numpy as np


df1 = pd.DataFrame({'area1_index': [0, 1, 2, 3, 4, 5], 
                    'area1_name': ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO']})

df2 = pd.DataFrame({'area2_index': [0, 1, 2, 3, 4, 5, 6], 
                    'area2_name': ['MN', 'AL', 'CT', 'TX', 'AK', 'AR', 'CA']})


df1_np = {col : df1[col].to_numpy() for col in df1.columns}
df2_np = {col : df2[col].to_numpy() for col in df2.columns}

area1_index,area1_name = df1_np['area1_index'], df1_np['area1_name'] 
area2_index,area2_name = df2_np['area2_index'], df2_np['area2_name'] 

unique_areas = np.union1d(area1_name,area2_name)

# Use pd.Series for fast lookup mapping
area1_map = pd.Series(area1_index, index = area1_name)
area2_map = pd.Series(area2_index, index = area2_name)

area1_final = area1_map.reindex(unique_areas).values
area2_final = area2_map.reindex(unique_areas).values

new_merged_df = pd.DataFrame({
'area_name' : unique_areas,
'area1_idx' : area1_final,
'area2_idx' : area2_final
})
'''
  area_name  area1_idx  area2_idx
0        AK        1.0        4.0
1        AL        0.0        1.0
2        AR        3.0        5.0
3        AZ        2.0        NaN
4        CA        4.0        6.0
5        CO        5.0        NaN
6        CT        NaN        2.0
7        MN        NaN        0.0
8        TX        NaN        3.0
'''
</code></pre>
","0","Answer"
"79459794","79459705","<p><code>read_csv</code> allows to use <code>regex</code> in <code>sep=</code> so you can use <code>|</code> (as <code>OR</code>) to define many chars (or even strings) as delimiters.</p>
<p>Because there are spaces in text which shouldn't be used as separator so I use <code>, </code> (comma + space) and <code>\s{2,}</code> (2 or more whitespaces) to detect correct places (but this keeps date and time as single string):</p>
<pre><code>sep=r', |\s{2,}', engine='python'
</code></pre>
<hr />
<p>Minimal working code.</p>
<p>I use <code>io</code> only to create <code>file like object</code> in memory so everyone can simply copy and test it - but you could use filename (or even directly url to raw data <code>read_csv(&quot;https://raw.githubusercontent.com/logpai/loghub/refs/heads/master/Windows/Windows_2k.log&quot;, ...)</code>)</p>
<pre class=""lang-py prettyprint-override""><code>text = r'''
2016-09-28 04:30:30, Info                  CBS    Loaded Servicing Stack v6.1.7601.23505 with Core: C:\Windows\winsxs\amd64_microsoft-windows-servicingstack_31bf3856ad364e35_6.1.7601.23505_none_681aa442f6fed7f0\cbscore.dll
2016-09-28 04:30:31, Info                  CSI    00000001@2016/9/27:20:30:31.455 WcpInitialize (wcp.dll version 0.0.0.6) called (stack @0x7fed806eb5d @0x7fef9fb9b6d @0x7fef9f8358f @0xff83e97c @0xff83d799 @0xff83db2f)
2016-09-28 04:30:31, Info                  CSI    00000002@2016/9/27:20:30:31.458 WcpInitialize (wcp.dll version 0.0.0.6) called (stack @0x7fed806eb5d @0x7fefa006ade @0x7fef9fd2984 @0x7fef9f83665 @0xff83e97c @0xff83d799)
2016-09-28 04:30:31, Info                  CSI    00000003@2016/9/27:20:30:31.458 WcpInitialize (wcp.dll version 0.0.0.6) called (stack @0x7fed806eb5d @0x7fefa1c8728 @0x7fefa1c8856 @0xff83e474 @0xff83d7de @0xff83db2f)
2016-09-28 04:30:31, Info                  CBS    Ending TrustedInstaller initialization.
2016-09-28 04:30:31, Info                  CBS    Starting the TrustedInstaller main loop.
2016-09-28 04:30:31, Info                  CBS    TrustedInstaller service starts successfully.
2016-09-28 04:30:31, Info                  CBS    SQM: Initializing online with Windows opt-in: False
2016-09-28 04:30:31, Info                  CBS    SQM: Cleaning up report files older than 10 days.
2016-09-28 04:30:31, Info                  CBS    SQM: Requesting upload of all unsent reports.
'''

import pandas as pd
import io

file_like_object = io.StringIO(text)

df = pd.read_csv(file_like_object, sep=r', |\s{2,}', engine='python', names=['date', 'info', 'other', 'text'])

print(df)
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>                  date  info other                                               text
0  2016-09-28 04:30:30  Info   CBS  Loaded Servicing Stack v6.1.7601.23505 with Co...
1  2016-09-28 04:30:31  Info   CSI  00000001@2016/9/27:20:30:31.455 WcpInitialize ...
2  2016-09-28 04:30:31  Info   CSI  00000002@2016/9/27:20:30:31.458 WcpInitialize ...
3  2016-09-28 04:30:31  Info   CSI  00000003@2016/9/27:20:30:31.458 WcpInitialize ...
4  2016-09-28 04:30:31  Info   CBS            Ending TrustedInstaller initialization.
5  2016-09-28 04:30:31  Info   CBS           Starting the TrustedInstaller main loop.
6  2016-09-28 04:30:31  Info   CBS      TrustedInstaller service starts successfully.
7  2016-09-28 04:30:31  Info   CBS  SQM: Initializing online with Windows opt-in: ...
8  2016-09-28 04:30:31  Info   CBS  SQM: Cleaning up report files older than 10 days.
9  2016-09-28 04:30:31  Info   CBS      SQM: Requesting upload of all unsent reports.
</code></pre>
<hr />
<p>Other idea: pandas has function <code>read_fwf</code> to read fixed-width formatted data - and you can use width of columns to split it in correct way.</p>
<p>It seems it automatically splits your data but it needs to remove <code>,</code> from date after reading. But it also splits date and time as separate columns. It treats date (without time) as index.</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_fwf(file_like_object, names=['date', 'info', 'other', 'text'])

df['date'] = df['date'].str.rstrip(',')

print(df)
</code></pre>
<p>Using <code>widths</code> you can resolve this problem - but this needs to calculate all widths manually. And this still needs to remove <code>,</code></p>
<pre class=""lang-py prettyprint-override""><code>df = pd.read_fwf(file_like_object, names=['date', 'info', 'other', 'text'], widths=[21,22,7,1000])

df['date'] = df['date'].str.rstrip(',')
</code></pre>
<p>Probably using <code>colspecs</code> you can define <code>[start,end[</code> for every column and skip <code>,</code> but this still need to calculate values <code>start</code>,<code>end</code> manually.</p>
<hr />
<p>Other idea: sometimes it is simpler to read it as normal text and use <code>line.split()</code> or slicing <code>line[start:end]</code> (and other string functions, and also <code>if/elif/else</code>) to split columns and later use <code>DataFrame(data)</code>. It can be useful when file has more complex data (ie. text in many lines)</p>
<hr />
<p>Doc: <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer"">read_csv</a>, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html"" rel=""nofollow noreferrer"">read_fwf</a></p>
","0","Answer"
"79460049","79457388","<h4>Input data and code issues</h4>
<p>There's two problems with your example.</p>
<ol>
<li><p>Your input has an issue with it. In column 4, row 1: &quot;0.5 , Z20/1 ; 120&quot;
the second delimiter is a semicolon (&quot;;&quot;) instead of a comma (&quot;,&quot;). <br />
If your raw data has these issues, then you may want to clean or
preprocess it initially; for instance, converting semicolons to
commas.</p>
</li>
<li><p>Your split dataframe has columns <code>[0, 1, 2]</code>. You'll want to modify the columns to match your output of <code>[7, 8, 9]</code>. <br />
You can do this with the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_axis.html"" rel=""nofollow noreferrer""><code>set_axis</code></a> method. As in:</p>
<pre class=""lang-py prettyprint-override""><code>splitDf = var002_ar1.iloc[:, 4].str.split(', ', n=2, expand=True).set_axis([7, 8, 9], axis=1)
</code></pre>
<p>Now you can set it as you did before:</p>
<pre class=""lang-py prettyprint-override""><code>var002_ar1.loc[var002_ar1.iloc[:, 6] == 'LUB', [7, 8, 9]] = splitDf
</code></pre>
<p><sub><strong>Note</strong>: with your input issue in 1., <code>set_axis</code> will error as the semicolon will result in the split dataframe having only two columns instead of three (<code>Length mismatch</code>). You will have to fix that first.</sub></p>
</li>
</ol>
<h4>Fixed results</h4>
<p>Putting the new input and new code all together:</p>
<p>input:</p>
<pre class=""lang-none prettyprint-override""><code>20240801    CASH MAN1   120.00  Z20/1   0.5 , Z20/1 , 120   1   LUB
20240801    PAYTM   15720.00    CASH MAN1   0   1   JRNLCR
20240801    PAYTM   81343.00    CASH MAN2   0   2   JRNLCR
</code></pre>
<p>code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

var001_sf = input(&quot;Enter file name to import : &quot;)
var002_sf = var001_sf + &quot;.csv&quot;
var002_ar1 = pd.read_csv(var002_sf, sep='\t', header=None)

var002_ar1 = var002_ar1.dropna(axis=0)  # remove rows with na value
var002_ar1.iloc[:, [0, 1, 2, 3, 4, 5, 6]] = var002_ar1.iloc[:, [0, 1, 2, 3, 4, 5, 6]].map(str)

splitDf = var002_ar1.iloc[:, 4].str.split(', ', n=2, expand=True).set_axis([7, 8, 9], axis=1)
var002_ar1.loc[var002_ar1.iloc[:, 6] == 'LUB', [7, 8, 9]] = splitDf

var002_ar1.to_csv(&quot;t250221.csv&quot;, sep='\t')
</code></pre>
<p>output:</p>
<pre class=""lang-none prettyprint-override""><code>    0   1   2   3   4   5   6   7   8   9
0   20240801    CASH MAN1   120.0   Z20/1   0.5 , Z20/1 , 120   1   LUB 0.5     Z20/1   120
1   20240801    PAYTM   15720.0 CASH MAN1   0   1   JRNLCR          
2   20240801    PAYTM   81343.0 CASH MAN2   0   2   JRNLCR          
</code></pre>
","0","Answer"
"79461117","79461108","<p>Assuming you're starting off with, e.g.</p>
<pre class=""lang-py prettyprint-override""><code>df.columns = [&quot;Miscellaneous group | 00002928&quot;,  &quot;Alcoholic Beverages | 0000292&quot;,   &quot;Animal fats group | 000029&quot;]
</code></pre>
<p>The simplest solution looks like it would be to use a list comprehension to iterate over the column names and split on the <code>|</code> in your string and keep the first part of the resulting list, so:</p>
<pre class=""lang-py prettyprint-override""><code>df.columns = [col.split(&quot; | &quot;)[0] for col in columns]
</code></pre>
<p>This returns:</p>
<pre class=""lang-py prettyprint-override""><code>['Miscellaneous group', 'Alcoholic Beverages', 'Animal fats group']
</code></pre>
<p>Alternatively, you could do this with a regex:</p>
<pre class=""lang-py prettyprint-override""><code>import re

df.columns = [re.sub(r'\s*\|.*', '', col) for col in columns]
</code></pre>
<p>This looks for a string that begins with whitespace, followed by <code>|</code>, followed by anything and replaces it all with an empty string.</p>
<p>Final alternative:</p>
<pre class=""lang-py prettyprint-override""><code>columns = [re.sub(r'\s*\d+$', '', s) for s in columns]
</code></pre>
<p>This looks for whitespace followed by digits at the end of each string, so this would remove the trailing digits regardless of what preceded them (in case the <code>|</code> isn't always present), so it would produce:</p>
<pre class=""lang-py prettyprint-override""><code>['Miscellaneous group |', 'Alcoholic Beverages |', 'Animal fats group |']
</code></pre>
","0","Answer"
"79461145","79461108","<p>Using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>df.columns.str.split</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>columns = [&quot;Miscellaneous group | 00002928&quot;,  
           &quot;Alcoholic Beverages | 0000292&quot;,
           &quot;Animal fats group | 000029&quot;]

df = pd.DataFrame(columns=columns)

df.columns = df.columns.str.split(r'\s+\|', regex=True).str[0]
</code></pre>
<p>Or <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html"" rel=""nofollow noreferrer""><code>df.columns.str.replace</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>df.columns = df.columns.str.replace(r'\s+\|.*$', '', regex=True)
</code></pre>
<p>Also possible via <a href=""https://docs.python.org/3/library/functions.html#map"" rel=""nofollow noreferrer""><code>map</code></a> and <a href=""https://docs.python.org/3/library/re.html#re.sub"" rel=""nofollow noreferrer""><code>re.sub</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>import re

df.columns = map(lambda x: re.sub(r'\s+\|.*$', '', x), df.columns)
</code></pre>
<p>With <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>df.rename</code></a> you could apply logic like:</p>
<pre class=""lang-py prettyprint-override""><code>df = df.rename(columns=lambda x: x.split(' |')[0])
</code></pre>
<p>Or indeed via <a href=""https://docs.python.org/3/library/re.html#re.split"" rel=""nofollow noreferrer""><code>re.split</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>df = df.rename(columns=lambda x: re.split(r'\s+\|', x)[0])
</code></pre>
<p>For the regex pattern, see <a href=""https://regex101.com/r/r6SSoU/1"" rel=""nofollow noreferrer"">regex101</a>.</p>
","1","Answer"
"79462817","79461783","<p>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html"" rel=""nofollow noreferrer""><code>df.resample</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.aggregate.html"" rel=""nofollow noreferrer""><code>Resampler.agg</code></a> with <a href=""https://pandas.pydata.org/docs/user_guide/groupby.html#named-aggregation"" rel=""nofollow noreferrer"">named aggregation</a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>df.reset_index</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame(data)

keys = ['open', 'close', 'low', 'high']
values = ['first', 'last', 'min', 'max']

df_r = (df.resample('D', on='time')
        .agg(**{k: ('price', v) for k, v in zip(keys, values)})
        ).reset_index(drop=True)

inc = df_r.close &gt; df_r.open
dec = df_r.open &gt; df_r.close

p = figure()
p.segment(df_r.index, df_r.high, df_r.index, df_r.low, color=&quot;black&quot;)
p.vbar(df_r.index[dec], 0.6, df_r.open[dec], df_r.close[dec], color=&quot;#eb3c40&quot;)
p.vbar(df_r.index[inc], 0.6, df_r.open[inc], df_r.close[inc], fill_color=&quot;white&quot;,
       line_color=&quot;#49a3a3&quot;, line_width=2)
show(p)
</code></pre>
<p>Result:</p>
<p><a href=""https://i.sstatic.net/3GLbVlDel.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3GLbVlDel.png"" alt=""same plot"" /></a></p>
<p><strong>Intermediate</strong></p>
<pre class=""lang-py prettyprint-override""><code># `df_r` before `reset_index`: df.resample('D', on='time').agg(...)

            open  close  low  high
time                              
2025-01-01    10     40    5    40
2025-01-02    40     20   10    50
2025-01-03    20     30   20    35
</code></pre>
<p>Note that <code>kind</code> for <code>df.resample</code> is deprecated since 2.2.0. Just use <code>datetime</code>, which you already have.</p>
<hr />
<p>To get proper dates on the x-axis:</p>
<ul>
<li>Set a <code>width</code> for <a href=""https://docs.bokeh.org/en/latest/docs/reference/models/glyphs/vbar.html#bokeh.models.VBar"" rel=""nofollow noreferrer""><code>vbar</code></a> in milliseconds (0.6 * 24 hours to mimick your initial width).</li>
<li>Use <code>x_axis_type='datetime'</code> inside <a href=""https://docs.bokeh.org/en/latest/docs/reference/plotting/figure.html"" rel=""nofollow noreferrer""><code>figure</code></a>.</li>
<li>Use <a href=""https://docs.bokeh.org/en/latest/docs/reference/models/tickers.html#bokeh.models.DaysTicker"" rel=""nofollow noreferrer""><code>DaysTicker</code></a> and <a href=""https://docs.bokeh.org/en/latest/docs/reference/models/formatters.html#bokeh.models.DatetimeTickFormatter"" rel=""nofollow noreferrer""><code>DatetimeTickFormatter</code></a> to get dates for the major x-axis ticks.</li>
<li>In this case, don't use <code>df.reset_index</code>.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>from bokeh.models import DatetimeTickFormatter, DaysTicker

df = pd.DataFrame(data)

keys = ['open', 'close', 'low', 'high']
values = ['first', 'last', 'min', 'max']

df_r = (df.resample('D', on='time')
        .agg(**{k: ('price', v) for k, v in zip(keys, values)})
        )

inc = df_r.close &gt; df_r.open
dec = df_r.open &gt; df_r.close
w = 0.6*24*60*60*1000

p = figure(x_axis_type='datetime')

start = 1
end = len(df)+1
p.xaxis.ticker = DaysTicker(days=list(range(start, end)))
p.xaxis.formatter = DatetimeTickFormatter(days='%Y-%m-%d')

p.segment(df_r.index, df_r.high, df_r.index, df_r.low, color=&quot;black&quot;)
p.vbar(df_r.index[dec], w, df_r.open[dec], df_r.close[dec], color=&quot;#eb3c40&quot;)
p.vbar(df_r.index[inc], w, df_r.open[inc], df_r.close[inc], fill_color=&quot;white&quot;,
       line_color=&quot;#49a3a3&quot;, line_width=2)
show(p)
</code></pre>
<p>Result:</p>
<p><a href=""https://i.sstatic.net/pV4dzXfgl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pV4dzXfgl.png"" alt=""plot with dates x-axis"" /></a></p>
","1","Answer"
"79463217","79460679","<p>I'm not familiar with <code>pdfplumber</code>, but for your last three lines, I would avoid the <code>iterrows</code> loop, by using some <code>pandas</code> built-in methods :</p>
<pre class=""lang-py prettyprint-override""><code>    [...]
    df = pd.DataFrame(tables[1:], columns=tables[0])
    matching = df[df.isin([rut]).any(axis=1)]
    return matching
[...]
</code></pre>
<p>It will look for <code>rut</code> value in all columns, and return all the rows that contain the value in a new dataframe.</p>
<p>The interesting thing is that you can search for a number of things at once with :</p>
<pre class=""lang-py prettyprint-override""><code>df[df.isin([value_1, value_2, &quot;string_1&quot;,&quot;string_2&quot;]).any(axis=1)]
</code></pre>
<p>Give it a try !</p>
<p>However, the fastest method can actually depend on the type of match (strings, floats, ...).</p>
","0","Answer"
"79464538","79464463","<p>Hoping this helps, using data similar to the examples you shared.</p>
<pre><code>data = {
    'name': ['John', 'John', 'Jane', 'Jane', 'Doe', 'Doe'],
    'city': ['LA', 'LA', 'SF', 'SF', 'SD', 'SD'],
    'item': ['Peanut Butter', 'Jelly', 'Peanut Butter', 'Peanut Butter', 
    'Jelly', 'Jelly']
}

df = pd.DataFrame(data)
</code></pre>
<p>We can then create a dictionary of the string pairs from <strong>name</strong> and <strong>city</strong> with the values being a list of the items associated with the pairs.</p>
<pre><code>unique_dict = df.groupby(['name', 'city'])['item'].apply(list).to_dict()
</code></pre>
<p>Once we have this we define a function to handle the logic of what each pair needs and can apply it to the dataframe.</p>
<pre><code>def determine_needs(items):
    if 'Peanut Butter' in items and 'Jelly' in items:
        return None
    elif 'Peanut Butter' in items:
        return 'Jelly'
    elif 'Jelly' in items:
        return 'Peanut Butter'
    else:
        return None

df['needs'] = df.apply(lambda row: determine_needs(unique_dict[(row['name'], row['city'])]), axis=1)
</code></pre>
","1","Answer"
"79464648","79464314","<p>This is a <a href=""https://github.com/pandas-dev/pandas/issues/54654"" rel=""nofollow noreferrer"">bug</a> that has been <a href=""https://github.com/pandas-dev/pandas/pull/54687"" rel=""nofollow noreferrer"">fixed</a> in <a href=""https://pandas.pydata.org/docs/whatsnew/v2.2.0.html#conversion"" rel=""nofollow noreferrer"">pandas 2.2.0</a>:</p>
<blockquote>
<p>Bug in <code>DataFrame.astype()</code> when called with <code>str</code> on unpickled array - the array might change in-place (<a href=""https://github.com/pandas-dev/pandas/issues/54654"" rel=""nofollow noreferrer"">GH 54654</a>)</p>
</blockquote>
<p>As noted by <a href=""https://github.com/Itayazolay"" rel=""nofollow noreferrer"">Itayazolay</a> in the <a href=""https://github.com/pandas-dev/pandas/pull/54687"" rel=""nofollow noreferrer"">PR</a>, regarding the pickle <a href=""https://github.com/pandas-dev/pandas/issues/54654#issue-1858285780"" rel=""nofollow noreferrer"">MRE</a> used there:</p>
<blockquote>
<p>The problem is not exactly with pickle, it's just a quick way to reproduce the problem.<br>
The problem is that the code here attempts to check if two arrays have the same memory (or share memory) and it does so incorrectly - <code>result is arr</code><br>
See <a href=""https://github.com/numpy/numpy/issues/24478"" rel=""nofollow noreferrer"">numpy/numpy#24478</a> for more technical details.</p>
</blockquote>
<p>If you're using a version &lt; 2.2 and cannot upgrade, you could try manually applying the fix mentioned in the PR and recompiling <code>&quot;.../pandas/_libs/lib.pyx&quot;</code>.</p>
<p>At <a href=""https://github.com/pandas-dev/pandas/blob/2.0.x/pandas/_libs/lib.pyx#L759-L760"" rel=""nofollow noreferrer"">#L759</a>:</p>
<pre class=""lang-py prettyprint-override""><code>    if copy and result is arr:
        result = result.copy()
</code></pre>
<p>Required change:</p>
<pre class=""lang-py prettyprint-override""><code>    if copy and (result is arr or np.may_share_memory(arr, result)):
        result = result.copy()
</code></pre>
<p>There are now some extra comments in <code>&quot;.../pandas/_libs/lib.pyx&quot;</code>, version 2.3.x, together with adjusted logic. See <a href=""https://github.com/pandas-dev/pandas/blob/2.3.x/pandas/_libs/lib.pyx#L777-L785"" rel=""nofollow noreferrer"">#L777-L785</a>:</p>
<pre class=""lang-py prettyprint-override""><code>    if result is arr or np.may_share_memory(arr, result):
        # if np.asarray(..) did not make a copy of the input arr, we still need
        #  to do that to avoid mutating the input array
        # GH#54654: share_memory check is needed for rare cases where np.asarray
        #  returns a new object without making a copy of the actual data
        if copy:
            result = result.copy()
        else:
            already_copied = False
</code></pre>
","2","Answer"
"79465222","79464768","<p>quantize() converts the data to a binary format</p>
<p>You have to load the file with a prefix quantized
When you want to load, the &quot;quantized://&quot; prefix tells CatBoost to expect binary data</p>
<pre><code># Load from Drive
pool2 = cb.Pool(f&quot;quantized://path_to_dir/{'cbpool'}&quot;) 

# You can verify the loaded pool 
print(&quot;Number of features:&quot;, pool2.num_col())
print(&quot;Number of samples:&quot;, pool2.num_row())
</code></pre>
<p>Note: quantized:// is a protocol identifier</p>
","1","Answer"
"79465480","79464633","<p>Should be pretty simple to just iterate through the rows skipping every 1st row and copy the value of the same cell offset by 1 row (up).<br>
Of course it could also be done by looping the value cells and copy each value to the same cell offset by 1 row (down).
<sub>If there are more columns after K you can also set the max column for the iteration, but otherwise the max will default to the last used column.
</sub></p>
<pre><code>
...

## Iterate the rows, starting from row 3 and only need columns starting at Column G
for row in ws.iter_rows(min_row=3, min_col=7):
    if row[0].row % 2 == 0:  # If its an even row skip  
        continue
    else:
        for cell in row:  # For the cells in colunn G to the last column (K) 
            # Copy the cell value of the cell 1 row above 
            cell.value = cell.offset(row=-1).value


...

</code></pre>
<p>Example Sheet updated<br>
[<img src=""https://i.sstatic.net/xFSX6H9i.png"" alt=""Complete Sheet1"" /></p>
<p>Filtered on A1.results.0.test1<br>
<a href=""https://i.sstatic.net/08SoW0CY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/08SoW0CY.png"" alt=""Filtered on A1.results.0.test1"" /></a></p>
","2","Answer"
"79466767","79436039","<p>I'm updating my previous response. In @mozway's <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html"" rel=""nofollow noreferrer"">KDTree</a> solution
@Shaun Han you can use below lines of code for similar phase diagram with clear line boundaries.</p>
<pre><code># Plot
fig, ax = plt.subplots(figsize=(8, 8))

for (name, color), coords in df3.groupby(['label', 'color'])[['x', 'y']]:
    polygon = shapely.convex_hull(MultiPoint(coords.to_numpy()))
    
    # Fill category with color
    ax.fill(*polygon.exterior.xy, color=color, alpha=0.9, edgecolor='black', linewidth=1)

    # Annotate category name
    ax.annotate(name, polygon.centroid.coords[0], ha='center', va='center', fontsize=10, color='black', weight='bold')

ax.set_xlabel(&quot;Log pO2(g)&quot;)
ax.set_ylabel(&quot;Log pS2O(g)&quot;)
ax.set_title(&quot;Fe-O-S Phase Stability Diagram with Black Boundaries&quot;)

plt.show()
</code></pre>
<p>Output:
<a href=""https://i.sstatic.net/TMLc6TdJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TMLc6TdJ.png"" alt=""phase diagram"" /></a></p>
<p>updating this line helped me to get clear line boundaries:</p>
<pre><code>ax.fill(*polygon.exterior.xy, color=color, alpha=0.9, edgecolor='black', linewidth=1)
</code></pre>
","1","Answer"
"79466954","79466365","<p>Try with .str accessor: <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.Series.str.html</a></p>
<pre><code>df = pd.read_csv('test.csv')
print(df)
df['First_name'] = df['First_name'].str.replace('\\','')
df
</code></pre>
<p>output:</p>
<pre><code>  First_name job_level
0   Andrew \       SSE
1       Kyle        SE
First_name  job_level
0   Andrew  SSE
1   Kyle    SE
</code></pre>
","0","Answer"
"79467099","79467071","<p>In:</p>
<pre><code>am = pd.DataFrame(0, columns=df[&quot;Worker&quot;], index=df[&quot;Worker&quot;])
# This way, it is impossible that the dataframe is not square,
</code></pre>
<p>your DataFrame is indeed square, but when you later assign values in the loop, if you have a manager that is not in &quot;Worker&quot;, this will create a new row:</p>
<pre><code>am.at[row[&quot;manager&quot;], row[&quot;Worker&quot;]]
</code></pre>
<p>Better avoid the loop, use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html"" rel=""nofollow noreferrer""><code>crosstab</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>reindex</code></a> on the whole set of nodes:</p>
<pre><code>am = pd.crosstab(df['manager'], df['Worker'])
nodes = am.index.union(am.columns)
am = am.reindex(index=nodes, columns=nodes, fill_value=0)
</code></pre>
<p>Even better, if you don't really need the adjacency matrix, directly create the graph with <a href=""https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html"" rel=""nofollow noreferrer""><code>nx.from_pandas_edgelist</code></a>:</p>
<pre><code>G = nx.from_pandas_edgelist(df, source='manager', target='Worker',
                            create_using=nx.DiGraph)
</code></pre>
<p>Example:</p>
<pre><code># input
df = pd.DataFrame({'manager': ['A', 'B', 'A'], 'Worker': ['D', 'E', 'F']})

# adjacency matrix
   A  B  D  E  F
A  0  0  1  0  1
B  0  0  0  1  0
D  0  0  0  0  0
E  0  0  0  0  0
F  0  0  0  0  0

# adjacency matrix with your code
Worker    D    E    F
Worker               
D       0.0  0.0  0.0
E       0.0  0.0  0.0
F       0.0  0.0  0.0
A       1.0  NaN  1.0  # those rows are created 
B       NaN  1.0  NaN  # after initializing am
</code></pre>
<p>Graph:</p>
<p><a href=""https://i.sstatic.net/1d924X3L.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1d924X3L.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79467699","79436611","<p>It seems that, indeed, your dataset uses more than one single format for dates. For one, you can almost always ignore <code>00:00</code>, since it's the default hour when you are parsing a date to either <code>datetime.datetime</code> or <code>pd.to_datetime</code>. The difference between <code>00:00:00</code> and <code>00:00</code> is just the formatting of both excel and .csv files and is not itself an error. The issue would be, inmy opinion, handle both formats <code>YY/mm/dd</code> and <code>dd/mm/YY</code>. Take into account that excel does not necessarily need to have one single data type for a column. This means that <code>2025-08-01 00:00:00</code> is interpreted as a date whereas <code>13/01/2025</code> is a <em>string</em>. The same thing happens with the .csv file. <code>01/09/2025 00:00</code> is a datetime and <code>13/01/2025</code> is a <em>string</em>.</p>
<p>What you need to do is parse the correct format to pd.to_datetime like this.</p>
<pre><code>df[&quot;Asiento contable (Fecha de contabilización)&quot;] = pd.to_datetime(df[&quot;Asiento contable (Fecha de contabilización)&quot;], format='mixed', dayfirst=True)
</code></pre>
<p>as stayed in the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer"">docs</a></p>
","0","Answer"
"79467870","79467796","<p>You need to use the <code>|</code> and <code>&amp;</code> operators instead of <code>or</code> and <code>and</code>. The former performs bitwise operation. The latter are logical operators and return <em>True</em> or <em>False</em>. You are also missing a closing parenthesis before groupby operation.</p>
<pre><code>df = pd.DataFrame({
    'schoolID':['schoolID1', 'schoolID2', 'schoolID3'], 
    'SchoolName':['school1', 'school2', 'school3'], 
    'StudentID':[123, 456, 789], 
    'status':['open', 'closed', 'unknown'], 
    'state':['FL', 'NY', 'CA']})




df = df.where(((df['status']==&quot;open&quot;) | (df['status']==&quot;closed&quot;)) 
&amp; (df['state']!=&quot;FL&quot;)).groupby(['schoolID', 'SchoolName']).agg(count=('StudentID', 'count'))

print(df)
</code></pre>
<p>Output</p>
<pre><code>schoolID  SchoolName   count    
schoolID2 school2         1
</code></pre>
","2","Answer"
"79467876","79467663","<p>As stated by others, this question is too generic and doesn't provide much info about the issue. However, the best thing you can do is to simply read all files separately and concat them without creating said list like that and appending constantly.</p>
<pre><code>df1 = pd.read_csv(path_to_file1, ...)
df2 = pd.read_csv(path_to_file2, ...)
df3 = pd.read_csv(path_to_file3, ...)
df4 = pd.read_csv(path_to_file4, ...)
df5 = pd.read_csv(path_to_file5, ...)
df6 = pd.read_csv(path_to_file6, ...)
df7 = pd.read_csv(path_to_file7, ...)
df8 = pd.read_csv(path_to_file8, ...)

df_final = pd.concat(
  [df1, df2, df3, df4, df5, df6, df7, df8],
  **kwargs
)
</code></pre>
<p>Or you could just concatenate 2 files per execution and store the resulting file and do it recursively until only two files are to concat. Note that, when I mean recursively, I don't mean coding a recursive function, since it would be too memory costly. Create a script to concat 2 files and store the result and then use that result as one of the dfs to concat in the next execution of the script.</p>
","0","Answer"
"79468002","79467698","<p>I think your issue is that the dtype is float for those columns you want changed.</p>
<p>The following assumes every column other than date is a column you want changed. If it's not, you can use the following as a starting point.</p>
<pre><code>import io

data = '''    Date    Solar   MaxRH   AvgAirTemp
0   3/1/1983    -9.00   -9.0    -99.00
1   3/2/1983    -9.00   -9.0    0.31
2   3/3/1983    -9.00   -9.0    -99.00
3   3/4/1983    -9.00   -9.0    8.62
4   3/5/1983    19.97   64.6    8.91'''
df = pd.read_csv(io.StringIO(data), sep=' \s+', engine='python')
df.set_index('Date', inplace=True)
print(df)

new_df = df[df &gt; -9]
new_df

    Solar   MaxRH   AvgAirTemp
Date            
3/1/1983    NaN NaN NaN
3/2/1983    NaN NaN 0.310
3/3/1983    NaN NaN NaN
3/4/1983    NaN NaN 8.620
3/5/1983    19.970  64.600  8.910
</code></pre>
<p>If you don't want to reset index or need to pick and choose columns, you use .iloc. The following selects every row, and every column starting at 1 (0 is fisrt column, Date)</p>
<pre><code>df.iloc[ : , 1:][df.iloc[ : , 1:] &gt; -9]
</code></pre>
","0","Answer"
"79468028","79467944","<p>A possible solution, which <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.compare.html"" rel=""nofollow noreferrer""><code>compares</code></a> the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mask.html"" rel=""nofollow noreferrer""><code>masked</code></a> dataframes:</p>
<pre><code>threshold = 0.5
m = (df1-df2).abs().le(threshold)
print(df1.mask(m).compare(df2.mask(m), result_names = ('df1','df2')))
</code></pre>
<p>Output:</p>
<pre><code># threshold = 0.5
Empty DataFrame
Columns: []
Index: []

# threshold = 0.1
     B     
   df1  df2
1  2.0  2.2
</code></pre>
","3","Answer"
"79468139","79467944","<p>I took a look at the <code>.compare</code> source code and I think this is somehow similar and efficient for large datasets (my opinion)</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

# Create sample DataFrames
df1 = pd.DataFrame(np.random.randint(0, 100, size=(5000000, 10)), columns=[f'col{i}' for i in range(10)])
df2 = df1.copy()

# Introduce some differences
df2.iloc[1000, 2] = 999  # Change one value
df2.iloc[50000, 5] = 888  # Change another value


# Compute absolute difference mask
mask = (df1 - df2).abs() &gt; 5

df1_diff = (
    df1[mask] # get only the diffrent data (the rest will be NAN)
    .dropna(how='all') # remove the rows with no changes (all NaN)
    .melt(var_name='Column', ignore_index=False) # add the column name (where the diffrence is) into a column 
    .dropna() # remvoe the excess data (in the rows with diffrences some columns were equal ;D )
    .reset_index() # we need the index for later
)

df2_diff = (
    df2[mask]
    .dropna(how='all')
    .melt(var_name='Column', ignore_index=False)
    .dropna()
    .reset_index()
)

# Merge on index and column to align differences
df1_diff.merge(df2_diff, on=['index', 'Column'], how='inner', suffixes=['_df1', '_df2'])
</code></pre>
","1","Answer"
"79468152","79467944","<p>Depending on your ultimate goal, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.testing.assert_frame_equal.html"" rel=""nofollow noreferrer""><code>assert_frame_equal</code></a> with the <code>atol</code> parameter may work.</p>
<pre><code>from pandas.testing import assert_frame_equal

# specify dtypes for the reproducible example
# otherwise assert_frame_equal flags different dtypes (int vs. float)
df1 = pd.DataFrame({&quot;A&quot;: [1,1,1], &quot;B&quot;: [2,2,2], &quot;C&quot;: [3,3,3]}, dtype=float)
df2 = pd.DataFrame({&quot;A&quot;: [1,1,1], &quot;B&quot;: [2,2.2,2], &quot;C&quot;: [3,3,3]}, dtype=float)

assert_frame_equal(df1, df2, atol=0.5)
</code></pre>
","2","Answer"
"79468252","79467071","<p>First of all, your &quot;adjacency matrix&quot; is not the real one, but the &quot;incidence matrix&quot; indeed.</p>
<p>I didn't find a straightforward utility in <a href=""/questions/tagged/networkx"" class=""s-tag post-tag"" title=""show questions tagged &#39;networkx&#39;"" aria-label=""show questions tagged &#39;networkx&#39;"" rel=""tag"" aria-labelledby=""tag-networkx-tooltip-container"" data-tag-menu-origin=""Unknown"">networkx</a> that support generating the directed graph from the incidence matrix. However, with <a href=""/questions/tagged/igraph"" class=""s-tag post-tag"" title=""show questions tagged &#39;igraph&#39;"" aria-label=""show questions tagged &#39;igraph&#39;"" rel=""tag"" aria-labelledby=""tag-igraph-tooltip-container"" data-tag-menu-origin=""Unknown"">igraph</a> package in the <a href=""/questions/tagged/r"" class=""s-tag post-tag"" title=""show questions tagged &#39;r&#39;"" aria-label=""show questions tagged &#39;r&#39;"" rel=""tag"" aria-labelledby=""tag-r-tooltip-container"" data-tag-menu-origin=""Unknown"">r</a> environment, there is such functionality that can show how it should work. For example</p>
<pre class=""lang-r prettyprint-override""><code>library(igraph)

df &lt;- data.frame(
    manager = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;),
    worker = c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;)
)
am &lt;- table(df)
g &lt;- graph_from_biadjacency_matrix(am, directed = TRUE, mode = &quot;out&quot;)
plot(g)
</code></pre>
<p>where</p>
<pre class=""lang-r prettyprint-override""><code>&gt; print(df)
  manager worker
1       A      D
2       B      E
3       A      F

&gt; print(am)
       worker
manager D E F
      A 1 0 1
      B 0 1 0
</code></pre>
<p>such that <code>g</code> can be visualized as below</p>
<p><a href=""https://i.sstatic.net/rUM1oDFk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUM1oDFk.png"" alt=""enter image description here"" /></a></p>
<p>Again, the real &quot;adjacency matrix&quot; should look like this</p>
<pre><code>&gt; as_adjacency_matrix(g)
5 x 5 sparse Matrix of class &quot;dgCMatrix&quot;
  A B D E F
A . . 1 . 1
B . . . 1 .
D . . . . .
E . . . . .
F . . . . .
</code></pre>
","0","Answer"
"79468755","79468372","<p>One possible problem should be duplicated index values, possible solution is avoid loops with assign in <code>loc</code>:</p>
<pre><code>df = pd.DataFrame({'value': [10, 20, 30, 40]}, 
                  index=pd.to_datetime([
                  &quot;2021-01-01 00:00:00&quot;,
                  &quot;2021-01-01 00:00:00&quot;,
                  &quot;2021-01-01 00:30:00&quot;,
                  &quot;2021-01-01 01:00:00&quot;]))

#custom function
def some_function(x):
    return range(len(x))

def helper(group):
    group['results'] = some_function(group) 
    return group

out = df.groupby(pd.Grouper(freq=&quot;1h&quot;), group_keys=False).apply(helper)
print(out)
                     value  results
2021-01-01 00:00:00     10        0
2021-01-01 00:00:00     20        1
2021-01-01 00:30:00     30        2
2021-01-01 01:00:00     40        0
</code></pre>
<p>Another problem should be different length between length of groups and array/list returned from your custom function, here si solution for found this problematic data:</p>
<pre><code>df = pd.DataFrame({'value': [10, 20, 30, 40]}, 
                  index=pd.to_datetime([
                  &quot;2021-01-01 00:00:00&quot;,
                  &quot;2021-01-01 00:00:00&quot;,
                  &quot;2021-01-01 00:30:00&quot;,
                  &quot;2021-01-01 01:00:00&quot;]))

#simulate different lengths of lists returned from function
def some_function(x):
    if len(x) == 1:
        return range(len(x) * 2)
    else:
        return range(len(x))

def helper(group):
    print (group)
    print (f'Length of group is {len(group)}')
    print (f'Length of output from function is {len(some_function(group))}')
    group['results'] = some_function(group) 
    return group

out = df.groupby(pd.Grouper(freq=&quot;1h&quot;), group_keys=False).apply(helper)
# print(out)
</code></pre>
<hr />
<pre><code>                     value
2021-01-01 00:00:00     10
2021-01-01 00:00:00     20
2021-01-01 00:30:00     30
Length of group is 3
Length of output from function is 3
                     value
2021-01-01 01:00:00     40
Length of group is 1
Length of output from function is 2
</code></pre>
","0","Answer"
"79469085","79469073","<p>You can try use <a href=""https://stackoverflow.com/questions/15197673/using-pythons-eval-vs-ast-literal-eval"">ast.literal_eval()</a> to properly convert the string representations of lists to actual Python lists and
Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer"">explode()</a></p>
<pre><code>import pandas as pd
import ast


data = [
    {&quot;Book Name&quot;: &quot;Book 1&quot;, &quot;Languages&quot;: &quot;['Portuguese','English']&quot;},
    {&quot;Book Name&quot;: &quot;Book 2&quot;, &quot;Languages&quot;: &quot;['English','Japanese']&quot;},
    {&quot;Book Name&quot;: &quot;Book 3&quot;, &quot;Languages&quot;: &quot;['Spanish','Italian','English']&quot;}
]

df = pd.DataFrame(data)
df['Languages'] = df['Languages'].apply(ast.literal_eval)
exploded_df = df.explode('Languages')
language_to_books = exploded_df.groupby('Languages')['Book Name'].apply(list).to_dict()
print(language_to_books)
</code></pre>
<p>The Dict</p>
<pre><code>{
  &quot;English&quot;: [&quot;Book 1&quot;, &quot;Book 2&quot;, &quot;Book 3&quot;],
  &quot;Italian&quot;: [&quot;Book 3&quot;],
  &quot;Japanese&quot;: [&quot;Book 2&quot;],
  &quot;Portuguese&quot;: [&quot;Book 1&quot;],
  &quot;Spanish&quot;: [&quot;Book 3&quot;]
}
</code></pre>
","0","Answer"
"79469277","79469073","<p>If you're looking for <strong>faster and more memory-efficient</strong> solution then you can use <code>defaultdict</code>. Unlike <code>explode()</code> it doesn’t create unnecessary rows and <code>Iterates</code> over the DataFrame only once.</p>
<p>This will useful for <code>large datasets</code>.</p>
<pre><code>import pandas as pd
import ast
from collections import defaultdict

data = {
    &quot;Book Name&quot;: [&quot;Book 1&quot;, &quot;Book 2&quot;, &quot;Book 3&quot;],
    &quot;Languages&quot;: [&quot;['Portuguese','English']&quot;, &quot;['English','Japanese']&quot;, &quot;['Spanish','Italian','English']&quot;]
}

df = pd.DataFrame(data)
df[&quot;Languages&quot;] = df[&quot;Languages&quot;].apply(ast.literal_eval)

# Create a defaultdict to store books per language
language_dict = defaultdict(list)

for book, languages in zip(df[&quot;Book Name&quot;], df[&quot;Languages&quot;]):
    for lang in languages:
        language_dict[lang].append(book)

# Convert defaultdict to normal dic.
result = dict(language_dict)

# Print result
print(result)
</code></pre>
<p>Output:</p>
<pre><code>{'Portuguese': ['Book 1'],
 'English': ['Book 1', 'Book 2', 'Book 3'],
 'Japanese': ['Book 2'],
 'Spanish': ['Book 3'],
 'Italian': ['Book 3']}
</code></pre>
","0","Answer"
"79469303","79467265","<p>The code I've shared above was adapted for brevity and to not share confidential data. It turns out I opened a <code>pd.ExcelWriter</code> before calling the function and never closed it. Removing that line allows the code to save the workbook as expected. I still don't know why it raised no error, I would have noticed the issue much sooner.</p>
","0","Answer"
"79469377","79468954","<p>Explaination:</p>
<ul>
<li>When you group a <strong>DataFrame by a single column</strong>, it means you create separate groups based on <strong>one category</strong> (e.g., <code>name</code>).</li>
<li>when you <strong>group by two columns</strong>, you are creating <strong>subgroups</strong> within the main groups.</li>
</ul>
<p>To visualize this properly using <code>Matplotlib</code>, follow these steps:</p>
<ul>
<li>Create <strong>subplots</strong> for each <code>unique value</code> in the first grouping column (e.g., <code>name</code>).</li>
<li>Plot <strong>separate lines</strong> in each <code>subplot</code> for the second
grouping column (e.g., <code>coeff</code>).</li>
<li>Use <strong>loops</strong> to iterate through the groups.</li>
</ul>
<p>Code:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt

# DataFrame
data = {
    &quot;name&quot;: [&quot;F2&quot;] * 14 + [&quot;F3&quot;] * 14,
    &quot;N&quot;: [&quot;F2_1&quot;, &quot;F2_1&quot;, &quot;F2_2&quot;, &quot;F2_2&quot;, &quot;F2_3&quot;, &quot;F2_3&quot;, &quot;F2_4&quot;, &quot;F2_4&quot;, &quot;F2_5&quot;, &quot;F2_5&quot;, &quot;F2_6&quot;, &quot;F2_6&quot;, &quot;F2_7&quot;, &quot;F2_7&quot;,
          &quot;F3_1&quot;, &quot;F3_1&quot;, &quot;F3_2&quot;, &quot;F3_2&quot;, &quot;F3_3&quot;, &quot;F3_3&quot;, &quot;F3_4&quot;, &quot;F3_4&quot;, &quot;F3_5&quot;, &quot;F3_5&quot;, &quot;F3_6&quot;, &quot;F3_6&quot;, &quot;F3_7&quot;, &quot;F3_7&quot;],
    &quot;coeff&quot;: [1, 5] * 7 + [1, 5] * 7,
    &quot;X1&quot;: [0, 0, 25, 25, 55, 55, 85, 85, 95, 95, 120, 120, 150, 150,
           0, 0, 25, 25, 55, 55, 85, 85, 95, 95, 120, 120, 150, 150],
    &quot;Y1&quot;: [0.56451, 0.45005, 0.53641, 0.53641, 0.45067, 0.38828, 0.33279, 0.35315, 0.2756, 0.32571, 0.159, 0.27583, 0.05648, 0.21128,
           0.42757, 0.32409, 0.36033, 0.2701, 0.2161, 0.17014, 0.08191, 0.10512, 0.04468, 0.09686, 0.07025, 0.033, 0.10689, 0.01334]
}

df = pd.DataFrame(data)

# Create a figure with subplots for each unique 'name'
unique_names = df[&quot;name&quot;].unique()
fig, axes = plt.subplots(len(unique_names), 1, figsize=(8, 6), sharex=True)

# Ensure axes is iterable (for a single subplot case)
if len(unique_names) == 1:
    axes = [axes]

# Iterate over unique names and plot for each coeff
for ax, name in zip(axes, unique_names):
    subset = df[df[&quot;name&quot;] == name]
    for coeff, group in subset.groupby(&quot;coeff&quot;):
        ax.plot(group[&quot;X1&quot;], group[&quot;Y1&quot;], marker=&quot;o&quot;, label=f&quot;Coeff {coeff}&quot;)

    ax.set_title(f&quot;Subplot for {name}&quot;)
    ax.legend()
    ax.set_ylabel(&quot;Y1&quot;)

plt.xlabel(&quot;X1&quot;)
plt.tight_layout()
plt.show()
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/VVL5D3th.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VVL5D3th.png"" alt=""subplot"" /></a></p>
","0","Answer"
"79469543","79469509","<p>Simply use the lambda function like this:</p>
<pre><code>df[&quot;foo&quot;] = df[&quot;foo&quot;].apply(lambda row: [x + 10 for x in row])
</code></pre>
","2","Answer"
"79470036","79469956","<p>I think you're counting all previous views and carts, but if I understood correctly you only need to track the last purchase time for each user-product combination and filter accordingly.</p>
<pre><code># assuming df is your pandas dataframe

# Some useful sorting
df = df.sort_values(by=['user', 'product', 'event_time'])

# Link last purchase time for each user-product combination
purchases = df[df['event_type'] == 'purchase'].copy()
purchases['last_purchase'] = purchases['event_time']
last_purchases = purchases.groupby(['user', 'product'])['last_purchase'].last().reset_index()

# Add the last purchase times into the main dataframe: every row in the main dataframe will be correlated to the last purchase time. This will be very useful when counting views / add to cart events.
df = pd.merge(df, last_purchases, on=['user', 'product'], how='left')

# We are interested only in views and carts that happened before the last purchase
views_carts = df[df['event_type'].isin(['view', 'cart'])].copy()
views_carts = views_carts[views_carts['event_time'] &lt; views_carts['last_purchase']]

# Count everything up
counts = views_carts.groupby(['user', 'product', 'last_purchase', 'event_type']).size().unstack(fill_value=0)
counts = counts.rename(columns={'view': 'view_before_bought', 'cart': 'cart_before_bought'}).reset_index()
</code></pre>
<p>If you need to merge back your inormation in the original dataframe:</p>
<pre><code>purchases = pd.merge(purchases, counts, on=['user', 'product', 'last_purchase'], how='left').fillna(0)

# Merge back the counts into the original dataframe
final_df = pd.merge(df, purchases[['id', 'view_before_bought', 'cart_before_bought']], on='id', how='left').fillna(0)


print(final_df.head(10))
</code></pre>
<p>Please note I dind't test it properly, pls feel free to let me know in the comment if it can be helpful.</p>
<p>I apologise for bad english, I'm not native speaker.</p>
","0","Answer"
"79470175","79470048","<p>I don't get your error, but I still get an IndexError with your code when a status is not found.</p>
<p>Anyway, since your <code>dfdefs</code> is static, better provide a simple dictionary to the function (you could use a DataFrame but it would need to be processed for each item):</p>
<pre><code>def condFormat(s, dic=dfdefs.set_index('STATUS')['COLOR'].to_dict()):
    dcolors = {&quot;GREEN&quot;: &quot;rgb(146, 208, 80)&quot;,
               &quot;YELLOW&quot;: &quot;rgb(255, 255, 153)&quot;,
               &quot;RED&quot;: &quot;rgb(218, 150, 148)&quot;,
               None: &quot;rgb(255, 255, 255)&quot;}
    return f'background-color: {dcolors.get(dic.get(s), &quot;&quot;)}'

dfhealth.style.map(condFormat)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/owtkPBA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/owtkPBA4.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79470390","79470317","<p>In Python 3.8+, you should be able to use <a href=""https://peps.python.org/pep-0572/"" rel=""nofollow noreferrer"">Assignment Expressions</a>:</p>
<pre class=""lang-py prettyprint-override""><code>df[[&quot;new_foo&quot;, &quot;new_bar&quot;]] = df.apply(
    lambda row: a_func(
        row[&quot;original_foo&quot;][(start := row.foo - bar) : start + baz],
        row[&quot;original_bar&quot;][start : start + baz],
    ),
    axis=1,
    result_type=&quot;expand&quot;,
)

</code></pre>
","0","Answer"
"79470501","79469073","<p>You can do this by iterating through the DataFrame and updating a dictionary dynamically.</p>
<pre><code>import pandas as pd
import ast

data = {
    &quot;Book Name&quot;: [&quot;Book 1&quot;, &quot;Book 2&quot;, &quot;Book 3&quot;],
    &quot;Languages&quot;: [&quot;['Portuguese','English']&quot;, &quot;['English','Japanese']&quot;, &quot;['Spanish','Italian','English']&quot;]
}
df = pd.DataFrame(data)
df[&quot;Languages&quot;] = df[&quot;Languages&quot;].apply(ast.literal_eval)
language_dict = {}
for _, row in df.iterrows():
    book_name = row[&quot;Book Name&quot;]
    for lang in row[&quot;Languages&quot;]:
        if lang in language_dict:
            if isinstance(language_dict[lang], list):
                language_dict[lang].append(book_name)
            else:
                language_dict[lang] = [language_dict[lang], book_name]
        else:
            language_dict[lang] = book_name

print(language_dict)
</code></pre>
<p>Output will be</p>
<pre><code>{
  'Portuguese': 'Book 1',
  'English': ['Book 1', 'Book 2', 'Book 3'],
  'Japanese': 'Book 2',
  'Spanish': 'Book 3',
  'Italian': 'Book 3'
}
</code></pre>
","0","Answer"
"79470589","79470048","<p>Apparently I didn't read enough of the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.io.formats.style.Styler.applymap.html"" rel=""nofollow noreferrer"">documentation</a>. You can give <code>applymap</code> any kwargs used by the formatting function:</p>
<pre><code>def condFormat(s, dic=None):
    dcolors = {&quot;GREEN&quot;: &quot;rgb(146, 208, 80)&quot;,
               &quot;YELLOW&quot;: &quot;rgb(255, 255, 153)&quot;,
               &quot;RED&quot;: &quot;rgb(218, 150, 148)&quot;,
               None: &quot;rgb(255, 255, 255)&quot;}
    return f'background-color: {dcolors.get(dic.get(s), &quot;&quot;)}'

dic = dfdefs.set_index('STATUS')['COLOR'].to_dict()
dfhealth.style.applymap(condFormat, dic=dic)
</code></pre>
","0","Answer"
"79471173","79471155","<p>Here's a quick fix:</p>
<pre><code>row_as_ser = df.astype(object).iloc[0]
</code></pre>
","0","Answer"
"79471380","79471155","<p>I guess my question is around the use case. I see you're using</p>
<pre><code>df.query('t == 3')
</code></pre>
<p>which is using a string query making it hard to compare floats and integers. Python can evaluate the equality of floats and strings so would it make more sense to change this query function to something like:</p>
<pre><code>df.loc[df['t'] == 3]
</code></pre>
<p>This will maintain the integer type of 't'.</p>
","0","Answer"
"79471425","79467698","<p>I think your issue here is surrounding data types. You're writing a regular expression to replace substrings, but I imagine pandas is reading in the columns you're interested in as floats or integers. You can check this by running:</p>
<pre><code>weatherData_original.dtypes
</code></pre>
<p>One way around this is to set the dtype to str when loading the data. This will read in every column as a string value.</p>
<pre><code>data = {'Date': ['3/1/1983', '3/2/1983'], 'Solar': [-9.00, -9.00]}
df = pd.DataFrame(data, dtype=str)
</code></pre>
<p>Then you can run your regular expression replacements. I found your second regular expression to work well for me.</p>
<pre><code>new_df = df.replace(regex='^[-9].*', value=pd.NA)
</code></pre>
","0","Answer"
"79471898","79471484","<pre><code>import pandas as pd
df = pd.read_excel('file.xlsx', usecols='A:C,E:G')
df = pd.read_excel('file.xlsx', header=1)
df = pd.read_excel('file.xlsx', sheet_name='Sheet1')
</code></pre>
","0","Answer"
"79471944","79471155","<p>So there're multiple answers possible thanks to Ben Grossmann and user19077881, let me summarize it here (I'm new so I can't upvote Ben Grossmann's answer):</p>
<pre><code>row = df.query(&quot;t == 3&quot;).astype(object).squeeze()
row = df.loc[df[&quot;t&quot;] == 3].astype(object).squeeze()
</code></pre>
<p>The order is important:</p>
<ol>
<li>Convert the whole result of the query to &quot;object&quot; so pandas will not convert ints to floats and keep a global &quot;object&quot; type</li>
<li>Use squeeze() to transform the queried row to a Series</li>
</ol>
<p>And if there are multiple rows matching the query then extract the wanted row by using iloc between astype and squeeze:</p>
<pre><code>row = df.query(&quot;t == 3&quot;).astype(object).iloc[wanted_idx].squeeze()
</code></pre>
","-1","Answer"
"79471962","79471155","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>m = df['t'] == 3

s = (s.astype(object).squeeze() 
     if (s:=df.loc[m]).dtypes.nunique() &gt; 1 
     else s.squeeze()
     )
</code></pre>
<p>Output:</p>
<ul>
<li><strong>Multiple <code>dtypes</code> (OP's <code>df</code>):</strong></li>
</ul>
<pre class=""lang-py prettyprint-override""><code># for `df = pd.DataFrame(data)`
s

t      3
x    4.3
Name: 2, dtype: object
</code></pre>
<ul>
<li><strong>Shared <code>dtypes</code> (e.g. <code>int</code>):</strong></li>
</ul>
<pre class=""lang-py prettyprint-override""><code># for `df = pd.DataFrame(data).astype(int)`
s

t    3
x    4
Name: 2, dtype: int32
</code></pre>
<hr />
<p><strong>Explanation</strong></p>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>df.loc</code></a> with <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a> or indeed <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html"" rel=""nofollow noreferrer""><code>df.query</code></a> to select a <code>df</code> slice.</li>
<li>If <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.nunique.html"" rel=""nofollow noreferrer""><code>Series.nunique</code></a> for <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html"" rel=""nofollow noreferrer""><code>df.dtypes</code></a> &gt; 1, we need dtype <code>object</code>, else apply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.squeeze.html"" rel=""nofollow noreferrer""><code>df.squeeze</code></a> immediately to preserve the dtype already shared.</li>
</ul>
<p>This way you will only adjust the <code>dtype</code> when necessary.</p>
<hr />
<p>Slightly more fancy would be to accept all integers as compatible:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({'t': pd.Series([1,2], dtype=np.int32), 
                   'x': pd.Series([3,4], dtype=np.int64)}
                  )

m = df['t'] == 2
integers = lambda dtype: (np.int64 
                          if np.issubdtype(dtype, np.integer) 
                          else dtype)

s = (s.astype(object).squeeze() 
     if (s:=df.loc[m]).dtypes.map(integers).nunique() &gt; 1 
     else s.squeeze()
     )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>t    2
x    4
Name: 1, dtype: int64
</code></pre>
<ul>
<li>Here we adjust all integers in <code>df.dtypes</code> to <code>int64</code> where <a href=""https://numpy.org/doc/stable/reference/generated/numpy.issubdtype.html"" rel=""nofollow noreferrer""><code>np.issubdtype</code></a> with <code>np.integer</code> is <code>True</code> before checking the number of unique <code>dtypes</code>.</li>
</ul>
","0","Answer"
"79472701","79472665","<p>There is absolutely <strong>no need for pandas</strong> here. You're just using it as an expensive and slow container.</p>
<p>Just loop over the lists <strong>and dictionaries</strong>, in pure python:</p>
<pre><code>matched_plates = []
matches_sources_ra = []
matches_sources_dec = []

for lst in plates:
    for dic in lst:
        if 'sources' in dic:
            print(dic['plate ID'])
            matched_plates.append([dic['plate ID'], len(dic['sources'])])
            matches_sources_ra.append(dic['sources'][0][1])
            matches_sources_dec.append(dic['sources'][0][2])
</code></pre>
<p>Output:</p>
<pre><code>193a
194b
</code></pre>
<p>Output lists:</p>
<pre><code>matched_plates
# [['193a', 2], ['194b', 2]]

matches_sources_ra
# [99.28418829069784, 99.28418829069784]

matches_sources_dec
# [11.821604434173034, 11.821604434173034]
</code></pre>
","4","Answer"
"79472724","79472665","<p>Edit: I agree with the other response in that there is no need for pandas for this purpose. But if you need to work with the df for other reasons (and the data you gave us is just a subset) this is how you can create a df with the correct columns.</p>
<p>Why are the items in plates lists? None of those lists ever has more than one item. So you can clean them with</p>
<pre><code>clean_plates = [item[0] if item else {} for item in plates]
</code></pre>
<p>then you can work with the df columns, in particular <code>sources</code>:</p>
<pre><code>&gt;&gt;&gt; clean_df = pd.DataFrame.from_records(clean_plates)
&gt;&gt;&gt; clean_df
   plate ID    ra   dec                                            sources
0       NaN   NaN   NaN                                                NaN
1       NaN   NaN   NaN                                                NaN
2      193a  98.0  11.0  [[3352102441297986560, 99.28418829069784, 11.8...
3      194b  98.0  11.0  [[3352102441297986560, 99.28418829069784, 11.8...
4       NaN   NaN   NaN                                                NaN
5       NaN   NaN   NaN                                                NaN
6       NaN   NaN   NaN                                                NaN
7       NaN   NaN   NaN                                                NaN
8       NaN   NaN   NaN                                                NaN
9       NaN   NaN   NaN                                                NaN
10      NaN   NaN   NaN                                                NaN
11      NaN   NaN   NaN                                                NaN
12      NaN   NaN   NaN                                                NaN
13      NaN   NaN   NaN                                                NaN
</code></pre>
<p>Now you can iterate:</p>
<pre><code>&gt;&gt;&gt; for idx, row in clean_df.iterrows():
...     print(row[&quot;plate ID&quot;])
... 
nan
nan
193a
194b
nan
nan
# etc
</code></pre>
","1","Answer"
"79473267","79473246","<p>So given you have 14 columns:</p>
<pre><code>n_cols = 14
</code></pre>
<p>And assuming your column names follow the pattern implied:</p>
<pre><code>cols = [f&quot;fan{i}_rpm&quot; for i in range(1, n_cols + 1)]
</code></pre>
<p>Then with pandas/numpy, you can simply:</p>
<pre><code>df[cols] = np.random.randint(5675, 5725, (30, n_cols))
</code></pre>
","5","Answer"
"79473269","79473246","<p>Obvious way, using a loop:</p>
<pre><code>N = 14
for i in range(N):
    df[f'fan{i+1}_rpm'] = np.random.randint(5675, 5725, 30)
</code></pre>
<p>Smarter way, if the parameters are identical for all columns, with a 2D array:</p>
<pre><code>N = 14
df[[f'fan{i+1}_rpm' for i in range(N)]] = np.random.randint(5675, 5725, (30, N))
</code></pre>
","3","Answer"
"79473662","79473319","<p>There is neat method <code>df[&quot;col&quot;].unique()</code> which accomplishes what you did with the <code>drop_duplicates</code>.</p>
<p>When you loop through <code>ind_county</code> the resulting <code>i</code> is already county code (e.g. <code>A1</code>). That's why <code>ind_county[i]</code> fails. Simply replace it with <code>i</code>. Then you will have DataFrame for a given county so you can plot it, calculate mean (using <code>df[&quot;col&quot;].mean()</code> mehtod) etc.</p>
","0","Answer"
"79473798","79473658","<p>You can just use <code>return x.loc[x[col] == 0]</code> and so on which allows the use of a String variable. BTW this approach using <code>[]</code> is generally preferred for clarity and to avoid potential conflict with method names and also allows names with spaces (for which ‘dot’ approach cannot be used).
For example:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'name': ['irish1', 'irish2'],
                   'height': [1.8, 1.7]
                    })

def fun(x,col):
    return x.loc[x[col] == 1.7]


print(fun(df, 'height'))
</code></pre>
<p>gives</p>
<pre><code>     name  height
1  irish2     1.7
</code></pre>
","2","Answer"
"79473894","79473874","<p>You should use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.droplevel.html"" rel=""nofollow noreferrer""><code>droplevel</code></a>:</p>
<pre><code>cols = ['type', 'type2']
df['mean'] = (df.groupby(cols)['values']
                .rolling(window=3).mean()
                .droplevel(cols)
             )
</code></pre>
<p>Output:</p>
<pre><code>    values type type2  mean
0        1    A     C   NaN
1        2    B     D   NaN
2        3    A     C   NaN
3        4    B     D   NaN
4        5    A     C   3.0
5        6    B     D   4.0
6        7    A     C   5.0
7        8    B     D   6.0
8        9    A     C   7.0
9       10    B     D   8.0
10      11    A     C   9.0
11      12    B     D  10.0
12      13    A     C  11.0
13      14    B     D  12.0
14      15    A     C  13.0
</code></pre>
<p>And to avoid the NaNs, add <code>min_periods=1</code>:</p>
<pre><code>cols = ['type', 'type2']
df['mean'] = (df.groupby(cols)['values']
                .rolling(window=3, min_periods=1).mean()
                .droplevel(cols)
             )
</code></pre>
<p>Output:</p>
<pre><code>    values type type2  mean
0        1    A     C   1.0
1        2    B     D   2.0
2        3    A     C   2.0
3        4    B     D   3.0
4        5    A     C   3.0
5        6    B     D   4.0
6        7    A     C   5.0
7        8    B     D   6.0
8        9    A     C   7.0
9       10    B     D   8.0
10      11    A     C   9.0
11      12    B     D  10.0
12      13    A     C  11.0
13      14    B     D  12.0
14      15    A     C  13.0
</code></pre>
","2","Answer"
"79474055","79473967","<p>One way you could approach this is to merge on <code>DeliveryPeriod</code> then replace <code>FloatPrice</code> values where the <code>Vintage</code> isn't contained in the <code>Service</code>.</p>
<p>The only way I can think to check string contains is with <code>.apply(axis=1)</code>. I'm surprised there isn't a vectorized way to do this – or if there is, I don't know what it is.</p>
<pre><code>df_merge = df1.merge(df2, how='left')

mask = df_merge.apply(
    lambda row:
        isinstance(row['Vintage'], str) and
        row['Vintage'] in row['Service'],
    axis=1)

df_merge.loc[~mask, 'FloatPrice'] = 0
del df_merge['Service']
df_merge
</code></pre>
<pre><code>  BA Product  FixedPrice       Vintage DeliveryPeriod  FloatPrice
0  A     foo        10.0  Vintage 2025          Mar25         5.0
1  B     foo        11.0  Vintage 2025          Dec25         4.0
2  B     foo        12.0  Vintage 2024          Sep25         6.0
3  C     bar         2.0           NaN          Nov25         0.0
</code></pre>
","2","Answer"
"79474081","79473657","<p>Unlike the <code>.apply()</code> method of a DataFrame, which can take an <code>axis</code> argument, <code>groupby().apply()</code> applies fuctions group-wise and doesn't require an <code>axis</code> argument.</p>
<p>To fix the issue, simply remove <code>axis=0</code>;</p>
<pre><code>display(ch2m_cpaf.groupby('PROVIDING_COMPANY_CODE').apply(lambda df: df))
</code></pre>
<p>If you want to perform operations on rows or columns, you should do it inside the lambda function;</p>
<pre><code>display(ch2m_cpaf.groupby('PROVIDING_COMPANY_CODE').apply(lambda df: df.sum(axis=0)))  # Sum across columns
</code></pre>
","1","Answer"
"79474113","79473967","<p>Based on <a href=""https://stackoverflow.com/q/79473967/79474055#comment140158869_79473967"">your comment</a> replying to <a href=""https://stackoverflow.com/q/79473967/79474055#comment140158855_79473967"">Michael's</a>, you could extract the &quot;Vintage&quot; from <code>Service</code>:</p>
<pre><code>vintage2 = df2['Service'].str.extract('ICE - (.+)', expand=False)
</code></pre>
<pre><code>0    Vintage 2025
1    Vintage 2025
2    Vintage 2024
3    Vintage 2023
Name: Service, dtype: object
</code></pre>
<p>Then merge on that and fillna:</p>
<pre><code>df1.merge(
    df2.drop(columns='Service'),
    left_on=['DeliveryPeriod', 'Vintage'],
    right_on=['DeliveryPeriod', vintage2],
    how='left',
).fillna({'FloatPrice': 0})
</code></pre>
<pre><code>  BA Product  FixedPrice       Vintage DeliveryPeriod  FloatPrice
0  A     foo        10.0  Vintage 2025          Mar25         5.0
1  B     foo        11.0  Vintage 2025          Dec25         4.0
2  B     foo        12.0  Vintage 2024          Sep25         6.0
3  C     bar         2.0           NaN          Nov25         0.0
</code></pre>
","0","Answer"
"79474660","79464633","<p>I was able to solve this issue by looking for zero in the specific columns as below:</p>
<pre><code>import openpyxl as op
import os
import datetime

input_file_path = r&quot;C:\Users&quot;
result_filename = &quot;Test_Summary_&quot; + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + &quot;.xlsx&quot;
result_file_path = os.path.join(input_file_path, result_filename)
results = list()


def job_summary_gen():
    # this is the extension you want to detect
    extension = '.xlsx'
    file_list = []
    i = 0
    new_wb = op.Workbook()
    # sheet name update
    new_wb['Sheet'].title = &quot;Summary&quot;
    # wb2 = op.load_workbook(result_file_path)
    ws2 = new_wb['Summary']

    # If there are any rows already populated in the destination sheet start at next row otherwise start at row 1
    if ws2.max_row == 1:
        new_row = ws2.max_row
    else:
        new_row = ws2.max_row + 1

    for root, dirs_list, files_list in os.walk(input_file_path):
        for file_name in files_list:
            if os.path.splitext(file_name)[-1] == extension and file_name == 'testReport.xlsx':
                file_name_path = os.path.join(root, file_name)
                print(file_name_path)  # This is the full path of the filter file
                file_list.append(file_name_path)
                file_dir = os.path.dirname(file_name_path)
                folder_name = os.path.basename(file_dir)
                print(folder_name)
                wb1 = op.load_workbook(file_name_path)
                ws1 = wb1['Summary']

                for cell in ws1.iter_rows(min_col=3, max_col=3):
                    # print(cell[0].value)
                    # creating the header
                    if 'Partition' in cell[0].value.lower() and i &lt; 1:
                        # print(&quot;header workbook&quot;),
                        # Add 'File #' to first cell in destination row using row number as #
                        ws2.cell(row=new_row, column=1).value = &quot;File Name&quot;
                        for x in range(2, ws1.max_column + 1):
                            # Read each cell from col 1 to last used col
                            cell_value = ws1.cell(row=cell[0].row, column=x)
                            # Write last read cell to next empty row
                            ws2.cell(row=new_row, column=x).value = cell_value.value
                        # Increment to next unused row
                        new_row += 1
                        i += 1

                    if 'Partition' not in cell[0].value.lower():
                        # print(&quot;Match, copying cells to new workbook&quot;)
                        # Add 'File name' to first cell in destination row using row number as #
                        ws2.cell(row=new_row, column=1).value = folder_name
                        for x in range(2, ws1.max_column + 1):
                            # Read each cell from col 1 to last used col- column count starts from 1
                            # copying score here
                            if x &lt;= 7:
                                cell_value = ws1.cell(row=cell[0].row, column=x).value
                            else:
                                if ws1.cell(row=cell[0].row, column=x).value == 0:
                                    prev_cell = ws1.cell(row=cell[0].row - 1, column=x).value
                                    if prev_cell != 0:
                                        cell_value = prev_cell
                                    else:
                                        cell_value = ws1.cell(row=cell[0].row - 2, column=x).value
                                else:
                                    cell_value = ws1.cell(row=cell[0].row, column=x).value

                            # Write last read cell to next empty row
                            ws2.cell(row=new_row, column=x).value = cell_value

                        # Increment to next unused row
                        new_row += 1
    new_wb.save(result_file_path)

job_summary_gen()
</code></pre>
","-1","Answer"
"79475551","79475516","<p>Depending on how you want the words to be identified I see a few options. You could craft a regex with lookaheads, you could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extractall.html"" rel=""nofollow noreferrer""><code>extractall</code></a> words and compute a <code>set</code>, or you could test the words one by one and combine the boolean outputs with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html"" rel=""nofollow noreferrer""><code>numpy.logical_and</code></a>+<a href=""https://numpy.org/doc/stable/reference/generated/numpy.ufunc.reduce.html"" rel=""nofollow noreferrer""><code>reduce</code></a>:</p>
<pre><code># option 1: using lookaheads
pattern = ''.join(fr'(?=.*{w})' for w in kw)

df['regex_lookahead'] = df['col'].str.contains(pattern)

# option 2: using extract + groupby
pattern = '|'.join(map(re.escape, kw))
df['regex_groupby'] = (df['col'].str.extractall(f'({pattern})')[0]
                       .groupby(level=0).agg(set).eq(set(kw))
                      #.reindex(df.index, False) # if you want False on no-match 
                      )

# option 3: testing all words independently
import numpy as np
df['contains_reduce'] = np.logical_and.reduce([df['col'].str.contains(w)
                                               for w in kw])
</code></pre>
<p>Output:</p>
<pre><code>                 col  regex_lookahead regex_groupby  contains_reduce
0            nothing            False           NaN            False
1             banana            False         False            False
2  kiwi banana apple             True          True             True
3    bananapple kiwi             True         False             True
</code></pre>
<p><em>If you want the first option to only match full words, replace <code>fr'(?=.*{w})'</code> by <code>fr'(?=.*\b{w}\b)'</code>.</em></p>
<pre><code>                 col  regex_lookahead_words
0            nothing                  False
1             banana                  False
2  kiwi banana apple                   True
3    bananapple kiwi                  False
</code></pre>
<p>If you want to match words, you could also avoid any regex, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>split</code></a> the string into words and use a <code>set</code>:</p>
<pre><code>df['split_set'] = df['col'].str.split().transform(set).ge(set(kw))
</code></pre>
<p>Output:</p>
<pre><code>                 col  split_set
0            nothing      False
1             banana      False
2  kiwi banana apple       True
3    bananapple kiwi      False
</code></pre>
","0","Answer"
"79476555","79476439","<p>It not problem with <code>groupby()</code> but with object <code>relativedelta()</code>.</p>
<p>It seems it doesn't have functions like <code>__lt__()</code> (lower than), <code>__gt__()</code> (grater than) and similar to compare two values.</p>
<p>Try</p>
<pre><code>relativedelta(months=1) &lt; relateivedelta(months=2)
</code></pre>
<p>and you get the same error without <code>groupby</code></p>
<p>But this works</p>
<pre><code>relativedelta(months=1).months &lt; relativedelta(months=2).months
</code></pre>
<p>So you may have to create new column with integer values <code>.months</code> and use this column for comparition.</p>
<hr />
<p>I never used it before but I tested it with your code: <code>groupby()</code> can use function as <code>by=</code> and it sends index to this function</p>
<pre><code>groupbyby(by=lambda index:params.loc[index]['window'].months, ...)
</code></pre>
<p>But there is other problem: <code>relativedelta()</code> keeps <code>months=-18</code> as <code>years=-2,months=-6</code> so it would need to get both values as tuple</p>
<pre><code>groupbyby(by=lambda index:(params.loc[index]['window'].months.years, params.loc[index]['window'].months), ...)
</code></pre>
<p>or convert them to one value</p>
<pre><code>groupbyby(by=lambda index:(params.loc[index]['window'].months.years *12 + params.loc[index]['window'].months), ...)
</code></pre>
<hr />
<pre><code>from dateutil.relativedelta import relativedelta
import pandas as pd
import itertools

def expand_grid(data_dict):
    rows = itertools.product(*data_dict.values())
    return pd.DataFrame.from_records(rows, columns=data_dict.keys())

ref_dates = pd.date_range(start=&quot;2024-06&quot;, end=&quot;2025-02&quot;, freq=&quot;MS&quot;).tolist()
windows = [relativedelta(months=-30),relativedelta(months=-18)]
max_horizon = [relativedelta(months=2)]  
params = expand_grid({'ref_date': ref_dates, 'max_horizon': max_horizon, 'window': windows})

for name, group in params.groupby(by=lambda index:(params.loc[index]['window'].years*12+params.loc[index]['window'].months) ): 
    print(name)
    print(group)
</code></pre>
<hr />
<p>I found similar question for <code>relativedelta()</code> (12 years old) on Stackoverflow:</p>
<p><a href=""https://stackoverflow.com/questions/11704341/comparing-dateutil-relativedelta"">python - Comparing dateutil.relativedelta - Stack Overflow</a></p>
<p>And issue in <code>dateutil</code> repo: (year 2017)</p>
<p><a href=""https://github.com/dateutil/dateutil/issues/350"" rel=""nofollow noreferrer"">relativedelta does not implement __lt__ · Issue #350 · dateutil/dateutil</a></p>
","2","Answer"
"79476950","79476892","<p>I would use <code>for</code>-loop for this</p>
<pre><code>for name, group in df.groupby(...):
</code></pre>
<p>this way I could use <code>if/else</code> to run or skip some code.</p>
<p>To get first element in group:<br />
(I don't know why but <code>.first()</code> doesn't work as I expected - it asks for some offset)</p>
<pre><code>first_value = group.iloc[0]['first mover']
</code></pre>
<p>To get indexes of other rows (except first):</p>
<pre><code>group.index[1:]
</code></pre>
<p>and use them to set 0 in original <code>df</code></p>
<pre><code>df.loc[group.index[1:], 'first mover'] = 0
</code></pre>
<hr />
<p>Minimal working code which I used for tests:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
         'day': [1,2,3,4,5,6,7,8,], 
         'first mover': [1,1,0,0,0,1,0,1]
     })
     
N = 4

for name, group in df.groupby(by=lambda index:index//N):
    #print(f'\n---- group {name} ---\n')
    #print(group)

    first_value = group.iloc[0]['first mover']
    #print('first value:', first_value)
    
    if first_value == 1 :
        #print('&gt;&gt;&gt; change:', group.index[1:])
        df.loc[group.index[1:], 'first mover'] = 0
        
print('\n--- df ---\n')        
print(df)        
</code></pre>
","0","Answer"
"79476971","79476892","<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

# Create sample DataFrames
df = pd.DataFrame(
    {
        &quot;day&quot;: [*range(1, 21)],
        &quot;first mover&quot;: np.random.randint(0, 2, 20),
    }
)

# if the day-1 (1, 5, 9, ...) is dividable by 4
# and 
# the 'first mover' == 1
# result is 1 otherwise keep as 0

df['first mover edited'] = df.apply(lambda row: ( ( (row.day-1) % 4 == 0 ) and 
( row['first mover'] == 1 ) )*1, axis=1)

df['group'] = (df['day']-1) // 4

df
</code></pre>
<pre><code>|   day |   first mover |   first mover edited |   group |
|------:|--------------:|---------------------:|--------:|
|     1 |             1 |                    1 |       0 |
|     2 |             0 |                    0 |       0 |
|     3 |             1 |                    0 |       0 |
|     4 |             0 |                    0 |       0 |
|     5 |             1 |                    1 |       1 |
|     6 |             1 |                    0 |       1 |
|     7 |             0 |                    0 |       1 |
|     8 |             1 |                    0 |       1 |
|     9 |             0 |                    0 |       2 |
|    10 |             0 |                    0 |       2 |
|    11 |             0 |                    0 |       2 |
|    12 |             0 |                    0 |       2 |
|    13 |             1 |                    1 |       3 |
|    14 |             0 |                    0 |       3 |
|    15 |             1 |                    0 |       3 |
|    16 |             0 |                    0 |       3 |
|    17 |             0 |                    0 |       4 |
|    18 |             1 |                    0 |       4 |
|    19 |             0 |                    0 |       4 |
|    20 |             0 |                    0 |       4 |
</code></pre>
<p>you can edit the 'first mover' column this is for demonstration</p>
","1","Answer"
"79477094","79476892","<p>You could use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a>:</p>
<pre><code>group = (np.arange(len(df))//4)[::-1]
df['out'] = (df.groupby(group)['first mover']
               .transform(lambda x: np.r_[x.iloc[0], np.zeros(len(x)-1, x.dtype)])
             )
</code></pre>
<p>However, a much easier and efficient approach would be to slice every 4 row from the end (starting on the -4), then to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>reindex</code></a> with zeros:</p>
<pre><code>df['out'] = (df['first mover'].iloc[-4::-4]
             .reindex(df.index, fill_value=0)
             )
</code></pre>
<p>or, using a boolean mask and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.where.html"" rel=""nofollow noreferrer""><code>where</code></a>:</p>
<pre><code># identify first of every 4 rows, from the end
m = np.arange(len(df)-1,-1,-1)%4 == 3
df['out'] = df['first mover'].where(m, 0)
</code></pre>
<p>Output:</p>
<pre><code>   day  first mover  out
0    1            1    1
1    2            1    0
2    3            0    0
3    4            0    0
4    5            0    0
5    6            1    0
6    7            0    0
7    8            1    0
</code></pre>
<p>Intermediates:</p>
<pre><code>   day  first mover  group  iloc      m  out
0    1            1      1   1.0   True    1
1    2            1      1   NaN  False    0
2    3            0      1   NaN  False    0
3    4            0      1   NaN  False    0
4    5            0      0   0.0   True    0
5    6            1      0   NaN  False    0
6    7            0      0   NaN  False    0
7    8            1      0   NaN  False    0
</code></pre>
","3","Answer"
"79477663","79477395","<p>You can try <a href=""https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html#pandas-pivot-table"" rel=""nofollow noreferrer""><code>pandas.pivot_table</code></a> like below</p>
<pre><code>df.pivot_table(index=&quot;User&quot;, columns=&quot;Action&quot;, fill_value=0, aggfunc=len).reset_index()
</code></pre>
<p>or, a shorter code with <a href=""https://df.value_counts().unstack(fill_value=0)"" rel=""nofollow noreferrer""><code>pandas.unstack</code></a></p>
<pre><code>df.value_counts().unstack(fill_value=0).reset_index()
</code></pre>
<p>which gives</p>
<pre><code>Action User  Page1  Page2  Page3
0        AA      2      1      0
1        BB      0      1      1
2        CC      0      0      2
</code></pre>
<h1>data</h1>
<pre><code>import pandas as pd

data = {
    &quot;User&quot;: [&quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;, &quot;CC&quot;],
    &quot;Action&quot;: [&quot;Page1&quot;, &quot;Page1&quot;, &quot;Page2&quot;, &quot;Page2&quot;, &quot;Page3&quot;, &quot;Page3&quot;, &quot;Page3&quot;],
}

df = pd.DataFrame(data)
</code></pre>
","1","Answer"
"79478125","79455667","<p>I manage to make it like this:</p>
<pre><code>def create_order(length, phase):
    first_list = [i for i in range(length)]
    second_list = [i for i in range(length, length * 2)]
    start_i = 0
    out_list = []
    for i in range(phase, length + 1, phase):
        out_list.extend(first_list[start_i:i])
        out_list.extend(second_list[start_i:i])
        start_i = i
    return out_list

df1 = pd.DataFrame({'col': [1,2,3,4]})
df2 = pd.DataFrame({'col': [5,6,7,8]})
order = create_order(len(df1), 2)
out = pd.concat([df1, df2]).iloc[order]
</code></pre>
","0","Answer"
"79479145","79478558","<p>If you need only <code>OK</code>, <code>NAN</code> then you could use <code>.isin()</code> to get <code>True</code>/<code>False</code><br />
and later you can convert it to strings.</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'lookup_id':[111, 222, 333, 444, 777, 1089, 3562]})
df_past = pd.DataFrame({'id_number': [111, 23, 333, 444, 10101, 777, 222]})

df['result'] = df['lookup_id'].isin(df_past['id_number'])
df['result'] = np.where(df['result'], 'OK', 'NAN')

print(df)
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>   lookup_id result
0        111     OK
1        222     OK
2        333     OK
3        444     OK
4        777     OK
5       1089    NAN
6       3562    NAN
</code></pre>
<hr />
<p>If you need more information from other dataframe then you can use <code>.apply(function)</code> and inside function get more data and format it</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'lookup_id':[111, 222, 333, 444, 777, 1089, 3562]})
df_past = pd.DataFrame({'id_number': [111, 23, 333, 444, 10101, 777, 222]})

df['result'] = df['lookup_id'].isin(df_past['id_number'])

def function(lookup_id):
    #print('lookup_id:', lookup_id)

    row = df_past[ df_past['id_number'] == lookup_id ]
    #print(row)

    if not row.empty:
        return f'{lookup_id}, {row.index[0]}'
       
    return 'NAN'
    
df['result'] = df['lookup_id'].apply(function)

print(df)
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>   lookup_id  result
0        111  111, 0
1        222  222, 6
2        333  333, 2
3        444  444, 3
4        777  777, 5
5       1089     NAN
6       3562     NAN
</code></pre>
","1","Answer"
"79479186","79478558","<p>A lot of functionality that has to do with comparing values in Pandas can be achieved through <code>merge</code>. In this case:</p>
<pre><code>lookup_id = pd.DataFrame([111, 222, 333, 444, 777, 1089, 3562], columns=['id'])
id_number = pd.DataFrame([111, 23, 333, 444, 10101, 777, 222], columns=['id'])

lookup_id.merge(id_number, how='left', indicator=True)
</code></pre>
<p>returns</p>
<pre><code>    id      _merge
0   111     both
1   222     both
2   333     both
3   444     both
4   777     both
5   1089    left_only
6   3562    left_only
</code></pre>
<p>The <code>indicator</code> parameter also accepts a string that it will use as the column title, and you can further process the output:</p>
<pre><code>(lookup_id
    .merge(id_number, how='left', indicator='corr')
    .assign(corr=lambda df: df['corr'].map({'both':'OK', 'left_only':'NAN'}))
)
</code></pre>
<p>which then returns:</p>
<pre><code>    id      corr
0   111     OK
1   222     OK
2   333     OK
3   444     OK
4   777     OK
5   1089    NAN
6   3562    NAN
</code></pre>
","1","Answer"
"79480028","79470598","<p>As noted this is possible. Take a deep breath, because your data requires a lot of massaging to work with numpy/h5py/HDF5 limitations. Here are the issues:</p>
<ul>
<li>h5py doesn't directly support array objects in a dataset. (e.g., an array of arrays).</li>
<li>h5py also doesn't directly support &quot;ragged&quot; arrays (e.g., arrays of
different length in the same dataset.)</li>
<li>HDF5 doesn't support Unicode text. This is a problem when you have
numpy arrays of strings (b/c numpy uses Unicode text).</li>
</ul>
<p>There are workarounds for each of these issues....but it gets &quot;a little&quot; complicated. Arrays of text get especially complicated. :-)</p>
<ul>
<li>Dealing with the Unicode limitation is &quot;relatively easy&quot; -- you use
<code>encode('utf-8')</code> to convert each string.</li>
<li>h5py has a dedicated datatype for the array limitation --
<code>h5py.vlen_dtype()</code>. It is covered in the h5py docs, so I won't go
into details here.</li>
</ul>
<p>With the the variety of datatypes you have (int, float, bool, str, and arrays of each), there is a 2nd consideration:</p>
<ul>
<li>Do you save all of the dataframe data as ONE Dataset?</li>
<li>Or, do you save each series/column as an individual dataset under ONE
group?</li>
</ul>
<p>Both are possible. (I have done it both ways.) The coding to create ONE dataset is far more complicated (to write and to explain). If this is your first attempt to write from Pandas to HDF5 with h5py, it will be difficult to follow. Writing as multiple datasets is &quot;less complicated&quot; (although I would hardly call it simple).</p>
<p>The procedure is below. It includes brief comments to explain the data manipulation required for arrays of arrays. I created a small dataframe at the beginning to represent all of the datatypes from your question. Run it once to &quot;see&quot; what's going on. The remainder of the code should work as-is with your code.</p>
<pre><code>import pandas as pd
import numpy as np
import h5py

#+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+#
# Create a dataframe with representative datatypes

n_rows = 4
col1_vals = [float(10*i) for i in range(n_rows)]
col2_vals = [i for i in range(n_rows)]
col3_vals = [bool(i%2) for i in range(n_rows)]
col4_vals = ['a', 'bcdef','gh','ijkl']

col5_vals = [np.array([.1,.2]), np.array([.3,.4]), 
             np.array([.5,.6]), np.array([.7,.8,.9])]
col6_vals = [np.array([11,22,33]), np.array([44,55]), 
             np.array([66,]), np.array([77,88,99])]
col7_vals = [np.array(['AB','CD', 'EF']), np.array(['GHI','JKL']), 
             np.array(['MNO', 'PQR']), np.array(['ST', 'UVWX']) ]

df = pd.DataFrame({'col1': col1_vals, 'col2': col2_vals, 'col3': col3_vals, 
                   'col4': col4_vals, 'col5': col5_vals, 'col6': col6_vals,
                   'col7': col7_vals})
                  
print(df)

#+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+#
# Extract columns/series from the dataframe and write to
# HDF5 as individual datasets

n_rows = df.shape[0] 
             
with h5py.File('test_by_cols.h5','w') as h5f:
    grp = h5f.create_group('from_pandas') 

    for col in df.columns:
        if df[col].dtype != 'object':
        # this is for int, float and bool scalars    
            grp.create_dataset(col, data=df[col].to_numpy())
            
        else:
            #print('working on object data')
            # this is for everything else
            row0_col_data = df.at[0,col]
            if isinstance(row0_col_data,str):
            # scalar strings are pandas &quot;object&quot; datatypes, but DO NOT require special handling    
                grp.create_dataset(col, data=df[col].to_numpy())

            if isinstance(row0_col_data,np.ndarray): 
            # ALL arrays require the vlen_dtype and must be added row-by-row to the H5 file
                if not isinstance(row0_col_data[0],np.str_):                     
                    ds_dt = np.dtype( [(col, h5py.vlen_dtype(np.dtype(type(row0_col_data[0])))),] )
                    ds = grp.create_dataset(col, shape=(n_rows,), dtype=ds_dt)
                    for row in range(n_rows):
                        ds[row] = tuple([df.at[row,col],])
                    
                else:
                    # string arrays are similar to above, but must also be encoded first
                    max_len = max([len(s) for row in df[col].to_numpy() for s in row])
                    ds_dt = np.dtype( [(col, h5py.vlen_dtype(f'S{max_len}')),] )
                    ds = grp.create_dataset(col, shape=(n_rows,), dtype=ds_dt)
                    for row in range(n_rows):
                        row_col_data = np.array([s.encode('utf-8') for s in df.at[row,col]])
                        ds[row] = tuple([row_col_data,])

#+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+#
# Confirm data was written to the H5 file correctly

print('\nH5 data read\n')
with h5py.File('test_by_cols.h5') as h5f:
    grp = h5f['from_pandas']
    for ds in grp.keys():
        print(f'For {ds}:')       
        for row in grp[ds]:
            print(row)     
</code></pre>
","1","Answer"
"79480272","79479347","<p>One way to deal with data adjacencies like this is to make the y-axis logarithmic, which sometimes solves the problem, but in your case the effect is limited. My suggestion is to create a subplot with three groups of numbers. Besides, minimise the gaps between the graphs to make them appear as one graph. The key to setting up the subplot is to share the x-axis and the graph ratio. Finally, limits are set for each y-axis to make the text annotations easier to read.</p>
<pre><code>from plotly.subplots import make_subplots
import plotly.graph_objects as go

fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0, row_heights=[0.3,0.4,0.3])

groups = {'Angola':2, 'Brasil':1, 'Cabo Verde':3, 'Moçambique':2, 'Portugal':2, 'São Tomé and Príncipe':3}

for k,v in zip(groups.keys(), groups.values()):
  dff = df_long[df_long['Country'] == k]
  text_position = 'bottom center' if k == 'Moçambique' else 'top center'
  fig.add_trace(go.Scatter(
      x=dff['Year'],
      y=dff['Population'],
      name=k,
      text=dff['Population'],
      showlegend = True,
      hovertemplate = None,
      textfont = dict(size = 15),
      textposition = text_position,
      texttemplate = &quot;%{text:.3s}&quot;,
      mode='lines+markers+text'
  ),row=v, col=1)

fig.update_yaxes(range=[210_000_000,222_000_000],row=1, col=1)
fig.update_yaxes(range=[10_000_000,80_000_000],row=2, col=1)
fig.update_yaxes(range=[200_000,620_000],row=3, col=1)
fig.update_layout(margin=dict(t=20, b=0, l=0,r=0))
fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/AwTcNS8J.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AwTcNS8J.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79480463","79480437","<p>Yes, you can achieve this in Pandas using .idxmax(axis=1), which returns the column name where the maximum value occurs.</p>
<p>Documentation - <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.idxmax.html#pandas-dataframe-idxmax"" rel=""nofollow noreferrer"">pandas.DataFrame.idxmax</a></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {
    'Car': ['Car1', 'Car2'],
    'Day 1': [4, 8],
    'Day 2': [7, 2],
    'Day 4': [3, 1]
}

df = pd.DataFrame(data)

df.set_index('Car', inplace=True)

# Find the column name where the maximum value occurs
df['When'] = df.idxmax(axis=1)

print(df)
</code></pre>
<p>Output:-</p>
<pre class=""lang-none prettyprint-override""><code>      Day 1  Day 2  Day 4   When
Car                             
Car1      4      7      3  Day 2
Car2      8      2      1  Day 1
</code></pre>
","1","Answer"
"79481920","79481878","<p>As far as I know, there is no way to handle this via pandas options. You can however modify the values in the columns, or if you need to retain original values as well, create a new column and call that when you want it displayed.</p>
<pre><code>df['ValuesForDisplay'] = df['Values'].apply(lambda x: [round(item, 2) for item in x])
</code></pre>
","1","Answer"
"79481935","79481878","<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>dataframe.apply</code></a></p>
<pre><code>format_numbers = lambda x : [f&quot;{num:.2f}&quot; for num in x]
df['Values'] = df['Values'].apply(format_numbers)
</code></pre>
<p>Note that this will convert the numbers to strings, so if you need to perform numerical operations on the data, you may need to convert them back to numbers. You can use <code>np.float64</code> or <code>float</code> instead at that time</p>
<pre><code>format_numbers = lambda x : [np.float64(f&quot;{num:.2f}&quot;) for num in x]
df['Values'] = df['Values'].apply(format_numbers
</code></pre>
<p>Now , if this is a <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">minimal reproducible</a>  example and your code consists more data it is not good to use <code>df.apply</code> and then convert back to <code>np.float64</code> it will take <code>O(n * m)</code>. Alternatively , you can use</p>
<pre><code>df['Values'] = df['Values'].apply(np.array)
# vectorized operations
df['Values'] = df['Values'].apply(lambda x: np.format_float_positional(x, precision=2))

print(df)
</code></pre>
<p>And if you have multi-core processor, you can use parallel processing libraries like <code>joblib</code> or <code>dask</code> to speed up the formatting process.</p>
<pre><code>from joblib import Parallel, delayed

def format_numbers(x):
    return [f&quot;{num:.2f}&quot; for num in x]

df['Values'] = Parallel(n_jobs=-1)(delayed(format_numbers)(x) for x in df['Values'])
</code></pre>
<p>See <a href=""https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html"" rel=""nofollow noreferrer""><code>joblib.Parallel</code></a> for more information</p>
","0","Answer"
"79482135","79480120","<p>According to <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"" rel=""nofollow noreferrer"">MinMaxScaler DOC</a>:</p>
<blockquote>
<p>X : array-like of shape <strong>(n_samples, n_features)</strong> The data used to compute
the per-feature minimum and maximum used for later <strong>scaling along the
features axis</strong>.</p>
</blockquote>
<p>When you reshape your data here:</p>
<pre><code>df_test_1.loc[:,df_test.columns[1]].values.reshape(1,-1)
</code></pre>
<p>you get 1 row data with 4 columns in your case (and only 1 value in each of them) instead of 1 column with 4 rows .</p>
<p>You can fix your code one of the following ways:</p>
<ol>
<li>Fix reshape dimensions:
<code>df_test_1.loc[:,col]=scaler.fit_transform(df_test_1.loc[:,col].values.reshape(-1,1))</code></li>
<li>Pass a dataframe instead of reshaped Series by indexing using list: <code>df_test_1.loc[:,col]=scaler.fit_transform(df_test_1.loc[:,[col]])</code></li>
<li>Passing all required columns at once:</li>
</ol>
<pre><code>df_test_2=df_test.copy()
scaler = MinMaxScaler()
cols = df_test.columns
df_test_2.loc[:,cols]=scaler.fit_transform(df_test_2.loc[:,cols])
</code></pre>
","1","Answer"
"79482405","79482376","<pre><code>df.iloc[df['stage'].diff().idxmax():]
</code></pre>
<p>First, find the first transition from 0 to 1 is by computing the difference between consecutive values in the <code>stage</code> column (using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html"" rel=""nofollow noreferrer""><code>diff</code></a>). Then use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html"" rel=""nofollow noreferrer""><code>idxmax</code></a> to locate the index where the first transition occurs.</p>
<p><em>NB: In case, there are transitions that differ more than 1 unit, then use: <code>df.iloc[df['stage'].diff().gt(0).idxmax():]</code></em></p>
<p>Output:</p>
<pre><code>   stage  h1  h2  h3
3      1   3  33  55
4      0   5  44  33
</code></pre>
","4","Answer"
"79482408","79482376","<p>This should be faster than looping:</p>
<pre><code># Using boolean indexing to find the cutoff index
cutoff_index = df[df['stage'] != 0].index.min()

# Slice and reset index
df = df.iloc[cutoff_index:].reset_index(drop=True)
print(df)
</code></pre>
","2","Answer"
"79482693","79482097","<p>I can't test it but it can be common problem with <code>lambda</code> in <code>for</code>-loops.</p>
<p>And maybe some functions in pandas can be &quot;lazy&quot; and pandas may execute them later (when data will be really needed) so it may run <code>lambda</code> after executing all code in <code>for</code>-loop.</p>
<p><code>lambda</code> doesn't copy value from varaible <code>column</code> to code in <code>lambda</code> but it keeps only reference to this variable and it gets value from variable later - when variable will already have last value <code>Z</code>.</p>
<p>It needs to assign value to parameter in <code>lambda</code> and this will copy value.</p>
<pre><code>lambda val, col=column: highlight_columns(val, default_colors[col]), subset=[col]
</code></pre>
","1","Answer"
"79483090","79483085","<p>If need replace values less like maximum by <code>Series</code> use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.where.html"" rel=""nofollow noreferrer""><code>DataFrame.where</code></a> or <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.clip.html"" rel=""nofollow noreferrer""><code>DataFrame.clip</code></a>:</p>
<pre><code>mask = data &gt; max_vals
out  = data.where(mask, max_vals, axis=1)
</code></pre>
<hr />
<pre><code>out = data.clip(lower=max_vals, axis=1)
print (out)
   a  b  c   d   e
0  3  4  7   6   7
1  3  4  5   6   7
2  7  8  5  10  11
</code></pre>
<p>If need replace maximum use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html"" rel=""nofollow noreferrer""><code>DataFrame.mask</code></a> or <code>clip</code>:</p>
<pre><code>mask = data &gt; max_vals
out  = data.mask(mask, max_vals, axis=1)
</code></pre>
<hr />
<pre><code>out = data.clip(upper=max_vals, axis=1)
print (out)
   a  b  c  d  e
0  1  2  5  4  5
1  2  3  4  5  6
2  3  4  5  6  7
</code></pre>
","2","Answer"
"79483357","79482097","<p>The issue with your refactored version is due to &quot;late binding&quot;, as explained in the <a href=""https://stackoverflow.com/a/79482693/18470692"">answer</a> by <a href=""https://stackoverflow.com/users/1832058/furas"">@furas</a>. See also: <a href=""https://docs.python.org/3/faq/programming.html#why-do-lambdas-defined-in-a-loop-with-different-values-all-return-the-same-result"" rel=""nofollow noreferrer"">Why do lambdas defined in a loop with different values all return the same result?</a>.</p>
<p>For your use case, consider using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.apply.html"" rel=""nofollow noreferrer""><code>Styler.apply</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

default_colors = {
    # &quot;A&quot;: &quot;brown&quot;,
    &quot;B&quot;: &quot;blue&quot;,
    &quot;C&quot;: &quot;green&quot;,
    &quot;X&quot;: &quot;orange&quot;,
    &quot;Y&quot;: &quot;red&quot;,
    # &quot;Z&quot;: &quot;purple&quot;
}

def highlight_columns(s, default_colors):
    if s.name in default_colors:
        val = f'color: {default_colors[s.name]}; font-weight: bold;'
        return [val]*len(s)
    return ['']*len(s)
    
styled_df = df.style.apply(highlight_columns, axis=0, 
                           default_colors=default_colors)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/BHklGZQz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BHklGZQz.png"" alt=""print styled_df"" /></a></p>
<p><strong>Explanation</strong></p>
<ul>
<li>Apply <code>highlight_columns</code> to each column (<code>axis=0</code>).</li>
<li>Check if the column name (<code>s.name</code>) exists in <code>default_colors</code>.</li>
<li>Return a <code>list</code> (list-like object) of same length with the appropriate CSS or <code>''</code>.</li>
</ul>
","2","Answer"
"79483682","79481878","<p>Define a formatter that takes a list and prints it the way you want it. Then apply it to the dataframe:</p>
<pre><code>formatter = lambda l: ', '.join('{:0.2f}'.format(i) for i in l)
df.style.format(formatter)
</code></pre>
<p>Should print out what you want:</p>
<pre><code>    Values
0   0.12, 0.00, 0.00
1   0.00, 0.00, 0.00
</code></pre>
","1","Answer"
"79483704","79483656","<p>A possible solution:</p>
<pre><code>boundaries = np.where(np.diff(aa) != 0)[0] + 1 #group boundaries

get_idx_freqs = lambda i, d: (np.concatenate(([0], i))[d &gt;= 2], d[d &gt;= 2])
idx, freqs = get_idx_freqs(boundaries, np.diff(np.r_[0, boundaries, len(aa)]))
</code></pre>
<p>The process begins by detecting group boundaries with <code>np.where(np.diff(aa)!=0)[0]+1</code>, which locates indices where the value changes and marks the start of new groups. Next, group lengths are computed by concatenating the starting index, the change indices, and the array's end using <code>np.r_[0, boundaries, len(aa)]</code>, and then applying <code>np.diff</code> to obtain the lengths of these groups. Finally, a lambda function applies a mask (<code>d&gt;=2</code>) to both the start indices and the group lengths, filtering out any groups of only one element. (See <a href=""https://numpy.org/doc/stable/reference/generated/numpy.diff.html"" rel=""nofollow noreferrer""><code>np.diff</code></a> and <a href=""https://numpy.org/doc/2.2/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>np.where</code></a>.)</p>
<p>Output:</p>
<pre><code># aa=np.array([1,2,2,3,3,3,4,4,4,4,5,5,5,5,5])
(array([ 1,  3,  6, 10]), array([2, 3, 4, 5]))

# aa=np.array([1,1,1,np.nan,np.nan,1,1,np.nan])
(array([0, 5]), array([3, 2]))
</code></pre>
","1","Answer"
"79483768","79481878","<p>To properly format the data I am using a function to iterate over each list to format the number manually. It doesn't seem to be very optimised approach as it takes <strong>O(M*N)</strong> -&gt; where M is number of element in each list and N is number of rows in dataframe.</p>
<pre><code>&gt;&gt;&gt; df = pd.DataFrame(data={'Value': [[0.1231245678, 0, 0], [1e-10,0,0]]})
&gt;&gt;&gt; df
                  Value
0  [0.1231245678, 0, 0]
1         [1e-10, 0, 0]
&gt;&gt;&gt; func = lambda X: [f&quot;{i:.2f}&quot; for i in X]
&gt;&gt;&gt; df = df['Value'].apply(func)
&gt;&gt;&gt; df
0    [0.12, 0.00, 0.00]
1    [0.00, 0.00, 0.00]
Name: Value, dtype: object
</code></pre>
","0","Answer"
"79483956","79483911","<p>You need to compare the values row by row and filter the rows where the values in the two columns are not equal, try like below:</p>
<pre><code># Loop and display differences
for a, b in thisdict.items():
    # Compare the columns row by row
    mismatches = dfx[dfx[a] != dfx[b]]
    if not mismatches.empty:
        print(f&quot;Mismatches between '{a}' and '{b}':&quot;)
        print(mismatches[[a, b]])
</code></pre>
","3","Answer"
"79483974","79483911","<p>This kind of filtering for all rows is what Pandas and other dataframe frameworks give you for free.</p>
<p>By making the comparison on the columns you get a boolean series, which in turn can work as indices for the original dataframe and automatically select the columns that interest your:</p>
<pre><code>for a, b in thisdict.items():
     diff = dfx[dfx[a] != dfx[b]]
     print(diff[[a, b]])
</code></pre>
","1","Answer"
"79484836","79484795","<pre><code>df = df.sort_values(by=['store_nbr', 'product_sku', 'date'])
df['price_change'] = df.groupby(['store_nbr', 'product_sku'])['retail'].apply(lambda x: x != x.shift()).values
df['date_price_change'] = df['date'].where(df['price_change']).ffill()
</code></pre>
","1","Answer"
"79485812","79485775","<p>Assuming your headers start with a letter from <code>[A-Za-z]</code> you can pass a regex with a positive lookahead for the comma-option to <code>sep</code> inside <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>pd.read_csv</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from io import StringIO

csv = &quot;&quot;&quot;some file header row number 1
some file header row number two
some more information about data in that file
column1;column2;column3,column4,column5
1,234;2,665;0,888;3,891;3,762
2,232;1,233;0,888;3,789;3,524&quot;&quot;&quot;

df = pd.read_csv(StringIO(csv), 
                 sep=r';|,(?=[A-Za-z])', 
                 skiprows=3, 
                 engine='python', 
                 decimal=',')
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>   column1  column2  column3  column4  column5
0    1.234    2.665    0.888    3.891    3.762
1    2.232    1.233    0.888    3.789    3.524
</code></pre>
<p><a href=""https://regex101.com/r/UMW0NJ/1"" rel=""nofollow noreferrer"">Regex explanation</a>. Or use: <code>r';|,(?=[^\d])'</code> (not followed by a digit).</p>
<p>You can of course adjust the regex if you need a more strict solution to ensure that the option for the comma won't match for the subsequent rows.</p>
<hr />
<p><strong>Edit:</strong> to clarify, the equivalent for your file path is:</p>
<pre class=""lang-py prettyprint-override""><code>pd.read_csv(&quot;mypath\test.csv&quot;, sep=r';|,(?=[A-Za-z])', ...)

# replacing `StringIO(csv)` with `&quot;mypath\test.csv&quot;`
</code></pre>
<p><em>Not</em>:</p>
<pre class=""lang-py prettyprint-override""><code>pd.read_csv(StringIO(&quot;mypath\test.csv&quot;), sep=r';|,(?=[A-Za-z])', ...)
</code></pre>
<p>which generates the error mentioned in the <a href=""https://stackoverflow.com/questions/79485775/csv-to-dataframe-with-several-delimiters-and-software-bug/79485812#comment140180192_79485812"">comment</a>:</p>
<pre class=""lang-py prettyprint-override""><code>EmptyDataError: No columns to parse from file
</code></pre>
","2","Answer"
"79485928","79485898","<p>One option could be to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.repeat.html"" rel=""nofollow noreferrer""><code>Index.repeat</code></a> the rows based on the number of days, generate the intermediate days with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>groupby.cumcount</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.to_timedelta.html"" rel=""nofollow noreferrer""><code>pd.to_timedelta</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html"" rel=""nofollow noreferrer""><code>value_counts</code></a>:</p>
<pre><code># ensure datetime
df[['date_from', 'date_to']] = df[['date_from', 'date_to']].apply(pd.to_datetime)

# number of days between from and to
n = df['date_to'].sub(df['date_from']).dt.days

# repeat the days
s = df.loc[df.index.repeat(n+1), 'date_from']
# increment to create intermediates and count
out = (s.add(pd.to_timedelta(s.groupby(level=0).cumcount(), unit='day'))
        .value_counts(sort=False)
      )
</code></pre>
<p>Output:</p>
<pre><code>2019-08-01    1
2019-08-02    1
2019-08-03    1
2019-08-04    2
2019-08-05    2
2019-08-06    1
2019-08-07    2
2019-08-08    1
2019-08-09    1
Name: count, dtype: int64
</code></pre>
<p>Intermediates:</p>
<pre><code># n
0    4
1    3
2    2
dtype: int64

# s
0   2019-08-01
0   2019-08-01
0   2019-08-01
0   2019-08-01
0   2019-08-01
1   2019-08-04
1   2019-08-04
1   2019-08-04
1   2019-08-04
2   2019-08-07
2   2019-08-07
2   2019-08-07
Name: date_from, dtype: datetime64[ns]

# s.add(pd.to_timedelta(s.groupby(level=0).cumcount(), unit='day'))
0   2019-08-01
0   2019-08-02
0   2019-08-03
0   2019-08-04
0   2019-08-05
1   2019-08-04
1   2019-08-05
1   2019-08-06
1   2019-08-07
2   2019-08-07
2   2019-08-08
2   2019-08-09
dtype: datetime64[ns]
</code></pre>
<h3>another option, using a generator</h3>
<pre><code>import numpy as np

out = pd.Series.value_counts(np.fromiter((d for f, t in
                                          zip(df['date_from'], df['date_to'])
                              for d in pd.date_range(f,t)), 'datetime64[ns]'),
                             sort=False)
</code></pre>
<p>Output:</p>
<pre><code>2019-08-01    1
2019-08-02    1
2019-08-03    1
2019-08-04    2
2019-08-05    2
2019-08-06    1
2019-08-07    2
2019-08-08    1
2019-08-09    1
Name: count, dtype: int64
</code></pre>
<h3>Timings</h3>
<p>on 3 rows:</p>
<pre><code># pandas repeat + groupby.cumcount+timedelta + value_counts
1.6 ms ± 169 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

# numpy iterator + value_counts
604 µs ± 32.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</code></pre>
<p>on 30K rows:</p>
<pre><code># pandas repeat + groupby.cumcount+timedelta + value_counts
18.1 ms ± 577 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

# numpy iterator + value_counts
3.52 s ± 70.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
","1","Answer"
"79486221","79473874","<p>For big data, we utilize <em><strong>NumPy</strong></em> to efficiently compute rolling averages, as it outperforms traditional methods in speed. When necessary, we group the data and incorporate <em><strong>scikit-learn's FunctionTransformer</strong></em> in our pipelines to maintain organized code. This approach ensures that our analysis remains quick, efficient, and scalable.</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.preprocessing import FunctionTransformer


data = {
    'values': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],
    'type': ['A','B','A','B','A','B','A','B','A','B','A','B','A','B','A'],
    'type2': ['C','D','C','D','C','D','C','D','C','D','C','D','C','D','C']
}
df = pd.DataFrame(data)

# Rolling window function using NumPy
def rollingWindow_UsingNumpy(arr, window):
    cumsum = np.cumsum(arr, dtype=float)
    cumsum[window:] = cumsum[window:] - cumsum[:-window]
    adjusted_window_size = np.minimum(np.arange(1, len(arr) + 1), window)
    return cumsum / adjusted_window_size

# Define Sklearn transformer for batch processing
rolling_transformer = FunctionTransformer(lambda x: rollingWindow_UsingNumpy(x, window=3),
                                           validate=False)

# Apply rolling mean with Sklearn in an optimized way
df['mean'] = df.groupby(['type', 'type2'], group_keys=False)['values'].transform(
    lambda x: rolling_transformer.transform(x.to_numpy())
)
print(df)
'''
   values type type2  mean
0        1    A     C   1.0
1        2    B     D   2.0
2        3    A     C   2.0
3        4    B     D   3.0
4        5    A     C   3.0
5        6    B     D   4.0
6        7    A     C   5.0
7        8    B     D   6.0
8        9    A     C   7.0
9       10    B     D   8.0
10      11    A     C   9.0
11      12    B     D  10.0
12      13    A     C  11.0
13      14    B     D  12.0
14      15    A     C  13.0
'''
</code></pre>
","0","Answer"
"79487462","79487265","<p>That is correct, the DateTime column is a Series, not a date. You are trying to apply <code>isoformat()</code> to the entire column <code>df['DateTime']</code>, and it does not have this method.</p>
<p>Each column of a DataFrame is a Series. Series have methods to deal directly with dates that you can access with <code>.dt</code>. Alternatively, you can apply a function to every element of the series with <code>apply</code> or <code>map</code>.</p>
<p>It is not clear whether you want to hold the DateTime values as strings, each one the date in isoformat, or if you just want to print the dataframe with this format.</p>
<p>If you want to hold a column with the dates converted to strings you could do one of these two:</p>
<pre><code>df['DateTime1'] = df['DateTime'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f')

# OR
df['DateTime2'] = df['DateTime'].apply(lambda d: d.isoformat())
</code></pre>
<p>If you just want to print them, you can also do this:</p>
<pre><code>df.style.format('{0:%Y-%m-%dT%H:%M:%S.%f}', subset=['DateTime'])

# OR
df.style.format(lambda d: d.isoformat(), subset=['DateTime'])
</code></pre>
","2","Answer"
"79487588","79485898","<p>Your calculations are taking a long time maybe because a dataframe is not the best data structure for what you are trying to do. What you want is probably an Interval Tree or a Segment Tree, which will allow you to cut the time it takes to find all intervals that intersect with a date from linear to logarithmic (in the number of intervals).</p>
<p>However, here is a quick pandas implementation</p>
<pre><code>res = pd.DataFrame()
res['date'] = pd.date_range(df['date_from'].min(), df['date_to'].max())

def in_interval(d):
    return (df['date_from']&lt;=d) &amp; (df['date_to']&gt;=d)
res['count'] = df2['date'].apply(lambda d: df[in_interval(d)].shape[0])
</code></pre>
","0","Answer"
"79487618","79481317","<p>Rethinking this, I came up with a more performant solution. Iterating over each pair of x0 and x1 values and converting them from floating point positions to the corresponding indices of the bins array works and is much faster than calling df.loc() within a loop.</p>
<pre><code> def count_unmarked_strips(df, total_width, strip_width):
    n_total = int(total_width // strip_width)
    bins = np.ones(n_total, dtype=bool)
    for x_range in df[['x0', 'x1']].to_numpy():
        x0, x1 = (x_range // strip_width).astype(int)
        bins[x0:x1+1] = False

    return bins.sum()
</code></pre>
","0","Answer"
"79489358","79489149","<p>A possible solution is to use <code>sklearn</code> -<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"" rel=""nofollow noreferrer""><code>sklearn.model_selection.StratifiedKFold</code></a>, which seems to do exactly what you are looking for and seamlessly integrates with pandas and numpy.</p>
","1","Answer"
"79489808","79489723","<p>Based on the comment by Nils Werner, I tried disabling multithreading in Numpy. And now I get the gains from parallelization. Interestingly, the serial version is also about twice as fast.</p>
<pre><code>import time
import os
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['NUMEXPR_MAX_THREADS'] = '1'
import pandas as pd
import numpy as np
import multiprocessing as mp
    
def linreg(df):
    y = df[['y']].values
    x = np.hstack([np.ones((df.shape[0], 1)), df[['treat']].values])
    
    xx_inv = np.linalg.inv(x.T @ x)
    beta_hat = xx_inv @ (x.T @ y)
    
    return pd.Series(beta_hat.flat, index=['intercept', 'coef'])

def shuffle_treat(df):
    df['treat'] = df['treat'].sample(frac=1, replace=False).values
    return df
    
def run_analysis(draws, seed):
    
    N = 5000
    df = pd.DataFrame({'treat':np.random.choice([0,1], size=N, replace=True)})
    df['u'] = np.random.normal(size=N)
    df['y'] = df.eval('10 + 5*treat + u')

    np.random.seed(seed)
    
    est = [linreg(shuffle_treat(df)) for k in range(draws)]

    est = pd.concat(est, axis=0, sort=False, keys=range(draws), names=['k', 'param'])
    
    return est

draws = 500
param_list = [dict(draws=draws, seed=1029), dict(draws=draws, seed=1029)]
param_list_sleep = [dict(draws=draws, seed=1029, sleep=5), dict(draws=draws, seed=1029, sleep=5)]

def run_analysis_wrapper(params):
    run_analysis(**params)

start = time.time()
for params in param_list:
    run_analysis_wrapper(params)
end = time.time()
print(f'double run 1 process: {(end - start):.2f} sec')

start = time.time()
with mp.Pool(processes=2) as pool:
    pool.map(run_analysis_wrapper, param_list)
end = time.time()
print(f'double run 2 processes: {(end - start):.2f} sec')
</code></pre>
<p>Output:</p>
<pre><code>double run 1 process: 1.34 sec
double run 2 processes: 0.67 sec
</code></pre>
","2","Answer"
"79489814","79489723","<p>Your test data are too small to get meaningful timing data.</p>
<p>The sleep serves no purpose other than to &quot;prove&quot; that you have multiple processes running.</p>
<p>Thus, if we increase the number of entries in <em>param_list</em> to, say 10, then we see the benefits of multiprocessing.</p>
<pre><code>import pandas as pd
import numpy as np
import multiprocessing as mp
from timeit import timeit
    
def linreg(df):
    y = df[['y']].values
    x = np.hstack([np.ones((df.shape[0], 1)), df[['treat']].values])
    xx_inv = np.linalg.inv(x.T @ x)
    beta_hat = xx_inv @ (x.T @ y)
    return pd.Series(beta_hat.flat, index=['intercept', 'coef'])

def shuffle_treat(df):
    df['treat'] = df['treat'].sample(frac=1, replace=False).values
    return df
    
def run_analysis(draws, seed):
    N = 5000
    df = pd.DataFrame({'treat':np.random.choice([0,1], size=N, replace=True)})
    df['u'] = np.random.normal(size=N)
    df['y'] = df.eval('10 + 5*treat + u')
    np.random.seed(seed)
    est = [linreg(shuffle_treat(df)) for k in range(draws)]
    return pd.concat(est, axis=0, sort=False, keys=range(draws), names=['k', 'param'])

def run_analysis_wrapper(params):
    run_analysis(**params)

def serial(param_list):
    for params in param_list:
        run_analysis_wrapper(params)

def multi(param_list):
    with mp.Pool() as pool:
        pool.map(run_analysis_wrapper, param_list)

if __name__ == &quot;__main__&quot;:
    param_list = [dict(draws=500, seed=1029) for _ in range(10)]
    for func in serial, multi:
        s = timeit(lambda: func(param_list), number=1)
        print(func.__name__, f&quot;{s:.4f}s&quot;)
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>serial 1.8292s
multi 0.8249s
</code></pre>
<p><strong>Platform:</strong></p>
<pre><code>Python 3.13.2
pandas 2.2.3
numpy 2.2.3
MacOS 15.3.1
Apple M2 (effectively 8 CPUs)
</code></pre>
<p><strong>Note:</strong></p>
<p>Setting of environment variables OMP_NUM_THREADS, MKL_NUM_THREADS, OPENBLAS_NUM_THREADS and NUMEXPR_MAX_THREADS makes no measurable difference to the timings on my platform</p>
","3","Answer"
"79490227","79485898","<p>You can use <strong>explode</strong> method and get the number of dates for all ranges:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    'date_from': pd.to_datetime(['2019-08-01', '2019-08-04', '2019-08-07']),
    'date_to': pd.to_datetime(['2019-08-05', '2019-08-07', '2019-08-09'])
})

df['date_range'] = df.apply(lambda x: pd.date_range(x['date_from'], x['date_to']), axis=1)

df_exploded = df.explode('date_range')

# Count occurrences of each date
result = df_exploded['date_range'].value_counts().sort_index().reset_index()
result.columns = ['date', 'count']

print(result)
</code></pre>
","0","Answer"
"79490397","79490056","<p>You could use a function factory to simplify the syntax:</p>
<pre><code>def quantile(q=0.5, **kwargs):
    def f(series):
        return series.quantile(q, **kwargs)
    return f
    
df.groupby('dummy').agg(
    bell_med=('bell', 'median'),
    bell_mean=('bell', 'mean'),
    fish_med=('fish', 'median'),
    fish_mean=('fish', 'mean'),
    bell_q10=('bell', quantile(0.1)),
    fish_q10=('fish', quantile(0.1)),
)
</code></pre>
<p>If you have many combinations, you could also combine this with a dictionary comprehension and parameter expansion:</p>
<pre><code>df.groupby('dummy').agg(**{'bell_med': ('bell', 'median'),
                           'bell_mean': ('bell', 'mean'),
                           'fish_med': ('fish', 'median'),
                           'fish_mean': ('fish', 'mean'),
                           },
                        **{f'{c}_q{100*q:02n}': (c, quantile(q))
                           for q in [0.1] # add more if needed
                           for c in ['bell', 'fish']
                          }
                       )
</code></pre>
<p>Output:</p>
<pre><code>
       bell_med  bell_mean  fish_med  fish_mean  bell_q10  fish_q10
dummy                                                              
1     -0.063454  -0.058557      10.0       9.92 -1.553682       6.0
</code></pre>
","3","Answer"
"79490599","79490056","<p>Consider <code>lambda</code> to expand the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.quantile.html"" rel=""nofollow noreferrer""><strong><code>Series.quantile</code></strong></a> call:</p>
<pre class=""lang-py prettyprint-override""><code>agg_df = df.groupby('dummy').agg(
    bell_med=('bell', 'median'),
    bell_mean=('bell', 'mean'),
    fish_med=('fish', 'median'),
    fish_mean=('fish', 'mean'),
    bell_q10=('bell', lambda ser: ser.quantile(0.1)),
    fish_q10=('fish', lambda ser: ser.quantile(0.1))
)

agg_df
#        bell_med  bell_mean  fish_med  fish_mean  bell_q10  fish_q10
# dummy                                                              
# 1     -0.063454  -0.058557      10.0       9.92 -1.553682       6.0
</code></pre>
<p>To borrow @mozway's dynamic <a href=""https://stackoverflow.com/a/79490397/1422451"">solution</a> to dictionarize the aggregate functions, an <a href=""https://stackoverflow.com/questions/36805071/dictionary-comprehension-with-lambda-functions-gives-wrong-results#comment61184240_36805118"">extra lambda input</a> is needed within the dict comprehension:</p>
<pre class=""lang-py prettyprint-override""><code>agg_df = df.groupby('dummy').agg(
    **{
        'bell_med': ('bell', 'median'),
        'bell_mean': ('bell', 'mean'),
        'fish_med': ('fish', 'median'),
        'fish_mean': ('fish', 'mean'),
    },
    **{
        f'{c}_q{100*q:02n}': (c, lambda ser, q=q: ser.quantile(q))
        for q in [0.1, 0.5, 0.9]
        for c in ['bell', 'fish']
    }
)

agg_df
#        bell_med  bell_mean  fish_med  fish_mean  bell_q10  fish_q10  bell_q50  fish_q50  bell_q90  fish_q90
# dummy                                                                                                      
# 1     -0.063454  -0.058557      10.0       9.92 -1.553682       6.0 -0.063454      10.0  1.045002      14.0


</code></pre>
","2","Answer"
"79490644","79489149","<p>You can try to use the function <code>train_test_split</code> with stratifying for some column what you need. See the docs of <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split"" rel=""nofollow noreferrer""><code>sklearn.model_selection.train_test_split</code></a> .</p>
<pre><code>import pandas as pd
from sklearn.model_selection import train_test_split

data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],
    'Age': [25, 30, 35, 40, 28, 33, 38, 45],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'New York', 'Los Angeles', 'Chicago', 'Houston'],
    'Salary': [70000, 80000, 90000, 100000, 75000, 85000, 95000, 110000]
}

df = pd.DataFrame(data)

# Split the dataset into two parts, stratifying by the 'City' column
part1, part2 = train_test_split(df, test_size=0.5, random_state=42, stratify=df['City'], shuffle=True)

print(&quot;Part 1:&quot;)
print(part1)
print(&quot;\nPart 2:&quot;)
print(part2)
</code></pre>
<p>Results:</p>
<pre><code>Part 1:
    Name  Age         City  Salary
3  David   40      Houston  100000
4    Eve   28     New York   75000
6  Grace   38      Chicago   95000
5  Frank   33  Los Angeles   85000

Part 2:
      Name  Age         City  Salary
0    Alice   25     New York   70000
2  Charlie   35      Chicago   90000
7    Henry   45      Houston  110000
1      Bob   30  Los Angeles   80000
</code></pre>
","1","Answer"
"79490823","79485898","<p>One option is with <a href=""https://pyjanitor-devs.github.io/pyjanitor/api/functions/#janitor.functions.conditional_join.get_join_indices"" rel=""nofollow noreferrer"">get_join_indices</a> from <a href=""https://pyjanitor-devs.github.io/pyjanitor/"" rel=""nofollow noreferrer"">pyjanitor</a> (I am a contributor to this library):</p>
<pre><code># create a DataFrame of unique dates
import pandas as pd
import janitor as jn
dates = {'dates': pd.date_range(df.min(axis=None), df.max(axis=None))}
dates = pd.DataFrame(dates)
# use get_join_indices from pyjanitor to get matching locations (inner join)
join_conditions = [('dates', 'date_from', '&gt;='), ('dates', 'date_to', '&lt;=')]
left_positions, right_positions = jn.get_join_indices(
    dates,
    df,
    conditions=join_conditions)
# get counts with `value_counts`
counts = pd.Series(left_positions).value_counts(sort=False)
# final result:
dates.assign(counts=counts)
</code></pre>
<pre><code>       dates  counts
0 2019-08-01       1
1 2019-08-02       1
2 2019-08-03       1
3 2019-08-04       2
4 2019-08-05       2
5 2019-08-06       1
6 2019-08-07       2
7 2019-08-08       1
8 2019-08-09       1
</code></pre>
<p>it should stay performant even for large dataframes</p>
","1","Answer"
"79491665","79491662","<p>While I lack the technical understanding to fully explain it, it seems like the session passed into `to_sql()` does not have access to the active transaction context. So when it tries to insert the foreign key, it (rightly) complains that it does not exist yet in its parent table, because it only exist in the transaction.</p>
<p>I did replace it with the iterative approach:</p>
<pre><code>for index, row in df_raw.iterrows():

    insert_row = ssql.sqltbl_raw_plates_data.insert().values(**row)

    session.execute(insert_row)
</code></pre>
<p>And that has solved the issue. So the topic, at least functionally is solved. Though if anyone has technical insights to better understand it, or indeed a way how it could work with `to_sql()`, feel free to share!</p>
","0","Answer"
"79492074","79492024","<p>Add this line so that if there are NaT values in the 'TicketDate' column, it converts to None rather than throwing an error.</p>
<pre><code>df['TicketDate'] = df['TicketDate'].fillna(pd.NaT).apply(
    lambda x: x.strftime('%Y-%m-%d') if pd.notna(x) else None
)
</code></pre>
","1","Answer"
"79492974","79485072","<p>Found my solution, posting as an answer in case anyone else finds this through Google or something.</p>
<p>Anon Coward's comment about needing .item() did make the test work, however it broke the actual code when I ran it on data, so obviously I had to find another solution.</p>
<p>It turns out, all I had to do was make the numbers in the test into floats instead of integers - so the INSERT and final lines of my test turned into</p>
<pre><code>testobj.cur.execute(&quot;&quot;&quot;INSERT INTO thingies VALUES (1.0, 2.0, &quot;3.0&quot;, 4.0, 5.0, &quot;yay thingy&quot;)&quot;&quot;&quot;)
</code></pre>
<p>and</p>
<pre><code>testobj.get_thingy(pd.Series({'col1': 1.0, 'col2': 2.0, 'col3': 3.0, 'col4': 4.0, 'col5': 5.0}))
</code></pre>
<p>This makes the test work without having to change the code I'm testing.</p>
","0","Answer"
"79493192","79492749","<p>OK - so this sort of works but gives me double entries in the box plot legend ...</p>
<p><a href=""https://i.sstatic.net/jDqxstFd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jDqxstFd.png"" alt=""enter image description here"" /></a></p>
<pre><code>import holoviews as hv
import hvplot.pandas
import pandas as pd

data = pd.read_csv('https://raw.githubusercontent.com/ChrisWalshaw/DataViz/master/Data/Products/DailySales.csv', index_col=0)
data.index = pd.to_datetime(data.index)
print(data.head())

selected = ['A', 'F', 'L']

plot1 = data[selected].hvplot.line(
    frame_height=300, frame_width=300,
)
plot2 = data[selected].hvplot.hist(
    frame_height=300, frame_width=300,
)
plot3 = data[selected].melt().hvplot.box(
    frame_height=300, frame_width=300, by='variable', c='variable',
    cmap=hv.Cycle.default_cycles['default_colors'],
)
plot = plot1 + plot2 + plot3
hvplot.show(plot)
</code></pre>
","0","Answer"
"79493210","79493179","<p><strong>.astype(str)</strong> resolved the issue.</p>
<pre><code>df = pd.json_normalize(data[field]).astype(str)
</code></pre>
<p>Basically, need to convert all the types to string to overcome the type isseus.</p>
","0","Answer"
"79493552","79493514","<p>You're escaping the backslashes, so the doublequotes aren't being taken literally. As a result, the shell is treating them as string delimiters, not passing them to python.</p>
<p>The simplest fix would be to put the entire argument in single quotes.</p>
<pre><code>python script.py --input '[{&quot;A&quot;:&quot;322|985&quot;,&quot;B&quot;:3}]'
</code></pre>
<p>There's no need to call <code>json.dumps(input_data)</code>. <code>input_data</code> is a JSON string, not data, so it doesn't need to be converted to JSON.</p>
<p><code>json_data</code> is already a list because the JSON has <code>[]</code>. You don't need to wrap it in another list when calling <code>pd.DataFrame()</code>.</p>
<p>So the corrected version of <code>validate_input()</code> is:</p>
<pre><code>def validate_input(input_data):

    if isinstance(input_data, pd.DataFrame):
        return input_data  # Already a DataFrame, return as is

    json_data = json.loads(input_data)

    return pd.DataFrame(json_data)  # Convert JSON serializable to DataFrame
</code></pre>
","2","Answer"
"79493666","79493638","<p>You can try something like this:</p>
<pre><code>df_input = {'supply_temp':0, 'liquid_mass_flow':0,'air_inlet_temp':0}

flow = 60
inputs = np.array([45,flow*988/60000,35])

df_input = {key: input for key, input in  zip(df_input.keys(),inputs)}
</code></pre>
","0","Answer"
"79494082","79494025","<p>The script itself works and CSV can be easily imported into Excel. Alternatively, export the data directly <code>.to_excel('your_excle_file.xlsx')</code> and open it in Excel.</p>
<hr />
<p>Since you are already operating with <code>pandas</code>, just use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_html.html"" rel=""nofollow noreferrer""><code>pandas.read_html</code></a> which BeautifulSoup uses in the background to scrape the tables.</p>
<pre><code>import pandas as pd

df_list = pd.read_html('https://www.dsebd.org/top_20_share.php', match='TRADING CODE')

# first table only
df_list[0].to_csv(&quot;first_top_20_share_value_22.csv&quot;)

# all three tables in concat
pd.concat(df_list, ignore_index=True).to_csv(&quot;all_top_20_share_value_22.csv&quot;)
</code></pre>
","2","Answer"
"79495510","79455667","<pre><code>import numpy as np
import pandas as pd

df1 = pd.DataFrame({'col1': [1, 2, 3, 4]})
df2 = pd.DataFrame({'col1': [11, 22, 33, 44]})

df1Num = df1['col1'].values
df2Num = df2['col1'].values
window = 2 

order = np.argsort(
    np.r_[np.arange(len(df1Num)) // window, np.arange(len(df2Num)) // window]
)
'''
[0 1 4 5 2 3 6 7]
'''
res  = np.r_[df1Num,df2Num][order]
'''
[ 1  2 11 22  3  4 33 44]
'''

res_df = pd.DataFrame({'col1' : res})
'''
   col1
0     1
1     2
2    11
3    22
4     3
5     4
6    33
7    44
'''

</code></pre>
","0","Answer"
"79495549","79455667","<p>This can be easily done using Pandas indexing.</p>
<pre class=""lang-py prettyprint-override""><code>df1 = pd.DataFrame({'column1': [1,2,3,4]})
df2 = pd.DataFrame({'column1': [5,6,7,8]})
assert df1.shape == df2.shape

n = df1.shape[0]
df3 = pd.DataFrame({'column1': 0}, index=np.arange(n*2))
idx = np.tile(np.array([True, True, False, False]), n // 2)
df3.loc[idx] = df1.to_numpy()
df3.loc[~idx] = df2.to_numpy()
print(df3)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>   column1
0        1
1        2
2        5
3        6
4        3
5        4
6        7
7        8
</code></pre>
<p>Alternative:</p>
<pre class=""lang-py prettyprint-override""><code>n = df1.shape[0]
df3 = pd.DataFrame({'column1': 0}, index=np.arange(n*2))
idx = np.stack([np.arange(0, n*2, 4), np.arange(1, n*2, 4)]).T.flatten()
df3.loc[idx] = df1.to_numpy()
df3.loc[idx + 2] = df2.to_numpy()
</code></pre>
","0","Answer"
"79495580","79452360","<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'date_parts': [[29, 'August', 2024], [28, 'August', 2024], [27, 'August', 2024]]})

# Convert components to NumPy array
dates = np.array(df['date_parts'].tolist())

aa = np.char.add(
np.char.add(dates[:,2],'-'),
np.char.add(dates[:,1],'-') + dates[:,0]  
)
'''
['2024-August-29' '2024-August-28' '2024-August-27']
'''
df['release_date'] = pd.to_datetime(aa, format='%Y-%B-%d')
'''
         date_parts release_date
0  [29, August, 2024]   2024-08-29
1  [28, August, 2024]   2024-08-28
2  [27, August, 2024]   2024-08-27
'''
</code></pre>
","0","Answer"
"79495639","79495612","<p>Defining the type to <strong>ArrayType(MapType(StringType(), StringType()))</strong> resolved the issue</p>
<pre><code>schema = StructType([
    StructField('Id', StringType(), True), \
    StructField('Field', ArrayType(MapType(StringType(), StringType())), True))]
</code></pre>
","0","Answer"
"79495812","79495724","<pre><code>data = pd.read_csv(
    'Log_jeden_den.log',
 sep=r'\s+(?=(?:[^&quot;]*&quot;[^&quot;]*&quot;)*[^&quot;]*$)(?=(?:[^\[]*\[[^\]]*\])*[^\]]*$)',
    engine='python',
    na_values='-',
    header=None,
    usecols=[0, 3, 4, 5, 6, 7, 8],
    names=['ip', 'time', 'request', 'status', 'size', 'referer', 'user_agent'],
    converters={'time': parse_datetime,
                'request': parse_str,
                'status': parse_int,
                'size': parse_int,
                'referer': parse_str,
                'user_agent': parse_str})
</code></pre>
<p><a href=""https://i.sstatic.net/zQXzX95n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zQXzX95n.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79496244","79496178","<p>First: when it reads data then it should convert values to integers so there is no need to use <code>map(int, ...)</code>. And <code>apply( ...list ...)</code> creates lists so there is no need to use <code>eval()</code>.</p>
<hr />
<p>Problem is because <code>groupby().apply()</code> created DataFrame with name <code>0</code> instead of <code>&quot;Số&quot;</code>and later it raised error in <code>grouped_data[&quot;Số&quot;].apply(...)</code>, not <code>grouped_data.loc[:, &quot;Số&quot;]</code></p>
<p>You can reduce code to</p>
<pre><code>grouped_data = df.groupby(&quot;Ngày&quot;)[&quot;Số&quot;].apply(list).reset_index(name=&quot;Số&quot;)
</code></pre>
<p>which will convert to list and set name <code>&quot;Số&quot;</code> again. I uses <code>[&quot;Số&quot;]</code> instead of <code>[[&quot;Số&quot;]]</code></p>
<p>Because pandas keep data as numpy.array so you can get</p>
<pre><code>data_matrix = grouped_data[&quot;Số&quot;].values
</code></pre>
<hr />
<p>Full code used for tests:</p>
<p>I used <code>io.StringIO</code> only to create file-like object in memory - so everyone can simply copy and run it - but you can use filename.</p>
<pre><code>import numpy as np
import pandas as pd


text = '''Ngày,Số
07/03/2025,8
07/03/2025,9
06/03/2025,6
06/03/2025,10
06/03/2025,18
06/03/2025,14
'''

import io

df = pd.read_csv(io.StringIO(text), encoding=&quot;utf-8&quot;, sep=&quot;,&quot;)
#df = pd.read_csv(&quot;C:/Users/Admin/lonum_fixed.csv&quot;, encoding=&quot;utf-8&quot;, sep=&quot;,&quot;)
df.columns = df.columns.str.strip()
print('----')
print(df)
print('----')
print(df.dtypes)

grouped_data = df.groupby(&quot;Ngày&quot;)[&quot;Số&quot;].apply(list).reset_index(name=&quot;Số&quot;)
print('---')
print(grouped_data)
print('----')
print('type:', type(grouped_data))

print('---')
print('type:', type(grouped_data[&quot;Số&quot;].values))
print('----')
print('values  :', grouped_data[&quot;Số&quot;].values)
print('np.array:', np.array(grouped_data[&quot;Số&quot;]))

data_matrix = grouped_data[&quot;Số&quot;].values
#data_matrix = np.array(grouped_data[&quot;Số&quot;])

print('----')
print('data_matrix:', data_matrix)
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>----
         Ngày  Số
0  07/03/2025   8
1  07/03/2025   9
2  06/03/2025   6
3  06/03/2025  10
4  06/03/2025  18
5  06/03/2025  14
----
Ngày    object
Số       int64
dtype: object
---
         Ngày               Số
0  06/03/2025  [6, 10, 18, 14]
1  07/03/2025           [8, 9]
----
type: &lt;class 'pandas.core.frame.DataFrame'&gt;
---
type: &lt;class 'numpy.ndarray'&gt;
----
values  : [list([6, 10, 18, 14]) list([8, 9])]
np.array: [list([6, 10, 18, 14]) list([8, 9])]
----
data_matrix: [list([6, 10, 18, 14]) list([8, 9])]
</code></pre>
","0","Answer"
"79496536","79496133","<p>It probably would be easiest to append the new data.</p>
<p>Append the new data using:</p>
<pre><code>df.to_excel(writer, sheet_name='New Sheet')
</code></pre>
<p>Opening the file in the writer shouldn't override the current information, and specifying the sheet name should avoid the override.</p>
<pre><code>xl = pd.ExcelFile('foo.xls')

xl.sheet_names  # see all sheet names
</code></pre>
<p>This will get the current sheet names so you can ensure there is no overlap.</p>
<p>Hope this helps.</p>
","0","Answer"
"79496737","79496136","<p>With a categorical axis, <code>plt</code> will use an integer index &quot;under the hood&quot;. Here, since you are using a lineplot, it tries to come up with a reasonable step:</p>
<pre class=""lang-py prettyprint-override""><code>dx.plot(ax=axes[0])
axes[0].get_xticks()

# array([-2.,  0.,  2.,  4.,  6.,  8., 10., 12.])
</code></pre>
<p>With a barplot, you would get the more logical:</p>
<pre class=""lang-py prettyprint-override""><code>dx.plot.bar(ax=axes[0])
axes[0].get_xticks()

# array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
</code></pre>
<p>You can use <a href=""https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xticks.html"" rel=""nofollow noreferrer""><code>Axes.set_xticks</code></a> and <a href=""https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html"" rel=""nofollow noreferrer""><code>Axes.set_xticklabels</code></a> to fix this. E.g.,</p>
<pre class=""lang-py prettyprint-override""><code>ticks = range(len(dx))

# only label at rank 0
labels = [f&quot;{x[0].strftime('%Y-%m-%d')}, {int(x[1])}&quot; 
          if x[1] == 0 else '' for x in dx.index]

axes[0].set_xticks(ticks=ticks)
axes[0].set_xticklabels(labels=labels, rotation=90)
</code></pre>
<p>It's easier to see now that we need the appropriate index matches for <a href=""https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.axvline.html"" rel=""nofollow noreferrer""><code>Axes.axvline</code></a>. We can apply <a href=""https://numpy.org/doc/stable/reference/generated/numpy.nonzero.html"" rel=""nofollow noreferrer""><code>np.nonzero</code></a> to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.get_level_values.html"" rel=""nofollow noreferrer""><code>Index.get_level_values</code></a> and then add the lines in a loop:</p>
<pre class=""lang-py prettyprint-override""><code>def add_vlines(ax, rank, color):
    indices = np.nonzero(dx.index.get_level_values('rank') == rank)[0]
    for index in indices:
        ax.axvline(x=index, color=color, linestyle='dotted')

add_vlines(axes[0], 1, 'blue')
add_vlines(axes[0], 100, 'red')
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/HlyxqXIO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HlyxqXIO.png"" alt=""plot"" /></a></p>
","1","Answer"
"79496752","79496746","<p>This is a known bug for <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_sas.html"" rel=""nofollow noreferrer""><code>pd.read_sas</code></a>, first reported <a href=""https://github.com/pandas-dev/pandas/issues/30051"" rel=""nofollow noreferrer"">here</a>: zeros might get interpreted as 5.397605346934028e-79, the lowest IBM float value. See also <a href=""https://github.com/pandas-dev/pandas/issues/50670"" rel=""nofollow noreferrer"">here</a>.</p>
<p>User <a href=""https://github.com/davidlorenzana"" rel=""nofollow noreferrer"">davidlorenzana</a> suggests the following <a href=""https://github.com/pandas-dev/pandas/issues/30051#issuecomment-962297253"" rel=""nofollow noreferrer"">'dirty patch'</a>:</p>
<pre class=""lang-py prettyprint-override""><code>df = df.replace(5.397605346934028e-79, 0)
</code></pre>
<p>This makes sense if you look at the description of the <a href=""https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2021/DataFiles/DPQ_L.htm"" rel=""nofollow noreferrer"">data</a>. E.g., for <code>DPQ010</code> we should get:</p>
<p><a href=""https://i.sstatic.net/82quhh8T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82quhh8T.png"" alt=""DPQ010"" /></a></p>
<p>After using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html"" rel=""nofollow noreferrer""><code>df.replace</code></a>, we can see:</p>
<pre class=""lang-py prettyprint-override""><code>df['DPQ010'].value_counts(dropna=False).sort_index()

DPQ010
0.0    3703
1.0    1205
2.0     334
3.0     256
7.0       5
9.0      16
NaN     818
Name: count, dtype: int64
</code></pre>
","1","Answer"
"79496786","79495612","<blockquote>
<p>Is there a way to infer particular column (Id) alone to double and remain other columns</p>
</blockquote>
<p>You have to use DoubleType if you want Double like below</p>
<p>remaining will  be same as you have done.</p>
<pre><code>schema = StructType([
    StructField('Id', DoubleType(), True), \
    StructField('Field', ArrayType(MapType(StringType(), StringType())), True))]
</code></pre>
","0","Answer"
"79497086","79497048","<p>I filter out the <code>Nan</code> or Null values first and use <code>0</code> as the value for them in the new columns.</p>
<pre><code>import numpy
df[&quot;min&quot;] = df[df[&quot;values&quot;].notnull()][&quot;values&quot;].map(lambda x: int(minimum) if x and (minimum:=str(x).split(&quot; - &quot;)[0]) else 0)df[&quot;max&quot;] = 
df[df[&quot;values&quot;].notnull()].map(lambda x: int(maximum) if x and &quot; - &quot; in str(x) and (maximum:=str(x).split(&quot; - &quot;)[1]) else 0)
</code></pre>
<p>However, the columns have a <code>.0</code> decimal point. How to get rid of it?</p>
","0","Answer"
"79497222","79497048","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html"" rel=""nofollow noreferrer""><code>Series.str.extract</code></a> and convert output to <a href=""https://pandas.pydata.org/docs/user_guide/integer_na.html"" rel=""nofollow noreferrer""><code>Int64</code></a>:</p>
<pre><code>df = pd.DataFrame({'values': [np.nan, '123 - 456', '4 - 9']})

df[['min','max']]=df[&quot;values&quot;].str.extract(r'(\d+)\s*-\s*(\d+)').astype('Int64')

print (df)
      values   min   max
0        NaN  &lt;NA&gt;  &lt;NA&gt;
1  123 - 456   123   456
2      4 - 9     4     9
</code></pre>
","0","Answer"
"79497579","79497570","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.mean.html"" rel=""nofollow noreferrer""><code>GroupBy.mean</code></a> by consecutive values created by compared <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html"" rel=""nofollow noreferrer""><code>Series.shift</code></a>ed values with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>Series.cumsum</code></a>, last remove first level and get original order of columns by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>DataFrame.reindex</code></a>:</p>
<pre><code>out =(df.groupby([df['C2'].ne(df['C2'].shift()).cumsum(),'C2'],sort=False)['C1']
        .mean()
        .droplevel(0)
        .reset_index()
        .reindex(df.columns, axis=1))
print (out)
          C1  C2
0  15.000000  10
1  30.000000  16
2   5.500000  23
3   4.666667  10
</code></pre>
<p>How it working:</p>
<pre><code>print (df.assign(compared=df['C2'].ne(df['C2'].shift()),
                 cumsum=df['C2'].ne(df['C2'].shift()).cumsum()))
   C1  C2  compared  cumsum
0  10  10      True       1
1  20  10     False       1
2  30  16      True       2
3   5  23      True       3
4   6  23     False       3
5   8  10      True       4
6   4  10     False       4
7   2  10     False       4
</code></pre>
<p>Thank you @ouroboros1 for another easier solution with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.agg.html"" rel=""nofollow noreferrer""><code>GroupBy.agg</code></a>:</p>
<pre><code>out = (df.groupby(df['C2'].ne(df['C2'].shift()).cumsum(), as_index=False)
         .agg({'C1': 'mean', 'C2': 'first'}))
print (out)
          C1  C2
0  15.000000  10
1  30.000000  16
2   5.500000  23
3   4.666667  10
</code></pre>
","2","Answer"
"79497748","79497724","<p>You should be able to directly use <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a> with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>iloc</code></a> (or <code>loc</code>):</p>
<pre><code>df = pd.DataFrame(aa)
out = df.iloc[conditions]
</code></pre>
<p>Note that <code>conditions</code> should be a <strong>tuple</strong> of arrays/lists/iterables, if not you should convert it:</p>
<pre><code>out = df.iloc[tuple(conditions)]
</code></pre>
<p>Output:</p>
<pre><code>   0  2
0  0  2
1  3  5
</code></pre>
","3","Answer"
"79498593","79467759","<p>It's to do with whether or not autocommit is set on snowflake. This particularly works for me.</p>
<pre><code>try:
    session.sql(&quot;commit&quot;).collect()
    session.sql(&quot;insert into DB.SC.T1 ('col1','col2') values (99,'ninenine')&quot;).collect()
    raise Exception(&quot;transaction Test&quot;)
    session.sql(&quot;insert into DB.SC.T2 ('col1','col2') values (88,'eightyeight')&quot;).collect()
    session.sql(&quot;commit&quot;).collect()
except:
    session.sql(&quot;rollback&quot;).collect()
</code></pre>
","0","Answer"
"79499172","79499156","<p>Is your goal to get a list of the values in your <code>id</code> column? you can cast your <code>Series</code> to a list if that is the case. <code>ids = list(row['id'])</code> Then you will get something like <code>[1, 2, 3, 4]</code> with your values from your id column from the records returned by the the filtering you did with the <code>Delete</code> column previously.</p>
","0","Answer"
"79499334","79499303","<p>The issue lies in your <code>add</code> function. You defined the function to return the tuple <code>x['a'] + 1, x['b'] + 1</code>, causing to &quot;flip&quot; the values between column a and b.</p>
<p>The function you pass to <code>apply</code> should in your case not know anything about columns.<br />
Simply define the function as:</p>
<pre><code>def add(x):
    return x + 1

df.loc[:, [&quot;a&quot;, &quot;b&quot;]] = df[[&quot;a&quot;, &quot;b&quot;]].apply(add)
</code></pre>
<p>You can even remove the <code>axis</code> assignment when passing the function as a parameter to apply.<br />
As you call apply only on the dataframe columns <code>'a'</code> and <code>'b'</code> you don't need to specify in your add function that those are the columns you want to add + 1 to.</p>
","4","Answer"
"79499981","79499286","<p><code>df.loc[(None if df['Comments'] is None else df['Comments'].apply(len) != 0)]</code> doesn't work the way you want.</p>
<p><code>df['Comments'] is None</code> will always be False since <code>df['Comments']</code> is a Series. Therefore your code is equivalent to:</p>
<pre><code>df.loc[df['Comments'].apply(len) != 0]
</code></pre>
<p>Now, if your column/Series contains non iterables (like <code>None</code>), <code>.apply(len)</code> will fail.</p>
<p>You could use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.len.html"" rel=""nofollow noreferrer""><code>str.len</code></a> that handles <code>None</code>/<code>NaN</code> correctly:</p>
<pre><code>out = df[df['Comments'].str.len().fillna(0).ne(0)]

# or, since a len cannot be negative
out = df[df['Comments'].str.len().gt(0)]
</code></pre>
<p>Example:</p>
<pre><code>df = pd.DataFrame({'Comments': [None, ['abc'], []]})
out = df[df['Comments'].str.len().gt(0)]
print(out)
</code></pre>
<p>Output:</p>
<pre><code>  Comments
1    [abc]
</code></pre>
<p>Another alternative (not really needed here) would have been to combine <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.notna.html"" rel=""nofollow noreferrer""><code>notna</code></a> and your test on the length with the vectorial boolean operator <code>&amp;</code>:</p>
<pre><code>out = df[df['Comments'].notna() &amp; df['Comments'].str.len().ne(0)]
</code></pre>
<h4>Multiple columns</h4>
<p>If you have multiple columns you can use the same logic, combining the boolean Series with the vectorial OR (<code>|</code>):</p>
<pre><code>out = df[df['Comments'].str.len().gt(0)
        |df['Rejection_Comments'].str.len().gt(0)
        ]
</code></pre>
<p>Or, with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.any.html"" rel=""nofollow noreferrer""><code>any</code></a>:</p>
<pre><code>out = df[df[['Comments', 'Rejection_Comments']]
         .apply(lambda s: s.str.len().gt(0)).any(axis=1)
        ]
</code></pre>
<p>Or with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html"" rel=""nofollow noreferrer""><code>map</code></a>:</p>
<pre><code>out = df[df[cols].map(lambda x: hasattr(x, '__iter__') and len(x)&gt;0).any(axis=1)]
</code></pre>
<p>Or with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.logical_or.html"" rel=""nofollow noreferrer""><code>numpy.logical_or</code></a>:</p>
<pre><code>import numpy as np

cols = ['Comments', 'Rejection_Comments']
out = df[np.logical_or.reduce([df[c].str.len().gt(0) for c in cols])]
</code></pre>
","1","Answer"
"79500084","79499303","<p>This is your original DataFrame:</p>
<pre><code>     a    b    c
1   10   20   30
2  100  200  300
</code></pre>
<p>Now, look at the output of <code>df[['a', 'b']].apply(add, axis=1)</code>:</p>
<pre><code>df[['a', 'b']].apply(add, axis=1)

1    (11, 101)
2    (21, 201)
dtype: object
</code></pre>
<p>This creates a <strong>Series</strong> of tuples, which means you have two items <code>(11, 101)</code> and <code>(21, 201)</code>, and those are <strong>objects</strong> (tuples). <strong>The first item will be assigned to <code>a</code>, the second to <code>b</code>.</strong></p>
<p>Let see what happens if you were assigning two strings instead:</p>
<pre><code>df.loc[:, ['a', 'b']] = ['x', 'y']

   a  b    c
1  x  y   30
2  x  y  300
</code></pre>
<p>The first item (<code>x</code>) gets assigned to <code>a</code>, the second (<code>y</code>) to <code>b</code>.</p>
<p>Your unexpected behavior is due to a combination of two things:</p>
<ul>
<li>you are ignoring the index with <code>.loc[:, ...]</code></li>
<li>the right hand side is a <strong>Series</strong> (of objects)</li>
</ul>
<p>If you remove either condition, this wouldn't work:</p>
<pre><code># let's assign on the columns directly
df[['a', 'b']] = df[['a', 'b']].apply(add, axis=1)

# KeyError: 0


# let's convert the output to list
df[['a', 'b']] = df[['a', 'b']].apply(add, axis=1).tolist()

#      a    b    c
# 1   11   21   30
# 2  101  201  300
</code></pre>
<p>In addition, your error only occurred because <strong>you had the same number of rows and columns in the selection</strong>. This would have raised an error with 3 columns:</p>
<pre><code>df.loc[:, ['a', 'b', 'c']] = df[['a', 'b', 'c']].apply(add, axis=1)

# ValueError: Must have equal len keys and value when setting with an iterable
</code></pre>
<h4>Take home message</h4>
<p>If you need to use a function with <code>apply</code> and <code>axis=1</code> and you want to output several &quot;columns&quot;, either convert the output to lists if you have the same columns as output:</p>
<pre><code>df[['a', 'b']] = df[['a', 'b']].apply(add, axis=1).tolist()
</code></pre>
<p>Or output a DataFrame by making the function return a Series:</p>
<pre><code>def add(x):
    return pd.Series({'a': x['a']+1, 'b': x['b']+1})

df[['a', 'b']] = df[['a', 'b']].apply(add, axis=1)
</code></pre>
<p>In any case, never use <code>df.loc[:, ...]</code> unless you know why you're doing this (i.e. you're purposely breaking the index alignment).</p>
<h4>Vectorial operations</h4>
<p>Of course, the above assumes you have complex, non-vectorized functions to use. If your goal is to perform a simple addition:</p>
<pre><code># adding 1 to both a and b
df[['a', 'b']] += 1

# adding 1 to a and 2 to b
df[['a', 'b']] += [1, 2]

# adding 1 to a and 2 to b, using add
df[['a', 'b']] = df[['a', 'b']].add([1, 2])

# adding 1 to a and 2 to b, using a dictionary
df[['a', 'b']] = df[['a', 'b']].add({'b': 2, 'a': 1})
</code></pre>
","3","Answer"
"79500241","79500226","<p>You can first <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html"" rel=""nofollow noreferrer""><code>convert_dtypes</code></a> to ensure <a href=""https://pandas.pydata.org/docs/user_guide/integer_na.html"" rel=""nofollow noreferrer"">nullables dtypes</a> will be used:</p>
<pre><code>df3 = pd.concat([df1.convert_dtypes(),
                 df2.convert_dtypes()],
                ignore_index=True
               )

# or after
df3 = pd.concat([df1, df2], ignore_index=True).convert_dtypes()
</code></pre>
<p>Output:</p>
<pre><code>   x     y     z     a     b
0  2     3  True  Test  &lt;NA&gt;
1  4  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     6
</code></pre>
<p><em>Note however that this would only handle the <strong>NAs</strong>, if you have a mix of integers and floats for instance, this will convert the integers to float.</em></p>
<p>If you want to keep integers even in this case, then force everything to be objects:</p>
<pre><code>df3 = pd.concat([df1.astype(object), df2.astype(object)], ignore_index=True)
</code></pre>
","2","Answer"
"79501433","79499156","<p>Replace your if statement with the following:</p>
<pre><code>delete_rows = df.loc[df[&quot;Delete&quot;]]
if not delete_rows.empty:
    row = delete_rows.iloc[0]
    entryid = row[&quot;id&quot;]
</code></pre>
<hr />
<h2>Explanation</h2>
<p>Consider the following two-column dataframe:</p>
<pre><code>df = pd.DataFrame(
    {&quot;Delete&quot;: [False, True, False, False],
    &quot;id&quot;: [101, 102, 103, 104]}
)
</code></pre>
<p>Printed out, it looks like this</p>
<pre class=""lang-none prettyprint-override""><code>   Delete   id
0   False  101
1    True  102
2   False  103
3   False  104
</code></pre>
<p>The number that you're after in this scenario is the 102 in the &quot;id&quot; column, where the Delete entry is True.</p>
<p><code>df.loc[df[&quot;Delete&quot;]]</code> is the following &quot;filtered&quot; dataframe:</p>
<pre class=""lang-none prettyprint-override""><code>   Delete   id
1    True  102
</code></pre>
<p>It consists of all of the rows (of which there is only one) that have a <code>True</code> in the delete column.</p>
<p>Things look promising: we've pulled out the row that we want. Keep in mind, however, that this is still a <strong>dataframe</strong> object. That is, it is &quot;two-dimensional&quot;. To access an entry, we need to specify a column <strong>and a row</strong> (even though there's only one row in this dataframe).</p>
<p><code>df.loc[df[&quot;Delete&quot;]].empty</code> is <code>False</code> in this case. It would be <code>True</code> if (and only if) there were no rows where the &quot;Delete&quot; entry is <code>True</code>. So, your if-condition successfully confirms that there is actually a row corresponding to a checked Delete-box.</p>
<p>Your next line probably doesn't do what you think it does:</p>
<pre><code>df.loc[df[&quot;Delete&quot;] != df.loc[df[&quot;Delete&quot;]].empty]
</code></pre>
<p>Because we're in if-block at this point, we know that <code>df.loc[df[&quot;Delete&quot;]].empty</code> is <code>False</code>. So, everything that you have here is the same as</p>
<pre><code>df.loc[df[&quot;Delete&quot;] != False]
</code></pre>
<p>In other words, we're getting the rows where the Delete entry is True. But that's exactly what <code>df.loc[df[&quot;Delete&quot;]]</code> was in the first place. Just as before, the result here is a dataframe rather than a series (i.e. it is still a two-dimensional array, not a one-dimensional array, which is what you seem to be after at this point).</p>
<p>For a way to successfully get a series object for the only row of this dataframe, there are a few approaches you could take. What I opted for here was to use <code>.iloc[0]</code>. For a dataframe <code>df</code>, <code>df.iloc[0]</code> gives you the first row, as a series object. So, <code>df.loc[df[&quot;Delete&quot;]].iloc[0]</code> gives you the first (and only) row of the filtered dataframe, which is what we wanted.</p>
<p>As an (arguably more principled) alternative, you could also use <code>df.loc[df[&quot;Delete&quot;]].squeeze()</code>. <code>.squeeze()</code> will take a one-row or one-column dataframe and make it into a series (and will return the same dataframe if it has multiple rows and columns).</p>
<p>One way or the other, the series <code>row</code> that you get looks like this when printed:</p>
<pre class=""lang-none prettyprint-override""><code>Delete    True
id         102
Name: 1, dtype: object
</code></pre>
<p>Once you properly get the row, all that remains is to extract the id-column entry, hence <code>row[&quot;id&quot;]</code>.</p>
","0","Answer"
"79501853","79501838","<pre><code>import csv

details = [
    {&quot;names&quot;: [&quot;ramu&quot;, &quot;Ravi&quot;, &quot;Naik&quot;], &quot;ages&quot;: [39, 40, 45], &quot;department&quot;: &quot;admin&quot;, &quot;location&quot;: &quot;Bangalore&quot;},
    {&quot;names&quot;: [&quot;Kiran&quot;, &quot;Kumar&quot;, &quot;Joseph&quot;], &quot;ages&quot;: [35, 26, 52], &quot;department&quot;: &quot;IT&quot;, &quot;location&quot;: &quot;Hyderabad&quot;},
    {&quot;names&quot;: [&quot;Rajesh&quot;, &quot;Nakul&quot;], &quot;ages&quot;: [34, 40], &quot;department&quot;: &quot;Payroll&quot;, &quot;location&quot;: &quot;Chennai&quot;},
    {&quot;names&quot;: [&quot;Mukesh&quot;, &quot;Kundan&quot;, &quot;Kishore&quot;], &quot;ages&quot;: [27, 33, 37], &quot;department&quot;: &quot;IT&quot;, &quot;location&quot;: &quot;Bangalore&quot;}
]

with open(&quot;details.csv&quot;, &quot;w&quot;, newline=&quot;&quot;) as file:
    writer = csv.writer(file)
    writer.writerow([&quot;names&quot;, &quot;ages&quot;, &quot;department&quot;, &quot;location&quot;])
    
    for entry in details:
        names = &quot;,&quot;.join(entry[&quot;names&quot;])
        ages = &quot;,&quot;.join(map(str, entry[&quot;ages&quot;]))
        writer.writerow([names, ages, entry[&quot;department&quot;], entry[&quot;location&quot;]])

print(&quot;CSV file 'details.csv' has been created.&quot;)


</code></pre>
","1","Answer"
"79502774","79502746","<p>The two suggestions in the warning can be implemented as follows:</p>
<pre class=""lang-py prettyprint-override""><code>sampled_df = df.groupby(&quot;locale&quot;, group_keys=False)[['query', 'score']] \
  .apply(lambda x: x.sample(min(5, len(x)), random_state=42)) \
  .reset_index(drop=True)
</code></pre>
<p>Note the <code>[['query', 'score']]</code> after <code>groupby</code>.</p>
<p>And:</p>
<pre class=""lang-py prettyprint-override""><code>sampled_df = df.groupby(&quot;locale&quot;, group_keys=False) \
  .apply(lambda x: x.sample(min(5, len(x)), random_state=42), include_groups=False) \
  .reset_index(drop=True)
</code></pre>
<p>Here, <code>include_groups=False</code> should be added to the <code>apply</code> function.</p>
<p>These suggestions should work both for your current pandas version and for future versions.</p>
","3","Answer"
"79503002","79479705","<p>My Solution:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
from matplotlib.patches import Arc

# Calculating the angle in radians
club_path = np.radians(5.89)  # Club Path
face_angle = np.radians(3.12)  # Face Angle

# Create a figure and an axis
fig, ax = plt.subplots()

# Axis limits
ax.set_xlim(-1, 1)
ax.set_ylim(-1, 1)

# Draw x- and y-axis
ax.axhline(0, color='black', linewidth=0.5, ls='--')
ax.axvline(0, color='black', linewidth=1.5, ls='--')

# Draw angles
club_vector = np.array([np.sin(club_path), np.cos(club_path)])
face_vector = np.array([np.sin(face_angle), np.cos(face_angle)])

ax.quiver(0, 0, club_vector[0], club_vector[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Club Path (5.89°)')
ax.quiver(0, 0, face_vector[0], face_vector[1], angles='xy', scale_units='xy', scale=1, color='orange', label='Face Angle (3.12°)')

# Calculte angle between the to vectors
dot_product = np.dot(club_vector, face_vector)
norm_club = np.linalg.norm(club_vector)
norm_face = np.linalg.norm(face_vector)

# Calculating the angle in radians
angle_radians = np.arccos(dot_product / (norm_club * norm_face))
angle_degrees = np.degrees(angle_radians)

# Add angle in ledgend
ax.legend(title=f&quot;Face to Path: {angle_degrees:.2f}°&quot;)

# Delete diagram frame
for spine in ax.spines.values():
    spine.set_visible(False)

# Delete x- and y-axis
ax.set_xticks([])
ax.set_yticks([])

# Print diagram
plt.show()
</code></pre>
<p>Result:</p>
<p><a href=""https://i.sstatic.net/CQVfJjrk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CQVfJjrk.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79503662","79503619","<p>You can split the input in two depending on the target column, then reshape with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html"" rel=""nofollow noreferrer""><code>melt</code></a>, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html"" rel=""nofollow noreferrer""><code>pivot</code></a> (with help of <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>groupby.cumcount</code></a>), and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>join</code></a>:</p>
<pre><code># columns to consider
cols = ['11', '00']

# first reshape the columns with the lists
tmp1 = (df_matrix[cols]
        .melt(value_name='col', ignore_index=False)
        .explode('col').reset_index()
        .assign(n=lambda x: x.groupby(['index', 'variable']).cumcount()+1)
       )
# then reshape the columns with the values
tmp2 = (df_matrix.drop(columns=cols, errors='ignore')
        .melt(var_name='col', ignore_index=False)
        .reset_index()
       )

# merge, reshape, rename columns
out = tmp1.merge(tmp2, how='left').pivot(index='index', columns=['variable', 'n'], values='value')
out.columns = out.columns.map(lambda x: f'{x[0]}_{x[1]}')

# join to original
out = df_matrix.join(out)
</code></pre>
<p>Output:</p>
<pre><code>                   11                  00  P4-1  P4-2  P4-3  P4-4  P4-5  P4-6  P4-7  11_1  11_2  11_3  00_1  00_2  00_3
0  [P4-1, P4-2, P4-3]  [P4-4, P4-6, P4-7]     1     6     5     2   NaN   6.0     3   1.0   6.0   5.0   2.0   6.0   3.0
1  [P4-1, P4-3, P4-4]  [P4-2, P4-5, P4-7]     2     8     2     3   2.0   NaN     2   2.0   2.0   3.0   8.0   2.0   2.0
</code></pre>
<p>Intermediates:</p>
<pre><code># tmp1
    index variable   col  n
0       0       11  P4-1  1
1       0       11  P4-2  2
2       0       11  P4-3  3
3       1       11  P4-1  1
4       1       11  P4-3  2
5       1       11  P4-4  3
6       0       00  P4-4  1
7       0       00  P4-6  2
8       0       00  P4-7  3
9       1       00  P4-2  1
10      1       00  P4-5  2
11      1       00  P4-7  3

# tmp2
    index   col  value
0       0  P4-1    1.0
1       1  P4-1    2.0
2       0  P4-2    6.0
3       1  P4-2    8.0
4       0  P4-3    5.0
5       1  P4-3    2.0
6       0  P4-4    2.0
7       1  P4-4    3.0
8       0  P4-5    NaN
9       1  P4-5    2.0
10      0  P4-6    6.0
11      1  P4-6    NaN
12      0  P4-7    3.0
13      1  P4-7    2.0
</code></pre>
","1","Answer"
"79503666","79503619","<p>Here is the script:</p>
<pre><code>import pandas as pd
import numpy as np

df_matrix_data = {
    '11': [['P4-1', 'P4-2', 'P4-3'], ['P4-1', 'P4-3', 'P4-4']],
    '00': [['P4-4', 'P4-6', 'P4-7'], ['P4-2', 'P4-5', 'P4-7']],
    'P4-1': [1, 2], 'P4-2': [6, 8], 'P4-3': [5, 2], 'P4-4': [2, 3], 
    'P4-5': [np.nan, 2], 'P4-6': [6, np.nan], 'P4-7': [3, 2]
}

df_matrix = pd.DataFrame.from_dict(df_matrix_data)

def extract_values(row, col_name):
    return [row[item] if item in row else np.nan for item in row[col_name]]

for col in ['11', '00']:
    extracted_values = df_matrix.apply(lambda row: extract_values(row, col), axis=1)
    df_expanded = pd.DataFrame(extracted_values.tolist(), columns=[f&quot;{col}_{i+1}&quot; for i in range(extracted_values.str.len().max())])
    df_matrix = pd.concat([df_matrix, df_expanded], axis=1)

print(df_matrix)
</code></pre>
<p>00_2 column is adding point zero to the numbers. If you want to remove these, please add this line above &quot;df_matrix = ...&quot; line.</p>
<p><code>df_expanded = df_expanded.map(lambda x: int(x) if pd.notna(x) else np.nan)</code></p>
","1","Answer"
"79504753","79503619","<p>Another possible solution:</p>
<pre><code>df_matrix.assign(
    **{f&quot;{k}_{i+1}&quot;: df_matrix.apply(
        lambda row: row[row[k][i]], axis=1) 
       for k in ['11', '00'] for i in range(3)})
</code></pre>
<p>It uses a dictionary comprehension within <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>assign</code></a>, iterating over each key (e.g., '11') and list index (0-2), then generates columns like <code>11_1</code> by mapping the list's element (e.g., <code>row['11'][0]</code>) to its corresponding value in the row via <code>lambda</code>.</p>
<hr />
<p>To avoid the inefficient <code>apply</code>:</p>
<pre><code>df_matrix.assign(
    **{f&quot;{k}_{i+1}&quot;: df_matrix.values[
    np.arange(len(df_matrix)), 
    df_matrix.columns.get_indexer(df_matrix[k].str[i])]
       for k in ['11', '00'] for i in range(3)})
</code></pre>
<p>It uses <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.get_indexer.html"" rel=""nofollow noreferrer""><code>index.get_indexer</code></a> to convert column names to numeric indices.</p>
<p>Output:</p>
<pre><code>                   11                  00  P4-1  P4-2  P4-3  P4-4  P4-5  P4-6  \
0  [P4-1, P4-2, P4-3]  [P4-4, P4-6, P4-7]     1     6     5     2   NaN   6.0   
1  [P4-1, P4-3, P4-4]  [P4-2, P4-5, P4-7]     2     8     2     3   2.0   NaN   

   P4-7  11_1  11_2  11_3  00_1  00_2  00_3  
0     3     1     6     5     2   6.0     3  
1     2     2     2     3     8   2.0     2
</code></pre>
","1","Answer"
"79504830","79479705","<p>building from what you provided</p>
<p>input image <code>cwVZTYigC.png</code>
<a href=""https://i.sstatic.net/Umb4fGrE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Umb4fGrE.png"" alt=""cwVZTYigC.png"" /></a></p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Arc
import matplotlib.image as mpimg

# read the image
img = mpimg.imread('cwVZTYigC.png')

# Calculating the angle in radians
club_path = np.radians(5.89)  # Club Path
face_angle = np.radians(3.12)  # Face Angle

# Create a figure and an axis
fig, ax = plt.subplots()

# Axis limits
ax.set_xlim(-1, 1)
ax.set_ylim(-1, 1)

# Draw x- and y-axis
# ax.axhline(0, color='black', linewidth=0.5, ls='--')
# ax.axvline(0, color='black', linewidth=1.5, ls='--')

# Draw angles
club_vector = np.array([np.sin(club_path), np.cos(club_path)])
face_vector = np.array([np.sin(face_angle), np.cos(face_angle)])

# draw arrows
ax.quiver(0, 0, club_vector[0], club_vector[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Club Path (5.89°)')
ax.quiver(0, 0, face_vector[0], face_vector[1], angles='xy', scale_units='xy', scale=1, color='orange', label='Face Angle (3.12°)')

# draw lines
ax.plot([0, -club_vector[0]], [0, -club_vector[1]], color='blue', linewidth=3)
ax.plot([0, -face_vector[0]], [0, -face_vector[1]], color='orange', linewidth=3)

# Calculte angle between the to vectors
dot_product = np.dot(club_vector, face_vector)
norm_club = np.linalg.norm(club_vector)
norm_face = np.linalg.norm(face_vector)

# Calculating the angle in radians
angle_radians = np.arccos(dot_product / (norm_club * norm_face))
angle_degrees = np.degrees(angle_radians)

# Add angle in ledgend
ax.legend(title=f&quot;Face to Path: {angle_degrees:.2f}°&quot;, loc='lower right')

# Delete diagram frame
for spine in ax.spines.values():
    spine.set_visible(False)

# Delete x- and y-axis
ax.set_xticks([])
ax.set_yticks([])

# show image
ax.imshow(img, extent=[-1, 1, -1, 1])

# Print diagram
plt.show()
</code></pre>
<p>output</p>
<p><a href=""https://i.sstatic.net/Kmd904Gy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Kmd904Gy.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79505261","79505221","<p>You can do it this way by creating a key from hour in dataframe index and join on a dataframe created from the list of tuples.</p>
<pre><code>df.reset_index().assign(key=df.index.hour).merge(pd.DataFrame(c, columns=['key', 'price']), how='left')
</code></pre>
<p>Output:</p>
<pre><code>             timestamp item  key  price
0  2025-01-01 00:00:00    A    0    NaN
1  2025-01-01 01:00:00    B    1  100.0
2  2025-01-01 02:00:00    C    2  202.0
3  2025-01-01 03:00:00    D    3    NaN
4  2025-01-01 04:00:00    E    4    NaN
5  2025-01-01 05:00:00    F    5    NaN
6  2025-01-01 06:00:00    G    6  772.0
7  2025-01-01 07:00:00    H    7    NaN
8  2025-01-01 08:00:00    I    8    NaN
9  2025-01-01 09:00:00    J    9    NaN
10 2025-01-01 10:00:00    K   10    NaN
</code></pre>
<p>Or even shorted:</p>
<pre><code>df.assign(price=df.index.hour.map(pd.Series(c[:,1], index=c[:,0])))
</code></pre>
<p>Output:</p>
<pre><code>                    item  price
timestamp                      
2025-01-01 00:00:00    A    NaN
2025-01-01 01:00:00    B  100.0
2025-01-01 02:00:00    C  202.0
2025-01-01 03:00:00    D    NaN
2025-01-01 04:00:00    E    NaN
2025-01-01 05:00:00    F    NaN
2025-01-01 06:00:00    G  772.0
2025-01-01 07:00:00    H    NaN
2025-01-01 08:00:00    I    NaN
2025-01-01 09:00:00    J    NaN
2025-01-01 10:00:00    K    NaN
</code></pre>
","1","Answer"
"79505267","79505221","<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.take.html"" rel=""nofollow noreferrer"">Index.take</a> returns the index of your dataframe based on the position and we can use the first column of your array to get the index.</p>
<pre><code>df.loc[df.index.take(c[:, 0]), 'price'] = c[:, 1]
</code></pre>
<p>You can also use a combination of loc and iloc.</p>
<pre><code>
df.loc[df.iloc[c[:, 0]].index, 'price'] = c[:, 1]
</code></pre>
<p>End result:</p>
<pre><code>                    item  price
2025-01-01 00:00:00    A    NaN
2025-01-01 01:00:00    B  100.0
2025-01-01 02:00:00    C  202.0
2025-01-01 03:00:00    D    NaN
2025-01-01 04:00:00    E    NaN
2025-01-01 05:00:00    F    NaN
2025-01-01 06:00:00    G  772.0
2025-01-01 07:00:00    H    NaN
2025-01-01 08:00:00    I    NaN
2025-01-01 09:00:00    J    NaN
2025-01-01 10:00:00    K    NaN
</code></pre>
","3","Answer"
"79506266","79505931","<p>Looks like you might have empty trailing spaces. Hence the error:</p>
<pre><code>Original error: remaining bytes non-empty
</code></pre>
<p>Polars is stricter than pandas on file formatting. Pandas will infer formatting but Polars will not.</p>
<p>You can use this command to remove empty lines and white spaces:</p>
<pre><code>sed -i '/^\s*$/d' foo.g.vcf.csv
</code></pre>
<p>But I recommend you tell Polars to infer the schema from the whole file instead with:</p>
<pre><code>infer_schema_length=None
</code></pre>
<p>Or you can tell Polars to ignore parsing errors (I do not recommend this but it is an option) with:</p>
<pre><code>ignore_errors=True
</code></pre>
","2","Answer"
"79507150","79507131","<p>You can combine <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html"" rel=""nofollow noreferrer""><code>fillna</code></a> on Column_2 and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.bfill.html"" rel=""nofollow noreferrer""><code>bfill</code></a> on Column_1:</p>
<pre><code>df['Column_2'] = df['Column_2'].fillna(df['Column_1'].bfill())
</code></pre>
<p>Output:</p>
<pre><code>  Column_1 Column_2
0        F        A
1      NaN        B
2      NaN        G
3        G        C
4      NaN        H
5      NaN        D
6        H        D
</code></pre>
<p>Intermediates:</p>
<pre><code>  Column_1 Column_2 col1_bfill col2_fillna
0        F        A          F           A
1      NaN        B          G           B
2      NaN      NaN          G  ------&gt;  G
3        G        C          G           C
4      NaN      NaN          H  ------&gt;  H
5      NaN        D          H           D
6        H        D          H           D
</code></pre>
","4","Answer"
"79508255","79507978","<p>Here's what i get (linux mint 20.x , python 3.12.2, pandas 2.2.2)</p>
<pre><code>cat shmyg.py
import pandas as pd
import sys

if len(sys.argv) &lt; 3:
    print(f'usage: {sys.argv[0]} separator filename')
    sys.exit(1)

df = pd.read_csv(sys.argv[2], sep=sys.argv[1], header=None)
df.info()
print(df)

####

cat shmyg.txt 
12;0;5/15/2008;1:01:09;1;0;0;None;97;39;0.279;;;0;;;;;0;0;0;;;;;;;;;;;;;;;;

####
python shmyg.py ';' shmyg.txt 
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1 entries, 0 to 0
Data columns (total 37 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   0       1 non-null      int64  
 1   1       1 non-null      int64  
 2   2       1 non-null      object 
 3   3       1 non-null      object 
 4   4       1 non-null      int64  
 5   5       1 non-null      int64  
 6   6       1 non-null      int64  
 7   7       0 non-null      float64
 8   8       1 non-null      int64  
 9   9       1 non-null      int64  
 10  10      1 non-null      float64
 11  11      0 non-null      float64
 12  12      0 non-null      float64
 13  13      1 non-null      int64  
 14  14      0 non-null      float64
 15  15      0 non-null      float64
 16  16      0 non-null      float64
 17  17      0 non-null      float64
 18  18      1 non-null      int64  
 19  19      1 non-null      int64  
 20  20      1 non-null      int64  
 21  21      0 non-null      float64
 22  22      0 non-null      float64
 23  23      0 non-null      float64
 24  24      0 non-null      float64
 25  25      0 non-null      float64
 26  26      0 non-null      float64
 27  27      0 non-null      float64
 28  28      0 non-null      float64
 29  29      0 non-null      float64
 30  30      0 non-null      float64
 31  31      0 non-null      float64
 32  32      0 non-null      float64
 33  33      0 non-null      float64
 34  34      0 non-null      float64
 35  35      0 non-null      float64
 36  36      0 non-null      float64
dtypes: float64(24), int64(11), object(2)
memory usage: 428.0+ bytes
   0   1          2        3   4   5   6   7   8   9      10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
0  12   0  5/15/2008  1:01:09   1   0   0 NaN  97  39  0.279 NaN NaN   0 NaN NaN NaN NaN   0   0   0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN
</code></pre>
","0","Answer"
"79508502","79508495","<p>You can create a MultiIndex from a list of tuples (or list of lists) with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.from_tuples.html"" rel=""nofollow noreferrer""><code>pandas.MultiIndex.from_tuples</code></a>:</p>
<pre><code>pd.MultiIndex.from_tuples([lst[:3] for lst in data],
                          names=['Category', 'Sub Category', 'Sub Sub Category']
                         )
</code></pre>
<p>Output:</p>
<pre><code>MultiIndex([('Machine',    'Car',   'Sedan'),
            ('Machine',   'Bike', '2 Wheel'),
            ('Machine',   'Bike', '3 Wheel'),
            ( 'Animal', 'Donkey',     'Big')],
           names=['Category', 'Sub Category', 'Sub Sub Category'])
</code></pre>
<p>To create a DataFrame, you could split the sublists into index/values and pass them to the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>DataFrame</code></a> constructor:</p>
<pre><code>*index, values = map(list, zip(*data))
df = (pd.DataFrame({'Value': values}, index=index)
        .rename_axis(['Category', 'Sub Category', 'Sub Sub Category'])
     )
</code></pre>
<p>Output:</p>
<pre><code>                                        Value
Category Sub Category Sub Sub Category       
Machine  Car          Sedan                10
         Bike         2 Wheel               5
                      3 Wheel               4
Animal   Donkey       Big                   2
</code></pre>
<p>Your approach is also perfectly valid, you could just reuse the variable with the list of columns to use as index and chain the operations:</p>
<pre><code>df = (pd.DataFrame(data, columns=column_names)
        .set_index(column_names[:3])
     )
</code></pre>
","0","Answer"
"79509241","79504101","<pre><code>import random
import pandas as pd
import matplotlib.pyplot as plt

data = [(a * 100, b * 100, random.random()) for a in range(1,4) for b in range(1,5)]
df = pd.DataFrame(data, columns=[&quot;Parm_1&quot;, &quot;Parm_2&quot;, &quot;Result&quot;])

fig, ax = plt.subplots(figsize=(8, 5))

ax.plot(df.index, df[&quot;Result&quot;], &quot;o&quot;)

unique_parm1 = df[&quot;Parm_1&quot;].unique()
major_ticks = [df[df[&quot;Parm_1&quot;] == p].index[0] -0.5 for p in unique_parm1]
ax.set_xticks(major_ticks, labels=[str(p) for p in unique_parm1], minor=False)
ax.tick_params(axis=&quot;x&quot;, which=&quot;major&quot;, length=15, width=2)

ax.set_xticks(df[&quot;Parm_2&quot;].index, labels=[str(p) for p in df[&quot;Parm_2&quot;]], minor=True)
ax.tick_params(axis=&quot;x&quot;, which=&quot;minor&quot;, length=5)

plt.show()
</code></pre>
","0","Answer"
"79509771","79509755","<p>You can used <a href=""https://docs.python.org/3/library/itertools.html#itertools.batched"" rel=""nofollow noreferrer""><code>batched</code></a> to split each lines into batches of three, and then use <a href=""https://docs.python.org/3.3/library/functions.html#zip"" rel=""nofollow noreferrer""><code>zip</code></a> to transpose them:</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import batched
all_data = []
with open(old_file, &quot;r&quot;) as inputfile:
    skip_first_row = inputfile.readline()

    for line in inputfile:
        line_list = line.split()
        all_data += zip(*batched(line_list, 3))
</code></pre>
","0","Answer"
"79509774","79509755","<p>Another possible solution, based on <code>pandas</code> and <code>numpy</code>:</p>
<pre><code>txt = &quot;x1 y1 z1 x2 y2 z2 x3 y3 z3 data1x data1y data1z data2x data2y data2z data3x data3y data3z&quot;

pd.DataFrame(np.hstack(
    pd.read_csv(StringIO(txt), sep=r'\s+', header=None).values
    .reshape(-1, 3, 3))) # replace StringIO(txt) with 'your_file.csv'
</code></pre>
<p>It first reads the space-separated data from the file into a <code>pandas</code> datadrame using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>pd.read_csv</code></a> with <code>StringIO(txt)</code> as a placeholder for the file input. It then extracts the underlying <code>numpy</code> array using <code>.values</code>, reshapes it into a 3D array of shape <code>(-1, 3, 3)</code>. Finally, <a href=""https://numpy.org/doc/stable/reference/generated/numpy.hstack.html"" rel=""nofollow noreferrer""><code>np.hstack</code></a> is used to horizontally concatenate these sub-arrays.</p>
<p>Output:</p>
<pre><code>    0   1   2       3       4       5
0  x1  y1  z1  data1x  data1y  data1z
1  x2  y2  z2  data2x  data2y  data2z
2  x3  y3  z3  data3x  data3y  data3z
</code></pre>
","1","Answer"
"79509969","79509728","<p>You can write a quick function that returns a mapping of expressions that you can unpack right into your <code>DataFrame.group_by(...).agg</code>. This avoids any slow-ness of using <code>map_groups</code> and enables Polars to easily scan the query for any optimizations (provided you are working with a <code>LazyFrame</code>).</p>
<pre class=""lang-py prettyprint-override""><code>from io import BytesIO
import pandas as pd
import polars as pl

def describe(column, percentiles=[.25, .5, .75]):
    return {
        'count': column.count(),
        'null_count': column.null_count(),
        'mean': column.mean(),
        'std': column.std(),
        'min': column.min(),
        **{
            f'{pct*100:g}%': column.quantile(pct)
            for pct in percentiles
        },
        'max': column.max(),
    }

S = b'''group,value\n3,245\n3,28\n3,48\n1,113\n1,288\n1,165\n2,90\n2,21\n2,109'''

pl_df = pl.read_csv(BytesIO(S))

print(
    pl_df.group_by('group').agg(**describe(pl.col('value')))
)
</code></pre>
<p>Produces</p>
<pre><code>shape: (3, 10)
┌───────┬───────┬────────────┬────────────┬───┬───────┬───────┬───────┬─────┐
│ group ┆ count ┆ null_count ┆ mean       ┆ … ┆ 25%   ┆ 50%   ┆ 75%   ┆ max │
│ ---   ┆ ---   ┆ ---        ┆ ---        ┆   ┆ ---   ┆ ---   ┆ ---   ┆ --- │
│ i64   ┆ u32   ┆ u32        ┆ f64        ┆   ┆ f64   ┆ f64   ┆ f64   ┆ i64 │
╞═══════╪═══════╪════════════╪════════════╪═══╪═══════╪═══════╪═══════╪═════╡
│ 1     ┆ 3     ┆ 0          ┆ 188.666667 ┆ … ┆ 165.0 ┆ 165.0 ┆ 288.0 ┆ 288 │
│ 3     ┆ 3     ┆ 0          ┆ 107.0      ┆ … ┆ 48.0  ┆ 48.0  ┆ 245.0 ┆ 245 │
│ 2     ┆ 3     ┆ 0          ┆ 73.333333  ┆ … ┆ 90.0  ┆ 90.0  ┆ 109.0 ┆ 109 │
└───────┴───────┴────────────┴────────────┴───┴───────┴───────┴───────┴─────┘
</code></pre>
","3","Answer"
"79511704","79470747","<p>Try this:</p>
<pre><code>df2 = (df.astype({'SEQ':'str'}).assign(
    SEQ_NEXT = lambda x: x.groupby('ID')['SEQ'].shift(), 
    COMBINE = lambda x: x['SEQ_NEXT'] + ' - ' + x['SEQ']).dropna())


Output:

   ID SEQ SEQ_NEXT COMBINE
1   1   2        1   1 - 2
2   1   3        2   2 - 3
3   1   4        3   3 - 4
5   2   3        1   1 - 3
6   2   4        3   3 - 4
</code></pre>
","0","Answer"
"79524804","79475516","<p>Here is an answer.</p>
<pre><code>kw = ['apple','banana','kiwi']
regex_val = &quot;|&quot;.join(kw)

df = pd.DataFrame(data = {'col': ['apple|banana|kiwi|melon', 'apple|kiwi|melon', 'kiwi|melon', 'apple|banana']})
df['col2'] = df['col'].apply(lambda x: True if regex_val in x else False)
</code></pre>
<p>Then, the output is here.</p>
<pre><code>df

out:
                        col   col2
0   apple|banana|kiwi|melon   True
1          apple|kiwi|melon   False
2                kiwi|melon   False
3              apple|banana   False
</code></pre>
","0","Answer"
"79525012","79475516","<p>I have used simple operations used in python. The given example can be extended to dataframe (df['col'])</p>
<pre><code>kw = ['apple','banana','kiwi']
output_ros=[]

df = ['apple|banana|kiwi|melon', 'apple|kiwi|melon', 'kiwi|melon', 'apple|banana']

for j in df:
    if(all([True if i in j else False  for i in kw]))==True:
        ro=&quot;index &quot;+str(df.index(j))+': '+j
        output_ros.append(ro)

print(output_ros)


Output(Output is that row of the dataframe['col'] which has all of the elements present in list kw)

['index 0: apple|banana|kiwi|melon']
</code></pre>
","0","Answer"
"79538762","79479705","<p>@Hanna the flag belongs over the diagram. But the arrows are right. Thanks.</p>
<p>I finished the solution and I wanted to post the code:</p>
<pre><code>%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interact, Output
from IPython.display import display

# Beispiel-Daten erstellen
data = {
    'Club Path': [5.89, 5.2, 4.9, 5.7],
    'Face Angle': [3.12, 2.8, 3.3, 2.7]
}
df = pd.DataFrame(data)

# Ausgabe-Widget für das Diagramm
output = Output()

# Interaktive Tabelle erstellen
def create_table():
    table = pd.DataFrame(df)
    display(table)

# Funktion zum Zeichnen des Diagramms
def plot_data(selected_row):
    # Calculating the angle in radians
    selected_data = df.iloc[selected_row]
    club_path = np.radians(selected_data['Club Path'])  # Club Path
    face_angle = np.radians(selected_data['Face Angle'])  # Face Angle

    # Create a figure and an axis
    fig, ax = plt.subplots()

    # Axis limits
    ax.set_xlim(-1, 1)
    ax.set_ylim(-1, 1)

    # Draw x- and y-axis
    ax.axhline(0, color='black', linewidth=0.5, ls='--')
    ax.axvline(0, color='black', linewidth=1.5, ls='--')

    # Draw angles
    club_vector = np.array([np.sin(club_path), np.cos(club_path)])
    face_vector = np.array([np.sin(face_angle), np.cos(face_angle)])

    ax.quiver(0, 0, club_vector[0], club_vector[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Club Path (5.89°)')
    ax.quiver(0, 0, face_vector[0], face_vector[1], angles='xy', scale_units='xy', scale=1, color='orange', label='Face Angle (3.12°)')

    # draw lines
    ax.plot([0, -club_vector[0]], [0, -club_vector[1]], color='blue', linewidth=3)
    ax.plot([0, -face_vector[0]], [0, -face_vector[1]], color='orange', linewidth=3)

    # Calculte angle between the to vectors
    dot_product = np.dot(club_vector, face_vector)
    norm_club = np.linalg.norm(club_vector)
    norm_face = np.linalg.norm(face_vector)

    # Calculating the angle in radians
    angle_radians = np.arccos(dot_product / (norm_club * norm_face))
    angle_degrees = np.degrees(angle_radians)

    # Add angle in ledgend
    ax.legend(title=f&quot;Face to Path: {angle_degrees:.2f}°&quot;)

    # Delete diagram frame
    for spine in ax.spines.values():
        spine.set_visible(False)

    # Delete x- and y-axis
    ax.set_xticks([])
    ax.set_yticks([])

    # Print diagram
    plt.show()

# Interaktive Auswahl der Zeile
def on_row_click(selected_row):
    plot_data(selected_row)

# Tabelle anzeigen
create_table()

# Interaktive Auswahl der Zeile
interact(on_row_click, selected_row=(0, len(df)-1))

# Ausgabe-Widget anzeigen
display(output)
</code></pre>
<p>This is the solution. The only downside you need to klick the selector instead of the table row you wanna show.</p>
<p><a href=""https://i.sstatic.net/r7QhSkZ1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/r7QhSkZ1.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79566123","79485898","<p>I think this is one of the Fastest using Numpy.</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({
    'start_date': pd.to_datetime(['2019-08-01', '2019-08-04', '2019-08-07']),
    'end_date': pd.to_datetime(['2019-08-05', '2019-08-07', '2019-08-09']),
})

# Determine the minimum and maximum dates across all start and end dates
min_date = df['start_date'].min()
max_date = df['end_date'].max()

# Create a continuous range of dates from the overall minimum to the overall maximum
date_range = pd.date_range(min_date, max_date)

# Create a dictionary to map each date in the date range to its numerical index
date_to_index = {date: idx for idx, date in enumerate(date_range)}

# Get the numerical indices corresponding to the start dates of each event
start_idx = df['start_date'].map(date_to_index).to_numpy()

# Get the numerical indices corresponding to the end dates of each event
end_idx = df['end_date'].map(date_to_index).to_numpy()

# Initialize a NumPy array to store the count of active events for each day in the date range
matrix_container = np.zeros(len(date_range), dtype=np.int32)

allIdx = np.concatenate([np.arange(s,e+1) for s,e in zip(start_idx,end_idx)])

np.add.at(matrix_container, allIdx, 1)
'''
[1 1 1 2 2 1 2 1 1]
'''
result = pd.DataFrame({
    'date': date_range,
    'count': matrix_container
})

print(result)
'''
      date  count
0 2019-08-01      1
1 2019-08-02      1
2 2019-08-03      1
3 2019-08-04      2
4 2019-08-05      2
5 2019-08-06      1
6 2019-08-07      2
7 2019-08-08      1
8 2019-08-09      1
'''
</code></pre>
","0","Answer"
"79566440","79485898","<p>Solution using Pytorch :</p>
<pre><code>import pandas as pd
import torch
import numpy as np

df = pd.DataFrame({
    'start_date': pd.to_datetime(['2019-08-01', '2019-08-04', '2019-08-07']),
    'end_date': pd.to_datetime(['2019-08-05', '2019-08-07', '2019-08-09']),
})

minDate = df['start_date'].min()
maxDate = df['end_date'].max()

dateRange = pd.date_range(minDate,maxDate)

date_idx_relation = {date:idx for idx,date in enumerate(dateRange)}

start_idx = torch.tensor(df['start_date'].map(date_idx_relation))

end_idx = torch.tensor(df['end_date'].map(date_idx_relation))

allIdx = [torch.arange(s, e +1) for s,e in zip(start_idx,end_idx)]

flatIdx = torch.cat(allIdx)
'''
torch.bincount(input, weights=None, minlength=0)
input (Tensor) – 1-d int tensor
minlength (int) – minimum number of bins. Should be non-negative.
'''
counts = torch.bincount(input = flatIdx, minlength= len(dateRange))
print(counts)
result = pd.DataFrame({
    'date': dateRange,
    'count': counts.numpy()
})

print(result)
'''
        date  count
0 2019-08-01      1
1 2019-08-02      1
2 2019-08-03      1
3 2019-08-04      2
4 2019-08-05      2
5 2019-08-06      1
6 2019-08-07      2
7 2019-08-08      1
8 2019-08-09      1
'''


</code></pre>
","0","Answer"
"79571426","79476892","<p>I am reversing the array and then manipulating it.</p>
<pre><code>import numpy as np

arr = np.array([1, 1, 0, 0, 0, 1, 0, 1], dtype=np.uint8)
chunkSize = 4

n = len(arr)

# Universal formulae
pad = (chunkSize - (n % chunkSize )) % chunkSize

rev = np.pad(arr[::-1],(0,pad),constant_values= 0)
'''
[1 0 1 0 0 0 1 1]
'''
rev_reshaped = rev.reshape(-1,chunkSize)
'''
[[1 0 1 0]
 [0 0 1 1]]
'''

keep = rev_reshaped[:,0] = 1   
rev_reshaped[~keep] = 0 
rev_reshaped[keep,1:] = 0 


reshape_back = rev_reshaped.reshape(-1)[:len(arr)][::-1] 
'''
[0 0 0 1 0 0 0 0]
'''








</code></pre>
","0","Answer"
"79572540","79476892","<p>Solution using Pytorch :</p>
<pre><code>import torch

arr = torch.tensor([1, 1, 0, 0, 0, 1, 0, 1], dtype=torch.uint8)
chunkSize = 4
arr_len = arr.numel()
pad =( chunkSize - (arr_len % chunkSize)) % chunkSize

rev = torch.nn.functional.pad(arr.flip(0),(0,pad), value = 0)
'''
tensor([1, 0, 1, 0, 0, 0, 1, 1], dtype=torch.uint8)
'''
rev_reshaped = rev.view(-1,chunkSize)
'''
tensor([[1, 0, 1, 0],
        [0, 0, 1, 1]], dtype=torch.uint8)
'''

keep = rev_reshaped[:,0] == 1
'''
 Create a boolean tensor 'keep' by checking if the first element of each row in 
'rev_reshaped' is equal to 1.
 For rev_reshaped:
 [[1, 0, 1, 0],
  [0, 0, 1, 1]]
 keep will be tensor([True, False]).
'''

rev_reshaped[~keep]  = 0
'''
 For all rows where 'keep' is False, set all elements in that row to 0.
 In this case, the second row (where keep is False) becomes [0, 0, 0, 0].
 rev_reshaped is now:
 tensor([[1, 0, 1, 0],
         [0, 0, 0, 0]], dtype=torch.uint8)
'''

rev_reshaped[keep,1:] = 0
'''
 For all rows where 'keep' is True, set all elements from the second column onwards (index 1:) to 0.
 In this case, the first row (where keep is True) becomes [1, 0, 0, 0].
 rev_reshaped is now:
 tensor([[1, 0, 0, 0],
         [0, 0, 0, 0]], dtype=torch.uint8)
'''         

res = rev_reshaped.reshape(-1)[:arr_len].flip(0)
'''
 1. Flatten 'rev_reshaped' back into a 1D tensor using .reshape(-1).
    tensor([[1, 0, 0, 0], [0, 0, 0, 0]]) becomes tensor([1, 0, 0, 0, 0, 0, 0, 0]).
 2. Slice the flattened tensor up to the original length of 'arr' using [:arr_len].
    Since arr_len is 8, the tensor remains tensor([1, 0, 0, 0, 0, 0, 0, 0]).
 3. Reverse the order of elements in the sliced tensor using .flip(0).
    tensor([1, 0, 0, 0, 0, 0, 0, 0]) becomes tensor([0, 0, 0, 0, 0, 0, 0, 1]).
'''
print(res)
</code></pre>
","-1","Answer"
"79577794","79452715","<p>Take a look at this example and please go through the solution.</p>
<pre><code>import pandas as pd
import numpy as np

# large dataset
np.random.seed(0)
n = 10_000_000
segments_list = ['A', 'B']
df = pd.DataFrame({
    'segment': np.random.choice(segments_list, size=n),
    'date': np.random.randint(0, 25, size = n)
})
print(df.head(10))
# Bin definitions per segment
cuts = {
    'A': {'cut': np.array([0, 10, 20]), 'class': np.array(['Low', 'High'])},
    'B': {'cut': np.array([0, 5, 15, 30]), 'class': np.array(['Low', 'Mid', 'High'])}
}


container = np.empty(len(df),dtype= object)

# Get NumPy arrays of the 'segment' and 'date' columns for faster iteration
segment_vals = df['segment'].values
date_vals = df['date'].values

# Iterate through each segment defined in the 'cuts' dictionary
for seg, cutInfo in cuts.items():
 
  mask = (segment_vals == seg)

  # Get the 'date' values for the current segment
  seg_dates = date_vals[mask]

  # Use np.digitize to find the bin index for each 'date' value
  # based on the 'cut' array for the current segment.
  # Subtract 1 to make the bin indices 0-based for array indexing.
  binIdx = np.digitize(seg_dates, cutInfo['cut']) - 1

  # Clip the bin indices to ensure they are within the valid range
  # of the 'class' array for the current segment (from 0 to len(class) - 1).
  binIdx = np.clip(binIdx, 0, len(cutInfo['class']) - 1 )

  # Assign the corresponding class labels from the 'class' array
  # to the 'container' array at the indices where the mask is True 
  # (i.e., for the current segment).
  container[mask] = cutInfo['class'][binIdx]


df['Class'] = container


print(df.head(10))
'''
   segment  date Class
0       A     6   Low
1       B    10   Mid
2       B    21  High
3       A    20  High
4       B    21  High
5       B     7   Mid
6       B    23  High
7       B     5   Mid
8       B     7   Mid
9       B    15  High
'''
</code></pre>
","0","Answer"
"79581087","79448271","<p>For Huge datasets  :</p>
<pre><code>
import numpy as np
import pandas as pd


data = {'gr': [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;],
        'subgr': [&quot;i&quot;, &quot;ii&quot;, &quot;i&quot;, &quot;ii&quot;, &quot;i&quot;, &quot;ii&quot;, &quot;i&quot;, &quot;ii&quot;, &quot;i&quot;, &quot;ii&quot;, &quot;ii&quot;],
        'value': [2, 4, 2, 3, 5, 1, 2, 4, 1, 5, 11]}
df = pd.DataFrame(data)

# Convert to categorical for performance and efficient code generation

for col in ['gr', 'subgr']:
    df[col] = df[col].astype('category')
'''    
OR 

df['gr'] = df['gr'].astype('category')
df['subgr'] = df['subgr'].astype('category')
'''

# Extract numerical codes from categorical columns for NumPy operations
gr_codes = df['gr'].cat.codes.to_numpy()
subgr_codes = df['subgr'].cat.codes.to_numpy()

# Combine gr and subgr codes into a single structured array for unique identification
gr_subgr_compound_ids = np.rec.fromarrays([gr_codes, subgr_codes])

# Get unique combinations and an array mapping each original row to its 
# unique combination ID
unique_gr_subgr_combinations, inverse_map_to_unique_ids = np.unique(
    gr_subgr_compound_ids, return_inverse = True
)

# Sum values for each unique gr-subgr combination using bincount
# inverse_map_to_unique_ids acts as the &quot;bins&quot; for summing
gr_subgr_total_sums = np.bincount(inverse_map_to_unique_ids, weights = df['value'].values)

# Determine the rank order based on the total sums (highest sum gets rank 1)
# argsort gives ascending order, [::-1] reverses to descending
order_of_sums_by_rank = gr_subgr_total_sums.argsort()[::-1]

# Create an empty array to store the final ranks for each unique combination
calculated_ranks = np.empty_like(order_of_sums_by_rank)

# Assign ranks: the combination at order_of_sums_by_rank[0] gets rank 1, etc.
calculated_ranks[order_of_sums_by_rank] = np.arange(1, len(order_of_sums_by_rank) + 1)

# Map the calculated ranks back to each row of the original DataFrame
df['rank'] = calculated_ranks[inverse_map_to_unique_ids]

print(df)
'''
  gr subgr  value  rank
0      a        i      2     3
1      a       ii      4     4
2      a        i      2     3
3      a       ii      3     4
4      a        i      5     3
5      b       ii      1     2
6      b        i      2     5
7      b       ii      4     2
8      b        i      1     5
9      b       ii      5     2
10     c       ii     11     1
'''

</code></pre>
","0","Answer"
"79581552","79434242","<p>You should be able to merge these together (which implicitly sorts on the key due to the outer join) forward fill the index columns, then sort on those forward filled index columns.</p>
<p>The forward filling on the <code>&quot;area2_index&quot;</code> creates ties that can then fall back to the ordering of the <code>&quot;area1_index&quot;</code> column.</p>
<p>A shorthand for this is to use the <code>sort_values(…, key=…)</code> argument, which helps to preserve the original non-forward filled values in the result.</p>
<pre class=""lang-py prettyprint-override""><code>from numpy import nan
import pandas as pd

df1 = pd.DataFrame({'area1_index': [0,1,2,3,4,5], 'area1_name': ['AL','AK','AZ','AR','CA','CO']})
df2 = pd.DataFrame({'area2_index': [0,1,2,3,4,5,6], 'area2_name': ['MN','AL','CT','TX','AK','AR','CA']})

df3 = (
    df1.merge(df2, left_on='area1_name', right_on='area2_name', how='outer')
    .sort_values(['area2_index', 'area1_index'], key=lambda s: s.ffill())
    .reset_index(drop=True)
)
#    area1_index area1_name  area2_index area2_name
# 0          NaN        NaN          0.0         MN
# 1          0.0         AL          1.0         AL
# 2          NaN        NaN          2.0         CT
# 3          NaN        NaN          3.0         TX
# 4          1.0         AK          4.0         AK
# 5          2.0         AZ          NaN        NaN
# 6          3.0         AR          5.0         AR
# 7          4.0         CA          6.0         CA
# 8          5.0         CO          NaN        NaN


final = pd.DataFrame({'area1_index': [nan,0,nan,nan,1,2,3,4,5], 'area1_name': [nan,'AL',nan,nan,'AK','AZ','AR','CA','CO'], 'area2_index': [0,1,2,3,4,nan,5,6,nan], 'area2_name':['MN','AL','CT','TX','AK',nan,'AR','CA',nan]})
assert df3.equals(final) # True
</code></pre>
","0","Answer"
"79592257","79448271","<p>Solution using Polars.</p>
<pre><code>import polars as pl


df = pl.DataFrame({
    'group': [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;],
    'subgroup': [&quot;i&quot;, &quot;ii&quot;, &quot;i&quot;, &quot;ii&quot;, &quot;i&quot;, &quot;ii&quot;, &quot;i&quot;, &quot;ii&quot;, &quot;i&quot;, &quot;ii&quot;, &quot;ii&quot;],
    'value': [2, 4, 2, 3, 5, 1, 2, 4, 1, 5, 11]
})
'''
shape: (11, 3)
┌───────┬──────────┬───────┐
│ group ┆ subgroup ┆ value │
│ ---   ┆ ---      ┆ ---   │
│ str   ┆ str      ┆ i64   │
╞═══════╪══════════╪═══════╡
│ a     ┆ i        ┆ 2     │
│ a     ┆ ii       ┆ 4     │
│ a     ┆ i        ┆ 2     │
│ a     ┆ ii       ┆ 3     │
│ a     ┆ i        ┆ 5     │
│ …     ┆ …        ┆ …     │
│ b     ┆ i        ┆ 2     │
│ b     ┆ ii       ┆ 4     │
│ b     ┆ i        ┆ 1     │
│ b     ┆ ii       ┆ 5     │
│ c     ┆ ii       ┆ 11    │
└───────┴──────────┴───────┘
'''
df = df.with_columns([
pl.col('group').cast(pl.Categorical), 
pl.col('subgroup').cast(pl.Categorical)    
])

'''
shape: (11, 3)
┌───────┬──────────┬───────┐
│ group ┆ subgroup ┆ value │
│ ---   ┆ ---      ┆ ---   │
│ cat   ┆ cat      ┆ i64   │
╞═══════╪══════════╪═══════╡
│ a     ┆ i        ┆ 2     │
│ a     ┆ ii       ┆ 4     │
│ a     ┆ i        ┆ 2     │
│ a     ┆ ii       ┆ 3     │
│ a     ┆ i        ┆ 5     │
│ …     ┆ …        ┆ …     │
│ b     ┆ i        ┆ 2     │
│ b     ┆ ii       ┆ 4     │
│ b     ┆ i        ┆ 1     │
│ b     ┆ ii       ┆ 5     │
│ c     ┆ ii       ┆ 11    │
└───────┴──────────┴───────┘
'''

agg = df.group_by(['group','subgroup']).agg(
pl.col('value').sum().alias('sum_value')    
).sort(
'sum_value',descending = True    
).with_columns(pl.arange(1,pl.len() +1).alias('rank'))

'''
shape: (5, 4)
┌───────┬──────────┬───────────┬──────┐
│ group ┆ subgroup ┆ sum_value ┆ rank │
│ ---   ┆ ---      ┆ ---       ┆ ---  │
│ cat   ┆ cat      ┆ i64       ┆ i64  │
╞═══════╪══════════╪═══════════╪══════╡
│ c     ┆ ii       ┆ 11        ┆ 1    │
│ b     ┆ ii       ┆ 10        ┆ 2    │
│ a     ┆ i        ┆ 9         ┆ 3    │
│ a     ┆ ii       ┆ 7         ┆ 4    │
│ b     ┆ i        ┆ 3         ┆ 5    │
└───────┴──────────┴───────────┴──────┘
'''

df = df.join( agg, on = ['group','subgroup']).sort('rank')

print(df)
'''
shape: (11, 5)
┌───────┬──────────┬───────┬───────────┬──────┐
│ group ┆ subgroup ┆ value ┆ sum_value ┆ rank │
│ ---   ┆ ---      ┆ ---   ┆ ---       ┆ ---  │
│ cat   ┆ cat      ┆ i64   ┆ i64       ┆ i64  │
╞═══════╪══════════╪═══════╪═══════════╪══════╡
│ c     ┆ ii       ┆ 11    ┆ 11        ┆ 1    │
│ b     ┆ ii       ┆ 1     ┆ 10        ┆ 2    │
│ b     ┆ ii       ┆ 4     ┆ 10        ┆ 2    │
│ b     ┆ ii       ┆ 5     ┆ 10        ┆ 2    │
│ a     ┆ i        ┆ 2     ┆ 9         ┆ 3    │
│ …     ┆ …        ┆ …     ┆ …         ┆ …    │
│ a     ┆ i        ┆ 5     ┆ 9         ┆ 3    │
│ a     ┆ ii       ┆ 4     ┆ 7         ┆ 4    │
│ a     ┆ ii       ┆ 3     ┆ 7         ┆ 4    │
│ b     ┆ i        ┆ 2     ┆ 3         ┆ 5    │
│ b     ┆ i        ┆ 1     ┆ 3         ┆ 5    │
└───────┴──────────┴───────┴───────────┴──────┘
'''
</code></pre>
","0","Answer"
"79593631","79452715","<p>Solution Using only Polars . Manually done, hence,Clearly understandable for beginners. Just for Educational Purposes.</p>
<pre><code>import polars as pl
import numpy as np

np.random.seed(0)
n = 10_000_000
df = pl.DataFrame({
    'segment': np.random.choice(['A', 'B'], size=n),
    'date': np.random.randint(0, 25, size=n)
})


res = df.with_columns([

pl.when(pl.col('segment') == 'A').then(
    pl.when(pl.col('date') &lt; 10).then(pl.lit('Low')).otherwise(pl.lit('High'))
    ).when( pl.col('segment') == 'B').then(
    pl.when(pl.col('date') &lt; 5).then(pl.lit('Low')).when(pl.col('date') &lt; 15).then(pl.lit('Mid'))
    .otherwise(pl.lit('High'))    
    )
    .otherwise(np.nan).alias('Class')]
)
print(res.head(10))
'''
shape: (10, 3)
┌─────────┬──────┬───────┐
│ segment ┆ date ┆ Class │
│ ---     ┆ ---  ┆ ---   │
│ str     ┆ i32  ┆ str   │
╞═════════╪══════╪═══════╡
│ A       ┆ 6    ┆ Low   │
│ B       ┆ 10   ┆ Mid   │
│ B       ┆ 21   ┆ High  │
│ A       ┆ 20   ┆ High  │
│ B       ┆ 21   ┆ High  │
│ B       ┆ 7    ┆ Mid   │
│ B       ┆ 23   ┆ High  │
│ B       ┆ 5    ┆ Mid   │
│ B       ┆ 7    ┆ Mid   │
│ B       ┆ 15   ┆ High  │
└─────────┴──────┴───────┘
'''
</code></pre>
","0","Answer"
"79608048","79452715","<pre><code>import pandas as pd
import numpy as np


data = {
    'Segment': np.random.choice(['A', 'B', 'C'], size=10**7),
    'Date': np.random.randint(20230101, 20250301, size=10**7)
}
df = pd.DataFrame(data)


cuts = {
    &quot;A&quot;: {&quot;cut&quot;: [0, 20240101, 20240801, 20241201, 20250000], &quot;class&quot;: [&quot;out&quot;, &quot;training&quot;, &quot;validation&quot;, &quot;out&quot;]},
    &quot;B&quot;: {&quot;cut&quot;: [0, 20230701, 20240701, 20241201, 20250000], &quot;class&quot;: [&quot;out&quot;, &quot;training&quot;, &quot;validation&quot;, &quot;out&quot;]},
    &quot;C&quot;: {&quot;cut&quot;: [0, 20230701, 20240701, 20250101, 20250201], &quot;class&quot;: [&quot;out&quot;, &quot;training&quot;, &quot;validation&quot;, &quot;out&quot;]}
}

classArr = np.empty(df.shape[0],dtype = object)

for seg,rules in cuts.items():
    mask = df['Segment'].values == seg 
    seg_dates = df.loc[mask,'Date'].values    
    bins = np.array(rules['cut'])
    class_np = np.array(rules['class'])
    idx = np.searchsorted(bins, seg_dates, side = 'right') -1
    idx = np.clip(idx, 0, len(class_np) -1)
    classArr[mask] =  class_np[idx]

df['Class'] = classArr
print(df.head(10))   
'''
  Segment      Date     Class
0       B  20247412       out
1       C  20238399  training
2       A  20244649       out
3       A  20248293       out
4       B  20237773  training
5       B  20230962  training
6       B  20238465  training
7       B  20232447  training
8       A  20235937       out
9       C  20231294  training
''' 
</code></pre>
","0","Answer"
"79512382","79512283","<p>Here is the script:</p>
<pre><code>import pandas as pd
import networkx as nx
from collections import Counter


data = {
    &quot;Product&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;],
    &quot;Selling_Locations&quot;: [[1, 2, 3], [2, 5], [7, 8, 9, 10], [5, 4], [10, 11]]
}

df = pd.DataFrame(data)


G = nx.Graph()


for product in df[&quot;Product&quot;]:
    G.add_node(product)

for i in range(len(df)):
    for j in range(i + 1, len(df)):
        if set(df[&quot;Selling_Locations&quot;][i]) &amp; set(df[&quot;Selling_Locations&quot;][j]):
            G.add_edge(df[&quot;Product&quot;][i], df[&quot;Product&quot;][j])

groups = list(nx.connected_components(G))


product_to_locations = dict(zip(df[&quot;Product&quot;], df[&quot;Selling_Locations&quot;]))

group_representative_locations = []
for group in groups:
    locations = []
    for product in group:
        locations.extend(product_to_locations[product])
    
    location_counts = Counter(locations)
    
    most_frequent_location = max(location_counts, key=lambda x: (location_counts[x], x))
    
    group_representative_locations.append(most_frequent_location)

for i, (group, location) in enumerate(zip(groups, group_representative_locations), 1):
    print(f&quot;Group {i}: Products = {sorted(group)}, Representative Location = {location}&quot;)
</code></pre>
<p>Ouput:</p>
<p><a href=""https://i.sstatic.net/zObwBnL5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zObwBnL5.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79512824","79512793","<p>You can use another list like this</p>
<pre><code>first_list =[1,2,3,4,5,6]
filter_list=[]
for item in first_list:
    if item % 2 ==0:
        filter_list.append(item)
</code></pre>
","1","Answer"
"79512861","79512283","<p>First <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>split</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a> the locations, and use <a href=""https://networkx.org"" rel=""nofollow noreferrer""><code>networkx</code></a> to form a graph of the products and locations. Then find the <a href=""https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html"" rel=""nofollow noreferrer""><code>connected_components</code></a>:</p>
<pre><code>import networkx as nx

tmp = (df[['PRODUCT']]
       .assign(LOCATION=df['SELLING LOCATIONS'].str.split(', *'))
       .explode('LOCATION')
      )

G = nx.from_pandas_edgelist(tmp, source='PRODUCT', target='LOCATION')

products = set(df['PRODUCT'])

groups = [c&amp;products for c in nx.connected_components(G)]
</code></pre>
<p>Output:</p>
<pre><code>[{'A', 'B', 'D'}, {'C', 'E'}]
</code></pre>
<p>Intermediate <code>tmp</code>:</p>
<pre><code>  PRODUCT LOCATION
0       A        1
0       A        2
0       A        3
1       B        2
1       B        5
2       C        7
2       C        8
2       C        9
2       C       10
3       D        5
3       D        4
4       E       10
4       E       11
</code></pre>
<p>Alternative to build the graph:</p>
<pre><code>G = nx.from_edgelist((p, l.strip()) for p, s in
                     zip(df['PRODUCT'], df['SELLING LOCATIONS'])
                     for l in s.split(','))
</code></pre>
<p>Graph:</p>
<p><a href=""https://i.sstatic.net/ZSOLzpmS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZSOLzpmS.png"" alt=""networkx graph"" /></a></p>
","0","Answer"
"79513245","79513050","<p>We can use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.repeat.html"" rel=""nofollow noreferrer""><code>np.repeat</code></a> and after check matches cells for each row in your dataframe</p>
<pre><code>#df = df.set_index(&quot;index&quot;) #optional if index is a column
all_data = df[&quot;columnName&quot;].str.split(&quot;,&quot;, expand=True).to_numpy()
data_ordered = np.repeat([all_data[0, :-1]], df.shape[0], axis=0)
final_df = \
pd.DataFrame(np.where(np.equal(data_ordered[..., np.newaxis], 
                               all_data[:, np.newaxis, :])
                .any(axis=2), 
                      data_ordered, 
                      None), index=df.index)
print(final_df)

#           0      1       2     3
#index                            
#0      apple  peach  orange  pear
#1      apple   None  orange  pear
#2       None   None    None  pear
#3      apple  peach    None  None
#4       None   None  orange  None
</code></pre>
","0","Answer"
"79513279","79513050","<p>Here is another way:</p>
<pre><code>s = df['columnName'].str.strip(',').str.split(', ?').explode()

s.set_axis(pd.MultiIndex.from_frame(s.groupby(s).ngroup().reset_index())).unstack()
</code></pre>
<p>Output:</p>
<pre><code>0          0       1      2     3
index                            
0      apple  orange  peach  pear
1      apple  orange    NaN  pear
2        NaN     NaN    NaN  pear
3      apple     NaN  peach   NaN
4        NaN  orange    NaN   NaN
</code></pre>
","1","Answer"
"79513298","79513050","<p>A possible solution:</p>
<pre><code>aux = df['columnName'].str.split(',')
order = aux.iloc[0]
exploded = aux.explode()
m = exploded.values[:, None] == np.array(order)[None, :]

rows, cols = np.where(m)
result_rows = exploded.index

result = np.full((len(aux), len(order)), None)
result[result_rows, cols] = exploded.values[rows]

(pd.DataFrame(result, index=df.index, columns=order)
 .set_axis(range(len(order)), axis=1).reset_index())
</code></pre>
<p>With <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a>, it expands lists into individual rows for element-wise comparison. Afterwards, with <code>m = exploded.values[:, None] == np.array(order)[None, :]</code>, it uses numpy broadcasting to create a 2D boolean mask of matches.  Next, <a href=""https://numpy.org/doc/stable/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>np.where</code></a> finds matching positions, and <code>result[result_rows, cols] = ...</code> fills aligned values into a preallocated array.</p>
<hr />
<p>Another possible solution, based on dictionary comprehension:</p>
<pre><code>out = df['columnName'].str.split(',')

(pd.DataFrame([{
    x: x if x in row else None for x in out[0]} for row in out])
.set_axis(range(len(df)-1), axis=1).reset_index())
</code></pre>
<p>It constructs a dataframe where each row aligns elements to the order of the first row. For each row, a dictionary comprehension checks if elements from the first row exist (assigning <code>None</code> otherwise), creating aligned columns. <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_axis.html"" rel=""nofollow noreferrer""><code>set_axis</code></a> reindexes columns numerically, and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a> adds the original index as a column.</p>
<hr />
<p>Yet another possible solution, based on <code>merge</code>:</p>
<pre><code>out = df['columnName'].str.split(',', expand=True).T
pd.concat(
    [out[[0]]] + [out[[0]].merge(out[[i]], left_on=0, right_on=i, how='left')[[i]] 
     for i in out.columns[1:]], axis=1).T.reset_index()
</code></pre>
<p>It splits the <code>columnName</code> into a transposed dataframe (<code>out</code>) (assuming the last comma in the first row is a typo), then uses <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>pd.concat</code></a> to sequentially merge each subsequent column with the first column (<code>left_on=0</code>, <code>right_on=i</code>, via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>pd.merge</code></a>), aligning values based on the first column’s order. The result is transposed and combined, mimicking row-wise alignment to the first row’s structure. The final <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a> restores row indices.</p>
<hr />
<p>Output:</p>
<pre><code>   index      0      1       2     3
0      0  apple  peach  orange  pear
1      1  apple    NaN  orange   NaN
2      2    NaN    NaN     NaN  pear
3      3  apple  peach     NaN   NaN
4      4    NaN    NaN  orange   NaN
</code></pre>
","0","Answer"
"79513506","79512868","<p>Just get the last cell in the range column number and add 1 as you have with the other fields.</p>
<p>This code assumes the merge cells are row only.<br>
Also assumes the key name is cells is exactly the same as the name in the properties List</p>
<pre><code>import os
import openpyxl


def get_next_col(lc):   # lc = Left cell in the merge range
    for merge in ws.merged_cells:
        if lc in merge.coord:
            print(f&quot;Merge Range: {merge.coord}&quot;)
            return merge.top[-1][1]+1  # Return 2nd value of last tuple incremented by 1


def format_value(value):
    if isinstance(value, float) and value &gt; 1e10:  # Large numbers like NID
        return str(int(value))  # Convert to integer and string to avoid scientific notation
    elif isinstance(value, (int, float)):  # General number formatting
        return str(value)
    elif value is None:
        return &quot;N/A&quot;  # Handle missing values
    return str(value).strip()


# Define the properties to extract
properties = [
    &quot;Approval Memo of&quot;,
    &quot;Name of the Applicant&quot;,
    &quot;Name of Territory&quot;,
    &quot;Total Family Expenses&quot;,
    &quot;Ref&quot;,
    &quot;Amount&quot;,
    &quot;Total Amount&quot;
]

# Init Dictionary 
extracted_data = {}  

# Set working sheet name
target_sheet = &quot;Output Approval Templete&quot;


# Load the workbook
file_path = r&quot;D:\file\input\example.xlsx&quot;

if os.path.exists(file_path):
    print(&quot;File exists!\n&quot;)
else:
    print(&quot;File not found! Check the path.&quot;)
    exit()

wb = openpyxl.load_workbook(file_path, data_only=True)
ws = wb.active

# Check working sheet exists
if target_sheet not in wb.sheetnames:
    print(f&quot;Sheet '{target_sheet}' not found in the file.&quot;)
else:
    ws = wb[target_sheet]

# Process rows 
for row in ws.iter_rows():
    for cell in row:
        cv = cell.value
        if isinstance(cv, str):   # Strip if the cell value is a string
            cv = cv.strip()
        if cv in properties:  # Process only cells with value in the 'properties' List
            co = cell.coordinate
            print(f&quot;Processing '{cv}' in 'Properties' List at cell {co}&quot;)

            if co in ws.merged_cells:  # Check if the current cell is in a merge
                print('This is also a merged cell:')
                col = get_next_col(co)   # If merged get the next col number after the merge range
            else:
                col = cell.col_idx + 1   # If not merged get the next col number after the cell

            next_value = ws.cell(cell.row, col).value  # Get next cell value as determined by value of 'col'
            print(f&quot;Inserting Key: '{cv}' with Value: {next_value}&quot;)
            extracted_data[cv] = format_value(next_value)  # Add key and value to the dictionary
            print(&quot;-----------\n&quot;)

for key, val in extracted_data.items():
    print(f&quot;{key} : {val}&quot;)
</code></pre>
<br>
<p>Output<br>
<sub>Extracted data from example Sheet.</sub></p>
<pre><code>Approval Memo of : SHILPI AKTER
Name of the Applicant : SHILPI AKTER
Name of Territory : Comilla
Total Family Expenses : 30000
Ref : 22000
Amount : 5000
Total Amount : 3000
</code></pre>
","1","Answer"
"79513733","79513720","<p>Build a column <code>week</code> and then use grouping over the columns <code>year</code>, <code>month</code> and <code>week</code> and use <code>.ffill</code> and <code>.bfill</code>:</p>
<pre><code>df['week'] = df['day'].apply(lambda x: (x - 1) // 7 + 1)  # Assign week numbers
df['C'] = df.groupby(['year', 'month', 'week'])['C'].transform(lambda x: x.ffill().bfill())
</code></pre>
","0","Answer"
"79513789","79513766","<p>Here is one way:</p>
<pre><code>import numpy as np
import pandas as pd
x = np.asarray([1, 2])[:, np.newaxis]
y = np.asarray([3, 4, 5])
x, y = np.broadcast_arrays(x, y)
z = x + y
df = pd.DataFrame(zip(x.ravel(), y.ravel(), z.ravel()), columns=[&quot;x&quot;, &quot;y&quot;, &quot;z&quot;])
print(df)
#    x  y  z
# 0  1  3  4
# 1  1  4  5
# 2  1  5  6
# 3  2  3  5
# 4  2  4  6
# 5  2  5  7
</code></pre>
<p>But yes, you can also use meshgrid instead of orthogonal arrays + explicit broadcasting. You can also use NumPy instead of Pandas.</p>
<pre><code>x = np.asarray([1, 2])
y = np.asarray([3, 4, 5])
x, y = np.meshgrid(x, y, indexing='ij')
z = x + y
print(np.stack((x.ravel(), y.ravel(), z.ravel())).T)
# array([[1, 3, 4],
#        [1, 4, 5],
#        [1, 5, 6],
#        [2, 3, 5],
#        [2, 4, 6],
#        [2, 5, 7]])
</code></pre>
","5","Answer"
"79513895","79513766","<p>Not as efficient as low level numpy broadcasting, but you could use a cross-<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>:</p>
<pre><code>x = [1, 2]
y = [3, 4, 5]

df = (pd.DataFrame({'x': x})
        .merge(pd.DataFrame({'y': y}), how='cross')
        .eval('z = x+y') # or .assign(z=lambda d: d['x']+d['y'])
     )
</code></pre>
<p>Alternative with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.from_product.html"" rel=""nofollow noreferrer""><code>MultiIndex.from_product</code></a> if you have many combinations of arrays/lists:</p>
<pre><code>df = (pd.MultiIndex.from_product([x, y], names=['x', 'y'])
        .to_frame(index=False)
        .eval('z = x+y')
     )

# or in pure python
df = (pd.DataFrame(product(x, y), columns=['x', 'y'])
        .eval('z = x+y')
     )
</code></pre>
<p>Output:</p>
<pre><code>   x  y  z
0  1  3  4
1  1  4  5
2  1  5  6
3  2  3  5
4  2  4  6
5  2  5  7
</code></pre>
","2","Answer"
"79513925","79513766","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>df = (
  pd.DataFrame(
      np.array(np.meshgrid(x, y)).T.reshape(-1, 2), 
      columns=['x', 'y']
      )
  .assign(z=lambda df: df.sum(axis=1))
  )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>   x  y  z
0  1  3  4
1  1  4  5
2  1  5  6
3  2  3  5
4  2  4  6
5  2  5  7
</code></pre>
<p><strong>Explanation / Intermediate</strong></p>
<ul>
<li>Use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html"" rel=""nofollow noreferrer""><code>np.meshgrid</code></a> with default <code>indexing='xy'</code> (cartesian).</li>
<li>Pass result to <a href=""https://numpy.org/doc/stable/reference/generated/numpy.array.html"" rel=""nofollow noreferrer""><code>np.array</code></a> + <a href=""https://numpy.org/doc/stable/reference/generated/numpy.ndarray.transpose.html"" rel=""nofollow noreferrer""><code>.np.transpose</code></a> (<code>.T</code>) + <a href=""https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html"" rel=""nofollow noreferrer""><code>ndarray.reshape</code></a>:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>np.array(np.meshgrid(x, y)).T.reshape(-1, 2)

array([[1, 3],
       [1, 4],
       [1, 5],
       [2, 3],
       [2, 4],
       [2, 5]])
</code></pre>
<ul>
<li>Use inside <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>pd.DataFrame</code></a> and add <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html"" rel=""nofollow noreferrer""><code>df.sum</code></a> on <code>axis=1</code> via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>df.assign</code></a>.</li>
</ul>
","2","Answer"
"79514140","79513586","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>df['B2_wanted'] = (
    df.reindex(
        df.droplevel('i3').set_index('A', append=True).index
    )['B2'].to_numpy()
)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>             A  B1  B2  c1  B2_wanted
i1 i2 i3                             
1  10 101  103   2   1   1        2.0
      102  101   2   2   2        1.0
      103  100   1   2   2        NaN
   11 100  102   1   2   2        NaN
      101  103   1   2   2        NaN
   12 101  103   2   1   2        1.0
      103  101   1   1   2        1.0
2  10 100  103   1   2   1        NaN
      101  103   1   1   2        NaN
      102  101   1   1   1        1.0
</code></pre>
<p><strong>Explanation</strong></p>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.droplevel.html"" rel=""nofollow noreferrer""><code>df.droplevel</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>df.set_index</code></a> to exchange level 'i3' for column 'A' and select <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.index.html"" rel=""nofollow noreferrer""><code>df.index</code></a>.</li>
<li>Pass the new index to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>df.reindex</code></a> and assign 'B2' values via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.to_numpy.html"" rel=""nofollow noreferrer""><code>Series.to_numpy</code></a>.</li>
</ul>
","0","Answer"
"79515105","79515072","<p>One option could be to extract the last digits of the SSN, rework the names and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html"" rel=""nofollow noreferrer""><code>extract</code></a> from the full names, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>:</p>
<pre><code>import re

employees = (Employee_Info['Last_Name']+', '+Employee_Info['First_Name']).str.casefold()
pat = f&quot;({'|'.join(map(re.escape, employees.unique()))})&quot;
employees_info = df['Name'].str.lower().str.extract(pat, expand=False)

out = df[['Name']].merge(Employee_Info,
                         left_on=[employees, df['SSN'].str[-4:]],
                         right_on=[employees_info, Employee_Info['SSN'].str[-4:]],
                         how='left'
                        )
</code></pre>
<p>Output:</p>
<pre><code>           key_0 key_1           Name First_Name Last_Name          SSN  EmployeeID
0      doe, john  1234    Doe, John A       John       Doe  999-45-1234          12
1      doe, jane  9876    Doe, Jane B       JANE       DOE  999-45-9876          13
2  test, example  0192  Test, Example    Example      Test  999-45-0192          14
</code></pre>
","0","Answer"
"79515193","79515072","<p>Another possible solution:</p>
<pre><code>pd.merge(
    df2.assign(
        First_Name=df2['First_Name'].str.upper(), 
        Last_Name=df2['Last_Name'].str.upper()), 
    pd.concat([
        df1, 
        df1['Name'].str.replace(r'\s\D$', '', regex=True)
        .str.upper().str.split(', ', expand=True)], axis=1), 
    right_on=[1, 0], left_on=['First_Name', 'Last_Name'], 
    suffixes=['', '_y'])[df1.columns.to_list() + ['EmployeeID']]
</code></pre>
<p>It first modifies <code>df2</code> using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>assign</code></a> to create uppercase versions of the <code>First_Name</code> and <code>Last_Name</code> columns. Then, it constructs an extended version of <code>df1</code> using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>, where the <code>Name</code> column is processed with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.replace.html"" rel=""nofollow noreferrer""><code>str.replace()</code></a> to remove any trailing single-character initials and then split into separate first and last names using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>str.split(expand=True)</code></a>. The <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge()</code></a> function is then applied, aligning the transformed name columns (<code>First_Name</code> and <code>Last_Name</code>) with the corresponding split names from <code>df1</code>, using <code>right_on=[1, 0]</code> and <code>left_on=['First_Name', 'Last_Name']</code>. Finally, the output retains only the columns from <code>df1</code>, along with the <code>EmployeeID</code> column.</p>
<hr />
<p>The following updates the solution to contemplate the case mentioned by the OP in a comment below:</p>
<pre><code>pd.merge(
    df2.assign(
        First_Name=df2['First_Name'].str.upper(), 
        Last_Name=df2['Last_Name'].str.upper(), 
        aux=df2['SSN'].str.extract(r'.*\-(\d{4})$')),
    pd.concat([
        df1.assign(aux=df2['SSN'].str.extract(r'.*\-(\d{4})$')), 
        df1['Name'].str.replace(r'\s\D$', '', regex=True)
        .str.upper().str.split(', ', expand=True)], axis=1), 
    right_on=[1, 0, 'aux'], left_on=['First_Name', 'Last_Name', 'aux'], 
    suffixes=['', '_y'])[df1.columns.to_list() + ['EmployeeID']]
</code></pre>
<hr />
<p>Output:</p>
<pre><code>            Name          SSN  EmployeeID
0    Doe, John A  999-45-1234          12
1    Doe, Jane B  999-45-9876          13
2  Test, Example  999-45-0192          14
</code></pre>
","1","Answer"
"79516129","79515992","<p>Your code is breaking too early in the clause starting here:</p>
<pre><code>
if street_number % 2 == 0: # check if street number is even 
</code></pre>
<p>if the first rows &quot;Partie&quot; is not &quot;Pair&quot;, you won't assign anything, and you will hit the break below your else statement, exiting the loop.</p>
<p>To fix it, we can add the requirement that it matches to your conditional:</p>
<pre><code>def sort_df(first_df, second_df):
    for bano_index, row in second_df.iterrows():
        street_number = row['numero']
        street_name = row['nom_afnor']
        #print(f'first loop: {num_de_rue}')

        if street_name in first_df['Voie'].values: # check if street name exists
            reims_filtered = first_df.loc[first_df['Voie'] == street_name] # if it does create a dataframe containing only the elements matching the street name
            #print(reims_filtered)
            for reims_index, reims_matching_row in reims_filtered.iterrows(): # iterate over the rows the filtered dataframe
                #print(reims_matching_row)
                if street_number &gt;= reims_matching_row['Début'] and street_number &lt;= reims_matching_row['Fin']: # check if street number is between range of street segment
                    #print(f'Check range {street_number} {reims_matching_row['Périmètre élémentaire']}')
                    if street_number % 2 == 0 and reims_matching_row['Partie'] == 'Pair': # check if street number is even
                        #print(reims_index, street_number, reims_matching_row['Partie'])
                        print(f'Check column {street_number} {reims_matching_row[&quot;Périmètre élémentaire&quot;]}')
                        sector_to_assign = reims_matching_row[&quot;Périmètre élémentaire&quot;]
                        second_df.at[bano_index, 'Secteur Elementaire'] = sector_to_assign
                        break
                    elif street_number % 2 != 0 and reims_matching_row['Partie'] == 'Impair':
                        print(f'Check odd {street_number} {reims_matching_row[&quot;Périmètre élémentaire&quot;]}')
                        sector_to_assign = reims_matching_row[&quot;Périmètre élémentaire&quot;]
                        second_df.at[bano_index, 'Secteur Elementaire'] = sector_to_assign
                        break
                    break

    return second_df
</code></pre>
<p>Depending on the size of the data, you could imagine a subset by street, then a join on a range to speed things up.</p>
","0","Answer"
"79516476","79515992","<p>You can use conditional_join from pyjanitor to merge both dataframes based on your conditions.</p>
<pre><code># pip install pyjanitor
import janitor

final_df = (second_df.assign(p=(second_df['numero']%2).map({0: 'Pair', 1: 'Impair'}))
           .conditional_join(first_df, ('numero', 'Début', '&gt;='), ('numero', 'Fin', '&lt;='),
                       ('nom_afnor', 'Voie', '=='), ('p', 'Partie', '=='))
           .assign(**{&quot;Secteur Elementaire&quot;:lambda d: d['Périmètre élémentaire']})
           .loc[:, second_df.columns])
</code></pre>
<p>End result:</p>
<pre><code> numero        nom_afnor Secteur Elementaire
      1 AVENUE D EPERNAY           AVRANCHES
      2 AVENUE D EPERNAY  SCULPTEURS JACQUES
      6 AVENUE D EPERNAY  SCULPTEURS JACQUES
      7 AVENUE D EPERNAY           AVRANCHES
      8 AVENUE D EPERNAY  SCULPTEURS JACQUES
      9 AVENUE D EPERNAY           AVRANCHES
     10 AVENUE D EPERNAY  SCULPTEURS JACQUES
     12 AVENUE D EPERNAY  SCULPTEURS JACQUES
</code></pre>
","0","Answer"
"79516745","79516578","<p>One option to give you a general logic that would need to be fined tuned.</p>
<p>You can combine detection of outliers and of non-monotonic values (respective to their neighbors):</p>
<pre><code># parameters for outlier detection
window = 3  # increase this if you have a lot of data points
n_std = 1

g = df.sort_values(by='DATE').groupby('VIN')['MILEAGE']
r = g.rolling(window=window, min_periods=1, center=True)

std = r.std().droplevel('VIN')
median = r.median().droplevel('VIN')

# is the value not monotonic?
diff = g.diff().lt(0)
m1 = diff | diff.shift(-1)

# is the value an outlier?
m2 = ~df['MILEAGE'].between(median-n_std*std, median+n_std*std)

# filter out unwanted values
out = df[~(m1 &amp; m2)]
</code></pre>
<p>Output:</p>
<pre><code>  VIN  MILEAGE        DATE
0   a    10000  2024-01-01
1   a    20000  2024-02-01
3   a    30000  2024-04-01
5   a    40000  2024-06-01
</code></pre>
<p>Intermediates:</p>
<pre><code>  VIN  MILEAGE        DATE            std   median   diff     m1     m2  m1&amp;m2
0   a    10000  2024-01-01    7071.067812  15000.0  False  False  False  False
1   a    20000  2024-02-01  193476.958146  20000.0  False  False  False  False
2   a   350000  2024-03-01  187705.443004  30000.0  False   True   True   True
3   a    30000  2024-04-01  193591.152308  30000.0   True   True  False  False
4   a     1234  2024-05-01   20125.794030  30000.0   True   True   True   True
5   a    40000  2024-06-01   27411.701479  20617.0  False  False  False  False
</code></pre>
<p>Plot of the outlier detection bounds:</p>
<p><a href=""https://i.sstatic.net/bZY38yLU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZY38yLU.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79517148","79517122","<p>First thing, <code>fuzzywuzzy</code> has been replaced by <a href=""https://pypi.org/project/thefuzz/"" rel=""nofollow noreferrer""><code>thefuzz</code></a>, so make sure to use this library.</p>
<p>Then, you can replicate your error with this simple code:</p>
<pre><code>#from fuzzywuzzy import process
from thefuzz import process  # new library

def find_book(user_input,books):
    try:
        result = process.extractOne(user_input, books)
        if result:  
            match, score = result  
            if score &gt; 70:  
                return match
        return None  
    except:
        return None

books = pd.Series(['Book Name 1', 'Another title', 'title'])
user_input = 'Title'
match_book = find_book(user_input,books) # None
</code></pre>
<p>The issue is that you are passing a Series to <code>processOne</code>, which causes it to return a tuple of 3 values while you catch two. Because you use a broad <code>try</code>/<code>except</code> without specifying what kind of error you would expect, this error is silent and your code just returns None (see. <a href=""https://stackoverflow.com/questions/54948548/what-is-wrong-with-using-a-bare-except"">What is wrong with using a bare 'except'?</a>).</p>
<p>Use:</p>
<pre><code>result = process.extractOne(user_input, books.tolist())
</code></pre>
<p>You can simplify the full function code to:</p>
<pre><code>def find_book(user_input, books):
    result = process.extractOne(user_input, books.tolist())
    if result:  
        match, score = result  
        if score &gt; 70:  
                return match
</code></pre>
<h4>Why was it returning 3 values?</h4>
<p>When the input is a dictionary like object, processOne (actually the underlying <code>extractWithoutOrder</code> function) returns a tuple of <code>(choice, score, key)</code> (<a href=""https://github.com/seatgeek/thefuzz/blob/master/thefuzz/process.py#L174-L178"" rel=""nofollow noreferrer"">source</a>):</p>
<pre><code>    for choice, score, key in it:
        if is_lowered:
            score = int(round(score))

        yield (choice, score, key) if is_mapping else (choice, score)
</code></pre>
<p>Since a Series behaves like a dictionary, this also returns the index.</p>
<p>You could therefore also catch the index:</p>
<pre><code>match, score, index = result 
</code></pre>
<p>Or handle both cases with a catch-all:</p>
<pre><code>def find_book(user_input, books):
    result = process.extractOne(user_input, books)
    if result:  
        match, score, *_ = result  
        if score &gt; 70:  
                return match
</code></pre>
","3","Answer"
"79517154","79517122","<p>I think that <code>books</code> is a Pandas Series, but <code>fuzzywuzzy.process.extractOne</code> expects a list of strings as the second argument. So you may try to convert books to a list before passing it to <code>process.extractOne </code>writing:</p>
<pre><code>books = pd1[&quot;title&quot;].str.strip().str.lower().tolist()  
</code></pre>
","1","Answer"
"79517228","79517222","<p>You can't use <code>isin</code> with nested objects.</p>
<p>Use a nested list comprehension on the two columns (for efficiency with first convert the lists in &quot;woman&quot; to sets):</p>
<pre><code>couples['man_not_woman'] = [[x for x in m if x not in w]
                            for m, w in
                            zip(couples['man'],
                                couples['woman'].apply(set))]
</code></pre>
<p>Output:</p>
<pre><code>                                   man                               woman        man_not_woman
0           [fishing, biking, reading]           [biking, reading, movies]            [fishing]
1          [hunting, mudding, fishing]                 [fishing, drinking]   [hunting, mudding]
2           [reading, movies, running]                 [knitting, reading]    [movies, running]
3  [running, reading, biking, mudding]  [running, biking, fishing, movies]   [reading, mudding]
4          [movies, reading, yodeling]                            [movies]  [reading, yodeling]
</code></pre>
<p>Or, much easier, convert all your lists to <a href=""https://docs.python.org/3/library/stdtypes.html#set"" rel=""nofollow noreferrer""><code>set</code></a> and you will be able to perform an arithmetic operation directly:</p>
<pre><code>couples[['man', 'woman']] = couples[['man', 'woman']].map(set)

couples['man_not_woman'] = couples['man']-couples['woman']
</code></pre>
<p>Output:</p>
<pre><code>                                   man                               woman        man_not_woman
0           {fishing, biking, reading}           {biking, movies, reading}            {fishing}
1          {fishing, hunting, mudding}                 {fishing, drinking}   {hunting, mudding}
2           {movies, running, reading}                 {knitting, reading}    {movies, running}
3  {biking, mudding, running, reading}  {fishing, biking, movies, running}   {reading, mudding}
4          {movies, yodeling, reading}                            {movies}  {yodeling, reading}
</code></pre>
<p>For fun (but really overkill here), you could also <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a>, perform an anti-<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a>:</p>
<pre><code>couples['man_not_woman'] = (
 couples['man'].explode().reset_index()
 .merge(couples['woman'].explode().reset_index(),
        left_on=['index', 'man'],
        right_on=['index', 'woman'],
        how='outer', indicator=True)
 .query('_merge == &quot;left_only&quot;')
 .groupby('index')['man'].agg(list)
)
</code></pre>
","1","Answer"
"79517359","79515992","<p>You can use a list comprehension with bitwise filtering to get each appropriate elementary sector then convert that to a series and replace the 'Secteur Elementaire' column with the wanted values. For filtering odd and even, I used a dictionary to map 0 to 'Pair' and 1 to 'Impair'.</p>
<pre><code>odd_even = {0: 'Pair', 1: 'Impair'} #Make a dictionary to map odd and evens

second_df['Secteur Elementaire'] = pd.Series([
    (filtered.iloc[0] if not filtered.empty else 'tbd')  #If the filter can't find a result    
    for _, row in second_df.iterrows()
    for filtered in [first_df.loc[
        (first_df['Voie'] == row['nom_afnor']) &amp; #Filter bitwise 
        (row['numero'] &lt;= first_df['Fin']) &amp; 
        (row['numero'] &gt;= first_df['Début']) &amp;
        (first_df['Partie'] == odd_even[row['numero'] % 2]), #Use the dictionary
        'Périmètre élémentaire' #The column you want
    ]]
], index=second_df.index) #Need to specify the index for correct order
</code></pre>
<p>Since the series will be applied according to the index, you can use this principle to perform this on subsections of the dataframe if your dataframe is large.</p>
<p>Alternatively, you can make it a function and use apply, which might be more convenient and easier to debug.</p>
<pre><code>def find_perimeter(row):
    filtered = first_df.loc[
        (first_df['Voie'] == row['nom_afnor']) &amp;
        (row['numero'] &lt;= first_df['Fin']) &amp; 
        (row['numero'] &gt;= first_df['Début']) &amp;
        (first_df['Partie'] == odd_even[row['numero'] % 2]),
        'Périmètre élémentaire'
    ]
    return filtered.iloc[0] if not filtered.empty else 'tbd'

second_df['Secteur Elementaire'] = second_df.apply(find_perimeter, axis=1)
</code></pre>
<p>Both of these will fill your 'Secteur Elementaire' column with the filtered values. Be aware, if a street could fit multiple possible sectors, it will pick the sequential first in the dataframe because of <code>filtered.iloc[0]</code>.</p>
","0","Answer"
"79518453","79518311","<p>Let's look at row 14:</p>
<p>It's <code>5 - 4 + 1</code> but that <code>5</code> is from the previous calculation <code>4 - 2 + 3</code>, so the full calculation is really <code>4 - 2 + 3 - 4 + 1</code> and what that really involves are the cumulative sum of non-zero values and cumulative total of zeros (i.e. <code>4 + 3</code> (non-zeros) - <code>2 + 4</code> (zeros) + <code>1</code> (current value)).</p>
<p>With that in mind:</p>
<pre><code>cum_zero = (ser == 0).cumsum()
cum_zero *= (ser != 0)

right_before_zero = ser * (ser.shift(-1) == 0)
previous = right_before_zero.shift(1).cumsum().fillna(0) * (ser != 0)

ser2 = previous - cum_zero + ser
</code></pre>
","3","Answer"
"79518510","79518311","<p>Although this can only be done with Pandas in a rather convoluted way IMHO, here is a straightforward implementation using Numba (which should also be faster than all Pandas solutions):</p>
<pre class=""lang-py prettyprint-override""><code>import numba as nb
import numpy as np

@nb.njit(['(int32[:],)', '(int64[:],)'])
def compute(arr):
    res = np.empty(arr.size, dtype=arr.dtype)
    z_count = 0
    last_nnz_val = 0
    nnz_count = 0
    for i in range(arr.size):
        if arr[i] == 0:
            if i &gt; 0 and arr[i-1] != 0:   # If there is a switch from nnz to zero
                last_nnz_val += nnz_count - z_count   # Save the last nnz result
                z_count = 0
            z_count += 1
            res[i] = 0
        else:
            if i &gt; 0 and arr[i-1] == 0:   # If there is a switch from zero to nnz
                nnz_count = 0
            nnz_count += 1
            res[i] = last_nnz_val - z_count + nnz_count
    return res

# [...]
compute(ser.to_numpy())
</code></pre>
<p>Note the result is a basic Numpy array, but you can easily create a dataframe from it.</p>
<hr />
<h2>Benchmark</h2>
<p>Here are performance results on my machine (i5-9600KF CPU) on the tiny example dataset:</p>
<pre class=""lang-none prettyprint-override""><code>MichaelCao's answer:    886 µs
This answer:              2 µs   &lt;-----
</code></pre>
<p>On a 1000x larger dataset (repeated), I get:</p>
<pre class=""lang-none prettyprint-override""><code>MichaelCao's answer:   1240 µs
This answer:             20 µs   &lt;-----
</code></pre>
<p>It is <strong>much faster</strong> than the other answer.
I also get different output results so <strong>one of the answer implementation is certainly wrong</strong>.</p>
","3","Answer"
"79518849","79517222","<p>Approach using <code>isin()</code> doesn't work because it designed for checking in element-wise manner in series. You may try using set operations, it will give you the differences between the hobbies of the man &amp; woman.</p>
<pre><code>couples['hobbiess_man_only'] = couples.apply(lambda row: list(set(row[&quot;man&quot;]) - set(row[&quot;woman&quot;])), axis=1)
</code></pre>
<ol>
<li><p>Using <code>apply()</code> with <code>axis=1</code> to operate in row-wise</p>
</li>
<li><p>Convert each list to a <code>set()</code> &amp; compute the difference.</p>
<p><a href=""https://i.sstatic.net/65T0ZKhB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/65T0ZKhB.png"" alt=""enter image description here"" /></a></p>
<p>Hope this helps!</p>
</li>
</ol>
","0","Answer"
"79519107","79518999","<p>This is an issue with the <a href=""https://pypi.org/project/Bottleneck/"" rel=""nofollow noreferrer"">Bottleneck</a> optional dependency, used to accelerate some NaN-related routines. I think the wrong result happens due to <a href=""https://github.com/pandas-dev/pandas/issues/25307"" rel=""nofollow noreferrer"">loss of precision while calculating the mean</a>, since Bottleneck uses naive summation, while NumPy uses more accurate <a href=""https://en.wikipedia.org/wiki/Pairwise_summation"" rel=""nofollow noreferrer"">pairwise summation</a>.</p>
<p>You can disable Bottleneck with</p>
<pre><code>pd.set_option('compute.use_bottleneck', False)
</code></pre>
<p>to fall back to the NumPy handling.</p>
","3","Answer"
"79519846","79519830","<p>I don't think there is a much more straightforward approach. There is currently no <code>rolling.last</code> method.</p>
<p>You could however simplify a bit your code:</p>
<pre><code>def last_nonzero(s):
    return 0 if (x:=s[s != 0]).empty else x.iloc[-1]

df['b'] = (df['a'].shift(1, fill_value=0)
           .rolling(window=3, min_periods=1).apply(last_nonzero)
           .convert_dtypes()
          )
</code></pre>
<p>With a lambda:</p>
<pre><code>df['b'] = (df['a'].shift(1, fill_value=0)
           .rolling(window=3, min_periods=1)
           .apply(lambda s: 0 if (x:=s[s != 0]).empty else x.iloc[-1])
           .convert_dtypes()
          )
</code></pre>
<p>Actually, if you have a range index, you could also use a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.merge_asof.html"" rel=""nofollow noreferrer""><code>merge_asof</code></a> on the indices:</p>
<pre><code>window = 3
out = pd.merge_asof(
    df,
    df['a'].shift(1, fill_value=0).loc[lambda x: x != 0].rename('b'),
    left_index=True,
    right_index=True,
    tolerance=window-1,
    direction='backward',
).fillna({'b': 0})
</code></pre>
<p>Output:</p>
<pre><code>    a   b
0   0   0
1   0   0
2   1   0
3  -1   1
4  -1  -1
5   0  -1
6   0  -1
7   0  -1
8   0   0
9   0   0
10 -1   0
11  0  -1
12  0  -1
13  1  -1
14  0   1
</code></pre>
","4","Answer"
"79521746","79521718","<p><strong>Answer</strong></p>
<p>I'm not sure if you've figured out exactly what you want, but one way to do it might be to <strong>randomize the entire sample and then sort by the number of appearances.</strong></p>
<pre><code>out = (
    df.sample(frac=1)
    .sort_values('Name', key=lambda x: x.groupby(x).cumcount(), kind='stable')
    .reset_index(drop=True)
)
</code></pre>
<p>out:</p>
<pre><code>     Name Score
0     Amy    15
1     Sam    12
2    John     3
3   Sarah     7
4     Sam     4
5    John     9
6     Amy     5
7     Sam     2
8     Amy    10
9     Sam     6
10    Sam     8

</code></pre>
<hr />
<p><strong>Example</strong></p>
<pre><code>import pandas as pd
data = {'Name': ['Sam', 'John', 'Amy', 'Sam', 'Sam', 'Amy', 'Sam', 'Amy', 'Sarah', 'John', 'Sam'], 
        'Score':['2','3','5','4','6','10','8','15','7','9', '12'] }
df = pd.DataFrame(data)
</code></pre>
","0","Answer"
"79522426","79518641","<p>You can do it using col() method or using simple d1[col_name] (array access method):</p>
<pre><code># The Snowpark package is required for Python Worksheets. 
# You can add more packages by selecting them using the Packages control and then importing them.

import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col

def main(session: snowpark.Session): 

    df1 = session.create_dataframe([ (&quot;Premium-Life&quot;,1.0,1.1,1.2),(&quot;ExpAcct&quot;,0.5, -1.0, 0.0)],schema = [&quot;company_a&quot;,&quot;t0&quot;,&quot;t1&quot;,&quot;t2&quot;])

    df2 = session.create_dataframe([ (&quot;Premium&quot;, &quot;Premium-Life&quot;, &quot;Prem (Ann)&quot;),(&quot;Expenses&quot;, &quot;ExpAcct&quot;,&quot;MaintExp&quot;)],schema = [&quot;final&quot;,&quot;company_a&quot;,&quot;company_b&quot;])

    
    # return df1.join(df2, df1.company_a == df2.company_a ) -- static join
    col_name = &quot;company_a&quot;

    # alternative 1:
    return df1.join(df2, df1[col_name] == df2.company_a )

    # alternative 2:
    return df1.join(df2, df1.col(col_name) == df2.company_a )
</code></pre>
","0","Answer"
"79522795","79522783","<p>By default a non-existent time will raise an error.</p>
<p>There is an <code>nonexistent</code> option in <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.floor.html"" rel=""nofollow noreferrer""><code>Timestamp.floor</code></a> to shift the time forward/backward:</p>
<pre><code>(pd.Timestamp('2024-09-08 12:00:00-0300', tz='America/Santiago')
   .floor('D', nonexistent='shift_backward')
)
# Timestamp('2024-09-07 23:59:59-0400', tz='America/Santiago')

(pd.Timestamp('2024-09-08 12:00:00-0300', tz='America/Santiago')
   .floor('D', nonexistent='shift_forward')
)
# Timestamp('2024-09-08 01:00:00-0300', tz='America/Santiago')
</code></pre>
<p>Or, to get 23:00 passing <code>pd.Timedelta('-1h')</code> (not sure of the relevance to do this):</p>
<pre><code>(pd.Timestamp('2024-09-08 12:00:00-0300', tz='America/Santiago')
   .floor('D', nonexistent=pd.Timedelta('-1h'))
)
# Timestamp('2024-09-07 23:00:00-0400', tz='America/Santiago')
</code></pre>
","2","Answer"
"79524106","79524057","<p>This should work. Logic: First match, but do not include in the capture group <code>\1</code>, anything we do not want to keep at the beginning of the number with the negated character class <code>[^...]</code>.</p>
<p>Python (re module flavor regex):</p>
<pre><code>pattern = &quot;^[^\w+-\.]*([+-]?\d*\.?\d*)&quot;
replacement = \1
</code></pre>
<p>Regex Demo: <a href=""https://regex101.com/r/Halz4X/2"" rel=""nofollow noreferrer"">https://regex101.com/r/Halz4X/2</a></p>
<p>NOTES:</p>
<ul>
<li><code>^</code> Matches the beginning of the string.</li>
<li><code>[^\w+-\.]*</code> *Negated class <code>[^...]</code> that matches anything that is not an alphanumeric or underscore, <code>_</code>, character (<code>\w</code>), or a literal <code>+</code>, <code>-</code> or <code>.</code>, 0 or more times (<code>*</code>).</li>
<li><code>(</code> Begin first (and only) capture group, referred to in the replacement string with <code>\1</code>.</li>
<li>[+-]?\d*.?\d*
<ul>
<li><code>[+-]?</code> Match 0 or 1 (<code>?</code>) <code>+</code> or <code>-</code></li>
<li><code>\d*</code> Match 0 or more (<code>*</code>) digits <code>\d</code>.</li>
<li><code>\.</code> Match a literal dot, <code>.</code>.</li>
<li><code>\d*</code> Match 0 or more (<code>*</code>) digits <code>\d</code>.</li>
</ul>
</li>
<li><code>)</code> End capture (group 1 <code>\1</code>).</li>
<li>Replacement string: <code>\1</code> replaces the matched string with only the characters in the capture group 1.</li>
</ul>
","3","Answer"
"79524390","79524057","<p>Try to run the code below</p>
<pre><code>import re
p = re.compile(&quot;\&lt;|\&gt;&quot;)

df['col1'] = df['col1'].apply(lambda x: p.sub(&quot;&quot;, str(x)) if p.findall(str(x)) else x)
df['col2'] = df['col2'].apply(lambda x: p.sub(&quot;&quot;, str(x)) if p.findall(str(x)) else x)

</code></pre>
<p>Then you have the result.</p>
<pre><code>out:
    col1    col2
0   1   3.
1   NaN 5.0
2   3   NaN
3   NA  NA
</code></pre>
<p>and if you would like to get float type, you can try this.</p>
<pre><code>import numpy as np

df['col1'] = df['col1'].apply(lambda x: np.nan if x=='NA' else float(x))
df['col2'] = df['col2'].apply(lambda x: np.nan if x=='NA' else float(x))
</code></pre>
<p>Then you have the result as well.</p>
<pre><code>out:
    col1    col2
0   1.0   3.0
1   NaN   5.0
2   3.0   NaN
3   NaN   NaN
</code></pre>
","2","Answer"
"79524449","79524057","<p>This Python code produces the desired results on a <em>list</em> named <code>col2</code>. This may give you an idea on how this could be done with the dataframes. Here I am just working with a list.  <em>(Note: In my other answer, I focused only on getting the floats, but did not address the requirement of having all other elements replaced with <code>NaN</code>.)</em></p>
<p>APPROACH:  Get every element from the <code>col2</code>. Check to see if it is an element with a float, replace the value of the element with the float.  If the element does not contain a float, replace it with literal <code>NaN</code>.</p>
<p>ASSUMPTIONS:</p>
<ul>
<li>The string element containing a float has a string begins with 0 or 1 characters that is NOT an alphanumerical character, or underscore (<code>\w</code>), <code>_</code>, <code>.</code>, <code>#</code>,
<code>$</code>, <code>+</code>, or <code>-</code>; Followed by 0 or 1  <code>+</code> or <code>-</code> characters;</li>
<li>Followed by 0 or more digits (<code>\d</code>), followed by 0 or 1 literal <code>.</code> , followed by 0 or more  digits.</li>
<li>The string ends in a digit.</li>
<li>It may not consist of only of non-alphanumeric or non-underscore characters.</li>
</ul>
<p>PYTHON CODE (<code>re</code> regex module):</p>
<pre><code>import re

col2 = [&quot;1&quot;, &quot;np.nan&quot;, &quot;3&quot;, &quot;NA&quot;, &quot;3.&quot;, &quot;&lt;5.0&quot;, &quot;&gt;-5&quot;, &quot;-5&quot;, &quot;&gt;&gt;&gt;&gt;&gt;&gt;5&quot;, &quot;&lt;&lt;&lt;&lt;&quot;, &quot;&lt;&lt;4&quot;, &quot;&lt;&lt;4.9&quot;, &quot;&lt;4.9&quot;, &quot;nan&quot;, &quot;NA&quot;, &quot;&gt;&gt;&quot;, &quot;.&quot;, &quot;+&quot;, &quot;-&quot;, &quot;...9898d7afs33&quot;, &quot;#32211&quot;, &quot;..&quot;, &quot;$44&quot;, &quot;33$&quot;]

pattern_to_find_floats = r'^(?!\W+$)[^\w.#$+-]?([+-]?\d*\.?\d*)$'
pattern = re.compile(pattern_to_find_floats)

def repl(match1, pattern=pattern):
    if re.match(pattern, match1.group(0) ):
        # IF the string matches the pattern_to_find_floats. Return the updated string with float with optional +/- sign.
        return re.sub(pattern, r&quot;\1&quot;, match1.group(0))
    else: 
        return 'NaN'        
updated_col2 = []

for item in col2:
    # SYNTAX: updated_string = re.sub(pattern, replacement, original_string)
    updated_item = re.sub(r&quot;.*&quot;, repl, item) 
    updated_col2.append(updated_item)

print(f&quot;Original col2: {col2}&quot;)
print(f&quot;Updated col2: {updated_col2}&quot;)
    
</code></pre>
<p><em>Regex Demo (pattern_to_find_floats):</em> <a href=""https://regex101.com/r/7HfIly/6"" rel=""nofollow noreferrer"">https://regex101.com/r/7HfIly/6</a></p>
<p>RESULT:</p>
<pre><code>Original col2: ['1', 'np.nan', '3', 'NA', '3.', '&lt;5.0', '&gt;-5', '-5', '&gt;&gt;&gt;&gt;&gt;&gt;5', '&lt;&lt;&lt;&lt;', '&lt;&lt;4', '&lt;&lt;4.9', '&lt;4.9', 'nan', 'NA', '&gt;&gt;', '.', '+', '-', '...9898d7afs33', '#32211', '..', '$44', '33$']
Updated col2: ['1', 'NaN', '3', 'NaN', '3.', '5.0', '-5', '-5', 'NaN', 'NaN', 'NaN', 'NaN', '4.9', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']
</code></pre>
<p>CONVERSION FROM TO:</p>
<pre><code>Original --&gt; Replacement
1 --&gt; 1
np.nan --&gt; NaN
3 --&gt; 3
NA --&gt; NaN
3. --&gt; 3.
&lt;5.0 --&gt; 5.0
&gt;-5 --&gt; -5
-5 --&gt; -5
&gt;&gt;&gt;&gt;&gt;&gt;5 --&gt; NaN
&lt;&lt;&lt;&lt; --&gt; NaN
&lt;&lt;4 --&gt; NaN
&lt;&lt;4.9 --&gt; NaN
&lt;4.9 --&gt; 4.9
nan --&gt; NaN
NA --&gt; NaN
&gt;&gt; --&gt; NaN
. --&gt; NaN
+ --&gt; NaN
- --&gt; NaN
...9898d7afs33 --&gt; NaN
#32211 --&gt; NaN
.. --&gt; NaN
$44 --&gt; NaN
33$ --&gt; NaN
</code></pre>
<p>REGEX NOTES:</p>
<ul>
<li><code>^</code> Match beginning of string.</li>
<li><code>(?!\W+$)</code> <em>Negative lookahead</em> assertion <code>(?!...)</code> Matches if the string from beginning <code>^</code> to end <code>$</code> DOES NOT contain only (1 or more <code>+</code>) non-alphanumeric and non-underscore characters <code>\W</code>. Does not consume any characters.</li>
<li><code>[^\w.#$+-]?</code>  <em>Positive character class</em> <code>[...]</code> matches 0 or 1 (<code>+</code>) characters that is alphanumerical or underscore character (<code>\w</code>), literal <code>.</code>, <code>#</code>, <code>$</code>, <code>+</code>, <code>-</code>.</li>
<li><code>(</code> Begin capture group <code>(...)</code>, group <code>\1</code> for the <em>float</em>.</li>
<li><code>[+-]?</code> Match 0 or 1 <code>+</code> or <code>-</code> characters.</li>
<li><code>\d*</code> Match 0 or more digits <code>\d</code>.</li>
<li><code>\.?</code> Match 0 or 1 literal <code>.</code>.</li>
<li><code>\d*</code> Match 0 or more digits <code>\d</code>.</li>
<li><code>)</code> Close capture group, group <code>\1</code>.</li>
<li><code>$</code> Matches end of string.</li>
<li>The regex replacement string <code>r&quot;\1&quot;</code> replaces the entire match with the characters captured in <em>group 1</em> (<code>\1</code>).</li>
</ul>
","1","Answer"
"79525210","79524057","<p>I suggest using:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import re

df = pd.DataFrame(data = {'col1': [1,np.nan, '&gt;3', 'NA'], 'col2':[&quot;3.&quot;,&quot;&lt;5.0&quot;,np.nan, 'NA']})

# from &quot;&gt;|&lt;&quot; fields match only numbers to group 1
pattern=re.compile(r&quot;^[&lt;&gt;]([+-]?[0-9]*\.?[0-9]*)$&quot;)

(
    df
    # remove &quot;&gt;&quot; and &quot;&lt;&quot; with technique suggested by rich neadle
    .replace(pattern,r&quot;\1&quot;,regex=True)
    # map litearl &quot;NA&quot; to numpys Not a Number
    .replace(&quot;NA&quot;,np.nan)
    # cast float type across dataframe
    .astype(float)
)
</code></pre>
","2","Answer"
"79525911","79525897","<p>De-duplicate with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>groupby.cumcount</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html"" rel=""nofollow noreferrer""><code>pivot</code></a>:</p>
<pre><code>index = ['Device', 'Serial']
cols = ['Property']
values = 'Value'

(df.assign(n=df.groupby(index+cols).cumcount())
   .pivot(index=index+['n'], columns=cols, values=values)
   .reset_index(index).rename_axis(index=None, columns=None)
)
</code></pre>
<p>Output:</p>
<pre><code>       Device   Serial     Display Name Install Date  Publisher        Version
0  Computer2A  ABCSFT2   Microsoft Edge    2/22/2025  Microsoft  133.0.3065.92
1  Computer2A  ABCSFT2  MSVC++ 2022 X64    8/13/2024  Microsoft    14.40.33810
2  Computer2A  ABCSFT2  MSVC++ 2022 X86    8/13/2024  Microsoft    14.40.33810
</code></pre>
","0","Answer"
"79526182","79525802","<p>This is because you are using <code>df_idx</code> to offset the bars. <code>df_idx</code> will only be <code>0</code> or <code>1</code>.</p>
<p>When multiple data sets (traces) share a common x-axis (for vertical bars) or y-axis (for horizontal bars), the offsetgroup property allows you to align bars at the same position. Assigning the different offsetgroup value to these traces instructs Plotly to treat them as a different groups for offsetting, ensuring bars with matching coordinates don't overlay.</p>
<p>You need to create another variable for offsetting the positions.</p>
<p>Example:</p>
<pre><code>fig = go.Figure()
cols = ['NO2 mug/m^3', 'NO mug/m^3']  # Selected gases
dfs = [ag, snd]  # List of dataframes

offsetgroup = 0 # new variable for offsetting position

for col_idx, col in enumerate(cols):  # Iterate over gases
  for df_idx, df in enumerate(dfs):  # Iterate over DataFrames (locations)
    
        fig.add_trace(go.Bar(
            x=df['Date'],
            y=df[col],
            name=f'{df.Municipality.unique()[0]} - {col}',
            offsetgroup=str(offsetgroup),  # Group bars per location
            marker=dict(opacity=0.8),
            yaxis=&quot;y&quot; if col_idx == 0 else &quot;y2&quot;  # Assign second gas to secondary y-axis
        ))
        offsetgroup += 1


# Layout adjustments
layout_args = {
    &quot;barmode&quot;: &quot;group&quot;,  # Ensures bars are placed side-by-side
    &quot;xaxis&quot;: {&quot;title&quot;: &quot;Date&quot;},
    &quot;legend_title&quot;: &quot;Location - Gas&quot;
}

if len(cols) == 1:
    layout_args[&quot;yaxis&quot;] = {&quot;title&quot;: cols[0]}  # Single Y-axis case
else:
    layout_args[&quot;yaxis&quot;] = {&quot;title&quot;: cols[0]}
    layout_args[&quot;yaxis2&quot;] = {
        &quot;title&quot;: cols[1],
        &quot;overlaying&quot;: &quot;y&quot;,  # Overlay on primary y-axis
        &quot;side&quot;: &quot;right&quot;,
        &quot;showgrid&quot;: False
    }

fig.update_layout(**layout_args)
fig.show()
</code></pre>
","3","Answer"
"79526410","79526398","<p>In your example, it seems that the <strong>No. of Sports Field</strong> remains the same for a given <strong>City</strong>.</p>
<p>You can therefore use <code>first()</code> upon grouping by <strong>City</strong>, before sorting by <strong>No. of Sports Field</strong> :</p>
<p><code>df.groupby('City').first().sort_values(by='No. of Sport Fields')</code></p>
<p>This returns:</p>
<pre><code>    Athlete ID  No. of Sport Fields
City        
NJ  2234    64
SF  1223    75
NYC 4231    80
NYC.    2345    80
LA  1231    81
</code></pre>
","1","Answer"
"79526413","79526398","<p>I had to search for this the other day. This will group by AthleteID and agg rows but then keep the columns separate in the row instead of doing a typical agg</p>
<pre><code>df['idx'] = df.groupby('AthleteID').cumcount()+1
df = df.pivot_table(index=['AthleteID'], columns='idx', 
                values=[&quot;City&quot;,&quot;No. of Sport Fields&quot;], aggfunc='first')
df = df.sort_index(axis=1,level=1)
df.columns=[f'{x}_{y}' for x,y in df.columns]
</code></pre>
","0","Answer"
"79526518","79526121","<p>Something like this could work.</p>
<pre><code>df.assign(difference = df.groupby(['Date','item'])['cost'].transform('max').sub(df['cost']))
</code></pre>
<p>Output:</p>
<pre><code>  distributor Location        Date    item  cost  difference
0         GEC       KA  2025-02-01   Apple  10.0         0.0
1         GEC       TN  2025-02-01   Apple  10.0         0.0
2         GEC       AP  2025-02-01   Apple   9.0         1.0
3         GEC       KA  2025-02-02  Orange   8.0         0.5
4         GEC       TN  2025-02-02  Orange   7.0         1.5
5         GEC       AP  2025-02-02  Orange   8.5         0.0
6         GEC       KA  2025-02-03  Banana   6.0         0.0
7         GEC       TN  2025-02-03  Banana   6.0         0.0
8         GEC       AP  2025-02-03  Banana   6.0         0.0
</code></pre>
","0","Answer"
"79526547","79526121","<p>If I understood your question correctly, you want to aggregate the items and dates and see if there are variations in the price?</p>
<pre><code>import pandas as pd

# Your df:

data = {
    'distributor': ['GEC'] * 9,
    'Location': ['KA', 'TN', 'AP', 'KA', 'TN', 'AP', 'KA', 'TN', 'AP'],
    'Date': ['2025-02-01'] * 3 + ['2025-02-02'] * 3 + ['2025-02-03'] * 3,
    'item': ['Apple'] * 3 + ['Orange'] * 3 + ['Banana'] * 3,
    'cost': [10.0, 10.0, 9.0, 8.0, 7.0, 8.5, 6.0, 6.0, 6.0]
}

df = pd.DataFrame(data)

df
</code></pre>
<p>distributor Location    Date    item    cost</p>
<p>0   GEC KA  2025-02-01  Apple   10.0</p>
<p>1   GEC TN  2025-02-01  Apple   10.0</p>
<p>2   GEC AP  2025-02-01  Apple   9.0</p>
<p>3   GEC KA  2025-02-02  Orange  8.0</p>
<p>4   GEC TN  2025-02-02  Orange  7.0</p>
<p>5   GEC AP  2025-02-02  Orange  8.5</p>
<p>6   GEC KA  2025-02-03  Banana  6.0</p>
<p>7   GEC TN  2025-02-03  Banana  6.0</p>
<p>8   GEC AP  2025-02-03  Banana  6.0</p>
<pre><code># You can group by date and item
grouped = df.groupby(['Date', 'item'])

# Check cost variations
variation_df = grouped['cost'].agg(['nunique', 'min', 'max']).reset_index()

# If you want only those in df with variations, get rows that have more than 1 rows
variation_df = variation_df[variation_df['nunique'] &gt; 1]

# cost differences added to df
variation_df['cost_diff'] = variation_df['max'] - variation_df['min']

variation_df
</code></pre>
<p>Date    item    nunique min max cost_diff</p>
<p>0   2025-02-01  Apple   2   9.0 10.0    1.0</p>
<p>1   2025-02-02  Orange  3   7.0 8.5 1.5</p>
<p>Hope this was what you're looking for...</p>
","0","Answer"
"79526976","79526965","<p>You can initialize a pandas dataframe like this:</p>
<pre><code>df = pd.DataFrame({
    'Name': pd.Series(dtype='str'),
    'Age': pd.Series(dtype='int'),
    'Salary': pd.Series(dtype='float'),
    'Date': pd.Series(dtype='datetime64[ns]')
})
</code></pre>
<p>this will create an empty df with specified types per column.
Is this what you were looking for?</p>
<p>With that, you can also use a schema, as such:</p>
<pre><code>import pandas as pd
from typing import List, Dict, Any

def get_dataframe_schema() -&gt; Dict[str, Any]:
    return {
        'name': str,
        'age': int,
        'score': float
    }

def get_dataframe() -&gt; pd.DataFrame:
    schema = get_dataframe_schema()
    l = get_data()
    if not l:
        return pd.DataFrame(columns=schema.keys()).astype(schema)
    df = pd.DataFrame(l)
    return df.astype(schema)
</code></pre>
","0","Answer"
"79528216","79526965","<blockquote>
<p>the DataFrame constructor does not take [multiple] dtypes</p>
</blockquote>
<p>Instead use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html"" rel=""nofollow noreferrer""><code>.astype()</code></a>, which can take a mapping of columns-to-dtypes.</p>
<p>Setup:</p>
<pre><code>&gt;&gt;&gt; dtypes = {'foo': 'int64', 'bar': 'boolean'}
&gt;&gt;&gt; dfe = pd.DataFrame(columns=dtypes.keys())
&gt;&gt;&gt; dfe.dtypes.rename('dtype').to_frame()  # What we don't want: all object
      dtype
foo  object
bar  object
</code></pre>
<p>Run <code>.astype()</code>:</p>
<pre><code>&gt;&gt;&gt; dft = dfe.astype(dtypes)
&gt;&gt;&gt; dft.dtypes.rename('dtype').to_frame()  # What we want
       dtype
foo    int64
bar  boolean
&gt;&gt;&gt; dft  # Still empty - nothing up my sleeves ;)
Empty DataFrame
Columns: [foo, bar]
Index: []
</code></pre>
<p>You can combine them into one line:</p>
<pre><code>pd.DataFrame(columns=dtypes.keys()).astype(dtypes)
</code></pre>
","2","Answer"
"79528639","79527518","<p>The issue is not with pandasai; DuckDB failed to build. You can check this error in the DuckDB repository.</p>
<p><a href=""https://github.com/duckdb/duckdb/issues/9301"" rel=""nofollow noreferrer"">https://github.com/duckdb/duckdb/issues/9301</a></p>
<p>And the solution is provided there</p>
<pre class=""lang-bash prettyprint-override""><code>pip install duckdb --pre --upgrade
</code></pre>
","0","Answer"
"79529394","79529354","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>out = (
    sap_warehouse.set_index('Material')
    .combine_first(
        sap_scrapped.set_index('Material').assign(SSP=0)
        )
    .reset_index()
    )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>       Material  SSP  Scrapped
0  08-008099-00    0         2
1  10-000001-00    0         0
2  10-789001-00   10         7
</code></pre>
<p><strong>Explanation</strong></p>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>.set_index</code></a> to make 'Material' the index for both dfs and apply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.combine_first.html"" rel=""nofollow noreferrer""><code>.combine_first</code></a>.</li>
<li>Add 'SSP' to <code>sap_scrapped</code> as '0' for missing values via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>.assign</code></a>.</li>
<li>Reset the index with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>.reset_index</code></a>.</li>
</ul>
<hr />
<p><strong>Data used</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {'Material': {0: '08-008099-00', 1: '10-000001-00'}, 
        'Scrapped': {0: 2, 1: 5}}
sap_scrapped = pd.DataFrame(data)

data2 = {'Material': {0: '10-000001-00', 1: '10-789001-00'}, 
         'SSP': {0: 0, 1: 10}, 'Scrapped': {0: 0, 1: 7}}
sap_warehouse = pd.DataFrame(data2)
</code></pre>
","0","Answer"
"79531393","79531352","<p>The logic is not fully clear to me, but assuming your intervals are overlapping, that date1 is the start and date2 the end, and you want the max/min, you can perform a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a>, then filter the rows based on <code>df_group</code>:</p>
<pre><code># convert to datetime
df[['data1', 'data2']] = df[['data1', 'data2']].apply(pd.to_datetime)

# sort the values, identify overlapping dates
# form a new group, get the minimal interval
(df
 .sort_values(by=['id', 'data1', 'data2'])
 .assign(n=lambda x: x['data1'].gt(x['data2'].shift()).groupby(x['id']).cumsum())
 .groupby(['id', 'n'], as_index=False)
 .agg({'data1': 'max', 'data2': 'min', 'codice': set})
 # filter the groups that are complete based on &quot;df_group&quot;
 .loc[lambda x: x['id'].map(df_group.set_index('id')['code'].apply(set))
                       .eq(x['codice'])
     ]
)
</code></pre>
<p>Output:</p>
<pre><code>     id  n               data1               data2     codice
0  0111  0 2025-02-03 02:18:00 2025-02-03 02:43:00  {1, 3, 2}
</code></pre>
<p>For a dictionary, add:</p>
<pre><code>out.set_index('id')[['data1', 'data2']].astype(str).T.to_dict()
</code></pre>
<p>Output:</p>
<pre><code>{'0111': {'data1': '2025-02-03 02:18:00', 'data2': '2025-02-03 02:43:00'}}
</code></pre>
","0","Answer"
"79531675","79526325","<p>Your data has the duplicated date/times shown, but the aren't <em>consecutive</em>.   Sort the duplicated data if you want to see the duplicated dates together.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Synthetic data with non-consecutive duplicated dates.
data = {
    'ID': ['ID-1001', 'ID-1002', 'ID-1003', 'ID-1004', 'ID-1005', 'ID-1006', 'ID-1007', 'ID-1008', 'ID-1009', 'ID-1010'],
    'Name': ['Sensor-A', 'Sensor-B', 'Sensor-C', 'Sensor-D', 'Sensor-E', 'Sensor-F', 'Sensor-G', 'Sensor-H', 'Sensor-I', 'Sensor-J'],
    'Code': [330735, 330736, 330737, 330738, 330739, 330740, 330741, 330742, 330743, 330744],
    'Date': [
        '2022-01-01 12:00:00', '2022-01-01 11:00:00', '2022-01-01 13:00:00', '2022-01-01 14:00:00', 
        '2022-01-02 12:00:00', '2022-01-02 13:00:00', '2022-01-02 14:00:00', '2022-01-02 15:00:00', 
        '2022-01-01 12:00:00', '2022-01-02 15:00:00'
    ]
}

# Convert to DataFrame
dd_csv = pd.DataFrame(data)

# Ensure 'Date' is in datetime format
dd_csv['Date'] = pd.to_datetime(dd_csv['Date'])

duplicates_all = dd_csv['Date'].duplicated(keep=False)
print(dd_csv[duplicates_all])
print()
print(dd_csv[duplicates_all].sort_values(by=['Date']))  # sort the Dates
</code></pre>
<p>Output below.  Note that in the first instance, duplicates are listed by not together.</p>
<pre class=""lang-none prettyprint-override""><code>        ID      Name    Code                Date
0  ID-1001  Sensor-A  330735 2022-01-01 12:00:00
7  ID-1008  Sensor-H  330742 2022-01-02 15:00:00
8  ID-1009  Sensor-I  330743 2022-01-01 12:00:00
9  ID-1010  Sensor-J  330744 2022-01-02 15:00:00

        ID      Name    Code                Date
0  ID-1001  Sensor-A  330735 2022-01-01 12:00:00
8  ID-1009  Sensor-I  330743 2022-01-01 12:00:00
7  ID-1008  Sensor-H  330742 2022-01-02 15:00:00
9  ID-1010  Sensor-J  330744 2022-01-02 15:00:00
</code></pre>
","0","Answer"
"79531835","79531715","<p>The issue is that <em>each</em> Table Row (<a href=""https://developer.mozilla.org/en-US/docs/Web/HTML/Element/tr"" rel=""nofollow noreferrer""><code>&lt;tr&gt;</code></a>) contains only Table Header elements (<a href=""https://developer.mozilla.org/en-US/docs/Web/HTML/Element/th"" rel=""nofollow noreferrer""><code>th</code></a>), even though only the <em>first</em> row should logically have such elements. That is to say: your table should have Table Data Cell elements (<a href=""https://developer.mozilla.org/en-US/docs/Web/HTML/Element/td"" rel=""nofollow noreferrer""><code>td</code></a>) after the first row.</p>
<p>By default, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_html.html"" rel=""nofollow noreferrer""><code>pd.read_html</code></a> looks for <code>th</code> elements to determine what it should treat as the header. Hence, you end up with an empty dataframe with MultiIndex column labels. To avoid this, simply set <code>header=0</code>:</p>
<p><strong><a href=""https://stackoverflow.com/help/minimal-reproducible-example"">Minimal Reproducible Example</a></strong></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from io import StringIO

# proper `td` elements from row 1 onwards
correct_table = &quot;&quot;&quot;
&lt;table&gt;
    &lt;tr&gt;&lt;th&gt;col1&lt;/th&gt;&lt;th&gt;col2&lt;/th&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;1&lt;/th&gt;&lt;td&gt;A&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;2&lt;/th&gt;&lt;td&gt;B&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&quot;&quot;&quot;

df1 = pd.read_html(StringIO(correct_table))[0]

# only `th` elements
incorrect_table = &quot;&quot;&quot;
&lt;table&gt;
    &lt;tr&gt;&lt;th&gt;col1&lt;/th&gt;&lt;th&gt;col2&lt;/th&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;th&gt;A&lt;/th&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;th&gt;2&lt;/th&gt;&lt;th&gt;B&lt;/th&gt;&lt;/tr&gt;
&lt;/table&gt;
&quot;&quot;&quot;

df2 = pd.read_html(StringIO(incorrect_table), header=0)[0]

df1.equals(df2)
# True
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>   col1 col2
0     1    A
1     2    B
</code></pre>
<p>Above, <code>StringIO(...)</code> mimics your <code>url</code> file read. Without <code>header=0</code>, you get:</p>
<pre class=""lang-py prettyprint-override""><code>pd.read_html(StringIO(incorrect_table))[0].columns

MultiIndex([('col1', '1', '2'),
            ('col2', 'A', 'B')],
           )
</code></pre>
","1","Answer"
"79532241","79532217","<pre><code>import pandas as pd
import numpy as np

# Example DataFrame consisting of vx,vy,vz,x,y
data = pd.DataFrame({
...
})

# Pivot the data into 2D arrays for each component
x_unique = np.sort(data['x'].unique())
y_unique = np.sort(data['y'].unique())
nx, ny = len(x_unique), len(y_unique)

Vx = data.pivot(index='y', columns='x', values='Vx').values
Vy = data.pivot(index='y', columns='x', values='Vy').values
Vz = data.pivot(index='y', columns='x', values='Vz').values

# Compute grid spacing, typically dx=dy=1
dx = x_unique[1] - x_unique[0]
dy = y_unique[1] - y_unique[0]

# Compute partial derivatives using np.gradient
dVx_dx, dVx_dy = np.gradient(Vx, dx, dy)
dVy_dx, dVy_dy = np.gradient(Vy, dx, dy)
dVz_dx, dVz_dy = np.gradient(Vz, dx, dy)

# Print results
print(&quot;dVx/dx:\n&quot;, dVx_dx)
print(&quot;dVx/dy:\n&quot;, dVx_dy)
</code></pre>
","0","Answer"
"79532533","79532506","<p>You can create the new D columns by multiplying the B columns in <code>df1</code> by the Z column in <code>df2</code> then concatenating them by <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>. The column name can be renamed using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.from_product.html"" rel=""nofollow noreferrer""><code>MultiIndex.from_product</code></a>.</p>
<pre class=""lang-py prettyprint-override""><code>B_columns = df1.loc[:, 'B']
D_columns = B_columns.mul(df2['Z'], axis=0)

# Rename the top level from 'B' to 'D'
smalls = df1.columns.get_level_values(1).unique()
D_columns.columns = pd.MultiIndex.from_product([['D'], smalls], names=df1.columns.names)

df = pd.concat([df1, D_columns], axis=1)
</code></pre>
<p>Using two testing dataframes, <code>df1</code>:</p>
<pre><code>Capitals    A                   B                   C
Smalls      a   b   c   d   e   a   b   c   d   e   a   b   c   d   e
Date                                                            
01-01-25    1   2   3   4   5   1   2   3   4   5   1   2   3   4   5
01-02-25    1   2   3   4   5   1   2   3   4   5   1   2   3   4   5
01-03-25    1   2   3   4   5   1   2   3   4   5   1   2   3   4   5
01-04-25    1   2   3   4   5   1   2   3   4   5   1   2   3   4   5
</code></pre>
<p>and <code>df2</code>:</p>
<pre><code>             X   Y   Z
Date            
01-01-25    10  11  12
01-02-25    10  11  12
01-03-25    10  11  12
01-04-25    10  11  12
</code></pre>
<p>This code returns:</p>
<pre><code>Capitals    A                   B                   C                    D
Smalls      a   b   c   d   e   a   b   c   d   e   a   b   c   d   e    a   b   c   d   e
Date                                                                                
01-01-25    1   2   3   4   5   1   2   3   4   5   1   2   3   4   5   12  24  36  48  60
01-02-25    1   2   3   4   5   1   2   3   4   5   1   2   3   4   5   12  24  36  48  60
01-03-25    1   2   3   4   5   1   2   3   4   5   1   2   3   4   5   12  24  36  48  60
01-04-25    1   2   3   4   5   1   2   3   4   5   1   2   3   4   5   12  24  36  48  60
</code></pre>
","1","Answer"
"79532877","79532506","<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a> for add <code>D</code> level to <code>B</code> and another <code>concat</code> or <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>DataFrame.join</code></a> for append to original:</p>
<pre><code>np.random.seed(2)

mux = pd.MultiIndex.from_product([list('ABC'), list('abcde')])
df1 = pd.DataFrame(np.random.randint(10, size=(4, 15)), 
                   index=pd.date_range('01-01-25','01-04-25'), columns=mux)
print (df1)
            A              B              C            
            a  b  c  d  e  a  b  c  d  e  a  b  c  d  e
2025-01-01  8  8  6  2  8  7  2  1  5  4  4  5  7  3  6
2025-01-02  4  3  7  6  1  3  5  8  4  6  3  9  2  0  4
2025-01-03  2  4  1  7  8  2  9  8  7  1  6  8  5  9  9
2025-01-04  9  3  0  0  2  8  8  2  9  6  5  6  6  6  3

df2 = pd.DataFrame(np.random.randint(10, size=(4, 3)),
                   index=pd.date_range('01-01-25','01-04-25'),
                   columns=list('XYZ'))
print (df2)
            X  Y  Z
2025-01-01  8  2  1
2025-01-02  4  8  1
2025-01-03  6  9  5
2025-01-04  1  2  4
</code></pre>
<hr />
<pre><code>df3 = pd.concat({'D': df1.loc[:, 'B'].mul(df2['X'], axis=0)}, axis=1)
df = pd.concat([df1, df3], axis=1)
</code></pre>
<p>Or:</p>
<pre><code>df3 = pd.concat({'D': df1.loc[:, 'B'].mul(df2['X'], axis=0)}, axis=1)


df = df1.join(df3)
print (df)
            A              B              C               D                
            a  b  c  d  e  a  b  c  d  e  a  b  c  d  e   a   b   c   d   e
2025-01-01  8  8  6  2  8  7  2  1  5  4  4  5  7  3  6   7   2   1   5   4
2025-01-02  4  3  7  6  1  3  5  8  4  6  3  9  2  0  4   3   5   8   4   6
2025-01-03  2  4  1  7  8  2  9  8  7  1  6  8  5  9  9  10  45  40  35   5
2025-01-04  9  3  0  0  2  8  8  2  9  6  5  6  6  6  3  32  32   8  36  24
</code></pre>
","0","Answer"
"79532882","79532783","<p>If you have these 4 arrays, you start with putting them in a dictionary like so:</p>
<pre><code>EDUCATION_LEVELS = {
    'high_school': ['grade 9', 'grade 10', 'grade 11', 'grade 12'],
    'undergrad': ['BS', 'B.Com', 'B.Ed', 'BA', 'LLB'],
    'masters': ['MA', 'MBA', 'M.Ed', 'MPA', 'LLM', 'MTech', 'M.Tech'],
    'doctorate': ['PhD', 'MD', 'DO']  
    }
</code></pre>
<p>After that the easiest is to define a custom mapper and use <code>apply</code> on the column:</p>
<pre><code>def map_education(x):
    for k, v in EDUCATION_LEVELS.items():
        if x in v:
            return k
    return np.nan

df['education'].apply(map_education)
</code></pre>
<p>That should do the trick.</p>
","0","Answer"
"79532894","79532783","<p>My solution does not pretend to be perfect. If I understood you correctly, you would not want to paint a large dictionary of correspondence with your own hands. We can make a spread of the catalog that you have already prepared using the inclusion of the list. For convenience, all this logic can be hidden in a separate function.</p>
<pre class=""lang-py prettyprint-override""><code>def get_grade(education: str) -&gt; str | None:
    glossary = {
        &quot;high_school&quot;: [&quot;grade 9&quot;, &quot;grade 10&quot;, &quot;grade 11&quot;, &quot;grade 12&quot;],
        &quot;undergrad&quot;: [&quot;BS&quot;, &quot;B.Com&quot;, &quot;B.Ed&quot;, &quot;BA&quot;, &quot;LLB&quot;],
        &quot;masters&quot;: [&quot;MA&quot;, &quot;MBA&quot;, &quot;M.Ed&quot;, &quot;MPA&quot;, &quot;LLM&quot;, &quot;MTech&quot;, &quot;M.Tech&quot;],
        &quot;doctorate&quot;: [&quot;PhD&quot;, &quot;MD&quot;, &quot;DO&quot;],
    }
    # swapping the key and values
    glossary_pivot = {
        value: key for key, values in glossary.items() for value in values
    }
    return glossary_pivot.get(education, None)
</code></pre>
<p>The function takes the name of the education as an argument and returns the corresponding grade. If the transferred education is not in the directory, it returns None.
We apply it using the map method.</p>
<pre class=""lang-py prettyprint-override""><code>df['level'] = df['education'].map(get_grade)
</code></pre>
","0","Answer"
"79533013","79532998","<p>First select up to 3 values per group with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.sample.html"" rel=""nofollow noreferrer""><code>groupby.sample</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a> in order of the rows (with a secondary key created with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>groupby.cumcount</code></a>) and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer""><code>fillna</code></a>:</p>
<pre><code># columns to use a group
group = ['MANDANT', 'Fachabteilung', 'FACHEXPERTISE']
# column to fill
col = 'Leistungsgruppe'

# sampling 3 rows per group
sample = MAP.groupby(group)[group+[col]].sample(3)

# compute a key to enumerate the NA values
key_df = df.groupby(group)[col].transform(lambda x: x.isna().cumsum().sub(1))

# merging in order
# set original index (merge doesn't keep the index)
fill = pd.merge(df[group], sample,
         left_on=group+[key_df],
         right_on=group+[sample.groupby(group).cumcount()],
         how='left'
        )[col].set_axis(df.index)

# fill the missing values
df[col] = df[col].fillna(fill)
</code></pre>
<p>If you can have more values to fill in <code>df</code> than 3, then you could use a modulo to generate the merging key:</p>
<pre><code>map_size = df[group].merge(MAP.groupby(group, as_index=False).size())['size'].values
key_df = np.random.randint(3, size=len(df)) % map_size
</code></pre>
<p>Another option with a custom function. First aggregate <code>MAP</code> to keep 3 values, then sample with <a href=""https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html"" rel=""nofollow noreferrer""><code>numpy.random.choice</code></a> in a <code>groupby</code> operation from <code>df</code>:</p>
<pre><code># get samples per group
samples = MAP.groupby(group)[col].agg(lambda x: x.sample(3))

# fill with a custom function
def fill_resample(x, pool=samples):
    return x.fillna(pd.Series(np.random.choice(pool[x.name],
                                               size=len(x),
                                               replace=True),
                              index=x.index))

df[col] = df.groupby(group)[col] .transform(fill_resample)
</code></pre>
<p>Example output:</p>
<pre><code>     PERSONALNUMMER MANDANT Fachabteilung FACHEXPERTISE         Leistungsgruppe
4756        0209740      OM        HA2300          AQ10    Allgemeine Chirurgie
4820        0234212      OM        HA2300          AQ10      Endoprothetik Knie
4855        0251297      OM        HA2300          AQ10    Allgemeine Chirurgie
4750        0209326      OM        HA2300          AQ10  Wirbelsaeuleneingriffe
4992        4000404      OM        HA2300          AQ10    Endoprothetik Huefte
</code></pre>
","0","Answer"
"79533526","79519830","<pre><code>import pandas as pd
import torch


df = pd.DataFrame({
    'a': [0, 0, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0]
})

# Convert the 'a' column of the DataFrame to a PyTorch integer tensor.
atensor = torch.tensor(df['a'].values, dtype=torch.int32)

# Create a PyTorch tensor 'btensor' with the same shape as 'atensor', filled with zeros.
btensor = torch.zeros_like(atensor)

# Iterate through the 'atensor' starting from the 4th element (index 3).
for i in range(3, len(atensor)):
    # Extract a window of 3 elements from 'atensor' preceding the current index 'i'.
    win = atensor[i - 3: i]

    # Select only the non-zero elements from the 'win' tensor.
    nonzero = win[win != 0]

    # Check if there are any non-zero elements in the 'nonzero' tensor.
    if len(nonzero) &gt; 0:
        # If there are non-zero elements, assign the last non-zero element to 'btensor[i]'.
        btensor[i] = nonzero[-1]


df['res'] = btensor.numpy()
print(df)
'''
   a  res
0   0    0
1   0    0
2   1    0
3  -1    1
4  -1   -1
5   0   -1
6   0   -1
7   0   -1
8   0    0
9   0    0
10 -1    0
11  0   -1
12  0   -1
13  1   -1
14  0    1
'''
</code></pre>
","1","Answer"
"79533600","79530224","<p>If you have memory issues, don't use pandas for such a simple task. If the task really just summing up the second value to groups of the first, just do:</p>
<pre><code>sums = {}
with open(&quot;file.csv&quot;,&quot;r&quot;) as infile:
    line = infile.readline()
    while not line == &quot;&quot;:
        values = line.split(&quot;;&quot;) ## Use the appropriate separator here
        lineid, linevalue = values[0],values[1]
        try:
            sums[lineid] += int(linevalue)
        except:
            sums[lineid] = int(linevalue)
        line = infile.readline()

result = {&quot;ID&quot;:[],&quot;Value&quot;:[]}
for i in sorted(list(sums.keys())):
    outdata[&quot;ID&quot;].append(i)
    outdata[&quot;Value&quot;].append(sums[i])
</code></pre>
<p>If you really need to, you can then transform the dictionary <code>result</code> to a pandas df. But you could also just write the result to a new CSV and be done with it.</p>
<p>This should work for very large files, until the <code>sums</code> dictionary causes a memory overflow.</p>
","-1","Answer"
"79533750","79531484","<p>IIUC then you can rewrite the above for loop using <code>pd.concat</code> and then taking the <code>sum</code> over all columns and using column filtering to return on the sports columns finally add <code>to_list</code> to convert those sums to a list.</p>
<p>Try:</p>
<pre><code>sport = pd.concat([survey, CSurvey]).sum(axis=0)[sports].to_list()
</code></pre>
","1","Answer"
"79534168","79534147","<p>At first if you would like to use machine learning you could try this code. However, you might require some editing.</p>
<pre><code>import pandas as pd
from transformers import pipeline

classifier = pipeline(&quot;sentiment-analysis&quot;)

data = {'Name': [&quot;Evil Nights&quot;, &quot;Perfect Day&quot;, &quot;Unknown Song&quot;, 
                &quot;Dead End&quot;, &quot;Angel Love&quot;, &quot;Home Sweet Home&quot;, &quot;The Curse&quot;]}
df = pd.DataFrame(data)

def classify_sentiment(text):
    # First check for neutral/ambiguous terms
    neutral_terms = {'unknown', 'untitled', 'anonymous', 'mystery', 'secret'}
    if any(term in text.lower() for term in neutral_terms):
        return 'neutral'
    
    result = classifier(text)[0]
    label = result['label'].lower()
    score = result['score']
    
    if score &gt; 0.85:  # Higher threshold for more certainty
        return label
    else:
        return 'neutral'

df['Category'] = df['Name'].apply(classify_sentiment)

print(df)
</code></pre>
<p>Otherwise, you could use this code:</p>
<pre><code>import pandas as pd

data = {'Name': [&quot;Evil Nights&quot;, &quot;Perfect Day&quot;, &quot;Unknown Song&quot;, &quot;Dead End&quot;, &quot;Angel Love&quot;, &quot;Home Sweet Home&quot;, &quot;The Curse&quot;]}

df = pd.DataFrame(data)

negative_keywords = [&quot;Evil&quot;, &quot;Night&quot;, &quot;Problem&quot;, &quot;Sorrow&quot;, &quot;Dead&quot;, &quot;Curse&quot;]
positive_keywords = [&quot;Amazing&quot;, &quot;Angel&quot;, &quot;Perfect&quot;, &quot;Sunshine&quot;, &quot;Home&quot;, &quot;Live&quot;, &quot;Friends&quot;]

def categorize_song(name):
    if any(word in name for word in negative_keywords):
        return &quot;Negative&quot;
    elif any(word in name for word in positive_keywords):
        return &quot;Positive&quot;
    else:
        return &quot;Neither&quot;

df[&quot;Song Category&quot;] = df[&quot;Name&quot;].apply(categorize_song)

print(df)
</code></pre>
<p>Ouput:</p>
<p><a href=""https://i.sstatic.net/kuxykub8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kuxykub8.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79534328","79534293","<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.unstack.html"" rel=""nofollow noreferrer""><code>df.unstack</code></a> and again for only <code>level=1</code>:</p>
<pre class=""lang-py prettyprint-override""><code>out = df_init.unstack().unstack(level=1)
</code></pre>
<p>Alternatively, use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>df.stack</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.swaplevel.html"" rel=""nofollow noreferrer""><code>df.swap_level</code></a>. But you will have to add <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_index.html"" rel=""nofollow noreferrer""><code>df.sort_index</code></a> here:</p>
<pre class=""lang-py prettyprint-override""><code>out2 = df_init.stack(level=0, future_stack=True).swaplevel().sort_index(level=0)
</code></pre>
<hr />
<p>Equality check:</p>
<pre class=""lang-py prettyprint-override""><code>all(x.equals(df_final) for x in [out, out2])
# True
</code></pre>
","2","Answer"
"79534849","79534705","<pre><code>out = pd.DataFrame(df['values'].tolist(), index=df['keys'].tolist()).T
</code></pre>
<p>out:</p>
<pre><code>  panda1  panda2  panda3
0   eats   shoots   eats
1  shoots  leaves  leaves
</code></pre>
","3","Answer"
"79534946","79534710","<p>Load your nested second dataset via <code>json_normalize()</code> to get easy access to the individual attributes then use apply to construct a new column.</p>
<pre><code>import json
import pandas as pd

data = {
    &quot;some_id&quot;: &quot;123456&quot;,
    &quot;some_email&quot;: &quot;xyz.abc@acacceptance.com&quot;,
    &quot;some_number&quot; : 123456
}

phone_number = {
    &quot;some_number&quot; : 123456,
    &quot;Contact&quot; : {&quot;phone_number&quot;: 45464464646, &quot;contact?&quot;: &quot;Y&quot;}
}

df_data = pd.DataFrame(data, index=[0])
df_phone_number = pd.json_normalize(phone_number)

df_merged = pd.merge(df_data, df_phone_number, left_on='some_number', right_on='some_number', how='left')
df_merged[&quot;Contact&quot;] = df_merged.apply(lambda x: {&quot;phone_number&quot;: x[&quot;Contact.phone_number&quot;], &quot;contact?&quot;: x[&quot;Contact.contact?&quot;]}, axis=1)
df_merged = df_merged.drop(columns=['Contact.phone_number', 'Contact.contact?'])

json_str = df_merged.to_json(orient='records')
pretty_json = json.dumps(json.loads(json_str), indent=4)
print(pretty_json)
</code></pre>
<p>Giving you:</p>
<pre><code>[
    {
        &quot;some_id&quot;: &quot;123456&quot;,
        &quot;some_email&quot;: &quot;xyz.abc@acacceptance.com&quot;,
        &quot;some_number&quot;: 123456,
        &quot;Contact&quot;: {
            &quot;phone_number&quot;: 45464464646,
            &quot;contact?&quot;: &quot;Y&quot;
        }
    }
]
</code></pre>
","0","Answer"
"79535222","79534705","<p>Yep, using the walrus operator,</p>
<pre><code>(dfe:=df.explode('values')).assign(rows=dfe.groupby(['keys'])\
                           .cumcount())\
                           .pivot(index='rows',columns='keys',values='values')
</code></pre>
<p>Output:</p>
<pre><code>keys  panda1  panda2  panda3
rows                        
0       eats  shoots    eats
1     shoots  leaves  leaves
</code></pre>
","1","Answer"
"79535494","79535482","<p>df.iloc[:, 0:6] is using <code>.iloc</code> (integer-location based indexing) to select a subset of columns. Here is the details <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html</a></p>
<p>The syntax is:</p>
<p><code>df.iloc[row_selection, column_selection]</code></p>
<p>Explanation:</p>
<ol>
<li><p><code>:</code> (before the comma) → Selects all rows.</p>
</li>
<li><p><code>0:6</code> (after the comma) → Selects columns from index <code>0</code> to <code>5</code> (i.e., the first 6 columns, but not including column index 6).</p>
</li>
</ol>
<p>Understanding <code>pd.get_dummies():</code></p>
<ul>
<li><p><code>pd.get_dummies()</code> converts categorical columns into one-hot encoded variables. Details: <a href=""https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html</a></p>
</li>
<li><p>If a selected column contains categories (e.g., &quot;Male&quot;/&quot;Female&quot; or &quot;Red&quot;/&quot;Blue&quot;/&quot;Green&quot;), <code>pd.get_dummies()</code> creates new binary columns for each category.</p>
</li>
</ul>
<p>All together:</p>
<ol>
<li><p><code>df.iloc[:, 0:6]</code> selects the first 6 columns from <code>df</code>.</p>
</li>
<li><p><code>pd.get_dummies()</code> one-hot encodes categorical columns in this selection.</p>
</li>
<li><p><code>X</code> stores the transformed DataFrame, which is now ready for machine learning.</p>
</li>
</ol>
","1","Answer"
"79537372","79537356","<p><strong>Answer</strong></p>
<pre><code>df['product_max'] = (
    df.groupby('product_id')['sales'].transform('max')
      .eq(df['sales'])
)
</code></pre>
<p>df</p>
<pre><code>  product_id  week_number  sales  product_max
0         A1            1   1000        False
1         A1            2   2000        False
2         A1            3   3000         True
3         A2            1   8000         True
4         A2            2   4000        False
5         A2            3   2000        False
</code></pre>
<hr />
<p><strong>Example Code</strong></p>
<pre><code>import pandas as pd
data = {'product_id': ['A1', 'A1', 'A1', 'A2', 'A2', 'A2'], 'week_number': [1, 2, 3, 1, 2, 3], 'sales': [1000, 2000, 3000, 8000, 4000, 2000]}
df = pd.DataFrame(data)
</code></pre>
","1","Answer"
"79537661","79537565","<p>I believe you mean this am I right</p>
<pre><code>import re

pattern = r&quot;(?P&lt;D&gt;.8)(?P&lt;foo&gt;\d+)(?P&lt;bar&gt;[a-z]+)&quot;

# Regex pattern to capture named groups
group_pattern = re.compile(r&quot;\(\?P&lt;(?P&lt;name&gt;\w+)&gt;(?P&lt;regex&gt;.*?)\)&quot;)

# Extract named groups and their regex patterns
group_patterns = {match.group(&quot;name&quot;): match.group(&quot;regex&quot;) for match in group_pattern.finditer(pattern)}

print(group_patterns)
</code></pre>
","2","Answer"
"79537972","79537934","<p>First, check if <code>item</code> exists in <code>self</code> using <code>super().__contains__(item)</code>.  If it exists, return <code>self[item]</code> directly. Otherwise, fall back to the default <code>__getattribute__</code>.</p>
<p>Here is the solution:</p>
<pre><code>import pandas as pd

class Container(pd.Series):
    def __init__(self, **kwargs):
        object.__setattr__(self, &quot;_custom_attrs&quot;, kwargs)
        super().__init__(kwargs)

    def __getattr__(self, item):
        if item in self._custom_attrs:
            return self._custom_attrs[item]
        if item in self.index:
            return super().__getitem__(item)
        raise AttributeError(f&quot;'{self.__class__.__name__}' object has no attribute '{item}'&quot;)

    def __getattribute__(self, item):
        if item in {&quot;_custom_attrs&quot;, &quot;index&quot;, &quot;dtype&quot;, &quot;_mgr&quot;, &quot;_data&quot;}:
            return object.__getattribute__(self, item)
        
        custom_attrs = object.__getattribute__(self, &quot;_custom_attrs&quot;)
        if item in custom_attrs:
            return custom_attrs[item]

        return pd.Series.__getattribute__(self, item)

container = Container(
    a=12,
    b=[12, 10, 20],
    c='string',
    name='this value'
)

print(container.a)     
print(container.b)     
print(container.c)     
print(container.name)
</code></pre>
","1","Answer"
"79538473","79538323","<p>You need to add the content of the file as attachment</p>
<pre><code>file = 'Report_' + str(datetime.now()) + '.xlsx'
df.toPandas().to_excel(file, index=False, sheet_name='FileReport')
with open(file, 'rb') as f:
    file_data = f.read()

email.add_attachment(file_data, maintype=&quot;application&quot;, subtype=&quot;vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;, filename=file)
</code></pre>
","0","Answer"
"79538474","79538323","<p><strong>Short answer:</strong></p>
<p>to_csv is returning a string which is what you are calling add_attachment() with as parameter.</p>
<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer"">to_csv docs</a></p>
<p>to_excel is not returning anything, it is only used for its side-effects (e.g. writing the in memory content to a file), thus add_attachment() has nothing to attach.</p>
<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html"" rel=""nofollow noreferrer"">to_excel docs</a></p>
<p><strong>Possible Solution</strong></p>
<p>First do the writing of the excel file, then open that file and attach it separately to the email like this:</p>
<pre><code>file = 'Report_' + datetime.now().strftime('%Y%m%d_%H%M%S') + '.xlsx'
df.toPandas().to_excel(file, index=False, sheet_name='FileReport')

with open(file, 'rb') as f:
    email.add_attachment(f.read(), maintype='application', subtype='vnd.openxmlformats-officedocument.spreadsheetml.sheet', filename=file)
</code></pre>
","1","Answer"
"79538579","79538310","<p>XLSX files contain metadata like the creation timestamp, which change with every newly written file. Plaintext CSV files do not contain such variable metadata, and thus their contents are entirely predictable.</p>
","3","Answer"
"79539551","79539480","<p>You could convert your column labels to Strings:</p>
<pre><code>from datetime import time
import pandas as pd
df = pd.DataFrame(index=['ABC','DEF'], data={str(time(9)):[2,4],str(time(10)):[6,8]})
df.to_parquet('MWE.parquet')
df1 = pd.read_parquet('MWE.parquet')
print(df1)
</code></pre>
<p>gives:</p>
<pre><code>     09:00:00  10:00:00
ABC         2         6
DEF         4         8
</code></pre>
","2","Answer"
"79539822","79537565","<p>Check if the regular expression <code>\(\?P&lt;(.+?)&gt;((?:[^)]|(?&lt;=\\)\))+)\)</code> meets your requirements.<br>An explanation of the regular expression can be found at <a href=""https://regex101.com/r/d7MvHp/1"" rel=""nofollow noreferrer"">https://regex101.com/r/d7MvHp/1</a></p>
<pre class=""lang-py prettyprint-override""><code>import re

regex = r&quot;\(\?P&lt;(.+?)&gt;((?:[^)]|(?&lt;=\\)\))+)\)&quot;
test_str = (&quot;(?P&lt;D&gt;.8)(?P&lt;foo&gt;\(\d+\))(?P&lt;bar&gt;[a-z]+)&quot;)
d = {m.group(1) : m.group(2) for m in re.finditer(regex, test_str)}
print(d)
</code></pre>
<p>Output <code>{'D': '.8', 'foo': '\\(\\d+\\)', 'bar': '[a-z]+'}</code></p>
<p>The snippet can be checked at <a href=""https://rextester.com/KTMZ92055"" rel=""nofollow noreferrer"">https://rextester.com/KTMZ92055</a></p>
","1","Answer"
"79540218","79540085","<p>A way to deal with this is to use pandas' <code>cut</code> method to bin the <code>variance</code> columns into user-defined categories:</p>
<pre><code>bins = [-1, -0.25, -.05, 0.05, 0.25, 1]
names = ['-25%+', '-5 to -25%', '-5 to 5%', '5 to 25%', '25%+']
df['MyCategories'] = pd.cut(df['variance'], bins, labels=names)
</code></pre>
<p>You may then create your histogram as follows:</p>
<pre><code>px.histogram(df, 'MyCategories', category_orders=dict(variance = names))
</code></pre>
<p>...which returns:</p>
<p><a href=""https://i.sstatic.net/AJ6esum8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJ6esum8.png"" alt=""enter image description here"" /></a></p>
","2","Answer"
"79540984","79540595","<p>You're probably encountering issues with how Python's <code>json</code> deserializes numbers from JSON responses. Consider the following example:</p>
<pre class=""lang-py prettyprint-override""><code>from httmock import all_requests, response, HTTMock
import pandas as pd
import requests

@all_requests
def response_content(url, request):
    headers = { 'Content-Type': 'application/json' }
    content = '{ &quot;Data&quot;: [ { &quot;CurrentValue&quot;: 0.00001 }, { &quot;CurrentValue&quot;: 0.00002 } ] }'
    return response(200, content, headers, None, 5, request)

with HTTMock(response_content):
    response = requests.get('http://example.org/')
    data = response.json()[&quot;Data&quot;]
    df = pd.DataFrame(data=data, dtype=str)
    print(df)
</code></pre>
<p>Which outputs the DataFrame:</p>
<pre class=""lang-none prettyprint-override""><code>  CurrentValue
0        1e-05
1        2e-05
</code></pre>
<p>To get more control over deserialization of the JSON response you can import the <code>json</code> module and invoke <code>json.loads()</code> directly, e.g.:</p>
<pre class=""lang-py prettyprint-override""><code>from httmock import all_requests, response, HTTMock
import json
import pandas as pd
import requests

@all_requests
def response_content(url, request):
    headers = { 'Content-Type': 'application/json' }
    content = '{ &quot;Data&quot;: [ { &quot;CurrentValue&quot;: 0.00001 }, { &quot;CurrentValue&quot;: 0.00002 } ] }'
    return response(200, content, headers, None, 5, request)

with HTTMock(response_content):
    response = requests.get('http://example.org/')
    data = json.loads(response.text, parse_float=None)[&quot;Data&quot;]
    df = pd.DataFrame(data=data)
    print(df)
</code></pre>
<p>This outputs the DataFrame:</p>
<pre class=""lang-none prettyprint-override""><code>   CurrentValue
0       0.00001
1       0.00002
</code></pre>
","1","Answer"
"79541684","79541633","<p>Do the genes have to be strictly consecutive (i.e. adjacent) ? if not:</p>
<p>You can get the duplicated genes, then for each of them get all the rows that match it, then loop over them to add a suffix</p>
<pre><code>
import pandas as pd

df_genes_data = {&quot;gene_id&quot;: [&quot;g0&quot;, &quot;g1&quot;, &quot;g1&quot;, &quot;g2&quot;, &quot;g3&quot;, &quot;g4&quot;, &quot;g4&quot;, &quot;g4&quot;]}
df_genes = pd.DataFrame.from_dict(df_genes_data)
print(df_genes.to_string())

duplicated_genes = df_genes[df_genes[&quot;gene_id&quot;].duplicated()][&quot;gene_id&quot;]
for gene in duplicated_genes:
    df_gene = df_genes[df_genes[&quot;gene_id&quot;] == gene]
    for i, (idx, row) in enumerate(df_gene.iterrows()):
        df_genes.loc[idx, &quot;gene_id&quot;] = row[&quot;gene_id&quot;] + f&quot;_TE{i+1}&quot;

print(df_genes)
</code></pre>
<p>out:</p>
<pre><code>  gene_id
0      g0
1  g1_TE1
2  g1_TE2
3      g2
4      g3
5  g4_TE1
6  g4_TE2
7  g4_TE3
</code></pre>
<p>if they have to be strictly adjacent then the answer would change</p>
","0","Answer"
"79541714","79541633","<p>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html"" rel=""noreferrer""><code>shift</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.ne.html"" rel=""noreferrer""><code>ne</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.cumsum.html"" rel=""noreferrer""><code>cumsum</code></a> to group the consecutive values, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.SeriesGroupBy.transform.html"" rel=""noreferrer""><code>groupby.transform('size')</code></a> to identify the groups of more than 2 values, and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.SeriesGroupBy.cumcount.html"" rel=""noreferrer""><code>groupby.cumcount</code></a> to increment the name:</p>
<pre><code># Series as name for shorter reference
s = df_genes['gene_id']
# group consecutive occurrences
group = s.ne(s.shift()).cumsum()
# form group and save as &quot;g&quot; for efficiency
g = s.groupby(group)
# identify groups with more than 1 value
m = g.transform('size').gt(1)
# increment values
df_genes.loc[m, 'gene_id'] += '_TE'+g.cumcount().add(1).astype(str)
</code></pre>
<p>Output:</p>
<pre><code>  gene_id
0      g0
1  g1_TE1
2  g1_TE2
3      g2
4      g3
5  g4_TE1
6  g4_TE2
7  g4_TE3
</code></pre>
<p>Intermediates:</p>
<pre><code>  gene_id  group      m  cumcount+1 suffix
0      g0      1  False           1       
1      g1      2   True           1   _TE1
2      g1      2   True           2   _TE2
3      g2      3  False           1       
4      g3      4  False           1       
5      g4      5   True           1   _TE1
6      g4      5   True           2   _TE2
7      g4      5   True           3   _TE3
</code></pre>
","6","Answer"
"79541782","79541633","<p>Another possible solution:</p>
<pre><code>df_genes['gene_id'] = np.where(
    # logical condition that detects whether gene_id needs edition
    (m := df_genes['gene_id'].eq(df_genes['gene_id'].shift())) | 
    df_genes['gene_id'].eq(df_genes['gene_id'].shift(-1)), 

    # if gene_id needs edition
    df_genes['gene_id'] + '_TE' + 
    (m.cumsum() - m.cumsum().where(~m).ffill().fillna(0).astype(int) + 1)
    .astype(str), # (m.cumsum() - ...) generates 1, 2, ... sequence

    # otherwise
    df_genes['gene_id'])
</code></pre>
<p>The solution uses <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.eq.html"" rel=""nofollow noreferrer""><code>eq</code></a> to identify consecutive duplicates via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html"" rel=""nofollow noreferrer""><code>shift</code></a>, creates a mask <code>m</code> for tracking duplicates, then calculates positional indices using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>cumsum</code></a> and resets counters at group boundaries via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.where.html"" rel=""nofollow noreferrer""><code>where</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.ffill.html"" rel=""nofollow noreferrer""><code>ffill</code></a>. The positional integers are cast to strings with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.astype.html"" rel=""nofollow noreferrer""><code>astype</code></a>, and <a href=""https://numpy.org/doc/stable/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>np.where</code></a> conditionally appends <code>_TE</code> suffixes only to consecutive duplicates.</p>
<p>Output:</p>
<pre><code>  gene_id
0      g0
1  g1_TE1
2  g1_TE2
3      g2
4      g3
5  g4_TE1
6  g4_TE2
7  g4_TE3
</code></pre>
","2","Answer"
"79542540","79542528","<p>The leading spaces in your input seem to be throwing Pandas off (and some other CSV processors I can think of).</p>
<p>Try the skipinitialspace=True option to make Pandas ignore every space between a comma and a quote char:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.read_csv(&quot;input.csv&quot;, skipinitialspace=True)
print(df)
</code></pre>
<pre class=""lang-none prettyprint-override""><code>   Version       Date  ... Tracking Number          Author
0      0.1  22AUG2022  ...              NR  Sarah Marshall
1      0.2  23SEP2022  ...          N-1234     Bill Walter
2      0.3  09MAY2023  ...          N-1235   George Orwell

[3 rows x 6 columns]
</code></pre>
<p>Expounding a bit, you can see the difference, where quotes are present, between the default of not skipping the initial space and skipping:</p>
<pre class=""lang-py prettyprint-override""><code>from io import StringIO

data = &quot;&quot;&quot;
Col1
&quot;foo&quot;
 &quot;foo&quot;
&quot; foo&quot;
&quot;&quot;&quot;.strip()

print(&quot;no skip&quot;)

df = pd.read_csv(StringIO(data))
for _, x in df.iterrows():
    print(repr(x[&quot;Col1&quot;]))

print()
print(&quot;skip initial&quot;)

df = pd.read_csv(StringIO(data), skipinitialspace=True)
for _, x in df.iterrows():
    print(repr(x[&quot;Col1&quot;]))
</code></pre>
<pre class=""lang-none prettyprint-override""><code>no skip
'foo'
' &quot;foo&quot;'
' foo'

skip initial
'foo'
'foo'
' foo'
</code></pre>
<ul>
<li><code>&quot;foo&quot;</code> doesn't matter as it doesn't have any space</li>
<li><code> &quot;foo&quot;</code> is up for interpretation as the space precedes the (default) quote char</li>
<li><code>&quot; foo&quot;</code> also doesn't matter as the space is encoded inside the (default) quote char</li>
</ul>
","5","Answer"
"79542607","79541633","<p>An option (&quot;run-length-encoding&quot;-based approach) is probably using <a href=""https://docs.python.org/3/library/itertools.html#itertools.groupby"" rel=""nofollow noreferrer""><code>itertools.groupby</code></a> and <a href=""https://docs.python.org/3/library/itertools.html#itertools.chain.from_iterable"" rel=""nofollow noreferrer""><code>itertools.chain</code></a></p>
<pre><code>from itertools import groupby, chain

x = df_genes['gene_id']
df_genes['gene_id'] = x + np.array(list(chain.from_iterable([''] if (l:=len(list(g)))==1 else &quot;_TE&quot; + np.arange(1,l+1).astype(str) for _, g in groupby(x))))
</code></pre>
<p>and finally you will obtain</p>
<pre><code>  gene_id
0      g0
1  g1_TE1
2  g1_TE2
3      g2
4      g3
5  g4_TE1
6  g4_TE2
7  g4_TE3
</code></pre>
","2","Answer"
"79542852","79531444","<p>If you don't use <code>on</code> parameter for <code>pd.merge</code>, I can explain like below.</p>
<p><code>both</code> is generated only when if nothing has changed in every column.
And when if any of the columns has changed,
<code>left_only</code> is generated  in the row after the change,
<code>right_only</code> is generated in the row before the change.</p>
<p>Here is an answer to compare these dataframes.</p>
<p>First, you need to set criteria columns, like Primary Key.</p>
<p>Let's say the criteria columns are ['Ctry', 'Cat'].</p>
<pre><code># Changing the columns' name of base_df
base_df.columns = [i + '_base' if i not in ['Ctry', 'Cat'] else i for i in base_df.columns]

# Merging on criteria columns
merged_df = pd.merge(current_df, base_df, how=&quot;outer&quot;, on=['Ctry', 'Cat'], indicator=True)

# Detecting the part that changes
merged_df['is_changed'] =\
       np.where(merged_df['_merge']=='left_only', 'new_item', 
         np.where(merged_df['Rank']!=merged_df['Rank_base'], 'Rank_changed', 
           np.where(merged_df['Issr']!=merged_df['Issr_base'], 'Issr_changed', 
            np.where(merged_df['Ref']!=merged_df['Ref_base'], 'Ref_changed', 'No_changed'))))

final_df = merged_df[current_df.columns.tolist() + ['is_changed']]
</code></pre>
<p>Then, you can get below.</p>
<pre><code>&gt;&gt;&gt; final_df

+----+--------+--------+--------+--------+-------+--------------+
|    |   Rank | Ctry   | Cat    | Issr   |   Ref | is_changed   |
|----+--------+--------+--------+--------+-------+--------------|
|  0 |     10 | A      | Book   | Y      |   100 | No_changed   |
|  1 |     14 | B      | Laptop |        |   101 | Issr_changed |
|  2 |     15 | C      | Pen    | J      |   102 | No_changed   |
|  3 |     50 | D      | Pen    | U      |   103 | Issr_changed |
|  4 |     24 | K      | Pencil | W      |   101 | new_item     |
|  5 |     24 | RT     | Pencil |        |   201 | new_item     |
+----+--------+--------+--------+--------+-------+--------------+
</code></pre>
","0","Answer"
"79543034","79543011","<p>Pandas automatically unpacks single-element lists due to type inference. This issue is influenced by column &quot;C&quot; (integer dtype), which alters dtype behavior.</p>
<p>Force <code>dtype=object</code> on column &quot;A&quot;:</p>
<pre><code>df[&quot;A&quot;] = df[&quot;A&quot;].astype(object)
</code></pre>
<p>This prevents implicit unpacking of nested lists.</p>
<p>Use <code>.at[...]</code> instead of <code>.loc[...]</code> for single assignments:</p>
<pre><code>df.at['y', 'A'] = [[2]]
</code></pre>
<p><code>.at[...]</code> avoids Pandas' internal dtype inference.</p>
<p>Remove column &quot;C&quot; if not needed. Its presence influences automatic type conversion.</p>
","2","Answer"
"79543539","79542528","<blockquote>
<p>I can of course loop through the CSV rows and strip out the leading/trailing quotes for each of the values, but since quoted</p>
</blockquote>
<p>You can get desired result without writing loop, as <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_html.html"" rel=""nofollow noreferrer""><code>pandas.DataFrame.to_html</code></a> allows you to specify <em>formatter</em> for each column. Consider following simple example</p>
<pre><code>import pandas as pd

def strip_double_quotes(obj):
    return f'{obj}'.strip('&quot;')

df = pd.DataFrame({'Version':['&quot;0.1&quot;','&quot;0.2&quot;','&quot;0.3&quot;'],'Code':['&quot;A&quot;','&quot;C&quot;','&quot;AC&quot;']})
html = df.to_html(formatters={'Version': strip_double_quotes, 'Code': strip_double_quotes})
print(html)
</code></pre>
<p>gives output</p>
<pre><code>&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Version&lt;/th&gt;
      &lt;th&gt;Code&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;AC&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</code></pre>
<p>Explanation: I provide formatters as dict, where keys are column names and values are functions. Keep in mind that each function you provide must be able to accept 1 argument and return unicode string.</p>
","0","Answer"
"79543710","79542528","<p>The last row has an escaped character, the comma in <code>&quot;A\, C&quot;</code>.</p>
<p><code>&quot;0.3&quot;, &quot;09MAY2023&quot;, &quot;A\, C&quot;, &quot;New update.&quot;, &quot;N-1235&quot;, &quot;George Orwell&quot;</code></p>
<p>Therefore, we need to provide a parameter to <code>read_csv()</code> that tells the function to treat the comma as an escaped character instead of a delimiter.</p>
<p>To do this, we will change the function's default behavior which is set to <code>quoting=csv.QUOTE_MINIMAL</code> which implies that only fields containing special characters are quoted. we need to change this to <code>quoting=csv.QUOTE_NONE</code> which will escape delimiters instead of enclosing them in quotation marks and use the argument specified in <code>escapechar</code> to know which character should be escaped.</p>
<ol>
<li>import pandas and csv modules.</li>
<li>use the <code>quoting=csv.QUOTE_NONE</code> and <code>escapechar='\\'</code> parameters inside <code>pd.read_csv()</code> function.</li>
<li>change column names to remove the double quotes.</li>
<li>iterate through the columns and use the string <code>replace()</code> method to remove the double quotes from the data.</li>
</ol>
<pre><code>import pandas as pd
import csv

df = pd.read_csv(&quot;myfile.csv&quot;, quoting=csv.QUOTE_NONE, escapechar='\\')

df.columns = ['Version', 'Date', 'Code', 'Description', 'Tracking Number', 'Author']

for col in df.columns:
    # Check if the column is of object type (string)
    if df[col].dtype == 'object':
        df[col] = df[col].str.replace('&quot;', '', regex=False)
</code></pre>
","0","Answer"
"79544117","79541633","<pre><code>(
    df_genes.assign(
        result=lambda d: d.groupby(
            df_genes.gene_id.ne(df_genes.gene_id.shift()).fillna(True).cumsum()
        ).gene_id.transform(
            lambda x: (
                x if len(x) == 1 else x.notna().cumsum().astype(str).radd(&quot;_TE&quot;).radd(x)
            )
        )
    )
)
</code></pre>
","0","Answer"
"79544162","79543945","<p>To see what is happening, let's add <code>asfreq</code> after the resample and you can see what is passed in to the next chained function:</p>
<pre><code>interval8df.resample('10s').asfreq()
</code></pre>
<p>Output:</p>
<pre><code>Hi
2025-12-02 17:39:00    NaN
2025-12-02 17:39:10    NaN
2025-12-02 17:39:20    NaN
2025-12-02 17:39:30   84.0
2025-12-02 17:39:40    NaN
2025-12-02 17:39:50    NaN
2025-12-02 17:40:00    NaN
2025-12-02 17:40:10  124.0
</code></pre>
<p>And, since you doing interpolation, the lower bound is not seen hence the nulls for seconds 00, 10, 20.  While doing <code>mean</code> with out interpolating you, are just doing a window of 10s means of values.  Since you have values within each 10s interval you are getting that mean values returned.</p>
","0","Answer"
"79544248","79544212","<p>Just filter by <code>Tag</code> column and <code>cumsum()</code>:</p>
<pre><code>df[&quot;RBQ&quot;]=df[df[&quot;Tag&quot;]!=2][&quot;Qty&quot;].cumsum()
df[&quot;RSQ&quot;]=df[df[&quot;Tag&quot;]==2][&quot;Qty&quot;].cumsum()
df1=df.ffill().fillna(0)
df1.to_markdown()
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">DateTime</th>
<th style=""text-align: right;"">Tag</th>
<th style=""text-align: right;"">Qty</th>
<th style=""text-align: right;"">RBQ</th>
<th style=""text-align: right;"">RSQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">2025-01-01 13:00</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">270</td>
<td style=""text-align: right;"">270</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">2025-01-03 13:22</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">32</td>
<td style=""text-align: right;"">302</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: left;"">2025-01-10 12:33</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">44</td>
<td style=""text-align: right;"">302</td>
<td style=""text-align: right;"">44</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: left;"">2025-01-22 10:04</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">120</td>
<td style=""text-align: right;"">302</td>
<td style=""text-align: right;"">164</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">2025-01-29 09:30</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">182</td>
<td style=""text-align: right;"">484</td>
<td style=""text-align: right;"">164</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td style=""text-align: left;"">2025-02-02 15:05</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">216</td>
<td style=""text-align: right;"">700</td>
<td style=""text-align: right;"">164</td>
</tr>
</tbody>
</table></div>
","2","Answer"
"79544372","79544212","<p>You can use a condition and the methods mask and where to create both columns</p>
<pre><code>cond = df['Tag'].eq(2)
df['RBQ'] = df['Qty'].mask(cond, 0).cumsum()
df['RSQ'] = df['Qty'].where(cond, 0).cumsum()
</code></pre>
<p>Another solution is to use the same condition and pivot the dataframe based on that.</p>
<pre><code>df2 = (df.join(df.assign(cols=df['Tag'].eq(2).map({True: 'RSQ', False: 'RBQ'}))
               .pivot(columns='cols', values='Qty')
               .fillna(0).cumsum())
      )
</code></pre>
<p>End result:</p>
<pre><code>        DateTime  Tag  Qty   RBQ   RSQ
2025-01-01 13:00    1  270 270.0   0.0
2025-01-03 13:22    1   32 302.0   0.0
2025-01-10 12:33    2   44 302.0  44.0
2025-01-22 10:04    2  120 302.0 164.0
2025-01-29 09:30    3  182 484.0 164.0
2025-02-02 15:05    1  216 700.0 164.0
</code></pre>
<p>Edit: Here is a slightly modified version that takes in consideration the DateTime column. I modified the first value in the datetime column as an example.</p>
<pre><code>cond = df['Tag'].eq(2)
tmp = df.sort_values(by='DateTime')

df['RBQ'] = tmp['Qty'].mask(cond, 0).cumsum()
df['RSQ'] = tmp['Qty'].where(cond, 0).cumsum()
</code></pre>
<p>For the second solution, you will have to use merge instead of join.</p>
<pre><code>df2 = pd.merge(df, (df.assign(cols=df['Tag'].eq(2).map({True: 'RSQ', False: 'RBQ'}))
                   .pivot(index='DateTime', columns='cols', values='Qty')
                   .fillna(0).cumsum()), on='DateTime')
</code></pre>
<p>End result:</p>
<pre><code>           DateTime  Tag  Qty   RBQ   RSQ
2025-02-04 13:00:00    1  270 700.0 164.0
2025-01-03 13:22:00    1   32  32.0   0.0
2025-01-10 12:33:00    2   44  32.0  44.0
2025-01-22 10:04:00    2  120  32.0 164.0
2025-01-29 09:30:00    3  182 214.0 164.0
2025-02-02 15:05:00    1  216 430.0 164.0
</code></pre>
","2","Answer"
"79545323","79545304","<pre><code>import pandas as pd
import re
from collections import defaultdict
def fix_typos(df, columns_to_clean):
    df_cleaned = df.copy()
    for col in columns_to_clean:
        if col not in df.columns:
            continue
        df_cleaned[col] = df_cleaned[col].astype(str)
        string_groups = defaultdict(list)
        for value in df_cleaned[col].unique():
            base = value[:-3] if len(value) &gt; 3 else value
            string_groups[base].append(value)
        corrections = {}
        for base, variants in string_groups.items():
            if len(variants) &gt; 1:
                variant_counts = {variant: df_cleaned[col].eq(variant).sum() for variant in variants}
                most_common = max(variant_counts.items(), key=lambda x: x[1])[0]
                has_suffix = False
                for variant in variants:
                    if variant != most_common and len(variant) == len(most_common) + 3:
                        has_suffix = True
                if has_suffix or len(variants) &gt; 2:
                    for variant in variants:
                        other_variants = [v for v in variants if v != variant]
                        is_clean = all(
                            (len(v) == len(variant) + 3 and v.startswith(variant)) 
                            for v in other_variants
                        )
                        
                        if is_clean and variant_counts[variant] &gt; 1:
                            for v in other_variants:
                                corrections[v] = variant
                        elif any(variant.startswith(v) and len(variant) == len(v) + 3 for v in variants):
                            for v in variants:
                                if variant.startswith(v) and len(variant) == len(v) + 3:
                                    corrections[variant] = v
                                    break
        df_cleaned[col] = df_cleaned[col].replace(corrections)
    return df_cleaned
df = pd.read_csv(&quot;purchases.csv&quot;)  
columns_to_clean = [&quot;City&quot;, &quot;Name&quot;, &quot;Store&quot;]
cleaned_df = fix_typos(df, columns_to_clean)
print(cleaned_df)
</code></pre>
<p>I didn't have your data and checked against random ones to verify code completion on your assignment</p>
<p>I hope my corrected code works for you.</p>
","-1","Answer"
"79545392","79545386","<p>IIUC,</p>
<pre><code>df3d.groupby(['X','Y'], as_index=False)['Z'].agg(['count','min','max']).rename_axis('d2dindex').reset_index().merge(df3d)
</code></pre>
<p>Output:</p>
<pre><code>        d2dindex          X          Y  count  min   max     Z
0              0 -51.995857 -49.653017     11  0.0  10.0   0.0
1              0 -51.995857 -49.653017     11  0.0  10.0   1.0
2              0 -51.995857 -49.653017     11  0.0  10.0   2.0
3              0 -51.995857 -49.653017     11  0.0  10.0   3.0
4              0 -51.995857 -49.653017     11  0.0  10.0   4.0
...          ...        ...        ...    ...  ...   ...   ...
112206     10200  52.902015  -6.111877     11  0.0  10.0   6.0
112207     10200  52.902015  -6.111877     11  0.0  10.0   7.0
112208     10200  52.902015  -6.111877     11  0.0  10.0   8.0
112209     10200  52.902015  -6.111877     11  0.0  10.0   9.0
112210     10200  52.902015  -6.111877     11  0.0  10.0  10.0

[112211 rows x 7 columns]
</code></pre>
","1","Answer"
"79545512","79545484","<h3><strong>Solution 1: Convert to an R data.frame</strong></h3>
<p>The simplest solution could be to explicitly convert the pandas DataFrame to an R data.frame using <code>py_to_r()</code>. This way, R will display it using its native formatting:</p>
<pre><code>library(reticulate)
pd &lt;- import(&quot;pandas&quot;)
df &lt;- pd$DataFrame(data = list(a = 1:5, b = letters[1:5]))
r_df &lt;- py_to_r(df)
print(r_df)
</code></pre>
<p>This converts the pandas DataFrame into an R data.frame, which will be displayed in the standard R format.</p>
<h3><strong>Solution 2: Render the HTML representation</strong></h3>
<p>If you prefer to maintain the HTML formatting provided by pandas (for instance, in an R Markdown document or in RStudio’s Viewer), you can use the DataFrame’s <code>to_html()</code> method and then render the HTML:</p>
<pre><code>library(reticulate)
library(htmltools)
pd &lt;- import(&quot;pandas&quot;)
df &lt;- pd$DataFrame(data = list(a = 1:5, b = letters[1:5]))
HTML(df$to_html())
</code></pre>
<p>This will display the table with enhanced formatting in environments that support HTML.</p>
","0","Answer"
"79545771","79545762","<p>It's because you're setting the <code>inclusive</code> argument to <code>left</code>. Set it to <code>both</code> or don't set it as <code>both</code> is the default.</p>
<pre><code>pd.date_range(&quot;2016-09-01&quot;, &quot;2006-03-01&quot;, freq=&quot;-6MS&quot;)
</code></pre>
<pre><code>DatetimeIndex(['2016-09-01', '2016-03-01', '2015-09-01', '2015-03-01',
               '2014-09-01', '2014-03-01', '2013-09-01', '2013-03-01',
               '2012-09-01', '2012-03-01', '2011-09-01', '2011-03-01',
               '2010-09-01', '2010-03-01', '2009-09-01', '2009-03-01',
               '2008-09-01', '2008-03-01', '2007-09-01', '2007-03-01',
               '2006-09-01', '2006-03-01'],
              dtype='datetime64[ns]', freq='-6MS')
</code></pre>
","1","Answer"
"79545806","79545762","<p>In this line</p>
<p><code>pd.date_range(&quot;2016-09-01&quot;, &quot;2006-03-01&quot;, freq=&quot;-6MS&quot;, inclusive=&quot;left&quot;) date 2006-03-01</code></p>
<p>the date <code>2006-03-01</code> is excluded because <code>inclusive='left'</code> omits the end date if it falls on the boundary, as stated on <a href=""https://pandas.pydata.org/docs/reference/api/pandas.date_range.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.date_range.html</a>.</p>
<p>When the previous date is 2006-09-01, the ending date, after adding a 6-month interval, is 2006-03-01. It lands exactly on that date &quot;2006-03-01&quot;.</p>
<p>However, dates within the same landing month are included as long as the end date is later than the generated date. Specifically, <code>2006-03-01</code> will be included if the end date falls between <code>2006-03-02</code> and <code>2006-03-31</code>.</p>
<p>This code will inlcude 2006-03-01 :</p>
<pre><code>import pandas as pd
dates = pd.date_range(&quot;2016-09-01&quot;, &quot;2006-03-31&quot;, freq=&quot;-6MS&quot;, inclusive=&quot;left&quot;)
print(dates)
</code></pre>
<p>However, if the ending date is in the fourth month, the date 2006-03-01 will not be included:</p>
<pre><code>import pandas as pd
dates = pd.date_range(&quot;2016-09-01&quot;, &quot;2006-04-01&quot;, freq=&quot;-6MS&quot;, inclusive=&quot;left&quot;)
print(dates)
</code></pre>
<p>To explicitly include <code>2006-03-01</code>, you may use <code>inclusive=&quot;both&quot;</code>, which ensures that both the start and end dates are included. Alternatively, omitting the <code>inclusive</code> parameter defaults to <code>&quot;both&quot;</code>, achieving the same result.</p>
","2","Answer"
"79546176","79519830","<pre><code>import pandas as pd
import torch


df = pd.DataFrame({
    'a': [0, 0, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0]
})

# Convert the 'a' column to a PyTorch integer tensor
atensor = torch.tensor(df['a'].values, dtype=torch.int32)
a_len = len(atensor)  # Store the length of the tensor for later use

# Create a column vector of indices from 0 to len(atensor) - 1
row_indices = torch.arange(len(atensor)).unsqueeze(1)
'''
row_indices :
tensor([[ 0],
        [ 1],
        [ 2],
        [ 3],
        [ 4],
        [ 5],
        [ 6],
        [ 7],
        [ 8],
        [ 9],
        [10],
        [11],
        [12],
        [13],
        [14]])
'''

# Create a row vector of offsets [1, 2, 3]
backward_steps = torch.arange(1, 4).unsqueeze(0)
'''
tensor([[1, 2, 3]])
'''

# Calculate indices for rolling windows of size 3
# Each row represents the indices of elements 1, 2, and 3 positions before the current element
idx = row_indices - backward_steps
'''
tensor([[-1, -2, -3],
        [ 0, -1, -2],
        [ 1,  0, -1],
        [ 2,  1,  0],
        [ 3,  2,  1],
        [ 4,  3,  2],
        [ 5,  4,  3],
        [ 6,  5,  4],
        [ 7,  6,  5],
        [ 8,  7,  6],
        [ 9,  8,  7],
        [10,  9,  8],
        [11, 10,  9],
        [12, 11, 10],
        [13, 12, 11]])
'''

# Clamp the indices to be non-negative (replace negative indices with 0)
idxClamp = torch.clamp(idx, 0)
'''
tensor([[ 0,  0,  0],
        [ 0,  0,  0],
        [ 1,  0,  0],
        [ 2,  1,  0],
        [ 3,  2,  1],
        [ 4,  3,  2],
        [ 5,  4,  3],
        [ 6,  5,  4],
        [ 7,  6,  5],
        [ 8,  7,  6],
        [ 9,  8,  7],
        [10,  9,  8],
        [11, 10,  9],
        [12, 11, 10],
        [13, 12, 11]])
'''

# Gather values from 'atensor' using the clamped indices to create rolling windows
win = atensor[idxClamp]
'''
tensor([[ 0,  0,  0],
        [ 0,  0,  0],
        [ 0,  0,  0],
        [ 1,  0,  0],
        [-1,  1,  0],
        [-1, -1,  1],
        [ 0, -1, -1],
        [ 0,  0, -1],
        [ 0,  0,  0],
        [ 0,  0,  0],
        [ 0,  0,  0],
        [-1,  0,  0],
        [ 0, -1,  0],
        [ 0,  0, -1],
        [ 1,  0,  0]], dtype=torch.int32)
'''

# Create a tensor where nonzero elements in 'win' are replaced with their 
# reverse indices [3, 2, 1] and zero elements are replaced with -infinity

validIdx = torch.where(win != 0, torch.arange(3, 0, -1), float('-inf'))

# Find the index of the maximum value in each row of 'validIdx'
# This gives the index of the last nonzero element in each rolling window
last_nonzero_idx = validIdx.argmax(dim=1)

# Gather the last nonzero values from 'win' using the 'last_nonzero_idx'
last_nonzero_values = win[torch.arange(len(atensor)), last_nonzero_idx]
'''
tensor([ 0,  0,  0,  1, -1, -1, -1, -1,  0,  0,  0, -1, -1, -1,  1],
       dtype=torch.int32)
'''

df['res'] = last_nonzero_values
print(df)

'''
   a  res
0   0    0
1   0    0
2   1    0
3  -1    1
4  -1   -1
5   0   -1
6   0   -1
7   0   -1
8   0    0
9   0    0
10 -1    0
11  0   -1
12  0   -1
13  1   -1
14  0    1
'''
</code></pre>
","0","Answer"
"79546261","79546240","<p>If you want to shuffle the rows within a group, use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.sample.html"" rel=""nofollow noreferrer""><code>groupby.sample</code></a>:</p>
<pre><code>df.groupby(['UserID', 'rank_group']).sample(frac=1)
</code></pre>
<p>Example output:</p>
<pre><code>    UserID  Col2  Col3  rank_group
0        1     2     3           1
1        1     5     6           1
21       1    11    12           2
20       1     8     9           2
45       1    14    15           3
46       1    17    18           3
48       2     5     6           1
47       2     2     3           1
60       2     8     9           2
61       2    11    12           2
71       2    17    18           3
70       2    14    15           3
</code></pre>
<p>If you want to shuffle the groups keeping the relative order within a group constant, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html"" rel=""nofollow noreferrer""><code>sample</code></a> the unique groups, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>:</p>
<pre><code>(df[['UserID', 'rank_group']].drop_duplicates().sample(frac=1)
 .merge(df, how='left')
)
</code></pre>
<p>Example output:</p>
<pre><code>    UserID  rank_group  Col2  Col3
0        2           1     2     3
1        2           1     5     6
2        1           3    14    15
3        1           3    17    18
4        1           2     8     9
5        1           2    11    12
6        2           2     8     9
7        2           2    11    12
8        2           3    14    15
9        2           3    17    18
10       1           1     2     3
11       1           1     5     6
</code></pre>
<p>And, if the index is important:</p>
<pre><code>(df[['UserID', 'rank_group']].drop_duplicates().sample(frac=1)
 .merge(df.reset_index(), how='left')
 .set_index('index').rename_axis(df.index.name)
)
</code></pre>
<p>Example output:</p>
<pre><code>    UserID  rank_group  Col2  Col3
70       2           3    14    15
71       2           3    17    18
0        1           1     2     3
1        1           1     5     6
20       1           2     8     9
21       1           2    11    12
60       2           2     8     9
61       2           2    11    12
45       1           3    14    15
46       1           3    17    18
47       2           1     2     3
48       2           1     5     6
</code></pre>
","2","Answer"
"79546733","79545484","<p>I needed to update R, not just RStudio!</p>
","0","Answer"
"79547122","79546920","<p>You can use <strong>openpyxl</strong> to achieve this.</p>
<pre><code>import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.cell.text import InlineFont 
from openpyxl.cell.rich_text import TextBlock, CellRichText

data = {
    'Gene_name': ['sdsR', 'arrS', 'gadF'],
    'Genes_in_same_transcription_unit': ['pphA, sdsR', 'arrS', 'mdtF, mdtE, gadF, gadE']
}
df = pd.DataFrame(data)

# Create a new Excel workbook and select the active worksheet
wb = Workbook()
ws = wb.active

# Define InlineFont colors
red = InlineFont(color='00FF0000')
black = InlineFont(color='00000000')

# Write the DataFrame to the worksheet
for r in dataframe_to_rows(df, index=False, header=True):
    ws.append(r)

unique_values = set(df['Gene_name'])

# Iterate based on column/header locations
for row in ws.iter_rows(min_row=2, min_col=2, max_col=2):
    for cell in row:
        # Individual strings stripped for whitespace
        parts = [s.strip() for s in cell.value.split(',')]
        rich_text_cell = CellRichText()

        for i, part in enumerate(parts):
            font = black if part in unique_values else red
            # Add separator unless last instance
            separator = &quot;, &quot; if i &lt; len(parts) - 1 else &quot;&quot;
            # Append values to cell
            rich_text_cell.append(TextBlock(font, part + separator))

        cell.value = rich_text_cell

wb.save(&quot;highlighted.xlsx&quot;)
</code></pre>
","1","Answer"
"79548218","79547724","<p>There are many ways of doing this depending on the data you have.. however by far the simplest is to coerce your data into a time series index if it's not already and use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html"" rel=""nofollow noreferrer""><code>.resample(&quot;1s&quot;).mean()</code></a></p>
<p>generate some sample data</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; count = 3600
&gt;&gt;&gt; df = pd.DataFrame({f&quot;freq {i}MHz&quot;: np.random.randn(count)*100 for i in range(1,201)}, index=pd.date_range(&quot;2025-04-01&quot;, periods=count, freq=&quot;250ms&quot;)).T
&gt;&gt;&gt; df
             2025-04-01 00:00:00.000  2025-04-01 00:00:00.250  2025-04-01 00:00:00.500  ...  2025-04-01 00:14:59.250  2025-04-01 00:14:59.500  2025-04-01 00:14:59.750
freq 1MHz                  56.277885               -76.715028               -19.436522  ...                10.744845                70.650393                21.947893
freq 2MHz                 -12.898406                 1.598879                93.373505  ...               -40.860931               -60.643461                -2.759319
freq 3MHz                 -65.795367               -86.950974                -2.421849  ...              -103.419081                49.461729                82.793567
freq 4MHz                  83.350473               191.231577              -161.003622  ...              -132.219173               131.218199                25.113404
freq 5MHz                 142.520352               125.725121                -3.235449  ...               -10.971120               108.124412              -106.275392
...                              ...                      ...                      ...  ...                      ...                      ...                      ...
freq 196MHz                77.526046                24.296306               -32.023278  ...                32.674445                30.273234               -84.255384
freq 197MHz                69.552259               199.034193              -150.456317  ...               105.675402               -32.833817               -41.417296
freq 198MHz              -129.568614                43.051751               -10.824167  ...               -11.545223               135.946161                 9.608785
freq 199MHz               151.030454                -5.387144                71.144257  ...              -101.057261                68.122765              -130.901913
freq 200MHz                25.296514                80.701457                32.373565  ...               146.546715              -155.170539               -78.732363

[200 rows x 3600 columns]
</code></pre>
<p>now just resample!</p>
<p>note <code>.resample()</code> expects the data to be in columns (<code>axis=1</code> is deprecated &quot;<em>Deprecated since version 2.0.0: Use <code>frame.T.resample(…)</code> instead.</em>&quot;), so you can transpose with <code>.T</code> and then back again after the resample</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df.T.resample(&quot;1s&quot;).mean().T
             2025-04-01 00:00:00  2025-04-01 00:00:01  2025-04-01 00:00:02  2025-04-01 00:00:03  ...  2025-04-01 00:14:56  2025-04-01 00:14:57  2025-04-01 00:14:58  2025-04-01 00:14:59
freq 1MHz              15.117181           -31.716795            23.756633           -27.578701  ...           -19.287829            -6.923784             2.077375            32.955678
freq 2MHz              16.831645            56.396253            26.996447             0.327802  ...            63.255776            68.122855            29.455577            -7.643232
freq 3MHz             -55.707993            28.662803           -59.010300             6.659348  ...           -71.702346            47.037690           -27.076411            61.648608
freq 4MHz              11.924516           -79.776633           114.552432            62.607081  ...            82.446966             1.601780            30.801114             2.600828
freq 5MHz              61.761102             0.689386            35.821693           -13.006058  ...           -87.572613           -18.939267           -45.864923           -14.326446
...                          ...                  ...                  ...                  ...  ...                  ...                  ...                  ...                  ...
freq 196MHz            18.919907           -62.561788            15.818990           -26.336276  ...            11.755455           -42.537646           -79.619288           -28.461854
freq 197MHz            10.340424           -40.260711           -15.458829           -19.250989  ...            27.373582           -15.024511            -5.521782           -14.690702
freq 198MHz           -38.281829            44.141023            -3.625441           -16.368935  ...            36.209582            26.942521           -61.729879            10.357420
freq 199MHz            61.499773           -23.392824           -21.751043           -18.025937  ...            20.019479             9.421139           -11.937625           -14.725595
freq 200MHz            46.654067            16.341845            43.656015            45.139867  ...           -18.569374            31.027033             7.913154           -31.834015

[200 rows x 900 columns]
</code></pre>
<p>Closely inspecting a smaller section (8 values from the first 2 rows), you can see it working on a smaller scale</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df_mini = df.iloc[0:2,0:8].T
&gt;&gt;&gt; df_mini
                          freq 1MHz  freq 2MHz
2025-04-01 00:00:00.000   56.277885 -12.898406
2025-04-01 00:00:00.250  -76.715028   1.598879
2025-04-01 00:00:00.500  -19.436522  93.373505
2025-04-01 00:00:00.750  100.342389 -14.747396
2025-04-01 00:00:01.000   24.979359  70.526222
2025-04-01 00:00:01.250   15.195554  98.314063
2025-04-01 00:00:01.500 -209.241802  36.403973
2025-04-01 00:00:01.750   42.199710  20.340755
&gt;&gt;&gt; df_mini.resample(&quot;1s&quot;).mean()
                     freq 1MHz  freq 2MHz
2025-04-01 00:00:00  15.117181  16.831645
2025-04-01 00:00:01 -31.716795  56.396253
</code></pre>
","0","Answer"
"79548933","79529354","<p>You can use <code>data-fingerprint</code> tool: <a href=""https://pypi.org/project/data-fingerprint/#description"" rel=""nofollow noreferrer"">https://pypi.org/project/data-fingerprint/#description</a> to check differencess between those two dataframes, and than based on the difference insert the ones that are missing, here is an example:</p>
<pre><code>import pandas as pd
import polars as pl

from data_fingerprint.src.comparator import get_data_report
from data_fingerprint.src.models import DataReport
from data_fingerprint.src.utils import get_dataframe

if __name__ == &quot;__main__&quot;:
    sap_scrapped: pl.DataFrame = pl.DataFrame(
        {&quot;Material&quot;: [&quot;08-008099-0&quot;, &quot;10-000001-00&quot;], &quot;Scrapped&quot;: [2, 5]}
    )
    sap_warehouse: pl.DataFrame = pl.DataFrame(
        {
            &quot;Material&quot;: [&quot;10-000001-00&quot;, &quot;10-789001-00&quot;],
            &quot;SSP&quot;: [0, 10],
            &quot;Scrapped&quot;: [0, 7],
        }
    )
    
    # get difference information
    report: DataReport = get_data_report(
        sap_scrapped,
        sap_warehouse.drop(&quot;Scrapped&quot;),
        &quot;sap_scrapped&quot;,
        &quot;sap_warehouse&quot;,
        grouping_columns=[&quot;Material&quot;],
    )
    
    # transform difference to dataframe
    difference: pl.DataFrame = get_dataframe(report)
    
    # filter out difference from &quot;sap_scrapped&quot;
    to_insert_materials: pl.DataFrame = difference.filter(
        pl.col(&quot;source&quot;) == &quot;sap_scrapped&quot;
    )

    # gather information
    to_insert: pl.DataFrame = sap_scrapped.filter(
        pl.col(&quot;Material&quot;).is_in(to_insert_materials[&quot;Material&quot;])
    ).with_columns(pl.lit(0).alias(&quot;SSP&quot;))
    print(to_insert)
</code></pre>
<blockquote>
<p>You can also use pandas.DataFrame instead of polars.DataFrame</p>
</blockquote>
","1","Answer"
"79549060","79548243","<p>Based on my understanding, your code calculates the sum of weighted values (<code>w @ beta</code>) grouped by the unique values in column <code>&quot;a&quot;</code>. However, since Pandas cannot handle CVXPY variables, this approach results in errors.</p>
<p>The hstack method, on the other hand, uses native CVXPY functions like cp.sum() and cp.hstack(), making it fully compatible and error-free while giving the same result. Therefore, it’s better to use the hstack approach.‍‍‍</p>
<p>for example:</p>
<pre><code>import cvxpy as cp
import pandas as pd

# toy example for demonstration purpose
target = pd.DataFrame(data={&quot;a&quot;: [&quot;X&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;Z&quot;], &quot;b&quot;: [1] * 5})
w = cp.Variable(target.shape[0])
beta = cp.Variable(target.shape[0])
</code></pre>
<pre><code>def loss_func2(w, beta):
    x = cp.hstack([
        cp.sum(w[target[&quot;a&quot;] == group] * beta[target[&quot;a&quot;] == group])
        for group in target[&quot;a&quot;].unique()
    ])

    y = target.groupby(&quot;a&quot;)[&quot;b&quot;].sum().values
    return cp.norm2(x - y) ** 2
</code></pre>
","1","Answer"
"79549483","79549461","<p>Using map is generally used for transformations for one value per row. So if you wanted to set the number 8 in the first row to 7 you could use map (say lambda x: x-1).<br />
However, since you want to output 3 separate variables you can use apply instead with pd.Series.<br />
Your approach of separating the columns into their own variables would work, but it's common practice to just use two brackets to illustrate columns in pandas. Let me know if that makes sense.</p>
<pre><code>mydf[&quot;d3&quot;], mydf[&quot;d2&quot;], mydf[&quot;d1&quot;] = mydf[&quot;mynum&quot;].apply(
    lambda x: pd.Series(last_3_divisions(x, 2))
)
print(mydf)

mydf[[&quot;d3&quot;, &quot;d2&quot;, &quot;d1&quot;]] = mydf[&quot;mynum&quot;].apply(
    lambda x: pd.Series(last_3_divisions(x, 2))
)
print(mydf)
</code></pre>
","1","Answer"
"79549993","79519830","<p>Method in Numpy :</p>
<pre><code>import numpy as np
import pandas as pd

# Create a sample Pandas DataFrame with a column 'a' containing numerical values, including zeros and non-zeros.
df = pd.DataFrame({'a': [0, 0, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0]})

# Extract the 'a' column as a NumPy array for more efficient numerical operations.
aVal = df['a'].to_numpy()

# Determine the length of the input array. This will be used for iterating and defining the output size.
n = len(aVal)

window_size = 3

# Create a 2D array of indices to access the elements within the rolling window for each position in 'aVal'.
# np.arange(n)[:, None] creates a column vector of indices [0, 1, ..., n-1].
# np.arange(1, window_size + 1) creates a row vector of offsets [1, 2, 3].
# Subtracting the offsets from the column vector effectively gives the indices of the preceding elements for each position.
idx = np.arange(n)[:, None] - np.arange(1, window_size + 1)

# Use the calculated indices to extract the values within the rolling windows from 'aVal'.
# np.clip(idx, 0, n - 1) ensures that any index that falls outside the valid range [0, n-1] is clipped to the nearest boundary.
# This handles the initial elements where looking back 'window_size' elements would go beyond the start of the array.
win = aVal[np.clip(idx, 0, n - 1)]

# Assign weights to the elements within each rolling window.
# Non-zero elements are assigned decreasing weights based on their recency (rightmost gets the lowest positive weight).
# Zero elements are assigned a weight of negative infinity to ensure they are not considered the &quot;last&quot; non-zero.
weights = np.where(win != 0, np.arange(window_size, 0, -1), -np.inf)

# Find the index of the maximum weight along each row of the 'weights' array.
# argmax(axis=1) returns the index of the largest value in each row.
# Since non-zero elements have positive weights, this effectively finds the position of the rightmost (most recent) non-zero element in the 'win' array for each original position.
last_pos = weights.argmax(axis=1)

# Use the row indices (0 to n-1) and the 'last_pos' (column indices within 'win') to extract the actual values of the last non-zero elements from the 'win' array.
res = win[np.arange(n), last_pos]

df['res'] = res
print(df)
'''
    a  res
0   0    0
1   0    0
2   1    0
3  -1    1
4  -1   -1
5   0   -1
6   0   -1
7   0   -1
8   0    0
9   0    0
10 -1    0
11  0   -1
12  0   -1
13  1   -1
14  0    1

'''
</code></pre>
","0","Answer"
"79550420","79550403","<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.cummin.html"" rel=""nofollow noreferrer""><code>cummin</code></a> to compute your second condition:</p>
<pre><code>df_GPS[df_GPS['__UTCs__'].ge(22960) &amp; df_GPS['s01[m]'].lt(16).cummin()]
</code></pre>
<p>Output:</p>
<pre><code>                  time  __UTCs__    Altitude  s01[m]  s5.5[m]  s10[m]
2  2024-06-21 06:22:40     22960  605.630573       1        2       0
3  2024-06-21 06:22:41     22961  605.476367       3        3       0
4  2024-06-21 06:22:42     22962  605.322161       2        1       1
5  2024-06-21 06:22:43     22963  605.268389       4        1       0
6  2024-06-21 06:22:44     22964  605.559398       1        3       1
7  2024-06-21 06:22:45     22965  606.630573       2        9       0
8  2024-06-21 06:22:46     22966  607.476367      15       13       3
</code></pre>
<p>Intermediates:</p>
<pre><code>    __UTCs__  s01[m]  __UTCs__ &gt;= 22960  s01[m] &lt; 16  (s01[m] &lt; 16).cummin()      &amp;
0      22958       1              False         True                    True  False
1      22959       3              False         True                    True  False
2      22960       1               True         True                    True   True
3      22961       3               True         True                    True   True
4      22962       2               True         True                    True   True
5      22963       4               True         True                    True   True
6      22964       1               True         True                    True   True
7      22965       2               True         True                    True   True
8      22966      15               True         True                    True   True
9      22967      23               True        False                   False  False
10     22968      20               True        False                   False  False
11     22969      18               True        False                   False  False
12     22970       1               True         True                   False  False
13     22971      15               True         True                   False  False
</code></pre>
<p>A potentially more robust approach if you have many conditions and want the first stretch of all True:</p>
<pre><code>m = df_GPS['__UTCs__'].ge(22960) &amp; df_GPS['s01[m]'].lt(16)
m2 = m.ne(m.shift(fill_value=m.iloc[0])).cumsum().eq(1) &amp; m

out = df_GPS[m2]
</code></pre>
<p>Intermediates:</p>
<pre><code>    __UTCs__  s01[m]      m  shift     ne  cumsum  eq(1)    &amp; m
0      22958       1  False  False  False       0  False  False
1      22959       3  False  False  False       0  False  False
2      22960       1   True  False   True       1   True   True
3      22961       3   True   True  False       1   True   True
4      22962       2   True   True  False       1   True   True
5      22963       4   True   True  False       1   True   True
6      22964       1   True   True  False       1   True   True
7      22965       2   True   True  False       1   True   True
8      22966      15   True   True  False       1   True   True
9      22967      23  False   True   True       2  False  False
10     22968      20  False  False  False       2  False  False
11     22969      18  False  False  False       2  False  False
12     22970       1   True  False   True       3  False  False
13     22971      15   True   True  False       3  False  False
</code></pre>
","2","Answer"
"79550649","79550639","<p>If you want python objects, there is no &quot;efficient&quot; way to perform conversions. Strings in numpy are not necessarily handled efficiently compared to numeric data.</p>
<p>The list comprehension a perfectly valid option. Alternatively:</p>
<pre><code>list(map(str, pd.Series([1234, 123, 345]).unique()))
</code></pre>
<p>Or with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.drop_duplicates.html"" rel=""nofollow noreferrer""><code>drop_duplicates</code></a> in place of <code>unique</code>:</p>
<pre><code>pd.Series([1234, 123, 345]).drop_duplicates().astype(str).tolist()
</code></pre>
<p>Output:</p>
<pre><code>['1234', '123', '345']
</code></pre>
<p>Comparison of speeds, working with strings in numpy is not faster than loops:</p>
<pre><code># initializing an array with 1M items
a = np.arange(1_000_000)

%timeit a.astype(str).tolist()
277 ms ± 95.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

%timeit [str(x) for x in a]
194 ms ± 2.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

%timeit list(map(str, a))
159 ms ± 2.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
","1","Answer"
"79550898","79550795","<p>The confusion is caused by writing the .CSV with a <code>;</code> separator but ignoring this on reading. Use:</p>
<pre><code>    else:
        data = pd.read_csv(&quot;MyPassword_test.txt&quot;)
        data = pd.concat([data, input_entries_df], ignore_index=True)
        print(data)
        data.to_csv(&quot;MyPassword_test.txt&quot;, index=False)
</code></pre>
","2","Answer"
"79550900","79550639","<p>It's because:</p>
<pre><code>series.unique()
</code></pre>
<p>Returns a <code>numpy.ndarray</code> object, and using <code>.astype(str)</code> with numpy returns some numpy text type:</p>
<pre><code>&gt;&gt;&gt; pd.Series([1234, 123, 345]).unique()
array([1234,  123,  345])
&gt;&gt;&gt; np.array([1234, 123, 345]).astype(str)
array(['1234', '123', '345'], dtype='&lt;U21')
</code></pre>
<p>According to the <a href=""https://numpy.org/doc/stable/reference/arrays.dtypes.html#specifying-and-constructing-data-types"" rel=""nofollow noreferrer"">numpy docs</a> the python type <code>str</code> will be converted to to scalar type <code>np.str_</code>, so some array with <code>dtype('U&lt;length&gt;')</code> will result.</p>
","1","Answer"
"79550904","79550795","<p>Your core issue here is that in the case of an existing file, you rewrite the file with a non-default separator <code>sep=&quot;;&quot;</code> and thus:</p>
<ul>
<li>the first entry works (using commas)</li>
<li>the second entry works reading commas but writing semi-colons</li>
<li>the third (and subsequent) entry fails reading expecting commas but finding none.</li>
</ul>
<p>The fix is to use a standard separator. Either accept the default everywhere or override it everywhere.</p>
<p>In any case, you should stop using <code>._append()</code> and switch to <code>pandas.concat()</code></p>
<p>I would likely use the <code>csv</code> package here rather than <code>pandas</code> but if you are keen on <code>pandas</code>, I might suggest.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas
import os

def save_password(website_name, username, password):
    password_file = &quot;MyPassword_test.txt&quot;   # I might think about passing this in as a fourth argument
    new_record= {
        &quot;Website&quot;: [website_name],
        &quot;Username/Email&quot;: [username],
        &quot;Password&quot;: [password]
    }

    ## --------------
    ## If the file does not exist, create it with the required columns
    ## Note: use columns=list(new_record.keys()) if you want to be explicit
    ## --------------
    if not os.path.isfile(password_file):
        pandas \
            .DataFrame(columns=new_record) \
            .to_csv(password_file, index=False)
    ## --------------

    ## --------------
    ## Since we now know the file exists, we can always append the new record
    ## --------------
    pandas \
        .concat([
            pandas.read_csv(password_file),
            pandas.DataFrame(new_record)
        ]) \
        .to_csv(password_file, index=False)
    ## --------------

## --------------
## I try to avoid asking for input in methods that could accept arguments
## --------------
website_name = input(&quot;Website: &quot;)
username = input(&quot;Username: &quot;)
password = input(&quot;Password: &quot;)
save_password(website_name, username, password)
## --------------
</code></pre>
","1","Answer"
"79550921","79550891","<p>You can use <code>groupby.agg()</code></p>
<pre><code>out = df.groupby(['col_a', 'col_d'], as_index=False).agg(
    {
        'col_b': lambda x: ' | '.join(x),  
        'col_c': 'sum'                    
    }
)


print(out)

 col_a     col_d         col_b       col_c
0   Alex     UK       Milk | Wheat      6
1   Alex    USA           Sugar         4
2  David  Spain           Rice          3
</code></pre>
","0","Answer"
"79551657","79544423","<p>Several optimisations can be applied to <code>compute_per_complex_pr</code>.</p>
<p>First of all, <code>pairwise_df.loc[candidate_indices]</code> can be optimised by converting <code>candidate_indices</code> to a Numpy array and using <code>iloc</code> instead.</p>
<p>Actually, <code>sorted(candidate_indices)</code> is also bit slow because it is a set of integer <em>Python object</em> and code operating on pure-Python data structures are generally painfully slow (in CPython which is the standard Python interpreter). Thus, we can convert the set before and then sort it with Numpy so the sort can be significantly faster.</p>
<p>The thing is converting a <code>set</code> to Numpy is a bit slow because it even iterating to this data-structure is already quite slow... On simple way to fix this issue it to use a native language (like C++ or Rust) so to never pay the cost of inherently slow pure-Python objets. An alternative solution is to use a fast bit-set package like <a href=""https://pypi.org/project/bitarray/"" rel=""nofollow noreferrer""><code>bitarray</code></a>. The bad news is that bitarray cannot directly operate on Numpy arrays so it does not bring optimal performance (still because of the manipulation of slow pure-Python data structure), but it is at least significantly better than a naive <code>set</code>. We can Create a bit-set with the right size with <code>candidate_indices = bitarray(len(pairwise_df))</code> and then fill it rather quickly with <code>candidate_indices[gene_to_pair_indices[gene]] = True</code>. The good news is that we do not need to sort anything now because we can directly index the dataframe with a Numpy boolean array created from the bitset. We can do that with <code>unpackbits</code>. However, the result is an array of <code>uint8</code> items. We can use <code>view(bool)</code> so to reinterpret the 0-1 values to booleans very cheaply. One side effect of this approach is that the array can be a bit bigger than expected because bitarray pack bits in bytes so the output is a multiple of 8 (because bytes are octets on all mainstream modern machines). We can slice the array so to get the relevant part.</p>
<p>Last but not least, it is better to operate on categorical data than strings in this case. Indeed, strings are inherently slow because they are stored in pure-Python objects (yes, still them again). They should be converted in categorical data they are repeated many times or can be considered as being part of a limited set of a known set of strings. Categorical data are actually integers with a table associating the integer to string object. Besides speed, categorical data also take significantly less memory here.</p>
<p>Here is the final code:</p>
<pre><code>from bitarray import bitarray

def fast_compute_per_complex_pr(corr_df, terms_df):
    pairwise_df = binary(corr_df)
    pairwise_df = quick_sort(pairwise_df).reset_index(drop=True)
    pairwise_df['gene1'] = pairwise_df['gene1'].astype(&quot;category&quot;)
    pairwise_df['gene2'] = pairwise_df['gene2'].astype(&quot;category&quot;)
    
    # Precompute a mapping from each gene to the row indices in the pairwise DataFrame where it appears.
    gene_to_pair_indices = {}
    for i, (gene_a, gene_b) in enumerate(zip(pairwise_df[&quot;gene1&quot;], pairwise_df[&quot;gene2&quot;])):
        gene_to_pair_indices.setdefault(gene_a, []).append(i)
        gene_to_pair_indices.setdefault(gene_b, []).append(i)
    
    # Initialize AUC scores (one for each complex) with NaNs.
    auc_scores = np.full(len(terms_df), np.nan)
    
    # Loop over each gene complex
    for idx, row in terms_df.iterrows():
        gene_set = set(row.used_genes)

        # Collect all row indices in the pairwise data where either gene belongs to the complex.
        candidate_indices = bitarray(len(pairwise_df))
        for gene in gene_set:
            if gene in gene_to_pair_indices:
                candidate_indices[gene_to_pair_indices[gene]] = True
        
        if not candidate_indices.any():
            continue
        
        # Select only the relevant pairwise comparisons.
        selected_rows = np.unpackbits(candidate_indices).view(bool)[:len(pairwise_df)]
        sub_df = pairwise_df.iloc[selected_rows]
        # A prediction is 1 if both genes in the pair are in the complex; otherwise 0.
        predictions = (sub_df[&quot;gene1&quot;].isin(gene_set) &amp; sub_df[&quot;gene2&quot;].isin(gene_set)).astype(int)
        
        if predictions.sum() == 0:
            continue

        # Compute cumulative true positives and derive precision and recall.
        true_positive_cumsum = predictions.cumsum()
        precision = true_positive_cumsum / (np.arange(len(predictions)) + 1)
        recall = true_positive_cumsum / true_positive_cumsum.iloc[-1]
        
        if len(recall) &lt; 2 or recall.iloc[-1] == 0:
            continue

        auc_scores[idx] = metrics.auc(recall, precision)
    
    # Add the computed AUC scores to the terms DataFrame.
    terms_df[&quot;auc_score&quot;] = auc_scores
    return terms_df
</code></pre>
<p>This code seems about <strong>2.5 times faster</strong> on my machine (with a i5-9600KF CPU on Windows). Note that the code before the loop take 20% of the time now since the loop is about 3 times faster.</p>
<p>To make this much faster, one way would be to parallelise the computation. That being said, I do not expect multithreading to be significantly faster because of the CPython GIL and I do not expect multiprocessing to be a silver-bullet because of data copying/pickling (which should result in a much higher memory consumption). Thus, I think it would be better to convert this part of the code in a native language like C++ first, and then parallelise the loop with tools like OpenMP. The result will be a faster execution not only due to parallelism but also faster operations thanks to a native execution.</p>
<p>One benefit with native language is that you can directly operate on <code>pairwise_df.iloc[selected_rows]</code> without creating a new data structure. A skilled developper can even do the operation chunk by chunk in a SIMD-friendly way so <code>predictions</code> can be computed much more efficiently. Similarly, <code>np.unpackbits</code> is not needed since you can directly iterate on the bit-array. This makes the operation more cache friendly (so it can then scale better once running in parallel). Last but not least, note that most of the values in the bit-array are set to False with the current dataset (&gt;99%). This means this data structure is rather sparse. One can replace the bit-array data structure with something more memory efficient at the expense of a more complex code and slower sequential execution (but likely faster in parallel).</p>
","4","Answer"
"79551671","79551328","<p>You are in a good path if you are already thinking about optimization of your code. I must however point out, that writing good quality code, comes with the cost of spending a lot of time learning your tools, in this case the <code>pandas</code> library. This <a href=""https://www.youtube.com/watch?v=5JnMutdy6Fw"" rel=""nofollow noreferrer"">video </a>is how I was introduced to the topic, and personally I believe it helped me a lot.</p>
<p>If I understand correctly you want to: <strong>filter</strong> specific crime types, <strong>group</strong> them by month and <strong>add</strong> up occurrences, and finally <strong>plot</strong> monthly crime evolution for each type.</p>
<p>Trying out your code three times back to back I got 4.4346, 3.6758 and 3.9400 s execution time -&gt; mean 4.0168 s (not counting time taken to load dataset, used <code>time.perf_counter()</code>). The data used where taken from <a href=""https://data.cityofnewyork.us/Public-Safety/NYPD-Arrests-Data-Historic-/8h9b-rp9u/about_data"" rel=""nofollow noreferrer"">NYPD database</a> (please include your data source when posting questions).</p>
<p><code>crime_counts</code> is what we call, a <a href=""https://en.wikipedia.org/wiki/Pivot_table"" rel=""nofollow noreferrer"">pivot table</a>, and it handles what you did separately for each crime type, while also saving them in an analysis-friendly <code>pd.DataFrame</code> format.</p>
<pre><code>t1 = time.perf_counter()
# changing string based date to datetime object
df[&quot;ARREST_DATE&quot;] = pd.to_datetime(df[&quot;ARREST_DATE&quot;], format='%m/%d/%Y')
# create pd.Series object of data on a monthly frequency [length = df length]
df[&quot;ARREST_MONTH&quot;] = df[&quot;ARREST_DATE&quot;].dt.to_period('M') # no one's stopping you from adding new columns



# Filter the specific crime types
crime_select = [&quot;DANGEROUS DRUGS&quot;, &quot;ASSAULT 3 &amp; RELATED OFFENSES&quot;, &quot;PETIT LARCENY&quot;, &quot;FELONY ASSAULT&quot;, &quot;DANGEROUS WEAPONS&quot;]
filtered = df.loc[df[&quot;OFNS_DESC&quot;].isin(crime_select), [&quot;ARREST_MONTH&quot;, &quot;OFNS_DESC&quot;]]

crime_counts = (filtered
                .groupby([&quot;ARREST_MONTH&quot;, &quot;OFNS_DESC&quot;])
                .size()
                .unstack(fill_value=0))  # Converts grouped data into a DataFrame

# Plot results
crime_counts.plot(figsize=(12,6), title=&quot;Monthly Crime Evolution&quot;)
plt.xlabel(&quot;Arrest Month&quot;)
plt.ylabel(&quot;Number of Arrests&quot;)
plt.legend(title=&quot;Crime Type&quot;)
plt.grid(True)

t2 = time.perf_counter()
print(f&quot;Time taken to complete operations: {t2 - t1:0.4f} s&quot;)

plt.show()
</code></pre>
<p>Above code completed three runs in 2.5432, 2.6067 and 2.4947 s -&gt; mean 2.5482 s. Adding up to a ~<strong>36.56%</strong> speed increase.</p>
<p><em><strong>Note:</strong></em> Did you include the dataset loading time into your execution time measurements? I found that by keeping <code>df</code> loaded and only running the calculations part, yields about 3.35s for your code, and 1.85s for mine.</p>
","1","Answer"
"79551790","79544960","<p>The underlying functions of Pandas/Numpy are mostly using C libraries; but when you use <code>df.apply</code>, you throw those out the window. Generally, there's a better way if you look into their documentation.</p>
<p>For example, what you have here:</p>
<pre class=""lang-py prettyprint-override""><code>df['market_9'] = df.apply(
    lambda x: &quot;9&quot; if x['Date'] &gt;= x['market_9_start'] and x['Date'] &lt; x['market_9_end'] else None,
    axis=1,
)
</code></pre>
<p>Could be re-written as:</p>
<pre class=""lang-py prettyprint-override""><code>df.loc[df.Date.ge(df.market_9_start) &amp; df.Date.lt(df.market_9_end), &quot;market_9&quot;] = &quot;9&quot;
</code></pre>
<h5>For a far greater speed increase than trying to use multiprocessing.</h5>
<hr />
<p>Another example - Change this:</p>
<pre class=""lang-py prettyprint-override""><code>df['Date'] = df['timestamp'].apply(lambda x: pd.to_datetime(x*1000000))
</code></pre>
<p>Into this:</p>
<pre class=""lang-py prettyprint-override""><code>df[&quot;Date&quot;] = pd.to_datetime(df[&quot;timestamp&quot;], unit=&quot;s&quot;)
</code></pre>
","1","Answer"
"79551894","79551824","<p>the problem seems to be that the discrepancy in the 75th percentile results is likely due to a mismatch in data precision, hidden decimals, or the order of the data. What I recommend is to verify your data, such as confirm the number of rows in PostgreSQL matches Pandas and also check values with exact precision because PostgreSQL may store values with hidden decimal places, impacting interpolation. You can retrieve exact values with full precision in PostgreSQL with this query:</p>
<p><code>SELECT obs1::numeric(10,6) FROM my_table ORDER BY obs1;</code></p>
<p>I also recommend confirming both systems analyze the same dataset size, in other words check a row count mismatch.</p>
<p><code>SELECT COUNT (obs1) FROM my_table;  -- Should return 25</code></p>
<p>If it does not return check extra rows in the database not shown in your sample data.</p>
<p>Additionally, sometimes, pandas can be someway very strictly while PostgreSQL avoid some decimal numbers meaning is not as strict as Pandas, so you can use explicit casting in PostgreSQL to avoid rounding:</p>
<p><code>SELECT percentile_cont(0.75) WITHIN GROUP (ORDER BY obs1::numeric(10,6)) FROM my_table;</code></p>
<p>And if you want to discard interpolation, use <strong><code>percentile_disc()</code></strong> in PostgreSQL to avoid it. Like this:</p>
<p><strong><code>SELECT percentile_disc(0.75) WITHIN GROUP (ORDER BY obs1)</code></strong></p>
<p>Hope it works!</p>
","1","Answer"
"79551938","79551904","<p>The full code with correction is provided below:</p>
<pre><code>
import pandas as pd
file_path = &quot;ECP_Unedited.csv&quot;
df = pd.read_csv(file_path)

multivalueColumns = [&quot;Job Perks&quot;, &quot;Professional Development Opportunities&quot;, &quot;Insurance Benefits&quot;]

for column in multivalueColumns:
    if column in df.columns:
        df[column] = df[column].fillna(&quot;&quot;)
        split_data = df[column].str.get_dummies(sep=&quot;;&quot;)
        split_data = split_data.add_prefix(f&quot;{column}_&quot;)
        df = pd.concat([df, split_data], axis=1)

df.drop(columns=multivalueColumns, inplace=True)

transformed_file_path = &quot;Transformed_ECP_Unedited.csv&quot;
df.to_csv(transformed_file_path, index=False)

print(f&quot;Transformed file saved as: {transformed_file_path}&quot;)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/3K95c3ul.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3K95c3ul.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79552176","79551904","<p>You could combine <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.get_dummies.html"" rel=""nofollow noreferrer""><code>str.get_dummies</code></a>, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.add_prefix.html"" rel=""nofollow noreferrer""><code>add_prefix</code></a>, and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>pd.concat</code></a> with a generator:</p>
<pre><code>out = pd.concat(
    (
        df[col].str.get_dummies(sep='; ').add_prefix(f'{col}_')
        for col in multivalueColumns
    ),
    axis=1,
)
</code></pre>
<p>Output:</p>
<pre><code>   Job Perks_Certification Programs  Job Perks_Cross Training  Job Perks_Leadership Development Programs  Job Perks_Online Courses  Insurance Benefits_Accident Insurance  Insurance Benefits_Dental Insurance  Insurance Benefits_Life Insurance
0                                 1                         1                                          0                         1                                      0                                    1                                  1
1                                 0                         0                                          1                         1                                      1                                    0                                  1
</code></pre>
","3","Answer"
"79552679","79552670","<p>Maybe easier is processing dictionary from last row, <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pop.html"" rel=""nofollow noreferrer""><code>DataFrame.pop</code></a> trick is for remove original column <code>realized_price</code>:</p>
<pre><code>d = df.iloc[-1].to_dict()
d['simulated_price'] = d.pop('realized_price')
d['simulation'] = 'realized_price'
df.loc[len(df.pop('realized_price'))] = d
</code></pre>
<p>Alternative:</p>
<pre><code>last = df.columns[-1]
d = df.iloc[-1].to_dict()
d['simulated_price'] = d.pop(last)
d['simulation'] = last
df.loc[len(df.pop(last))] = d

print (df)

  maturity_date      simulation  simulated_price
0    30/06/2010               1         0.539333
1    30/06/2010               2         0.544000
2    30/06/2010               3         0.789667
3    30/06/2010               4         0.190333
4    30/06/2010               5         0.413667
5    30/06/2010  realized_price         0.611000
</code></pre>
<p>Another idea is use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>DataFrame.loc</code></a> for set new row with default index of DataFrame by select last row in <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>DataFrame.iloc</code></a>, <code>rename</code> and reappend <code>simulation</code> with new value <code>realized_price</code> in <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html"" rel=""nofollow noreferrer""><code>Series.reindex</code></a>:</p>
<pre><code>s = (df.iloc[-1].drop(['simulated_price','simulation'])
                .rename({'realized_price':'simulated_price'})
                .reindex(df.columns[:-1], fill_value='realized_price'))

df.loc[len(df.pop('realized_price'))] = s

print (df)

  maturity_date      simulation  simulated_price
0    30/06/2010               1         0.539333
1    30/06/2010               2         0.544000
2    30/06/2010               3         0.789667
3    30/06/2010               4         0.190333
4    30/06/2010               5         0.413667
5    30/06/2010  realized_price         0.611000
</code></pre>
<p>Alternative is first reassign column <code>simulation</code>, then get last row and processing <code>Series</code>:</p>
<pre><code>s = (df.assign(simulation='realized_price')
       .iloc[-1]
       .drop(['simulated_price'])
       .rename({'realized_price':'simulated_price'}))

df.loc[len(df.pop('realized_price'))] = s

print (df)


  maturity_date      simulation  simulated_price
0    30/06/2010               1         0.539333
1    30/06/2010               2         0.544000
2    30/06/2010               3         0.789667
3    30/06/2010               4         0.190333
4    30/06/2010               5         0.413667
5    30/06/2010  realized_price         0.611000
</code></pre>
<p>Another idea with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>:</p>
<pre><code>out = (pd.concat([df, 
                  df.iloc[[-1]]
                    .assign(simulation='realized_price',
                            simulated_price=df['realized_price'].iat[0])],
                 ignore_index=True)
        .drop('realized_price', axis=1))
print (out)
  maturity_date      simulation  simulated_price
0    30/06/2010               1         0.539333
1    30/06/2010               2         0.544000
2    30/06/2010               3         0.789667
3    30/06/2010               4         0.190333
4    30/06/2010               5         0.413667
5    30/06/2010  realized_price         0.611000
</code></pre>
","2","Answer"
"79552777","79548332","<p>Glad to know you're able to figure out the resolution. Posting this as a solution so that people came across similar issue might find it handy.</p>
<p>It worked for me when I used correct connection string:</p>
<pre><code>import logging as rilg
import pandas as pd
import azure.functions as func
from sqlalchemy import create_engine, inspect
import pyodbc

sql_alchemy_connection_url = (
    &quot;mssql+pyodbc://username:psswrd@rithwik.database.windows.net:1433/test1?&quot;
    &quot;driver=ODBC+Driver+17+for+SQL+Server&amp;Encrypt=yes&amp;TrustServerCertificate=no&amp;Connection+Timeout=30&quot;
)

def sqlAlchemy_getConnection(connection_url):
    return create_engine(connection_url)

app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)

@app.route(route=&quot;http_trigger&quot;)
def http_trigger(req: func.HttpRequest) -&gt; func.HttpResponse:
    rilg.info('Hello Rithwik.')
    ritheng = sqlAlchemy_getConnection(sql_alchemy_connection_url)
    tester = inspect(ritheng)
    sink = &quot;Teams&quot;  
    get_colss = tester.get_columns(sink, &quot;dbo&quot;)
    datatyp = {
        column['name']: column['type'] 
        for column in get_colss
    }
    ridf = pd.DataFrame({
        'ID': [8,7],
        'Season_ID': [1, 1],
        'Team_Name': ['Test Rith','Team Cho']
    })
    temp_table = &quot;test&quot; 
    ridf.to_sql(temp_table, ritheng, schema=&quot;dbo&quot;, if_exists='replace', index=False, dtype=datatyp)
    return func.HttpResponse(&quot;Data Sent&quot;, status_code=200)
</code></pre>
<p>Output:</p>
<p><img src=""https://i.imgur.com/vSI55r9.png"" alt=""enter image description here"" /></p>
","1","Answer"
"79552870","79552829","<p>Create helper DataFrame <code>df1</code> for <code>N</code> months starting by <code>base</code>, use cross join with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>DataFrame.merge</code></a> for all combinations, so possible sum less and greater or equal values. Last use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html"" rel=""nofollow noreferrer""><code>DataFrame.pivot</code></a> for new months columns:</p>
<pre><code>df['Departing_dt'] = pd.to_datetime(df['Departing'], format='%b%Y')
df['Arriving_dt']  = pd.to_datetime(df['Arriving'], format='%b%Y')

N = 3
base = pd.to_datetime(&quot;APR2025&quot;, format=&quot;%b%Y&quot;)

df1 = pd.DataFrame({'MonthDate':
                   [base + pd.DateOffset(months=x) for x in range(N)]})

df2 = pd.merge(df.reset_index(), df1, how='cross')

df2['sum'] = (df2['MonthDate'].lt(df2['Departing_dt']).astype(int) +
              df2['MonthDate'].ge(df2['Arriving_dt']).astype(int))

out = (df2.pivot(index=['index','Location','Departing','Arriving'], 
                 columns='MonthDate', 
                 values='sum')
         .rename(columns=lambda x: x.strftime(&quot;%b%Y&quot;).upper())
         .reset_index([1,2,3])
         .rename_axis(index=None, columns=None))

print(out)
  Location Departing Arriving  APR2025  MAY2025  JUN2025
0    Paris   MAY2025  MAR2025        2        1        1
1   Dallas   JUN2025  JUN2025        1        1        1
2    Tokyo   APR2025  JUN2025        0        0        1
3   London   JUN2025  APR2025        2        2        1
</code></pre>
<p>Similar solution with using months periods:</p>
<pre><code>df['Departing_dt'] = (pd.to_datetime(df['Departing'], format='%b%Y')
                        .dt.to_period('M'))
df['Arriving_dt']  = (pd.to_datetime(df['Arriving'], format='%b%Y')
                        .dt.to_period('M'))

N = 3
base = pd.to_datetime(&quot;APR2025&quot;, format=&quot;%b%Y&quot;)
month_periods = pd.period_range(start=base.to_period('M'), periods=N, freq='M')

df1 = pd.DataFrame({'MonthDate': month_periods})

df2 = pd.merge(df.reset_index(), df1, how='cross')

df2['sum'] = (df2['MonthDate'].lt(df2['Departing_dt']).astype(int) +
              df2['MonthDate'].ge(df2['Arriving_dt']).astype(int))


out = (df2.pivot(index=['index','Location','Departing','Arriving'], 
                 columns='MonthDate', 
                 values='sum')
         .rename(columns=lambda x: x.strftime(&quot;%b%Y&quot;).upper())
         .reset_index([1,2,3])
         .rename_axis(index=None, columns=None))

print(out)
  Location Departing Arriving  APR2025  MAY2025  JUN2025
0    Paris   MAY2025  MAR2025        2        1        1
1   Dallas   JUN2025  JUN2025        1        1        1
2    Tokyo   APR2025  JUN2025        0        0        1
3   London   JUN2025  APR2025        2        2        1
</code></pre>
<p>Numpy alternative work similar, instead cross join is used broadcasting for sum matched datetimes:</p>
<pre><code>Departing = pd.to_datetime(df['Departing'], format='%b%Y')
Arriving = pd.to_datetime(df['Arriving'], format='%b%Y')

N = 3 
base = pd.to_datetime(&quot;APR2025&quot;, format=&quot;%b%Y&quot;)
month_dates = [base + pd.DateOffset(months=i) for i in range(N)]
month_names = [d.strftime(&quot;%b%Y&quot;).upper() for d in month_dates]
    
departing_np = Departing.to_numpy()[:, None]
arriving_np = Arriving.to_numpy()[:, None]
month_dates_np = np.array(month_dates, dtype='datetime64[ns]')[None, :]

arr = ((month_dates_np &lt; departing_np).astype(int) +
       (month_dates_np &gt;= arriving_np).astype(int))

df = df.join(pd.DataFrame(arr, index=df.index, columns=month_names))
print(df)
  Location Departing Arriving  APR2025  MAY2025  JUN2025
0    Paris   MAY2025  MAR2025        2        1        1
1   Dallas   JUN2025  JUN2025        1        1        1
2    Tokyo   APR2025  JUN2025        0        0        1
3   London   JUN2025  APR2025        2        2        1
</code></pre>
<p><strong>Performance comparison</strong>:</p>
<p><a href=""https://i.sstatic.net/M3RlR7pB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M3RlR7pB.png"" alt=""comparison"" /></a></p>
<pre><code>import pandas as pd
#pip install perfplot
import perfplot


def jez1(df):
    df['Departing_dt'] = pd.to_datetime(df['Departing'], format='%b%Y')
    df['Arriving_dt']  = pd.to_datetime(df['Arriving'], format='%b%Y')
    
    N = 3
    base = pd.to_datetime(&quot;APR2025&quot;, format=&quot;%b%Y&quot;)
    
    df1 = pd.DataFrame({'MonthDate':
                       [base + pd.DateOffset(months=x) for x in range(N)]})
    
    df2 = pd.merge(df.reset_index(), df1, how='cross')
    
    df2['sum'] = (df2['MonthDate'].lt(df2['Departing_dt']).astype(int) +
                  df2['MonthDate'].ge(df2['Arriving_dt']).astype(int))
    
    out = (df2.pivot(index=['index','Location','Departing','Arriving'], 
                     columns='MonthDate', 
                     values='sum')
             .rename(columns=lambda x: x.strftime(&quot;%b%Y&quot;).upper())
             .reset_index([1,2,3])
             .rename_axis(index=None, columns=None))
    return out

def jez2(df):
    df['Departing_dt'] = (pd.to_datetime(df['Departing'], format='%b%Y')
                            .dt.to_period('M'))
    df['Arriving_dt']  = (pd.to_datetime(df['Arriving'], format='%b%Y')
                            .dt.to_period('M'))
    
    N = 3
    base = pd.to_datetime(&quot;APR2025&quot;, format=&quot;%b%Y&quot;)
    month_periods = pd.period_range(start=base.to_period('M'), periods=N, freq='M')
    
    df1 = pd.DataFrame({'MonthDate': month_periods})
    
    df2 = pd.merge(df.reset_index(), df1, how='cross')
    
    df2['sum'] = (df2['MonthDate'].lt(df2['Departing_dt']).astype(int) +
                  df2['MonthDate'].ge(df2['Arriving_dt']).astype(int))
    
    
    out = (df2.pivot(index=['index','Location','Departing','Arriving'], 
                     columns='MonthDate', 
                     values='sum')
             .rename(columns=lambda x: x.strftime(&quot;%b%Y&quot;).upper())
             .reset_index([1,2,3])
             .rename_axis(index=None, columns=None))

    return out

def jez3(df):
    Departing = pd.to_datetime(df['Departing'], format='%b%Y')
    Arriving = pd.to_datetime(df['Arriving'], format='%b%Y')
    
    N = 3 
    base = pd.to_datetime(&quot;APR2025&quot;, format=&quot;%b%Y&quot;)
    month_dates = [base + pd.DateOffset(months=i) for i in range(N)]
    month_names = [d.strftime(&quot;%b%Y&quot;).upper() for d in month_dates]
        
    departing_np = Departing.to_numpy()[:, None]
    arriving_np = Arriving.to_numpy()[:, None]
    month_dates_np = np.array(month_dates, dtype='datetime64[ns]')[None, :]
    
    arr = ((month_dates_np &lt; departing_np).astype(int) +
           (month_dates_np &gt;= arriving_np).astype(int))
    
    out = df.join(pd.DataFrame(arr, index=df.index, columns=month_names))

    return out

def emi(x):
    next_3_months = [(pd.to_datetime('today') + pd.DateOffset(months=month)).strftime(&quot;%b%Y&quot;) for month in [0, 1, 2]]
    add_dates = [pd.date_range(y, z, freq='ME').strftime(&quot;%b%Y&quot;) for y, z in zip( x['Arriving'], x['Departing'])]
    add_person = pd.DataFrame({'Location' : x['Location'], '+ dates':add_dates}).explode('+ dates').value_counts().reset_index()
    add_person = add_person.loc[add_person['+ dates'].isin(next_3_months)].copy()
    
    sub_dates = [pd.date_range(y, z, freq='ME').strftime(&quot;%b%Y&quot;) for y, z in zip( x['Departing'], x['Arriving'])]
    sub_person = pd.DataFrame({'Location' : x['Location'], '- dates':sub_dates}).explode('- dates').value_counts().reset_index()
    sub_person = sub_person.loc[sub_person['- dates'].isin(next_3_months)].copy()
    
    counts = add_person.merge(sub_person, left_on=['Location', '+ dates'], right_on=['Location', '- dates'], suffixes=[' add', ' sub'], how='outer')
    counts['date'] = counts['+ dates'].fillna(counts['- dates'])
    counts['change from 1'] = counts['count add'].fillna(0) - counts['count sub'].fillna(0)
    pivot_counts = counts.pivot(index='Location', columns='date', values='change from 1').reset_index()
    
    x = x.merge(pivot_counts, on='Location', how='left').fillna(0)
    x[[month for month in next_3_months if month not in x.columns]] = 0
    x[next_3_months] = (x[next_3_months] + 1).astype(int)
    
    return x


def make_df(n):
    np.random.seed(523)

    Lo = ['Paris', 'Dallas', 'Tokyo', 'London']
    L = ['MAY2025', 'JUN2025', 'SEP2025', 'MAR2025', 'JUL2026']


    df = pd.DataFrame({
        'Location': np.random.choice(Lo, size=n),
        'Departing': np.random.choice(L, size=n),
        'Arriving': np.random.choice(L, size=n)})

    return df


perfplot.show(
    setup=make_df,
    kernels=[jez1, jez2, jez3, emi],
    n_range=[2**k for k in range(2, 20)],
    logx=True,
    logy=True,
    equality_check=False,  
    xlabel='len(df)')
</code></pre>
","2","Answer"
"79553001","79552829","<p>Firstly I would get a list of the next 3 months (including current)</p>
<pre><code>next_3_months = [(pd.to_datetime('today') + pd.DateOffset(months=month)).strftime(&quot;%b%Y&quot;) for month in [0, 1, 2]]
['Apr2025', 'May2025', 'Jun2025']
</code></pre>
<p>Then use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.date_range.html"" rel=""nofollow noreferrer"">pd.date_range()</a> to get all the months between an arrival and departure - i.e. all the months were there will be an additional person.  Create a temporary dataframe with this as a column alongside the location column.  You can then use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer"">df.explode()</a> to get one row for each location and each month they have an additional person.  Then filter these months to only those in the next 3.</p>
<pre><code>add_dates = [pd.date_range(y, z, freq='m').strftime(&quot;%b%Y&quot;) for y, z in zip( x['Arriving'], x['Departing'])]
add_person = pd.DataFrame({'Location' : x['Location'], '+ dates':add_dates}).explode('+ dates').value_counts().reset_index()
add_person = add_person.loc[add_person['+ dates'].isin(next_3_months)].copy()

  Location  + dates  count
0   London  Apr2025      1
1   London  May2025      1
2    Paris  Apr2025      1
</code></pre>
<p>Repeat the same for all dates between departure and arrival - i.e. all the months where there will be one less person</p>
<pre><code>sub_dates = [pd.date_range(y, z, freq='m').strftime(&quot;%b%Y&quot;) for y, z in zip( x['Departing'], x['Arriving'])]
sub_person = pd.DataFrame({'Location' : x['Location'], '- dates':sub_dates}).explode('- dates').value_counts().reset_index()
sub_person = sub_person.loc[sub_person['- dates'].isin(next_3_months)].copy()
</code></pre>
<p>Then outer merge these two together, and work out what the difference would be for each location per month.  Pivot this to have the months as columns.</p>
<pre><code>counts = add_person.merge(sub_person, left_on=['Location', '+ dates'], right_on=['Location', '- dates'], suffixes=[' add', ' sub'], how='outer')
counts['date'] = counts['+ dates'].fillna(counts['- dates'])
counts['change from 1'] = counts['count add'].fillna(0) - counts['count sub'].fillna(0)
pivot_counts = counts.pivot(index='Location', columns='date', values='change from 1').reset_index()

date Location  APR2025  MAY2025
0      London      1.0      1.0
1       Paris      1.0      NaN
2       Tokyo     -1.0     -1.0
</code></pre>
<p>Merge this back onto the original dataframe, if a month is missing, add it in, fill nan/missing values with 0 and add 1 to everything (1 being the starting number of people, which is then changed by arrivals/departures).</p>
<pre><code>x = x.merge(pivot_counts, on='Location', how='left').fillna(0)
x[[month for month in next_3_months if month not in x.columns]] = 0
x[next_3_months] = (x[next_3_months] + 1).astype(int)

  Location Departing Arriving  Apr2025  May2025  Jun2025
0    Paris   MAY2025  MAR2025        2        1        1
1   Dallas   JUN2025  JUN2025        1        1        1
2    Tokyo   APR2025  JUN2025        0        0        1
3   London   JUN2025  APR2025        2        2        1
</code></pre>
","1","Answer"
"79553364","79550255","<p>Try this code:</p>
<pre><code>import pandas as pd
import numpy as np
import Levenshtein

data = {
    'English': [&quot;hello&quot;, &quot;helo&quot;, &quot;hello&quot;, &quot;hallo&quot;],
    'Spanish': [&quot;hola&quot;, &quot;hola&quot;, &quot;hola_a&quot;, &quot;a_aureola&quot;],
    'Count': [23, 2, 1, 1],
    'AF': [0, 1, 0, 1] 
}
df = pd.DataFrame(data)
print(df)

# Function to check if two rows are duplicates
def are_duplicates(row_1, row_2, levenshtein_threshold=1):
    english_1, spanish_1 = row_1['English'], row_1['Spanish']
    english_2, spanish_2 = row_2['English'], row_2['Spanish']

    # Check for exact match in English
    if english_1 == english_2:
        # Check for overlap in Spanish
        spanish_overlap_1 = set(spanish_1.split('_'))
        spanish_overlap_2 = set(spanish_2.split('_'))
        if spanish_overlap_1.intersection(spanish_overlap_2):
            return True
        
    # Check for Levenshtein distance for near matches
    elif Levenshtein.distance(english_1, english_2) &lt; levenshtein_threshold and spanish_1 == spanish_2:
        return True
    
    return False

# Identify duplicates and merge them
def merge_duplicates(df):
    merged_rows = []

    processed_indices = set()   # Create a list to track processed indices

    for i, row_1 in df.iterrows():
        if i in processed_indices:
            continue
        
        merged_row = row_1.copy() # Start merging duplicates for row_1
        
        for j, row_2 in df.iterrows():
            if i != j and j not in processed_indices:
                if are_duplicates(row_1, row_2):
                    # Update the merged row with the sum of numerical columns
                    merged_row['Count'] += row_2['Count']
                    # Assuming 'AF' and other numerical columns should be summed too
                    for col in df.select_dtypes(include=[np.number]).columns:
                        if col != 'Count':  # Avoid summing 'Count' twice
                            merged_row[col] += row_2[col]
                    processed_indices.add(j)
        
        # Append the merged row to the merged_rows list
        merged_rows.append(merged_row)

    merged_df = pd.DataFrame(merged_rows)
    return merged_df.drop_duplicates()

merged_df = merge_duplicates(df)

print(merged_df)
</code></pre>
<p>You will need to install the &quot;Levenshtein&quot; library thought :</p>
<pre><code>pip install python-Levenshtein
</code></pre>
<p>Here is the original df:</p>
<pre><code>Original DataFrame:
  English    Spanish  Count  AF
0   hello       hola     23   0
1    helo       hola      2   1
2   hello     hola_a      1   0
3   hallo  a_aureola      1   1
</code></pre>
<p>And here is the data after merging:</p>
<pre><code>Merged DataFrame:
  English    Spanish  Count  AF
0   hello       hola     24   0
1    helo       hola      2   1
3   hallo  a_aureola      1   1
</code></pre>
","0","Answer"
"79553372","79553359","<p>If you could provide some sample data, it would help, as it is not quite enough information.</p>
<p>Meanwhile here is a sample code to calculate unique task types that were not executed by  leavers.</p>
<pre><code>import pandas as pd

data = {
    'task type': ['Development', 'Testing', 'Design', 'Development', 'Testing', 'Analysis'],
    'Assignee+': ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve', 'Frank']
}
df = pd.DataFrame(data)

print(df)

leavers = ['Alice', 'Bob']

executed_by_leavers = df.loc[df['Assignee+'].isin(leavers), 'task type']
unique_executed_task_types = executed_by_leavers.unique()

print(&quot;Unique task types not executed by leavers:&quot;, unique_executed_task_types)
</code></pre>
<p>Original df:</p>
<pre><code>     task type Assignee+
0  Development     Alice
1      Testing       Bob
2       Design   Charlie
3  Development      Dave
4      Testing       Eve
5     Analysis     Frank
</code></pre>
<p>So if Alice and Bob are the leavers then Development and Testing are the unique task not executed by leavers.</p>
","0","Answer"
"79553490","79542528","<p>This it is :</p>
<p><code>df = df.applymap(lambda y: y.strip(&quot;'\&quot;&quot;) if isinstance(y, str) else y)</code></p>
","0","Answer"
"79553742","79553487","<p>Since you have lists, using pandas would not be more efficient.</p>
<p>You could use <a href=""https://docs.python.org/3/library/itertools.html"" rel=""nofollow noreferrer""><code>itertools</code></a> to enumerate the edges, and <a href=""https://docs.python.org/3/library/collections.html#collections.Counter"" rel=""nofollow noreferrer""><code>collections.Counter</code></a> to count them, then build the graph and plot with a width based on the weight:</p>
<pre><code>from itertools import combinations, chain
from collections import Counter
import networkx as nx

c = Counter(chain.from_iterable(combinations(sorted(l), 2) for l in df['nodes']))

G = nx.Graph()
G.add_weighted_edges_from((*e, w) for e, w in c.items())

pos = nx.spring_layout(G)
nx.draw_networkx(G, pos)

for *e, w in G.edges(data='weight'):
    nx.draw_networkx_edges(G, pos, edgelist=[e], width=w)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/cwa7tw4g.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cwa7tw4g.png"" alt=""networkx graph from weighted edges"" /></a></p>
<p>Used input:</p>
<pre><code>df = pd.DataFrame({'user': ['A', 'B', 'C'],
                   'nodes': [[0, 1, 3], [1, 2, 4], [0, 3]],
                  })
</code></pre>
","1","Answer"
"79554453","79554434","<p>This line should be corrected.</p>
<pre><code>my_df['fruits'].apply(lambda x: True for i in my_list if i in x)
</code></pre>
<p>The lambda function returns a generator, not a boolean (<code>True</code>/<code>False value)</code>. On the other hand, the <code>apply()</code> method expects a return value for each row of the DataFrame, and this return value should be a Boolean (or any other type), not a generator object. Additionally, missing <code>any()</code> to Check for Matches.</p>
<p>The corrected line is:</p>
<pre><code>my_df['check'] = my_df['fruits'].apply(lambda x: any(v in x for v in my_list))
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/JpcC5DW2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpcC5DW2.png"" alt=""enter image description here"" /></a></p>
<p>Sources:</p>
<p><a href=""https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions"" rel=""nofollow noreferrer"">https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions</a></p>
<p><a href=""https://docs.python.org/3/tutorial/classes.html#generator-expressions"" rel=""nofollow noreferrer"">https://docs.python.org/3/tutorial/classes.html#generator-expressions</a></p>
<p><a href=""https://docs.python.org/3/library/functions.html#any"" rel=""nofollow noreferrer"">https://docs.python.org/3/library/functions.html#any</a></p>
","1","Answer"
"79554724","79554601","<p>I think this is a less a foundry questions than a pandas question :)</p>
<p>You might need to change the locale in your code ?</p>
<pre><code>
# Change locale for commas as decimal separators, e.g., 'fr_FR' for French
locale.setlocale(locale.LC_ALL, 'fr_FR.UTF-8')
</code></pre>
","1","Answer"
"79555074","79555053","<p>You can build a group, then apply your functions with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a> the outputs:</p>
<pre><code>group = ['id', 'hrz', 'tenor']
cols = df.columns.difference(group+['date'])

g = df.groupby(group)[cols]

out = (pd.concat({'ks_test': g.agg(ks_test),
                  'cvm_test': g.agg(cvm_test),
                 }, names=['test'])
       .sort_index(level=group, kind='stable', sort_remaining=False)
       .reset_index()
      )
</code></pre>
<p>Output:</p>
<pre><code>       test   id  hrz tenor         1         2         3         4
0   ks_test  AAA    2    1y  0.278182  0.166364  0.254545  0.224545
1  cvm_test  AAA    2    1y  0.220803  0.044730  0.158839  0.118321
2   ks_test  AAA    2    6m  0.278182  0.166364  0.254545  0.224545
3  cvm_test  AAA    2    6m  0.220803  0.044730  0.158839  0.118321
</code></pre>
<p>Alternatively, pass your functions and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>stack</code></a> (in this case the name is defined by the name of the function) :</p>
<pre><code>group = ['id', 'hrz', 'tenor']
cols = df.columns.difference(group+['date'])

out = (
    df.groupby(group)[cols]
    .agg([ks_test, cvm_test])
    .rename_axis([None, 'test'], axis=1)
    .stack()
    .reset_index()
)
</code></pre>
<p>Output:</p>
<pre><code>    id  hrz tenor      test         1         2         3         4
0  AAA    2    1y   ks_test  0.278182  0.166364  0.254545  0.224545
1  AAA    2    1y  cvm_test  0.220803  0.044730  0.158839  0.118321
2  AAA    2    6m   ks_test  0.278182  0.166364  0.254545  0.224545
3  AAA    2    6m  cvm_test  0.220803  0.044730  0.158839  0.118321
</code></pre>
","3","Answer"
"79555077","79555053","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.agg.html"" rel=""nofollow noreferrer""><code>GroupBy.agg</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>DataFrame.stack</code></a> for reshape last level of MultiIndex in columns:</p>
<pre><code>cols = ['id','hrz', 'tenor']
out = (df.groupby(cols)[df.columns.difference(cols + ['date'], sort=False)]
        .agg([ks_test, cvm_test])
        .rename_axis([None, 'test'], axis=1)
        .stack(future_stack=True)
        .reset_index())

print (out)
    id  hrz tenor      test         1         2         3         4
0  AAA    2    1y   ks_test  0.278182  0.166364  0.254545  0.224545
1  AAA    2    1y  cvm_test  0.220803  0.044730  0.158839  0.118321
2  AAA    2    6m   ks_test  0.278182  0.166364  0.254545  0.224545
3  AAA    2    6m  cvm_test  0.220803  0.044730  0.158839  0.118321
</code></pre>
<p>How it working:</p>
<pre><code>print (df.groupby(cols)[df.columns.difference(cols +['date'], sort=False)]
        .agg([ks_test, cvm_test]))

                      1                   2                  3            \
                ks_test  cvm_test   ks_test cvm_test   ks_test  cvm_test   
id  hrz tenor                                                              
AAA 2   1y     0.278182  0.220803  0.166364  0.04473  0.254545  0.158839   
        6m     0.278182  0.220803  0.166364  0.04473  0.254545  0.158839   

                      4            
                ks_test  cvm_test  
id  hrz tenor                      
AAA 2   1y     0.224545  0.118321  
        6m     0.224545  0.118321  
</code></pre>
","2","Answer"
"79555137","79553359","<p>You can do the exact same thing if you have the list of the people you want, or you can have the people that dont leave:</p>
<pre><code>import pandas as pd


src = {'task type': ['install', 'upgrade', 'install', 'remove','upgrade'],
       'users': ['Dave', 'Carol', 'Mike', 'Alice', 'Dave']
       }

leavers = ['Carol', 'Alice']
stays = [&quot;Dave&quot;, &quot;Mike&quot;]



df = pd.DataFrame(src)

df_leavers = df.loc[df['users'].isin(leavers)]

df_stays = df.loc[df['users'].isin(stays)]


### Or if you want to just have the not leavers:

df_not_leavers = df.loc[~df['users'].isin(leavers)]
</code></pre>
<p>the <code>df['users'].isin(leavers)</code> is a mask, and the <code>~</code> says the mask to be the oposite</p>
","0","Answer"
"79555784","79555775","<p>Please use the following line:</p>
<pre><code>df_grouped = df.groupby(&quot;Name&quot;).agg(list).reset_index()
</code></pre>
<p>When you run this line</p>
<pre><code>df_grouped = df.groupby(&quot;Name&quot;)[&quot;DomainCode&quot;].apply(list).reset_index()
</code></pre>
<p>It returns 1 instead of 2 because Pandas is storing the list as a single string ('[Pr, Gov]') rather than a true Python list.</p>
<p>For conversion to real list (from comment):</p>
<pre><code>import ast
fake_list = &quot;['Pr', 'Gov']&quot;  
real_list = ast.literal_eval(fake_list)
print(real_list)  
for item in real_list:
    print(item)
</code></pre>
<p>Or:</p>
<pre><code>fake_list = &quot;Pr, Gov&quot;
real_list = fake_list.split(&quot;, &quot;)
print(real_list)
</code></pre>
<p>Or:</p>
<pre><code>import json
fake_list = '[&quot;Pr&quot;, &quot;Gov&quot;]'
real_list = json.loads(fake_list)
print(real_list)
</code></pre>
<p>Or:</p>
<pre><code>import re
fake_list = &quot;['Pr', 'Gov']&quot;
real_list = re.findall(r&quot;'(.*?)'&quot;, fake_list)
print(real_list)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/1KuMB583.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1KuMB583.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79555789","79555775","<p>Try this</p>
<pre><code>import pandas as pd

d = {
    'Name': ['DataSource', 'DataSource'],
    'DomainCode': ['Pr', 'Gov'],
    'DomainName': ['Private', 'Government']
}
df = pd.DataFrame(data=d)


result = df.groupby(&quot;Name&quot;).agg({
    'DomainCode': list,
    'DomainName': list
}).reset_index()

print(result)
</code></pre>
","0","Answer"
"79556132","79555826","<p>There are several things that probably won't work in you example:</p>
<ul>
<li><p>In Dask, collections are immutables, you don't create an empty dask dataframe to fill it later, its lazy and distributed.</p>
</li>
<li><p>Custom aggregations are more complex than that, as stated <a href=""https://docs.dask.org/en/latest/generated/dask.dataframe.Aggregation.html"" rel=""nofollow noreferrer"">in the documentation</a>, it works on SeriesGroupBy objects, not lists.</p>
</li>
<li><p>There are typos in your example, and it lacks the initial DataFrame.</p>
</li>
</ul>
<p>Custom Dask aggregations are really hard to get right, however, in your case, I found a simpler solution:</p>
<pre><code>import pandas as pd
import dask.dataframe as dd
df = pd.DataFrame.from_dict({'id':[1,1,2,2,2,4], 'name':['a','b','c','d','e','f']})
ddf = dd.from_pandas(df, 2)
ddf2 = ddf.groupby('id').agg(list)
ddf2[&quot;name2&quot;] = ddf2[&quot;name&quot;].apply(lambda l: &quot;|&quot;.join(l))
ddf2.compute()
</code></pre>
<p>Which produces:</p>
<pre><code>    name    name2
id      
1   [a, b]  a|b
2   [c, d, e]   c|d|e
4   [f] f
</code></pre>
","3","Answer"
"79557455","79557429","<p>u may have a file named <code>pandas.py</code> in your current working directory</p>
<p>try running this in a new cell:</p>
<p><code>import os</code></p>
<p><code>os.listdir()</code></p>
<p>if u see anything named pandas.py or pandas.pyc, delete or rename:</p>
<p>if u renamed or deleted, also remove the <code>.pyc</code> cache</p>
<p><code>!find . -name &quot;*.pyc&quot; -delete</code></p>
<p>restart the kernel and try again</p>
","1","Answer"
"79558260","79536363","<p>There isn’t an official pandas equivalent to what STATA or <code>tidylog</code> in R does, unfortunately — pandas operations are usually silent unless you manually check the results. That said, your monkeypatching approach is actually pretty solid, and I’ve done something similar before.</p>
<p>Here’s a slightly cleaner version that keeps the original method and logs how many duplicates were dropped:</p>
<pre><code>import pandas as pd
import logging

logging.basicConfig(level=logging.INFO)

def my_drop_duplicates(self, *args, **kwargs):
    n_before = self.shape[0]
    result = pd.DataFrame._original_drop_duplicates(self, *args, **kwargs)
    n_after = result.shape[0]
    logging.info(f&quot;Dropped {n_before - n_after} duplicate rows&quot;)
    return result

# Save the original method
pd.DataFrame._original_drop_duplicates = pd.DataFrame.drop_duplicates
# Monkeypatch with the new one
pd.DataFrame.drop_duplicates = my_drop_duplicates
</code></pre>
<p>This way, you're not mutating the original DataFrame (which is important since <code>drop_duplicates()</code> returns a copy by default), and you still get the feedback.</p>
<p>Alternatively, if you don’t want to monkeypatch globally (which can be risky in bigger projects or notebooks), you can subclass <code>DataFrame</code>:</p>
<pre><code>class LoggedDataFrame(pd.DataFrame):
    def drop_duplicates(self, *args, **kwargs):
        n_before = self.shape[0]
        result = super().drop_duplicates(*args, **kwargs)
        n_after = result.shape[0]
        print(f&quot;Dropped {n_before - n_after} duplicate rows&quot;)
        return result

# Usage
df = LoggedDataFrame({&quot;x&quot;: [1, 1, 2, 2, 3]})
df = df.drop_duplicates()
</code></pre>
<p>I also looked into <code>pandas-log</code> before, but yeah — looks pretty abandoned. Haven’t seen anything like <code>tidylog</code> that’s actively maintained for pandas. Would love it if someone made a clean utility package for this!</p>
<p>Hope that helps.</p>
","2","Answer"
"79558918","79558903","<p>The error happens because you're trying to assign 3600 timestamps as the index (rows) to a DataFrame that only has 200 rows.
The number of items in the index must match the number of rows in the data, as you say the original data has 200 index and in the error it says 3600, 3600.</p>
","0","Answer"
"79559177","79541633","<pre><code>import pandas as pd

df = pd.DataFrame({'id': ['g0', 'g1', 'g1', 'g2', 'g3', 'g4', 'g4', 'g4']})

aa = (df['id'] != df['id'].shift()).cumsum()

bb  = df.groupby(aa)['id'].transform(
lambda x : x if len(x) == 1 else [f&quot;{x.iloc[0]}_TE{i+1}&quot;  for i in range(len(x))]    
)

'''
0        g0
1    g1_TE1
2    g1_TE2
3        g2
4        g3
5    g4_TE1
6    g4_TE2
7    g4_TE3
Name: id, dtype: object
'''
</code></pre>
","0","Answer"
"79559198","79513766","<p>Use einops :</p>
<pre><code>import numpy as np
import pandas as pd
from einops import repeat

# shape (2,)
x = np.array([1, 2])  

# shape (3,)     
y = np.array([3, 4, 5])     

xGrid  =repeat(x, 'i -&gt; i j', j = len(y))
'''
[[1 1 1]
 [2 2 2]]
'''
yGrid = repeat(y, 'j -&gt; i j', i = len(x))
''' 
[[3 4 5]
 [3 4 5]]
'''
zGrid = xGrid + yGrid

res = np.stack([xGrid.ravel(),yGrid.ravel(),zGrid.ravel()],axis=1)
print(res)
df = pd.DataFrame(res, columns=['x', 'y', 'z'])
print(df)
'''
   x  y  z
0  1  3  4
1  1  4  5
2  1  5  6
3  2  3  5
4  2  4  6
5  2  5  7
'''
</code></pre>
","1","Answer"
"79560150","79560135","<p>You could use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html"" rel=""nofollow noreferrer""><code>str.contains</code></a> to identify the target rows, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.repeat.html"" rel=""nofollow noreferrer""><code>Index.repeat</code></a> to duplicate them, finally <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>groupby.cumcount</code></a> to update the new rows:</p>
<pre><code>N = 3 # number of rows to create
# identify target rows
m = test['type'].str.contains('TestString')
# repeat them
out = test.loc[test.index.repeat(m.mul(N-1).add(1))]
# divide duration
out.loc[m, 'duration'] /= N
# compute the cumcount
cc = out.loc[m].groupby(level=0).cumcount()
# increment the onset
out.loc[m, 'onset'] += cc*5
# add the suffix
out.loc[m, 'type'] += '_'+cc.add(1).astype(str)

# optionally, reset the index
out.reset_index(drop=True, inplace=True)
</code></pre>
<p><em>NB. this assumes that the original index does not have duplicated indices.</em></p>
<p>Output:</p>
<pre><code>   onset  duration              type
0      1         2             Instr
1      3        15          Remember
2     18         5   SocTestString_1
3     23         5   SocTestString_2
4     28         5   SocTestString_3
5     33         2            Rating
6     35         5  SelfTestString_1
7     40         5  SelfTestString_2
8     45         5  SelfTestString_3
9     50        15               XXX
</code></pre>
<p>Intermediates (without updating the original columns and resetting the index):</p>
<pre><code>   onset  duration            type      m    cc  cc*5 _{cc+1}
0      1         2           Instr  False  &lt;NA&gt;  &lt;NA&gt;     NaN
1      3        15        Remember  False  &lt;NA&gt;  &lt;NA&gt;     NaN
2     18        15   SocTestString   True     0     0      _1
2     18        15   SocTestString   True     1     5      _2
2     18        15   SocTestString   True     2    10      _3
3     33         2          Rating  False  &lt;NA&gt;  &lt;NA&gt;     NaN
4     35        15  SelfTestString   True     0     0      _1
4     35        15  SelfTestString   True     1     5      _2
4     35        15  SelfTestString   True     2    10      _3
5     50        15             XXX  False  &lt;NA&gt;  &lt;NA&gt;     NaN

</code></pre>
","1","Answer"
"79560527","79557064","<p>Your C series:</p>
<pre><code>In [173]: df['c']
Out[173]: 
0    nan
1    NaN
Name: c, dtype: object

In [174]: df['c'].values
Out[174]: array(['nan', nan], dtype=object)
</code></pre>
<p>It's a mix of string and numpy float <code>nan</code>.</p>
<p>Converted to numpy float:</p>
<pre><code>In [175]: df['c'].astype('float')
Out[175]: 
0   NaN
1   NaN
Name: c, dtype: float64
</code></pre>
<p>But with the pandas Dtype, the <code>np.nan</code> is converted to <code>&lt;NA&gt;</code>, which I guess is the Float64 dtypes own 'null' value.  The <code>pd.NA</code> in [0,'b'] displays this way.</p>
<pre><code>In [176]: df['c'].astype('Float64')
Out[176]: 
0     NaN
1    &lt;NA&gt;
Name: c, dtype: Float64
</code></pre>
<p><code>isnull/isna</code> applied to this <code>float64</code> Series gives the unexpected <code>False</code> for the <code>np.nan</code> value:</p>
<pre><code>In [177]: pd.isna(_176)
Out[177]: 
0    False
1     True
Name: c, dtype: bool
</code></pre>
<p>Even though individually it returns <code>True</code>:</p>
<pre><code>In [178]: df['c'].astype('Float64').loc[0]
Out[178]: np.float64(nan)

In [179]: df['c'].astype('Float64').loc[1]
Out[179]: &lt;NA&gt;

In [180]: pd.isnull(_179)
Out[180]: True
</code></pre>
<p>My guess is that when evaluating a 'Float64' Series, <code>isnull</code> is only checking for the <code>&lt;NA&gt;</code> value.  I'm not familiar enough with pandas Dtypes to say whether this is intentional or a bug.</p>
<p>ps</p>
<p><code>Float64</code> can convert <code>None</code> to <code>pd.NA</code>, but <code>float</code> can't.</p>
","2","Answer"
"79561782","79561773","<p>You can't export colors to a text/CSV file. Furthermore, you ran your command after  <code>to_csv</code>, which wouldn't affect it if this was supported.</p>
<p>You can export as <strong>xlsx</strong> by chaining the <code>style.map</code> to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.to_excel.html"" rel=""nofollow noreferrer""><code>to_excel</code></a>:</p>
<pre><code>(
    df.style.map(
        lambda x: 'background-color: green' if x == 'Pass' else '',
        subset=['Status'],
    ).to_excel('color_test.xlsx')
)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/DX6nca4E.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DX6nca4E.png"" alt=""enter image description here"" /></a></p>
","2","Answer"
"79562412","79562359","<p>You could sort by the hour and the cumcount of the timestamp groups to programmatically get the order that's accepted by <code>tz_localize</code>. Of course this would need to be slightly adapted if you have multiple days or even years of data, but the idea remains the same.</p>
<p>If so desired you could finally sort by the original number col (or whatever dedicated serial index) to restore the original order.</p>
<pre><code>import pandas as pd

data = [
    [&quot;2024-10-27T00:00&quot;, 1, &quot;A&quot;],
    [&quot;2024-10-27T00:15&quot;, 2, &quot;B&quot;],
    [&quot;2024-10-27T00:30&quot;, 3, &quot;C&quot;],
    [&quot;2024-10-27T00:45&quot;, 4, &quot;D&quot;],
    [&quot;2024-10-27T01:00&quot;, 5, &quot;E&quot;],
    [&quot;2024-10-27T01:15&quot;, 6, &quot;F&quot;],
    [&quot;2024-10-27T01:30&quot;, 7, &quot;G&quot;],
    [&quot;2024-10-27T01:45&quot;, 8, &quot;H&quot;],
    [&quot;2024-10-27T02:00&quot;, 9, &quot;I&quot;],
    [&quot;2024-10-27T02:00&quot;, 13, &quot;M&quot;],
    [&quot;2024-10-27T02:15&quot;, 10, &quot;J&quot;],
    [&quot;2024-10-27T02:15&quot;, 14, &quot;N&quot;],
    [&quot;2024-10-27T02:30&quot;, 11, &quot;K&quot;],
    [&quot;2024-10-27T02:30&quot;, 15, &quot;O&quot;],
    [&quot;2024-10-27T02:45&quot;, 12, &quot;L&quot;],
    [&quot;2024-10-27T02:45&quot;, 16, &quot;P&quot;],
    [&quot;2024-10-27T03:00&quot;, 17, &quot;Q&quot;],
    [&quot;2024-10-27T03:15&quot;, 18, &quot;R&quot;],
    [&quot;2024-10-27T03:30&quot;, 19, &quot;S&quot;],
    [&quot;2024-10-27T03:45&quot;, 20, &quot;T&quot;],
]

df = pd.DataFrame(data, columns=[&quot;timestamp&quot;, &quot;number&quot;, &quot;letter&quot;])

df[&quot;timestamp&quot;] = pd.to_datetime(df[&quot;timestamp&quot;])

df = df.assign(
    cumcount=lambda df: df.groupby(&quot;timestamp&quot;).cumcount(),
    hour=df.timestamp.dt.hour
    ).sort_values([&quot;hour&quot;, &quot;cumcount&quot;])

df.set_index(&quot;timestamp&quot;, inplace=True)

df.index = df.index.tz_localize('Europe/Amsterdam',ambiguous='infer')

df
</code></pre>
","1","Answer"
"79562750","79519830","<p>Here is an option:</p>
<pre><code>df['a'].rolling(3).agg(lambda x: x.where(x.ne(0)).bfill().ffill().iloc[-1]).shift().fillna(0)
</code></pre>
<p>Output:</p>
<pre><code>0     0.0
1     0.0
2     0.0
3     1.0
4    -1.0
5    -1.0
6    -1.0
7    -1.0
8     0.0
9     0.0
10    0.0
11   -1.0
12   -1.0
13   -1.0
14    1.0
</code></pre>
","0","Answer"
"79562804","79562296","<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.DateOffset.html"" rel=""nofollow noreferrer""><code>DateOffset</code></a>'s <code>weekday</code> parameter does <strong>not</strong> shift the timestamp to the previous/next value, instead it replaces the day in the current week to that of this week's Sunday (weeks are Mon-Sun by default):</p>
<pre><code>         date                   week   week_Sun real_last_Sun
0  2025-01-31  2025-01-27/2025-02-02 2025-02-02    2025-01-26
1  2025-02-28  2025-02-24/2025-03-02 2025-03-02    2025-02-23
2  2025-03-31  2025-03-31/2025-04-06 2025-04-06    2025-03-30
3  2025-04-30  2025-04-28/2025-05-04 2025-05-04    2025-04-27
4  2025-05-31  2025-05-26/2025-06-01 2025-06-01    2025-05-25
5  2025-06-30  2025-06-30/2025-07-06 2025-07-06    2025-06-29
6  2025-07-31  2025-07-28/2025-08-03 2025-08-03    2025-07-27
7  2025-08-31  2025-08-25/2025-08-31 2025-08-31    2025-08-31
8  2025-09-30  2025-09-29/2025-10-05 2025-10-05    2025-09-28
9  2025-10-31  2025-10-27/2025-11-02 2025-11-02    2025-10-26
10 2025-11-30  2025-11-24/2025-11-30 2025-11-30    2025-11-30
11 2025-12-31  2025-12-29/2026-01-04 2026-01-04    2025-12-28
</code></pre>
<p>If you want the last Sunday of the month, you have to handle the different cases. If the last day of the month is the one you want, then pick it, else you have to pick the previous week's Sunday. There are different ways to do this, for example using a period:</p>
<pre><code>def last_day(timestamp, day='SUN'):
    end_week = timestamp.to_period(f'W-{day}').end_time.normalize()
    if end_week.month != timestamp.month:
        return end_week-pd.DateOffset(days=7)
    return end_week
</code></pre>
<p>Output:</p>
<pre><code>         date                   week   week_Sun   last_Sun   last_Mon
0  2025-01-31  2025-01-27/2025-02-02 2025-02-02 2025-01-26 2025-01-27
1  2025-02-28  2025-02-24/2025-03-02 2025-03-02 2025-02-23 2025-02-24
2  2025-03-31  2025-03-31/2025-04-06 2025-04-06 2025-03-30 2025-03-31
3  2025-04-30  2025-04-28/2025-05-04 2025-05-04 2025-04-27 2025-04-28
4  2025-05-31  2025-05-26/2025-06-01 2025-06-01 2025-05-25 2025-05-26
5  2025-06-30  2025-06-30/2025-07-06 2025-07-06 2025-06-29 2025-06-30
6  2025-07-31  2025-07-28/2025-08-03 2025-08-03 2025-07-27 2025-07-28
7  2025-08-31  2025-08-25/2025-08-31 2025-08-31 2025-08-31 2025-08-25
8  2025-09-30  2025-09-29/2025-10-05 2025-10-05 2025-09-28 2025-09-29
9  2025-10-31  2025-10-27/2025-11-02 2025-11-02 2025-10-26 2025-10-27
10 2025-11-30  2025-11-24/2025-11-30 2025-11-30 2025-11-30 2025-11-24
11 2025-12-31  2025-12-29/2026-01-04 2026-01-04 2025-12-28 2025-12-29
</code></pre>
","-1","Answer"
"79563157","79563104","<p><code>isna</code> is not an aggregating method of SeriesGroupby object. See <a href=""https://pandas.pydata.org/docs/reference/groupby.html#seriesgroupby-computations-descriptive-stats"" rel=""nofollow noreferrer"">docs</a>  Try this:</p>
<pre><code>no_transactions = df['transaction_id'].isna()
df.loc[no_transactions].groupby('customer_id', as_index=False)['visit_id'].count()
</code></pre>
<p>Output:</p>
<pre><code>   customer_id  visit_id
0           30         1
1           54         2
2           96         1
</code></pre>
","0","Answer"
"79563159","79563104","<p>The first step is to outer join both dataframes so that all data is aligned in a single dataframe:</p>
<pre><code>df = pd.merge(visits, transactions, on='visit_id', how='outer')
</code></pre>
<p>When you do that visits that do not have an associated transaction_id will have their transaction_id values filled with a null value. We can use that to make a mask of the values we want:</p>
<pre><code>
mask = df[&quot;transaction_id&quot;].isnull()
</code></pre>
<p>With that mask being complete we can then apply that to our joined df to get a df with only non-transaction vists</p>
<pre><code>non_transaction_visits = df[mask]
</code></pre>
<p>Since the problem is only asking to count how many times each customer was there without transactions we just need to count the number of times that each customer_id is present in this table.</p>
<pre><code>print(non_transaction_visits['customer_id'].value_counts())
</code></pre>
","0","Answer"
"79563321","79563265","<pre><code>result = pd.DataFrame(index=titles_df.index)
# filter out duplicate entity names
# this might be unnecessary, but I do not know your dataset
entity_names = names_df[entity_col].unique()

# precompile regex patterns so we can reuse them during the loop
patterns = {entity: re.compile(r'\b' + re.escape(entity) + r'\b') 
            for entity in entity_names}

# create a boolean mask for each entity name, True if it appears in the title
for entity, pattern in patterns.items():
    # using the precompiled regex and pandas str.contains for searching string columns
    result[entity] = titles_df[title_col].str.contains(pattern, regex=True)

# join that with entity names where True
# this is far faster than apply() in a loop
extracted_entities = result.apply(
    lambda x: ', '.join(x[x].index), axis=1
)
</code></pre>
<p>The main oversight in your code was not using the vectorized .str.contains method. It is precompiled C code, and searches all the rows 'at once' with a single call, so we can avoid a Python loop that has a relatively massive overhead. Also, your original code recompiles the regular expression for each name for every title. As I understand your desired result, this wastes time because the names do not change so the regex does not change either. We can compile the regex for each name a single time and use it repeatedly.</p>
<p>Thank you for providing what you had done so far rather than just describing the problem you were having and expecting someone to do it for you. Even if you had described your use case perfectly, the provided code made it a lot clearer what you were trying to accomplish and why you were having issues.</p>
<p>Please respond with a comment if you are still having issues with code performance. There are libraries that might provide more performant methods for doing this, but implementing them is much more complicated than the above optimizations and I think these changes will be the largest improvement relative to the effort to implement them.</p>
","0","Answer"
"79563673","79563265","<p>Your code is slow because you are recreating the pattern for each row, even though it it the same, and because you are using <code>apply</code>.</p>
<p>You can speed this up by defining the regex only once:</p>
<pre><code>import re

pattern = r'\b(%s)\b' % '|'.join(map(re.escape, Names['ENTITY_NAME']))
# '\\b(XYZ|ABC|NGA|METRO|DPAC)\\b'

Titles['extracted_entity_name'] = Titles['title'].str.findall(pattern).str.join(', ')
</code></pre>
<p>Depending on the length of the DataFrame, you could speed it up using a list comprehension:</p>
<pre><code>Titles['extracted_entity_name'] = [', '.join(re.findall(pattern, s))
                                   for s in Titles['title']]
</code></pre>
<p>Output:</p>
<pre><code>                                      title extracted_entity_name
0  testing some text XYZ testing some text.                   XYZ
1                 XYZ, ABC some random text              XYZ, ABC
2              some text DPAC random random                  DPAC
</code></pre>
<p>Timings:</p>
<pre><code># 3 rows: str.findall + str.join
255 µs ± 1.11 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
# 3 rows: list comprehension
75.6 µs ± 371 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

# 300 rows: str.findall + str.join
617 µs ± 11.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
# 300 rows: list comprehension
570 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

# 30k rows: str.findall + str.join
34.1 ms ± 347 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
# 30k: list comprehension
50.3 ms ± 208 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

# 300k rows: str.findall + str.join
352 ms ± 5.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
# 300k: list comprehension
508 ms ± 2.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
","3","Answer"
"79565122","79565121","<p>I found out that the problem is specifically triggered by the import order of <code>tensorflow</code> and <code>pandas</code>. If <code>pandas</code> package is imported <em>after</em> the import of <code>tensorflow</code>, even if <code>pandas</code> is not used in the subsequent code, the execution of <code>next()</code> runs into an infinite loop.</p>
<p>I would like to know if anybody encounters the same problem and can explain why the import order is causing this issue.</p>
","1","Answer"
"79566124","79566113","<p>The output of <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.SeriesGroupBy.sum.html"" rel=""nofollow noreferrer""><code>groupby.sum</code></a> on a single column is a Series.</p>
<p>If you want to get a <code>DataFrame</code> as output of the <code>groupby.sum</code>, pass <code>as_index=False</code> to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a>:</p>
<pre><code>df.groupby('Region', as_index=False)['Sale'].sum()
</code></pre>
<p>Output:</p>
<pre><code>          Region    Sale
0           Asia  620012
1         Europe  120225
2  North America  195356
</code></pre>
<p>If you want the top region (the region with the maximum sum), you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.idxmax.html"" rel=""nofollow noreferrer""><code>idxmax</code></a>:</p>
<pre><code>df.groupby('Region')['Sale'].sum().idxmax()
</code></pre>
<p>Output: <code>Asia</code></p>
<p><em>NB. If you want the first &quot;group&quot;, then it is simply the smallest lexicographically sorted value in Region. There is no need to compute a <code>groupby.sum</code> for that, just <code>df['Region'].min()</code>. And for the unique groups: <code>sorted(df['Region'].unique())</code>.</em></p>
","-1","Answer"
"79566134","79566113","<p>The issue arises because after calling:</p>
<pre><code>df2 = df.groupby('Region')['Sale'].sum()
</code></pre>
<p>df2 is a <strong>Series</strong>, not a DataFrame. The <code>'Region'</code> column becomes the <strong>index</strong>, not a regular column anymore, so trying to access <code>df2[['Region']]</code> throws an error since <code>'Region'</code> is not a column.</p>
<p>Here's what you can do, reset the index to convert the index back to a column</p>
<pre><code>df2 = df.groupby('Region')['Sale'].sum().reset_index()
print(df2)
</code></pre>
<p>Now, 'Region' is a normal column in df2, and you can access it like this:</p>
<pre><code>region_list = df2['Region'].tolist()
</code></pre>
","0","Answer"
"79566140","79566113","<p>alright bro, let me explain<br />
You are doing it like this</p>
<pre><code>df2 = df.groupby('Region')['Sale'].sum()
</code></pre>
<p>and then you trying to access</p>
<pre><code>df2[['Region']]
</code></pre>
<p>and it returns a series with as &quot;Region&quot; the index</p>
<p>Try like this:</p>
<pre><code>df2 = df.groupby('Region')['Sale'].sum().reset_index()
</code></pre>
<p>I think it should work since u want DataFrame</p>
","0","Answer"
"79566891","79562296","<p>weekdays are:<br />
0: Monday<br />
1: Tuesday<br />
2: Wednesday<br />
3: Thursday<br />
4: Friday<br />
5: Saturday<br />
6: Sunday</p>
<pre><code>import pandas as pd
import calendar

def last_weekday_of_month(year, month, weekday):
    last_day = pd.Timestamp(year, month, calendar.monthrange(year, month)[1]) 
    days_to_subtract = (last_day.weekday() - weekday) % 7 
    return last_day - pd.DateOffset(days=days_to_subtract)
print(last_weekday_of_month(2025, 12, 6)) 
</code></pre>
<p>Output:
2025-12-28 00:00:00</p>
","1","Answer"
"79567551","79567429","<p>IIUC, you can do this with this command, this is one of the reason I like to use the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_axis.html#pandas-dataframe-set-axis"" rel=""nofollow noreferrer""><code>set_axis</code></a> method in dataframes.</p>
<pre><code>table_name = 'table_1'
df[list(mapping_dict[table_name].values())+['values']].set_axis(list(mapping_dict[table_name].keys())+['values'], axis=1)
</code></pre>
<p>Output:</p>
<pre><code>   id    dt_start      dt_end  values
0   1  2025-10-01  2025-10-01    52.3
1   2  2025-10-02  2025-10-02    60.0
2   3  2025-10-01  2025-10-01    70.6

</code></pre>
<p>Or,</p>
<pre><code>table_name = 'table_2'
df[list(mapping_dict[table_name].values())+['values']].set_axis(list(mapping_dict[table_name].keys())+['values'], axis=1)
</code></pre>
<p>Output:</p>
<pre><code>   id    dt_start      dt_end  values
0   0  2025-02-10  2025-02-10    52.3
1   5  2025-02-15  2025-02-15    60.0
2   4  2025-02-20  2025-02-20    70.6
</code></pre>
<p>And, much like @khushalvaland you can create function for resuse:</p>
<pre><code>def gen_table(df, mapping, cols) -&gt; pd.DataFrame:
    return (df[list(mapping.values())+cols]
              .set_axis(list(mapping.keys())+cols, 
                        axis=1))

gen_table(df, mapping = mapping_dict['table_1'], cols =['values'])
gen_table(df, mapping = mapping_dict['table_2'], cols =['values'])
</code></pre>
<p>Output:</p>
<pre><code>   id    dt_start      dt_end  values
0   1  2025-10-01  2025-10-01    52.3
1   2  2025-10-02  2025-10-02    60.0
2   3  2025-10-01  2025-10-01    70.6
</code></pre>
<p>and</p>
<pre><code>   id    dt_start      dt_end  values
0   0  2025-02-10  2025-02-10    52.3
1   5  2025-02-15  2025-02-15    60.0
2   4  2025-02-20  2025-02-20    70.6
</code></pre>
","3","Answer"
"79568073","79567767","<p>Is Ther not missing the transformation of type.</p>
<pre><code>def monthly_transactions(transactions: pd.DataFrame) -&gt; pd.DataFrame:
    &quot;&quot;&quot;
    convert 'trans_date'-column of DataFrames into format 'JJJJ-MM'.

    Args:
        transactions (pd.DataFrame): A DataFrame with column name 'trans_date' in date format.

    Returns:
        pd.DataFrame: A DataFrame with 'trans_date'-column im format 'JJJJ-MM' (as string).
    &quot;&quot;&quot;
    transactions['trans_date'] = pd.to_datetime(transactions['trans_date'])
    transactions['trans_date'] = transactions['trans_date'].dt.to_period('M').astype(str)
    return transactions
</code></pre>
","-1","Answer"
"79568702","79568474","<pre><code>df['new']=df[['col1','col2']].apply(lambda x: 1 if len(set(x['col1'].split('|')).intersection(set(x['col2'].split('|')))) &gt;=1 else 0,axis=1)
</code></pre>
","2","Answer"
"79568923","79568474","<p>I would ameloriate your code as follows</p>
<pre><code>import pandas as pd
df=pd.DataFrame({'col1':['a|b|c','a|b|c','a|b|c'],'col2':['b|d|e|a','d|e|f','a']})
df['new']=df[['col1','col2']].apply(lambda x: int(bool(set(x['col1'].split('|')).intersection(set(x['col2'].split('|'))))),axis=1)
print(df)
</code></pre>
<p>gives output</p>
<pre><code>    col1     col2  new
0  a|b|c  b|d|e|a    1
1  a|b|c    d|e|f    0
2  a|b|c        a    1
</code></pre>
<p>Explanation: <code>bool</code> gives True if set is not empty else False, then <code>int</code> is used to convert True to 1 and False 0.</p>
","0","Answer"
"79569251","79569195","<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html"" rel=""nofollow noreferrer""><code>groupby.filter</code></a> filters a full group based on its members.</p>
<p>What you want is simply to filter rows. You do not need <code>groupby</code>:</p>
<pre><code>out = my_df[my_df['col_2'].le(1)]
</code></pre>
<p>Output:</p>
<pre><code>  col_1  col_2
0     A      1
2     B      1
4     C      1
</code></pre>
","3","Answer"
"79569520","79569500","<p>A quick an easy approach, if you know the categories, would be to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>reindex</code></a>:</p>
<pre><code>(df_sample.reindex(['pes', 'tes', 'des'], level=1)
          .plot(kind='bar', stacked=True)
)
</code></pre>
<p>A more canonical (but more complex) approach would be to make the second level an ordered <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Categorical.html"" rel=""nofollow noreferrer""><code>Categorical</code></a>:</p>
<pre><code>order = pd.CategoricalDtype(['pes', 'tes', 'des'], ordered=True)

(df_sample
 .set_axis(pd.MultiIndex.from_frame(df_sample.index.to_frame()
                                             .astype({1: order}),
                                    names=[None, None]))
 .sort_index()
 .plot(kind='bar', stacked=True)
)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/JpG5aiO2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpG5aiO2.png"" alt=""enter image description here"" /></a></p>
","2","Answer"
"79569886","79569831","<p>Here is the function which create 2 samples and then combine them</p>
<pre><code>import pandas as pd

def custom_sample(df):
    location_samples = (
        df.groupby('location', group_keys=False)
          .apply(lambda g: g.sample(n=min(2, len(g)), random_state=42))
    )
    
    color_samples = (
        df.groupby('color', group_keys=False)
          .apply(lambda g: g.sample(n=1, random_state=42))
    )
    
    combined_sample = pd.concat([location_samples, color_samples]).drop_duplicates()
    
    return combined_sample.reset_index(drop=True)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>  character location   color
0     mario   castle     red
1     luigi   castle   green
2    bowser  dungeon    blue
3       boo  dungeon   white
4     yoshi   forest   green
5      toad   forest    blue
6    lakitu   forest  yellow
</code></pre>
","1","Answer"
"79570349","79570345","<p>Searching for a solution to your problem I found <a href=""https://www.kdnuggets.com/how-to-perform-data-aggregation-over-time-series-data-with-pandas"" rel=""nofollow noreferrer"">this</a> article. It says that you can do the aggregation via resampling, for example:</p>
<pre><code>df.resample('Y').mean()
</code></pre>
<p>and you can change the frequencies to</p>
<p>You can change the resample frequencies, such as:</p>
<ul>
<li>D (daily)</li>
<li>W (weekly)</li>
<li>M (monthly)</li>
<li>Q (quarterly)</li>
<li>A (yearly)</li>
</ul>
<p><a href=""https://docs.vultr.com/python/third-party/pandas/DataFrame/aggregate"" rel=""nofollow noreferrer"">Here</a> you can read more about aggregation. The function for average is <code>mean</code>, for maximum is <code>max</code> and for minimum is <code>min</code>.</p>
","0","Answer"
"79570513","79570508","<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.get_dummies.html"" rel=""nofollow noreferrer""><code>str.get_dummies</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.astype.html"" rel=""nofollow noreferrer""><code>astype</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>df['column1'].str.get_dummies(sep=',').astype(bool)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>   some1  some2  some3
0   True   True   True
1  False   True   True
2   True  False  False
3   True  False   True
</code></pre>
<p><strong>Data used</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {'column1': [&quot;some1,some2,some3&quot;, &quot;some2,some3&quot;, &quot;some1&quot;, &quot;some1,some3&quot;]}
df = pd.DataFrame(data)
</code></pre>
","4","Answer"
"79570574","79570561","<p>So, if I understand correctly, when you write &quot;4/10&quot; to Excel it wrongly becomes a date, even after using <code>str()</code>.<br/>
The <code>str()</code> function in Python only changes the data type within Python; it doesn't embed information that forces Excel to treat it as pure text.<br/></p>
<p>But you can control the cell formatting directly</p>
<pre><code>from openpyxl import Workbook

wb = Workbook()
ws = wb.active

data = ['4/10', 'another']

ws['A1'] = 'Material'

for i, value in enumerate(data):
    cell = ws.cell(row=i + 2, column=1)
    cell.value = value
    cell.number_format = '@'  # '@' represents text 

wb.save(&quot;output_openpyxl.xlsx&quot;)
</code></pre>
","4","Answer"
"79570596","79570561","<p>Without seeing the code, I guess you need to format the Excel cells you're writing to, as &quot;text&quot; number format.</p>
<p>To do it with <strong>openpyxl</strong>, take a look at this <a href=""https://stackoverflow.com/a/60004738/17480736"">similar StackOverflow question</a>. @SuperScienceGrl answer suggests this code:</p>
<pre><code>cell = ws['A1']
cell.number_format = '@'
</code></pre>
","1","Answer"
"79570851","79569269","<p>Here's one approach:</p>
<pre class=""lang-py prettyprint-override""><code>cols = ['employeeID', 'groupName', 'vesting_year', 'agreementDate']

out = (
   df.merge(
       df.groupby(cols)['vesting_value_CAD'].sum()
         .groupby(cols[:-1]).cumsum()
         .rename('total_vesting_value_CAD'), 
       on=cols
   )
   .assign(
       total_vesting_value_CAD=lambda x: x['total_vesting_value_CAD'] 
       - x['vesting_value_CAD']
       )
   )
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>   employeeID groupName  agreementID agreementDate  trancheID  vesting_year  \
0           2         A            7    2025-03-01         28          2025   
1           2         A            7    2025-03-01         29          2026   
2           2         A            1    2025-04-01         26          2026   
3           2         A            1    2025-03-01         27          2027   
4           2         A            8    2025-02-01         30          2026   
5           2         B            9    2025-03-01         31          2026   
6           2         A            6    2025-03-01         32          2026   

   vesting_value_CAD  total_vesting_value_CAD  
0                200                        0  
1                300                       90  
2                400                      390  
3                500                        0  
4                 50                        0  
5                 30                        0  
6                 40                      350  
</code></pre>
<p><strong>Explanation / Intermediates</strong></p>
<ul>
<li>Start with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>.groupby</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.sum.html"" rel=""nofollow noreferrer""><code>groupby.sum</code></a> to get the sum per date:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>cols = ['employeeID', 'groupName', 'vesting_year', 'agreementDate']

employeeID  groupName  vesting_year  agreementDate
2           A          2025          2025-03-01       200
                       2026          2025-02-01        50
                                     2025-03-01       340
                                     2025-04-01       400
                       2027          2025-03-01       500
            B          2026          2025-03-01        30
Name: vesting_value_CAD, dtype: int64
</code></pre>
<ul>
<li>Now, chain a <code>groupby</code> on the same columns (now: index levels) except 'agreementDate' (<code>cols[:-1]</code>) + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.cumsum.html"" rel=""nofollow noreferrer""><code>groupby.cumsum</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.rename.html"" rel=""nofollow noreferrer""><code>Series.rename</code></a>:</li>
</ul>
<pre><code>(df.groupby(cols)['vesting_value_CAD'].sum()
   .groupby(cols[:-1]).cumsum()
   .rename('total_vesting_value_CAD'))

employeeID  groupName  vesting_year  agreementDate
2           A          2025          2025-03-01       200
                       2026          2025-02-01        50
                                     2025-03-01       390
                                     2025-04-01       790
                       2027          2025-03-01       500
            B          2026          2025-03-01        30
Name: total_vesting_value_CAD, dtype: int64
</code></pre>
<ul>
<li>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>.merge</code></a> on <code>cols</code> to add to the original <code>df</code> and use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>.assign</code></a> to correct column 'total_vesting_value_CAD' by subtracting column 'vesting_value_CAD'.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>df.merge(...).loc[:, ['vesting_value_CAD', 'total_vesting_value_CAD']]

   vesting_value_CAD  total_vesting_value_CAD
0                200                      200 # 200 - 200 = 0
1                300                      390 # 390 - 300 = 90
2                400                      790 # 790 - 400 = 390
3                500                      500 # etc.
4                 50                       50
5                 30                       30
6                 40                      390
</code></pre>
","2","Answer"
"79572558","79554601","<p>The problem was decimal type values.</p>
<p>They were exported with point and not comma</p>
<p>Conventing them to double, i resolved hte problem</p>
","0","Answer"
"79573047","79573037","<p>Just specify the folder in your directory.</p>
<pre><code>subset.to_csv('output_data/false_relationships.csv', index=False, header=True, encoding='utf-8')
</code></pre>
<p>Or you can specify the absolute path</p>
<pre><code>subset.to_csv('/absolute-path/output_data/false_relationships.csv', index=False, header=True, encoding='utf-8')
</code></pre>
<p>If you need to join paths you can use <code>os.path</code></p>
<pre><code>import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

filepath = 'files/one.txt'
request_path = os.path.join(BASE_DIR, filepath)
</code></pre>
","2","Answer"
"79573123","79570345","<p>I think you can use <code>.groupby( )</code> and <code>.PeriodIndex( )</code> and apply it to the data what you want. Here are some examples.</p>
<pre><code>df.groupby(pd.PeriodIndex(df['Year'], freq=&quot;Y&quot;))['Temperature'].mean() # yearly average
df.groupby(pd.PeriodIndex(df['Month'], freq=&quot;M&quot;))['Temperature'].mean() # monthly average
df.groupby(pd.PeriodIndex(df['Day'], freq=&quot;D&quot;))['Temperature'].mean() # daily average

df.groupby(pd.PeriodIndex(df['Year'], freq=&quot;Y&quot;))['Temperature'].max() # yearly maximum 
df.groupby(pd.PeriodIndex(df['Month'], freq=&quot;M&quot;))['Temperature'].max() # monthly maximum 
df.groupby(pd.PeriodIndex(df['Day'], freq=&quot;D&quot;))['Temperature'].max() # daily maximum  

df.groupby(pd.PeriodIndex(df['Year'], freq=&quot;Y&quot;))['Temperature'].min() # yearly minimum  
df.groupby(pd.PeriodIndex(df['Month'], freq=&quot;M&quot;))['Temperature'].min() # monthly minimum
df.groupby(pd.PeriodIndex(df['Day'], freq=&quot;D&quot;))['Temperature'].min() # daily minimum
</code></pre>
","0","Answer"
"79573229","79573192","<p>It looks to me that both Series after <code>convert_dtypes</code> behave inconsistently.</p>
<p>By default, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.max.html"" rel=""nofollow noreferrer""><code>max</code></a> uses <code>skipna=True</code>, so it should ignore the NaNs:</p>
<pre><code>sr = pd.Series([1.5, 0.0])
(sr / sr).max()             # 1.0 
(sr / sr).max(skipna=True)  # 1.0 
(sr / sr).max(skipna=False) # np.float64(nan)
</code></pre>
<p>However after <code>convert_dtypes</code> the parameter has no effect:</p>
<pre><code>sr = pd.Series([1.5, 0.0]).convert_dtypes(dtype_backend='pyarrow')
(sr / sr).max(skipna=True)  # 1.0
(sr / sr).max(skipna=False) # 1.0   # should be NA

sr = pd.Series([1.5, 0.0]).convert_dtypes()
(sr / sr).max(skipna=True)  # &lt;NA&gt;  # should be 1.0
(sr / sr).max(skipna=False) # &lt;NA&gt;
</code></pre>
<p>I'd consider opening a bug report.</p>
","0","Answer"
"79574244","79572227","<p>TL;DR use <code>Index[pa.engines.pandas_engine.Date]</code></p>
<p>Pandera as of now does not support <code>datetime.date</code> series data type, but it has a semantic representation of a date type column for each library (pandas, polars, pyarrow etc). Date type for <code>pandas.DataFrame</code>s is <code>pa.engines.pandas_engine.Date</code> , for the others you can see the <a href=""https://pandera.readthedocs.io/en/stable/reference/dtypes.html#data-types"" rel=""nofollow noreferrer"">API docs</a>.</p>
<p>From the <a href=""https://pandera.readthedocs.io/en/stable/reference/generated/pandera.engines.pandas_engine.Date.html#pandera.engines.pandas_engine.Date"" rel=""nofollow noreferrer"">pandera documentation</a>:</p>
<blockquote>
<p>class <strong>pandera.engines.pandas_engine.Date(<em>to_datetime_kwargs=None</em>)</strong></p>
<p>Semantic representation of a date data type.</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code># schema.py
import pandera as pa
from pandera.typing import Index

class DateIndexModel(pa.DataFrameModel):
    date: Index[pa.engines.pandas_engine.Date]
</code></pre>
","1","Answer"
"79574814","79574803","<p>Your question is not fully clear. If you want the first and last row of each group (assuming the dates are sorted) you could go with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>stack</code></a>:</p>
<pre><code>(df.groupby(level=['Date', 'TagId']).agg(['first', 'last'])
   .stack(future_stack=True)
)
</code></pre>
<p>Output:</p>
<pre><code>                                        X         Y                     Time
Date       TagId                                                            
01/06/2022 0x24025F44AD21 first  3.121672  5.188564  1900-01-01 08:37:17.000
                          last   3.121672  5.188564  1900-01-01 08:37:17.800
           0x24046130B076 first  7.611438  2.346377  1900-01-01 12:31:05.600
                          last   7.533170  2.035381  1900-01-01 12:31:06.400
</code></pre>
<p>If you're not sure they are, you could use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a> and <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a>:</p>
<pre><code>df.loc[df.groupby(level=['Date', 'TagId'])['Time']
         .transform(lambda x: x.isin([x.min(), x.max()]))]
</code></pre>
<p>Output:</p>
<pre><code>                                  X         Y                     Time
Date       TagId                                                      
01/06/2022 0x24025F44AD21  3.121672  5.188564  1900-01-01 08:37:17.000
           0x24025F44AD21  3.121672  5.188564  1900-01-01 08:37:17.800
           0x24046130B076  7.611438  2.346377  1900-01-01 12:31:05.600
           0x24046130B076  7.533170  2.035381  1900-01-01 12:31:06.400
</code></pre>
","0","Answer"
"79575838","79575173","<p>You need to format the string to the given format.</p>
<pre><code>df_pandas['Reported'] = pd.to_datetime(df_pandas['Reported'])
df_pandas['Reported'] = df_pandas['Reported'].dt.strftime('%d/%m/%Y %H:%M:%S')
</code></pre>
","0","Answer"
"79575940","79575741","<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.mean.html"" rel=""nofollow noreferrer""><code>df.groupby(...).mean(...)</code></a> for this</p>
<pre><code>series_metric_mean = df.groupby('Metric').mean(['_BETTER', '_SAME', '_WORSE'])
</code></pre>
<pre class=""lang-none prettyprint-override""><code>            _BETTER     _SAME    _WORSE
Metric                                 
f1-score   0.260784  0.613438  0.588507
precision  0.291267  0.605598  0.606261
recall     0.241546  0.621743  0.585389
</code></pre>
<p>(to keep <code>f1-score</code> as last row it needs <code>groupby(..., sort=False)</code>)</p>
<hr />
<p>It gives <code>Series</code> only with <code>_BETTER, _SAME, _WORSE</code> (and <code>Metric</code> as index) so it needs to</p>
<ul>
<li>add <code>__dataset</code> with <code>All</code>.</li>
<li>add suffix <code>_avg</code> in <code>Metric</code> (at this moment it is still as index).</li>
<li>convert it to <code>DataFrame()</code> (using <code>reset_index()</code> -  it converts index <code>Metric</code> to normal column).</li>
<li><code>concat</code>enate it to original data.</li>
</ul>
<pre><code>series_metric_mean['___dataset'] = 'All'

series_metric_mean.index = series_metric_mean.index + '_avg'

df_metric_mean = series_metric_mean.reset_index()

df = pd.concat([df, df_metric_mean]).reset_index(drop=True)
</code></pre>
<hr />
<p>Full working code with example data as text.<br />
I use <code>io</code> only to create file-like object so everyone can simply copy and run it</p>
<pre><code>text1 = &quot;&quot;&quot;_BETTER     _SAME    _WORSE   ___dataset     Metric
  0.373802  0.816794  0.568783      Train      precision
  0.391304  0.865229  0.519324      Train      recall
  0.382353  0.840314  0.542929      Train      f1-score
  0.500000  1.000000  0.583333      Val        precision
  0.333333  1.000000  0.736842      Val        recall
  0.400000  1.000000  0.651163      Val        f1-score
  0.000000  0.000000  0.666667      Test       precision
  0.000000  0.000000  0.500000      Test       recall
  0.000000  0.000000  0.571429      Test       f1-score
&quot;&quot;&quot;

# example from @Adeva1 answer 
text2 = &quot;&quot;&quot;_BETTER     _SAME    _WORSE ___dataset     Metric
0.568783  0.568783  0.568783    Train  precision
0.519324  0.519324  0.519324    Train     recall
0.542929  0.542929  0.542929    Train   f1-score
0.583333  0.583333  0.583333      Val  precision
0.736842  0.736842  0.736842      Val     recall
0.651163  0.651163  0.651163      Val   f1-score
0.651163  0.651163  0.651163     Test  precision
0.500000  0.500000  0.500000     Test     recall
0.571429  0.571429  0.571429     Test   f1-score
&quot;&quot;&quot;

import pandas as pd
import io

df = pd.read_csv(io.StringIO(text1), sep='\\s+')
#print(df)

series_metric_mean = df.groupby('Metric', sort=False).mean(['_BETTER', '_SAME', '_WORSE'])

series_metric_mean['___dataset'] = 'All'
series_metric_mean.index = series_metric_mean.index + '_avg'
print(series_metric_mean)

df_metric_mean = series_metric_mean.reset_index()
print(df_metric_mean)

df = pd.concat([df, df_metric_mean]).reset_index(drop=True)
print(df)
</code></pre>
<p>Result (for <code>text1</code>)</p>
<pre class=""lang-none prettyprint-override""><code>                _BETTER     _SAME    _WORSE ___dataset
Metric                                                
f1-score_avg   0.260784  0.613438  0.588507        All
precision_avg  0.291267  0.605598  0.606261        All
recall_avg     0.241546  0.621743  0.585389        All

          Metric   _BETTER     _SAME    _WORSE ___dataset
0   f1-score_avg  0.260784  0.613438  0.588507        All
1  precision_avg  0.291267  0.605598  0.606261        All
2     recall_avg  0.241546  0.621743  0.585389        All

     _BETTER     _SAME    _WORSE ___dataset         Metric
0   0.373802  0.816794  0.568783      Train      precision
1   0.391304  0.865229  0.519324      Train         recall
2   0.382353  0.840314  0.542929      Train       f1-score
3   0.500000  1.000000  0.583333        Val      precision
4   0.333333  1.000000  0.736842        Val         recall
5   0.400000  1.000000  0.651163        Val       f1-score
6   0.000000  0.000000  0.666667       Test      precision
7   0.000000  0.000000  0.500000       Test         recall
8   0.000000  0.000000  0.571429       Test       f1-score
9   0.291267  0.605598  0.606261        All  precision_avg
10  0.241546  0.621743  0.585389        All     recall_avg
11  0.260784  0.613438  0.588507        All   f1-score_avg
</code></pre>
<p>Result (for <code>text2</code> with example from @Adeva1 answer)</p>
<pre class=""lang-none prettyprint-override""><code>            _BETTER     _SAME    _WORSE
Metric                                 
precision  0.601093  0.601093  0.601093
recall     0.585389  0.585389  0.585389
f1-score   0.588507  0.588507  0.588507

                _BETTER     _SAME    _WORSE ___dataset
Metric                                                
precision_avg  0.601093  0.601093  0.601093        All
recall_avg     0.585389  0.585389  0.585389        All
f1-score_avg   0.588507  0.588507  0.588507        All

          Metric   _BETTER     _SAME    _WORSE ___dataset
0  precision_avg  0.601093  0.601093  0.601093        All
1     recall_avg  0.585389  0.585389  0.585389        All
2   f1-score_avg  0.588507  0.588507  0.588507        All

     _BETTER     _SAME    _WORSE ___dataset         Metric
0   0.568783  0.568783  0.568783      Train      precision
1   0.519324  0.519324  0.519324      Train         recall
2   0.542929  0.542929  0.542929      Train       f1-score
3   0.583333  0.583333  0.583333        Val      precision
4   0.736842  0.736842  0.736842        Val         recall
5   0.651163  0.651163  0.651163        Val       f1-score
6   0.651163  0.651163  0.651163       Test      precision
7   0.500000  0.500000  0.500000       Test         recall
8   0.571429  0.571429  0.571429       Test       f1-score
9   0.601093  0.601093  0.601093        All  precision_avg
10  0.585389  0.585389  0.585389        All     recall_avg
11  0.588507  0.588507  0.588507        All   f1-score_avg
</code></pre>
<hr />
<p>Doc:</p>
<ul>
<li><a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby"" rel=""nofollow noreferrer"">pandas.DataFrame.groupby</a></li>
<li><a href=""https://pandas.pydata.org/docs/user_guide/merging.html"" rel=""nofollow noreferrer"">Merge, join, concatenate and compare</a></li>
</ul>
","3","Answer"
"79576142","79576076","<p>You can achieve this without loops or custom functions by using a self-merge and filtering with vectorized conditions in pandas.</p>
<pre><code>import pandas as pd

# original data
df = pd.DataFrame({
    'employeeID': [2, 2, 2, 2, 2],
    'groupName': ['A', 'A', 'A', 'A', 'B'],
    'agreementID': [8, 7, 6, 1, 9],
    'agreementDate': ['2/1/2025', '3/1/2025', '3/1/2025', '1/1/2025', '3/1/2025'],
    'vesting_value_CAD': [50, 200, 40, 500, 30],
    'expiryDate': ['3/31/2025', None, None, '1/31/2025', None]
})

# convert dates
df['agreementDate'] = pd.to_datetime(df['agreementDate'])
df['expiryDate'] = pd.to_datetime(df['expiryDate'])

# self merge on employeeID, groupName and agreementDate
merged = df.merge(df, on=['employeeID', 'groupName', 'agreementDate'], suffixes=('', '_other'))

# filter: expiryDate_other &gt;= agreementDate and different agreementID
condition = (
    (merged['expiryDate_other'] &gt;= merged['agreementDate']) &amp;
    (merged['agreementID'] != merged['agreementID_other'])
)

# apply condition
filtered = merged[condition]

# sum vesting_value_CAD_other by group
result = (
    filtered
    .groupby(['employeeID', 'groupName', 'agreementDate'])['vesting_value_CAD_other']
    .sum()
    .reset_index(name='conditional_sum')
)

# merge with original DataFrame
final_df = df.merge(result, on=['employeeID', 'groupName', 'agreementDate'], how='left')
final_df['conditional_sum'] = final_df['conditional_sum'].fillna(0)

print(final_df)
</code></pre>
","0","Answer"
"79576171","79576076","<p>I know you asked for pandas, but in Polars this is really simple (I'm assuming in your example that the missing <code>expiryDate</code>s are simply repeated values:</p>
<pre class=""lang-py prettyprint-override""><code>df = pl.DataFrame({
    'employeeID': [2, 2, 2, 2, 2],
    'groupName': ['A', 'A', 'A', 'A', 'B'],
    'agreementID': [8, 7, 6, 1, 9],
    'agreementDate': ['2/1/2025', '3/1/2025', '3/1/2025', '1/1/2025', '3/1/2025'],
    'vesting_value_CAD': [50, 200, 40, 500, 30],
    'expiryDate': ['3/31/2025', '3/31/2025', '3/31/2025', '1/31/2025', '1/31/2025']
})

df = df.lazy().with_columns(pl.col(&quot;^.*Date$&quot;).str.to_date(&quot;%m/%d/%Y&quot;))
out = (df.join(df, on=[&quot;employeeID&quot;, &quot;groupName&quot;, &quot;agreementDate&quot;])
   .filter((pl.col.expiryDate_right &gt;= pl.col.agreementDate) &amp;
           (pl.col.agreementID != pl.col.agreementID_right))
   .group_by([&quot;employeeID&quot;, &quot;groupName&quot;, &quot;agreementDate&quot;])
   .agg(pl.col.vesting_value_CAD_right.sum())
   .collect())
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>┌────────────┬───────────┬───────────────┬─────────────────────────┐
│ employeeID ┆ groupName ┆ agreementDate ┆ vesting_value_CAD_right │
│ ---        ┆ ---       ┆ ---           ┆ ---                     │
│ i64        ┆ str       ┆ date          ┆ i64                     │
╞════════════╪═══════════╪═══════════════╪═════════════════════════╡
│ 2          ┆ A         ┆ 2025-03-01    ┆ 240                     │
└────────────┴───────────┴───────────────┴─────────────────────────┘
</code></pre>
","1","Answer"
"79576306","79576300","<p>Matplotlib does not offer a built-in function in its core library to enable hover effects. For this functionality, you may consider using the <a href=""https://mplcursors.readthedocs.io"" rel=""nofollow noreferrer""><code>mplcursor</code>  library</a>. Kindly try running the code below after installing <code>mplcursors</code>.</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
import mplcursors

x = np.random.rand(20)
y = np.random.rand(20)
colors = np.random.rand(20)
area = (30 * np.random.rand(20))**2

metadata = [f&quot;Point {i}, Value: ({x[i]:.2f}, {y[i]:.2f})&quot; for i in range(len(x))]

fig, ax = plt.subplots()
scatter = ax.scatter(x, y, s=area, c=colors, alpha=0.5)

plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Interactive Scatter Plot')

cursor = mplcursors.cursor(scatter, hover=True)

@cursor.connect(&quot;add&quot;)
def on_add(sel):
    sel.annotation.set_text(metadata[sel.index])

plt.show()
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/BGKRFvzu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BGKRFvzu.png"" alt=""enter image description here"" /></a></p>
","2","Answer"
"79577485","79577437","<p>You need to use <code>pd.read_csv()</code> with the correct delimiter. You can read more about it <a href=""https://www.geeksforgeeks.org/how-to-read-a-csv-file-to-a-dataframe-with-custom-delimiter-in-pandas/"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>import pandas as pd
df = pd.read_csv('C:\\XXXXX\\Python_Learn\\pandas.csv', delimiter='|')
print(df.to_string())
</code></pre>
","2","Answer"
"79577646","79577490","<p>You can create custom transformer, using <code>BaseEstimator</code> and <code>TransformerMixin</code></p>
<p>for example:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler

class GroupScaler(BaseEstimator, TransformerMixin):
    def __init__(self, group_column, scaler=None):
        self.group_column = group_column
        self.scaler = scaler or StandardScaler()
        self.scalers_ = {}

    def fit(self, X, y=None):
        self.scalers_ = {}
        for group, group_data in X.groupby(self.group_column):
            scaler = clone(self.scaler)
            scaler.fit(group_data.drop(columns=[self.group_column]))
            self.scalers_[group] = scaler
        return self

    def transform(self, X):
        X_scaled = []
        for group, group_data in X.groupby(self.group_column):
            scaler = self.scalers_[group]
            scaled = scaler.transform(group_data.drop(columns=[self.group_column]))
            group_df = pd.DataFrame(scaled, index=group_data.index, columns=group_data.columns.drop(self.group_column))
            group_df[self.group_column] = group
            X_scaled.append(group_df)
        return pd.concat(X_scaled).sort_index()

from sklearn.base import clone

df = pd.DataFrame({'group': [1, 1, 1, 2, 2, 2], 'x': [1, 2, 3, 10, 20, 30]})
scaler = GroupScaler(group_column='group')
scaled_df = scaler.fit_transform(df)

print(scaled_df)
</code></pre>
<p>Output:</p>
<pre><code>          x  group
0 -1.224745      1
1  0.000000      1
2  1.224745      1
3 -1.224745      2
4  0.000000      2
5  1.224745      2
</code></pre>
","1","Answer"
"79578300","79578293","<p>You can use <code>df.replace(&quot;?&quot;, pd.NA)</code> to properly encode <code>&quot;?&quot;</code> as missing value. This will ensure that those are properly handled in all operations.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {&quot;x&quot;: [1, 2, &quot;?&quot;], &quot;y&quot;: [3, &quot;?&quot;, 5]}
df = pd.DataFrame(data)

print(df.isnull().sum())
# x    0
# y    0

df = df.replace(&quot;?&quot;, pd.NA)
print(df.isnull().sum())
# x    1
# y    1
</code></pre>
","5","Answer"
"79578325","79578293","<p>You can also specify missing values during DataFrame creation, specially if you are reading this from a file.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.read_csv(&quot;some_file.csv&quot;, na_values=[&quot;?&quot;])
</code></pre>
","3","Answer"
"79578988","79578969","<p>Another possible solution:</p>
<pre><code>df['B'] = df['B'].map(lambda x: {'t': 0} if pd.isna(x) else x)
</code></pre>
<p>Output:</p>
<pre><code>   A         B
0  1  {'t': 1}
1  2  {'t': 2}
2  3  {'t': 0}
3  4  {'t': 0}
</code></pre>
","1","Answer"
"79579301","79578969","<blockquote>
<p>Is there a restriction on what values can be used for fillna?</p>
</blockquote>
<p>After scrying <a href=""https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/generic.py#L7142-L7446"" rel=""nofollow noreferrer"">source code</a> I determined that doing</p>
<pre><code>df.fillna({'B': {'t': 0}})
</code></pre>
<p>does actually call fillna method of pandas.Series (B column) ramming <code>{'t': 0}</code> as its' first argument that is</p>
<pre><code>df['B'].fillna({'t': 0})
</code></pre>
<p>according to <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html"" rel=""nofollow noreferrer""><code>pandas.Series.fillna</code> docs</a> it does assume that each key in dict given denotes index of Series and value is value to use at given index. Consider following simple example</p>
<pre><code>import pandas as pd
s = pd.Series([0.0,float(&quot;NaN&quot;),float(&quot;NaN&quot;)])
print(s.fillna({0: 9.9, 1: 99.9}))
</code></pre>
<p>gives output</p>
<pre><code>0     0.0
1    99.9
2     NaN
dtype: float64
</code></pre>
<p>Observe that NaN at 1 was replace whilst at 2 was not as there is not key in dict. As your <code>B</code> column has not <code>t</code> index it does nothing.</p>
<p>You might use <code>numpy.where</code> to replace NaN with dict as value, consider following example</p>
<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'A':[0.5,1.5,2.5],'B':[np.nan, {'t': 1}, np.nan]})
df['B'] = np.where(df['B'].isna(), {'t': 0}, df['B'])
print(df)
</code></pre>
<p>gives output</p>
<pre><code>     A         B
0  0.5  {'t': 0}
1  1.5  {'t': 1}
2  2.5  {'t': 0}
</code></pre>
","1","Answer"
"79580316","79580309","<p>Here is the full code:</p>
<pre><code>import pandas as pd

def get_12h_period(dt):
    # Determine if the time is in AM (00:00-11:59) or PM (12:00-23:59)
    if dt.hour &lt; 12:
        return pd.Period(year=dt.year, month=dt.month, day=dt.day, 
                         hour=0, freq='12h')
    else:
        return pd.Period(year=dt.year, month=dt.month, day=dt.day, 
                         hour=12, freq='12h')
dt = pd.to_datetime(&quot;2025-04-17 18:35&quot;)
period = get_12h_period(dt)
print(period)
</code></pre>
<p>Another solutin using floor and period:</p>
<pre><code>import pandas as pd
def get_12h_period(dt):
    # Floor to the nearest 12-hour block (00:00 or 12:00)
    floored = dt.floor('12h', ambiguous='infer')
    # Adjust if floored incorrectly (e.g., 18:35 -&gt; 12:00, not 18:00)
    if floored.hour == 0 and dt.hour &gt;= 12:
        floored += pd.Timedelta(hours=12)
    return floored.to_period('12h')

dt = pd.to_datetime(&quot;2025-04-17 18:35&quot;)
print(get_12h_period(dt))
</code></pre>
<p>For large dataset, this solution could be the best one:</p>
<pre><code>import pandas as pd

def get_12h_period(dt_index):
    # Floor to nearest 12H, then adjust misaligned times (e.g., 18:00 → 12:00)
    floored = dt_index.floor('12h')
    mask = (dt_index.hour &gt;= 12) &amp; (floored.hour != 12)
    floored = floored.where(~mask, floored - pd.Timedelta(hours=12))
    return floored.to_period('12h')
dt_index = pd.to_datetime([&quot;2025-04-17 18:35&quot;])
print(get_12h_period(dt_index))
</code></pre>
<p>Output:</p>
<pre><code>2025-04-17 12:00
</code></pre>
","3","Answer"
"79580557","79580520","<p>Modify the <code>verif</code> function to return a <code>pd.Series</code> with keys matching the target column names (<code>&quot;S&quot;</code> and <code>&quot;COMMENT&quot;</code>):</p>
<pre><code>import pandas as pd

# Create the DataFrame
df = pd.DataFrame({
    'nombre': [10, 25, 42, 18, 73]
})
df[&quot;COMMENT&quot;] = df[&quot;S&quot;] = None

# Define the function
def verif(row):
    if row[&quot;nombre&quot;] % 10 == 0:
        c = &quot;Ends with 0&quot;
        e = &quot;OK&quot;
    else:
        c = &quot;Do not end with 0&quot;
        e = &quot;KO&quot;
    # Return Series with keys matching target column names
    return pd.Series({&quot;S&quot;: e, &quot;COMMENT&quot;: c})

# Apply the function
mask = df[&quot;nombre&quot;] % 2 == 0
df.loc[mask, [&quot;S&quot;, &quot;COMMENT&quot;]] = df.loc[mask].apply(verif, axis=1)

print(df)
</code></pre>
<p>alternative approach is :</p>
<p>If you want to make the <code>verif</code> function reusable for other DataFrames with different column names, you can return a tuple instead of a <code>pd.Series</code>:</p>
<pre><code>def verif(row):
    if row[&quot;nombre&quot;] % 10 == 0:
        c = &quot;Ends with 0&quot;
        e = &quot;OK&quot;
    else:
        c = &quot;Do not end with 0&quot;
        e = &quot;KO&quot;
    # Return a tuple of values
    return e, c

# Use a lambda to assign the returned values to specific columns
df.loc[mask, [&quot;S&quot;, &quot;COMMENT&quot;]] = df.loc[mask].apply(
    lambda row: pd.Series(verif(row)), axis=1)
</code></pre>
","1","Answer"
"79581360","79580520","<p>Here's another solution without <code>.apply</code> that is using the <code>.where</code> method (since <code>.apply</code> is rather slow): With</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({
    &quot;nombre&quot;: [10, 25, 42, 18, 73],
    &quot;COMMENT&quot;: None,
    &quot;S&quot;: None,
})
</code></pre>
<p>you could do</p>
<pre class=""lang-py prettyprint-override""><code>df.loc[mask, [&quot;COMMENT&quot;, &quot;S&quot;]] = (
    df.loc[mask := df[&quot;nombre&quot;].mod(2).eq(0), [&quot;nombre&quot;]]
    .assign(
        # Build basic columns COMMENT and S, and the mod 2 mask
        COMMENT=&quot;Ends with 0&quot;,
        S=&quot;OK&quot;,
        m=lambda df: df[&quot;nombre&quot;].mod(10).eq(0)
    ).assign(
        # Modify columns COMMENT and S based on mask m
        COMMENT=lambda df: df[&quot;COMMENT&quot;].where(df[&quot;m&quot;], &quot;Does not end with 0&quot;),
        S=lambda df: df[&quot;S&quot;].where(df[&quot;m&quot;], &quot;KO&quot;)
    )
)
</code></pre>
<p>and get</p>
<pre class=""lang-none prettyprint-override""><code>   nombre              COMMENT     S
0      10          Ends with 0    OK
1      25                 None  None
2      42  Does not end with 0    KO
3      18  Does not end with 0    KO
4      73                 None  None
</code></pre>
","1","Answer"
"79581667","79580520","<pre><code>import pandas as pd

df = pd.DataFrame({
    'nombre': [10, 25, 42, 18, 73]
})
df['S'] = None
df['COMMENT'] = None

def check(row):
    if row['nombre'] % 10 == 0:
        return pd.Series({'S': 'OK', 'COMMENT': 'Ends with 0'})
    else:
        return pd.Series({'S': 'KO', 'COMMENT': 'Do not end with 0'})

mask = df['nombre'] % 2 == 0

df.loc[mask, ['S', 'COMMENT']] = df.loc[mask].apply(check, axis=1)

print(df)
</code></pre>
","-1","Answer"
"79581734","79580520","<p><em>Assuming you can modify all of the code:</em></p>
<p>Generally, Pandas isn't built for working with rows; it's <a href=""https://stackoverflow.com/a/55557758/4518341"" title=""cs95's answer on &quot;How can I iterate over rows in a Pandas DataFrame?&quot;"">slower</a> and less idiomatic. So, don't use <code>.apply()</code> in the first place; you can use vectorized methods like <a href=""https://numpy.org/doc/stable/reference/generated/numpy.select.html"" rel=""nofollow noreferrer""><code>np.select()</code></a> instead, for example:</p>
<pre><code>df = pd.DataFrame({
    'nombre': [10, 25, 42, 18, 73]
})

mod2 = df[&quot;nombre&quot;] % 2 == 0
mod10 = df[&quot;nombre&quot;] % 10 == 0
for col, (verified, unverified) in [
    (&quot;S&quot;, (&quot;OK&quot;, &quot;KO&quot;)),
    (&quot;COMMENT&quot;, (&quot;Ends with 0&quot;, &quot;Do not end with 0&quot;)),
]:
    df[col] = np.select(
        [mod10, mod2],
        [verified, unverified],
        None
    )
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>   nombre     S            COMMENT
0      10    OK        Ends with 0
1      25  None               None
2      42    KO  Do not end with 0
3      18    KO  Do not end with 0
4      73  None               None
</code></pre>
","0","Answer"
"79583723","79583700","<p>The misunderstanding is that in Python <code>True == 1</code> and <code>False == 0</code> (see <a href=""https://docs.python.org/3/library/stdtypes.html#typebool"" rel=""nofollow noreferrer"">bool</a>). Suppose we have:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {'col': [0.5, 0.8, 1]}
df = pd.DataFrame(data)

df['col'].gt(0.7)
</code></pre>
<p>When we chain <code>.lt(0.9)</code>, this check takes place on the <em>result</em> of <code>.gt(0.7)</code>:</p>
<pre><code>0    False # 0 &lt; 0.9 (True)
1     True # 1 &lt; 0.9 (False)
2     True # 1 &lt; 0.9 (False)
Name: col, dtype: bool
</code></pre>
<p>Use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.between.html"" rel=""nofollow noreferrer""><code>Series.between</code></a> instead, with <code>inclusive</code> to control the comparison operators:</p>
<pre><code>df['col'].between(0.7, 0.9, inclusive='neither')

0    False # 0.5
1     True # 0.8
2    False # 1
Name: col, dtype: bool
</code></pre>
","4","Answer"
"79584484","79584468","<p>Here is your solution:</p>
<pre><code>import pandas as pd

data = {
    'user': ['John Doe', 'Jane Doe'],
    'start': [pd.Timestamp('2025-03-21 11:30:35'), pd.Timestamp('2023-12-31 01:02:03')],
    'end': [pd.Timestamp('2025-03-21 13:05:26'), pd.Timestamp('2024-01-02 03:04:05')],
}
df = pd.DataFrame(data)

interval_index = pd.IntervalIndex.from_arrays(df['start'], df['end'], closed='both')
df.set_index(interval_index, inplace=True)
df.drop(columns=['start', 'end'], inplace=True)
# check user
query_time = pd.Timestamp(&quot;2024-01-01 12:00:00&quot;)
active_users = df[df.index.contains(query_time)]
print(active_users)
</code></pre>
<p>Output:</p>
<pre><code>D:\python&gt;python test.py
                                                user
[2023-12-31 01:02:03, 2024-01-02 03:04:05]  Jane Doe
</code></pre>
","1","Answer"
"79585039","79585017","<p>This should work:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    'Animal ID': [1, 2, 1, 3, 2],
    'Intake Date': [
        '12/07/2017 02:07:00 PM',
        '12/08/2017 01:10:00 PM',
        '01/06/2018 12:03:00 PM',
        '01/07/2018 01:10:00 PM',
        '01/08/2018 04:15:00 PM'
    ]
})

df['Intake Date'] = pd.to_datetime(df['Intake Date'])
df = df.sort_values(by=['Animal ID', 'Intake Date'])

df['Shelter Visits'] = df.groupby('Animal ID').cumcount() + 1
</code></pre>
<p>The code is</p>
<ol>
<li>sorting by animal ID and intake date so that the cumulative count will work</li>
<li>adding a field whose value is 0 for the first row, incremented by 1 for every extra row, for each animal ID</li>
<li>since the index is zero-indexed, we add 1 to it</li>
</ol>
","1","Answer"
"79585133","79584468","<p>You can also create a mask and filter rows you need like this</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    'user': ['John Doe', 'Jane Doe'],
    'start': ['2025-03-21 11:30:35', '2023-12-31 01:02:03'],
    'end':   ['2025-03-21 13:05:26', '2024-01-02 03:04:05']
})

df['start'] = pd.to_datetime(df['start'])
df['end'] = pd.to_datetime(df['end'])

query_start = pd.Timestamp('2024-01-01')
query_end = pd.Timestamp('2024-01-31')

mask = (df['start'] &lt;= query_end) &amp; (df['end'] &gt;= query_start)
active_users = df[mask]

print(active_users)
</code></pre>
","0","Answer"
"79585312","79585207","<p><code>fillna()</code> interprets the Series by column labels (not row index). So you could do either Option 1 , 2 or 3. (I prefer option 1 ) in the code below</p>
<p>Code to reproduce:</p>
<pre><code>import numpy as np
import pandas as pd

# Initial DataFrame with NaNs
df_1 = pd.DataFrame(
    index=[0, 1, 2],
    columns=['a', 'b', 'c'],
    data=[[1, np.NaN, np.NaN], [2, np.NaN, 4], [np.NaN, 2, -3]],
)

series_1 = pd.Series([6, 5, 4], index=[0, 1, 2])

# Option1
df_2 = df_1.apply(lambda row: row.fillna(series_1[row.name]), axis=1)
print(df_2)

# Option2
df_2 = df_1.T.fillna(series_1).T
print(df_2)

# Option3
fill_df = pd.DataFrame({col: series_1 for col in df_1.columns})
df_2 = df_1.fillna(fill_df)
print(df_2)
</code></pre>
","1","Answer"
"79585314","79585207","<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.where.html"" rel=""nofollow noreferrer""><code>df.where</code></a> and replace where <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.notna.html"" rel=""nofollow noreferrer""><code>df.notna</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>df_1.where(df_1.notna(), series_1, axis=0)

     a    b    c
0  1.0  6.0  6.0
1  2.0  5.0  4.0
2  4.0  2.0 -3.0
</code></pre>
<p>Or similar, via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mask.html"" rel=""nofollow noreferrer""><code>df.mask</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html"" rel=""nofollow noreferrer""><code>df.isna</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>df_1.mask(df_1.isna(), series_1, axis=0)
</code></pre>
<hr />
<p>Since your values are <em>numeric</em>, you can make <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer""><code>df.fillna</code></a> work as well via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.add.html"" rel=""nofollow noreferrer""><code>df.add</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>df_1.fillna(df_1.fillna(0).add(series_1, axis=0))
</code></pre>
<p>But that will naturally break with non-numeric values.</p>
","3","Answer"
"79585444","79585340","<p>I would use <code>.apply()</code> to run function which will get <code>row</code> and it will check value from <code>String</code> with value(s) from <code>Substring</code>. And it would return list/tuple with values <code>True</code>/<code>False</code> which it could assign to new columns <code>Result 1</code>, <code>Result 2</code></p>
<p>Because <code>&quot;substring&quot; in &quot;text&quot;</code> gives <code>True</code> or <code>False</code> so it doesn't need to use <code>if/else</code></p>
<p>I use <code>lower()</code> to make it case insensitive.</p>
<pre><code>import pandas as pd

# --- example data ---

df = pd.DataFrame({
    'String':[
        'fooDisplay &quot;screen 1&quot; other text', 
        'foobar Display &amp;quot;Screen 2&amp;quot; more text', 
        'barDisplay &amp;quot;Screen 2&amp;quot;useless text', 
        'Link=&quot;Screen 1&quot;'
    ], 
    'Substring': ['Screen 1', 'GFX', 'Screen 2', 'Screen 1'],
})

# --- code ---

def check_substrings(row):
    string_lower = row['String'].lower()
    substring = row['Substring']

    return (
        f&quot;Display \&quot;{substring}\&quot;&quot;.lower() in string_lower,
        f&quot;Display &amp;quot;{substring}&amp;quot;&quot;.lower() in string_lower,
    )

# axis=1 - to work with rows instead of columns
# result_type='expand' - to split result to separated columns
df[['Result 1', 'Result 2']] = df.apply(check_substrings, axis=1, result_type='expand')

print(df)
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>                                          String Substring  Result 1  Result 2
0               fooDisplay &quot;screen 1&quot; other text  Screen 1      True     False
1  foobar Display &amp;quot;Screen 2&amp;quot; more text       GFX     False     False
2    barDisplay &amp;quot;Screen 2&amp;quot;useless text  Screen 2     False      True
3                                Link=&quot;Screen 1&quot;  Screen 1     False     False

</code></pre>
","1","Answer"
"79585817","79585207","<p>If you want to match based on index, you should use join, fill the na using mask or where, and drop the new column at the end.</p>
<pre><code>tmp = df_1.join(series_1.rename('NA'))
df_2 = tmp.where(tmp.notna(), tmp['NA'], axis=0).drop(columns='NA')
</code></pre>
<p>or as one line.</p>
<pre><code>df_2 = (df_1.join(series_1.rename('NA'))
      .pipe(lambda x: x.where(x.notna(), x['NA'], axis=0))
      .drop(columns='NA'))
</code></pre>
<p>End result:</p>
<pre><code>  a   b    c
1.0 6.0  6.0
2.0 5.0  4.0
4.0 2.0 -3.0
</code></pre>
","0","Answer"
"79585879","79583054","<p>To put the comment of @balderman as an answer: do this in SQL.</p>
<p>Using SQL:</p>
<ul>
<li><p>avoids the overheads of copying data from the database to Python, and then back to the database</p>
</li>
<li><p>removes the issues with data types</p>
</li>
</ul>
<p>Try this in your favorite SQL tool, such as SQL*Plus:</p>
<pre><code>
CREATE TABLE new_emp AS SELECT * FROM emp;
</code></pre>
<p>To do this in your application code, you could do:</p>
<pre><code>with Session(engine) as session:
    connection = session.connection()
    cursor = connection.connection.cursor()
    sql = &quot;&quot;&quot;create table new_emp as select * from emp&quot;&quot;&quot;
    cursor.execute(sql)
</code></pre>
<p>But really this is the kind of thing to do in SQL*Plus since it is likely to be an uncommon operation.  Or perhaps you would directly with python-oracledb without the overhead of Pandas &amp; SQLAlchemy:</p>
<pre><code>import getpass
import oracledb

un = 'cj'
cs = 'localhost/orclpdb1'
pw = getpass.getpass(f'Enter password for {un}@{cs}: ')

with oracledb.connect(user=un, password=pw, dsn=cs) as connection:
    with connection.cursor() as cursor:
        sql = &quot;&quot;&quot;create table new_emp as select * from emp&quot;&quot;&quot;
        cursor.execute(sql)
</code></pre>
","1","Answer"
"79586633","79585543","<p>If your goal is to ensure low memory usage, I would just use the <code>csv</code> package to stream read and write your data of interest. While doing reading I would keep track of the data you needed to construct the end results you seek.</p>
<pre class=""lang-py prettyprint-override""><code>import csv
import numpy    # for random
import json     # for nice printing

## ---------------------------------
## Simulate a CSV file with arbitrary number of rows
## ---------------------------------
def get_next_row(limit):
    for i in range(limit):
        yield f&quot;{i},{numpy.random.rand()},{numpy.random.randint(0, 10)}\n&quot;
## ---------------------------------

FIELDNAMES = [&quot;id&quot;, &quot;value&quot;, &quot;category&quot;]
FILTERED_FILENAME = &quot;filtered.csv&quot;

## ---------------------------------
## The values you seek
## ---------------------------------
total_value = 0
grouped_data = {}
## ---------------------------------

## ---------------------------------
## A reader that reads from the simulated CSV file
## the generator here allows you to test with almost
## any size of data
## ---------------------------------
reader = csv.reader(get_next_row(100_000)) # lots of rows. feel free to set to 10m
## ---------------------------------

with open(FILTERED_FILENAME, &quot;w&quot;, newline=&quot;&quot;) as filtered_file:
    ## ---------------------------------
    ## Open the filtered file and write the header
    ## ---------------------------------
    writer = csv.writer(filtered_file)
    writer.writerow(FIELDNAMES)
    ## ---------------------------------

    ## ---------------------------------
    ## Stream read the CSV file
    ## ---------------------------------
    for row in reader:
        _, value, category = map(float, row)

        ## ---------------------------------
        ## construct our running total
        ## ---------------------------------
        total_value += value
        ## ---------------------------------

        ## ---------------------------------
        ## Construct grouping on the fly
        ## ---------------------------------
        target = grouped_data.setdefault(category, {&quot;sum&quot;:0, &quot;count&quot;:0})
        target[&quot;sum&quot;] += value
        target[&quot;count&quot;] += 1
        ## ---------------------------------

        ## ---------------------------------
        ## Stream write filtered data to a new CSV file
        ## ---------------------------------
        if category &gt; 5:
            writer.writerow(row)
        ## ---------------------------------
    ## ---------------------------------

print(&quot;Example Results:&quot;)

print(f&quot;\tTotal: {total_value}&quot;)

print(&quot;\tGrouped Totals:&quot;)
for key, value in grouped_data.items():
    print(f&quot;\t\t{key}: {value[&quot;sum&quot;] /value[&quot;count&quot;]}&quot;)

print(&quot;\tFiltered:&quot;)
with open(FILTERED_FILENAME, &quot;r&quot;) as filtered_file:
    reader = csv.reader(filtered_file)
    for row in reader:
        print(f&quot;\t\t{row}{ &quot; &quot; * 20 }&quot;, end=&quot;\r&quot;)
    print(&quot;\n&quot;)
</code></pre>
<p>Giving you something like:</p>
<pre><code>Example Results:
        Total: 49901.74831312535
        Grouped Totals:
                7.0: 0.5040226167253907
                6.0: 0.49718272454103724
                0.0: 0.49830374656445003
                4.0: 0.5020429187707787
                3.0: 0.4945115761171055
                9.0: 0.4959816647030425
                1.0: 0.49440301035945405
                8.0: 0.5026981953484214
                2.0: 0.49715019019186407
                5.0: 0.5038847853189612
        Filtered:
                ['99999', '0.26387763159374467', '7'] 
</code></pre>
","0","Answer"
"79587196","79587011","<p>I think the best option is to use the HDF5 format (via <code>pandas.HDFStore</code>). It lets you store large DataFrames and load only the columns or rows you need, without reading the whole file into memory. But if you need even more flexibility or scalability, then it’s probably time to switch to a proper database.</p>
","1","Answer"
"79587504","79587477","<p>Use “df.describe()”
or “df.info(verbose=True)”</p>
<ul>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html</a></li>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html</a></li>
</ul>
","1","Answer"
"79587565","79587477","<p>You can use <code>isnull()</code>. Here is the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.isnull.html"" rel=""nofollow noreferrer"">doc</a></p>
<pre><code>import pandas as pd
df = pd.read_csv(&quot;your_data.csv&quot;)
missing_values = df.isnull().sum()
missing_values = missing_values[missing_values &gt; 0]
print(missing_values)
</code></pre>
","0","Answer"
"79588691","79588678","<p>You can certainly chain all your commands to avoid using intermediate variables, and combine all filters into a single expression (for example defining the condition in a col:regex dictionary and using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>loc</code></a> with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html"" rel=""nofollow noreferrer""><code>numpy.logical_and</code></a><a href=""https://numpy.org/doc/stable/reference/generated/numpy.ufunc.reduce.html"" rel=""nofollow noreferrer""><code>.reduce</code></a>):</p>
<pre><code>conditions = {'A': r'\bFOO\b', 'B': r'\bBAR\b'}

(pd.read_excel(XL, sheet_name=SHEET, usecols=COLUMNS).dropna()
   .loc[lambda x: np.logical_and.reduce([x[col].str.contains(cond, regex=True)
                                         for col, cond in conditions.items()])]
   .to_excel(OUTPUT, sheet_name=OUTSHEET, index=False)
)
</code></pre>
<p>Alternative with a custom filtering function:</p>
<pre><code>def cust_filter(df):
    m1 = df['A'].str.contains(r'\bFOO\b', regex=True)
    m2 = df['B'].str.contains(r'\bBAR\b', regex=True)
    return df[m1 &amp; m2]

(pd.read_excel(XL, sheet_name=SHEET, usecols=COLUMNS).dropna()
   .pipe(cust_filter)
   .to_excel(OUTPUT, sheet_name=OUTSHEET, index=False)
)
</code></pre>
<p>Example input:</p>
<pre><code>     A    B      C
0  ABC  GHI  other
1  FOO  BAR  other
2  FOO  JKL  other
3  DEF  BAR  other
</code></pre>
<p>Example output:</p>
<pre><code>     A    B      C
1  FOO  BAR  other
</code></pre>
","2","Answer"
"79588870","79587011","<p>In Polars the primary data storage format (the one that gets the most attention &amp; optimizations) is Parquet, which has support for both efficient slicing of the data as well as reading only those columns which are necessary.</p>
","3","Answer"
"79589683","79589664","<p>I changed your dictionary constructor to construct date time objects rather than have a set of strings with form <code>datetime.datetime(#, #, #, #, #): ##.#</code>, as I interpreted it to be in dictionary form:</p>
<pre><code>my_dict1 = {'data.outdoor.temperature': {'unit': '℃', 'list': {datetime.datetime(2025, 4, 23, 10, 0): '22.3', datetime.datetime(2025, 4, 23, 14, 0): '21.3', datetime.datetime(2025, 4, 23, 18, 0): '18.2', }}, 'data.indoor.temperature': {'unit': '℃', 'list': {datetime.datetime(2025, 4, 23, 10, 0): '23.2', datetime.datetime(2025, 4, 23, 14, 0): '23.5', datetime.datetime(2025, 4, 23, 18, 0): '22.9'}}}
</code></pre>
<p>Afterwards, I constructed a dictionary representation, then appended the <code>unit</code> column as an index, turned the list column into a Series (unpacking it), before stacking the whole thing and resetting the index.</p>
<pre><code>pd.DataFrame.from_dict(my_dict1, orient='index').set_index('unit', append=True)['list'].apply(pd.Series).stack().reset_index()
</code></pre>
<p>This yielded the following:</p>
<pre><code>                    level_0 unit             level_2     0
0  data.outdoor.temperature    ℃ 2025-04-23 10:00:00  22.3
1  data.outdoor.temperature    ℃ 2025-04-23 14:00:00  21.3
2  data.outdoor.temperature    ℃ 2025-04-23 18:00:00  18.2
3   data.indoor.temperature    ℃ 2025-04-23 10:00:00  23.2
4   data.indoor.temperature    ℃ 2025-04-23 14:00:00  23.5
5   data.indoor.temperature    ℃ 2025-04-23 18:00:00  22.9
</code></pre>
<p>(It might appear misaligned because the ℃ character is wider than the standard monospaced character.) You can then rename columns by assigning to <code>df.columns</code>. I chose <code>df.columns = ['where', 'unit', 'date', 'temp']</code>.</p>
<p>You can clean up <code>where</code> pretty simply: <code>df['where'] = df['where'].str.replace(r'^data\.', '', regex=True)</code>.</p>
<p>The constructor also gives <code>temp</code> as a string, I would do <code>df['temp'] = df['temp'].pipe(pd.to_numeric)</code> to make it numeric.</p>
","0","Answer"
"79589688","79589524","<p>It's not very clear to me how your weighted average works. But it seems you just want to flip your multi-index and resort. That's pretty trivial:</p>
<pre class=""lang-py prettyprint-override""><code>out.columns = out.columns.swaplevel(0, 1)
out.sort_index(axis=1)  # reassignment may be necessary
</code></pre>
<p>That yields as follows:</p>
<pre><code>colour           green            ...        red                                 
                % of a  avg of b  ...   sum of d weighted avg of a, weighted by b
city                              ...                                            
Buenos Aires  0.181522  0.400507  ...  25.530276                         0.462020
London        0.197473  0.500865  ...  19.753725                         0.445790
New York      0.177483  0.489295  ...  23.524630                         0.498965

[3 rows x 12 columns]
</code></pre>
","1","Answer"
"79590243","79590117","<p><code>if not x</code> checks if <code>x</code> is an empty string. if it is empty it returns <code>''</code>, which is an empty string without any content.</p>
<pre><code>def convert_dtype(x):
    if not x:
        return ''

</code></pre>
<p><code>try: return str(x)</code> tries to convert and return <code>x</code> as a string.</p>
<pre><code>    try:
        return str(x)
</code></pre>
<p>if converting and returning <code>x</code> as a string doesn't work, it returns <code>''</code>.</p>
<pre><code>    except:
        return ''
</code></pre>
<p>Basically, if the content of the column is empty from the start or can't be converted to string it's discarded and replaced with a string not having any content. I can't judge however if this is a good approach, it depends on what you are trying to accomplish with your application. Your column will only contain strings afterwards nonetheless.</p>
","3","Answer"
"79591328","79563265","<p>Just use Flashtext : pip install flashtext</p>
<pre><code>import pandas as pd
import re
from flashtext import KeywordProcessor
import pandas as pd

Names = pd.DataFrame({
    'ENTITY_NAME': ['XYZ', 'ABC', 'NGA', 'METRO','DPAC']
})

Titles = pd.DataFrame({
    'title': ['testing some text XYZ testing some text.',
              'XYZ, ABC some random text',
              'some text DPAC random random']
})

# Setup
keyword_processor = KeywordProcessor(case_sensitive=True)
keyword_processor.add_keywords_from_list(Names['ENTITY_NAME'].tolist())

# Apply to Titles
Titles['extracted_entity_name'] = Titles['title'].apply(
    lambda x: ', '.join(keyword_processor.extract_keywords(x))
)
print(Titles)
'''
                                      title extracted_entity_name
0  testing some text XYZ testing some text.                   XYZ
1                 XYZ, ABC some random text              XYZ, ABC
2              some text DPAC random random                  DPAC
'''
</code></pre>
","1","Answer"
"79591438","79591383","<p>A possible solution:</p>
<pre><code># the array with the 6 nan values
arr_nan = np.full(
    df3['c3'].map( 
        lambda x: np.size(x) if isinstance(x, np.ndarray) else 0).max(), np.nan)

df3.assign(c3 = df3['c3'].map(
    lambda y: arr_nan if not isinstance(y, np.ndarray) else y))
</code></pre>
<p>This solution first determines the length of the arrays in <code>c3</code>, and then replaces all non-array entries in <code>c3</code> by the array of 6 <code>np.nan</code>.</p>
<p>Output:</p>
<pre><code>  c1  c2                              c3
0  A   1              [1, 2, 3, 4, 5, 6]
1  B   2            [6, 7, 8, 9, 10, 11]
2  C   3  [nan, nan, nan, nan, nan, nan]
3  D   4  [nan, nan, nan, nan, nan, nan]
4  E   5  [nan, nan, nan, nan, nan, nan]
</code></pre>
","1","Answer"
"79591600","79591534","<p><strong>Method 1</strong></p>
<p>Use this method if you have consecutive branches and no gaps, like in the given sample (I think method 1 easier).</p>
<pre><code># True if it's the last month of a quarter, otherwise False
cond = (series.index.month % 3) == 0

# keep only the values at quarter-end, shift and ffill
out = series.where(cond).shift().ffill()
</code></pre>
<p>out</p>
<pre><code>2023-08-01           NaN
2023-09-01           NaN
2023-10-01    2629410.80
2023-11-01    2629410.80
2023-12-01    2629410.80
2024-01-01    3397805.34
2024-02-01    3397805.34
2024-03-01    3397805.34
2024-04-01    3139235.65
2024-05-01    3139235.65
dtype: float64
</code></pre>
<p><strong>Method 2</strong></p>
<p>If you think it might not be consecutive, use method 2.</p>
<pre><code>out = (
    series.resample('QE').last().shift(freq='1D')
          .reindex(series.index)
          .resample('QE').transform('first')
)
</code></pre>
<p>same result</p>
<hr />
<p><strong>Example Code</strong></p>
<pre><code>import pandas as pd

series = pd.Series(
    [1515000.08, 2629410.80, 2548748.40, 2494398.04, 3397805.34,
     3285501.49, 3173978.74, 3139235.65, 2927895.84, 2750708.29],
    index=pd.to_datetime([
        '2023-08-01', '2023-09-01', '2023-10-01', '2023-11-01', '2023-12-01',
        '2024-01-01', '2024-02-01', '2024-03-01', '2024-04-01', '2024-05-01'])
)
</code></pre>
","2","Answer"
"79591716","79591713","<p>I tried one way and wondered if there was another way...</p>
<pre><code>da.assign_coords(year = da.time.dt.year, monthday = da.time.dt.strftime(&quot;%m-%d&quot;)).groupby(['year', 'monthday']).mean('time')
</code></pre>
","0","Answer"
"79591751","79591713","<p>This is a solution for you:</p>
<pre><code>import xarray as xr
import numpy as np
import pandas as pd

time = pd.date_range(&quot;2000-01-01&quot;, &quot;2001-12-31&quot;, freq=&quot;D&quot;)
time = time[~((time.month == 2) &amp; (time.day == 29))] 

lon = np.linspace(100, 110, 5)
lat = np.linspace(30, 35, 4)
data = np.random.rand(len(time), len(lon), len(lat))

da = xr.DataArray(
    data,
    coords={&quot;time&quot;: time, &quot;lon&quot;: lon, &quot;lat&quot;: lat},
    dims=[&quot;time&quot;, &quot;lon&quot;, &quot;lat&quot;],
    name=&quot;pr&quot;
)

years = da.time.dt.year.values
month_day = da.time.dt.strftime('%m-%d').values

unique_years = np.unique(years)
unique_month_day = np.unique(month_day)

multi_index = pd.MultiIndex.from_arrays([years, month_day], names=('year', 'monthly'))

da_4d = da.copy()
da_4d.coords['time'] = multi_index
da_4d = da_4d.unstack('time')

print(da_4d)
</code></pre>
","1","Answer"
"79591959","79591383","<p>Get the index of the rows where you have na values, and create a Series with an equal amount of rows, and with the same index.</p>
<pre><code>idx = df3[df3['c3'].isna()].index
df3.loc[idx, 'c3'] = pd.Series([np.full((6,), np.nan)] * len(idx), index=idx)
</code></pre>
<p>End result:</p>
<pre><code>c1  c2                             c3
 A   1             [1, 2, 3, 4, 5, 6]
 B   2           [6, 7, 8, 9, 10, 11]
 C   3 [nan, nan, nan, nan, nan, nan]
 D   4 [nan, nan, nan, nan, nan, nan]
 E   5 [nan, nan, nan, nan, nan, nan]
</code></pre>
","0","Answer"
"79592727","79592693","<p>There are different ways of calculating the week of the year</p>
<ul>
<li><p><code>fromisocalendar</code> uses ISO standard, where week 1 is the week containing <em>January 4th</em>. Or in other words, the first week containing at <em>least 4 days of the new year</em>.</p>
</li>
<li><p><code>%W</code> uses week containing the <em>first monday</em> of the year as week 1. All days before this are considered to be week 0.</p>
</li>
</ul>
<p>For 2025 these two methods for counting weeks give different results. Ie week 1 in ISO is from 2024-12-30 to 2025-01-05 whereas week 1 following <code>%W</code> logic is from 2025-01-06 to 2025-01-12. Thus, also your week 17 from <code>to_datetime</code> is one week ahead.</p>
<p>If you want <code>pd.to_datetime</code> to behave the same as <code>pd.Timestamp.fromisocalendar</code> use <code>%V</code> as format specifier instead of <code>%W</code>.</p>
<p>See also the <a href=""https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes"" rel=""nofollow noreferrer"">docs</a></p>
","2","Answer"
"79592991","79541633","<p>Solution Using Polars for Huge datasets</p>
<pre><code>import polars as pl

df = pl.DataFrame({'id': ['g0', 'g1', 'g1', 'g2', 'g3', 'g4', 'g4', 'g4']})

df = df.with_columns(
(pl.col('id') != pl.col('id').shift(1)).cum_sum().alias('gr')     
)
'''
shape: (8, 2)
┌─────┬──────┐
│ id  ┆ gr   │
│ --- ┆ ---  │
│ str ┆ u32  │
╞═════╪══════╡
│ g0  ┆ null │
│ g1  ┆ 1    │
│ g1  ┆ 1    │
│ g2  ┆ 2    │
│ g3  ┆ 3    │
│ g4  ┆ 4    │
│ g4  ┆ 4    │
│ g4  ┆ 4    │
└─────┴──────┘
'''
df = df.with_columns(
pl.col('id').cum_count().over('gr').alias('gr_cumcount'),
pl.len().over('gr').alias('gr_len')
)
'''
shape: (8, 4)
┌─────┬──────┬─────────────┬─────────┐
│ id  ┆ gr   ┆ gr_cumcount ┆ gr_len │
│ --- ┆ ---  ┆ ---         ┆ ---     │
│ str ┆ u32  ┆ u32         ┆ u32     │
╞═════╪══════╪═════════════╪═════════╡
│ g0  ┆ null ┆ 1           ┆ 1       │
│ g1  ┆ 1    ┆ 1           ┆ 2       │
│ g1  ┆ 1    ┆ 2           ┆ 2       │
│ g2  ┆ 2    ┆ 1           ┆ 1       │
│ g3  ┆ 3    ┆ 1           ┆ 1       │
│ g4  ┆ 4    ┆ 1           ┆ 3       │
│ g4  ┆ 4    ┆ 2           ┆ 3       │
│ g4  ┆ 4    ┆ 3           ┆ 3       │
└─────┴──────┴─────────────┴─────────┘
'''
df = df.with_columns(
pl.when(pl.col('gr_len') == 1).then(pl.col('id')).otherwise(
pl.format('{}_TE{}',pl.col('id'),pl.col('gr_cumcount') )    
).alias('new_id')    
)
'''
shape: (8, 5)
┌─────┬──────┬─────────────┬────────┬────────┐
│ id  ┆ gr   ┆ gr_cumcount ┆ gr_len ┆ new_id │
│ --- ┆ ---  ┆ ---         ┆ ---    ┆ ---    │
│ str ┆ u32  ┆ u32         ┆ u32    ┆ str    │
╞═════╪══════╪═════════════╪════════╪════════╡
│ g0  ┆ null ┆ 1           ┆ 1      ┆ g0     │
│ g1  ┆ 1    ┆ 1           ┆ 2      ┆ g1_TE1 │
│ g1  ┆ 1    ┆ 2           ┆ 2      ┆ g1_TE2 │
│ g2  ┆ 2    ┆ 1           ┆ 1      ┆ g2     │
│ g3  ┆ 3    ┆ 1           ┆ 1      ┆ g3     │
│ g4  ┆ 4    ┆ 1           ┆ 3      ┆ g4_TE1 │
│ g4  ┆ 4    ┆ 2           ┆ 3      ┆ g4_TE2 │
│ g4  ┆ 4    ┆ 3           ┆ 3      ┆ g4_TE3 │
└─────┴──────┴─────────────┴────────┴────────┘
'''

</code></pre>
","0","Answer"
"79593329","79585207","<p>Another possible solution:</p>
<pre><code>df_1.combine_first(
    pd.concat([series_1] * df_1.shape[1], axis = 1, keys = df_1.columns))
</code></pre>
<p>It uses <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.combine_first.html"" rel=""nofollow noreferrer""><code>combine_first</code></a> to fill missing values in <code>df_1</code> by merging it with a new dataframe created by <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>, where the <code>series_1</code> is repeated across all columns of <code>df_1</code> and aligned by index. This way, the missing (<code>NaN</code>) entries in <code>df_1</code> are filled column-wise with the corresponding values from <code>series_1</code>, without needing to apply <code>fillna</code> individually to each column.</p>
<p>Output:</p>
<pre><code>     a    b    c
0  1.0  6.0  6.0
1  2.0  5.0  4.0
2  4.0  2.0 -3.0
</code></pre>
","0","Answer"
"79594514","79594460","<p>When you run <code>apply()</code> on DataFrame then it sends row to function and you have to get column from this row.</p>
<pre><code>def myfunction(row):
    url = row['Page Links']
    ...code...
    row['new column 1'] = &quot;value1&quot;
    row['new column 2'] = &quot;value2&quot;
    return row

df = df.apply(myfunction, axis=1)
</code></pre>
<p>And if you want only get urls then you may need to run <code>apply()</code> only on one column</p>
<pre><code>def myfunction(url):
    ... code ...
    return pd.Series([&quot;value1&quot;, &quot;value2&quot;])

df[[&quot;new column 1&quot;, &quot;new column 2&quot;]] = df[&quot;Page Links&quot;].apply(myfunction)
</code></pre>
<hr />
<p>Full working code for tests - with data directly in code so everyone can simply copy and test it.</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'Page Links': [
'https://www.example.com',
'https://www.example2.com',
'https://www.example3.com',
'https://www.example4.com'
]})

print(df)

print('--- version 1 ---')
def myfunction1(row):
    url = row['Page Links']
    print(url)
    row['New Column 1.1'] = 'Hello 1'
    row['New Column 1.2'] = 'World 1'
    return row

df = df.apply(myfunction1, axis=1)
print(df)

print('--- version 2 ---')
def myfunction2(url):
    print(url)
    return pd.Series(['Hello 2', 'World 2'])

df[[&quot;New Column 2.1&quot;, &quot;New Column 2.2&quot;]] = df['Page Links'].apply(myfunction2)
print(df)
</code></pre>
","1","Answer"
"79595521","79595489","<pre><code># code for example
import io
import pandas as pd
txt = '''
Start of new file:,,,,,,,,,,,,,,,,
MISCOUNT: 0,,,,,,,,,,,,,,,,
SampleNumber,C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,C15
0,3472,3030,2813,2695,2649,2636,2634,2632,2635,2635,2626,2624,2625,2623,2633,2597
1,2582,2581,2576,2561,2538,2511,2498,2490,2487,2484,2481,2481,2475,2475,2469,2475
2,2472,2474,2472,2474,2474,2474,2478,2474,2476,2484,2485,2490,2484,2485,2478,2486
'''

# answer
df = pd.read_csv(io.StringIO(txt), header=2) # use file_path instead io.StringIO(txt)
</code></pre>
<p>df</p>
<pre><code>   SampleNumber    C0    C1    C2  ...   C12   C13   C14   C15
0             0  3472  3030  2813  ...  2625  2623  2633  2597
1             1  2582  2581  2576  ...  2475  2475  2469  2475
2             2  2472  2474  2472  ...  2484  2485  2478  2486

[3 rows x 17 columns]
</code></pre>
","0","Answer"
"79595822","79595804","<p>Here is the solution:</p>
<pre><code>import pandas as pd

world = pd.read_csv(&quot;worldstats.csv&quot;)

cond = world[&quot;country&quot;].isin([&quot;Brazil&quot;, &quot;Canada&quot;, &quot;Denmark&quot;])
world = world[cond]


world = world.set_index([&quot;year&quot;, &quot;country&quot;]).sort_index()

def format_world_data(df):
    output = &quot;&quot;
    for year, group in df.groupby(level=0):
        output += f&quot;year {year}\n&quot;
        output += f&quot;{'country':&lt;10}{'Population':&gt;15}{'GDP':&gt;15}\n&quot;
        for (yr, country), row in group.iterrows():
            output += f&quot;{country:&lt;10}{row['Population']:&gt;15.1f}{row['GDP']:&gt;15.6e}\n&quot;
        output += &quot;\n&quot;
    return output

formatted_output = format_world_data(world)
print(formatted_output)
</code></pre>
<p>Here is the optimized version (Might be good for large or medium dataset):</p>
<pre><code>import pandas as pd

world = pd.read_csv(&quot;worldstats.csv&quot;)
world = world.loc[world[&quot;country&quot;].isin([&quot;Brazil&quot;, &quot;Canada&quot;, &quot;Denmark&quot;])]
world = world.set_index([&quot;year&quot;, &quot;country&quot;]).sort_index()

def format_world_data_large(df, pop_width=15, gdp_width=15, decimals=1):
    lines = []
    for year, group in df.groupby(level=0):
        lines.append(f&quot;year {year}&quot;)
        lines.append(f&quot;{'country':&lt;10}{'Population':&gt;{pop_width}}{'GDP':&gt;{gdp_width}}&quot;)
        for row in group.itertuples():
            lines.append(f&quot;{row.Index[1]:&lt;10}{row.Population:&gt;{pop_width}.{decimals}f}{row.GDP:&gt;{gdp_width}.6e}&quot;)
        lines.append(&quot;&quot;)
    return &quot;\n&quot;.join(lines)

formatted_text = format_world_data_large(world)
print(formatted_text)
</code></pre>
<p>Without loop:</p>
<pre><code>import pandas as pd

world = pd.read_csv(&quot;worldstats.csv&quot;)
world = world[world[&quot;country&quot;].isin([&quot;Brazil&quot;, &quot;Canada&quot;, &quot;Denmark&quot;])]
world = world.set_index([&quot;year&quot;, &quot;country&quot;]).sort_index()

def format_world_data_optimized(df, pop_width=15, gdp_width=15, decimals=1):
    df['Population'] = df['Population'].map(lambda x: f'{x:&gt;{pop_width}.{decimals}f}')
    df['GDP'] = df['GDP'].map(lambda x: f'{x:&gt;{gdp_width}.6e}')
    
    def generate_output(group):
        year = group.index.get_level_values('year')[0]
        header = f&quot;{'country':&lt;10}{'Population':&gt;{pop_width}}{'GDP':&gt;{gdp_width}}&quot;
        data_rows = [f&quot;{row.name[1]:&lt;10}{row['Population']}{row['GDP']}&quot; for _, row in group.iterrows()]
        return f&quot;year {year}\n{header}\n&quot; + &quot;\n&quot;.join(data_rows) + &quot;\n&quot;
    
    all_formatted_text = &quot;&quot;.join(df.groupby(level=0).apply(generate_output))
    return all_formatted_text

formatted_text = format_world_data_optimized(world)
print(formatted_text)
</code></pre>
<p>Output:</p>
<pre><code>year 1960
country        Population            GDP
Brazil         72493585.0   1.516557e+10
Canada         17909009.0   4.109345e+10
Denmark         4579603.0   6.248947e+09

year 1961
country        Population            GDP
Brazil         74706888.0   1.523685e+10
Canada         18271000.0   4.076797e+10
Denmark         4611687.0   6.933842e+09

year 1962
country        Population            GDP
Brazil         77007549.0   1.992629e+10
Canada         18614000.0   4.197885e+10
Denmark         4647727.0   7.812968e+09
</code></pre>
","0","Answer"
"79596267","79587011","<p>If speed is of more concern than disk space, you could store <a href=""https://numpy.org/doc/stable/reference/generated/numpy.memmap.html"" rel=""nofollow noreferrer"">memory mapped numpy arrays.</a> There is very little overhead for data access and you could acceess any chunk you need at the moment.</p>
","1","Answer"
"79596456","79595809","<p>You can pick the low quality parameter and make random data that is within the parameter. It's a simple method and idk if it's going to solve your problem.</p>
<p>Useful link: <a href=""https://www.datacamp.com/pt/tutorial/techniques-to-handle-missing-data-values"" rel=""nofollow noreferrer"">https://www.datacamp.com/pt/tutorial/techniques-to-handle-missing-data-values</a></p>
<p>Good luck!</p>
","0","Answer"
"79596652","79596631","<p>Your changeMonth() function is good but you don't need the global CurrentMonth line (it's unnecessary unless you specifically modify a global variable, which you're not doing here).</p>
<p>Now, if you want to apply it manually to a pandas column, you can use .apply() on your DataFrame</p>
<pre><code>import pandas as pd

# Define the English to French month mapping
month_translation = {
    &quot;Jan&quot;: &quot;Janvier&quot;,
    &quot;Feb&quot;: &quot;Février&quot;,
    &quot;Mar&quot;: &quot;Mars&quot;,
    &quot;Apr&quot;: &quot;Avril&quot;,
    &quot;May&quot;: &quot;Mai&quot;,
    &quot;Jun&quot;: &quot;Juin&quot;,
    &quot;Jul&quot;: &quot;Juillet&quot;,
    &quot;Aug&quot;: &quot;Août&quot;,
    &quot;Sep&quot;: &quot;Septembre&quot;,
    &quot;Oct&quot;: &quot;Octobre&quot;,
    &quot;Nov&quot;: &quot;Novembre&quot;,
    &quot;Dec&quot;: &quot;Décembre&quot;
}

# Function to change the month
def change_month(month):
    return month_translation.get(month, &quot;&quot;)

# Example DataFrame
df = pd.DataFrame({
    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
})

# Apply the function to the 'Month' column
df['Month_French'] = df['Month'].apply(change_month)

print(df)
</code></pre>
","0","Answer"
"79596965","79595489","<p>welcome to StackOverflow. 👋</p>
<p>I tried what you tried and I actually got what you want (I use input.csv):</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.read_csv(&quot;input.csv&quot;, skiprows=2, index_col=0) # index_col=&quot;SampleNumber&quot; also works

print(df)
</code></pre>
<pre class=""lang-none prettyprint-override""><code>               C0     C1    C2    C3    C4    C5  ...   C10   C11   C12   C13   C14   C15
SampleNumber                                      ...
0             3472  3030  2813  2695  2649  2636  ...  2626  2624  2625  2623  2633  2597
1             2582  2581  2576  2561  2538  2511  ...  2481  2481  2475  2475  2469  2475
2             2472  2474  2472  2474  2474  2474  ...  2485  2490  2484  2485  2478  2486
3             2485  2483  2488  2488  2485  2486  ...  2485  2483  2485  2483  2490  2473
4             2475  2472  2474  2477  2479  2482  ...  2483  2482  2484  2483  2477  2483
5             2481  2482  2482  2465  2455  2450  ...  2444  2465  2470  2467  2440  2467

[6 rows x 16 columns]
</code></pre>
<p>I agree with your diagnosis of the problem, but cannot figure out how that would happen. I did a count of commas in the CSV you provided and it looked good (consistent), and ran the CSV through some other tools I use for CSV analysis and they didn't complain, and the outputs looked correct to my eyes. So... ???</p>
<p>I even tried different combinations of kwargs to read_csv() and still got what I believe you want:</p>
<pre class=""lang-py prettyprint-override""><code>...

s = repr(df) # string-ify the previous df I printed above

for header_arg in [
    {&quot;skiprows&quot;: 2},
    {&quot;header&quot;: 2},
]:
    for index_arg in [
        {&quot;index_col&quot;: 0},
        {&quot;index_col&quot;: &quot;SampleNumber&quot;},
    ]:
        kwargs = dict(header_arg)
        kwargs.update(index_arg)

        df = pd.read_csv(&quot;input.csv&quot;, **kwargs)

        assert repr(df) == s

        print(f&quot;{kwargs}: good&quot;)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>...

{'skiprows': 2, 'index_col': 0             }: good
{'skiprows': 2, 'index_col': 'SampleNumber'}: good
{'header':   2, 'index_col': 0             }: good
{'header':   2, 'index_col': 'SampleNumber'}: good
</code></pre>
","0","Answer"
"79597069","79596631","<p>Let use python context <em>with</em> <code>calendar.different_locale</code> like this:</p>
<pre><code>import calendar
import pandas as pd

df = pd.DataFrame({'month_abbr':['Jan', 'Apr', 'May', 'Jun', 'Jul', 'Aug']})

keys=list(calendar.month_abbr)
with calendar.different_locale('fr_FR'):
    dd = dict(zip(keys, map(str.title, list(calendar.month_name))))

df['month_name'] = df['month_abbr'].map(dd)

df
</code></pre>
<p>Output:</p>
<pre><code>  month_abbr month_name
0        Jan    Janvier
1        Apr      Avril
2        May        Mai
3        Jun       Juin
4        Jul    Juillet
5        Aug       Août
</code></pre>
","5","Answer"
"79597089","79595809","<p>Since the missing data in the olive oil dataset is systematic specifically, UV absorption and FAEES values are missing for poor-quality (&quot;Lampante&quot;) samples traditional imputation methods are not appropriate. Instead, the best strategy is to treat the missingness itself as an informative signal. This can be done by creating new binary features indicating whether each value is missing (e.g., <code>UV_missing</code>, <code>FAEES_missing</code>).</p>
<p>For the missing UV and FAEES values, a conservative imputation method should be used, such as filling with a constant outside the normal range (e.g., -1), ensuring that no false patterns are introduced. This approach preserves all samples, retains important features, and allows classification models (especially tree-based ones) to learn from both the observed values and the missingness patterns. Dropping rows or columns would discard valuable information and significantly weaken the model’s ability to classify Lampante oils.</p>
<p>Therefore, I think creating missingness indicators combined with conservative imputation offers the most robust and informative solution.</p>
","0","Answer"
"79597187","79596631","<p>Question seems a bit unclear to me,<br />
Do you want to replace the month column in dataframe itself?<br />
If yes something like this can work:</p>
<pre><code>def changeMonth(month):
    if month==&quot;Jan&quot;:
            return &quot;Janvier&quot;
    elif month==&quot;Feb&quot;:
            return &quot;Février&quot;
    elif month==&quot;Mar&quot;:
        return &quot;Mars&quot;
    elif month==&quot;Apr&quot;:
        return &quot;Avril&quot;
    elif month==&quot;May&quot;:
        return &quot;Mai&quot;
    elif month==&quot;Jun&quot;:
        return &quot;Juin&quot;
    elif month==&quot;Jul&quot;:
        return &quot;Juillet&quot;
    elif month==&quot;Aug&quot;:
        return &quot;Août&quot;
    elif month==&quot;Sep&quot;:
        return &quot;Septembre&quot;
    elif month==&quot;Oct&quot;:
        return &quot;Octobre&quot;
    elif month==&quot;Nov&quot;:
        return &quot;Novembre&quot;
    elif &quot;Dec&quot;:
            return &quot;Décembre&quot;

    # If an exact match is not confirmed, this last case will be used if provided
    else:
        return &quot;&quot;

df['month']=df['month'].apply(changeMonth)

</code></pre>
<p>But if you want to change the month while just printing the list out, you can use list comprehension,</p>
<pre><code>french_month_list = [changeMonth(i) for i in df['ic_graph']['month'].tolist()]
</code></pre>
","0","Answer"
"79597616","79597604","<p>When dealing with files that might not always be available, you can use this approach to safely merge existing DataFrames without encountering initialization errors. The solution dynamically loads available data, identifies common columns across all successfully loaded datasets, and merges them automatically:</p>
<pre><code>import os
import pandas as pd
from functools import reduce

# Update these with your actual file paths and column headers
file_config = [
    ('data/source1.csv', ['id', 'name', 'date']),
    ('data/source2.csv', ['id', 'value', 'date']),
    ('data/source3.csv', ['id', 'category', 'notes'])
]

def safe_dataframe_merge():
    &quot;&quot;&quot;Handles DataFrame merging with missing file tolerance&quot;&quot;&quot;
    loaded_sets = []
    
    # Load available files
    for path, headers in file_config:
        if os.path.exists(path):
            loaded_sets.append(
                pd.read_csv(path, header=None, names=headers)
            )
    
    # Exit early if no data found
    if not loaded_sets:
        return pd.DataFrame()
    
    # Find columns common to all loaded DataFrames
    common_fields = reduce(
        lambda x, y: x.intersection(y),
        (df.columns for df in loaded_sets)
    )
    
    # Combine data while preserving structure
    return pd.concat(
        [df[common_fields] for df in loaded_sets],
        ignore_index=True
    )

# Usage example
merged_data = safe_dataframe_merge()
</code></pre>
<p>This implementation checks for file existence before loading, skips any missing sources entirely, and ensures only columns present in all available datasets get merged. The <code>reduce</code> operation efficiently finds common columns between all loaded DataFrames, while the list-based approach prevents reference errors to uninitialized variables. If none of the files exist, it gracefully returns an empty DataFrame instead of throwing errors. You can modify the <code>file_config</code> list to add or remove data sources without changing the core logic.</p>
","0","Answer"
"79597621","79597604","<p>Your code is already very good. You have many <strong>repeated</strong> elements in your current version. To make it cleaner, you could use <strong>list comprehension</strong> like <code>[function(x) for x in a_list]</code></p>
<pre><code>files = [file1, file2, file3]
headers = [header_1, header_2, header_3]

dfs = [pd.read_csv(f, header=None, names=h, sep=',') for f, h in zip(files, headers) if os.path.exists(f)]

if dfs:
    common_columns = set.intersection(*(set(df.columns) for df in dfs))
    concatenated_df = pd.concat([df[list(common_columns)] for df in dfs], ignore_index=True)
else:
    concatenated_df = pd.DataFrame()
</code></pre>
","1","Answer"
"79597977","79597131","<p>There isn't a date dtype in pandas, only a datetime. If you want arrow to convert date to datetime, use `date_as_object=False`. See the <a href=""https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table.to_pandas"" rel=""nofollow noreferrer"">pyarrow doc</a></p>
<pre><code>import datetime

import pyarrow as pa
import pyarrow.parquet as pq

table = pa.table({&quot;date&quot;: [datetime.date(2025, 1, 1)]})
pq.write_table(table, &quot;foo.parquet&quot;)

pq.read_table(&quot;foo.parquet&quot;).to_pandas(date_as_object=False)
</code></pre>
","0","Answer"
"79598445","79598340","<p>As stated in the question's comments, your code it's already fine and it only misses the actual return of a <code>DataFrame</code>, since your <code>result</code> is actually a <code>Series</code> and it lacks of the column name you desired (<code>seconds_to_first_pyrchase</code>).</p>
<p>It's pretty straight foreward that it's needed to transform the <code>Series</code> into a <code>DataFrame</code> and add the column name; this can be achieved simply by using a combination of <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rename.html#pandas-series-rename"" rel=""nofollow noreferrer""><code>Series.rename()</code></a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reset_index.html#pandas-series-reset-index"" rel=""nofollow noreferrer""><code>Series.reset_index()</code></a>.</p>
<p>From the docs of <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reset_index.html#pandas-series-reset-index"" rel=""nofollow noreferrer""><code>Series.reset_index()</code></a>:</p>
<blockquote>
<p><strong>Returns: Series or DataFrame or None</strong></p>
<blockquote>
<p>When <em>drop</em> is False (the default), a DataFrame is returned. The newly created columns will come first in the DataFrame, followed by the original Series values. When <em>drop</em> is True, a Series is returned. In either case, if <code>inplace=True</code>, no value is returned.</p>
</blockquote>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>result = result.rename('seconds_to_first_purchase').reset_index()
</code></pre>
<p>So, the entire code would be</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = [
    {'user_id': 'u1', 'event_type': 'login', 'timestamp': '2023-01-01 10:00:00'},
    {'user_id': 'u1', 'event_type': 'purchase', 'timestamp': '2023-01-01 10:05:00'},
    {'user_id': 'u2', 'event_type': 'login', 'timestamp': '2023-01-01 09:00:00'},
    {'user_id': 'u2', 'event_type': 'scroll', 'timestamp': '2023-01-01 09:03:00'},
    {'user_id': 'u3', 'event_type': 'login', 'timestamp': '2023-01-01 11:00:00'},
    {'user_id': 'u3', 'event_type': 'purchase', 'timestamp': '2023-01-01 11:20:00'},
]

df = pd.DataFrame(data)
df['timestamp'] = pd.to_datetime(df['timestamp'])

first_event = df.groupby('user_id')['timestamp'].min()
first_purchase = df[df['event_type'] == 'purchase'].groupby('user_id')['timestamp'].min()
result = (first_purchase - first_event).dt.total_seconds()
# Just to show the type and the result object (it's a Series)
print(type(result))
print(result)
# Transforming the Series into a DataFrame
result = result.rename('seconds_to_first_purchase').reset_index()
# Here you can see how the Series is now a DataFrame
print(type(result))
print(result)
</code></pre>
<p>Output:</p>
<pre><code>&lt;class 'pandas.core.series.Series'&gt;
user_id
u1     300.0
u2       NaN
u3    1200.0
Name: timestamp, dtype: float64
&lt;class 'pandas.core.frame.DataFrame'&gt;
  user_id  seconds_to_first_purchase
0      u1                      300.0
1      u2                        NaN
2      u3                     1200.0
</code></pre>
","0","Answer"
"79598695","79598498","<p>If you don't have a compiler installed you need to use a pre-compiled wheel for installation. Therefore make sure <code>wheel</code> is installed <code>pip3 install wheel</code> .</p>
<p>The list of available wheel files of <code>pandas</code> is listed here: <a href=""https://pypi.org/project/pandas/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/pandas/#files</a></p>
<p>There is no Windows version for the aarch64 architecture, only for x86_64. Based on this list I see two options:</p>
<ol>
<li><p>Use a Python x86_64 version and let Windows perform the x86_64 to aarch64 emulation (Python 3.9-3.13 wheels are available). This will slow down everything a bit.</p>
</li>
<li><p>Install a Linux Python version in a WSL environment. That should download and install and use the Linux aarm64 version of Pandas.</p>
</li>
</ol>
","0","Answer"
"79599446","79599081","<p>Really all you need to achieve what you want and maybe to have a bit neater code is the following;</p>
<p>I'm not sure of your reason to including the IO and file open bits of the code.<br>
Pandas ExcelWriter will create the XLSX file on it's own.<br></p>
<p>When writing the dataframe, you can include its header. The header will be Bolded, Center aligned and have a border by default, so there is no need to do separately.<br></p>
<p>Then you can just border the data rows.</p>
<p>With the merged header row, again just border all the cells in the row, no need to worry about excluding the merged cells.<br>
Then add the merges. I have changed this to a dictionary and loop (in this case doesn't really reduce the coding much).<br>
The merged cells will re-do their borders on the affected cells.</p>
<p>You can use the Autofit to resize the columns at the end or size as you wish</p>
<pre><code>import pandas as pd


output = 'rbd_inversion.xlsx'
df = &lt;whereever the data for the dataframe comes from&gt;

with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
    # Get dimensions
    num_rows, num_cols = df.shape
    start_col = 1  # Column B
    start_row = 2

    # Start data from row 3 (Excel), which is index 2
    # Write the dataframe with the Header
    df.to_excel(writer, sheet_name='Sheet1', startrow=start_row, startcol=start_col, index=False)

    workbook = writer.book
    worksheet = writer.sheets['Sheet1']

    # Formats
    header_format = workbook.add_format({'bold': True, 'align': 'center', 'valign': 'vcenter', 'border': 1})
    bordered_format = workbook.add_format({'border': 1})

    # Apply borders to written data (from B3 to K[bottom row])
    for row in range(start_row+1, start_row+num_rows+1):
        for col in range(start_col, num_cols+1):
            value = df.iloc[row-(start_row+1), col-1]
            worksheet.write(row, col, value, bordered_format)

    # Merged headers in row 2 (index 1)
    merge_row = 1
    # Create dictionary of the merged cells and their values
    merge_dict = {'F2:G2': 'portfolio', 'H2:I2': 'inversion information', 'J2:K2': '% of inversion'}
    # Border all the cells in the row with cell value None.
    for m_col in range(start_col, num_cols):  # All columns in the row (restrict if you feel you must)
        worksheet.write_blank(merge_row, m_col, None, bordered_format)
    # Run a loop on the merge dictionary and add each merge to the sheet
    for m_range, m_value in merge_dict.items():
        worksheet.merge_range(m_range, m_value, header_format)

    worksheet.autofit()

</code></pre>
<p><a href=""https://i.sstatic.net/EDR65IyZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EDR65IyZ.png"" alt=""Output Sheet"" /></a></p>
","3","Answer"
"79599516","79599353","<p>Prepare the replacement string for each iteration before inserting it in the <code>re.sub(pattern, replacement, text_string)</code> function.</p>
<p><em>Updated based on new information on test string and desired output.</em></p>
<p>REGEX PATTERN (Python re module):</p>
<pre><code>^func\([^),]+(,[^,)]+)+\)
</code></pre>
<p><em>Regex demo:</em> <a href=""https://regex101.com/r/aKyQSs/4"" rel=""nofollow noreferrer"">https://regex101.com/r/aKyQSs/4</a></p>
<p>PYTHON</p>
<pre><code>import re

df = {
    'col1':['func(a,b) and func(c,d)','func(a) and func(c)','func(b) and func(c,d)','func(a,b,c) and func(d,e,f)'], 
    'col2':['e','b','a','g']
}

df['col3'] = []

pattern = r'^func\([^),]+(,[^,)]+)+\)'

for i, x in enumerate(df['col2']): 
    replacement = r'func(%s)' % x
    result = re.sub(pattern, replacement, df['col1'][i])
    df['col3'].append((result))

print(df['col3'])


</code></pre>
<p>OUTPUT:</p>
<pre><code>['func(e) and func(c,d)', 'func(a) and func(c)', 'func(b) and func(c,d)', 'func(g) and func(d,e,f)']
</code></pre>
<p>REGEX NOTES:</p>
<ul>
<li><code>^</code> Match beginning of string <code>^</code>.</li>
<li><code>func\(</code> Match literal <code>func(</code></li>
<li><code>[^),]+</code> <em>Negated character class</em> <code>[^...]</code>. Match any character that is not a literal <code>)</code> or <code>,</code>, 1 or more times (<code>+</code>).</li>
<li><code>(</code>
<ul>
<li><code>,</code> Match literal <code>,</code>.</li>
<li><code>[^),]+</code> <em>Negated character class</em> <code>[^...]</code>. Match any character that is not a literal <code>)</code> or <code>,</code>,  1 or more times (<code>+</code>).</li>
</ul>
</li>
<li><code>)+</code>  Match 1 or more times (<code>+</code>).</li>
<li><code>\)</code> Match literal <code>)</code>.</li>
</ul>
","2","Answer"
"79599724","79598340","<p>I grouped by <code>user_id</code> to get the first event timestamp, then did the same for <code>'purchase'</code> events. Instead of subtracting the Series directly, I used <code>pd.concat()</code> to combine both into one DataFrame. Then I used <code>.assign()</code> with <code>.dt.total_seconds()</code> to calculate the difference. This gave me a clean DataFrame where I could see the first event, the first purchase (if it happened), and the time difference in seconds. It also kept users with no purchase in the output, which you needed. Made things much easier to debug and extend.</p>
<p>The entire code should be</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Sample data
data = [
    {'user_id': 'u1', 'event_type': 'login', 'timestamp': '2023-01-01 10:00:00'},
    {'user_id': 'u1', 'event_type': 'purchase', 'timestamp': '2023-01-01 10:05:00'},
    {'user_id': 'u2', 'event_type': 'login', 'timestamp': '2023-01-01 09:00:00'},
    {'user_id': 'u2', 'event_type': 'scroll', 'timestamp': '2023-01-01 09:03:00'},
    {'user_id': 'u3', 'event_type': 'login', 'timestamp': '2023-01-01 11:00:00'},
    {'user_id': 'u3', 'event_type': 'purchase', 'timestamp': '2023-01-01 11:20:00'},
]
df = pd.DataFrame(data)
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Step 1: First overall event per user
first_event = df.groupby('user_id')['timestamp'].min().rename('first_event_time')

# Step 2: First 'purchase' event per user
first_purchase = (
    df[df['event_type'] == 'purchase']
    .groupby('user_id')['timestamp']
    .min()
    .rename('first_purchase_time')
)

# Step 3: Combine and calculate time delta in seconds
result = (
    pd.concat([first_event, first_purchase], axis=1)
    .assign(seconds_to_first_purchase=lambda x: (
        (x['first_purchase_time'] - x['first_event_time']).dt.total_seconds()
    ))
    .reset_index()
)

print(result)
</code></pre>
","-1","Answer"
"79600130","79600043","<p>There appear to be at least two methods.</p>
<h1>1. <code>pyarrow.memory_map</code></h1>
<p>If you didn't know what to look for this would have been difficult to find.</p>
<pre class=""lang-py prettyprint-override""><code>with pyarrow.memory_map('df.arrow', 'r') as source:
    data = pyarrow.ipc.open_file(source).read_all()
    df = data.to_pandas()
</code></pre>
<h1>2. <code>pandas.read_feather</code></h1>
<p>Perhaps somewhat surprisingly, the <code>pandas.read_feather</code> method appears to be compatiable with the v2 format as well as the v1.</p>
<pre class=""lang-py prettyprint-override""><code>df = pandas.read_feather('df.arrow')
</code></pre>
<h1>Performance</h1>
<p>There does appear to be a significant performance difference between the two methods. I did some performance testing and found the following:</p>
<ul>
<li>The first method is about 2.5x faster.</li>
<li>Arrow is approximately 100x faster than CSV.</li>
</ul>
","2","Answer"
"79600494","79600443","<p>Here is the full code:</p>
<blockquote>
<p>Runtime: TotalMilliseconds : 483.8704. Performance optimized.</p>
</blockquote>
<pre><code>import pandas as pd

data = {
    'Name': ['John', 'John', 'Bob', 'Alice', 'Alice', 'Alice'],
    'Source': ['A', 'B', 'B', 'Z', 'Y', 'X'],
    'Description': ['Text1', 'Longer text', 'Text2', 'Longer text', 'The Longest text', 'Text3'],
    'Value': [1, 4, 2, 5, 3, 6]
}
df = pd.DataFrame(data)

df['desc_len'] = df['Description'].str.len()
max_len_idx = df.groupby('Name')['desc_len'].idxmax()
longest_rows = df.loc[max_len_idx].copy()

sorted_sources = (
    df.sort_values(['Name', 'Source'])
    .groupby('Name')['Source']
    .agg(list)
    .str.join(', ')
)

result = (
    longest_rows
    .merge(sorted_sources.rename('Source_agg'), 
           left_on='Name', 
           right_index=True)
    .drop(columns=['Source', 'desc_len'])
    .rename(columns={'Source_agg': 'Source'})
    [['Name', 'Source', 'Description', 'Value']]
    .sort_values('Name')
    .reset_index(drop=True)
)

print(result)
</code></pre>
<p>Output:</p>
<pre><code>    Name   Source       Description  Value
0  Alice  X, Y, Z  The Longest text      3
1    Bob        B             Text2      2
2   John     A, B       Longer text      4
</code></pre>
","0","Answer"
"79600517","79600443","<p>You can use a groupby aggregation to gather the sorted sources and the
location (indexes) of the longest description. From there you can do a
self join along those indexes to carry the values &amp; descriptions forward.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.DataFrame({
    'Name': ['John', 'John', 'Bob', 'Alice', 'Alice', 'Alice'],
    'Source': ['A', 'B', 'B', 'Z', 'Y', 'X'],
    'Description': ['Text1', 'Longer text', 'Text2', 'Longer text', 'The Longest text', 'Text3'],
    'Value': [1, 4, 2, 5, 3, 6]
})

print(
    df
    .assign(
        desc_length=lambda df: df['Description'].str.len().fillna(0)
    )
    .groupby('Name', as_index=False).agg(
        Source=('Source', sorted),
        length_indexes=('desc_length', 'idxmax'),
    )
    .merge(df.drop(columns=['Name', 'Source']), left_on='length_indexes', right_index=True)
    .drop(columns=['length_indexes'])
)

#     Name     Source       Description  Value
# 0  Alice  [X, Y, Z]  The Longest text      3
# 1    Bob        [B]             Text2      2
# 2   John     [A, B]       Longer text      4
</code></pre>
","0","Answer"
"79600525","79600443","<p>Something like this might work for you (watch out for indentation when copy):</p>
<pre class=""lang-py prettyprint-override""><code>df = df.sort_values(by='Description',
                    key=lambda col: col.str.len(),
                    ascending=False
                   )
       .groupby('Name', as_index=False)
       .agg({
           'Source': lambda x: ', '.join(sorted(x)),
           'Description': 'first',
           'Value': 'first'
       })
</code></pre>
<p>Output:</p>
<pre><code>    Name   Source       Description  Value
0  Alice  X, Y, Z  The Longest text      3
1    Bob        B             Text2      2
2   John     A, B       Longer text      4
</code></pre>
<p>Little note, passing string <code>'first'</code> as aggregation function is possible because first is function name (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.first.html#pandas-series-first"" rel=""nofollow noreferrer""><code>first</code></a> function) and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html#pandas-dataframe-agg"" rel=""nofollow noreferrer""><code>agg</code></a> function accepts strings with function names instead of true functions. You can probably replace 'first' with <code>lambda x: x.iloc[0]</code> if makes more sense (and because <code>first</code> function is deprecated since version 2.1).</p>
","1","Answer"
"79600554","79600418","<p>Here’s how you can implement it:</p>
<pre><code>import pandas as pd
data = {
    &quot;function&quot;: [&quot;test1&quot;, &quot;test2&quot;, &quot;test3&quot;, &quot;test4&quot;, &quot;test5&quot;, &quot;test6&quot;, 
                 &quot;test7&quot;, &quot;test8&quot;, &quot;test9&quot;, &quot;test10&quot;, &quot;test11&quot;, &quot;test12&quot;],
    &quot;service&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;AO&quot;, &quot;M&quot;, &quot;A&quot;, &quot;PO&quot;, &quot;MP&quot;, &quot;YU&quot;, &quot;Z&quot;, &quot;R&quot;, &quot;E&quot;, &quot;YU&quot;],
    &quot;month&quot;: [&quot;January&quot;, &quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, 
              &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot;]
}


df = pd.DataFrame(data)

def selectDataRange(var: str, df: pd.DataFrame):
    # Check if the month exists in the DataFrame
    if var not in df['month'].values:
        return &quot;Month not found in data.&quot;    
    month_index = df[df['month'] == var].index[0] # Get the index of the month
    # Define the start and end index for slicing
    start_index = max(month_index - 6, 0)  # Ensure it doesn't go below 0
    end_index = min(month_index + 6 + 1, len(df))  # Ensure it doesn't exceed the length of df
    # Select the rows and return the resulting DataFrame
    result_df = df.iloc[start_index:end_index]
    return result_df

selected_month = 'December'
result = selectDataRange(selected_month, df)
print(result)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>  Output:

   function service      month
5     test6      PO       June
6     test7      MP       July
7     test8      YU     August
8     test9       Z  September
9    test10       R    October
10   test11       E   November
11   test12      YU   December
</code></pre>
","0","Answer"
"79600618","79599116","<p>Are you looking to compare rows using productkey and return the difference between them like this?</p>
<pre><code>  tbl_type productkey productsubcategorykey weightunitmeasurecode
0   source          1           Electronics                    lb
1   target          1           Electronics                    kg
2   source          2                 Books                    oz
3   target          2                  Food                     g
4   source          3                Sports                    lb
5   target          3                  Home                    lb


productkey  tbl_type      productsubcategorykey weightunitmeasurecode
1           source,target                  None                 lb,kg
2           source,target            Books,Food                  oz,g
3           source,target           Sports,Home                  None

</code></pre>
<p>If so, <code>df.groupby().agg()</code> is a good option that is faster than a <code>for</code> loop. You can use this:</p>
<pre class=""lang-py prettyprint-override""><code>def compare_rows(x: pd.Series):
    assert len(x) == 2
    if x.iloc[0] != x.iloc[1]:
        return ','.join(x) 
    else:
        return None

df.groupby('productkey').agg(compare_rows)
</code></pre>
<p>That code can also be written as a single line:</p>
<pre class=""lang-py prettyprint-override""><code>df.groupby('productkey').agg(lambda x:','.join(x) if x.iloc[0] != x.iloc[1] else None)
</code></pre>
<p>I referenced several answers to <a href=""https://stackoverflow.com/q/27298178/20948146"">this question</a> (especially <a href=""https://stackoverflow.com/a/27298308/20948146"">1</a>, <a href=""https://stackoverflow.com/a/74641478/20948146"">2</a>, and <a href=""https://stackoverflow.com/questions/27298178/concatenate-strings-from-several-rows-using-pandas-groupby#comment122588906_59284106"">3</a>).</p>
","1","Answer"
"79600633","79600418","<p>I will guess that this is what you want:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = {
    &quot;function&quot;: [&quot;test1&quot;,&quot;test2&quot;,&quot;test3&quot;,&quot;test4&quot;,&quot;test5&quot;,&quot;test6&quot;,&quot;test7&quot;,&quot;test8&quot;,&quot;test9&quot;,&quot;test10&quot;,&quot;test11&quot;,&quot;test12&quot;],
    &quot;service&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;AO&quot;, &quot;M&quot; ,&quot;A&quot;, &quot;PO&quot;, &quot;MP&quot;, &quot;YU&quot;, &quot;Z&quot;, &quot;R&quot;, &quot;E&quot;, &quot;YU&quot;],
    &quot;month&quot;: [&quot;January&quot;,&quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot;]
}

df = pd.DataFrame(data)

selected_month = &quot;January&quot;
selected_month_idx = df[df[&quot;month&quot;] == selected_month].index[0]
six_months_indices = [i % len(df) for i in range(selected_month_idx - 2, selected_month_idx + 4)]
six_months_df = df.loc[six_months_indices] # add .reset_index(drop=True) if needed
</code></pre>
<p>Output:</p>
<pre><code>   function service     month
10   test11       E  November
11   test12      YU  December
0     test1       A   January
1     test2       B  February
2     test3      AO     March
3     test4       M     April
</code></pre>
<p>You will always get 6 months and selected month will always be third in those six months. Months will go in circle.</p>
<p>Little note, if you want to selected month appear on some other place in those six months play around with bounds in <code>range</code> function. For example, if you want selected month to be first use <code>range(selected_month_idx, selected_month_idx + 6)</code> or if you want it to be last use <code>range(selected_month_idx - 5, selected_month_idx + 1)</code>. For any in between or to change number of shown months play more with bounds.</p>
","0","Answer"
"79600799","79600747","<p>Because of how the descriptor protocol works, you need to set the side effect of the <code>method</code> instance returned by the <code>__get__</code> method of the thing you are patching, not the thing you are patching itself.</p>
<pre class=""lang-py prettyprint-override""><code>with patch(&quot;pandas.core.series.Series.apply&quot;) as mock_apply:
    mock_apply.__get__.return_value.side_effect = (lambda val: str(val))
</code></pre>
<p>When you write <code>a.foo(b)</code>, it's not <code>type(a).foo</code> that gets called directly, but rather <code>type(a).foo.__get__(a, type(a))</code>.</p>
","2","Answer"
"79601014","79600924","<p>json_normalize may work after adding prefixes, I didn't try but this puts all your data on one line.</p>
<pre><code>from flatten_json import flatten

prefix_json_2_explode = {}
for d in json_2_explode:
    prefix_json_2_explode.update({d['name'] + '_' + key: value for key, value in d.items()})
print(prefix_json_2_explode)
dict_flattened = (flatten(d, '.') for d in [prefix_json_2_explode])
df = pd.DataFrame(dict_flattened)
df

</code></pre>
<pre><code>  Foo_scalar Foo_units Foo_parameter.0.no_1 Foo_parameter.0.no_2  \
0         43         m                   45                 1038   

  Foo_parameter.0.no_3 Foo_name Yoo_scalar Yoo_units Yoo_parameter.0.no_1  \
0                  356      Foo       54.1         s                   78   

  Yoo_parameter.0.no_2 Yoo_parameter.0.no_3 Yoo_name Baz_scalar Baz_units  \
0                  103                  356      Yoo     1123.1        Hz   

  Baz_parameter.0.no_1 Baz_parameter.0.no_2 Baz_parameter.0.no_3 Baz_name  
0                   21                   43                 3577      Baz  
</code></pre>
","0","Answer"
"79601173","79599353","<p>Problem is because <code>.+</code> can match also <code>)</code> - and regex is &quot;greedy&quot; and it tries to match the longest possible string - so in your code <code>.+</code> matches <code>b,c) and func(d,e,f</code> instead of <code>b,c</code></p>
<p>It need to use <code>[^\)]+</code> to match everything excepte <code>)</code> - so it will stop on first <code>)</code> instead of second <code>)</code> (or on any further <code>)</code> if you will have more <code>func()</code>)</p>
<pre><code>r'^(func\()[^\)]+,[^\)]+(\).+)'
</code></pre>
<hr />
<p>Full working code.</p>
<p>I added code which uses <code>df.apply()</code> instead of <code>for</code>-loop</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    'col1':[
        'func(a,b) and func(c,d) and func2(z)',
        'func(a) and func(c) and func2(z)',
        'func(b) and func(c,d) and func2(z)',
        'func(a,b,c) and func(d,e,f) and func2(z)'],
    'col2':['e','b','a','g']})

import re

# df['col3'] = ''
#
# for i,x in enumerate(df['col2']):
#     pattern = r'^(func\()[^\)]+,[^\)]+(\).+)'
#     replacement = fr'\1{x}\2'
#     df.loc[i,'col3'] = re.sub(pattern,replacement,df.loc[i,'col1'])
#     if df.loc[i,'col3'] != df.loc[i,'col1']:
#         print(df.loc[i,'col1'],'---&gt;',df.loc[i,'col3'])
#

def convert(row):
    pattern = r'^(func\()[^\)]+,[^\)]+(\).+)'
    replacement = fr'\1{row['col2']}\2'

    result = re.sub(pattern, replacement, row['col1'])

    if result != row['col1']:
        print(row['col1'], '---&gt;', result)

    return result

df['col3'] = df.apply(convert, axis=1)

print(df)


print('--- changed ---')

changed = df[ df['col3'] != df['col1'] ]
changed.apply(lambda row:print(row['col1'],'---&gt;', row['col3']), axis=1)


</code></pre>
","1","Answer"
"79601805","79601755","<p>Consider the recently added <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.case_when.html"" rel=""nofollow noreferrer""><strong><code>pandas.Series.case_when</code></strong></a> method.</p>
<pre class=""lang-py prettyprint-override""><code>df['windows_version'] = df['twitter'].case_when(
    [
        (df['twitter'].str.lower().str.contains('windows 11'), 'windows 11'),
        (df['twitter'].str.lower().str.contains('windows 10'), 'windows 10'),
        (pd.Series(True), 'windows 8 or older')
    ]
)
</code></pre>
","1","Answer"
"79601839","79601812","<p>I would use <code>df.apply()</code> with a custom function.</p>
<p>This is a straightforward example.</p>
<pre><code>import numpy as np
from functools import partial

def split_addresses(row, col):
    r = row[col].split(' ')
    if len(r) &lt; 2:
        first_word = &quot; &quot;.join(r)
        last_word = np.nan
    else:
        first_word = &quot; &quot;.join(r[:-1])
        last_word = r[-1]
    return first_word, last_word

_fun = partial(split_addresses, col='Address2') #chose which columns you want to process

splits = df.apply(_fun, axis=1)
df[&quot;StringStart&quot;] = pd.Series([s[0] for s in splits])
df[&quot;StringEnd&quot;] = pd.Series([s[1] for s in splits])

print(df)

                        Address1    Address2 StringStart   LastWord  StringEnd
0                 3 Steel Street  Saltmarket  Saltmarket     Street        NaN
1            1 Arnprior Crescent  Castlemilk  Castlemilk   Crescent        NaN
2  40 Bargeddie Street Blackhill   Blackhill   Blackhill  Blackhill        NaN
</code></pre>
","2","Answer"
"79601841","79601812","<p>Using <code>str.extract()</code> might be better for several reasons: it handles all cases, offers precision with regular expressions, and eliminates the risk of value errors.</p>
<pre><code>import pandas as pd

data = {
    'Address1': ['3 Steel Street', '1 Arnprior Crescent', '40 Bargeddie Street Blackhill'],
    'Address2': ['Saltmarket', 'Castlemilk East', 'Blackhill']
}
df = pd.DataFrame(data)

df[['StringStart', 'LastWord']] = df['Address1'].str.rsplit(' ', n=1, expand=True)

df[['FirstWord_Address2', 'Remaining_Address2']] = (
    df['Address2'].str.extract(r'^(\S+)\s*(.*)$')
)

print(df)
</code></pre>
<p>Or:</p>
<pre><code>df[['Address1_Prefix', 'Address1_LastWord']] = df['Address1'].str.extract(r'^(.*\b)\s+(\S+)$')

df[['Address2_FirstWord', 'Address2_Remaining']] = df['Address2'].str.extract(r'^(\S+)\s*(.*)$')
</code></pre>
<p>Output:</p>
<pre><code>                        Address1         Address2          StringStart   LastWord FirstWord_Address2 Remaining_Address2
0                 3 Steel Street       Saltmarket              3 Steel     Street         Saltmarket
1            1 Arnprior Crescent  Castlemilk East           1 Arnprior   Crescent         Castlemilk               East
2  40 Bargeddie Street Blackhill        Blackhill  40 Bargeddie Street  Blackhill          Blackhill
</code></pre>
","3","Answer"
"79601910","79601886","<p>2nd Attempt:</p>
<pre><code>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

print(df)

plt.figure(figsize=(12, 8))
cmap = LinearSegmentedColormap.from_list(
    name='redgreenred', 
    colors=['darkred', 'red','lightgreen','darkgreen','lightgreen','red','darkred']
)
sns.heatmap(df, cmap=cmap, center= 0, annot=True, vmin=-.06, vmax=.06)
plt.title(&quot;Heatmap&quot;)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/KPQny0qG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KPQny0qG.png"" alt=""enter image description here"" /></a></p>
<p>IIUC, you can use <code>matplotlib.colors.LinearSegmentColormap</code>:</p>
<pre><code>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

print(df)

plt.figure(figsize=(12, 8))
cmap = LinearSegmentedColormap.from_list(
    name='redgreenred', 
    colors=['red', 'green','red']
)
sns.heatmap(df, cmap=cmap, center= 0, annot=True, vmin=-.03, vmax=.03)
plt.title(&quot;Heatmap&quot;)
plt.show()
</code></pre>
<p>Output:</p>
<pre><code>             min   mean    max
ALU       -0.008  0.000  0.034
BRENT     -0.017  0.000  0.023
CU        -0.011  0.000  0.013
DTD_BRENT -0.011  0.000  0.019
GASOIL    -0.009  0.000  0.035
GOLD      -0.008  0.000  0.009
HH        -0.033 -0.001  0.009
JET_CCN   -0.009  0.000  0.033
</code></pre>
<p><a href=""https://i.sstatic.net/82p66ztT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82p66ztT.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79601935","79601755","<p>The problem with your function is that you are using the entire series each loop. Your ifs looks like this:</p>
<pre><code>if pd.Series([True, True, False, True])
</code></pre>
<p>You feed the same series over and over, but python expects a single Boolean value, not an entire Series. You need to use .all() or .any() to return a single Boolean value based on your requirements. In this case it would be wrong to use either one of them because we need to apply the function to each value, and not use the whole series over and over.</p>
<p>Series.apply is just a fancy for loop(loops trough each value as a for loop). You apply the function to each value individually.</p>
<p>This is how your function should look like.</p>
<pre><code>def windows_ver(s):
    s = s.lower()
    if 'windows 11' in s:
        return 'windows 11'
    elif 'windows 10' in s :
        return 'windows 10'
    return 'windows 8 or older'

df['twitter'].apply(windows_ver)
</code></pre>
<p>A better option is to use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.select.html"" rel=""nofollow noreferrer"">numpy select</a> because is much faster than apply.</p>
<pre><code>import numpy as np

# Your series
s = df['twitter'].str.lower()
# Your conditions
cond = [s.str.contains('windows 11'), s.str.contains('windows 10')]
# The result for each condition
res = ['windows 11', 'windows 10']

df['windows_version'] = np.select(cond, res, 'windows 8 or older')
# Windows 8 or older is the &quot;else&quot; in case all your conditions fail and it will return windows 8 as a default value.
</code></pre>
<p>End result:</p>
<pre><code>                                            twitter    windows_version
    I just upgraded to Windows 11 and it's amazing!         windows 11
                    Still using Windows 10 at work.         windows 10
            My old laptop runs Windows 8 just fine. windows 8 or older
                   Remember Windows XP? Good times. windows 8 or older
                    Windows 11 is great for gaming.         windows 11
    I'm not switching from Windows 10 anytime soon.         windows 10
          Using Windows Vista on my backup machine. windows 8 or older
Just bought a new PC with Windows 11 pre-installed.         windows 11
</code></pre>
","4","Answer"
"79602487","79601812","<p><strong>TL;DR</strong></p>
<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>.reindex</code></a> to add missing columns:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

(
    pd.Series(['Hello', 'world'])
      .str.split(n=1, expand=True)
      .reindex(pd.RangeIndex(2), axis=1)
)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>       0   1
0  Hello NaN
1  world NaN
</code></pre>
<hr />
<p>With <code>expand=True</code> both <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>Series.str.split</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.rsplit.html"" rel=""nofollow noreferrer""><code>.rsplit</code></a> will return a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>pd.DataFrame</code></a> with a default <a href=""https://pandas.pydata.org/docs/reference/api/pandas.RangeIndex.html"" rel=""nofollow noreferrer""><code>pd.RangeIndex</code></a>. Hence, with <code>n=1</code>, the result has either one column (<code>0</code>) or two (<code>0, 1</code>, or: <code>pd.RangeIndex(n+1)</code>).</p>
<p>Realizing this, you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>df.reindex</code></a> with <code>axis=1</code> to ensure a consistent number of output columns. Missing columns get added with <code>NaN</code> values. Here's a wrapper:</p>
<pre class=""lang-py prettyprint-override""><code>def split_expand(series, n=1, rsplit=False):
    splitter = series.str.rsplit if rsplit else series.str.split
    result = splitter(n=n, expand=True)
    if result.shape[1] &lt; n+1:
        return result.reindex(pd.RangeIndex(n+1), axis=1)
    return result

df[['StringStart', 'LastWord']] = split_expand(df['Address1'], rsplit=True)
df[['FirstWord', 'StringEnd']] = split_expand(df['Address2'])
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>                        Address1    Address2          StringStart   LastWord  \
0                 3 Steel Street  Saltmarket              3 Steel     Street   
1            1 Arnprior Crescent  Castlemilk           1 Arnprior   Crescent   
2  40 Bargeddie Street Blackhill   Blackhill  40 Bargeddie Street  Blackhill   

    FirstWord  StringEnd  
0  Saltmarket        NaN  
1  Castlemilk        NaN  
2   Blackhill        NaN   
</code></pre>
","1","Answer"
"79603328","79603293","<p>Pandas is not what you want. The <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate"" rel=""nofollow noreferrer"">orig.interpolate(axis=1)</a> is your doom.</p>
<p>By default, <code>pandas.DataFrame.interpolate()</code> assumes the columns are equally spaced when interpolating across columns. Even though you have labels (temperatures), it ignores them. <code>(axis=1)</code> tells it to go column by column, equally spaced, with no care for label.</p>
<p>I was about to represent SciPy's <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html"" rel=""nofollow noreferrer"">interp1d</a> as an alternative, but it's deprecated, which I did not know until now. The replacements are in their <a href=""https://docs.scipy.org/doc/scipy-1.15.2/tutorial/interpolate/1D.html#tutorial-interpolate-1dsection"" rel=""nofollow noreferrer"">documentation</a>, which include <a href=""https://numpy.org/doc/stable/reference/generated/numpy.interp.html#numpy.interp"" rel=""nofollow noreferrer"">numpy.interp()</a> and more.</p>
","1","Answer"
"79603335","79603298","<p>The <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html"" rel=""noreferrer"">documentation for the <code>pandas.Timestamp</code> type</a> says:</p>
<blockquote>
<p>Timestamp is the pandas equivalent of python’s Datetime and is interchangeable with it in most cases.</p>
</blockquote>
<p>So we can look up <a href=""https://docs.python.org/3/library/datetime.html#datetime-objects"" rel=""noreferrer"">the Python documentation for <code>datetime</code> objects</a>, where we find:</p>
<blockquote>
<p>Like a date object, datetime assumes the current Gregorian calendar extended in both directions; like a time object, datetime assumes there are exactly 3600*24 seconds in every day.</p>
</blockquote>
<p>In other words, it assumes that the current rules for calculating leap years apply at any point in history, even though they were actually introduced in 1582, and adopted by different countries over the next few centuries. (The technical term for this is a &quot;proleptic Gregorian calendar&quot;.)</p>
<p>Standard Python has <a href=""https://docs.python.org/3/library/datetime.html#datetime.MINYEAR"" rel=""noreferrer"">a <code>datetime.MINYEAR</code> constant</a>:</p>
<blockquote>
<p>The smallest year number allowed in a date or datetime object. MINYEAR is 1.</p>
</blockquote>
<p>So the lowest year divisible by 4, (and not by 100, so meeting the Gregorian definition of leap year as well as the Julian one) would be <strong>4</strong>.</p>
<p>However, Pandas also has <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.min.html"" rel=""noreferrer""><code>pandas.Timestamp.min</code></a>:</p>
<blockquote>
<p>Timestamp.min = Timestamp('1677-09-21 00:12:43.145224193')</p>
</blockquote>
<p>(In case you're wondering, that's 2<sup>63</sup> nanoseconds before <a href=""https://en.wikipedia.org/wiki/Unix_time"" rel=""noreferrer"">January 1, 1970</a>, i.e. the limit of a 64-bit signed integer with nanosecond resolution.)</p>
<p>So you probably want a year after 1677, meaning <strong>the earliest available year would be 1680</strong>.</p>
","8","Answer"
"79603346","79603298","<p>I ended up doing the same reasoning <a href=""https://stackoverflow.com/a/79603335/4518341"">as @IMSoP</a> but using pandas only so less general:</p>
<p>First I searched for the very first available year in Pandas:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

start_date = pd.Timestamp.min
</code></pre>
<p>Then I wrote a small function to check if a year is a leap year and applied to the range of date from my first ever year to the next 50 (which is of course overkill but I felt safer):</p>
<pre class=""lang-py prettyprint-override""><code>def is_leap_year(year):
    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)

for year in range(start_date.year, start_date.year + 50):
    if is_leap_year(year):
        print(f&quot;Oldest leap year in pandas: {year}&quot;)
        break
</code></pre>
<p>final answer is consistent with the datetime based answer (which is great):</p>
<pre class=""lang-none prettyprint-override""><code>Oldest leap year in pandas: 1680
</code></pre>
","2","Answer"
"79603594","79570591","<p>here my solution to solve the issue of memory</p>
<pre><code>import os.path
import csv
from io import BytesIO`
import pandas as pd

# create the path to the CSV file
data_local_path = os.getcwd() + '\\'
csv_filename = 'fr.openfoodfacts.org.products.csv'
csv_local_path = data_local_path + csv_filename
print (csv_local_path)

#Generate the path to the futur file
clean_filename = 'fr.openfoodfacts.org.products-clean.csv'
clean_local_path = data_local_path + clean_filename
print(clean_local_path)

#replace some delimiters by delimiter tab
if not os.path.isfile(clean_local_path):
    with open(csv_local_path, 'r',encoding='utf-8') as csv_file, open(clean_local_path, 'a', encoding='utf-8') as clean_file:
        for row in csv_file:
            clean_file.write(row.replace('\n\t', '\t'))

# read and load the csv file part by part
# Define the chunk size
chunk_size = 10000  # Adjust the chunk size as needed
# Initialize an empty list to store the chunks
chunks = []
# Read the CSV file in chunks
for chunk in pd.read_csv(clean_local_path, quoting=csv.QUOTE_NONE, sep='\t', encoding='utf_8', on_bad_lines='warn', chunksize=chunk_size):
    # Process each chunk (e.g., append to a list)
    chunks.append(chunk)

# Concatenate all chunks into a single DataFrame if needed
data = pd.concat(chunks, ignore_index=True)
# display dataframe info
data.info()
</code></pre>
","0","Answer"
"79603688","79570591","<p>Based on errors you have posted it seems you have at least two problems:</p>
<ul>
<li>CSV format error (columns are not interpreted correctly);</li>
<li>Memory limitation.</li>
</ul>
<p>Instead of using <code>pandas</code> that will load the whole the dataset into the memory you may switch for <code>duckdb</code> which:</p>
<ul>
<li>Is able to parse this CSV and cope with CSV errors;</li>
<li>Can query the file without loading the whole dataset into memory.</li>
</ul>
<p>If you try to aggregate it, you will find there is at least one ill shaped row:</p>
<pre><code>InvalidInputException: Invalid Input Error: CSV Error on Line: 63096
</code></pre>
<p>Error proof version goes as follow:</p>
<pre><code>import duckdb as db

dataset = db.read_csv('en.openfoodfacts.org.products.csv', sep=&quot;\t&quot;, header=True, ignore_errors=True)
</code></pre>
<p>Then you can manipulate it without loading the whole dataset into memory:</p>
<pre><code>sample = db.query(&quot;SELECT * FROM dataset LIMIT 100;&quot;)
shape = dataset.shape  # (3742827, 209)
query = db.query(&quot;SELECT MAX(created_t) FROM dataset;&quot;)  # 1746166648 
</code></pre>
<p>If you really need a Pandas DataFrame, provided you have enough memory, you can you can generate it by issuing:</p>
<pre><code>df = dataset.df()
</code></pre>
","0","Answer"
"79604039","79604001","<p>Your method is inefficient as it explodes then drops the duplicates. Ensure to drop the duplicates first then merge:</p>
<pre><code>d = df.mask(df.eq(''))[['code', 'Name', 'Age']].drop_duplicates()
df.merge(d, how = 'outer', on = ['Name', 'Age']).dropna(subset='code_y') 

    link code_x  Name  Age code_y
0      1     xx   Tom   20     xx
1      1     xx   Tom   20     xy
3      2     xx   Tom   20     xx
4      2     xx   Tom   20     xy
6      3     xy   Tom   20     xx
7      3     xy   Tom   20     xy
9      4          Tom   20     xx
10     4          Tom   20     xy
12     5     aa  nick   21     aa
13     5     aa  nick   21     ab
14     6     ab  nick   21     aa
15     6     ab  nick   21     ab
16     7     aa  nick   21     aa
17     7     aa  nick   21     ab
</code></pre>
","1","Answer"
"79604042","79604001","<p>In this case, you don't need to use groupby or explode to get the same result. Just do some filtering using the regex and drop the duplicates before the merge.</p>
<pre><code>cols = ['code', 'Name', 'Age']
temp = df.loc[~df['code'].str.contains(r'^\s*$', na=True), cols].drop_duplicates()

df2 = pd.merge(df, temp, on=['Name', 'Age'])
</code></pre>
<p>End result:</p>
<pre><code>    link code_x  Name  Age code_y
0      1     xx   Tom   20     xx
1      1     xx   Tom   20     xy
2      2     xx   Tom   20     xx
3      2     xx   Tom   20     xy
4      3     xy   Tom   20     xx
5      3     xy   Tom   20     xy
6      4    NaN   Tom   20     xx
7      4    NaN   Tom   20     xy
8      5     aa  nick   21     aa
9      5     aa  nick   21     ab
10     6     ab  nick   21     aa
11     6     ab  nick   21     ab
12     7     aa  nick   21     aa
13     7     aa  nick   21     ab
</code></pre>
","1","Answer"
"79604115","79604038","<p>I'm indebted to <a href=""https://stackoverflow.com/a/31002878/2749397"">this answer</a>.</p>
<p><a href=""https://i.sstatic.net/Jpx7lmx2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jpx7lmx2.png"" alt=""enter image description here"" /></a></p>
<p>The code that produced the scrollable figure is below, using only Numpy and Matplotlib.</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Slider
from itertools import product

n_bars = 100
n_shown = 10

names = [&quot;&quot;.join(t) for t in product(&quot;ABCDE&quot;, &quot;ABCDE&quot;, &quot;ABCDE&quot;)][:n_bars]
np.random.seed(20250502)
values = np.random.randint(1, 11, n_bars)

fig, ax_d = plt.subplot_mosaic(&quot;a&quot; * 7 + &quot;b&quot;)
ax_d[&quot;a&quot;].barh(names, values)
ax_d[&quot;a&quot;].axis([-0.2, 10.2, -0.5, n_shown-0.5])
ax_d[&quot;a&quot;].invert_yaxis()

spos = Slider(ax_d[&quot;b&quot;], &quot;&quot;, -0.5, n_bars-n_shown-0.5, orientation=&quot;vertical&quot;)
def update(val):
    pos = spos.val
    ax_d[&quot;a&quot;].axis([-0.2, 10.2, pos+n_shown, pos])
    fig.canvas.draw_idle()
spos.on_changed(update)

plt.show()
</code></pre>
<hr />
<p>The code above is OK, but I want to post a more refined (overly refined?) version of it… that somehow addresses <strong>the OP desire of an answer dealing with data incoming from a CSV file</strong>: below, I write a CSV file from my data and next I read from it (3 statements, no external modules).</p>
<p><a href=""https://i.sstatic.net/MhBJszpB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MhBJszpB.png"" alt=""enter image description here"" /></a></p>
<p>Below the code, it's quite longer than the original one, but not more complex. Note that I've inverted the direction of scrolling and suppressed the display of the value from the Slider object.</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Slider
from itertools import product

# create some data
n_bars = 100
_names = [&quot;&quot;.join(t) for t in product(&quot;ABCDE&quot;, &quot;ABCDE&quot;, &quot;ABCDE&quot;)][:n_bars]
np.random.seed(20250502)
_values = np.random.randint(1, 11, n_bars)

# write data to CSV
with open(&quot;dummy.csv&quot;, &quot;w&quot;) as out:
    out.write(&quot;Names,Values\n&quot;)
    for n, v in zip(_names, _values):
        out.write(&quot;%s,%d\n&quot; % (n, v))

# read data from CSV
with open(&quot;dummy.csv&quot;) as inp:
    inp.readline()
    names, values = zip(
        *[[n, int(v)] for n, v in [line.strip().split(&quot;,&quot;) for line in inp.readlines()]]
    )

# how many data points? how many bars in the scrollable window
n_bars = len(names)
n_shown = 10

# we need an Axes for the bars, a thinner Axes for the Slider
bars, slider = &quot;b&quot;, &quot;s&quot;
fig, ax_d = plt.subplot_mosaic(bars * 19 + slider, layout=&quot;constrained&quot;)
fig.suptitle(&quot;A Scrollable Window Over a Largish Plot&quot;)

# plot the bars
ax_d[bars].barh(names, values)
ax_d[bars].axvline(color=&quot;grey&quot;, lw=1)
ax_d[bars].set_xlabel(&quot;Value&quot;)
ax_d[bars].set_ylabel(&quot;Name&quot;)

# adjust x- and y-axis limits
ax_d[bars].set_xlim(xmin=-0.2)
ax_d[bars].set_ylim([-0.5, n_shown - 0.5])
# inverting the y-axis forces the names to go from top to bottom
ax_d[bars].invert_yaxis()
# instantiate the Slider inside the thinner Axes
s = Slider(
    ax_d[slider], &quot;&quot;, 0, n_bars - n_shown, orientation=&quot;vertical&quot;, valinit=0, valstep=1
)
s.valtext.set_visible(False)  # ty https://stackoverflow.com/a/15477329/2749397
ax_d[slider].invert_yaxis()


# the callback
def update(val):
    # higher y value comes first because I've inverted the y-axis
    ax_d[bars].set_ylim([val + n_shown - 0.5, val - 0.5])
    fig.canvas.draw_idle()


# when we call the callback?
s.on_changed(update)

# That's All Folks
plt.show()
</code></pre>
","3","Answer"
"79604122","79600443","<p>This is numpy solution. But <code>np.lexsort()</code> has <strong>O(N log N)</strong> time complexity. Hence not Good for Huge datasets:</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({
    'name': ['John', 'John', 'Bob', 'Alice', 'Alice', 'Alice'],
    'source': ['A', 'B', 'B', 'Z', 'Y', 'X'],
    'text': ['Text1', 'Longer text', 'Text2', 'Longer text', 'The Longest text', 'Text3'],
    'value': [1, 4, 2, 5, 3, 6]
})
'''
Convert the string names into numerical categories for efficient sorting.
This creates two things: df['name_cat'] (the numerical categories) 
and name_idx (the unique names).
'''
df['name_cat'], name_idx = pd.factorize(df['name'])
'''
Next, the code calculates the length of each string in the 'text' column.
'''
txt_len = df['text'].str.len().to_numpy()

'''
core of finding the longest text per name involves sorting.
-txt_len : Sorting by negative length puts the longest texts first 
within each name group.
'''
text_order = np.lexsort((-txt_len, df['name_cat']))

sorted_names = df['name_cat'].to_numpy()[text_order]
'''
To find the longest text for each unique name, we need to identify
the first occurrence of each name in the sorted list.
Since the list is sorted by name (and then by reverse text length), 
the first occurrence of each name will correspond to the 
row with the longest text for that name.
'''
_, unique_sorted_names_idx = np.unique(sorted_names, return_index =True)
'''
aa = TextOrder[unique_sorted_names_idx]
bb = df['text'][aa]
print(bb)
Now, we will use this concept here.
'''
longest_text_idx  = text_order[unique_sorted_names_idx]

names_with_LongestText = df.iloc[longest_text_idx][['name','text','value']].set_index('name')
'''
                   text  value
name                          
John        Longer text      4
Bob               Text2      2
Alice  The Longest text      3
'''
all_sources = df.groupby('name')['source'].agg(lambda x : ','.join(sorted(x)))
'''
name
Alice    X,Y,Z
Bob          B
John       A,B
Name: source, dtype: object
'''
final_data = names_with_LongestText.copy()

final_data['all_sources'] = final_data.index.map(all_sources)
final_data.reset_index(inplace=True)
print(final_data)
'''
    name              text  value all_sources
0   John       Longer text      4         A,B
1    Bob             Text2      2           B
2  Alice  The Longest text      3       X,Y,Z
'''

</code></pre>
","0","Answer"
"79604222","79604183","<p>If you want to randomly sample values from the &quot;temperature&quot; column for each day, up to 10 values per day, but also want to handle days with fewer than 10 entries, here's my suggestion on how to do it.</p>
<p>This code checks how many rows there are per day — if there are fewer than 10, it just takes as many as possible. If there are zero, it skips that day entirely. The best part is that it gives you back the original <code>DataFrame</code> rows, not the <code>Series</code> from <code>Groupby</code>.</p>
<pre><code>import pandas as pd

def sample_temperature(group, n=10):
    if len(group) == 0:
        return pd.DataFrame()  # return empty if there is nothing in the group
    return group.sample(min(len(group), n))

# Make sure the 'time' column is in datetime format
df['time'] = pd.to_datetime(df['time'])

# Group by day and sample
sampled_df = (df.groupby(pd.Grouper(key='time', freq='D')).apply(lambda g: sample_temperature(g, n=10)).reset_index(drop=True))
</code></pre>
","1","Answer"
"79604604","79600443","<p>Solution using Polars :</p>
<pre><code>import polars as pl

df = pl.DataFrame({
    'name': ['John', 'John', 'Bob', 'Alice', 'Alice', 'Alice'],
    'source': ['A', 'B', 'B', 'Z', 'Y', 'X'],
    'text': ['Text1', 'Longer text', 'Text2', 'Longer text', 'The Longest text', 'Text3'],
    'value': [1, 4, 2, 5, 3, 6]
})


longest_text = df.with_columns(
    pl.col('text').str.len_chars().alias('text_len'))\
    .sort(['name', 'text_len'], descending=[False, True])\
    .group_by('name').agg( pl.col(['text','value']).first() )
   
'''
shape: (3, 3)
┌───────┬──────────────────┬───────┐
│ name  ┆ text             ┆ value │
│ ---   ┆ ---              ┆ ---   │
│ str   ┆ str              ┆ i64   │
╞═══════╪══════════════════╪═══════╡
│ Alice ┆ The Longest text ┆ 3     │
│ Bob   ┆ Text2            ┆ 2     │
│ John  ┆ Longer text      ┆ 4     │
'''

sourceAgg = df.group_by('name').agg(
pl.col('source').unique()
).with_columns(pl.col('source').list.join(', '))
'''
┌───────┬─────────┐
│ name  ┆ source  │
│ ---   ┆ ---     │
│ str   ┆ str     │
╞═══════╪═════════╡
│ Bob   ┆ B       │
│ Alice ┆ Z, X, Y │
│ John  ┆ B, A    │
└───────┴─────────┘
'''

mix = longest_text.join( sourceAgg, on = 'name')

'''
┌───────┬──────────────────┬───────┬─────────┐
│ name  ┆ text             ┆ value ┆ source  │
│ ---   ┆ ---              ┆ ---   ┆ ---     │
│ str   ┆ str              ┆ i64   ┆ str     │
╞═══════╪══════════════════╪═══════╪═════════╡
│ Bob   ┆ Text2            ┆ 2     ┆ B       │
│ Alice ┆ The Longest text ┆ 3     ┆ Y, X, Z │
│ John  ┆ Longer text      ┆ 4     ┆ A, B    │
└───────┴──────────────────┴───────┴─────────┘
'''
</code></pre>
","0","Answer"
"79606029","79605997","<p>Take a look at the documentation for more info: <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html</a></p>
<p>Get all your your columns in a list</p>
<pre><code>print(df_CL_HHA.columns.tolist()) 
</code></pre>
<p>then depending on which columns you want to delete, specify them below:</p>
<pre><code> df_CL_HHA_2=df_CL_HHA.drop(columns=['column1', 'column2'])
</code></pre>
<p>It is a question of how you are using the key word parameter (columns), if you want to specify the columns to drop: given the df below:</p>
<pre><code>df
   A  B   C   D
0  0  1   2   3
1  4  5   6   7
2  8  9  10  11
</code></pre>
<p>From the docs its as simple as:</p>
<pre class=""lang-py prettyprint-override""><code>df.drop(columns=['B', 'C'])
   A   D
0  0   3
1  4   7
2  8  11
</code></pre>
","0","Answer"
"79606124","79605997","<p><strong><a href=""https://stackoverflow.com/help/minimal-reproducible-example"">Minimal Reproducible Example</a></strong></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.DataFrame([['M1', 1.0, 'M1', 3.0],
                   ['M2', 2.0, 'M2', 4.0]], columns=['ENZYME','AVG']*2)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># df

  ENZYME  AVG ENZYME  AVG # remove only second 'ENZYME' column
0     M1  1.0     M1  3.0
1     M2  2.0     M2  4.0
</code></pre>
<hr />
<p><strong>Option 1</strong> (via <code>df.loc</code>)</p>
<pre class=""lang-py prettyprint-override""><code>out = df.loc[:, ~(df.columns.duplicated() &amp; (df.columns=='ENZYME'))].copy()
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>  ENZYME  AVG  AVG
0     M1  1.0  3.0
1     M2  2.0  4.0
</code></pre>
<p><strong>Explanation</strong></p>
<ul>
<li>Check <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.duplicated.html"" rel=""nofollow noreferrer""><code>Index.duplicated</code></a> for <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html"" rel=""nofollow noreferrer""><code>df.columns</code></a> and <code>df.columns == 'ENZYME'</code> and get the inverse of the combined condition (<code>~</code>).</li>
<li>Use inside <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>df.loc</code></a> and chain <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html"" rel=""nofollow noreferrer""><code>.copy</code></a> to avoid potential issues further down the line (see <a href=""https://stackoverflow.com/q/20625582/18470692"">SettingWithCopyWarning</a>).</li>
</ul>
<hr />
<p><strong>Option 2</strong> (via <code>df.set_index</code> + <code>df.drop</code>)</p>
<pre class=""lang-py prettyprint-override""><code>out2 = df.set_index(df.iloc[:, 0]).drop(columns='ENZYME').reset_index()

out2.equals(out)
# True
</code></pre>
<p>Here we set the first 'ENZYME' column as the index (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>df.set_index</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>df.iloc</code></a>), <em>then</em> use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html"" rel=""nofollow noreferrer""><code>df.drop</code></a>, and reset the index again (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>df.reset_index</code></a>).</p>
<hr />
<p>N.B. The issue with your current approach is that <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html"" rel=""nofollow noreferrer""><code>df.drop</code></a> looks at the <em>labels</em> and simply drops <em>each</em> column label that matches the label(s) you pass. That is just one of many reasons why it is a good idea to avoid working with duplicate column labels.</p>
<p>You can deduplicate column labels with <a href=""https://github.com/pandas-dev/pandas/blob/2.0.x/pandas/io/common.py#L1219"" rel=""nofollow noreferrer""><code>.io.common.dedup_names</code></a> (for the pre-2.0 version, see <a href=""https://stackoverflow.com/q/71700347"">here</a>). This will append a period and autonumeric:</p>
<pre class=""lang-py prettyprint-override""><code>deduped = pd.io.common.dedup_names(df.columns, is_potential_multiindex=False)
# ['ENZYME', 'AVG', 'ENZYME.1', 'AVG.1']
</code></pre>
<p>So that you could technically also get the desired result by doing something like:</p>
<pre class=""lang-py prettyprint-override""><code>m = ~pd.Index(deduped).str.startswith('ENZYME.')
out3 = df.loc[:, m].copy()

out3.equals(out)
# True
</code></pre>
","1","Answer"
"79606225","79605997","<p>If you're 'ENZYME' columns are all the same and all you want to keep are only the distinct AVG columns then something like this works well,</p>
<pre><code>df.T.drop_duplicates().T
</code></pre>
<p>As @ouroboros suggests for an improvement,</p>
<pre><code>df.T.drop_duplicates().T.astype({'AVG': float})
</code></pre>
","1","Answer"
"79606532","79605688","<p>IIUC, you want to have True after an entry and False after an exit? So you could just use masks (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.where.html"" rel=""nofollow noreferrer""><code>where</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.mask.html"" rel=""nofollow noreferrer""><code>mask</code></a>), <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.ffill.html"" rel=""nofollow noreferrer""><code>ffill</code></a> to propagate the previous state, and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html"" rel=""nofollow noreferrer""><code>shift</code></a> because everything is relative to the previous row:</p>
<pre><code>df['out'] = (df['entry_signal'].where(df['entry_signal'])
             .astype('boolean')
             .mask(df['exit_signal'], False)
             .ffill().fillna(False)
             .shift(fill_value=False)
            )
</code></pre>
<p>Or, using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.case_when.html"" rel=""nofollow noreferrer""><code>case_when</code></a>:</p>
<pre><code>df['out'] = (pd.Series(index=df.index, dtype='boolean')
               .case_when([(df['entry_signal'], True),
                           (df['exit_signal'], False),
                          ])
               .ffill().shift().fillna(False)
            )
</code></pre>
<p>Output:</p>
<pre><code>            Close  RSI_10  SMA_200  entry_signal  exit_signal    out
2025-04-21    110     NaN      100         False        False  False
2025-04-22    111    20.0      100          True        False  False
2025-04-23    112     NaN      100         False        False   True
2025-04-24    113    41.0      100         False        False   True
2025-04-25    114    20.0      100          True        False   True
2025-04-27    115     NaN      100         False        False   True
2025-04-28    116    61.0      100         False         True   True
</code></pre>
<p>Intermediates:</p>
<pre><code>            Close  RSI_10  SMA_200  entry_signal  exit_signal  masked  ffill    out
2025-04-21    110     NaN      100         False        False    &lt;NA&gt;   &lt;NA&gt;  False
2025-04-22    111    20.0      100          True        False    True   True  False
2025-04-23    112     NaN      100         False        False    &lt;NA&gt;   True   True
2025-04-24    113    41.0      100         False        False    &lt;NA&gt;   True   True
2025-04-25    114    20.0      100          True        False    True   True   True
2025-04-27    115     NaN      100         False        False    &lt;NA&gt;   True   True
2025-04-28    116    61.0      100         False         True   False  False   True
</code></pre>
","2","Answer"
"79606773","79606593","<p>The details depend on your exact file, generally you could:</p>
<p>1. read the file with no headers<br />
2. concat the first 2 rows with agg<br />
3. use that as columns for the rest of the data<br />
4 . do the above for every sheet and concat all at the end</p>
<p>something along these lines:</p>
<pre><code>sheets = pd.read_excel('power_distribution_schedule.xlsx', sheet_name=None, header=None)
lodf = []
for df in sheets.values():
    cols = df[:2].fillna('').agg(' '.join, axis=0).values
    df = df[2:]
    df.columns = cols
    lodf.append(df)
pd.concat(lodf)
</code></pre>
<p>note that the columns with null in them will have a space at the beginning (' name') - if that's a problem, use strip</p>
","0","Answer"
"79606822","79606785","<p>If there is default index is possible select by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>DataFrame.loc</code></a>:</p>
<pre><code>selected_month_idx = df[df[&quot;month&quot;] == selected_month].index[0]

start = np.clip(selected_month_idx, 0, len(df) - 6)

six_month_window = df.loc[start : start + 5]
print(six_month_window)
</code></pre>
<p>If always match value in condition, get position of first <strong>True</strong> by <a href=""https://numpy.org/doc/2.2/reference/generated/numpy.argmax.html"" rel=""nofollow noreferrer""><code>np.argmax</code></a>, count start of first value by <a href=""https://numpy.org/doc/2.1/reference/generated/numpy.clip.html"" rel=""nofollow noreferrer""><code>np.clip</code></a> and select by position in <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>DataFrame.iloc</code></a>:</p>
<pre><code>selected_month_idx = np.argmax(df[&quot;month&quot;] == selected_month)

start = np.clip(selected_month_idx, 0, len(df) - 6)

six_month_window = df.iloc[start : start + 6]
</code></pre>
","1","Answer"
"79606860","79536363","<p>I ended up with the following, which seem sufficient for my use case:</p>
<pre><code>import pandas
import logging
import pandas_flavor as pf


@pf.register_dataframe_method
def startlog(self):
    obj = SubclassedDataFrame2(self)
    obj.initial_df = self
    return obj

@pf.register_dataframe_method
def midlog(self):
    return self.endlog().startlog()

@pf.register_dataframe_method
def endlog(self):
    if(self.shape != self.initial_df.shape):
        nrow0, ncol0 = self.initial_df.shape
        nrow1, ncol1 = self.shape
        dr = nrow0 - nrow1
        dc = ncol0 - ncol1
        msg = &quot;&quot;
        if dr &gt; 0:
            msg += f&quot;Removed {dr:,d}/{nrow0:,d} ({dr/nrow0:.2%}) rows. &quot;
        elif dr &lt; 0:
            msg += f&quot;Added {dr:,d}/{nrow0:,d} ({dr/nrow0:.2%}) rows. &quot;
        if dc &gt; 0:
            msg += f&quot;Removed {dc:,d}/{ncol0:,d} ({dc/ncol0:.2%}) columns.&quot;
        elif dc &lt; 0:
            msg += f&quot;Added {dc:,d}/{ncol0:,d} ({dc/ncol0:.2%}) columns.&quot;
        logging.getLogger(&quot;pandas-utils&quot;).info(msg)
    else:
        nchanged = (self != self.initial_df)
        # missings are different than themselves
        nchanged[self.isna() &amp; self.initial_df.isna()] = False
        nchanged = nchanged.sum().sum()
        ntot = self.initial_df.size
        logging.getLogger(&quot;pandas-utils&quot;).info(f&quot;Changed {nchanged:,d}/{ntot:,d} ({nchanged/ntot:.2%}) values&quot;)
    del self.initial_df
    return self
</code></pre>
<p>To be used like this:</p>
<pre><code>patentsdf.startlog().dropna(subset=[&quot;person_address&quot;, &quot;person_ctry_code&quot;], how=&quot;any&quot;).endlog()
</code></pre>
<p>This works for all operations that modify rows or columns, like dropping NAs or duplicates.</p>
<p>For merge, just call <code>pd.merge</code> with <code>indicator=True</code> and then <code>value_counts()</code> the resulting <code>_merge</code> column.</p>
","1","Answer"
"79608106","79601755","<p>You can create a new column with the following function, which checks the Windows version mentioned in each entry:</p>
<pre><code>def windows_ver(twi):
    if twi.lower().__contains__('windows 10'):
        return 'windows 10'
    elif twi.lower().__contains__('windows 11'):
        return 'windows 11'
    else:
        return 'windows 8 or older'

df['windows'] = df.twitter.apply(windows_ver)
</code></pre>
<p>This function processes each value in the <code>twitter</code> column, checks if it mentions a Windows version, and returns the appropriate label.</p>
","0","Answer"
"79608142","79608124","<p>Altair doesn’t support hiding lines from the chart while keeping them in the legend by default. If you use <code>transform_filter(selection)</code>, it filters out the data entirely. The line disappears from the chart <em>and</em> the legend. If you use <code>opacity</code>, you can dim the unselected lines, but they’re still there, which isn't what you want.</p>
<p>The workaround is to layer two charts:</p>
<ul>
<li><p>One chart has all the lines but sets their opacity to 0. This makes them invisible but keeps them in the legend so all series are always shown there.</p>
</li>
<li><p>The second chart applies the selection filter and only shows the selected lines.</p>
</li>
</ul>
<p>You bind the selection to the legend, and because the color encoding is shared, the colors should stay consistent no matter what’s selected. This should give you full legend visibility, proper filtering, and no unwanted interactivity on hidden lines.</p>
","1","Answer"
"79608410","79608369","<p>You're using a <a href=""https://seaborn.pydata.org/generated/seaborn.displot.html"" rel=""nofollow noreferrer""><code>displot</code></a>, which by default will compute a <a href=""https://seaborn.pydata.org/generated/seaborn.histplot.html"" rel=""nofollow noreferrer""><code>histplot</code></a> with automatic bins. Those bins are not necessarily centered on integer values. You could manually pass a list of bins, but in your case this would be more meaningful to just consider Age as a category.</p>
<p>You probably want to use a <a href=""https://seaborn.pydata.org/generated/seaborn.barplot.html"" rel=""nofollow noreferrer""><code>barplot</code></a>:</p>
<pre><code>sns.barplot(
    data=df.groupby(['Age', 'Fruit'], as_index=False).size(),
    x='Age',
    y='size',
    hue='Fruit',
)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/cciIDGgY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cciIDGgY.png"" alt=""seaborn barplot"" /></a></p>
<p>Or, without seaborn, using a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html"" rel=""nofollow noreferrer""><code>crosstab</code></a>:</p>
<pre><code>pd.crosstab(df['Age'], df['Fruit']).plot.bar()
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/eImNncvI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eImNncvI.png"" alt=""pandas crosstab + bar plot"" /></a></p>
","4","Answer"
"79608544","79608369","<p>You need <code>discrete=True</code> to tell seaborn that the x values are discrete. Adding <code>shrink=0.8</code> will leave some space between the bars.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt

# Data
np.random.seed(42)
n = 5000
df = pd.DataFrame({
    'PERSON': np.random.randint(100000, 999999, n),
    'Fruit': np.random.choice(['Banana', 'Strawberry'], n),
    'Age': np.random.randint(9, 18, n)
})

sns.displot(
    data=df,
    x='Age',
    hue='Fruit',
    multiple='dodge',
    discrete=True,
    shrink=0.8)

plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/ry61emkZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ry61emkZ.png"" alt=""sns.displot with discrete data"" /></a>.
Note that <code>sns.displot()</code> is a figure-level function that creates a grid of one or more subplots, with a common legend outside.  <code>sns.countplot()</code> is an axes-level function, that creates a single subplot with a legend inside.</p>
<p>An alternative is creating a <code>countplot</code>:</p>
<pre class=""lang-py prettyprint-override""><code>sns.countplot(
    data=df,
    x='Age',
    hue='Fruit',
    dodge=True
)
</code></pre>
<p><a href=""https://i.sstatic.net/oD5PdwA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oD5PdwA4.png"" alt=""sns.countplot"" /></a></p>
","4","Answer"
"79608735","79607513","<p>Had the same issue recently when using <code>Decimal</code> with <code>read_csv</code>. The problem was caused by bad values in the column — things like empty strings, <code>&quot;NaN&quot;</code>, or values with commas. <code>Decimal()</code> is super strict and throws <code>ConversionSyntax</code> if it doesn’t get a clean number.</p>
<p>What worked for me was wrapping it with a safer function:</p>
<pre><code>from decimal import Decimal, InvalidOperation

def to_decimal_safe(x):
    try:
        return Decimal(x.replace(',', ''))  # if your numbers have commas
    except (InvalidOperation, ValueError):
        return None  # or Decimal('0.0') / np.nan, depending on what you want
</code></pre>
<pre><code>df = pd.read_csv(&quot;file.csv&quot;, converters={&quot;column_name&quot;: to_decimal_safe})
</code></pre>
<p>Haven’t had the error since. You might want to check your CSV for weird characters too, just in case.</p>
","0","Answer"
"79608887","79608662","<p>You can try using this command:</p>
<pre><code>python -m pip install --upgrade pandas
</code></pre>
","1","Answer"
"79609134","79596458","<p>The stable release of <a href=""https://pypi.org/project/pandas-ods-reader/#data"" rel=""nofollow noreferrer"">pandas-ods-reader</a> (i.e. 1.0.1) requires Python &lt;3.13, &gt;=3.9. There's no apparent support yet for Python 3.13.</p>
<p>You can use Python 3.12 for this package. Be careful of downgrading your Python especially if it's the default. It is recommended to use <a href=""https://docs.python.org/3/tutorial/venv.html"" rel=""nofollow noreferrer"">virtual environments</a> or <a href=""https://github.com/pyenv/pyenv"" rel=""nofollow noreferrer"">pyenv</a> to manage multiple Python versions without directly modifying the system's default Python installation.</p>
","0","Answer"
"79610660","79610568","<p>Problem is that you use very old <code>Python 3.8</code> (end of life in 2024 - see: <a href=""https://devguide.python.org/versions/"" rel=""nofollow noreferrer"">Status of Python versions</a>) which can install only <code>pandas 2.0.3</code> but code works correctly with <code>pandas 2.2.3</code> or even with <code>pandas 2.2.2</code> (as @NaveedAhmed mentioned in comments).</p>
<p>So you may have to update <code>Python</code> to work with newer <code>pandas</code>.</p>
<hr />
<p>In <code>Python3.8</code> works for me</p>
<p><code>dictionary</code>:</p>
<pre><code>df.loc[&quot;bnd&quot;] = [bnd1, &quot;N/A&quot;]
df.loc[&quot;bnd&quot;] = {&quot;val&quot;: bnd2, &quot;unit&quot;: &quot;N/A&quot;}
</code></pre>
<p><code>pandas.Series</code>:</p>
<pre><code>df.loc[&quot;bnd&quot;] = [bnd1, &quot;N/A&quot;]
df.loc[&quot;bnd&quot;] = pd.Series([bnd2, &quot;N/A&quot;], index=[&quot;val&quot;, &quot;unit&quot;])
</code></pre>
<hr />
<p>Full code for tests:</p>
<pre><code>import sys
import numpy as np
import pandas as pd

print('python:', sys.version)
print('pandas:', pd.__version__)
print('numpy :', np.__version__)

bnd1 = np.array([[1,2],[3,4]])
bnd2 = np.array([[5,6],[7,8]])

print('--- dictionary ---')
df = pd.DataFrame(columns=[&quot;val&quot;, &quot;unit&quot;])
df.loc[&quot;bnd&quot;] = [bnd1, &quot;N/A&quot;]
df.loc[&quot;bnd&quot;] = {&quot;val&quot;: bnd2, &quot;unit&quot;: &quot;N/A&quot;}
print(df.to_string())

print('--- pandas.Series ---')
df = pd.DataFrame(columns=[&quot;val&quot;, &quot;unit&quot;])
df.loc[&quot;bnd&quot;] = [bnd1, &quot;N/A&quot;]
df.loc[&quot;bnd&quot;] = pd.Series([bnd2, &quot;N/A&quot;], index=[&quot;val&quot;, &quot;unit&quot;])
print(df.to_string())

print('--- list ---')
df = pd.DataFrame(columns=[&quot;val&quot;, &quot;unit&quot;])
df.loc[&quot;bnd&quot;] = [bnd1, &quot;N/A&quot;]
df.loc[&quot;bnd&quot;] = [bnd2, &quot;N/A&quot;]
print(df.to_string())
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>python: 3.8.20 (default, Sep  7 2024, 18:35:07) 
[GCC 13.2.0]
pandas: 2.0.3
numpy : 1.24.4
--- dictionary ---
                  val unit
bnd  [[5, 6], [7, 8]]  N/A
--- pandas.Series ---
                  val unit
bnd  [[5, 6], [7, 8]]  N/A
--- list ---
Traceback ....
...
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
</code></pre>
","2","Answer"
"79611952","79611884","<pre><code>rule combine_tables:
    input:
        # Static sample list (use checkpoints if dynamically generated)
        expand(&quot;results/{sample}/data.csv&quot;, sample=SAMPLES)
    output:
        &quot;results/combined/all_data.csv&quot;
    run:
        import pandas as pd
        dfs = []
        missing_files = []
        corrupt_files = []
        
        # Process files in consistent order
        for file_path in sorted(input, key=lambda x: x.split(&quot;/&quot;)[1]):  # Sort by sample
            # Handle missing files (shouldn't occur if Snakemake workflow is correct)
            if not os.path.exists(file_path):
                missing_files.append(file_path)
                continue
                
            # Handle corrupt/unreadable files
            try:
                df = pd.read_csv(file_path)
                # Add sample metadata column
                sample_id = file_path.split(&quot;/&quot;)[1]
                df.insert(0, &quot;sample&quot;, sample_id)  # Add sample column at start
                dfs.append(df)
            except Exception as e:
                corrupt_files.append((file_path, str(e)))
        
        # Validation reporting
        if missing_files:
            raise FileNotFoundError(f&quot;Missing {len(missing_files)} files: {missing_files}&quot;)
        if corrupt_files:
            raise ValueError(f&quot;Corrupt files detected:\n&quot; + &quot;\n&quot;.join(
                [f&quot;{f[0]}: {f[1]}&quot; for f in corrupt_files]))
        if not dfs:
            raise ValueError(&quot;No valid dataframes to concatenate&quot;)
            
        # Concatenate and save
        combined = pd.concat(dfs, ignore_index=True)
        combined.to_csv(output[0], index=False)
</code></pre>
","1","Answer"
"79612021","79611944","<pre><code>import pandas as pd
from functools import reduce

# Sample data
final_df = pd.DataFrame({
    '2025-05-07': [104, 87, 34],
    '2025-05-08': [7, 0, 2]
}, index=['a', 'b', 'c'])

date_str_list = ['2025-05-07', '2025-05-08']

# Create list of conditions using list comprehension
conditions = [final_df[date_str] &gt; 1 for date_str in date_str_list]

# Combine conditions using logical AND
if conditions:
    filter_condition = reduce(lambda x, y: x &amp; y, conditions)  
else:
    filter_condition = pd.Series([True]*len(final_df))  # No filtering if empty list

# Apply filter
final_filtered_df = final_df[filter_condition]

print(&quot;Filtered DataFrame:&quot;)
print(final_filtered_df)
</code></pre>
","0","Answer"
"79612219","79611944","<p>Another way to do this is to transpose the dataframe so the date is the index (<code>.T</code>) and convert the index to pandas datetime with <code>pd.to_datetime</code>.</p>
<p>Like <a href=""https://stackoverflow.com/users/11769133/milos-stojanovic"">@milos Stojanovich</a> said, it's important to keep the precedence of the operators correct. I find that in pandas it's convenient to use the . version of the comparison operators e.g. <code>something.gt(1) &amp; otherthing.gt(2)</code> instead of <code>(1 &lt; something) &amp; (2 &lt; otherthing)</code></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
final_df = pd.DataFrame({
    '2025-05-07': [104, 87, 34],
    '2025-05-08': [7, 0, 2]
}, index=['a', 'b', 'c'])

final_df = final_df.T  # transpose, date is now the index, a,b,c are column
#            a   b   c
#2025-05-07  104  87  34
#2025-05-08    7   0   2

# convert the index to datetime
final_df.index = pd.to_datetime(final_df.index) # convert to datetime
print(&quot;Dataframe (transposed)\n&quot;, final_df)

# we could have constructed this explicitly
assert final_df.equals(pd.DataFrame({
    'a': [104, 7], 'b': [87, 0], 'c': [34, 2]
}, index=pd.to_datetime(['2025-05-07', '2025-05-08'])))

# the requested dates
date_str_list = ['2025-05-07', '2025-05-08']
dates = pd.to_datetime(date_str_list)
if not date_str_list:
    # not sure what should happen &quot;on empty&quot;: 
    #could be &quot;return None&quot; or &quot;return final_df&quot; if we are in a function
    raise ValueError(&quot;No datetime selected&quot;)

# Reduce the dataframe to the desired date-times. datetime really needs a .loc for some reason
final_df_on_dates = final_df.loc[dates]

# the condition:
# the method version of the comparison (.gt for greater-than (&lt;), .eq for ==) are useful for precedence control
# and we have .all to reduce on the 'a','b','c' column 
# i.e. df.loc[date].all(axis=1) == df.loc[date]['a'].gt(1) &amp; df.loc[date]['b'].gt(1) &amp; df.loc[date]['c'].gt(1)
condition = final_df_on_dates.gt(1).all(axis=1)

# apply the condition
final_final_df = final_df_on_dates[condition]
print(&quot;final dataframe after filtering\n&quot;, final_final_df)


</code></pre>
","1","Answer"
"79613050","79613039","<p>You can use <code>pandas</code>' built-in functionality -- use <code>.shift()</code> along with <code>.cumsum()</code> on a comparison to identify when the value changes, then increment a counter accordingly.</p>
<p>Here's how to do it:</p>
<pre><code>import pandas as pd  

my_list = ['Apple', 'Apple', 'Orange', 'Orange','Orange','Banana'] 

df = pd.DataFrame(my_list, columns=['List'])  
df['Value'] = (df['List'] != df['List'].shift()).cumsum()  

print(df)
</code></pre>
","4","Answer"
"79613063","79613039","<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html"" rel=""noreferrer""><code>pandas.factorize</code></a>, and add <code>1</code> if you need the category numbers to start with <code>1</code> instead of <code>0</code>:</p>
<pre><code>import pandas as pd

my_list = ['Apple', 'Apple', 'Orange', 'Orange','Orange','Banana']
grouping = pd.DataFrame(my_list, columns=['List'])

grouping['code'] = pd.factorize(grouping['List'])[0] + 1
print(grouping)
</code></pre>
<p>Output:</p>
<pre><code>     List  code
0   Apple     1
1   Apple     1
2  Orange     2
3  Orange     2
4  Orange     2
5  Banana     3
</code></pre>
","9","Answer"
"79613138","79613039","<p>If you are using pandas, you should just use the pandas function as suggested by the other answers, but there exists a very simple approach in vanilla python:</p>
<pre><code>my_list = ['Apple', 'Apple', 'Orange', 'Orange','Orange','Banana']
factors = {}
result = []

for item in my_list:
    result.append(factors.setdefault(item, len(factors) + 1))
print(result) # [1, 1, 2, 2, 2, 3]
</code></pre>
<p>Note, this doesn't depend on repeated items in the list being lumped together.</p>
<p>Note, you could even use a list-comprehension, although the purist in me doesn't like list comprehensions with side-effects, but in this case I might let that go:</p>
<pre><code>my_list = ['Apple', 'Apple', 'Orange', 'Orange','Orange','Banana']
factors = {}
result = [factors.setdefault(item, len(factors) + 1) for item in my_list]
</code></pre>
<p>If they <em>are</em> lumped together, though, here's a fun little itertools-based one-liner:</p>
<pre><code>&gt;&gt;&gt; from itertools import groupby, count
&gt;&gt;&gt; [c for c, (_, g) in zip(count(1), groupby(my_list)) for _ in g]
[1, 1, 2, 2, 2, 3]
</code></pre>
<p>Probably, I would go with the <code>dict</code> approach in production code, it's less cryptic. Although, I suspect it may be more performant on large datasets (this would be a micro-optimization)</p>
","0","Answer"
"79613243","79613039","<p>You can try <a href=""https://docs.python.org/3/library/stdtypes.html#dict.fromkeys"" rel=""nofollow noreferrer""><code>dict.fromkeys</code></a> like below</p>
<pre><code>d = {v:(i+1) for i, v in enumerate(dict.fromkeys(my_list))}
list(map(d.get, my_list))
</code></pre>
<p>which gives</p>
<pre><code>[1, 1, 2, 2, 2, 3]
</code></pre>
","1","Answer"
"79614287","79613039","<p>There are many different ways to do this, some with sorting implications:</p>
<pre><code>grouping['Label_groupby'] = grouping.groupby('List', sort=False).ngroup().add(1)

grouping['Label_factorize'] = grouping['List'].factorize()[0] + 1

grouping['Label_CategoryCode'] = grouping['List'].astype('category').cat.codes + 1
</code></pre>
<p>Output:</p>
<pre><code>     List  Label_groupby  Label_factorize  Label_CategoryCode
0   Apple              1                1                   1
1   Apple              1                1                   1
2  Orange              2                2                   3
3  Orange              2                2                   3
4  Orange              2                2                   3
5  Banana              3                3                   2
</code></pre>
","1","Answer"
"79614436","79608662","<p>Problem resolved: It appears that the cause was a setting relating to my work's VPN. Turning the VPN off allowed for the update to take place.</p>
","0","Answer"
"79614444","79614376","<p>How have you dropped the duplicates? You should only consider the &quot;record_id&quot; column.</p>
<p>For example:</p>
<pre><code>consolidate_ptids = [2, 3]
surv_both = pd.DataFrame({'record_id': [1, 2, 3, 1, 2, 2],
                          'other_col': ['a', 'b', 'c', 'd', 'e', 'e']})
surv_both.drop_duplicates(inplace=True)

out = surv_both[surv_both['record_id'].isin(consolidate_ptids)]
</code></pre>
<p>Output:</p>
<pre><code>   record_id other_col
1          2         b
2          3         c
4          2         e  # record_id is duplicated, not the row
</code></pre>
<p>Now, if you use the <code>subset=['record_id']</code> parameter, this will ensure that there are no duplicates in the selecting column:</p>
<pre><code>surv_both.drop_duplicates(subset=['record_id'], inplace=True)
</code></pre>
<p>Alternatively, combine your filter with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.duplicated.html"" rel=""nofollow noreferrer""><code>duplicated</code></a>:</p>
<pre><code>out = surv_both[
    surv_both['record_id'].isin(consolidate_ptids)
    &amp; ~surv_both['record_id'].duplicated()
]
</code></pre>
<p>Output:</p>
<pre><code>   record_id other_col
1          2         b
2          3         c
</code></pre>
","0","Answer"
"79614853","79614700","<p>pandas makes the assumption that the major axis of a bar-chart is always categorical, and therefore converts your values to strings prior to plotting. This means that it forces matplotlib to render a label for every bar you have.</p>
<p>The way to do this with minimal changes to your code would be to manually override the  <code>yticklabels</code> with your own custom ones. You can create a Series that contains the year (as a string) whenever the year in the current row is different than that of the next row. Then fill in empty strings for the other case when the year of the current row is the same as the next row.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

s = pd.Series([2000, 2001, 2002, 2003]).repeat(3)
print(
    pd.DataFrame({
        'orig': s,
        'filtered': s.pipe(lambda s: s.astype('string').where(s != s.shift(), ''))
    })
)
#    orig filtered
# 0  2000    2000
# 0  2000
# 0  2000
# 1  2001    2001
# 1  2001
# 1  2001
# 2  2002    2002
# 2  2002
# 2  2002
# 3  2003    2003
# 3  2003
# 3  2003
</code></pre>
<p>Putting this into action in your code would look like:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

df = pd.read_csv('NYPD_Arrests_Data__Historic__20250113_111.csv')

df['ARREST_DATE'] = pd.to_datetime(df['ARREST_DATE'], format = '%m/%d/%Y')
df['ARREST_MONTH'] = df['ARREST_DATE'].dt.to_period('M').dt.to_timestamp()

# crimes, attributes and renames
crimes = ['DANGEROUS DRUGS', 'DANGEROUS WEAPONS', 'ASSAULT 3 &amp; RELATED OFFENSES', 'FELONY ASSAULT']
attributes = ['PERP_RACE']
titles = ['Race']

# loops plot creation over each attribute
for attr, title in zip(attributes, titles):
    fig, axes = plt.subplots(1, len(crimes), figsize = (4 * len(crimes), 6), sharey = 'row')

    for i, crime in enumerate(crimes):
        ax = axes[i]
        crime_df = df[df['OFNS_DESC'] == crime]

        pivot = pd.crosstab(crime_df['ARREST_MONTH'], crime_df[attr])

        # plots stacked horizontal bars
        pivot.plot(kind = 'barh', stacked = True, ax = ax, width = 0.9, legend = False)
        ax.set_title(crime)
        ax.set_xlabel('Frequency')
        ax.set_ylabel('Month' if i == 0 else '')  # shows the y-axis only on first plot
        ax.xaxis.set_tick_params(labelsize = 8)
        ax.yaxis.set_tick_params(size=0)

        yticklabels = (
            pivot.index.year.to_series()
            .pipe(
                lambda s: s.astype('string').where(s != s.shift(), '')
            )
        )
        ax.set_yticklabels(yticklabels)

axes.flat[0].invert_yaxis()
handles, labels = axes.flat[0].get_legend_handles_labels()
fig.legend(handles, labels, title = title, loc = 'upper center', ncol = len(df[attr].unique()), bbox_to_anchor = (0.5, 0.94))
fig.suptitle(f'Crime Frequency Distribution by Year and {title}', fontsize = 20)
plt.tight_layout(rect = [0, 0, 1, 0.90])
plt.show()
</code></pre>
<p>Note that I also inverted the y-axis to make the dates increase as the viewer moves their eyes down the chart.
This is done with the <code>axes.flat[0].invert_yaxis()</code> line (it inverts tha axis on all charts since they share the y-axis)</p>
<p><a href=""https://i.sstatic.net/Ix0j1pUW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ix0j1pUW.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79614951","79614935","<p>You could also do an <code>inner join</code> through a <code>merge</code>. It would be more helpful if those dataframes were reproducible.</p>
<pre><code>import pandas as pd

df1 = pd.DataFrame(data={&quot;_iTOW&quot;: [205466,205467,205468,205469,205470,205471,
                             205472,205473,205494,205495,206495,206505]})
df2 = pd.DataFrame(data={&quot;_iTOW&quot;: [205470,205471,205473,205494,205495]})

outer = df1.merge(df2, on= &quot;_iTOW&quot;)

print(outer)
</code></pre>
","0","Answer"
"79614962","79614935","<p>I want to filter out rows of DF1 where column value of Dataframe 1 is present in column value of Dataframe 2 -&gt; <strong>This expression is very unclear.</strong></p>
<p>If your goal is to filter the <code>_Ttow</code> column in df1 that has the same value as the value in the <code>_iTow</code> column in df2, use the following code</p>
<p><strong>Answer</strong></p>
<pre><code>res = df1[df1['_iTOW'].isin(df2['_iTOW'])]
</code></pre>
<p>res</p>
<pre><code>   messageIndex  instanceId   _iTOW  fix  carIsOn
0          9860           0  205467    3        1
1          9861           0  205468    3        1
2          9862           0  205469    3        1
3          9863           0  205470    3        1
4          9864           0  205471    3        1
5          9865           0  205472    3        1
</code></pre>
<p><strong>Example Code</strong></p>
<pre><code>import pandas as pd

df1 = pd.DataFrame({
    'messageIndex': [9860, 9861, 9862, 9863, 9864, 9865, 9866, 9867, 9868, 9869, 14021],
    'instanceId': [0]*11,
    '_iTOW': [205467, 205468, 205469, 205470, 205471, 205472, 205473, 205474, 205475, 205476, 205505],
    'fix': [3]*11,
    'carIsOn': [1]*11
})

df2 = pd.DataFrame({
    'messageIndex': [9860, 9861, 9862, 9863, 9864, 9865, 9866],
    'instanceId': [0]*7,
    '_iTOW': [205467, 205468, 205469, 205470, 205471, 205472, 205455]
})
</code></pre>
","1","Answer"
"79615288","79615284","<p>The issue you're encountering (e.g., <em>unhashable type: 'dict'</em>) happens because dictionaries are mutable and unhashable, so <code>drop_duplicates()</code> doesn't work directly on them.</p>
<p>To deduplicate rows where one of the columns contains dictionaries, you can:</p>
<ol>
<li><p><strong>Convert dictionaries to strings</strong>, use <code>drop_duplicates()</code>, then</p>
</li>
<li><p><strong>Convert the strings back to dictionaries</strong> (if needed).</p>
</li>
</ol>
<p>Here’s a clean and simple way to achieve your desired output:</p>
<p><a href=""https://code.livegap.com/?st=a50pbcrjkjk"" rel=""nofollow noreferrer"">https://code.livegap.com/?st=a50pbcrjkjk</a></p>
","1","Answer"
"79615371","79615284","<p>DataFrame is structured incorrectly. when you need to preserve structure or define row counts, Use <code>list</code> as values for each keys. Use the following logics for the rest o the code.</p>
<pre><code>import pandas as pd

# Create data with LIST WRAPPERS to ensure single-row structure
data = {
   &quot;some_id&quot;: [&quot;xxx&quot;],  # Wrap scalar values in lists
   &quot;some_email&quot;: [&quot;abc.xyz@somedomain.com&quot;],
   &quot;This is Sample&quot;: [
       [  # Double-nested list to maintain list structure in DataFrame
           {&quot;a&quot;: &quot;22&quot;, &quot;b&quot;: &quot;Y&quot;, &quot;c&quot;: &quot;33&quot;, &quot;d&quot;: &quot;x&quot;},
           {&quot;a&quot;: &quot;44&quot;, &quot;b&quot;: &quot;N&quot;, &quot;c&quot;: &quot;55&quot;, &quot;d&quot;: &quot;Y&quot;},
           {&quot;a&quot;: &quot;22&quot;, &quot;b&quot;: &quot;Y&quot;, &quot;c&quot;: &quot;33&quot;, &quot;d&quot;: &quot;x&quot;},
           {&quot;a&quot;: &quot;44&quot;, &quot;b&quot;: &quot;N&quot;, &quot;c&quot;: &quot;55&quot;, &quot;d&quot;: &quot;Y&quot;},
           {&quot;a&quot;: &quot;22&quot;, &quot;b&quot;: &quot;Y&quot;, &quot;c&quot;: &quot;33&quot;, &quot;d&quot;: &quot;x&quot;},
           {&quot;a&quot;: &quot;44&quot;, &quot;b&quot;: &quot;N&quot;, &quot;c&quot;: &quot;55&quot;, &quot;d&quot;: &quot;Y&quot;}
       ]
   ]
}

df = pd.DataFrame(data)

# Deduplicate the list of dictionaries
if not df.empty:
   sample_list = df[&quot;This is Sample&quot;].iloc[0]
   seen = set()
   deduped = []
   
   for d in sample_list:
       # Create hashable identifier from sorted dictionary items
       identifier = tuple(sorted(d.items(), key=lambda x: x[0]))
       if identifier not in seen:
           seen.add(identifier)
           deduped.append(d)
   
   df.at[0, &quot;This is Sample&quot;] = deduped

print(df)
</code></pre>
","1","Answer"
"79615377","79615284","<p>Another possible solution:</p>
<pre><code>nondupes = ~df['This is Sample'].map(lambda x: tuple(sorted(x.items()))).duplicated()

df[nondupes]
</code></pre>
<p>First, it uses <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> to transform each dictionary in the column into a sorted tuple of its key–value pairs, ensuring that dictionaries with the same content but different key orders are treated equally. Then, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.duplicated.html"" rel=""nofollow noreferrer""><code>duplicated</code></a> identifies the repeated entries among these tuples, and the bitwise <code>NOT</code> operator <code>~</code> inverts the boolean series to mark unique rows as <code>True</code>. Finally, <code>df[nondupes]</code> returns only those rows where <code>&quot;This is Sample&quot;</code> contains a unique dictionary.</p>
<p>Output:</p>
<pre><code>  some_id              some_email                              This is Sample
0     xxx  abc.xyz@somedomain.com  {'a': '22', 'b': 'Y', 'c': '33', 'd': 'x'}
1     xxx  abc.xyz@somedomain.com  {'a': '44', 'b': 'N', 'c': '55', 'd': 'Y'}
</code></pre>
","0","Answer"
"79615610","79615560","<p>You could do it with numpy:</p>
<pre><code>v=df.iloc[:,1:].values # a numpy array with only numerical values of your table (I assume that df.iloc[:,0] is column A, and column A is the date. Not clear from your example format)

df[(((v[:,:,None] == v[:,None,:]) &amp; (v[:,:,None]&gt;1)).sum(axis=2)&gt;1).any(axis=1)]
</code></pre>
<p>If <code>df</code> is (almost your table, with a simpler &quot;A&quot; because I was too lazy to copy your datetimes)</p>
<pre><code>     A    B    C        D         E         F         G    H         I
0    0  0.0  0.0      0.0      0.00      0.00      0.00  0.0      0.00
1    1  0.0  0.0  19598.8      0.00  19598.80      0.00  0.0      0.00
2    2  0.0  0.0      0.0      0.00      0.00      0.00  0.0      0.00
3    3  0.0  0.0      0.0      0.00  19823.35      0.00  0.0      0.00
4    4  0.0  0.0      0.0      0.00      0.00      0.00  0.0      0.00
5    5  0.0  0.0      0.0      0.00      0.00      0.00  0.0      0.00
6    6  0.0  0.0      0.0  19975.95      0.00  19975.95  0.0  19975.95
7    7  0.0  0.0      0.0      0.00      0.00      0.00  0.0      0.00
8    8  0.0  0.0  19830.2      0.00      0.00      0.00  0.0      0.00
9    9  0.0  0.0      0.0      0.00      0.00      0.00  0.0      0.00
10  10  0.0  0.0      0.0      0.00      0.00      0.00  0.0      0.00
11  11  0.0  0.0      0.0      0.00      0.00      0.00  0.0      0.00
</code></pre>
<p>Then my code returns</p>
<pre><code>1  1  0.0  0.0  19598.8      0.00  19598.8      0.00  0.0      0.00
6  6  0.0  0.0      0.0  19975.95      0.0  19975.95  0.0  19975.95
</code></pre>
<p>So explanation.
<code>v[:,:,None]</code> is a 3D array containing the same data as <code>v</code> (so the 12×8 numbers), organized in a 12×8×1 shape (so 12×8 [singleton]). Likewise <code>v[:,None,:]</code> are the same data but in a 12×1×8 array. So <code>v[:,:,None][i,j,0]</code> is the same as <code>v[i,j]</code> which is the same as <code>v[:,None,:][i,0,j]</code>.</p>
<p>Because of broadcasting, <code>v[:,:,None]==v[:,None,:]</code> is a 12×8×8 array, whose element [i,j,k] is True iff <code>v[i,j,0]==v[i,0,k]</code>.</p>
<p>So <code>(v[:,:,None] == v[:,None,:]) &amp; (v[:,:,None]&gt;1)</code> is a 12×8×8 array whose element [i,j,k] is True iff <code>v[i,j,0]==v[i,0,k]</code> and that is greater than 1.</p>
<p>Obviously, each element is equal to itself, so for any row <code>i</code> and any column <code>j</code> we have <code>v[i,j,0]==v[i,0,j]</code>. But the question is &quot;is it also equal to another element of row <code>i</code> (still, while being greater than 1). That question is answered by <code>.sum(axis=2)</code> :<br />
<code>((v[:,:,None] == v[:,None,:]) &amp; (v[:,:,None]&gt;1)).sum(axis=2)</code> is, for each row <code>i</code> and column <code>j</code>, the number of elements in that row that are equal to <code>v[i,j]</code> (while being greater than 1).</p>
<p>So <code>((v[:,:,None] == v[:,None,:]) &amp; (v[:,:,None]&gt;1)).sum(axis=2)&gt;1</code> is a 12×8 array that is True iff element <code>v[i,j]</code> is greater than 1 and is equal to at least another number of row <code>i</code> than itself.</p>
<p><code>(((v[:,:,None] == v[:,None,:]) &amp; (v[:,:,None]&gt;1)).sum(axis=2)&gt;1).any(axis=1)</code> tells if there is any <code>True</code> in each row. So it is a 1D array whose element <code>[i]</code> is True iff in row <code>i</code> we find at least one value greater than 1 that is equal to at least another value of the same row.</p>
<p>That is the boolean filter I use to select only the row that interest you.</p>
<p>Note that I used <code>==</code> to compare two values because you didn't say anything else, and your example shows exact values. It may be enough if those identical values are just effectively copies of the same variable. Or parsed by the same parser from the same exact decimal representation.
But generally speaking it is a bad idea to compare floats with <code>==</code>. If you are using floats, that is probably because you perform some computation with them. And then the infamous <code>0.1+0.2==0.3 ⇒ False</code> (and many other examples: generally speaking floats are not exact representation) may yield to unexpected result.</p>
<p>So, you may want to to replace that <code>==</code> by something like <code>np.isclose</code></p>
<pre><code>df[((np.isclose(v[:,:,None], v[:,None,:]) &amp; (v[:,:,None]&gt;1)).sum(axis=2)&gt;1).any(axis=1)]
</code></pre>
","0","Answer"
"79615704","79615698","<p>You can try <code>groupby()</code> along with <code>sum()</code> to group the data by the <code>'date'</code> column and calculate the total sales per day.</p>
<pre><code>
result = df.groupby('date')['sales'].sum().reset_index()
</code></pre>
<p>If you'd like to keep <code>date</code> as the index, you can remove <code>.reset_index()</code>.</p>
","0","Answer"
"79615768","79615560","<p>A simple approach could be to select the columns of interest, then identify if any value is <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.duplicated.html"" rel=""nofollow noreferrer""><code>duplicated</code></a> within a row. Then select the matching rows with <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a>:</p>
<pre><code>mask = df.loc[:, 'B':].T

out = df[mask.apply(lambda x: x.duplicated(keep=False)).where(mask &gt;= 1).any()]
</code></pre>
<p>A potentially more efficient approach could be to use <a href=""/questions/tagged/numpy"" class=""s-tag post-tag"" title=""show questions tagged &#39;numpy&#39;"" aria-label=""show questions tagged &#39;numpy&#39;"" rel=""tag"" aria-labelledby=""tag-numpy-tooltip-container"" data-tag-menu-origin=""Unknown"">numpy</a>. Select the values, mask the values below 1, <a href=""https://numpy.org/doc/2.2/reference/generated/numpy.ndarray.sort.html"" rel=""nofollow noreferrer""><code>sort</code></a> them and identify if any 2 are identical in a row with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.diff.html"" rel=""nofollow noreferrer""><code>diff</code></a> + <a href=""https://numpy.org/doc/stable/reference/generated/numpy.isclose.html"" rel=""nofollow noreferrer""><code>isclose</code></a>:</p>
<pre><code>mask = df.loc[:, 'B':].where(lambda x: x&gt;=1).values
mask.sort()
out = df[np.isclose(np.diff(mask), 0).any(axis=1)]
</code></pre>
<p>Output:</p>
<pre><code>                A  B  C        D         E        F         G  H         I
1  5/7/2025 21:15  0  0  19598.8      0.00  19598.8      0.00  0      0.00
6  5/7/2025 22:30  0  0      0.0  19975.95      0.0  19975.95  0  19975.95
</code></pre>
","0","Answer"
"79616013","79615990","<p>One numpy solution could be</p>
<pre><code>df['roll'] = [[]]+np.lib.stride_tricks.sliding_window_view(df.values, (2,3)).reshape(-1, 6).tolist()
</code></pre>
<p>After that, your <code>df</code> becomes</p>
<pre><code>    0   1   2                   roll
0   1   2   3                     []
1   4   5   6     [1, 2, 3, 4, 5, 6]
2   7   8   9     [4, 5, 6, 7, 8, 9]
3  10  11  12  [7, 8, 9, 10, 11, 12]
</code></pre>
<p>Some explanation:
<code>R=np.lib.stride_tricks.sliding_window_view(df.values, (2,3))</code> is a 4D array, whose each subarray <code>R[i,j]</code> is a subarray of <code>df.values</code> starting from value <code>i,j</code> and with shape <code>(2,3)</code>. So we can see <code>R</code> as a 2D array of 2D arrays. Since <code>df.values</code> shape is (4,3), there are 3 rows (3 way to select 2 consecutive rows from <code>df.values</code>) and 1 column (1 way to select 3 consecutive columns from <code>df.values</code>) in <code>R</code>. So its shape is (3,1,2,3). And <code>R[i,j,k,l]</code> is same as <code>df.values[i+k,j+l]</code></p>
<p>By calling <code>reshape(-1,6)</code> on this, we reorganize that as wanted.
<code>R.reshape(-1,6)[i,j]</code> being same as <code>df.values[i+j//3, j%3]</code></p>
<p>I then convert that to a list and prepend an empty list for the first row.</p>
<p>I suppose that in reality, number of rows in <code>df.values</code> is not necessarily 3, and number of row to roll is not necessarily 2. So a more generic version would be</p>
<pre><code>H,W=df.values.shape
n=2
df['roll'] = [[]]*(n-1)+np.lib.stride_tricks.sliding_window_view(df.values, (n,W)).reshape(-1, W*n).tolist()
</code></pre>
","2","Answer"
"79616159","79615990","<p>This doesn't need windowing, IIUC, you can use df.shift:</p>
<pre><code>x = df.apply(lambda x: x.tolist(), axis=1)
df[3] = (x.shift() + x)
</code></pre>
<p>Output:</p>
<pre><code>    0   1   2                      3
0   1   2   3                    NaN
1   4   5   6     [1, 2, 3, 4, 5, 6]
2   7   8   9     [4, 5, 6, 7, 8, 9]
3  10  11  12  [7, 8, 9, 10, 11, 12]
</code></pre>
<p>Adding window sizing:</p>
<pre><code>import pandas as pd
import numpy as np
from functools import reduce

df = pd.DataFrame(np.arange(99).reshape(-1,3))
x = df.apply(lambda x: x.tolist(), axis=1)

#change window size here
window_size = 3 

df[3] = reduce(lambda x, y: x+y, [x.shift(i) for i in range(window_size,-1,-1)])

df
</code></pre>
<p>Output:</p>
<pre><code>     0   1   2                                                 3
0    0   1   2                                               NaN
1    3   4   5                                               NaN
2    6   7   8                                               NaN
3    9  10  11            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
4   12  13  14         [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
5   15  16  17      [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
6   18  19  20   [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
7   21  22  23  [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
8   24  25  26  [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]
9   27  28  29  [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
10  30  31  32  [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
11  33  34  35  [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
12  36  37  38  [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]
13  39  40  41  [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
14  42  43  44  [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
15  45  46  47  [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
16  48  49  50  [39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
17  51  52  53  [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
18  54  55  56  [45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
19  57  58  59  [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
20  60  61  62  [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]
21  63  64  65  [54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]
22  66  67  68  [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]
23  69  70  71  [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
24  72  73  74  [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]
25  75  76  77  [66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]
26  78  79  80  [69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]
27  81  82  83  [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
28  84  85  86  [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]
29  87  88  89  [78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
30  90  91  92  [81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92]
31  93  94  95  [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
32  96  97  98  [87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]
</code></pre>
","1","Answer"
"79616248","79610568","<p>The issue is that when you try to insert a numpy array into a pandas DataFrame, pandas can't process the data correctly. To fix this, you can use either a <code>pd.Series</code> or a dictionary for better alignment:</p>
<p>first way: Using <code>pd.Series</code>:</p>
<pre><code>df.loc[&quot;bnd&quot;] = pd.Series([bnd2, &quot;N/A&quot;], index=[&quot;val&quot;, &quot;unit&quot;])
</code></pre>
<p>OR</p>
<p>second way: Using dictionary:</p>
<pre><code>df.loc[&quot;bnd&quot;] = {&quot;val&quot;: bnd2, &quot;unit&quot;: &quot;N/A&quot;}
</code></pre>
<p>good luck mate</p>
","1","Answer"
"79616889","79615990","<p>You can try <a href=""https://docs.python.org/3/library/itertools.html#itertools.chain"" rel=""nofollow noreferrer""><code>itertools.chain</code></a> like below</p>
<pre><code>from itertools import chain
def rolling_concat(x, wnd = 1):
  return [list(chain(*x[i-wnd:i+1])) if i&gt;= wnd else [] for i, v in enumerate(x)]
</code></pre>
<p>such that, for example, given a list <code>x</code></p>
<pre><code>x = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18]]

print(f'n=1:\n {rolling_concat(x, 1)}\n')
print(f'n=2:\n {rolling_concat(x, 2)}\n')
print(f'n=3:\n {rolling_concat(x, 3)}\n')
</code></pre>
<p>you will see</p>
<pre><code>n=1:
 [[], [1, 2, 3, 4, 5, 6], [4, 5, 6, 7, 8, 9], [7, 8, 9, 10, 11, 12], [10, 11, 12, 13, 14, 15], [13, 14, 15, 16, 17, 18]]

n=2:
 [[], [], [1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12], [7, 8, 9, 10, 11, 12, 13, 14, 15], [10, 11, 12, 13, 14, 15, 16, 17, 18]]

n=3:
 [[], [], [], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]
</code></pre>
","2","Answer"
"79616983","79615990","<p>Using native python:</p>
<pre><code>x = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18]]

[[*i, j + i if j else j] for i, j in zip(x, [[]] + x[:-1])]


[[1, 2, 3, []], [4, 5, 6, [1, 2, 3, 4, 5, 6]], [7, 8, 9, [4, 5, 6, 7, 8, 9]], [10, 11, 12, [7, 8, 9, 10, 11, 12]]]
</code></pre>
","1","Answer"
"79617113","79617077","<p>Find the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.idxmax.html"" rel=""nofollow noreferrer"">index of the highest value in each group</a> for the <code>Time</code> column, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer"">select those rows</a>:</p>
<pre><code>df.loc[df.groupby([&quot;Gender&quot;, &quot;Grade&quot;])[&quot;Time&quot;].idxmax()]
</code></pre>
<p><code>max</code> is a red herring; <code>idxmax</code> gives you much more to work with.</p>
","3","Answer"
"79617321","79617077","<p>Set Name as index with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a>, then replace <code>groupby.max</code> with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.idxmax.html"" rel=""nofollow noreferrer""><code>groupby.idxmax</code></a>:</p>
<pre><code>df.set_index('Name').groupby(['Gender', 'Grade'])['Time'].idxmax()
</code></pre>
<p>Output:</p>
<pre><code>Gender  Grade
F       10       Lilly
        11        Jane
M       10        John
        11        Matt
Name: Time, dtype: object
</code></pre>
","2","Answer"
"79618270","79618258","<p>The solution is to use <code>bbox_inches = 'tight'</code> in the <code>plt.savefig()</code> function:</p>
<pre><code>import matplotlib.pyplot as plt

plt.savefig(&quot;histogram.png&quot;,bbox_inches='tight')
plt.savefig(&quot;histogram.pdf&quot;, bbox_inches='tight')
</code></pre>
<p><a href=""https://i.sstatic.net/f5sj8EL6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f5sj8EL6.png"" alt=""enter image description here"" /></a></p>
","3","Answer"
"79618479","79618357","<p>The documentation of <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html#pandas.read_sql_query"" rel=""nofollow noreferrer"">read_sql_query</a> says the following:</p>
<pre><code>params : list, tuple or mapping, optional, default: None

    List of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249’s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}.
</code></pre>
<p>Since you use the psycopg2 driver the parameters should be noted as @JonSG has mentioned. It should be:</p>
<pre><code>select *
FROM public.bq_results br 
WHERE cast(&quot;eventDate&quot; as date) between 
  TO_DATE(%(test_start_date)s, 'YYYYMMDD') AND TO_DATE(%(test_end_date)s, 'YYYYMMDD')
limit 10000
</code></pre>
<p>Hope this works.</p>
","2","Answer"
"79618997","79618972","<p>Load the data in a pandas DF and then iterate over the rows. For each row, run a loop for the count value.<br />
It is an inefficient approach. There can be a better way.</p>
<pre><code>df = pd.read_csv(&quot;if in csv&quot;) # load the data in df. I don't know what format it is in.
expanded_rows = []

for _, row in df.iterrows():
    for i in range(row['Count']):
        expanded_rows.append({
            'Category': row['Category'],
            'Datetime': row['Datetime'] + timedelta(hours=i),
            'Value': row['Value']
        })

expanded_df = pd.DataFrame(expanded_rows)

expanded_df.sort_values(by=['Category', 'Datetime'], inplace=True)
</code></pre>
","0","Answer"
"79619074","79618972","<p>To transform the compressed table into the desired expanded format, each row must be unpacked based on the <code>Count</code> field by generating consecutive hourly timestamps starting from the given <code>Datetime</code>. For each row, we replicate the <code>Value</code> for the number of hours specified by <code>Count</code>, incrementing the timestamp by one hour for each replication. This can be efficiently done using Python with Pandas by iterating through each row, creating new entries with updated timestamps, and compiling the results into a new DataFrame. Sorting the final output by <code>Category</code> and <code>Datetime</code> ensures the structure aligns with the expected chronological order. This approach effectively restores the original granularity of the time series data while maintaining category-wise separation.</p>
","0","Answer"
"79619076","79619061","<p>You could rework the dictionary and use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>assign</code></a>:</p>
<pre><code>out = df.assign(**{col: df.get(k) for k, v in mapping.items() for col in v})
</code></pre>
<p><em>NB. <code>assign</code> is not in place, either use this in chained commands, or reassign to <code>df</code>.</em></p>
<p>Or you could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>reindex</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_axis.html"" rel=""nofollow noreferrer""><code>set_axis</code></a>:</p>
<pre><code>dic = {v: k for k, l in mapping.items() for v in l}
out = (df.reindex(columns=df.rename(columns=dic).columns)
         .set_axis(df.columns, axis=1)
      )
</code></pre>
<p>Output:</p>
<pre><code>    A1   A2   A3   A4   A5
0    1    1    1    4    4
1   11   11   11   44   44
2  111  111  111  444  444
</code></pre>
","4","Answer"
"79619095","79619061","<p>Your solution is very good. However, might be little slow for large dataframes.</p>
<p>This solution may be faster:</p>
<pre><code>for k, v in mapping.items():
    df.loc[:, v] = df[k].values[:, None]
</code></pre>
<p>Or:</p>
<pre><code>df_update = {col: df[key].values for key, cols in mapping.items() for col in cols}
df = df.assign(**df_update)
</code></pre>
","2","Answer"
"79619191","79601117","<p>I would do it like below with adding a fake red dot outside the y range:</p>
<pre><code>import numpy as np
import pandas as pd
import plotly.express as px

# define your frames 
frame = np.arange(10)
#define x values for each frame 
x = [np.arange(f+1) for f in frame] 
# add the y values 
y =  x 
# define the colors 
color = [ ['blue' if val &lt;3 else 'red' for val in xx] for xx in x] 
#put that in a dataframe 
df = pd.DataFrame().assign(frame = frame,x=x,y=y,color=color) 
#explode the lists of list 
df = df.explode(['x','y','color']) 
# add a fake point on frame 0 with red color and y value below the y range min 
df.loc[-1] = [0,3,-2,'red']  

# plot it 
px.scatter(df,x='x',y='y',color='color',animation_frame='frame').update_yaxes(range=[-1,10]).update_xaxes(range=[-1,10])
</code></pre>
<p>you will just see a somehow weird behavior for the first red point, sliding from the bottom but not sure if/how it can be done in a better way.</p>
<p>Another approach could be to define it with correct x and y but with a size at 0. Then the glitch would be that the dot will &quot;grow&quot; in size on the correct spot.</p>
","1","Answer"
"79619394","79608752","<p>To change the size of your bubbles just do something like this:</p>
<pre><code>
multiplier = 1.04  # made bigger by 4%
df[&quot;circle_size&quot;] = df[&quot;circle_size&quot;]*multiplier
</code></pre>
<p>But you still need to change the maximum size of the bubbles:</p>
<p>Here i changed it to the biggest bubble size:</p>
<pre class=""lang-py prettyprint-override""><code>biggest_bubble_size = max(df[&quot;circle_size&quot;])

fig = px.scatter(
        df,
        x=&quot;x&quot;, 
        y=&quot;y&quot;, 
        color=&quot;color&quot;,
        size='circle_size',
        size_max= biggest_bubble_size,   #change the maximum size of bubbles
        text=&quot;lib_acte&quot;,
        hover_name=&quot;lib_acte&quot;,
        color_discrete_map={&quot;red&quot;: &quot;red&quot;, &quot;green&quot;: &quot;green&quot;},
        title=&quot;chart&quot;
      )
</code></pre>
<p>Now the bubbles seam too large, so i recommend setting the multiplier to something like this:</p>
<pre class=""lang-py prettyprint-override""><code>multiplier = 0.1
</code></pre>
<p>And for the points being too close together just change the graph scale acording to the furthest point:</p>
<pre class=""lang-py prettyprint-override""><code>maxrange = max(df[&quot;x&quot;].max(), df[&quot;y&quot;].max())
maxrange = maxrange*1.2  #to give some space to the furthest point

number_of_lines = 10 #changes the number of lines in the graph
</code></pre>
<pre class=""lang-py prettyprint-override""><code>fig.update_layout(
        {
            
            'yaxis': {
                &quot;range&quot;: [-maxrange, maxrange],
                'zerolinewidth': 2, 
                &quot;zerolinecolor&quot;: &quot;red&quot;,
                &quot;tick0&quot;: -maxrange,
                &quot;dtick&quot;:maxrange/number_of_lines,
            },
            'xaxis': {
                &quot;range&quot;: [-maxrange, maxrange],
                'zerolinewidth': 2, 
                &quot;zerolinecolor&quot;: &quot;gray&quot;,
                &quot;tick0&quot;: -maxrange,
                &quot;dtick&quot;: maxrange/number_of_lines,
                #  &quot;scaleanchor&quot;: 'y'
            },
           
            &quot;height&quot;: 800,
            
            
        }
    )
</code></pre>
<p><a href=""https://i.sstatic.net/xdImDEiI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xdImDEiI.png"" alt=""enter image description here"" /></a></p>
","0","Answer"
"79619973","79619950","<p>The issue with both attempts is that you are looking for a substring of the columns name. Except for the <code>date</code> column there is not a full match between the strings in the <code>animals</code> list and the actual column names.</p>
<p>One possibility is to use <code>filter</code> with  <code>.join</code> to construct the regex if using <code>.filter</code>, or a &quot;manual&quot; list comprehension with strings operations (for example <code>in</code> or <code>.startswith</code>).</p>
<p>You can also &quot;hardcode&quot; <code>&quot;date&quot;</code> so the <code>animals</code> list only contains animals.</p>
<pre><code>&gt;&gt;&gt; animals = [&quot;cat&quot;]
&gt;&gt;&gt; df.filter(regex=&quot;date|&quot; + &quot;|&quot;.join(animals))
         date  cats_fostered  cats_adopted
0  2023-01-22              1             1
1  2023-11-16              2             2
2  2024-06-30              3             3
3  2024-08-16              4             4
4  2025-01-22              5             5
&gt;&gt;&gt; animals = [&quot;cat&quot;, &quot;rabbit&quot;]
&gt;&gt;&gt; df.filter(regex=&quot;date|&quot; + &quot;|&quot;.join(animals))
         date  cats_fostered  cats_adopted  rabbits_fostered  rabbits_adopted
0  2023-01-22              1             1                 1                1
1  2023-11-16              2             2                 2                2
2  2024-06-30              3             3                 3                3
3  2024-08-16              4             4                 4                4
4  2025-01-22              5             5                 5                5
</code></pre>
","1","Answer"
"79619980","79619950","<p>If you just would like to simple substring matching and don't want to match date exactly, please use this solution:</p>
<pre><code>animals = [&quot;date&quot;, &quot;cat&quot;, &quot;rabbit&quot;]

pattern = &quot;|&quot;.join(animals)

filtered_df = df.filter(regex=pattern)

print(filtered_df)
</code></pre>
<p>If you would to match date exactly, this solution may be better:</p>
<pre><code>df = pd.DataFrame(animal_data)

pattern = &quot;|&quot;.join([f&quot;^{animal}$&quot; if animal == &quot;date&quot; else animal for animal in animals])

filtered_df = df.filter(regex=pattern)

print(filtered_df)
</code></pre>
<p>This one also maybe better. Date matches exactly but not case-sensitive:</p>
<pre><code> 
animals = [&quot;date&quot;, &quot;cat&quot;, &quot;rabbit&quot;]
pattern = &quot;(?i)&quot; + &quot;|&quot;.join([f&quot;^{col}$&quot; if col == &quot;date&quot; else col for col in animals])
filtered_df = df.filter(regex=pattern)
</code></pre>
<p>Output:</p>
<pre><code>0  2023-01-22              1             1                 1                1
1  2023-11-16              2             2                 2                2
2  2024-06-30              3             3                 3                3
3  2024-08-16              4             4                 4                4
4  2025-01-22              5             5                 5                5
</code></pre>
","-1","Answer"
"79620138","79619950","<p>Another possible solution:</p>
<pre><code>df[[x for x in df.columns if any([y == x[:len(y)] for y in animals])]]
</code></pre>
<p>This solution checks if any element from the list <code>animals</code> matches the start of each column name, preserving only those that do. Specifically, the list comprehension <code>[x for x in df.columns if any([y == x[:len(y)] for y in animals])]</code> iterates over <code>df.columns</code>, and for each column name <code>x</code>, checks if any <code>y</code> in animals matches the prefix of <code>x</code> (i.e., <code>x[:len(y)]</code>). If there is a match, the column is included in the resulting list passed to <code>df[...]</code>.</p>
<p>Output:</p>
<pre><code>         date  cats_fostered  cats_adopted  rabbits_fostered  rabbits_adopted
0  2023-01-22              1             1                 1                1
1  2023-11-16              2             2                 2                2
2  2024-06-30              3             3                 3                3
3  2024-08-16              4             4                 4                4
4  2025-01-22              5             5                 5                5
</code></pre>
","2","Answer"
"79620264","79619950","<p>I think the comment suggesting the use of <code>startswith</code> to filter out unwanted columns is a pretty good way to handle this:</p>
<pre><code>df.filter(col for col in df.columns if any(col.startswith(name) for name in animals))
</code></pre>
<p>That said, there's always a chance you'll accidentally match things you don't want, like grabbing <em>caterpillar</em> when you're only after <em>cat</em>, or <em>dogfish</em> instead of just <em>dog</em>. But if you try using something like <code>regex='data|cat|rabbit'</code>, it can actually make things messier (not that you'll actually have any <em>caterpillar</em> or <em>dogfish</em> or <em>bobcat</em> in your data, but you get the idea).</p>
<p>Of course, you could come up with a more precise pattern to match. But I think a cleaner approach, if it works in your situation, is to use a <code>MultiIndex</code>. I guess you could split your column names and then easily pull out just the animals you care about, like this:</p>
<pre><code>import pandas as pd

animal_data = {
    &quot;date&quot;: [&quot;2023-01-22&quot;,&quot;2023-11-16&quot;,&quot;2024-06-30&quot;,&quot;2024-08-16&quot;,&quot;2025-01-22&quot;],
    &quot;cats_fostered&quot;: [1,2,3,4,5],
    &quot;cats_adopted&quot;: [1,2,3,4,5],
    &quot;dogs_fostered&quot;: [1,2,3,4,5],
    &quot;dogs_adopted&quot;: [1,2,3,4,5],
    &quot;rabbits_fostered&quot;: [1,2,3,4,5],
    &quot;rabbits_adopted&quot;: [1,2,3,4,5]
}

animals = ['cats', 'rabbits']

df = pd.DataFrame(animal_data).set_index('date')
df.columns = df.columns.str.split(&quot;_&quot;, expand=True)

print(df[animals])

#                cats          rabbits        
#            fostered adopted fostered adopted
# date                                        
# 2023-01-22        1       1        1       1
# 2023-11-16        2       2        2       2
# 2024-06-30        3       3        3       3
# 2024-08-16        4       4        4       4
# 2025-01-22        5       5        5       5
</code></pre>
<p>So if you're dealing with consistently named columns, switching to a <code>MultiIndex</code> setup can make your life easier.</p>
","1","Answer"
"79620354","79620333","<p>Based on your comment, you could shift all cols up one and add a col 0 like this:</p>
<pre><code>
import pandas as pd
data = [[5011025, 234], [5012025, 937], [5013025, 625]]
df = pd.DataFrame(data)
df.columns = df.columns + 1
df[0] = '   '
df = df.sort_index(axis=1)
</code></pre>
","2","Answer"
"79620862","79620822","<p>Looking at your error there might be some column in which already some numpy arrays are present.</p>
<p>Use this code, it will load without crashing you can debug further:</p>
<pre><code>df = pd.read_csv(&quot;xxxxxx.csv&quot;, dtype=object)
</code></pre>
<p>Check your csv first and verify below link, lt is not a numpy error it is some issue with csv loading</p>
<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html</a></p>
","1","Answer"
"79620912","79620883","<p>Maybe repeat a shorter <code>DataFrame</code> until it matches with the length of the longer one, use <code>pd.concat</code> and combine with slicing.</p>
<pre><code>import pandas as pd
import numpy as np

df1 = pd.DataFrame({'A': range(7)})   # length 7  
df2 = pd.DataFrame({'B': range(43)})  # length 43
repeats = -(-len(df2) // len(df1))  # Ceiling division
df1_repeated = pd.concat([df1] * repeats, ignore_index=True).iloc[:len(df2)]
df_comb = pd.concat([df1_repeated, df2], axis=1)
print(df_comb)
</code></pre>
","1","Answer"
"79620941","79620822","<p>I found the root cause. It is certainly numpy version issue. After I installed faiss using the pip install <code>conda install -c pytorch faiss-cpu</code> , the numpy version is overwritten to 1.26.4. Uninstall numpy and install again to get numpy 2.2.5 and use <code>conda install faiss-cpu</code>  to install faiss, all issues are resolved.</p>
","0","Answer"
"79621298","79600443","<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({
    'name': ['John', 'John', 'Bob', 'Alice', 'Alice', 'Alice'],
    'source': ['A', 'B', 'B', 'Z', 'Y', 'X'],
    'text': ['Text1', 'Longer text', 'Text2', 'Longer text', 'The Longest text', 'Text3'],
    'value': [1, 4, 2, 5, 3, 6]
})

longest_text =( df.assign(txtlen = df['text'].str.len())
    .loc[lambda x: x.groupby('name')['txtlen'].idxmax()]
    .drop(['txtlen','source'],axis =1)
    .set_index('name')
    .reset_index()
)
'''
    name              text  value
0  Alice  The Longest text      3
1    Bob             Text2      2
2   John       Longer text      4
'''

all_sources = df.groupby('name')['source'].apply(lambda x: ','.join(np.sort(x.unique())))
'''
name
Alice    X,Y,Z
Bob          B
John       A,B
Name: source, dtype: object
'''

longest_text['allSources'] = longest_text['name'].map(all_sources)
res = longest_text 
'''
    name              text  value allSources
0  Alice  The Longest text      3      X,Y,Z
1    Bob             Text2      2          B
2   John       Longer text      4        A,B
'''
</code></pre>
","0","Answer"
"79621734","79621650","<p>The issue is mainly with how you're trying to convert a target destinyfuction into a sampling distribution using differences and cumulative sums:</p>
<p>I have normalised the PDF from desired function, Integrated the PDF to get Cumulative destiny function, inverted CDF to map uniform samples to real time stamps and finally used the obtained time stamps as an event distribution.</p>
<p>Here is the working code with a screenshot:<br />
<a href=""https://snipboard.io/Y8ULpf.jpg"" rel=""nofollow noreferrer"">https://snipboard.io/Y8ULpf.jpg</a></p>
<pre><code>!pip install --upgrade scipy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d
from scipy import integrate
from scipy.integrate import cumulative_trapezoid
event_num = 50000
days = np.linspace(0, 365, 1000)  

density_raw = 0.5 * (np.sin((days / 58) - (np.pi / 2)) + 1)  

pdf = density_raw / np.trapz(density_raw, days)

cdf = cumulative_trapezoid(pdf, days, initial=0)
cdf /= cdf[-1]  

inv_cdf = interp1d(cdf, days)
samples = np.sort(np.random.rand(event_num)) 
event_days = inv_cdf(samples)  

event_dates = pd.to_datetime(event_days, unit=&quot;D&quot;, origin=&quot;2024-01-01&quot;)
density_per_day = pd.Series(event_dates.date).value_counts().sort_index()

plt.plot(days, pdf)
plt.title(&quot;Desired Probability Density Function (PDF)&quot;)
plt.xlabel(&quot;Day of year&quot;)
plt.ylabel(&quot;Density&quot;)
plt.grid(True)
plt.show()

plt.plot(density_per_day.index, density_per_day.values, marker='o', linestyle='-')
plt.xlabel(&quot;Date&quot;)
plt.ylabel(&quot;Number of events / day&quot;)
plt.title(&quot;Generated Event Density by Day&quot;)
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()
</code></pre>
","0","Answer"
"79621792","79620883","<p>If you want to combine the two DataFrames to obtain an output DataFrame of the length of the longest input with repetitions of the smallest input that restart like <code>itertools.cycle</code>,
you could compute a common key (with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.arange.html"" rel=""nofollow noreferrer""><code>numpy.arange</code></a> and the modulo (<code>%</code>) operator) to perform a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>:</p>
<pre><code>out = (df1.merge(df2, left_on=np.arange(len(df1))%len(df2),
                      right_on=np.arange(len(df2))%len(df1))
          .drop(columns=['key_0'])
      )
</code></pre>
<p>Output:</p>
<pre><code>  col1 col2 col3 col4
0    A    X    a    Y
1    B    X    b    Y
2    C    X    c    Y
3    D    X    a    Y
4    E    X    b    Y
5    F    X    c    Y
6    G    X    a    Y
</code></pre>
<p>Intermediate without dropping the merging key:</p>
<pre><code>   key_0 col1 col2 col3 col4
0      0    A    X    a    Y
1      1    B    X    b    Y
2      2    C    X    c    Y
3      0    D    X    a    Y
4      1    E    X    b    Y
5      2    F    X    c    Y
6      0    G    X    a    Y
</code></pre>
<p>Used inputs:</p>
<pre><code># df1
  col1 col2
0    A    X
1    B    X
2    C    X
3    D    X
4    E    X
5    F    X
6    G    X

# df2
  col3 col4
0    a    Y
1    b    Y
2    c    Y
</code></pre>
","2","Answer"
"79621814","79621793","<p>First of all by default, <em><strong>Remember that Matplotlib automatically adjusts the y-axis limits based on the range of your data, so the axis may start at the minimum value in your dataset rather than zero</strong></em>  so that's why Matplotlib automatically sets the y-axis range. If your water production values start at 108 and never go below that, Matplotlib will set the bottom of the y-axis to 108 (or slightly below), not 0. This is why your plot starts at 108 instead of 0-it is autoscaling to the minimum and maximum values present in your data.</p>
<p>So the first thing to do is to make the y-axis always start at 0 (which is common for production or quantity plots), you need to manually set the y-axis limits after plotting. You can do this with <strong><code>plt.ylim()</code></strong> or by getting the axis object and using <strong><code>set_ylim()</code>.</strong></p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
filename = &quot;Waterproduction.csv&quot;
data = pd.read_csv(filename, sep=&quot;\\t&quot;, encoding=&quot;Latin-1&quot;, engine=&quot;python&quot;)
year = data.iloc[:, 0]
water = data.iloc[:, 2].astype(float)  # Ensure water data is float
plt.figure()
plt.plot(year, water, marker='o')
plt.xlabel(&quot;Years&quot;)
plt.ylabel(&quot;Waterproduction (mill. m³)&quot;)
plt.title(&quot;Waterproduction&quot;)
plt.ylim(bottom=0)  # This line forces y-axis to start at 0
plt.show()
</code></pre>
<p>Just a suggestion is to  put your water data as float, this is because your plot may look odd and to make it look better ensure your data is numeric (float or int), not string</p>
<p>Or if you are using axes directly you apply this:</p>
<pre><code>fig, ax = plt.subplots()
ax.plot(year, water, marker='o')
ax.set_xlabel(&quot;Years&quot;)
ax.set_ylabel(&quot;Waterproduction (mill. m³)&quot;)
ax.set_title(&quot;Waterproduction&quot;)
ax.set_ylim(bottom=0)  # Set y-axis minimum to 0
plt.show()
</code></pre>
","1","Answer"
"79621969","79621955","<p>Use <code>groupby.transform</code> ?</p>
<p>This does the job. This works on large datasets.</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    &quot;group&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;],
    &quot;value&quot;: [10, 14, 3, 4, 9, 20],
})
df[&quot;value_centered&quot;] = df[&quot;value&quot;] - df.groupby(&quot;group&quot;)[&quot;value&quot;].transform(&quot;mean&quot;)

print(df)
</code></pre>
","0","Answer"
"79623221","79623174","<p>I think you could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.dropna.html"" rel=""nofollow noreferrer""><code>dropna</code></a>, then compute the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.pct_change.html"" rel=""nofollow noreferrer""><code>pct_change</code></a> and only keep the max finite value:</p>
<pre><code>series_test.dropna().pct_change().loc[np.isfinite].max()
</code></pre>
<p>Or maybe:</p>
<pre><code>s.pct_change().where(np.isfinite, 0).max()
</code></pre>
<p>Example output for the second approach:</p>
<pre><code>[0, nan, 50] - 0.0
[0, 0, 0] - 0.0
[0, 0, 50] - 0.0
[nan, nan, 50] - 0.0
[nan, nan, 0] - 0.0
[0, 0, nan] - 0.0
[0, nan, 0] - 0.0
[1, nan, 5] - 4.0
[0, 1, 5] - 4.0
[0, 1, nan] - 0.0
</code></pre>
<p>Edit: given your comment, it looks like you want to use the first and last non-zero values to compute the percentage change.</p>
<p>In this case, I'd use a custom function:</p>
<pre><code>def pct_chg(s):
    tmp = s[s&gt;0]
    if len(tmp)&gt;1:
        return (tmp.iloc[-1]-tmp.iloc[0])/tmp.iloc[0]
    return 0
</code></pre>
<p>Which should be equivalent to the more verbose:</p>
<pre><code>(series_test
 .where(s&gt;0).bfill().ffill()
 .iloc[[0, -1]].pct_change().fillna(0).iloc[-1]
)
</code></pre>
<p>Example:</p>
<pre><code>[0, nan, 50] - 0
[0, 0, 0] - 0
[0, 0, 50] - 0
[nan, nan, 50] - 0
[nan, nan, 0] - 0
[0, 0, nan] - 0
[0, nan, 0] - 0
[1, nan, 5] - 4.0
[0, 1, 5] - 4.0
[0, 1, nan] - 0
[1, 1.5, 1.6] - 0.6000000000000001
</code></pre>
","2","Answer"
"79623438","79600443","<p>This is one of the Most Efficient Solution for Huge Datasets. Use Numba  + Numpy :</p>
<pre><code> import pandas as pd
import numpy as np
from numba import njit

df = pd.DataFrame({
    'name': ['John', 'John', 'Bob', 'Alice', 'Alice', 'Alice'],
    'source': ['A', 'B', 'B', 'Z', 'Y', 'X'],
    'text': ['Text1', 'Longer text', 'Text2', 'Longer text', 'The Longest text', 'Text3'],
    'value': [1, 4, 2, 5, 3, 6]
})

df['name_cat'],_ = pd.factorize(df['name'])
txt_len= df['text'].str.len().to_numpy()
name_cat_np = df['name_cat'].to_numpy()

@njit
def get_longest_text_indices(name_cat_np, textlen):
    seen = {}
    for i in range(len(name_cat_np)):
        aa = name_cat_np[i]
        if aa not in seen or textlen[i] &gt; textlen[seen[aa]] :
            seen[aa] = i 
    return np.array(list(seen.values()))        


idx_longest_text =  get_longest_text_indices(name_cat_np, txt_len)
res = df.iloc[idx_longest_text].copy()
'''
    name source              text  value  name_cat
1   John      B       Longer text      4        0
2    Bob      B             Text2      2        1
4  Alice      Y  The Longest text      3        2
'''
all_sources = df.groupby('name')['source'].agg(
lambda x: ','.join(sorted(x))    
)

res['all_sources'] = res['name'].map(all_sources)
res.reset_index(drop=True, inplace=True)
res_final = res.drop(columns=['name_cat', 'source'])

'''
   name              text  value all_sources
0   John       Longer text      4        A,B
1    Bob             Text2      2          B
2  Alice  The Longest text      3      X,Y,Z
'''
</code></pre>
","0","Answer"
"79623743","79623716","<pre><code>
# Make a copy of the DataFrame
df_copy = df.copy()
import pandas as pd

result = []

for drink in order:
    idx = df_copy[df_copy['Drink'] == drink].index.min()
    if pd.notna(idx):
        result.append(df_copy.loc[idx])
        df_copy = df_copy.drop(index=idx)


ordered_df = pd.DataFrame(result)
</code></pre>
","1","Answer"
"79623798","79623716","<p>On second thought, I think you really want this:</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'Drink': ['Beer', 'Beer', 'Wine', 'Wine', 'Wine', 'Whisky', 'Whisky'], 'Units': [14, 5, 9, 15, 7, 12, 17]})

order = ['Wine', 'Beer', 'Whisky', 'Beer', 'Wine', 'Whisky']
orders = pd.Series(order)

idx = orders.to_frame().assign(sortkey=orders.groupby(orders).cumcount())

df_out = df.assign(sortkey=df.groupby('Drink', sort=False).cumcount())\
           .set_index(['Drink','sortkey'])\
           .reindex(idx).reset_index()\
           .drop('sortkey', axis=1)
print(df_out)
</code></pre>
<p>Output:</p>
<pre><code>    Drink  Units
0    Wine      9
1    Beer     14
2  Whisky     12
3    Beer      5
4    Wine     15
5  Whisky     17
</code></pre>
<hr />
<p>IIUC, you want to sort based on Drink,</p>
<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'Drink': ['Beer', 'Beer', 'Wine', 'Wine', 'Wine', 'Whisky', 'Whisky'], 'Units': [14, 5, 9, 15, 7, 12, 17]})

order = ['Wine', 'Beer', 'Whisky', 'Beer', 'Wine', 'Whisky']

orderDtype = pd.CategoricalDtype(categories = ['Wine', 'Beer', 'Whisky'], ordered = True)

df_out = df.assign(sortkey=df.groupby('Drink', sort=False).cumcount(), Drink=df['Drink'].astype(orderDtype))\
           .sort_values(['sortkey', 'Drink'])\
           .drop('sortkey', axis=1)

print(df_out)
</code></pre>
<p>Output:</p>
<pre><code>    Drink  Units
2    Wine      9
0    Beer     14
5  Whisky     12
3    Wine     15
1    Beer      5
6  Whisky     17
4    Wine      7
</code></pre>
","3","Answer"
"79623836","79623819","<ul>
<li><p><code>pandas.read_excel()</code> tries to download the file using standard HTTP.</p>
</li>
<li><p>If the URL is <strong>not reachable</strong>, <code>pandas</code> downloads an <strong>HTML page or an error message</strong>, not an Excel file.</p>
</li>
<li><p>This leads to <code>ValueError</code> or <code>XLRDError: Unsupported format</code> because it’s reading a non-Excel file.</p>
</li>
</ul>
<p>Check What You're Actually Downloading, You can debug with:</p>
<p>print<code>(response.headers['Content-Type']) </code><br />
<code>print(response.content[:200])</code><br />
If you see <code>text/html</code>,  you're likely getting a login page or error page.</p>
","0","Answer"
"79623935","79623819","<p>This error is happening because <code>pd.read_excel(url)</code> can't identify the file type automatically, and the second error suggests that the file is either unsupported or not a valid Excel file.</p>
<p>If you check on your browser, you will notice that URL is valid and will download the .XLS file. This occurs because the server is not identifying it as a valid request and return an error instead of returning the valid file.</p>
<p>To solve this problem you can import request on your code to access the .XLS file and read it with pandas:</p>
<pre><code>import requests
import pandas as pd

url = 'https://intranet2.sbs.gob.pe/estadistica/financiera/2024/Julio/C-4252-jl2024.XLS'

headers = {
    &quot;User-Agent&quot;: (
        &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0&quot;
    )
}

response = requests.get(url, headers=headers)

with open('file.xls', 'wb') as f:
    f.write(response.content)


df = pd.read_excel('file.xls', engine='xlrd')
print(df.head())
</code></pre>
<p>The code above should work and solve your problem.</p>
","1","Answer"
"79624133","79623716","<p>Another possible solution:</p>
<pre><code>order_df = pd.DataFrame(order, columns=['Drink'])

(order_df.assign(
    o = order_df.groupby('Drink').cumcount())
.merge(df.assign(
    o = df.groupby('Drink').cumcount()), 
       how='left', on=['Drink', 'o']).drop('o', axis=1))
</code></pre>
<p>This solution constructs a temporary dataframe <code>order_df</code> from the desired drink order and uses  <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>groupby().cumcount()</code></a> to create an occurrence index <code>o</code> that tracks the position of each repeated drink. This same technique is applied to the original <code>df</code>, generating matching indices for repeated drink entries. By <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merging</code></a> the two dataframes on both <code>'Drink'</code> and <code>'o'</code>, the code aligns the entries from <code>df</code> to match the specific sequence in <code>order</code>, including duplicates in the desired positions. Finally, the temporary index column <code>o</code> is dropped.</p>
<p>Output:</p>
<pre><code>    Drink  Units
0    Wine      9
1    Beer     14
2  Whisky     12
3    Beer      5
4    Wine     15
5  Whisky     17
</code></pre>
","2","Answer"
"79624136","79623716","<p>I think you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.merge.html#pandas-merge"" rel=""nofollow noreferrer""><code>pd.merge</code></a> like below</p>
<pre><code>pd.DataFrame({'Drink': order})\
.assign(id=lambda x: x.groupby('Drink').cumcount())\
.merge(df.assign(id = lambda x:x.groupby('Drink').cumcount()))\
.drop('id', axis=1)
</code></pre>
<p>which gives</p>
<pre><code>    Drink  Units
0    Wine      9
1    Beer     14
2  Whisky     12
3    Beer      5
4    Wine     15
5  Whisky     17
</code></pre>
","1","Answer"
"79624275","79624103","<h2>Concat, sort, drop duplicates</h2>
<p>A more pandas-idiomatic and efficient method is to:</p>
<ol>
<li><p>Concatenate <code>dfA</code> and <code>dfB</code>.</p>
</li>
<li><p>Sort the combined data by 'ID' and 'registerDate' (ensuring the latest dates come first for each ID).</p>
</li>
<li><p>Drop duplicates, keeping the first occurrence for each 'ID'.</p>
</li>
</ol>
<p>This leverages pandas' optimized functions effectively.</p>
<h3>Prerequisite steps</h3>
<pre><code># Change string dates to datetime
def convert_dates(df, columns, date_format='%d/%m/%y'):
    for col in columns:
        df[col] = pd.to_datetime(df[col], format=date_format, errors='coerce')

convert_dates(dfA, ['dateFound', 'registerDate'])
convert_dates(dfB, ['dateFound', 'date_registered'])

# Rename columns in dfB
dfB_renamed = dfB.rename(columns={
    'thing_status': 'status',
    'date_registered': 'registerDate'
})
</code></pre>
<h3>Code</h3>
<pre class=""lang-py prettyprint-override""><code># 1. concat
combined_df = pd.concat([dfA, dfB_renamed], ignore_index=True)

# 2. Sort
combined_df_sorted = combined_df.sort_values(
    by=['ID', 'registerDate'],
    ascending=[True, False])

# 3. drop_duplicates
final_df = combined_df_sorted.drop_duplicates(subset=['ID'], keep='first')

final_df
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>ID  status  dateFound registerDate
 1       2 2024-04-06   2024-04-23
 2       2 2024-01-01   2024-01-01
 3       2 2024-01-04   2024-01-05
</code></pre>
<h3>Setup for reference</h3>
<pre><code>import pandas as pd
import numpy as np

# Sample dfA
data_a = {
    'ID': [1, 2, 3],
    'status': [1, 2, 2],
    'dateFound': [np.nan, '1/1/24', '2/1/24'],
    'registerDate': ['5/3/24', '1/1/24', '3/1/24']
}
dfA = pd.DataFrame(data_a)

# Sample dfB
data_b = {
    'ID': [1, 2, 3],
    'thing_status': [2, 1, 2],
    'dateFound': ['6/4/24', np.nan, '4/1/24'],
    'date_registered': ['23/4/24', '24/12/23', '5/1/24']
}
dfB = pd.DataFrame(data_b)
</code></pre>
","1","Answer"
"79624276","79624229","<p>That error usually means Python can't find the file in the location it's looking at, even if it looks like it's &quot;there&quot; in Jupyter Notebook.</p>
<p>A couple of things to check:</p>
<ol>
<li><p>Is the notebook in the same folder as the CSV file?<br />
If not, you’ll need to either move the CSV to the same folder or use the full file path.</p>
</li>
<li><p>Double-check the filename<br />
Make sure there are no typos, extra spaces, or different capitalization. File names are case-sensitive on some systems.</p>
</li>
<li><p>Double-check the working directory<br />
In a code cell, run this to check what folder your notebook is currently running in:</p>
<pre><code>import os os.getcwd() 
</code></pre>
<p>This will show the current working directory. Make sure your CSV is there, or update the path accordingly.</p>
</li>
</ol>
","1","Answer"
"79624335","79624224","<p>Export the columns to frame, convert Price values to titlecase and recreate the multi index columns back.</p>
<pre><code>cols = df.columns.to_frame().assign(Price=lambda x: x['Price'].str.title())
df.columns = pd.MultiIndex.from_frame(cols)
</code></pre>

<p>Before:</p>
<pre class=""lang-none prettyprint-override""><code>Ticker     A           ZTS           
Price  close open  low low close open
0         14    4   58  66    43   96
1         14  106   35  11   190  172
2         76  182   83  64   151   39
3         53   78  143  94    34   78
4         60   61   76  87   189   47
5         49  117   84  59   154  152
</code></pre>
<p>After:</p>
<pre class=""lang-none prettyprint-override""><code>Ticker     A           ZTS           
Price  Close Open  Low Low Close Open
0         14    4   58  66    43   96
1         14  106   35  11   190  172
2         76  182   83  64   151   39
3         53   78  143  94    34   78
4         60   61   76  87   189   47
5         49  117   84  59   154  152
</code></pre>
","1","Answer"
"79624396","79624224","<p>You can use <code>rename</code> with <code>level</code> parameter:</p>
<p>Where df,</p>
<pre class=""lang-none prettyprint-override""><code>Ticker     A          ZTS           
Price  close open low low close open
0          0    1   2   3     4    5
1          6    7   8   9    10   11
2         12   13  14  15    16   17
3         18   19  20  21    22   23
4         24   25  26  27    28   29
5         30   31  32  33    34   35
</code></pre>
<p>Use,</p>
<pre class=""lang-py prettyprint-override""><code>df.rename(columns=lambda x: x.title(), level=1)
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>Ticker     A          ZTS           
Price  Close Open Low Low Close Open
0          0    1   2   3     4    5
1          6    7   8   9    10   11
2         12   13  14  15    16   17
3         18   19  20  21    22   23
4         24   25  26  27    28   29
5         30   31  32  33    34   35
</code></pre>
","2","Answer"
"79624461","79624459","<p><code>pd.merge(dfA, dfB, on='productID', how='inner')</code></p>
<p>you can use <code>merge</code> along with the <code>how</code> type</p>
","1","Answer"
"79624545","79624459","<p>The pandas merge function has an argument &quot;validate&quot; that is used to check the dataframes for the type of merge: one-to-one, one-to-many, many-to-one or many-to-many.</p>
<p>This can be used to explicitly state what merge type is required for the data.</p>
<p>example:</p>
<pre class=""lang-py prettyprint-override""><code>df_a = pd.DataFrame.from_dict({
    &quot;product_id&quot;: [1, 2, 3],
    &quot;name&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]
})

df_b = pd.DataFrame.from_dict({
    &quot;purchase_id&quot;: [3, 2, 1],
    &quot;product_id&quot;: [1, 2, 3],
    &quot;quantity&quot;: [&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;],
    &quot;name&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]
})

df_merge = pd.merge(left=df_a, right=df_b, on=&quot;product_id&quot;, how=&quot;left&quot;, validate=&quot;1:m&quot;)
print(df_merge.info())
</code></pre>
<p>returns:</p>
<pre><code>Data columns (total 5 columns):
 #   Column       Non-Null Count  Dtype 
---  ------       --------------  ----- 
 0   product_id   3 non-null      int64 
 1   name_x       3 non-null      object
 2   purchase_id  3 non-null      int64 
 3   quantity     3 non-null      object
 4   name_y       3 non-null      object
dtypes: int64(2), object(3)
memory usage: 252.0+ bytes
None
</code></pre>
<p>pandas.merge also has another argmument: <code>suffixes</code>, that is used to define what suffix needs to be used if columns in the resulting dataframe overlap names like in this case with <code>name</code>. The default is <em><code>x</code></em> for left and <code>y</code> for the right dataframe.</p>
<p>Hopefully this helps!</p>
<p><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html"" rel=""nofollow noreferrer"">Pandas Merge Documentation</a></p>
","0","Answer"
"79625701","79600924","<p>I believe what you're looking for is to apply <code>pd.json_normalize</code> individually to each JSON object and transform the result into a <code>pd.Series</code> with a two-level index. The first level should be <code>'name'</code>, and the second level should consist of all other parameters from each atomic JSON item. A one-liner for this could be:</p>
<pre><code>df['col_json'].apply(lambda x: pd.json_normalize(x).set_index('name').stack())
</code></pre>
<p>The output from the above line looks like this:</p>
<p><a href=""https://i.sstatic.net/HiCZl6Oy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HiCZl6Oy.jpg"" alt=""output 1"" /></a></p>
<p>Or, to dig deeper, you have to specify the <code>record_path</code> and <code>meta</code> parameters of <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html"" rel=""nofollow noreferrer"">json_normalize</a> because of the inner list that wraps the parameter dictionary:</p>
<pre><code>df[&quot;col_json&quot;].apply(
    lambda x: pd.json_normalize(
        data=x, record_path=&quot;parameter&quot;, meta=[&quot;scalar&quot;, &quot;units&quot;, &quot;name&quot;]
    )
    .set_index(&quot;name&quot;)
    .stack()
)
</code></pre>
<p><a href=""https://i.sstatic.net/3EJkbAlD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3EJkbAlD.jpg"" alt=""output 2"" /></a></p>
<p>If preserving the <code>MultiIndex</code> isn't important, you can simply concatenate the result with the main <code>DataFrame</code>, kind of:</p>
<pre><code>output = pd.concat([
    df.drop(columns='col_json'),
    df['col_json'].apply(lambda x: pd.json_normalize(x).set_index('name').stack())
], axis='columns')
</code></pre>
<p>However, if you do want to keep the <code>MultiIndex</code>, you'll need to add a new column level to the original <code>DataFrame</code> before concatenation. For example:</p>
<pre><code>json_normalize = lambda x: (
    pd.json_normalize(x, 'parameter', ['scalar', 'units', 'name'])
    .set_index('name')
    .stack()
)

# make a new level at the very top of column levels and fill it with 'name'
add_level = lambda df, name: pd.concat([df], keys=[name], axis=1)

output = pd.concat(
    [
        df.drop(columns='col_json').pipe(add_level, 'Main'),
        df['col_json'].apply(json_normalize),
    ], 
    axis='columns'
)
</code></pre>
<p><a href=""https://i.sstatic.net/BLiSMrzu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BLiSMrzu.jpg"" alt=""final output"" /></a></p>
","1","Answer"
"79625811","79619577","<p>I tried to use plotly and that was able to give me the result I was looking for -</p>
<pre><code>import plotly.express as px
autompg_multi_index = autompg.query(&quot;yr&lt;=80&quot;).groupby(['yr', 'origin'])['mpg'].mean().reset_index()
fig = px.bar(autompg_multi_index, x='yr', y='mpg', color='origin', barmode='group', text='mpg')
fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')
fig.show()
</code></pre>
<p><a href=""https://i.sstatic.net/kEOOrKEb.png"" rel=""nofollow noreferrer"">Grouped bar chart with labels on top with 2 places after decimal point.</a></p>
<p>My solution is to abandon hvplot and migrate to plotly.</p>
","0","Answer"
"79625940","79600924","<p>I've defined a function <code>process_entries</code>  to process each <code>list</code> of <code>json</code> objects. This converts each entry into a <code>dict</code> with prefixed keys. This function is applied to each row of the dataframe's <code>json</code> column. Then let us convert the resulting <code>dict</code>s into a dataframe and join it with the original dataframe's other columns.</p>
<pre><code>import pandas as pd

json_2_explode = [
    {
        &quot;scalar&quot;: &quot;43&quot;,
        &quot;units&quot;: &quot;m&quot;,
        &quot;parameter&quot;: [{&quot;no_1&quot;: &quot;45&quot;, &quot;no_2&quot;: &quot;1038&quot;, &quot;no_3&quot;: &quot;356&quot;}],
        &quot;name&quot;: &quot;Foo&quot;,
    },
    {
        &quot;scalar&quot;: &quot;54.1&quot;,
        &quot;units&quot;: &quot;s&quot;,
        &quot;parameter&quot;: [{&quot;no_1&quot;: &quot;78&quot;, &quot;no_2&quot;: &quot;103&quot;, &quot;no_3&quot;: &quot;356&quot;}],
        &quot;name&quot;: &quot;Yoo&quot;,
    },
    {
        &quot;scalar&quot;: &quot;1123.1&quot;,
        &quot;units&quot;: &quot;Hz&quot;,
        &quot;parameter&quot;: [{&quot;no_1&quot;: &quot;21&quot;, &quot;no_2&quot;: &quot;43&quot;, &quot;no_3&quot;: &quot;3577&quot;}],
        &quot;name&quot;: &quot;Baz&quot;,
    },
]

df = pd.DataFrame(data={'col1': [11, 9, 23, 1],
                       'col2': [7, 3, 1, 12],
                       'col_json': [json_2_explode,
                                    json_2_explode,
                                    json_2_explode,
                                    json_2_explode]},
                  index=[0, 1, 2, 3])

def process_entries(json_list):
    row = {}
    for entry in json_list:
        name = entry['name']
        
        # Add scalar and units
        row[f&quot;{name}_scalar&quot;] = entry['scalar']
        row[f&quot;{name}_units&quot;] = entry['units']
        
        # Add parameters (assuming each parameter list has one dict)
        params = entry['parameter'][0]
        
        for key, value in params.items():
            row[f&quot;{name}_{key}&quot;] = value
            
        # Add name as a separate field
        row[f&quot;{name}_name&quot;] = name
        
    return row

# process each json list into a dictionary of new columns
processed = df['col_json'].apply(process_entries)
processed_df = pd.DataFrame(processed.tolist())

# join with original columns
result_df = pd.concat([df[['col1', 'col2']], processed_df], axis=1)

print(result_df)

</code></pre>
","1","Answer"
"79626031","79624103","<h2>Concat, groupby, select max date</h2>
<p>This fundamentally isn't a merge (AKA join); I would call it a de-duplication instead.</p>
<p>One way to approach this is to concat the dataframes (after renaming the columns to match), groupby <code>ID</code>, and select the max <code>registerDate</code> for each (after converting to datetime).</p>
<pre><code>df_result = (
    pd.concat([df_a, df_b], ignore_index=True)
    .pipe(lambda d:
        d[d['registerDate'] == d.groupby('ID')['registerDate'].transform('max')])
    .sort_values('ID')  # For display
)
</code></pre>
<pre class=""lang-none prettyprint-override""><code>   ID  status  dateFound registerDate
3   1       2 2024-04-06   2024-04-23
1   2       2 2024-01-01   2024-01-01
5   3       2 2024-01-04   2024-01-05
</code></pre>
<h3>Setup for reference</h3>
<pre><code>df_a = pd.DataFrame({
    'ID': [1, 2, 3],
    'status': [1, 2, 2],
    'dateFound': [pd.NaT, pd.Timestamp('2024-01-01'), pd.Timestamp('2024-01-02')],
    'registerDate': [pd.Timestamp('2024-03-05'), pd.Timestamp('2024-01-01'), pd.Timestamp('2024-01-03')]})

df_b = pd.DataFrame({
    'ID': [1, 2, 3],
    'status': [2, 1, 2],
    'dateFound': [pd.Timestamp('2024-04-06'), pd.NaT, pd.Timestamp('2024-01-04')],
    'registerDate': [pd.Timestamp('2024-04-23'), pd.Timestamp('2023-12-24'), pd.Timestamp('2024-01-05')]})
</code></pre>
","2","Answer"
"79626731","79626316","<p>If your requirement is that <code>secondDateTime</code> must be later than <code>initialDateTime</code>, use a condition to add 1 day to <code>secondDateTime</code>:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({
    &quot;initialDate&quot;: [&quot;2025-1-1&quot;],
    &quot;initialTime&quot;: [&quot;10:25 PM&quot;],
    &quot;secondTime&quot;: [&quot;1:25 AM&quot;]
})

d1 = pd.to_datetime(df[&quot;initialDate&quot;] + &quot; &quot; + df[&quot;initialTime&quot;])
d2 = pd.to_datetime(df[&quot;initialDate&quot;] + &quot; &quot; + df[&quot;secondTime&quot;])
df[&quot;initialDateTime&quot;] = d1
df[&quot;secondDateTime&quot;] = d2 + pd.to_timedelta(np.where(d2 &lt; d1, 1, 0), unit=&quot;d&quot;)
</code></pre>
","1","Answer"
"79627295","79600924","<p>I'd firstly try to flatten in such a way to have this form:</p>
<p><a href=""https://i.sstatic.net/IrSiReWk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IrSiReWk.png"" alt=""Flattened DataFrame"" /></a></p>
<p>and then group by <code>'name'</code>, prefix each group by it's name, and finally merge them together.</p>
<p>Here how can you achieve this (<code>df_flattened</code> will hold the shown form):</p>
<pre class=""lang-py prettyprint-override""><code># explode 'parameter' to flatten the list as well
df_normalized = pd.json_normalize(df['col_json'].explode()).explode('parameter')

# you can now explode 'parameter' by df_normalized['parameter'].apply(pd.Series)
df_flattened = df_normalized.drop('parameter', axis=1).join(df_normalized['parameter'].apply(pd.Series))

# group by 'name' and merge the groups
df_exploded = pd.concat(
    [df_g.drop('name', axis=1).add_prefix(name + '_').reset_index(drop=True) for name, df_g in df_flattened.groupby('name')],
    axis=1
)

# join together the rest of original df with exploded one
df_final = df.drop('col_json', axis=1).join(df_exploded)
</code></pre>
","1","Answer"
"79628081","79627995","<p>Type checkers use the types from <a href=""https://github.com/pandas-dev/pandas-stubs/tree/b5a97356f4eabce9b6c01eed7d0737e709a37753"" rel=""nofollow noreferrer""><code>pandas-stubs</code></a> instead of those directly defined in <code>pandas</code>. There we find the correct type: <a href=""https://github.com/pandas-dev/pandas-stubs/blob/b5a97356f4eabce9b6c01eed7d0737e709a37753/pandas-stubs/_libs/tslibs/timedeltas.pyi#L51"" rel=""nofollow noreferrer""><code>TimeDeltaUnitChoices</code></a>.</p>
<p>Since it comes from the stubs, <code>TimeDeltaUnitChoices</code> doesn't exist at runtime, so you'll need to import it under <code>if TYPE_CHECKING</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from pandas._libs.tslibs.timedeltas import TimeDeltaUnitChoices
</code></pre>
<p>Usages of it will then need to be quoted to avoid evaluation:</p>
<pre class=""lang-py prettyprint-override""><code>period: 'TimeDeltaUnitChoices' = 'minutes'
#       ^^^^^^^^^^^^^^^^^^^^^^ quoted
</code></pre>
<p>Alternatively:</p>
<pre class=""lang-py prettyprint-override""><code>from __future__ import annotations

period: TimeDeltaUnitChoices = 'minutes'
</code></pre>
","3","Answer"
"79628479","79625976","<p>Try this instead :</p>
<pre><code>x = pd.DataFrame(my_data2[&quot;trestbps&quot;])  
y = pd.DataFrame(my_data2[&quot;chol&quot;])
</code></pre>
<p>Or</p>
<pre><code>x = my_data2[&quot;trestbps&quot;].values.reshape(-1, 1)   
my_data2[&quot;chol&quot;].values.reshape(-1, 1)  
</code></pre>
<p>The reason is that scikit-learn models work with 2D arrays.</p>
","0","Answer"
"79628549","79627856","<p>Your example is a bit ambiguous, but assuming you want to select rows for which the Error is &lt; 2.5 and the next 5 Errors are also &lt; 5, then use a reverse <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.window.rolling.Rolling.min.html"" rel=""nofollow noreferrer""><code>rolling.min</code></a> and a window of 6 (current row + next 5 rows):</p>
<pre><code>df['mask'] = df.loc[::-1, 'Error'].lt(2.5).rolling(window=6).min().eq(1)
</code></pre>
<p>Output:</p>
<pre><code>       Error    Inst   mask
0   2.595795  204267  False
1   2.568556  204268  False
2   2.562618  204269  False
4   2.538956  204271  False
5   2.520247  204272  False
6   2.498345  204273   True
7   2.474890  204274  False
8   2.467736  204275  False
9   2.471115  204276  False
10  2.466424  204280  False
11  2.495388  204284  False
12  2.520301  204285  False
13  2.604358  204291  False
14  2.553243  204299  False
15  2.490774  204302   True
16  2.452384  204303  False
17  2.434171  204304  False
18  2.404764  204305  False
19  2.388775  204306  False
20  2.384337  204307  False
</code></pre>
<p>If you only want to consider the next rows and ignore the current row's value, the you first need to <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html"" rel=""nofollow noreferrer""><code>shift</code></a>:</p>
<pre><code>df['mask'] = df.loc[::-1, 'Error'].shift().lt(2.5).rolling(window=5).min()
</code></pre>
<p>Output:</p>
<pre><code>       Error    Inst   mask
0   2.595795  204267  False
1   2.568556  204268  False
2   2.562618  204269  False
4   2.538956  204271  False
5   2.520247  204272   True
6   2.498345  204273   True
7   2.474890  204274  False
8   2.467736  204275  False
9   2.471115  204276  False
10  2.466424  204280  False
11  2.495388  204284  False
12  2.520301  204285  False
13  2.604358  204291  False
14  2.553243  204299   True
15  2.490774  204302   True
16  2.452384  204303  False
17  2.434171  204304  False
18  2.404764  204305  False
19  2.388775  204306  False
20  2.384337  204307  False
</code></pre>
<p>In this case, however, 4 rows are selected.</p>
<p>If you want to select the rows, use the mask for <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a> instead of assigning the result to a new column:</p>
<pre><code>out = df.loc[df.loc[::-1, 'Error'].lt(2.5).rolling(window=6).min().eq(1), 'Inst']
</code></pre>
<p>Output:</p>
<pre><code>6     204273
15    204302
Name: Inst, dtype: int64
</code></pre>
","0","Answer"
"79628915","79628910","<p>You could use a vectorial approach with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.isna.html"" rel=""nofollow noreferrer""><code>isna</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.cummin.html"" rel=""nofollow noreferrer""><code>cummin</code></a> to perform <a href=""https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a>.</p>
<p>First let's use one column as example:</p>
<pre><code># identify NaNs
m1 = df['A'].isna()
# Identify external NaNs
m2 = (m1.cummin()|m1[::-1].cummin())

out= df.loc[m2 | ~m1, 'A']
</code></pre>
<p>Output:</p>
<pre><code>0      NaN
1      NaN
2      NaN
3      1.0
4      4.0
5      6.0
6      6.0
7      9.0
9     13.0
10     NaN
11     NaN
Name: A, dtype: float64
</code></pre>
<p>Then you can vectorize to the whole DataFrame and aggregate with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.all.html"" rel=""nofollow noreferrer""><code>all</code></a>:</p>
<pre><code>m1 = df.isna()
m2 = (m1.cummin()|m1[::-1].cummin())

out= df.loc[(m2 | ~m1).all(axis=1)]
</code></pre>
<p>Output:</p>
<pre><code>       A     B
0    NaN   NaN
1    NaN   NaN
2    NaN   NaN
3    1.0  11.0
4    4.0   3.0
5    6.0  16.0
6    6.0  13.0
9   13.0  12.0
10   NaN   NaN
11   NaN   NaN
</code></pre>
<p>Another option would be to leverage <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html"" rel=""nofollow noreferrer""><code>interpolate</code></a> with <code>limit_area='inside'</code>:</p>
<pre><code># is the cell not NaN?
m1 = df.notna()
# is the cell an external NaN?
m2 = df.interpolate(limit_area='inside').isna()

out = df[(m1|m2).all(axis=1)]
</code></pre>
","2","Answer"
"79629077","79628859","<p>I think this solution may help you:</p>
<pre><code>import pandas as pd
import numpy as np

data = {
    'Cohort': [1]*4 + [2]*4,
    'age': [18, 19, 20, 21]*2,
    'Year 1': [1, 2, 3, 4, 1.5, 2.5, 3.5, 4.5],
    'year 2': [None, 1, 2, 3, None, 1.5, 2.5, 3.5],
    'year 3': [None, None, 1, 2, None, None, 1.5, 2.5],
    'year 4': [None, None, None, 1, None, None, None, 1.5]
}

df = pd.DataFrame(data)
df_long = df.melt(
    id_vars=['Cohort', 'age'],
    var_name='Year',
    value_name='Value'
)

df_long['Year_Num'] = df_long['Year'].str.extract(r'(\d+)').astype(int)

df_long['Age_Commence'] = df_long['age'] - (df_long['Year_Num'] - 1)

df_long = df_long.dropna(subset=['Value'])

pivot_df = df_long.pivot(
    index='Year_Num',
    columns=['Cohort', 'Age_Commence'],
    values='Value'
)

pivot_df = pivot_df.sort_index(axis=1, level=[0, 1])
pivot_df.index = [f'Year {i}' for i in pivot_df.index]

def clean_cell(x):
    if pd.isna(x):
        return ''
    elif float(x).is_integer():
        return str(int(x))
    else:
        return str(x)

pivot_clean = pivot_df.apply(lambda col: col.map(clean_cell))

print(pivot_clean)
</code></pre>
<p>This vectorized version may be good for large dataframes (not good for small dataframes):</p>
<pre><code>import pandas as pd
import numpy as np

data = {
    'Cohort': [1]*4 + [2]*4,
    'age': [18, 19, 20, 21]*2,
    'Year 1': [1, 2, 3, 4, 1.5, 2.5, 3.5, 4.5],
    'year 2': [None, 1, 2, 3, None, 1.5, 2.5, 3.5],
    'year 3': [None, None, 1, 2, None, None, 1.5, 2.5],
    'year 4': [None, None, None, 1, None, None, None, 1.5]
}

df = pd.DataFrame(data)

df_long = pd.melt(
    df,
    id_vars=['Cohort', 'age'],
    var_name='Year',
    value_name='Value'
)

df_long['Year_Num'] = df_long['Year'].str.extract(r'(\d+)').astype(np.uint8)

df_long['Age_Commence'] = df_long['age'] - (df_long['Year_Num'] - 1)

df_long.dropna(subset=['Value'], inplace=True)

pivot_df = df_long.pivot_table(
    index='Year_Num',
    columns=['Cohort', 'Age_Commence'],
    values='Value',
    aggfunc='first'
)

pivot_df.sort_index(axis=1, level=[0, 1], inplace=True)

pivot_df.index = [f'Year {i}' for i in pivot_df.index]

arr = pivot_df.to_numpy()
mask_int = np.isclose(arr % 1, 0, equal_nan=False)
formatted = np.where(np.isnan(arr), '', np.where(mask_int, arr.astype(int).astype(str), arr.astype(str)))

pivot_clean = pd.DataFrame(
    formatted,
    index=pivot_df.index,
    columns=pivot_df.columns
)

print(pivot_clean)
</code></pre>
<p>Output:</p>
<pre><code>Cohort        1             2
Age_Commence 18 19 20 21   18   19   20   21
Year 1        1  2  3  4  1.5  2.5  3.5  4.5
Year 2        1  2  3     1.5  2.5  3.5
Year 3        1  2        1.5  2.5
Year 4        1           1.5
</code></pre>
","0","Answer"
"79629579","79629556","<p>A possible solution:</p>
<pre><code>m = (df.groupby('group_id', group_keys=False)
    .apply(lambda x: 
        x['related_id'].isin(x.index[x['status'].eq('active')]), 
        include_groups=False))

df.loc[m, 'status'] = 'resolved'
</code></pre>
<p>This uses <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> combined with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a> to process each <code>group_id</code> separately. Inside each group, it checks whether the <code>related_id</code> of each row matches the index of any row where <code>status</code> is <code>active</code>, using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isin.html"" rel=""nofollow noreferrer""><code>isin</code></a> to return a boolean mask. This mask is assigned to <code>m</code>, which flags the rows that meet the condition. Finally, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>loc</code></a> is used to update the <code>status</code> of the matching rows to <code>resolved</code>.</p>
<p>Output:</p>
<pre><code>   id group_id    status  related_id
0   1        A   pending         NaN
1   2        A    active         NaN
2   3        A  resolved         1.0
3   4        B   pending         NaN
4   5        B    active         NaN
5   6        B  resolved         4.0
6   7        B  resolved         4.0
</code></pre>
","0","Answer"
"79629926","79629556","<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.apply.html"" rel=""nofollow noreferrer""><code>groupby.apply</code></a> is a slow operation, you can leverage other pandas operations for efficient processing.</p>
<p>Your conditions are a bit ambiguous. &quot;There is at least one other row within the same group_id where status is 'active' <strong>and the related_id matches the id of the current row</strong>.&quot; The statement in <strong>bold</strong> is not true in your example.</p>
<p>So, assuming two options, you can either use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a> or <a href=""https://pandas.pydata.org/docs/reference/api/pandas.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>.</p>
<p>If you want to identify rows that are &quot;pending&quot;, with at least one &quot;active&quot; in the group and the related_id is in the same group:</p>
<pre><code># is the row &quot;pending&quot;?
m1 = df['status'].eq('pending')

# is the related id in the same group?
m2 = df['related_id'].map(df.set_index('id')['group_id']).eq(df['group_id'])

# is there at least one &quot;active&quot; status in the group?
m3 = df['status'].eq('active').groupby(df['group_id']).transform('any')

df.loc[m1 &amp; m2 &amp; m3, 'status'] = 'resolved'
</code></pre>
<p>Output:</p>
<pre><code>   id group_id    status  related_id
0   1        A   pending         NaN
1   2        A    active         NaN
2   3        A  resolved         1.0
3   4        B   pending         NaN
4   5        B    active         NaN
5   6        B  resolved         4.0
6   7        B  resolved         4.0
</code></pre>
<p>Intermediates:</p>
<pre><code>   id group_id   status  related_id     m1    m2    m3  m1 &amp; m2 &amp; m3   updated
0   1        A  pending         NaN  False  True  True         False       NaN
1   2        A   active         NaN  False  True  True         False       NaN
2   3        A  pending         1.0   True  True  True          True  resolved
3   4        B  pending         NaN  False  True  True         False       NaN
4   5        B   active         NaN  False  True  True         False       NaN
5   6        B  pending         4.0   True  True  True          True  resolved
6   7        B  pending         4.0   True  True  True          True  resolved
</code></pre>
<p>If you want to match the value of the referenced  related_id, then perform a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>:</p>
<pre><code># is the status &quot;pending&quot;?
m1 = df['status'].eq('pending')

# for status pending rows, perform a lookup by id (merge)
# determine if the status is &quot;pending&quot;
m2 = (df.loc[m1, ['related_id', 'group_id']].reset_index()
        .merge(df[['id', 'group_id', 'status']],
               left_on=['related_id', 'group_id'], right_on=['id', 'group_id'])
        .set_index('index')['status'].eq('pending')
     )

# for rows matching the two conditions above, update the value
df.loc[m1 &amp; m2, 'status'] = 'resolved'
</code></pre>
<p>Output:</p>
<pre><code>   id group_id    status  related_id
0   1        A   pending         NaN
1   2        A    active         NaN
2   3        A  resolved         1.0
3   4        B   pending         NaN
4   5        B    active         NaN
5   6        B  resolved         4.0
6   7        B  resolved         4.0
</code></pre>
<p>Intermediates:</p>
<pre><code>   id group_id   status  related_id merged_status     m1    m2  m1 &amp; m2 updated row
0   1        A  pending         NaN           NaN   True   NaN    False         NaN
1   2        A   active         NaN           NaN  False   NaN    False         NaN
2   3        A  pending         1.0       pending   True  True     True    resolved
3   4        B  pending         NaN           NaN   True   NaN    False         NaN
4   5        B   active         NaN           NaN  False   NaN    False         NaN
5   6        B  pending         4.0       pending   True  True     True    resolved
6   7        B  pending         4.0       pending   True  True     True    resolved
</code></pre>
<p>If you meant that <strong>related_id</strong> should match the <strong>index</strong>, then modify the <code>merge</code> approach a bit:</p>
<pre><code>df['merged_status'] = (df.loc[m1, ['related_id', 'group_id']].reset_index()
        .merge(df[['id', 'group_id', 'status']],
               left_on=['related_id', 'group_id'], right_on=['id', 'group_id'])
        .set_index('index')['status'].eq('active')
     )

df['m1'] = m1
df['m2'] = m2
df['m1 &amp; m2'] = m1 &amp; m2

df.loc[m1&amp;m2, 'active'] = 'resolved'
</code></pre>
<p>Intermediates:</p>
<pre><code>   id group_id    status  related_id merged_status     m1    m2  m1 &amp; m2
0   1        A   pending         NaN           NaN  False  True    False
1   2        A    active         NaN           NaN  False  True    False
2   3        A  resolved         1.0         False   True  True     True
3   4        B   pending         NaN           NaN  False  True    False
4   5        B    active         NaN           NaN  False  True    False
5   6        B  resolved         4.0         False   True  True     True
6   7        B  resolved         4.0         False   True  True     True
</code></pre>
","1","Answer"
"79630449","79630359","<p>Before you print, run</p>
<pre><code>pd.set_option('display.width', 800)
</code></pre>
<p>or change 800 to whatever you want the width to be. The default is 80.</p>
<p>See the <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html"" rel=""nofollow noreferrer"">Pandas User Guide</a>.</p>
<p>Relevant excerpt for the <code>display.width</code> option:</p>
<pre class=""lang-none prettyprint-override""><code>display.width : int
    Width of the display in characters. In case python/IPython is running in
    a terminal this can be set to None and pandas will correctly auto-detect
    the width.
    Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a
    terminal and hence it is not possible to correctly detect the width.
    [default: 80] [currently: 80]
</code></pre>
","2","Answer"
"79630563","79629420","<p>You're getting this error: <code>AttributeError: 'Engine' object has no attribute 'cursor'</code> because starting with pandas 2.0, you can’t pass a SQLAlchemy <code>Engine</code> directly into <code>read_sql()</code> anymore. Pandas now expects a <code>Connection</code> object instead — the <code>Engine</code> itself doesn’t have <code>.cursor()</code>, but a connection created from it does.</p>
<p>You need to open a connection from your engine before <code>calling read_sql()</code>:</p>
<pre><code>with self.engine.connect() as connection:
    return self._pd.read_sql(query, connection, params=params, **kwargs)
</code></pre>
","0","Answer"
"79630739","79630693","<p>can be complete using <code>df.query()</code> Need to alter the filtering element of <code>filter_ls</code> .</p>
<p>This works</p>
<pre><code>filter_ls=['col1&gt;3', '(col1&lt;4) &amp; (col2&lt;8)', '(col2&gt;7) &amp; (col3&gt;14)']
df.query(filter_ls[2])
</code></pre>
","2","Answer"
"79630896","79625976","<p>I agree with TenukiPy's <a href=""https://stackoverflow.com/a/79628479/30085652"">answer</a>, but I'll add my preferred solution. You can just add another set up brackets, [[]], around your x matrix as follows:</p>
<p>(note that I change the variable name <code>x</code> to <code>X</code> to denote that it's a matrix/<code>pd.DataFrame</code> rather than a vector/<code>pd.Series</code>)</p>
<pre class=""lang-py prettyprint-override""><code>X = my_data2[[&quot;trestbps&quot;]]
y = my_data2[&quot;chol&quot;]

...

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)
</code></pre>
","1","Answer"
"79631164","79631026","<p>The thing is that <code>janitor.complete</code> and other Pandas methods uses the <code>_constructor_expand</code> to create a good old <code>DataFrame</code>.</p>
<p>If <code>_constructor_expand</code> is not defined in your DataFrame class, the pandas will go to user the default <code>pd.DataFrame</code> constructor.</p>
<hr />
<p>We need to add the <code>_constructor_expand</code> property to the <code>MyDataFrame</code> class and have it return that class.</p>
<pre class=""lang-py prettyprint-override""><code>    # for the constructor_expand method, since that is
    # the one used by janitor's complete method
    @property
    def _constructor_expand(self):
        return MyDataFrame
</code></pre>
","-1","Answer"
"79631460","79600924","<p>It is a bit tricky but with deeply (5!) nested loops I managed to get the data in your desired shape. Approach:</p>
<ul>
<li>Flatten each JSON object in the <code>col_json</code> column into a single dictionary with unique keys.</li>
<li>Use <code>pd.json_normalize</code> to handle nested structure <code>parameter</code>.</li>
<li>Concatenate the flattened JSON data with the original DataFrame.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Sample DataFrame as in your code
json_2_explode = ...
df = ...

# Flatten JSON and expand into columns
def flatten_json_list(json_list):
    flattened = {}
    for item in json_list:
        prefix = item['name']
        for key, value in item.items():
            if key == 'parameter':  # Handle nested parameter
                for i, param in enumerate(value):
                    for param_key, param_value in param.items():
                        flattened[f&quot;{prefix}_parameter.{i}.{param_key}&quot;] = param_value
            else:
                flattened[f&quot;{prefix}_{key}&quot;] = value
    return flattened

# Apply flattening to each row
flattened_data = df['col_json'].apply(flatten_json_list)
flattened_df   = pd.DataFrame(flattened_data.tolist())

# Concatenate with the original DataFrame
result = pd.concat([df.drop(columns=['col_json']), flattened_df], axis=1)

print(result)

# Save to CSV
result.to_csv('flattened_data.csv', index=False)
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>11,7,43,m,45,1038,356,Foo,54.1,s,78,103,356,Yoo,1123.1,Hz,21,43,3577,Baz
 9,3,43,m,45,1038,356,Foo,54.1,s,78,103,356,Yoo,1123.1,Hz,21,43,3577,Baz
23,1,43,m,45,1038,356,Foo,54.1,s,78,103,356,Yoo,1123.1,Hz,21,43,3577,Baz
1,12,43,m,45,1038,356,Foo,54.1,s,78,103,356,Yoo,1123.1,Hz,21,43,3577,Baz
</code></pre>
","1","Answer"
"79632048","79631997","<p><a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.html#:%7E:text=An%20Index%20instance%20can%20only%20contain%20hashable%20objects.%20An%20Index%20instance%20can%20not%20hold%20numpy%20float16%20dtype."" rel=""nofollow noreferrer"">&quot;An Index instance can only contain hashable objects&quot;</a>  - the validation is performed lazily. pandas performs hashability validation for index labels <strong>only at lookup time</strong> not at series creation time.</p>
<p><em>Pandas employs lazy initialization for its internal index-to-position mapping. The series constructor doesn't immediately validate that all index values are hashable; instead, it defers the creation of its internal hashtable until a lookup operation requires it.</em> - <a href=""https://github.com/pandas-dev/pandas/issues/60925"" rel=""nofollow noreferrer"">Existing issue</a></p>
<blockquote>
<p><strong>why?</strong><br />
Maybe this will be consistent with other Python libraries that prioritize performance by avoiding unnecessary checks during initialization.</p>
<p>python is about being '<a href=""https://realpython.com/duck-typing-python/"" rel=""nofollow noreferrer"">duck typed'</a>: if it walks like a duck and quacks like a duck, it's duck enough for our program.</p>
<p>objects are evaluated based on behavior rather than explicit type checking.</p>
</blockquote>
<p><a href=""https://github.com/pandas-dev/pandas/blob/main/pandas/core/indexes/base.py"" rel=""nofollow noreferrer"">Check this</a></p>
<pre><code>@property
def _engine(self):
    # Lazy initialization pattern
    if not hasattr(self, &quot;_engine_type&quot;):
        self._engine_type = self._get_engine_type()
    if not hasattr(self, &quot;_engine&quot;):
        self._engine = self._engine_type(self.values, self.dtype)
    return self._engine
</code></pre>
<p>You can implemnt a pre-procesing fun taht Convert any unhashable values to hashable equivalents</p>
<pre><code>import pandas as pd
def ensure_hashable_index(values):
    
    result = []
    for val in values:
        if isinstance(val, list):
            result.append(tuple(val))
        elif isinstance(val, dict):
            result.append(frozenset(val.items()))
        else:
            result.append(val)
    return result


series = pd.Series([1, 2, 3], index=ensure_hashable_index([['A'], 'B', 'C']))
</code></pre>
","1","Answer"
"79632096","79631713","<p>In short this happens during creation of a new index, you can circumvent the creation of a new index by passing the <code>keys</code> to <code>pd.concatenate</code> or using <code>ignore_index=True</code> and setting the index manually afterwards.</p>
<p>Internally does happen during <code>left_df.keys().union(right_df.keys(), sort=False)</code>. It's possibly a bug that <code>sort</code> is not forwarded to the lower levels here. Pandas will still <em>try</em> to sort them given that one level is monotonic increasing (here <code>[1, 2, 3]</code>) to ensure it really stays sorted.</p>
<p>The solution is to construct your new index outside:</p>
<pre class=""lang-py prettyprint-override""><code>new_index = pd.MultiIndex.from_tuples(np.concatenate([left_df.keys(), right_df.keys()]))
df = pd.concat([left_df, right_df], axis=1, sort=False, ignore_index=True).reindex(new_index, axis=1)
</code></pre>
<p>I guess you can also work with <code>keys</code> of <code>pd.concatenate</code> but I thought this way is a bit easier.</p>
","0","Answer"
"79632100","79632060","<p>Fyi known issue - <a href=""https://github.com/pandas-dev/pandas/issues/54981"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/issues/54981</a></p>
<p><strong>Work arround :</strong> use both forward and backward filling</p>
<pre><code>sub_df[f&quot;Δ {col}&quot;] = sub_df[col].ffill().bfill().pct_change(fill_method=None)
</code></pre>
<p>or else I belive this cn also be calculated manually for standard percentage change calculations</p>
<pre><code>s_ffilled = sub_df[col].ffill()
sub_df[f&quot;Δ {col}&quot;] = (s_ffilled / s_ffilled.shift(1) - 1)
</code></pre>
","1","Answer"
"79632124","79631997","<p>Pandas doesn't check if each element is hashable at the time of creation. But when you perform operations like <code>.loc[]</code> which rely hashable keys, it raises <code>TypeError</code>.</p>
<p>The flexibility exists because pandas prioritizes performance and avoids unwanted type-checking during index creation.</p>
<p><strong>Using unhashable index labels (like lists) is not recommended</strong>, as it breaks core pandas functionality. Users are expected to follow best practices (e.g., using tuples instead of lists for hierarchical indices).</p>
<p>This behavior is a side effect of pandas' internal design, not an intentional feature. Always use hashable types (e.g., strings, numbers, tuples) for index labels to ensure compatibility with pandas' operations.</p>
","1","Answer"
"79632865","79632701","<p>I'm not sure about .xls (since it is almost two decades out from Excel 2007) but you could write to a .csv and open that using Excel.</p>
","0","Answer"
"79632874","79632857","<p>Are you overthinking it? Can you just do global find-and-replace for the values you care about?</p>
<p>Given your sample data, this does something similar to what you're asking for. (Remember, you asked for it to work &quot;in place would be preferrable&quot; so this is destructive to the input file.)</p>
<pre class=""lang-py prettyprint-override""><code>replacements = {
    &quot;sometext_1_sometext&quot; : &quot;ONE&quot;,
    &quot;sometext_2_sometext&quot; : &quot;TWO&quot;,
    &quot;sometext_3_sometext&quot; : &quot;THREE&quot;
}

with open('Infrastructure.csv', 'r+') as file:
    data = file.read()
    for old, new in replacements.items():
        data = data.replace(old, new)
    file.seek(0)
    file.truncate()
    file.write(data)
</code></pre>
","1","Answer"
"79632910","79632701","<p>From the pandas documentation it does seem like they dropped the .xls writing (<a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html</a>), even though reading is still possible.</p>
<p>Building upon the previous answer, the <a href=""https://pypi.org/project/pyexcel/"" rel=""nofollow noreferrer"">pyexcel</a> library seems to support csv to xls conversion. You can find an example usage for xlsx here: <a href=""https://stackoverflow.com/a/26456641/21414975"">https://stackoverflow.com/a/26456641/21414975</a></p>
","0","Answer"
"79632949","79631672","<p>If I understand you correctly, for each sensor and timestamp, if values exist in both <code>df1</code> and <code>df2</code>, then use value from <code>df2</code>. Otherwise, use whatever value that is available. This is a perfect job for <code>combine_first</code>.</p>
<pre class=""lang-py prettyprint-override""><code># Merge on the indexes. df1 gets to keep its original column names but df2's
# columns are suffixed with &quot;_y&quot;. If you have sensors whose names end with &quot;_y&quot;,
# pick a different suffix.
merged = df1.merge(
    df2, how=&quot;outer&quot;, left_index=True, right_index=True, suffixes=(&quot;&quot;, &quot;_y&quot;)
)

# For those columns that end with &quot;_y&quot; (or your custom suffix), combine its
# value with the original column from df1
y_cols = merged.columns[merged.columns.str.endswith(&quot;_y&quot;)]
for y_col in y_cols:
    x_col = y_col.rstrip(&quot;_y&quot;)
    merged[x_col] = merged[y_col].combine_first(merged[x_col])

# Drop the &quot;_y&quot; columns
merged.drop(columns=y_cols, inplace=True)
</code></pre>
<p>This way you get to keep the columns order from <code>df1</code>. Any new columns from <code>df2</code> will be appended to the right.</p>
","0","Answer"
"79633350","79633258","<p>Not sure if there is a better solution, but, as mentioned in <a href=""https://stackoverflow.com/questions/79633258/#comment140448594_79633258"">furas's comment</a> you can use the HTML tag <code>&lt;b&gt;…&lt;/b&gt;</code> for the elements that should be bold-faced. You can achieve this, for example, by adding the following line:</p>
<pre class=""lang-py prettyprint-override""><code># Create your data dict (as before)
data = {
...
}

# Add HTML tag for boldface
data[&quot;lib_acte&quot;] = [(f&quot;&lt;b&gt;{el}&lt;/b&gt;&quot; if ft == &quot;bold&quot; else el) for el, ft in 
                    zip(data[&quot;lib_acte&quot;], data[&quot;textfont&quot;])]

# Create dataframe (as before)
df = pd.DataFrame(data)
</code></pre>
<p>The <a href=""https://plotly.com/chart-studio-help/adding-HTML-and-links-to-charts/"" rel=""nofollow noreferrer"">documentation</a> says:</p>
<blockquote>
<p>Chart Studio uses a subset of HTML tags to do things like newline (<code>&lt;br&gt;</code>), bold (<code>&lt;b&gt;&lt;/b&gt;</code>), italics (<code>&lt;i&gt;&lt;/i&gt;</code>), and hyperlinks (<code>&lt;a href=’…’&gt;&lt;/a&gt;</code>). Tags <code>&lt;em&gt;</code>, <code>&lt;sup&gt;</code>, and <code>&lt;sub&gt;</code> are also supported.</p>
</blockquote>
<p>Here, <em>Chart Studio</em> is the online service built on top of and provided by the makers of <code>plotly</code>. While not explicitly stated, I am quite sure that the same subset of HTML tags also applies to <code>plotly</code> (stand-alone) – I just tried successfully with <code>&lt;b&gt;</code>, <code>&lt;sup&gt;</code>, and <code>&lt;a href=…&gt;</code>.</p>
","2","Answer"
"79633357","79633258","<p>You can make all labels bold using <code>fig.update_traces(textfont_weight=&quot;bold&quot;)</code> prior to <code>fig.show()</code>. <code>plotly</code> does not support styling individual labels within a trace. However, you can split your dataset into normal and bold labels and plot them separately (Note that I had to set the circle scaling manually to keep it consistent in both plots:</p>
<pre class=""lang-py prettyprint-override""><code>[...]

df = pd.DataFrame(data)

normal_df = df[df[&quot;textfont&quot;] == &quot;normal&quot;]
bold_df   = df[df[&quot;textfont&quot;] == &quot;bold&quot;]

size_max = 20

fig = px.scatter(
    normal_df,
    x=&quot;x&quot;,
    y=&quot;y&quot;,
    color=&quot;color&quot;,
    size='circle_size',
    size_max=size_max,
    text=&quot;lib_acte&quot;,
    hover_name=&quot;lib_acte&quot;,
    color_discrete_map={&quot;red&quot;: &quot;red&quot;, &quot;green&quot;: &quot;green&quot;},
    title=&quot;chart&quot;
)

scale = size_max / bold_df[&quot;circle_size&quot;].max()

fig.add_scatter(
    x=bold_df[&quot;x&quot;],
    y=bold_df[&quot;y&quot;],
    mode=&quot;markers+text&quot;,
    marker=dict(size=bold_df[&quot;circle_size&quot;] * scale, color=bold_df[&quot;color&quot;], sizemode=&quot;diameter&quot;),
    text=bold_df[&quot;lib_acte&quot;],
    textposition='middle right',
    textfont=dict(size=14, color='black', family=&quot;Inter&quot;, weight=&quot;bold&quot;),
    hoverinfo=&quot;skip&quot;,
    showlegend=True,
    name=bold_df[&quot;color&quot;].iloc[0],
    xaxis=&quot;x&quot;,
    yaxis=&quot;y&quot;
)

[...]
</code></pre>
<p><a href=""https://i.sstatic.net/raFoqZkZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/raFoqZkZ.png"" alt=""bold labels for red"" /></a></p>
","1","Answer"
"79633364","79633258","<pre class=""lang-py prettyprint-override""><code>fig.update_traces(
    textfont={
        &quot;weight&quot;: &quot;bold&quot;
    }
)
</code></pre>
<p>Edit to add:</p>
<p>If you only want some markers to be bold (or if you generally want to apply different fonts to different subsets of markers), you can do the following:</p>
<ol>
<li><p>If there is some other field that distinguishes the subsets, you can use that:</p>
<pre><code>fig.update_traces(
    textfont={
        &quot;weight&quot;: &quot;bold&quot;
    },
    selector={
        &quot;marker_color&quot;:&quot;red&quot;
    }
)
</code></pre>
<p>I believe this does work with your existing data:<br />
<a href=""https://i.sstatic.net/oJlh6apA.png"" rel=""nofollow noreferrer"">resulting screenshot</a></p>
</li>
<li><p>If the criteria is not already baked into the figure in some way that you can use <code>selector</code>, you can manually create separate traces. I.E. you could do an initial <code>px.scatter()</code> followed by <code>fig.add_scatter</code>, and give each one different <code>name</code>.</p>
<pre><code>fig = px.scatter(..., name='trace1')
fig.add_scatter(..., name='trace2')

fig.update_traces(
    textfont={
        &quot;weight&quot;: &quot;bold&quot;
    },
    selector={
        &quot;name&quot;:&quot;trace1&quot;
    }
)
</code></pre>
</li>
</ol>
<p>I prefer option 1 (when possible) because it still adds all the markers in a single call to <code>px.scatter</code> without the use of <code>add_scatter</code>.</p>
<p>--end edit--</p>
<p>It looks like plotly.graph_objects.Scatter takes in a parameter &quot;textfont&quot; (<a href=""https://plotly.com/python-api-reference/generated/plotly.graph_objects.Scatter.html#plotly.graph_objects.scatter.Unselected.textfont"" rel=""nofollow noreferrer"">documentation</a>).</p>
<p>But plotly.express.scatter, on the other hand, does not (<a href=""https://plotly.com/python-api-reference/generated/plotly.express.scatter"" rel=""nofollow noreferrer"">documentation</a>).</p>
<p>So to change the font the easiest way I could find was with <code>fig.update_traces</code> as shown above. I am not 100% sure but I think the <code>fig</code> that is returned by <code>px.scatter</code> is a <code>graph_objects.Figure</code> object which is probably why the field <code>textfont</code> is available when you use <code>fig.update_traces</code>.</p>
<p>For what it's worth, I prefer plotly express usually because of its seamless use with pandas.</p>
","2","Answer"
"79633375","79633336","<p>Intervals can be <strong>arbitrary</strong> within an Index. You can chose an Interval of 1 day, then 3 days. Overlapping, with a gap, etc.</p>
<pre><code>dates = ['2025-01-01', '2025-01-02', '2025-01-05', '2025-01-10']
idx = pd.IntervalIndex.from_breaks(pd.DatetimeIndex(dates))
</code></pre>
<p>Output:</p>
<pre><code>IntervalIndex([(2025-01-01 00:00:00, 2025-01-02 00:00:00],
               (2025-01-02 00:00:00, 2025-01-05 00:00:00],
               (2025-01-05 00:00:00, 2025-01-10 00:00:00]],
              dtype='interval[datetime64[ns], right]')
</code></pre>
<p>Periods are specialized interval-like objects with a meaningful frequency. They are meant to have a <strong>fixed frequency</strong> within an Index (e.g. a day, a week). Once you define a type of Period (e.g. weeks ending on Sundays) a given timestamp necessarily falls between two well-defined bounds:</p>
<pre><code>dates = ['2025-01-01', '2025-01-02', '2025-01-05', '2025-01-10']

pd.PeriodIndex(dates, freq='D')
# PeriodIndex(['2025-01-01', '2025-01-02', '2025-01-05', '2025-01-10'], dtype='period[D]')

pd.PeriodIndex(dates, freq='W')
# PeriodIndex(['2024-12-30/2025-01-05', '2024-12-30/2025-01-05',
#              '2024-12-30/2025-01-05', '2025-01-06/2025-01-12'],
#             dtype='period[W-SUN]')
</code></pre>
<p>There is actually another type of Interval-like Index that might be useful if do not care about specific Timestamps, but rather a difference to an arbitrary starting point, a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.TimedeltaIndex.html"" rel=""nofollow noreferrer""><code>TimedeltaIndex</code></a>:</p>
<pre><code>idx = pd.DatetimeIndex(dates)
idx - idx[0]

# TimedeltaIndex(['0 days', '1 days', '4 days', '9 days'], dtype='timedelta64[ns]', freq=None)
</code></pre>
<p>In summary:</p>
<ul>
<li><code>IntervalIndex[datetime64]</code>: arbitrary intervals, not necessarily contiguous nor with a fixed frequency</li>
<li><code>PeriodIndex</code>: defined frequency intervals, with specific/regular start/end dates</li>
<li><code>TimedeltaIndex</code>: arbitrary time delta from an unspecifed starting point</li>
</ul>
","3","Answer"
"79633615","79633459","<p>From the logs, it may be inferred that <code>Column1</code> is being inferred as <code>null</code> in Parquet format file.</p>
<p>Please try updating type of <code>Column1</code> as:
<code>df['Column1'] = df['Column1'].astype('Int64')</code></p>
<pre><code>import pandas as pd
import clickhouse_driver
toy_df = pd.DataFrame({
    'Column1': [None, None, None, None],
    'Column2': ['A', 'B', 'C', 'D']
})
toy_df['Column1'] = toy_df['Column1'].astype('Int64')
toy_df.to_parquet('../data/parquet/toy_df.parquet')
client.execute(
'''
CREATE TABLE toy ENGINE = MergeTree() ORDER BY tuple()
AS SELECT * FROM file('toy_df.parquet', 'Parquet')
'''
)
client.execute('SELECT * FROM toy')
</code></pre>
<p>PS: You may also fill null values with
<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer"">fillna</a></p>
<pre><code>toy_df['Column1'] = toy_df['Column1'].astype('Int64')
toy_df = toy_df.fillna(0) # replace None with 0
toy_df.to_parquet('../data/parquet/toy_df.parquet')
</code></pre>
","1","Answer"
"79633958","79633910","<p>The column type appears to be text, try to convert to number to have the proper axis representation.</p>
<p>Edit:</p>
<p>To be sure, next time add usable test data instead of images.</p>
","1","Answer"
"79634241","79634157","<p>The problem is most likely an issue with the data types in your assignment statement on Line 5. To diagnose, check the data types:</p>
<pre><code>print(dummy.dtype)
print(model_data_utsset.loc[model_data_utsset['cmdb_ci'] == ci, feats].dtypes)
</code></pre>
<p>If the types are different, try converting <code>dummy</code> to a string before the assignment and see if that fixes it.</p>
","0","Answer"
"79634385","79632776","<p><a href=""https://github.com/spyder-ide/spyder/issues/24461"" rel=""nofollow noreferrer"">According to the Spyder maintainer</a>, this is a bug that they will now look at.</p>
","0","Answer"
"79634422","79632877","<p>There are two ideas I would suggest. Combined, they can reduce memory usage on this example from 11 GB to 0.4 GB.</p>
<h3>Initial benchmarking</h3>
<p>I ran your program on my 4-core system, (with <code>core_count = 4</code>) and measured the maximum RSS of the parent process. For me, the parent process uses 11 GB. This will be our baseline.</p>
<pre><code>import resource
def get_maxrss_bytes():
    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * 1024
if __name__ == &quot;__main__&quot;:
    print(f&quot;{get_maxrss_bytes() / 1e6:.2f} MB used at max RSS&quot;)
</code></pre>
<p>(Note: This benchmark ignores child process RSS. Most pages are likely to be shared anyway.)</p>
<h3>Eagerly consume map results</h3>
<p>The first thing I would try is to eagerly consume map results. Rather than buffering all map results, iterate over your map results, and save each result to disk.</p>
<p>Example:</p>
<pre><code>    with pq.ParquetWriter('processed.parquet', schema=schema, compression='GZIP') as writer:
        with ProcessPoolExecutor(core_count) as executor:
            # Disable chunking - we've already done this
            for result in executor.map(process, chunks, chunksize=1):
                transformed_batch = pa.RecordBatch.from_pandas(result, schema=schema)
                writer.write_batch(transformed_batch)
</code></pre>
<p>This reduces the memory usage to 3.4 GB.</p>
<p>Warning: The resulting parquet file is 67% larger. I suspect this is because the size of the row group is quite small, which wastes space because a fixed amount of metadata must be stored for each row group. Increasing <code>chunk_size</code> to 1e6 reduces this storage cost, at the cost of using more memory. Alternatively, you could add code to buffer up dataframes until you get a million rows worth of data to write to the parquet file.</p>
<p>Note: I found <a href=""https://stackoverflow.com/a/79628423/530160"">this answer</a> to be a very helpful reference when writing this.</p>
<h3>Use lazy map</h3>
<p>I added a bunch of print statements to the <code>process()</code> function and <code>for result in ...</code> loop, and I found that it would often run hundreds of calls to <code>process()</code> before running the main loop once. This can happen if one of the calls to <code>process()</code> runs slowly.</p>
<p>What you'd really like is to limit the number of results stored in memory at once. I found <a href=""https://stackoverflow.com/a/75622960/530160"">an answer which has some helpful code for this.</a> Copying their code into your program, I called it like this.</p>
<pre><code>    # Allocate transformation task
    with pq.ParquetWriter('processed.parquet', schema=schema, compression='GZIP') as writer:
        with ProcessPoolExecutor(core_count) as executor:
            # Prepare 2 extra jobs while waiting for jobs to complete
            n_outstanding_jobs = core_count + 2
            for result in lazy_executor_map(process, chunks, executor, n_concurrent=n_outstanding_jobs):
                transformed_batch = pa.RecordBatch.from_pandas(result, schema=schema)
                writer.write_batch(transformed_batch)
</code></pre>
<p>This reduces the memory use to 0.43 GB.</p>

","3","Answer"
"79634493","79631295","<p>I was a little unclear on exactly what you want to achieve, but if I understand correctly then this might help:</p>
<pre class=""lang-py prettyprint-override""><code># Because I see 11.7 in the &quot;Fact History&quot; column, this cutoff is used to decide if a month was partial or not
MINIMUM_MONTHS_WORKED_IN_FULL_YEAR = 11

df_worked_years = df2.query(&quot;not `Fact History`.isna()&quot;)
row_earliest_year = df_worked_years.query(&quot;`Year` == `Year`.min()&quot;).iloc[0]
earliest_year_is_partial_year = bool(row_earliest_year[&quot;Fact History&quot;] &lt; MINIMUM_MONTHS_WORKED_IN_FULL_YEAR)

earliest_year = row_earliest_year[&quot;Year&quot;]
year_cutoff = earliest_year - (0 if earliest_year_is_partial_year else 1)
all_years = df2[&quot;Year&quot;].tolist()
incomplete_years = [year for year in all_years if year &lt;= year_cutoff]

print(incomplete_years)
</code></pre>
<p>Basically:</p>
<ul>
<li><p>find the earliest year with some work done (by checking that there aren't NaN values in that row)</p>
</li>
<li><p>determine if the earliest year should be included or not (based on if 12 months were worked that year)</p>
</li>
<li><p>return all years earlier than the first worked year, including the first year if it was partial</p>
<ul>
<li><p>note that the list comprehension used to compare <code>year &lt;= year_cutoff</code> could have been done with a pandas query to get all relevant rows of the dataframe</p>
<ul>
<li><p><code>df_incomplete_years = df2.query(&quot;`Year` &lt;= cutoff_year&quot;)</code></p>
</li>
<li><p>then: convert from dataframe to a list with <code>df_incomplete_years[&quot;Year&quot;].tolist()</code></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
","0","Answer"
"79634845","79634831","<p>When you use <code>strftime()</code>, by default it converts the Timestamp to a string, but when pandas stores it back in the DataFrame, it tries to convert it back to a datetime-like format and  &quot;Z&quot; appears in your second example because you explicitly added it in the format string.</p>
<p>You can either Force the column to remain as strings,</p>
<p><code>df['TRAN_DATE'] = df['TRAN_DATE'].dt.strftime(&quot;%Y-%m-%d&quot;) + &quot;T17:00:00&quot;</code></p>
<p>or you can Properly construct new datetime objects with your desired time component by updating it to</p>
<p><code>df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'].dt.date.astype(str) + &quot;T17:00:00&quot;)</code></p>
","2","Answer"
"79636003","79635993","<pre><code>import pandas as pd
import numpy as np

# Original DataFrame
df = pd.DataFrame({
    'KeyID': [1,1,1,1,2,2,2,3,3,3],
    'number': ['a','a','c','d','a','b','c','a','b','c']
})

# Find KeyIDs that don't have a 'd'
missing_d = df.groupby('KeyID')['number'].apply(lambda x: 'd' not in x.values)
missing_keys = missing_d[missing_d].index

# Create rows with NaN for missing 'd' KeyIDs
new_rows = pd.DataFrame({'KeyID': missing_keys, 'number': np.nan})

# Append to the original DataFrame
df = pd.concat([df, new_rows], ignore_index=True)

# Optional: sort by KeyID for readability
df = df.sort_values(by='KeyID').reset_index(drop=True)

print(df)
</code></pre>
","4","Answer"
"79636013","79635993","<p>You can first identify the KeyID with a missing &quot;d&quot; (with set operations), then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a> the missing rows:</p>
<pre><code># identify KeyIDs with missing &quot;d&quot;
missing = set(df['KeyID']) - set(df.loc[df['number'].eq('d'), 'KeyID'])

# add missing rows, optionally reorder the rows
out = (pd.concat([df, pd.DataFrame({'KeyID': list(missing)})])
         .sort_values(by='KeyID', kind='stable', ignore_index=True)
      )
</code></pre>
<p>Alternatively, you could use an outer-<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a> with indicator and mask the new rows (<code>right_only</code>):</p>
<pre><code>out = (df.merge(pd.DataFrame({'KeyID': df['KeyID'].unique(), 'number': 'd'}),
                how='outer', indicator=True)
         .assign(number=lambda x: x['number'].mask(x.pop('_merge').eq('right_only')))
      )
</code></pre>
<p>Output:</p>
<pre><code>    KeyID number
0       1      a
1       1      a
2       1      c
3       1      d
4       2      a
5       2      b
6       2      c
7       2    NaN
8       3      a
9       3      b
10      3      c
11      3    NaN
</code></pre>
","3","Answer"
"79636014","79634020","<p>From the comments I understand you do not want to do use <code>n_apple, n_cherry, n_banana = row['apple'], row['cherry'], row['banana']</code> because in your setting it will create too long lines. However you can indeed use it when you put parentheses around it:</p>
<p>The final formatting is for you to decide</p>
<pre class=""lang-py prettyprint-override""><code>n_apple, n_cherry, n_banana = (
    row['apple'], 
    row['cherry'],
    row['banana'],
)
</code></pre>
<p>Same for the left hand sideside:</p>
<pre class=""lang-py prettyprint-override""><code>(
    n_apple, 
    n_cherry, 
    n_banana,
) = row['apple'], row['cherry'], row['banana']
</code></pre>
","0","Answer"
"79636452","79635981","<p>Your problem lies with nested arrays of variable length. A lot of binary formats don't really support that, and for a good reason.</p>
<p>there are several options here:</p>
<ol>
<li>if your data is small (say, if all arrays put together are less than 3-5K elements long), you can save it as json, which supports the nested arrays (but does not support numpy arrays, so we'll need to convert them.</li>
</ol>
<pre><code>import json
data = {
    'a': [np.array([1.,2.]), np.array([6.,7.,.6]), np.array([np.nan])],
    'b': np.array([99., 66., 88.])
}
# convert arrays to lists
data2 = {}
for k, vs in data.items():
    if isinstance(vs, list):
        vs2 = []
        for v in vs:
            vs2.append(list(v))
        data2[k] = vs2
    elif isinstance(vs, np.ndarray):
        data2[k] = vs.tolist()
    
print(json.dumps(data2)) # this is for display purposes
# with open('file.json', 'w') as f: json.dump(data2, f)
</code></pre>
<ol start=""2"">
<li>if the data is large, or you want binary format specifically, you'll need to convert lists of arrays into arrays.</li>
</ol>
<pre><code>import h5py
data = {
    'a': [np.array([1.,2.]), np.array([6.,7.,.6]), np.array([np.nan])],
    'b': np.array([99., 66., 88.])
}

#writing
with h5py.File('file.h5', 'w') as f:
    for k, vs in data.items():
        g = f.create_group(k)
        if isinstance(vs, np.ndarray):
            g.create_dataset('data', data=vs)
            g.create_dataset('len', data=np.array([0]))
        if isinstance(vs, list):
            lengths = []
            for v in vs:
                lengths.append(len(v))
            g.create_dataset('data', data=np.concatenate(vs))
            g.create_dataset('len', data=np.array(lengths))

# reading
data2 = {}
with h5py.File('file.h5', 'r') as f:
    for k in f.keys():
        g = f.get(k)
        lens = np.array(g.get('len'))
        arrs = np.array(g.get('data'))
        if (lens == np.array([0])).all():
            data2[k] = arrs
        else:
            vs = []
            data2[k] = vs
            beg = 0
            for l in lens:
                end = beg + l
                vs.append(arrs[beg:end])
                beg = end
print(data2)
</code></pre>
<p>EDIT:
3. if numpy arrays are really long (like images or smth) it may be counter-productive to concatenate them, and it may be better to dump them inside of hdf5 group with numbered keys like <code>'arr0', 'arr1', 'arr2'</code>, etc.</p>
","0","Answer"
"79636705","79636672","<p>The <a href=""https://github.com/pandas-dev/pandas/blob/0691c5cf90477d3503834d983f69350f250a6ff7/pandas/core/series.py#L734C1-L787C49"" rel=""nofollow noreferrer""><code>name</code></a> attribute of a <code>Series</code> object is a <a href=""https://docs.python.org/3/glossary.html#term-descriptor"" rel=""nofollow noreferrer"">descriptor</a> that operates on the string attribute <code>'_name'</code>. This <code>'_name'</code> is included in <code>_metadata</code> by default, provided <code>_metadata</code> hasn't been overridden by the user. As a result, the name is preserved when the object is modified. Therefore, when overriding <code>_metadata</code>, it's necessary to explicitly include <code>'_name'</code> alongside any custom properties:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd


class MySeries(pd.Series):

    _metadata = ['my_property', '_name']

    @property
    def _constructor(self):
        return MySeries


seq = MySeries([*'abc'], name='data')

print(f'{seq[0:1].name = }')
print(f'{seq[[0, 1]].name = }')

# Output:
# seq[0:1].name = 'data'
# seq[[0, 1]].name = 'data'
</code></pre>
","1","Answer"
"79636714","79635993","<p>Another possible solution:</p>
<pre><code>(df.groupby('KeyID', as_index=False)
 .agg(lambda x: x if 'd' in set(x) else pd.concat([x,pd.Series(np.nan)]))
 .explode('number'))
</code></pre>
<p>This uses <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> to group the dataframe by <code>KeyID</code>, and applies <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>agg</code></a> with a custom lambda function to check if the group contains <code>'d'</code>; if not, it appends a missing value (<code>np.nan</code>) using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>. The lambda ensures that each group either remains the same or gains an additional <code>NaN</code> row if <code>'d'</code> is missing. Finally, the list-like groups are flattened into rows using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer""><code>explode</code></a>.</p>
<p>Output:</p>
<pre><code>   KeyID number
0      1      a
0      1      a
0      1      c
0      1      d
1      2      a
1      2      b
1      2      c
1      2    NaN
2      3      a
2      3      b
2      3      c
2      3    NaN
</code></pre>
","2","Answer"
"79637365","79627670","<p>Spark's parquet with zstd has an open <a href=""https://github.com/apache/parquet-java/issues/2689"" rel=""nofollow noreferrer"">bug</a> and results only in level 3 compression. It is sad that it is still not fixed in Apache 4.0preview.</p>
<p>I verified the same with the following steps to reproduce the issue:</p>
<p>First, the difference in <code>meta</code> between the parquet files generated by pandas vs spark:</p>
<pre><code>    created_by: parquet-cpp-arrow version 19.0.1      created_by: parquet-mr version 1.13.1 (build db41...
    num_row_groups: 1                                 num_row_groups: 2
    format_version: 2.6                               format_version: 1.0
    serialized_size: 1905                             serialized_size: 1001
    max_definition_level: 1                           max_definition_level: 0
    compression: ZSTD (space_saved: 69%)              compression: ZSTD (space_saved: 84%)
    compression: ZSTD (space_saved: 94%)              compression: ZSTD (space_saved: 91%)
    compression: ZSTD (space_saved: 84%)              compression: ZSTD (space_saved: 79%)
</code></pre>
<p>The differences in file sizes:</p>
<pre><code>    &gt;&gt;&gt; import pandas as pd
    &gt;&gt;&gt; df = pd.read_parquet('spark.parquet')
    &gt;&gt;&gt; df.to_parquet('pandas.parquet', engine='pyarrow', compression=&quot;zstd&quot;, compression_level=3, index=False)
    &gt;&gt;&gt;
    $ ls -l *t
    -rw-rw-r-- 1 bss bss 235850125 May 22 14:23 pandas.parquet
    -r--r--r-- 1 bss bss 237780532 May 20 11:55 spark.parquet
    $
</code></pre>
","1","Answer"
"79637461","79519830","<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'a': [0, 0, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0]})

windowSize = 3

res = df['a'].replace(0, np.nan).shift(1).rolling( 
    window = windowSize, min_periods = 1
).apply( lambda x: x[~np.isnan(x)].iloc[-1] if (~np.isnan(x)).any() else 0 )

# Replace NaN values in 'res' with 0
res_filled = res.fillna(0)

df['res'] = res_filled 
print(df)
'''
   a  res
0   0  0.0
1   0  0.0
2   1  0.0
3  -1  1.0
4  -1 -1.0
5   0 -1.0
6   0 -1.0
7   0 -1.0
8   0  0.0
9   0  0.0
10 -1  0.0
11  0 -1.0
12  0 -1.0
13  1 -1.0
14  0  1.0
'''
</code></pre>
","0","Answer"
"79637679","79637569","<p>I had initially found a way to get my table, although not very elegant:</p>
<pre><code>df = df_initial.set_index(['State', 'Capital', 'Year', 'Type of residence']).groupby(&quot;Metric&quot;)
numerator = df.get_group(&quot;Squared meters&quot;).drop(columns=[&quot;Metric&quot;])
denominator = df.get_group(&quot;Nb rooms&quot;).drop(columns=[&quot;Metric&quot;])
ratio = numerator.div(denominator).reset_index()
</code></pre>
<p><strong>Edit</strong> : please see tdy’s comment above, where tdy gives a very nice and concise answer to my question. Thanks very much tdy!</p>
","1","Answer"
"79638086","79638042","<p>When you import from Excel, <code>pandas.read_excel()</code> may import mixed types (e.g., numbers + text or empty cells), causing the whole column to default to <code>object</code> (string). That’s why <code>.round()</code> silently does nothing — it only works on numeric types.</p>
<p>You can check it by this code:</p>
<pre class=""lang-py prettyprint-override""><code>print(df.dtypes) # Squared meters / room    object
</code></pre>
<p>To fix this you can convert it to numeric type first:</p>
<pre class=""lang-py prettyprint-override""><code>df[&quot;Squared meters / room&quot;] = pd.to_numeric(df[&quot;Squared meters / room&quot;], errors=&quot;coerce&quot;)
df[&quot;Squared meters / room&quot;] = df[&quot;Squared meters / room&quot;].round(1)
</code></pre>
<p>also if you have some <code>NaN</code> values there, make sure to fix them beforehand somehow.</p>
<pre class=""lang-py prettyprint-override""><code>df[&quot;Squared meters / room&quot;] = df[&quot;Squared meters / room&quot;].fillna(0)  # or .dropna()
</code></pre>
<p>Full code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Step 1: Read Excel normally
df = pd.read_excel(r&quot;C:\Users\...\df.xlsx&quot;, engine=&quot;openpyxl&quot;, sheet_name=&quot;df&quot;)

# Step 2: Replace the dash &quot;-&quot; with NaN
df[&quot;Squared meters / room&quot;] = df[&quot;Squared meters / room&quot;].replace(&quot;-&quot;, pd.NA)

# Step 3: Convert column to numeric (this will coerce any bad values to NaN)
df[&quot;Squared meters / room&quot;] = pd.to_numeric(df[&quot;Squared meters / room&quot;], errors=&quot;coerce&quot;)

# Step 4: Round the numeric values
df[&quot;Squared meters / room&quot;] = df[&quot;Squared meters / room&quot;].round(1)

# Optional: Fill or drop missing values
# df[&quot;Squared meters / room&quot;] = df[&quot;Squared meters / room&quot;].fillna(0)
# OR: df = df.dropna(subset=[&quot;Squared meters / room&quot;])

# Final check
print(df[&quot;Squared meters / room&quot;].dtype)  # Should be float64
print(df[&quot;Squared meters / room&quot;].head())
</code></pre>
","2","Answer"
"79638097","79637569","<p>You can use the <code>pivot_table()</code> function to reshape your long-format DataFrame into a wide format. This turns the values in the &quot;Metric&quot; column—like &quot;Nb rooms&quot; and &quot;Squared meters&quot;—into separate columns.<br />
Once that's done, you simply divide the &quot;Squared meters&quot; column by the &quot;Nb rooms&quot; column to calculate the &quot;Squared meters per room&quot; ratio. This gives you one row per observation, along with the new ratio as an additional column. If you'd rather keep your data in long format, that’s also possible with a different approach.</p>
<p>Sample code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Original data
df = pd.DataFrame({
    'State': ['New York', 'California', 'Illinois', 'Texas', 'Arizona', 'Pennsylvania', 'Texas', 'California',
              'New York', 'California', 'Illinois', 'Texas', 'Arizona', 'Pennsylvania', 'Texas', 'California'],
    'Capital': ['New York City', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego',
                'New York City', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego'],
    'Year': [2021, 2022, 2023, 2024, 2016, 2017, 2018, 2019,
             2021, 2022, 2023, 2024, 2016, 2017, 2018, 2019],
    'Type of residence': ['House', 'Apartment', 'House', 'Apartment', 'House', 'Apartment', 'House', 'Apartment',
                          'House', 'Apartment', 'House', 'Apartment', 'House', 'Apartment', 'House', 'Apartment'],
    'Metric': ['Nb rooms', 'Nb rooms', 'Nb rooms', 'Nb rooms', 'Nb rooms', 'Nb rooms', 'Nb rooms', 'Nb rooms',
               'Squared meters', 'Squared meters', 'Squared meters', 'Squared meters', 'Squared meters', 'Squared meters', 'Squared meters', 'Squared meters'],
    'Amount': ['3', '4', '5', '6', '3', '6', '5', '4', '48', '59', '100', '250', '276', '340', '405', '470']
})

df['Amount'] = pd.to_numeric(df['Amount'])

# Pivot the Metric values into columns
pivoted = df.pivot_table(
    index=['State', 'Capital', 'Year', 'Type of residence'],
    columns='Metric',
    values='Amount'
).reset_index()

# Compute the ratio
pivoted['Squared meters / room'] = pivoted['Squared meters'] / pivoted['Nb rooms']

# Optional: clean up columns
result = pivoted[['State', 'Capital', 'Year', 'Type of residence', 'Squared meters / room']]
result = result.rename(columns={'Year': 'Date'})

print(result)
</code></pre>
","1","Answer"
"79638120","79637569","<p>Another possible solution:</p>
<pre><code>(df.set_index(df.columns[:-1].to_list())
 .unstack('Metric').assign(
     result = lambda x: x['Amount']['Squared meters'].astype(float) / 
     x['Amount']['Nb rooms'].astype(float))
 .drop(columns='Amount').droplevel(1, axis=1).reset_index())
</code></pre>
<p>This solution uses a combination of <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a>, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html"" rel=""nofollow noreferrer""><code>unstack</code></a>, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>assign</code></a>, and other reshaping functions in <code>pandas</code>. First, the dataframe is indexed by all columns except <code>Amount</code> using <code>set_index</code>, then <code>unstack('Metric')</code> pivots the long-format data into wide format, creating one column per metric (e.g., 'Nb rooms' and 'Squared meters') under a hierarchical column index. The <code>assign</code> method is then used to compute the ratio by dividing the <code>Squared meters</code> amount by the <code>Nb rooms</code> amount (both cast to floats). The intermediate <code>Amount</code> column is dropped with <code>drop(columns='Amount')</code>, the second level of the multi-index columns is removed via <code>droplevel(1, axis=1)</code>, and finally, <code>reset_index()</code> flattens the index back into columns.</p>
<p>Output:</p>
<pre><code>          State        Capital  Year Type of residence      result
0       Arizona        Phoenix  2016             House   92.000000
1    California    Los Angeles  2022         Apartment   14.750000
2    California      San Diego  2019         Apartment  117.500000
3      Illinois        Chicago  2023             House   20.000000
4      New York  New York City  2021             House   16.000000
5  Pennsylvania   Philadelphia  2017         Apartment   56.666667
6         Texas        Houston  2024         Apartment   41.666667
7         Texas    San Antonio  2018             House   81.000000
</code></pre>
","2","Answer"
"79638501","79638487","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html"" rel=""nofollow noreferrer""><code>wide_to_long</code></a> for reshape and then aggregate <code>min</code> and <code>max</code>:</p>
<pre><code>out = (pd.wide_to_long(df.reset_index(),
                       stubnames=['start','end'],
                       i=['index', 'name'],
                       j=' ',
                       sep=&quot;_&quot;)
         .groupby('name')
         .agg(start=('start', 'min'), end=('end', 'max'))
         .reset_index())
print (out)
  name  start   end
0  ABC    100   400
1  DEF     50  1000
</code></pre>
<p>How it working:</p>
<pre><code>print(pd.wide_to_long(df.reset_index(),
                      stubnames=['start','end'],
                      i=['index', 'name'],
                      j=' ',
                      sep=&quot;_&quot;))

              start   end
index name               
0     ABC  1    100   200
           2    300   400
1     ABC  1    100   200
           2    300   400
2     ABC  1    150   250
           2    300   400
3     DEF  1    300   200
           2    300   900
4     DEF  1     50   200
           2    300  1000
</code></pre>
<p>Another idea is create minimal and maximal columns first and then aggregate:</p>
<pre><code>out = (df.assign(start = df.filter(like='start').min(axis=1),
                 end = df.filter(like='end').max(axis=1))
        .groupby('name')
        .agg(start=('start', 'min'), end=('end', 'max'))
        .reset_index())

print (out)
  name  start   end
0  ABC    100   400
1  DEF     50  1000
</code></pre>
<p>How it working:</p>
<pre><code>print(df.assign(start = df.filter(like='start').min(axis=1),
                end = df.filter(like='end').max(axis=1)))

   start_1  end_1  start_2  end_2 name  start   end
0      100    200      300    400  ABC    100   400
1      100    200      300    400  ABC    100   400
2      150    250      300    400  ABC    150   400
3      300    200      300    900  DEF    300   900
4       50    200      300   1000  DEF     50  1000
</code></pre>
","3","Answer"
"79638529","79638487","<p>Another possible solution:</p>
<pre><code>d = df.set_index('name')
(pd.concat([
    d.filter(like='start').groupby(level=0).min().min(axis=1).rename('start'),
    d.filter(like='end').groupby(level=0).max().max(axis=1).rename('end')], 
           axis=1, names=['start', 'end'])
 .reset_index())
</code></pre>
<p>This first sets the <code>name</code> column as the index (<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a>), then uses <a href=""https://pandas.pydata.org/docs/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a> to combine two computed dataframes: one for the minimum <code>start</code> values and another for the maximum <code>end</code> values. The <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.filter.html"" rel=""nofollow noreferrer""><code>filter</code></a> method isolates columns containing <code>start</code> or <code>end</code>, and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> (with <code>level=0</code>) groups by the index (<code>name</code>). The inner <code>min()</code> or <code>max()</code> aggregates within groups, while the outer <code>min(axis=1)</code> or <code>max(axis=1)</code> computes the row-wise minimum/maximum across columns. Finally, <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a> restores <code>name</code> as a column.</p>
<p>We can avoid executing <code>groupby</code> twice in the following way:</p>
<pre><code>d = df.set_index('name')
col_funcs = {'start': 'min', 'end': 'max'}
(pd.concat(
    [d.filter(like=k).agg(v, axis=1).rename(k) for k, v in col_funcs.items()],
    axis=1)
 .groupby(level=0).agg(col_funcs)
 .reset_index())
</code></pre>
<p>Output:</p>
<pre><code>  name  start   end
0  ABC    100   400
1  DEF     50  1000
</code></pre>
","4","Answer"
"79638904","79638655","<p>Function &quot;not supported by <code>DataFrame.apply()</code>&quot; just means that Pandas <strong>cannot guarantee</strong> the result you expect from your function if it mutates the DataFrame during <code>apply()</code>. If you get <em>wrong (or unexpected)</em> results, it can't be considered as a bug in Pandas. Neither that means that if you get <em>right</em> results <em>now</em>, that new versions of Pandas should save this behavior. There is no easy way for Pandas to &quot;allow&quot; or &quot;disallow&quot; you to use mutating user defined function for <code>apply()</code>, as long as it is valid Python code.</p>
<p>The recommended way to use <code>DataFrame.apply()</code> is to get some new DataFrame from the existing one, for ex.:</p>
<pre class=""lang-py prettyprint-override""><code>df_new = df.apply(func, axis='columns')
</code></pre>
<p>If you need to perform arithmetic operations on your dataframe or column values, you should prefer built-in Pandas or Numpy functions whenever possible. It also will be more computationally effective, because most of them are vectorized.</p>
<p>The simplest way to modify values of a column in Pandas DataFrame is (self-)assignment:</p>
<pre class=""lang-py prettyprint-override""><code>df.column = &lt;expression, maybe using df.column&gt;
</code></pre>
<p>So, instead of applying <code>remean_points()</code> function, you can use:</p>
<pre class=""lang-py prettyprint-override""><code>reviews.points -= review_points_mean
</code></pre>
<p>PS. Also, it seems that there're some inaccuracies in your question:</p>
<ul>
<li><p>there is no <code>reviews_points</code> DataFrame in Kaggle tutorial, but <code>reviews</code></p>
</li>
<li><p>there is no <code>remean_code</code> function, but <code>remean_points()</code></p>
</li>
<li><p><code>reviews.points</code> is of type <code>pandas.Series,</code> not DataFrame</p>
</li>
</ul>
","1","Answer"
"79638944","79638867","<p>... The problem is</p>
<p><code>&quot;index&quot;</code></p>
<p>so whenever you have a dataset with the column name &quot;index&quot; it will throw an IndexError...</p>
<p>You can just write it with a capital letter</p>
<p><code>&quot;Index&quot;</code></p>
<p>and everything is fine again... -.-</p>
","1","Answer"
"79639251","79631026","<p><kbd>pandas 2.2.3</kbd> <kbd>janitor 0.31.0</kbd></p>
<p>Janitor functions usually create new data structures either from scratch - using <code>pandas.DataFrame</code> as a blueprint - or by combining data with methods like <code>pandas.merge</code>. In the first case, we get a fresh DataFrame with the default <code>_metadata</code>. In the second, the returned object may not retain the original type, nor is its metadata necessarily preserved (see related discussions in the <a href=""https://github.com/pandas-dev/pandas/issues?q=is%3Aissue%20state%3Aopen%20_metadata"" rel=""nofollow noreferrer"">pandas-dev GitHub repository</a>).</p>
<p>To ensure that metadata is maintained, we can define a custom piping function. For example:</p>
<pre class=""lang-py prettyprint-override""><code>@pf.register_dataframe_method
def pipe_meta(data: pd.DataFrame, func: callable, *args, **kwargs):
    obj = func(data, *args, **kwargs)
    if isinstance(obj, pd.DataFrame):
        obj = data.__class__(obj)
        for meta in data._metadata:
            setattr(obj, meta, getattr(data, meta, None))
    return obj
</code></pre>
<p>Then, we can complete the initial task like this:</p>
<pre><code>df2 = df.regvar().pipe_meta(janitor.complete, index, &quot;Taxon&quot;, sort=True).printvar()
</code></pre>
","1","Answer"
"79639373","79639319","<p>A possible solution:</p>
<pre><code>def fill(df):
    # infer_objects because the first elem of A may be NA
    s = df['A'].infer_objects(copy=False).ffill()
    idx = (s.eq(df['B']) &amp; df['B'].eq(df['C'])).idxmax()
    df.loc[:idx, 'A'] = s[:idx]
    return df

cond = df['A'].notna().cumsum()
df.groupby(cond, as_index=False).apply(fill).reset_index(level=0, drop=True)
</code></pre>
<p>It first constructs a grouping key with <code>cond = df['A'].notna().cumsum()</code>, where <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.notna.html"" rel=""nofollow noreferrer""><code>notna</code></a> marks non-missing values in <code>A</code>, and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>cumsum</code></a> creates a unique group label for each contiguous block of rows following a non-null value. Then, the dataframe is grouped with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> using this key and processed by the <code>fill</code> function. Inside <code>fill</code>, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ffill.html"" rel=""nofollow noreferrer""><code>ffill</code></a> forward-fills <code>A</code>, and the index where <code>A</code>, <code>B</code>, and <code>C</code> first become equal is found via boolean comparisons and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.idxmax.html"" rel=""nofollow noreferrer""><code>idxmax</code></a>. The column <code>A</code> is then updated up to that point within the group. Finally, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index(level=0, drop=True)</code></a> removes the added group index.</p>
<p>Output:</p>
<pre><code>                        A     B     C
2025-05-22 00:00:00  sell   buy  sell
2025-05-22 00:15:00  sell   buy  sell
2025-05-22 00:30:00  sell  sell  sell
2025-05-22 00:45:00   NaN   buy  sell
2025-05-22 01:00:00   NaN   buy   buy
2025-05-22 01:15:00   buy   NaN  sell
2025-05-22 01:30:00   buy   buy  sell
2025-05-22 01:45:00   buy   buy   buy
2025-05-22 02:00:00   NaN   buy   buy
2025-05-22 02:15:00   NaN   NaN   NaN
</code></pre>
","2","Answer"
"79639421","79639319","<p>You can achieve this by forward-filling column <code>&quot;A&quot;</code> <strong>row by row</strong>, but <strong>only as long as all three columns match after the fill</strong>. Once they don't match, you leave <code>&quot;A&quot;</code> as <code>NaN</code>.</p>
<pre><code>import pandas as pd
import numpy as np

# Original DataFrame
df = pd.DataFrame({
    &quot;A&quot;: [&quot;sell&quot;, np.nan, np.nan, np.nan, np.nan, &quot;buy&quot;, np.nan, np.nan, np.nan, np.nan],
    &quot;B&quot;: [&quot;buy&quot;, &quot;buy&quot;, &quot;sell&quot;, &quot;buy&quot;, &quot;buy&quot;, np.nan, &quot;buy&quot;, &quot;buy&quot;, &quot;buy&quot;, np.nan],
    &quot;C&quot;: [&quot;sell&quot;, &quot;sell&quot;, &quot;sell&quot;, &quot;sell&quot;, &quot;buy&quot;, &quot;sell&quot;, &quot;sell&quot;, &quot;buy&quot;, &quot;buy&quot;, np.nan]
}, index=pd.date_range(&quot;2025-05-22&quot;, periods=10, freq=&quot;15min&quot;))

# Copy the DataFrame to avoid modifying the original directly
df_result = df.copy()

# Forward-fill manually with logic
for i in range(1, len(df)):
    if pd.isna(df_result.iloc[i][&quot;A&quot;]):
        # Temporarily forward-fill 'A'
        df_result.at[df_result.index[i], &quot;A&quot;] = df_result.at[df_result.index[i - 1], &quot;A&quot;]
        
        # Check if all three columns are equal
        row = df_result.iloc[i]
        if not (row[&quot;A&quot;] == row[&quot;B&quot;] == row[&quot;C&quot;]):
            # If not equal, revert the fill
            df_result.at[df_result.index[i], &quot;A&quot;] = np.nan

print(df_result)
</code></pre>
","0","Answer"
"79639642","79623688","<p>What you have is pretty close to the best you can get, simply swap out each of the type hints for a <code>pandas.core.series.Series</code> to get the correct type hints. Here is an example implementation modeled after your example.</p>
<pre><code>import pandas as pd
import pandas.core.series
import io


class TypedDataFrame(pd.DataFrame):
    day: pd.core.series.Series
    country: pd.core.series.Series
    weather: pd.core.series.Series


def _main():
    df: TypedDataFrame = TypedDataFrame(pd.read_csv(io.StringIO(
        'day,country,weather\n'
        '1,&quot;CAN&quot;,&quot;sunny&quot;\n'
        '2,&quot;GBR&quot;,&quot;cloudy&quot;\n'
        '4,&quot;IND&quot;,&quot;rain&quot;'
    )))

    print(type(df.weather))
    print(df.country[0])


if __name__ == '__main__':
    _main()
</code></pre>
<p>And here are what the type hints looks like in Pycharm at least, I would expect VSCode or others to be similar:</p>
<p><a href=""https://i.sstatic.net/ykdFdvu0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ykdFdvu0.png"" alt=""Showing type hints in Pycharm for field"" /></a></p>
<p><a href=""https://i.sstatic.net/xD09XziI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xD09XziI.png"" alt=""Showing type hints in Pycharm for field attributes"" /></a></p>
","1","Answer"
"79639923","79639750","<p>You'll need to use a list comprehension:</p>
<pre><code>def cashflow_series(ch1=1,ch2=2):
    return {0:ch1,0.5:ch2,1:7,2:8,3:9}

df.assign(cashflows=lambda x: [cashflow_series(ch1=x.loc[i, 'Characteristic1'],
                                               ch2=x.loc[i, 'Characteristic3'])
                               for i in x.index])
</code></pre>
<p>Or with parameter unpacking:</p>
<pre><code>df.assign(cashflows=lambda x: [cashflow_series(*params)
                               for params in
                               df[['Characteristic1', 'Characteristic2']].values])
</code></pre>
<p>Output:</p>
<pre><code>           stockprice  Characteristic1  Characteristic2  Characteristic3                         cashflows
Company A         100                1                5                1  {0: 1, 0.5: 1, 1: 7, 2: 8, 3: 9}
Company B         103                3                7                4  {0: 3, 0.5: 4, 1: 7, 2: 8, 3: 9}
Company C         240                3                1                6  {0: 3, 0.5: 6, 1: 7, 2: 8, 3: 9}
</code></pre>
","4","Answer"
"79639963","79639167","<p>Not very clear what exactly you are expecting as an output. However I can give a bit of guidence, you will need to work out the other bits.</p>
<p>The data can be obtained through the api. It is far more robust and efficient as opposed to using Selenium. So here's an example from the url you provided.</p>
<p><strong>Code:</strong></p>
<pre><code>import requests
import pandas as pd

apiUrl = 'https://api.racingtv.com/racing/iq-results/2025-05-11/leopardstown/1310/sectional?'
headers = {
'accept':'application/json',
'authorization':'',
'content-type':'application/json',
'referer':'https://www.racingtv.com/',
'sec-ch-ua':'&quot;Chromium&quot;;v=&quot;136&quot;, &quot;Google Chrome&quot;;v=&quot;136&quot;, &quot;Not.A/Brand&quot;;v=&quot;99&quot;',
'sec-ch-ua-mobile':'?0',
'sec-ch-ua-platform':'&quot;Windows&quot;',
'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
'x-requested-with':'racingtv-web/5.3.0'
    }

jsonData = requests.get(apiUrl, headers=headers).json()
data = jsonData['race']

df = pd.json_normalize(
    data,
    record_path=['runners', 'sectionals'],
    meta=[
        'scheduled_start_time',
        'time_index_score',
        'meeting_average_time_index',
        'win_time_vs_par',
        ['runners', 'id'],
        ['runners', 'finish_position'],
        ['runners', 'horse_name'],
        ['runners', 'finishing_time']
    ],
    errors='ignore'  # in case some meta paths don’t exist for all entries
)
</code></pre>
<p><strong>Output: first 10 rows</strong></p>
<pre><code>print(df.head(10).to_string())
   sector_number sector_name  sector_time  time_behind_leader  sector_position sector_pace_category  sector_time_percentile  cumulative_sector_time formatted_cumulative_sector_time       scheduled_start_time time_index_score meeting_average_time_index win_time_vs_par runners.id runners.finish_position runners.horse_name runners.finishing_time
0              0          1f        19.47                0.58                7            very_slow                     NaN                   19.47                           19.47s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765416                       1        Zodiac Bear                 23.39s
1              1          2f        12.84                0.96                7            very_slow                    0.55                   32.30                            32.3s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765416                       1        Zodiac Bear                 23.39s
2              2          3f        12.21                0.97                7                  par                    0.73                   44.52                           44.52s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765416                       1        Zodiac Bear                 23.39s
3              3          4f        11.88                0.71                6                 fast                    0.83                   56.40                            56.4s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765416                       1        Zodiac Bear                 23.39s
4              4          5f        11.54                0.43                6                 fast                    0.93                   67.94                      1min, 7.94s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765416                       1        Zodiac Bear                 23.39s
5              5          6f        11.30                0.00                1            very_fast                    1.00                   79.24                     1min, 19.24s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765416                       1        Zodiac Bear                 23.39s
6              6          7f        12.09                0.00                1            very_fast                    0.77                   91.33                     1min, 31.33s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765416                       1        Zodiac Bear                 23.39s
7              0          1f        19.06                0.17                2                 slow                     NaN                   19.06                           19.06s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765380                       2           Esherann                 24.54s
8              1          2f        12.67                0.40                3            very_slow                    0.59                   31.74                           31.74s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765380                       2           Esherann                 24.54s
9              2          3f        12.34                0.53                3                  par                    0.69                   44.07                           44.07s  2025-05-11T14:10:00+01:00              6.8                        5.9       -1.195511    7765380                       2           Esherann                 24.54s
</code></pre>
","1","Answer"
"79640167","79639750","<p>Your code is working fine. Just need simple modification.</p>
<pre><code>def cashflow_series(ch1=1, ch2=2):
    return {0: ch1, 0.5: ch2, 1: 7, 2: 8, 3: 9}


df = df.assign(
    cashflows=lambda x: x.apply(
        lambda row: cashflow_series(
            ch1=row[&quot;Characteristic1&quot;], ch2=row[&quot;Characteristic3&quot;]
        ),
        axis=1,
    )
)

print(df.to_string())
</code></pre>
","1","Answer"
"79640466","79640408","<pre><code>import pandas as pd
import re
from collections import Counter


word_list = ['apple', 'banana', 'dog', 'machine learning', 'data science', 'big data']

# Simulated sample text data (5k * 5 = 25,000 rows)
texts = 5000 * [
    'I love apple and banana',
    'The dog runs fast',
    'machine learning is great',
    'apple banana dog',
    'data science and big data are related'
]


df = pd.DataFrame({'idx': range(len(texts)), 'text': texts})


escaped_terms = [re.escape(term) for term in sorted(word_list, key=lambda x: -len(x))]
pattern = re.compile(r'\b(?:' + '|'.join(escaped_terms) + r')\b', flags=re.IGNORECASE)


def count_words(text):
    if not isinstance(text, str):
        return {term: 0 for term in word_list}
    matches = pattern.findall(text)
    counts = Counter(m.lower() for m in matches)
    return {term: counts.get(term.lower(), 0) for term in word_list}


results = df['text'].apply(lambda x: count_words(x))


print(results)
</code></pre>
","0","Answer"
"79640645","79640408","<blockquote>
<p>need to make some very fast Python function</p>
</blockquote>
<p>This is not clear requirement, use units of time, e.g. you have x ms to deal with each row.</p>
<blockquote>
<p>get counts of words as dicts for each row of data.</p>
</blockquote>
<p>I suggest trying <a href=""https://pypi.org/project/flashtext/"" rel=""nofollow noreferrer"">flashtext</a> which does exploit algorithm of same name. Inventor of said algorithm claims it is faster than using regex. You might create for example following function to get desired output</p>
<pre><code>import collections
from flashtext import KeywordProcessor
def get_word_counts(text, kwproc):
    keywords_found = kwproc.extract_keywords(text)
    keywords_count = collections.Counter(dict.fromkeys(kwproc.get_all_keywords(), 0))
    keywords_count.update(keywords_found)
    return keywords_count
some_words = ['able', 'baker', 'charlie', 'dog', 'sky shark']
some_text = 'Why sky shark never replaced able dog? Was dog just superior?'
keyword_processor = KeywordProcessor()
keyword_processor.add_keywords_from_list(some_words)
print(dict(get_word_counts(some_text, keyword_processor)))
</code></pre>
<p>gives output</p>
<pre><code>{'able': 1, 'baker': 0, 'charlie': 0, 'dog': 2, 'sky shark': 1}
</code></pre>
<p>Explanation: <code>keywords_found</code> is list holding extracted words, I create <code>collections.Counter</code> which initially have keys being all known words and values being zeros, then update to get actual counts. Keep in mind this function returns <code>collections.Counter</code> rather than <code>dict</code>, nonetheless you should be able to use it like dict, because it is subclass of dict and LISKOV, however if you need to have just <code>dict</code> instance AT ANY PRICE then it is possible to convert it to one as show above.</p>
","1","Answer"
"79641621","79641353","<p>You have to loop over fleet no or date depending upon the kind of graph you want.</p>
<p>Note - I have renamed 'date' to 'month', 'fleet no' to 'fleet_no' and 'kilometres' to 'kms'.</p>
<p><strong>Step 1</strong> - Ensure all combinations of fleet no and date are present in the dataset. This step can be skipped if you know for sure all combinations are present.</p>
<pre><code>fleets = sorted(list(df['fleet no'].unique()))
months = sorted(list(df['date'].unique()))

# can skip this if you are sure all combinations are present in the dataset
new_df = {'month': [], 'fleet_no': [], 'kms': []}
for fleet in fleets:
    for month in months:
        new_df['month'].append(month)
        new_df['fleet_no'].append(fleet)
        try:
            kms = df[(df['date']==month)&amp;(df['fleet no']==fleet)].get('kilometres').iloc[0]
        except IndexError:
            kms = 0
        new_df['kms'].append(kms)
new_df = pd.DataFrame(new_df)

</code></pre>
<p><strong>Step 2</strong> - Plotting
Now there are 2 options here. If you want to check month by month variation of the given fleet, use option A. If you want to check how much each fleet did in a month, use option B. Both codes are similar, only month and fleet_no are interchanged.</p>
<p><em>Option A</em></p>
<pre><code>bar_width = 0.4
gap = 1.2
x = [i*bar_width*(len(months)) + i*gap for i in range(len(fleets))]

fig, ax = plt.subplots(figsize=(13,5))

for i, month in enumerate(months):
    subset = new_df[new_df['month'] == month]
    curr_x = [j+i*bar_width for j in x]
    ax.bar(curr_x, subset['kms'], width=bar_width, label=month)
ax.set_xticks([i+bar_width*len(months)/2 for i in x])
ax.set_xticklabels(fleets)
ax.legend()

plt.show()
</code></pre>
<p><em>Option B</em></p>
<pre><code>bar_width = 0.4
gap = 1.2
x = [i*bar_width*(len(fleets)) + i*gap for i in range(len(months))]

fig, ax = plt.subplots(figsize=(13,5))

for i, fleet in enumerate(fleets):
    subset = new_df[new_df['fleet_no'] == fleet]
    curr_x = [j+i*bar_width for j in x]
    ax.bar(curr_x, subset['kms'], width=bar_width, label=fleet)
ax.set_xticks([i+bar_width*len(fleets)/2 for i in x])
ax.set_xticklabels(months)
ax.legend()

plt.show()
</code></pre>
","1","Answer"
"79641887","79641847","<p>You can try replace <code>or or or</code> with <code>in</code> expression.</p>
<pre><code>WHERE datetime in (datetime_row1_df, datetime_row2_df, ..., datetime_row3500_df)
</code></pre>
<p>You can try <code>df.to_sql(&quot;my_table_tmp&quot;)</code> to insert all data into temporary table and then issue sql query.</p>
<pre class=""lang-sql prettyprint-override""><code>insert into my_table 
select * from my_table_tmp 
where my_table_tmp.id not in (select id from my_table)
</code></pre>
<p>Or even better use primary key constrain to filter out existing records</p>
<pre class=""lang-sql prettyprint-override""><code>insert into my_table 
select * from my_table_tmp 
on conflict(id) do nothing
</code></pre>
","-1","Answer"
"79642048","79641847","<p>Bulk <code>Insert</code> for a large dataset is not recommended. You should use <code>COPY FROM</code> to upload your dataset to postgres instantaneously.</p>
<pre><code>COPY my_table FROM '/app/records.csv' csv header;
</code></pre>
<p>Data validation and data sanitization should be done on the dataframe and the column names must match with that in postgres.</p>
<h2><a href=""https://www.psycopg.org/psycopg3/docs/basic/copy.html#writing-data-row-by-row"" rel=""nofollow noreferrer"">Writing data row-by-row</a></h2>
<blockquote>
<p>Using a copy operation you can load data into the database from any Python iterable (a list of tuples, or any iterable of sequences): the Python values are adapted as they would be in normal querying. To perform such operation use a <code>COPY ... FROM STDIN</code> with <a href=""https://www.psycopg.org/psycopg3/docs/api/cursors.html#psycopg.Cursor.copy"" rel=""nofollow noreferrer"" title=""psycopg.Cursor.copy""><code>Cursor.copy()</code></a> and use <a href=""https://www.psycopg.org/psycopg3/docs/api/copy.html#psycopg.Copy.write_row"" rel=""nofollow noreferrer"" title=""psycopg.Copy.write_row""><code>write_row()</code></a> on the resulting object in a <code>with</code> block. On exiting the block the operation will be concluded:</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>records = [(10, 20, &quot;hello&quot;), (40, None, &quot;world&quot;)]

with cursor.copy(&quot;COPY sample (col1, col2, col3) FROM STDIN&quot;) as copy:
    for record in records:
        copy.write_row(record)
</code></pre>
<blockquote>
<p>If an exception is raised inside the block, the operation is interrupted and the records inserted so far are discarded.</p>
</blockquote>
<p>This is another way of using <code>COPY FROM</code> when you have a python iterable instead of a pandas dataframe. Note that it is not recommended to use for loop on a large dataframe as that will also take a lot of processing time.</p>
<hr />
<p>If your dataset has rows that conflict with some of the existing rows in the table, then you have to copy the dataset to a temporary table first. Then you do a bulk insert operation from the temporary table to the main table with <a href=""https://www.postgresql.org/docs/current/sql-insert.html#SQL-ON-CONFLICT"" rel=""nofollow noreferrer""><code>ON CONFLICT DO UPDATE clause</code></a> to overwrite the conflicting rows.</p>
<p><a href=""https://stackoverflow.com/users/4022109/frank-liao"">Frank Liao</a><a href=""https://stackoverflow.com/a/48020691/6538535""> </a>'s answer <a href=""https://stackoverflow.com/a/48020691/6538535"">posted</a> the exact query that you need.</p>
<blockquote>
<pre><code>sql = &quot;&quot;&quot;
CREATE TABLE temp_h (
    time ,
    name,
    description
);
COPY temp_h FROM STDIN With CSV;

INSERT INTO table_a(time, name, description)
SELECT *
FROM temp_h ON conflict (time) 
DO update set name=EXCLUDED.name, description=EXCLUDED.description;

DROP TABLE temp_h;
&quot;&quot;&quot;

</code></pre>
</blockquote>
<p>If the temporary table has no conflicting rows with the main table, then you can also <a href=""https://fle.github.io/temporarily-disable-all-indexes-of-a-postgresql-table.html"" rel=""nofollow noreferrer"">temporarily disable all indexes of the main table</a> for the insert operation. This is useful for the future operations when your dataset is already clean and no rows in the temporary table conflict with the rows in the main table.</p>
<blockquote>
<p>When you run a large query (insert/update) on a huge table with several indexes, these indexes can seriously slow the query execution.</p>
<p>With Postgresql it can be very faster to disable the indexes before runing the query and reindex all the table afterwards.</p>
</blockquote>
<p>As answered by <a href=""https://dba.stackexchange.com/users/21011/milovan-zogovic"">Milovan Zogovic</a> in this <a href=""https://dba.stackexchange.com/a/55575"">answer</a>:</p>
<blockquote>
<p>The problem was with indexes. The <code>history</code> table had 160M indexed rows. By running either <code>COPY FROM</code> or <code>INSERT INTO .. SELECT</code> it was taking a lot of time not to insert rows, but to update indexes. When i disabled indexes, it imported 3M rows in 10 seconds. Now i need to find faster way of reindexing the big table.</p>
</blockquote>
<p>But you cannot use <code>INSERT ON CONFLICT</code> here as it requires indexing.</p>
","1","Answer"
"79642229","79638487","<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({
    'start_1': [100, 100, 150, 300, 50],
    'end_1': [200, 200, 250, 200, 200],
    'start_2': [300, 300, 300, 300, 300],
    'end_2': [400, 400, 400, 900, 1000],
    'name': ['ABC', 'ABC', 'ABC', 'DEF', 'DEF']
})
'''
   start_1  end_1  start_2  end_2 name
0      100    200      300    400  ABC
1      100    200      300    400  ABC
2      150    250      300    400  ABC
3      300    200      300    900  DEF
4       50    200      300   1000  DEF
'''
res = df.groupby('name').agg(
    start_min = ('start_1', lambda x: np.minimum(x.values, df.loc[x.index, 'start_2'].values).min()),
    end_max = ('end_1',     lambda x: np.maximum(x.values, df.loc[x.index, 'end_2'].values).max())
).reset_index()

'''
  name  start_min  end_max
0  ABC        100      400
1  DEF         50     1000
'''
</code></pre>
","1","Answer"
"79642723","79641325","<pre class=""lang-py prettyprint-override""><code># Fix all the timestamp columns
from pandas.api.types import is_datetime64_any_dtype as is_datetime
for column in Data_df.columns:
    if is_datetime(Data_df[column]):
        Data_df[column] = Data_df[column].astype('int64').astype('datetime64[ns]')
</code></pre>
<p>I took Mark's answer and combined it with <a href=""https://stackoverflow.com/a/57187654/1754273"">this answer</a> to come up with the above code that seems to work and be quite performant.</p>
","0","Answer"
"79642764","79642757","<p>Call <code>read_csv</code> with <code>dtype=&quot;string&quot;</code> and <code>parse_dates=['send_date']</code>.</p>
<p>Code</p>
<pre><code>import pandas

df = pandas.read_csv(&quot;test.csv&quot;, dtype=&quot;string&quot;, parse_dates=['send_date'])

print(df.dtypes)
# name           string[python]
# zip_code       string[python]
# send_date      datetime64[ns]
# is_customer    string[python]
# dtype: object

print(df)
#        name zip_code  send_date is_customer
# 0  Madeline    04321 2025-04-13        true
# 1      Theo    32255 2025-04-08        true
# 2    Granny    84564 2025-04-15       false
</code></pre>
<p>Input file (test.csv)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>zip_code</th>
<th>send_date</th>
<th>is_customer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Madeline</td>
<td>04321</td>
<td>2025-04-13</td>
<td>true</td>
</tr>
<tr>
<td>Theo</td>
<td>32255</td>
<td>2025-04-08</td>
<td>true</td>
</tr>
<tr>
<td>Granny</td>
<td>84564</td>
<td>2025-04-15</td>
<td>false</td>
</tr>
</tbody>
</table></div>
","5","Answer"
"79643652","79627856","<p>Your requirement is a bit Odd. Please go through the solution to understand.</p>
<pre><code>import pandas as pd
import numpy as np
data = {
    'error': [2.595795, 2.568556, 2.562618, 2.538956, 2.520247,
              2.498345, 2.474890, 2.467736, 2.471115, 2.466424,
              2.495388, 2.520301, 2.604358, 2.553243, 2.490774,
              2.452384, 2.434171, 2.404764, 2.388775, 2.384337],
    'inst': [204267, 204268, 204269, 204271, 204272,
             204273, 204274, 204275, 204276, 204280,
             204284, 204285, 204291, 204299, 204302,
             204303, 204304, 204305, 204306, 204307]
}
df = pd.DataFrame(data)
#print(df)

threshold = 2.5
window_size = 3

below_threshold = df['error'] &lt;  threshold
'''
Question :&quot;If we look at a window of 3 errors starting from the current row,
 how many of those 3 errors are 'good' (below threshold = 2.5)?&quot;
 
rolling_valid_count gives us a number: 0.0, 1.0, 2.0, or 3.0 (in window_size = 3 case).
For example, if rolling_valid_count is 2.0 for a window of 3, 
it means 2 out of 3 errors were below 2.5, but one was not.
'''
rolling_valid_count = below_threshold[::-1].rolling(
window = window_size,min_periods = window_size    
).sum()[::-1]
#df['rolling_valid_count']  = rolling_valid_count

'''
Double Check.

When rolling_valid_count is exactly equal to window_size(3), 
it means that every single error in that 3-row window 
(starting from the current row) 
has met our &quot;below threshold&quot; criterion.
'''
full_window_start_mask = rolling_valid_count == window_size
#df['full_window_start_mask'] = full_window_start_mask
'''
Question : &quot;Where do we first see a continuous stretch of window_size good errors?&quot;
Now we need a condition:
(Current row is a &quot;full window start&quot;) AND (Previous row was not a &quot;full window start&quot;)

full_window_start_mask:                             [F, F, F, F, T, T, T, T, F, F, ...]
~full_window_start_mask.shift(1,fill_value =False): [T, T, T, T, T, F, F, F, F, T, ...] 
------------------------------------------------------------------
actual_start_of_block_mask:       [F, F, F, F, T, F, F, F, F, F, ...]

we use this method in Pandas for finding the start of consecutive runs of True values.
'''
start_of_new_block_mask = (
full_window_start_mask &amp; 
(~full_window_start_mask.shift(1,fill_value = False))
) 
#df['start_of_new_block_mask'] = start_of_new_block_mask

start_inst_values = df.loc[start_of_new_block_mask, 'inst']
#df['start_inst_values'] = start_inst_values 

'''

'''
print(start_inst_values)
'''
5     204273
14    204302
Name: inst, dtype: int64
'''
</code></pre>
<p>Here is a clear code(use this only after understanding above explanations) :</p>
<pre><code> import pandas as pd
import numpy as np
data = {
    'error': [2.595795, 2.568556, 2.562618, 2.538956, 2.520247,
              2.498345, 2.474890, 2.467736, 2.471115, 2.466424,
              2.495388, 2.520301, 2.604358, 2.553243, 2.490774,
              2.452384, 2.434171, 2.404764, 2.388775, 2.384337],
    'inst': [204267, 204268, 204269, 204271, 204272,
             204273, 204274, 204275, 204276, 204280,
             204284, 204285, 204291, 204299, 204302,
             204303, 204304, 204305, 204306, 204307]
}
df = pd.DataFrame(data)
#print(df)

threshold = 2.5
window_size = 3

full_window_start_mask = (
df['error']
.lt(threshold)[::-1]
.rolling(window_size, min_periods = window_size)
.sum()[::-1].eq(window_size)
)
#df['full_window_start_mask'] = full_window_start_mask

prev_not_full_window = ~full_window_start_mask.shift(1,fill_value = False)
#df['prev_not_full_window'] = prev_not_full_window

start_inst_values = df.loc[full_window_start_mask &amp; prev_not_full_window, 'inst']

print(start_inst_values)
'''
5     204273
14    204302
Name: inst, dtype: int64
'''
</code></pre>
","0","Answer"
"79644131","79644117","<p>Use the <code>dtype</code> argument to <code>read_excel</code> to load that column as a string, not a number.</p>
","1","Answer"
"79644347","79644324","<p>To extract the date and the cumulative total <strong>right before each outage</strong> (i.e., where <code>col1</code> changes from <code>1</code> to <code>0</code>), we can identify the rows where <code>col1 == 0</code>, then look at the row just before each of those and get the <code>Date</code> and <code>col2</code> values. <strong>See demo:</strong> <a href=""https://www.online-python.com/zmTHRWUfKr"" rel=""nofollow noreferrer"">https://www.online-python.com/zmTHRWUfKr</a></p>
<pre><code>import pandas as pd
data = {
    &quot;Date&quot;: [
        &quot;5/29/2025&quot;, &quot;5/31/2025&quot;, &quot;6/1/2025&quot;, &quot;6/2/2025&quot;,
        &quot;6/3/2025&quot;, &quot;6/4/2025&quot;, &quot;6/5/2025&quot;, &quot;6/6/2025&quot;,
        &quot;6/7/2025&quot;, &quot;6/8/2025&quot;, &quot;6/9/2025&quot;, &quot;6/10/2025&quot;
    ],
    &quot;col1&quot;: [1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1],
    &quot;col2&quot;: [1, 2, 3, 4, 0, 0, 0, 1, 2, 3, 4, 5]
}
df = pd.DataFrame(data)
df[&quot;Date&quot;] = pd.to_datetime(df[&quot;Date&quot;])

# Find where a 1 is followed by a 0 (transition point)
transition_idx = df[(df[&quot;col1&quot;] == 1) &amp; (df[&quot;col1&quot;].shift(-1) == 0)].index

result = df.loc[transition_idx, [&quot;Date&quot;, &quot;col2&quot;]].rename(columns={&quot;col2&quot;: &quot;DaysOnline&quot;})

# If the last row is 1, include it as well
if df[&quot;col1&quot;].iloc[-1] == 1:
    result = pd.concat([
        result,
        df.iloc[[-1]][[&quot;Date&quot;, &quot;col2&quot;]].rename(columns={&quot;col2&quot;: &quot;DaysOnline&quot;})
    ])

print(result)
</code></pre>
","1","Answer"
"79644399","79644379","<p>A possible solution:</p>
<pre><code>m = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce').isna()
df[m &amp; df['Date'].ne('')]
</code></pre>
<p>All dates that are not parsable will be converted to <code>NaT</code> with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>pd.to_datetime</code></a>. Thus, we need to look at the <code>NaT</code> to identify the ones that do not correspond to empty dates (<code>''</code>).</p>
<p>Output:</p>
<pre><code>   ID        Date
4   5  |199/142/4
</code></pre>
","2","Answer"
"79644928","79623716","<pre><code>import pandas as pd

df = pd.DataFrame({
    'Drink': ['Beer', 'Beer', 'Wine', 'Wine', 'Wine', 'Whisky', 'Whisky'],
    'Units': [14, 5, 9, 15, 7, 12, 17]
})
order = ['Wine', 'Beer', 'Whisky', 'Beer', 'Wine', 'Whisky']

res = df.assign(
drinkCount = df.groupby('Drink').cumcount()    
).merge(
pd.DataFrame({'Drink': order})
.assign(drinkCount = lambda x: x.groupby('Drink').cumcount()),
on = ['Drink','drinkCount'],
how = 'inner'    
)#.drop(columns = 'drinkCount')

'''
   Drink  Units  drinkCount
0    Beer     14           0
1    Beer      5           1
2    Wine      9           0
3    Wine     15           1
4  Whisky     12           0
5  Whisky     17           1
'''
</code></pre>
","0","Answer"
"79645114","79620883","<p><strong>Formulae</strong> :</p>
<ol>
<li><p><strong>a // b      : Floor</strong></p>
</li>
<li><p><strong>-(-a // b)  : Ceiling</strong></p>
</li>
</ol>
<p>Where <code>a</code> is equivalent to <code>maxLen(df1,df2)</code> and <code>b</code> is equivalent to <code>len(df)</code></p>
<pre><code>import pandas as pd
import numpy as np

df1 = pd.DataFrame({'A': range(10)})
df2 = pd.DataFrame({'B': range(30)})

max_len = max(len(df1),len(df2))
reps1 = -(-max_len // len(df1))

def expand_to_length(df,max_len):
    reps = -(-max_len // len(df))
    tiled_array = np.tile(df.values,(reps,1))[:max_len]
    res_df = pd.DataFrame(tiled_array, columns = df.columns)
    return res_df

df1Expanded = expand_to_length(df1,max_len)
df2Expanded = expand_to_length(df2,max_len)
 
combined = pd.concat([df1Expanded,df2Expanded],axis =1)
print(combined)
'''
    A   B
0   0   0
1   1   1
2   2   2
3   3   3
4   4   4
5   5   5
6   6   6
7   7   7
8   8   8
9   9   9
10  0  10
11  1  11
12  2  12
13  3  13
14  4  14
15  5  15
16  6  16
17  7  17
18  8  18
19  9  19
20  0  20
21  1  21
22  2  22
23  3  23
24  4  24
25  5  25
26  6  26
27  7  27
28  8  28
29  9  29

'''
</code></pre>
","0","Answer"
"79645200","79643566","<p>According to the <a href=""https://pandas.pydata.org/docs/user_guide/timeseries.html#time-date-components"" rel=""nofollow noreferrer"">documentation</a>,</p>
<blockquote>
<p>is_year_end      Logical indicating if last day of year (defined by frequency)</p>
</blockquote>
<p>With the help of other users in the community, I have understood that &quot;defined by frequency&quot; means that, for example, if the series contains only business days, the accessor <code>.is_year_end</code> is not going to test if each date in the series is 12/31, but it will test if each date in the series is the last business day of the year.</p>
","1","Answer"
"79647251","79647209","<p>You're trying to assign a value to <code>row[8] = &quot;buy!&quot;</code>, but <code>row</code> is an immutable namedtuple — <strong>you cannot modify it this way</strong>. So the <code>'buy?'</code> column in your DataFrame remains empty.</p>
<pre><code># Update 'buy?' column based on logic
for i in range(len(stock_data)):
    sma200 = stock_data.iloc[i]['SMA200']
    close = stock_data.iloc[i]['Close']
    if pd.notna(sma200) and sma200 &gt; close:
        stock_data.at[stock_data.index[i], 'buy?'] = 'buy!'
</code></pre>
<p><code>pd.notna(sma200)</code> is used to avoid comparing <code>NaN</code> values (which occur in the initial rows because of the rolling mean).</p>
","0","Answer"
"79647338","79647209","<p>If you want to get column filled with values based on other columns values, you might use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>pandas.DataFrame.apply</code></a> with <code>axis=1</code>, consider following simple example</p>
<pre><code>import pandas as pd
def relation(row):
    if row.x &lt; row.y:
        return &quot;lesser&quot;
    elif row.x &gt; row.y:
        return &quot;greater&quot;
    else:
        return &quot;equal&quot;
df = pd.DataFrame({&quot;x&quot;:[1,2,3,4,5],&quot;y&quot;:[5,4,3,2,1]})
df[&quot;rel&quot;] = df.apply(relation, axis=1)
print(df)
</code></pre>
<p>gives output</p>
<pre><code>   x  y      rel
0  1  5   lesser
1  2  4   lesser
2  3  3    equal
3  4  2  greater
4  5  1  greater
</code></pre>
","2","Answer"
"79647710","79647209","<p>In other answer you have example how to use <code>.apply()</code><br />
but you can also directly create column with values <code>True</code>/<code>False</code></p>
<pre><code>stock_data['buy?'] = (stock_data['SMA200'] &gt; stock_data['Close'])
</code></pre>
<p>And eventually you can  change <code>True</code> to <code>buy!</code> and <code>False</code> to empty string
using:</p>
<ul>
<li>pandas.DataFrame.<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html"" rel=""nofollow noreferrer"">map()</a></li>
</ul>
<pre><code>stock_data['buy?'] = stock_data['buy?'].map({True: 'buy!', False: &quot;&quot;})
</code></pre>
<ul>
<li>pandas.DataFrame.<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html"" rel=""nofollow noreferrer"">replace()</a></li>
</ul>
<pre><code>stock_data['buy?'] = stock_data['buy?'].replace({True: 'buy!', False: &quot;&quot;})
</code></pre>
<ul>
<li><a href=""https://numpy.org/doc/2.2/reference/generated/numpy.where.html"" rel=""nofollow noreferrer"">numpy.where()</a></li>
</ul>
<pre><code>stock_data['buy?'] = np.where(stock_data['buy?'], 'buy!', &quot;&quot;)
</code></pre>
<hr />
<p>You can even do it in one line</p>
<pre><code>stock_data['buy?'] = (stock_data['SMA200'] &gt; stock_data['Close']).map({True: 'buy!', False: &quot;&quot;})
</code></pre>
<pre><code>stock_data['buy?'] = (stock_data['SMA200'] &gt; stock_data['Close']).replace({True: 'buy!', False: &quot;&quot;})
</code></pre>
<pre><code>stock_data['buy?'] = np.where((stock_data['SMA200'] &gt; stock_data['Close']), 'buy!', &quot;&quot;)
</code></pre>
<hr />
<p>Minimal working example used for tests:</p>
<pre><code>#import yfinance as yf
import pandas as pd
import numpy as np

#stock_data = yf.download(tickers=&quot;AAPL&quot;, interval=&quot;1d&quot;, period=&quot;3y&quot;) 
#stock_data['SMA50'] = stock_data['Close'].rolling(window=50).mean()
#stock_data['SMA200'] = stock_data['Close'].rolling(window=200).mean()

stock_data = pd.DataFrame({&quot;Close&quot;:[1,2,3,4,5],&quot;SMA200&quot;:[5,4,3,2,1]})

stock_data['buy?'] = (stock_data['SMA200'] &gt; stock_data['Close'])

stock_data['buy?_map'] = stock_data['buy?'].map({True: 'buy!', False: &quot;&quot;})
stock_data['buy?_replace'] = stock_data['buy?'].replace({True: 'buy!', False: &quot;&quot;})
stock_data['buy?_where'] = np.where(stock_data['buy?'], 'buy!', &quot;&quot;)

stock_data['buy?_map_2'] = (stock_data['SMA200'] &gt; stock_data['Close']).map({True: 'buy!', False: &quot;&quot;})
stock_data['buy?_replace_2'] = (stock_data['SMA200'] &gt; stock_data['Close']).replace({True: 'buy!', False: &quot;&quot;})
stock_data['buy?_where_2'] = np.where((stock_data['SMA200'] &gt; stock_data['Close']), 'buy!', &quot;&quot;)

print(stock_data)
</code></pre>
","0","Answer"
"79647750","79647209","<h3>Mistakes:</h3>
<ul>
<li><p><code>row[8] = &quot;buy!&quot;</code> doesn't modify the DataFrame — it's modifying a <strong>temporary tuple</strong>, not the actual DataFrame.</p>
</li>
<li><p>Also, using <code>itertuples()</code> only returns tuples which are temporary, use <code>iterrows()</code> or <code>.apply()</code> operations to actually modify the rows</p>
</li>
</ul>
<h3>Possible Solution:</h3>
<pre><code>import yfinance as yf
import pandas as pd

# Download historical data
stock_data = yf.download(tickers=&quot;AAPL&quot;, interval=&quot;1d&quot;, period=&quot;3y&quot;)

# Calculate moving averages
stock_data['SMA50'] = stock_data['Close'].rolling(window=50).mean()
stock_data['SMA200'] = stock_data['Close'].rolling(window=200).mean()

# Create 'buy?' column based on condition
stock_data['buy?'] = stock_data.apply(lambda row: &quot;buy!&quot; if pd.notna(row['SMA200']) and row['SMA200'] &gt; row['Close'] else &quot;&quot;, axis=1)

# Export to Excel
output_file = &quot;AAPL.xlsx&quot;
stock_data.to_excel(output_file, sheet_name=&quot;datas&quot;)

print(f&quot;Data saved to {output_file}&quot;)
</code></pre>
<h4>Explanation:</h4>
<ul>
<li><p><code>pd.notna(row['SMA200'])</code> ensures you avoid comparing <code>NaN</code> values early in the SMA calculation.</p>
</li>
<li><p><code>.apply(..., axis=1)</code> properly updates each row with the correct <code>&quot;buy!&quot;</code> label.</p>
</li>
<li><p><code>to_excel()</code> writes headers by default, which is usually helpful.</p>
</li>
</ul>
<p>Hope it helps!</p>
","1","Answer"
"79648124","79648028","<p>Here's the answer:</p>
<pre><code>import pandas as pd
import os
import sys
import json

directory = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop')

dtype_dic = {'customerNumber' : str, 'itemNumber':str}

if os.path.isfile(directory + '/' + 'Price_Checker.csv'):
    print(&quot;Program Running.... please standby&quot;)
else:
    print(&quot;Required file Price_Checker.csv not found on user's Desktop&quot;)
    sys.exit()

df = pd.read_csv(directory + r'\\Price_Checker.csv', dtype=str)

# Group by customerNumber and gather itemNumbers
grouped = df.groupby(&quot;customerNumber&quot;)[&quot;itemNumber&quot;].apply(list).reset_index()

# Format as desired JSON
json_data = []
for _, row in grouped.iterrows():
    customer_entry = {
        &quot;customerInformation&quot;: {
            &quot;customerNumber&quot;: row[&quot;customerNumber&quot;]
        },
        &quot;itemInformation&quot;: [
            {&quot;itemNumber&quot;: item} for item in row[&quot;itemNumber&quot;]
        ]
    }
    json_data.append(customer_entry)

# Save or print the JSON
with open(&quot;output.json&quot;, &quot;w&quot;) as f:
    json.dump(json_data, f, indent=2)

# Optional: print to console
print(json.dumps(json_data, indent=2))
</code></pre>
","0","Answer"
"79648203","79648028","<p>You can use nested list comprehension with <code>groupby</code>:</p>
<pre><code>directory = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop')

dtype_dic = {'customerNumber' : str, 'itemNumber':str}

if os.path.isfile(directory + '/' + 'Price_Checker.csv'):
    print(&quot;Program Running.... please standby&quot;)
else:
    print(&quot;Required file Price_Checker.csv not found on user's Desktop&quot;)
    sys.exit()

df = pd.read_csv(directory + r'\\Price_Checker.csv', dtype=str)


json_data = ([{'customerInformation':{'customerNumber':name},
              'itemInformation':[{'itemNumber':item} for item in g]}
               for name, g in df.groupby('customerNumber')['itemNumber']])

# Save or print the JSON
with open(&quot;output.json&quot;, &quot;w&quot;) as f:
    json.dump(json_data, f, indent=2)

# Optional: print to console
print(json.dumps(json_data, indent=2))
</code></pre>
","1","Answer"
"79649940","79649939","<p>Turns out that how you pickle makes a difference. If the dataframe was pickled through a pickle dump, it works:</p>
<pre class=""lang-py prettyprint-override""><code>with open('test.pkl','wb') as handle:
    pickle.dump(df,handle)
</code></pre>
<p>On second computer, in different file:</p>
<pre class=""lang-py prettyprint-override""><code>with open('path/to/test.pkl','rb') as handle:
    data = pickle.load(handle)
# or:
data = pd.read_pickle('path/to/test.pkl')
</code></pre>
<p><strong>However</strong>: If you select <code>pickle.HIGHEST_PROTOCOL</code> as the protocol for dumping, the same error will arise no matter how the pickle is loaded</p>
<pre class=""lang-py prettyprint-override""><code># Gave pickle with same missing module error on loading:
with open('test.pkl','wb') as handle:
    pickle.dump(df,handle,protocol=pickle.HIGHEST_PROTOCOL)
</code></pre>
<p>I have not done any investigation on why the error appears and came to this solution simply through trial and error.</p>
","0","Answer"
"79650133","79650039","<p>The problem was how Taipy binds values to the table, but finally managed to get it done :) Here is how should the <strong>filter_category</strong> function look like:</p>
<pre><code>def filter_category(state):
    if not state.selected_filter:
        return
    
    state.filter_display_texts.append(state.selected_filter)
    value = state.data.groupby(state.selected_filter)[['depositAmount', 'withdrawalAmount', 'totalRevenue']].sum().reset_index()
    state.tables_data.append(value)
    print('Filtering data by:', state.selected_filter)
    print('State tables', state.tables_data)
    with builder.Page() as table_data:
        with builder.layout(&quot;1&quot;):
            for idx, _ in enumerate(state.tables_data):
                with builder.part(&quot;card&quot;):
                    builder.table(data=f&quot;{{tables_data[{idx}]}}&quot;, page_size=10)
</code></pre>
","2","Answer"
"79650658","79650039","<p>Aside from the approach using &quot;partials&quot; (which you have correctly resolved in your answer by binding the variable name as a string, i.e. <code>data=&quot;{data_variable_name}&quot;</code>, rather than the object itself), there are 2 other approaches that might interest you.</p>
<p>First, I made the imports and created a mock dataframe:</p>
<pre class=""lang-py prettyprint-override""><code>from taipy.gui import Gui
import taipy.gui.builder as tgb
import pandas as pd

# Load data
data = pd.DataFrame(
    [
        [&quot;Partner A&quot;, &quot;Region 1&quot;, &quot;User 1&quot;, &quot;Group 1&quot;, 1000, 500, 1500],
        [&quot;Partner B&quot;, &quot;Region 2&quot;, &quot;User 2&quot;, &quot;Group 2&quot;, 2000, 1000, 3000],
        [&quot;Partner C&quot;, &quot;Region 1&quot;, &quot;User 3&quot;, &quot;Group 1&quot;, 1500, 700, 2200],
        [&quot;Partner A&quot;, &quot;Region 2&quot;, &quot;User 4&quot;, &quot;Group 3&quot;, 1200, 600, 1800],
        [&quot;Partner B&quot;, &quot;Region 1&quot;, &quot;User 5&quot;, &quot;Group 2&quot;, 800, 400, 1200],
        [&quot;Partner C&quot;, &quot;Region 2&quot;, &quot;User 6&quot;, &quot;Group 1&quot;, 900, 300, 1200],
        [&quot;Partner A&quot;, &quot;Region 1&quot;, &quot;User 7&quot;, &quot;Group 3&quot;, 1100, 550, 1650],
        [&quot;Partner B&quot;, &quot;Region 2&quot;, &quot;User 8&quot;, &quot;Group 2&quot;, 1300, 650, 1950],
        [&quot;Partner C&quot;, &quot;Region 1&quot;, &quot;User 9&quot;, &quot;Group 1&quot;, 1400, 700, 2100],
    ],
    columns=[ &quot;partnerName&quot;, &quot;regionName&quot;, &quot;userId&quot;, &quot;groupName&quot;, &quot;depositAmount&quot;, &quot;withdrawalAmount&quot;, &quot;totalRevenue&quot;],
)
</code></pre>
<h1>Alternative 1: Table rebuild property</h1>
<p>If you change certain properties of the table (e.g. column names), by default, the changes may not be reflected on the page. You can change this behaviour by setting <a href=""https://docs.taipy.io/en/latest/refmans/gui/viselements/generic/table/#the-rebuild-property"" rel=""nofollow noreferrer""><code>rebuild</code></a> to True.</p>
<p>Now the code looks like:</p>
<pre class=""lang-py prettyprint-override""><code># Imports and dataframe definition
...

selected_filter = None
filters = [&quot;partnerName&quot;, &quot;regionName&quot;, &quot;userId&quot;, &quot;groupName&quot;]

table_df = data

def filter_category(state):
    df = data.groupby(by=state.selected_filter)[[&quot;depositAmount&quot; ,&quot;withdrawalAmount&quot; ,&quot;totalRevenue&quot;]].sum()
    state.table_df = df.reset_index()
    
with tgb.Page() as page:
    tgb.selector(value=&quot;{selected_filter}&quot;, lov=&quot;{filters}&quot;, dropdown=True, label=&quot;Select filter&quot;, on_change=filter_category, mode=&quot;radio&quot;)
    tgb.table(&quot;{table_df}&quot;, rebuild=True)

Gui(page=page).run(title=&quot;Analytics Dashboard&quot;, dark_mode=True)
</code></pre>
<h1>Alternative 2: Table aggregation</h1>
<p>Otherwise, you could also remove selectors entirely and instead use the built-in <a href=""https://docs.taipy.io/en/latest/refmans/gui/viselements/generic/table/#aggregation"" rel=""nofollow noreferrer"">table aggregation</a> filters.</p>
<pre class=""lang-py prettyprint-override""><code># Imports and dataframe definition
...

with tgb.Page() as page:
    tgb.table(
        &quot;{data}&quot;,
        group_by__partnerName=True,
        group_by__regionName=True,
        group_by__userId=True,
        group_by__groupName=True,
        apply__depositAmount=&quot;sum&quot;,
        apply__withdrawalAmount=&quot;sum&quot;,
        apply__totalRevenue=&quot;sum&quot;,
    )

Gui(page=page).run(title=&quot;Analytics Dashboard&quot;, dark_mode=True)
</code></pre>
<p>Now the table column headers have buttons which you can click to aggregate by that column. The documentation provides more details such as if the column names were strings and can't be passed as a kwarg.</p>
","2","Answer"
"79651129","79651120","<p>Your approach works fine, however <code>style</code> does not modify the DataFrame in place. Instead it returns a special object that can be displayed (for instance in a notebook) or exported to a file.</p>
<p>You could see the HTML version in jupyter with:</p>
<pre><code>df.style.format(my_formatter)
</code></pre>
<p>(this should be the last statement of the current cell!)</p>
<p>Or a text version with:</p>
<pre><code>print(df.style.format(my_formatter).to_string())
</code></pre>
<p><a href=""https://i.sstatic.net/8bcZhdTK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8bcZhdTK.png"" alt=""jupyter output"" /></a></p>
<p><a href=""https://i.sstatic.net/EuvmkjZP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EuvmkjZP.png"" alt=""text output"" /></a></p>
<p>Note that your approach is however quite slow. If you have homogeneous dtypes, you could take advantage of the builtin <code>thousands</code> parameter:</p>
<pre><code>df.style.format(thousands=',')
</code></pre>
<p>Or if you want to use a custom format per column, pass a dictionary:</p>
<pre><code>df.style.format({c: '{:,d}' for c in df.select_dtypes('number')})
</code></pre>
<p>And, finally, if you want to change the data to strings and return a DataFrame, you would need to use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html"" rel=""nofollow noreferrer""><code>map</code></a>:</p>
<pre><code>out = df.map(my_formatter)
</code></pre>
","4","Answer"
"79651252","79628910","<p>Requirement : Eliminate all sandwich nans</p>
<pre><code>import numpy as np

data = np.array([np.nan, np.nan, 1, 4, 6, 6, 9, np.nan, 13, np.nan], dtype=np.float64)
'''
[nan nan  1.  4.  6.  6.  9. nan 13. nan]
'''
valid = ~np.isnan(data)

isValidBefore = np.maximum.accumulate(valid)
isValidAfter = np.maximum.accumulate(valid[::-1])[::-1]

sandwich_nan = np.isnan(data) &amp; isValidBefore &amp; isValidAfter
res = data[~sandwich_nan]
print(res)
'''
[nan nan  1.  4.  6.  6.  9. 13. nan]
'''
</code></pre>
","1","Answer"
"79651374","79517222","<pre><code>import pandas as pd

couples = pd.DataFrame({
    'man': [
        ['f', 'b', 'r'],
        ['hunting', 'mud', 'f'],
        ['r', 'mvs', 'run'],
        ['run', 'r', 'b', 'mud'],
        ['mvs', 'r', 'yod']
    ],
    'woman': [
        ['b', 'r', 'mvs'],
        ['f', 'drinking'],
        ['knitting', 'r'],
        ['run', 'b', 'f', 'mvs'],
        ['mvs']
    ]
})

def uniques(row):
    manSet,womanSet = set(row['man']),set(row['woman'])
    pdSeries = pd.Series({
    'man_unique'   :list(manSet - womanSet),
    'woman_unique' :list(womanSet - manSet)  
    })
    return pdSeries

couples[['man_unique', 'woman_unique']] = couples.apply(uniques,axis =1)
'''
                 man             woman      man_unique woman_unique
0          [f, b, r]       [b, r, mvs]             [f]        [mvs]
1  [hunting, mud, f]     [f, drinking]  [hunting, mud]   [drinking]
2      [r, mvs, run]     [knitting, r]      [run, mvs]   [knitting]
3   [run, r, b, mud]  [run, b, f, mvs]        [r, mud]     [f, mvs]
4      [mvs, r, yod]             [mvs]        [r, yod]           []
'''    
    
</code></pre>
","0","Answer"
"79651408","79651091","<p>I suspect that your issue is here:</p>
<pre><code>df = df[df.duration &gt;= 1] &amp; (df.duration &lt;= 60)
</code></pre>
<p>This is creating a result that is <code>n * m</code> rows and columns. Where <code>n</code> is the number of rows matching <code>df[df.duration &gt;= 1]</code> and <code>m</code> is the number of rows in <code>df</code>. I strongly suspect what you want to do is filter <code>df</code> for rows matching that combined criteria (duration between 1 and 60), so you likely wanted to do:</p>
<pre><code>df = df[(df.duration &gt;= 1) &amp; (df.duration &lt;= 60)]
</code></pre>
<p>The following will likely do the same in the event you find it simpler:</p>
<pre><code>df = df[df.duration.between(1, 60)]
</code></pre>
<p>For Reference:</p>
<pre><code>import pandas

df = pandas.DataFrame([
    {&quot;foo&quot;: -1}, {&quot;foo&quot;: -2}, {&quot;foo&quot;: -3},
    {&quot;foo&quot;: 1}, {&quot;foo&quot;: 2}, {&quot;foo&quot;: 3},
    {&quot;foo&quot;: 11}, {&quot;foo&quot;: 12}, {&quot;foo&quot;: 13},
])

print(&quot;-------------------&quot;)
print(&quot;Rows matching df.foo &lt;= 10&quot;)
print(df.foo &lt;= 10)
print(&quot;-------------------\n&quot;)

print(&quot;-------------------&quot;)
print(&quot;Rows matching df[df.foo &gt;= 1]&quot;)
print(df[df.foo &gt;= 1])
print(&quot;-------------------\n&quot;)

print(&quot;-------------------&quot;)
print(&quot;Your current result&quot;)
print(&quot;Rows matching df[df.foo &gt;= 1] &amp; (df.foo &lt;= 10)&quot;)
print(df[df.foo &gt;= 1] &amp; (df.foo &lt;= 10))
print(&quot;-------------------\n&quot;)

print(&quot;-------------------&quot;)
print(&quot;Likely your objective&quot;)
print(&quot;Rows matching df[(df.foo &gt;= 1) &amp; (df.foo &lt;= 10)]&quot;)
print(df[(df.foo &gt;= 1) &amp; (df.foo &lt;= 10)])
print(&quot;-------------------\n&quot;)
</code></pre>
<p>That will give you:</p>
<pre><code>-------------------
Rows matching df.foo &lt;= 10
0     True
1     True
2     True
3     True
4     True
5     True
6    False
7    False
8    False
Name: foo, dtype: bool
-------------------

-------------------
Rows matching df[df.foo &gt;= 1]
   foo
3    1
4    2
5    3
6   11
7   12
8   13
-------------------

-------------------
Your current result
Rows matching df[df.foo &gt;= 1] &amp; (df.foo &lt;= 10)
     foo      0      1      2      3      4      5      6      7      8
3  False  False  False  False  False  False  False  False  False  False
4  False  False  False  False  False  False  False  False  False  False
5  False  False  False  False  False  False  False  False  False  False
6  False  False  False  False  False  False  False  False  False  False
7  False  False  False  False  False  False  False  False  False  False
8  False  False  False  False  False  False  False  False  False  False
-------------------

-------------------
Likely your objective
Rows matching df[(df.foo &gt;= 1) &amp; (df.foo &lt;= 10)]
   foo
3    1
4    2
5    3
-------------------
</code></pre>
","0","Answer"
"79651502","79651472","<p>The simplest way is to add a dedicated method for the group-by operation.</p>
<p>The example below is complete.</p>
<pre class=""lang-py prettyprint-override""><code>example_df = pd.DataFrame({
    'strata': ['A', 'A', 'A', 'B', 'B'],
    'x': [-1, 0, 1, -1, 1],
    'y': [-1, 0, 1, 1, -1],
})

def stratified_correlation(df: pd.DataFrame) -&gt; pd.Series:
  return pd.Series(
      dict(
          n_examples=df['x'].nunique(),
          correlation=df['x'].corr(df['y']),
      )
  )


(
    example_df
    .groupby('strata')
    .apply(stratified_correlation)
    .style.format(
        formatter={'correlation': '{:,.02%}'},
        thousands=',',
        precision=0,
    )
)
</code></pre>
","0","Answer"
"79653454","79653054","<p>You can do merge it and update the index:</p>
<pre><code>import pandas as pd

base = pd.DataFrame({
    'grp': ['A', 'A', 'B', 'B', 'A', 'B', 'C'],
    'id': ['a', 'b', 'a', 'b', 'a', 'a', 'c'],
    'date': ['2025-01-01'] * 4 + ['2025-01-02'] * 3,
    'cat1': [0] * 7,
    'cat2': [0] * 7,
})

update = pd.DataFrame({
    'date': ['2025-01-01', '2025-01-01', '2025-01-02', '2025-01-02'],
    'id': ['a', 'b', 'a', 'b'],
    'cat1': [1, 0, 1, 0],
    'cat2': [0, 1, 1, 1],
})

# Set the index for both DataFrames
base_indexed = base.set_index(['id', 'date'])
update_indexed = update.set_index(['id', 'date'])

# Update the base DataFrame with values from the update DataFrame
base_indexed.update(update_indexed)

# Reset the index and specify the column order
desired_output = base_indexed.reset_index()[['grp', 'id', 'date', 'cat1', 'cat2']]

print(desired_output)
</code></pre>
<p>Output:</p>
<pre><code>  grp id        date  cat1  cat2
0   A  a  2025-01-01     1     0
1   A  b  2025-01-01     0     1
2   B  a  2025-01-01     1     0
3   B  b  2025-01-01     0     1
4   A  a  2025-01-02     1     1
5   B  a  2025-01-02     1     1
6   C  c  2025-01-02     0     0
</code></pre>
","0","Answer"
"79653557","79652595","<p>You are already getting the local time from Kolkata. Its not adding to UTC, the <code>+5:30</code> just tells you that Kolkata time is at <code>+5:30</code> from UTC time.</p>
<p>If you want to get rid of that information, then just you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.tz_localize.html#pandas.Timestamp.tz_localize"" rel=""nofollow noreferrer""><code>tz_localize</code></a>, like this:</p>
<pre><code>`df['timestamp'] = df['timestamp'].dt.tz_localize(None)`
</code></pre>
<p>This tells it what timezone it should tell you the difference between it and UTC time. In this case, since you don't want to see this, setting it to &quot;None&quot; removes this information altogether.</p>
","0","Answer"
"79653567","79653549","<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

start_date = &quot;2024-09-01&quot;
end_date = &quot;2025-04-30&quot;

# date range with UK timezone (Europe/London)
date_range = pd.date_range(start=start_date, end=end_date, freq='h', tz='Europe/London')
dummy_data = np.zeros((len(date_range), 1))

df = pd.DataFrame(dummy_data, index=date_range)


# Sunday March 30th at 1am
print(df.resample('86400000ms').agg('sum').loc[&quot;2025-03-29&quot;: &quot;2025-04-01&quot;])
#                              0
# 2025-03-29 23:00:00+00:00  0.0
# 2025-03-31 00:00:00+01:00  0.0
# 2025-04-01 00:00:00+01:00  0.0
print( df.resample('1d').agg('sum').loc[&quot;2025-03-29&quot;: &quot;2025-04-01&quot;])
#                             0
# 2025-03-29 00:00:00+00:00  0.0
# 2025-03-30 00:00:00+00:00  0.0
# 2025-03-31 00:00:00+01:00  0.0
# 2025-04-01 00:00:00+01:00  0.0
</code></pre>
<p>Above is a minimal example to reproduce your problem. I believe the issue is with resampling by <code>86400000ms</code>, which causes an error when skipping the DST transition on Sunday, March 30th at 1 a.m. Why not resample by <code>'1d'</code> instead?</p>
","2","Answer"
"79653933","79653054","<p>If your indices in <code>base</code> and merging keys in <code>update</code> are unique, you could <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>/<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.update.html"" rel=""nofollow noreferrer""><code>update</code></a>:</p>
<pre><code>keys = ['id', 'date']
base.update(base[keys].reset_index().merge(update).set_index('index'))
</code></pre>
<p>A variant of that would be to align <code>update</code> to <code>base</code> with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>reindex</code></a>:</p>
<pre><code>keys = ['id', 'date']
tmp = update.set_index(keys).reindex(base[keys]).set_index(base.index)
base.update(tmp)
</code></pre>
<p>Modified <code>base</code>:</p>
<pre><code>  grp id        date  cat1  cat2
0   A  a  2025-01-01     1     0
1   A  b  2025-01-01     0     1
2   B  a  2025-01-01     1     0
3   B  b  2025-01-01     0     1
4   A  a  2025-01-02     1     1
5   B  a  2025-01-02     1     1
6   C  c  2025-01-02     0     0
</code></pre>
","0","Answer"
"79653959","79639167","<p>Use this as your template request from the comment above:</p>
<pre><code>import requests
import pandas as pd
from datetime import datetime


dateStr = '2025-06-04'

apiUrl = 'https://api.racingtv.com/racing/results/list/{dateStr}?'

headers = {
'accept':'application/json',
'authorization':'',
'content-type':'application/json',
'referer':'https://www.racingtv.com/',
'sec-ch-ua':'&quot;Chromium&quot;;v=&quot;136&quot;, &quot;Google Chrome&quot;;v=&quot;136&quot;, &quot;Not.A/Brand&quot;;v=&quot;99&quot;',
'sec-ch-ua-mobile':'?0',
'sec-ch-ua-platform':'&quot;Windows&quot;',
'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
'x-requested-with':'racingtv-web/5.3.0'
    }

jsonData = requests.get(apiUrl, headers=headers).json()

trackData = jsonData['meetings']
sectional_api_urls = {}
for each in trackData:
    trackSlug = each['track']['slug']
    sectional_api_urls[trackSlug] = []
    completed_races = each['completed_races']
    for completed_race in completed_races:
        startTime = completed_race['start_time_scheduled']
        dt = datetime.fromisoformat(startTime)
        timeStr = dt.strftime('%H%M') 
        
        sectional_api_urls[trackSlug].append(timeStr)
        
dfs = []
for trackSlug, timesList in sectional_api_urls.items():
    for timeStr in timesList:
        apiUrl = f'https://api.racingtv.com/racing/iq-results/{dateStr}{trackSlug}/{timeStr}/sectional?'
        try:
            print(apiUrl)
            jsonData = requests.get(apiUrl, headers=headers).json()
            data = jsonData['race']
            
            df = pd.json_normalize(
                data,
                record_path=['runners', 'sectionals'],
                meta=[
                    'scheduled_start_time',
                    'time_index_score',
                    'meeting_average_time_index',
                    'win_time_vs_par',
                    ['runners', 'id'],
                    ['runners', 'finish_position'],
                    ['runners', 'horse_name'],
                    ['runners', 'finishing_time']
                ],
                errors='ignore'  # in case some meta paths don’t exist for all entries
            )
        except Exception as e:
            print(e)
        
        df[['track', 'timeStr']] = trackSlug, timeStr
        dfs.append(df)
    
    
    
output = pd.concat(dfs)
</code></pre>
","0","Answer"
"79654035","79653909","<p>According to <a href=""https://docs.streamlit.io/develop/api-reference/data/st.data_editor"" rel=""nofollow noreferrer"">docs</a>, you can't color cell if it's not in disabled column.</p>
<blockquote>
<p>Styles from pandas.Styler will only be applied to non-editable columns.</p>
</blockquote>
<p>If disabling <code>rating</code> column is acceptable to you, you can use <a href=""https://pandas.pydata.org/docs/dev/reference/api/pandas.io.formats.style.Styler.map.html"" rel=""nofollow noreferrer""><code>pandas.Styler</code></a> like this:</p>
<pre class=""lang-py prettyprint-override""><code>import streamlit as st
import pandas as pd

df = pd.DataFrame(
    [
       {&quot;command&quot;: &quot;test 1&quot;, &quot;rating&quot;: 4, &quot;is_widget&quot;: True},
       {&quot;command&quot;: &quot;test 2&quot;, &quot;rating&quot;: 5, &quot;is_widget&quot;: False},
       {&quot;command&quot;: &quot;test 3&quot;, &quot;rating&quot;: 1, &quot;is_widget&quot;: True},
   ]
)

# you can use .applymap(), but it's deprecated
df = df.style.map(
    lambda val: 'background-color: yellow' if val &lt; 2 else '',
    subset=['rating']
)

edited_df = st.data_editor(df, disabled=[&quot;rating&quot;])
</code></pre>
","0","Answer"
"79654134","79653826","<p>Considering you want to plot depth, I have plotted your data on a reversed y-axis.</p>
<p>You can use a bar chart with specified heights and bottoms, see code below:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame(
    {
        &quot;depth in&quot;: [1000, 1020, 1040],
        &quot;depth out&quot;: [1020, 1040, 1060],
        &quot;Parameter&quot;: [10, 15, 5],
    }
)

# Create figure
fig, ax = plt.subplots(1, 1)

# Bar plot
ax.bar(
    x=df['Parameter'],                        # x-axis value of bars
    height=df['depth out'] - df['depth in'],  # Length of bars
    bottom=df['depth in'])                    # Starting value of bar

# Format plot
ax.set_ylabel('Depth')
ax.set_xlabel('Parameter')
ax.set_xlim([0, 20])
ax.yaxis.set_inverted(True)  # Reversing y-axis so depth goes down
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/gwd7cRWI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gwd7cRWI.png"" alt=""enter image description here"" /></a></p>
","1","Answer"
"79655104","79654798","<p>If there is no white-spaces in your CSV files so the input is something like that:</p>
<pre><code>A,T,C,G,T,A,G,C,0
A,T,C,G,0,A,T,C,G
A,T,C,G,T,A,G,C,0
</code></pre>
<p>Then you can use the very-fast <strong><code>np.loadtxt</code></strong> that way:</p>
<pre class=""lang-py prettyprint-override""><code>arr = np.loadtxt('tiny.csv', dtype='U1', delimiter=',')
</code></pre>
<p>It takes about <strong>5 seconds</strong> to load a CSV of your size (i.e. with 3 million columns and 162 rows) on my machine.</p>
<p>Then, the problem is to convert that to a Pandas dataframe quickly. Pandas strings are pure-Python ones so they are inherently inefficient. Once in memory, the 486 million reference-counted garbage-collected dynamically-allocated objects takes a lot of memory and time to be computed. That being said, Python apparently cache strings having only 1 character so they do not take so much space in practice. On my machine, the whole dataframe takes about 6 GiB (12 bytes/item while only 1 byte/item is technically possible). Here is the code to create the dataframe from the Numpy array:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame(arr, dtype=str)
</code></pre>
<p>It takes about <strong>9 seconds</strong> to load the array and create the dataframe on my machine.</p>
<hr />
<h2>Alternative solutions</h2>
<p>An alternative solution is to use <em>categorical columns</em>. Such data-type can compact items so 1 item only take at most few bytes (close to 1 here). Moreover, categorical columns are stored as efficient Numpy arrays internally. I expect your dataframe to take about 0.5 GiB, so 48 times less than with strings! Such column are also computed several order of magnitude faster than strings! There is one downside: <strong>creating categorical columns is painfully slow</strong>: it takes a minute for a 100 times smaller dataframe...</p>
<p>One workaround solution is to reinterpret the Numpy array as 8-bit integers, create the dataframe from that and manipulate the items as integers. This is very fast though not super convenient to use. Here is the code:</p>
<pre><code>arr = np.loadtxt('file.csv', dtype='S1', delimiter=',')
arr = arr.view(np.uint8)
out = pd.DataFrame(arr, dtype=np.uint8)
</code></pre>
<p>This takes about 4 seconds on my machine (mainly because the <code>S1</code> type is faster than the <code>U1</code> type which is not needed here anymore).</p>
<p>If you try to cheat and convert this dataframe to category by any means, then this will take a HUGE amount of time. The main issue is that the categorical datatype is unfortunately not really optimized yet so the inherently inefficient strings are actually better... at least to create the dataframe. Indeed, operations are much faster on categorical columns than string-based ones.</p>
","3","Answer"
"79655339","79655324","<p>The easiest way is to simply sort the data according to the measure you want to consider and then take the top N rows using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html"" rel=""nofollow noreferrer""><code>head</code></a>. You can sort in ascending or descending order which is equivalent to taking the bottom N rows.</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code># Sample data
h25 = pd.DataFrame({
    'month': ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar'],
    'impactedservice': ['Email', 'Network', 'Email', 'Network', 'Email', 'Network'],
    'resolvetime': [10, 15, 25, 5, 30, 20]
})

ISresult  = h25.groupby(['month','impactedservice']).agg({
    'resolvetime': ['count','median','mean', 'min', 'max','std']
})

print(ISresult.sort_values(by=('resolvetime', 'mean'), ascending=False).head(3))
</code></pre>
<p><code>ISresult</code> is:</p>
<pre><code>                      resolvetime                         
                            count median  mean min max std
month impactedservice                                     
Feb   Email                     1   25.0  25.0  25  25 NaN
      Network                   1    5.0   5.0   5   5 NaN
Jan   Email                     1   10.0  10.0  10  10 NaN
      Network                   1   15.0  15.0  15  15 NaN
Mar   Email                     1   30.0  30.0  30  30 NaN
      Network                   1   20.0  20.0  20  20 NaN
</code></pre>
<p>And after considering top 3 according to the mean, we get:</p>
<pre><code>                      resolvetime                         
                            count median  mean min max std
month impactedservice                                     
Mar   Email                     1   30.0  30.0  30  30 NaN
Feb   Email                     1   25.0  25.0  25  25 NaN
Mar   Network                   1   20.0  20.0  20  20 NaN
</code></pre>
","0","Answer"
"79655493","79655383","<p>This is an <a href=""https://en.wikipedia.org/wiki/Multiple_subset_sum"" rel=""nofollow noreferrer"">NP-hard</a> problem but if you do not need a perfect solution (with group sums as close as possible), a greedy approach should be a possible alternative.</p>
<p>In the below code, I sort the sum_counts from largest to smallest and then assign each to the group that currently have the smallest total sum. This will give a better result than simply cyclically assigning values.</p>
<pre class=""lang-py prettyprint-override""><code>def greedy_partition(data):
    # get group with smallest sum and add to that (largest value first)
    
    sorted_data = sorted(data, key=lambda x: x[1], reverse=True)
    
    groups = [[] for _ in range(5)]
    group_sums = [0] * 5
    
    for sid, count in sorted_data:
        min_sum_index = group_sums.index(min(group_sums))
        groups[min_sum_index].append(sid)
        group_sums[min_sum_index] += count
    
    return groups, group_sums

data = [('A', 10), ('B', 5), ('C', 8), ('D', 12), ('E', 9), ('F', 13)]

groups, group_sums = greedy_partition(data)
for i, (group, total) in enumerate(zip(groups, group_sums)):
    sids = ','.join(group)
    print(f&quot;Group{i+1}: sID={sids} | sum_count_total={total}&quot;)
</code></pre>
<p>Result for the small test data:</p>
<pre><code>Group1: sID=F | sum_count_total=13
Group2: sID=D | sum_count_total=12
Group3: sID=A | sum_count_total=10
Group4: sID=E | sum_count_total=9
Group5: sID=C,B | sum_count_total=13
</code></pre>
","0","Answer"
"79655712","79655669","<p>If I understood correctly, you need to find the indexes of the non-zero elements. I usually solve such tasks using <code>numpy.where()</code> function.</p>
<p>Consider the following dataset:</p>
<pre class=""lang-py prettyprint-override""><code>d = np.zeros((10,5))
rows = np.array([1,5,8,5])
cols = np.array([0,1,4,3])
d[rows,cols] = 0.5
df = pd.DataFrame(d,columns=[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;])
</code></pre>
<p>Where the df is all filled with zeros except few elements.</p>
<p>Finding the indexes of non-zero elements:</p>
<pre class=""lang-py prettyprint-override""><code>M = df.to_numpy()
rows, cols = np.where(M&gt;0)
</code></pre>
<p>Then extracting the dataframe row/column names:</p>
<pre class=""lang-py prettyprint-override""><code>col_names = df.columns[cols].to_list()
row_names = df.index[rows].to_list()
</code></pre>
<p>If you need to find unique pairs then you can put the col_names and row_names into a dataframe and drop duplicates:</p>
<pre><code>df = pd.DataFrame({&quot;r&quot;:row_names,&quot;c&quot;:col_names})
df_unique = df.drop_duplicates(subset = ['r','c'], keep=False)
</code></pre>
","0","Answer"
"79656133","79656093","<p>The series can be grouped by the <code>weekofyear</code> attribute</p>
<pre><code>grp = sample_series.groupby([x.weekofyear for x in sample_series.index])
# also possible
# grp = sample_series.groupby(by={i:i.weekofyear for i in range_dex})

for g in grp:
    print(g[1].size, g[1].index[0])
</code></pre>
<pre class=""lang-none prettyprint-override""><code>7 2025-01-06 00:00:00
7 2025-01-13 00:00:00
7 2025-01-20 00:00:00
4 2025-01-27 00:00:00
</code></pre>
<p><a href=""https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.Series.groupby.html"" rel=""nofollow noreferrer"">Series.groupby docs</a></p>
<blockquote>
<p>If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups</p>
</blockquote>
<p>Similar use case using a <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"" rel=""nofollow noreferrer"">frequency defined by a string</a>:</p>
<pre><code>range_dex = pd.date_range(start=&quot;2025-01-06&quot;, freq='D', periods=26)
sample_series = pd.Series(range_dex, index=range_dex)

grp = sample_series.groupby(pd.Grouper(freq='1W-MON', label='left', origin=&quot;start&quot;))

print(grp.size())
</code></pre>
<pre class=""lang-none prettyprint-override""><code>2024-12-30    1
2025-01-06    7
2025-01-13    7
2025-01-20    7
2025-01-27    4
Freq: W-MON, dtype: int64
</code></pre>
","0","Answer"
"79656189","79655383","<p>I do not recommend a greedy approach to this method. Linear programming (especially for this small of a problem) will be efficient:</p>
<pre class=""lang-py prettyprint-override""><code>import string

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.optimize import milp, LinearConstraint
import scipy.sparse as sp

n = 18  # 6 in OP example
n_groups = 5

# Synthetic data longer than OP example
rand = np.random.default_rng(seed=0)
sum_count = pd.Series(
    name='sum_count',
    index=pd.Index(
        name='sID', data=tuple(string.ascii_uppercase[:n]), dtype=pd.StringDtype(),
    ),
    data=rand.integers(low=1, high=20, size=n),
)
print(sum_count)
print()

# Optimisation variables: group min, group max, n_groups*n binary assignments of value to group
cost = np.zeros(2 + n_groups*n)
cost[:2] = -1, 1  # Minimize -min + max
integrality = np.ones(2 + n_groups*n, dtype=np.int8)  # assignments are binary
integrality[:2] = 0  # min and max are continuous
lb = np.zeros(2 + n_groups*n)
ub = np.ones(2 + n_groups*n)  # assignments are binary
lb[:2] = -np.inf  # min, max are unbounded
ub[:2] = +np.inf

# Constraint utility matrices
zero_col = sp.csc_array((n_groups, 1))
# each group subtotal = dot product of assignments and counts
group_subtotal_transform = sp.kron(
    sp.eye_array(n_groups),
    sum_count.values[np.newaxis, :],
)

# Enforce the minimum
# min &lt;= assignments dot counts: -min + assignments.counts &gt;= 0
min_constraint = LinearConstraint(
    A=sp.hstack(
    (
        np.full(shape=(n_groups, 1), fill_value=-1),  # -min
        zero_col,  # + 0*max
        group_subtotal_transform,  # + assignments.counts
        ), format='csc',
    ),
    lb=0,  # &gt;= 0
)

# Enforce the maximum
# max &gt;= assignments dot counts: max - assignments.counts &gt;= 0
max_constraint = LinearConstraint(
    A=sp.hstack(
        (
            zero_col,  # 0*min
            np.ones(shape=(n_groups, 1)),  # + max
            -group_subtotal_transform,  # - assignments.counts
        ), format='csc',
    ),
    lb=0,  # &gt;= 0
)

# The assignments must be exclusive per group
# 1 == sum(assignments) per group
exclusive_constraint = LinearConstraint(
    A=sp.hstack(
        (
            sp.csc_array((n, 2)),  # 0*min + 0*max
            sp.kron(  # + assignment sum per group
                np.ones(shape=(1, n_groups)),
                sp.eye_array(n),
            ),
        ),
        format='csc',
    ),
    lb=1, ub=1,  # == 1
)

# Depict the constraint sparsity
fig, ax = plt.subplots()
ax.set_title('Constraint sparsity pattern')
ax.imshow(
    sp.vstack((
        min_constraint.A.toarray(),
        max_constraint.A.toarray(),
        exclusive_constraint.A.toarray(),
    )).toarray().clip(-1, 1),
)

# Solve the linear program
result = milp(
    c=cost, integrality=integrality, bounds=(lb, ub),
    constraints=(min_constraint, max_constraint, exclusive_constraint),
)
assert result.success, result.message
print(result.message)

# Unpack the solution
(group_min, group_max), assign = np.split(result.x, (2,))
assign = assign.reshape((n_groups, n)).T &gt; 0.5
group_sums = sum_count @ assign
group_index = pd.RangeIndex(name='group', start=0, stop=n_groups)
sum_df = pd.DataFrame(
    index=group_index,
    data={
        'group_name': [
            ','.join(sum_count.index[col])
            for col in assign.T
        ],
        'group_sum': group_sums,
    },
)

# Print the solution
print('Assignments:')
print(pd.DataFrame(columns=group_index, index=sum_count.index, data=assign.astype(int)))
print()

print('Group min:', group_min)
print('Group max:', group_max)
print('Group sums:')
print(sum_df)

plt.show()
</code></pre>
<pre class=""lang-none prettyprint-override""><code>sID
A    17
B    13
C    10
D     6
E     6
F     1
G     2
H     1
I     4
J    16
K    13
L    18
M    10
N    12
O    19
P    14
Q    13
R    11
Name: sum_count, dtype: int64

Optimization terminated successfully. (HiGHS Status 7: Optimal)
Assignments:
group  0  1  2  3  4
sID                 
A      0  0  0  1  0
B      0  1  0  0  0
C      1  0  0  0  0
D      1  0  0  0  0
E      0  0  0  1  0
F      0  0  0  1  0
G      1  0  0  0  0
H      0  0  1  0  0
I      0  0  1  0  0
J      0  0  0  0  1
K      0  0  0  1  0
L      0  0  1  0  0
M      0  0  0  0  1
N      0  0  0  0  1
O      1  0  0  0  0
P      0  0  1  0  0
Q      0  1  0  0  0
R      0  1  0  0  0

Group min: 37.0
Group max: 38.0
Group sums:
      group_name  group_sum
group                      
0        C,D,G,O         37
1          B,Q,R         37
2        H,I,L,P         37
3        A,E,F,K         37
4          J,M,N         38
</code></pre>
<p><a href=""https://i.sstatic.net/TpsCI5aJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TpsCI5aJ.png"" alt=""sparsity"" /></a></p>
","3","Answer"
"79656695","79656645","<p>Your <code>df.loc[idx2] = df.loc[idx1].values</code> command is incorrect, since you're converting to numpy array you lose index alignment.</p>
<p>If you want to swap the (i,j) / (j,i) values (like your code attempts to do), you could just <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.swaplevel.html"" rel=""nofollow noreferrer""><code>swaplevel</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.update.html"" rel=""nofollow noreferrer""><code>update</code></a>:</p>
<pre><code>df.update(df.swaplevel())
</code></pre>
<p>Note however that this won't make (i,j) and (j,i) equal since you swapped them.</p>
<p>If you want the (i,j) and (j,i) rows to be equal, then you first need to add a rule to select only half of the combinations. For instance you could use i &lt; j (and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Index.get_level_values.html"" rel=""nofollow noreferrer""><code>Index.get_level_values</code></a>):</p>
<pre><code>m = df.index.get_level_values(0) &lt; df.index.get_level_values(1)
df.update(df[m].swaplevel())
</code></pre>
<p>Updated <code>df</code>:</p>
<pre><code>      0   1
0 1  44  47
  2  64  67
  3  67   9
1 1  83  21
  2  88  12  # updated with row (2, 1)
  3  70  88
2 1  88  12  # left unchanged
  2  58  65
  3  39  87
</code></pre>
","0","Answer"
"79657724","79657712","<p>You can use multi-level column access to resolve this issue.</p>
<p>Here is the example:</p>
<pre class=""lang-py prettyprint-override""><code>import yfinance as yf

# Download 1-minute intraday data for Airbus 
df = yf.download(&quot;AIR.DE&quot;, interval=&quot;1m&quot;, period=&quot;1d&quot;)
# For multi-level columns, you might need:
x = df.index.tolist()  # This should work for the index
y = df[('Close', 'AIR.DE')].tolist()  # Multi-level column access
</code></pre>
","2","Answer"
"79657939","79657477","<p>As you've not shown any workings, versions etc  and what you actually expect (and the problem's being thrown ..... - you need to !) , that said, (and I'm assuming a lot ... ) what you are after is something along the following (for DEMONSTRATION PURPOSES ONLY ! ). if this is wide of the mark , then you need to start filling in the blanks ... , a key point being the multi_level_index=False , as yfinance returns multi level and ta barfs at that.</p>
<p>python3.11 , yfinance: 0.2.61 macos: big sur</p>
<pre><code>cat /tmp/mardy.py
import yfinance as yf
import pandas as pd
from ta import add_all_ta_features

df = yf.download(
    'AAPL',
    start='2024-01-01', end='2025-06-06', interval='1d', auto_adjust=True,
    multi_level_index=False )

df.reset_index(inplace=True)

df_ta = add_all_ta_features( df, open=&quot;Open&quot;,
    high=&quot;High&quot;, low=&quot;Low&quot;, close=&quot;Close&quot;, volume=&quot;Volume&quot;, fillna=True)

print(df_ta[['Date', 'Close', 'momentum_rsi', 'trend_macd', 'volume_adi']].tail())

#test it

python /tmp/mardy.py 
[*********************100%***********************]  1 of 1 completed
          Date       Close  momentum_rsi  trend_macd    volume_adi
353 2025-05-30  200.850006     45.832349   -1.618094  8.422789e+08
354 2025-06-02  201.699997     46.999536   -1.548190  8.625457e+08
355 2025-06-03  203.270004     49.177876   -1.350536  8.924214e+08
356 2025-06-04  202.820007     48.561801   -1.216186  8.639840e+08
357 2025-06-05  200.630005     45.569809   -1.271767  8.203628e+08
</code></pre>
","0","Answer"
"79658161","79658047","<p>With some minor modifications (and corrections) to only use methods which return a DF then you can just chain the methods rather than repeatedly creating variables. This is also clear and concise; any one operation can be commented out for testing. So your code becomes:</p>
<pre><code>df = (df
    .dropna(how='all')            # remove empty rows
    .iloc[:-1,:]                  # remove last row
    .rename(columns= {df.columns[0]: &quot;Resource&quot;})    # change name of the first column
    .astype({&quot;Resource&quot;: int})    # change column type
    .rename(columns = lambda s: s.replace('Avg of ', '').replace('Others', 'others'))  # alter column names
    .set_index(&quot;Resource&quot;)        # use 'Resource' column as index
    .sort_index(axis=0)           # sort df by index value
    .div(100)                       # divide each entry by 100
    .round(4)                     # round to 4 decimals
    .sort_index(axis = 1)  # order columns in ascending alphabetical order
    )
</code></pre>
","0","Answer"
"79658458","79653120","<p>Schema name should be defined within Pandas <code>to_sql</code>. When trying <code>postgresql+psycopg2://user:password@host:6543/custom_schema?client_encoding=utf8</code> SQLAlchemy tried connecting to <code>custom_schema</code> which is actually a database not a schema and nothing like <code>custom_schema</code> exists. Below is the working code.</p>
<pre><code>conn_str = postgresql+psycopg2://user:password@host:6543/postgres?client_encoding=utf8
engine = create_engine(url = conn_str)
df.to_sql('tbl', engine, if_exists='append', index=False, schema='custom_schema')
</code></pre>
","0","Answer"
"79659492","79659380","<p>You can identify the row with range and transform the range into the same format as &quot;Delivery Period&quot;.</p>
<pre class=""lang-py prettyprint-override""><code>mask = df['Delivery Period'].str.contains(' to ')

# create a period range where it has date range
df.loc[mask, 'range'] = df.loc[mask]['Delivery Period'].str.split(' to ')
df.loc[mask, 'range'] = df.loc[mask].range.transform(lambda x: pd.period_range(x[0], x[1], freq='M'))

# explode the period range and format to MMM yyyy
df = df.explode('range')
df['range'] = df.range.dt.strftime('%B %Y')

# combine the new and original column 
df['Delivery Period'] = df.range.combine_first(df['Delivery Period']).drop(columns='range')
</code></pre>
","2","Answer"
"79660122","79660030","<p>I understand that you only want the first column from the table that contains specific substring, however, unless you write some code with for loops, this does not look efficient.
That is go through all data, in a top-to-bottom and left-to-right way and apply <code>in</code> operator to check whether the substring is in your data.</p>
<p>Another way to approach this problem can be using pandas.
You get all the data in a column checked using <code>your_data_series.str.contains(&quot;your_substring&quot;)</code> and apply <code>any</code> method.</p>
<pre class=""lang-py prettyprint-override""><code>for series in your_dataframe:
    if series.str.contains(&quot;your_substing&quot;).any():
        print(series.name)
        break
</code></pre>
<p>Pandas documentation for <a href=""https://pandas.pydata.org/docs/user_guide/text.html#testing-for-strings-that-match-or-contain-a-pattern"" rel=""nofollow noreferrer"">Testing for strings that match or contain a pattern</a></p>
","0","Answer"
"79660204","79660030","<p>This is probably not the most efficient way of doing this, but it works.</p>
<pre class=""lang-py prettyprint-override""><code>manager_cols = [col for col in df.columns if col.startswith('manager_id')]

def find_manager(row):
    for col in manager_cols:
        if 'FR' in str(row[col]):
            return col
    return None

df['manager'] = df.apply(find_manager, axis=1)
</code></pre>
","0","Answer"
"79660227","79660030","<p>First, find all columns that are relevant, i.e. start <code>manager_id</code>:</p>
<pre class=""lang-py prettyprint-override""><code>manager_columns = [col for col in df.columns if col.startswith('manager_id')]
</code></pre>
<p>Now find your substring within your selection and return the first column value that satisfies the condition:</p>
<pre class=""lang-py prettyprint-override""><code>df['result'] = df[manager_columns].apply(lambda row: next((val for val in row if 'FR' in val), None), axis=1)
</code></pre>
","0","Answer"
"79660235","79660030","<p>If you want to find the first &quot;manager_id&quot; column that contains &quot;FR&quot; for each row, then select the &quot;manager_id&quot; columns with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.filter.html"" rel=""nofollow noreferrer""><code>filter</code></a>, then <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a> <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html"" rel=""nofollow noreferrer""><code>str.contains</code></a> per column, finally identify the first match with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html"" rel=""nofollow noreferrer""><code>idxmax</code></a>+<a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.any.html"" rel=""nofollow noreferrer""><code>any</code></a>:</p>
<pre><code>tmp = df.filter(like='manager_id').apply(lambda c: c.str.contains('FR'))
df['result'] = tmp.idxmax(axis=1).where(tmp.any(axis=1))
</code></pre>
<p>Output:</p>
<pre><code>    id    name rank manager_id manager_id_1 manager_id_2 manager_id_3        result
0  xx1  namexx    T     xx2-TR       xx3-FR       xx4-FR      xx22-ER  manager_id_1
1  xx2  namexx    T     xx2-TR       xx3-HR       xx4-GR      xx22-ER           NaN
</code></pre>
<p><em>NB. if you want to select all columns including and after &quot;manager_id&quot;, irrespective of the presence of &quot;manager_id&quot;, replace the <code>df.filter(...)</code> with <code>df.loc[:, 'manager_id':]</code>.</em></p>
<p>Intermediate <code>tmp</code>:</p>
<pre><code>   manager_id  manager_id_1  manager_id_2  manager_id_3
0       False          True          True         False
1       False         False         False         False
</code></pre>
","1","Answer"
"79661012","79657102","<p>The reason of <code>KeyError: 'impactedservice,priority'</code> comes in when the specified column/row key is not found in the the data frame header</p>
<pre><code>result2025 = df.groupby(['impactedservice','priority'],as_index=False).agg({'resolvetime': ['count','mean']})
</code></pre>
<p>And looking at this condition I assume you have 2 separate columns, so you would need to modify the cohort to a list to make it work</p>
<pre><code>cohort = ['impactedservice','priority']
result2025 = df.groupby(cohort,as_index=False).agg({'resolvetime': ['count','mean']})
</code></pre>
","0","Answer"
"79661282","79661119","<p>Define an variant onf <code>func</code> that tells us about the argument, what apply is giving it:</p>
<pre><code>In [274]: def func1(a):
     ...:   print(a, type(a), a.shape)
     ...:   if a % 3 == 0: return 1
     ...:   if a % 3 == 1: return 0
     ...:   else: return None
     ...: 
</code></pre>
<p>With an <code>arr</code> that returns object dtype array:</p>
<pre><code>In [276]: arr = np.array([[2],[1], [3], [4], [5], [6]])

In [277]: np.apply_along_axis(func1, 1, arr)
[2] &lt;class 'numpy.ndarray'&gt; (1,)
[1] &lt;class 'numpy.ndarray'&gt; (1,)
[3] &lt;class 'numpy.ndarray'&gt; (1,)
[4] &lt;class 'numpy.ndarray'&gt; (1,)
[5] &lt;class 'numpy.ndarray'&gt; (1,)
[6] &lt;class 'numpy.ndarray'&gt; (1,)
Out[277]: array([None, 0, 1, 0, None, 1], dtype=object)
</code></pre>
<p>The equivalent action with a list comprehension:</p>
<pre><code>In [278]: [func1(a) for a in arr]
[2] &lt;class 'numpy.ndarray'&gt; (1,)
[1] &lt;class 'numpy.ndarray'&gt; (1,)
[3] &lt;class 'numpy.ndarray'&gt; (1,)
[4] &lt;class 'numpy.ndarray'&gt; (1,)
[5] &lt;class 'numpy.ndarray'&gt; (1,)
[6] &lt;class 'numpy.ndarray'&gt; (1,)
Out[278]: [None, 0, 1, 0, None, 1]
</code></pre>
<p>An object dtype array is no better than a list.</p>
<p>Your function will only work with size 1 arrays.  With more, the <code>if</code> line raises an error:</p>
<pre><code>In [279]: func1(np.array([1,2]))
[1 2] &lt;class 'numpy.ndarray'&gt; (2,)

ValueError                                Traceback (most recent call last)
Cell In[279], line 1
----&gt; 1 func1(np.array([1,2]))

Cell In[274], line 3, in func1(a)
      1 def func1(a):
      2   print(a, type(a), a.shape)
----&gt; 3   if a % 3 == 0: return 1
      4   if a % 3 == 1: return 0
      5   else: return None

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>
<p>As for times, the plain list comprehension is much faster</p>
<pre><code>In [280]: timeit np.apply_along_axis(func, 1, arr)
212 μs ± 791 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [281]: timeit [func(a) for a in arr]
77.9 μs ± 247 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
</code></pre>
<p>The apply would be convenient if <code>arr</code> is a (2,3,1) array</p>
<pre><code>In [282]: np.apply_along_axis(func1, 2, arr.reshape(2,3,1))
[2] &lt;class 'numpy.ndarray'&gt; (1,)
[1] &lt;class 'numpy.ndarray'&gt; (1,)
[3] &lt;class 'numpy.ndarray'&gt; (1,)
[4] &lt;class 'numpy.ndarray'&gt; (1,)
[5] &lt;class 'numpy.ndarray'&gt; (1,)
[6] &lt;class 'numpy.ndarray'&gt; (1,)
Out[282]: 
array([[None, 0, 1],
       [0, None, 1]], dtype=object)
</code></pre>
<p>Since the function only works with scalar or size 1 arguments, we could also use <code>np.vectorize</code>:</p>
<pre><code>In [284]: np.vectorize(func, otypes=[object])(arr)
Out[284]: 
array([[None],
       [0],
       [1],
       [0],
       [None],
       [1]], dtype=object)

In [285]: timeit np.vectorize(func, otypes=[object])(arr)
35.5 μs ± 126 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
</code></pre>
<h2>edit</h2>
<p>Using your <code>pandas</code> call, and my <code>func1</code></p>
<pre class=""lang-none prettyprint-override""><code>1 &lt;class 'numpy.int64'&gt; ()
2 &lt;class 'numpy.int64'&gt; ()
---------
TypeError         
...
</code></pre>
<p>As docs, with <code>raw</code>, the function gets a <code>ndarray</code> (even if you pass one element of the row: <code>row[0]</code>).  Note that the shape is <code>()</code>, scalar.</p>
<p><code>df.apply(func1, axis=1, raw=True)</code> passes shape <code>(1,)</code> arrays to the function.</p>
<p>And the error stack does show that it's having problems with assigning the <code>None</code> result to a <code>int</code> dtype array:</p>
<pre><code>File ~\miniconda3\envs\mypy\Lib\site-packages\numpy\lib\_shape_base_impl.py:409, in apply_along_axis(func1d, axis, arr, *args, **kwargs)
    407 buff[ind0] = res
    408 for ind in inds:
--&gt; 409     buff[ind] = asanyarray(func1d(inarr_view[ind], *args, **kwargs))
    411 res = transpose(buff, buff_permute)
    412 return conv.wrap(res)
</code></pre>
<p>E.G. assigning <code>None</code> to a simple int array produces this error:</p>
<pre><code>In [304]: x = np.zeros(3, dtype=int); x
Out[304]: array([0, 0, 0])
In [305]: x[0]=1
In [306]: x[1] = None
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[306], line 1
----&gt; 1 x[1] = None

TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'
</code></pre>
","1","Answer"
"79662396","79653120","<blockquote>
<p>Sample connection string: <code>postgresql+psycopg2://user:password@host:6543/custom_schema?client_encoding=utf8</code></p>
<p>After replacing custom_schema with <code>postgres</code> it works fine.</p>
</blockquote>
<p><strong>Database != schema</strong>. <code>postgres</code> is a database name(default database).</p>
<p>The connection string should include both <strong>database name</strong> and <strong>custom schema name</strong>:</p>
<pre><code>postgresql+psycopg2://user:password@host:6543/&lt;db_name&gt;?currentSchema=&lt;schema_name&gt;&amp;client_encoding=utf8
</code></pre>
<hr />
<p>System views to verify available databases and schemata:</p>
<pre><code>SELECT datname FROM pg_database;

SELECT * FROM INFORMATION_SCHEMA.SCHEMATA;
</code></pre>
<p><strong><a href=""https://dbfiddle.uk/AOQ6A_NN"" rel=""nofollow noreferrer"">db&lt;&gt;fiddle demo</a></strong></p>
","1","Answer"
"79662515","79662505","<pre class=""lang-py prettyprint-override""><code>import numpy as np
plt.set_xticks(np.arange(1900, 2020, 5))
</code></pre>
<p>np.arange has parameters (min value, max value, distance between tick marks). This should do the trick.</p>
","3","Answer"
"79662906","79662638","<p>Based on your example, I would suggest something along these lines.</p>
<pre><code>import numpy as np
import scipy
import pandas as pd
from graph_tool.all import *

df = pd.DataFrame({'p1': [1, 1, 2, 2, 2, 2, 3, 3, 3, 3], 
                   'p2': [2, 4, 3, 4, 5, 14, 4, 5, 14, 17]})
rows = df['p1']

def get_sparse_symmetric_adjacency_matrix(df, col1, col2):
    # Get col1 and col2 as NumPy arrays
    rows, cols = df[col1].values, df[col2].values
    # Floating point types will not work here
    assert pd.api.types.is_integer_dtype(rows), &quot;col1 must have integer type&quot;
    assert pd.api.types.is_integer_dtype(cols), &quot;col2 must have integer type&quot;
    # Calculate size of matrix. If this is not provided, SciPy will only make
    # the matrix large enough to contain the values that are present
    num_nodes = max(np.max(rows), np.max(cols)) + 1
    # All values that are present in the matrix are set to one
    ones = np.ones_like(rows)
    # Create matrix
    # Use col1 and col2 as index into sparse matrix
    sparse = scipy.sparse.coo_matrix((ones, (rows, cols)), shape=(num_nodes, num_nodes))
    # Make matrix symmetric
    sparse = sparse + sparse.T
    # Remove duplicates created by transpose (e.g. if you had both 2-&gt;1 and 1-&gt;2 in
    # original list
    sparse = (sparse &gt;= 1).astype('int8')
    return sparse


new_sparse_matrix = get_sparse_symmetric_adjacency_matrix(df, 'p1', 'p2')
new_g = Graph(new_sparse_matrix, directed=False)
print(new_sparse_matrix.toarray())
graph_draw(new_g,vertex_text=new_g.vertex_index)
</code></pre>
<p>Explanation:</p>
<ul>
<li>An edge list graph representation is nearly equivalent to the way that a COO adjacency matrix is represented. The COO matrix is represented by the coordinates within the matrix of each data value, and the data value. This makes it very fast to create.</li>
<li>We need to make the matrix symmetric. I do this by taking the transpose of the matrix. Technically, this step is not required for non-directed graphs if you can assure that the element in the p1 column is always less than the element in the p2 column.</li>
<li>I also remove duplicates. Arguably not necessary if you don't have duplicate edges in your original edge list.</li>
</ul>
<p>Graph this draws:</p>
<p><a href=""https://i.sstatic.net/AJbqE1p8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJbqE1p8.png"" alt=""graph"" /></a></p>
","1","Answer"
"79663450","79660430","<p>I think you should <strong>disable writing headers again</strong> since your Excel file already contains them:<br />
<code>df.to_excel(writer, sheet_name='Sheet1', startcol=3, startrow=1, index=False, header=False)</code></p>
<p>This way, only the values from <code>data.csv</code> will be written starting at cell D2 without inserting a new row of headers and triggering version suffixes like <code>.1</code>, <code>.2</code>, etc.</p>
","0","Answer"
"79664839","79664817","<p>A possible solution:</p>
<pre><code>df.set_index(['D1', 'D2'])['Value'].unstack(fill_value=0).reindex(
    index=['A', 'C', 'E'],
    columns=['B', 'D', 'F'],
    fill_value=0
).to_numpy()
</code></pre>
<p>This works by first using <code>df.set_index(['D1', 'D2'])</code> (<a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a>) to transform the <code>D1</code> and <code>D2</code> columns into a hierarchical <code>MultiIndex</code>, preparing the data for reshaping. Then, we select the <code>Value</code> column to work with the relevant data, and <code>unstack(fill_value=0)</code> (<a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unstack.html"" rel=""nofollow noreferrer""><code>unstack</code></a>) is called on this resulting <code>Series</code>. This <code>unstack</code> operation pivots the innermost level of the <code>MultiIndex</code> (<code>D2</code>) from rows into new columns, effectively creating the 2D matrix structure and automatically filling any non-existent cells with <code>0</code>. Finally, <code>reindex(index=['A', 'C', 'E'], columns=['B', 'D', 'F'], fill_value=0)</code> (<a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>reindex</code></a>) is applied to ensure the rows and columns are in the order specified (<code>A/C/E</code> for rows, <code>B/D/F</code> for columns), and any values not present are also filled with <code>0</code>, before <code>.to_numpy()</code> converts to a numpy array.</p>
<p>Output:</p>
<pre><code>array([[1, 1, 0],
       [0, 2, 0],
       [4, 0, 3]])
</code></pre>
","2","Answer"
"79664880","79664817","<p>Your input table looks like effect of melt-ing DataFrame, therefore I propose to unmelt it, i.e. pivoting following way, let <code>file.csv</code> content be</p>
<pre><code>D1,D2,Value
A,B,1
C,D,2
E,F,3
A,D,1
E,B,4
</code></pre>
<p>then</p>
<pre><code>import pandas as pd
df = pd.read_csv('file.csv')
arr = df.pivot(index='D1', columns='D2').to_numpy()
print(arr)
</code></pre>
<p>gives output</p>
<pre><code>[[ 1.  1. nan]
 [nan  2. nan]
 [ 4. nan  3.]]
</code></pre>
<p>Note that <code>arr</code> now hold floats (hence trailing dots), so if you need to have integer value, remember to convert it (e.g. <code>int(arr[1][1])</code> will give 2)</p>
<p><em>(tested in pandas 2.1.4)</em></p>
","1","Answer"
"79665026","79664986","<p>This should do what you want and print the results:</p>
<pre><code>lst = [
    [&quot;abc&quot;],
    [&quot;START&quot;],
    [&quot;cdef&quot;],
    [&quot;START&quot;],
    [&quot;fhg&quot;],
    [&quot;cdef&quot;],
]


l = []
intermediate = []
to_start_adding = False
i = 0
for item in lst:
    if to_start_adding:
        intermediate.append(item)
    if item == [&quot;START&quot;]:
        to_start_adding = True
        intermediate = [item]
    if lst[min(i+1,len(lst)-1)] == [&quot;START&quot;] or i+1 == len(lst):
        if intermediate != []:
            print(intermediate)
    i += 1
</code></pre>
","0","Answer"
"79665033","79664986","<p>if you want to use pandas you could create a <code>boolean mask</code> to identify the occurences of <code>START</code> . Then take the <code>cumsum()</code> to assign a unique group number to each occurrence. Then <code>groupby</code> the group number, excluding all groups before the first occurrence of <code>START</code> :</p>
<pre><code>import pandas as pd
import numpy as np

lst = [
    [&quot;abc&quot;],
    [&quot;START&quot;],
    [&quot;cdef&quot;],
    [&quot;START&quot;],
    [&quot;fhg&quot;],
    [&quot;cdef&quot;],
]

df = pd.DataFrame(lst, columns=['Input'])

#create boolean mask
mask = df['Input'].eq('START')

#Intermediate Result
0    False
1     True
2    False
3     True
4    False
5    False
Name: Input, dtype: bool



#assign group number to each occurrence of start
df['Group'] = mask.cumsum()

#Intermediate Result
 Input  Group
0    abc      0
1  START      1
2   cdef      1
3  START      2
4    fhg      2
5   cdef      2




#create list for each group in groupby excluding groups before the 
#first occurrence of 'START'
grouped_lists = [group['Input'].tolist() for _, group in df[df['Group'] &gt; 0].groupby('Group')]



print(grouped_lists)
[['START', 'cdef'], ['START', 'fhg', 'cdef']]
</code></pre>
","0","Answer"
"79665091","79665073","<p>To convert your header and data in numpy arrays you can do the following:</p>
<pre><code>df = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6],
    'C': [7, 8, 9]
})

# Extract headers as a NumPy array
headers = df.columns.to_numpy()

# Extract data as a NumPy array
data = df.to_numpy()

print(&quot;Headers:&quot;, headers)
Headers: ['A' 'B' 'C']

print(&quot;Data:\n&quot;, data)
Data:
 [[1 4 7]
 [2 5 8]
 [3 6 9]]
</code></pre>
","2","Answer"
"79665098","79664986","<p>I would do something like this:</p>
<pre><code>lst = [
    [&quot;abc&quot;],
    [&quot;START&quot;],
    [&quot;cdef&quot;],
    [&quot;START&quot;],
    [&quot;fhg&quot;],
    [&quot;cdef&quot;],
]

# build list of positions of &quot;START&quot; in your list
w = [_  for _, v in enumerate(lst) if v[0] == &quot;START&quot;]

for i in w:
    # if there is next value in the list, we print everything between i and next value
    if len(w)-1 &gt; w.index(i):
        print(lst[i:w[w.index(i)+1]])
    else:
    # otherwise we go all the way to the end of the list
        print(lst[i:])
</code></pre>
","2","Answer"
"79665133","79656511","<p>The mechanics of <code>apply()</code>:</p>
<ul>
<li>When using <code>apply()</code>, the row parameter is a Series object</li>
<li>By default, <code>axis=0</code> processes columns. You need <code>axis=1</code> for rows</li>
<li>The Series index is your DataFrame's column names</li>
</ul>
<p>Here's the corrected code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import datetime as dt

df = pd.DataFrame(
   [ [
      dt.date(2025,6,5), dt.date(2025,6,6) ],[
      dt.date(2025,6,7), dt.date(2025,6,8) ]
   ],
   columns=['A','B'], index=['Row1','Row2']
)

# Correct way to use apply
df['MaxDate'] = df.apply(lambda row: max(row), axis=1)

# Or more simply
df['MaxDate'] = df[['A', 'B']].max(axis=1)
</code></pre>
","1","Answer"
"79665498","79663044","<p>An improvment you could make for step 2 is to perform the VWAP by doing vectorized sums on the volume and volume * price fields. This is generally faster than DataFrameGroupBy.apply.</p>
<p>I measure this as being about 20x faster than your current step 2, and produces results that are within about 1e-14 of your original approach.</p>
<pre><code>vwap = (trades
    .assign(Trade_Price_Times_Volume=trades['Trade_Price']*trades['Trade_Volume'])
    .groupby([pd.Grouper(key='Time', freq='1min'), 'Symbol'])
    .agg({'Trade_Price_Times_Volume':'sum', 'Trade_Volume':'sum'})
    .assign(Trade_Price=lambda x: x['Trade_Price_Times_Volume'] / x['Trade_Volume'])
    .drop(columns=['Trade_Price_Times_Volume', 'Trade_Volume'])
    .unstack()
    .ffill()
    .droplevel(0, axis=1)
)
</code></pre>
","0","Answer"
"79665512","79665487","<p>You're having this problem because your program can't locate the path to your cert file.</p>
<p>To solve it, you can export the following env var and run your program.</p>
<pre class=""lang-bash prettyprint-override""><code>export SSL_CERT_FILE=$(python3 -m certifi)

python3 your_script.py
</code></pre>
<p>Or</p>
<pre class=""lang-py prettyprint-override""><code>import certifi
os.environ[&quot;SSL_CERT_FILE&quot;] = certifi.where()
</code></pre>
","0","Answer"
"79665578","79665533","<p>The <code>SettingsWithCopyWorning</code> in pandas, when you using <code>.loc</code>, occurred because you are modifying a copy of <code>dataframe</code>.
And you didn't use the view of the original. So this warning is designed to alert you to the possibility that your changes may not reflected in the original <code>dataframe</code> as you might intend.
So you have to explicitly create copy of the slice. It also tells pandas that you are intentionally working with a new <code>dataframe</code> and silences the warning.</p>
<pre><code>df = pd.DataFrame({'A' : [1, 2, 3], 'B': [4, ,5, 6]})
sub_df = df.loc[df.A &gt; 1].copy()
sub_df.loc[:, 'C'] = sub_df['A'] + sub_df['B']
</code></pre>
","1","Answer"
"79665756","79665475","<p>Welcome to SO!</p>
<p>The problem is that you are not submitting enough data to the server, the server will evaluate this as an incorrect request and send you the home page. You can verify this by saving the response to a file and opening it in a browser.</p>
<p>So I opened developer tools and copied the exact same request that the web page sends and here is my solution, using <code>bs4</code> and <code>requests</code>. Suggestion: add datetime to format start and end dates automatically.</p>
<pre class=""lang-py prettyprint-override""><code>import requests
from bs4 import BeautifulSoup
import pandas as pd
from io import StringIO

session = requests.Session()

response = session.get('https://www.cocorahs.org/ViewData/StationPrecipSummary.aspx')

soup = BeautifulSoup(response.content, &quot;html.parser&quot;)
view_state = soup.find(&quot;input&quot;, {&quot;name&quot;: &quot;__VIEWSTATE&quot;, &quot;value&quot;: True})[&quot;value&quot;]
view_state_generator = soup.find(&quot;input&quot;, {&quot;name&quot;: &quot;__VIEWSTATEGENERATOR&quot;, &quot;value&quot;: True})[&quot;value&quot;]
event_validation = soup.find(&quot;input&quot;, {&quot;name&quot;: &quot;__EVENTVALIDATION&quot;, &quot;value&quot;: True})[&quot;value&quot;]

response = session.post('https://www.cocorahs.org/ViewData/StationPrecipSummary.aspx', data={
    &quot;__EVENTTARGET&quot;: &quot;&quot;,
    &quot;__EVENTARGUMENT&quot;: &quot;&quot;,
    &quot;__LASTFOCUS&quot;: &quot;&quot;,
    &quot;VAM_Group&quot;: &quot;&quot;,
    &quot;__VIEWSTATE&quot;: view_state,
    &quot;VAM_JSE&quot;: &quot;1&quot;,
    &quot;__VIEWSTATEGENERATOR&quot;: view_state_generator,
    &quot;__EVENTVALIDATION&quot;: event_validation,
    &quot;obsSwitcher:ddlObsUnits&quot;: &quot;usunits&quot;,
    &quot;tbStation1&quot;: &quot;MD-BL-13&quot;,
    &quot;tbStation2&quot;: &quot;&quot;,
    &quot;tbStation3&quot;: &quot;&quot;,
    &quot;ucDateRangeFilter:dcStartDate:di&quot;: &quot;8/1/2019&quot;,
    &quot;ucDateRangeFilter:dcStartDate:hfDate&quot;: &quot;2019-08-01&quot;,
    &quot;ucDateRangeFilter:dcEndDate:di&quot;: &quot;8/10/2019&quot;,
    &quot;ucDateRangeFilter:dcEndDate:hfDate&quot;: &quot;2019-08-10&quot;,
    &quot;btnSubmit&quot;: &quot;Get+Summary&quot;,
})

table = BeautifulSoup(response.content, &quot;html.parser&quot;).find(&quot;table&quot;, id=&quot;dgReports&quot;)

if table is None:
    raise RuntimeError(&quot;table#dgReports not found&quot;)

df = pd.read_html(StringIO(str(table)))[0]

print(df)
</code></pre>
","0","Answer"
"79665904","79665899","<p>I needed to use <code>axis=1</code>:</p>
<pre><code>df_dm.dropna(axis=1, how=&quot;all&quot;, inplace=True)
</code></pre>
<p>I was only dropping rows with all Nans since <code>axis=0</code> is the standard.</p>
","0","Answer"
"79665914","79662505","<p>The course is doing a bad job of teaching you how to work with time series data. Use actual datetimes, not integers; use <code>resample</code> instead of <code>agg</code>.</p>
<p>You <em>can</em> change the tick frequency (though you <em>shouldn't</em> change the tick frequency in this case because for most scales five years is an illegible mess).</p>
<pre class=""lang-py prettyprint-override""><code>import matplotlib
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

# Random datetimes through 2020
rand = np.random.default_rng(seed=0)
dates = pd.DatetimeIndex(
    name='date',
    data=np.sort(
        rand.integers(
            low=np.datetime64('1900-01-01', 'D').astype(np.int64),
            high=np.datetime64('2025-01-01', 'D').astype(np.int64), size=150,
        ).astype('datetime64[D]')
    ),
)

df_data = pd.DataFrame(
    index=dates,
    data={
        'prize': rand.uniform(low=10_000, high=50_000, size=dates.size),
    },
)
prize_year = df_data['prize'].resample('YS').count()

fig, ax = plt.subplots()
ax.scatter(prize_year.index, prize_year)
ax.xaxis.set_major_locator(matplotlib.dates.YearLocator(base=5))
ax.set_xlim(
    left=np.datetime64('1900-01-01', 'D'),
    right=np.datetime64('2020-01-01', 'D'),
)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/3A3kr7lD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3A3kr7lD.png"" alt=""bad graph"" /></a></p>
","0","Answer"
"79666080","79665790","<p>This is a workaround, but it may solve your problem. The idea is to replace the NaN value of <code>related_player_id</code> columns by -1 (rarely used as an SQL key).</p>
<p>Here is one simple python code to do that:</p>
<pre class=""lang-py prettyprint-override""><code>player_match_event_df[&quot;related_player_id&quot;] = player_match_event_df[&quot;related_player_id&quot;].fillna(-1)
player_match_event_df[&quot;related_player_id&quot;] = player_match_event_df[&quot;related_player_id&quot;].astype('Int64')
</code></pre>
<p>If you really need to have null values in <code>related_player_id</code> column in the SQL table use the following PostgreSQL query:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
player_match_event_id, 
player_id, 
CASE WHEN related_player_id = -1 THEN NULL 
ELSE related_player_id
END AS related_player_id,
match_event_id
FROM player_match_event
</code></pre>
<p>Make sure that none of the value of <code>related_player_id</code> is equal to -1. If it is the case just replace -1 by another value.</p>
","0","Answer"
"79666081","79665790","<blockquote>
<p>I think the issue must be with the fact it is of type float and contains NaN values.</p>
</blockquote>
<p>That's correct (and reproducible): <a href=""https://dbfiddle.uk/5d2awRiZ"" rel=""nofollow noreferrer""><sub>demo at db&lt;&gt;fiddle</sub></a></p>
<pre class=""lang-sql prettyprint-override""><code>create table test(id bigint);
insert into test select 'NaN'::float;
</code></pre>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>ERROR:  bigint out of range
</code></pre>
</blockquote>
<p>If it's db-first, <code>player_match_event_id</code> and <code>player_id</code> should reduce from <code>int64</code> to <code>int32</code> and <code>Int64</code> with <code>NA</code>'s/<code>None</code>'s instead of <code>NaN</code>'s should work fine for <code>related_player_id</code>. Your idea with <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html"" rel=""nofollow noreferrer""><code>df.astype()</code></a> is good but I think you confused your dataframes, applying that cast on one thing (<code>player_match_event_df</code>) but later inspected and tried to write another, unchanged dataframe to the database (<code>df</code>) - assuming it's not just in the snippets you showed.</p>
<p>This:</p>
<blockquote>
<p>I try to load this to the database, using execute_values I get columns and values this way</p>
</blockquote>
<p>Shouldn't be necessary. There's a <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html"" rel=""nofollow noreferrer""><code>df.to_sql()</code></a> with <code>method='multi'</code> that can handle this for you. You can also build a faster method <a href=""https://hakibenita.com/fast-load-data-python-postgresql#results-summary"" rel=""nofollow noreferrer"">using Postgres' <code>copy</code> protocol</a>, and use that, or <a href=""https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method"" rel=""nofollow noreferrer"">pass it as a callable</a>.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from math import nan
df = pd.DataFrame({ &quot;match_event_id&quot;: [150573493,150573495,150573550,150573587,150573592,150573591,150573605,150573625,150573634,150573635,150573633,150573672,150573646,150573648,150573681,150573689 ]
                   ,&quot;player_id&quot;: [37623454,787556,37736097,978873,323669,191273,73555,191273,29763093,320908,320715,37677530,820,12447486,37677330,220042 ]
                   ,&quot;related_player_id&quot;: [nan,nan,nan,37677332.0,37757705.0,788849.0,787556.0,32113.0,35095.0,320430.0,37493565.0,12844068.0,37425064.0,73555.0,29723379.0,nan] } )
#this really is enough
df['related_player_id'] = df['related_player_id'].astype(pd.Int64Dtype()) 

from sqlalchemy.dialects.postgresql import (BIGINT,INTEGER)
from sqlalchemy import create_engine
engine = create_engine(&quot;postgresql+psycopg2://your_db_username:your_db_pass@localhost/your_db_name&quot;)
with engine.connect() as conn:
    df.to_sql( name='your_target_table'
              ,con=conn
              ,if_exists='append'#or 'fail' or 'replace'
              ,dtype={ 'match_event_id': INTEGER
                      ,'player_id': INTEGER
                      ,'related_player_id': BIGINT }
              ,method='multi' )
</code></pre>
<hr />
<p>If it's app-first, I don't think your mapping is correct. That Python-side <code>int64</code> type used for <code>match_event_id</code> and <code>player_id</code> is <a href=""https://www.postgresql.org/docs/current/datatype-numeric.html#DATATYPE-NUMERIC-TABLE"" rel=""nofollow noreferrer""><code>bigint</code> in Postgres</a>, not a regular <code>int</code>: another name for a regular <code>int</code> is <code>int4</code> (4 bytes, 32 bits), and <code>bigint</code> is also known as <code>int8</code> (8 bytes, 64 bits) - hence, Postgres' <code>int8</code> is Python's <code>int64</code>.</p>
<p>Regular <code>float</code>/<code>double precision</code> type in Postgres is already 8 bytes, so <code>float64</code> values in <code>related_player_id</code> field should directly map to just <code>float</code> in Postgres.</p>
<p>You can update your target table:</p>
<pre class=""lang-sql prettyprint-override""><code>alter table player_match_event
  alter column match_event_id type bigint
 ,alter column player_id type bigint
 ,alter column related_player_id type float;
</code></pre>
<p>But seeing that you have foreign keys in place, you'll need to cascade this change to other tables as well.</p>
","0","Answer"
"79666135","79665790","<p><strong>Here is the whole code</strong></p>
<p>First of all, I converted your data into a CSV file named (TestData.csv) for creating the DataFrame. You can also do it in different way.
I think, comments in the complete code are enough for understanding. I avoid Not Available Number (NaN) data(related_player_id)  conversion by uploading is as null in postgreSQL.</p>
<p>let's see the code.</p>
<pre><code>import pandas as pd
from sqlalchemy import create_engine
import psycopg2

#Create the DataFrame
data = pd.read_csv('e:/learning/python/testData.txt')

# Create connection with postgreSQL using psycopg2
conn = psycopg2.connect(
    host=&quot;localhost&quot;,
    port=&quot;5432&quot;,
    database=&quot;DB_Learning&quot;,
    user=&quot;postgres&quot;,
    password=&quot;****&quot;
)
cur = conn.cursor()
#looping throughout the DataFrame for inserting the records
for _, row in data.iterrows():
    cur.execute(&quot;&quot;&quot;
        INSERT INTO player_match_event (match_event_id, player_id, related_player_id)
        VALUES (%s, %s, %s)
    &quot;&quot;&quot;, (
        int(row['match_event_id']),
        int(row['player_id']),
        None if pd.isna(row['related_player_id']) else int(row['related_player_id']) #This line will manage NaN values 
    ))

# Commit the changes and close
conn.commit()
cur.close()
conn.close()
</code></pre>
<p>Please knock me if there are thing for understanding.
Thank you.</p>
","0","Answer"